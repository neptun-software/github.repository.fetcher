{
  "metadata": {
    "timestamp": 1736559500301,
    "page": 77,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Plachtaa/VITS-fast-fine-tuning",
      "stars": 4792,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": "DATA.MD",
          "type": "blob",
          "size": 2.1767578125,
          "content": "本仓库的pipeline支持多种声音样本上传方式，您只需根据您所持有的样本选择任意一种或其中几种即可。  \n\n1.`.zip`文件打包的，按角色名排列的短音频，该压缩文件结构应如下所示：  \n```\nYour-zip-file.zip\n├───Character_name_1\n├   ├───xxx.wav\n├   ├───...\n├   ├───yyy.mp3\n├   └───zzz.wav\n├───Character_name_2\n├   ├───xxx.wav\n├   ├───...\n├   ├───yyy.mp3\n├   └───zzz.wav\n├───...\n├\n└───Character_name_n\n    ├───xxx.wav\n    ├───...\n    ├───yyy.mp3\n    └───zzz.wav\n```\n注意音频的格式和名称都不重要，只要它们是音频文件。  \n质量要求：2秒以上，10秒以内，尽量不要有背景噪音。  \n数量要求：一个角色至少10条，最好每个角色20条以上。  \n2. 以角色名命名的长音频文件，音频内只能有单说话人，背景音会被自动去除。命名格式为：`{CharacterName}_{random_number}.wav`  \n(例如：`Diana_234135.wav`, `MinatoAqua_234252.wav`)，必须是`.wav`文件，长度要在20分钟以内（否则会内存不足）。  \n\n3. 以角色名命名的长视频文件，视频内只能有单说话人，背景音会被自动去除。命名格式为：`{CharacterName}_{random_number}.mp4`  \n(例如：`Taffy_332452.mp4`, `Dingzhen_957315.mp4`)，必须是`.mp4`文件，长度要在20分钟以内（否则会内存不足）。  \n注意：命名中，`CharacterName`必须是英文字符，`random_number`是为了区分同一个角色的多个文件，必须要添加，该数字可以为0~999999之间的任意整数。   \n\n4. 包含多行`{CharacterName}|{video_url}`的`.txt`文件，格式应如下所示：\n```\nChar1|https://xyz.com/video1/\nChar2|https://xyz.com/video2/\nChar2|https://xyz.com/video3/\nChar3|https://xyz.com/video4/\n```\n视频内只能有单说话人，背景音会被自动去除。目前仅支持来自bilibili的视频，其它网站视频的url还没测试过。  \n若对格式有疑问，可以在[这里](https://drive.google.com/file/d/132l97zjanpoPY4daLgqXoM7HKXPRbS84/view?usp=sharing)找到所有格式对应的数据样本。\n"
        },
        {
          "name": "DATA_EN.MD",
          "type": "blob",
          "size": 2.2099609375,
          "content": "The pipeline of this repo supports multiple voice uploading options，you can choose one or more options depending on the data you have.\n\n1. Short audios packed by a single `.zip` file, whose file structure should be as shown below:\n```\nYour-zip-file.zip\n├───Character_name_1\n├   ├───xxx.wav\n├   ├───...\n├   ├───yyy.mp3\n├   └───zzz.wav\n├───Character_name_2\n├   ├───xxx.wav\n├   ├───...\n├   ├───yyy.mp3\n├   └───zzz.wav\n├───...\n├\n└───Character_name_n\n    ├───xxx.wav\n    ├───...\n    ├───yyy.mp3\n    └───zzz.wav\n```\nNote that the format of the audio files does not matter as long as they are audio files。  \nQuality requirement: >=2s, <=10s, contain as little background sound as possible.   \nQuantity requirement: at least 10 per character, 20+ per character is recommended.\n\n2. Long audio files named by character names, which should contain single character voice only. Background sound is \nacceptable since they will be automatically removed. File name format `{CharacterName}_{random_number}.wav`  \n(E.G. `Diana_234135.wav`, `MinatoAqua_234252.wav`), must be `.wav` files.  \n  \n\n3. Long video files named by character names, which should contain single character voice only. Background sound is \nacceptable since they will be automatically removed. File name format `{CharacterName}_{random_number}.mp4`  \n(E.G. `Taffy_332452.mp4`, `Dingzhen_957315.mp4`), must be `.mp4` files.  \nNote: `CharacterName` must be English characters only, `random_number` is to identify multiple files for one character,\nwhich is compulsory to add. It could be a random integer between 0~999999.\n\n4. A `.txt` containing multiple lines of`{CharacterName}|{video_url}`, which should be formatted as follows:\n```\nChar1|https://xyz.com/video1/\nChar2|https://xyz.com/video2/\nChar2|https://xyz.com/video3/\nChar3|https://xyz.com/video4/\n```\nOne video should contain single speaker only. Currently supports videos links from bilibili, other websites are yet to be tested.\nHaving questions regarding to data format? Fine data samples of all format from [here](https://drive.google.com/file/d/132l97zjanpoPY4daLgqXoM7HKXPRbS84/view?usp=sharing).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "LOCAL.md",
          "type": "blob",
          "size": 6.4599609375,
          "content": "# Train locally\n### Build environment\n0. Make sure you have installed `Python==3.8`, CMake & C/C++ compilers, ffmpeg; \n1. Clone this repository;\n2. Run `pip install -r requirements.txt`;\n3. Install GPU version PyTorch: (Make sure you have CUDA 11.6 or 11.7 installed)\n    ```\n   # CUDA 11.6\n    pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n    # CUDA 11.7\n    pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n   ```\n4. Install necessary libraries for dealing video data:\n    ```\n   pip install imageio==2.4.1\n   pip install moviepy\n   ```\n5. Build monotonic align (necessary for training)\n    ```\n    cd monotonic_align\n    mkdir monotonic_align\n    python setup.py build_ext --inplace\n    cd ..\n    ```\n6. Download auxiliary data for training\n    ```\n    mkdir pretrained_models\n    # download data for fine-tuning\n    wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/sampled_audio4ft_v2.zip\n    unzip sampled_audio4ft_v2.zip\n    # create necessary directories\n    mkdir video_data\n    mkdir raw_audio\n    mkdir denoised_audio\n    mkdir custom_character_voice\n    mkdir segmented_character_voice\n   ```\n7. Download pretrained model, available options are:\n    ```\n   CJE: Trilingual (Chinese, Japanese, English)\n   CJ: Dualigual (Chinese, Japanese)\n   C: Chinese only\n   ```\n   ### Linux\n   To download `CJE` model, run the following:\n    ```\n   wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/pretrained_models/D_trilingual.pth -O ./pretrained_models/D_0.pth\n   wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/pretrained_models/G_trilingual.pth -O ./pretrained_models/G_0.pth\n   wget https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/configs/uma_trilingual.json -O ./configs/finetune_speaker.json\n   ```\n   To download `CJ` model, run the following:\n   ```\n   wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/D_0-p.pth -O ./pretrained_models/D_0.pth\n   wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/G_0-p.pth -O ./pretrained_models/G_0.pth\n   wget https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/config.json -O ./configs/finetune_speaker.json\n   ```\n   To download `C` model, run the follwoing:\n   ```\n   wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/D_0.pth -O ./pretrained_models/D_0.pth\n   wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/G_0.pth -O ./pretrained_models/G_0.pth\n   wget https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/config.json -O ./configs/finetune_speaker.json\n   ```\n   ### Windows\n   Manually download `G_0.pth`, `D_0.pth`, `finetune_speaker.json` from the URLs in one of the options described above.\n   \n   Rename all `G` models to `G_0.pth`, `D` models to `D_0.pth`, config files (`.json`) to `finetune_speaker.json`.  \n   Put `G_0.pth`, `D_0.pth` under `pretrained_models` directory;  \n   Put `finetune_speaker.json` under `configs` directory  \n   \n   #### Please note that when you download one of them, the previous model will be overwritten.\n9. Put your voice data under corresponding directories, see [DATA.MD](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/DATA_EN.MD) for detailed different uploading options.\n   ### Short audios\n   1. Prepare your data according to [DATA.MD](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/DATA_EN.MD) as a single `.zip` file;  \n   2. Put your file under directory `./custom_character_voice/`;\n   3. run `unzip ./custom_character_voice/custom_character_voice.zip -d ./custom_character_voice/`\n   \n   ### Long audios\n   1. Name your audio files according to [DATA.MD](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/DATA_EN.MD);\n   2. Put your renamed audio files under directory `./raw_audio/`\n   \n   ### Videos\n   1. Name your video files according to [DATA.MD](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/DATA_EN.MD);\n   2. Put your renamed video files under directory `./video_data/`\n10. Process all audio data.\n   ```\n   python scripts/video2audio.py\n   python scripts/denoise_audio.py\n   python scripts/long_audio_transcribe.py --languages \"{PRETRAINED_MODEL}\" --whisper_size large\n   python scripts/short_audio_transcribe.py --languages \"{PRETRAINED_MODEL}\" --whisper_size large\n   python scripts/resample.py\n   ```\n   Replace `\"{PRETRAINED_MODEL}\"` with one of `{CJ, CJE, C}` according to your previous model choice.  \n   Make sure you have a minimum GPU memory of 12GB. If not, change the argument `whisper_size` to `medium` or `small`.\n\n10. Process all text data.  \n   If you choose to add auxiliary data, run `python preprocess_v2.py --add_auxiliary_data True --languages \"{PRETRAINED_MODEL}\"`  \n   If not, run `python preprocess_v2.py --languages \"{PRETRAINED_MODEL}\"`  \n   Do replace `\"{PRETRAINED_MODEL}\"` with one of `{CJ, CJE, C}` according to your previous model choice.\n\n11. Start Training.  \n   Run `python finetune_speaker_v2.py -m ./OUTPUT_MODEL --max_epochs \"{Maximum_epochs}\" --drop_speaker_embed True`  \n   Do replace `{Maximum_epochs}` with your desired number of epochs. Empirically, 100 or more is recommended.  \n   To continue training on previous checkpoint, change the training command to: `python finetune_speaker_v2.py -m ./OUTPUT_MODEL --max_epochs \"{Maximum_epochs}\" --drop_speaker_embed False --cont True`. Before you do this, make sure you have previous `G_latest.pth` and `D_latest.pth` under `./OUTPUT_MODEL/` directory.  \n   To view training progress, open a new terminal and `cd` to the project root directory, run `tensorboard --logdir=./OUTPUT_MODEL`, then visit `localhost:6006` with your web browser.\n\n12. After training is completed, you can use your model by running:  \n   `python VC_inference.py --model_dir ./OUTPUT_MODEL/G_latest.pth --share True`\n13. To clear all audio data, run:  \n   ### Linux\n   ```\n   rm -rf ./custom_character_voice/* ./video_data/* ./raw_audio/* ./denoised_audio/* ./segmented_character_voice/* ./separated/* long_character_anno.txt short_character_anno.txt\n   ```\n   ### Windows\n   ```\n   del /Q /S .\\custom_character_voice\\* .\\video_data\\* .\\raw_audio\\* .\\denoised_audio\\* .\\segmented_character_voice\\* .\\separated\\* long_character_anno.txt short_character_anno.txt\n   ```\n\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.478515625,
          "content": "[中文文档请点击这里](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/README_ZH.md)\n# VITS Fast Fine-tuning\nThis repo will guide you to add your own character voices, or even your own voice, into existing VITS TTS model\nto make it able to do the following tasks in less than 1 hour:  \n\n1. Many-to-many voice conversion between any characters you added & preset characters in the model.\n2. English, Japanese & Chinese Text-to-Speech synthesis with the characters you added & preset characters  \n  \n\nWelcome to play around with the base models!  \nChinese & English & Japanese：[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer) Author: Me  \n\nChinese & Japanese：[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai) Author: [SayaSS](https://github.com/SayaSS)  \n\nChinese only：(No running huggingface spaces) Author: [Wwwwhy230825](https://github.com/Wwwwhy230825)\n\n\n### Currently Supported Tasks:\n- [x] Clone character voice from 10+ short audios\n- [x] Clone character voice from long audio(s) >= 3 minutes (one audio should contain single speaker only)\n- [x] Clone character voice from videos(s) >= 3 minutes (one video should contain single speaker only)\n- [x] Clone character voice from BILIBILI video links (one video should contain single speaker only)\n\n### Currently Supported Characters for TTS & VC:\n- [x] Any character you wish as long as you have their voices!\n(Note that voice conversion can only be conducted between any two speakers in the model)\n\n\n\n## Fine-tuning\nSee [LOCAL.md](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/LOCAL.md) for local training guide.  \nAlternatively, you can perform fine-tuning on [Google Colab](https://colab.research.google.com/drive/1pn1xnFfdLK63gVXDwV4zCXfVeo8c-I-0?usp=sharing)\n\n\n### How long does it take? \n1. Install dependencies (3 min)\n2. Choose pretrained model to start. The detailed differences between them are described in [Colab Notebook](https://colab.research.google.com/drive/1pn1xnFfdLK63gVXDwV4zCXfVeo8c-I-0?usp=sharing)\n3. Upload the voice samples of the characters you wish to add，see [DATA.MD](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/DATA_EN.MD) for detailed uploading options.\n4. Start fine-tuning. Time taken varies from 20 minutes ~ 2 hours, depending on the number of voices you uploaded.\n\n\n## Inference or Usage (Currently support Windows only)\n0. Remember to download your fine-tuned model!\n1. Download the latest release\n2. Put your model & config file into the folder `inference`, which are named `G_latest.pth` and `finetune_speaker.json`, respectively.\n3. The file structure should be as follows:\n```\ninference\n├───inference.exe\n├───...\n├───finetune_speaker.json\n└───G_latest.pth\n```\n4. run `inference.exe`, the browser should pop up automatically.\n5. Note: you must install `ffmpeg` to enable voice conversion feature.\n\n## Use in MoeGoe\n0. Prepare downloaded model & config file, which are named `G_latest.pth` and `moegoe_config.json`, respectively.\n1. Follow [MoeGoe](https://github.com/CjangCjengh/MoeGoe) page instructions to install, configure path, and use.\n\n## Looking for help?\nIf you have any questions, please feel free to open an [issue](https://github.com/Plachtaa/VITS-fast-fine-tuning/issues/new) or join our [Discord](https://discord.gg/TcrjDFvm5A) server.\n"
        },
        {
          "name": "README_ZH.md",
          "type": "blob",
          "size": 4.0087890625,
          "content": "English Documentation Please Click [here](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/README.md)\n# VITS 快速微调\n这个代码库会指导你如何将自定义角色（甚至你自己），加入预训练的VITS模型中，在1小时内的微调使模型具备如下功能：  \n1. 在 模型所包含的任意两个角色 之间进行声线转换\n2. 以 你加入的角色声线 进行中日英三语 文本到语音合成。  \n\n本项目使用的底模涵盖常见二次元男/女配音声线（来自原神数据集）以及现实世界常见男/女声线（来自VCTK数据集），支持中日英三语，保证能够在微调时快速适应新的声线。\n\n欢迎体验微调所使用的底模！  \n\n中日英：[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer) 作者：我  \n\n中日：[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai) 作者：[SayaSS](https://github.com/SayaSS)  \n\n纯中文：（没有huggingface demo）作者：[Wwwwhy230825](https://github.com/Wwwwhy230825)\n\n### 目前支持的任务:\n- [x] 从 10条以上的短音频 克隆角色声音\n- [x] 从 3分钟以上的长音频（单个音频只能包含单说话人） 克隆角色声音\n- [x] 从 3分钟以上的视频（单个视频只能包含单说话人） 克隆角色声音\n- [x] 通过输入 bilibili视频链接（单个视频只能包含单说话人） 克隆角色声音\n\n### 目前支持声线转换和中日英三语TTS的角色\n- [x] 任意角色（只要你有角色的声音样本）\n（注意：声线转换只能在任意两个存在于模型中的说话人之间进行）\n\n\n\n\n## 微调\n若希望于本地机器进行训练，请参考[LOCAL.md](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/LOCAL.md)以进行。  \n另外，也可以选择使用 [Google Colab](https://colab.research.google.com/drive/1pn1xnFfdLK63gVXDwV4zCXfVeo8c-I-0?usp=sharing)\n进行微调任务。\n### 我需要花多长时间？\n1. 安装依赖 (10 min在Google Colab中)\n2. 选择预训练模型，详细区别参见[Colab 笔记本页面](https://colab.research.google.com/drive/1pn1xnFfdLK63gVXDwV4zCXfVeo8c-I-0?usp=sharing)。\n3. 上传你希望加入的其它角色声音，详细上传方式见[DATA.MD](https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/DATA.MD)\n4. 进行微调，根据选择的微调方式和样本数量不同，花费时长可能在20分钟到2小时不等。\n\n微调结束后可以直接下载微调好的模型，日后在本地运行（不需要GPU）\n\n## 本地运行和推理\n0. 记得下载微调好的模型和config文件！\n1. 下载最新的Release包（在Github页面的右侧）\n2. 把下载的模型和config文件放在 `inference`文件夹下, 其文件名分别为 `G_latest.pth` 和 `finetune_speaker.json`。\n3. 一切准备就绪后，文件结构应该如下所示:\n```\ninference\n├───inference.exe\n├───...\n├───finetune_speaker.json\n└───G_latest.pth\n```\n4. 运行 `inference.exe`, 浏览器会自动弹出窗口, 注意其所在路径不能有中文字符或者空格.\n5. 请注意，声线转换功能需要安装`ffmpeg`才能正常使用.\n\n## 在MoeGoe使用\n0. MoeGoe以及类似其它VITS推理UI使用的config格式略有不同，需要下载的文件为模型`G_latest.pth`和配置文件`moegoe_config.json`\n1. 按照[MoeGoe](https://github.com/CjangCjengh/MoeGoe)页面的提示配置路径即可使用。\n2. MoeGoe在输入句子时需要使用相应的语言标记包裹句子才能正常合成。（日语用[JA], 中文用[ZH], 英文用[EN]），例如：  \n[JA]こんにちわ。[JA]  \n[ZH]你好！[ZH]  \n[EN]Hello![EN]  \n\n## 帮助\n如果你在使用过程中遇到了任何问题，可以在[这里](https://github.com/Plachtaa/VITS-fast-fine-tuning/issues/new)开一个issue，或者加入Discord服务器寻求帮助：[Discord](https://discord.gg/TcrjDFvm5A)。\n"
        },
        {
          "name": "VC_inference.py",
          "type": "blob",
          "size": 6.591796875,
          "content": "import os\nimport numpy as np\nimport torch\nfrom torch import no_grad, LongTensor\nimport argparse\nimport commons\nfrom mel_processing import spectrogram_torch\nimport utils\nfrom models import SynthesizerTrn\nimport gradio as gr\nimport librosa\nimport webbrowser\n\nfrom text import text_to_sequence, _clean_text\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nimport logging\nlogging.getLogger(\"PIL\").setLevel(logging.WARNING)\nlogging.getLogger(\"urllib3\").setLevel(logging.WARNING)\nlogging.getLogger(\"markdown_it\").setLevel(logging.WARNING)\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\nlogging.getLogger(\"asyncio\").setLevel(logging.WARNING)\n\nlanguage_marks = {\n    \"Japanese\": \"\",\n    \"日本語\": \"[JA]\",\n    \"简体中文\": \"[ZH]\",\n    \"English\": \"[EN]\",\n    \"Mix\": \"\",\n}\nlang = ['日本語', '简体中文', 'English', 'Mix']\ndef get_text(text, hps, is_symbol):\n    text_norm = text_to_sequence(text, hps.symbols, [] if is_symbol else hps.data.text_cleaners)\n    if hps.data.add_blank:\n        text_norm = commons.intersperse(text_norm, 0)\n    text_norm = LongTensor(text_norm)\n    return text_norm\n\ndef create_tts_fn(model, hps, speaker_ids):\n    def tts_fn(text, speaker, language, speed):\n        if language is not None:\n            text = language_marks[language] + text + language_marks[language]\n        speaker_id = speaker_ids[speaker]\n        stn_tst = get_text(text, hps, False)\n        with no_grad():\n            x_tst = stn_tst.unsqueeze(0).to(device)\n            x_tst_lengths = LongTensor([stn_tst.size(0)]).to(device)\n            sid = LongTensor([speaker_id]).to(device)\n            audio = model.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8,\n                                length_scale=1.0 / speed)[0][0, 0].data.cpu().float().numpy()\n        del stn_tst, x_tst, x_tst_lengths, sid\n        return \"Success\", (hps.data.sampling_rate, audio)\n\n    return tts_fn\n\ndef create_vc_fn(model, hps, speaker_ids):\n    def vc_fn(original_speaker, target_speaker, record_audio, upload_audio):\n        input_audio = record_audio if record_audio is not None else upload_audio\n        if input_audio is None:\n            return \"You need to record or upload an audio\", None\n        sampling_rate, audio = input_audio\n        original_speaker_id = speaker_ids[original_speaker]\n        target_speaker_id = speaker_ids[target_speaker]\n\n        audio = (audio / np.iinfo(audio.dtype).max).astype(np.float32)\n        if len(audio.shape) > 1:\n            audio = librosa.to_mono(audio.transpose(1, 0))\n        if sampling_rate != hps.data.sampling_rate:\n            audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=hps.data.sampling_rate)\n        with no_grad():\n            y = torch.FloatTensor(audio)\n            y = y / max(-y.min(), y.max()) / 0.99\n            y = y.to(device)\n            y = y.unsqueeze(0)\n            spec = spectrogram_torch(y, hps.data.filter_length,\n                                     hps.data.sampling_rate, hps.data.hop_length, hps.data.win_length,\n                                     center=False).to(device)\n            spec_lengths = LongTensor([spec.size(-1)]).to(device)\n            sid_src = LongTensor([original_speaker_id]).to(device)\n            sid_tgt = LongTensor([target_speaker_id]).to(device)\n            audio = model.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt)[0][\n                0, 0].data.cpu().float().numpy()\n        del y, spec, spec_lengths, sid_src, sid_tgt\n        return \"Success\", (hps.data.sampling_rate, audio)\n\n    return vc_fn\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_dir\", default=\"./G_latest.pth\", help=\"directory to your fine-tuned model\")\n    parser.add_argument(\"--config_dir\", default=\"./finetune_speaker.json\", help=\"directory to your model config file\")\n    parser.add_argument(\"--share\", default=False, help=\"make link public (used in colab)\")\n\n    args = parser.parse_args()\n    hps = utils.get_hparams_from_file(args.config_dir)\n\n\n    net_g = SynthesizerTrn(\n        len(hps.symbols),\n        hps.data.filter_length // 2 + 1,\n        hps.train.segment_size // hps.data.hop_length,\n        n_speakers=hps.data.n_speakers,\n        **hps.model).to(device)\n    _ = net_g.eval()\n\n    _ = utils.load_checkpoint(args.model_dir, net_g, None)\n    speaker_ids = hps.speakers\n    speakers = list(hps.speakers.keys())\n    tts_fn = create_tts_fn(net_g, hps, speaker_ids)\n    vc_fn = create_vc_fn(net_g, hps, speaker_ids)\n    app = gr.Blocks()\n    with app:\n        with gr.Tab(\"Text-to-Speech\"):\n            with gr.Row():\n                with gr.Column():\n                    textbox = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=\"こんにちわ。\", elem_id=f\"tts-input\")\n                    # select character\n                    char_dropdown = gr.Dropdown(choices=speakers, value=speakers[0], label='character')\n                    language_dropdown = gr.Dropdown(choices=lang, value=lang[0], label='language')\n                    duration_slider = gr.Slider(minimum=0.1, maximum=5, value=1, step=0.1,\n                                                label='速度 Speed')\n                with gr.Column():\n                    text_output = gr.Textbox(label=\"Message\")\n                    audio_output = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn = gr.Button(\"Generate!\")\n                    btn.click(tts_fn,\n                              inputs=[textbox, char_dropdown, language_dropdown, duration_slider,],\n                              outputs=[text_output, audio_output])\n        with gr.Tab(\"Voice Conversion\"):\n            gr.Markdown(\"\"\"\n                            录制或上传声音，并选择要转换的音色。\n            \"\"\")\n            with gr.Column():\n                record_audio = gr.Audio(label=\"record your voice\", source=\"microphone\")\n                upload_audio = gr.Audio(label=\"or upload audio here\", source=\"upload\")\n                source_speaker = gr.Dropdown(choices=speakers, value=speakers[0], label=\"source speaker\")\n                target_speaker = gr.Dropdown(choices=speakers, value=speakers[0], label=\"target speaker\")\n            with gr.Column():\n                message_box = gr.Textbox(label=\"Message\")\n                converted_audio = gr.Audio(label='converted audio')\n            btn = gr.Button(\"Convert!\")\n            btn.click(vc_fn, inputs=[source_speaker, target_speaker, record_audio, upload_audio],\n                      outputs=[message_box, converted_audio])\n    webbrowser.open(\"http://127.0.0.1:7860\")\n    app.launch(share=args.share)\n\n"
        },
        {
          "name": "attentions.py",
          "type": "blob",
          "size": 11.7998046875,
          "content": "import copy\r\nimport math\r\nimport numpy as np\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nimport commons\r\nimport modules\r\nfrom modules import LayerNorm\r\n   \r\n\r\nclass Encoder(nn.Module):\r\n  def __init__(self, hidden_channels, filter_channels, n_heads, n_layers, kernel_size=1, p_dropout=0., window_size=4, **kwargs):\r\n    super().__init__()\r\n    self.hidden_channels = hidden_channels\r\n    self.filter_channels = filter_channels\r\n    self.n_heads = n_heads\r\n    self.n_layers = n_layers\r\n    self.kernel_size = kernel_size\r\n    self.p_dropout = p_dropout\r\n    self.window_size = window_size\r\n\r\n    self.drop = nn.Dropout(p_dropout)\r\n    self.attn_layers = nn.ModuleList()\r\n    self.norm_layers_1 = nn.ModuleList()\r\n    self.ffn_layers = nn.ModuleList()\r\n    self.norm_layers_2 = nn.ModuleList()\r\n    for i in range(self.n_layers):\r\n      self.attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout, window_size=window_size))\r\n      self.norm_layers_1.append(LayerNorm(hidden_channels))\r\n      self.ffn_layers.append(FFN(hidden_channels, hidden_channels, filter_channels, kernel_size, p_dropout=p_dropout))\r\n      self.norm_layers_2.append(LayerNorm(hidden_channels))\r\n\r\n  def forward(self, x, x_mask):\r\n    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\r\n    x = x * x_mask\r\n    for i in range(self.n_layers):\r\n      y = self.attn_layers[i](x, x, attn_mask)\r\n      y = self.drop(y)\r\n      x = self.norm_layers_1[i](x + y)\r\n\r\n      y = self.ffn_layers[i](x, x_mask)\r\n      y = self.drop(y)\r\n      x = self.norm_layers_2[i](x + y)\r\n    x = x * x_mask\r\n    return x\r\n\r\n\r\nclass Decoder(nn.Module):\r\n  def __init__(self, hidden_channels, filter_channels, n_heads, n_layers, kernel_size=1, p_dropout=0., proximal_bias=False, proximal_init=True, **kwargs):\r\n    super().__init__()\r\n    self.hidden_channels = hidden_channels\r\n    self.filter_channels = filter_channels\r\n    self.n_heads = n_heads\r\n    self.n_layers = n_layers\r\n    self.kernel_size = kernel_size\r\n    self.p_dropout = p_dropout\r\n    self.proximal_bias = proximal_bias\r\n    self.proximal_init = proximal_init\r\n\r\n    self.drop = nn.Dropout(p_dropout)\r\n    self.self_attn_layers = nn.ModuleList()\r\n    self.norm_layers_0 = nn.ModuleList()\r\n    self.encdec_attn_layers = nn.ModuleList()\r\n    self.norm_layers_1 = nn.ModuleList()\r\n    self.ffn_layers = nn.ModuleList()\r\n    self.norm_layers_2 = nn.ModuleList()\r\n    for i in range(self.n_layers):\r\n      self.self_attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout, proximal_bias=proximal_bias, proximal_init=proximal_init))\r\n      self.norm_layers_0.append(LayerNorm(hidden_channels))\r\n      self.encdec_attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout))\r\n      self.norm_layers_1.append(LayerNorm(hidden_channels))\r\n      self.ffn_layers.append(FFN(hidden_channels, hidden_channels, filter_channels, kernel_size, p_dropout=p_dropout, causal=True))\r\n      self.norm_layers_2.append(LayerNorm(hidden_channels))\r\n\r\n  def forward(self, x, x_mask, h, h_mask):\r\n    \"\"\"\r\n    x: decoder input\r\n    h: encoder output\r\n    \"\"\"\r\n    self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(device=x.device, dtype=x.dtype)\r\n    encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\r\n    x = x * x_mask\r\n    for i in range(self.n_layers):\r\n      y = self.self_attn_layers[i](x, x, self_attn_mask)\r\n      y = self.drop(y)\r\n      x = self.norm_layers_0[i](x + y)\r\n\r\n      y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\r\n      y = self.drop(y)\r\n      x = self.norm_layers_1[i](x + y)\r\n      \r\n      y = self.ffn_layers[i](x, x_mask)\r\n      y = self.drop(y)\r\n      x = self.norm_layers_2[i](x + y)\r\n    x = x * x_mask\r\n    return x\r\n\r\n\r\nclass MultiHeadAttention(nn.Module):\r\n  def __init__(self, channels, out_channels, n_heads, p_dropout=0., window_size=None, heads_share=True, block_length=None, proximal_bias=False, proximal_init=False):\r\n    super().__init__()\r\n    assert channels % n_heads == 0\r\n\r\n    self.channels = channels\r\n    self.out_channels = out_channels\r\n    self.n_heads = n_heads\r\n    self.p_dropout = p_dropout\r\n    self.window_size = window_size\r\n    self.heads_share = heads_share\r\n    self.block_length = block_length\r\n    self.proximal_bias = proximal_bias\r\n    self.proximal_init = proximal_init\r\n    self.attn = None\r\n\r\n    self.k_channels = channels // n_heads\r\n    self.conv_q = nn.Conv1d(channels, channels, 1)\r\n    self.conv_k = nn.Conv1d(channels, channels, 1)\r\n    self.conv_v = nn.Conv1d(channels, channels, 1)\r\n    self.conv_o = nn.Conv1d(channels, out_channels, 1)\r\n    self.drop = nn.Dropout(p_dropout)\r\n\r\n    if window_size is not None:\r\n      n_heads_rel = 1 if heads_share else n_heads\r\n      rel_stddev = self.k_channels**-0.5\r\n      self.emb_rel_k = nn.Parameter(torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels) * rel_stddev)\r\n      self.emb_rel_v = nn.Parameter(torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels) * rel_stddev)\r\n\r\n    nn.init.xavier_uniform_(self.conv_q.weight)\r\n    nn.init.xavier_uniform_(self.conv_k.weight)\r\n    nn.init.xavier_uniform_(self.conv_v.weight)\r\n    if proximal_init:\r\n      with torch.no_grad():\r\n        self.conv_k.weight.copy_(self.conv_q.weight)\r\n        self.conv_k.bias.copy_(self.conv_q.bias)\r\n      \r\n  def forward(self, x, c, attn_mask=None):\r\n    q = self.conv_q(x)\r\n    k = self.conv_k(c)\r\n    v = self.conv_v(c)\r\n    \r\n    x, self.attn = self.attention(q, k, v, mask=attn_mask)\r\n\r\n    x = self.conv_o(x)\r\n    return x\r\n\r\n  def attention(self, query, key, value, mask=None):\r\n    # reshape [b, d, t] -> [b, n_h, t, d_k]\r\n    b, d, t_s, t_t = (*key.size(), query.size(2))\r\n    query = query.view(b, self.n_heads, self.k_channels, t_t).transpose(2, 3)\r\n    key = key.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\r\n    value = value.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\r\n\r\n    scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1))\r\n    if self.window_size is not None:\r\n      assert t_s == t_t, \"Relative attention is only available for self-attention.\"\r\n      key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\r\n      rel_logits = self._matmul_with_relative_keys(query /math.sqrt(self.k_channels), key_relative_embeddings)\r\n      scores_local = self._relative_position_to_absolute_position(rel_logits)\r\n      scores = scores + scores_local\r\n    if self.proximal_bias:\r\n      assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\r\n      scores = scores + self._attention_bias_proximal(t_s).to(device=scores.device, dtype=scores.dtype)\r\n    if mask is not None:\r\n      scores = scores.masked_fill(mask == 0, -1e4)\r\n      if self.block_length is not None:\r\n        assert t_s == t_t, \"Local attention is only available for self-attention.\"\r\n        block_mask = torch.ones_like(scores).triu(-self.block_length).tril(self.block_length)\r\n        scores = scores.masked_fill(block_mask == 0, -1e4)\r\n    p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\r\n    p_attn = self.drop(p_attn)\r\n    output = torch.matmul(p_attn, value)\r\n    if self.window_size is not None:\r\n      relative_weights = self._absolute_position_to_relative_position(p_attn)\r\n      value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\r\n      output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings)\r\n    output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\r\n    return output, p_attn\r\n\r\n  def _matmul_with_relative_values(self, x, y):\r\n    \"\"\"\r\n    x: [b, h, l, m]\r\n    y: [h or 1, m, d]\r\n    ret: [b, h, l, d]\r\n    \"\"\"\r\n    ret = torch.matmul(x, y.unsqueeze(0))\r\n    return ret\r\n\r\n  def _matmul_with_relative_keys(self, x, y):\r\n    \"\"\"\r\n    x: [b, h, l, d]\r\n    y: [h or 1, m, d]\r\n    ret: [b, h, l, m]\r\n    \"\"\"\r\n    ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\r\n    return ret\r\n\r\n  def _get_relative_embeddings(self, relative_embeddings, length):\r\n    max_relative_position = 2 * self.window_size + 1\r\n    # Pad first before slice to avoid using cond ops.\r\n    pad_length = max(length - (self.window_size + 1), 0)\r\n    slice_start_position = max((self.window_size + 1) - length, 0)\r\n    slice_end_position = slice_start_position + 2 * length - 1\r\n    if pad_length > 0:\r\n      padded_relative_embeddings = F.pad(\r\n          relative_embeddings,\r\n          commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]))\r\n    else:\r\n      padded_relative_embeddings = relative_embeddings\r\n    used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\r\n    return used_relative_embeddings\r\n\r\n  def _relative_position_to_absolute_position(self, x):\r\n    \"\"\"\r\n    x: [b, h, l, 2*l-1]\r\n    ret: [b, h, l, l]\r\n    \"\"\"\r\n    batch, heads, length, _ = x.size()\r\n    # Concat columns of pad to shift from relative to absolute indexing.\r\n    x = F.pad(x, commons.convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\r\n\r\n    # Concat extra elements so to add up to shape (len+1, 2*len-1).\r\n    x_flat = x.view([batch, heads, length * 2 * length])\r\n    x_flat = F.pad(x_flat, commons.convert_pad_shape([[0,0],[0,0],[0,length-1]]))\r\n\r\n    # Reshape and slice out the padded elements.\r\n    x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\r\n    return x_final\r\n\r\n  def _absolute_position_to_relative_position(self, x):\r\n    \"\"\"\r\n    x: [b, h, l, l]\r\n    ret: [b, h, l, 2*l-1]\r\n    \"\"\"\r\n    batch, heads, length, _ = x.size()\r\n    # padd along column\r\n    x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\r\n    x_flat = x.view([batch, heads, length**2 + length*(length -1)])\r\n    # add 0's in the beginning that will skew the elements after reshape\r\n    x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\r\n    x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\r\n    return x_final\r\n\r\n  def _attention_bias_proximal(self, length):\r\n    \"\"\"Bias for self-attention to encourage attention to close positions.\r\n    Args:\r\n      length: an integer scalar.\r\n    Returns:\r\n      a Tensor with shape [1, 1, length, length]\r\n    \"\"\"\r\n    r = torch.arange(length, dtype=torch.float32)\r\n    diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\r\n    return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)\r\n\r\n\r\nclass FFN(nn.Module):\r\n  def __init__(self, in_channels, out_channels, filter_channels, kernel_size, p_dropout=0., activation=None, causal=False):\r\n    super().__init__()\r\n    self.in_channels = in_channels\r\n    self.out_channels = out_channels\r\n    self.filter_channels = filter_channels\r\n    self.kernel_size = kernel_size\r\n    self.p_dropout = p_dropout\r\n    self.activation = activation\r\n    self.causal = causal\r\n\r\n    if causal:\r\n      self.padding = self._causal_padding\r\n    else:\r\n      self.padding = self._same_padding\r\n\r\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size)\r\n    self.conv_2 = nn.Conv1d(filter_channels, out_channels, kernel_size)\r\n    self.drop = nn.Dropout(p_dropout)\r\n\r\n  def forward(self, x, x_mask):\r\n    x = self.conv_1(self.padding(x * x_mask))\r\n    if self.activation == \"gelu\":\r\n      x = x * torch.sigmoid(1.702 * x)\r\n    else:\r\n      x = torch.relu(x)\r\n    x = self.drop(x)\r\n    x = self.conv_2(self.padding(x * x_mask))\r\n    return x * x_mask\r\n  \r\n  def _causal_padding(self, x):\r\n    if self.kernel_size == 1:\r\n      return x\r\n    pad_l = self.kernel_size - 1\r\n    pad_r = 0\r\n    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\r\n    x = F.pad(x, commons.convert_pad_shape(padding))\r\n    return x\r\n\r\n  def _same_padding(self, x):\r\n    if self.kernel_size == 1:\r\n      return x\r\n    pad_l = (self.kernel_size - 1) // 2\r\n    pad_r = self.kernel_size // 2\r\n    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\r\n    x = F.pad(x, commons.convert_pad_shape(padding))\r\n    return x\r\n"
        },
        {
          "name": "cmd_inference.py",
          "type": "blob",
          "size": 3.6201171875,
          "content": "\"\"\"该模块用于生成VITS文件\n使用方法\n\npython cmd_inference.py -m 模型路径 -c 配置文件路径 -o 输出文件路径 -l 输入的语言 -t 输入文本 -s 合成目标说话人名称\n\n可选参数\n-ns 感情变化程度\n-nsw 音素发音长度\n-ls 整体语速\n-on 输出文件的名称\n\n\"\"\"\n\nfrom pathlib import Path\nimport utils\nfrom models import SynthesizerTrn\nimport torch\nfrom torch import no_grad, LongTensor\nimport librosa\nfrom text import text_to_sequence, _clean_text\nimport commons\nimport scipy.io.wavfile as wavf\nimport os\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nlanguage_marks = {\n    \"Japanese\": \"\",\n    \"日本語\": \"[JA]\",\n    \"简体中文\": \"[ZH]\",\n    \"English\": \"[EN]\",\n    \"Mix\": \"\",\n}\n\n\ndef get_text(text, hps, is_symbol):\n    text_norm = text_to_sequence(text, hps.symbols, [] if is_symbol else hps.data.text_cleaners)\n    if hps.data.add_blank:\n        text_norm = commons.intersperse(text_norm, 0)\n    text_norm = LongTensor(text_norm)\n    return text_norm\n\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(description='vits inference')\n    #必须参数\n    parser.add_argument('-m', '--model_path', type=str, default=\"logs/44k/G_0.pth\", help='模型路径')\n    parser.add_argument('-c', '--config_path', type=str, default=\"configs/config.json\", help='配置文件路径')\n    parser.add_argument('-o', '--output_path', type=str, default=\"output/vits\", help='输出文件路径')\n    parser.add_argument('-l', '--language', type=str, default=\"日本語\", help='输入的语言')\n    parser.add_argument('-t', '--text', type=str, help='输入文本')\n    parser.add_argument('-s', '--spk', type=str, help='合成目标说话人名称')\n    #可选参数\n    parser.add_argument('-on', '--output_name', type=str, default=\"output\", help='输出文件的名称')\n    parser.add_argument('-ns', '--noise_scale', type=float,default= .667,help='感情变化程度')\n    parser.add_argument('-nsw', '--noise_scale_w', type=float,default=0.6, help='音素发音长度')\n    parser.add_argument('-ls', '--length_scale', type=float,default=1, help='整体语速')\n    \n    args = parser.parse_args()\n    \n    model_path = args.model_path\n    config_path = args.config_path\n    output_dir = Path(args.output_path)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    language = args.language\n    text = args.text\n    spk = args.spk\n    noise_scale = args.noise_scale\n    noise_scale_w = args.noise_scale_w\n    length = args.length_scale\n    output_name = args.output_name\n    \n    hps = utils.get_hparams_from_file(config_path)\n    net_g = SynthesizerTrn(\n        len(hps.symbols),\n        hps.data.filter_length // 2 + 1,\n        hps.train.segment_size // hps.data.hop_length,\n        n_speakers=hps.data.n_speakers,\n        **hps.model).to(device)\n    _ = net_g.eval()\n    _ = utils.load_checkpoint(model_path, net_g, None)\n    \n    speaker_ids = hps.speakers\n\n\n    if language is not None:\n        text = language_marks[language] + text + language_marks[language]\n        speaker_id = speaker_ids[spk]\n        stn_tst = get_text(text, hps, False)\n        with no_grad():\n            x_tst = stn_tst.unsqueeze(0).to(device)\n            x_tst_lengths = LongTensor([stn_tst.size(0)]).to(device)\n            sid = LongTensor([speaker_id]).to(device)\n            audio = net_g.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=noise_scale, noise_scale_w=noise_scale_w,\n                                length_scale=1.0 / length)[0][0, 0].data.cpu().float().numpy()\n        del stn_tst, x_tst, x_tst_lengths, sid\n\n        wavf.write(str(output_dir)+\"/\"+output_name+\".wav\",hps.data.sampling_rate,audio)\n    \n\n    \n    "
        },
        {
          "name": "commons.py",
          "type": "blob",
          "size": 4.8779296875,
          "content": "import math\r\nimport numpy as np\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\n\r\ndef init_weights(m, mean=0.0, std=0.01):\r\n  classname = m.__class__.__name__\r\n  if classname.find(\"Conv\") != -1:\r\n    m.weight.data.normal_(mean, std)\r\n\r\n\r\ndef get_padding(kernel_size, dilation=1):\r\n  return int((kernel_size*dilation - dilation)/2)\r\n\r\n\r\ndef convert_pad_shape(pad_shape):\r\n  l = pad_shape[::-1]\r\n  pad_shape = [item for sublist in l for item in sublist]\r\n  return pad_shape\r\n\r\n\r\ndef intersperse(lst, item):\r\n  result = [item] * (len(lst) * 2 + 1)\r\n  result[1::2] = lst\r\n  return result\r\n\r\n\r\ndef kl_divergence(m_p, logs_p, m_q, logs_q):\r\n  \"\"\"KL(P||Q)\"\"\"\r\n  kl = (logs_q - logs_p) - 0.5\r\n  kl += 0.5 * (torch.exp(2. * logs_p) + ((m_p - m_q)**2)) * torch.exp(-2. * logs_q)\r\n  return kl\r\n\r\n\r\ndef rand_gumbel(shape):\r\n  \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\r\n  uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\r\n  return -torch.log(-torch.log(uniform_samples))\r\n\r\n\r\ndef rand_gumbel_like(x):\r\n  g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\r\n  return g\r\n\r\n\r\ndef slice_segments(x, ids_str, segment_size=4):\r\n  ret = torch.zeros_like(x[:, :, :segment_size])\r\n  for i in range(x.size(0)):\r\n    idx_str = ids_str[i]\r\n    idx_end = idx_str + segment_size\r\n    try:\r\n      ret[i] = x[i, :, idx_str:idx_end]\r\n    except RuntimeError:\r\n      print(\"?\")\r\n  return ret\r\n\r\n\r\ndef rand_slice_segments(x, x_lengths=None, segment_size=4):\r\n  b, d, t = x.size()\r\n  if x_lengths is None:\r\n    x_lengths = t\r\n  ids_str_max = x_lengths - segment_size + 1\r\n  ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\r\n  ret = slice_segments(x, ids_str, segment_size)\r\n  return ret, ids_str\r\n\r\n\r\ndef get_timing_signal_1d(\r\n    length, channels, min_timescale=1.0, max_timescale=1.0e4):\r\n  position = torch.arange(length, dtype=torch.float)\r\n  num_timescales = channels // 2\r\n  log_timescale_increment = (\r\n      math.log(float(max_timescale) / float(min_timescale)) /\r\n      (num_timescales - 1))\r\n  inv_timescales = min_timescale * torch.exp(\r\n      torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment)\r\n  scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\r\n  signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\r\n  signal = F.pad(signal, [0, 0, 0, channels % 2])\r\n  signal = signal.view(1, channels, length)\r\n  return signal\r\n\r\n\r\ndef add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\r\n  b, channels, length = x.size()\r\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\r\n  return x + signal.to(dtype=x.dtype, device=x.device)\r\n\r\n\r\ndef cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\r\n  b, channels, length = x.size()\r\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\r\n  return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\r\n\r\n\r\ndef subsequent_mask(length):\r\n  mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\r\n  return mask\r\n\r\n\r\n@torch.jit.script\r\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\r\n  n_channels_int = n_channels[0]\r\n  in_act = input_a + input_b\r\n  t_act = torch.tanh(in_act[:, :n_channels_int, :])\r\n  s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\r\n  acts = t_act * s_act\r\n  return acts\r\n\r\n\r\ndef convert_pad_shape(pad_shape):\r\n  l = pad_shape[::-1]\r\n  pad_shape = [item for sublist in l for item in sublist]\r\n  return pad_shape\r\n\r\n\r\ndef shift_1d(x):\r\n  x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\r\n  return x\r\n\r\n\r\ndef sequence_mask(length, max_length=None):\r\n  if max_length is None:\r\n    max_length = length.max()\r\n  x = torch.arange(max_length, dtype=length.dtype, device=length.device)\r\n  return x.unsqueeze(0) < length.unsqueeze(1)\r\n\r\n\r\ndef generate_path(duration, mask):\r\n  \"\"\"\r\n  duration: [b, 1, t_x]\r\n  mask: [b, 1, t_y, t_x]\r\n  \"\"\"\r\n  device = duration.device\r\n  \r\n  b, _, t_y, t_x = mask.shape\r\n  cum_duration = torch.cumsum(duration, -1)\r\n  \r\n  cum_duration_flat = cum_duration.view(b * t_x)\r\n  path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\r\n  path = path.view(b, t_x, t_y)\r\n  path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\r\n  path = path.unsqueeze(1).transpose(2,3) * mask\r\n  return path\r\n\r\n\r\ndef clip_grad_value_(parameters, clip_value, norm_type=2):\r\n  if isinstance(parameters, torch.Tensor):\r\n    parameters = [parameters]\r\n  parameters = list(filter(lambda p: p.grad is not None, parameters))\r\n  norm_type = float(norm_type)\r\n  if clip_value is not None:\r\n    clip_value = float(clip_value)\r\n\r\n  total_norm = 0\r\n  for p in parameters:\r\n    param_norm = p.grad.data.norm(norm_type)\r\n    total_norm += param_norm.item() ** norm_type\r\n    if clip_value is not None:\r\n      p.grad.data.clamp_(min=-clip_value, max=clip_value)\r\n  total_norm = total_norm ** (1. / norm_type)\r\n  return total_norm\r\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data_utils.py",
          "type": "blob",
          "size": 10.6416015625,
          "content": "import time\r\nimport os\r\nimport random\r\nimport numpy as np\r\nimport torch\r\nimport torch.utils.data\r\nimport torchaudio\r\n\r\nimport commons\r\nfrom mel_processing import spectrogram_torch\r\nfrom utils import load_wav_to_torch, load_filepaths_and_text\r\nfrom text import text_to_sequence, cleaned_text_to_sequence\r\n\"\"\"Multi speaker version\"\"\"\r\n\r\n\r\nclass TextAudioSpeakerLoader(torch.utils.data.Dataset):\r\n    \"\"\"\r\n        1) loads audio, speaker_id, text pairs\r\n        2) normalizes text and converts them to sequences of integers\r\n        3) computes spectrograms from audio files.\r\n    \"\"\"\r\n\r\n    def __init__(self, audiopaths_sid_text, hparams, symbols):\r\n        self.audiopaths_sid_text = load_filepaths_and_text(audiopaths_sid_text)\r\n        self.text_cleaners = hparams.text_cleaners\r\n        self.max_wav_value = hparams.max_wav_value\r\n        self.sampling_rate = hparams.sampling_rate\r\n        self.filter_length = hparams.filter_length\r\n        self.hop_length = hparams.hop_length\r\n        self.win_length = hparams.win_length\r\n        self.sampling_rate = hparams.sampling_rate\r\n\r\n        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\r\n\r\n        self.add_blank = hparams.add_blank\r\n        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\r\n        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\r\n        self.symbols = symbols\r\n\r\n        random.seed(1234)\r\n        random.shuffle(self.audiopaths_sid_text)\r\n        self._filter()\r\n\r\n    def _filter(self):\r\n        \"\"\"\r\n        Filter text & store spec lengths\r\n        \"\"\"\r\n        # Store spectrogram lengths for Bucketing\r\n        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\r\n        # spec_length = wav_length // hop_length\r\n\r\n        audiopaths_sid_text_new = []\r\n        lengths = []\r\n        for audiopath, sid, text in self.audiopaths_sid_text:\r\n            # audiopath = \"./user_voice/\" + audiopath\r\n\r\n            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\r\n                audiopaths_sid_text_new.append([audiopath, sid, text])\r\n                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\r\n        self.audiopaths_sid_text = audiopaths_sid_text_new\r\n        self.lengths = lengths\r\n\r\n    def get_audio_text_speaker_pair(self, audiopath_sid_text):\r\n        # separate filename, speaker_id and text\r\n        audiopath, sid, text = audiopath_sid_text[0], audiopath_sid_text[1], audiopath_sid_text[2]\r\n        text = self.get_text(text)\r\n        spec, wav = self.get_audio(audiopath)\r\n        sid = self.get_sid(sid)\r\n        return (text, spec, wav, sid)\r\n\r\n    def get_audio(self, filename):\r\n        # audio, sampling_rate = load_wav_to_torch(filename)\r\n        # if sampling_rate != self.sampling_rate:\r\n        #     raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\r\n        #         sampling_rate, self.sampling_rate))\r\n        # audio_norm = audio / self.max_wav_value if audio.max() > 10 else audio\r\n        # audio_norm = audio_norm.unsqueeze(0)\r\n        audio_norm, sampling_rate = torchaudio.load(filename, frame_offset=0, num_frames=-1, normalize=True, channels_first=True)\r\n        # spec_filename = filename.replace(\".wav\", \".spec.pt\")\r\n        # if os.path.exists(spec_filename):\r\n        #     spec = torch.load(spec_filename)\r\n        # else:\r\n        #     try:\r\n        spec = spectrogram_torch(audio_norm, self.filter_length,\r\n                                 self.sampling_rate, self.hop_length, self.win_length,\r\n                                 center=False)\r\n        spec = spec.squeeze(0)\r\n            # except NotImplementedError:\r\n            #     print(\"?\")\r\n            # spec = torch.squeeze(spec, 0)\r\n            # torch.save(spec, spec_filename)\r\n        return spec, audio_norm\r\n\r\n    def get_text(self, text):\r\n        if self.cleaned_text:\r\n            text_norm = cleaned_text_to_sequence(text, self.symbols)\r\n        else:\r\n            text_norm = text_to_sequence(text, self.text_cleaners)\r\n        if self.add_blank:\r\n            text_norm = commons.intersperse(text_norm, 0)\r\n        text_norm = torch.LongTensor(text_norm)\r\n        return text_norm\r\n\r\n    def get_sid(self, sid):\r\n        sid = torch.LongTensor([int(sid)])\r\n        return sid\r\n\r\n    def __getitem__(self, index):\r\n        return self.get_audio_text_speaker_pair(self.audiopaths_sid_text[index])\r\n\r\n    def __len__(self):\r\n        return len(self.audiopaths_sid_text)\r\n\r\n\r\nclass TextAudioSpeakerCollate():\r\n    \"\"\" Zero-pads model inputs and targets\r\n    \"\"\"\r\n\r\n    def __init__(self, return_ids=False):\r\n        self.return_ids = return_ids\r\n\r\n    def __call__(self, batch):\r\n        \"\"\"Collate's training batch from normalized text, audio and speaker identities\r\n        PARAMS\r\n        ------\r\n        batch: [text_normalized, spec_normalized, wav_normalized, sid]\r\n        \"\"\"\r\n        # Right zero-pad all one-hot text sequences to max input length\r\n        _, ids_sorted_decreasing = torch.sort(\r\n            torch.LongTensor([x[1].size(1) for x in batch]),\r\n            dim=0, descending=True)\r\n\r\n        max_text_len = max([len(x[0]) for x in batch])\r\n        max_spec_len = max([x[1].size(1) for x in batch])\r\n        max_wav_len = max([x[2].size(1) for x in batch])\r\n\r\n        text_lengths = torch.LongTensor(len(batch))\r\n        spec_lengths = torch.LongTensor(len(batch))\r\n        wav_lengths = torch.LongTensor(len(batch))\r\n        sid = torch.LongTensor(len(batch))\r\n\r\n        text_padded = torch.LongTensor(len(batch), max_text_len)\r\n        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\r\n        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\r\n        text_padded.zero_()\r\n        spec_padded.zero_()\r\n        wav_padded.zero_()\r\n        for i in range(len(ids_sorted_decreasing)):\r\n            row = batch[ids_sorted_decreasing[i]]\r\n\r\n            text = row[0]\r\n            text_padded[i, :text.size(0)] = text\r\n            text_lengths[i] = text.size(0)\r\n\r\n            spec = row[1]\r\n            spec_padded[i, :, :spec.size(1)] = spec\r\n            spec_lengths[i] = spec.size(1)\r\n\r\n            wav = row[2]\r\n            wav_padded[i, :, :wav.size(1)] = wav\r\n            wav_lengths[i] = wav.size(1)\r\n\r\n            sid[i] = row[3]\r\n\r\n        if self.return_ids:\r\n            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid, ids_sorted_decreasing\r\n        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid\r\n\r\n\r\nclass DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):\r\n    \"\"\"\r\n    Maintain similar input lengths in a batch.\r\n    Length groups are specified by boundaries.\r\n    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\r\n\r\n    It removes samples which are not included in the boundaries.\r\n    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\r\n    \"\"\"\r\n\r\n    def __init__(self, dataset, batch_size, boundaries, num_replicas=None, rank=None, shuffle=True):\r\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\r\n        self.lengths = dataset.lengths\r\n        self.batch_size = batch_size\r\n        self.boundaries = boundaries\r\n\r\n        self.buckets, self.num_samples_per_bucket = self._create_buckets()\r\n        self.total_size = sum(self.num_samples_per_bucket)\r\n        self.num_samples = self.total_size // self.num_replicas\r\n\r\n    def _create_buckets(self):\r\n        buckets = [[] for _ in range(len(self.boundaries) - 1)]\r\n        for i in range(len(self.lengths)):\r\n            length = self.lengths[i]\r\n            idx_bucket = self._bisect(length)\r\n            if idx_bucket != -1:\r\n                buckets[idx_bucket].append(i)\r\n\r\n        try: \r\n            for i in range(len(buckets) - 1, 0, -1):\r\n                if len(buckets[i]) == 0:\r\n                    buckets.pop(i)\r\n                    self.boundaries.pop(i + 1)\r\n            assert all(len(bucket) > 0 for bucket in buckets)\r\n        # When one bucket is not traversed\r\n        except Exception as e:\r\n            print('Bucket warning ', e)\r\n            for i in range(len(buckets) - 1, -1, -1):\r\n                if len(buckets[i]) == 0:\r\n                    buckets.pop(i)\r\n                    self.boundaries.pop(i + 1)\r\n\r\n        num_samples_per_bucket = []\r\n        for i in range(len(buckets)):\r\n            len_bucket = len(buckets[i])\r\n            total_batch_size = self.num_replicas * self.batch_size\r\n            rem = (total_batch_size - (len_bucket % total_batch_size)) % total_batch_size\r\n            num_samples_per_bucket.append(len_bucket + rem)\r\n        return buckets, num_samples_per_bucket\r\n\r\n    def __iter__(self):\r\n        # deterministically shuffle based on epoch\r\n        g = torch.Generator()\r\n        g.manual_seed(self.epoch)\r\n\r\n        indices = []\r\n        if self.shuffle:\r\n            for bucket in self.buckets:\r\n                indices.append(torch.randperm(len(bucket), generator=g).tolist())\r\n        else:\r\n            for bucket in self.buckets:\r\n                indices.append(list(range(len(bucket))))\r\n\r\n        batches = []\r\n        for i in range(len(self.buckets)):\r\n            bucket = self.buckets[i]\r\n            len_bucket = len(bucket)\r\n            ids_bucket = indices[i]\r\n            num_samples_bucket = self.num_samples_per_bucket[i]\r\n\r\n            # add extra samples to make it evenly divisible\r\n            rem = num_samples_bucket - len_bucket\r\n            ids_bucket = ids_bucket + ids_bucket * (rem // len_bucket) + ids_bucket[:(rem % len_bucket)]\r\n\r\n            # subsample\r\n            ids_bucket = ids_bucket[self.rank::self.num_replicas]\r\n\r\n            # batching\r\n            for j in range(len(ids_bucket) // self.batch_size):\r\n                batch = [bucket[idx] for idx in ids_bucket[j * self.batch_size:(j + 1) * self.batch_size]]\r\n                batches.append(batch)\r\n\r\n        if self.shuffle:\r\n            batch_ids = torch.randperm(len(batches), generator=g).tolist()\r\n            batches = [batches[i] for i in batch_ids]\r\n        self.batches = batches\r\n\r\n        assert len(self.batches) * self.batch_size == self.num_samples\r\n        return iter(self.batches)\r\n\r\n    def _bisect(self, x, lo=0, hi=None):\r\n        if hi is None:\r\n            hi = len(self.boundaries) - 1\r\n\r\n        if hi > lo:\r\n            mid = (hi + lo) // 2\r\n            if self.boundaries[mid] < x and x <= self.boundaries[mid + 1]:\r\n                return mid\r\n            elif x <= self.boundaries[mid]:\r\n                return self._bisect(x, lo, mid)\r\n            else:\r\n                return self._bisect(x, mid + 1, hi)\r\n        else:\r\n            return -1\r\n\r\n    def __len__(self):\r\n        return self.num_samples // self.batch_size\r\n"
        },
        {
          "name": "finetune_speaker_v2.py",
          "type": "blob",
          "size": 14.5302734375,
          "content": "import os\nimport json\nimport argparse\nimport itertools\nimport math\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\n\nimport librosa\nimport logging\n\nlogging.getLogger('numba').setLevel(logging.WARNING)\n\nimport commons\nimport utils\nfrom data_utils import (\n  TextAudioSpeakerLoader,\n  TextAudioSpeakerCollate,\n  DistributedBucketSampler\n)\nfrom models import (\n  SynthesizerTrn,\n  MultiPeriodDiscriminator,\n)\nfrom losses import (\n  generator_loss,\n  discriminator_loss,\n  feature_loss,\n  kl_loss\n)\nfrom mel_processing import mel_spectrogram_torch, spec_to_mel_torch\n\n\ntorch.backends.cudnn.benchmark = True\nglobal_step = 0\n\n\ndef main():\n  \"\"\"Assume Single Node Multi GPUs Training Only\"\"\"\n  assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n\n  n_gpus = torch.cuda.device_count()\n  os.environ['MASTER_ADDR'] = 'localhost'\n  os.environ['MASTER_PORT'] = '8000'\n\n  hps = utils.get_hparams()\n  mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hps,))\n\n\ndef run(rank, n_gpus, hps):\n  global global_step\n  symbols = hps['symbols']\n  if rank == 0:\n    logger = utils.get_logger(hps.model_dir)\n    logger.info(hps)\n    utils.check_git_hash(hps.model_dir)\n    writer = SummaryWriter(log_dir=hps.model_dir)\n    writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n\n  # Use gloo backend on Windows for Pytorch\n  dist.init_process_group(backend=  'gloo' if os.name == 'nt' else 'nccl', init_method='env://', world_size=n_gpus, rank=rank)\n  torch.manual_seed(hps.train.seed)\n  torch.cuda.set_device(rank)\n\n  train_dataset = TextAudioSpeakerLoader(hps.data.training_files, hps.data, symbols)\n  train_sampler = DistributedBucketSampler(\n      train_dataset,\n      hps.train.batch_size,\n      [32,300,400,500,600,700,800,900,1000],\n      num_replicas=n_gpus,\n      rank=rank,\n      shuffle=True)\n  collate_fn = TextAudioSpeakerCollate()\n  train_loader = DataLoader(train_dataset, num_workers=2, shuffle=False, pin_memory=True,\n      collate_fn=collate_fn, batch_sampler=train_sampler)\n  # train_loader = DataLoader(train_dataset, batch_size=hps.train.batch_size, num_workers=2, shuffle=False, pin_memory=True,\n  #                           collate_fn=collate_fn)\n  if rank == 0:\n    eval_dataset = TextAudioSpeakerLoader(hps.data.validation_files, hps.data, symbols)\n    eval_loader = DataLoader(eval_dataset, num_workers=0, shuffle=False,\n        batch_size=hps.train.batch_size, pin_memory=True,\n        drop_last=False, collate_fn=collate_fn)\n\n  net_g = SynthesizerTrn(\n      len(symbols),\n      hps.data.filter_length // 2 + 1,\n      hps.train.segment_size // hps.data.hop_length,\n      n_speakers=hps.data.n_speakers,\n      **hps.model).cuda(rank)\n  net_d = MultiPeriodDiscriminator(hps.model.use_spectral_norm).cuda(rank)\n\n  # load existing model\n  if hps.cont:\n      try:\n          _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"G_latest.pth\"), net_g, None)\n          _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, \"D_latest.pth\"), net_d, None)\n          global_step = (epoch_str - 1) * len(train_loader)\n      except:\n          print(\"Failed to find latest checkpoint, loading G_0.pth...\")\n          if hps.train_with_pretrained_model:\n              print(\"Train with pretrained model...\")\n              _, _, _, epoch_str = utils.load_checkpoint(\"./pretrained_models/G_0.pth\", net_g, None)\n              _, _, _, epoch_str = utils.load_checkpoint(\"./pretrained_models/D_0.pth\", net_d, None)\n          else:\n              print(\"Train without pretrained model...\")\n          epoch_str = 1\n          global_step = 0\n  else:\n      if hps.train_with_pretrained_model:\n          print(\"Train with pretrained model...\")\n          _, _, _, epoch_str = utils.load_checkpoint(\"./pretrained_models/G_0.pth\", net_g, None)\n          _, _, _, epoch_str = utils.load_checkpoint(\"./pretrained_models/D_0.pth\", net_d, None)\n      else:\n          print(\"Train without pretrained model...\")\n      epoch_str = 1\n      global_step = 0\n  # freeze all other layers except speaker embedding\n  for p in net_g.parameters():\n      p.requires_grad = True\n  for p in net_d.parameters():\n      p.requires_grad = True\n  # for p in net_d.parameters():\n  #     p.requires_grad = False\n  # net_g.emb_g.weight.requires_grad = True\n  optim_g = torch.optim.AdamW(\n      net_g.parameters(),\n      hps.train.learning_rate,\n      betas=hps.train.betas,\n      eps=hps.train.eps)\n  optim_d = torch.optim.AdamW(\n      net_d.parameters(),\n      hps.train.learning_rate,\n      betas=hps.train.betas,\n      eps=hps.train.eps)\n  # optim_d = None\n  net_g = DDP(net_g, device_ids=[rank])\n  net_d = DDP(net_d, device_ids=[rank])\n\n  scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=hps.train.lr_decay)\n  scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=hps.train.lr_decay)\n\n  scaler = GradScaler(enabled=hps.train.fp16_run)\n\n  for epoch in range(epoch_str, hps.train.epochs + 1):\n    if rank==0:\n      train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, eval_loader], logger, [writer, writer_eval])\n    else:\n      train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, None], None, None)\n    scheduler_g.step()\n    scheduler_d.step()\n\n\ndef train_and_evaluate(rank, epoch, hps, nets, optims, schedulers, scaler, loaders, logger, writers):\n  net_g, net_d = nets\n  optim_g, optim_d = optims\n  scheduler_g, scheduler_d = schedulers\n  train_loader, eval_loader = loaders\n  if writers is not None:\n    writer, writer_eval = writers\n\n  # train_loader.batch_sampler.set_epoch(epoch)\n  global global_step\n\n  net_g.train()\n  net_d.train()\n  for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths, speakers) in enumerate(tqdm(train_loader)):\n    x, x_lengths = x.cuda(rank, non_blocking=True), x_lengths.cuda(rank, non_blocking=True)\n    spec, spec_lengths = spec.cuda(rank, non_blocking=True), spec_lengths.cuda(rank, non_blocking=True)\n    y, y_lengths = y.cuda(rank, non_blocking=True), y_lengths.cuda(rank, non_blocking=True)\n    speakers = speakers.cuda(rank, non_blocking=True)\n\n    with autocast(enabled=hps.train.fp16_run):\n      y_hat, l_length, attn, ids_slice, x_mask, z_mask,\\\n      (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths, speakers)\n\n      mel = spec_to_mel_torch(\n          spec,\n          hps.data.filter_length,\n          hps.data.n_mel_channels,\n          hps.data.sampling_rate,\n          hps.data.mel_fmin,\n          hps.data.mel_fmax)\n      y_mel = commons.slice_segments(mel, ids_slice, hps.train.segment_size // hps.data.hop_length)\n      y_hat_mel = mel_spectrogram_torch(\n          y_hat.squeeze(1),\n          hps.data.filter_length,\n          hps.data.n_mel_channels,\n          hps.data.sampling_rate,\n          hps.data.hop_length,\n          hps.data.win_length,\n          hps.data.mel_fmin,\n          hps.data.mel_fmax\n      )\n\n      y = commons.slice_segments(y, ids_slice * hps.data.hop_length, hps.train.segment_size) # slice\n\n      # Discriminator\n      y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n      with autocast(enabled=False):\n        loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g)\n        loss_disc_all = loss_disc\n    optim_d.zero_grad()\n    scaler.scale(loss_disc_all).backward()\n    scaler.unscale_(optim_d)\n    grad_norm_d = commons.clip_grad_value_(net_d.parameters(), None)\n    scaler.step(optim_d)\n\n    with autocast(enabled=hps.train.fp16_run):\n      # Generator\n      y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n      with autocast(enabled=False):\n        loss_dur = torch.sum(l_length.float())\n        loss_mel = F.l1_loss(y_mel, y_hat_mel) * hps.train.c_mel\n        loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * hps.train.c_kl\n\n        loss_fm = feature_loss(fmap_r, fmap_g)\n        loss_gen, losses_gen = generator_loss(y_d_hat_g)\n        loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n    optim_g.zero_grad()\n    scaler.scale(loss_gen_all).backward()\n    scaler.unscale_(optim_g)\n    grad_norm_g = commons.clip_grad_value_(net_g.parameters(), None)\n    scaler.step(optim_g)\n    scaler.update()\n\n    if rank==0:\n      if global_step % hps.train.log_interval == 0:\n        lr = optim_g.param_groups[0]['lr']\n        losses = [loss_disc, loss_gen, loss_fm, loss_mel, loss_dur, loss_kl]\n        logger.info('Train Epoch: {} [{:.0f}%]'.format(\n          epoch,\n          100. * batch_idx / len(train_loader)))\n        logger.info([x.item() for x in losses] + [global_step, lr])\n\n        scalar_dict = {\"loss/g/total\": loss_gen_all, \"loss/d/total\": loss_disc_all, \"learning_rate\": lr, \"grad_norm_g\": grad_norm_g}\n        scalar_dict.update({\"loss/g/fm\": loss_fm, \"loss/g/mel\": loss_mel, \"loss/g/dur\": loss_dur, \"loss/g/kl\": loss_kl})\n\n        scalar_dict.update({\"loss/g/{}\".format(i): v for i, v in enumerate(losses_gen)})\n        scalar_dict.update({\"loss/d_r/{}\".format(i): v for i, v in enumerate(losses_disc_r)})\n        scalar_dict.update({\"loss/d_g/{}\".format(i): v for i, v in enumerate(losses_disc_g)})\n        image_dict = {\n            \"slice/mel_org\": utils.plot_spectrogram_to_numpy(y_mel[0].data.cpu().numpy()),\n            \"slice/mel_gen\": utils.plot_spectrogram_to_numpy(y_hat_mel[0].data.cpu().numpy()),\n            \"all/mel\": utils.plot_spectrogram_to_numpy(mel[0].data.cpu().numpy()),\n            \"all/attn\": utils.plot_alignment_to_numpy(attn[0,0].data.cpu().numpy())\n        }\n        utils.summarize(\n          writer=writer,\n          global_step=global_step,\n          images=image_dict,\n          scalars=scalar_dict)\n\n      if global_step % hps.train.eval_interval == 0:\n        evaluate(hps, net_g, eval_loader, writer_eval)\n        \n        utils.save_checkpoint(net_g, None, hps.train.learning_rate, epoch,\n                              os.path.join(hps.model_dir, \"G_latest.pth\"))\n        \n        utils.save_checkpoint(net_d, None, hps.train.learning_rate, epoch,\n                              os.path.join(hps.model_dir, \"D_latest.pth\"))\n        # save to google drive\n        if os.path.exists(\"/content/drive/MyDrive/\"):\n            utils.save_checkpoint(net_g, None, hps.train.learning_rate, epoch,\n                                  os.path.join(\"/content/drive/MyDrive/\", \"G_latest.pth\"))\n\n            utils.save_checkpoint(net_d, None, hps.train.learning_rate, epoch,\n                                  os.path.join(\"/content/drive/MyDrive/\", \"D_latest.pth\"))\n        if hps.preserved > 0:\n          utils.save_checkpoint(net_g, None, hps.train.learning_rate, epoch,\n                                  os.path.join(hps.model_dir, \"G_{}.pth\".format(global_step)))\n          utils.save_checkpoint(net_d, None, hps.train.learning_rate, epoch,\n                                  os.path.join(hps.model_dir, \"D_{}.pth\".format(global_step)))\n          old_g = utils.oldest_checkpoint_path(hps.model_dir, \"G_[0-9]*.pth\",\n                                               preserved=hps.preserved)  # Preserve 4 (default) historical checkpoints.\n          old_d = utils.oldest_checkpoint_path(hps.model_dir, \"D_[0-9]*.pth\", preserved=hps.preserved)\n          if os.path.exists(old_g):\n            print(f\"remove {old_g}\")\n            os.remove(old_g)\n          if os.path.exists(old_d):\n            print(f\"remove {old_d}\")\n            os.remove(old_d)\n          if os.path.exists(\"/content/drive/MyDrive/\"):\n              utils.save_checkpoint(net_g, None, hps.train.learning_rate, epoch,\n                                    os.path.join(\"/content/drive/MyDrive/\", \"G_{}.pth\".format(global_step)))\n              utils.save_checkpoint(net_d, None, hps.train.learning_rate, epoch,\n                                    os.path.join(\"/content/drive/MyDrive/\", \"D_{}.pth\".format(global_step)))\n              old_g = utils.oldest_checkpoint_path(\"/content/drive/MyDrive/\", \"G_[0-9]*.pth\",\n                                                   preserved=hps.preserved)  # Preserve 4 (default) historical checkpoints.\n              old_d = utils.oldest_checkpoint_path(\"/content/drive/MyDrive/\", \"D_[0-9]*.pth\", preserved=hps.preserved)\n              if os.path.exists(old_g):\n                  print(f\"remove {old_g}\")\n                  os.remove(old_g)\n              if os.path.exists(old_d):\n                  print(f\"remove {old_d}\")\n                  os.remove(old_d)\n    global_step += 1\n    if epoch > hps.max_epochs:\n        print(\"Maximum epoch reached, closing training...\")\n        exit()\n\n  if rank == 0:\n    logger.info('====> Epoch: {}'.format(epoch))\n\n\ndef evaluate(hps, generator, eval_loader, writer_eval):\n    generator.eval()\n    with torch.no_grad():\n      for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths, speakers) in enumerate(eval_loader):\n        x, x_lengths = x.cuda(0), x_lengths.cuda(0)\n        spec, spec_lengths = spec.cuda(0), spec_lengths.cuda(0)\n        y, y_lengths = y.cuda(0), y_lengths.cuda(0)\n        speakers = speakers.cuda(0)\n\n        # remove else\n        x = x[:1]\n        x_lengths = x_lengths[:1]\n        spec = spec[:1]\n        spec_lengths = spec_lengths[:1]\n        y = y[:1]\n        y_lengths = y_lengths[:1]\n        speakers = speakers[:1]\n        break\n      y_hat, attn, mask, *_ = generator.module.infer(x, x_lengths, speakers, max_len=1000)\n      y_hat_lengths = mask.sum([1,2]).long() * hps.data.hop_length\n\n      mel = spec_to_mel_torch(\n        spec,\n        hps.data.filter_length,\n        hps.data.n_mel_channels,\n        hps.data.sampling_rate,\n        hps.data.mel_fmin,\n        hps.data.mel_fmax)\n      y_hat_mel = mel_spectrogram_torch(\n        y_hat.squeeze(1).float(),\n        hps.data.filter_length,\n        hps.data.n_mel_channels,\n        hps.data.sampling_rate,\n        hps.data.hop_length,\n        hps.data.win_length,\n        hps.data.mel_fmin,\n        hps.data.mel_fmax\n      )\n    image_dict = {\n      \"gen/mel\": utils.plot_spectrogram_to_numpy(y_hat_mel[0].cpu().numpy())\n    }\n    audio_dict = {\n      \"gen/audio\": y_hat[0,:,:y_hat_lengths[0]]\n    }\n    if global_step == 0:\n      image_dict.update({\"gt/mel\": utils.plot_spectrogram_to_numpy(mel[0].cpu().numpy())})\n      audio_dict.update({\"gt/audio\": y[0,:,:y_lengths[0]]})\n\n    utils.summarize(\n      writer=writer_eval,\n      global_step=global_step,\n      images=image_dict,\n      audios=audio_dict,\n      audio_sampling_rate=hps.data.sampling_rate\n    )\n    generator.train()\n\n\nif __name__ == \"__main__\":\n  main()\n"
        },
        {
          "name": "losses.py",
          "type": "blob",
          "size": 1.3427734375,
          "content": "import torch \r\nfrom torch.nn import functional as F\r\n\r\nimport commons\r\n\r\n\r\ndef feature_loss(fmap_r, fmap_g):\r\n  loss = 0\r\n  for dr, dg in zip(fmap_r, fmap_g):\r\n    for rl, gl in zip(dr, dg):\r\n      rl = rl.float().detach()\r\n      gl = gl.float()\r\n      loss += torch.mean(torch.abs(rl - gl))\r\n\r\n  return loss * 2 \r\n\r\n\r\ndef discriminator_loss(disc_real_outputs, disc_generated_outputs):\r\n  loss = 0\r\n  r_losses = []\r\n  g_losses = []\r\n  for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\r\n    dr = dr.float()\r\n    dg = dg.float()\r\n    r_loss = torch.mean((1-dr)**2)\r\n    g_loss = torch.mean(dg**2)\r\n    loss += (r_loss + g_loss)\r\n    r_losses.append(r_loss.item())\r\n    g_losses.append(g_loss.item())\r\n\r\n  return loss, r_losses, g_losses\r\n\r\n\r\ndef generator_loss(disc_outputs):\r\n  loss = 0\r\n  gen_losses = []\r\n  for dg in disc_outputs:\r\n    dg = dg.float()\r\n    l = torch.mean((1-dg)**2)\r\n    gen_losses.append(l)\r\n    loss += l\r\n\r\n  return loss, gen_losses\r\n\r\n\r\ndef kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\r\n  \"\"\"\r\n  z_p, logs_q: [b, h, t_t]\r\n  m_p, logs_p: [b, h, t_t]\r\n  \"\"\"\r\n  z_p = z_p.float()\r\n  logs_q = logs_q.float()\r\n  m_p = m_p.float()\r\n  logs_p = logs_p.float()\r\n  z_mask = z_mask.float()\r\n\r\n  kl = logs_p - logs_q - 0.5\r\n  kl += 0.5 * ((z_p - m_p)**2) * torch.exp(-2. * logs_p)\r\n  kl = torch.sum(kl * z_mask)\r\n  l = kl / torch.sum(z_mask)\r\n  return l\r\n"
        },
        {
          "name": "mel_processing.py",
          "type": "blob",
          "size": 3.8818359375,
          "content": "import math\r\nimport os\r\nimport random\r\nimport torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\nimport torch.utils.data\r\nimport numpy as np\r\nimport librosa\r\nimport librosa.util as librosa_util\r\nfrom librosa.util import normalize, pad_center, tiny\r\nfrom scipy.signal import get_window\r\nfrom scipy.io.wavfile import read\r\nfrom librosa.filters import mel as librosa_mel_fn\r\n\r\nMAX_WAV_VALUE = 32768.0\r\n\r\n\r\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\r\n    \"\"\"\r\n    PARAMS\r\n    ------\r\n    C: compression factor\r\n    \"\"\"\r\n    return torch.log(torch.clamp(x, min=clip_val) * C)\r\n\r\n\r\ndef dynamic_range_decompression_torch(x, C=1):\r\n    \"\"\"\r\n    PARAMS\r\n    ------\r\n    C: compression factor used to compress\r\n    \"\"\"\r\n    return torch.exp(x) / C\r\n\r\n\r\ndef spectral_normalize_torch(magnitudes):\r\n    output = dynamic_range_compression_torch(magnitudes)\r\n    return output\r\n\r\n\r\ndef spectral_de_normalize_torch(magnitudes):\r\n    output = dynamic_range_decompression_torch(magnitudes)\r\n    return output\r\n\r\n\r\nmel_basis = {}\r\nhann_window = {}\r\n\r\n\r\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\r\n    if torch.min(y) < -1.:\r\n        print('min value is ', torch.min(y))\r\n    if torch.max(y) > 1.:\r\n        print('max value is ', torch.max(y))\r\n\r\n    global hann_window\r\n    dtype_device = str(y.dtype) + '_' + str(y.device)\r\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\r\n    if wnsize_dtype_device not in hann_window:\r\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\r\n\r\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\r\n    y = y.squeeze(1)\r\n\r\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\r\n                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\r\n\r\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\r\n    return spec\r\n\r\n\r\ndef spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\r\n    global mel_basis\r\n    dtype_device = str(spec.dtype) + '_' + str(spec.device)\r\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\r\n    if fmax_dtype_device not in mel_basis:\r\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\r\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\r\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\r\n    spec = spectral_normalize_torch(spec)\r\n    return spec\r\n\r\n\r\ndef mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\r\n    if torch.min(y) < -1.:\r\n        print('min value is ', torch.min(y))\r\n    if torch.max(y) > 1.:\r\n        print('max value is ', torch.max(y))\r\n\r\n    global mel_basis, hann_window\r\n    dtype_device = str(y.dtype) + '_' + str(y.device)\r\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\r\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\r\n    if fmax_dtype_device not in mel_basis:\r\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\r\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\r\n    if wnsize_dtype_device not in hann_window:\r\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\r\n\r\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\r\n    y = y.squeeze(1)\r\n\r\n    spec = torch.stft(y.float(), n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\r\n        center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\r\n\r\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\r\n\r\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\r\n    spec = spectral_normalize_torch(spec)\r\n\r\n    return spec\r\n"
        },
        {
          "name": "models.py",
          "type": "blob",
          "size": 19.4326171875,
          "content": "import copy\r\nimport math\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nimport commons\r\nimport modules\r\nimport attentions\r\nimport monotonic_align\r\n\r\nfrom torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\r\nfrom torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\r\nfrom commons import init_weights, get_padding\r\n\r\n\r\nclass StochasticDurationPredictor(nn.Module):\r\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\r\n    super().__init__()\r\n    filter_channels = in_channels # it needs to be removed from future version.\r\n    self.in_channels = in_channels\r\n    self.filter_channels = filter_channels\r\n    self.kernel_size = kernel_size\r\n    self.p_dropout = p_dropout\r\n    self.n_flows = n_flows\r\n    self.gin_channels = gin_channels\r\n\r\n    self.log_flow = modules.Log()\r\n    self.flows = nn.ModuleList()\r\n    self.flows.append(modules.ElementwiseAffine(2))\r\n    for i in range(n_flows):\r\n      self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\r\n      self.flows.append(modules.Flip())\r\n\r\n    self.post_pre = nn.Conv1d(1, filter_channels, 1)\r\n    self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\r\n    self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\r\n    self.post_flows = nn.ModuleList()\r\n    self.post_flows.append(modules.ElementwiseAffine(2))\r\n    for i in range(4):\r\n      self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\r\n      self.post_flows.append(modules.Flip())\r\n\r\n    self.pre = nn.Conv1d(in_channels, filter_channels, 1)\r\n    self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\r\n    self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\r\n    if gin_channels != 0:\r\n      self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\r\n\r\n  def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\r\n    x = torch.detach(x)\r\n    x = self.pre(x)\r\n    if g is not None:\r\n      g = torch.detach(g)\r\n      x = x + self.cond(g)\r\n    x = self.convs(x, x_mask)\r\n    x = self.proj(x) * x_mask\r\n\r\n    if not reverse:\r\n      flows = self.flows\r\n      assert w is not None\r\n\r\n      logdet_tot_q = 0\r\n      h_w = self.post_pre(w)\r\n      h_w = self.post_convs(h_w, x_mask)\r\n      h_w = self.post_proj(h_w) * x_mask\r\n      e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\r\n      z_q = e_q\r\n      for flow in self.post_flows:\r\n        z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\r\n        logdet_tot_q += logdet_q\r\n      z_u, z1 = torch.split(z_q, [1, 1], 1)\r\n      u = torch.sigmoid(z_u) * x_mask\r\n      z0 = (w - u) * x_mask\r\n      logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\r\n      logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\r\n\r\n      logdet_tot = 0\r\n      z0, logdet = self.log_flow(z0, x_mask)\r\n      logdet_tot += logdet\r\n      z = torch.cat([z0, z1], 1)\r\n      for flow in flows:\r\n        z, logdet = flow(z, x_mask, g=x, reverse=reverse)\r\n        logdet_tot = logdet_tot + logdet\r\n      nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\r\n      return nll + logq # [b]\r\n    else:\r\n      flows = list(reversed(self.flows))\r\n      flows = flows[:-2] + [flows[-1]] # remove a useless vflow\r\n      z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\r\n      for flow in flows:\r\n        z = flow(z, x_mask, g=x, reverse=reverse)\r\n      z0, z1 = torch.split(z, [1, 1], 1)\r\n      logw = z0\r\n      return logw\r\n\r\n\r\nclass DurationPredictor(nn.Module):\r\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, gin_channels=0):\r\n    super().__init__()\r\n\r\n    self.in_channels = in_channels\r\n    self.filter_channels = filter_channels\r\n    self.kernel_size = kernel_size\r\n    self.p_dropout = p_dropout\r\n    self.gin_channels = gin_channels\r\n\r\n    self.drop = nn.Dropout(p_dropout)\r\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size//2)\r\n    self.norm_1 = modules.LayerNorm(filter_channels)\r\n    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\r\n    self.norm_2 = modules.LayerNorm(filter_channels)\r\n    self.proj = nn.Conv1d(filter_channels, 1, 1)\r\n\r\n    if gin_channels != 0:\r\n      self.cond = nn.Conv1d(gin_channels, in_channels, 1)\r\n\r\n  def forward(self, x, x_mask, g=None):\r\n    x = torch.detach(x)\r\n    if g is not None:\r\n      g = torch.detach(g)\r\n      x = x + self.cond(g)\r\n    x = self.conv_1(x * x_mask)\r\n    x = torch.relu(x)\r\n    x = self.norm_1(x)\r\n    x = self.drop(x)\r\n    x = self.conv_2(x * x_mask)\r\n    x = torch.relu(x)\r\n    x = self.norm_2(x)\r\n    x = self.drop(x)\r\n    x = self.proj(x * x_mask)\r\n    return x * x_mask\r\n\r\n\r\nclass TextEncoder(nn.Module):\r\n  def __init__(self,\r\n      n_vocab,\r\n      out_channels,\r\n      hidden_channels,\r\n      filter_channels,\r\n      n_heads,\r\n      n_layers,\r\n      kernel_size,\r\n      p_dropout):\r\n    super().__init__()\r\n    self.n_vocab = n_vocab\r\n    self.out_channels = out_channels\r\n    self.hidden_channels = hidden_channels\r\n    self.filter_channels = filter_channels\r\n    self.n_heads = n_heads\r\n    self.n_layers = n_layers\r\n    self.kernel_size = kernel_size\r\n    self.p_dropout = p_dropout\r\n\r\n    self.emb = nn.Embedding(n_vocab, hidden_channels)\r\n    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\r\n\r\n    self.encoder = attentions.Encoder(\r\n      hidden_channels,\r\n      filter_channels,\r\n      n_heads,\r\n      n_layers,\r\n      kernel_size,\r\n      p_dropout)\r\n    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\r\n\r\n  def forward(self, x, x_lengths):\r\n    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\r\n    x = torch.transpose(x, 1, -1) # [b, h, t]\r\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\r\n\r\n    x = self.encoder(x * x_mask, x_mask)\r\n    stats = self.proj(x) * x_mask\r\n\r\n    m, logs = torch.split(stats, self.out_channels, dim=1)\r\n    return x, m, logs, x_mask\r\n\r\n\r\nclass ResidualCouplingBlock(nn.Module):\r\n  def __init__(self,\r\n      channels,\r\n      hidden_channels,\r\n      kernel_size,\r\n      dilation_rate,\r\n      n_layers,\r\n      n_flows=4,\r\n      gin_channels=0):\r\n    super().__init__()\r\n    self.channels = channels\r\n    self.hidden_channels = hidden_channels\r\n    self.kernel_size = kernel_size\r\n    self.dilation_rate = dilation_rate\r\n    self.n_layers = n_layers\r\n    self.n_flows = n_flows\r\n    self.gin_channels = gin_channels\r\n\r\n    self.flows = nn.ModuleList()\r\n    for i in range(n_flows):\r\n      self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\r\n      self.flows.append(modules.Flip())\r\n\r\n  def forward(self, x, x_mask, g=None, reverse=False):\r\n    if not reverse:\r\n      for flow in self.flows:\r\n        x, _ = flow(x, x_mask, g=g, reverse=reverse)\r\n    else:\r\n      for flow in reversed(self.flows):\r\n        x = flow(x, x_mask, g=g, reverse=reverse)\r\n    return x\r\n\r\n\r\nclass PosteriorEncoder(nn.Module):\r\n  def __init__(self,\r\n      in_channels,\r\n      out_channels,\r\n      hidden_channels,\r\n      kernel_size,\r\n      dilation_rate,\r\n      n_layers,\r\n      gin_channels=0):\r\n    super().__init__()\r\n    self.in_channels = in_channels\r\n    self.out_channels = out_channels\r\n    self.hidden_channels = hidden_channels\r\n    self.kernel_size = kernel_size\r\n    self.dilation_rate = dilation_rate\r\n    self.n_layers = n_layers\r\n    self.gin_channels = gin_channels\r\n\r\n    self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\r\n    self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\r\n    self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\r\n\r\n  def forward(self, x, x_lengths, g=None):\r\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\r\n    x = self.pre(x) * x_mask\r\n    x = self.enc(x, x_mask, g=g)\r\n    stats = self.proj(x) * x_mask\r\n    m, logs = torch.split(stats, self.out_channels, dim=1)\r\n    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\r\n    return z, m, logs, x_mask\r\n\r\n\r\nclass Generator(torch.nn.Module):\r\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\r\n        super(Generator, self).__init__()\r\n        self.num_kernels = len(resblock_kernel_sizes)\r\n        self.num_upsamples = len(upsample_rates)\r\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\r\n        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\r\n\r\n        self.ups = nn.ModuleList()\r\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\r\n            self.ups.append(weight_norm(\r\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\r\n                                k, u, padding=(k-u)//2)))\r\n\r\n        self.resblocks = nn.ModuleList()\r\n        for i in range(len(self.ups)):\r\n            ch = upsample_initial_channel//(2**(i+1))\r\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\r\n                self.resblocks.append(resblock(ch, k, d))\r\n\r\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\r\n        self.ups.apply(init_weights)\r\n\r\n        if gin_channels != 0:\r\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\r\n\r\n    def forward(self, x, g=None):\r\n        x = self.conv_pre(x)\r\n        if g is not None:\r\n          x = x + self.cond(g)\r\n\r\n        for i in range(self.num_upsamples):\r\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\r\n            x = self.ups[i](x)\r\n            xs = None\r\n            for j in range(self.num_kernels):\r\n                if xs is None:\r\n                    xs = self.resblocks[i*self.num_kernels+j](x)\r\n                else:\r\n                    xs += self.resblocks[i*self.num_kernels+j](x)\r\n            x = xs / self.num_kernels\r\n        x = F.leaky_relu(x)\r\n        x = self.conv_post(x)\r\n        x = torch.tanh(x)\r\n\r\n        return x\r\n\r\n    def remove_weight_norm(self):\r\n        print('Removing weight norm...')\r\n        for l in self.ups:\r\n            remove_weight_norm(l)\r\n        for l in self.resblocks:\r\n            l.remove_weight_norm()\r\n\r\n\r\nclass DiscriminatorP(torch.nn.Module):\r\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\r\n        super(DiscriminatorP, self).__init__()\r\n        self.period = period\r\n        self.use_spectral_norm = use_spectral_norm\r\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\r\n        self.convs = nn.ModuleList([\r\n            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\r\n            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\r\n            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\r\n            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\r\n            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(get_padding(kernel_size, 1), 0))),\r\n        ])\r\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\r\n\r\n    def forward(self, x):\r\n        fmap = []\r\n\r\n        # 1d to 2d\r\n        b, c, t = x.shape\r\n        if t % self.period != 0: # pad first\r\n            n_pad = self.period - (t % self.period)\r\n            x = F.pad(x, (0, n_pad), \"reflect\")\r\n            t = t + n_pad\r\n        x = x.view(b, c, t // self.period, self.period)\r\n\r\n        for l in self.convs:\r\n            x = l(x)\r\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\r\n            fmap.append(x)\r\n        x = self.conv_post(x)\r\n        fmap.append(x)\r\n        x = torch.flatten(x, 1, -1)\r\n\r\n        return x, fmap\r\n\r\n\r\nclass DiscriminatorS(torch.nn.Module):\r\n    def __init__(self, use_spectral_norm=False):\r\n        super(DiscriminatorS, self).__init__()\r\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\r\n        self.convs = nn.ModuleList([\r\n            norm_f(Conv1d(1, 16, 15, 1, padding=7)),\r\n            norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\r\n            norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\r\n            norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\r\n            norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\r\n            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\r\n        ])\r\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\r\n\r\n    def forward(self, x):\r\n        fmap = []\r\n\r\n        for l in self.convs:\r\n            x = l(x)\r\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\r\n            fmap.append(x)\r\n        x = self.conv_post(x)\r\n        fmap.append(x)\r\n        x = torch.flatten(x, 1, -1)\r\n\r\n        return x, fmap\r\n\r\n\r\nclass MultiPeriodDiscriminator(torch.nn.Module):\r\n    def __init__(self, use_spectral_norm=False):\r\n        super(MultiPeriodDiscriminator, self).__init__()\r\n        periods = [2,3,5,7,11]\r\n\r\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\r\n        discs = discs + [DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods]\r\n        self.discriminators = nn.ModuleList(discs)\r\n\r\n    def forward(self, y, y_hat):\r\n        y_d_rs = []\r\n        y_d_gs = []\r\n        fmap_rs = []\r\n        fmap_gs = []\r\n        for i, d in enumerate(self.discriminators):\r\n            y_d_r, fmap_r = d(y)\r\n            y_d_g, fmap_g = d(y_hat)\r\n            y_d_rs.append(y_d_r)\r\n            y_d_gs.append(y_d_g)\r\n            fmap_rs.append(fmap_r)\r\n            fmap_gs.append(fmap_g)\r\n\r\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\r\n\r\n\r\n\r\nclass SynthesizerTrn(nn.Module):\r\n  \"\"\"\r\n  Synthesizer for Training\r\n  \"\"\"\r\n\r\n  def __init__(self,\r\n    n_vocab,\r\n    spec_channels,\r\n    segment_size,\r\n    inter_channels,\r\n    hidden_channels,\r\n    filter_channels,\r\n    n_heads,\r\n    n_layers,\r\n    kernel_size,\r\n    p_dropout,\r\n    resblock,\r\n    resblock_kernel_sizes,\r\n    resblock_dilation_sizes,\r\n    upsample_rates,\r\n    upsample_initial_channel,\r\n    upsample_kernel_sizes,\r\n    n_speakers=0,\r\n    gin_channels=0,\r\n    use_sdp=True,\r\n    **kwargs):\r\n\r\n    super().__init__()\r\n    self.n_vocab = n_vocab\r\n    self.spec_channels = spec_channels\r\n    self.inter_channels = inter_channels\r\n    self.hidden_channels = hidden_channels\r\n    self.filter_channels = filter_channels\r\n    self.n_heads = n_heads\r\n    self.n_layers = n_layers\r\n    self.kernel_size = kernel_size\r\n    self.p_dropout = p_dropout\r\n    self.resblock = resblock\r\n    self.resblock_kernel_sizes = resblock_kernel_sizes\r\n    self.resblock_dilation_sizes = resblock_dilation_sizes\r\n    self.upsample_rates = upsample_rates\r\n    self.upsample_initial_channel = upsample_initial_channel\r\n    self.upsample_kernel_sizes = upsample_kernel_sizes\r\n    self.segment_size = segment_size\r\n    self.n_speakers = n_speakers\r\n    self.gin_channels = gin_channels\r\n\r\n    self.use_sdp = use_sdp\r\n\r\n    self.enc_p = TextEncoder(n_vocab,\r\n        inter_channels,\r\n        hidden_channels,\r\n        filter_channels,\r\n        n_heads,\r\n        n_layers,\r\n        kernel_size,\r\n        p_dropout)\r\n    self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\r\n    self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\r\n    self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\r\n\r\n    if use_sdp:\r\n      self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\r\n    else:\r\n      self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\r\n\r\n    if n_speakers >= 1:\r\n      self.emb_g = nn.Embedding(n_speakers, gin_channels)\r\n\r\n  def forward(self, x, x_lengths, y, y_lengths, sid=None):\r\n\r\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\r\n    if self.n_speakers > 0:\r\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\r\n    else:\r\n      g = None\r\n\r\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\r\n    z_p = self.flow(z, y_mask, g=g)\r\n\r\n    with torch.no_grad():\r\n      # negative cross-entropy\r\n      s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\r\n      neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\r\n      neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\r\n      neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\r\n      neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\r\n      neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\r\n\r\n      attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\r\n      attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\r\n\r\n    w = attn.sum(2)\r\n    if self.use_sdp:\r\n      l_length = self.dp(x, x_mask, w, g=g)\r\n      l_length = l_length / torch.sum(x_mask)\r\n    else:\r\n      logw_ = torch.log(w + 1e-6) * x_mask\r\n      logw = self.dp(x, x_mask, g=g)\r\n      l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging\r\n\r\n    # expand prior\r\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\r\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\r\n\r\n    z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\r\n    o = self.dec(z_slice, g=g)\r\n    return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\r\n\r\n  def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\r\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\r\n    if self.n_speakers > 0:\r\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\r\n    else:\r\n      g = None\r\n\r\n    if self.use_sdp:\r\n      logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\r\n    else:\r\n      logw = self.dp(x, x_mask, g=g)\r\n    w = torch.exp(logw) * x_mask * length_scale\r\n    w_ceil = torch.ceil(w)\r\n    y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\r\n    y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\r\n    attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\r\n    attn = commons.generate_path(w_ceil, attn_mask)\r\n\r\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\r\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\r\n\r\n    z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\r\n    z = self.flow(z_p, y_mask, g=g, reverse=True)\r\n    o = self.dec((z * y_mask)[:,:,:max_len], g=g)\r\n    return o, attn, y_mask, (z, z_p, m_p, logs_p)\r\n\r\n  def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\r\n    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\r\n    g_src = self.emb_g(sid_src).unsqueeze(-1)\r\n    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\r\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\r\n    z_p = self.flow(z, y_mask, g=g_src)\r\n    z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\r\n    o_hat = self.dec(z_hat * y_mask, g=g_tgt)\r\n    return o_hat, y_mask, (z, z_p, z_hat)\r\n"
        },
        {
          "name": "models_infer.py",
          "type": "blob",
          "size": 13.8232421875,
          "content": "import math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport commons\nimport modules\nimport attentions\n\nfrom torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\nfrom torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\nfrom commons import init_weights, get_padding\n\n\nclass StochasticDurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n    super().__init__()\n    filter_channels = in_channels # it needs to be removed from future version.\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.log_flow = modules.Log()\n    self.flows = nn.ModuleList()\n    self.flows.append(modules.ElementwiseAffine(2))\n    for i in range(n_flows):\n      self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.flows.append(modules.Flip())\n\n    self.post_pre = nn.Conv1d(1, filter_channels, 1)\n    self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    self.post_flows = nn.ModuleList()\n    self.post_flows.append(modules.ElementwiseAffine(2))\n    for i in range(4):\n      self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.post_flows.append(modules.Flip())\n\n    self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n    self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n  def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n    x = torch.detach(x)\n    x = self.pre(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.convs(x, x_mask)\n    x = self.proj(x) * x_mask\n\n    if not reverse:\n      flows = self.flows\n      assert w is not None\n\n      logdet_tot_q = 0\n      h_w = self.post_pre(w)\n      h_w = self.post_convs(h_w, x_mask)\n      h_w = self.post_proj(h_w) * x_mask\n      e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n      z_q = e_q\n      for flow in self.post_flows:\n        z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n        logdet_tot_q += logdet_q\n      z_u, z1 = torch.split(z_q, [1, 1], 1)\n      u = torch.sigmoid(z_u) * x_mask\n      z0 = (w - u) * x_mask\n      logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n      logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n\n      logdet_tot = 0\n      z0, logdet = self.log_flow(z0, x_mask)\n      logdet_tot += logdet\n      z = torch.cat([z0, z1], 1)\n      for flow in flows:\n        z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n        logdet_tot = logdet_tot + logdet\n      nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n      return nll + logq # [b]\n    else:\n      flows = list(reversed(self.flows))\n      flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n      z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n      for flow in flows:\n        z = flow(z, x_mask, g=x, reverse=reverse)\n      z0, z1 = torch.split(z, [1, 1], 1)\n      logw = z0\n      return logw\n\n\nclass DurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, gin_channels=0):\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.gin_channels = gin_channels\n\n    self.drop = nn.Dropout(p_dropout)\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_1 = modules.LayerNorm(filter_channels)\n    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_2 = modules.LayerNorm(filter_channels)\n    self.proj = nn.Conv1d(filter_channels, 1, 1)\n\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, in_channels, 1)\n\n  def forward(self, x, x_mask, g=None):\n    x = torch.detach(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.conv_1(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_1(x)\n    x = self.drop(x)\n    x = self.conv_2(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_2(x)\n    x = self.drop(x)\n    x = self.proj(x * x_mask)\n    return x * x_mask\n\n\nclass TextEncoder(nn.Module):\n  def __init__(self,\n      n_vocab,\n      out_channels,\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout):\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n\n    self.emb = nn.Embedding(n_vocab, hidden_channels)\n    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n    self.encoder = attentions.Encoder(\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout)\n    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths):\n    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n    x = torch.transpose(x, 1, -1) # [b, h, t]\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n\n    x = self.encoder(x * x_mask, x_mask)\n    stats = self.proj(x) * x_mask\n\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    return x, m, logs, x_mask\n\n\nclass ResidualCouplingBlock(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      n_flows=4,\n      gin_channels=0):\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.flows = nn.ModuleList()\n    for i in range(n_flows):\n      self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n      self.flows.append(modules.Flip())\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    if not reverse:\n      for flow in self.flows:\n        x, _ = flow(x, x_mask, g=g, reverse=reverse)\n    else:\n      for flow in reversed(self.flows):\n        x = flow(x, x_mask, g=g, reverse=reverse)\n    return x\n\n\nclass PosteriorEncoder(nn.Module):\n  def __init__(self,\n      in_channels,\n      out_channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      gin_channels=0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n\n    self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n    self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n    self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths, g=None):\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n    x = self.pre(x) * x_mask\n    x = self.enc(x, x_mask, g=g)\n    stats = self.proj(x) * x_mask\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n    return z, m, logs, x_mask\n\n\nclass Generator(torch.nn.Module):\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n          x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n\n\n\nclass SynthesizerTrn(nn.Module):\n  \"\"\"\n  Synthesizer for Training\n  \"\"\"\n\n  def __init__(self,\n    n_vocab,\n    spec_channels,\n    segment_size,\n    inter_channels,\n    hidden_channels,\n    filter_channels,\n    n_heads,\n    n_layers,\n    kernel_size,\n    p_dropout,\n    resblock,\n    resblock_kernel_sizes,\n    resblock_dilation_sizes,\n    upsample_rates,\n    upsample_initial_channel,\n    upsample_kernel_sizes,\n    n_speakers=0,\n    gin_channels=0,\n    use_sdp=True,\n    **kwargs):\n\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.spec_channels = spec_channels\n    self.inter_channels = inter_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.resblock = resblock\n    self.resblock_kernel_sizes = resblock_kernel_sizes\n    self.resblock_dilation_sizes = resblock_dilation_sizes\n    self.upsample_rates = upsample_rates\n    self.upsample_initial_channel = upsample_initial_channel\n    self.upsample_kernel_sizes = upsample_kernel_sizes\n    self.segment_size = segment_size\n    self.n_speakers = n_speakers\n    self.gin_channels = gin_channels\n\n    self.use_sdp = use_sdp\n\n    self.enc_p = TextEncoder(n_vocab,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout)\n    self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n    self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n    self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n\n    if use_sdp:\n      self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n    else:\n      self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n\n    if n_speakers > 1:\n      self.emb_g = nn.Embedding(n_speakers, gin_channels)\n\n  def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n    if self.n_speakers > 0:\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n    else:\n      g = None\n\n    if self.use_sdp:\n      logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n    else:\n      logw = self.dp(x, x_mask, g=g)\n    w = torch.exp(logw) * x_mask * length_scale\n    w_ceil = torch.ceil(w)\n    y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n    y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n    attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n    attn = commons.generate_path(w_ceil, attn_mask)\n\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n\n    z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n    z = self.flow(z_p, y_mask, g=g, reverse=True)\n    o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n    return o, attn, y_mask, (z, z_p, m_p, logs_p)\n\n  def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n    g_src = self.emb_g(sid_src).unsqueeze(-1)\n    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n    z_p = self.flow(z, y_mask, g=g_src)\n    z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n    o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n    return o_hat, y_mask, (z, z_p, z_hat)\n\n"
        },
        {
          "name": "modules.py",
          "type": "blob",
          "size": 12.857421875,
          "content": "import copy\nimport math\nimport numpy as np\nimport scipy\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\nfrom torch.nn.utils import weight_norm, remove_weight_norm\n\nimport commons\nfrom commons import init_weights, get_padding\nfrom transforms import piecewise_rational_quadratic_transform\n\n\nLRELU_SLOPE = 0.1\n\n\nclass LayerNorm(nn.Module):\n  def __init__(self, channels, eps=1e-5):\n    super().__init__()\n    self.channels = channels\n    self.eps = eps\n\n    self.gamma = nn.Parameter(torch.ones(channels))\n    self.beta = nn.Parameter(torch.zeros(channels))\n\n  def forward(self, x):\n    x = x.transpose(1, -1)\n    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n    return x.transpose(1, -1)\n\n \nclass ConvReluNorm(nn.Module):\n  def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, n_layers, p_dropout):\n    super().__init__()\n    self.in_channels = in_channels\n    self.hidden_channels = hidden_channels\n    self.out_channels = out_channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.p_dropout = p_dropout\n    assert n_layers > 1, \"Number of layers should be larger than 0.\"\n\n    self.conv_layers = nn.ModuleList()\n    self.norm_layers = nn.ModuleList()\n    self.conv_layers.append(nn.Conv1d(in_channels, hidden_channels, kernel_size, padding=kernel_size//2))\n    self.norm_layers.append(LayerNorm(hidden_channels))\n    self.relu_drop = nn.Sequential(\n        nn.ReLU(),\n        nn.Dropout(p_dropout))\n    for _ in range(n_layers-1):\n      self.conv_layers.append(nn.Conv1d(hidden_channels, hidden_channels, kernel_size, padding=kernel_size//2))\n      self.norm_layers.append(LayerNorm(hidden_channels))\n    self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n    self.proj.weight.data.zero_()\n    self.proj.bias.data.zero_()\n\n  def forward(self, x, x_mask):\n    x_org = x\n    for i in range(self.n_layers):\n      x = self.conv_layers[i](x * x_mask)\n      x = self.norm_layers[i](x)\n      x = self.relu_drop(x)\n    x = x_org + self.proj(x)\n    return x * x_mask\n\n\nclass DDSConv(nn.Module):\n  \"\"\"\n  Dilated and Depth-Separable Convolution\n  \"\"\"\n  def __init__(self, channels, kernel_size, n_layers, p_dropout=0.):\n    super().__init__()\n    self.channels = channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.p_dropout = p_dropout\n\n    self.drop = nn.Dropout(p_dropout)\n    self.convs_sep = nn.ModuleList()\n    self.convs_1x1 = nn.ModuleList()\n    self.norms_1 = nn.ModuleList()\n    self.norms_2 = nn.ModuleList()\n    for i in range(n_layers):\n      dilation = kernel_size ** i\n      padding = (kernel_size * dilation - dilation) // 2\n      self.convs_sep.append(nn.Conv1d(channels, channels, kernel_size, \n          groups=channels, dilation=dilation, padding=padding\n      ))\n      self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n      self.norms_1.append(LayerNorm(channels))\n      self.norms_2.append(LayerNorm(channels))\n\n  def forward(self, x, x_mask, g=None):\n    if g is not None:\n      x = x + g\n    for i in range(self.n_layers):\n      y = self.convs_sep[i](x * x_mask)\n      y = self.norms_1[i](y)\n      y = F.gelu(y)\n      y = self.convs_1x1[i](y)\n      y = self.norms_2[i](y)\n      y = F.gelu(y)\n      y = self.drop(y)\n      x = x + y\n    return x * x_mask\n\n\nclass WN(torch.nn.Module):\n  def __init__(self, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0):\n    super(WN, self).__init__()\n    assert(kernel_size % 2 == 1)\n    self.hidden_channels =hidden_channels\n    self.kernel_size = kernel_size,\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n    self.p_dropout = p_dropout\n\n    self.in_layers = torch.nn.ModuleList()\n    self.res_skip_layers = torch.nn.ModuleList()\n    self.drop = nn.Dropout(p_dropout)\n\n    if gin_channels != 0:\n      cond_layer = torch.nn.Conv1d(gin_channels, 2*hidden_channels*n_layers, 1)\n      self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name='weight')\n\n    for i in range(n_layers):\n      dilation = dilation_rate ** i\n      padding = int((kernel_size * dilation - dilation) / 2)\n      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n                                 dilation=dilation, padding=padding)\n      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n      self.in_layers.append(in_layer)\n\n      # last one is not necessary\n      if i < n_layers - 1:\n        res_skip_channels = 2 * hidden_channels\n      else:\n        res_skip_channels = hidden_channels\n\n      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n      self.res_skip_layers.append(res_skip_layer)\n\n  def forward(self, x, x_mask, g=None, **kwargs):\n    output = torch.zeros_like(x)\n    n_channels_tensor = torch.IntTensor([self.hidden_channels])\n\n    if g is not None:\n      g = self.cond_layer(g)\n\n    for i in range(self.n_layers):\n      x_in = self.in_layers[i](x)\n      if g is not None:\n        cond_offset = i * 2 * self.hidden_channels\n        g_l = g[:,cond_offset:cond_offset+2*self.hidden_channels,:]\n      else:\n        g_l = torch.zeros_like(x_in)\n\n      acts = commons.fused_add_tanh_sigmoid_multiply(\n          x_in,\n          g_l,\n          n_channels_tensor)\n      acts = self.drop(acts)\n\n      res_skip_acts = self.res_skip_layers[i](acts)\n      if i < self.n_layers - 1:\n        res_acts = res_skip_acts[:,:self.hidden_channels,:]\n        x = (x + res_acts) * x_mask\n        output = output + res_skip_acts[:,self.hidden_channels:,:]\n      else:\n        output = output + res_skip_acts\n    return output * x_mask\n\n  def remove_weight_norm(self):\n    if self.gin_channels != 0:\n      torch.nn.utils.remove_weight_norm(self.cond_layer)\n    for l in self.in_layers:\n      torch.nn.utils.remove_weight_norm(l)\n    for l in self.res_skip_layers:\n     torch.nn.utils.remove_weight_norm(l)\n\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.convs1 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n                               padding=get_padding(kernel_size, dilation[2])))\n        ])\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1)))\n        ])\n        self.convs2.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c2(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)\n\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.convs = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1])))\n        ])\n        self.convs.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)\n\n\nclass Log(nn.Module):\n  def forward(self, x, x_mask, reverse=False, **kwargs):\n    if not reverse:\n      y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n      logdet = torch.sum(-y, [1, 2])\n      return y, logdet\n    else:\n      x = torch.exp(x) * x_mask\n      return x\n    \n\nclass Flip(nn.Module):\n  def forward(self, x, *args, reverse=False, **kwargs):\n    x = torch.flip(x, [1])\n    if not reverse:\n      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n      return x, logdet\n    else:\n      return x\n\n\nclass ElementwiseAffine(nn.Module):\n  def __init__(self, channels):\n    super().__init__()\n    self.channels = channels\n    self.m = nn.Parameter(torch.zeros(channels,1))\n    self.logs = nn.Parameter(torch.zeros(channels,1))\n\n  def forward(self, x, x_mask, reverse=False, **kwargs):\n    if not reverse:\n      y = self.m + torch.exp(self.logs) * x\n      y = y * x_mask\n      logdet = torch.sum(self.logs * x_mask, [1,2])\n      return y, logdet\n    else:\n      x = (x - self.m) * torch.exp(-self.logs) * x_mask\n      return x\n\n\nclass ResidualCouplingLayer(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      p_dropout=0,\n      gin_channels=0,\n      mean_only=False):\n    assert channels % 2 == 0, \"channels should be divisible by 2\"\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.half_channels = channels // 2\n    self.mean_only = mean_only\n\n    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n    self.enc = WN(hidden_channels, kernel_size, dilation_rate, n_layers, p_dropout=p_dropout, gin_channels=gin_channels)\n    self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n    self.post.weight.data.zero_()\n    self.post.bias.data.zero_()\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n    h = self.pre(x0) * x_mask\n    h = self.enc(h, x_mask, g=g)\n    stats = self.post(h) * x_mask\n    if not self.mean_only:\n      m, logs = torch.split(stats, [self.half_channels]*2, 1)\n    else:\n      m = stats\n      logs = torch.zeros_like(m)\n\n    if not reverse:\n      x1 = m + x1 * torch.exp(logs) * x_mask\n      x = torch.cat([x0, x1], 1)\n      logdet = torch.sum(logs, [1,2])\n      return x, logdet\n    else:\n      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n      x = torch.cat([x0, x1], 1)\n      return x\n\n\nclass ConvFlow(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, n_layers, num_bins=10, tail_bound=5.0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.num_bins = num_bins\n    self.tail_bound = tail_bound\n    self.half_channels = in_channels // 2\n\n    self.pre = nn.Conv1d(self.half_channels, filter_channels, 1)\n    self.convs = DDSConv(filter_channels, kernel_size, n_layers, p_dropout=0.)\n    self.proj = nn.Conv1d(filter_channels, self.half_channels * (num_bins * 3 - 1), 1)\n    self.proj.weight.data.zero_()\n    self.proj.bias.data.zero_()\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n    h = self.pre(x0)\n    h = self.convs(h, x_mask, g=g)\n    h = self.proj(h) * x_mask\n\n    b, c, t = x0.shape\n    h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2) # [b, cx?, t] -> [b, c, t, ?]\n\n    unnormalized_widths = h[..., :self.num_bins] / math.sqrt(self.filter_channels)\n    unnormalized_heights = h[..., self.num_bins:2*self.num_bins] / math.sqrt(self.filter_channels)\n    unnormalized_derivatives = h[..., 2 * self.num_bins:]\n\n    x1, logabsdet = piecewise_rational_quadratic_transform(x1,\n        unnormalized_widths,\n        unnormalized_heights,\n        unnormalized_derivatives,\n        inverse=reverse,\n        tails='linear',\n        tail_bound=self.tail_bound\n    )\n\n    x = torch.cat([x0, x1], 1) * x_mask\n    logdet = torch.sum(logabsdet * x_mask, [1,2])\n    if not reverse:\n        return x, logdet\n    else:\n        return x\n"
        },
        {
          "name": "monotonic_align",
          "type": "tree",
          "content": null
        },
        {
          "name": "preprocess_v2.py",
          "type": "blob",
          "size": 6.546875,
          "content": "import os\nimport argparse\nimport json\nimport sys\nsys.setrecursionlimit(500000)  # Fix the error message of RecursionError: maximum recursion depth exceeded while calling a Python object.  You can change the number as you want.\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--add_auxiliary_data\", type=bool, help=\"Whether to add extra data as fine-tuning helper\")\n    parser.add_argument(\"--languages\", default=\"CJE\")\n    args = parser.parse_args()\n    if args.languages == \"CJE\":\n        langs = [\"[ZH]\", \"[JA]\", \"[EN]\"]\n    elif args.languages == \"CJ\":\n        langs = [\"[ZH]\", \"[JA]\"]\n    elif args.languages == \"C\":\n        langs = [\"[ZH]\"]\n    new_annos = []\n    # Source 1: transcribed short audios\n    if os.path.exists(\"short_character_anno.txt\"):\n        with open(\"short_character_anno.txt\", 'r', encoding='utf-8') as f:\n            short_character_anno = f.readlines()\n            new_annos += short_character_anno\n    # Source 2: transcribed long audio segments\n    if os.path.exists(\"./long_character_anno.txt\"):\n        with open(\"./long_character_anno.txt\", 'r', encoding='utf-8') as f:\n            long_character_anno = f.readlines()\n            new_annos += long_character_anno\n\n    # Get all speaker names\n    speakers = []\n    for line in new_annos:\n        path, speaker, text = line.split(\"|\")\n        if speaker not in speakers:\n            speakers.append(speaker)\n    assert (len(speakers) != 0), \"No audio file found. Please check your uploaded file structure.\"\n    # Source 3 (Optional): sampled audios as extra training helpers\n    if args.add_auxiliary_data:\n        with open(\"./sampled_audio4ft.txt\", 'r', encoding='utf-8') as f:\n            old_annos = f.readlines()\n        # filter old_annos according to supported languages\n        filtered_old_annos = []\n        for line in old_annos:\n            for lang in langs:\n                if lang in line:\n                    filtered_old_annos.append(line)\n        old_annos = filtered_old_annos\n        for line in old_annos:\n            path, speaker, text = line.split(\"|\")\n            if speaker not in speakers:\n                speakers.append(speaker)\n        num_old_voices = len(old_annos)\n        num_new_voices = len(new_annos)\n        # STEP 1: balance number of new & old voices\n        cc_duplicate = num_old_voices // num_new_voices\n        if cc_duplicate == 0:\n            cc_duplicate = 1\n\n\n        # STEP 2: modify config file\n        with open(\"./configs/finetune_speaker.json\", 'r', encoding='utf-8') as f:\n            hps = json.load(f)\n\n        # assign ids to new speakers\n        speaker2id = {}\n        for i, speaker in enumerate(speakers):\n            speaker2id[speaker] = i\n        # modify n_speakers\n        hps['data'][\"n_speakers\"] = len(speakers)\n        # overwrite speaker names\n        hps['speakers'] = speaker2id\n        hps['train']['log_interval'] = 10\n        hps['train']['eval_interval'] = 100\n        hps['train']['batch_size'] = 16\n        hps['data']['training_files'] = \"final_annotation_train.txt\"\n        hps['data']['validation_files'] = \"final_annotation_val.txt\"\n        # save modified config\n        with open(\"./configs/modified_finetune_speaker.json\", 'w', encoding='utf-8') as f:\n            json.dump(hps, f, indent=2)\n\n        # STEP 3: clean annotations, replace speaker names with assigned speaker IDs\n        import text\n        cleaned_new_annos = []\n        for i, line in enumerate(new_annos):\n            path, speaker, txt = line.split(\"|\")\n            if len(txt) > 150:\n                continue\n            cleaned_text = text._clean_text(txt, hps['data']['text_cleaners'])\n            cleaned_text += \"\\n\" if not cleaned_text.endswith(\"\\n\") else \"\"\n            cleaned_new_annos.append(path + \"|\" + str(speaker2id[speaker]) + \"|\" + cleaned_text)\n        cleaned_old_annos = []\n        for i, line in enumerate(old_annos):\n            path, speaker, txt = line.split(\"|\")\n            if len(txt) > 150:\n                continue\n            cleaned_text = text._clean_text(txt, hps['data']['text_cleaners'])\n            cleaned_text += \"\\n\" if not cleaned_text.endswith(\"\\n\") else \"\"\n            cleaned_old_annos.append(path + \"|\" + str(speaker2id[speaker]) + \"|\" + cleaned_text)\n        # merge with old annotation\n        final_annos = cleaned_old_annos + cc_duplicate * cleaned_new_annos\n        # save annotation file\n        with open(\"./final_annotation_train.txt\", 'w', encoding='utf-8') as f:\n            for line in final_annos:\n                f.write(line)\n        # save annotation file for validation\n        with open(\"./final_annotation_val.txt\", 'w', encoding='utf-8') as f:\n            for line in cleaned_new_annos:\n                f.write(line)\n        print(\"finished\")\n    else:\n        # Do not add extra helper data\n        # STEP 1: modify config file\n        with open(\"./configs/finetune_speaker.json\", 'r', encoding='utf-8') as f:\n            hps = json.load(f)\n\n        # assign ids to new speakers\n        speaker2id = {}\n        for i, speaker in enumerate(speakers):\n            speaker2id[speaker] = i\n        # modify n_speakers\n        hps['data'][\"n_speakers\"] = len(speakers)\n        # overwrite speaker names\n        hps['speakers'] = speaker2id\n        hps['train']['log_interval'] = 10\n        hps['train']['eval_interval'] = 100\n        hps['train']['batch_size'] = 16\n        hps['data']['training_files'] = \"final_annotation_train.txt\"\n        hps['data']['validation_files'] = \"final_annotation_val.txt\"\n        # save modified config\n        with open(\"./configs/modified_finetune_speaker.json\", 'w', encoding='utf-8') as f:\n            json.dump(hps, f, indent=2)\n\n        # STEP 2: clean annotations, replace speaker names with assigned speaker IDs\n        import text\n\n        cleaned_new_annos = []\n        for i, line in enumerate(new_annos):\n            path, speaker, txt = line.split(\"|\")\n            if len(txt) > 150:\n                continue\n            cleaned_text = text._clean_text(txt, hps['data']['text_cleaners']).replace(\"[ZH]\", \"\")\n            cleaned_text += \"\\n\" if not cleaned_text.endswith(\"\\n\") else \"\"\n            cleaned_new_annos.append(path + \"|\" + str(speaker2id[speaker]) + \"|\" + cleaned_text)\n\n        final_annos = cleaned_new_annos\n        # save annotation file\n        with open(\"./final_annotation_train.txt\", 'w', encoding='utf-8') as f:\n            for line in final_annos:\n                f.write(line)\n        # save annotation file for validation\n        with open(\"./final_annotation_val.txt\", 'w', encoding='utf-8') as f:\n            for line in cleaned_new_annos:\n                f.write(line)\n        print(\"finished\")\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.357421875,
          "content": "Cython==0.29.21\r\nlibrosa==0.9.2\r\nmatplotlib==3.3.1\r\nscikit-learn==1.0.2\r\nscipy\r\nnumpy==1.21.6\r\ntensorboard\r\ntorch\r\ntorchvision\r\ntorchaudio\r\nunidecode\r\npyopenjtalk-prebuilt\r\njamo\r\npypinyin\r\njieba\r\nprotobuf\r\ncn2an\r\ninflect\r\neng_to_ipa\r\nko_pron\r\nindic_transliteration==2.3.37\r\nnum_thai==0.0.5\r\nopencc==1.1.1\r\ndemucs\r\ngit+https://github.com/openai/whisper.git \r\ngradio\r\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "text",
          "type": "tree",
          "content": null
        },
        {
          "name": "transforms.py",
          "type": "blob",
          "size": 8.4794921875,
          "content": "import torch\r\nfrom torch.nn import functional as F\r\n\r\nimport numpy as np\r\n\r\n\r\nDEFAULT_MIN_BIN_WIDTH = 1e-3\r\nDEFAULT_MIN_BIN_HEIGHT = 1e-3\r\nDEFAULT_MIN_DERIVATIVE = 1e-3\r\n\r\n\r\ndef piecewise_rational_quadratic_transform(inputs, \r\n                                           unnormalized_widths,\r\n                                           unnormalized_heights,\r\n                                           unnormalized_derivatives,\r\n                                           inverse=False,\r\n                                           tails=None, \r\n                                           tail_bound=1.,\r\n                                           min_bin_width=DEFAULT_MIN_BIN_WIDTH,\r\n                                           min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\r\n                                           min_derivative=DEFAULT_MIN_DERIVATIVE):\r\n\r\n    if tails is None:\r\n        spline_fn = rational_quadratic_spline\r\n        spline_kwargs = {}\r\n    else:\r\n        spline_fn = unconstrained_rational_quadratic_spline\r\n        spline_kwargs = {\r\n            'tails': tails,\r\n            'tail_bound': tail_bound\r\n        }\r\n\r\n    outputs, logabsdet = spline_fn(\r\n            inputs=inputs,\r\n            unnormalized_widths=unnormalized_widths,\r\n            unnormalized_heights=unnormalized_heights,\r\n            unnormalized_derivatives=unnormalized_derivatives,\r\n            inverse=inverse,\r\n            min_bin_width=min_bin_width,\r\n            min_bin_height=min_bin_height,\r\n            min_derivative=min_derivative,\r\n            **spline_kwargs\r\n    )\r\n    return outputs, logabsdet\r\n\r\n\r\ndef searchsorted(bin_locations, inputs, eps=1e-6):\r\n    bin_locations[..., -1] += eps\r\n    return torch.sum(\r\n        inputs[..., None] >= bin_locations,\r\n        dim=-1\r\n    ) - 1\r\n\r\n\r\ndef unconstrained_rational_quadratic_spline(inputs,\r\n                                            unnormalized_widths,\r\n                                            unnormalized_heights,\r\n                                            unnormalized_derivatives,\r\n                                            inverse=False,\r\n                                            tails='linear',\r\n                                            tail_bound=1.,\r\n                                            min_bin_width=DEFAULT_MIN_BIN_WIDTH,\r\n                                            min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\r\n                                            min_derivative=DEFAULT_MIN_DERIVATIVE):\r\n    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\r\n    outside_interval_mask = ~inside_interval_mask\r\n\r\n    outputs = torch.zeros_like(inputs)\r\n    logabsdet = torch.zeros_like(inputs)\r\n\r\n    if tails == 'linear':\r\n        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\r\n        constant = np.log(np.exp(1 - min_derivative) - 1)\r\n        unnormalized_derivatives[..., 0] = constant\r\n        unnormalized_derivatives[..., -1] = constant\r\n\r\n        outputs[outside_interval_mask] = inputs[outside_interval_mask]\r\n        logabsdet[outside_interval_mask] = 0\r\n    else:\r\n        raise RuntimeError('{} tails are not implemented.'.format(tails))\r\n\r\n    outputs[inside_interval_mask], logabsdet[inside_interval_mask] = rational_quadratic_spline(\r\n        inputs=inputs[inside_interval_mask],\r\n        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\r\n        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\r\n        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\r\n        inverse=inverse,\r\n        left=-tail_bound, right=tail_bound, bottom=-tail_bound, top=tail_bound,\r\n        min_bin_width=min_bin_width,\r\n        min_bin_height=min_bin_height,\r\n        min_derivative=min_derivative\r\n    )\r\n\r\n    return outputs, logabsdet\r\n\r\ndef rational_quadratic_spline(inputs,\r\n                              unnormalized_widths,\r\n                              unnormalized_heights,\r\n                              unnormalized_derivatives,\r\n                              inverse=False,\r\n                              left=0., right=1., bottom=0., top=1.,\r\n                              min_bin_width=DEFAULT_MIN_BIN_WIDTH,\r\n                              min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\r\n                              min_derivative=DEFAULT_MIN_DERIVATIVE):\r\n    if torch.min(inputs) < left or torch.max(inputs) > right:\r\n        raise ValueError('Input to a transform is not within its domain')\r\n\r\n    num_bins = unnormalized_widths.shape[-1]\r\n\r\n    if min_bin_width * num_bins > 1.0:\r\n        raise ValueError('Minimal bin width too large for the number of bins')\r\n    if min_bin_height * num_bins > 1.0:\r\n        raise ValueError('Minimal bin height too large for the number of bins')\r\n\r\n    widths = F.softmax(unnormalized_widths, dim=-1)\r\n    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\r\n    cumwidths = torch.cumsum(widths, dim=-1)\r\n    cumwidths = F.pad(cumwidths, pad=(1, 0), mode='constant', value=0.0)\r\n    cumwidths = (right - left) * cumwidths + left\r\n    cumwidths[..., 0] = left\r\n    cumwidths[..., -1] = right\r\n    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\r\n\r\n    derivatives = min_derivative + F.softplus(unnormalized_derivatives)\r\n\r\n    heights = F.softmax(unnormalized_heights, dim=-1)\r\n    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\r\n    cumheights = torch.cumsum(heights, dim=-1)\r\n    cumheights = F.pad(cumheights, pad=(1, 0), mode='constant', value=0.0)\r\n    cumheights = (top - bottom) * cumheights + bottom\r\n    cumheights[..., 0] = bottom\r\n    cumheights[..., -1] = top\r\n    heights = cumheights[..., 1:] - cumheights[..., :-1]\r\n\r\n    if inverse:\r\n        bin_idx = searchsorted(cumheights, inputs)[..., None]\r\n    else:\r\n        bin_idx = searchsorted(cumwidths, inputs)[..., None]\r\n\r\n    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\r\n    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\r\n\r\n    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\r\n    delta = heights / widths\r\n    input_delta = delta.gather(-1, bin_idx)[..., 0]\r\n\r\n    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\r\n    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)[..., 0]\r\n\r\n    input_heights = heights.gather(-1, bin_idx)[..., 0]\r\n\r\n    if inverse:\r\n        a = (((inputs - input_cumheights) * (input_derivatives\r\n                                             + input_derivatives_plus_one\r\n                                             - 2 * input_delta)\r\n              + input_heights * (input_delta - input_derivatives)))\r\n        b = (input_heights * input_derivatives\r\n             - (inputs - input_cumheights) * (input_derivatives\r\n                                              + input_derivatives_plus_one\r\n                                              - 2 * input_delta))\r\n        c = - input_delta * (inputs - input_cumheights)\r\n\r\n        discriminant = b.pow(2) - 4 * a * c\r\n        assert (discriminant >= 0).all()\r\n\r\n        root = (2 * c) / (-b - torch.sqrt(discriminant))\r\n        outputs = root * input_bin_widths + input_cumwidths\r\n\r\n        theta_one_minus_theta = root * (1 - root)\r\n        denominator = input_delta + ((input_derivatives + input_derivatives_plus_one - 2 * input_delta)\r\n                                     * theta_one_minus_theta)\r\n        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * root.pow(2)\r\n                                                     + 2 * input_delta * theta_one_minus_theta\r\n                                                     + input_derivatives * (1 - root).pow(2))\r\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\r\n\r\n        return outputs, -logabsdet\r\n    else:\r\n        theta = (inputs - input_cumwidths) / input_bin_widths\r\n        theta_one_minus_theta = theta * (1 - theta)\r\n\r\n        numerator = input_heights * (input_delta * theta.pow(2)\r\n                                     + input_derivatives * theta_one_minus_theta)\r\n        denominator = input_delta + ((input_derivatives + input_derivatives_plus_one - 2 * input_delta)\r\n                                     * theta_one_minus_theta)\r\n        outputs = input_cumheights + numerator / denominator\r\n\r\n        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * theta.pow(2)\r\n                                                     + 2 * input_delta * theta_one_minus_theta\r\n                                                     + input_derivatives * (1 - theta).pow(2))\r\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\r\n\r\n        return outputs, logabsdet\r\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 14.20703125,
          "content": "import os\r\nimport glob\r\nimport sys\r\nimport argparse\r\nimport logging\r\nimport json\r\nimport subprocess\r\nimport numpy as np\r\nfrom scipy.io.wavfile import read\r\nimport torch\r\nimport regex as re\r\n\r\nMATPLOTLIB_FLAG = False\r\n\r\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\r\nlogger = logging\r\n\r\n\r\n\r\nzh_pattern = re.compile(r'[\\u4e00-\\u9fa5]')\r\nen_pattern = re.compile(r'[a-zA-Z]')\r\njp_pattern = re.compile(r'[\\u3040-\\u30ff\\u31f0-\\u31ff]')\r\nkr_pattern = re.compile(r'[\\uac00-\\ud7af\\u1100-\\u11ff\\u3130-\\u318f\\ua960-\\ua97f]')\r\nnum_pattern=re.compile(r'[0-9]')\r\ncomma=r\"(?<=[.。!！?？；;，,、:：'\\\"‘“”’()（）《》「」~——])\"    #向前匹配但固定长度\r\ntags={'ZH':'[ZH]','EN':'[EN]','JP':'[JA]','KR':'[KR]'}\r\n\r\ndef tag_cjke(text):\r\n    '''为中英日韩加tag,中日正则分不开，故先分句分离中日再识别，以应对大部分情况'''\r\n    sentences = re.split(r\"([.。!！?？；;，,、:：'\\\"‘“”’()（）【】《》「」~——]+ *(?![0-9]))\", text) #分句，排除小数点\r\n    sentences.append(\"\")\r\n    sentences = [\"\".join(i) for i in zip(sentences[0::2],sentences[1::2])]\r\n    # print(sentences)\r\n    prev_lang=None\r\n    tagged_text = \"\"\r\n    for s in sentences:\r\n        #全为符号跳过\r\n        nu = re.sub(r'[\\s\\p{P}]+', '', s, flags=re.U).strip()   \r\n        if len(nu)==0:\r\n            continue\r\n        s = re.sub(r'[()（）《》「」【】‘“”’]+', '', s)\r\n        jp=re.findall(jp_pattern, s)\r\n        #本句含日语字符判断为日语\r\n        if len(jp)>0:  \r\n            prev_lang,tagged_jke=tag_jke(s,prev_lang)\r\n            tagged_text +=tagged_jke\r\n        else:\r\n            prev_lang,tagged_cke=tag_cke(s,prev_lang)\r\n            tagged_text +=tagged_cke\r\n    return tagged_text\r\n\r\ndef tag_jke(text,prev_sentence=None):\r\n    '''为英日韩加tag'''\r\n    # 初始化标记变量\r\n    tagged_text = \"\"\r\n    prev_lang = None\r\n    tagged=0\r\n    # 遍历文本\r\n    for char in text:\r\n        # 判断当前字符属于哪种语言\r\n        if jp_pattern.match(char):\r\n            lang = \"JP\"\r\n        elif zh_pattern.match(char):\r\n            lang = \"JP\"\r\n        elif kr_pattern.match(char):\r\n            lang = \"KR\"\r\n        elif en_pattern.match(char):\r\n            lang = \"EN\"\r\n        # elif num_pattern.match(char):\r\n        #     lang = prev_sentence\r\n        else:\r\n            lang = None\r\n            tagged_text += char\r\n            continue\r\n        # 如果当前语言与上一个语言不同，就添加标记\r\n        if lang != prev_lang:\r\n            tagged=1\r\n            if prev_lang==None: # 开头\r\n                tagged_text =tags[lang]+tagged_text\r\n            else:\r\n                tagged_text =tagged_text+tags[prev_lang]+tags[lang]\r\n\r\n            # 重置标记变量\r\n            prev_lang = lang\r\n\r\n        # 添加当前字符到标记文本中\r\n        tagged_text += char\r\n    \r\n    # 在最后一个语言的结尾添加对应的标记\r\n    if prev_lang:\r\n            tagged_text += tags[prev_lang]\r\n    if not tagged:\r\n        prev_lang=prev_sentence\r\n        tagged_text =tags[prev_lang]+tagged_text+tags[prev_lang]\r\n\r\n    return prev_lang,tagged_text\r\n\r\ndef tag_cke(text,prev_sentence=None):\r\n    '''为中英韩加tag'''\r\n    # 初始化标记变量\r\n    tagged_text = \"\"\r\n    prev_lang = None\r\n    # 是否全略过未标签\r\n    tagged=0\r\n    \r\n    # 遍历文本\r\n    for char in text:\r\n        # 判断当前字符属于哪种语言\r\n        if zh_pattern.match(char):\r\n            lang = \"ZH\"\r\n        elif kr_pattern.match(char):\r\n            lang = \"KR\"\r\n        elif en_pattern.match(char):\r\n            lang = \"EN\"\r\n        # elif num_pattern.match(char):\r\n        #     lang = prev_sentence\r\n        else:\r\n            # 略过\r\n            lang = None\r\n            tagged_text += char\r\n            continue\r\n\r\n        # 如果当前语言与上一个语言不同，添加标记\r\n        if lang != prev_lang:\r\n            tagged=1\r\n            if prev_lang==None: # 开头\r\n                tagged_text =tags[lang]+tagged_text\r\n            else:\r\n                tagged_text =tagged_text+tags[prev_lang]+tags[lang]\r\n\r\n            # 重置标记变量\r\n            prev_lang = lang\r\n\r\n        # 添加当前字符到标记文本中\r\n        tagged_text += char\r\n    \r\n    # 在最后一个语言的结尾添加对应的标记\r\n    if prev_lang:\r\n            tagged_text += tags[prev_lang]\r\n    # 未标签则继承上一句标签\r\n    if tagged==0:\r\n        prev_lang=prev_sentence\r\n        tagged_text =tags[prev_lang]+tagged_text+tags[prev_lang]\r\n    return prev_lang,tagged_text\r\n\r\n\r\n\r\ndef load_checkpoint(checkpoint_path, model, optimizer=None, drop_speaker_emb=False):\r\n    assert os.path.isfile(checkpoint_path)\r\n    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\r\n    iteration = checkpoint_dict['iteration']\r\n    learning_rate = checkpoint_dict['learning_rate']\r\n    if optimizer is not None:\r\n        optimizer.load_state_dict(checkpoint_dict['optimizer'])\r\n    saved_state_dict = checkpoint_dict['model']\r\n    if hasattr(model, 'module'):\r\n        state_dict = model.module.state_dict()\r\n    else:\r\n        state_dict = model.state_dict()\r\n    new_state_dict = {}\r\n    for k, v in state_dict.items():\r\n        try:\r\n            if k == 'emb_g.weight':\r\n                if drop_speaker_emb:\r\n                    new_state_dict[k] = v\r\n                    continue\r\n                v[:saved_state_dict[k].shape[0], :] = saved_state_dict[k]\r\n                new_state_dict[k] = v\r\n            else:\r\n                new_state_dict[k] = saved_state_dict[k]\r\n        except:\r\n            logger.info(\"%s is not in the checkpoint\" % k)\r\n            new_state_dict[k] = v\r\n    if hasattr(model, 'module'):\r\n        model.module.load_state_dict(new_state_dict)\r\n    else:\r\n        model.load_state_dict(new_state_dict)\r\n    logger.info(\"Loaded checkpoint '{}' (iteration {})\".format(\r\n        checkpoint_path, iteration))\r\n    return model, optimizer, learning_rate, iteration\r\n\r\n\r\ndef save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):\r\n    logger.info(\"Saving model and optimizer state at iteration {} to {}\".format(\r\n        iteration, checkpoint_path))\r\n    if hasattr(model, 'module'):\r\n        state_dict = model.module.state_dict()\r\n    else:\r\n        state_dict = model.state_dict()\r\n    torch.save({'model': state_dict,\r\n                'iteration': iteration,\r\n                'optimizer': optimizer.state_dict() if optimizer is not None else None,\r\n                'learning_rate': learning_rate}, checkpoint_path)\r\n\r\n\r\ndef summarize(writer, global_step, scalars={}, histograms={}, images={}, audios={}, audio_sampling_rate=22050):\r\n    for k, v in scalars.items():\r\n        writer.add_scalar(k, v, global_step)\r\n    for k, v in histograms.items():\r\n        writer.add_histogram(k, v, global_step)\r\n    for k, v in images.items():\r\n        writer.add_image(k, v, global_step, dataformats='HWC')\r\n    for k, v in audios.items():\r\n        writer.add_audio(k, v, global_step, audio_sampling_rate)\r\n\r\n\r\ndef extract_digits(f):\r\n    digits = \"\".join(filter(str.isdigit, f))\r\n    return int(digits) if digits else -1\r\n\r\n\r\ndef latest_checkpoint_path(dir_path, regex=\"G_[0-9]*.pth\"):\r\n    f_list = glob.glob(os.path.join(dir_path, regex))\r\n    f_list.sort(key=lambda f: extract_digits(f))\r\n    x = f_list[-1]\r\n    print(f\"latest_checkpoint_path:{x}\")\r\n    return x\r\n\r\n\r\ndef oldest_checkpoint_path(dir_path, regex=\"G_[0-9]*.pth\", preserved=4):\r\n    f_list = glob.glob(os.path.join(dir_path, regex))\r\n    f_list.sort(key=lambda f: extract_digits(f))\r\n    if len(f_list) > preserved:\r\n        x = f_list[0]\r\n        print(f\"oldest_checkpoint_path:{x}\")\r\n        return x\r\n    return \"\"\r\n\r\n\r\ndef plot_spectrogram_to_numpy(spectrogram):\r\n    global MATPLOTLIB_FLAG\r\n    if not MATPLOTLIB_FLAG:\r\n        import matplotlib\r\n        matplotlib.use(\"Agg\")\r\n        MATPLOTLIB_FLAG = True\r\n        mpl_logger = logging.getLogger('matplotlib')\r\n        mpl_logger.setLevel(logging.WARNING)\r\n    import matplotlib.pylab as plt\r\n    import numpy as np\r\n\r\n    fig, ax = plt.subplots(figsize=(10, 2))\r\n    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\r\n                   interpolation='none')\r\n    plt.colorbar(im, ax=ax)\r\n    plt.xlabel(\"Frames\")\r\n    plt.ylabel(\"Channels\")\r\n    plt.tight_layout()\r\n\r\n    fig.canvas.draw()\r\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\r\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\r\n    plt.close()\r\n    return data\r\n\r\n\r\ndef plot_alignment_to_numpy(alignment, info=None):\r\n    global MATPLOTLIB_FLAG\r\n    if not MATPLOTLIB_FLAG:\r\n        import matplotlib\r\n        matplotlib.use(\"Agg\")\r\n        MATPLOTLIB_FLAG = True\r\n        mpl_logger = logging.getLogger('matplotlib')\r\n        mpl_logger.setLevel(logging.WARNING)\r\n    import matplotlib.pylab as plt\r\n    import numpy as np\r\n\r\n    fig, ax = plt.subplots(figsize=(6, 4))\r\n    im = ax.imshow(alignment.transpose(), aspect='auto', origin='lower',\r\n                   interpolation='none')\r\n    fig.colorbar(im, ax=ax)\r\n    xlabel = 'Decoder timestep'\r\n    if info is not None:\r\n        xlabel += '\\n\\n' + info\r\n    plt.xlabel(xlabel)\r\n    plt.ylabel('Encoder timestep')\r\n    plt.tight_layout()\r\n\r\n    fig.canvas.draw()\r\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\r\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\r\n    plt.close()\r\n    return data\r\n\r\n\r\ndef load_wav_to_torch(full_path):\r\n    sampling_rate, data = read(full_path)\r\n    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\r\n\r\n\r\ndef load_filepaths_and_text(filename, split=\"|\"):\r\n    with open(filename, encoding='utf-8') as f:\r\n        filepaths_and_text = [line.strip().split(split) for line in f]\r\n    return filepaths_and_text\r\n\r\n\r\ndef str2bool(v):\r\n    if isinstance(v, bool):\r\n        return v\r\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\r\n        return True\r\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\r\n        return False\r\n    else:\r\n        raise argparse.ArgumentTypeError('Boolean value expected.')\r\n\r\n\r\ndef get_hparams(init=True):\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('-c', '--config', type=str, default=\"./configs/modified_finetune_speaker.json\",\r\n                        help='JSON file for configuration')\r\n    parser.add_argument('-m', '--model', type=str, default=\"pretrained_models\",\r\n                        help='Model name')\r\n    parser.add_argument('-n', '--max_epochs', type=int, default=50,\r\n                        help='finetune epochs')\r\n    parser.add_argument('--cont', type=str2bool, default=False, help='whether to continue training on the latest checkpoint')\r\n    parser.add_argument('--drop_speaker_embed', type=str2bool, default=False, help='whether to drop existing characters')\r\n    parser.add_argument('--train_with_pretrained_model', type=str2bool, default=True,\r\n                        help='whether to train with pretrained model')\r\n    parser.add_argument('--preserved', type=int, default=4,\r\n                        help='Number of preserved models')\r\n\r\n    args = parser.parse_args()\r\n    model_dir = os.path.join(\"./\", args.model)\r\n\r\n    if not os.path.exists(model_dir):\r\n        os.makedirs(model_dir)\r\n\r\n    config_path = args.config\r\n    config_save_path = os.path.join(model_dir, \"config.json\")\r\n    if init:\r\n        with open(config_path, \"r\") as f:\r\n            data = f.read()\r\n        with open(config_save_path, \"w\") as f:\r\n            f.write(data)\r\n    else:\r\n        with open(config_save_path, \"r\") as f:\r\n            data = f.read()\r\n    config = json.loads(data)\r\n\r\n    hparams = HParams(**config)\r\n    hparams.model_dir = model_dir\r\n    hparams.max_epochs = args.max_epochs\r\n    hparams.cont = args.cont\r\n    hparams.drop_speaker_embed = args.drop_speaker_embed\r\n    hparams.train_with_pretrained_model = args.train_with_pretrained_model\r\n    hparams.preserved = args.preserved\r\n    return hparams\r\n\r\n\r\ndef get_hparams_from_dir(model_dir):\r\n    config_save_path = os.path.join(model_dir, \"config.json\")\r\n    with open(config_save_path, \"r\") as f:\r\n        data = f.read()\r\n    config = json.loads(data)\r\n\r\n    hparams = HParams(**config)\r\n    hparams.model_dir = model_dir\r\n    return hparams\r\n\r\n\r\ndef get_hparams_from_file(config_path):\r\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\r\n        data = f.read()\r\n    config = json.loads(data)\r\n\r\n    hparams = HParams(**config)\r\n    return hparams\r\n\r\n\r\ndef check_git_hash(model_dir):\r\n    source_dir = os.path.dirname(os.path.realpath(__file__))\r\n    if not os.path.exists(os.path.join(source_dir, \".git\")):\r\n        logger.warn(\"{} is not a git repository, therefore hash value comparison will be ignored.\".format(\r\n            source_dir\r\n        ))\r\n        return\r\n\r\n    cur_hash = subprocess.getoutput(\"git rev-parse HEAD\")\r\n\r\n    path = os.path.join(model_dir, \"githash\")\r\n    if os.path.exists(path):\r\n        saved_hash = open(path).read()\r\n        if saved_hash != cur_hash:\r\n            logger.warn(\"git hash values are different. {}(saved) != {}(current)\".format(\r\n                saved_hash[:8], cur_hash[:8]))\r\n    else:\r\n        open(path, \"w\").write(cur_hash)\r\n\r\n\r\ndef get_logger(model_dir, filename=\"train.log\"):\r\n    global logger\r\n    logger = logging.getLogger(os.path.basename(model_dir))\r\n    logger.setLevel(logging.DEBUG)\r\n\r\n    formatter = logging.Formatter(\"%(asctime)s\\t%(name)s\\t%(levelname)s\\t%(message)s\")\r\n    if not os.path.exists(model_dir):\r\n        os.makedirs(model_dir)\r\n    h = logging.FileHandler(os.path.join(model_dir, filename),encoding=\"utf-8\")\r\n    h.setLevel(logging.DEBUG)\r\n    h.setFormatter(formatter)\r\n    logger.addHandler(h)\r\n    return logger\r\n\r\n\r\nclass HParams():\r\n    def __init__(self, **kwargs):\r\n        for k, v in kwargs.items():\r\n            if type(v) == dict:\r\n                v = HParams(**v)\r\n            self[k] = v\r\n\r\n    def keys(self):\r\n        return self.__dict__.keys()\r\n\r\n    def items(self):\r\n        return self.__dict__.items()\r\n\r\n    def values(self):\r\n        return self.__dict__.values()\r\n\r\n    def __len__(self):\r\n        return len(self.__dict__)\r\n\r\n    def __getitem__(self, key):\r\n        return getattr(self, key)\r\n\r\n    def __setitem__(self, key, value):\r\n        return setattr(self, key, value)\r\n\r\n    def __contains__(self, key):\r\n        return key in self.__dict__\r\n\r\n    def __repr__(self):\r\n        return self.__dict__.__repr__()"
        }
      ]
    }
  ]
}