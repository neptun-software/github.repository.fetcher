{
  "metadata": {
    "timestamp": 1736559880939,
    "page": 629,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "huggingface/speech-to-speech",
      "stars": 3664,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0634765625,
          "content": "tmp\ncache\nDockerfile\ndocker-compose.yml\n.dockerignore\n.gitignore\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0205078125,
          "content": "__pycache__\ntmp\ncache"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.271484375,
          "content": "FROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-devel\n\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /usr/src/app\n\n# Install packages\nRUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt ./\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n"
        },
        {
          "name": "Dockerfile.arm64",
          "type": "blob",
          "size": 0.2724609375,
          "content": "FROM nvcr.io/nvidia/l4t-pytorch:r35.2.1-pth2.0-py3\n\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /usr/src/app\n\n# Install packages\nRUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt ./\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . ."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08984375,
          "content": "                               Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [2024] [The HuggingFace Inc. team]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "LLM",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.890625,
          "content": "<div align=\"center\">\n  <div>&nbsp;</div>\n  <img src=\"logo.png\" width=\"600\"/> \n</div>\n\n# Speech To Speech: an effort for an open-sourced and modular GPT4-o\n\n\n## ðŸ“– Quick Index\n* [Approach](#approach)\n  - [Structure](#structure)\n  - [Modularity](#modularity)\n* [Setup](#setup)\n* [Usage](#usage)\n  - [Docker Server approach](#docker-server)\n  - [Server/Client approach](#serverclient-approach)\n  - [Local approach](#local-approach-running-on-mac)\n* [Command-line usage](#command-line-usage)\n  - [Model parameters](#model-parameters)\n  - [Generation parameters](#generation-parameters)\n  - [Notable parameters](#notable-parameters)\n\n## Approach\n\n### Structure\nThis repository implements a speech-to-speech cascaded pipeline consisting of the following parts:\n1. **Voice Activity Detection (VAD)**\n2. **Speech to Text (STT)**\n3. **Language Model (LM)**\n4. **Text to Speech (TTS)**\n\n### Modularity\nThe pipeline provides a fully open and modular approach, with a focus on leveraging models available through the Transformers library on the Hugging Face hub. The code is designed for easy modification, and we already support device-specific and external library implementations:\n\n**VAD** \n- [Silero VAD v5](https://github.com/snakers4/silero-vad)\n\n**STT**\n- Any [Whisper](https://huggingface.co/docs/transformers/en/model_doc/whisper) model checkpoint on the Hugging Face Hub through Transformers ðŸ¤—, including [whisper-large-v3](https://huggingface.co/openai/whisper-large-v3) and [distil-large-v3](https://huggingface.co/distil-whisper/distil-large-v3)\n- [Lightning Whisper MLX](https://github.com/mustafaaljadery/lightning-whisper-mlx?tab=readme-ov-file#lightning-whisper-mlx)\n- [Paraformer - FunASR](https://github.com/modelscope/FunASR)\n\n**LLM**\n- Any instruction-following model on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending) via Transformers ðŸ¤—\n- [mlx-lm](https://github.com/ml-explore/mlx-examples/blob/main/llms/README.md)\n- [OpenAI API](https://platform.openai.com/docs/quickstart)\n\n**TTS**\n- [Parler-TTS](https://github.com/huggingface/parler-tts) ðŸ¤—\n- [MeloTTS](https://github.com/myshell-ai/MeloTTS)\n- [ChatTTS](https://github.com/2noise/ChatTTS?tab=readme-ov-file)\n\n## Setup\n\nClone the repository:\n```bash\ngit clone https://github.com/huggingface/speech-to-speech.git\ncd speech-to-speech\n```\n\nInstall the required dependencies using [uv](https://github.com/astral-sh/uv):\n```bash\nuv pip install -r requirements.txt\n```\n\nFor Mac users, use the `requirements_mac.txt` file instead:\n```bash\nuv pip install -r requirements_mac.txt\n```\n\nIf you want to use Melo TTS, you also need to run:\n```bash\npython -m unidic download\n```\n\n\n## Usage\n\nThe pipeline can be run in two ways:\n- **Server/Client approach**: Models run on a server, and audio input/output are streamed from a client.\n- **Local approach**: Runs locally.\n\n### Recommended setup \n\n### Server/Client Approach\n\n1. Run the pipeline on the server:\n   ```bash\n   python s2s_pipeline.py --recv_host 0.0.0.0 --send_host 0.0.0.0\n   ```\n\n2. Run the client locally to handle microphone input and receive generated audio:\n   ```bash\n   python listen_and_play.py --host <IP address of your server>\n   ```\n\n### Local Approach (Mac)\n\n1. For optimal settings on Mac:\n   ```bash\n   python s2s_pipeline.py --local_mac_optimal_settings\n   ```\n\nThis setting:\n   - Adds `--device mps` to use MPS for all models.\n     - Sets LightningWhisperMLX for STT\n     - Sets MLX LM for language model\n     - Sets MeloTTS for TTS\n\n### Docker Server\n\n#### Install the NVIDIA Container Toolkit\n\nhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\n\n#### Start the docker container\n```docker compose up```\n\n\n\n### Recommended usage with Cuda\n\nLeverage Torch Compile for Whisper and Parler-TTS. **The usage of Parler-TTS allows for audio output streaming, further reducing the overall latency** ðŸš€:\n\n```bash\npython s2s_pipeline.py \\\n\t--lm_model_name microsoft/Phi-3-mini-4k-instruct \\\n\t--stt_compile_mode reduce-overhead \\\n\t--tts_compile_mode default \\\n  --recv_host 0.0.0.0 \\\n\t--send_host 0.0.0.0 \n```\n\nFor the moment, modes capturing CUDA Graphs are not compatible with streaming Parler-TTS (`reduce-overhead`, `max-autotune`).\n\n### Multi-language Support\n\nThe pipeline currently supports English, French, Spanish, Chinese, Japanese, and Korean.  \nTwo use cases are considered:\n\n- **Single-language conversation**: Enforce the language setting using the `--language` flag, specifying the target language code (default is 'en').\n- **Language switching**: Set `--language` to 'auto'. In this case, Whisper detects the language for each spoken prompt, and the LLM is prompted with \"`Please reply to my message in ...`\" to ensure the response is in the detected language.\n\nPlease note that you must use STT and LLM checkpoints compatible with the target language(s). For the STT part, Parler-TTS is not yet multilingual (though that feature is coming soon! ðŸ¤—). In the meantime, you should use Melo (which supports English, French, Spanish, Chinese, Japanese, and Korean) or Chat-TTS.\n\n#### With the server version:\n\nFor automatic language detection:\n\n```bash\npython s2s_pipeline.py \\\n    --stt_model_name large-v3 \\\n    --language auto \\\n    --mlx_lm_model_name mlx-community/Meta-Llama-3.1-8B-Instruct \\\n```\n\nOr for one language in particular, chinese in this example\n\n```bash\npython s2s_pipeline.py \\\n    --stt_model_name large-v3 \\\n    --language zh \\\n    --mlx_lm_model_name mlx-community/Meta-Llama-3.1-8B-Instruct \\\n```\n\n#### Local Mac Setup\n\nFor automatic language detection:\n\n```bash\npython s2s_pipeline.py \\\n    --local_mac_optimal_settings \\\n    --device mps \\\n    --stt_model_name large-v3 \\\n    --language auto \\\n    --mlx_lm_model_name mlx-community/Meta-Llama-3.1-8B-Instruct-4bit \\\n```\n\nOr for one language in particular, chinese in this example\n\n```bash\npython s2s_pipeline.py \\\n    --local_mac_optimal_settings \\\n    --device mps \\\n    --stt_model_name large-v3 \\\n    --language zh \\\n    --mlx_lm_model_name mlx-community/Meta-Llama-3.1-8B-Instruct-4bit \\\n```\n\n## Command-line Usage\n\n> **_NOTE:_** References for all the CLI arguments can be found directly in the [arguments classes](https://github.com/huggingface/speech-to-speech/tree/d5e460721e578fef286c7b64e68ad6a57a25cf1b/arguments_classes) or by running `python s2s_pipeline.py -h`.\n\n### Module level Parameters \nSee [ModuleArguments](https://github.com/huggingface/speech-to-speech/blob/d5e460721e578fef286c7b64e68ad6a57a25cf1b/arguments_classes/module_arguments.py) class. Allows to set:\n- a common `--device` (if one wants each part to run on the same device)\n- `--mode` `local` or `server`\n- chosen STT implementation \n- chosen LM implementation\n- chose TTS implementation\n- logging level\n\n### VAD parameters\nSee [VADHandlerArguments](https://github.com/huggingface/speech-to-speech/blob/d5e460721e578fef286c7b64e68ad6a57a25cf1b/arguments_classes/vad_arguments.py) class. Notably:\n- `--thresh`: Threshold value to trigger voice activity detection.\n- `--min_speech_ms`: Minimum duration of detected voice activity to be considered speech.\n- `--min_silence_ms`: Minimum length of silence intervals for segmenting speech, balancing sentence cutting and latency reduction.\n\n\n### STT, LM and TTS parameters\n\n`model_name`, `torch_dtype`, and `device` are exposed for each implementation of the Speech to Text, Language Model, and Text to Speech. Specify the targeted pipeline part with the corresponding prefix (e.g. `stt`, `lm` or `tts`, check the implementations' [arguments classes](https://github.com/huggingface/speech-to-speech/tree/d5e460721e578fef286c7b64e68ad6a57a25cf1b/arguments_classes) for more details).\n\nFor example:\n```bash\n--lm_model_name google/gemma-2b-it\n```\n\n### Generation parameters\n\nOther generation parameters of the model's generate method can be set using the part's prefix + `_gen_`, e.g., `--stt_gen_max_new_tokens 128`. These parameters can be added to the pipeline part's arguments class if not already exposed.\n\n## Citations\n\n### Silero VAD\n```bibtex\n@misc{Silero VAD,\n  author = {Silero Team},\n  title = {Silero VAD: pre-trained enterprise-grade Voice Activity Detector (VAD), Number Detector and Language Classifier},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/snakers4/silero-vad}},\n  commit = {insert_some_commit_here},\n  email = {hello@silero.ai}\n}\n```\n\n### Distil-Whisper\n```bibtex\n@misc{gandhi2023distilwhisper,\n      title={Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling},\n      author={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},\n      year={2023},\n      eprint={2311.00430},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n### Parler-TTS\n```bibtex\n@misc{lacombe-etal-2024-parler-tts,\n  author = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},\n  title = {Parler-TTS},\n  year = {2024},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/huggingface/parler-tts}}\n}\n```\n"
        },
        {
          "name": "STT",
          "type": "tree",
          "content": null
        },
        {
          "name": "TTS",
          "type": "tree",
          "content": null
        },
        {
          "name": "VAD",
          "type": "tree",
          "content": null
        },
        {
          "name": "arguments_classes",
          "type": "tree",
          "content": null
        },
        {
          "name": "baseHandler.py",
          "type": "blob",
          "size": 1.9501953125,
          "content": "from time import perf_counter\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseHandler:\n    \"\"\"\n    Base class for pipeline parts. Each part of the pipeline has an input and an output queue.\n    The `setup` method along with `setup_args` and `setup_kwargs` can be used to address the specific requirements of the implemented pipeline part.\n    To stop a handler properly, set the stop_event and, to avoid queue deadlocks, place b\"END\" in the input queue.\n    Objects placed in the input queue will be processed by the `process` method, and the yielded results will be placed in the output queue.\n    The cleanup method handles stopping the handler, and b\"END\" is placed in the output queue.\n    \"\"\"\n\n    def __init__(self, stop_event, queue_in, queue_out, setup_args=(), setup_kwargs={}):\n        self.stop_event = stop_event\n        self.queue_in = queue_in\n        self.queue_out = queue_out\n        self.setup(*setup_args, **setup_kwargs)\n        self._times = []\n\n    def setup(self):\n        pass\n\n    def process(self):\n        raise NotImplementedError\n\n    def run(self):\n        while not self.stop_event.is_set():\n            input = self.queue_in.get()\n            if isinstance(input, bytes) and input == b\"END\":\n                # sentinelle signal to avoid queue deadlock\n                logger.debug(\"Stopping thread\")\n                break\n            start_time = perf_counter()\n            for output in self.process(input):\n                self._times.append(perf_counter() - start_time)\n                if self.last_time > self.min_time_to_debug:\n                    logger.debug(f\"{self.__class__.__name__}: {self.last_time: .3f} s\")\n                self.queue_out.put(output)\n                start_time = perf_counter()\n\n        self.cleanup()\n        self.queue_out.put(b\"END\")\n\n    @property\n    def last_time(self):\n        return self._times[-1]\n    \n    @property\n    def min_time_to_debug(self):\n        return 0.001\n\n    def cleanup(self):\n        pass\n"
        },
        {
          "name": "connections",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.8515625,
          "content": "---\nservices:\n\n  pipeline:\n    build:\n      context: .\n      dockerfile: ${DOCKERFILE:-Dockerfile}\n    command: \n      - python3 \n      - s2s_pipeline.py \n      - --recv_host \n      - 0.0.0.0 \n      - --send_host \n      - 0.0.0.0 \n      - --lm_model_name \n      - microsoft/Phi-3-mini-4k-instruct \n      - --init_chat_role \n      - system\n      - --init_chat_prompt\n      - \"You are a helpful assistant\"\n      - --stt_compile_mode \n      - reduce-overhead \n      - --tts_compile_mode \n      - default\n    expose:\n      - 12345/tcp\n      - 12346/tcp\n    ports:\n      - 12345:12345/tcp\n      - 12346:12346/tcp\n    volumes:\n      - ./cache/:/root/.cache/\n      - ./s2s_pipeline.py:/usr/src/app/s2s_pipeline.py\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: ['0']\n              capabilities: [gpu]\n"
        },
        {
          "name": "listen_and_play.py",
          "type": "blob",
          "size": 4.0068359375,
          "content": "import socket\nimport threading\nfrom queue import Queue\nfrom dataclasses import dataclass, field\nimport sounddevice as sd\nfrom transformers import HfArgumentParser\n\n\n@dataclass\nclass ListenAndPlayArguments:\n    send_rate: int = field(default=16000, metadata={\"help\": \"In Hz. Default is 16000.\"})\n    recv_rate: int = field(default=16000, metadata={\"help\": \"In Hz. Default is 16000.\"})\n    list_play_chunk_size: int = field(\n        default=1024,\n        metadata={\"help\": \"The size of data chunks (in bytes). Default is 1024.\"},\n    )\n    host: str = field(\n        default=\"localhost\",\n        metadata={\n            \"help\": \"The hostname or IP address for listening and playing. Default is 'localhost'.\"\n        },\n    )\n    send_port: int = field(\n        default=12345,\n        metadata={\"help\": \"The network port for sending data. Default is 12345.\"},\n    )\n    recv_port: int = field(\n        default=12346,\n        metadata={\"help\": \"The network port for receiving data. Default is 12346.\"},\n    )\n\n\ndef listen_and_play(\n    send_rate=16000,\n    recv_rate=44100,\n    list_play_chunk_size=1024,\n    host=\"localhost\",\n    send_port=12345,\n    recv_port=12346,\n):\n    send_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    send_socket.connect((host, send_port))\n\n    recv_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    recv_socket.connect((host, recv_port))\n\n    print(\"Recording and streaming...\")\n\n    stop_event = threading.Event()\n    recv_queue = Queue()\n    send_queue = Queue()\n\n    def callback_recv(outdata, frames, time, status):\n        if not recv_queue.empty():\n            data = recv_queue.get()\n            outdata[: len(data)] = data\n            outdata[len(data) :] = b\"\\x00\" * (len(outdata) - len(data))\n        else:\n            outdata[:] = b\"\\x00\" * len(outdata)\n\n    def callback_send(indata, frames, time, status):\n        if recv_queue.empty():\n            data = bytes(indata)\n            send_queue.put(data)\n\n    def send(stop_event, send_queue):\n        while not stop_event.is_set():\n            data = send_queue.get()\n            send_socket.sendall(data)\n\n    def recv(stop_event, recv_queue):\n        def receive_full_chunk(conn, chunk_size):\n            data = b\"\"\n            while len(data) < chunk_size:\n                packet = conn.recv(chunk_size - len(data))\n                if not packet:\n                    return None  # Connection has been closed\n                data += packet\n            return data\n\n        while not stop_event.is_set():\n            data = receive_full_chunk(recv_socket, list_play_chunk_size * 2)\n            if data:\n                recv_queue.put(data)\n\n    try:\n        send_stream = sd.RawInputStream(\n            samplerate=send_rate,\n            channels=1,\n            dtype=\"int16\",\n            blocksize=list_play_chunk_size,\n            callback=callback_send,\n        )\n        recv_stream = sd.RawOutputStream(\n            samplerate=recv_rate,\n            channels=1,\n            dtype=\"int16\",\n            blocksize=list_play_chunk_size,\n            callback=callback_recv,\n        )\n        threading.Thread(target=send_stream.start).start()\n        threading.Thread(target=recv_stream.start).start()\n\n        send_thread = threading.Thread(target=send, args=(stop_event, send_queue))\n        send_thread.start()\n        recv_thread = threading.Thread(target=recv, args=(stop_event, recv_queue))\n        recv_thread.start()\n\n        input(\"Press Enter to stop...\")\n\n    except KeyboardInterrupt:\n        print(\"Finished streaming.\")\n\n    finally:\n        stop_event.set()\n        # Given that socket::recv is blocking in receive_data_chunk, shut it down to allow the thread to continue.\n        recv_socket.shutdown(socket.SHUT_RDWR)\n        recv_thread.join()\n        send_thread.join()\n        send_socket.close()\n        recv_socket.close()\n        print(\"Connection closed.\")\n\n\nif __name__ == \"__main__\":\n    parser = HfArgumentParser((ListenAndPlayArguments,))\n    (listen_and_play_kwargs,) = parser.parse_args_into_dataclasses()\n    listen_and_play(**vars(listen_and_play_kwargs))\n"
        },
        {
          "name": "logo.png",
          "type": "blob",
          "size": 64.6142578125,
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.4130859375,
          "content": "nltk==3.9.1\nparler_tts @ git+https://github.com/huggingface/parler-tts.git\nmelotts @ git+https://github.com/andimarafioti/MeloTTS.git#egg=MeloTTS  # made a copy of MeloTTS to have compatible versions of transformers\ntorch>=2.4.0\nsounddevice==0.5.0\nChatTTS>=0.1.1\nfunasr>=1.1.6\nfaster-whisper>=1.0.3\nmodelscope>=1.17.1\ndeepfilternet>=0.5.6\nopenai>=1.40.1\nuseful-moonshine @ git+https://github.com/andimarafioti/moonshine.git"
        },
        {
          "name": "requirements_mac.txt",
          "type": "blob",
          "size": 0.45703125,
          "content": "nltk==3.9.1\nparler_tts @ git+https://github.com/huggingface/parler-tts.git\nmelotts @ git+https://github.com/andimarafioti/MeloTTS.git#egg=MeloTTS  # made a copy of MeloTTS to have compatible versions of transformers\ntorch==2.4.0\nsounddevice==0.5.0\nlightning-whisper-mlx>=0.0.10\nmlx-lm>=0.14.0\nChatTTS>=0.1.1\nfunasr>=1.1.6\nfaster-whisper>=1.0.3\nmodelscope>=1.17.1\ndeepfilternet>=0.5.6\nopenai>=1.40.1\nuseful-moonshine @ git+https://github.com/andimarafioti/moonshine.git"
        },
        {
          "name": "s2s_pipeline.py",
          "type": "blob",
          "size": 17.5107421875,
          "content": "import logging\nimport os\nimport sys\nfrom copy import copy\nfrom pathlib import Path\nfrom queue import Queue\nfrom threading import Event\nfrom typing import Optional\nfrom sys import platform\nfrom VAD.vad_handler import VADHandler\nfrom arguments_classes.chat_tts_arguments import ChatTTSHandlerArguments\nfrom arguments_classes.language_model_arguments import LanguageModelHandlerArguments\nfrom arguments_classes.mlx_language_model_arguments import (\n    MLXLanguageModelHandlerArguments,\n)\nfrom arguments_classes.module_arguments import ModuleArguments\nfrom arguments_classes.paraformer_stt_arguments import ParaformerSTTHandlerArguments\nfrom arguments_classes.parler_tts_arguments import ParlerTTSHandlerArguments\nfrom arguments_classes.socket_receiver_arguments import SocketReceiverArguments\nfrom arguments_classes.socket_sender_arguments import SocketSenderArguments\nfrom arguments_classes.vad_arguments import VADHandlerArguments\nfrom arguments_classes.whisper_stt_arguments import WhisperSTTHandlerArguments\nfrom arguments_classes.faster_whisper_stt_arguments import (\n    FasterWhisperSTTHandlerArguments,\n)\nfrom arguments_classes.melo_tts_arguments import MeloTTSHandlerArguments\nfrom arguments_classes.open_api_language_model_arguments import OpenApiLanguageModelHandlerArguments\nfrom arguments_classes.facebookmms_tts_arguments import FacebookMMSTTSHandlerArguments\nimport torch\nimport nltk\nfrom rich.console import Console\nfrom transformers import (\n    HfArgumentParser,\n)\n\nfrom utils.thread_manager import ThreadManager\n\n# Ensure that the necessary NLTK resources are available\ntry:\n    nltk.data.find(\"tokenizers/punkt_tab\")\nexcept (LookupError, OSError):\n    nltk.download(\"punkt_tab\")\ntry:\n    nltk.data.find(\"tokenizers/averaged_perceptron_tagger_eng\")\nexcept (LookupError, OSError):\n    nltk.download(\"averaged_perceptron_tagger_eng\")\n\n# caching allows ~50% compilation time reduction\n# see https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit#heading=h.o2asbxsrp1ma\nCURRENT_DIR = Path(__file__).resolve().parent\nos.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = os.path.join(CURRENT_DIR, \"tmp\")\n\nconsole = Console()\nlogging.getLogger(\"numba\").setLevel(logging.WARNING)  # quiet down numba logs\n\n\ndef rename_args(args, prefix):\n    \"\"\"\n    Rename arguments by removing the prefix and prepares the gen_kwargs.\n    \"\"\"\n    gen_kwargs = {}\n    for key in copy(args.__dict__):\n        if key.startswith(prefix):\n            value = args.__dict__.pop(key)\n            new_key = key[len(prefix) + 1 :]  # Remove prefix and underscore\n            if new_key.startswith(\"gen_\"):\n                gen_kwargs[new_key[4:]] = value  # Remove 'gen_' and add to dict\n            else:\n                args.__dict__[new_key] = value\n\n    args.__dict__[\"gen_kwargs\"] = gen_kwargs\n\n\ndef parse_arguments():\n    parser = HfArgumentParser(\n        (\n            ModuleArguments,\n            SocketReceiverArguments,\n            SocketSenderArguments,\n            VADHandlerArguments,\n            WhisperSTTHandlerArguments,\n            ParaformerSTTHandlerArguments,\n            FasterWhisperSTTHandlerArguments,\n            LanguageModelHandlerArguments,\n            OpenApiLanguageModelHandlerArguments,\n            MLXLanguageModelHandlerArguments,\n            ParlerTTSHandlerArguments,\n            MeloTTSHandlerArguments,\n            ChatTTSHandlerArguments,\n            FacebookMMSTTSHandlerArguments,\n        )\n    )\n\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # Parse configurations from a JSON file if specified\n        return parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        # Parse arguments from command line if no JSON file is provided\n        return parser.parse_args_into_dataclasses()\n\n\ndef setup_logger(log_level):\n    global logger\n    logging.basicConfig(\n        level=log_level.upper(),\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    )\n    logger = logging.getLogger(__name__)\n\n    # torch compile logs\n    if log_level == \"debug\":\n        torch._logging.set_logs(graph_breaks=True, recompiles=True, cudagraphs=True)\n\n\ndef optimal_mac_settings(mac_optimal_settings: Optional[str], *handler_kwargs):\n    if mac_optimal_settings:\n        for kwargs in handler_kwargs:\n            if hasattr(kwargs, \"device\"):\n                kwargs.device = \"mps\"\n            if hasattr(kwargs, \"mode\"):\n                kwargs.mode = \"local\"\n            if hasattr(kwargs, \"stt\"):\n                kwargs.stt = \"moonshine\"\n            if hasattr(kwargs, \"llm\"):\n                kwargs.llm = \"mlx-lm\"\n            if hasattr(kwargs, \"tts\"):\n                kwargs.tts = \"melo\"\n\n\ndef check_mac_settings(module_kwargs):\n    if platform == \"darwin\":\n        if module_kwargs.device == \"cuda\":\n            raise ValueError(\n                \"Cannot use CUDA on macOS. Please set the device to 'cpu' or 'mps'.\"\n            )\n        if module_kwargs.llm != \"mlx-lm\":\n            logger.warning(\n                \"For macOS users, it is recommended to use mlx-lm. You can activate it by passing --llm mlx-lm.\"\n            )\n        if module_kwargs.tts != \"melo\":\n            logger.warning(\n                \"If you experiences issues generating the voice, considering setting the tts to melo.\"\n            )\n\n\ndef overwrite_device_argument(common_device: Optional[str], *handler_kwargs):\n    if common_device:\n        for kwargs in handler_kwargs:\n            if hasattr(kwargs, \"lm_device\"):\n                kwargs.lm_device = common_device\n            if hasattr(kwargs, \"tts_device\"):\n                kwargs.tts_device = common_device\n            if hasattr(kwargs, \"stt_device\"):\n                kwargs.stt_device = common_device\n            if hasattr(kwargs, \"paraformer_stt_device\"):\n                kwargs.paraformer_stt_device = common_device\n            if hasattr(kwargs, \"facebook_mms_device\"):\n                kwargs.facebook_mms_device = common_device\n\n\ndef prepare_module_args(module_kwargs, *handler_kwargs):\n    optimal_mac_settings(module_kwargs.local_mac_optimal_settings, module_kwargs)\n    if platform == \"darwin\":\n        check_mac_settings(module_kwargs)\n    overwrite_device_argument(module_kwargs.device, *handler_kwargs)\n\n\ndef prepare_all_args(\n    module_kwargs,\n    whisper_stt_handler_kwargs,\n    paraformer_stt_handler_kwargs,\n    faster_whisper_stt_handler_kwargs,\n    language_model_handler_kwargs,\n    open_api_language_model_handler_kwargs,\n    mlx_language_model_handler_kwargs,\n    parler_tts_handler_kwargs,\n    melo_tts_handler_kwargs,\n    chat_tts_handler_kwargs,\n    facebook_mms_tts_handler_kwargs,\n):\n    prepare_module_args(\n        module_kwargs,\n        whisper_stt_handler_kwargs,\n        faster_whisper_stt_handler_kwargs,\n        paraformer_stt_handler_kwargs,\n        language_model_handler_kwargs,\n        open_api_language_model_handler_kwargs,\n        mlx_language_model_handler_kwargs,\n        parler_tts_handler_kwargs,\n        melo_tts_handler_kwargs,\n        chat_tts_handler_kwargs,\n        facebook_mms_tts_handler_kwargs,\n    )\n\n    rename_args(whisper_stt_handler_kwargs, \"stt\")\n    rename_args(faster_whisper_stt_handler_kwargs, \"faster_whisper_stt\")\n    rename_args(paraformer_stt_handler_kwargs, \"paraformer_stt\")\n    rename_args(language_model_handler_kwargs, \"lm\")\n    rename_args(mlx_language_model_handler_kwargs, \"mlx_lm\")\n    rename_args(open_api_language_model_handler_kwargs, \"open_api\")\n    rename_args(parler_tts_handler_kwargs, \"tts\")\n    rename_args(melo_tts_handler_kwargs, \"melo\")\n    rename_args(chat_tts_handler_kwargs, \"chat_tts\")\n    rename_args(facebook_mms_tts_handler_kwargs, \"facebook_mms\")\n\n\ndef initialize_queues_and_events():\n    return {\n        \"stop_event\": Event(),\n        \"should_listen\": Event(),\n        \"recv_audio_chunks_queue\": Queue(),\n        \"send_audio_chunks_queue\": Queue(),\n        \"spoken_prompt_queue\": Queue(),\n        \"text_prompt_queue\": Queue(),\n        \"lm_response_queue\": Queue(),\n    }\n\n\ndef build_pipeline(\n    module_kwargs,\n    socket_receiver_kwargs,\n    socket_sender_kwargs,\n    vad_handler_kwargs,\n    whisper_stt_handler_kwargs,\n    faster_whisper_stt_handler_kwargs,\n    paraformer_stt_handler_kwargs,\n    language_model_handler_kwargs,\n    open_api_language_model_handler_kwargs,\n    mlx_language_model_handler_kwargs,\n    parler_tts_handler_kwargs,\n    melo_tts_handler_kwargs,\n    chat_tts_handler_kwargs,\n    facebook_mms_tts_handler_kwargs,\n    queues_and_events,\n):\n    stop_event = queues_and_events[\"stop_event\"]\n    should_listen = queues_and_events[\"should_listen\"]\n    recv_audio_chunks_queue = queues_and_events[\"recv_audio_chunks_queue\"]\n    send_audio_chunks_queue = queues_and_events[\"send_audio_chunks_queue\"]\n    spoken_prompt_queue = queues_and_events[\"spoken_prompt_queue\"]\n    text_prompt_queue = queues_and_events[\"text_prompt_queue\"]\n    lm_response_queue = queues_and_events[\"lm_response_queue\"]\n    if module_kwargs.mode == \"local\":\n        from connections.local_audio_streamer import LocalAudioStreamer\n\n        local_audio_streamer = LocalAudioStreamer(\n            input_queue=recv_audio_chunks_queue, output_queue=send_audio_chunks_queue\n        )\n        comms_handlers = [local_audio_streamer]\n        should_listen.set()\n    else:\n        from connections.socket_receiver import SocketReceiver\n        from connections.socket_sender import SocketSender\n\n        comms_handlers = [\n            SocketReceiver(\n                stop_event,\n                recv_audio_chunks_queue,\n                should_listen,\n                host=socket_receiver_kwargs.recv_host,\n                port=socket_receiver_kwargs.recv_port,\n                chunk_size=socket_receiver_kwargs.chunk_size,\n            ),\n            SocketSender(\n                stop_event,\n                send_audio_chunks_queue,\n                host=socket_sender_kwargs.send_host,\n                port=socket_sender_kwargs.send_port,\n            ),\n        ]\n\n    vad = VADHandler(\n        stop_event,\n        queue_in=recv_audio_chunks_queue,\n        queue_out=spoken_prompt_queue,\n        setup_args=(should_listen,),\n        setup_kwargs=vars(vad_handler_kwargs),\n    )\n\n    stt = get_stt_handler(module_kwargs, stop_event, spoken_prompt_queue, text_prompt_queue, whisper_stt_handler_kwargs, faster_whisper_stt_handler_kwargs, paraformer_stt_handler_kwargs)\n    lm = get_llm_handler(module_kwargs, stop_event, text_prompt_queue, lm_response_queue, language_model_handler_kwargs, open_api_language_model_handler_kwargs, mlx_language_model_handler_kwargs)\n    tts = get_tts_handler(module_kwargs, stop_event, lm_response_queue, send_audio_chunks_queue, should_listen, parler_tts_handler_kwargs, melo_tts_handler_kwargs, chat_tts_handler_kwargs, facebook_mms_tts_handler_kwargs)\n\n    return ThreadManager([*comms_handlers, vad, stt, lm, tts])\n\n\ndef get_stt_handler(module_kwargs, stop_event, spoken_prompt_queue, text_prompt_queue, whisper_stt_handler_kwargs, faster_whisper_stt_handler_kwargs, paraformer_stt_handler_kwargs):\n    if module_kwargs.stt == \"moonshine\":\n        from STT.moonshine_handler import MoonshineSTTHandler\n        return MoonshineSTTHandler(\n            stop_event,\n            queue_in=spoken_prompt_queue,\n            queue_out=text_prompt_queue,\n        )\n    if module_kwargs.stt == \"whisper\":\n        from STT.whisper_stt_handler import WhisperSTTHandler\n        return WhisperSTTHandler(\n            stop_event,\n            queue_in=spoken_prompt_queue,\n            queue_out=text_prompt_queue,\n            setup_kwargs=vars(whisper_stt_handler_kwargs),\n        )\n    elif module_kwargs.stt == \"whisper-mlx\":\n        from STT.lightning_whisper_mlx_handler import LightningWhisperSTTHandler\n        return LightningWhisperSTTHandler(\n            stop_event,\n            queue_in=spoken_prompt_queue,\n            queue_out=text_prompt_queue,\n            setup_kwargs=vars(whisper_stt_handler_kwargs),\n        )\n    elif module_kwargs.stt == \"paraformer\":\n        from STT.paraformer_handler import ParaformerSTTHandler\n        return ParaformerSTTHandler(\n            stop_event,\n            queue_in=spoken_prompt_queue,\n            queue_out=text_prompt_queue,\n            setup_kwargs=vars(paraformer_stt_handler_kwargs),\n        )\n    elif module_kwargs.stt == \"faster-whisper\":\n        from STT.faster_whisper_handler import FasterWhisperSTTHandler\n\n        return FasterWhisperSTTHandler(\n            stop_event,\n            queue_in=spoken_prompt_queue,\n            queue_out=text_prompt_queue,\n            setup_kwargs=vars(faster_whisper_stt_handler_kwargs),\n        )\n    else:\n        raise ValueError(\"The STT should be either whisper, whisper-mlx, or paraformer.\")\n\n\ndef get_llm_handler(\n    module_kwargs, \n    stop_event, \n    text_prompt_queue, \n    lm_response_queue, \n    language_model_handler_kwargs,\n    open_api_language_model_handler_kwargs,\n    mlx_language_model_handler_kwargs\n):\n    if module_kwargs.llm == \"transformers\":\n        from LLM.language_model import LanguageModelHandler\n        return LanguageModelHandler(\n            stop_event,\n            queue_in=text_prompt_queue,\n            queue_out=lm_response_queue,\n            setup_kwargs=vars(language_model_handler_kwargs),\n        )\n    elif module_kwargs.llm == \"open_api\":\n        from LLM.openai_api_language_model import OpenApiModelHandler\n        return OpenApiModelHandler(\n            stop_event,\n            queue_in=text_prompt_queue,\n            queue_out=lm_response_queue,\n            setup_kwargs=vars(open_api_language_model_handler_kwargs),\n        )\n\n    elif module_kwargs.llm == \"mlx-lm\":\n        from LLM.mlx_language_model import MLXLanguageModelHandler\n        return MLXLanguageModelHandler(\n            stop_event,\n            queue_in=text_prompt_queue,\n            queue_out=lm_response_queue,\n            setup_kwargs=vars(mlx_language_model_handler_kwargs),\n        )\n\n    else:\n        raise ValueError(\"The LLM should be either transformers or mlx-lm\")\n\n\ndef get_tts_handler(module_kwargs, stop_event, lm_response_queue, send_audio_chunks_queue, should_listen, parler_tts_handler_kwargs, melo_tts_handler_kwargs, chat_tts_handler_kwargs, facebook_mms_tts_handler_kwargs):\n    if module_kwargs.tts == \"parler\":\n        from TTS.parler_handler import ParlerTTSHandler\n        return ParlerTTSHandler(\n            stop_event,\n            queue_in=lm_response_queue,\n            queue_out=send_audio_chunks_queue,\n            setup_args=(should_listen,),\n            setup_kwargs=vars(parler_tts_handler_kwargs),\n        )\n    elif module_kwargs.tts == \"melo\":\n        try:\n            from TTS.melo_handler import MeloTTSHandler\n        except RuntimeError as e:\n            logger.error(\n                \"Error importing MeloTTSHandler. You might need to run: python -m unidic download\"\n            )\n            raise e\n        return MeloTTSHandler(\n            stop_event,\n            queue_in=lm_response_queue,\n            queue_out=send_audio_chunks_queue,\n            setup_args=(should_listen,),\n            setup_kwargs=vars(melo_tts_handler_kwargs),\n        )\n    elif module_kwargs.tts == \"chatTTS\":\n        try:\n            from TTS.chatTTS_handler import ChatTTSHandler\n        except RuntimeError as e:\n            logger.error(\"Error importing ChatTTSHandler\")\n            raise e\n        return ChatTTSHandler(\n            stop_event,\n            queue_in=lm_response_queue,\n            queue_out=send_audio_chunks_queue,\n            setup_args=(should_listen,),\n            setup_kwargs=vars(chat_tts_handler_kwargs),\n        )\n    elif module_kwargs.tts == \"facebookMMS\":\n        from TTS.facebookmms_handler import FacebookMMSTTSHandler\n        return FacebookMMSTTSHandler(\n            stop_event,\n            queue_in=lm_response_queue,\n            queue_out=send_audio_chunks_queue,\n            setup_args=(should_listen,),\n            setup_kwargs=vars(facebook_mms_tts_handler_kwargs),\n        )\n    else:\n        raise ValueError(\"The TTS should be either parler, melo or chatTTS\")\n\n\ndef main():\n    (\n        module_kwargs,\n        socket_receiver_kwargs,\n        socket_sender_kwargs,\n        vad_handler_kwargs,\n        whisper_stt_handler_kwargs,\n        paraformer_stt_handler_kwargs,\n        faster_whisper_stt_handler_kwargs,  # Add this line\n        language_model_handler_kwargs,\n        open_api_language_model_handler_kwargs,\n        mlx_language_model_handler_kwargs,\n        parler_tts_handler_kwargs,\n        melo_tts_handler_kwargs,\n        chat_tts_handler_kwargs,\n        facebook_mms_tts_handler_kwargs,\n    ) = parse_arguments()\n\n    setup_logger(module_kwargs.log_level)\n\n    prepare_all_args(\n        module_kwargs,\n        whisper_stt_handler_kwargs,\n        paraformer_stt_handler_kwargs,\n        faster_whisper_stt_handler_kwargs,  # Add this line\n        language_model_handler_kwargs,\n        open_api_language_model_handler_kwargs,\n        mlx_language_model_handler_kwargs,\n        parler_tts_handler_kwargs,\n        melo_tts_handler_kwargs,\n        chat_tts_handler_kwargs,\n        facebook_mms_tts_handler_kwargs,\n    )\n\n    queues_and_events = initialize_queues_and_events()\n\n    pipeline_manager = build_pipeline(\n        module_kwargs,\n        socket_receiver_kwargs,\n        socket_sender_kwargs,\n        vad_handler_kwargs,\n        whisper_stt_handler_kwargs,\n        faster_whisper_stt_handler_kwargs,  # Add this line\n        paraformer_stt_handler_kwargs,\n        language_model_handler_kwargs,\n        open_api_language_model_handler_kwargs,\n        mlx_language_model_handler_kwargs,\n        parler_tts_handler_kwargs,\n        melo_tts_handler_kwargs,\n        chat_tts_handler_kwargs,\n        facebook_mms_tts_handler_kwargs,\n        queues_and_events,\n    )\n\n    try:\n        pipeline_manager.start()\n    except KeyboardInterrupt:\n        pipeline_manager.stop()\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}