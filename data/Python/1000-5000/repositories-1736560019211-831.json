{
  "metadata": {
    "timestamp": 1736560019211,
    "page": 831,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "junxiaosong/AlphaZero_Gomoku",
      "stars": 3379,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0185546875,
          "content": "*.pyc\n__pycache__/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2017 junxiaosong\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.0810546875,
          "content": "## AlphaZero-Gomoku\nThis is an implementation of the AlphaZero algorithm for playing the simple board game Gomoku (also called Gobang or Five in a Row) from pure self-play training. The game Gomoku is much simpler than Go or chess, so that we can focus on the training scheme of AlphaZero and obtain a pretty good AI model on a single PC in a few hours. \n\nReferences:  \n1. AlphaZero: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\n2. AlphaGo Zero: Mastering the game of Go without human knowledge\n\n### Update 2018.2.24: supports training with TensorFlow!\n### Update 2018.1.17: supports training with PyTorch!\n\n### Example Games Between Trained Models\n- Each move with 400 MCTS playouts:  \n![playout400](https://raw.githubusercontent.com/junxiaosong/AlphaZero_Gomoku/master/playout400.gif)\n\n### Requirements\nTo play with the trained AI models, only need:\n- Python >= 2.7\n- Numpy >= 1.11\n\nTo train the AI model from scratch, further need, either:\n- Theano >= 0.7 and Lasagne >= 0.1      \nor\n- PyTorch >= 0.2.0    \nor\n- TensorFlow\n\n**PS**: if your Theano's version > 0.7, please follow this [issue](https://github.com/aigamedev/scikit-neuralnetwork/issues/235) to install Lasagne,  \notherwise, force pip to downgrade Theano to 0.7 ``pip install --upgrade theano==0.7.0``\n\nIf you would like to train the model using other DL frameworks, you only need to rewrite policy_value_net.py.\n\n### Getting Started\nTo play with provided models, run the following script from the directory:  \n```\npython human_play.py  \n```\nYou may modify human_play.py to try different provided models or the pure MCTS.\n\nTo train the AI model from scratch, with Theano and Lasagne, directly run:   \n```\npython train.py\n```\nWith PyTorch or TensorFlow, first modify the file [train.py](https://github.com/junxiaosong/AlphaZero_Gomoku/blob/master/train.py), i.e., comment the line\n```\nfrom policy_value_net import PolicyValueNet  # Theano and Lasagne\n```\nand uncomment the line \n```\n# from policy_value_net_pytorch import PolicyValueNet  # Pytorch\nor\n# from policy_value_net_tensorflow import PolicyValueNet # Tensorflow\n```\nand then execute: ``python train.py``  (To use GPU in PyTorch, set ``use_gpu=True`` and use ``return loss.item(), entropy.item()`` in function train_step in policy_value_net_pytorch.py if your pytorch version is greater than 0.5)\n\nThe models (best_policy.model and current_policy.model) will be saved every a few updates (default 50).  \n\n**Note:** the 4 provided models were trained using Theano/Lasagne, to use them with PyTorch, please refer to [issue 5](https://github.com/junxiaosong/AlphaZero_Gomoku/issues/5).\n\n**Tips for training:**\n1. It is good to start with a 6 * 6 board and 4 in a row. For this case, we may obtain a reasonably good model within 500~1000 self-play games in about 2 hours.\n2. For the case of 8 * 8 board and 5 in a row, it may need 2000~3000 self-play games to get a good model, and it may take about 2 days on a single PC.\n\n### Further reading\nMy article describing some details about the implementation in Chinese: [https://zhuanlan.zhihu.com/p/32089487](https://zhuanlan.zhihu.com/p/32089487) \n"
        },
        {
          "name": "best_policy_6_6_4.model",
          "type": "blob",
          "size": 407.9296875,
          "content": null
        },
        {
          "name": "best_policy_6_6_4.model2",
          "type": "blob",
          "size": 407.9296875,
          "content": null
        },
        {
          "name": "best_policy_8_8_5.model",
          "type": "blob",
          "size": 465.79296875,
          "content": null
        },
        {
          "name": "best_policy_8_8_5.model2",
          "type": "blob",
          "size": 465.79296875,
          "content": null
        },
        {
          "name": "game.py",
          "type": "blob",
          "size": 8.015625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author: Junxiao Song\n\"\"\"\n\nfrom __future__ import print_function\nimport numpy as np\n\n\nclass Board(object):\n    \"\"\"board for the game\"\"\"\n\n    def __init__(self, **kwargs):\n        self.width = int(kwargs.get('width', 8))\n        self.height = int(kwargs.get('height', 8))\n        # board states stored as a dict,\n        # key: move as location on the board,\n        # value: player as pieces type\n        self.states = {}\n        # need how many pieces in a row to win\n        self.n_in_row = int(kwargs.get('n_in_row', 5))\n        self.players = [1, 2]  # player1 and player2\n\n    def init_board(self, start_player=0):\n        if self.width < self.n_in_row or self.height < self.n_in_row:\n            raise Exception('board width and height can not be '\n                            'less than {}'.format(self.n_in_row))\n        self.current_player = self.players[start_player]  # start player\n        # keep available moves in a list\n        self.availables = list(range(self.width * self.height))\n        self.states = {}\n        self.last_move = -1\n\n    def move_to_location(self, move):\n        \"\"\"\n        3*3 board's moves like:\n        6 7 8\n        3 4 5\n        0 1 2\n        and move 5's location is (1,2)\n        \"\"\"\n        h = move // self.width\n        w = move % self.width\n        return [h, w]\n\n    def location_to_move(self, location):\n        if len(location) != 2:\n            return -1\n        h = location[0]\n        w = location[1]\n        move = h * self.width + w\n        if move not in range(self.width * self.height):\n            return -1\n        return move\n\n    def current_state(self):\n        \"\"\"return the board state from the perspective of the current player.\n        state shape: 4*width*height\n        \"\"\"\n\n        square_state = np.zeros((4, self.width, self.height))\n        if self.states:\n            moves, players = np.array(list(zip(*self.states.items())))\n            move_curr = moves[players == self.current_player]\n            move_oppo = moves[players != self.current_player]\n            square_state[0][move_curr // self.width,\n                            move_curr % self.height] = 1.0\n            square_state[1][move_oppo // self.width,\n                            move_oppo % self.height] = 1.0\n            # indicate the last move location\n            square_state[2][self.last_move // self.width,\n                            self.last_move % self.height] = 1.0\n        if len(self.states) % 2 == 0:\n            square_state[3][:, :] = 1.0  # indicate the colour to play\n        return square_state[:, ::-1, :]\n\n    def do_move(self, move):\n        self.states[move] = self.current_player\n        self.availables.remove(move)\n        self.current_player = (\n            self.players[0] if self.current_player == self.players[1]\n            else self.players[1]\n        )\n        self.last_move = move\n\n    def has_a_winner(self):\n        width = self.width\n        height = self.height\n        states = self.states\n        n = self.n_in_row\n\n        moved = list(set(range(width * height)) - set(self.availables))\n        if len(moved) < self.n_in_row *2-1:\n            return False, -1\n\n        for m in moved:\n            h = m // width\n            w = m % width\n            player = states[m]\n\n            if (w in range(width - n + 1) and\n                    len(set(states.get(i, -1) for i in range(m, m + n))) == 1):\n                return True, player\n\n            if (h in range(height - n + 1) and\n                    len(set(states.get(i, -1) for i in range(m, m + n * width, width))) == 1):\n                return True, player\n\n            if (w in range(width - n + 1) and h in range(height - n + 1) and\n                    len(set(states.get(i, -1) for i in range(m, m + n * (width + 1), width + 1))) == 1):\n                return True, player\n\n            if (w in range(n - 1, width) and h in range(height - n + 1) and\n                    len(set(states.get(i, -1) for i in range(m, m + n * (width - 1), width - 1))) == 1):\n                return True, player\n\n        return False, -1\n\n    def game_end(self):\n        \"\"\"Check whether the game is ended or not\"\"\"\n        win, winner = self.has_a_winner()\n        if win:\n            return True, winner\n        elif not len(self.availables):\n            return True, -1\n        return False, -1\n\n    def get_current_player(self):\n        return self.current_player\n\n\nclass Game(object):\n    \"\"\"game server\"\"\"\n\n    def __init__(self, board, **kwargs):\n        self.board = board\n\n    def graphic(self, board, player1, player2):\n        \"\"\"Draw the board and show game info\"\"\"\n        width = board.width\n        height = board.height\n\n        print(\"Player\", player1, \"with X\".rjust(3))\n        print(\"Player\", player2, \"with O\".rjust(3))\n        print()\n        for x in range(width):\n            print(\"{0:8}\".format(x), end='')\n        print('\\r\\n')\n        for i in range(height - 1, -1, -1):\n            print(\"{0:4d}\".format(i), end='')\n            for j in range(width):\n                loc = i * width + j\n                p = board.states.get(loc, -1)\n                if p == player1:\n                    print('X'.center(8), end='')\n                elif p == player2:\n                    print('O'.center(8), end='')\n                else:\n                    print('_'.center(8), end='')\n            print('\\r\\n\\r\\n')\n\n    def start_play(self, player1, player2, start_player=0, is_shown=1):\n        \"\"\"start a game between two players\"\"\"\n        if start_player not in (0, 1):\n            raise Exception('start_player should be either 0 (player1 first) '\n                            'or 1 (player2 first)')\n        self.board.init_board(start_player)\n        p1, p2 = self.board.players\n        player1.set_player_ind(p1)\n        player2.set_player_ind(p2)\n        players = {p1: player1, p2: player2}\n        if is_shown:\n            self.graphic(self.board, player1.player, player2.player)\n        while True:\n            current_player = self.board.get_current_player()\n            player_in_turn = players[current_player]\n            move = player_in_turn.get_action(self.board)\n            self.board.do_move(move)\n            if is_shown:\n                self.graphic(self.board, player1.player, player2.player)\n            end, winner = self.board.game_end()\n            if end:\n                if is_shown:\n                    if winner != -1:\n                        print(\"Game end. Winner is\", players[winner])\n                    else:\n                        print(\"Game end. Tie\")\n                return winner\n\n    def start_self_play(self, player, is_shown=0, temp=1e-3):\n        \"\"\" start a self-play game using a MCTS player, reuse the search tree,\n        and store the self-play data: (state, mcts_probs, z) for training\n        \"\"\"\n        self.board.init_board()\n        p1, p2 = self.board.players\n        states, mcts_probs, current_players = [], [], []\n        while True:\n            move, move_probs = player.get_action(self.board,\n                                                 temp=temp,\n                                                 return_prob=1)\n            # store the data\n            states.append(self.board.current_state())\n            mcts_probs.append(move_probs)\n            current_players.append(self.board.current_player)\n            # perform a move\n            self.board.do_move(move)\n            if is_shown:\n                self.graphic(self.board, p1, p2)\n            end, winner = self.board.game_end()\n            if end:\n                # winner from the perspective of the current player of each state\n                winners_z = np.zeros(len(current_players))\n                if winner != -1:\n                    winners_z[np.array(current_players) == winner] = 1.0\n                    winners_z[np.array(current_players) != winner] = -1.0\n                # reset MCTS root node\n                player.reset_player()\n                if is_shown:\n                    if winner != -1:\n                        print(\"Game end. Winner is player:\", winner)\n                    else:\n                        print(\"Game end. Tie\")\n                return winner, zip(states, mcts_probs, winners_z)\n"
        },
        {
          "name": "human_play.py",
          "type": "blob",
          "size": 2.8212890625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nhuman VS AI models\nInput your move in the format: 2,3\n\n@author: Junxiao Song\n\"\"\"\n\nfrom __future__ import print_function\nimport pickle\nfrom game import Board, Game\nfrom mcts_pure import MCTSPlayer as MCTS_Pure\nfrom mcts_alphaZero import MCTSPlayer\nfrom policy_value_net_numpy import PolicyValueNetNumpy\n# from policy_value_net import PolicyValueNet  # Theano and Lasagne\n# from policy_value_net_pytorch import PolicyValueNet  # Pytorch\n# from policy_value_net_tensorflow import PolicyValueNet # Tensorflow\n# from policy_value_net_keras import PolicyValueNet  # Keras\n\n\nclass Human(object):\n    \"\"\"\n    human player\n    \"\"\"\n\n    def __init__(self):\n        self.player = None\n\n    def set_player_ind(self, p):\n        self.player = p\n\n    def get_action(self, board):\n        try:\n            location = input(\"Your move: \")\n            if isinstance(location, str):  # for python3\n                location = [int(n, 10) for n in location.split(\",\")]\n            move = board.location_to_move(location)\n        except Exception as e:\n            move = -1\n        if move == -1 or move not in board.availables:\n            print(\"invalid move\")\n            move = self.get_action(board)\n        return move\n\n    def __str__(self):\n        return \"Human {}\".format(self.player)\n\n\ndef run():\n    n = 5\n    width, height = 8, 8\n    model_file = 'best_policy_8_8_5.model'\n    try:\n        board = Board(width=width, height=height, n_in_row=n)\n        game = Game(board)\n\n        # ############### human VS AI ###################\n        # load the trained policy_value_net in either Theano/Lasagne, PyTorch or TensorFlow\n\n        # best_policy = PolicyValueNet(width, height, model_file = model_file)\n        # mcts_player = MCTSPlayer(best_policy.policy_value_fn, c_puct=5, n_playout=400)\n\n        # load the provided model (trained in Theano/Lasagne) into a MCTS player written in pure numpy\n        try:\n            policy_param = pickle.load(open(model_file, 'rb'))\n        except:\n            policy_param = pickle.load(open(model_file, 'rb'),\n                                       encoding='bytes')  # To support python3\n        best_policy = PolicyValueNetNumpy(width, height, policy_param)\n        mcts_player = MCTSPlayer(best_policy.policy_value_fn,\n                                 c_puct=5,\n                                 n_playout=400)  # set larger n_playout for better performance\n\n        # uncomment the following line to play with pure MCTS (it's much weaker even with a larger n_playout)\n        # mcts_player = MCTS_Pure(c_puct=5, n_playout=1000)\n\n        # human player, input your move in the format: 2,3\n        human = Human()\n\n        # set start_player=0 for human first\n        game.start_play(human, mcts_player, start_player=1, is_shown=1)\n    except KeyboardInterrupt:\n        print('\\n\\rquit')\n\n\nif __name__ == '__main__':\n    run()\n"
        },
        {
          "name": "mcts_alphaZero.py",
          "type": "blob",
          "size": 7.66015625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nMonte Carlo Tree Search in AlphaGo Zero style, which uses a policy-value\nnetwork to guide the tree search and evaluate the leaf nodes\n\n@author: Junxiao Song\n\"\"\"\n\nimport numpy as np\nimport copy\n\n\ndef softmax(x):\n    probs = np.exp(x - np.max(x))\n    probs /= np.sum(probs)\n    return probs\n\n\nclass TreeNode(object):\n    \"\"\"A node in the MCTS tree.\n\n    Each node keeps track of its own value Q, prior probability P, and\n    its visit-count-adjusted prior score u.\n    \"\"\"\n\n    def __init__(self, parent, prior_p):\n        self._parent = parent\n        self._children = {}  # a map from action to TreeNode\n        self._n_visits = 0\n        self._Q = 0\n        self._u = 0\n        self._P = prior_p\n\n    def expand(self, action_priors):\n        \"\"\"Expand tree by creating new children.\n        action_priors: a list of tuples of actions and their prior probability\n            according to the policy function.\n        \"\"\"\n        for action, prob in action_priors:\n            if action not in self._children:\n                self._children[action] = TreeNode(self, prob)\n\n    def select(self, c_puct):\n        \"\"\"Select action among children that gives maximum action value Q\n        plus bonus u(P).\n        Return: A tuple of (action, next_node)\n        \"\"\"\n        return max(self._children.items(),\n                   key=lambda act_node: act_node[1].get_value(c_puct))\n\n    def update(self, leaf_value):\n        \"\"\"Update node values from leaf evaluation.\n        leaf_value: the value of subtree evaluation from the current player's\n            perspective.\n        \"\"\"\n        # Count visit.\n        self._n_visits += 1\n        # Update Q, a running average of values for all visits.\n        self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n\n    def update_recursive(self, leaf_value):\n        \"\"\"Like a call to update(), but applied recursively for all ancestors.\n        \"\"\"\n        # If it is not root, this node's parent should be updated first.\n        if self._parent:\n            self._parent.update_recursive(-leaf_value)\n        self.update(leaf_value)\n\n    def get_value(self, c_puct):\n        \"\"\"Calculate and return the value for this node.\n        It is a combination of leaf evaluations Q, and this node's prior\n        adjusted for its visit count, u.\n        c_puct: a number in (0, inf) controlling the relative impact of\n            value Q, and prior probability P, on this node's score.\n        \"\"\"\n        self._u = (c_puct * self._P *\n                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n        return self._Q + self._u\n\n    def is_leaf(self):\n        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\"\"\"\n        return self._children == {}\n\n    def is_root(self):\n        return self._parent is None\n\n\nclass MCTS(object):\n    \"\"\"An implementation of Monte Carlo Tree Search.\"\"\"\n\n    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n        \"\"\"\n        policy_value_fn: a function that takes in a board state and outputs\n            a list of (action, probability) tuples and also a score in [-1, 1]\n            (i.e. the expected value of the end game score from the current\n            player's perspective) for the current player.\n        c_puct: a number in (0, inf) that controls how quickly exploration\n            converges to the maximum-value policy. A higher value means\n            relying on the prior more.\n        \"\"\"\n        self._root = TreeNode(None, 1.0)\n        self._policy = policy_value_fn\n        self._c_puct = c_puct\n        self._n_playout = n_playout\n\n    def _playout(self, state):\n        \"\"\"Run a single playout from the root to the leaf, getting a value at\n        the leaf and propagating it back through its parents.\n        State is modified in-place, so a copy must be provided.\n        \"\"\"\n        node = self._root\n        while(1):\n            if node.is_leaf():\n                break\n            # Greedily select next move.\n            action, node = node.select(self._c_puct)\n            state.do_move(action)\n\n        # Evaluate the leaf using a network which outputs a list of\n        # (action, probability) tuples p and also a score v in [-1, 1]\n        # for the current player.\n        action_probs, leaf_value = self._policy(state)\n        # Check for end of game.\n        end, winner = state.game_end()\n        if not end:\n            node.expand(action_probs)\n        else:\n            # for end state，return the \"true\" leaf_value\n            if winner == -1:  # tie\n                leaf_value = 0.0\n            else:\n                leaf_value = (\n                    1.0 if winner == state.get_current_player() else -1.0\n                )\n\n        # Update value and visit count of nodes in this traversal.\n        node.update_recursive(-leaf_value)\n\n    def get_move_probs(self, state, temp=1e-3):\n        \"\"\"Run all playouts sequentially and return the available actions and\n        their corresponding probabilities.\n        state: the current game state\n        temp: temperature parameter in (0, 1] controls the level of exploration\n        \"\"\"\n        for n in range(self._n_playout):\n            state_copy = copy.deepcopy(state)\n            self._playout(state_copy)\n\n        # calc the move probabilities based on visit counts at the root node\n        act_visits = [(act, node._n_visits)\n                      for act, node in self._root._children.items()]\n        acts, visits = zip(*act_visits)\n        act_probs = softmax(1.0/temp * np.log(np.array(visits) + 1e-10))\n\n        return acts, act_probs\n\n    def update_with_move(self, last_move):\n        \"\"\"Step forward in the tree, keeping everything we already know\n        about the subtree.\n        \"\"\"\n        if last_move in self._root._children:\n            self._root = self._root._children[last_move]\n            self._root._parent = None\n        else:\n            self._root = TreeNode(None, 1.0)\n\n    def __str__(self):\n        return \"MCTS\"\n\n\nclass MCTSPlayer(object):\n    \"\"\"AI player based on MCTS\"\"\"\n\n    def __init__(self, policy_value_function,\n                 c_puct=5, n_playout=2000, is_selfplay=0):\n        self.mcts = MCTS(policy_value_function, c_puct, n_playout)\n        self._is_selfplay = is_selfplay\n\n    def set_player_ind(self, p):\n        self.player = p\n\n    def reset_player(self):\n        self.mcts.update_with_move(-1)\n\n    def get_action(self, board, temp=1e-3, return_prob=0):\n        sensible_moves = board.availables\n        # the pi vector returned by MCTS as in the alphaGo Zero paper\n        move_probs = np.zeros(board.width*board.height)\n        if len(sensible_moves) > 0:\n            acts, probs = self.mcts.get_move_probs(board, temp)\n            move_probs[list(acts)] = probs\n            if self._is_selfplay:\n                # add Dirichlet Noise for exploration (needed for\n                # self-play training)\n                move = np.random.choice(\n                    acts,\n                    p=0.75*probs + 0.25*np.random.dirichlet(0.3*np.ones(len(probs)))\n                )\n                # update the root node and reuse the search tree\n                self.mcts.update_with_move(move)\n            else:\n                # with the default temp=1e-3, it is almost equivalent\n                # to choosing the move with the highest prob\n                move = np.random.choice(acts, p=probs)\n                # reset the root node\n                self.mcts.update_with_move(-1)\n#                location = board.move_to_location(move)\n#                print(\"AI move: %d,%d\\n\" % (location[0], location[1]))\n\n            if return_prob:\n                return move, move_probs\n            else:\n                return move\n        else:\n            print(\"WARNING: the board is full\")\n\n    def __str__(self):\n        return \"MCTS {}\".format(self.player)\n"
        },
        {
          "name": "mcts_pure.py",
          "type": "blob",
          "size": 7.0234375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nA pure implementation of the Monte Carlo Tree Search (MCTS)\n\n@author: Junxiao Song\n\"\"\"\n\nimport numpy as np\nimport copy\nfrom operator import itemgetter\n\n\ndef rollout_policy_fn(board):\n    \"\"\"a coarse, fast version of policy_fn used in the rollout phase.\"\"\"\n    # rollout randomly\n    action_probs = np.random.rand(len(board.availables))\n    return zip(board.availables, action_probs)\n\n\ndef policy_value_fn(board):\n    \"\"\"a function that takes in a state and outputs a list of (action, probability)\n    tuples and a score for the state\"\"\"\n    # return uniform probabilities and 0 score for pure MCTS\n    action_probs = np.ones(len(board.availables))/len(board.availables)\n    return zip(board.availables, action_probs), 0\n\n\nclass TreeNode(object):\n    \"\"\"A node in the MCTS tree. Each node keeps track of its own value Q,\n    prior probability P, and its visit-count-adjusted prior score u.\n    \"\"\"\n\n    def __init__(self, parent, prior_p):\n        self._parent = parent\n        self._children = {}  # a map from action to TreeNode\n        self._n_visits = 0\n        self._Q = 0\n        self._u = 0\n        self._P = prior_p\n\n    def expand(self, action_priors):\n        \"\"\"Expand tree by creating new children.\n        action_priors: a list of tuples of actions and their prior probability\n            according to the policy function.\n        \"\"\"\n        for action, prob in action_priors:\n            if action not in self._children:\n                self._children[action] = TreeNode(self, prob)\n\n    def select(self, c_puct):\n        \"\"\"Select action among children that gives maximum action value Q\n        plus bonus u(P).\n        Return: A tuple of (action, next_node)\n        \"\"\"\n        return max(self._children.items(),\n                   key=lambda act_node: act_node[1].get_value(c_puct))\n\n    def update(self, leaf_value):\n        \"\"\"Update node values from leaf evaluation.\n        leaf_value: the value of subtree evaluation from the current player's\n            perspective.\n        \"\"\"\n        # Count visit.\n        self._n_visits += 1\n        # Update Q, a running average of values for all visits.\n        self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n\n    def update_recursive(self, leaf_value):\n        \"\"\"Like a call to update(), but applied recursively for all ancestors.\n        \"\"\"\n        # If it is not root, this node's parent should be updated first.\n        if self._parent:\n            self._parent.update_recursive(-leaf_value)\n        self.update(leaf_value)\n\n    def get_value(self, c_puct):\n        \"\"\"Calculate and return the value for this node.\n        It is a combination of leaf evaluations Q, and this node's prior\n        adjusted for its visit count, u.\n        c_puct: a number in (0, inf) controlling the relative impact of\n            value Q, and prior probability P, on this node's score.\n        \"\"\"\n        self._u = (c_puct * self._P *\n                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n        return self._Q + self._u\n\n    def is_leaf(self):\n        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\n        \"\"\"\n        return self._children == {}\n\n    def is_root(self):\n        return self._parent is None\n\n\nclass MCTS(object):\n    \"\"\"A simple implementation of Monte Carlo Tree Search.\"\"\"\n\n    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n        \"\"\"\n        policy_value_fn: a function that takes in a board state and outputs\n            a list of (action, probability) tuples and also a score in [-1, 1]\n            (i.e. the expected value of the end game score from the current\n            player's perspective) for the current player.\n        c_puct: a number in (0, inf) that controls how quickly exploration\n            converges to the maximum-value policy. A higher value means\n            relying on the prior more.\n        \"\"\"\n        self._root = TreeNode(None, 1.0)\n        self._policy = policy_value_fn\n        self._c_puct = c_puct\n        self._n_playout = n_playout\n\n    def _playout(self, state):\n        \"\"\"Run a single playout from the root to the leaf, getting a value at\n        the leaf and propagating it back through its parents.\n        State is modified in-place, so a copy must be provided.\n        \"\"\"\n        node = self._root\n        while(1):\n            if node.is_leaf():\n\n                break\n            # Greedily select next move.\n            action, node = node.select(self._c_puct)\n            state.do_move(action)\n\n        action_probs, _ = self._policy(state)\n        # Check for end of game\n        end, winner = state.game_end()\n        if not end:\n            node.expand(action_probs)\n        # Evaluate the leaf node by random rollout\n        leaf_value = self._evaluate_rollout(state)\n        # Update value and visit count of nodes in this traversal.\n        node.update_recursive(-leaf_value)\n\n    def _evaluate_rollout(self, state, limit=1000):\n        \"\"\"Use the rollout policy to play until the end of the game,\n        returning +1 if the current player wins, -1 if the opponent wins,\n        and 0 if it is a tie.\n        \"\"\"\n        player = state.get_current_player()\n        for i in range(limit):\n            end, winner = state.game_end()\n            if end:\n                break\n            action_probs = rollout_policy_fn(state)\n            max_action = max(action_probs, key=itemgetter(1))[0]\n            state.do_move(max_action)\n        else:\n            # If no break from the loop, issue a warning.\n            print(\"WARNING: rollout reached move limit\")\n        if winner == -1:  # tie\n            return 0\n        else:\n            return 1 if winner == player else -1\n\n    def get_move(self, state):\n        \"\"\"Runs all playouts sequentially and returns the most visited action.\n        state: the current game state\n\n        Return: the selected action\n        \"\"\"\n        for n in range(self._n_playout):\n            state_copy = copy.deepcopy(state)\n            self._playout(state_copy)\n        return max(self._root._children.items(),\n                   key=lambda act_node: act_node[1]._n_visits)[0]\n\n    def update_with_move(self, last_move):\n        \"\"\"Step forward in the tree, keeping everything we already know\n        about the subtree.\n        \"\"\"\n        if last_move in self._root._children:\n            self._root = self._root._children[last_move]\n            self._root._parent = None\n        else:\n            self._root = TreeNode(None, 1.0)\n\n    def __str__(self):\n        return \"MCTS\"\n\n\nclass MCTSPlayer(object):\n    \"\"\"AI player based on MCTS\"\"\"\n    def __init__(self, c_puct=5, n_playout=2000):\n        self.mcts = MCTS(policy_value_fn, c_puct, n_playout)\n\n    def set_player_ind(self, p):\n        self.player = p\n\n    def reset_player(self):\n        self.mcts.update_with_move(-1)\n\n    def get_action(self, board):\n        sensible_moves = board.availables\n        if len(sensible_moves) > 0:\n            move = self.mcts.get_move(board)\n            self.mcts.update_with_move(-1)\n            return move\n        else:\n            print(\"WARNING: the board is full\")\n\n    def __str__(self):\n        return \"MCTS {}\".format(self.player)\n"
        },
        {
          "name": "playout400.gif",
          "type": "blob",
          "size": 21.0166015625,
          "content": null
        },
        {
          "name": "policy_value_net.py",
          "type": "blob",
          "size": 5.0048828125,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nAn implementation of the policyValueNet in Theano and Lasagne\n\n@author: Junxiao Song\n\"\"\"\n\nfrom __future__ import print_function\nimport theano\nimport theano.tensor as T\nimport lasagne\nimport pickle\n\n\nclass PolicyValueNet():\n    \"\"\"policy-value network \"\"\"\n    def __init__(self, board_width, board_height, model_file=None):\n        self.board_width = board_width\n        self.board_height = board_height\n        self.learning_rate = T.scalar('learning_rate')\n        self.l2_const = 1e-4  # coef of l2 penalty\n        self.create_policy_value_net()\n        self._loss_train_op()\n        if model_file:\n            try:\n                net_params = pickle.load(open(model_file, 'rb'))\n            except:\n                # To support loading pretrained model in python3\n                net_params = pickle.load(open(model_file, 'rb'),\n                                         encoding='bytes')\n            lasagne.layers.set_all_param_values(\n                    [self.policy_net, self.value_net], net_params\n                    )\n\n    def create_policy_value_net(self):\n        \"\"\"create the policy value network \"\"\"\n        self.state_input = T.tensor4('state')\n        self.winner = T.vector('winner')\n        self.mcts_probs = T.matrix('mcts_probs')\n        network = lasagne.layers.InputLayer(\n                shape=(None, 4, self.board_width, self.board_height),\n                input_var=self.state_input\n                )\n        # conv layers\n        network = lasagne.layers.Conv2DLayer(\n                network, num_filters=32, filter_size=(3, 3), pad='same')\n        network = lasagne.layers.Conv2DLayer(\n                network, num_filters=64, filter_size=(3, 3), pad='same')\n        network = lasagne.layers.Conv2DLayer(\n                network, num_filters=128, filter_size=(3, 3), pad='same')\n        # action policy layers\n        policy_net = lasagne.layers.Conv2DLayer(\n                network, num_filters=4, filter_size=(1, 1))\n        self.policy_net = lasagne.layers.DenseLayer(\n                policy_net, num_units=self.board_width*self.board_height,\n                nonlinearity=lasagne.nonlinearities.softmax)\n        # state value layers\n        value_net = lasagne.layers.Conv2DLayer(\n                network, num_filters=2, filter_size=(1, 1))\n        value_net = lasagne.layers.DenseLayer(value_net, num_units=64)\n        self.value_net = lasagne.layers.DenseLayer(\n                value_net, num_units=1,\n                nonlinearity=lasagne.nonlinearities.tanh)\n        # get action probs and state score value\n        self.action_probs, self.value = lasagne.layers.get_output(\n                [self.policy_net, self.value_net])\n        self.policy_value = theano.function([self.state_input],\n                                            [self.action_probs, self.value],\n                                            allow_input_downcast=True)\n\n    def policy_value_fn(self, board):\n        \"\"\"\n        input: board\n        output: a list of (action, probability) tuples for each available\n            action and the score of the board state\n        \"\"\"\n        legal_positions = board.availables\n        current_state = board.current_state()\n        act_probs, value = self.policy_value(\n            current_state.reshape(-1, 4, self.board_width, self.board_height)\n            )\n        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n        return act_probs, value[0][0]\n\n    def _loss_train_op(self):\n        \"\"\"\n        Three loss terms：\n        loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n        \"\"\"\n        params = lasagne.layers.get_all_params(\n                [self.policy_net, self.value_net], trainable=True)\n        value_loss = lasagne.objectives.squared_error(\n                self.winner, self.value.flatten())\n        policy_loss = lasagne.objectives.categorical_crossentropy(\n                self.action_probs, self.mcts_probs)\n        l2_penalty = lasagne.regularization.apply_penalty(\n                params, lasagne.regularization.l2)\n        self.loss = self.l2_const*l2_penalty + lasagne.objectives.aggregate(\n                value_loss + policy_loss, mode='mean')\n        # policy entropy，for monitoring only\n        self.entropy = -T.mean(T.sum(\n                self.action_probs * T.log(self.action_probs + 1e-10), axis=1))\n        # get the train op\n        updates = lasagne.updates.adam(self.loss, params,\n                                       learning_rate=self.learning_rate)\n        self.train_step = theano.function(\n            [self.state_input, self.mcts_probs, self.winner, self.learning_rate],\n            [self.loss, self.entropy],\n            updates=updates,\n            allow_input_downcast=True\n            )\n\n    def get_policy_param(self):\n        net_params = lasagne.layers.get_all_param_values(\n                [self.policy_net, self.value_net])\n        return net_params\n\n    def save_model(self, model_file):\n        \"\"\" save model params to file \"\"\"\n        net_params = self.get_policy_param()  # get model params\n        pickle.dump(net_params, open(model_file, 'wb'), protocol=2)\n"
        },
        {
          "name": "policy_value_net_keras.py",
          "type": "blob",
          "size": 4.7802734375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nAn implementation of the policyValueNet with Keras\nTested under Keras 2.0.5 with tensorflow-gpu 1.2.1 as backend\n\n@author: Mingxu Zhang\n\"\"\" \n\nfrom __future__ import print_function\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.core import Activation, Dense, Flatten\nfrom keras.layers.merge import Add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nimport keras.backend as K\n\nfrom keras.utils import np_utils\n\nimport numpy as np\nimport pickle\n\n\nclass PolicyValueNet():\n    \"\"\"policy-value network \"\"\"\n    def __init__(self, board_width, board_height, model_file=None):\n        self.board_width = board_width\n        self.board_height = board_height \n        self.l2_const = 1e-4  # coef of l2 penalty \n        self.create_policy_value_net()   \n        self._loss_train_op()\n\n        if model_file:\n            net_params = pickle.load(open(model_file, 'rb'))\n            self.model.set_weights(net_params)\n        \n    def create_policy_value_net(self):\n        \"\"\"create the policy value network \"\"\"   \n        in_x = network = Input((4, self.board_width, self.board_height))\n\n        # conv layers\n        network = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n        network = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n        network = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n        # action policy layers\n        policy_net = Conv2D(filters=4, kernel_size=(1, 1), data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n        policy_net = Flatten()(policy_net)\n        self.policy_net = Dense(self.board_width*self.board_height, activation=\"softmax\", kernel_regularizer=l2(self.l2_const))(policy_net)\n        # state value layers\n        value_net = Conv2D(filters=2, kernel_size=(1, 1), data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n        value_net = Flatten()(value_net)\n        value_net = Dense(64, kernel_regularizer=l2(self.l2_const))(value_net)\n        self.value_net = Dense(1, activation=\"tanh\", kernel_regularizer=l2(self.l2_const))(value_net)\n\n        self.model = Model(in_x, [self.policy_net, self.value_net])\n        \n        def policy_value(state_input):\n            state_input_union = np.array(state_input)\n            results = self.model.predict_on_batch(state_input_union)\n            return results\n        self.policy_value = policy_value\n        \n    def policy_value_fn(self, board):\n        \"\"\"\n        input: board\n        output: a list of (action, probability) tuples for each available action and the score of the board state\n        \"\"\"\n        legal_positions = board.availables\n        current_state = board.current_state()\n        act_probs, value = self.policy_value(current_state.reshape(-1, 4, self.board_width, self.board_height))\n        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n        return act_probs, value[0][0]\n\n    def _loss_train_op(self):\n        \"\"\"\n        Three loss terms：\n        loss = (z - v)^2 + pi^T * log(p) + c||theta||^2\n        \"\"\"\n\n        # get the train op   \n        opt = Adam()\n        losses = ['categorical_crossentropy', 'mean_squared_error']\n        self.model.compile(optimizer=opt, loss=losses)\n\n        def self_entropy(probs):\n            return -np.mean(np.sum(probs * np.log(probs + 1e-10), axis=1))\n\n        def train_step(state_input, mcts_probs, winner, learning_rate):\n            state_input_union = np.array(state_input)\n            mcts_probs_union = np.array(mcts_probs)\n            winner_union = np.array(winner)\n            loss = self.model.evaluate(state_input_union, [mcts_probs_union, winner_union], batch_size=len(state_input), verbose=0)\n            action_probs, _ = self.model.predict_on_batch(state_input_union)\n            entropy = self_entropy(action_probs)\n            K.set_value(self.model.optimizer.lr, learning_rate)\n            self.model.fit(state_input_union, [mcts_probs_union, winner_union], batch_size=len(state_input), verbose=0)\n            return loss[0], entropy\n        \n        self.train_step = train_step\n\n    def get_policy_param(self):\n        net_params = self.model.get_weights()        \n        return net_params\n\n    def save_model(self, model_file):\n        \"\"\" save model params to file \"\"\"\n        net_params = self.get_policy_param()\n        pickle.dump(net_params, open(model_file, 'wb'), protocol=2)\n"
        },
        {
          "name": "policy_value_net_numpy.py",
          "type": "blob",
          "size": 3.94140625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nImplement the policy value network using numpy, so that we can play with the\ntrained AI model without installing any DL framwork\n\n@author: Junxiao Song\n\"\"\"\n\nfrom __future__ import print_function\nimport numpy as np\n\n\n# some utility functions\ndef softmax(x):\n    probs = np.exp(x - np.max(x))\n    probs /= np.sum(probs)\n    return probs\n\n\ndef relu(X):\n    out = np.maximum(X, 0)\n    return out\n\n\ndef conv_forward(X, W, b, stride=1, padding=1):\n    n_filters, d_filter, h_filter, w_filter = W.shape\n    # theano conv2d flips the filters (rotate 180 degree) first\n    # while doing the calculation\n    W = W[:, :, ::-1, ::-1]\n    n_x, d_x, h_x, w_x = X.shape\n    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n    h_out, w_out = int(h_out), int(w_out)\n    X_col = im2col_indices(X, h_filter, w_filter,\n                           padding=padding, stride=stride)\n    W_col = W.reshape(n_filters, -1)\n    out = (np.dot(W_col, X_col).T + b).T\n    out = out.reshape(n_filters, h_out, w_out, n_x)\n    out = out.transpose(3, 0, 1, 2)\n    return out\n\n\ndef fc_forward(X, W, b):\n    out = np.dot(X, W) + b\n    return out\n\n\ndef get_im2col_indices(x_shape, field_height,\n                       field_width, padding=1, stride=1):\n    # First figure out what the size of the output should be\n    N, C, H, W = x_shape\n    assert (H + 2 * padding - field_height) % stride == 0\n    assert (W + 2 * padding - field_width) % stride == 0\n    out_height = int((H + 2 * padding - field_height) / stride + 1)\n    out_width = int((W + 2 * padding - field_width) / stride + 1)\n\n    i0 = np.repeat(np.arange(field_height), field_width)\n    i0 = np.tile(i0, C)\n    i1 = stride * np.repeat(np.arange(out_height), out_width)\n    j0 = np.tile(np.arange(field_width), field_height * C)\n    j1 = stride * np.tile(np.arange(out_width), out_height)\n    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n\n    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n\n    return (k.astype(int), i.astype(int), j.astype(int))\n\n\ndef im2col_indices(x, field_height, field_width, padding=1, stride=1):\n    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n    # Zero-pad the input\n    p = padding\n    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n\n    k, i, j = get_im2col_indices(x.shape, field_height,\n                                 field_width, padding, stride)\n\n    cols = x_padded[:, k, i, j]\n    C = x.shape[1]\n    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n    return cols\n\n\nclass PolicyValueNetNumpy():\n    \"\"\"policy-value network in numpy \"\"\"\n    def __init__(self, board_width, board_height, net_params):\n        self.board_width = board_width\n        self.board_height = board_height\n        self.params = net_params\n\n    def policy_value_fn(self, board):\n        \"\"\"\n        input: board\n        output: a list of (action, probability) tuples for each available\n        action and the score of the board state\n        \"\"\"\n        legal_positions = board.availables\n        current_state = board.current_state()\n\n        X = current_state.reshape(-1, 4, self.board_width, self.board_height)\n        # first 3 conv layers with ReLu nonlinearity\n        for i in [0, 2, 4]:\n            X = relu(conv_forward(X, self.params[i], self.params[i+1]))\n        # policy head\n        X_p = relu(conv_forward(X, self.params[6], self.params[7], padding=0))\n        X_p = fc_forward(X_p.flatten(), self.params[8], self.params[9])\n        act_probs = softmax(X_p)\n        # value head\n        X_v = relu(conv_forward(X, self.params[10],\n                                self.params[11], padding=0))\n        X_v = relu(fc_forward(X_v.flatten(), self.params[12], self.params[13]))\n        value = np.tanh(fc_forward(X_v, self.params[14], self.params[15]))[0]\n        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n        return act_probs, value\n"
        },
        {
          "name": "policy_value_net_pytorch.py",
          "type": "blob",
          "size": 6.1962890625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nAn implementation of the policyValueNet in PyTorch\nTested in PyTorch 0.2.0 and 0.3.0\n\n@author: Junxiao Song\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\n\n\ndef set_learning_rate(optimizer, lr):\n    \"\"\"Sets the learning rate to the given value\"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\nclass Net(nn.Module):\n    \"\"\"policy-value network module\"\"\"\n    def __init__(self, board_width, board_height):\n        super(Net, self).__init__()\n\n        self.board_width = board_width\n        self.board_height = board_height\n        # common layers\n        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        # action policy layers\n        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n        self.act_fc1 = nn.Linear(4*board_width*board_height,\n                                 board_width*board_height)\n        # state value layers\n        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)\n        self.val_fc1 = nn.Linear(2*board_width*board_height, 64)\n        self.val_fc2 = nn.Linear(64, 1)\n\n    def forward(self, state_input):\n        # common layers\n        x = F.relu(self.conv1(state_input))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        # action policy layers\n        x_act = F.relu(self.act_conv1(x))\n        x_act = x_act.view(-1, 4*self.board_width*self.board_height)\n        x_act = F.log_softmax(self.act_fc1(x_act))\n        # state value layers\n        x_val = F.relu(self.val_conv1(x))\n        x_val = x_val.view(-1, 2*self.board_width*self.board_height)\n        x_val = F.relu(self.val_fc1(x_val))\n        x_val = F.tanh(self.val_fc2(x_val))\n        return x_act, x_val\n\n\nclass PolicyValueNet():\n    \"\"\"policy-value network \"\"\"\n    def __init__(self, board_width, board_height,\n                 model_file=None, use_gpu=False):\n        self.use_gpu = use_gpu\n        self.board_width = board_width\n        self.board_height = board_height\n        self.l2_const = 1e-4  # coef of l2 penalty\n        # the policy value net module\n        if self.use_gpu:\n            self.policy_value_net = Net(board_width, board_height).cuda()\n        else:\n            self.policy_value_net = Net(board_width, board_height)\n        self.optimizer = optim.Adam(self.policy_value_net.parameters(),\n                                    weight_decay=self.l2_const)\n\n        if model_file:\n            net_params = torch.load(model_file)\n            self.policy_value_net.load_state_dict(net_params)\n\n    def policy_value(self, state_batch):\n        \"\"\"\n        input: a batch of states\n        output: a batch of action probabilities and state values\n        \"\"\"\n        if self.use_gpu:\n            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n            log_act_probs, value = self.policy_value_net(state_batch)\n            act_probs = np.exp(log_act_probs.data.cpu().numpy())\n            return act_probs, value.data.cpu().numpy()\n        else:\n            state_batch = Variable(torch.FloatTensor(state_batch))\n            log_act_probs, value = self.policy_value_net(state_batch)\n            act_probs = np.exp(log_act_probs.data.numpy())\n            return act_probs, value.data.numpy()\n\n    def policy_value_fn(self, board):\n        \"\"\"\n        input: board\n        output: a list of (action, probability) tuples for each available\n        action and the score of the board state\n        \"\"\"\n        legal_positions = board.availables\n        current_state = np.ascontiguousarray(board.current_state().reshape(\n                -1, 4, self.board_width, self.board_height))\n        if self.use_gpu:\n            log_act_probs, value = self.policy_value_net(\n                    Variable(torch.from_numpy(current_state)).cuda().float())\n            act_probs = np.exp(log_act_probs.data.cpu().numpy().flatten())\n            value = value.data.cpu().numpy()[0][0]\n        else:\n            log_act_probs, value = self.policy_value_net(\n                    Variable(torch.from_numpy(current_state)).float())\n            act_probs = np.exp(log_act_probs.data.numpy().flatten())\n            value = value.data.numpy()[0][0]\n        act_probs = zip(legal_positions, act_probs[legal_positions])\n        return act_probs, value\n\n    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n        \"\"\"perform a training step\"\"\"\n        # wrap in Variable\n        if self.use_gpu:\n            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n            mcts_probs = Variable(torch.FloatTensor(mcts_probs).cuda())\n            winner_batch = Variable(torch.FloatTensor(winner_batch).cuda())\n        else:\n            state_batch = Variable(torch.FloatTensor(state_batch))\n            mcts_probs = Variable(torch.FloatTensor(mcts_probs))\n            winner_batch = Variable(torch.FloatTensor(winner_batch))\n\n        # zero the parameter gradients\n        self.optimizer.zero_grad()\n        # set learning rate\n        set_learning_rate(self.optimizer, lr)\n\n        # forward\n        log_act_probs, value = self.policy_value_net(state_batch)\n        # define the loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n        # Note: the L2 penalty is incorporated in optimizer\n        value_loss = F.mse_loss(value.view(-1), winner_batch)\n        policy_loss = -torch.mean(torch.sum(mcts_probs*log_act_probs, 1))\n        loss = value_loss + policy_loss\n        # backward and optimize\n        loss.backward()\n        self.optimizer.step()\n        # calc policy entropy, for monitoring only\n        entropy = -torch.mean(\n                torch.sum(torch.exp(log_act_probs) * log_act_probs, 1)\n                )\n        return loss.data[0], entropy.data[0]\n        #for pytorch version >= 0.5 please use the following line instead.\n        #return loss.item(), entropy.item()\n\n    def get_policy_param(self):\n        net_params = self.policy_value_net.state_dict()\n        return net_params\n\n    def save_model(self, model_file):\n        \"\"\" save model params to file \"\"\"\n        net_params = self.get_policy_param()  # get model params\n        torch.save(net_params, model_file)\n"
        },
        {
          "name": "policy_value_net_tensorflow.py",
          "type": "blob",
          "size": 6.5234375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nAn implementation of the policyValueNet in Tensorflow\nTested in Tensorflow 1.4 and 1.5\n\n@author: Xiang Zhong\n\"\"\"\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass PolicyValueNet():\n    def __init__(self, board_width, board_height, model_file=None):\n        self.board_width = board_width\n        self.board_height = board_height\n\n        # Define the tensorflow neural network\n        # 1. Input:\n        self.input_states = tf.placeholder(\n                tf.float32, shape=[None, 4, board_height, board_width])\n        self.input_state = tf.transpose(self.input_states, [0, 2, 3, 1])\n        # 2. Common Networks Layers\n        self.conv1 = tf.layers.conv2d(inputs=self.input_state,\n                                      filters=32, kernel_size=[3, 3],\n                                      padding=\"same\", data_format=\"channels_last\",\n                                      activation=tf.nn.relu)\n        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64,\n                                      kernel_size=[3, 3], padding=\"same\",\n                                      data_format=\"channels_last\",\n                                      activation=tf.nn.relu)\n        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=128,\n                                      kernel_size=[3, 3], padding=\"same\",\n                                      data_format=\"channels_last\",\n                                      activation=tf.nn.relu)\n        # 3-1 Action Networks\n        self.action_conv = tf.layers.conv2d(inputs=self.conv3, filters=4,\n                                            kernel_size=[1, 1], padding=\"same\",\n                                            data_format=\"channels_last\",\n                                            activation=tf.nn.relu)\n        # Flatten the tensor\n        self.action_conv_flat = tf.reshape(\n                self.action_conv, [-1, 4 * board_height * board_width])\n        # 3-2 Full connected layer, the output is the log probability of moves\n        # on each slot on the board\n        self.action_fc = tf.layers.dense(inputs=self.action_conv_flat,\n                                         units=board_height * board_width,\n                                         activation=tf.nn.log_softmax)\n        # 4 Evaluation Networks\n        self.evaluation_conv = tf.layers.conv2d(inputs=self.conv3, filters=2,\n                                                kernel_size=[1, 1],\n                                                padding=\"same\",\n                                                data_format=\"channels_last\",\n                                                activation=tf.nn.relu)\n        self.evaluation_conv_flat = tf.reshape(\n                self.evaluation_conv, [-1, 2 * board_height * board_width])\n        self.evaluation_fc1 = tf.layers.dense(inputs=self.evaluation_conv_flat,\n                                              units=64, activation=tf.nn.relu)\n        # output the score of evaluation on current state\n        self.evaluation_fc2 = tf.layers.dense(inputs=self.evaluation_fc1,\n                                              units=1, activation=tf.nn.tanh)\n\n        # Define the Loss function\n        # 1. Label: the array containing if the game wins or not for each state\n        self.labels = tf.placeholder(tf.float32, shape=[None, 1])\n        # 2. Predictions: the array containing the evaluation score of each state\n        # which is self.evaluation_fc2\n        # 3-1. Value Loss function\n        self.value_loss = tf.losses.mean_squared_error(self.labels,\n                                                       self.evaluation_fc2)\n        # 3-2. Policy Loss function\n        self.mcts_probs = tf.placeholder(\n                tf.float32, shape=[None, board_height * board_width])\n        self.policy_loss = tf.negative(tf.reduce_mean(\n                tf.reduce_sum(tf.multiply(self.mcts_probs, self.action_fc), 1)))\n        # 3-3. L2 penalty (regularization)\n        l2_penalty_beta = 1e-4\n        vars = tf.trainable_variables()\n        l2_penalty = l2_penalty_beta * tf.add_n(\n            [tf.nn.l2_loss(v) for v in vars if 'bias' not in v.name.lower()])\n        # 3-4 Add up to be the Loss function\n        self.loss = self.value_loss + self.policy_loss + l2_penalty\n\n        # Define the optimizer we use for training\n        self.learning_rate = tf.placeholder(tf.float32)\n        self.optimizer = tf.train.AdamOptimizer(\n                learning_rate=self.learning_rate).minimize(self.loss)\n\n        # Make a session\n        self.session = tf.Session()\n\n        # calc policy entropy, for monitoring only\n        self.entropy = tf.negative(tf.reduce_mean(\n                tf.reduce_sum(tf.exp(self.action_fc) * self.action_fc, 1)))\n\n        # Initialize variables\n        init = tf.global_variables_initializer()\n        self.session.run(init)\n\n        # For saving and restoring\n        self.saver = tf.train.Saver()\n        if model_file is not None:\n            self.restore_model(model_file)\n\n    def policy_value(self, state_batch):\n        \"\"\"\n        input: a batch of states\n        output: a batch of action probabilities and state values\n        \"\"\"\n        log_act_probs, value = self.session.run(\n                [self.action_fc, self.evaluation_fc2],\n                feed_dict={self.input_states: state_batch}\n                )\n        act_probs = np.exp(log_act_probs)\n        return act_probs, value\n\n    def policy_value_fn(self, board):\n        \"\"\"\n        input: board\n        output: a list of (action, probability) tuples for each available\n        action and the score of the board state\n        \"\"\"\n        legal_positions = board.availables\n        current_state = np.ascontiguousarray(board.current_state().reshape(\n                -1, 4, self.board_width, self.board_height))\n        act_probs, value = self.policy_value(current_state)\n        act_probs = zip(legal_positions, act_probs[0][legal_positions])\n        return act_probs, value\n\n    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n        \"\"\"perform a training step\"\"\"\n        winner_batch = np.reshape(winner_batch, (-1, 1))\n        loss, entropy, _ = self.session.run(\n                [self.loss, self.entropy, self.optimizer],\n                feed_dict={self.input_states: state_batch,\n                           self.mcts_probs: mcts_probs,\n                           self.labels: winner_batch,\n                           self.learning_rate: lr})\n        return loss, entropy\n\n    def save_model(self, model_path):\n        self.saver.save(self.session, model_path)\n\n    def restore_model(self, model_path):\n        self.saver.restore(self.session, model_path)\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 8.583984375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\nAn implementation of the training pipeline of AlphaZero for Gomoku\n\n@author: Junxiao Song\n\"\"\"\n\nfrom __future__ import print_function\nimport random\nimport numpy as np\nfrom collections import defaultdict, deque\nfrom game import Board, Game\nfrom mcts_pure import MCTSPlayer as MCTS_Pure\nfrom mcts_alphaZero import MCTSPlayer\nfrom policy_value_net import PolicyValueNet  # Theano and Lasagne\n# from policy_value_net_pytorch import PolicyValueNet  # Pytorch\n# from policy_value_net_tensorflow import PolicyValueNet # Tensorflow\n# from policy_value_net_keras import PolicyValueNet # Keras\n\n\nclass TrainPipeline():\n    def __init__(self, init_model=None):\n        # params of the board and the game\n        self.board_width = 6\n        self.board_height = 6\n        self.n_in_row = 4\n        self.board = Board(width=self.board_width,\n                           height=self.board_height,\n                           n_in_row=self.n_in_row)\n        self.game = Game(self.board)\n        # training params\n        self.learn_rate = 2e-3\n        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n        self.temp = 1.0  # the temperature param\n        self.n_playout = 400  # num of simulations for each move\n        self.c_puct = 5\n        self.buffer_size = 10000\n        self.batch_size = 512  # mini-batch size for training\n        self.data_buffer = deque(maxlen=self.buffer_size)\n        self.play_batch_size = 1\n        self.epochs = 5  # num of train_steps for each update\n        self.kl_targ = 0.02\n        self.check_freq = 50\n        self.game_batch_num = 1500\n        self.best_win_ratio = 0.0\n        # num of simulations used for the pure mcts, which is used as\n        # the opponent to evaluate the trained policy\n        self.pure_mcts_playout_num = 1000\n        if init_model:\n            # start training from an initial policy-value net\n            self.policy_value_net = PolicyValueNet(self.board_width,\n                                                   self.board_height,\n                                                   model_file=init_model)\n        else:\n            # start training from a new policy-value net\n            self.policy_value_net = PolicyValueNet(self.board_width,\n                                                   self.board_height)\n        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n                                      c_puct=self.c_puct,\n                                      n_playout=self.n_playout,\n                                      is_selfplay=1)\n\n    def get_equi_data(self, play_data):\n        \"\"\"augment the data set by rotation and flipping\n        play_data: [(state, mcts_prob, winner_z), ..., ...]\n        \"\"\"\n        extend_data = []\n        for state, mcts_prob, winner in play_data:\n            for i in [1, 2, 3, 4]:\n                # rotate counterclockwise\n                equi_state = np.array([np.rot90(s, i) for s in state])\n                equi_mcts_prob = np.rot90(np.flipud(\n                    mcts_prob.reshape(self.board_height, self.board_width)), i)\n                extend_data.append((equi_state,\n                                    np.flipud(equi_mcts_prob).flatten(),\n                                    winner))\n                # flip horizontally\n                equi_state = np.array([np.fliplr(s) for s in equi_state])\n                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n                extend_data.append((equi_state,\n                                    np.flipud(equi_mcts_prob).flatten(),\n                                    winner))\n        return extend_data\n\n    def collect_selfplay_data(self, n_games=1):\n        \"\"\"collect self-play data for training\"\"\"\n        for i in range(n_games):\n            winner, play_data = self.game.start_self_play(self.mcts_player,\n                                                          temp=self.temp)\n            play_data = list(play_data)[:]\n            self.episode_len = len(play_data)\n            # augment the data\n            play_data = self.get_equi_data(play_data)\n            self.data_buffer.extend(play_data)\n\n    def policy_update(self):\n        \"\"\"update the policy-value net\"\"\"\n        mini_batch = random.sample(self.data_buffer, self.batch_size)\n        state_batch = [data[0] for data in mini_batch]\n        mcts_probs_batch = [data[1] for data in mini_batch]\n        winner_batch = [data[2] for data in mini_batch]\n        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\n        for i in range(self.epochs):\n            loss, entropy = self.policy_value_net.train_step(\n                    state_batch,\n                    mcts_probs_batch,\n                    winner_batch,\n                    self.learn_rate*self.lr_multiplier)\n            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n            kl = np.mean(np.sum(old_probs * (\n                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n                    axis=1)\n            )\n            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n                break\n        # adaptively adjust the learning rate\n        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n            self.lr_multiplier /= 1.5\n        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n            self.lr_multiplier *= 1.5\n\n        explained_var_old = (1 -\n                             np.var(np.array(winner_batch) - old_v.flatten()) /\n                             np.var(np.array(winner_batch)))\n        explained_var_new = (1 -\n                             np.var(np.array(winner_batch) - new_v.flatten()) /\n                             np.var(np.array(winner_batch)))\n        print((\"kl:{:.5f},\"\n               \"lr_multiplier:{:.3f},\"\n               \"loss:{},\"\n               \"entropy:{},\"\n               \"explained_var_old:{:.3f},\"\n               \"explained_var_new:{:.3f}\"\n               ).format(kl,\n                        self.lr_multiplier,\n                        loss,\n                        entropy,\n                        explained_var_old,\n                        explained_var_new))\n        return loss, entropy\n\n    def policy_evaluate(self, n_games=10):\n        \"\"\"\n        Evaluate the trained policy by playing against the pure MCTS player\n        Note: this is only for monitoring the progress of training\n        \"\"\"\n        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n                                         c_puct=self.c_puct,\n                                         n_playout=self.n_playout)\n        pure_mcts_player = MCTS_Pure(c_puct=5,\n                                     n_playout=self.pure_mcts_playout_num)\n        win_cnt = defaultdict(int)\n        for i in range(n_games):\n            winner = self.game.start_play(current_mcts_player,\n                                          pure_mcts_player,\n                                          start_player=i % 2,\n                                          is_shown=0)\n            win_cnt[winner] += 1\n        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1]) / n_games\n        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n                self.pure_mcts_playout_num,\n                win_cnt[1], win_cnt[2], win_cnt[-1]))\n        return win_ratio\n\n    def run(self):\n        \"\"\"run the training pipeline\"\"\"\n        try:\n            for i in range(self.game_batch_num):\n                self.collect_selfplay_data(self.play_batch_size)\n                print(\"batch i:{}, episode_len:{}\".format(\n                        i+1, self.episode_len))\n                if len(self.data_buffer) > self.batch_size:\n                    loss, entropy = self.policy_update()\n                # check the performance of the current model,\n                # and save the model params\n                if (i+1) % self.check_freq == 0:\n                    print(\"current self-play batch: {}\".format(i+1))\n                    win_ratio = self.policy_evaluate()\n                    self.policy_value_net.save_model('./current_policy.model')\n                    if win_ratio > self.best_win_ratio:\n                        print(\"New best policy!!!!!!!!\")\n                        self.best_win_ratio = win_ratio\n                        # update the best_policy\n                        self.policy_value_net.save_model('./best_policy.model')\n                        if (self.best_win_ratio == 1.0 and\n                                self.pure_mcts_playout_num < 5000):\n                            self.pure_mcts_playout_num += 1000\n                            self.best_win_ratio = 0.0\n        except KeyboardInterrupt:\n            print('\\n\\rquit')\n\n\nif __name__ == '__main__':\n    training_pipeline = TrainPipeline()\n    training_pipeline.run()\n"
        }
      ]
    }
  ]
}