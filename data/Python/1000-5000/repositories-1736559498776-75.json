{
  "metadata": {
    "timestamp": 1736559498776,
    "page": 75,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "microsoft/LLMLingua",
      "stars": 4797,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 6.7333984375,
          "content": "## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n##\n## Get latest from https://github.com/github/gitignore/blob/main/VisualStudio.gitignore\n\n# User-specific files\n*.rsuser\n*.suo\n*.user\n*.userosscache\n*.sln.docstates\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n*.userprefs\n\n# Mono auto generated files\nmono_crash.*\n\n# Build results\n[Dd]ebug/\n[Dd]ebugPublic/\n[Rr]elease/\n[Rr]eleases/\nx64/\nx86/\n[Ww][Ii][Nn]32/\n[Aa][Rr][Mm]/\n[Aa][Rr][Mm]64/\nbld/\n[Bb]in/\n[Oo]bj/\n[Ll]og/\n[Ll]ogs/\n\n# Visual Studio 2015/2017 cache/options directory\n.vs/\n# Uncomment if you have tasks that create the project's static files in wwwroot\n#wwwroot/\n\n# Visual Studio 2017 auto generated files\nGenerated\\ Files/\n\n# MSTest test Results\n[Tt]est[Rr]esult*/\n[Bb]uild[Ll]og.*\n\n# NUnit\n*.VisualState.xml\nTestResult.xml\nnunit-*.xml\n\n# Build Results of an ATL Project\n[Dd]ebugPS/\n[Rr]eleasePS/\ndlldata.c\n\n# Benchmark Results\nBenchmarkDotNet.Artifacts/\n\n# .NET Core\nproject.lock.json\nproject.fragment.lock.json\nartifacts/\n\n# ASP.NET Scaffolding\nScaffoldingReadMe.txt\n\n# StyleCop\nStyleCopReport.xml\n\n# Files built by Visual Studio\n*_i.c\n*_p.c\n*_h.h\n*.ilk\n*.meta\n*.obj\n*.iobj\n*.pch\n*.pdb\n*.ipdb\n*.pgc\n*.pgd\n*.rsp\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp\n*.tmp_proj\n*_wpftmp.csproj\n*.log\n*.tlog\n*.vspscc\n*.vssscc\n.builds\n*.pidb\n*.svclog\n*.scc\n\n# Chutzpah Test files\n_Chutzpah*\n\n# Visual C++ cache files\nipch/\n*.aps\n*.ncb\n*.opendb\n*.opensdf\n*.sdf\n*.cachefile\n*.VC.db\n*.VC.VC.opendb\n\n# Visual Studio profiler\n*.psess\n*.vsp\n*.vspx\n*.sap\n\n# Visual Studio Trace Files\n*.e2e\n\n# TFS 2012 Local Workspace\n$tf/\n\n# Guidance Automation Toolkit\n*.gpState\n\n# ReSharper is a .NET coding add-in\n_ReSharper*/\n*.[Rr]e[Ss]harper\n*.DotSettings.user\n\n# TeamCity is a build add-in\n_TeamCity*\n\n# DotCover is a Code Coverage Tool\n*.dotCover\n\n# AxoCover is a Code Coverage Tool\n.axoCover/*\n!.axoCover/settings.json\n\n# Coverlet is a free, cross platform Code Coverage Tool\ncoverage*.json\ncoverage*.xml\ncoverage*.info\n\n# Visual Studio code coverage results\n*.coverage\n*.coveragexml\n\n# NCrunch\n_NCrunch_*\n.*crunch*.local.xml\nnCrunchTemp_*\n\n# MightyMoose\n*.mm.*\nAutoTest.Net/\n\n# Web workbench (sass)\n.sass-cache/\n\n# Installshield output folder\n[Ee]xpress/\n\n# DocProject is a documentation generator add-in\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\n\n# Click-Once directory\npublish/\n\n# Publish Web Output\n*.[Pp]ublish.xml\n*.azurePubxml\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n*.pubxml\n*.publishproj\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\nPublishScripts/\n\n# NuGet Packages\n*.nupkg\n# NuGet Symbol Packages\n*.snupkg\n# The packages folder can be ignored because of Package Restore\n**/[Pp]ackages/*\n# except build/, which is used as an MSBuild target.\n!**/[Pp]ackages/build/\n# Uncomment if necessary however generally it will be regenerated when needed\n#!**/[Pp]ackages/repositories.config\n# NuGet v3's project.json files produces more ignorable files\n*.nuget.props\n*.nuget.targets\n\n# Microsoft Azure Build Output\ncsx/\n*.build.csdef\n\n# Microsoft Azure Emulator\necf/\nrcf/\n\n# Windows Store app package directories and files\nAppPackages/\nBundleArtifacts/\nPackage.StoreAssociation.xml\n_pkginfo.txt\n*.appx\n*.appxbundle\n*.appxupload\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n*.[Cc]ache\n# but keep track of directories ending in .cache\n!?*.[Cc]ache/\n\n# Others\nClientBin/\n~$*\n*~\n*.dbmdl\n*.dbproj.schemaview\n*.jfm\n*.pfx\n*.publishsettings\norleans.codegen.cs\n\n# Including strong name files can present a security risk\n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n#*.snk\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n#bower_components/\n\n# RIA/Silverlight projects\nGenerated_Code/\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\nUpgradeLog*.htm\nServiceFabricBackup/\n*.rptproj.bak\n\n# SQL Server files\n*.mdf\n*.ldf\n*.ndf\n\n# Business Intelligence projects\n*.rdl.data\n*.bim.layout\n*.bim_*.settings\n*.rptproj.rsuser\n*- [Bb]ackup.rdl\n*- [Bb]ackup ([0-9]).rdl\n*- [Bb]ackup ([0-9][0-9]).rdl\n\n# Microsoft Fakes\nFakesAssemblies/\n\n# GhostDoc plugin setting file\n*.GhostDoc.xml\n\n# Node.js Tools for Visual Studio\n.ntvs_analysis.dat\nnode_modules/\n\n# Visual Studio 6 build log\n*.plg\n\n# Visual Studio 6 workspace options file\n*.opt\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n*.vbw\n\n# Visual Studio 6 auto-generated project file (contains which files were open etc.)\n*.vbp\n\n# Visual Studio 6 workspace and project file (working project files containing files to include in project)\n*.dsw\n*.dsp\n\n# Visual Studio 6 technical files\n*.ncb\n*.aps\n\n# Visual Studio LightSwitch build output\n**/*.HTMLClient/GeneratedArtifacts\n**/*.DesktopClient/GeneratedArtifacts\n**/*.DesktopClient/ModelManifest.xml\n**/*.Server/GeneratedArtifacts\n**/*.Server/ModelManifest.xml\n_Pvt_Extensions\n\n# Paket dependency manager\n.paket/paket.exe\npaket-files/\n\n# FAKE - F# Make\n.fake/\n\n# CodeRush personal settings\n.cr/personal\n\n# Python Tools for Visual Studio (PTVS)\n__pycache__/\n*.pyc\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n*.tss\n\n# Telerik's JustMock configuration file\n*.jmconfig\n\n# BizTalk build output\n*.btp.cs\n*.btm.cs\n*.odx.cs\n*.xsd.cs\n\n# OpenCover UI analysis results\nOpenCover/\n\n# Azure Stream Analytics local run output\nASALocalRun/\n\n# MSBuild Binary and Structured Log\n*.binlog\n\n# NVidia Nsight GPU debugger configuration file\n*.nvuser\n\n# MFractors (Xamarin productivity tool) working folder\n.mfractor/\n\n# Local History for Visual Studio\n.localhistory/\n\n# Visual Studio History (VSHistory) files\n.vshistory/\n\n# BeatPulse healthcheck temp database\nhealthchecksdb\n\n# Backup folder for Package Reference Convert tool in Visual Studio 2017\nMigrationBackup/\n\n# Ionide (cross platform F# VS Code tools) working folder\n.ionide/\n\n# Fody - auto-generated XML schema\nFodyWeavers.xsd\n\n# VS Code files for those working on multiple tools\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n*.code-workspace\n\n# Local History for Visual Studio Code\n.history/\n\n# Windows Installer files from build outputs\n*.cab\n*.msi\n*.msix\n*.msm\n*.msp\n\n# JetBrains Rider\n*.sln.iml\n*.egg-info\n\n# build\nbuild/*\ndist/*\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.84375,
          "content": "default_language_version:\n  python: python3\nexclude: 'dotnet'\nci:\n  autofix_prs: true\n  autoupdate_commit_msg: '[pre-commit.ci] pre-commit suggestions'\n  autoupdate_schedule: 'quarterly'\n\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n    - id: check-added-large-files\n    - id: check-ast\n    - id: check-yaml\n    - id: check-toml\n    - id: check-json\n    - id: check-byte-order-marker\n      exclude: .gitignore\n    - id: check-merge-conflict\n    - id: detect-private-key\n    - id: trailing-whitespace\n    - id: end-of-file-fixer\n    - id: no-commit-to-branch\n  - repo: https://github.com/pycqa/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n    - id: black\n  - repo: https://github.com/nbQA-dev/nbQA\n    rev: 1.7.1\n    hooks:\n      - id: nbqa-black\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.43359375,
          "content": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n"
        },
        {
          "name": "DOCUMENT.md",
          "type": "blob",
          "size": 26.6435546875,
          "content": "# LLMLingua Series Documentation\n\n## Principles\n\n1. **Prompt Sensitivity**: Different components of a prompt, like instructions and questions, vary in sensitivity to compression. Contexts or documents, for example, are less sensitive. It's advisable to separate these components in the prompt for demonstrations, instructions, and questions.\n2. **Granular Division**: For multi-document QA and few-shot learning, divide demonstrations and contexts into independent granularities. This helps with budget control and document reordering.\n3. **Essential Character Preservation**: Preserve essential characters as required by the scenario rules. **Support for this feature is now available in Structured Prompt Compression and LLMLingua-2**.\n4. **Optimization through Experimentation**: Experiment with various target compression ratios and other hyperparameters to optimize performance.\n\n## Basic Usage\n\nWith **LLMLingua**, you can easily compress your prompts. Here’s how you can do it:\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor()\ncompressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n\n# > {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\\nLets think step step\\nSam bought 1 boxes x00 oflters.\\nHe bought 12 * 300ters in total\\nSam then took 5 boxes 6ters0ters.\\nHe sold these boxes for 5 *5\\nAfterelling these  boxes there were 3030 highlighters remaining.\\nThese form 330 / 3 = 110 groups of three pens.\\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\\nIn total, then, he earned $220 + $15 = $235.\\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\\nThe answer is 115',\n#  'origin_tokens': 2365,\n#  'compressed_tokens': 211,\n#  'ratio': '11.2x',\n#  'saving': ', Saving $0.1 in GPT-4.'}\n\n## Or use the phi-2 model,\nllm_lingua = PromptCompressor(\"microsoft/phi-2\")\n\n## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need <8GB GPU memory.\n## Before that, you need to pip install optimum auto-gptq\nllm_lingua = PromptCompressor(\"TheBloke/Llama-2-7b-Chat-GPTQ\", model_config={\"revision\": \"main\"})\n```\n\nTo try **LongLLMLingua** in your scenarios, you can use\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor()\ncompressed_prompt = llm_lingua.compress_prompt(\n    prompt_list,\n    question=question,\n    ratio=0.55,\n    # Set the special parameter for LongLLMLingua\n    condition_in_question=\"after_condition\",\n    reorder_context=\"sort\",\n    dynamic_context_compression_ratio=0.3, # or 0.4\n    condition_compare=True,\n    context_budget=\"+100\",\n    rank_method=\"longllmlingua\",\n)\n```\n\nTo try **LLMLingua-2** in your scenarios, you can use\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n    use_llmlingua2=True,\n)\ncompressed_prompt = llm_lingua.compress_prompt(prompt, rate=0.33, force_tokens = ['\\n', '?'])\n\n## Or use LLMLingua-2-small model\nllm_lingua = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n    use_llmlingua2=True,\n)\n```\n\n## Advanced Usage\n\n### Utilizing Small Models\n\n### Using phi-2\n\nThanks to the efforts of the community, phi-2 is now available for use in LLMLingua.\n\nBefore using it, please update your transformers to the GitHub version by running `pip install -U git+https://github.com/huggingface/transformers.git`.\n\n```python\nllm_lingua = PromptCompressor(\"microsoft/phi-2\")\n```\n\n### Quantized Models\n\n(Long)LLMLingua supports the use of quantized small models such as `TheBloke/Llama-2-7b-Chat-GPTQ`, which require less than 8GB of GPU memory.\n\nTo begin, ensure you install the necessary packages with:\n\n```bash\npip install optimum auto-gptq\n```\n\nThen, initialize your model as follows:\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor(\"TheBloke/Llama-2-7b-Chat-GPTQ\", model_config={\"revision\": \"main\"})\n```\n\n### Structured Prompt Compression\n\nSplit text into sections, decide on whether to compress and its rate. Use `<llmlingua></llmlingua>` tags for context segmentation, with optional `rate` and `compress` parameters.\n\n```python\nstructured_prompt = \"\"\"<llmlingua, compress=False>Speaker 4:</llmlingua><llmlingua, rate=0.4> Thank you. And can we do the functions for content? Items I believe are 11, three, 14, 16 and 28, I believe.</llmlingua><llmlingua, compress=False>\nSpeaker 0:</llmlingua><llmlingua, rate=0.4> Item 11 is a communication from Council on Price recommendation to increase appropriation in the general fund group in the City Manager Department by $200 to provide a contribution to the Friends of the Long Beach Public Library. Item 12 is communication from Councilman Super Now. Recommendation to increase appropriation in the special advertising and promotion fund group and the city manager's department by $10,000 to provide support for the end of summer celebration. Item 13 is a communication from Councilman Austin. Recommendation to increase appropriation in the general fund group in the city manager department by $500 to provide a donation to the Jazz Angels . Item 14 is a communication from Councilman Austin. Recommendation to increase appropriation in the general fund group in the City Manager department by $300 to provide a donation to the Little Lion Foundation. Item 16 is a communication from Councilman Allen recommendation to increase appropriation in the general fund group in the city manager department by $1,020 to provide contribution to Casa Korero, Sew Feria Business Association, Friends of Long Beach Public Library and Dave Van Patten. Item 28 is a communication. Communication from Vice Mayor Richardson and Council Member Muranga. Recommendation to increase appropriation in the general fund group in the City Manager Department by $1,000 to provide a donation to Ron Palmer Summit. Basketball and Academic Camp.</llmlingua><llmlingua, compress=False>\nSpeaker 4:</llmlingua><llmlingua, rate=0.6> We have a promotion and a second time as councilman served Councilman Ringa and customers and they have any comments.</llmlingua>\"\"\"\ncompressed_prompt = llm_lingua.structured_compress_prompt(structured_prompt, instruction=\"\", question=\"\", rate=0.5)\nprint(compressed_prompt['compressed_prompt'])\n\n# > Speaker 4:. And can we do the functions for content? Items I believe are11,,116 28,.\n# Speaker 0: a from Council on Price to increase the fund group the Manager0 provide a the the1 is Councilman Super Now. the special group the provide the summerman a the Jazzels a communication from Councilman Austin. Recommendation to increase appropriation in the general fund group in the City Manager department by $300 to provide a donation to the Little Lion Foundation. Item 16 is a communication from Councilman Allen recommendation to increase appropriation in the general fund group in the city manager department by $1,020 to provide contribution to Casa Korero, Sew Feria Business Association, Friends of Long Beach Public Library and Dave Van Patten. Item 28 is a communication. Communication from Vice Mayor Richardson and Council Member Muranga. Recommendation to increase appropriation in the general fund group in the City Manager Department by $1,000 to provide a donation to Ron Palmer Summit. Basketball and Academic Camp.\n# Speaker 4: We have a promotion and a second time as councilman served Councilman Ringa and customers and they have any comments.\n```\n\n### Compress Json data\n\nYou can specify the compression method for each key and value by passing a config or a yaml config file. Each key must include four parameters: `rate` indicates the compression ratio for the corresponding value, `compress` indicates whether the corresponding value is compressed, `value_type` indicates the data type of the value, and `pair_remove` indicates whether the key-value pair can be completely deleted.\n\n```python\njson_data = {\n    \"id\": 987654,\n    \"name\": \"John Doe\",\n    \"skills\": [\"Java\",\"Python\",\"Machine Learning\",\"Cloud Computing\",\"AI Development\"],\n    \"biography\": \"John Doe, born in New York in 1985, is a renowned software engineer with over 10 years of experience in the field. John graduated from MIT with a degree in Computer Science and has since worked with several Fortune 500 companies. He has a passion for developing innovative software solutions and has contributed to numerous open source projects. John is also an avid writer and speaker at tech conferences, sharing his insights on emerging technologies and their impact on the business world. In his free time, John enjoys hiking, reading science fiction novels, and playing the piano. At TechCorp, John was responsible for leading a team of software engineers and overseeing the development of scalable web applications. He played a key role in driving the adoption of cloud technologies within the company, significantly enhancing the efficiency of their digital operations. In his John on developingedge AI and implementing machine learning solutions for various business applications. He was instrumental in developing a predictive analytics tool that transformed the company's approach to data-driven decision making.\"\n}\njson_config = {\n    \"id\": {\n        \"rate\": 1,\n        \"compress\": False,\n        \"value_type\": \"int\",\n        \"pair_remove\": True\n    },\n    \"name\": {\n        \"rate\": 0.7,\n        \"compress\": False,\n        \"value_type\": \"str\",\n        \"pair_remove\": False\n    },\n    \"skills\": {\n        \"rate\": 0.2,\n        \"compress\": True,\n        \"value_type\": \"list\",\n        \"pair_remove\": True\n    },\n    \"biography\": {\n        \"rate\": 0.3,\n        \"compress\": True,\n        \"value_type\": \"str\",\n        \"pair_remove\": True\n    }\n}\ncompressed_prompt = llm_lingua.compress_json(json_data, json_config, use_keyvalue_level_filter=True)\nprint(compressed_prompt['compressed_prompt'])\n# > {'id': 987654, 'name': 'John Doe', 'skills': ['', '', '', '', 'AI'], 'biography': \",York in a has several for developing has avid and speaker at,on and enjoys reading fiction playing. At Tech John for and of scalable He in the of cloud technologies,significantly enhancing the efficiency of their digital operations. In his John on developingedge AI and implementing machine learning solutions for various business applications. He was instrumental in developing a predictive analytics tool that transformed the company's approach to data-driven decision making.\"}\n```\n\n### Integration with LangChain\n\nThanks to the contributions of Ayo Ayibiowu (@thehapyone), (Long)LLMLingua can be seamlessly integrated into LangChain. Here's an example of how to initialize (Long)LLMLingua within LangChain:\n\n```python\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_community.retrievers.document_compressors import LLMLinguaCompressor\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0)\n\ncompressor = LLMLinguaCompressor(model_name=\"openai-community/gpt2\", device_map=\"cpu\")\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor, base_retriever=retriever\n)\n\ncompressed_docs = compression_retriever.get_relevant_documents(\n    \"What did the president say about Ketanji Jackson Brown\"\n)\npretty_print_docs(compressed_docs)\n```\n\nFor a more detailed guide, please refer to [Notebook](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/retrievers/llmlingua.ipynb).\n\n### Integration with LlamaIndex\n\nThanks to the contributions of Jerry Liu (@jerryjliu), (Long)LLMLingua can be seamlessly integrated into LlamaIndex. Here's an example of how to initialize (Long)LLMLingua within LlamaIndex:\n\n```python\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.response_synthesizers import CompactAndRefine\nfrom llama_index.indices.postprocessor import LongLLMLinguaPostprocessor\n\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=\"Given the context, please answer the final question\",\n    target_token=300,\n    rank_method=\"longllmlingua\",\n    additional_compress_kwargs={\n        \"condition_compare\": True,\n        \"condition_in_question\": \"after\",\n        \"context_budget\": \"+100\",\n        \"reorder_context\": \"sort\",  # Enables document reordering\n        \"dynamic_context_compression_ratio\": 0.4, # Enables dynamic compression ratio\n    },\n)\n```\n\nFor a more detailed guide, please refer to [RAGLlamaIndex Example](https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb).\n\n### Training Your Own LLMLingua-2\n\nNot performing well on some domain-specific tasks? Don't worry, we've released code to help you build your own training data by instructing GPT-4 to compress your custom corpus and train compressors on the distilled data.\n\n#### Data collection\n\nFirst, format your data to a list of dict, with each dict containing at least two keys: _idx_ and _prompt_. [**format_data.py**](./experiments/llmlingua2/data_collection/format_data.py) illustrates how we format the meetingbank data.\n\nThen, instruct GPT-4 to compress the original context.\n\n```bash\ncd experiments/llmlingua2/data_collection/\npython compress.py --load_origin_from <your data path> \\\n--chunk_size 512 \\\n--compressor llmcomp \\\n--model_name gpt-4-32k \\\n--save_path <compressed data save path>\n\n```\n\nThen, assign label to the original words and filter out poor compression samples.\n\n```bash\ncd experiments/llmlingua2/data_collection/\npython label_word.py \\\n--load_prompt_from <compressed data save path> \\\n--window_size 400 \\\n--save_path <labeled data save path> \\\n\n```\n\nFilter out some poorly compressed / labeled samples.\n\n```bash\ncd experiments/llmlingua2/data_collection/\npython filter.py --load_path <labeled data save path> \\\n--save_path <kept data save path>\n```\n\n#### Compressor Training\n\nThe [**model_training**](./experiments/llmlingua2/model_training) folder contains the code to train compressor on the distilled data.\n\n```bash\ncd cd experiments/llmlingua2/model_training/\npython train_roberta.py --data_path <kept data save path>\n```\n\n## Detailed of Pramater\n\n### Initialization\n\nInitialize **LLMLingua**, **LongLLMLingua**, and **LLMLingua-2** with the following parameters:\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor(\n    model_name=\"NousResearch/Llama-2-7b-hf\", # Default model, use \"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\" or \"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\" for LLMLingua-2\n    device_map=\"cuda\",  # Device environment (e.g., 'cuda', 'cpu', 'mps')\n    model_config={},  # Configuration for the Huggingface model\n    open_api_config={},  # Configuration for OpenAI Embedding\n    use_llmlingua2=False, # Whether to use llmlingua-2\n)\n```\n\n#### Parameters\n\n- **model_name** (str): Name of the small language model from Huggingface, use \"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\" or \"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\" for LLMLingua-2. Defaults to \"NousResearch/Llama-2-7b-hf\".\n- **device_map** (str): The computing environment. Options include 'cuda', 'cpu', 'mps', 'balanced', 'balanced_low_0', 'auto'. Default is 'cuda'.\n- **model_config** (dict, optional): Configuration for the Huggingface model. Defaults to {}.\n- **open_api_config** (dict, optional): Configuration for OpenAI Embedding in coarse-level prompt compression. Defaults to {}.\n- **use_llmlingua2** (bool, optional): Whether to use llmlingua-2 for prompt compression. Defaults is False.\n\n### Function Call\n\nUtilize (Long)LLMLingua for prompt compression with a range of customizable parameters:\n\n```python\ncompressed_prompt = llm_lingua.compress_prompt(\n    context: List[str],  # Context or documents (low compression sensitivity)\n    instruction: str = \"\",  # Instruction (high compression sensitivity)\n    question: str = \"\",  # Question (high compression sensitivity)\n    ratio: float = 0.5,  # Target compression ratio (default: 0.5)\n    target_token: float = -1,  # Target compression token count (default: -1)\n    # Additional parameters to fine-tune the compression\n    iterative_size: int = 200,  # Segment size for iterative token-level compression\n    force_context_ids: List[int] = None,  # Forced retention of specific context indices\n    force_context_number: int = None,  # Forced retention of a specific number of contexts\n    use_sentence_level_filter: bool = False,  # Enables sentence-level compression\n    use_context_level_filter: bool = True,  # Enables context-level compression\n    use_token_level_filter: bool = True,  # Enables token-level compression\n    keep_split: bool = False,  # Retains newline separators in the prompt\n    keep_first_sentence: int = 0,  # Retains the first 'k' sentences in each context\n    keep_last_sentence: int = 0,  # Retains the last 'k' sentences in each context\n    keep_sentence_number: int = 0,  # Retains a specific number of sentences in each context\n    high_priority_bonus: int = 100,  # Priority bonus for ranking sentences\n    context_budget: str = \"+100\",  # Budget for context-level compression\n    token_budget_ratio: float = 1.4,  # Budget ratio for sentence-level compression\n    condition_in_question: str = \"none\",  # Enables question-aware compression\n    reorder_context: str = \"original\",  # Method for reordering context before compression\n    dynamic_context_compression_ratio: float = 0.0,  # Ratio for dynamic context compression\n    condition_compare: bool = False,  # Enables iterative token-level question-aware fine-grained compression\n    add_instruction: bool = False,  # Adds instruction before the prompt\n    rank_method: str = \"longllmlingua\",  # Method for ranking in coarse-level compression\n    concate_question: bool = True,  # Includes the question in the compressed prompt\n    # Parameters for LLMLingua-2\n    target_context: int = -1,  # Context Budget for Coarse-level Prompt Compression\n    context_level_rate: float = 1.0, # Compression rate for Coarse-level Prompt Compression\n    context_level_target_token: int = -1, # Token Budget for Coarse-level Prompt Compression\n    return_word_label: bool = False, # Whether to return words with corresponding labels. Default is False.\n    word_sep: str = '\\t\\t|\\t\\t', # The sep token used in fn_labeled_original_prompt to partition words.\n    label_sep: str = \" \", # The sep token used in fn_labeled_original_prompt to partition word and label.\n    token_to_word: str = 'mean', # How to convert token probability to word probability. Default is 'mean'.\n    force_tokens: List[str] = [], # List of specific tokens to always include in the compressed result. Default is [].\n    force_reserve_digit: bool = False, # Whether to forcibly reserve tokens that containing digit (0,...,9). Default is False.\n    drop_consecutive: bool = False, # Whether to drop tokens which are in 'force_tokens' but appears consecutively in compressed prompt. Default is False\n    chunk_end_tokens: List[str] = [\".\", \"\\n\"] # The early stop tokens for segmenting chunk. Default is [\".\", \"\\n\"].\n)\n```\n\n### Parameters\n\n- **context** (str or List[str]): Contexts, documents, or demonstrations in the prompt, exhibiting low sensitivity to compression.\n- **instruction** (str): General instruction within the prompt, displaying high sensitivity to compression.\n- **question** (str): General question within the prompt, also highly sensitive to compression.\n- **ratio** (float, optional): The target compression ratio, where larger values result in fewer retained tokens. The default is set to 0.5.\n- **target_token** (float): The target number of tokens post-compression. This parameter is mutually exclusive with **ratio**. Default is -1.\n- **iterative_size** (int): Segment size for Iterative Token-level Prompt Compression. The default is set to 200.\n- **force_context_ids** (List[int], optional): Indexes of context elements to be forcefully retained. Default is None.\n- **force_context_number** (int, optional): The number of contexts to be forcefully retained in Coarse-level Prompt Compression. Default is None.\n- **use_sentence_level_filter** (bool, optional): Enables sentence-level prompt compression. Default is False.\n- **use_context_level_filter** (bool, optional): Enables context-level prompt compression. Default is True.\n- **use_token_level_filter** (bool, optional): Enables token-level prompt compression. Default is True.\n- **keep_split** (bool, optional): Determines whether to retain all newline separators (\"\\n\\n\") in the prompt. Default is False.\n- **keep_first_sentence** (int, optional): Specifies whether to retain the first 'k' sentences in each context. Default is 0.\n- **keep_last_sentence** (int, optional): Specifies whether to retain the last 'k' sentences in each context. Default is 0.\n- **keep_sentence_number** (int, optional): Specifies the number of sentences to retain in each context. Default is 0.\n- **high_priority_bonus** (int, optional): Assigns a priority bonus to sentences retained by the **keep_first_sentence** or **keep_last_sentence** settings. Default is 100.\n- **context_budget** (str, optional): Budget for Coarse-level Prompt Compression, with supported operators like \"\\*1.5\" or \"+100\". Default is \"+100\".\n- **token_budget_ratio** (float, optional): Budget ratio for sentence-level Prompt Compression. Default is 1.4.\n- **condition_in_question** (str, optional): Determines the use of question-aware coarse-level prompt compression. Options include \"none\", \"after\", \"before\". Default is \"none\".\n- **reorder_context** (str, optional): Method for document reordering before compression in LongLLMLingua. Options include \"original\", \"sort\", \"two_stage\". Default is \"original\".\n- **dynamic_context_compression_ratio** (float, optional): Ratio for dynamic context compression in LongLLMLingua. Default is 0.0.\n- **condition_compare** (bool, optional): Enables Iterative Token-level Question-aware Fine-Grained Compression in LongLLMLingua. Default is False.\n- **add_instruction** (bool, optional): Determines whether to add an instruction before the prompt in Iterative Token-level Question-aware Fine-Grained Compression. Default is False.\n- **rank_method** (str, optional): Selects the ranking method for Coarse-level Prompt Compression, with support for various embedding and reranker methods, as well as LLMLingua and LongLLMLingua. Default is \"llmlingua\".\n  - \"llmlingua\": Employs the coarse-grained prompt compression technique of **LLMLingua**.\n  - \"longllmlingua\": Utilizes the question-aware coarse-grained prompt compression method in **LongLLMLingua** (recommended).\n  - Traditional Retrieval Methods:\n    - \"bm25\": A bag-of-words retrieval function that ranks documents based on the occurrence of query terms, irrespective of their proximity within the documents.\n    - \"gzip\": A retrieval method based on GZIP compression. For further information, see [GZIP Retrieval Method](https://aclanthology.org/2023.findings-acl.426).\n  - Embedding-Based Retrieval Methods:\n    - \"sentbert\": An embedding-based retrieval method. Learn more at [SentenceBERT](https://www.sbert.net).\n    - \"openai\": Utilizes \"text-embedding-ada-002\" as the embedding model from OpenAI.\n    - \"bge\": An embedding-based retrieval method using \"BAAI/bge-large-en-v1.5\". For additional information, visit [BGE-Large-EN-V1.5](https://huggingface.co/BAAI/bge-large-en-v1.5).\n    - \"voyageai\": An embedding-based retrieval method provided by VoyageAI. More details at [VoyageAI](https://www.voyageai.com).\n    - \"jinza\": An embedding-based retrieval method using \"jinaai/jina-embeddings-v2-base-en\". Further details are available at [JinaAI Embeddings](https://huggingface.co/jinaai/jina-embeddings-v2-base-en).\n  - Reranker Methods:\n    - \"bge_reranker\": A reranker-based method using \"BAAI/bge-reranker-large\". More information can be found at [BGE Reranker Large](https://huggingface.co/BAAI/bge-reranker-large).\n    - \"bge_llmembedder\": A reranker-based method using \"BAAI/llm-embedder\". For more details, refer to [BAAI LLM Embedder](https://huggingface.co/BAAI/llm-embedder).\n    - \"cohere\": A reranker-based method using \"rerank-english-v2.0\" from Cohere. Learn more at [Cohere Rerank](https://cohere.com/rerank).\n- **concate_question** (bool, optional): Determines whether to include the question in the compressed prompt. Default is True.\n- **target_context** (int): The maximum number of contexts to be achieved in context level compression. Default is -1 (no compression on context level).\n- **context_level_rate** (float): The compression rate target to be achieved in context level. Default is 1.0 (no compression on context level).\n- **context_level_target_token** (int): The maximum number of tokens to be achieved in context level compression. Default is -1 (no compression on context level).\n- **return_word_label** (bool): Whether to return words with corresponding labels. Default is False.\n- **word_sep** (str): The sep token used in fn_labeled_original_prompt to partition words. Only used when return_word_label==True. Default is '\\t\\t|\\t\\t'\n- **label_sep** (str): The sep token used in fn_labeled_original_prompt to partition word and label. Only used when return_word_label==True. Default is ' '\n- **token_to_word** (str): The method to convert token probability to word probability. Default is 'mean'\n- **force_tokens** (List[str], optional): List of specific tokens to always include in the compressed result. Default is [].\n- **force_reserve_digit** (bool, optional): Whether to forcibly reserve tokens that containing digit (0,...,9). Default is False.\n- **drop_consecutive** (bool, optinal): Whether to drop tokens which are in 'force_tokens' but appears consecutively in compressed prompt. Default is False.\n- **chunk_end_tokens** (List[str], optinal): The early stop tokens for segmenting chunk. Default is [\".\", \"\\n\"].\n\n### Response\n\n- **compressed_prompt** (str): The compressed prompt.\n- **origin_tokens** (int): Number of tokens in the original prompt.\n- **compressed_tokens** (int): Number of tokens in the compressed prompt.\n- **ratio** (str): Actual compression ratio.\n- **saving** (str): Savings in GPT-4 cost.\n\nAdditional Response Parameter for LLMLingua-2.\n\n- **fn_labeled_original_prompt** (str): original words along with their labels indicating whether to reserve in compressed prompt, in the format (word1 label_sep label2 word_sep word2 label_sep label2 ...). Only return when return_word_label==True.\n- **compressed_prompt_list** (str): List of the compressed prompt.\n\n### Post-Processing\n\nRecover the original response from a compressed prompt:\n\n```python\nrecovered_response = llm_lingua.recover(\n    original_prompt: str,\n    compressed_prompt: str,\n    response: str,\n)\n```\n\n#### Parameters\n\n- **original_prompt** (str): The original prompt.\n- **compressed_prompt** (str): The compressed prompt.\n- **response** (str): The response from black-box LLMs based on the compressed prompt.\n\n#### Response\n\n- **recovered_response** (str): The recovered response, integrating the original prompt's context.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1142578125,
          "content": "    MIT License\n\n    Copyright (c) Microsoft Corporation.\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.2958984375,
          "content": ".PHONY: install style test\n\nPYTHON := python\nCHECK_DIRS := llmlingua tests\n\ninstall:\n\t@${PYTHON} setup.py bdist_wheel\n\t@${PYTHON} -m pip install dist/sdtools*\n\nstyle:\n\tblack $(CHECK_DIRS)\n\tisort -rc $(CHECK_DIRS)\n\tflake8 $(CHECK_DIRS)\n\ntest:\n\t@${PYTHON} -m pytest -n auto --dist=loadfile -s -v ./tests/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.515625,
          "content": "<div style=\"display: flex; align-items: center;\">\n    <div style=\"width: 100px; margin-right: 10px; height:auto;\" align=\"left\">\n        <img src=\"images/LLMLingua_logo.png\" alt=\"LLMLingua\" width=\"100\" align=\"left\">\n    </div>\n    <div style=\"flex-grow: 1;\" align=\"center\">\n        <h2 align=\"center\">LLMLingua Series | Effectively Deliver Information to LLMs via Prompt Compression</h2>\n    </div>\n</div>\n\n<p align=\"center\">\n    | <a href=\"https://llmlingua.com/\"><b>Project Page</b></a> |\n    <a href=\"https://aclanthology.org/2023.emnlp-main.825/\"><b>LLMLingua</b></a> |\n    <a href=\"https://aclanthology.org/2024.acl-long.91/\"><b>LongLLMLingua</b></a> |\n    <a href=\"https://aclanthology.org/2024.findings-acl.57/\"><b>LLMLingua-2</b></a> |\n    <a href=\"https://huggingface.co/spaces/microsoft/LLMLingua\"><b>LLMLingua Demo</b></a> |\n    <a href=\"https://huggingface.co/spaces/microsoft/LLMLingua-2\"><b>LLMLingua-2 Demo</b></a> |\n</p>\n\nhttps://github.com/microsoft/LLMLingua/assets/30883354/eb0ea70d-6d4c-4aa7-8977-61f94bb87438\n\n## News\n- 🍩 [24/12/13] We are excited to announce the release of our KV cache-centric analysis work, [SCBench](https://aka.ms/SCBench), which evaluates long-context methods from a KV cache perspective.\n- 👘 [24/09/16] We are pleased to announce the release of our KV cache offloading work, [RetrievalAttention](https://aka.ms/RetrievalAttention), which accelerates long-context LLM inference via vector retrieval.\n- 🌀  [24/07/03] We're excited to announce the release of [MInference](https://aka.ms/MInference) to speed up Long-context LLMs' inference, reduces inference latency by up to **10X** for pre-filling on an A100 while maintaining accuracy in **1M tokens prompt**! For more information, check out our [paper](https://arxiv.org/abs/2407.02490), visit the [project page](https://aka.ms/MInference).\n- 🧩 LLMLingua has been integrated into [Prompt flow](https://microsoft.github.io/promptflow/integrations/tools/llmlingua-prompt-compression-tool.html), a streamlined tool framework for LLM-based AI applications.\n- 🦚 We're excited to announce the release of **LLMLingua-2**, boasting a 3x-6x speed improvement over LLMLingua! For more information, check out our [paper](https://aclanthology.org/2024.findings-acl.57/), visit the [project page](https://llmlingua.com/llmlingua2.html), and explore our [demo](https://huggingface.co/spaces/microsoft/LLMLingua-2).\n- 👾 LLMLingua has been integrated into [LangChain](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/retrievers/llmlingua.ipynb) and [LlamaIndex](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/LongLLMLingua.ipynb), two widely-used RAG frameworks.\n- 🤳 Talk slides are available in [AI Time Jan, 24](https://drive.google.com/file/d/1fzK3wOvy2boF7XzaYuq2bQ3jFeP1WMk3/view?usp=sharing).\n- 🖥 EMNLP'23 slides are available in [Session 5](https://drive.google.com/file/d/1GxQLAEN8bBB2yiEdQdW4UKoJzZc0es9t/view) and [BoF-6](https://drive.google.com/file/d/1LJBUfJrKxbpdkwo13SgPOqugk-UjLVIF/view).\n- 📚 Check out our new [blog post](https://medium.com/@iofu728/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7) discussing RAG benefits and cost savings through prompt compression. See the script example [here](https://github.com/microsoft/LLMLingua/blob/main/examples/Retrieval.ipynb).\n- 🎈 Visit our [project page](https://llmlingua.com/) for real-world case studies in RAG, Online Meetings, CoT, and Code.\n- 👨‍🦯 Explore our ['./examples'](./examples) directory for practical applications, including [LLMLingua-2](./examples/LLMLingua2.ipynb), [RAG](./examples/RAG.ipynb), [Online Meeting](./examples/OnlineMeeting.ipynb), [CoT](./examples/CoT.ipynb), [Code](./examples/Code.ipynb), and [RAG using LlamaIndex](./examples/RAGLlamaIndex.ipynb).\n\n## TL;DR\n\nLLMLingua utilizes a compact, well-trained language model (e.g., GPT2-small, LLaMA-7B) to identify and remove non-essential tokens in prompts. This approach enables efficient inference with large language models (LLMs), achieving up to 20x compression with minimal performance loss.\n\n- [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://aclanthology.org/2023.emnlp-main.825/) (EMNLP 2023)<br>\n  _Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang and Lili Qiu_\n\nLongLLMLingua mitigates the 'lost in the middle' issue in LLMs, enhancing long-context information processing. It reduces costs and boosts efficiency with prompt compression, improving RAG performance by up to 21.4% using only 1/4 of the tokens.\n\n- [LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression](https://aclanthology.org/2024.acl-long.91/) (ACL 2024 and ICLR ME-FoMo 2024)<br>\n  _Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang and Lili Qiu_\n\nLLMLingua-2, a small-size yet powerful prompt compression method trained via data distillation from GPT-4 for token classification with a BERT-level encoder, excels in task-agnostic compression. It surpasses LLMLingua in handling out-of-domain data, offering 3x-6x faster performance.\n\n- [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](https://aclanthology.org/2024.findings-acl.57/) (ACL 2024 Findings)<br>\n  _Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang_\n\n## 🎥 Overview\n\n![Background](./images/LLMLingua_motivation.png)\n\n- Ever encountered the token limit when asking ChatGPT to summarize lengthy texts?\n- Frustrated with ChatGPT forgetting previous instructions after extensive fine-tuning?\n- Experienced high costs using GPT3.5/4 API for experiments despite excellent results?\n\nWhile Large Language Models like ChatGPT and GPT-4 excel in generalization and reasoning, they often face challenges like prompt length limits and prompt-based pricing schemes.\n\n![Motivation for LLMLingua](./images/motivation.png)\n\nNow you can use **LLMLingua**, **LongLLMLingua**, and **LLMLingua-2**!\n\nThese tools offer an efficient solution to compress prompts by up to **20x**, enhancing the utility of LLMs.\n\n- 💰 **Cost Savings**: Reduces both prompt and generation lengths with minimal overhead.\n- 📝 **Extended Context Support**: Enhances support for longer contexts, mitigates the \"lost in the middle\" issue, and boosts overall performance.\n- ⚖️ **Robustness**: No additional training needed for LLMs.\n- 🕵️ **Knowledge Retention**: Maintains original prompt information like ICL and reasoning.\n- 📜 **KV-Cache Compression**: Accelerates inference process.\n- 🪃 **Comprehensive Recovery**: GPT-4 can recover all key information from compressed prompts.\n\n![Framework of LLMLingua](./images/LLMLingua.png)\n\n![Framework of LongLLMLingua](./images/LongLLMLingua.png)\n\n![Framework of LLMLingua-2](./images/LLMLingua-2.png)\n\nPS: This demo is based on the [alt-gpt](https://github.com/feedox/alt-gpt) project. Special thanks to @Livshitz for their valuable contribution.\n\nIf you find this repo helpful, please cite the following papers:\n\n```bibtex\n@inproceedings{jiang-etal-2023-llmlingua,\n    title = \"{LLML}ingua: Compressing Prompts for Accelerated Inference of Large Language Models\",\n    author = \"Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.emnlp-main.825\",\n    doi = \"10.18653/v1/2023.emnlp-main.825\",\n    pages = \"13358--13376\",\n}\n```\n\n```bibtex\n@inproceedings{jiang-etal-2024-longllmlingua,\n    title = \"{L}ong{LLML}ingua: Accelerating and Enhancing {LLM}s in Long Context Scenarios via Prompt Compression\",\n    author = \"Huiqiang Jiang and Qianhui Wu and and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.91\",\n    pages = \"1658--1677\",\n}\n```\n\n```bibtex\n@inproceedings{pan-etal-2024-llmlingua,\n    title = \"{LLML}ingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression\",\n    author = \"Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor Ruhle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand and virtual meeting\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.57\",\n    pages = \"963--981\",\n}\n```\n\n## 🎯 Quick Start\n\n#### 1. **Installing LLMLingua:**\n\nTo get started with LLMLingua, simply install it using pip:\n\n```bash\npip install llmlingua\n```\n\n#### 2. **Using LLMLingua Series Methods for Prompt Compression:**\n\nWith **LLMLingua**, you can easily compress your prompts. Here’s how you can do it:\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor()\ncompressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n\n# > {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\\nLets think step step\\nSam bought 1 boxes x00 oflters.\\nHe bought 12 * 300ters in total\\nSam then took 5 boxes 6ters0ters.\\nHe sold these boxes for 5 *5\\nAfterelling these  boxes there were 3030 highlighters remaining.\\nThese form 330 / 3 = 110 groups of three pens.\\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\\nIn total, then, he earned $220 + $15 = $235.\\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\\nThe answer is 115',\n#  'origin_tokens': 2365,\n#  'compressed_tokens': 211,\n#  'ratio': '11.2x',\n#  'saving': ', Saving $0.1 in GPT-4.'}\n\n## Or use the phi-2 model,\nllm_lingua = PromptCompressor(\"microsoft/phi-2\")\n\n## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need <8GB GPU memory.\n## Before that, you need to pip install optimum auto-gptq\nllm_lingua = PromptCompressor(\"TheBloke/Llama-2-7b-Chat-GPTQ\", model_config={\"revision\": \"main\"})\n```\n\nTo try **LongLLMLingua** in your scenarios, you can use\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor()\ncompressed_prompt = llm_lingua.compress_prompt(\n    prompt_list,\n    question=question,\n    rate=0.55,\n    # Set the special parameter for LongLLMLingua\n    condition_in_question=\"after_condition\",\n    reorder_context=\"sort\",\n    dynamic_context_compression_ratio=0.3, # or 0.4\n    condition_compare=True,\n    context_budget=\"+100\",\n    rank_method=\"longllmlingua\",\n)\n```\n\nTo try **LLMLingua-2** in your scenarios, you can use\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n    use_llmlingua2=True, # Whether to use llmlingua-2\n)\ncompressed_prompt = llm_lingua.compress_prompt(prompt, rate=0.33, force_tokens = ['\\n', '?'])\n\n## Or use LLMLingua-2-small model\nllm_lingua = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n    use_llmlingua2=True, # Whether to use llmlingua-2\n)\n```\n\n#### 3. **Advanced usage - Structured Prompt Compression:**\n\nSplit text into sections, decide on whether to compress and its rate. Use `<llmlingua></llmlingua>` tags for context segmentation, with optional rate and compress parameters.\n\n```python\nstructured_prompt = \"\"\"<llmlingua, compress=False>Speaker 4:</llmlingua><llmlingua, rate=0.4> Thank you. And can we do the functions for content? Items I believe are 11, three, 14, 16 and 28, I believe.</llmlingua><llmlingua, compress=False>\nSpeaker 0:</llmlingua><llmlingua, rate=0.4> Item 11 is a communication from Council on Price recommendation to increase appropriation in the general fund group in the City Manager Department by $200 to provide a contribution to the Friends of the Long Beach Public Library. Item 12 is communication from Councilman Super Now. Recommendation to increase appropriation in the special advertising and promotion fund group and the city manager's department by $10,000 to provide support for the end of summer celebration. Item 13 is a communication from Councilman Austin. Recommendation to increase appropriation in the general fund group in the city manager department by $500 to provide a donation to the Jazz Angels . Item 14 is a communication from Councilman Austin. Recommendation to increase appropriation in the general fund group in the City Manager department by $300 to provide a donation to the Little Lion Foundation. Item 16 is a communication from Councilman Allen recommendation to increase appropriation in the general fund group in the city manager department by $1,020 to provide contribution to Casa Korero, Sew Feria Business Association, Friends of Long Beach Public Library and Dave Van Patten. Item 28 is a communication. Communication from Vice Mayor Richardson and Council Member Muranga. Recommendation to increase appropriation in the general fund group in the City Manager Department by $1,000 to provide a donation to Ron Palmer Summit. Basketball and Academic Camp.</llmlingua><llmlingua, compress=False>\nSpeaker 4:</llmlingua><llmlingua, rate=0.6> We have a promotion and a second time as councilman served Councilman Ringa and customers and they have any comments.</llmlingua>\"\"\"\ncompressed_prompt = llm_lingua.structured_compress_prompt(structured_prompt, instruction=\"\", question=\"\", rate=0.5)\nprint(compressed_prompt['compressed_prompt'])\n\n# > Speaker 4:. And can we do the functions for content? Items I believe are11,,116 28,.\n# Speaker 0: a from Council on Price to increase the fund group the Manager0 provide a the the1 is Councilman Super Now. the special group the provide the summerman a the Jazzels a communication from Councilman Austin. Recommendation to increase appropriation in the general fund group in the City Manager department by $300 to provide a donation to the Little Lion Foundation. Item 16 is a communication from Councilman Allen recommendation to increase appropriation in the general fund group in the city manager department by $1,020 to provide contribution to Casa Korero, Sew Feria Business Association, Friends of Long Beach Public Library and Dave Van Patten. Item 28 is a communication. Communication from Vice Mayor Richardson and Council Member Muranga. Recommendation to increase appropriation in the general fund group in the City Manager Department by $1,000 to provide a donation to Ron Palmer Summit. Basketball and Academic Camp.\n# Speaker 4: We have a promotion and a second time as councilman served Councilman Ringa and customers and they have any comments.\n```\n\n#### 4. **Learning More:**\n\nTo understand how to apply LLMLingua and LongLLMLingua in real-world scenarios like RAG, Online Meetings, CoT, and Code, please refer to our [**examples**](./examples). For detailed guidance, the [**documentation**](./DOCUMENT.md) provides extensive recommendations on effectively utilizing LLMLingua.\n\n#### 5. **Data collection and model training of LLMLingua-2:**\n\nTo train the compressor on your custom data, please refer to our [**data_collection**](./experiments/llmlingua2/data_collection) and [**model_training**](./experiments/llmlingua2/model_training).\n\n## Frequently Asked Questions\n\nFor more insights and answers, visit our [FAQ section](./Transparency_FAQ.md).\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.69140625,
          "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.8 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/opensource/security/definition), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/opensource/security/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/opensource/security/pgpkey).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://aka.ms/opensource/security/msrc).\n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/opensource/security/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/opensource/security/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->\n"
        },
        {
          "name": "SUPPORT.md",
          "type": "blob",
          "size": 0.46875,
          "content": "# Support\r\n\r\n## How to file issues and get help\r\n\r\nThis project uses GitHub Issues to track bugs and feature requests. Please search the existing\r\nissues before filing new issues to avoid duplicates.  For new issues, file your bug or\r\nfeature request as a new Issue.\r\n\r\nFor help and questions about using this project, please refer the [document](./DOCUMENT.md).\r\n\r\n## Microsoft Support Policy\r\n\r\nSupport for this **PROJECT or PRODUCT** is limited to the resources listed above.\r\n"
        },
        {
          "name": "Transparency_FAQ.md",
          "type": "blob",
          "size": 10.896484375,
          "content": "# LLMLingua's Responsible AI FAQ\n\n## What is LLMLingua?\n\n- LLMLingua is a simple and efficient method to compress prompt up to 20x and keeping the original prompt knowledge like ICL, reasoning, etc.\n- LLMLingua takes user-defined prompts and compression goals as input, and outputs a compressed prompt, which may often result in a form of expression that is difficult for humans to understand.\n\n## What can LLMLingua do?\n\n- LLMLingua can simultaneously reduce the length of prompts and the output of LLMs (20%-30%), thus saving API calls;\n- Compressed prompts from LLMLingua can be directly used with black-box LLMs, such as ChatGPT, GPT-4, and Claude;\n- By compressing prompts, LLMLingua allows for more information to be included within the original token length, thereby improving model performance;\n- LLMLingua relies on a small language model, like GPT-2 or LLaMA-7b, for perplexity calculations, which is a relatively low-cost approach;\n- Compressed prompts generated by LLMLingua can be understood by LLMs, preserving their original capabilities in downstream tasks and keeping the original prompt knowledge like ICL, reasoning, etc. LLMs can also recover the essential information from the compressed prompts;\n- LLMLingua is a robustness method, no need any training for the LLMs;\n- Additionally, LLMLingua can be used to compress KV-Cache, which speeds up inference.\n\n## What is/are LLMLingua’s intended use(s)?\n\n- Users who call black-box LLM APIs similar to GPT-4, those who utilize ChatGPT to handle longer content, as well as model deployers and cloud service providers, can benefit from these techniques.\n\n## How was LLMLingua evaluated? What metrics are used to measure performance?\n\n- In our experiments, we conducted a detailed evaluation of the performance of compressed prompts across various tasks, particularly in those involving LLM-specific capabilities, such as In-Context Learning, reasoning tasks, summarization, and conversation tasks. We assessed our approach using compression ratio and performance loss as evaluation metrics.\n\n## What are the limitations of LLMLingua? How can users minimize the impact of LLMLingua’s limitations when using the system?\n\n- The potential harmful, false or biased responses using the compressed prompts would likely be unchanged. Thus using LLMLingua has no inherent benefits or risks when it comes to those types of responsible AI issues.\n- LLMLingua may struggle to perform well at particularly high compression ratios, especially when the original prompts are already quite short.\n\n## What operational factors and settings allow for effective and responsible use of LLMLingua?\n\n- Users can set parameters such as the boundaries between different components (instruction, context, question) in the prompt, compression goals, and the small model used for compression calculations. Afterward, they can input the compressed prompt into black-box LLMs for use.\n\n## What is instruction, context, and question?\n\nIn our approach, we divide the prompts into three distinct modules: instruction, context, and question. Each prompt necessarily contains a question, but the presence of context and instruction is not always guaranteed.\n\n- Question: This refers to the directives given by the user to the LLMs, such as inquiries, questions, or requests. Positioned after the instruction and context modules, the question module has a high sensitivity to compression.\n- Context: This module provides the supplementary context needed to address the question, such as documents, demonstrations, web search results, or API call results. Located between the instruction and question modules, its sensitivity to compression is relatively low.\n- Instruction: This module consists of directives given by the user to the LLMs, such as task descriptions. Placed before the instruction and context modules, the instruction module exhibits a high sensitivity to compression.\n\n## Is there a need or benefit to finetune a small model specifically for this purpose?\n\nRefer the [discussion](https://github.com/microsoft/LLMLingua/discussions/57).\n\n**TL;DR**: Fine-tuning is beneficial, but the improvement is not very significant.\n\nOur current understanding is that any Language Model can be used to estimate the importance distribution of tokens. And we believe that the higher the compression rate of the LM itself (followed \"LM is a compressor\"), the more accurate the estimation will be. This is particularly true in terms of the model's exposure to more tokens during the pre-training process.\n\nTherefore, we consider that any LM can potentially serve as a compressor for prompt compression, with different LMs sharing the same essential token distribution. In our previous experiments, we found that alignment might have some impact, but it is minimal – about 1-2 points. Perhaps a more refined alignment method could significantly enhance performance.\n\n## How to choose the compressor model (small language model)?\n\nRefer the [discussion](https://github.com/microsoft/LLMLingua/discussions/57), [issue](https://github.com/microsoft/LLMLingua/issues/83).\n\nOur current understanding is that any Language Model can be used to estimate the importance distribution of tokens. And we believe that the higher the compression rate of the LM itself (followed \"LM is a compressor\"), the more accurate the estimation will be. This is particularly true in terms of the model's exposure to more tokens during the pre-training process.\n\nTherefore, we consider that any LM can potentially serve as a compressor for prompt compression, with different LMs sharing the same essential token distribution. In our previous experiments, we found that alignment might have some impact, but it is minimal – about 1-2 points. Perhaps a more refined alignment method could significantly enhance performance.\n\n## How to use LLMLingua in web-deploy model?\n\nRefer the [issue1](https://github.com/microsoft/LLMLingua/issues/44), [issue2](https://github.com/microsoft/LLMLingua/issues/65), and [issue3](https://github.com/microsoft/LLMLingua/issues/70).\n\nWe require an API that can return the logprobs of the input prompt. Currently, we have found that OpenAI and [FastChat](https://github.com/lm-sys/FastChat/pull/2612) offer this feature. We plan to support it soon.\n\n```python\nlogp = openai.Completion.create(\n    model=\"davinci-002\",\n    prompt=\"Please return the logprobs\",\n    logprobs=0,\n    max_tokens=0,\n    echo=True,\n    temperature=0,\n)\nOut[3]:\n<OpenAIObject text_completion id=-at > JSON: {\n  \"id\": \"\",\n  \"object\": \"text_completion\",\n  \"created\": 1707295146,\n  \"model\": \"davinci-002\",\n  \"choices\": [\n    {\n      \"text\": \"Please return the logprobs\",\n      \"index\": 0,\n      \"logprobs\": {\n        \"tokens\": [\n          \"Please\",\n          \" return\",\n          \" the\",\n          \" log\",\n          \"pro\",\n          \"bs\"\n        ],\n        \"token_logprobs\": [\n          null,\n          -6.9668007,\n          -2.047512,\n          -8.885729,\n          -13.960022,\n          -5.479665\n        ],\n        \"top_logprobs\": null,\n        \"text_offset\": [\n          0,\n          6,\n          13,\n          17,\n          21,\n          24\n        ]\n      },\n      \"finish_reason\": \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 6,\n    \"total_tokens\": 6\n  }\n}\n```\n\n## How to reproduce the result in LLMLingua Series work?\n\nWe release the parameter in the [issue1](https://github.com/microsoft/LLMLingua/issues/76), [issue2](https://github.com/microsoft/LLMLingua/issues/86).\n\n**LLMLingua**:\n\n```python\nprompt  = compressor.compress_prompt(\n    context=xxx,\n    instruction=xxx,\n    question=xxx,\n    ratio=0.75,\n    iterative_size=100,\n    context_budget=\"*2\",\n)\n```\n\n**LongLLMLingua**:\n\n```python\ncompressed_prompt = llm_lingua.compress_prompt(\n    demonstration.split(\"\\n\"),\n    instruction,\n    question,\n    0.55,\n    use_sentence_level_filter=False,\n    condition_in_question=\"after_condition\",\n    reorder_context=\"sort\",\n    dynamic_context_compression_ratio=0.3, # or 0.4\n    condition_compare=True,\n    context_budget=\"+100\",\n    rank_method=\"longllmlingua\",\n)\n```\n\nExperiments in LLMLingua and most experiments in LongLLMLingua were conducted in completion mode, whereas chat mode tends to be more sensitive to token-level compression. However, OpenAI has currently disabled GPT-3.5-turbo's completion; you can use GPT-3.5-turbo-instruction or Azure OpenAI service instead.\n\n**LLMLingua-2**:\n\n```python\nfrom llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n    use_llmlingua2=True, # Whether to use llmlingua-2\n)\ncompressed_prompt = llm_lingua.compress_prompt(prompt, rate=0.33, force_tokens = ['\\n', '?'])\n\n## Or use LLMLingua-2-small model\nllm_lingua = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n    use_llmlingua2=True, # Whether to use llmlingua-2\n)\n```\n\nAnd you can find the details of the LLMLingua-2 experiments at [experiments/llmlingua2](./examples/llmlingua2).\n\n## How to use LLMLingua in LangChain and LlamaIndex?\n\n### Integration with LangChain\n\nThanks to the contributions of Ayo Ayibiowu (@thehapyone), (Long)LLMLingua can be seamlessly integrated into LangChain. Here's an example of how to initialize (Long)LLMLingua within LangChain:\n\n```python\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_community.retrievers.document_compressors import LLMLinguaCompressor\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0)\n\ncompressor = LLMLinguaCompressor(model_name=\"openai-community/gpt2\", device_map=\"cpu\")\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor, base_retriever=retriever\n)\n\ncompressed_docs = compression_retriever.get_relevant_documents(\n    \"What did the president say about Ketanji Jackson Brown\"\n)\npretty_print_docs(compressed_docs)\n```\n\nFor a more detailed guide, please refer to [Notebook](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/retrievers/llmlingua.ipynb).\n\n### Integration with LlamaIndex\n\nThanks to the contributions of Jerry Liu (@jerryjliu), (Long)LLMLingua can be seamlessly integrated into LlamaIndex. Here's an example of how to initialize (Long)LLMLingua within LlamaIndex:\n\n```python\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.response_synthesizers import CompactAndRefine\nfrom llama_index.indices.postprocessor import LongLLMLinguaPostprocessor\n\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=\"Given the context, please answer the final question\",\n    target_token=300,\n    rank_method=\"longllmlingua\",\n    additional_compress_kwargs={\n        \"condition_compare\": True,\n        \"condition_in_question\": \"after\",\n        \"context_budget\": \"+100\",\n        \"reorder_context\": \"sort\",  # Enables document reordering\n        \"dynamic_context_compression_ratio\": 0.4, # Enables dynamic compression ratio\n    },\n)\n```\n\nFor a more detailed guide, please refer to [RAGLlamaIndex Example](https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb).\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "experiments",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "llmlingua",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.1904296875,
          "content": "[tool.black]\nline-length = 88\ntarget-version = ['py38']\ninclude = '\\.pyi?$'\n\n[tool.isort]\natomic = true\nprofile = \"black\"\nline_length = 88\nskip_gitignore = true\nknown_first_party = [\"llmlingua\"]\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.470703125,
          "content": "[isort]\ndefault_section = FIRSTPARTY\nensure_newline_before_comments = True\nforce_grid_wrap = 0\ninclude_trailing_comma = True\nknown_first_party = sdtools\nknown_third_party =\n    imblearn\n    numpy\n    pandas\n    pytorch-tabnet\n    scipy\n    sklearn\n    torch\n    torchaudio\n    torchvision\n    torch_xla\n    tqdm\n    xgboost\n\nline_length = 119\nlines_after_imports = 2\nmulti_line_output = 3\nuse_parentheses = True\n\n[flake8]\nignore = E203, E501, E741, W503, W605\nmax-line-length = 119\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.078125,
          "content": "# Copyright (c) 2023 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n\nfrom setuptools import find_packages, setup\n\n# PEP0440 compatible formatted version, see:\n# https://www.python.org/dev/peps/pep-0440/\n#\n# release markers:\n#   X.Y\n#   X.Y.Z   # For bugfix releases\n#\n# pre-release markers:\n#   X.YaN   # Alpha release\n#   X.YbN   # Beta release\n#   X.YrcN  # Release Candidate\n#   X.Y     # Final release\n\n# version.py defines the VERSION and VERSION_SHORT variables.\n# We use exec here so we don't import allennlp whilst setting up.\nVERSION = {}  # type: ignore\nwith open(\"llmlingua/version.py\", \"r\") as version_file:\n    exec(version_file.read(), VERSION)\n\nINSTALL_REQUIRES = [\n    \"transformers>=4.26.0\",\n    \"accelerate\",\n    \"torch\",\n    \"tiktoken\",\n    \"nltk\",\n    \"numpy\",\n]\nQUANLITY_REQUIRES = [\n    \"black==21.4b0\",\n    \"flake8>=3.8.3\",\n    \"isort>=5.5.4\",\n    \"pre-commit\",\n    \"pytest\",\n    \"pytest-xdist\",\n]\nDEV_REQUIRES = INSTALL_REQUIRES + QUANLITY_REQUIRES\n\nsetup(\n    name=\"llmlingua\",\n    version=VERSION[\"VERSION\"],\n    author=\"The LLMLingua team\",\n    author_email=\"hjiang@microsoft.com\",\n    description=\"To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.\",\n    long_description=open(\"README.md\", encoding=\"utf8\").read(),\n    long_description_content_type=\"text/markdown\",\n    keywords=\"Prompt Compression, LLMs, Inference Acceleration, Black-box LLMs, Efficient LLMs\",\n    license=\"MIT License\",\n    url=\"https://github.com/microsoft/LLMLingua\",\n    classifiers=[\n        \"Intended Audience :: Science/Research\",\n        \"Development Status :: 3 - Alpha\",\n        \"Programming Language :: Python :: 3\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    package_dir={\"\": \".\"},\n    packages=find_packages(\".\"),\n    extras_require={\n        \"dev\": DEV_REQUIRES,\n        \"quality\": QUANLITY_REQUIRES,\n    },\n    install_requires=INSTALL_REQUIRES,\n    include_package_data=True,\n    python_requires=\">=3.8.0\",\n    zip_safe=False,\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}