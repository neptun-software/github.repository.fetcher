{
  "metadata": {
    "timestamp": 1736559897113,
    "page": 651,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lllyasviel/Paints-UNDO",
      "stars": 3631,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.1123046875,
          "content": "hf_token.txt\nhf_download/\nresults/\n*.csv\n*.onnx\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control\n.pdm.toml\n.pdm-python\n.pdm-build/\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.228515625,
          "content": "# Paints-Undo\n\nPaintsUndo: A Base Model of Drawing Behaviors in Digital Paintings\n\nPaints-Undo is a project aimed at providing base models of human drawing behaviors with a hope that future AI models can better align with the real needs of human artists.\n\nThe name \"Paints-Undo\" is inspired by the similarity that, the model's outputs look like pressing the \"undo\" button (usually Ctrl+Z) many times in digital painting software.\n\nPaints-Undo presents a family of models that take an image as input and then output the drawing sequence of that image. The model displays all kinds of human behaviors, including but not limited to sketching, inking, coloring, shading, transforming, left-right flipping, color curve tuning, changing the visibility of layers, and even changing the overall idea during the drawing process.\n\n*This page does not contain any examples. All examples are in the below Git page:*\n\n[>>> Click Here to See the Example Page <<<](https://lllyasviel.github.io/pages/paints_undo/)\n\n**This GitHub repo is the only official page of PaintsUndo. We do not have any other websites.**\n\n**Do note that many fake websites of PaintsUndo are on Google and social media recently.**\n\n# Get Started\n\nYou can deploy PaintsUndo locally via:\n\n    git clone https://github.com/lllyasviel/Paints-UNDO.git\n    cd Paints-UNDO\n    conda create -n paints_undo python=3.10\n    conda activate paints_undo\n    pip install xformers\n    pip install -r requirements.txt\n    python gradio_app.py\n\n(If you do not know how to use these commands, you can paste those commands to ChatGPT and ask ChatGPT to explain and give more detailed instructions.)\n\nThe inference is tested with 24GB VRAM on Nvidia 4090 and 3090TI. It may also work with 16GB VRAM, but does not work with 8GB. My estimation is that, under extreme optimization (including weight offloading and sliced attention), the theoretical minimal VRAM requirement is about 10~12.5 GB.\n\nYou can expect to process one image in about 5 to 10 minutes, depending on your settings. As a typical result, you will get a video of 25 seconds at FPS 4, with resolution 320x512, or 512x320, or 384x448, or 448x384.\n\nBecause the processing time, in most cases, is significantly longer than most tasks/quota in HuggingFace Space, I personally do not highly recommend to deploy this to HuggingFace Space, to avoid placing an unnecessary burden on the HF servers.\n\nIf you do not have required computation devices and still wants an online solution, one option is to wait us to release a Colab notebook (but I am not sure if Colab free tier will work). \n\n# Model Notes\n\nWe currently release two models `paints_undo_single_frame` and `paints_undo_multi_frame`. Let's call them single-frame model and multi-frame model.\n\nThe single-frame model takes one image and an `operation step` as input, and outputs one single image. Assuming that an artwork can always be created with 1000 human operations (for example, one brush stroke is one operation), and the `operation step` is an int number from 0 to 999. The number 0 is the finished final artwork, and the number 999 is the first brush stroke drawn on the pure white canvas. You can understand this model as an \"undo\" (or called Ctrl+Z) model. You input the final image, and indicate how many times you want to \"Ctrl+Z\", and the model will give you a \"simulated\" screenshot after those \"Ctrl+Z\"s are pressed. If your `operation step` is 100, then it means you want to simulate \"Ctrl+Z\" 100 times on this image to get the appearance after the 100-th \"Ctrl+Z\".\n\nThe multi-frame model takes two images as inputs and output 16 intermediate frames between the two input images. The result is much more consistent than the single-frame model, but also much slower, less \"creative\", and limited in 16 frames.\n\nIn this repo, the default method is to use them together. We will first infer the single-frame model about 5-7 times to get 5-7 \"keyframes\", and then we use the multi-frame model to \"interpolate\" those keyframes to actually generate a relatively long video.\n\nIn theory this system can be used in many ways and even give infinitely long video, but in practice results are good when the final frame count is about 100-500.\n\n### Model Architecture (paints_undo_single_frame)\n\nThe model is a modified architecture of SD1.5 trained on different betas scheduler, clip skip, and the aforementioned `operation step` condition. To be specific, the model is trained with the betas of:\n\n`betas = torch.linspace(0.00085, 0.020, 1000, dtype=torch.float64)`\n\nFor comparison, the original SD1.5 is trained with the betas of:\n\n`betas = torch.linspace(0.00085 ** 0.5, 0.012 ** 0.5, 1000, dtype=torch.float64) ** 2`\n\nYou can notice the difference in the ending betas and the removed square. The choice of this scheduler is based on our internal user study.\n\nThe last layer of the text encoder CLIP ViT-L/14 is permanently removed. It is now mathematically consistent to always set CLIP Skip to 2 (if you use diffusers).\n\nThe `operation step` condition is added to layer embeddings in a way similar to SDXL's extra embeddings.\n\nAlso, since the solo purpose of this model is to process existing images, the model is strictly aligned with WD14 tagger without any other augmentations. You should always use WD14 tagger (the one in this repo) to process the input image to get the prompt. Otherwise, the results may be defective. Human-written prompts are not tested.\n\n### Model Architecture (paints_undo_multi_frame)\n\nThis model is trained by resuming from [VideoCrafter](https://github.com/AILab-CVC/VideoCrafter) family, but the original Crafter's `lvdm` is not used and all training/inference codes are completely implemented from scratch. (BTW, now the codes are based on modern Diffusers.) Although the initial weights are resumed from VideoCrafter, the topology of neural network is modified a lot, and the network behavior is now largely different from original Crafter after extensive training. \n\nThe overall architecture is like Crafter with 5 components, 3D-UNet, VAE, CLIP, CLIP-Vision, Image Projection.\n\n**VAE**: The VAE is the exactly same anime VAE extracted from [ToonCrafter](https://github.com/ToonCrafter/ToonCrafter). Thanks ToonCrafter a lot for providing the excellent anime temporal VAE for Crafters.\n\n**3D-UNet**: The 3D-UNet is modified from Crafters's `lvdm` with revisions to attention modules. Other than some minor changes in codes, the major change is that now the UNet are trained and supports temporal windows in Spatial Self Attention layers. You can change the codes in `diffusers_vdm.attention.CrossAttention.temporal_window_for_spatial_self_attention` and `temporal_window_type` to activate three types of attention windows:\n\n1. \"prv\" mode: Each frame's Spatial Self-Attention also attend to full spatial contexts of its previous frame. The first frame only attend itself.\n2. \"first\": Each frame's Spatial Self-Attention also attend to full spatial contexts of the first frame of the entire sequence. The first frame only attend its self.\n3. \"roll\": Each frame's Spatial Self-Attention also attend to full spatial contexts of its previous and next frames, based on the ordering of `torch.roll`.\n\nNote that this is by default disabled in inference to save GPU memory.\n\n**CLIP**: The CLIP of SD2.1.\n\n**CLIP-Vision**: Our implementation of Clip Vision (ViT/H) that supports arbitrary aspect ratios by interpolating the positional embedding. After experimenting with linear interpolation, nearest neighbor, and Rotary Positional Encoding (RoPE), our final choice is nearest neighbor. Note that this is different from Crafter methods that resize or center-crop images to 224x224.\n\n**Image Projection**: Our implementation of a tiny transformer that takes two frames as inputs and outputs 16 image embeddings for each frame. Note that this is different from Crafter methods that only use one image.\n\n# Tutorial\n\nAfter you get into the Gradio interface:\n\nStep 0: Upload an image or just click an Example image on the bottom of the page.\n\nStep 1: In the UI titled \"step 1\", click generate prompts to get the global prompt.\n\nStep 2: In the UI titled \"step 2\", click \"Generate Key Frames\". You can change seeds or other parameters on the left.\n\nStep 3: In the UI titled \"step 3\", click \"Generate Video\". You can change seeds or other parameters on the left.\n\n# Cite\n\n    @Misc{paintsundo,\n      author = {Paints-Undo Team},\n      title  = {Paints-Undo GitHub Page},\n      year   = {2024},\n    }\n\n# Applications\n\nTypical use cases of PaintsUndo:\n\n1. Use PaintsUndo as a base model to analyze human behavior to build AI tools that align with human behavior and human demands, for seamless collaboration between AI and humans in a perfectly controlled workflow.\n\n2. Combine PaintsUndo with sketch-guided image generators to achieve “PaintsRedo”, so as to move forward or backward arbitrarily in any of your finished/unfinished artworks to enhance human creativity power. &ast;\n\n3. Use PaintsUndo to view different possible procedures of your own artworks for artistic inspirations.\n\n4. Use the outputs of PaintsUndo as a kind of video/movie After Effects to achieve specific creative purposes.\n\nand much more ...\n\n&ast; *this is already possible - if you use PaintsUndo to Undo 500 steps, and want to Redo 100 steps with different possibilities, you can use ControlNet to finish it (so that it becomes step 0) and then undo 400 steps. More integrated solution is still under experiments.*\n\n# Related Works\n\nAlso read ...\n\n[Stylized Neural Painting](https://jiupinjia.github.io/neuralpainter/)\n\n[Learning to Paint With Model-based Deep Reinforcement Learning](https://github.com/hzwer/ICCV2019-LearningToPaint)\n\n[Paint Transformer: Feed Forward Neural Painting with Stroke Prediction](https://github.com/Huage001/PaintTransformer)\n\n[ProcessPainter: Learn Painting Process from Sequence Data](https://github.com/nicolaus-huang/ProcessPainter)\n\n[Decomposing Time-Lapse Paintings into Layers](https://cragl.cs.gmu.edu/timemap/)\n\n# Disclaimer\n\nThis project aims to develop base models of human drawing behaviors, facilitating future AI systems to better meet the real needs of human artists. Users are granted the freedom to create content using this tool, but they are expected to comply with local laws and use it responsibly. Users must not employ the tool to generate false information or incite confrontation. The developers do not assume any responsibility for potential misuse by users.\n"
        },
        {
          "name": "diffusers_helper",
          "type": "tree",
          "content": null
        },
        {
          "name": "diffusers_vdm",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradio_app.py",
          "type": "blob",
          "size": 13.2587890625,
          "content": "import os\n\nos.environ['HF_HOME'] = os.path.join(os.path.dirname(__file__), 'hf_download')\nresult_dir = os.path.join('./', 'results')\nos.makedirs(result_dir, exist_ok=True)\n\n\nimport functools\nimport os\nimport random\nimport gradio as gr\nimport numpy as np\nimport torch\nimport wd14tagger\nimport memory_management\nimport uuid\n\nfrom PIL import Image\nfrom diffusers_helper.code_cond import unet_add_coded_conds\nfrom diffusers_helper.cat_cond import unet_add_concat_conds\nfrom diffusers_helper.k_diffusion import KDiffusionSampler\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers.models.attention_processor import AttnProcessor2_0\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers_vdm.pipeline import LatentVideoDiffusionPipeline\nfrom diffusers_vdm.utils import resize_and_center_crop, save_bcthw_as_mp4\n\n\nclass ModifiedUNet(UNet2DConditionModel):\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        m = super().from_config(*args, **kwargs)\n        unet_add_concat_conds(unet=m, new_channels=4)\n        unet_add_coded_conds(unet=m, added_number_count=1)\n        return m\n\n\nmodel_name = 'lllyasviel/paints_undo_single_frame'\ntokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\ntext_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\").to(torch.float16)\nvae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\").to(torch.bfloat16)  # bfloat16 vae\nunet = ModifiedUNet.from_pretrained(model_name, subfolder=\"unet\").to(torch.float16)\n\nunet.set_attn_processor(AttnProcessor2_0())\nvae.set_attn_processor(AttnProcessor2_0())\n\nvideo_pipe = LatentVideoDiffusionPipeline.from_pretrained(\n    'lllyasviel/paints_undo_multi_frame',\n    fp16=True\n)\n\nmemory_management.unload_all_models([\n    video_pipe.unet, video_pipe.vae, video_pipe.text_encoder, video_pipe.image_projection, video_pipe.image_encoder,\n    unet, vae, text_encoder\n])\n\nk_sampler = KDiffusionSampler(\n    unet=unet,\n    timesteps=1000,\n    linear_start=0.00085,\n    linear_end=0.020,\n    linear=True\n)\n\n\ndef find_best_bucket(h, w, options):\n    min_metric = float('inf')\n    best_bucket = None\n    for (bucket_h, bucket_w) in options:\n        metric = abs(h * bucket_w - w * bucket_h)\n        if metric <= min_metric:\n            min_metric = metric\n            best_bucket = (bucket_h, bucket_w)\n    return best_bucket\n\n\n@torch.inference_mode()\ndef encode_cropped_prompt_77tokens(txt: str):\n    memory_management.load_models_to_gpu(text_encoder)\n    cond_ids = tokenizer(txt,\n                         padding=\"max_length\",\n                         max_length=tokenizer.model_max_length,\n                         truncation=True,\n                         return_tensors=\"pt\").input_ids.to(device=text_encoder.device)\n    text_cond = text_encoder(cond_ids, attention_mask=None).last_hidden_state\n    return text_cond\n\n\n@torch.inference_mode()\ndef pytorch2numpy(imgs):\n    results = []\n    for x in imgs:\n        y = x.movedim(0, -1)\n        y = y * 127.5 + 127.5\n        y = y.detach().float().cpu().numpy().clip(0, 255).astype(np.uint8)\n        results.append(y)\n    return results\n\n\n@torch.inference_mode()\ndef numpy2pytorch(imgs):\n    h = torch.from_numpy(np.stack(imgs, axis=0)).float() / 127.5 - 1.0\n    h = h.movedim(-1, 1)\n    return h\n\n\ndef resize_without_crop(image, target_width, target_height):\n    pil_image = Image.fromarray(image)\n    resized_image = pil_image.resize((target_width, target_height), Image.LANCZOS)\n    return np.array(resized_image)\n\n\n@torch.inference_mode()\ndef interrogator_process(x):\n    return wd14tagger.default_interrogator(x)\n\n\n@torch.inference_mode()\ndef process(input_fg, prompt, input_undo_steps, image_width, image_height, seed, steps, n_prompt, cfg,\n            progress=gr.Progress()):\n    rng = torch.Generator(device=memory_management.gpu).manual_seed(int(seed))\n\n    memory_management.load_models_to_gpu(vae)\n    fg = resize_and_center_crop(input_fg, image_width, image_height)\n    concat_conds = numpy2pytorch([fg]).to(device=vae.device, dtype=vae.dtype)\n    concat_conds = vae.encode(concat_conds).latent_dist.mode() * vae.config.scaling_factor\n\n    memory_management.load_models_to_gpu(text_encoder)\n    conds = encode_cropped_prompt_77tokens(prompt)\n    unconds = encode_cropped_prompt_77tokens(n_prompt)\n\n    memory_management.load_models_to_gpu(unet)\n    fs = torch.tensor(input_undo_steps).to(device=unet.device, dtype=torch.long)\n    initial_latents = torch.zeros_like(concat_conds)\n    concat_conds = concat_conds.to(device=unet.device, dtype=unet.dtype)\n    latents = k_sampler(\n        initial_latent=initial_latents,\n        strength=1.0,\n        num_inference_steps=steps,\n        guidance_scale=cfg,\n        batch_size=len(input_undo_steps),\n        generator=rng,\n        prompt_embeds=conds,\n        negative_prompt_embeds=unconds,\n        cross_attention_kwargs={'concat_conds': concat_conds, 'coded_conds': fs},\n        same_noise_in_batch=True,\n        progress_tqdm=functools.partial(progress.tqdm, desc='Generating Key Frames')\n    ).to(vae.dtype) / vae.config.scaling_factor\n\n    memory_management.load_models_to_gpu(vae)\n    pixels = vae.decode(latents).sample\n    pixels = pytorch2numpy(pixels)\n    pixels = [fg] + pixels + [np.zeros_like(fg) + 255]\n\n    return pixels\n\n\n@torch.inference_mode()\ndef process_video_inner(image_1, image_2, prompt, seed=123, steps=25, cfg_scale=7.5, fs=3, progress_tqdm=None):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    frames = 16\n\n    target_height, target_width = find_best_bucket(\n        image_1.shape[0], image_1.shape[1],\n        options=[(320, 512), (384, 448), (448, 384), (512, 320)]\n    )\n\n    image_1 = resize_and_center_crop(image_1, target_width=target_width, target_height=target_height)\n    image_2 = resize_and_center_crop(image_2, target_width=target_width, target_height=target_height)\n    input_frames = numpy2pytorch([image_1, image_2])\n    input_frames = input_frames.unsqueeze(0).movedim(1, 2)\n\n    memory_management.load_models_to_gpu(video_pipe.text_encoder)\n    positive_text_cond = video_pipe.encode_cropped_prompt_77tokens(prompt)\n    negative_text_cond = video_pipe.encode_cropped_prompt_77tokens(\"\")\n\n    memory_management.load_models_to_gpu([video_pipe.image_projection, video_pipe.image_encoder])\n    input_frames = input_frames.to(device=video_pipe.image_encoder.device, dtype=video_pipe.image_encoder.dtype)\n    positive_image_cond = video_pipe.encode_clip_vision(input_frames)\n    positive_image_cond = video_pipe.image_projection(positive_image_cond)\n    negative_image_cond = video_pipe.encode_clip_vision(torch.zeros_like(input_frames))\n    negative_image_cond = video_pipe.image_projection(negative_image_cond)\n\n    memory_management.load_models_to_gpu([video_pipe.vae])\n    input_frames = input_frames.to(device=video_pipe.vae.device, dtype=video_pipe.vae.dtype)\n    input_frame_latents, vae_hidden_states = video_pipe.encode_latents(input_frames, return_hidden_states=True)\n    first_frame = input_frame_latents[:, :, 0]\n    last_frame = input_frame_latents[:, :, 1]\n    concat_cond = torch.stack([first_frame] + [torch.zeros_like(first_frame)] * (frames - 2) + [last_frame], dim=2)\n\n    memory_management.load_models_to_gpu([video_pipe.unet])\n    latents = video_pipe(\n        batch_size=1,\n        steps=int(steps),\n        guidance_scale=cfg_scale,\n        positive_text_cond=positive_text_cond,\n        negative_text_cond=negative_text_cond,\n        positive_image_cond=positive_image_cond,\n        negative_image_cond=negative_image_cond,\n        concat_cond=concat_cond,\n        fs=fs,\n        progress_tqdm=progress_tqdm\n    )\n\n    memory_management.load_models_to_gpu([video_pipe.vae])\n    video = video_pipe.decode_latents(latents, vae_hidden_states)\n    return video, image_1, image_2\n\n\n@torch.inference_mode()\ndef process_video(keyframes, prompt, steps, cfg, fps, seed, progress=gr.Progress()):\n    result_frames = []\n    cropped_images = []\n\n    for i, (im1, im2) in enumerate(zip(keyframes[:-1], keyframes[1:])):\n        im1 = np.array(Image.open(im1[0]))\n        im2 = np.array(Image.open(im2[0]))\n        frames, im1, im2 = process_video_inner(\n            im1, im2, prompt, seed=seed + i, steps=steps, cfg_scale=cfg, fs=3,\n            progress_tqdm=functools.partial(progress.tqdm, desc=f'Generating Videos ({i + 1}/{len(keyframes) - 1})')\n        )\n        result_frames.append(frames[:, :, :-1, :, :])\n        cropped_images.append([im1, im2])\n\n    video = torch.cat(result_frames, dim=2)\n    video = torch.flip(video, dims=[2])\n\n    uuid_name = str(uuid.uuid4())\n    output_filename = os.path.join(result_dir, uuid_name + '.mp4')\n    Image.fromarray(cropped_images[0][0]).save(os.path.join(result_dir, uuid_name + '.png'))\n    video = save_bcthw_as_mp4(video, output_filename, fps=fps)\n    video = [x.cpu().numpy() for x in video]\n    return output_filename, video\n\n\nblock = gr.Blocks().queue()\nwith block:\n    gr.Markdown('# Paints-Undo')\n\n    with gr.Accordion(label='Step 1: Upload Image and Generate Prompt', open=True):\n        with gr.Row():\n            with gr.Column():\n                input_fg = gr.Image(sources=['upload'], type=\"numpy\", label=\"Image\", height=512)\n            with gr.Column():\n                prompt_gen_button = gr.Button(value=\"Generate Prompt\", interactive=False)\n                prompt = gr.Textbox(label=\"Output Prompt\", interactive=True)\n\n    with gr.Accordion(label='Step 2: Generate Key Frames', open=True):\n        with gr.Row():\n            with gr.Column():\n                input_undo_steps = gr.Dropdown(label=\"Operation Steps\", value=[400, 600, 800, 900, 950, 999],\n                                               choices=list(range(1000)), multiselect=True)\n                seed = gr.Slider(label='Stage 1 Seed', minimum=0, maximum=50000, step=1, value=12345)\n                image_width = gr.Slider(label=\"Image Width\", minimum=256, maximum=1024, value=512, step=64)\n                image_height = gr.Slider(label=\"Image Height\", minimum=256, maximum=1024, value=640, step=64)\n                steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=50, step=1)\n                cfg = gr.Slider(label=\"CFG Scale\", minimum=1.0, maximum=32.0, value=3.0, step=0.01)\n                n_prompt = gr.Textbox(label=\"Negative Prompt\",\n                                      value='lowres, bad anatomy, bad hands, cropped, worst quality')\n\n            with gr.Column():\n                key_gen_button = gr.Button(value=\"Generate Key Frames\", interactive=False)\n                result_gallery = gr.Gallery(height=512, object_fit='contain', label='Outputs', columns=4)\n\n    with gr.Accordion(label='Step 3: Generate All Videos', open=True):\n        with gr.Row():\n            with gr.Column():\n                # Note that, at \"Step 3: Generate All Videos\", using \"1girl, masterpiece, best quality\"\n                # or \"1boy, masterpiece, best quality\" or just \"masterpiece, best quality\" leads to better results.\n                # Do NOT modify this to use the prompts generated from Step 1 !!\n                i2v_input_text = gr.Text(label='Prompts', value='1girl, masterpiece, best quality')\n                i2v_seed = gr.Slider(label='Stage 2 Seed', minimum=0, maximum=50000, step=1, value=123)\n                i2v_cfg_scale = gr.Slider(minimum=1.0, maximum=15.0, step=0.5, label='CFG Scale', value=7.5,\n                                          elem_id=\"i2v_cfg_scale\")\n                i2v_steps = gr.Slider(minimum=1, maximum=60, step=1, elem_id=\"i2v_steps\",\n                                      label=\"Sampling steps\", value=50)\n                i2v_fps = gr.Slider(minimum=1, maximum=30, step=1, elem_id=\"i2v_motion\", label=\"FPS\", value=4)\n            with gr.Column():\n                i2v_end_btn = gr.Button(\"Generate Video\", interactive=False)\n                i2v_output_video = gr.Video(label=\"Generated Video\", elem_id=\"output_vid\", autoplay=True,\n                                            show_share_button=True, height=512)\n        with gr.Row():\n            i2v_output_images = gr.Gallery(height=512, label=\"Output Frames\", object_fit=\"contain\", columns=8)\n\n    input_fg.change(lambda: [\"\", gr.update(interactive=True), gr.update(interactive=False), gr.update(interactive=False)],\n                    outputs=[prompt, prompt_gen_button, key_gen_button, i2v_end_btn])\n\n    prompt_gen_button.click(\n        fn=interrogator_process,\n        inputs=[input_fg],\n        outputs=[prompt]\n    ).then(lambda: [gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=False)],\n           outputs=[prompt_gen_button, key_gen_button, i2v_end_btn])\n\n    key_gen_button.click(\n        fn=process,\n        inputs=[input_fg, prompt, input_undo_steps, image_width, image_height, seed, steps, n_prompt, cfg],\n        outputs=[result_gallery]\n    ).then(lambda: [gr.update(interactive=True), gr.update(interactive=True), gr.update(interactive=True)],\n           outputs=[prompt_gen_button, key_gen_button, i2v_end_btn])\n\n    i2v_end_btn.click(\n        inputs=[result_gallery, i2v_input_text, i2v_steps, i2v_cfg_scale, i2v_fps, i2v_seed],\n        outputs=[i2v_output_video, i2v_output_images],\n        fn=process_video\n    )\n\n    dbs = [\n        ['./imgs/1.jpg', 12345, 123],\n        ['./imgs/2.jpg', 37000, 12345],\n        ['./imgs/3.jpg', 3000, 3000],\n    ]\n\n    gr.Examples(\n        examples=dbs,\n        inputs=[input_fg, seed, i2v_seed],\n        examples_per_page=1024\n    )\n\nblock.queue().launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "memory_management.py",
          "type": "blob",
          "size": 1.6953125,
          "content": "import torch\nfrom contextlib import contextmanager\n\n\nhigh_vram = False\ngpu = torch.device('cuda')\ncpu = torch.device('cpu')\n\ntorch.zeros((1, 1)).to(gpu, torch.float32)\ntorch.cuda.empty_cache()\n\nmodels_in_gpu = []\n\n\n@contextmanager\ndef movable_bnb_model(m):\n    if hasattr(m, 'quantization_method'):\n        m.quantization_method_backup = m.quantization_method\n        del m.quantization_method\n    try:\n        yield None\n    finally:\n        if hasattr(m, 'quantization_method_backup'):\n            m.quantization_method = m.quantization_method_backup\n            del m.quantization_method_backup\n    return\n\n\ndef load_models_to_gpu(models):\n    global models_in_gpu\n\n    if not isinstance(models, (tuple, list)):\n        models = [models]\n\n    models_to_remain = [m for m in set(models) if m in models_in_gpu]\n    models_to_load = [m for m in set(models) if m not in models_in_gpu]\n    models_to_unload = [m for m in set(models_in_gpu) if m not in models_to_remain]\n\n    if not high_vram:\n        for m in models_to_unload:\n            with movable_bnb_model(m):\n                m.to(cpu)\n            print('Unload to CPU:', m.__class__.__name__)\n        models_in_gpu = models_to_remain\n\n    for m in models_to_load:\n        with movable_bnb_model(m):\n            m.to(gpu)\n        print('Load to GPU:', m.__class__.__name__)\n\n    models_in_gpu = list(set(models_in_gpu + models))\n    torch.cuda.empty_cache()\n    return\n\n\ndef unload_all_models(extra_models=None):\n    global models_in_gpu\n\n    if extra_models is None:\n        extra_models = []\n\n    if not isinstance(extra_models, (tuple, list)):\n        extra_models = [extra_models]\n\n    models_in_gpu = list(set(models_in_gpu + extra_models))\n\n    return load_models_to_gpu([])\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2041015625,
          "content": "diffusers==0.28.0\ntransformers==4.41.1\ngradio==4.31.5\nbitsandbytes==0.43.1\naccelerate==0.30.1\nprotobuf==3.20\nopencv-python\ntensorboardX\nsafetensors\npillow\neinops\ntorch\npeft\nxformers\nonnxruntime\nav\ntorchvision\n"
        },
        {
          "name": "wd14tagger.py",
          "type": "blob",
          "size": 3.2509765625,
          "content": "# https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags\n\n\nimport os\nimport csv\nimport numpy as np\nimport onnxruntime as ort\n\nfrom PIL import Image\nfrom onnxruntime import InferenceSession\nfrom torch.hub import download_url_to_file\n\n\nglobal_model = None\nglobal_csv = None\n\n\ndef download_model(url, local_path):\n    if os.path.exists(local_path):\n        return local_path\n\n    temp_path = local_path + '.tmp'\n    download_url_to_file(url=url, dst=temp_path)\n    os.rename(temp_path, local_path)\n    return local_path\n\n\ndef default_interrogator(image, threshold=0.35, character_threshold=0.85, exclude_tags=\"\"):\n    global global_model, global_csv\n\n    model_name = \"wd-v1-4-moat-tagger-v2\"\n\n    model_onnx_filename = download_model(\n        url=f'https://huggingface.co/lllyasviel/misc/resolve/main/{model_name}.onnx',\n        local_path=f'./{model_name}.onnx',\n    )\n\n    model_csv_filename = download_model(\n        url=f'https://huggingface.co/lllyasviel/misc/resolve/main/{model_name}.csv',\n        local_path=f'./{model_name}.csv',\n    )\n\n    if global_model is not None:\n        model = global_model\n    else:\n        # assert 'CUDAExecutionProvider' in ort.get_available_providers(), 'CUDA Install Failed!'\n        # model = InferenceSession(model_onnx_filename, providers=['CUDAExecutionProvider'])\n        model = InferenceSession(model_onnx_filename, providers=['CPUExecutionProvider'])\n        global_model = model\n\n    input = model.get_inputs()[0]\n    height = input.shape[1]\n\n    if isinstance(image, str):\n        image = Image.open(image)  # RGB\n    elif isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n    else:\n        image = image\n\n    ratio = float(height) / max(image.size)\n    new_size = tuple([int(x*ratio) for x in image.size])\n    image = image.resize(new_size, Image.LANCZOS)\n    square = Image.new(\"RGB\", (height, height), (255, 255, 255))\n    square.paste(image, ((height-new_size[0])//2, (height-new_size[1])//2))\n\n    image = np.array(square).astype(np.float32)\n    image = image[:, :, ::-1]  # RGB -> BGR\n    image = np.expand_dims(image, 0)\n\n    if global_csv is not None:\n        csv_lines = global_csv\n    else:\n        csv_lines = []\n        with open(model_csv_filename) as f:\n            reader = csv.reader(f)\n            next(reader)\n            for row in reader:\n                csv_lines.append(row)\n        global_csv = csv_lines\n\n    tags = []\n    general_index = None\n    character_index = None\n    for line_num, row in enumerate(csv_lines):\n        if general_index is None and row[2] == \"0\":\n            general_index = line_num\n        elif character_index is None and row[2] == \"4\":\n            character_index = line_num\n        tags.append(row[1])\n\n    label_name = model.get_outputs()[0].name\n    probs = model.run([label_name], {input.name: image})[0]\n\n    result = list(zip(tags, probs[0]))\n\n    general = [item for item in result[general_index:character_index] if item[1] > threshold]\n    character = [item for item in result[character_index:] if item[1] > character_threshold]\n\n    all = character + general\n    remove = [s.strip() for s in exclude_tags.lower().split(\",\")]\n    all = [tag for tag in all if tag[0] not in remove]\n\n    res = \", \".join((item[0].replace(\"(\", \"\\\\(\").replace(\")\", \"\\\\)\") for item in all)).replace('_', ' ')\n    return res\n"
        }
      ]
    }
  ]
}