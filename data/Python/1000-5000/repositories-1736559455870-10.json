{
  "metadata": {
    "timestamp": 1736559455870,
    "page": 10,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "yfeng95/PRNet",
      "stars": 4976,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.9873046875,
          "content": "group: travis_latest\nlanguage: python\ncache: pip\npython:\n    - 2.7\n    - 3.6\n    #- nightly\n    #- pypy\n    #- pypy3\nmatrix:\n    allow_failures:\n        - python: nightly\n        - python: pypy\n        - python: pypy3\ninstall:\n    - pip install -r requirements.txt\n    - pip install flake8  # pytest  # add another testing frameworks later\nbefore_script:\n    # stop the build if there are Python syntax errors or undefined names\n    - flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics\n    # exit-zero treats all errors as warnings.  The GitHub editor is 127 chars wide\n    - flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\nscript:\n    - ls\n    - wget \"https://drive.google.com/uc?export=download&confirm=D2ug&id=1UoE-XuW1SDLUjZmJPkIZ1MLxvQFgmTFH\"\n    - ls\n    - python run_basics.py  # Can run only with python and tensorflow\nnotifications:\n    on_success: change\n    on_failure: change  # `always` will be the setting once code changes slow down\n"
        },
        {
          "name": "Data",
          "type": "tree",
          "content": null
        },
        {
          "name": "Docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2018 Yao Feng\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.505859375,
          "content": "# Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network\n\n<p align=\"center\"> \n<img src=\"Docs/images/prnet.gif\">\n</p>\n\n\n\nThis is an official python implementation of PRN. \n\nPRN is a method to jointly regress dense alignment and 3D face shape in an end-to-end manner. More examples on Multi-PIE and 300VW can be seen in [YouTube](https://youtu.be/tXTgLSyIha8) .\n\nThe main features are:\n\n* **End-to-End**  our method can directly regress the 3D facial structure and dense alignment from a single image bypassing 3DMM fitting.\n\n* **Multi-task**  By regressing position map, the 3D geometry along with semantic meaning can be obtained. Thus, we can effortlessly complete the tasks of dense alignment, monocular 3D face reconstruction, pose estimation, etc.\n\n* **Faster than real-time**  The method can run at over 100fps(with GTX 1080) to regress a position map.\n\n* **Robust** Tested on facial images in unconstrained conditions.  Our method is robust to poses, illuminations and occlusions. \n\n  \n\n## Applications\n\n### Basics(Evaluated in paper)\n\n* #### Face Alignment\n\nDense alignment of both visible and non-visible points(including 68 key points). \n\nAnd the **visibility** of  points(1 for visible and 0 for non-visible).\n\n![alignment](Docs/images/alignment.jpg)\n\n* #### 3D Face Reconstruction\n\nGet the 3D vertices and corresponding colours from a single image.  Save the result as mesh data(.obj), which can be opened with [Meshlab](http://www.meshlab.net/) or Microsoft [3D Builder](https://developer.microsoft.com/en-us/windows/hardware/3d-print/3d-builder-resources). Notice that, the texture of non-visible area is distorted due to self-occlusion.\n\n**New**: \n\n1. you can choose to output mesh with its original pose(default) or with front view(which means all output meshes are aligned)\n2. obj file can now also written with texture map(with specified texture size), and you can set non-visible texture to 0. \n\n\n\n![alignment](Docs/images/reconstruct.jpg)\n\n\n\n### More(To be added)\n\n* #### 3D Pose Estimation\n\n  Rather than only use 68 key points to calculate the camera matrix(easily effected by expression and poses), we use all vertices(more than 40K) to calculate a more accurate pose.\n\n  #### ![pose](Docs/images/pose.jpg)\n\n* #### Depth image\n\n  ![pose](Docs/images/depth.jpg)\n\n* #### Texture Editing\n\n  * Data Augmentation/Selfie Editing\n\n    modify special parts of input face, eyes for example:\n\n    ![pose](Docs/images/eye.jpg)\n\n  * Face Swapping\n\n    replace the texture with another, then warp it to original pose and use Poisson editing to blend images.\n\n    ![pose](Docs/images/swapping.jpg)\n\n    \n\n\n\n\n## Getting Started\n\n### Prerequisite\n\n* Python 2.7 (numpy, skimage, scipy)\n\n* TensorFlow >= 1.4\n\n  Optional:\n\n* dlib (for detecting face.  You do not have to install if you can provide bounding box information. )\n\n* opencv2 (for showing results)\n\nGPU is highly recommended. The run time is ~0.01s with GPU(GeForce GTX 1080) and ~0.2s with CPU(Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz).\n\n### Usage\n\n1. Clone the repository\n\n```bash\ngit clone https://github.com/YadiraF/PRNet\ncd PRNet\n```\n\n2. Download the PRN trained model at [BaiduDrive](https://pan.baidu.com/s/10vuV7m00OHLcsihaC-Adsw) or [GoogleDrive](https://drive.google.com/file/d/1UoE-XuW1SDLUjZmJPkIZ1MLxvQFgmTFH/view?usp=sharing), and put it into `Data/net-data`\n\n3. Run the test code.(test AFLW2000 images)\n\n   `python run_basics.py #Can run only with python and tensorflow`\n\n4. Run with your own images\n\n   `python demo.py -i <inputDir> -o <outputDir> --isDlib True  `\n\n   run `python demo.py --help` for more details.\n\n5. For Texture Editing Apps:\n\n   `python demo_texture.py -i image_path_1 -r image_path_2 -o output_path   `\n\n   run `python demo_texture.py --help` for more details.\n\n\n\n## Training\n\nThe core idea of the paper is:\n\nUsing position map to represent face geometry&alignment information, then learning this with an Encoder-Decoder Network.\n\nSo, the training steps:\n\n1. generate position map ground truth.\n\n   the example of generating position map of 300W_LP dataset can be seen in [generate_posmap_300WLP](https://github.com/YadiraF/face3d/blob/master/examples/8_generate_posmap_300WLP.py)\n\n2. an encoder-decoder network to learn mapping from rgb image to position map.\n\n   the weight mask can be found in the folder `Data/uv-data`\n\nWhat you can custom:\n\n1. the UV space of position map.\n\n   you can change the parameterization method, or change the resolution of UV space.\n\n2. the backbone of encoder-decoder network\n\n   this demo uses residual blocks. VGG, mobile-net are also ok.\n\n3. the weight mask\n\n   you can change the weight to focus more on which part your project need more.\n\n4. the training data\n\n   if you have scanned 3d face, it's better to train PRN with your own data. Before that, you may need use ICP to align your face meshes.\n\n\n\n## FQA\n\n1. How to **speed up**?\n\n   a. network inference part\n\n   you can train a smaller network or use a smaller position map as input.\n\n   b. render part\n\n   you can refer to  [c++ version](https://github.com/YadiraF/face3d/blob/master/face3d/mesh/render.py). \n\n   c. other parts like detecting face, writing obj\n\n   the best way is to rewrite them in c++.\n\n2. How to improve the **precision**?\n\n   a. geometry precision.\n\n   Due to the restriction of training data, the precision of reconstructed face from this demo has little detail. You can train the network with your own detailed data or do post-processing like shape-from-shading to add details.\n\n   b. texture precision.\n\n   I just added an option to specify the texture size. When the texture size > face size in original image, and render new facial image with [texture mapping](https://github.com/YadiraF/face3d/blob/04869dcee1455d1fa5b157f165a6878c550cf695/face3d/mesh/render.py), there will be little resample error.\n\n   \n\n## Changelog\n\n* 2018/7/19 add training part. can specify the resolution of the texture map.\n* 2018/5/10 add texture editing examples(for data augmentation, face swapping)\n* 2018/4/28 add visibility of vertices, output obj file with texture map, depth image\n* 2018/4/26 can output mesh with front view\n* 2018/3/28 add pose estimation\n* 2018/3/12  first release(3d reconstruction and dense alignment)\n\n\n\n## License\n\nCode: under MIT license.\n\nTrained model file: please see [issue 28](https://github.com/YadiraF/PRNet/issues/28), thank [Kyle McDonald](https://github.com/kylemcdonald) for his answer.\n\n\n\n## Citation\n\nIf you use this code, please consider citing:\n\n```\n@inProceedings{feng2018prn,\n  title     = {Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network},\n  author    = {Yao Feng and Fan Wu and Xiaohu Shao and Yanfeng Wang and Xi Zhou},\n  booktitle = {ECCV},\n  year      = {2018}\n}\n```\n\n\n\n## Contacts\n\nPlease contact _fengyao@sjtu.edu.cn_  or open an issue for any questions or suggestions.\n\nThanks! (●'◡'●)\n\n\n\n## Acknowledgements\n\n- Thanks [BFM team](https://faces.dmi.unibas.ch/bfm/), [Xiangyu Zhu](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm), and [Anil Bas](https://github.com/anilbas/3DMMasSTN) for sharing 3D data.\n- Thanks Patrik Huber for sharing his work  [eos](https://github.com/patrikhuber/eos), which helps me a lot in studying 3D Face Reconstruction.\n- Thanks the authors of  [3DMMasSTN](https://github.com/anilbas/3DMMasSTN), [DenseReg](https://github.com/ralpguler/DenseReg), [3dmm_cnn](https://github.com/anhttran/3dmm_cnn), [vrn](https://github.com/AaronJackson/vrn), [pix2vertex](https://github.com/matansel/pix2vertex), [face-alignment](https://github.com/1adrianb/face-alignment) for making their excellent works publicly available. \n"
        },
        {
          "name": "TestImages",
          "type": "tree",
          "content": null
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 7.1025390625,
          "content": "import numpy as np\nimport os\nfrom skimage.io import imread, imsave\nfrom skimage.transform import estimate_transform, warp\nfrom time import time\n\nfrom predictor import PosPrediction\n\nclass PRN:\n    ''' Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network\n    Args:\n        is_dlib(bool, optional): If true, dlib is used for detecting faces.\n        prefix(str, optional): If run at another folder, the absolute path is needed to load the data.\n    '''\n    def __init__(self, is_dlib = False, prefix = '.'):\n\n        # resolution of input and output image size.\n        self.resolution_inp = 256\n        self.resolution_op = 256\n\n        #---- load detectors\n        if is_dlib:\n            import dlib\n            detector_path = os.path.join(prefix, 'Data/net-data/mmod_human_face_detector.dat')\n            self.face_detector = dlib.cnn_face_detection_model_v1(\n                    detector_path)\n\n        #---- load PRN \n        self.pos_predictor = PosPrediction(self.resolution_inp, self.resolution_op)\n        prn_path = os.path.join(prefix, 'Data/net-data/256_256_resfcn256_weight')\n        if not os.path.isfile(prn_path + '.data-00000-of-00001'):\n            print(\"please download PRN trained model first.\")\n            exit()\n        self.pos_predictor.restore(prn_path)\n\n        # uv file\n        self.uv_kpt_ind = np.loadtxt(prefix + '/Data/uv-data/uv_kpt_ind.txt').astype(np.int32) # 2 x 68 get kpt\n        self.face_ind = np.loadtxt(prefix + '/Data/uv-data/face_ind.txt').astype(np.int32) # get valid vertices in the pos map\n        self.triangles = np.loadtxt(prefix + '/Data/uv-data/triangles.txt').astype(np.int32) # ntri x 3\n        \n        self.uv_coords = self.generate_uv_coords()        \n\n    def generate_uv_coords(self):\n        resolution = self.resolution_op\n        uv_coords = np.meshgrid(range(resolution),range(resolution))\n        uv_coords = np.transpose(np.array(uv_coords), [1,2,0])\n        uv_coords = np.reshape(uv_coords, [resolution**2, -1]);\n        uv_coords = uv_coords[self.face_ind, :]\n        uv_coords = np.hstack((uv_coords[:,:2], np.zeros([uv_coords.shape[0], 1])))\n        return uv_coords\n\n    def dlib_detect(self, image):\n        return self.face_detector(image, 1)\n\n    def net_forward(self, image):\n        ''' The core of out method: regress the position map of a given image.\n        Args:\n            image: (256,256,3) array. value range: 0~1\n        Returns:\n            pos: the 3D position map. (256, 256, 3) array.\n        '''\n        return self.pos_predictor.predict(image)\n\n    def process(self, input, image_info = None):\n        ''' process image with crop operation.\n        Args:\n            input: (h,w,3) array or str(image path). image value range:1~255. \n            image_info(optional): the bounding box information of faces. if None, will use dlib to detect face. \n\n        Returns:\n            pos: the 3D position map. (256, 256, 3).\n        '''\n        if isinstance(input, str):\n            try:\n                image = imread(input)\n            except IOError:\n                print(\"error opening file: \", input)\n                return None\n        else:\n            image = input\n\n        if image.ndim < 3:\n            image = np.tile(image[:,:,np.newaxis], [1,1,3])\n\n        if image_info is not None:\n            if np.max(image_info.shape) > 4: # key points to get bounding box\n                kpt = image_info\n                if kpt.shape[0] > 3:\n                    kpt = kpt.T\n                left = np.min(kpt[0, :]); right = np.max(kpt[0, :]); \n                top = np.min(kpt[1,:]); bottom = np.max(kpt[1,:])\n            else:  # bounding box\n                bbox = image_info\n                left = bbox[0]; right = bbox[1]; top = bbox[2]; bottom = bbox[3]\n            old_size = (right - left + bottom - top)/2\n            center = np.array([right - (right - left) / 2.0, bottom - (bottom - top) / 2.0])\n            size = int(old_size*1.6)\n        else:\n            detected_faces = self.dlib_detect(image)\n            if len(detected_faces) == 0:\n                print('warning: no detected face')\n                return None\n\n            d = detected_faces[0].rect ## only use the first detected face (assume that each input image only contains one face)\n            left = d.left(); right = d.right(); top = d.top(); bottom = d.bottom()\n            old_size = (right - left + bottom - top)/2\n            center = np.array([right - (right - left) / 2.0, bottom - (bottom - top) / 2.0 + old_size*0.14])\n            size = int(old_size*1.58)\n\n        # crop image\n        src_pts = np.array([[center[0]-size/2, center[1]-size/2], [center[0] - size/2, center[1]+size/2], [center[0]+size/2, center[1]-size/2]])\n        DST_PTS = np.array([[0,0], [0,self.resolution_inp - 1], [self.resolution_inp - 1, 0]])\n        tform = estimate_transform('similarity', src_pts, DST_PTS)\n        \n        image = image/255.\n        cropped_image = warp(image, tform.inverse, output_shape=(self.resolution_inp, self.resolution_inp))\n\n        # run our net\n        #st = time()\n        cropped_pos = self.net_forward(cropped_image)\n        #print 'net time:', time() - st\n\n        # restore \n        cropped_vertices = np.reshape(cropped_pos, [-1, 3]).T\n        z = cropped_vertices[2,:].copy()/tform.params[0,0]\n        cropped_vertices[2,:] = 1\n        vertices = np.dot(np.linalg.inv(tform.params), cropped_vertices)\n        vertices = np.vstack((vertices[:2,:], z))\n        pos = np.reshape(vertices.T, [self.resolution_op, self.resolution_op, 3])\n        \n        return pos\n            \n    def get_landmarks(self, pos):\n        '''\n        Args:\n            pos: the 3D position map. shape = (256, 256, 3).\n        Returns:\n            kpt: 68 3D landmarks. shape = (68, 3).\n        '''\n        kpt = pos[self.uv_kpt_ind[1,:], self.uv_kpt_ind[0,:], :]\n        return kpt\n\n\n    def get_vertices(self, pos):\n        '''\n        Args:\n            pos: the 3D position map. shape = (256, 256, 3).\n        Returns:\n            vertices: the vertices(point cloud). shape = (num of points, 3). n is about 40K here.\n        '''\n        all_vertices = np.reshape(pos, [self.resolution_op**2, -1])\n        vertices = all_vertices[self.face_ind, :]\n\n        return vertices\n\n    def get_colors_from_texture(self, texture):\n        '''\n        Args:\n            texture: the texture map. shape = (256, 256, 3).\n        Returns:\n            colors: the corresponding colors of vertices. shape = (num of points, 3). n is 45128 here.\n        '''\n        all_colors = np.reshape(texture, [self.resolution_op**2, -1])\n        colors = all_colors[self.face_ind, :]\n\n        return colors\n\n\n    def get_colors(self, image, vertices):\n        '''\n        Args:\n            pos: the 3D position map. shape = (256, 256, 3).\n        Returns:\n            colors: the corresponding colors of vertices. shape = (num of points, 3). n is 45128 here.\n        '''\n        [h, w, _] = image.shape\n        vertices[:,0] = np.minimum(np.maximum(vertices[:,0], 0), w - 1)  # x\n        vertices[:,1] = np.minimum(np.maximum(vertices[:,1], 0), h - 1)  # y\n        ind = np.round(vertices).astype(np.int32)\n        colors = image[ind[:,1], ind[:,0], :] # n x 3\n\n        return colors\n\n\n\n\n\n\n\n\n"
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 7.7802734375,
          "content": "import numpy as np\nimport os\nfrom glob import glob\nimport scipy.io as sio\nfrom skimage.io import imread, imsave\nfrom skimage.transform import rescale, resize\nfrom time import time\nimport argparse\nimport ast\n\nfrom api import PRN\n\nfrom utils.estimate_pose import estimate_pose\nfrom utils.rotate_vertices import frontalize\nfrom utils.render_app import get_visibility, get_uv_mask, get_depth_image\nfrom utils.write import write_obj_with_colors, write_obj_with_texture\n\ndef main(args):\n    if args.isShow or args.isTexture:\n        import cv2\n        from utils.cv_plot import plot_kpt, plot_vertices, plot_pose_box\n\n    # ---- init PRN\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu # GPU number, -1 for CPU\n    prn = PRN(is_dlib = args.isDlib)\n\n    # ------------- load data\n    image_folder = args.inputDir\n    save_folder = args.outputDir\n    if not os.path.exists(save_folder):\n        os.mkdir(save_folder)\n\n    types = ('*.jpg', '*.png')\n    image_path_list= []\n    for files in types:\n        image_path_list.extend(glob(os.path.join(image_folder, files)))\n    total_num = len(image_path_list)\n\n    for i, image_path in enumerate(image_path_list):\n\n        name = image_path.strip().split('/')[-1][:-4]\n\n        # read image\n        image = imread(image_path)\n        [h, w, c] = image.shape\n        if c>3:\n            image = image[:,:,:3]\n\n        # the core: regress position map\n        if args.isDlib:\n            max_size = max(image.shape[0], image.shape[1])\n            if max_size> 1000:\n                image = rescale(image, 1000./max_size)\n                image = (image*255).astype(np.uint8)\n            pos = prn.process(image) # use dlib to detect face\n        else:\n            if image.shape[0] == image.shape[1]:\n                image = resize(image, (256,256))\n                pos = prn.net_forward(image/255.) # input image has been cropped to 256x256\n            else:\n                box = np.array([0, image.shape[1]-1, 0, image.shape[0]-1]) # cropped with bounding box\n                pos = prn.process(image, box)\n        \n        image = image/255.\n        if pos is None:\n            continue\n\n        if args.is3d or args.isMat or args.isPose or args.isShow:\n            # 3D vertices\n            vertices = prn.get_vertices(pos)\n            if args.isFront:\n                save_vertices = frontalize(vertices)\n            else:\n                save_vertices = vertices.copy()\n            save_vertices[:,1] = h - 1 - save_vertices[:,1]\n\n        if args.isImage:\n            imsave(os.path.join(save_folder, name + '.jpg'), image)\n\n        if args.is3d:\n            # corresponding colors\n            colors = prn.get_colors(image, vertices)\n\n            if args.isTexture:\n                if args.texture_size != 256:\n                    pos_interpolated = resize(pos, (args.texture_size, args.texture_size), preserve_range = True)\n                else:\n                    pos_interpolated = pos.copy()\n                texture = cv2.remap(image, pos_interpolated[:,:,:2].astype(np.float32), None, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT,borderValue=(0))\n                if args.isMask:\n                    vertices_vis = get_visibility(vertices, prn.triangles, h, w)\n                    uv_mask = get_uv_mask(vertices_vis, prn.triangles, prn.uv_coords, h, w, prn.resolution_op)\n                    uv_mask = resize(uv_mask, (args.texture_size, args.texture_size), preserve_range = True)\n                    texture = texture*uv_mask[:,:,np.newaxis]\n                write_obj_with_texture(os.path.join(save_folder, name + '.obj'), save_vertices, prn.triangles, texture, prn.uv_coords/prn.resolution_op)#save 3d face with texture(can open with meshlab)\n            else:\n                write_obj_with_colors(os.path.join(save_folder, name + '.obj'), save_vertices, prn.triangles, colors) #save 3d face(can open with meshlab)\n\n        if args.isDepth:\n            depth_image = get_depth_image(vertices, prn.triangles, h, w, True)\n            depth = get_depth_image(vertices, prn.triangles, h, w)\n            imsave(os.path.join(save_folder, name + '_depth.jpg'), depth_image)\n            sio.savemat(os.path.join(save_folder, name + '_depth.mat'), {'depth':depth})\n\n        if args.isMat:\n            sio.savemat(os.path.join(save_folder, name + '_mesh.mat'), {'vertices': vertices, 'colors': colors, 'triangles': prn.triangles})\n\n        if args.isKpt or args.isShow:\n            # get landmarks\n            kpt = prn.get_landmarks(pos)\n            np.savetxt(os.path.join(save_folder, name + '_kpt.txt'), kpt)\n\n        if args.isPose or args.isShow:\n            # estimate pose\n            camera_matrix, pose = estimate_pose(vertices)\n            np.savetxt(os.path.join(save_folder, name + '_pose.txt'), pose) \n            np.savetxt(os.path.join(save_folder, name + '_camera_matrix.txt'), camera_matrix) \n\n            np.savetxt(os.path.join(save_folder, name + '_pose.txt'), pose)\n\n        if args.isShow:\n            # ---------- Plot\n            image_pose = plot_pose_box(image, camera_matrix, kpt)\n            cv2.imshow('sparse alignment', plot_kpt(image, kpt))\n            cv2.imshow('dense alignment', plot_vertices(image, vertices))\n            cv2.imshow('pose', plot_pose_box(image, camera_matrix, kpt))\n            cv2.waitKey(0)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network')\n\n    parser.add_argument('-i', '--inputDir', default='TestImages/', type=str,\n                        help='path to the input directory, where input images are stored.')\n    parser.add_argument('-o', '--outputDir', default='TestImages/results', type=str,\n                        help='path to the output directory, where results(obj,txt files) will be stored.')\n    parser.add_argument('--gpu', default='0', type=str,\n                        help='set gpu id, -1 for CPU')\n    parser.add_argument('--isDlib', default=True, type=ast.literal_eval,\n                        help='whether to use dlib for detecting face, default is True, if False, the input image should be cropped in advance')\n    parser.add_argument('--is3d', default=True, type=ast.literal_eval,\n                        help='whether to output 3D face(.obj). default save colors.')\n    parser.add_argument('--isMat', default=False, type=ast.literal_eval,\n                        help='whether to save vertices,color,triangles as mat for matlab showing')\n    parser.add_argument('--isKpt', default=False, type=ast.literal_eval,\n                        help='whether to output key points(.txt)')\n    parser.add_argument('--isPose', default=False, type=ast.literal_eval,\n                        help='whether to output estimated pose(.txt)')\n    parser.add_argument('--isShow', default=False, type=ast.literal_eval,\n                        help='whether to show the results with opencv(need opencv)')\n    parser.add_argument('--isImage', default=False, type=ast.literal_eval,\n                        help='whether to save input image')\n    # update in 2017/4/10\n    parser.add_argument('--isFront', default=False, type=ast.literal_eval,\n                        help='whether to frontalize vertices(mesh)')\n    # update in 2017/4/25\n    parser.add_argument('--isDepth', default=False, type=ast.literal_eval,\n                        help='whether to output depth image')\n    # update in 2017/4/27\n    parser.add_argument('--isTexture', default=False, type=ast.literal_eval,\n                        help='whether to save texture in obj file')\n    parser.add_argument('--isMask', default=False, type=ast.literal_eval,\n                        help='whether to set invisible pixels(due to self-occlusion) in texture as 0')\n    # update in 2017/7/19\n    parser.add_argument('--texture_size', default=256, type=int,\n                        help='size of texture map, default is 256. need isTexture is True')\n    main(parser.parse_args())\n"
        },
        {
          "name": "demo_texture.py",
          "type": "blob",
          "size": 4.0791015625,
          "content": "import numpy as np\nimport os\nfrom glob import glob\nimport scipy.io as sio\nfrom skimage.io import imread, imsave\nfrom skimage.transform import rescale, resize\nfrom time import time\nimport argparse\nimport ast\nimport matplotlib.pyplot as plt\nimport argparse\n\nfrom api import PRN\nfrom utils.render import render_texture\nimport cv2\n\n\ndef texture_editing(prn, args):\n    # read image\n    image = imread(args.image_path)\n    [h, w, _] = image.shape\n\n    #-- 1. 3d reconstruction -> get texture. \n    pos = prn.process(image) \n    vertices = prn.get_vertices(pos)\n    image = image/255.\n    texture = cv2.remap(image, pos[:,:,:2].astype(np.float32), None, interpolation=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT,borderValue=(0))\n    \n    #-- 2. Texture Editing\n    Mode = args.mode\n    # change part of texture(for data augumentation/selfie editing. Here modify eyes for example)\n    if Mode == 0: \n        # load eye mask\n        uv_face_eye = imread('Data/uv-data/uv_face_eyes.png', as_grey=True)/255. \n        uv_face = imread('Data/uv-data/uv_face.png', as_grey=True)/255.\n        eye_mask = (abs(uv_face_eye - uv_face) > 0).astype(np.float32)\n\n        # texture from another image or a processed texture\n        ref_image = imread(args.ref_path)\n        ref_pos = prn.process(ref_image)\n        ref_image = ref_image/255.\n        ref_texture = cv2.remap(ref_image, ref_pos[:,:,:2].astype(np.float32), None, interpolation=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT,borderValue=(0))\n\n        # modify texture\n        new_texture = texture*(1 - eye_mask[:,:,np.newaxis]) + ref_texture*eye_mask[:,:,np.newaxis]\n    \n    # change whole face(face swap)\n    elif Mode == 1: \n        # texture from another image or a processed texture\n        ref_image = imread(args.ref_path)\n        ref_pos = prn.process(ref_image)\n        ref_image = ref_image/255.\n        ref_texture = cv2.remap(ref_image, ref_pos[:,:,:2].astype(np.float32), None, interpolation=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT,borderValue=(0))\n        ref_vertices = prn.get_vertices(ref_pos)\n        new_texture = ref_texture#(texture + ref_texture)/2.\n\n    else:\n        print('Wrong Mode! Mode should be 0 or 1.')\n        exit()\n\n\n    #-- 3. remap to input image.(render)\n    vis_colors = np.ones((vertices.shape[0], 1))\n    face_mask = render_texture(vertices.T, vis_colors.T, prn.triangles.T, h, w, c = 1)\n    face_mask = np.squeeze(face_mask > 0).astype(np.float32)\n    \n    new_colors = prn.get_colors_from_texture(new_texture)\n    new_image = render_texture(vertices.T, new_colors.T, prn.triangles.T, h, w, c = 3)\n    new_image = image*(1 - face_mask[:,:,np.newaxis]) + new_image*face_mask[:,:,np.newaxis]\n\n    # Possion Editing for blending image\n    vis_ind = np.argwhere(face_mask>0)\n    vis_min = np.min(vis_ind, 0)\n    vis_max = np.max(vis_ind, 0)\n    center = (int((vis_min[1] + vis_max[1])/2+0.5), int((vis_min[0] + vis_max[0])/2+0.5))\n    output = cv2.seamlessClone((new_image*255).astype(np.uint8), (image*255).astype(np.uint8), (face_mask*255).astype(np.uint8), center, cv2.NORMAL_CLONE)\n   \n    # save output\n    imsave(args.output_path, output) \n    print('Done.')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Texture Editing by PRN')\n\n    parser.add_argument('-i', '--image_path', default='TestImages/AFLW2000/image00081.jpg', type=str,\n                        help='path to input image')\n    parser.add_argument('-r', '--ref_path', default='TestImages/trump.jpg', type=str, \n                        help='path to reference image(texture ref)')\n    parser.add_argument('-o', '--output_path', default='TestImages/output.jpg', type=str, \n                        help='path to save output')\n    parser.add_argument('--mode', default=1, type=int, \n                        help='ways to edit texture. 0 for modifying parts, 1 for changing whole')\n    parser.add_argument('--gpu', default='0', type=str, \n                        help='set gpu id, -1 for CPU')\n\n    # ---- init PRN\n    os.environ['CUDA_VISIBLE_DEVICES'] = parser.parse_args().gpu # GPU number, -1 for CPU\n    prn = PRN(is_dlib = True) \n\n    texture_editing(prn, parser.parse_args())"
        },
        {
          "name": "predictor.py",
          "type": "blob",
          "size": 5.80078125,
          "content": "import tensorflow as tf\nimport tensorflow.contrib.layers as tcl\nfrom tensorflow.contrib.framework import arg_scope\nimport numpy as np\n\ndef resBlock(x, num_outputs, kernel_size = 4, stride=1, activation_fn=tf.nn.relu, normalizer_fn=tcl.batch_norm, scope=None):\n    assert num_outputs%2==0 #num_outputs must be divided by channel_factor(2 here)\n    with tf.variable_scope(scope, 'resBlock'):\n        shortcut = x\n        if stride != 1 or x.get_shape()[3] != num_outputs:\n            shortcut = tcl.conv2d(shortcut, num_outputs, kernel_size=1, stride=stride, \n                        activation_fn=None, normalizer_fn=None, scope='shortcut')\n        x = tcl.conv2d(x, num_outputs/2, kernel_size=1, stride=1, padding='SAME')\n        x = tcl.conv2d(x, num_outputs/2, kernel_size=kernel_size, stride=stride, padding='SAME')\n        x = tcl.conv2d(x, num_outputs, kernel_size=1, stride=1, activation_fn=None, padding='SAME', normalizer_fn=None)\n\n        x += shortcut       \n        x = normalizer_fn(x)\n        x = activation_fn(x)\n    return x\n\n\nclass resfcn256(object):\n    def __init__(self, resolution_inp = 256, resolution_op = 256, channel = 3, name = 'resfcn256'):\n        self.name = name\n        self.channel = channel\n        self.resolution_inp = resolution_inp\n        self.resolution_op = resolution_op\n\n    def __call__(self, x, is_training = True):\n        with tf.variable_scope(self.name) as scope:\n            with arg_scope([tcl.batch_norm], is_training=is_training, scale=True):\n                with arg_scope([tcl.conv2d, tcl.conv2d_transpose], activation_fn=tf.nn.relu, \n                                     normalizer_fn=tcl.batch_norm, \n                                     biases_initializer=None, \n                                     padding='SAME',\n                                     weights_regularizer=tcl.l2_regularizer(0.0002)):\n                    size = 16  \n                    # x: s x s x 3\n                    se = tcl.conv2d(x, num_outputs=size, kernel_size=4, stride=1) # 256 x 256 x 16\n                    se = resBlock(se, num_outputs=size * 2, kernel_size=4, stride=2) # 128 x 128 x 32\n                    se = resBlock(se, num_outputs=size * 2, kernel_size=4, stride=1) # 128 x 128 x 32\n                    se = resBlock(se, num_outputs=size * 4, kernel_size=4, stride=2) # 64 x 64 x 64\n                    se = resBlock(se, num_outputs=size * 4, kernel_size=4, stride=1) # 64 x 64 x 64\n                    se = resBlock(se, num_outputs=size * 8, kernel_size=4, stride=2) # 32 x 32 x 128\n                    se = resBlock(se, num_outputs=size * 8, kernel_size=4, stride=1) # 32 x 32 x 128\n                    se = resBlock(se, num_outputs=size * 16, kernel_size=4, stride=2) # 16 x 16 x 256\n                    se = resBlock(se, num_outputs=size * 16, kernel_size=4, stride=1) # 16 x 16 x 256\n                    se = resBlock(se, num_outputs=size * 32, kernel_size=4, stride=2) # 8 x 8 x 512\n                    se = resBlock(se, num_outputs=size * 32, kernel_size=4, stride=1) # 8 x 8 x 512\n\n                    pd = tcl.conv2d_transpose(se, size * 32, 4, stride=1) # 8 x 8 x 512 \n                    pd = tcl.conv2d_transpose(pd, size * 16, 4, stride=2) # 16 x 16 x 256 \n                    pd = tcl.conv2d_transpose(pd, size * 16, 4, stride=1) # 16 x 16 x 256 \n                    pd = tcl.conv2d_transpose(pd, size * 16, 4, stride=1) # 16 x 16 x 256 \n                    pd = tcl.conv2d_transpose(pd, size * 8, 4, stride=2) # 32 x 32 x 128 \n                    pd = tcl.conv2d_transpose(pd, size * 8, 4, stride=1) # 32 x 32 x 128 \n                    pd = tcl.conv2d_transpose(pd, size * 8, 4, stride=1) # 32 x 32 x 128 \n                    pd = tcl.conv2d_transpose(pd, size * 4, 4, stride=2) # 64 x 64 x 64 \n                    pd = tcl.conv2d_transpose(pd, size * 4, 4, stride=1) # 64 x 64 x 64 \n                    pd = tcl.conv2d_transpose(pd, size * 4, 4, stride=1) # 64 x 64 x 64 \n                    \n                    pd = tcl.conv2d_transpose(pd, size * 2, 4, stride=2) # 128 x 128 x 32\n                    pd = tcl.conv2d_transpose(pd, size * 2, 4, stride=1) # 128 x 128 x 32\n                    pd = tcl.conv2d_transpose(pd, size, 4, stride=2) # 256 x 256 x 16\n                    pd = tcl.conv2d_transpose(pd, size, 4, stride=1) # 256 x 256 x 16\n\n                    pd = tcl.conv2d_transpose(pd, 3, 4, stride=1) # 256 x 256 x 3\n                    pd = tcl.conv2d_transpose(pd, 3, 4, stride=1) # 256 x 256 x 3\n                    pos = tcl.conv2d_transpose(pd, 3, 4, stride=1, activation_fn = tf.nn.sigmoid)#, padding='SAME', weights_initializer=tf.random_normal_initializer(0, 0.02))\n                                \n                    return pos\n    @property\n    def vars(self):\n        return [var for var in tf.global_variables() if self.name in var.name]\n\n\nclass PosPrediction():\n    def __init__(self, resolution_inp = 256, resolution_op = 256): \n        # -- hyper settings\n        self.resolution_inp = resolution_inp\n        self.resolution_op = resolution_op\n        self.MaxPos = resolution_inp*1.1\n\n        # network type\n        self.network = resfcn256(self.resolution_inp, self.resolution_op)\n\n        # net forward\n        self.x = tf.placeholder(tf.float32, shape=[None, self.resolution_inp, self.resolution_inp, 3])  \n        self.x_op = self.network(self.x, is_training = False)\n        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n\n    def restore(self, model_path):        \n        tf.train.Saver(self.network.vars).restore(self.sess, model_path)\n \n    def predict(self, image):\n        pos = self.sess.run(self.x_op, \n                    feed_dict = {self.x: image[np.newaxis, :,:,:]})\n        pos = np.squeeze(pos)\n        return pos*self.MaxPos\n\n    def predict_batch(self, images):\n        pos = self.sess.run(self.x_op, \n                    feed_dict = {self.x: images})\n        return pos*self.MaxPos\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.04296875,
          "content": "numpy>=1.14.3\nscikit-image\nscipy\ntensorflow\n"
        },
        {
          "name": "run_basics.py",
          "type": "blob",
          "size": 1.689453125,
          "content": "import numpy as np\nimport os\nfrom glob import glob\nimport scipy.io as sio\nfrom skimage.io import imread, imsave\nfrom time import time\n\nfrom api import PRN\nfrom utils.write import write_obj_with_colors\n\n# ---- init PRN\nos.environ['CUDA_VISIBLE_DEVICES'] = '0' # GPU number, -1 for CPU\nprn = PRN(is_dlib = False) \n\n\n# ------------- load data\nimage_folder = 'TestImages/AFLW2000/'\nsave_folder = 'TestImages/AFLW2000_results'\nif not os.path.exists(save_folder):\n    os.mkdir(save_folder)\n\ntypes = ('*.jpg', '*.png')\nimage_path_list= []\nfor files in types:\n    image_path_list.extend(glob(os.path.join(image_folder, files)))\ntotal_num = len(image_path_list)\n\nfor i, image_path in enumerate(image_path_list):\n    # read image\n    image = imread(image_path)\n\n    # the core: regress position map    \n    if 'AFLW2000' in image_path:\n        mat_path = image_path.replace('jpg', 'mat')\n        info = sio.loadmat(mat_path)\n        kpt = info['pt3d_68']\n        pos = prn.process(image, kpt) # kpt information is only used for detecting face and cropping image\n    else:\n        pos = prn.process(image) # use dlib to detect face\n\n    # -- Basic Applications\n    # get landmarks\n    kpt = prn.get_landmarks(pos)\n    # 3D vertices\n    vertices = prn.get_vertices(pos)\n    # corresponding colors\n    colors = prn.get_colors(image, vertices)\n\n    # -- save\n    name = image_path.strip().split('/')[-1][:-4]\n    np.savetxt(os.path.join(save_folder, name + '.txt'), kpt) \n    write_obj_with_colors(os.path.join(save_folder, name + '.obj'), vertices, prn.triangles, colors) #save 3d face(can open with meshlab)\n\n    sio.savemat(os.path.join(save_folder, name + '_mesh.mat'), {'vertices': vertices, 'colors': colors, 'triangles': prn.triangles})\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}