{
  "metadata": {
    "timestamp": 1736559464502,
    "page": 24,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ifzhang/ByteTrack",
      "stars": 4945,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.8828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# output\ndocs/api\n.code-workspace.code-workspace\n*.pkl\n*.npy\n*.pth\n*.onnx\n*.engine\nevents.out.tfevents*\npretrained\nYOLOX_outputs"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.3779296875,
          "content": "FROM nvcr.io/nvidia/tensorrt:21.09-py3\n\nENV DEBIAN_FRONTEND=noninteractive\nARG USERNAME=user\nARG WORKDIR=/workspace/ByteTrack\n\nRUN apt-get update && apt-get install -y \\\n        automake autoconf libpng-dev nano python3-pip \\\n        curl zip unzip libtool swig zlib1g-dev pkg-config \\\n        python3-mock libpython3-dev libpython3-all-dev \\\n        g++ gcc cmake make pciutils cpio gosu wget \\\n        libgtk-3-dev libxtst-dev sudo apt-transport-https \\\n        build-essential gnupg git xz-utils vim \\\n        libva-drm2 libva-x11-2 vainfo libva-wayland2 libva-glx2 \\\n        libva-dev libdrm-dev xorg xorg-dev protobuf-compiler \\\n        openbox libx11-dev libgl1-mesa-glx libgl1-mesa-dev \\\n        libtbb2 libtbb-dev libopenblas-dev libopenmpi-dev \\\n    && sed -i 's/# set linenumbers/set linenumbers/g' /etc/nanorc \\\n    && apt clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN git clone https://github.com/ifzhang/ByteTrack \\\n    && cd ByteTrack \\\n    && git checkout 3434c5e8bc6a5ae8ad530528ba8d9a431967f237 \\\n    && mkdir -p YOLOX_outputs/yolox_x_mix_det/track_vis \\\n    && sed -i 's/torch>=1.7/torch==1.9.1+cu111/g' requirements.txt \\\n    && sed -i 's/torchvision==0.10.0/torchvision==0.10.1+cu111/g' requirements.txt \\\n    && sed -i \"s/'cuda'/0/g\" tools/demo_track.py \\\n    && pip3 install pip --upgrade \\\n    && pip3 install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html \\\n    && python3 setup.py develop \\\n    && pip3 install cython \\\n    && pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' \\\n    && pip3 install cython_bbox gdown \\\n    && ldconfig \\\n    && pip cache purge\n\nRUN git clone https://github.com/NVIDIA-AI-IOT/torch2trt \\\n    && cd torch2trt \\\n    && git checkout 0400b38123d01cc845364870bdf0a0044ea2b3b2 \\\n    # https://github.com/NVIDIA-AI-IOT/torch2trt/issues/619\n    && wget https://github.com/NVIDIA-AI-IOT/torch2trt/commit/8b9fb46ddbe99c2ddf3f1ed148c97435cbeb8fd3.patch \\\n    && git apply 8b9fb46ddbe99c2ddf3f1ed148c97435cbeb8fd3.patch \\\n    && python3 setup.py install\n\nRUN echo \"root:root\" | chpasswd \\\n    && adduser --disabled-password --gecos \"\" \"${USERNAME}\" \\\n    && echo \"${USERNAME}:${USERNAME}\" | chpasswd \\\n    && echo \"%${USERNAME}    ALL=(ALL)   NOPASSWD:    ALL\" >> /etc/sudoers.d/${USERNAME} \\\n    && chmod 0440 /etc/sudoers.d/${USERNAME}\nUSER ${USERNAME}\nRUN sudo chown -R ${USERNAME}:${USERNAME} ${WORKDIR}\nWORKDIR ${WORKDIR}"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2021 Yifu Zhang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.0859375,
          "content": "# ByteTrack\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bytetrack-multi-object-tracking-by-1/multi-object-tracking-on-mot17)](https://paperswithcode.com/sota/multi-object-tracking-on-mot17?p=bytetrack-multi-object-tracking-by-1)\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bytetrack-multi-object-tracking-by-1/multi-object-tracking-on-mot20-1)](https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=bytetrack-multi-object-tracking-by-1)\n\n#### ByteTrack is a simple, fast and strong multi-object tracker.\n\n<p align=\"center\"><img src=\"assets/sota.png\" width=\"500\"/></p>\n\n> [**ByteTrack: Multi-Object Tracking by Associating Every Detection Box**](https://arxiv.org/abs/2110.06864)\n> \n> Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, Xinggang Wang\n> \n> *[arXiv 2110.06864](https://arxiv.org/abs/2110.06864)*\n\n## Demo Links\n| Google Colab Demo | Huggingface Demo |                  YouTube Tutorial                   | Original Paper: ByteTrack |\n|:-----------------:|:----------------:|:---------------------------------------------------:|:-------------------------:|\n|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bDilg4cmXFa8HCKHbsZ_p16p0vrhLyu0?usp=sharing)|[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/bytetrack)|[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/QCG8QMhga9k)|[arXiv 2110.06864](https://arxiv.org/abs/2110.06864) |\n* Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio).\n\n\n## Abstract\nMulti-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 scores ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU.\n<p align=\"center\"><img src=\"assets/teasing.png\" width=\"400\"/></p>\n\n## News\n* (2022.07) Our paper is accepted by ECCV 2022!\n* (2022.06) A [nice re-implementation](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/mot/bytetrack) by Baidu [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)!\n\n## Tracking performance\n### Results on MOT challenge test set\n| Dataset    |  MOTA | IDF1 | HOTA | MT | ML | FP | FN | IDs | FPS |\n|------------|-------|------|------|-------|-------|------|------|------|------|\n|MOT17       | 80.3 | 77.3 | 63.1 | 53.2% | 14.5% | 25491 | 83721 | 2196 | 29.6 |\n|MOT20       | 77.8 | 75.2 | 61.3 | 69.2% | 9.5%  | 26249 | 87594 | 1223 | 13.7 |\n\n### Visualization results on MOT challenge test set\n<img src=\"assets/MOT17-01-SDP.gif\" width=\"400\"/>   <img src=\"assets/MOT17-07-SDP.gif\" width=\"400\"/>\n<img src=\"assets/MOT20-07.gif\" width=\"400\"/>   <img src=\"assets/MOT20-08.gif\" width=\"400\"/>\n\n## Installation\n### 1. Installing on the host machine\nStep1. Install ByteTrack.\n```shell\ngit clone https://github.com/ifzhang/ByteTrack.git\ncd ByteTrack\npip3 install -r requirements.txt\npython3 setup.py develop\n```\n\nStep2. Install [pycocotools](https://github.com/cocodataset/cocoapi).\n\n```shell\npip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n```\n\nStep3. Others\n```shell\npip3 install cython_bbox\n```\n### 2. Docker build\n```shell\ndocker build -t bytetrack:latest .\n\n# Startup sample\nmkdir -p pretrained && \\\nmkdir -p YOLOX_outputs && \\\nxhost +local: && \\\ndocker run --gpus all -it --rm \\\n-v $PWD/pretrained:/workspace/ByteTrack/pretrained \\\n-v $PWD/datasets:/workspace/ByteTrack/datasets \\\n-v $PWD/YOLOX_outputs:/workspace/ByteTrack/YOLOX_outputs \\\n-v /tmp/.X11-unix/:/tmp/.X11-unix:rw \\\n--device /dev/video0:/dev/video0:mwr \\\n--net=host \\\n-e XDG_RUNTIME_DIR=$XDG_RUNTIME_DIR \\\n-e DISPLAY=$DISPLAY \\\n--privileged \\\nbytetrack:latest\n```\n\n## Data preparation\n\nDownload [MOT17](https://motchallenge.net/), [MOT20](https://motchallenge.net/), [CrowdHuman](https://www.crowdhuman.org/), [Cityperson](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md), [ETHZ](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md) and put them under <ByteTrack_HOME>/datasets in the following structure:\n```\ndatasets\n   |——————mot\n   |        └——————train\n   |        └——————test\n   └——————crowdhuman\n   |         └——————Crowdhuman_train\n   |         └——————Crowdhuman_val\n   |         └——————annotation_train.odgt\n   |         └——————annotation_val.odgt\n   └——————MOT20\n   |        └——————train\n   |        └——————test\n   └——————Cityscapes\n   |        └——————images\n   |        └——————labels_with_ids\n   └——————ETHZ\n            └——————eth01\n            └——————...\n            └——————eth07\n```\n\nThen, you need to turn the datasets to COCO format and mix different training data:\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/convert_mot17_to_coco.py\npython3 tools/convert_mot20_to_coco.py\npython3 tools/convert_crowdhuman_to_coco.py\npython3 tools/convert_cityperson_to_coco.py\npython3 tools/convert_ethz_to_coco.py\n```\n\nBefore mixing different datasets, you need to follow the operations in [mix_xxx.py](https://github.com/ifzhang/ByteTrack/blob/c116dfc746f9ebe07d419caa8acba9b3acfa79a6/tools/mix_data_ablation.py#L6) to create a data folder and link. Finally, you can mix the training data:\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/mix_data_ablation.py\npython3 tools/mix_data_test_mot17.py\npython3 tools/mix_data_test_mot20.py\n```\n\n\n## Model zoo\n\n### Ablation model\n\nTrain on CrowdHuman and MOT17 half train, evaluate on MOT17 half val\n\n| Model    |  MOTA | IDF1 | IDs | FPS |\n|------------|-------|------|------|------|\n|ByteTrack_ablation [[google]](https://drive.google.com/file/d/1iqhM-6V_r1FpOlOzrdP_Ejshgk0DxOob/view?usp=sharing), [[baidu(code:eeo8)]](https://pan.baidu.com/s/1W5eRBnxc4x9V8gm7dgdEYg) | 76.6 | 79.3 | 159 | 29.6 |\n\n### MOT17 test model\n\nTrain on CrowdHuman, MOT17, Cityperson and ETHZ, evaluate on MOT17 train.\n\n* **Standard models**\n\n| Model    |  MOTA | IDF1 | IDs | FPS |\n|------------|-------|------|------|------|\n|bytetrack_x_mot17 [[google]](https://drive.google.com/file/d/1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5/view?usp=sharing), [[baidu(code:ic0i)]](https://pan.baidu.com/s/1OJKrcQa_JP9zofC6ZtGBpw) | 90.0 | 83.3 | 422 | 29.6 |\n|bytetrack_l_mot17 [[google]](https://drive.google.com/file/d/1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz/view?usp=sharing), [[baidu(code:1cml)]](https://pan.baidu.com/s/1242adimKM6TYdeLU2qnuRA) | 88.7 | 80.7 | 460 | 43.7 |\n|bytetrack_m_mot17 [[google]](https://drive.google.com/file/d/11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun/view?usp=sharing), [[baidu(code:u3m4)]](https://pan.baidu.com/s/1fKemO1uZfvNSLzJfURO4TQ) | 87.0 | 80.1 | 477 | 54.1 |\n|bytetrack_s_mot17 [[google]](https://drive.google.com/file/d/1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj/view?usp=sharing), [[baidu(code:qflm)]](https://pan.baidu.com/s/1PiP1kQfgxAIrnGUbFP6Wfg) | 79.2 | 74.3 | 533 | 64.5 |\n\n* **Light models**\n\n| Model    |  MOTA | IDF1 | IDs | Params(M) | FLOPs(G) |\n|------------|-------|------|------|------|-------|\n|bytetrack_nano_mot17 [[google]](https://drive.google.com/file/d/1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX/view?usp=sharing), [[baidu(code:1ub8)]](https://pan.baidu.com/s/1dMxqBPP7lFNRZ3kFgDmWdw) | 69.0 | 66.3 | 531 | 0.90 | 3.99 |\n|bytetrack_tiny_mot17 [[google]](https://drive.google.com/file/d/1LFAl14sql2Q5Y9aNFsX_OqsnIzUD_1ju/view?usp=sharing), [[baidu(code:cr8i)]](https://pan.baidu.com/s/1jgIqisPSDw98HJh8hqhM5w) | 77.1 | 71.5 | 519 | 5.03 | 24.45 |\n\n\n\n### MOT20 test model\n\nTrain on CrowdHuman and MOT20, evaluate on MOT20 train.\n\n\n| Model    |  MOTA | IDF1 | IDs | FPS |\n|------------|-------|------|------|------|\n|bytetrack_x_mot20 [[google]](https://drive.google.com/file/d/1HX2_JpMOjOIj1Z9rJjoet9XNy_cCAs5U/view?usp=sharing), [[baidu(code:3apd)]](https://pan.baidu.com/s/1bowJJj0bAnbhEQ3_6_Am0A) | 93.4 | 89.3 | 1057 | 17.5 |\n\n\n## Training\n\nThe COCO pretrained YOLOX model can be downloaded from their [model zoo](https://github.com/Megvii-BaseDetection/YOLOX/tree/0.1.0). After downloading the pretrained models, you can put them under <ByteTrack_HOME>/pretrained.\n\n* **Train ablation model (MOT17 half train and CrowdHuman)**\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/train.py -f exps/example/mot/yolox_x_ablation.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth\n```\n\n* **Train MOT17 test model (MOT17 train, CrowdHuman, Cityperson and ETHZ)**\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/train.py -f exps/example/mot/yolox_x_mix_det.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth\n```\n\n* **Train MOT20 test model (MOT20 train, CrowdHuman)**\n\nFor MOT20, you need to clip the bounding boxes inside the image.\n\nAdd clip operation in [line 134-135 in data_augment.py](https://github.com/ifzhang/ByteTrack/blob/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/data_augment.py#L134), [line 122-125 in mosaicdetection.py](https://github.com/ifzhang/ByteTrack/blob/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/datasets/mosaicdetection.py#L122), [line 217-225 in mosaicdetection.py](https://github.com/ifzhang/ByteTrack/blob/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/datasets/mosaicdetection.py#L217), [line 115-118 in boxes.py](https://github.com/ifzhang/ByteTrack/blob/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/utils/boxes.py#L115).\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/train.py -f exps/example/mot/yolox_x_mix_mot20_ch.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth\n```\n\n* **Train custom dataset**\n\nFirst, you need to prepare your dataset in COCO format. You can refer to [MOT-to-COCO](https://github.com/ifzhang/ByteTrack/blob/main/tools/convert_mot17_to_coco.py) or [CrowdHuman-to-COCO](https://github.com/ifzhang/ByteTrack/blob/main/tools/convert_crowdhuman_to_coco.py). Then, you need to create a Exp file for your dataset. You can refer to the [CrowdHuman](https://github.com/ifzhang/ByteTrack/blob/main/exps/example/mot/yolox_x_ch.py) training Exp file. Don't forget to modify get_data_loader() and get_eval_loader in your Exp file. Finally, you can train bytetrack on your dataset by running:\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/train.py -f exps/example/mot/your_exp_file.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth\n```\n\n\n## Tracking\n\n* **Evaluation on MOT17 half val**\n\nRun ByteTrack:\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/track.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse\n```\nYou can get 76.6 MOTA using our pretrained model.\n\nRun other trackers:\n```shell\npython3 tools/track_sort.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse\npython3 tools/track_deepsort.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse\npython3 tools/track_motdt.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse\n```\n\n* **Test on MOT17**\n\nRun ByteTrack:\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/track.py -f exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar -b 1 -d 1 --fp16 --fuse\npython3 tools/interpolation.py\n```\nSubmit the txt files to [MOTChallenge](https://motchallenge.net/) website and you can get 79+ MOTA (For 80+ MOTA, you need to carefully tune the test image size and high score detection threshold of each sequence).\n\n* **Test on MOT20**\n\nWe use the input size 1600 x 896 for MOT20-04, MOT20-07 and 1920 x 736 for MOT20-06, MOT20-08. You can edit it in [yolox_x_mix_mot20_ch.py](https://github.com/ifzhang/ByteTrack/blob/main/exps/example/mot/yolox_x_mix_mot20_ch.py)\n\nRun ByteTrack:\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/track.py -f exps/example/mot/yolox_x_mix_mot20_ch.py -c pretrained/bytetrack_x_mot20.pth.tar -b 1 -d 1 --fp16 --fuse --match_thresh 0.7 --mot20\npython3 tools/interpolation.py\n```\nSubmit the txt files to [MOTChallenge](https://motchallenge.net/) website and you can get 77+ MOTA (For higher MOTA, you need to carefully tune the test image size and high score detection threshold of each sequence).\n\n## Applying BYTE to other trackers\n\nSee [tutorials](https://github.com/ifzhang/ByteTrack/tree/main/tutorials).\n\n## Combining BYTE with other detectors\n\nSuppose you have already got the detection results 'dets' (x1, y1, x2, y2, score) from other detectors, you can simply pass the detection results to BYTETracker (you need to first modify some post-processing code according to the format of your detection results in [byte_tracker.py](https://github.com/ifzhang/ByteTrack/blob/main/yolox/tracker/byte_tracker.py)):\n\n```\nfrom yolox.tracker.byte_tracker import BYTETracker\ntracker = BYTETracker(args)\nfor image in images:\n   dets = detector(image)\n   online_targets = tracker.update(dets, info_imgs, img_size)\n```\n\nYou can get the tracking results in each frame from 'online_targets'. You can refer to [mot_evaluators.py](https://github.com/ifzhang/ByteTrack/blob/main/yolox/evaluators/mot_evaluator.py) to pass the detection results to BYTETracker.\n\n## Demo\n\n<img src=\"assets/palace_demo.gif\" width=\"600\"/>\n\n```shell\ncd <ByteTrack_HOME>\npython3 tools/demo_track.py video -f exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar --fp16 --fuse --save_result\n```\n\n## Deploy\n\n1.  [ONNX export and ONNXRuntime](./deploy/ONNXRuntime)\n2.  [TensorRT in Python](./deploy/TensorRT/python)\n3.  [TensorRT in C++](./deploy/TensorRT/cpp)\n4.  [ncnn in C++](./deploy/ncnn/cpp)\n5.  [Deepstream](./deploy/DeepStream)\n\n## Citation\n\n```\n@article{zhang2022bytetrack,\n  title={ByteTrack: Multi-Object Tracking by Associating Every Detection Box},\n  author={Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},\n  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n  year={2022}\n}\n```\n\n## Acknowledgement\n\nA large part of the code is borrowed from [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX), [FairMOT](https://github.com/ifzhang/FairMOT), [TransTrack](https://github.com/PeizeSun/TransTrack) and [JDE-Cpp](https://github.com/samylee/Towards-Realtime-MOT-Cpp). Many thanks for their wonderful works.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "deploy",
          "type": "tree",
          "content": null
        },
        {
          "name": "exps",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.25390625,
          "content": "# TODO: Update with exact module version\nnumpy\ntorch>=1.7\nopencv_python\nloguru\nscikit-image\ntqdm\ntorchvision>=0.10.0\nPillow\nthop\nninja\ntabulate\ntensorboard\nlap\nmotmetrics\nfilterpy\nh5py\n\n# verified versions\nonnx==1.8.1\nonnxruntime==1.8.0\nonnx-simplifier==0.3.5\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.6005859375,
          "content": "[isort]\nline_length = 100\nmulti_line_output = 3\nbalanced_wrapping = True\nknown_standard_library = setuptools\nknown_third_party = tqdm,loguru\nknown_data_processing = cv2,numpy,scipy,PIL,matplotlib,scikit_image\nknown_datasets = pycocotools\nknown_deeplearning = torch,torchvision,caffe2,onnx,apex,timm,thop,torch2trt,tensorrt,openvino,onnxruntime\nknown_myself = yolox\nsections = FUTURE,STDLIB,THIRDPARTY,data_processing,datasets,deeplearning,myself,FIRSTPARTY,LOCALFOLDER\nno_lines_before=STDLIB,THIRDPARTY,datasets\ndefault_section = FIRSTPARTY\n\n[flake8]\nmax-line-length = 100\nmax-complexity = 18\nexclude = __init__.py\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.630859375,
          "content": "#!/usr/bin/env python\n# Copyright (c) Megvii, Inc. and its affiliates. All Rights Reserved\n\nimport re\nimport setuptools\nimport glob\nfrom os import path\nimport torch\nfrom torch.utils.cpp_extension import CppExtension\n\ntorch_ver = [int(x) for x in torch.__version__.split(\".\")[:2]]\nassert torch_ver >= [1, 3], \"Requires PyTorch >= 1.3\"\n\n\ndef get_extensions():\n    this_dir = path.dirname(path.abspath(__file__))\n    extensions_dir = path.join(this_dir, \"yolox\", \"layers\", \"csrc\")\n\n    main_source = path.join(extensions_dir, \"vision.cpp\")\n    sources = glob.glob(path.join(extensions_dir, \"**\", \"*.cpp\"))\n\n    sources = [main_source] + sources\n    extension = CppExtension\n\n    extra_compile_args = {\"cxx\": [\"-O3\"]}\n    define_macros = []\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            \"yolox._C\",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n\n\nwith open(\"yolox/__init__.py\", \"r\") as f:\n    version = re.search(\n        r'^__version__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]',\n        f.read(), re.MULTILINE\n    ).group(1)\n\n\nwith open(\"README.md\", \"r\") as f:\n    long_description = f.read()\n\n\nsetuptools.setup(\n    name=\"yolox\",\n    version=version,\n    author=\"basedet team\",\n    python_requires=\">=3.6\",\n    long_description=long_description,\n    ext_modules=get_extensions(),\n    classifiers=[\"Programming Language :: Python :: 3\", \"Operating System :: OS Independent\"],\n    cmdclass={\"build_ext\": torch.utils.cpp_extension.BuildExtension},\n    packages=setuptools.find_namespace_packages(),\n)\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorials",
          "type": "tree",
          "content": null
        },
        {
          "name": "videos",
          "type": "tree",
          "content": null
        },
        {
          "name": "yolox",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}