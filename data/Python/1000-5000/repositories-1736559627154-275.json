{
  "metadata": {
    "timestamp": 1736559627154,
    "page": 275,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "tensorflow/datasets",
      "stars": 4331,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3125,
          "content": "# Compiled python modules.\n*.pyc\n\n# Byte-compiled\n_pycache__/\n.cache/\n\n# Python egg metadata, regenerated from source files by setuptools.\n/*.egg-info\n.eggs/\n\n# PyPI distribution artifacts.\nbuild/\ndist/\n\n# Tests\n.pytest_cache/\n\n# Type checking\n.pytype/\n\n# Other\n*.DS_Store\n\n# PyCharm\n.idea\n\n# Visual Studio Code\n.vscode\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 14.5458984375,
          "content": "# This Pylint rcfile contains a best-effort configuration to uphold the\n# best-practices and style described in the Google Python style guide:\n#   https://google.github.io/styleguide/pyguide.html\n#\n# Its canonical open-source location is:\n#   https://google.github.io/styleguide/pylintrc\n\n[MASTER]\n\n# Add files or directories to the blacklist. They should be base names, not\n# paths.\nignore=third_party\n\n# Add files or directories matching the regex patterns to the blacklist. The\n# regex matches against base names, not paths.\nignore-patterns=\n\n# Pickle collected data for later comparisons.\npersistent=no\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Use multiple processes to speed up Pylint.\njobs=4\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n# A comma-separated list of package or module names from where C extensions may\n# be loaded. Extensions are loading into the active Python interpreter and may\n# run arbitrary code\nextension-pkg-whitelist=\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED\nconfidence=\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).You can also use \"--disable=all\" to\n# disable everything first and then reenable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use\"--disable=all --enable=classes\n# --disable=W\"\ndisable=abstract-method,\n        apply-builtin,\n        arguments-differ,\n        attribute-defined-outside-init,\n        backtick,\n        bad-option-value,\n        basestring-builtin,\n        buffer-builtin,\n        c-extension-no-member,\n        consider-using-enumerate,\n        cmp-builtin,\n        cmp-method,\n        coerce-builtin,\n        coerce-method,\n        delslice-method,\n        div-method,\n        duplicate-code,\n        eq-without-hash,\n        execfile-builtin,\n        file-builtin,\n        filter-builtin-not-iterating,\n        fixme,\n        getslice-method,\n        global-statement,\n        hex-method,\n        idiv-method,\n        implicit-str-concat-in-sequence,\n        import-error,\n        import-self,\n        import-star-module-level,\n        inconsistent-return-statements,\n        input-builtin,\n        intern-builtin,\n        invalid-str-codec,\n        locally-disabled,\n        long-builtin,\n        long-suffix,\n        map-builtin-not-iterating,\n        misplaced-comparison-constant,\n        missing-function-docstring,\n        metaclass-assignment,\n        next-method-called,\n        next-method-defined,\n        no-absolute-import,\n        no-else-break,\n        no-else-continue,\n        no-else-raise,\n        no-else-return,\n        no-init,  # added\n        no-member,\n        no-name-in-module,\n        no-self-use,\n        nonzero-method,\n        oct-method,\n        old-division,\n        old-ne-operator,\n        old-octal-literal,\n        old-raise-syntax,\n        parameter-unpacking,\n        print-statement,\n        raising-string,\n        range-builtin-not-iterating,\n        raw_input-builtin,\n        rdiv-method,\n        reduce-builtin,\n        relative-import,\n        reload-builtin,\n        round-builtin,\n        setslice-method,\n        signature-differs,\n        standarderror-builtin,\n        suppressed-message,\n        sys-max-int,\n        too-few-public-methods,\n        too-many-ancestors,\n        too-many-arguments,\n        too-many-boolean-expressions,\n        too-many-branches,\n        too-many-instance-attributes,\n        too-many-locals,\n        too-many-nested-blocks,\n        too-many-public-methods,\n        too-many-return-statements,\n        too-many-statements,\n        trailing-newlines,\n        unichr-builtin,\n        unicode-builtin,\n        unnecessary-pass,\n        unpacking-in-except,\n        useless-else-on-loop,\n        useless-object-inheritance,\n        useless-suppression,\n        using-cmp-argument,\n        wrong-import-order,\n        xrange-builtin,\n        zip-builtin-not-iterating,\n\n\n[REPORTS]\n\n# Set the output format. Available formats are text, parseable, colorized, msvs\n# (visual studio) and html. You can also give a reporter class, eg\n# mypackage.mymodule.MyReporterClass.\noutput-format=text\n\n# Put messages in a separate file for each module / package specified on the\n# command line instead of printing them on stdout. Reports (if any) will be\n# written in a file name \"pylint_global.[txt|html]\". This option is deprecated\n# and it will be removed in Pylint 2.0.\nfiles-output=no\n\n# Tells whether to display a full report or only the messages\nreports=no\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\n#msg-template=\n\n\n[BASIC]\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=main,_\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Include a hint for the correct naming format with invalid-name\ninclude-naming-hint=no\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\nproperty-classes=abc.abstractproperty,cached_property.cached_property,cached_property.threaded_cached_property,cached_property.cached_property_with_ttl,cached_property.threaded_cached_property_with_ttl\n\n# Regular expression matching correct function names\nfunction-rgx=^(?:(?P<exempt>setUp|tearDown|setUpModule|tearDownModule)|(?P<camel_case>_?[A-Z][a-zA-Z0-9]*)|(?P<snake_case>_?[a-z][a-z0-9_]*))$\n\n# Regular expression matching correct variable names\nvariable-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct constant names\nconst-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct attribute names\nattr-rgx=^_{0,2}[a-z][a-z0-9_]*$\n\n# Regular expression matching correct argument names\nargument-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class attribute names\nclass-attribute-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct inline iteration names\ninlinevar-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class names\nclass-rgx=^_?[A-Z][a-zA-Z0-9]*$\n\n# Regular expression matching correct module names\nmodule-rgx=^(_?[a-z][a-z0-9_]*|__init__)$\n\n# Regular expression matching correct method names\nmethod-rgx=(?x)^(?:(?P<exempt>_[a-z0-9_]+__|runTest|setUp|tearDown|setUpTestCase|tearDownTestCase|setupSelf|tearDownClass|setUpClass|(test|assert)_*[A-Z0-9][a-zA-Z0-9_]*|next)|(?P<camel_case>_{0,2}[A-Z][a-zA-Z0-9_]*)|(?P<snake_case>_{0,2}[a-z][a-z0-9_]*))$\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=(__.*__|main|test.*|.*test|.*Test)$\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=10\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager,contextlib2.contextmanager\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis. It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n\n[FORMAT]\n\n# Maximum number of characters on a single line.\nmax-line-length=80\n\n# TODO(https://github.com/PyCQA/pylint/issues/3352): Direct pylint to exempt\n# lines made too long by directives to pytype.\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=(?x)(\n  ^\\s*(\\#\\ )?<?https?://\\S+>?$|\n  ^\\s*(from\\s+\\S+\\s+)?import\\s+.+$)\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=yes\n\n# List of optional constructs for which whitespace checking is disabled. `dict-\n# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\\n222: 2}.\n# `trailing-comma` allows a space between comma and closing bracket: (a, ).\n# `empty-line` allows space-only lines.\nno-space-check=\n\n# Maximum number of lines in a module\nmax-module-lines=99999\n\n# String used as indentation unit.  The internal Google style guide mandates 2\n# spaces.  Google's externaly-published style guide says 4, consistent with\n# PEP 8.  Here, we use 2 spaces, for conformity with many open-sourced Google\n# projects (like TensorFlow).\nindent-string='  '\n\n# Number of spaces of indent required inside a hanging  or continued line.\nindent-after-paren=4\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=TODO\n\n\n[STRING]\n\n# This flag controls whether inconsistent-quotes generates a warning when the\n# character used as a quote delimiter is used inconsistently within a module.\ncheck-quote-consistency=yes\n\n\n[VARIABLES]\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# A regular expression matching the name of dummy variables (i.e. expectedly\n# not used).\ndummy-variables-rgx=^\\*{0,2}(_$|unused_|dummy_)\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,_cb\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six,six.moves,past.builtins,future.builtins,functools\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format\nlogging-modules=logging,absl.logging,tensorflow.io.logging\n\n\n[SIMILARITIES]\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n\n[SPELLING]\n\n# Spelling dictionary name. Available dictionaries: none. To make it working\n# install python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[IMPORTS]\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=regsub,\n                   TERMIOS,\n                   Bastion,\n                   rexec,\n                   sets\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant, absl\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n# List of modules that can be imported at any level, not just the top level\n# one.\nallow-any-import-level=tensorflow\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,\n                  _fields,\n                  _replace,\n                  _source,\n                  _make\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls,\n                            class_\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=StandardError,\n                       Exception,\n                       BaseException\n"
        },
        {
          "name": ".style.yapf",
          "type": "blob",
          "size": 0.1015625,
          "content": "[style]\nbased_on_style = google\nindent_width = 2\ndedent_closing_brackets = True\nsplit_before_dot = True\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.3017578125,
          "content": "# This is the list of TensorFlow Datasets authors for copyright purposes.\n#\n# This does not necessarily list everyone who has contributed code, since in\n# some cases, their employer may be the copyright holder.  To see the full list\n# of contributors, see the revision history in source control.\n\nGoogle Inc.\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 35.591796875,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to\n[Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n\n### Changed\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n### Security\n\n## [4.9.7] - 2024-10-30\n\n### Added\n\n- New datasets.\n\n### Changed\n\n- `CroissantBuilder`'s API to generate TFDS datasets from Croissant files.\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n- Versions for existing datasets.\n\n### Security\n\n## [4.9.6] - 2024-06-04\n\n### Added\n\n-   Full support for Python 3.12.\n\n## [4.9.5] - 2024-05-30\n\n### Added\n\n-   Support to download and prepare datasets using the\n    [Parquet](https://parquet.apache.org) data format.\n    ```python\n    builder = tfds.builder('fashion_mnist', file_format='parquet')\n    builder.download_and_prepare()\n    ds = builder.as_dataset(split='train')\n    print(next(iter(ds)))\n    ```\n\n-   [`tfds.data_source`](https://www.tensorflow.org/datasets/api_docs/python/tfds/data_source)\n    is pickable, thus working smoothly with\n    [PyGrain](https://github.com/google/grain). Learn more by following the\n    [tutorial](https://www.tensorflow.org/datasets/data_source).\n\n-   TFDS plays nicely with\n    [Croissant](https://mlcommons.org/working-groups/croissant). Learn more by\n    following the\n    [recipe](https://colab.research.google.com/github/mlcommons/croissant/blob/main/python/mlcroissant/recipes/tfds_croissant_builder.ipynb).\n\n### Changed\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n### Security\n\n## [4.9.4] - 2023-12-16\n\n### Added\n\n-   A new [CroissantBuilder](https://www.tensorflow.org/datasets/format_specific_dataset_builders#croissantbuilder)\n    which initializes a DatasetBuilder based on a [Croissant](https://github.com/mlcommons/croissant)\n    metadata file.\n-   New conversion options between different bounding boxes formats.\n-   Better support for `HuggingfaceDatasetBuilder`.\n-   A [script](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/scripts/convert_format.py)\n    to convert a dataset from one format to another.\n\n### Changed\n\n### Deprecated\n\n-   Python 3.9 support. TFDS now uses Python 3.10\n\n### Removed\n\n### Fixed\n\n### Security\n\n## [4.9.3] - 2023-09-08\n\n### Added\n\n-   [Segment Anything](https://ai.facebook.com/datasets/segment-anything-downloads)\n    (SA-1B) dataset.\n\n### Changed\n\n-   Hugging Face datasets accept `None` values for any features. TFDS has no\n    `tfds.features.Optional`, so `None` values are converted to default values.\n    Those default values used to be `0` and `0.0` for int and float. Now, it's\n    `-inf` as defined by NumPy (e.g., `np.iinfo(np.int32).min` or\n    `np.finfo(np.float32).min`). This avoids ambiguous values when `0` and `0.0`\n    exist in the values of the dataset. The roadmap is to implement\n    `tfds.features.Optional`.\n\n### Deprecated\n\n-   Python 3.8 support. As per\n    [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html), TFDS now\n    uses Python>=3.9.\n\n### Removed\n\n### Fixed\n\n### Security\n\n## [4.9.2] - 2023-04-13\n\n### Added\n\n-   [Experimental] A list of freeform text tags can now be attached to a\n    `BuilderConfig`. For example:\n    ```py\n    BUILDER_CONFIGS = [\n        tfds.core.BuilderConfig(name=\"foo\", tags=[\"foo\", \"live\"]),\n        tfds.core.BuilderConfig(name=\"bar\", tags=[\"bar\", \"old\"]),\n    ]\n    ```\n    The tags are recorded with the dataset metadata and can later be retrieved\n    using the info object:\n    ```py\n    builder.info.config_tags  # [\"foo\", \"live\"]\n    ```\n    This feature is experimental and there are no guidelines on tags format.\n\n### Changed\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n-   Fixed generated proto files (see issue [4858](https://github.com/tensorflow/datasets/issues/4858)).\n\n### Security\n\n## [4.9.1] - 2023-04-11\n\n### Added\n\n### Changed\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n-   The installation on macOS now works (see issues\n    [4805](https://github.com/tensorflow/datasets/issues/4805) and\n    [4852](https://github.com/tensorflow/datasets/issues/4852)). The ArrayRecord\n    dependency is lazily loaded, so the\n    [TensorFlow-less path](https://www.tensorflow.org/datasets/tfless_tfds) is\n    not possible at the moment on macOS. A fix for this will follow soon.\n\n### Security\n\n## [4.9.0] - 2023-04-04\n\n### Added\n\n-   Native support for JAX and PyTorch. TensorFlow is no longer a dependency for\n    reading datasets. See the\n    [documentation](https://www.tensorflow.org/datasets/tfless_tfds).\n-   Added minival split to\n    [LVIS dataset](https://www.tensorflow.org/datasets/catalog/lvis).\n-   [Mixed-human](https://www.tensorflow.org/datasets/catalog/robomimic_mh) and\n    [machine-generated](https://www.tensorflow.org/datasets/catalog/robomimic_mg)\n    robomimic datasets.\n-   WebVid dataset.\n-   ImagenetPI dataset.\n-   [Wikipedia](https://www.tensorflow.org/datasets/catalog/wikipedia) for\n    20230201.\n\n### Changed\n\n-   Support for `tensorflow=2.12`.\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n### Security\n\n## [4.8.3] - 2023-02-27\n\n### Added\n\n### Changed\n\n### Deprecated\n\n-   Python 3.7 support: this version and future version use Python 3.8.\n\n### Removed\n\n### Fixed\n\n-   Flag `ignore_verifications` from Hugging Face's `datasets.load_dataset` is\n    deprecated, and used to cause errors in `tfds.load(huggingface:foo)`.\n\n### Security\n\n## [4.8.2] - 2023-01-17\n\n### Deprecated\n\n-   Python 3.7 support: this is the last version of TFDS supporting Python 3.7.\n    Future versions will use Python 3.8.\n\n### Fixed\n\n-   `tfds new` and `tfds build` better support the new recommended datasets\n    organization, where individual datasets have their own package under\n    `datasets/`, builder class is called `Builder` and is defined within module\n    `${dsname}_dataset_builder.py`.\n\n### Security\n\n## [4.8.1] - 2023-01-02\n\n### Changed\n\n- Added file `valid_tags.txt` to not break builds.\n- TFDS no longer relies on TensorFlow DTypes. We chose NumPy DTypes to keep the\ntyping expressiveness, while dropping the heavy dependency on TensorFlow. We\nmigrated all our internal datasets. Please, migrate accordingly:\n    - `tf.bool`: `np.bool_`\n    - `tf.string`: `np.str_`\n    - `tf.int64`, `tf.int32`, etc: `np.int64`, `np.int32`, etc\n    - `tf.float64`, `tf.float32`, etc: `np.float64`, `np.float32`, etc\n\n\n## [4.8.0] - 2022-12-21\n\n### Added\n\n-   [API] `DatasetBuilder`'s description and citations can be specified in\n    dedicated `README.md` and `CITATIONS.bib` files, within the dataset package\n    (see https://www.tensorflow.org/datasets/add_dataset).\n-   Tags can be associated to Datasets, in the `TAGS.txt` file. For\n    now, they are only used in the generated documentation.\n-   [API][Experimental] New `ViewBuilder` to define datasets as transformations\n    of existing datasets. Also adds `tfds.transform` with functionality to apply\n    transformations.\n-   Loggers are also called on `tfds.as_numpy(...)`, base `Logger` class has a\n    new corresponding method.\n-   `tfds.core.DatasetBuilder` can have a default limit for the number of\n    simultaneous downloads. `tfds.download.DownloadConfig` can override it.\n-   `tfds.features.Audio` supports storing raw audio data for lazy decoding.\n-   The number of shards can be overridden when preparing a dataset:\n    `builder.download_and_prepare(download_config=tfds.download.DownloadConfig(num_shards=42))`.\n    Alternatively, you can configure the min and max shard size if you want TFDS\n    to compute the number of shards for you, but want to have control over the\n    shard sizes.\n\n### Changed\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n### Security\n\n## [4.7.0] - 2022-10-04\n\n### Added\n\n-   [API] Added\n    [TfDataBuilder](https://www.tensorflow.org/datasets/format_specific_dataset_builders#datasets_based_on_tfdatadataset)\n    that is handy for storing experimental ad hoc TFDS datasets in notebook-like\n    environments such that they can be versioned, described, and easily shared\n    with teammates.\n-   [API] Added options to create format-specific dataset builders. The new API\n    now includes a number of NLP-specific builders, such as:\n    -   [CoNNL](https://www.tensorflow.org/datasets/format_specific_dataset_builders#conll)\n    -   [CoNNL-U](https://www.tensorflow.org/datasets/format_specific_dataset_builders#conllu)\n-   [API] Added `tfds.beam.inc_counter` to reduce `beam.metrics.Metrics.counter`\n    boilerplate\n-   [API] Added options to group together existing TFDS datasets into\n    [dataset collections](https://www.tensorflow.org/datasets/dataset_collections)\n    and to perform simple operations over them.\n-   [Documentation] update, specifically:\n    -   [New guide](https://www.tensorflow.org/datasets/format_specific_dataset_builders)\n        on format-specific dataset builders;\n    -   [New guide](https://www.tensorflow.org/datasets/add_dataset_collection)\n        on adding new dataset collections to TFDS;\n    -   Updated [TFDS CLI](https://www.tensorflow.org/datasets/cli)\n        documentation.\n-   [TFDS CLI] Supports custom config through Json (e.g. `tfds build my_dataset\n    --config='{\"name\": \"my_custom_config\", \"description\": \"Abc\"}'`)\n-   New datasets:\n    -   [conll2003](https://www.tensorflow.org/datasets/catalog/conll2003)\n    -   [universal_dependency 2.10](https://www.tensorflow.org/datasets/catalog/universal_dependency)\n    -   [bucc](https://www.tensorflow.org/datasets/catalog/bucc)\n    -   [i_naturalist2021](https://www.tensorflow.org/datasets/catalog/i_naturalist2021)\n    -   [mtnt](https://www.tensorflow.org/datasets/catalog/mtnt) Machine\n        Translation of Noisy Text.\n    -   [placesfull](https://www.tensorflow.org/datasets/catalog/placesfull)\n    -   [tatoeba](https://www.tensorflow.org/datasets/catalog/tatoeba)\n    -   [user_libri_audio](https://www.tensorflow.org/datasets/catalog/user_libri_audio)\n    -   [user_libri_text](https://www.tensorflow.org/datasets/catalog/user_libri_text)\n    -   [xtreme_pos](https://www.tensorflow.org/datasets/catalog/xtreme_pos)\n    -   [yahoo_ltrc](https://www.tensorflow.org/datasets/catalog/yahoo_ltrc)\n-   Updated datasets:\n    -   [C4](https://www.tensorflow.org/datasets/catalog/c4) was updated to\n        version 3.1.\n    -   [common_voice](https://www.tensorflow.org/datasets/catalog/common_voice)\n        was updated to a more recent snapshot.\n    -   [wikipedia](https://www.tensorflow.org/datasets/catalog/wikipedia) was\n        updated with the `20220620` snapshot.\n-   New dataset collections, such as\n    [xtreme](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/dataset_collections/xtreme/xtreme.py)\n    and\n    [LongT5](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/dataset_collections/longt5/longt5.py)\n\n### Changed\n\n-   The base `Logger` class expects more information to be passed to the\n    `as_dataset` method. This should only be relevant to people who have\n    implemented and registered custom `Logger` class(es).\n-   You can set `DEFAULT_BUILDER_CONFIG_NAME` in a `DatasetBuilder` to change\n    the default config if it shouldn't be the first builder config defined in\n    `BUILDER_CONFIGS`.\n\n### Deprecated\n\n### Removed\n\n### Fixed\n\n-   Various datasets\n-   In Linux, when loading a dataset from a directory that is not your home\n    (`~`) directory, a new `~` directory is not created in the current directory\n    (fixes [#4117](https://github.com/tensorflow/datasets/issues/4117)).\n\n### Security\n\n## [4.6.0] - 2022-06-01\n\n### Added\n\n-   Support for community datasets on GCS.\n-   [API] `tfds.builder_from_directory` and `tfds.builder_from_directories`, see\n    https://www.tensorflow.org/datasets/external_tfrecord#directly_from_folder.\n-   [API] Dash (\"-\") support in split names.\n-   [API] `file_format` argument to `download_and_prepare` method, allowing user\n    to specify an alternative file format to store prepared data (e.g.\n    \"riegeli\").\n-   [API] `file_format` to `DatasetInfo` string representation.\n-   [API] Expose the return value of Beam pipelines. This allows for users to\n    read the Beam metrics.\n-   [API] Expose Feature `tf_example_spec` to public.\n-   [API] `doc` kwarg on `Feature`s, to describe a feature.\n-   [Documentation] Features description is shown on\n    [TFDS Catalog](https://www.tensorflow.org/datasets/catalog/overview).\n-   [Documentation] More metadata about HuggingFace datasets in TFDS catalog.\n-   [Performance] Parallel load of metadata files.\n-   [Testing] TFDS tests are now run using GitHub actions - misc improvements\n    such as caching and sharding.\n-   [Testing] Improvements to MockFs.\n-   New datasets.\n\n### Changed\n\n-   [API] `num_shards` is now optional in the shard name.\n\n### Removed\n\n-   TFDS pathlib API, migrated to a self-contained `etils.epath` (see\n    https://github.com/google/etils).\n\n### Fixed\n\n-   Various datasets.\n-   Dataset builders that are defined adhoc (e.g. in Colab).\n-   Better `DatasetNotFoundError` messages.\n-   Don't set `deterministic` on a global level but locally in interleave, so it\n    only apply to interleave and not all transformations.\n-   Google drive downloader.\n\n## [4.5.2] - 2022-01-31\n\n### Added\n\n-   [API] `split=tfds.split_for_jax_process('train')` (alias of\n    `tfds.even_splits('train', n=jax.process_count())[jax.process_index()]`).\n-   [Documentation] update.\n\n### Fixed\n\n-   Import bug on Windows (#3709).\n\n## [4.5.0] - 2022-01-25\n\n### Added\n\n-   [API] Better split API:\n    -   Splits can be selected using shards: `split='train[3shard]'`.\n    -   Underscore supported in numbers for better readability:\n        `split='train[:500_000]'`.\n    -   Select the union of all splits with `split='all'`.\n    -   [`tfds.even_splits`](https://www.tensorflow.org/datasets/splits#tfdseven_splits_multi-host_training)\n        is more precise and flexible:\n    -   Return splits exactly of the same size when passed\n        `tfds.even_splits('train', n=3, drop_remainder=True)`.\n    -   Works on subsplits `tfds.even_splits('train[:75%]', n=3)` or even\n        nested.\n    -   Can be composed with other splits: `tfds.even_splits('train', n=3)[0] +\n        'test'`.\n-   [API] `serialize_example` / `deserialize_example` methods on features to\n    encode/decode example to proto: `example_bytes =\n    features.serialize_example(example_data)`.\n-   [API] `Audio` feature now supports `encoding='zlib'` for better compression.\n-   [API] Features specs are exposed in proto for better compatibility with\n    other languages.\n-   [API] Create beam pipeline using TFDS as input with\n    [tfds.beam.ReadFromTFDS](https://www.tensorflow.org/datasets/api_docs/python/tfds/beam/ReadFromTFDS).\n-   [API] Support setting the file formats in `tfds build\n    --file_format=tfrecord`.\n-   [API] Typing annotations exposed in `tfds.typing`.\n-   [API] `tfds.ReadConfig` has a new `assert_cardinality=False` argument to\n    disable cardinality.\n-   [API] `tfds.display_progress_bar(True)` for functional control.\n-   [API] DatasetInfo exposes `.release_notes`.\n-   Support for huge number of shards (>99999).\n-   [Performance] Faster dataset generation (using tfrecords).\n-   [Testing] Mock dataset now supports nested datasets\n-   [Testing] Customize the number of sub examples\n-   [Documentation] Community datasets:\n    https://www.tensorflow.org/datasets/community_catalog/overview.\n-   [Documentation]\n    [Guide on TFDS and determinism](https://www.tensorflow.org/datasets/determinism).\n-   [[RLDS](https://github.com/google-research/rlds)] Support for nested\n    datasets features.\n-   [[RLDS](https://github.com/google-research/rlds)] New datasets: Robomimic,\n    D4RL Ant Maze, RLU Real World RL, and RLU Atari with ordered episodes.\n-   New datasets.\n\n### Deprecated\n\n-   Python 3.6 support: this is the last version of TFDS supporting Python 3.6.\n    Future versions will use Python 3.7.\n\n### Fixed\n\n-   Misc bugs.\n\n## [4.4.0] - 2021-07-28\n\n### Added\n\n-   [API]\n    [`PartialDecoding` support](https://www.tensorflow.org/datasets/decode#only_decode_a_sub-set_of_the_features),\n    to decode only a subset of the features (for performances).\n-   [API] `tfds.features.LabeledImage` for semantic segmentation (like image but\n    with additional `info.features['image_label'].name` label metadata).\n-   [API] float32 support for `tfds.features.Image` (e.g. for depth map).\n-   [API] Loading datasets from files now supports custom\n    `tfds.features.FeatureConnector`.\n-   [API] All FeatureConnector can now have a `None` dimension anywhere\n    (previously restricted to the first position).\n-   [API] `tfds.features.Tensor()` can have arbitrary number of dynamic\n    dimension (`Tensor(..., shape=(None, None, 3, None)`)).\n-   [API] `tfds.features.Tensor` can now be serialised as bytes, instead of\n    float/int values (to allow better compression): `Tensor(...,\n    encoding='zlib')`.\n-   [API] Support for datasets with `None` in `tfds.as_numpy`.\n-   Script to add TFDS metadata files to existing TF-record (see\n    [doc](https://www.tensorflow.org/datasets/external_tfrecord)).\n-   [TESTING] `tfds.testing.mock_data` now supports:\n    -   non-scalar tensors with dtype `tf.string`;\n    -   `builder_from_files` and path-based community datasets.\n-   [Documentation] Catalog now exposes links to\n    [KnowYourData visualisations](https://knowyourdata-tfds.withgoogle.com/).\n-   [Documentation] Guide on\n    [common implementation gotchas](https://www.tensorflow.org/datasets/common_gotchas).\n-   Many new reinforcement learning datasets. ### Changed\n-   [API] Dataset generated with `disable_shuffling=True` are now read in\n    generation order.\n\n### Fixed\n\n-   File format automatically restored (for datasets generated with\n    `tfds.builder(..., file_format=)`).\n-   Dynamically set number of worker threads during extraction.\n-   Update progress bar during download even if downloads are cached.\n-   Misc bug fixes.\n\n## [4.3.0] - 2021-05-06\n\n### Added\n\n-   [API] `dataset.info.splits['train'].num_shards` to expose the number of\n    shards to the user.\n-   [API] `tfds.features.Dataset` to have a field containing sub-datasets (e.g.\n    used in RL datasets).\n-   [API] dtype and `tf.uint16` support in `tfds.features.Video`.\n-   [API] `DatasetInfo.license` field to add redistributing information.\n-   [API] `.copy`, `.format` methods to GPath objects.\n-   [Performances] `tfds.benchmark(ds)` (compatible with any iterator, not just\n    `tf.data`, better colab representation).\n-   [Performances] Faster `tfds.as_numpy()` (avoid extra `tf.Tensor` <>\n    `np.array` copy).\n-   [Testing] Support for custom `BuilderConfig` in `DatasetBuilderTest`.\n-   [Testing] `DatasetBuilderTest` now has a `dummy_data` class property which\n    can be used in `setUpClass`.\n-   [Testing] `add_tfds_id` and cardinality support to `tfds.testing.mock_data`.\n-   [Documentation] Better `tfds.as_dataframe` visualisation (Sequence, ragged\n    tensor, semantic masks with `use_colormap`).\n-   [Experimental] Community datasets support. To allow dynamically import\n    datasets defined outside the TFDS repository.\n-   [Experimental] Hugging-face compatibility wrapper to use Hugging-face\n    datasets directly in TFDS.\n-   [Experimental] Riegeli format support.\n-   [Experimental] `DatasetInfo.disable_shuffling` to force examples to be read\n    in generation order.\n-   New datasets.\n\n### Fixed\n\n-   Many bugs.\n\n## [4.2.0] - 2021-01-06\n\n### Added\n\n-   [CLI] `tfds build` to the CLI. See\n    [documentation](https://www.tensorflow.org/datasets/cli#tfds_build_download_and_prepare_a_dataset).\n-   [API] `tfds.features.Dataset` to represent nested datasets.\n-   [API] `tfds.ReadConfig(add_tfds_id=True)` to add a unique id to the example\n    `ex['tfds_id']` (e.g. `b'train.tfrecord-00012-of-01024__123'`).\n-   [API] `num_parallel_calls` option to `tfds.ReadConfig` to overwrite to\n    default `AUTOTUNE` option.\n-   [API] `tfds.ImageFolder` support for `tfds.decode.SkipDecoder`.\n-   [API] Multichannel audio support to `tfds.features.Audio`.\n-   [API] `try_gcs` to `tfds.builder(..., try_gcs=True)`\n-   Better `tfds.as_dataframe` visualization (ffmpeg video if installed,\n    bounding boxes,...).\n-   [TESTING] Allow `max_examples_per_splits=0` in `tfds build\n    --max_examples_per_splits=0` to test `_split_generators` only (without\n    `_generate_examples`).\n-   New datasets.\n\n### Changed\n\n-   [API] DownloadManager now returns\n    [Pathlib-like](https://docs.python.org/3/library/pathlib.html#basic-use)\n    objects.\n-   [API] Simpler `BuilderConfig` definition: class `VERSION` and\n    `RELEASE_NOTES` are applied to all `BuilderConfig`. Config description is\n    now optional.\n-   [API] To guarantee better deterministic, new validations are performed on\n    the keys when creating a dataset (to avoid filenames as keys\n    (non-deterministic) and restrict key to `str`, `bytes` and `int`). New\n    errors likely indicates an issue in the dataset implementation.\n-   [API] `tfds.core.benchmark` now returns a `pd.DataFrame` (instead of a\n    `dict`).\n-   [API] `tfds.units` is not visible anymore from the public API.\n-   Datasets updates.\n\n### Deprecated\n\n### Removed\n\n-   Configs for all text datasets. Only plain text version is kept. For example:\n    `multi_nli/plain_text` -> `multi_nli`.\n\n### Fixed\n\n-   [API] Datasets returned by `tfds.as_numpy` are compatible with `len(ds)`.\n-   Support 0-len sequence with images of dynamic shape (Fix #2616).\n-   Progression bar correctly updated when copying files.\n-   Better debugging and error message (e.g. human readable size,...).\n-   Many bug fixes (GPath consistency with pathlib, s3 compatibility, TQDM\n    visual artifacts, GCS crash on windows, re-download when checksums updated,\n    ...).\n\n## [4.1.0] - 2020-11-04\n\n### Added\n\n-   It is now easier to create datasets outside TFDS repository (see our updated\n    [dataset creation guide](https://www.tensorflow.org/datasets/add_dataset)).\n-   When generating a dataset, if download fails for any reason, it is now\n    possible to manually download the data. See\n    [doc](https://www.tensorflow.org/datasets/overview#manual_download_if_download_fails).\n-   `tfds.core.as_path` to create pathlib.Path-like objects compatible with GCS\n    (e.g. `tfds.core.as_path('gs://my-bucket/labels.csv').read_text()`).\n-   `verify_ssl=` option to `tfds.download.DownloadConfig` to disable SSH\n    certificate during download.\n-   New datasets. ### Changed\n-   All dataset inherit from `tfds.core.GeneratorBasedBuilder`. Converting a\n    dataset to beam now only require changing `_generate_examples` (see\n    [example and doc](https://www.tensorflow.org/datasets/beam_datasets#instructions)).\n-   `_split_generators` should now returns `{'split_name':\n    self._generate_examples(), ...}` (but current datasets are backward\n    compatible).\n-   Better `pathlib.Path`, `os.PathLike` compatibility: `dl_manager.manual_dir`\n    now returns a pathlib-Like object. Example: `python text =\n    (dl_manager.manual_dir / 'downloaded-text.txt').read_text()` Note: Other\n    `dl_manager.download`, `.extract`,... will return pathlib-like objects in\n    future versions. `FeatureConnector`,... and most functions should accept\n    `PathLike` objects. Let us know if some functions you need are missing.\n-   `--record_checksums` now assume the new dataset-as-folder model.\n\n### Deprecated\n\n-   `tfds.core.SplitGenerator`, `tfds.core.BeamBasedBuilder` are deprecated and\n    will be removed in a future version.\n\n### Fixed\n\n-   `BuilderConfig` are now compatible with Beam datasets #2348\n-   `tfds.features.Images` can accept encoded `bytes` images directly (useful\n    when used with `img_name, img_bytes =\n    dl_manager.iter_archive('images.zip')`).\n-   Doc API now show deprecated methods, abstract methods to overwrite are now\n    documented.\n-   You can generate `imagenet2012` with only a single split (e.g. only the\n    validation data). Other split will be skipped if not present.\n\n## [4.0.1] - 2020-10-09\n\n### Fixed\n\n-   `tfds.load` when generation code isn't present.\n-   GCS compatibility.\n\n## [4.0.0] - 2020-10-06\n\n### Added\n\n-   Dataset-as-folder: Dataset can now be self-contained module in a folder with\n    checksums, dummy data,... This simplify implementing datasets outside the\n    TFDS repository.\n-   `tfds.load` can now load dataset without using the generation class. So\n    `tfds.load('my_dataset:1.0.0')` can work even if `MyDataset.VERSION ==\n    '2.0.0'` (See #2493).\n-   TFDS CLI (see https://www.tensorflow.org/datasets/cli for detail).\n-   `tfds.testing.mock_data` does not require metadata files anymore!\n-   `tfds.as_dataframe(ds, ds_info)` with custom visualisation\n    ([example](https://www.tensorflow.org/datasets/overview#tfdsas_dataframe)).\n-   `tfds.even_splits` to generate subsplits (e.g. `tfds.even_splits('train',\n    n=3) == ['train[0%:33%]', 'train[33%:67%]', ...]`.\n-   `DatasetBuilder.RELEASE_NOTES` property.\n-   `tfds.features.Image` now supports PNG with 4-channels.\n-   `tfds.ImageFolder` now supports custom shape, dtype.\n-   Downloaded URLs are available through `MyDataset.url_infos`.\n-   `skip_prefetch` option to `tfds.ReadConfig`.\n-   `as_supervised=True` support for `tfds.show_examples`, `tfds.as_dataframe`.\n-   tfds.features can now be saved/loaded, you may have to overwrite\n    [FeatureConnector.from_json_content](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeatureConnector?version=nightly#from_json_content)\n    and `FeatureConnector.to_json_content` to support this feature.\n-   Script to detect dead-urls.\n-   New datasets.\n\n### Changed\n\n-   `tfds.as_numpy()` now returns an iterable which can be iterated multiple\n    times. To migrate: `next(ds)` -> `next(iter(ds))`.\n-   Rename `tfds.features.text.Xyz` -> `tfds.deprecated.text.Xyz`.\n\n### Removed\n\n-   `DatasetBuilder.IN_DEVELOPMENT` property.\n-   `tfds.core.disallow_positional_args` (should use Py3 `*,` instead).\n-   Testing against TF 1.15. Requires Python 3.6.8+.\n\n### Fixed\n\n-   Better archive extension detection for `dl_manager.download_and_extract`.\n-   Fix `tfds.__version__` in TFDS nightly to be PEP440 compliant\n-   Fix crash when GCS not available.\n-   Improved open-source workflow, contributor guide, documentation.\n-   Many other internal cleanups, bugs, dead code removal, py2->py3 cleanup,\n    pytype annotations,...\n-   Datasets updates.\n\n## [3.2.1] - 2020-08-12\n\n### Fixed\n\n-   Issue with GCS on Windows.\n\n## [3.2.0] - 2020-07-10\n\n### Added\n\n-   [API] `tfds.ImageFolder` and `tfds.TranslateFolder` to easily create custom\n    datasets with your custom data.\n-   [API] `tfds.ReadConfig(input_context=)` to shard dataset, for better\n    multi-worker compatibility (#1426).\n-   [API] The default `data_dir` can be controlled by the `TFDS_DATA_DIR`\n    environment variable.\n-   [API] Better usability when developing datasets outside TFDS: downloads are\n    always cached, checksums are optional.\n-   Scripts to help deployment/documentation (Generate catalog documentation,\n    export all metadata files, ...).\n-   [Documentation] Catalog display images\n    ([example](https://www.tensorflow.org/datasets/catalog/sun397#sun397standard-part2-120k)).\n-   [Documentation] Catalog shows which dataset have been recently added and are\n    only available in `tfds-nightly`\n    <span class=\"material-icons\">nights_stay</span>.\n-   [API] `tfds.show_statistics(ds_info)` to display\n    [FACETS OVERVIEW](https://pair-code.github.io/facets/). Note: This require\n    the dataset to have been generated with the statistics.\n\n### Deprecated\n\n-   `tfds.features.text` encoding API. Please use\n    [tensorflow_text](https://www.tensorflow.org/tutorials/tensorflow_text/intro)\n    instead.\n\n### Removed\n\n-   `tfds.load('image_label_folder')` in favor of the more user-friendly\n    `tfds.ImageFolder`.\n\n### Fixed\n\n-   Fix deterministic example order on Windows when path was used as key (this\n    only impacts a few datasets). Now example order should be the same on all\n    platforms.\n-   Misc performances improvements for both generation and reading (e.g. use\n    `__slot__`, fix parallelisation bug in `tf.data.TFRecordReader`, ...).\n-   Misc fixes (typo, types annotations, better error messages, fixing dead\n    links, better windows compatibility, ...).\n\n## [3.1.0] - 2020-04-29\n\n### Added\n\n-   [API] `tfds.builder_cls(name)` to access a DatasetBuilder class by name\n-   [API] `info.split['train'].filenames` for access to the tf-record files.\n-   [API] `tfds.core.add_data_dir` to register an additional data dir.\n-   [Testing] Support for custom decoders in `tfds.testing.mock_data`.\n-   [Documentation] Shows which datasets are only present in `tfds-nightly`.\n-   [Documentation] Display images for supported datasets.\n\n### Changed\n\n-   Rename `tfds.core.NamedSplit`, `tfds.core.SplitBase` -> `tfds.Split`. Now\n    `tfds.Split.TRAIN`,... are instance of `tfds.Split`.\n-   Rename `interleave_parallel_reads` -> `interleave_cycle_length` for\n    `tfds.ReadConfig`.\n-   Invert ds, ds_info argument orders for `tfds.show_examples`.\n\n### Deprecated\n\n-   `tfds.features.text` encoding API. Please use `tensorflow_text` instead.\n\n### Removed\n\n-   `num_shards` argument from `tfds.core.SplitGenerator`. This argument was\n    ignored as shards are automatically computed.\n-   Most `ds.with_options` which where applied by TFDS. Now use `tf.data`\n    default.\n\n### Fixed\n\n-   Better error messages.\n-   Windows compatibility.\n\n## [3.0.0] - 2020-04-16\n\n### Added\n\n-   `DownloadManager` is now pickable (can be used inside Beam pipelines).\n-   `tfds.features.Audio`:\n    -   Support float as returned value.\n    -   Expose sample_rate through `info.features['audio'].sample_rate`.\n    -   Support for encoding audio features from file objects.\n-   More datasets.\n\n### Changed\n\n-   New `image_classification` section. Some datasets have been move there from\n    `images`.\n-   `DownloadConfig` does not append the dataset name anymore (manual data\n    should be in `<manual_dir>/` instead of `<manual_dir>/<dataset_name>/`).\n-   Tests now check that all `dl_manager.download` urls has registered\n    checksums. To opt-out, add `SKIP_CHECKSUMS = True` to your\n    `DatasetBuilderTestCase`.\n-   `tfds.load` now always returns `tf.compat.v2.Dataset`. If you're using still\n    using `tf.compat.v1`:\n    -   Use `tf.compat.v1.data.make_one_shot_iterator(ds)` rather than\n        `ds.make_one_shot_iterator()`.\n    -   Use `isinstance(ds, tf.compat.v2.Dataset)` instead of `isinstance(ds,\n        tf.data.Dataset)`.\n\n### Deprecated\n\n-   The `tfds.features.text` encoding API is deprecated. Please use\n    [tensorflow_text](https://www.tensorflow.org/tutorials/tensorflow_text/intro)\n    instead.\n-   `num_shards` argument of `tfds.core.SplitGenerator` is currently ignored and\n    will be removed in the next version.\n\n### Removed\n\n-   Legacy mode `tfds.experiment.S3` has been removed\n-   `in_memory` argument has been removed from `as_dataset`/`tfds.load` (small\n    datasets are now auto-cached).\n-   `tfds.Split.ALL`.\n\n### Fixed\n\n-   Various bugs, better error messages, documentation improvements.\n\n## [2.1.0] - 2020-02-25\n\n### Added\n\n-   Datasets expose `info.dataset_size` and `info.download_size`.\n-   [Auto-caching small datasets](https://www.tensorflow.org/datasets/performances#auto-caching).\n-   Datasets expose their cardinality `num_examples =\n    tf.data.experimental.cardinality(ds)` (Requires tf-nightly or TF >= 2.2.0)\n-   Get the number of example in a sub-splits with:\n    `info.splits['train[70%:]'].num_examples`\n\n### Changes\n\n-   All datasets generated with 2.1.0 cannot be loaded with previous version\n    (previous datasets can be read with `2.1.0` however).\n\n### Deprecated\n\n-   `in_memory` argument is deprecated and will be removed in a future version.\n\n## [2.0.0] - 2020-01-24\n\n### Added\n\n-   Several new datasets. Thanks to all the\n    [contributors](https://github.com/tensorflow/datasets/graphs/contributors)!\n-   Support for nested `tfds.features.Sequence` and `tf.RaggedTensor`\n-   Custom `FeatureConnector`s can override the `decode_batch_example` method\n    for efficient decoding when wrapped inside a\n    `tfds.features.Sequence(my_connector)`.\n-   Beam datasets can use a `tfds.core.BeamMetadataDict` to store additional\n    metadata computed as part of the Beam pipeline.\n-   Beam datasets' `_split_generators` accepts an additional `pipeline` kwargs\n    to define a pipeline shared between all splits.\n\n### Changed\n\n-   The default versions of all datasets are now using the S3 slicing API. See\n    the [guide](https://www.tensorflow.org/datasets/splits) for details.\n-   `shuffle_files` defaults to False so that dataset iteration is deterministic\n    by default. You can customize the reading pipeline, including shuffling and\n    interleaving, through the new `read_config` parameter in\n    [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load).\n-   `urls` kwargs renamed `homepage` in `DatasetInfo`\n\n### Deprecated\n\n-   Python2 support: this is the last version of TFDS that will support\n    Python 2. Going forward, we'll only support and test against Python 3.\n-   The previous split API is still available, but is deprecated. If you wrote\n    `DatasetBuilder`s outside the TFDS repository, please make sure they do not\n    use `experiments={tfds.core.Experiment.S3: False}`. This will be removed in\n    the next version, as well as the `num_shards` kwargs from `SplitGenerator`.\n\n### Fixed\n\n-   Various other bug fixes and performance improvements. Thank you for all the\n    reports and fixes!\n\n## [1.3.0] - 2019-10-21\n\n### Fixed\n\n-   Misc bugs and performance improvements.\n\n## [1.2.0] - 2019-08-19\n\n### Added\n\n#### Features\n\n-   Add `shuffle_files` argument to `tfds.load` function. The semantic is the\n    same as in `builder.as_dataset` function, which for now means that by\n    default, files will be shuffled for `TRAIN` split, and not for other splits.\n    Default behaviour will change to always be False at next major release.\n-   Most datasets now support the new S3 API\n    ([documentation](https://github.com/tensorflow/datasets/blob/master/docs/splits.md#two-apis-s3-and-legacy)).\n-   Support for uint16 PNG images.\n\n#### Datasets\n\n-   AFLW2000-3D\n-   Amazon_US_Reviews\n-   binarized_mnist\n-   BinaryAlphaDigits\n-   Caltech Birds 2010\n-   Coil100\n-   DeepWeeds\n-   Food101\n-   MIT Scene Parse 150\n-   RockYou leaked password\n-   Stanford Dogs\n-   Stanford Online Products\n-   Visual Domain Decathlon\n\n### Fixed\n\n-   Crash while shuffling on Windows\n-   Various documentation improvements\n\n## [1.1.0] - 2019-07-23\n\n### Added\n\n#### Features\n\n-   `in_memory` option to cache small dataset in RAM.\n-   Better sharding, shuffling and sub-split.\n-   It is now possible to add arbitrary metadata to `tfds.core.DatasetInfo`\n    which will be stored/restored with the dataset. See `tfds.core.Metadata`.\n-   Better proxy support, possibility to add certificate.\n-   `decoders` kwargs to override the default feature decoding\n    ([guide](https://github.com/tensorflow/datasets/tree/master/docs/decode.md)).\n\n#### Datasets\n\n-   [downsampled_imagenet](https://github.com/tensorflow/datasets/tree/master/docs/datasets.md#downsampled_imagenet).\n-   [patch_camelyon](https://github.com/tensorflow/datasets/tree/master/docs/datasets.md#patch_camelyon).\n-   [coco](https://github.com/tensorflow/datasets/tree/master/docs/datasets.md#coco)\n    2017 (with and without panoptic annotations).\n-   uc_merced.\n-   trivia_qa.\n-   super_glue.\n-   so2sat.\n-   snli.\n-   resisc45.\n-   pet_finder.\n-   mnist_corrupted.\n-   kitti.\n-   eurosat.\n-   definite_pronoun_resolution.\n-   curated_breast_imaging_ddsm.\n-   clevr.\n-   bigearthnet.\n\n## [1.0.2] - 2019-05-01\n\n### Added\n\n-   [Apache Beam support](https://www.tensorflow.org/datasets/beam_datasets).\n-   Direct GCS access for MNIST (with `tfds.load('mnist', try_gcs=True)`).\n-   More datasets.\n-   Option to turn off tqdm bar (`tfds.disable_progress_bar()`).\n\n### Fixed\n\n-   Subsplit do not depends on the number of shard anymore\n    (https://github.com/tensorflow/datasets/issues/292).\n-   Various bugs.\n\n## [1.0.1] - 2019-02-15\n\n### Added\n\n-   Dataset\n    [`celeb_a_hq`](https://github.com/tensorflow/datasets/blob/master/docs/datasets.md#celeb_a_hq).\n\n### Fixed\n\n-   Bug #52 that was putting the process in Eager mode by default.\n\n## [1.0.0] - 2019-02-14\n\n### Added\n\n-   25 datasets.\n-   Ready to be used `tensorflow-datasets`.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.16015625,
          "content": "# How to Contribute\n\nThanks for your interest in our library! Please see our [guide to contributing](https://www.tensorflow.org/datasets/contribute) to get started."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.986328125,
          "content": "# TensorFlow Datasets\n\nTensorFlow Datasets provides many public datasets as `tf.data.Datasets`.\n\n[![Unittests](https://github.com/tensorflow/datasets/actions/workflows/pytest.yml/badge.svg)](https://github.com/tensorflow/datasets/actions/workflows/pytest.yml)\n[![PyPI version](https://badge.fury.io/py/tensorflow-datasets.svg)](https://badge.fury.io/py/tensorflow-datasets)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![Tutorial](https://img.shields.io/badge/doc-tutorial-blue.svg)](https://www.tensorflow.org/datasets/overview)\n[![API](https://img.shields.io/badge/doc-api-blue.svg)](https://www.tensorflow.org/datasets/api_docs/python/tfds)\n[![Catalog](https://img.shields.io/badge/doc-datasets-blue.svg)](https://www.tensorflow.org/datasets/catalog/overview#all_datasets)\n\n## Documentation\n\nTo install and use TFDS, we strongly encourage to start with our\n[**getting started guide**](https://www.tensorflow.org/datasets/overview). Try\nit interactively in a\n[Colab notebook](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb).\n\nOur documentation contains:\n\n* [Tutorials and guides](https://www.tensorflow.org/datasets/overview)\n* List of all [available datasets](https://www.tensorflow.org/datasets/catalog/overview#all_datasets)\n* The [API reference](https://www.tensorflow.org/datasets/api_docs/python/tfds)\n\n```python\n# !pip install tensorflow-datasets\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\n# Construct a tf.data.Dataset\nds = tfds.load('mnist', split='train', as_supervised=True, shuffle_files=True)\n\n# Build your input pipeline\nds = ds.shuffle(1000).batch(128).prefetch(10).take(5)\nfor image, label in ds:\n  pass\n```\n\n## TFDS core values\n\nTFDS has been built with these principles in mind:\n\n* **Simplicity**: Standard use-cases should work out-of-the box\n* **Performance**: TFDS follows\n  [best practices](https://www.tensorflow.org/guide/data_performance)\n  and can achieve state-of-the-art speed\n* **Determinism/reproducibility**: All users get the same examples in the same\n  order\n* **Customisability**: Advanced users can have fine-grained control\n\nIf those use cases are not satisfied, please send us\n[feedback](https://github.com/tensorflow/datasets/issues).\n\n## Want a certain dataset?\n\nAdding a dataset is really straightforward by following\n[our guide](https://www.tensorflow.org/datasets/add_dataset).\n\nRequest a dataset by opening a\n[Dataset request GitHub issue](https://github.com/tensorflow/datasets/issues/new?assignees=&labels=dataset+request&template=dataset-request.md&title=%5Bdata+request%5D+%3Cdataset+name%3E).\n\nAnd vote on the current\n[set of requests](https://github.com/tensorflow/datasets/labels/dataset%20request)\nby adding a thumbs-up reaction to the issue.\n\n### Citation\n\nPlease include the following citation when using `tensorflow-datasets` for a\npaper, in addition to any citation specific to the used datasets.\n\n```bibtex\n@misc{TFDS,\n  title = {{TensorFlow Datasets}, A collection of ready-to-use datasets},\n  howpublished = {\\url{https://www.tensorflow.org/datasets}},\n}\n```\n\n#### *Disclaimers*\n\n*This is a utility library that downloads and prepares public datasets. We do*\n*not host or distribute these datasets, vouch for their quality or fairness, or*\n*claim that you have license to use the dataset. It is your responsibility to*\n*determine whether you have permission to use the dataset under the dataset's*\n*license.*\n\n*If you're a dataset owner and wish to update any part of it (description,*\n*citation, etc.), or do not want your dataset to be included in this*\n*library, please get in touch through a GitHub issue. Thanks for your*\n*contribution to the ML community!*\n\n*If you're interested in learning more about responsible AI practices, including*\n*fairness, please see Google AI's [Responsible AI Practices](https://ai.google/education/responsible-ai-practices).*\n\n*`tensorflow/datasets` is Apache 2.0 licensed. See the\n[`LICENSE`](https://github.com/tensorflow/datasets/blob/master/LICENSE) file.*\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.103515625,
          "content": "[build-system]\n# PEP 508 specs\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 9.6611328125,
          "content": "# coding=utf-8\n# Copyright 2024 The TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"tensorflow/datasets is a library of datasets ready to use with TensorFlow.\n\ntensorflow/datasets is a library of public datasets ready to use with\nTensorFlow. Each dataset definition contains the logic necessary to download and\nprepare the dataset, as well as to read it into a model using the\n`tf.data.Dataset` API.\n\nUsage outside of TensorFlow is also supported.\n\nSee the README on GitHub for further documentation.\n\"\"\"\n\nimport itertools\nimport os\nimport sys\n\nimport pkg_resources\nimport setuptools\n\n# To enable importing version.py directly, we add its path to sys.path.\nversion_path = os.path.join(os.path.dirname(__file__), 'tensorflow_datasets')\nsys.path.append(version_path)\nfrom version import __version__  # pytype: disable=import-error  # pylint: disable=g-import-not-at-top\n\nif datestring := os.environ.get('TFDS_NIGHTLY_TIMESTAMP'):\n  project_name = 'tfds-nightly'\n  # Version as `X.Y.Z.dev199912312459`\n  curr_version = pkg_resources.parse_version(__version__)\n  __version__ = f'{curr_version.base_version}.dev{datestring}'\nelse:\n  project_name = 'tensorflow-datasets'\n\nDOCLINES = __doc__.split('\\n')\n\nREQUIRED_PKGS = [\n    'absl-py',\n    # Min version of 0.5.0 as old array_record wheel are bugged on all\n    # platform except 'x86_64'. See\n    # https://github.com/google/array_record/issues/71\n    'array_record>=0.5.0;platform_system==\"Linux\"',\n    'dm-tree',\n    'etils[edc,enp,epath,epy,etree]>=1.6.0;python_version<\"3.11\"',\n    'etils[edc,enp,epath,epy,etree]>=1.9.1;python_version>=\"3.11\"',\n    'immutabledict',\n    'numpy',\n    'promise',\n    'protobuf>=3.20',\n    'psutil',\n    'pyarrow',\n    'requests>=2.19.0',\n    'simple_parsing',\n    'tensorflow-metadata',\n    'termcolor',\n    'toml',\n    'tqdm',\n    'wrapt',\n    # Standard library backports\n    'importlib_resources;python_version<\"3.9\"',\n]\n\nTESTS_DEPENDENCIES = [\n    'dill',\n    'jax[cpu]==0.4.28',\n    'jupyter',\n    'pytest',\n    'pytest-shard',\n    'pytest-xdist',\n    # Lazy-deps required by core\n    'apache-beam',\n    'conllu',\n    'mlcroissant>=1.0.9',\n    'pandas',\n    'pydub',\n    # Required by scripts/documentation/\n    'pyyaml',\n    'tensorflow-io[tensorflow];python_version<\"3.12\"',\n]\n\n# Additional deps for formatting\nDEV_DEPENDENCIES = [\n    'pylint>=2.6.0',\n    'yapf',\n]\n\n# Static files needed by datasets.\nDATASET_FILES = [\n    'datasets/imagenet2012/labels.txt',\n    'datasets/imagenet2012/validation_labels.txt',\n    'datasets/lvis/classes.txt',\n    'datasets/ogbg_molpcba/tasks.txt',\n    'datasets/quickdraw/labels.txt',\n    'datasets/smartwatch_gestures/labels.txt',\n    'image_classification/caltech101_labels.txt',\n    'image_classification/categories_places365.txt',\n    'image_classification/cbis_ddsm_calc_distributions.txt',\n    'image_classification/cbis_ddsm_calc_types.txt',\n    'image_classification/cbis_ddsm_mass_margins.txt',\n    'image_classification/cbis_ddsm_mass_shapes.txt',\n    'image_classification/cbis_ddsm_patch_labels.txt',\n    'image_classification/dtd_key_attributes.txt',\n    'image_classification/food-101_classes.txt',\n    'image_classification/i_naturalist2018/inaturalist2018_labels.txt',\n    'image_classification/i_naturalist2018/inaturalist2018_supercategories.txt',\n    'image_classification/i_naturalist2021/i_naturalist2021_labels.txt',\n    'image_classification/i_naturalist2021/i_naturalist2021_supercategories.txt',\n    'image_classification/imagenet_resized_labels.txt',\n    'image_classification/imagenette_labels.txt',\n    'image_classification/imagewang_labels.txt',\n    'image_classification/inaturalist_labels.txt',\n    'image_classification/inaturalist_supercategories.txt',\n    'image_classification/placesfull/categories_placesfull.txt',\n    'image_classification/plantae_k_urls.txt',\n    'image_classification/sun397_labels.txt',\n    'image_classification/sun397_tfds_te.txt',\n    'image_classification/sun397_tfds_tr.txt',\n    'image_classification/sun397_tfds_va.txt',\n    'object_detection/open_images_classes_all.txt',\n    'object_detection/open_images_classes_boxable.txt',\n    'object_detection/open_images_classes_trainable.txt',\n    'video/tao/labels.txt',\n    'video/ucf101_labels.txt',\n    'video/youtube_vis/labels.txt',\n]\n\n# Extra dependencies required by specific datasets\nDATASET_EXTRAS = {\n    # In alphabetical order\n    'aflw2k3d': ['scipy'],\n    'beir': ['apache-beam'],\n    'ble_wind_field': ['gcsfs', 'zarr'],\n    'c4': [\n        'apache-beam',\n        'gcld3',\n        'langdetect',\n        # nltk==3.8.2 is broken: https://github.com/nltk/nltk/issues/3293\n        'nltk==3.8.1',\n        'tldextract',\n    ],\n    'c4_wsrs': ['apache-beam'],\n    'cats_vs_dogs': ['matplotlib'],\n    'colorectal_histology': ['Pillow'],\n    'common_voice': ['pydub'],  # and ffmpeg installed\n    'duke_ultrasound': ['scipy'],\n    'eurosat': ['scikit-image', 'tifffile', 'imagecodecs'],\n    'groove': ['pretty_midi', 'pydub'],\n    'gtzan': ['pydub'],\n    'imagenet2012_corrupted': [\n        # This includes pre-built source; you may need to use an alternative\n        # route to install OpenCV\n        'opencv-python',\n        'scikit-image',\n        'scipy',\n    ],\n    'librispeech': ['pydub'],  # and ffmpeg installed\n    'lsun': ['tensorflow-io[tensorflow]'],\n    # sklearn version required to avoid conflict with librosa from\n    # https://github.com/scikit-learn/scikit-learn/issues/14485\n    # See https://github.com/librosa/librosa/issues/1160\n    'nsynth': ['crepe>=0.0.11', 'librosa', 'scikit-learn==0.20.3'],\n    'ogbg_molpcba': ['pandas', 'networkx'],\n    'pet_finder': ['pandas'],\n    'qm9': ['pandas'],\n    'robonet': ['h5py'],  # and ffmpeg installed\n    # envlogger is not available for Python versions >= 3.11 or non Linux\n    # platforms: https://pypi.org/project/envlogger/#files\n    'locomotion': ['envlogger;python_version<\"3.11\" and sys_platform==\"linux\"'],\n    'robosuite_panda_pick_place_can': [\n        'envlogger;python_version<\"3.11\" and sys_platform==\"linux\"'\n    ],\n    'smartwatch_gestures': ['pandas'],\n    'svhn': ['scipy'],\n    'the300w_lp': ['scipy'],\n    'wake_vision': ['pandas'],\n    'wider_face': ['Pillow'],\n    'wiki_dialog': ['apache-beam'],\n    'wikipedia': ['apache-beam', 'mwparserfromhell', 'mwxml'],\n    'wsc273': ['bs4', 'lxml'],\n    'youtube_vis': ['pycocotools'],\n}\n\n# Those datasets have dependencies which conflict with the rest of TFDS, so\n# running them in an isolated environments.\nISOLATED_DATASETS = ('nsynth', 'lsun')\n\n# Extra dataset deps are required for the tests\nall_dataset_dependencies = list(\n    itertools.chain.from_iterable(\n        deps\n        for ds_name, deps in DATASET_EXTRAS.items()\n        if ds_name not in ISOLATED_DATASETS\n    )\n)\n\nTESTS_ALL_DEPENDENCIES = TESTS_DEPENDENCIES + all_dataset_dependencies\n# `datasets` needs to be installed separately in Python >= 3.10 due to\n# conflicts between `multiprocess` and `apache-beam` libraries. See\n# https://github.com/uqfoundation/multiprocess/issues/125\nHUGGINGFACE_ALL_DEPENDENCIES = [\n    dep for dep in TESTS_ALL_DEPENDENCIES if not dep.startswith('apache-beam')\n] + ['datasets']\n\nEXTRAS = {\n    'matplotlib': ['matplotlib'],\n    'tensorflow': ['tensorflow>=2.1'],\n    'tf-nightly': ['tf-nightly'],\n    'tensorflow-data-validation': ['tensorflow-data-validation'],\n    'tests-all': TESTS_ALL_DEPENDENCIES,\n    'dev': TESTS_DEPENDENCIES + DEV_DEPENDENCIES,\n    'huggingface': HUGGINGFACE_ALL_DEPENDENCIES,\n}\nEXTRAS.update(DATASET_EXTRAS)\n\nsetuptools.setup(\n    name=project_name,\n    version=__version__,\n    description=DOCLINES[0],\n    long_description='\\n'.join(DOCLINES[2:]),\n    author='Google Inc.',\n    author_email='packages@tensorflow.org',\n    url='https://github.com/tensorflow/datasets',\n    download_url='https://github.com/tensorflow/datasets/tags',\n    license='Apache 2.0',\n    packages=setuptools.find_packages(),\n    package_data={\n        'tensorflow_datasets': DATASET_FILES + [\n            # Bundle `datasets/` folder in PyPI releases\n            'datasets/*/*',\n            'core/utils/colormap.csv',\n            'scripts/documentation/templates/*',\n            'url_checksums/*',\n            'checksums.tsv',\n            'community-datasets.toml',\n            'dataset_collections/*/*.md',\n            'dataset_collections/*/*.bib',\n            'core/valid_tags.txt',\n        ],\n    },\n    exclude_package_data={\n        'tensorflow_datasets': [\n            'dummy_data/*',\n        ],\n    },\n    scripts=[],\n    install_requires=REQUIRED_PKGS,\n    python_requires='>=3.10',\n    extras_require=EXTRAS,\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3 :: Only',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    ],\n    keywords='tensorflow machine learning datasets',\n    entry_points={\n        'console_scripts': [\n            'tfds = tensorflow_datasets.scripts.cli.main:launch_cli'\n        ],\n    },\n    # Include_package_data is required for setup.py to recognize the MANIFEST.in\n    #   https://python-packaging.readthedocs.io/en/latest/non-code-files.html\n    include_package_data=True,\n)\n"
        },
        {
          "name": "tensorflow_datasets",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}