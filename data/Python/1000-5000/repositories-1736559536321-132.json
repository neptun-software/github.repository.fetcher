{
  "metadata": {
    "timestamp": 1736559536321,
    "page": 132,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "timesler/facenet-pytorch",
      "stars": 4647,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1181640625,
          "content": "__pycache__\n.vscode\n.ipynb_checkpoints\nruns\nbuild\ndist\n*.egg-info\n*tmp*\n.coverage\nhtmlcov\ntest.py\n.cache\n.ipython\n.local\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.11328125,
          "content": "[submodule \"dependencies/facenet\"]\n\tpath = dependencies/facenet\n\turl = https://github.com/davidsandberg/facenet.git\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.044921875,
          "content": "MIT License\n\nCopyright (c) 2019 Timothy Esler\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.1005859375,
          "content": "# Face Recognition Using Pytorch \n\n*You can also read a translated version of this file [in Chinese 简体中文版](README_cn.md).*\n\n[![Downloads](https://pepy.tech/badge/facenet-pytorch)](https://pepy.tech/project/facenet-pytorch)\n\n[![Code Coverage](https://img.shields.io/codecov/c/github/timesler/facenet-pytorch.svg)](https://codecov.io/gh/timesler/facenet-pytorch)\n\nThis is a repository for Inception Resnet (V1) models in pytorch, pretrained on VGGFace2 and CASIA-Webface.\n\nPytorch model weights were initialized using parameters ported from David Sandberg's [tensorflow facenet repo](https://github.com/davidsandberg/facenet).\n\nAlso included in this repo is an efficient pytorch implementation of MTCNN for face detection prior to inference. These models are also pretrained. To our knowledge, this is the fastest MTCNN implementation available.\n\n## Table of contents\n\n* [Table of contents](#table-of-contents)\n* [Quick start](#quick-start)\n* [Pretrained models](#pretrained-models)\n* [Example notebooks](#example-notebooks)\n  + [*Complete detection and recognition pipeline*](#complete-detection-and-recognition-pipeline)\n  + [*Face tracking in video streams*](#face-tracking-in-video-streams)\n  + [*Finetuning pretrained models with new data*](#finetuning-pretrained-models-with-new-data)\n  + [*Guide to MTCNN in facenet-pytorch*](#guide-to-mtcnn-in-facenet-pytorch)\n  + [*Performance comparison of face detection packages*](#performance-comparison-of-face-detection-packages)\n  + [*The FastMTCNN algorithm*](#the-fastmtcnn-algorithm)\n* [Running with docker](#running-with-docker)\n* [Use this repo in your own git project](#use-this-repo-in-your-own-git-project)\n* [Conversion of parameters from Tensorflow to Pytorch](#conversion-of-parameters-from-tensorflow-to-pytorch)\n* [References](#references)\n\n## Quick start\n\n1. Install:\n    \n    ```bash\n    # With pip:\n    pip install facenet-pytorch\n    \n    # or clone this repo, removing the '-' to allow python imports:\n    git clone https://github.com/timesler/facenet-pytorch.git facenet_pytorch\n    \n    # or use a docker container (see https://github.com/timesler/docker-jupyter-dl-gpu):\n    docker run -it --rm timesler/jupyter-dl-gpu pip install facenet-pytorch && ipython\n    ```\n    \n1. In python, import facenet-pytorch and instantiate models:\n    \n    ```python\n    from facenet_pytorch import MTCNN, InceptionResnetV1\n    \n    # If required, create a face detection pipeline using MTCNN:\n    mtcnn = MTCNN(image_size=<image_size>, margin=<margin>)\n    \n    # Create an inception resnet (in eval mode):\n    resnet = InceptionResnetV1(pretrained='vggface2').eval()\n    ```\n    \n1. Process an image:\n    \n    ```python\n    from PIL import Image\n    \n    img = Image.open(<image path>)\n\n    # Get cropped and prewhitened image tensor\n    img_cropped = mtcnn(img, save_path=<optional save path>)\n\n    # Calculate embedding (unsqueeze to add batch dimension)\n    img_embedding = resnet(img_cropped.unsqueeze(0))\n\n    # Or, if using for VGGFace2 classification\n    resnet.classify = True\n    img_probs = resnet(img_cropped.unsqueeze(0))\n    ```\n\nSee `help(MTCNN)` and `help(InceptionResnetV1)` for usage and implementation details.\n\n## Pretrained models\n\nSee: [models/inception_resnet_v1.py](models/inception_resnet_v1.py)\n\nThe following models have been ported to pytorch (with links to download pytorch state_dict's):\n\n|Model name|LFW accuracy (as listed [here](https://github.com/davidsandberg/facenet))|Training dataset|\n| :- | :-: | -: |\n|[20180408-102900](https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180408-102900-casia-webface.pt) (111MB)|0.9905|CASIA-Webface|\n|[20180402-114759](https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180402-114759-vggface2.pt) (107MB)|0.9965|VGGFace2|\n\nThere is no need to manually download the pretrained state_dict's; they are downloaded automatically on model instantiation and cached for future use in the torch cache. To use an Inception Resnet (V1) model for facial recognition/identification in pytorch, use:\n\n```python\nfrom facenet_pytorch import InceptionResnetV1\n\n# For a model pretrained on VGGFace2\nmodel = InceptionResnetV1(pretrained='vggface2').eval()\n\n# For a model pretrained on CASIA-Webface\nmodel = InceptionResnetV1(pretrained='casia-webface').eval()\n\n# For an untrained model with 100 classes\nmodel = InceptionResnetV1(num_classes=100).eval()\n\n# For an untrained 1001-class classifier\nmodel = InceptionResnetV1(classify=True, num_classes=1001).eval()\n```\n\nBoth pretrained models were trained on 160x160 px images, so will perform best if applied to images resized to this shape. For best results, images should also be cropped to the face using MTCNN (see below).\n\nBy default, the above models will return 512-dimensional embeddings of images. To enable classification instead, either pass `classify=True` to the model constructor, or you can set the object attribute afterwards with `model.classify = True`. For VGGFace2, the pretrained model will output logit vectors of length 8631, and for CASIA-Webface logit vectors of length 10575.\n\n## Example notebooks\n\n### *Complete detection and recognition pipeline*\n\nFace recognition can be easily applied to raw images by first detecting faces using MTCNN before calculating embedding or probabilities using an Inception Resnet model. The example code at [examples/infer.ipynb](examples/infer.ipynb) provides a complete example pipeline utilizing datasets, dataloaders, and optional GPU processing.\n\n### *Face tracking in video streams*\n\nMTCNN can be used to build a face tracking system (using the `MTCNN.detect()` method). A full face tracking example can be found at [examples/face_tracking.ipynb](examples/face_tracking.ipynb).\n\n![](examples/tracked.gif)\n\n### *Finetuning pretrained models with new data*\n\nIn most situations, the best way to implement face recognition is to use the pretrained models directly, with either a clustering algorithm or a simple distance metrics to determine the identity of a face. However, if finetuning is required (i.e., if you want to select identity based on the model's output logits), an example can be found at [examples/finetune.ipynb](examples/finetune.ipynb).\n\n### *Guide to MTCNN in facenet-pytorch*\n\nThis guide demonstrates the functionality of the MTCNN module. Topics covered are:\n\n* Basic usage\n* Image normalization\n* Face margins\n* Multiple faces in a single image\n* Batched detection\n* Bounding boxes and facial landmarks\n* Saving face datasets\n\nSee the [notebook on kaggle](https://www.kaggle.com/timesler/guide-to-mtcnn-in-facenet-pytorch).\n\n### *Performance comparison of face detection packages*\n\nThis notebook demonstrates the use of three face detection packages:\n\n1. facenet-pytorch\n1. mtcnn\n1. dlib\n\nEach package is tested for its speed in detecting the faces in a set of 300 images (all frames from one video), with GPU support enabled. Performance is based on Kaggle's P100 notebook kernel. Results are summarized below.\n\n|Package|FPS (1080x1920)|FPS (720x1280)|FPS (540x960)|\n|---|---|---|---|\n|facenet-pytorch|12.97|20.32|25.50|\n|facenet-pytorch (non-batched)|9.75|14.81|19.68|\n|dlib|3.80|8.39|14.53|\n|mtcnn|3.04|5.70|8.23|\n\n![](examples/performance-comparison.png)\n\nSee the [notebook on kaggle](https://www.kaggle.com/timesler/comparison-of-face-detection-packages).\n\n### *The FastMTCNN algorithm*\n\nThis algorithm demonstrates how to achieve extremely efficient face detection specifically in videos, by taking advantage of similarities between adjacent frames.\n\nSee the [notebook on kaggle](https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution).\n\n## Running with docker\n\nThe package and any of the example notebooks can be run with docker (or nvidia-docker) using:\n\n```bash\ndocker run --rm -p 8888:8888\n    -v ./facenet-pytorch:/home/jovyan timesler/jupyter-dl-gpu \\\n    -v <path to data>:/home/jovyan/data\n    pip install facenet-pytorch && jupyter lab \n```\n\nNavigate to the examples/ directory and run any of the ipython notebooks.\n\nSee [timesler/jupyter-dl-gpu](https://github.com/timesler/docker-jupyter-dl-gpu) for docker container details.\n\n## Use this repo in your own git project\n\nTo use this code in your own git repo, I recommend first adding this repo as a submodule. Note that the dash ('-') in the repo name should be removed when cloning as a submodule as it will break python when importing:\n\n`git submodule add https://github.com/timesler/facenet-pytorch.git facenet_pytorch`\n\nAlternatively, the code can be installed as a package using pip:\n\n`pip install facenet-pytorch`\n\n## Conversion of parameters from Tensorflow to Pytorch\n\nSee: [models/utils/tensorflow2pytorch.py](models/tensorflow2pytorch.py)\n\nNote that this functionality is not needed to use the models in this repo, which depend only on the saved pytorch `state_dict`'s. \n\nFollowing instantiation of the pytorch model, each layer's weights were loaded from equivalent layers in the pretrained tensorflow models from [davidsandberg/facenet](https://github.com/davidsandberg/facenet).\n\nThe equivalence of the outputs from the original tensorflow models and the pytorch-ported models have been tested and are identical:\n\n---\n\n`>>> compare_model_outputs(mdl, sess, torch.randn(5, 160, 160, 3).detach())`\n\n```\nPassing test data through TF model\n\ntensor([[-0.0142,  0.0615,  0.0057,  ...,  0.0497,  0.0375, -0.0838],\n        [-0.0139,  0.0611,  0.0054,  ...,  0.0472,  0.0343, -0.0850],\n        [-0.0238,  0.0619,  0.0124,  ...,  0.0598,  0.0334, -0.0852],\n        [-0.0089,  0.0548,  0.0032,  ...,  0.0506,  0.0337, -0.0881],\n        [-0.0173,  0.0630, -0.0042,  ...,  0.0487,  0.0295, -0.0791]])\n\nPassing test data through PT model\n\ntensor([[-0.0142,  0.0615,  0.0057,  ...,  0.0497,  0.0375, -0.0838],\n        [-0.0139,  0.0611,  0.0054,  ...,  0.0472,  0.0343, -0.0850],\n        [-0.0238,  0.0619,  0.0124,  ...,  0.0598,  0.0334, -0.0852],\n        [-0.0089,  0.0548,  0.0032,  ...,  0.0506,  0.0337, -0.0881],\n        [-0.0173,  0.0630, -0.0042,  ...,  0.0487,  0.0295, -0.0791]],\n       grad_fn=<DivBackward0>)\n\nDistance 1.2874517096861382e-06\n```\n\n---\n\nIn order to re-run the conversion of tensorflow parameters into the pytorch model, ensure you clone this repo _with submodules_, as the davidsandberg/facenet repo is included as a submodule and parts of it are required for the conversion.\n\n## References\n\n1. David Sandberg's facenet repo: [https://github.com/davidsandberg/facenet](https://github.com/davidsandberg/facenet)\n\n1. F. Schroff, D. Kalenichenko, J. Philbin. _FaceNet: A Unified Embedding for Face Recognition and Clustering_, arXiv:1503.03832, 2015. [PDF](https://arxiv.org/pdf/1503.03832)\n\n1. Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman. _VGGFace2: A dataset for recognising face across pose and age_, International Conference on Automatic Face and Gesture Recognition, 2018. [PDF](http://www.robots.ox.ac.uk/~vgg/publications/2018/Cao18/cao18.pdf)\n\n1. D. Yi, Z. Lei, S. Liao and S. Z. Li. _CASIAWebface: Learning Face Representation from Scratch_, arXiv:1411.7923, 2014. [PDF](https://arxiv.org/pdf/1411.7923)\n\n1. K. Zhang, Z. Zhang, Z. Li and Y. Qiao. _Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks_, IEEE Signal Processing Letters, 2016. [PDF](https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/spl.pdf)\n"
        },
        {
          "name": "README_cn.md",
          "type": "blob",
          "size": 13.53125,
          "content": "# 使用 Pytorch 进行人脸识别\n\n*Click [here](README.md) to return to the English document*\n\n> 译者注：\n>\n> 本项目 [facenet-pytorch](https://github.com/timesler/facenet-pytorch) 是一个十分方便的人脸识别库，可以通过 [pip](https://pypi.org/project/facenet-pytorch/) 直接安装。\n>\n> 库中包含了两个重要功能\n>\n> - 人脸检测：使用MTCNN算法\n> - 人脸识别：使用FaceNet算法\n>\n> 利用这个库，可以轻松实现人脸检测和人脸向量映射操作。\n>\n> 为了方便中文开发者研究学习人脸识别相关任务、贡献代码，我将本项目的README文件以及位于 `examples` 里面的几个示例脚本中必要的部分翻译成了中文，以供参考。\n>\n> 向本项目的所有贡献者致敬。\n>\n> 英译汉：[远哥挺乐](https://github.com/yuan2001425)\n>\n> Translator's Note:\n>\n> This project [facenet-pytorch](https://github.com/timesler/facenet-pytorch) is a very convenient face recognition library that can be installed directly via [pip](https://pypi.org/project/facenet-pytorch/).\n>\n> The library contains two important features:\n>\n> - Face detection: using the MTCNN algorithm\n> - Face recognition: using the FaceNet algorithm\n>\n> With this library, one can easily carry out face detection and face vector mapping operations.\n>\n> In order to facilitate Chinese developers in studying face recognition and contributing code, I have translated the README file of this project and some necessary parts of several example scripts located in the `examples` directory into Chinese.\n>\n> Salute to all contributors to this project.\n>\n> Translated from English to Chinese by [远哥挺乐](https://github.com/yuan2001425).\n\n[![下载](https://pepy.tech/badge/facenet-pytorch)](https://pepy.tech/project/facenet-pytorch)\n\n[![代码覆盖率](https://img.shields.io/codecov/c/github/timesler/facenet-pytorch.svg)](https://codecov.io/gh/timesler/facenet-pytorch)\n\n|Python | 3.10 | 3.10 3.9 | 3.9 3.8 |\n| :---: | :---: | :---: | :---: |\n| 测试结果 | [![测试状态](https://github.com/timesler/facenet-pytorch/actions/workflows/python-3.10.yml/badge.svg?branch=master)](https://github.com/timesler/facenet-pytorch/actions?query=workflow%3A%22Python+3.10%22+branch%3Amaster) | [![测试状态](https://github.com/timesler/facenet-pytorch/actions/workflows/python-3.9.yml/badge.svg?branch=master)](https://github.com/timesler/facenet-pytorch/actions?query=workflow%3A%22Python+3.9%22+branch%3Amaster) | [![测试状态](https://github.com/timesler/facenet-pytorch/actions/workflows/python-3.8.yml/badge.svg?branch=master)](https://github.com/timesler/facenet-pytorch/actions?query=workflow%3A%22Python+3.8%22+branch%3Amaster) |\n\n[![xscode](https://img.shields.io/badge/Available%20on-xs%3Acode-blue?style=?style=plastic&logo=appveyor&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAMAAACdt4HsAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRF////////VXz1bAAAAAJ0Uk5T/wDltzBKAAAAlUlEQVR42uzXSwqAMAwE0Mn9L+3Ggtgkk35QwcnSJo9S+yGwM9DCooCbgn4YrJ4CIPUcQF7/XSBbx2TEz4sAZ2q1RAECBAiYBlCtvwN+KiYAlG7UDGj59MViT9hOwEqAhYCtAsUZvL6I6W8c2wcbd+LIWSCHSTeSAAECngN4xxIDSK9f4B9t377Wd7H5Nt7/Xz8eAgwAvesLRjYYPuUAAAAASUVORK5CYII=)](https://xscode.com/timesler/facenet-pytorch)\n\n这是 pytorch 中 Inception Resnet (V1) 模型的存储库，在 VGGFace2 和 CASIA-Webface 上进行了预训练。\n\nPytorch 模型权重使用从 David Sandberg 的 [tensorflow Facenet repo](https://github.com/davidsandberg/facenet) 移植的参数进行初始化。\n\n该存储库中还包含 MTCNN 的高效 pytorch 实现，用于推理之前的人脸检测。这些模型也是经过预训练的。据我们所知，这是最快的 MTCNN 实现。\n\n## 目录\n\n* [目录](#table-of-contents)\n* [快速启动](#quick-start)\n* [预训练模型](#pretrained-models)\n* [示例笔记本](#example-notebooks)\n  + [*完整的检测和识别流程*](#complete-detection-and-recognition-pipeline)\n  + [*视频流中的人脸跟踪*](#face-tracking-in-video-streams)\n  + [*使用新数据微调预训练模型*](#finetuning-pretrained-models-with-new-data)\n  + [*facenet-pytorch 中的 MTCNN 指南*](#guide-to-mtcnn-in-facenet-pytorch)\n  + [*人脸检测包的性能比较*](#performance-comparison-of-face-detection-packages)\n  + [*FastMTCNN 算法*](#the-fastmtcnn-algorithm)\n* [使用 docker 运行](#running-with-docker)\n* [在您自己的 git 项目中使用此存储库](#use-this-repo-in-your-own-git-project)\n* [Tensorflow 到 Pytorch 的参数转换](#conversion-of-parameters-from-tensorflow-to-pytorch)\n* [参考资料](#references)\n\n## 快速启动\n\n1. 安装：\n   \n````bash\n# 使用pip安装：\npip install facenet-pytorch\n\n# 或克隆此存储库，删除“-”以允许 python 导入：\ngit clone https://github.com/timesler/facenet-pytorch.git facenet_pytorch\n\n# 或使用 docker 容器（参见 https://github.com/timesler/docker-jupyter-dl-gpu）：\ndocker run -it --rm timesler/jupyter-dl-gpu pip install facenet-pytorch && ipython\n````\n\n2. 在python中，导入 facenet-pytorch 并实例化模型：\n   \n````python\nfrom facenet_pytorch import MTCNN, InceptionResnetV1\n\n# 如果需要，使用 MTCNN 创建人脸检测模型：\nmtcnn = MTCNN(image_size=<image_size>, margin=<margin>)\n\n# 创建一个 inception resnet（在 eval 模式下）：\nresnet = InceptionResnetV1(pretrained='vggface2').eval()\n````\n\n3. 处理图像：\n   \n````python\nfrom PIL import Image\n\nimg = Image.open(<image path>)\n\n# 获取裁剪和预白化的图像张量\nimg_cropped = mtcnn(img, save_path=<optional save path>)\n\n# 计算嵌入（解压缩以添加批量维度）\nimg_embedding = resnet(img_cropped.unsqueeze(0))\n\n# 或者，如果用于 VGGFace2 分类\nresnet.classify = True\nimg_probs = resnet(img_cropped.unsqueeze(0))\n````\n\n有关使用和实现详细信息，请参阅 `help(MTCNN)` 和 `help(InceptionResnetV1)` 。\n\n## 预训练模型\n\n请参阅：[models/inception_resnet_v1.py](models/inception_resnet_v1.py)\n\n以下模型已移植到 pytorch（包含下载 pytorch state_dict 的链接）：\n\n|模型名称|LFW 准确度（如[此处](https://github.com/davidsandberg/facenet)列出）|训练数据集|\n| :- | :-: | -: |\n|[20180408-102900](https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180408-102900-casia-webface.pt) (111MB)|0.9905|CASIA-Webface|\n|[20180402-114759](https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180402-114759-vggface2.pt) (107MB)|0.9965|VGGFace2|\n\n无需手动下载预训练的state_dict；它们会在模型实例化时自动下载，并缓存在 torch 缓存中以供将来使用。要在 pytorch 中使用 Inception Resnet (V1) 模型进行面部识别/识别，请使用：\n\n````python\nfrom facenet_pytorch import InceptionResnetV1\n\n# 对于在 VGGFace2 上预训练的模型\nmodel = InceptionResnetV1(pretrained='vggface2').eval()\n\n# 对于在 CASIA-Webface 上预训练的模型\nmodel = InceptionResnetV1(pretrained='casia-webface').eval()\n\n# 对于具有 100 个类的未经训练的模型\nmodel = InceptionResnetV1(num_classes=100).eval()\n\n# 对于未经训练的 1001 类分类器\nmodel = InceptionResnetV1(classify=True, num_classes=1001).eval()\n````\n\n两个预训练模型均在 160x160 像素图像上进行训练，因此如果应用于调整为该形状的图像，则效果最佳。为了获得最佳结果，还应该使用 MTCNN 将图像裁剪到脸部（见下文）。\n\n默认情况下，上述模型将返回 512 维图像嵌入。要启用分类，请将 `classify=True` 传递给模型构造函数，或者您可以随后使用 `model.classify = True` 设置对象属性。对于 VGGFace2，预训练模型将输出长度为 8631 的 logit 向量，对于 CASIA-Webface 则输出长度为 10575 的 logit 向量。\n\n## 示例笔记本\n\n### *完整的检测和识别流程*\n\n通过首先使用 MTCNN 检测人脸，然后使用 Inception Resnet 模型计算嵌入或概率，可以轻松地将人脸识别应用于原始图像。 [examples/infer_cn.ipynb](examples/infer_cn.ipynb) 中的示例代码提供了一个利用数据集、数据加载器和可选 GPU 处理的完整示例流程。\n\n### *视频流中的人脸跟踪*\n\nMTCNN 可用于构建人脸跟踪系统（使用 `MTCNN.detect()` 方法）。完整的面部跟踪示例可以在 [examples/face_tracking_cn.ipynb](examples/face_tracking_cn.ipynb) 中找到。\n\n![](examples/tracked.gif)\n\n### *使用新数据微调预训练模型*\n\n在大多数情况下，实现人脸识别的最佳方法是直接使用预训练模型，通过聚类算法或简单的距离度量来确定人脸的身份。但是，如果需要微调（即，如果您想根据模型的输出 logits 选择标识），可以在 [examples/finetune_cn.ipynb](examples/finetune_cn.ipynb) 中找到示例。\n\n### *facenet-pytorch 中的 MTCNN 指南*\n\n本指南演示了 MTCNN 模块的功能。涵盖的主题有：\n\n* 基本用法\n* 图像标准化\n* 面边距\n* 单张图像中的多个面孔\n* 批量检测\n* 边界框和面部标志\n* 保存人脸数据集\n\n请参阅[kaggle 笔记本](https://www.kaggle.com/timesler/guide-to-mtcnn-in-facenet-pytorch)。\n\n### *人脸检测包的性能比较*\n\n本笔记本演示了三个人脸检测包的使用：\n\n1. facenet-pytorch\n2. mtcnn\n3. dlib\n\n每个包都经过测试，测试其在启用 GPU 支持的情况下检测一组 300 张图像（来自一个视频的所有帧）中的面部的速度。性能基于 Kaggle 的 P100 笔记本内核。结果总结如下。\n\n|套餐|FPS (1080x1920)|FPS (720x1280)|FPS (540x960)|\n|---|---|---|---|\n|facenet-pytorch|12.97|20.32|25.50|\n|facenet-pytorch（非批处理）|9.75|14.81|19.68|\n|dlib|3.80|8.39|14.53|\n|mtcnn|3.04|5.70|8.23|\n\n![](examples/performance-comparison.png)\n\n请参阅[kaggle 笔记本](https://www.kaggle.com/timesler/comparison-of-face-detection-packages)。\n\n### *FastMTCNN 算法*\n\n该算法演示了如何通过利用相邻帧之间的相似性来实现极其高效的人脸检测，特别是在视频中。\n\n请参阅[kaggle 笔记本](https://www.kaggle.com/timesler/fast-mtcnn- detector-55-fps-at-full-resolution)。\n\n## 使用 docker 运行\n\n该包和任何示例笔记本都可以使用 docker（或 nvidia-docker）运行：\n\n````bash\ndocker run --rm -p 8888:8888\n    -v ./facenet-pytorch:/home/jovyan timesler/jupyter-dl-gpu \\\n    -v <path to data>:/home/jovyan/data\n    pip install facenet-pytorch && jupyter lab \n````\n\n导航到 example/ 目录并运行任何 ipython 笔记本。\n\n有关 docker 容器的详细信息，请参阅 [timesler/jupyter-dl-gpu](https://github.com/timesler/docker-jupyter-dl-gpu)。\n\n## 在您自己的 git 项目中使用此存储库\n\n要在您自己的 git 存储库中使用此代码，我建议首先将此存储库添加为子模块。请注意，当克隆为子模块时，应删除存储库名称中的破折号（“-”），因为它会在导入时破坏 python：\n\n`git submodule add https://github.com/timesler/facenet-pytorch.git facenet_pytorch`\n\n或者，可以使用 pip 将代码安装为包：\n\n`pip install facenet-pytorch`\n\n## Tensorflow 到 Pytorch 的参数转换\n\n请参阅：[models/utils/tensorflow2pytorch.py](models/tensorflow2pytorch.py)\n\n请注意，使用此存储库中的模型不需要此功能，该功能仅依赖于pytorch保存的 `state_dict`。\n\n实例化 pytorch 模型后，每层的权重均从 [davidsandberg/facenet](https://github.com/davidsandberg/facenet) 的预训练Tensorflow模型中的等效层加载。\n\n原始Tensorflow模型和 pytorch 移植模型的输出的等效性已经过测试并且是相同的：\n\n---\n\n`>>> compare_model_outputs(mdl, sess, torch.randn(5, 160, 160, 3).detach())`\n\n````\nPassing test data through TF model （通过TF模型传递测试数据）\n\ntensor([[-0.0142,  0.0615,  0.0057,  ...,  0.0497,  0.0375, -0.0838],\n        [-0.0139,  0.0611,  0.0054,  ...,  0.0472,  0.0343, -0.0850],\n        [-0.0238,  0.0619,  0.0124,  ...,  0.0598,  0.0334, -0.0852],\n        [-0.0089,  0.0548,  0.0032,  ...,  0.0506,  0.0337, -0.0881],\n        [-0.0173,  0.0630, -0.0042,  ...,  0.0487,  0.0295, -0.0791]])\n\nPassing test data through PT model （通过PT模型传递测试数据）\n\ntensor([[-0.0142,  0.0615,  0.0057,  ...,  0.0497,  0.0375, -0.0838],\n        [-0.0139,  0.0611,  0.0054,  ...,  0.0472,  0.0343, -0.0850],\n        [-0.0238,  0.0619,  0.0124,  ...,  0.0598,  0.0334, -0.0852],\n        [-0.0089,  0.0548,  0.0032,  ...,  0.0506,  0.0337, -0.0881],\n        [-0.0173,  0.0630, -0.0042,  ...,  0.0487,  0.0295, -0.0791]],\n       grad_fn=<DivBackward0>)\n\nDistance 1.2874517096861382e-06 （距离1.2874517096861382e-06）\n````\n\n---\n\n为了重新运行Tensorflow参数到 pytorch 模型的转换，请确保使用子模块克隆此存储库，因为 davidsandberg/facenet 存储库作为子模块包含在内，并且转换需要其中的一部分。\n\n## 参考资料\n\n1. David Sandberg's facenet repo: [https://github.com/davidsandberg/facenet](https://github.com/davidsandberg/facenet)\n2. F. Schroff, D. Kalenichenko, J. Philbin. _FaceNet: A Unified Embedding for Face Recognition and Clustering_, arXiv:1503.03832, 2015. [PDF](https://arxiv.org/pdf/1503.03832)\n\n3. Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman. _VGGFace2: A dataset for recognising face across pose and age_, International Conference on Automatic Face and Gesture Recognition, 2018. [PDF](http://www.robots.ox.ac.uk/~vgg/publications/2018/Cao18/cao18.pdf)\n\n4. D. Yi, Z. Lei, S. Liao and S. Z. Li. _CASIAWebface: Learning Face Representation from Scratch_, arXiv:1411.7923, 2014. [PDF](https://arxiv.org/pdf/1411.7923)\n\n5. K. Zhang, Z. Zhang, Z. Li and Y. Qiao. _Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks_, IEEE Signal Processing Letters, 2016. [PDF](https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/spl.pdf)\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0.3837890625,
          "content": "from .models.inception_resnet_v1 import InceptionResnetV1\nfrom .models.mtcnn import MTCNN, PNet, RNet, ONet, prewhiten, fixed_image_standardization\nfrom .models.utils.detect_face import extract_face\nfrom .models.utils import training\n\nimport warnings\nwarnings.filterwarnings(\n    action=\"ignore\", \n    message=\"This overload of nonzero is deprecated:\\n\\tnonzero()\", \n    category=UserWarning\n)"
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.10546875,
          "content": "coverage:\n  status:\n    project: off\n    patch: off\ncodecov: \n  token: 1e4f3aaa-9c74-4888-9408-71cc7fcfb64c\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "dependencies",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.294921875,
          "content": "import setuptools, os\n\nPACKAGE_NAME = 'facenet-pytorch'\nVERSION = '2.5.2'\nAUTHOR = 'Tim Esler'\nEMAIL = 'tim.esler@gmail.com'\nDESCRIPTION = 'Pretrained Pytorch face detection and recognition models'\nGITHUB_URL = 'https://github.com/timesler/facenet-pytorch'\n\nparent_dir = os.path.dirname(os.path.realpath(__file__))\nimport_name = os.path.basename(parent_dir)\n\nwith open('{}/README.md'.format(parent_dir), 'r') as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name=PACKAGE_NAME,\n    version=VERSION,\n    author=AUTHOR,\n    author_email=EMAIL,\n    description=DESCRIPTION,\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url=GITHUB_URL,\n    packages=[\n        'facenet_pytorch',\n        'facenet_pytorch.models',\n        'facenet_pytorch.models.utils',\n        'facenet_pytorch.data',\n    ],\n    package_dir={'facenet_pytorch':'.'},\n    package_data={'': ['*net.pt']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    install_requires=[\n        'numpy>=1.24.0,<2.0.0',\n        'Pillow>=10.2.0,<10.3.0',\n        'requests>=2.0.0,<3.0.0',\n        'torch>=2.2.0,<=2.3.0',\n        'torchvision>=0.17.0,<=0.18.0',\n        'tqdm>=4.0.0,<5.0.0',\n    ],\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}