{
  "metadata": {
    "timestamp": 1736560046030,
    "page": 866,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "sail-sg/EditAnything",
      "stars": 3349,
      "defaultBranch": "main",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.0458984375,
          "content": "# Edit Anything by Segment-Anything\n\n[![HuggingFace space](https://img.shields.io/badge/🤗-HuggingFace%20Space-cyan.svg)](https://huggingface.co/spaces/shgao/EditAnything)\n\nThis is an ongoing project aims to **Edit and Generate Anything** in an image,\npowered by [Segment Anything](https://github.com/facebookresearch/segment-anything), [ControlNet](https://github.com/lllyasviel/ControlNet),\n[BLIP2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2), [Stable Diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion), etc.\n\nAny forms of contribution and suggestion\nare very welcomed!\n\n# News🔥\n2023/08/09 - Revise UI and code, fixed multiple known issues.\n\n2023/07/25 - EditAnything is accepted by the ACM MM demo track.\n\n2023/06/09 - Support cross-image region drag and merge, unleash creative fusion!\n\n2023/05/24 - Support multiple high-quality character editing: clothes, haircut, colored contact lenses.\n\n2023/05/22 - Support sketch to image by adjusting mask align strength in `sketch2image.py`!\n\n2023/05/13 - Support interactive segmentation with click operation!  \n\n2023/05/11 - Support tile model for detail refinement!\n\n2023/05/04 - New demos of Beauty/Handsome Edit/Generation is released!\n\n2023/05/04 - ControlNet-based inpainting model on any lora model is supported now. EditAnything can operate on any base/lord models without the requirements of inpainting model.\n\n<details>\n  <summary> More update logs. </summary>\n\n    \n2023/05/01 - Models V0.4 based on Stable Diffusion 1.5/2.1 are released. New models are trained with more data and iterations.[Model Zoo](https://github.com/sail-sg/EditAnything#model-zoo)\n\n2023/04/20 - We support the Customized editing with DreamBooth.\n\n2023/04/17 - We support the SAM mask to semantic segmentation mask.\n\n2023/04/17 - We support different alignment degrees bettween edited parts and the SAM mask, check it out on [DEMO](https://huggingface.co/spaces/shgao/EditAnything)!\n\n2023/04/15 - [Gradio demo on Huggingface](https://huggingface.co/spaces/shgao/EditAnything) is released!\n\n2023/04/14 - New model trained with LAION dataset is released.\n\n2023/04/13 - Support pretrained model auto downloading and gradio in `sam2image.py`.\n\n2023/04/12 - An initial version of text-guided edit-anything is in `sam2groundingdino_edit.py`(object-level) and `sam2vlpart_edit.py`(part-level).\n\n2023/04/10 - An initial version of edit-anything is in `sam2edit.py`.\n\n2023/04/10 - We transfer the pretrained model into diffusers style, the pretrained model is auto loaded when using `sam2image_diffuser.py`. Now you can combine our pretrained model with different base models easily!\n\n</details>\n\n2023/04/09 - We released a pretrained model of StableDiffusion based ControlNet that generate images conditioned by SAM segmentation.\n\n# Features\n\n**Try our [![HuggingFace DEMO](https://img.shields.io/badge/🤗-HuggingFace%20Space-cyan.svg)](https://huggingface.co/spaces/shgao/EditAnything)🔥🔥🔥**\n## Unleash creative fusion: Cross-image region drag and merge!🔥\n<img width=\"1268\" alt=\"image\" src=\"https://github.com/sail-sg/EditAnything/assets/20515144/7f997db8-d2ea-4341-a7d7-dfe8ac5dd338\">\n<img width=\"1283\" alt=\"image\" src=\"https://github.com/sail-sg/EditAnything/assets/20515144/da34126c-d0fc-4020-85b6-1d99bed806e1\">\n\n\n\n\n\n## Clothes editing!🔥\n<img width=\"1357\" alt=\"image\" src=\"https://github.com/sail-sg/EditAnything/assets/20515144/03452a0f-83ae-4257-995c-f3d8b71d4f1d\">\n\n## Haircut editing!🔥\n<img width=\"1406\" alt=\"image\" src=\"https://github.com/sail-sg/EditAnything/assets/20515144/9091e3c9-c7e1-485d-bfe6-de5b21e83814\">\n\n## Colored contact lenses!🔥\n<img width=\"1080\" alt=\"image\" src=\"https://github.com/sail-sg/EditAnything/assets/20515144/d9c8a136-e12c-4df4-aed0-a7c287e0ef3c\">\n\n## Human replacement with tile refinement!🔥\n\n<img width=\"839\" alt=\"image\" src=\"https://github.com/sail-sg/EditAnything/assets/20515144/31883059-2bbc-442c-88aa-04d3a1da7abc\">\n\n\n## Draw your Sketch and Generate your Image!🔥\nprompt: \"a paint of  a  tree in the ground with a river.\"\n<div>\n<img width=\"250\" alt=\"image\" src=\"images/sk1.png\">\n<img width=\"250\" alt=\"image\" src=\"images/sk1_ex1.png\">\n<img width=\"250\" alt=\"image\" src=\"images/sk1_ex2.png\">\n</div>\n\n<details>\n  <summary> More demos. </summary>\n\nprompt: \"a paint, river, mountain, sun, cloud, beautiful field.\"\n<div>\n<img width=\"250\" alt=\"image\" src=\"images/sk4.png\">\n<img width=\"250\" alt=\"image\" src=\"images/sk4_ex1.png\">\n<img width=\"250\" alt=\"image\" src=\"images/sk4_ex2.png\">\n</div>\n\nprompt: \"a man, midsplit center parting hair, HD.\"\n<div>\n<img width=\"250\" alt=\"image\" src=\"images/sk2.png\">\n<img width=\"250\" alt=\"image\" src=\"images/sk2_ex1.png\">\n<img width=\"250\" alt=\"image\" src=\"images/sk2_ex2.png\">\n</div>\n\nprompt: \"a woman, long hair, detailed facial details, photorealistic, HD, beautiful face, solo, candle, brown hair, blue eye.\"\n<div>\n<img width=\"250\" alt=\"image\" src=\"images/sk3.png\">\n<img width=\"250\" alt=\"image\" src=\"images/sk3_ex1.png\">\n<img width=\"250\" alt=\"image\" src=\"images/sk3_ex2.png\">\n</div>\n</details>\n\nAlso, you could use the generated image and sam model to refine your sketch definitely!\n\n## Generate/Edit your beauty!!!🔥🔥🔥\n**Edit Your beauty and Generate Your beauty**\n<div>\n<img width=\"277\" alt=\"image\" src=\"images/beauty_edit.gif\">\n<img width=\"300\" alt=\"image\" src=\"images/beauty_demo.gif\">\n</div>\n\n\n## Customized editing with layout alignment control.\n<img width=\"1392\" alt=\"image\" src=\"https://user-images.githubusercontent.com/20515144/233339751-2c9e4ec8-e884-4c0e-95de-42512eccee85.png\">\nEditAnything+DreamBooth: Train a customized DreamBooth Model with `tools/train_dreambooth_inpaint.py` and replace the base model in `sam2edit.py` with the trained model.\n\n## Image Editing with layout alignment control.\n<img width=\"1040\" alt=\"image\" src=\"https://user-images.githubusercontent.com/20515144/233106460-14eb0e5a-cbc1-457d-aad3-a56796f7bee1.png\">\n\n## Keep the layout and Generate your season!\n<div>\n    <img src=\"images/paint.jpg\" height=256  alt=\"original paint\">\n    <img src=\"images/seg.png\" height=256  alt=\"SAM\">\n</div>\n\nHuman Prompt: \"A paint of spring/summer/autumn/winter field.\"\n<div>\n    <img src=\"images/spring.png\" height=256 alt=\"spring\">\n    <img src=\"images/summer.png\" height=256 alt=\"summer\">\n    <img src=\"images/autumn.png\" height=256 alt=\"autumn\">\n    <img src=\"images/winter.png\" height=256 alt=\"winter\">\n</div>\n\n## Edit Specific Thing by Text-Grounding and Segment-Anything\n### Editing by Text-guided Part Mask\nText Grounding: \"dog head\"\n\nHuman Prompt: \"cute dog\"\n![p](images/sample_dog_head.jpg)\n\n<details>\n  <summary> More demos. </summary>\n    \nText Grounding: \"cat eye\"\n\nHuman Prompt: \"A cute small humanoid cat\"\n![p](images/sample_cat_eye.jpg)\n    \n</details>\n\n### Editing by Text-guided Object Mask\nText Grounding: \"bench\"\n\nHuman Prompt: \"bench\" \n![p](images/sample_bench.jpg)\n\n## Edit Anything by Segment-Anything\n\nHuman Prompt: \"esplendent sunset sky, red brick wall\"\n![p](images/edit_sample2.jpg)\n\n\n<details>\n  <summary> More demos. </summary>\n    \nHuman Prompt: \"chairs by the lake, sunny day, spring\"\n![p](images/edit_sample1.jpg)\n    \n</details>\n\n\n## Generate Anything by Segment-Anything\n\nBLIP2 Prompt: \"a large white and red ferry\"\n![p](images/sample1.jpg)\n(1:input image; 2: segmentation mask; 3-8: generated images.)\n\n<details>\n  <summary> More demos. </summary>\n\nBLIP2 Prompt: \"a cloudy sky\"\n![p](images/sample2.jpg)\n\nBLIP2 Prompt: \"a black drone flying in the blue sky\"\n![p](images/sample3.jpg)\n\n </details>\n\n1) The human prompt and BLIP2 generated prompt build the text instruction.\n2) The SAM model segment the input image to generate segmentation mask without category.\n3) The segmentation mask and text instruction guide the image generation.\n\n## Generate semantic labels for each SAM mask.\n![p](images/sample_semantic.jpg)\n```\npython sam2semantic.py\n\n```\n\nHighlight features:\n- Pretrained ControlNet with SAM mask as condition enables the image generation with fine-grained control.\n- category-unrelated SAM mask enables more forms of editing and generation.\n- BLIP2 text generation enables text guidance-free control.\n\n# Setup\n\n**Create a environment**\n\n```bash\n    conda env create -f environment.yaml\n    conda activate control\n```\n\n**Install BLIP2 and SAM**\n\nPut these models in `models` folder.\n```bash\n# BLIP2 and SAM will be audo installed by running app.py\npip install git+https://github.com/huggingface/transformers.git\n\npip install git+https://github.com/facebookresearch/segment-anything.git\n\n# For text-guided editing\npip install git+https://github.com/openai/CLIP.git\n\npip install git+https://github.com/facebookresearch/detectron2.git\n\npip install git+https://github.com/IDEA-Research/GroundingDINO.git\n```\n\n**Download pretrained model**\n```bash\n\n# Segment-anything ViT-H SAM model will be auto downloaded. \n\n# BLIP2 model will be auto downloaded.\n\n# Part Grounding Swin-Base Model.\nwget https://github.com/Cheems-Seminar/segment-anything-and-name-it/releases/download/v1.0/swinbase_part_0a0000.pth\n\n# Grounding DINO Model.\nwget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth\n\n# Get pretrained model from huggingface. \n# No need to download this! But please install safetensors for reading the ckpt.\n\n```\n\n\n**Run Demo**\n```bash\npython app.py\n# or\npython editany.py\n# or\npython sam2image.py\n# or\npython sam2vlpart_edit.py\n# or\npython sam2groundingdino_edit.py\n```\n\n\n# Model Zoo\n\n| Model | Features | Download Path |\n|-------|----------|---------------|\n|SAM Pretrained(v0-1) | Good Nature Sense | [shgao/edit-anything-v0-1-1](https://huggingface.co/shgao/edit-anything-v0-1-1) |\n|LAION Pretrained(v0-3) | Good Face        | [shgao/edit-anything-v0-3](https://huggingface.co/shgao/edit-anything-v0-3)\n|LAION Pretrained(v0-4) | Support StableDiffusion 1.5/2.1, More training data and iterations, Good Face        | [shgao/edit-anything-v0-4-sd15](https://huggingface.co/shgao/edit-anything-v0-4-sd15) [shgao/edit-anything-v0-4-sd21](https://huggingface.co/shgao/edit-anything-v0-4-sd21)\n\n\n# Training\n\n1. Generate training dataset with `dataset_build.py`.\n2. Transfer stable-diffusion model with `tool_add_control_sd21.py`.\n2. Train model with `sam_train_sd21.py`.\n\n\n# Acknowledgement\n```\n@InProceedings{gao2023editanything,\n  author = {Gao, Shanghua and Lin, Zhijie and Xie, Xingyu and Zhou, Pan and Cheng, Ming-Ming and Yan, Shuicheng},\n  title = {EditAnything: Empowering Unparalleled Flexibility in Image Editing and Generation},\n  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia, Demo track},\n  year = {2023},\n}\n```\n\nThis project is based on:\n\n[Segment Anything](https://github.com/facebookresearch/segment-anything),\n[ControlNet](https://github.com/lllyasviel/ControlNet),\n[BLIP2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2),\n[MDT](https://github.com/sail-sg/MDT),\n[Stable Diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion),\n[Large-scale Unsupervised Semantic Segmentation](https://github.com/LUSSeg),\n[Grounded Segment Anything: From Objects to Parts](https://github.com/Cheems-Seminar/segment-anything-and-name-it),\n[Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)\n\nThanks for these amazing projects!\n"
        },
        {
          "name": "annotator",
          "type": "tree",
          "content": null
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 4.55078125,
          "content": "import gradio as gr\nimport os\n\nfrom editany import create_demo as create_demo_edit_anything\nfrom sam2image import create_demo as create_demo_generate_anything\nfrom editany_beauty import create_demo as create_demo_beauty\nfrom editany_handsome import create_demo as create_demo_handsome\nfrom editany_lora import EditAnythingLoraModel, init_sam_model, init_blip_processor, init_blip_model\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\nDESCRIPTION = f'''# [Edit Anything](https://github.com/sail-sg/EditAnything)\n**Edit anything and keep the layout by segmenting anything in the image.**\n'''\n\nsam_generator, mask_predictor = init_sam_model()\nblip_processor = init_blip_processor()\nblip_model = init_blip_model()\n\nsd_models_path = snapshot_download(\"shgao/sdmodels\")\n\nwith gr.Blocks() as demo:\n    gr.Markdown(DESCRIPTION)\n    with gr.Tabs():\n        with gr.TabItem('🖌Edit Anything'):\n            # model = EditAnythingLoraModel(base_model_path=\"stabilityai/stable-diffusion-2-inpainting\",\n            #                               controlmodel_name='LAION Pretrained(v0-4)-SD21',\n            #                               lora_model_path=None, use_blip=True, extra_inpaint=False,\n            #                               sam_generator=sam_generator,\n            #                               mask_predictor=mask_predictor,\n            #                               blip_processor=blip_processor,\n            #                               blip_model=blip_model)\n            # create_demo_edit_anything(model.process, model.process_image_click)\n            model = EditAnythingLoraModel(base_model_path=\"runwayml/stable-diffusion-v1-5\",\n                                          controlmodel_name='LAION Pretrained(v0-4)-SD15',\n                                          lora_model_path=None, use_blip=True, extra_inpaint=True,\n                                          sam_generator=sam_generator,\n                                          mask_predictor=mask_predictor,\n                                          blip_processor=blip_processor,\n                                          blip_model=blip_model)\n            create_demo_edit_anything(model.process, model.process_image_click)\n        with gr.TabItem(' 👩‍🦰Beauty Edit/Generation'):\n            lora_model_path = hf_hub_download(\n                \"mlida/Cute_girl_mix4\", \"cuteGirlMix4_v10.safetensors\")\n            model = EditAnythingLoraModel(base_model_path=os.path.join(sd_models_path, \"chilloutmix_NiPrunedFp32Fix\"),\n                                          lora_model_path=lora_model_path, use_blip=True, extra_inpaint=True,\n                                          sam_generator=sam_generator,\n                                          mask_predictor=mask_predictor,\n                                          blip_processor=blip_processor,\n                                          blip_model=blip_model,\n                                          lora_weight=0.5,\n                                          )\n            create_demo_beauty(model.process, model.process_image_click)\n        # with gr.TabItem(' 👨‍🌾Handsome Edit/Generation'):\n        #     model = EditAnythingLoraModel(base_model_path=os.path.join(sd_models_path, \"Realistic_Vision_V2.0\"),\n        #                                   lora_model_path=None, use_blip=True, extra_inpaint=True,\n        #                                   sam_generator=sam_generator,\n        #                                   mask_predictor=mask_predictor,\n        #                                   blip_processor=blip_processor,\n        #                                   blip_model=blip_model)\n        #     create_demo_handsome(model.process, model.process_image_click)\n        # with gr.TabItem('Edit More'):\n        #     model = EditAnythingLoraModel(base_model_path=\"andite/anything-v4.0\",\n        #                                   lora_model_path=None, use_blip=True, extra_inpaint=True,\n        #                                   sam_generator=sam_generator,\n        #                                   mask_predictor=mask_predictor,\n        #                                   blip_processor=blip_processor,\n        #                                   blip_model=blip_model,\n        #                                   lora_weight=0.5,\n        #                                   )\n            create_demo_beauty(model.process, model.process_image_click)\n        # with gr.TabItem('Generate Anything'):\n        #     create_demo_generate_anything()\n    # with gr.Tabs():\n    #     gr.Markdown(SHARED_UI_WARNING)\n\ndemo.queue(api_open=False).launch(server_name='0.0.0.0', share=False)\n"
        },
        {
          "name": "cldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 0.01953125,
          "content": "save_memory = False\n"
        },
        {
          "name": "dataset_build.py",
          "type": "blob",
          "size": 1.2138671875,
          "content": "from PIL import Image\nimport json\n\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\nimport os\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\ndef get_blip2_text(image):\n    inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = model.generate(**inputs, max_new_tokens=50)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    return generated_text\n\n\ndata_path = \"files\"\nsave_path = \"\"\n\nimage_names = os.listdir(data_path)\nimage_names = sorted(image_names)\n\ntext_data = {}\nf = open(\"data.txt\",\"w\")\nfor each in image_names:\n    if '.jpg' in each:\n        this_data = {}\n        this_data['target'] = each\n        this_data['source'] = each[:-4]+'.json'\n        this_image = Image.open(os.path.join(data_path, each))\n        print(each)\n        generated_text = get_blip2_text(this_image)\n        this_data['prompt'] = generated_text\n        print(this_data)\n        f.write(str(this_data)+\"\\n\")\nf.close()\n\n        \n\n\n\n"
        },
        {
          "name": "editany.py",
          "type": "blob",
          "size": 1.0556640625,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nimport os\nimport gradio as gr\nfrom diffusers.utils import load_image\nfrom editany_lora import EditAnythingLoraModel, config_dict\nfrom editany_demo import create_demo_template\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\n\ndef create_demo(process, process_image_click=None):\n\n    examples = None\n    INFO = f\"\"\"\n    ## EditAnything https://github.com/sail-sg/EditAnything\n    \"\"\"\n    WARNING_INFO = None\n\n    demo = create_demo_template(\n        process,\n        process_image_click,\n        examples=examples,\n        INFO=INFO,\n        WARNING_INFO=WARNING_INFO,\n        enable_auto_prompt_default=True,\n    )\n    return demo\n\n\nif __name__ == \"__main__\":\n    model = EditAnythingLoraModel(\n        base_model_path=\"runwayml/stable-diffusion-v1-5\",\n        controlmodel_name='LAION Pretrained(v0-4)-SD15',\n        lora_model_path=None, use_blip=True, extra_inpaint=True,\n    )\n    demo = create_demo(model.process, model.process_image_click)\n    demo.queue().launch(server_name=\"0.0.0.0\")\n"
        },
        {
          "name": "editany_beauty.py",
          "type": "blob",
          "size": 6.982421875,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nimport os\nimport gradio as gr\nfrom diffusers.utils import load_image\nfrom editany_lora import EditAnythingLoraModel, config_dict\nfrom editany_demo import create_demo_template\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\n\ndef create_demo(process, process_image_click=None):\n\n    examples = [\n        [\n            \"dudou,1girl, beautiful face, solo, candle, brown hair, long hair, <lora:flowergirl:0.9>,ulzzang-6500-v1.1,(raw photo:1.2),((photorealistic:1.4))best quality ,masterpiece, illustration, an extremely delicate and beautiful, extremely detailed ,CG ,unity ,8k wallpaper, Amazing, finely detail, masterpiece,best quality,official art,extremely detailed CG unity 8k wallpaper,absurdres, incredibly absurdres, huge filesize, ultra-detailed, highres, extremely detailed,beautiful detailed girl, extremely detailed eyes and face, beautiful detailed eyes,cinematic lighting,1girl,see-through,looking at viewer,full body,full-body shot,outdoors,arms behind back,(chinese clothes) <lora:cuteGirlMix4_v10:1>\",\n            \"(((mole))),sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, bad anatomy,(long hair:1.4),DeepNegative,(fat:1.2),facing away, looking away,tilted head, lowres,bad anatomy,bad hands, text, error, missing fingers,extra digit, fewer digits, cropped, worstquality, low quality, normal quality,jpegartifacts,signature, watermark, username,blurry,bad feet,cropped,poorly drawn hands,poorly drawn face,mutation,deformed,worst quality,low quality,normal quality,jpeg artifacts,signature,watermark,extra fingers,fewer digits,extra limbs,extra arms,extra legs,malformed limbs,fused fingers,too many fingers,long neck,cross-eyed,mutated hands,polar lowres,bad body,bad proportions,gross proportions,text,error,missing fingers,missing arms,missing legs,extra digit, extra arms, extra leg, extra foot,(freckles),(mole:2)\",\n            5,\n        ],\n        [\n            \"best quality, ultra high res, (photorealistic:1.4), (detailed beautiful girl:1.4), (medium breasts:0.8), looking_at_viewer, Detailed facial details, beautiful detailed eyes, (multicolored|blue|pink hair: 1.2), green eyes, slender, haunting smile, (makeup:0.3), red lips, <lora:cuteGirlMix4_v10:0.7>, highly detailed clothes, (ulzzang-6500-v1.1:0.3)\",\n            \"EasyNegative, paintings, sketches, ugly, 3d, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, manboobs, backlight,(ugly:1.3), (duplicate:1.3), (morbid:1.2), (mutilated:1.2), (tranny:1.3), mutated hands, (poorly drawn hands:1.3), blurry, (bad anatomy:1.2), (bad proportions:1.3), extra limbs, (disfigured:1.3), (more than 2 nipples:1.3), (more than 1 navel:1.3), (missing arms:1.3), (extra legs:1.3), (fused fingers:1.6), (too many fingers:1.6), (unclear eyes:1.3), bad hands, missing fingers, extra digit, (futa:1.1), bad body, double navel, mutad arms, hused arms, (puffy nipples, dark areolae, dark nipples, rei no himo, inverted nipples, long nipples), NG_DeepNegative_V1_75t, pubic hair, fat rolls, obese, bad-picture-chill-75v\",\n            8,\n        ],\n        [\n            \"best quality, ultra high res, (photorealistic:1.4), (detailed beautiful girl:1.4), (medium breasts:0.8), looking_at_viewer, Detailed facial details, beautiful detailed eyes, (blue|pink hair), green eyes, slender, smile, (makeup:0.4), red lips, (full body, sitting, beach), <lora:cuteGirlMix4_v10:0.7>, highly detailed clothes, (ulzzang-6500-v1.1:0.3)\",\n            \"asyNegative, paintings, sketches, ugly, 3d, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, manboobs, backlight,(ugly:1.3), (duplicate:1.3), (morbid:1.2), (mutilated:1.2), (tranny:1.3), mutated hands, (poorly drawn hands:1.3), blurry, (bad anatomy:1.2), (bad proportions:1.3), extra limbs, (disfigured:1.3), (more than 2 nipples:1.3), (more than 1 navel:1.3), (missing arms:1.3), (extra legs:1.3), (fused fingers:1.6), (too many fingers:1.6), (unclear eyes:1.3), bad hands, missing fingers, extra digit, (futa:1.1), bad body, double navel, mutad arms, hused arms, (puffy nipples, dark areolae, dark nipples, rei no himo, inverted nipples, long nipples), NG_DeepNegative_V1_75t, pubic hair, fat rolls, obese, bad-picture-chill-75v\",\n            7,\n        ],\n        [\n            \"mix4, whole body shot, ((8k, RAW photo, highest quality, masterpiece), High detail RAW color photo professional close-up photo, shy expression, cute, beautiful detailed girl, detailed fingers, extremely detailed eyes and face, beautiful detailed nose, beautiful detailed eyes, long eyelashes, light on face, looking at viewer, (closed mouth:1.2), 1girl, cute, young, mature face, (full body:1.3), ((small breasts)), realistic face, realistic body, beautiful detailed thigh,s, same eyes color, (realistic, photo realism:1. 37), (highest quality), (best shadow), (best illustration), ultra high resolution, physics-based rendering, cinematic lighting), solo, 1girl, highly detailed, in office, detailed office, open cardigan, ponytail contorted, beautiful eyes ,sitting in office,dating, business suit, cross-laced clothes, collared shirt, beautiful breast, small breast, Chinese dress, white pantyhose, natural breasts, pink and white hair, <lora:cuteGirlMix4_v10:1>\",\n            \"paintings, sketches, (worst quality:2), (low quality:2), (normal quality:2), cloth, underwear, bra, low-res, normal quality, ((monochrome)), ((grayscale)), skin spots, acne, skin blemishes, age spots, glans, bad nipples, long nipples, bad vagina, extra fingers,fewer fingers,strange fingers,bad hand, ng_deepnegative_v1_75t, bad-picture-chill-75v\",\n            7,\n        ],\n    ]\n    INFO = f\"\"\"\n    ## Generate Your Beauty powered by EditAnything https://github.com/sail-sg/EditAnything\n    This model is good at generating beautiful female.\n    \"\"\"\n    WARNING_INFO = f\"\"\"### [NOTE]  the model is collected from the Internet for demo only, please do not use it for commercial purposes.\n    We are not responsible for possible risks using this model.\n    Lora model from https://civitai.com/models/14171/cutegirlmix4 Thanks!\n    \"\"\"\n    demo = create_demo_template(\n        process,\n        process_image_click,\n        examples=examples,\n        INFO=INFO,\n        WARNING_INFO=WARNING_INFO,\n    )\n    return demo\n\n\nif __name__ == \"__main__\":\n    sd_models_path = snapshot_download(\"shgao/sdmodels\")\n    lora_model_path = hf_hub_download(\n        \"mlida/Cute_girl_mix4\", \"cuteGirlMix4_v10.safetensors\"\n    )\n    model = EditAnythingLoraModel(\n        base_model_path=os.path.join(\n            sd_models_path, \"chilloutmix_NiPrunedFp32Fix\"),\n        lora_model_path=lora_model_path,\n        use_blip=True,\n        extra_inpaint=True,\n        lora_weight=0.5,\n    )\n    demo = create_demo(model.process, model.process_image_click)\n    demo.queue().launch(server_name=\"0.0.0.0\")\n"
        },
        {
          "name": "editany_demo.py",
          "type": "blob",
          "size": 17.91015625,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nimport gradio as gr\n\nimport numpy as np\nimport cv2\nfrom cv2 import imencode\nimport base64\n\ndef create_demo_template(\n    process,\n    process_image_click=None,\n    examples=None,\n    INFO=\"EditAnything https://github.com/sail-sg/EditAnything\",\n    WARNING_INFO=None,\n    enable_auto_prompt_default=False,\n):\n\n    print(\"The GUI is not fully tested yet. Please open an issue if you find bugs.\")\n    block = gr.Blocks()\n    with block as demo:\n        clicked_points = gr.State([])\n        origin_image = gr.State(None)\n        click_mask = gr.State(None)\n        ref_clicked_points = gr.State([])\n        ref_origin_image = gr.State(None)\n        ref_click_mask = gr.State(None)\n        with gr.Row():\n            gr.Markdown(INFO)\n        with gr.Row(equal_height=False):\n            with gr.Column():\n                with gr.Tab(\"Click🖱\"):\n                    source_image_click = gr.Image(\n                        type=\"pil\",\n                        interactive=True,\n                        label=\"Image: Upload an image and click the region you want to edit.\",\n                    )\n                    with gr.Column():\n                        with gr.Row():\n                            point_prompt = gr.Radio(\n                                choices=[\"Foreground Point\",\n                                         \"Background Point\"],\n                                value=\"Foreground Point\",\n                                label=\"Point Label\",\n                                interactive=True,\n                                show_label=False,\n                            )\n                            with gr.Row():\n                                clear_button_click = gr.Button(\n                                    value=\"Clear Points\", interactive=True\n                                )\n                                clear_button_image = gr.Button(\n                                    value=\"Reset Image\", interactive=True\n                                )\n                        with gr.Row():\n                            run_button_click = gr.Button(\n                                label=\"Run EditAnying\", interactive=True\n                            )\n                with gr.Tab(\"Brush🖌️\"):\n                    source_image_brush = gr.Image(\n                        source=\"upload\",\n                        label=\"Image: Upload an image and cover the region you want to edit with sketch\",\n                        type=\"numpy\",\n                        tool=\"sketch\",\n                        brush_color=\"#00FFBF\"\n                    )\n                    run_button = gr.Button(\n                        label=\"Run EditAnying\", interactive=True)\n                with gr.Tab(\"All region\"):\n                    source_image_clean = gr.Image(\n                        source=\"upload\",\n                        label=\"Image: Upload an image\",\n                        type=\"numpy\",\n                    )\n                    run_button_allregion = gr.Button(\n                        label=\"Run EditAnying\", interactive=True)\n                with gr.Row():\n                    # enable_all_generate = gr.Checkbox(\n                    #     label=\"All Region Generation\", value=False\n                    # )\n                    control_scale = gr.Slider(\n                        label=\"SAM Mask Alignment Strength\",\n                        # info=\"Large value -> strict alignment with SAM mask\",\n                        minimum=0,\n                        maximum=1,\n                        value=0.5,\n                        step=0.1,\n                    )\n                    with gr.Row():\n                        num_samples = gr.Slider(\n                            label=\"Images\", minimum=1, maximum=12, value=2, step=1\n                        )\n                        seed = gr.Slider(\n                            label=\"Seed\",\n                            minimum=-1,\n                            maximum=2147483647,\n                            step=1,\n                            randomize=True,\n                        )\n                with gr.Column():\n                    with gr.Row():\n                        enable_auto_prompt = gr.Checkbox(\n                            label=\"Prompt Auto Generation (Enable this may makes your prompt not working)\",\n                            # info=\"\",\n                            value=enable_auto_prompt_default,\n                        )\n                    with gr.Row():\n                        a_prompt = gr.Textbox(\n                            label=\"Positive Prompt\",\n                            info=\"Text in the expected things of edited region\",\n                            value=\"best quality, extremely detailed,\",\n                        )\n                        n_prompt = gr.Textbox(\n                            label=\"Negative Prompt\",\n                            value=\"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, NSFW\",\n                        )\n\n                with gr.Row():\n                    enable_tile = gr.Checkbox(\n                        label=\"High-resolution Refinement\",\n                        info=\"Slow inference\",\n                        value=True,\n                    )\n                    refine_alignment_ratio = gr.Slider(\n                        label=\"Similarity with Initial Results\",\n                        # info=\"Large value -> strict alignment with input image. Small value -> strong global consistency\",\n                        minimum=0.0,\n                        maximum=1.0,\n                        value=0.95,\n                        step=0.05,\n                    )\n\n                with gr.Accordion(\"Cross-image Drag Options\", open=False):\n                    # ref_image = gr.Image(\n                    #     source='upload', label=\"Upload a reference image\", type=\"pil\", value=None)\n                    ref_image = gr.Image(\n                        source=\"upload\",\n                        label=\"Upload a reference image and cover the region you want to use with sketch\",\n                        type=\"pil\",\n                        tool=\"sketch\",\n                        brush_color=\"#00FFBF\",\n                    )\n                    with gr.Row():\n                        ref_auto_prompt = gr.Checkbox(\n                            label=\"Ref. Auto Prompt\", value=True\n                        )\n                        ref_prompt = gr.Textbox(\n                            label=\"Prompt\",\n                            info=\"Text in the prompt of edited region\",\n                            value=\"best quality, extremely detailed, \",\n                        )\n                    # ref_image = gr.Image(\n                    #     type=\"pil\", interactive=True,\n                    #     label=\"Image: Upload an image and click the region you want to use as reference.\",\n                    # )\n                    # with gr.Column():\n                    #     with gr.Row():\n                    #         ref_point_prompt = gr.Radio(\n                    #             choices=[\"Foreground Point\", \"Background Point\"],\n                    #             value=\"Foreground Point\",\n                    #             label=\"Point Label\",\n                    #             interactive=True, show_label=False)\n                    #         ref_clear_button_click = gr.Button(\n                    #             value=\"Clear Click Points\", interactive=True)\n                    #         ref_clear_button_image = gr.Button(\n                    #             value=\"Clear Image\", interactive=True)\n                    with gr.Row():\n                        reference_attn = gr.Checkbox(\n                            label=\"reference_attn\", value=True)\n                        reference_adain = gr.Checkbox(\n                            label=\"reference_adain\", value=True\n                        )\n                    with gr.Row():\n                        ref_sam_scale = gr.Slider(\n                            label=\"Pos Control Scale\",\n                            minimum=0,\n                            maximum=1.0,\n                            value=0.3,\n                            step=0.1,\n                        )\n                        ref_inpaint_scale = gr.Slider(\n                            label=\"Content Control Scale\",\n                            minimum=0,\n                            maximum=1.0,\n                            value=0.2,\n                            step=0.1,\n                        )\n\n                    with gr.Row():\n                        ref_textinv = gr.Checkbox(\n                            label=\"Use textual inversion token\", value=False\n                        )\n                        ref_textinv_path = gr.Textbox(\n                            label=\"textual inversion token path\",\n                            info=\"Text in the inversion token path\",\n                            value=None,\n                        )\n                    with gr.Accordion(\"Advanced options\", open=False):\n                        style_fidelity = gr.Slider(\n                            label=\"Style fidelity\",\n                            minimum=0,\n                            maximum=1.,\n                            value=0.,\n                            step=0.1,\n                        )\n                        attention_auto_machine_weight = gr.Slider(\n                            label=\"Attention Reference Weight\",\n                            minimum=0,\n                            maximum=1.0,\n                            value=1.0,\n                            step=0.01,\n                        )\n                        gn_auto_machine_weight = gr.Slider(\n                            label=\"GroupNorm Reference Weight\",\n                            minimum=0,\n                            maximum=1.0,\n                            value=1.0,\n                            step=0.01,\n                        )\n                        ref_scale = gr.Slider(\n                            label=\"Frequency Reference Guidance Scale\",\n                            minimum=0,\n                            maximum=1.0,\n                            value=0.0,\n                            step=0.1,\n                        )\n\n                with gr.Accordion(\"Advanced Options\", open=False):\n                    mask_image = gr.Image(\n                        source=\"upload\",\n                        label=\"Upload a predefined mask of edit region: Switch to Brush mode when using this!\",\n                        type=\"numpy\",\n                        value=None,\n                    )\n                    image_resolution = gr.Slider(\n                        label=\"Image Resolution\",\n                        minimum=256,\n                        maximum=768,\n                        value=512,\n                        step=64,\n                    )\n                    refine_image_resolution = gr.Slider(\n                        label=\"Image Resolution\",\n                        minimum=256,\n                        maximum=8192,\n                        value=1024,\n                        step=64,\n                    )\n                    guess_mode = gr.Checkbox(label=\"Guess Mode\", value=False)\n                    detect_resolution = gr.Slider(\n                        label=\"SAM Resolution\",\n                        minimum=128,\n                        maximum=2048,\n                        value=1024,\n                        step=1,\n                    )\n                    ddim_steps = gr.Slider(\n                        label=\"Steps\", minimum=1, maximum=100, value=30, step=1\n                    )\n                    scale = gr.Slider(\n                        label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                    alpha_weight = gr.Slider(\n                        label=\"Alpha weight\", info=\"Alpha mixing with original image\", minimum=0,\n                        maximum=1, value=0.0, step=0.1)\n                    use_scale_map = gr.Checkbox(\n                        label='Use scale map', value=False)\n                    eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n                    condition_model = gr.Textbox(\n                        label=\"Condition model path\",\n                        info=\"Text in the Controlnet model path in hugglingface\",\n                        value=\"EditAnything\",\n                    )\n            with gr.Column():\n                result_gallery_refine = gr.Gallery(\n                    label=\"Output High quality\", show_label=True, elem_id=\"gallery\", preview=False)\n                result_gallery_init = gr.Gallery(\n                    label=\"Output Low quality\", show_label=True, elem_id=\"gallery\", height=\"auto\")\n                result_gallery_ref = gr.Gallery(\n                    label=\"Output Ref\", show_label=False, elem_id=\"gallery\", height=\"auto\")\n                result_text = gr.Text(label=\"ALL Prompt Text\")\n\n        ips = [\n            source_image_brush,\n            gr.State(False),  # enable_all_generate\n            mask_image,\n            control_scale,\n            enable_auto_prompt,\n            a_prompt,\n            n_prompt,\n            num_samples,\n            image_resolution,\n            detect_resolution,\n            ddim_steps,\n            guess_mode,\n            scale,\n            seed,\n            eta,\n            enable_tile,\n            refine_alignment_ratio,\n            refine_image_resolution,\n            alpha_weight,\n            use_scale_map,\n            condition_model,\n            ref_image,\n            attention_auto_machine_weight,\n            gn_auto_machine_weight,\n            style_fidelity,\n            reference_attn,\n            reference_adain,\n            ref_prompt,\n            ref_sam_scale,\n            ref_inpaint_scale,\n            ref_auto_prompt,\n            ref_textinv,\n            ref_textinv_path,\n            ref_scale,\n        ]\n        run_button.click(\n            fn=process,\n            inputs=ips,\n            outputs=[\n                result_gallery_refine,\n                result_gallery_init,\n                result_gallery_ref,\n                result_text,\n            ],\n        )\n        ips_allregion = [\n            source_image_clean,\n            gr.State(True),  # enable_all_generate\n            mask_image,\n            control_scale,\n            enable_auto_prompt,\n            a_prompt,\n            n_prompt,\n            num_samples,\n            image_resolution,\n            detect_resolution,\n            ddim_steps,\n            guess_mode,\n            scale,\n            seed,\n            eta,\n            enable_tile,\n            refine_alignment_ratio,\n            refine_image_resolution,\n            alpha_weight,\n            use_scale_map,\n            condition_model,\n            ref_image,\n            attention_auto_machine_weight,\n            gn_auto_machine_weight,\n            style_fidelity,\n            reference_attn,\n            reference_adain,\n            ref_prompt,\n            ref_sam_scale,\n            ref_inpaint_scale,\n            ref_auto_prompt,\n            ref_textinv,\n            ref_textinv_path,\n            ref_scale,\n        ]\n        run_button_allregion.click(\n            fn=process,\n            inputs=ips_allregion,\n            outputs=[\n                result_gallery_refine,\n                result_gallery_init,\n                result_gallery_ref,\n                result_text,\n            ],\n        )\n\n        ip_click = [\n            origin_image,\n            gr.State(False),  # enable_all_generate\n            click_mask,\n            control_scale,\n            enable_auto_prompt,\n            a_prompt,\n            n_prompt,\n            num_samples,\n            image_resolution,\n            detect_resolution,\n            ddim_steps,\n            guess_mode,\n            scale,\n            seed,\n            eta,\n            enable_tile,\n            refine_alignment_ratio,\n            refine_image_resolution,\n            alpha_weight,\n            use_scale_map,\n            condition_model,\n            ref_image,\n            attention_auto_machine_weight,\n            gn_auto_machine_weight,\n            style_fidelity,\n            reference_attn,\n            reference_adain,\n            ref_prompt,\n            ref_sam_scale,\n            ref_inpaint_scale,\n            ref_auto_prompt,\n            ref_textinv,\n            ref_textinv_path,\n            ref_scale,\n        ]\n\n        run_button_click.click(\n            fn=process,\n            inputs=ip_click,\n            outputs=[\n                result_gallery_refine,\n                result_gallery_init,\n                result_gallery_ref,\n                result_text,\n            ],\n        )\n\n        source_image_click.upload(\n            lambda image: image.copy() if image is not None else None,\n            inputs=[source_image_click],\n            outputs=[origin_image],\n        )\n        source_image_click.select(\n            process_image_click,\n            inputs=[origin_image, point_prompt,\n                    clicked_points, image_resolution],\n            outputs=[source_image_click, clicked_points, click_mask],\n            show_progress=True,\n            queue=True,\n        )\n        clear_button_click.click(\n            fn=lambda original_image: (original_image.copy(), [], None)\n            if original_image is not None\n            else (None, [], None),\n            inputs=[origin_image],\n            outputs=[source_image_click, clicked_points, click_mask],\n        )\n        clear_button_image.click(\n            fn=lambda: (None, [], None, None, None),\n            inputs=[],\n            outputs=[\n                source_image_click,\n                clicked_points,\n                click_mask,\n                result_gallery_init,\n                result_text,\n            ],\n        )\n\n        if examples is not None:\n            with gr.Row():\n                ex = gr.Examples(\n                    examples=examples,\n                    fn=process,\n                    inputs=[a_prompt, n_prompt, scale],\n                    outputs=[result_gallery_init],\n                    cache_examples=False,\n                )\n        if WARNING_INFO is not None:\n            with gr.Row():\n                gr.Markdown(WARNING_INFO)\n    return demo\n"
        },
        {
          "name": "editany_handsome.py",
          "type": "blob",
          "size": 3.1123046875,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nimport os\nimport gradio as gr\nfrom diffusers.utils import load_image\nfrom editany_lora import EditAnythingLoraModel, config_dict\nfrom editany_demo import create_demo_template\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\n\ndef create_demo(process, process_image_click=None):\n\n    examples = [\n        [\n            \"1man, muscle,full body, vest, short straight hair, glasses, Gym, barbells, dumbbells, treadmills, boxing rings, squat racks, plates, dumbbell racks soft lighting, masterpiece, best quality, 8k uhd, film grain, Fujifilm XT3 photorealistic painting art by midjourney and greg rutkowski <lora:asianmale_v10:0.6>\",\n            \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\",\n            6,\n        ],\n        [\n            \"1man, 25 years- old, full body, wearing long-sleeve white shirt and tie, muscular rand black suit, soft lighting, masterpiece, best quality, 8k uhd, dslr, film grain, Fujifilm XT3 photorealistic painting art by midjourney and greg rutkowski <lora:asianmale_v10:0.6> <lora:uncutPenisLora_v10:0.6>\",\n            \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\",\n            6,\n        ],\n    ]\n\n    print(\"The GUI is not fully tested yet. Please open an issue if you find bugs.\")\n\n    INFO = f\"\"\"\n    ## Generate Your Handsome powered by EditAnything https://github.com/sail-sg/EditAnything \n    This model is good at generating handsome male.\n    \"\"\"\n    WARNING_INFO = f\"\"\"### [NOTE]  the model is collected from the Internet for demo only, please do not use it for commercial purposes.\n    We are not responsible for possible risks using this model.\n    Base model from https://huggingface.co/SG161222/Realistic_Vision_V2.0 Thanks!\n    \"\"\"\n    demo = create_demo_template(\n        process,\n        process_image_click,\n        examples=examples,\n        INFO=INFO,\n        WARNING_INFO=WARNING_INFO,\n    )\n    return demo\n\n\nif __name__ == \"__main__\":\n    model = EditAnythingLoraModel(\n        base_model_path=\"Realistic_Vision_V2.0\", lora_model_path=None, use_blip=True\n    )\n    demo = create_demo(model.process, model.process_image_click)\n    demo.queue().launch(server_name=\"0.0.0.0\")\n"
        },
        {
          "name": "editany_lora.py",
          "type": "blob",
          "size": 35.390625,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nfrom torchvision.utils import save_image\nfrom PIL import Image\nfrom pytorch_lightning import seed_everything\nimport subprocess\nfrom collections import OrderedDict\nimport re\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\nimport os\nimport requests\nfrom io import BytesIO\nfrom annotator.util import resize_image, HWC3, resize_points, get_bounding_box, save_input_to_file\n\nimport torch\nfrom safetensors.torch import load_file\nfrom collections import defaultdict\nfrom diffusers import StableDiffusionControlNetPipeline\nfrom diffusers import ControlNetModel, UniPCMultistepScheduler\n\nfrom utils.stable_diffusion_controlnet import ControlNetModel2\nfrom utils.stable_diffusion_controlnet_inpaint import StableDiffusionControlNetInpaintPipeline, \\\n    StableDiffusionControlNetInpaintMixingPipeline, prepare_mask_image\n# need the latest transformers\n# pip install git+https://github.com/huggingface/transformers.git\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nfrom diffusers import ControlNetModel\nimport PIL.Image\n\n# Segment-Anything init.\n# pip install git+https://github.com/facebookresearch/segment-anything.git\ntry:\n    from segment_anything import (\n        sam_model_registry,\n        SamAutomaticMaskGenerator,\n        SamPredictor,\n    )\nexcept ImportError:\n    print(\"segment_anything not installed\")\n    result = subprocess.run(\n        [\n            \"pip\",\n            \"install\",\n            \"git+https://github.com/facebookresearch/segment-anything.git\",\n        ],\n        check=True,\n    )\n    print(f\"Install segment_anything {result}\")\n    from segment_anything import (\n        sam_model_registry,\n        SamAutomaticMaskGenerator,\n        SamPredictor,\n    )\nif not os.path.exists(\"./models/sam_vit_h_4b8939.pth\"):\n    result = subprocess.run(\n        [\n            \"wget\",\n            \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n            \"-P\",\n            \"models\",\n        ],\n        check=True,\n    )\n    print(f\"Download sam_vit_h_4b8939.pth {result}\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nconfig_dict = OrderedDict(\n    [\n        (\"LAION Pretrained(v0-4)-SD15\", \"shgao/edit-anything-v0-4-sd15\"),\n        (\"LAION Pretrained(v0-4)-SD21\", \"shgao/edit-anything-v0-4-sd21\"),\n        (\"LAION Pretrained(v0-3)-SD21\", \"shgao/edit-anything-v0-3\"),\n        (\"SAM Pretrained(v0-1)-SD21\", \"shgao/edit-anything-v0-1-1\"),\n    ]\n)\n\n\ndef init_sam_model(sam_generator=None, mask_predictor=None):\n    if sam_generator is not None and mask_predictor is not None:\n        return sam_generator, mask_predictor\n    sam_checkpoint = \"models/sam_vit_h_4b8939.pth\"\n    model_type = \"default\"\n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n    sam.to(device=device)\n    sam_generator = (\n        SamAutomaticMaskGenerator(\n            sam) if sam_generator is None else sam_generator\n    )\n    mask_predictor = SamPredictor(\n        sam) if mask_predictor is None else mask_predictor\n    return sam_generator, mask_predictor\n\n\ndef init_blip_processor():\n    blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n    return blip_processor\n\n\ndef init_blip_model():\n    blip_model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16, device_map=\"auto\"\n    )\n    return blip_model\n\n\ndef get_pipeline_embeds(pipeline, prompt, negative_prompt, device):\n    # https://github.com/huggingface/diffusers/issues/2136\n    \"\"\"Get pipeline embeds for prompts bigger than the maxlength of the pipe\n    :param pipeline:\n    :param prompt:\n    :param negative_prompt:\n    :param device:\n    :return:\n    \"\"\"\n    max_length = pipeline.tokenizer.model_max_length\n\n    # # simple way to determine length of tokens\n    # count_prompt = len(re.split(r\",\", prompt))\n    # count_negative_prompt = len(re.split(r\",\", negative_prompt))\n\n    # # create the tensor based on which prompt is longer\n    # if count_prompt >= count_negative_prompt:\n    #     input_ids = pipeline.tokenizer(\n    #         prompt, return_tensors=\"pt\", truncation=False\n    #     ).input_ids.to(device)\n    #     shape_max_length = input_ids.shape[-1]\n    #     negative_ids = pipeline.tokenizer(\n    #         negative_prompt,\n    #         truncation=False,\n    #         padding=\"max_length\",\n    #         max_length=shape_max_length,\n    #         return_tensors=\"pt\",\n    #     ).input_ids.to(device)\n    # else:\n    #     negative_ids = pipeline.tokenizer(\n    #         negative_prompt, return_tensors=\"pt\", truncation=False\n    #     ).input_ids.to(device)\n    #     shape_max_length = negative_ids.shape[-1]\n    #     input_ids = pipeline.tokenizer(\n    #         prompt,\n    #         return_tensors=\"pt\",\n    #         truncation=False,\n    #         padding=\"max_length\",\n    #         max_length=shape_max_length,\n    #     ).input_ids.to(device)\n\n    # concat_embeds = []\n    # neg_embeds = []\n    # for i in range(0, shape_max_length, max_length):\n    #     concat_embeds.append(pipeline.text_encoder(\n    #         input_ids[:, i: i + max_length])[0])\n    #     neg_embeds.append(pipeline.text_encoder(\n    #         negative_ids[:, i: i + max_length])[0])\n\n    input_ids = pipeline.tokenizer(\n            prompt, return_tensors=\"pt\", truncation=False\n        ).input_ids.to(device)\n\n    negative_ids = pipeline.tokenizer(\n            negative_prompt, return_tensors=\"pt\", truncation=False\n        ).input_ids.to(device)\n    \n    shape_max_length = max(input_ids.shape[-1],negative_ids.shape[-1])\n    \n    if input_ids.shape[-1]>negative_ids.shape[-1]:\n        negative_ids = pipeline.tokenizer(\n            negative_prompt,\n            truncation=False,\n            padding=\"max_length\",\n            max_length=shape_max_length,\n            return_tensors=\"pt\",\n        ).input_ids.to(device)\n    else:\n        input_ids = pipeline.tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=False,\n        padding=\"max_length\",\n        max_length=shape_max_length,\n    ).input_ids.to(device)\n\n    concat_embeds = []\n    neg_embeds = []\n    for i in range(0, shape_max_length, max_length):\n        concat_embeds.append(pipeline.text_encoder(\n            input_ids[:, i: i + max_length])[0])\n        neg_embeds.append(pipeline.text_encoder(\n            negative_ids[:, i: i + max_length])[0])\n\n    return torch.cat(concat_embeds, dim=1), torch.cat(neg_embeds, dim=1)\n\n\ndef load_lora_weights(pipeline, checkpoint_path, multiplier, device, dtype):\n    LORA_PREFIX_UNET = \"lora_unet\"\n    LORA_PREFIX_TEXT_ENCODER = \"lora_te\"\n    # load LoRA weight from .safetensors\n    print('device: {}'.format(device))\n    if isinstance(checkpoint_path, str):\n        state_dict = load_file(checkpoint_path, device=device)\n\n        updates = defaultdict(dict)\n        for key, value in state_dict.items():\n            # it is suggested to print out the key, it usually will be something like below\n            # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n\n            layer, elem = key.split(\".\", 1)\n            updates[layer][elem] = value\n\n        # directly update weight in diffusers model\n        for layer, elems in updates.items():\n\n            if \"text\" in layer:\n                layer_infos = layer.split(\n                    LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n                curr_layer = pipeline.text_encoder\n            else:\n                layer_infos = layer.split(\n                    LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n                curr_layer = pipeline.unet\n\n            # find the target layer\n            temp_name = layer_infos.pop(0)\n            while len(layer_infos) > -1:\n                try:\n                    curr_layer = curr_layer.__getattr__(temp_name)\n                    if len(layer_infos) > 0:\n                        temp_name = layer_infos.pop(0)\n                    elif len(layer_infos) == 0:\n                        break\n                except Exception:\n                    if len(temp_name) > 0:\n                        temp_name += \"_\" + layer_infos.pop(0)\n                    else:\n                        temp_name = layer_infos.pop(0)\n\n            # get elements for this layer\n            weight_up = elems[\"lora_up.weight\"].to(dtype)\n            weight_down = elems[\"lora_down.weight\"].to(dtype)\n            alpha = elems[\"alpha\"]\n            if alpha:\n                alpha = alpha.item() / weight_up.shape[1]\n            else:\n                alpha = 1.0\n\n            # update weight\n            if len(weight_up.shape) == 4:\n                curr_layer.weight.data += (\n                    multiplier\n                    * alpha\n                    * torch.mm(\n                        weight_up.squeeze(3).squeeze(2),\n                        weight_down.squeeze(3).squeeze(2),\n                    )\n                    .unsqueeze(2)\n                    .unsqueeze(3)\n                )\n            else:\n                curr_layer.weight.data += (\n                    multiplier * alpha * torch.mm(weight_up, weight_down)\n                )\n    else:\n        for ckptpath in checkpoint_path:\n            state_dict = load_file(ckptpath, device=device)\n\n            updates = defaultdict(dict)\n            for key, value in state_dict.items():\n                # it is suggested to print out the key, it usually will be something like below\n                # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n\n                layer, elem = key.split(\".\", 1)\n                updates[layer][elem] = value\n\n            # directly update weight in diffusers model\n            for layer, elems in updates.items():\n                if \"text\" in layer:\n                    layer_infos = layer.split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\n                        \"_\"\n                    )\n                    curr_layer = pipeline.text_encoder\n                else:\n                    layer_infos = layer.split(\n                        LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n                    curr_layer = pipeline.unet\n\n                # find the target layer\n                temp_name = layer_infos.pop(0)\n                while len(layer_infos) > -1:\n                    try:\n                        curr_layer = curr_layer.__getattr__(temp_name)\n                        if len(layer_infos) > 0:\n                            temp_name = layer_infos.pop(0)\n                        elif len(layer_infos) == 0:\n                            break\n                    except Exception:\n                        if len(temp_name) > 0:\n                            temp_name += \"_\" + layer_infos.pop(0)\n                        else:\n                            temp_name = layer_infos.pop(0)\n\n                # get elements for this layer\n                weight_up = elems[\"lora_up.weight\"].to(dtype)\n                weight_down = elems[\"lora_down.weight\"].to(dtype)\n                alpha = elems[\"alpha\"]\n                if alpha:\n                    alpha = alpha.item() / weight_up.shape[1]\n                else:\n                    alpha = 1.0\n\n                # update weight\n                if len(weight_up.shape) == 4:\n                    curr_layer.weight.data += (\n                        multiplier\n                        * alpha\n                        * torch.mm(\n                            weight_up.squeeze(3).squeeze(2),\n                            weight_down.squeeze(3).squeeze(2),\n                        )\n                        .unsqueeze(2)\n                        .unsqueeze(3)\n                    )\n                else:\n                    curr_layer.weight.data += (\n                        multiplier * alpha * torch.mm(weight_up, weight_down)\n                    )\n    return pipeline\n\n\ndef make_inpaint_condition(image, image_mask):\n    image = image / 255.0\n    assert (\n        image.shape[0:1] == image_mask.shape[0:1]\n    ), \"image and image_mask must have the same image size\"\n    image[image_mask > 128] = -1.0  # set as masked pixel\n    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return image\n\n\ndef obtain_generation_model(\n    base_model_path,\n    lora_model_path,\n    controlnet_path,\n    generation_only=False,\n    extra_inpaint=True,\n    lora_weight=1.0,\n):\n    controlnet = []\n    controlnet.append(\n        ControlNetModel2.from_pretrained(\n            controlnet_path, torch_dtype=torch.float16)\n    )  # sam control\n    if (not generation_only) and extra_inpaint:  # inpainting control\n        print(\"Warning: ControlNet based inpainting model only support SD1.5 for now.\")\n        controlnet.append(\n            ControlNetModel.from_pretrained(\n                \"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16\n            )  # inpainting controlnet\n        )\n\n    if generation_only and extra_inpaint:\n        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n            base_model_path,\n            controlnet=controlnet,\n            torch_dtype=torch.float16,\n            safety_checker=None,\n        )\n    else:\n        pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n            base_model_path,\n            controlnet=controlnet,\n            torch_dtype=torch.float16,\n            safety_checker=None,\n        )\n    if lora_model_path is not None:\n        pipe = load_lora_weights(\n            pipe, [lora_model_path], lora_weight, \"cpu\", torch.float32\n        )\n    # speed up diffusion process with faster scheduler and memory optimization\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    # remove following line if xformers is not installed\n    pipe.enable_xformers_memory_efficient_attention()\n\n    pipe.enable_model_cpu_offload()\n    return pipe\n\n\ndef obtain_tile_model(base_model_path, lora_model_path, lora_weight=1.0):\n    controlnet = ControlNetModel2.from_pretrained(\n        \"lllyasviel/control_v11f1e_sd15_tile\", torch_dtype=torch.float16\n    )  # tile controlnet\n    if (\n        base_model_path == \"runwayml/stable-diffusion-v1-5\"\n        or base_model_path == \"stabilityai/stable-diffusion-2-inpainting\"\n    ):\n        print(\"base_model_path\", base_model_path)\n        pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\",\n            controlnet=controlnet,\n            torch_dtype=torch.float16,\n            safety_checker=None,\n        )\n    else:\n        pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n            base_model_path,\n            controlnet=controlnet,\n            torch_dtype=torch.float16,\n            safety_checker=None,\n        )\n    if lora_model_path is not None:\n        pipe = load_lora_weights(\n            pipe, [lora_model_path], lora_weight, \"cpu\", torch.float32\n        )\n    # speed up diffusion process with faster scheduler and memory optimization\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    # remove following line if xformers is not installed\n    pipe.enable_xformers_memory_efficient_attention()\n\n    pipe.enable_model_cpu_offload()\n    return pipe\n\n\ndef show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n    full_img = None\n\n    # for ann in sorted_anns:\n    for i in range(len(sorted_anns)):\n        ann = anns[i]\n        m = ann[\"segmentation\"]\n        if full_img is None:\n            full_img = np.zeros((m.shape[0], m.shape[1], 3))\n            map = np.zeros((m.shape[0], m.shape[1]), dtype=np.uint16)\n        map[m != 0] = i + 1\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        full_img[m != 0] = color_mask\n    full_img = full_img * 255\n    # anno encoding from https://github.com/LUSSeg/ImageNet-S\n    res = np.zeros((map.shape[0], map.shape[1], 3))\n    res[:, :, 0] = map % 256\n    res[:, :, 1] = map // 256\n    res.astype(np.float32)\n    full_img = Image.fromarray(np.uint8(full_img))\n    return full_img, res\n\n\nclass EditAnythingLoraModel:\n    def __init__(\n        self,\n        base_model_path=\"../chilloutmix_NiPrunedFp32Fix\",\n        lora_model_path=\"../40806/mix4\",\n        use_blip=True,\n        blip_processor=None,\n        blip_model=None,\n        sam_generator=None,\n        controlmodel_name=\"LAION Pretrained(v0-4)-SD15\",\n        # used when the base model is not an inpainting model.\n        extra_inpaint=True,\n        tile_model=None,\n        lora_weight=1.0,\n        alpha_mixing=None,\n        mask_predictor=None,\n    ):\n        self.device = device\n        self.use_blip = use_blip\n\n        # Diffusion init using diffusers.\n        self.default_controlnet_path = config_dict[controlmodel_name]\n        self.base_model_path = base_model_path\n        self.lora_model_path = lora_model_path\n        self.defalut_enable_all_generate = False\n        self.extra_inpaint = extra_inpaint\n        self.last_ref_infer = False\n        self.pipe = obtain_generation_model(\n            base_model_path,\n            lora_model_path,\n            self.default_controlnet_path,\n            generation_only=False,\n            extra_inpaint=extra_inpaint,\n            lora_weight=lora_weight,\n        )\n        # self.pipe.load_textual_inversion(\"textual_inversion_cat/learned_embeds.bin\")\n        # Segment-Anything init.\n        self.sam_generator, self.mask_predictor = init_sam_model(\n            sam_generator, mask_predictor\n        )\n        # BLIP2 init.\n        if use_blip:\n            if blip_processor is not None:\n                self.blip_processor = blip_processor\n            else:\n                self.blip_processor = init_blip_processor()\n\n            if blip_model is not None:\n                self.blip_model = blip_model\n            else:\n                self.blip_model = init_blip_model()\n\n        # tile model init.\n        if tile_model is not None:\n            self.tile_pipe = tile_model\n        else:\n            self.tile_pipe = obtain_tile_model(\n                base_model_path, lora_model_path, lora_weight=lora_weight\n            )\n\n    def get_blip2_text(self, image):\n        inputs = self.blip_processor(image, return_tensors=\"pt\").to(\n            self.device, torch.float16\n        )\n        generated_ids = self.blip_model.generate(**inputs, max_new_tokens=50)\n        generated_text = self.blip_processor.batch_decode(\n            generated_ids, skip_special_tokens=True\n        )[0].strip()\n        return generated_text\n\n    def get_sam_control(self, image):\n        masks = self.sam_generator.generate(image)\n        full_img, res = show_anns(masks)\n        return full_img, res\n\n    def get_click_mask(self, image, clicked_points):\n        self.mask_predictor.set_image(image)\n        # Separate the points and labels\n        points, labels = zip(*[(point[:2], point[2])\n                               for point in clicked_points])\n\n        # Convert the points and labels to numpy arrays\n        input_point = np.array(points)\n        input_label = np.array(labels)\n\n        masks, _, _ = self.mask_predictor.predict(\n            point_coords=input_point,\n            point_labels=input_label,\n            multimask_output=False,\n        )\n\n        return masks\n\n    @torch.inference_mode()\n    def process_image_click(\n        self,\n        original_image: gr.Image,\n        point_prompt: gr.Radio,\n        clicked_points: gr.State,\n        image_resolution,\n        evt: gr.SelectData,\n    ):\n        # Get the clicked coordinates\n        clicked_coords = evt.index\n        x, y = clicked_coords\n        label = point_prompt\n        lab = 1 if label == \"Foreground Point\" else 0\n        clicked_points.append((x, y, lab))\n\n        input_image = np.array(original_image, dtype=np.uint8)\n        H, W, C = input_image.shape\n        input_image = HWC3(input_image)\n        img = resize_image(input_image, image_resolution)\n\n        # Update the clicked_points\n        resized_points = resize_points(\n            clicked_points, input_image.shape, image_resolution\n        )\n        mask_click_np = self.get_click_mask(img, resized_points)\n\n        # Convert mask_click_np to HWC format\n        mask_click_np = np.transpose(mask_click_np, (1, 2, 0)) * 255.0\n\n        mask_image = HWC3(mask_click_np.astype(np.uint8))\n        mask_image = cv2.resize(\n            mask_image, (W, H), interpolation=cv2.INTER_LINEAR)\n        # mask_image = Image.fromarray(mask_image_tmp)\n\n        # Draw circles for all clicked points\n        edited_image = input_image\n        for x, y, lab in clicked_points:\n            # Set the circle color based on the label\n            color = (255, 0, 0) if lab == 1 else (0, 0, 255)\n\n            # Draw the circle\n            edited_image = cv2.circle(edited_image, (x, y), 20, color, -1)\n\n        # Set the opacity for the mask_image and edited_image\n        opacity_mask = 0.75\n        opacity_edited = 1.0\n\n        # Combine the edited_image and the mask_image using cv2.addWeighted()\n        overlay_image = cv2.addWeighted(\n            edited_image,\n            opacity_edited,\n            (mask_image *\n             np.array([0 / 255, 255 / 255, 0 / 255])).astype(np.uint8),\n            opacity_mask,\n            0,\n        )\n\n        return (\n            Image.fromarray(overlay_image),\n            clicked_points,\n            Image.fromarray(mask_image),\n        )\n\n    @torch.inference_mode()\n    @save_input_to_file  # for debug use\n    def process(\n        self,\n        source_image,\n        enable_all_generate,\n        mask_image,\n        control_scale,\n        enable_auto_prompt,\n        a_prompt,\n        n_prompt,\n        num_samples,\n        image_resolution,\n        detect_resolution,\n        ddim_steps,\n        guess_mode,\n        scale,\n        seed,\n        eta,\n        enable_tile=True,\n        refine_alignment_ratio=None,\n        refine_image_resolution=None,\n        alpha_weight=0.5,\n        use_scale_map=False,\n        condition_model=None,\n        ref_image=None,\n        attention_auto_machine_weight=1.0,\n        gn_auto_machine_weight=1.0,\n        style_fidelity=0.5,\n        reference_attn=True,\n        reference_adain=True,\n        ref_prompt=None,\n        ref_sam_scale=None,\n        ref_inpaint_scale=None,\n        ref_auto_prompt=False,\n        ref_textinv=True,\n        ref_textinv_path=None,\n        ref_scale=None,\n    ):\n\n        if condition_model is None or condition_model == \"EditAnything\":\n            this_controlnet_path = self.default_controlnet_path\n        else:\n            this_controlnet_path = condition_model\n        input_image = (\n            source_image[\"image\"]\n            if isinstance(source_image, dict)\n            else np.array(source_image, dtype=np.uint8)\n        )\n        if mask_image is None:\n            if enable_all_generate != self.defalut_enable_all_generate:\n                self.pipe = obtain_generation_model(\n                    self.base_model_path,\n                    self.lora_model_path,\n                    this_controlnet_path,\n                    enable_all_generate,\n                    self.extra_inpaint,\n                )\n                self.defalut_enable_all_generate = enable_all_generate\n            if enable_all_generate:\n                mask_image = (\n                    np.ones((input_image.shape[0],\n                             input_image.shape[1], 3)) * 255\n                )\n            else:\n                mask_image = source_image[\"mask\"]\n        else:\n            mask_image = np.array(mask_image, dtype=np.uint8)\n        if self.default_controlnet_path != this_controlnet_path:\n            print(\n                \"To Use:\",\n                this_controlnet_path,\n                \"Current:\",\n                self.default_controlnet_path,\n            )\n            print(\"Change condition model to:\", this_controlnet_path)\n            self.pipe = obtain_generation_model(\n                self.base_model_path,\n                self.lora_model_path,\n                this_controlnet_path,\n                enable_all_generate,\n                self.extra_inpaint,\n            )\n            self.default_controlnet_path = this_controlnet_path\n            torch.cuda.empty_cache()\n        if self.last_ref_infer:\n            print(\"Redefine the model to overwrite the ref mode\")\n            self.pipe = obtain_generation_model(\n                self.base_model_path,\n                self.lora_model_path,\n                this_controlnet_path,\n                enable_all_generate,\n                self.extra_inpaint,\n            )\n            self.last_ref_infer = False\n\n        if ref_image is not None:\n            ref_mask = ref_image[\"mask\"]\n            ref_image = ref_image[\"image\"]\n            if ref_auto_prompt or ref_textinv:\n                bbox = get_bounding_box(\n                    np.array(ref_mask) / 255\n                )  # reverse the mask to make 1 the choosen region\n                cropped_ref_mask = ref_mask.crop(\n                    (bbox[0], bbox[1], bbox[2], bbox[3]))\n                cropped_ref_image = ref_image.crop(\n                    (bbox[0], bbox[1], bbox[2], bbox[3]))\n                # cropped_ref_image.save(\"debug.jpg\")\n                cropped_ref_image = np.array(cropped_ref_image) * (\n                    np.array(cropped_ref_mask)[:, :, :3] / 255.0\n                )\n                cropped_ref_image = Image.fromarray(\n                    cropped_ref_image.astype(\"uint8\"))\n\n            if ref_auto_prompt:\n                generated_prompt = self.get_blip2_text(cropped_ref_image)\n                ref_prompt += generated_prompt\n                a_prompt += generated_prompt\n            print(\"Generated ref text:\", ref_prompt)\n            print(\"Generated input text:\", a_prompt)\n            self.last_ref_infer = True\n            # ref_image = cropped_ref_image\n            # ref_mask = cropped_ref_mask\n            if ref_textinv:\n                try:\n                    self.pipe.load_textual_inversion(ref_textinv_path)\n                    print(\"Load textinv embedding from:\", ref_textinv_path)\n                except:\n                    print(\"No textinvert embeddings found.\")\n                    ref_data_path = \"./utils/tmp/textinv/img\"\n                    if not os.path.exists(ref_data_path):\n                        os.makedirs(ref_data_path)\n                    cropped_ref_image.save(\n                        os.path.join(ref_data_path, 'ref.png'))\n                    print(\"Ref image region is save to:\", ref_data_path)\n                    print(\n                        \"Plese finetune with run_texutal_inversion.sh in utils folder to get the textinvert embeddings.\")\n\n        else:\n            ref_mask = None\n\n        with torch.no_grad():\n            if self.use_blip and enable_auto_prompt:\n                print(\"Generating text:\")\n                blip2_prompt = self.get_blip2_text(input_image)\n                print(\"Generated text:\", blip2_prompt)\n                if len(a_prompt) > 0:\n                    a_prompt = blip2_prompt + \",\" + a_prompt\n                else:\n                    a_prompt = blip2_prompt\n\n            input_image = HWC3(input_image)\n\n            img = resize_image(input_image, image_resolution)\n            H, W, C = img.shape\n\n            print(\"Generating SAM seg:\")\n            # the default SAM model is trained with 1024 size.\n            full_segmask, detected_map = self.get_sam_control(\n                resize_image(input_image, detect_resolution)\n            )\n\n            detected_map = HWC3(detected_map.astype(np.uint8))\n            detected_map = cv2.resize(\n                detected_map, (W, H), interpolation=cv2.INTER_LINEAR\n            )\n\n            control = torch.from_numpy(detected_map.copy()).float().cuda()\n            control = control.unsqueeze(dim=0)\n            control = einops.rearrange(control, \"b h w c -> b c h w\").clone()\n\n            mask_imag_ori = HWC3(mask_image.astype(np.uint8))\n            mask_image_tmp = cv2.resize(\n                mask_imag_ori, (W, H), interpolation=cv2.INTER_LINEAR\n            )\n            mask_image = Image.fromarray(mask_image_tmp)\n\n            if seed == -1:\n                seed = random.randint(0, 65535)\n            seed_everything(seed)\n            generator = torch.manual_seed(seed)\n            postive_prompt = a_prompt\n            negative_prompt = n_prompt\n            prompt_embeds, negative_prompt_embeds = get_pipeline_embeds(\n                self.pipe, postive_prompt, negative_prompt, \"cuda\"\n            )\n\n            if enable_all_generate and self.extra_inpaint:\n                if ref_image is not None:\n                    print(\"Not support yet.\")\n                    return\n                x_samples = self.pipe(\n                    prompt_embeds=prompt_embeds,\n                    negative_prompt_embeds=negative_prompt_embeds,\n                    num_images_per_prompt=num_samples,\n                    num_inference_steps=ddim_steps,\n                    generator=generator,\n                    height=H,\n                    width=W,\n                    image=[control.type(torch.float16)],\n                    controlnet_conditioning_scale=[float(control_scale)],\n                    guidance_scale=scale,\n                    guess_mode=guess_mode,\n                ).images\n            else:\n                multi_condition_image = []\n                multi_condition_scale = []\n                multi_condition_image.append(control.type(torch.float16))\n                multi_condition_scale.append(float(control_scale))\n                ref_multi_condition_scale = []\n                if ref_image is not None:\n                    ref_multi_condition_scale.append(float(ref_sam_scale))\n                if self.extra_inpaint:\n                    inpaint_image = make_inpaint_condition(img, mask_image_tmp)\n                    multi_condition_image.append(\n                        inpaint_image.type(torch.float16))\n                    multi_condition_scale.append(1.0)\n                    if ref_image is not None:\n                        ref_multi_condition_scale.append(\n                            float(ref_inpaint_scale))\n                if use_scale_map:\n                    scale_map_tmp = source_image[\"mask\"]\n                    tmp = HWC3(scale_map_tmp.astype(np.uint8))\n                    scale_map_tmp = cv2.resize(\n                        tmp, (W, H), interpolation=cv2.INTER_LINEAR)\n                    scale_map_tmp = Image.fromarray(scale_map_tmp)\n                    controlnet_conditioning_scale_map = 1.0 - \\\n                        prepare_mask_image(scale_map_tmp).float()\n                    print('scale map:', controlnet_conditioning_scale_map.size())\n                else:\n                    controlnet_conditioning_scale_map = None\n\n                if isinstance(self.pipe, StableDiffusionControlNetInpaintMixingPipeline):\n                    x_samples = self.pipe(\n                        image=img,\n                        mask_image=mask_image,\n                        prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds,\n                        num_images_per_prompt=num_samples,\n                        num_inference_steps=ddim_steps,\n                        generator=generator,\n                        controlnet_conditioning_image=multi_condition_image,\n                        height=H,\n                        width=W,\n                        controlnet_conditioning_scale=multi_condition_scale,\n                        guidance_scale=scale,\n                        alpha_weight=alpha_weight,\n                        controlnet_conditioning_scale_map=controlnet_conditioning_scale_map\n                    ).images\n                else:\n                    x_samples = self.pipe(\n                        image=img,\n                        mask_image=mask_image,\n                        prompt_embeds=prompt_embeds,\n                        negative_prompt_embeds=negative_prompt_embeds,\n                        num_images_per_prompt=num_samples,\n                        num_inference_steps=ddim_steps,\n                        generator=generator,\n                        controlnet_conditioning_image=multi_condition_image,\n                        height=H,\n                        width=W,\n                        controlnet_conditioning_scale=multi_condition_scale,\n                        guidance_scale=scale,\n                        ref_image=ref_image,\n                        ref_mask=ref_mask,\n                        ref_prompt=ref_prompt,\n                        attention_auto_machine_weight=attention_auto_machine_weight,\n                        gn_auto_machine_weight=gn_auto_machine_weight,\n                        style_fidelity=style_fidelity,\n                        reference_attn=reference_attn,\n                        reference_adain=reference_adain,\n                        ref_controlnet_conditioning_scale=ref_multi_condition_scale,\n                        guess_mode=guess_mode,\n                        ref_scale=ref_scale,\n                    ).images\n            results = [x_samples[i] for i in range(num_samples)]\n\n            results_tile = []\n            if enable_tile:\n                prompt_embeds, negative_prompt_embeds = get_pipeline_embeds(\n                    self.tile_pipe, postive_prompt, negative_prompt, \"cuda\"\n                )\n                for i in range(num_samples):\n                    img_tile = PIL.Image.fromarray(\n                        resize_image(\n                            np.array(x_samples[i]), refine_image_resolution)\n                    )\n                    if i == 0:\n                        mask_image_tile = cv2.resize(\n                            mask_imag_ori,\n                            (img_tile.size[0], img_tile.size[1]),\n                            interpolation=cv2.INTER_LINEAR,\n                        )\n                        mask_image_tile = Image.fromarray(mask_image_tile)\n                    if isinstance(self.pipe, StableDiffusionControlNetInpaintMixingPipeline):\n                        x_samples_tile = self.tile_pipe(\n                            image=img_tile,\n                            mask_image=mask_image_tile,\n                            prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds,\n                            num_images_per_prompt=1,\n                            num_inference_steps=ddim_steps,\n                            generator=generator,\n                            controlnet_conditioning_image=img_tile,\n                            height=img_tile.size[1],\n                            width=img_tile.size[0],\n                            controlnet_conditioning_scale=1.0,\n                            alignment_ratio=refine_alignment_ratio,\n                            guidance_scale=scale,\n                            alpha_weight=alpha_weight,\n                            controlnet_conditioning_scale_map=controlnet_conditioning_scale_map\n                        ).images\n                    else:\n                        x_samples_tile = self.tile_pipe(\n                            image=img_tile,\n                            mask_image=mask_image_tile,\n                            prompt_embeds=prompt_embeds,\n                            negative_prompt_embeds=negative_prompt_embeds,\n                            num_images_per_prompt=1,\n                            num_inference_steps=ddim_steps,\n                            generator=generator,\n                            controlnet_conditioning_image=img_tile,\n                            height=img_tile.size[1],\n                            width=img_tile.size[0],\n                            controlnet_conditioning_scale=1.0,\n                            alignment_ratio=refine_alignment_ratio,\n                            guidance_scale=scale,\n                            guess_mode=guess_mode,\n                        ).images\n                    results_tile += x_samples_tile\n\n        return results_tile, results, [full_segmask, mask_image], postive_prompt\n\n    def download_image(url):\n        response = requests.get(url)\n        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n"
        },
        {
          "name": "editany_nogradio.py",
          "type": "blob",
          "size": 0.5380859375,
          "content": "import pickle\nfrom editany_lora import EditAnythingLoraModel\nmodel = EditAnythingLoraModel(\n    base_model_path=\"runwayml/stable-diffusion-v1-5\",\n    controlmodel_name='LAION Pretrained(v0-4)-SD15',\n    lora_model_path=None, use_blip=False, extra_inpaint=True,\n)\n\nwith open('input_data.pkl', 'rb') as f:\n    input_data = pickle.load(f)\n\nprint(input_data)\n    \nrefined, output, ref, text = model.process(*input_data['args'], **input_data['kwargs'])\n\noutput\n\n# a woman in a tan suit and white shirt\n\n# best quality, extremely detailed,iron man wallpaper"
        },
        {
          "name": "editany_test.py",
          "type": "blob",
          "size": 7.41015625,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nimport os\nimport gradio as gr\nfrom diffusers.utils import load_image\nfrom editany_lora import EditAnythingLoraModel, config_dict\nfrom editany_demo import create_demo_template\nfrom huggingface_hub import hf_hub_download, snapshot_download\n\n\ndef create_demo(process, process_image_click=None):\n\n    examples = [\n        [\n            \"dudou,1girl, beautiful face, solo, candle, brown hair, long hair, <lora:flowergirl:0.9>,ulzzang-6500-v1.1,(raw photo:1.2),((photorealistic:1.4))best quality ,masterpiece, illustration, an extremely delicate and beautiful, extremely detailed ,CG ,unity ,8k wallpaper, Amazing, finely detail, masterpiece,best quality,official art,extremely detailed CG unity 8k wallpaper,absurdres, incredibly absurdres, huge filesize, ultra-detailed, highres, extremely detailed,beautiful detailed girl, extremely detailed eyes and face, beautiful detailed eyes,cinematic lighting,1girl,see-through,looking at viewer,full body,full-body shot,outdoors,arms behind back,(chinese clothes) <lora:cuteGirlMix4_v10:1>\",\n            \"(((mole))),sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, bad anatomy,(long hair:1.4),DeepNegative,(fat:1.2),facing away, looking away,tilted head, lowres,bad anatomy,bad hands, text, error, missing fingers,extra digit, fewer digits, cropped, worstquality, low quality, normal quality,jpegartifacts,signature, watermark, username,blurry,bad feet,cropped,poorly drawn hands,poorly drawn face,mutation,deformed,worst quality,low quality,normal quality,jpeg artifacts,signature,watermark,extra fingers,fewer digits,extra limbs,extra arms,extra legs,malformed limbs,fused fingers,too many fingers,long neck,cross-eyed,mutated hands,polar lowres,bad body,bad proportions,gross proportions,text,error,missing fingers,missing arms,missing legs,extra digit, extra arms, extra leg, extra foot,(freckles),(mole:2)\",\n            5,\n        ],\n        [\n            \"best quality, ultra high res, (photorealistic:1.4), (detailed beautiful girl:1.4), (medium breasts:0.8), looking_at_viewer, Detailed facial details, beautiful detailed eyes, (multicolored|blue|pink hair: 1.2), green eyes, slender, haunting smile, (makeup:0.3), red lips, <lora:cuteGirlMix4_v10:0.7>, highly detailed clothes, (ulzzang-6500-v1.1:0.3)\",\n            \"EasyNegative, paintings, sketches, ugly, 3d, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, manboobs, backlight,(ugly:1.3), (duplicate:1.3), (morbid:1.2), (mutilated:1.2), (tranny:1.3), mutated hands, (poorly drawn hands:1.3), blurry, (bad anatomy:1.2), (bad proportions:1.3), extra limbs, (disfigured:1.3), (more than 2 nipples:1.3), (more than 1 navel:1.3), (missing arms:1.3), (extra legs:1.3), (fused fingers:1.6), (too many fingers:1.6), (unclear eyes:1.3), bad hands, missing fingers, extra digit, (futa:1.1), bad body, double navel, mutad arms, hused arms, (puffy nipples, dark areolae, dark nipples, rei no himo, inverted nipples, long nipples), NG_DeepNegative_V1_75t, pubic hair, fat rolls, obese, bad-picture-chill-75v\",\n            8,\n        ],\n        [\n            \"best quality, ultra high res, (photorealistic:1.4), (detailed beautiful girl:1.4), (medium breasts:0.8), looking_at_viewer, Detailed facial details, beautiful detailed eyes, (blue|pink hair), green eyes, slender, smile, (makeup:0.4), red lips, (full body, sitting, beach), <lora:cuteGirlMix4_v10:0.7>, highly detailed clothes, (ulzzang-6500-v1.1:0.3)\",\n            \"asyNegative, paintings, sketches, ugly, 3d, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, manboobs, backlight,(ugly:1.3), (duplicate:1.3), (morbid:1.2), (mutilated:1.2), (tranny:1.3), mutated hands, (poorly drawn hands:1.3), blurry, (bad anatomy:1.2), (bad proportions:1.3), extra limbs, (disfigured:1.3), (more than 2 nipples:1.3), (more than 1 navel:1.3), (missing arms:1.3), (extra legs:1.3), (fused fingers:1.6), (too many fingers:1.6), (unclear eyes:1.3), bad hands, missing fingers, extra digit, (futa:1.1), bad body, double navel, mutad arms, hused arms, (puffy nipples, dark areolae, dark nipples, rei no himo, inverted nipples, long nipples), NG_DeepNegative_V1_75t, pubic hair, fat rolls, obese, bad-picture-chill-75v\",\n            7,\n        ],\n        [\n            \"mix4, whole body shot, ((8k, RAW photo, highest quality, masterpiece), High detail RAW color photo professional close-up photo, shy expression, cute, beautiful detailed girl, detailed fingers, extremely detailed eyes and face, beautiful detailed nose, beautiful detailed eyes, long eyelashes, light on face, looking at viewer, (closed mouth:1.2), 1girl, cute, young, mature face, (full body:1.3), ((small breasts)), realistic face, realistic body, beautiful detailed thigh,s, same eyes color, (realistic, photo realism:1. 37), (highest quality), (best shadow), (best illustration), ultra high resolution, physics-based rendering, cinematic lighting), solo, 1girl, highly detailed, in office, detailed office, open cardigan, ponytail contorted, beautiful eyes ,sitting in office,dating, business suit, cross-laced clothes, collared shirt, beautiful breast, small breast, Chinese dress, white pantyhose, natural breasts, pink and white hair, <lora:cuteGirlMix4_v10:1>\",\n            \"paintings, sketches, (worst quality:2), (low quality:2), (normal quality:2), cloth, underwear, bra, low-res, normal quality, ((monochrome)), ((grayscale)), skin spots, acne, skin blemishes, age spots, glans, bad nipples, long nipples, bad vagina, extra fingers,fewer fingers,strange fingers,bad hand, ng_deepnegative_v1_75t, bad-picture-chill-75v\",\n            7,\n        ],\n    ]\n    INFO = f\"\"\"\n    ## Generate Your Beauty powered by EditAnything https://github.com/sail-sg/EditAnything\n    This model is good at generating beautiful female.\n    \"\"\"\n    WARNING_INFO = f\"\"\"### [NOTE]  the model is collected from the Internet for demo only, please do not use it for commercial purposes.\n    We are not responsible for possible risks using this model.\n    Lora model from https://civitai.com/models/14171/cutegirlmix4 Thanks!\n    \"\"\"\n    demo = create_demo_template(\n        process,\n        process_image_click,\n        examples=examples,\n        INFO=INFO,\n        WARNING_INFO=WARNING_INFO,\n    )\n    return demo\n\n\nif __name__ == \"__main__\":\n    # sd_models_path = snapshot_download(\"shgao/sdmodels\")\n    # lora_model_path = hf_hub_download(\n    #     \"mlida/Cute_girl_mix4\", \"cuteGirlMix4_v10.safetensors\")\n    # model = EditAnythingLoraModel(base_model_path=\"andite/anything-v4.0\",\n    #                               lora_model_path=None, use_blip=True, extra_inpaint=True,\n    #                               lora_weight=0.5,\n    #                               )\n    sd_models_path = snapshot_download(\"shgao/sdmodels\")\n    lora_model_path = hf_hub_download(\n        \"mlida/Cute_girl_mix4\", \"cuteGirlMix4_v10.safetensors\"\n    )\n    model = EditAnythingLoraModel(\n        base_model_path=os.path.join(\n            sd_models_path, \"chilloutmix_NiPrunedFp32Fix\"),\n        lora_model_path=lora_model_path,\n        use_blip=True,\n        extra_inpaint=True,\n        lora_weight=0.5,\n    )\n    demo = create_demo(model.process, model.process_image_click)\n    demo.queue().launch(server_name=\"0.0.0.0\", share=True)\n"
        },
        {
          "name": "environment.yaml",
          "type": "blob",
          "size": 0.859375,
          "content": "name: control\nchannels:\n  - pytorch\n  - defaults\ndependencies:\n  - python=3.8.5\n  - pip=20.3\n  - cudatoolkit=11.3\n  - pytorch=1.13.1\n  - torchvision=0.14.1\n  - numpy=1.23.1\n  - pip:\n      - gradio==3.35.2\n      - albumentations==1.3.0\n      - opencv-contrib-python==4.3.0.36\n      - imageio==2.9.0\n      - imageio-ffmpeg==0.4.2\n      - pytorch-lightning==1.5.0\n      - omegaconf==2.1.1\n      - test-tube>=0.7.5\n      - streamlit==1.12.1\n      - einops==0.3.0\n      - webdataset==0.2.5\n      - kornia==0.6\n      - open_clip_torch==2.0.2\n      - invisible-watermark>=0.1.5\n      - streamlit-drawable-canvas==0.8.0\n      - torchmetrics==0.6.0\n      - timm==0.6.12\n      - addict==2.4.0\n      - yapf==0.32.0\n      - prettytable==3.6.0\n      - safetensors==0.2.7\n      - basicsr==1.4.2\n      - diffusers==0.17.1\n      - accelerate==0.17.0\n      - transformers==4.30.2\n      - xformers\n"
        },
        {
          "name": "font",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "ldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.4580078125,
          "content": "numpy==1.23.1\ngradio\nalbumentations==1.3.0\nopencv-contrib-python\nimageio==2.9.0\nimageio-ffmpeg==0.4.2\npytorch-lightning==1.5.0\nomegaconf==2.1.1\ntest-tube>=0.7.5\nstreamlit==1.12.1\neinops==0.3.0\nwebdataset==0.2.5\nkornia==0.6\nopen_clip_torch==2.0.2\ninvisible-watermark>=0.1.5\nstreamlit-drawable-canvas==0.8.0\ntorchmetrics==0.6.0\ntimm==0.6.12\naddict==2.4.0\nyapf==0.32.0\nprettytable==3.6.0\nsafetensors==0.2.7\nbasicsr==1.4.2\ndiffusers\naccelerate==0.17.0\ntransformers==4.27.4\n"
        },
        {
          "name": "sam2groundingdino_edit.py",
          "type": "blob",
          "size": 13.763671875,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nfrom torchvision.utils import save_image\nfrom PIL import Image\nfrom pytorch_lightning import seed_everything\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\nimport requests\nfrom io import BytesIO\nfrom annotator.util import resize_image, HWC3\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nuse_blip = False\nuse_gradio = False\n\n# Diffusion init using diffusers.\n\nimport groundingdino.datasets.transforms as T\nfrom groundingdino.models import build_model\nfrom groundingdino.util import box_ops\nfrom groundingdino.util.slconfig import SLConfig\nfrom groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\nfrom groundingdino.util.inference import annotate, load_image, predict\nfrom segment_anything import build_sam, SamPredictor\nfrom segment_anything.utils.amg import remove_small_regions\n\n# diffusers==0.14.0 required.\nfrom diffusers import ControlNetModel, UniPCMultistepScheduler\nfrom utils.stable_diffusion_controlnet_inpaint import StableDiffusionControlNetInpaintPipeline\nimport torch\n\nbase_model_path = \"stabilityai/stable-diffusion-2-inpainting\"\ncontrolnet_path = \"shgao/edit-anything-v0-1-1\"\n\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n    base_model_path, controlnet=controlnet, torch_dtype=torch.float16\n)\n# speed up diffusion process with faster scheduler and memory optimization\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# remove following line if xformers is not installed\npipe.enable_xformers_memory_efficient_attention()\n\n# pipe.enable_model_cpu_offload() # disable for now because of unknow bug in accelerate\npipe.to(device)\n\n# Segment-Anything init.\n# pip install git+https://github.com/facebookresearch/segment-anything.git\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n\nsam_checkpoint = \"./models/sam_vit_h_4b8939.pth\"\ngroundingdino_checkpoint = \"./models/groundingdino_swint_ogc.pth\"\ngroundingdino_config_file = \"./GroundingDINO_SwinT_OGC.py\"\nmodel_type = \"default\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\n\n\ndef load_groundingdino_model(model_config_path, model_checkpoint_path):\n    args = SLConfig.fromfile(model_config_path)\n    args.device = device\n    model = build_model(args)\n    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n    print(load_res)\n    _ = model.eval()\n    return model\n\n\ngrounding_model = load_groundingdino_model(groundingdino_config_file, groundingdino_checkpoint).to(device)\nsam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device=device))\n\n# BLIP2 init.\nif use_blip:\n    # need the latest transformers\n    # pip install git+https://github.com/huggingface/transformers.git\n    from transformers import AutoProcessor, Blip2ForConditionalGeneration\n\n    processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n    blip_model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n    blip_model.to(device)\n    blip_model.to(device)\n\n\ndef get_blip2_text(image):\n    inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = blip_model.generate(**inputs, max_new_tokens=50)\n    generated_text = processor.batch_decode(\n        generated_ids, skip_special_tokens=True)[0].strip()\n    return generated_text\n\n\ndef show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    full_img = None\n\n    # for ann in sorted_anns:\n    for i in range(len(sorted_anns)):\n        ann = anns[i]\n        m = ann['segmentation']\n        if full_img is None:\n            full_img = np.zeros((m.shape[0], m.shape[1], 3))\n            map = np.zeros((m.shape[0], m.shape[1]), dtype=np.uint16)\n        map[m != 0] = i + 1\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        full_img[m != 0] = color_mask\n    full_img = full_img * 255\n    # anno encoding from https://github.com/LUSSeg/ImageNet-S\n    res = np.zeros((map.shape[0], map.shape[1], 3))\n    res[:, :, 0] = map % 256\n    res[:, :, 1] = map // 256\n    res.astype(np.float32)\n    return full_img, res\n\n\ndef get_sam_control(image):\n    masks = mask_generator.generate(image)\n    full_img, res = show_anns(masks)\n    return full_img, res\n\n\ndef prompt2mask(original_image, caption, box_threshold=0.25, text_threshold=0.25, num_boxes=2):\n    def image_transform_grounding(init_image):\n        transform = T.Compose([\n            T.RandomResize([800], max_size=1333),\n            T.ToTensor(),\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        image, _ = transform(init_image, None)  # 3, h, w\n        return init_image, image\n\n    image_np = np.array(original_image, dtype=np.uint8)\n    caption = caption.lower()\n    caption = caption.strip()\n    if not caption.endswith(\".\"):\n        caption = caption + \".\"\n    _, image_tensor = image_transform_grounding(original_image)\n    boxes, logits, phrases = predict(grounding_model,\n                                     image_tensor, caption, box_threshold, text_threshold, device='cpu')\n    print(logits)\n    print('number of boxes: ', boxes.size(0))\n    # exit(0)\n    # from PIL import Image, ImageDraw, ImageFont\n    H, W = original_image.size[1], original_image.size[0]\n    boxes = boxes * torch.Tensor([W, H, W, H])\n    boxes[:, :2] = boxes[:, :2] - boxes[:, 2:] / 2\n    boxes[:, 2:] = boxes[:, 2:] + boxes[:, :2]\n    # draw = ImageDraw.Draw(original_image)\n    # for box in boxes:\n    #     # from 0..1 to 0..W, 0..H\n    #     # box = box * torch.Tensor([W, H, W, H])\n    #     # # from xywh to xyxy\n    #     # box[:2] -= box[2:] / 2\n    #     # box[2:] += box[:2]\n    #     # random color\n    #     color = tuple(np.random.randint(0, 255, size=3).tolist())\n    #     # draw\n    #     x0, y0, x1, y1 = box\n    #     x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n    #\n    #     draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n    # original_image.save('debug.jpg')\n    # exit(0)\n\n    final_m = torch.zeros((image_np.shape[0], image_np.shape[1]))\n\n    if boxes.size(0) > 0:\n        sam_predictor.set_image(image_np)\n\n        transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes, image_np.shape[:2])\n        masks, _, _ = sam_predictor.predict_torch(\n            point_coords=None,\n            point_labels=None,\n            boxes=transformed_boxes.to(device),\n            multimask_output=False,\n        )\n\n        # remove small disconnected regions and holes\n        fine_masks = []\n        for mask in masks.to('cpu').numpy():  # masks: [num_masks, 1, h, w]\n            fine_masks.append(remove_small_regions(mask[0], 400, mode=\"holes\")[0])\n        masks = np.stack(fine_masks, axis=0)[:, np.newaxis]\n        masks = torch.from_numpy(masks)\n\n        num_obj = min(len(logits), num_boxes)\n        for obj_ind in range(num_obj):\n            # box = boxes[obj_ind]\n\n            m = masks[obj_ind][0]\n            final_m += m\n    final_m = (final_m > 0).to('cpu').numpy()\n    # print(final_m.max(), final_m.min())\n    return np.dstack((final_m, final_m, final_m)) * 255\n\n\ndef process(input_image, mask_prompt, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution,\n            ddim_steps, guess_mode, strength, scale, seed, eta):\n    with torch.no_grad():\n        mask_image = np.array(prompt2mask(input_image, mask_prompt), dtype=np.uint8)\n        input_image = np.array(input_image, dtype=np.uint8)[:, :, :3]\n\n        if use_blip:\n            print(\"Generating text:\")\n            blip2_prompt = get_blip2_text(input_image)\n            print(\"Generated text:\", blip2_prompt)\n            if len(prompt) > 0:\n                prompt = blip2_prompt + ',' + prompt\n            else:\n                prompt = blip2_prompt\n            print(\"All text:\", prompt)\n\n        input_image = HWC3(input_image)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        print(\"Generating SAM seg:\")\n        # the default SAM model is trained with 1024 size.\n        full_segmask, detected_map = get_sam_control(\n            resize_image(input_image, detect_resolution))\n\n        detected_map = HWC3(detected_map.astype(np.uint8))\n        detected_map = cv2.resize(\n            detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(\n            detected_map.copy()).float().cuda()\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        mask_image = HWC3(mask_image.astype(np.uint8))\n        mask_image = cv2.resize(\n            mask_image, (W, H), interpolation=cv2.INTER_LINEAR)\n        mask_image = Image.fromarray(mask_image)\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n        generator = torch.manual_seed(seed)\n        x_samples = pipe(\n            image=img,\n            mask_image=mask_image,\n            prompt=[prompt + ', ' + a_prompt] * num_samples,\n            negative_prompt=[n_prompt] * num_samples,\n            num_images_per_prompt=num_samples,\n            num_inference_steps=ddim_steps,\n            generator=generator,\n            controlnet_conditioning_image=control.type(torch.float16),\n            height=H,\n            width=W,\n        ).images\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [full_segmask, mask_image] + results\n\n\ndef download_image(url):\n    response = requests.get(url)\n    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n\n# disable gradio when not using GUI.\nif not use_gradio:\n    image_path = \"assets/dog.png\"\n    input_image_pil = Image.open(image_path).convert('RGB')\n    input_image = np.array(input_image_pil, dtype=np.uint8)[:, :, :3]\n\n    mask_prompt = 'bench.'\n    # mask_image = np.array(prompt2mask(input_image, mask_prompt), dtype=np.uint8)\n    prompt = \"cat\"\n    a_prompt = 'best quality, extremely detailed'\n    n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n    num_samples = 3\n    image_resolution = 512\n    detect_resolution = 512\n    ddim_steps = 30\n    guess_mode = False\n    strength = 1.0\n    scale = 9.0\n    seed = -1\n    eta = 0.0\n\n    outputs = process(input_image_pil, mask_prompt, prompt, a_prompt, n_prompt, num_samples, image_resolution,\n                      detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta)\n\n    image_list = []\n    input_image = resize_image(input_image, 512)\n    image_list.append(torch.tensor(input_image))\n    for i in range(len(outputs)):\n        each = outputs[i]\n        if type(each) is not np.ndarray:\n            each = np.array(each, dtype=np.uint8)\n        each = resize_image(each, 512)\n        print(i, each.shape)\n        image_list.append(torch.tensor(each))\n\n    image_list = torch.stack(image_list).permute(0, 3, 1, 2)\n\n    save_image(image_list, \"sample.jpg\", nrow=3,\n               normalize=True, value_range=(0, 255))\nelse:\n    print(\"The GUI is not tested yet. Please open an issue if you find bugs.\")\n    block = gr.Blocks().queue()\n    with block:\n        with gr.Row():\n            gr.Markdown(\n                \"## Edit Anything powered by ControlNet+SAM+BLIP2+Stable Diffusion\")\n        with gr.Row():\n            with gr.Column():\n                input_image = gr.Image(source='upload', type=\"numpy\")\n                prompt = gr.Textbox(label=\"Prompt\")\n                run_button = gr.Button(label=\"Run\")\n                with gr.Accordion(\"Advanced options\", open=False):\n                    num_samples = gr.Slider(\n                        label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n                    image_resolution = gr.Slider(\n                        label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                    strength = gr.Slider(\n                        label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                    guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                    detect_resolution = gr.Slider(\n                        label=\"SAM Resolution\", minimum=128, maximum=2048, value=1024, step=1)\n                    ddim_steps = gr.Slider(\n                        label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                    scale = gr.Slider(\n                        label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                    seed = gr.Slider(label=\"Seed\", minimum=-1,\n                                     maximum=2147483647, step=1, randomize=True)\n                    eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n                    mask_prompt = gr.Textbox(\n                        label=\"Mask Prompt\", value='best quality, extremely detailed')\n                    a_prompt = gr.Textbox(\n                        label=\"Added Prompt\", value='best quality, extremely detailed')\n                    n_prompt = gr.Textbox(label=\"Negative Prompt\",\n                                          value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n            with gr.Column():\n                result_gallery = gr.Gallery(\n                    label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n        ips = [input_image, mask_prompt, prompt, a_prompt, n_prompt, num_samples, image_resolution,\n               detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n        run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n    block.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "sam2image.py",
          "type": "blob",
          "size": 12.083984375,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nfrom diffusers.utils import load_image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom torchvision.utils import save_image\nfrom PIL import Image\nfrom pytorch_lightning import seed_everything\nimport subprocess\nfrom collections import OrderedDict\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\nimport os\nfrom annotator.util import resize_image, HWC3\n\n\ndef create_demo():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    use_blip = True\n    use_gradio = True\n\n    # Diffusion init using diffusers.\n    # diffusers==0.14.0 required.\n\n    base_model_path = \"stabilityai/stable-diffusion-2-1\"\n\n    config_dict = OrderedDict([('SAM Pretrained(v0-1)', 'shgao/edit-anything-v0-1-1'),\n                               ('LAION Pretrained(v0-3)', 'shgao/edit-anything-v0-3'),\n                               ('LAION Pretrained(v0-4)', 'shgao/edit-anything-v0-4-sd21'),\n                               ])\n\n    def obtain_generation_model(controlnet_path):\n        controlnet = ControlNetModel.from_pretrained(\n            controlnet_path, torch_dtype=torch.float16)\n        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n            base_model_path, controlnet=controlnet, torch_dtype=torch.float16\n        )\n        # speed up diffusion process with faster scheduler and memory optimization\n        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n        # remove following line if xformers is not installed\n        pipe.enable_xformers_memory_efficient_attention()\n        # pipe.enable_model_cpu_offload() # disable for now because of unknow bug in accelerate\n        pipe.to(device)\n        return pipe\n\n    global default_controlnet_path\n    default_controlnet_path = config_dict['LAION Pretrained(v0-4)']\n    pipe = obtain_generation_model(default_controlnet_path)\n\n    # Segment-Anything init.\n    # pip install git+https://github.com/facebookresearch/segment-anything.git\n    try:\n        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n    except ImportError:\n        print('segment_anything not installed')\n        result = subprocess.run(['pip', 'install', 'git+https://github.com/facebookresearch/segment-anything.git'],\n                                check=True)\n        print(f'Install segment_anything {result}')\n    if not os.path.exists('./models/sam_vit_h_4b8939.pth'):\n        result = subprocess.run(\n            ['wget', 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth', '-P', 'models'],\n            check=True)\n        print(f'Download sam_vit_h_4b8939.pth {result}')\n    sam_checkpoint = \"models/sam_vit_h_4b8939.pth\"\n    model_type = \"default\"\n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n    sam.to(device=device)\n    mask_generator = SamAutomaticMaskGenerator(sam)\n\n    # BLIP2 init.\n    if use_blip:\n        # need the latest transformers\n        # pip install git+https://github.com/huggingface/transformers.git\n        from transformers import AutoProcessor, Blip2ForConditionalGeneration\n\n        processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n        blip_model = Blip2ForConditionalGeneration.from_pretrained(\n            \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n        blip_model.to(device)\n        blip_model.to(device)\n\n    def get_blip2_text(image):\n        inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n        generated_ids = blip_model.generate(**inputs, max_new_tokens=50)\n        generated_text = processor.batch_decode(\n            generated_ids, skip_special_tokens=True)[0].strip()\n        return generated_text\n\n    def show_anns(anns):\n        if len(anns) == 0:\n            return\n        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n        full_img = None\n\n        # for ann in sorted_anns:\n        for i in range(len(sorted_anns)):\n            ann = anns[i]\n            m = ann['segmentation']\n            if full_img is None:\n                full_img = np.zeros((m.shape[0], m.shape[1], 3))\n                map = np.zeros((m.shape[0], m.shape[1]), dtype=np.uint16)\n            map[m != 0] = i + 1\n            color_mask = np.random.random((1, 3)).tolist()[0]\n            full_img[m != 0] = color_mask\n        full_img = full_img * 255\n        # anno encoding from https://github.com/LUSSeg/ImageNet-S\n        res = np.zeros((map.shape[0], map.shape[1], 3))\n        res[:, :, 0] = map % 256\n        res[:, :, 1] = map // 256\n        res.astype(np.float32)\n        full_img = Image.fromarray(np.uint8(full_img))\n        return full_img, res\n\n    def get_sam_control(image):\n        masks = mask_generator.generate(image)\n        full_img, res = show_anns(masks)\n        return full_img, res\n\n    def process(condition_model, input_image, enable_auto_prompt, prompt, a_prompt, n_prompt, num_samples,\n                image_resolution, detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta):\n\n        global default_controlnet_path\n        global pipe\n        print(\"To Use:\", config_dict[condition_model], \"Current:\", default_controlnet_path)\n        if default_controlnet_path != config_dict[condition_model]:\n            print(\"Change condition model to:\", config_dict[condition_model])\n            pipe = obtain_generation_model(config_dict[condition_model])\n            default_controlnet_path = config_dict[condition_model]\n\n        with torch.no_grad():\n            if use_blip and (enable_auto_prompt or len(prompt) == 0):\n                print(\"Generating text:\")\n                blip2_prompt = get_blip2_text(input_image)\n                print(\"Generated text:\", blip2_prompt)\n                if len(prompt) > 0:\n                    prompt = blip2_prompt + ',' + prompt\n                else:\n                    prompt = blip2_prompt\n                print(\"All text:\", prompt)\n\n            input_image = HWC3(input_image)\n\n            img = resize_image(input_image, image_resolution)\n            H, W, C = img.shape\n\n            print(\"Generating SAM seg:\")\n            # the default SAM model is trained with 1024 size.\n            full_segmask, detected_map = get_sam_control(\n                resize_image(input_image, detect_resolution))\n\n            detected_map = HWC3(detected_map.astype(np.uint8))\n            detected_map = cv2.resize(\n                detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n            control = torch.from_numpy(\n                detected_map.copy()).float().cuda()\n            control = torch.stack([control for _ in range(num_samples)], dim=0)\n            control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n            if seed == -1:\n                seed = random.randint(0, 65535)\n            seed_everything(seed)\n            print(\"control.shape\", control.shape)\n            generator = torch.manual_seed(seed)\n            x_samples = pipe(\n                prompt=[prompt + ', ' + a_prompt] * num_samples,\n                negative_prompt=[n_prompt] * num_samples,\n                num_images_per_prompt=num_samples,\n                num_inference_steps=ddim_steps,\n                generator=generator,\n                height=H,\n                width=W,\n                image=control.type(torch.float16),\n            ).images\n\n            results = [x_samples[i] for i in range(num_samples)]\n        return [full_segmask] + results, prompt\n\n    # disable gradio when not using GUI.\n    if not use_gradio:\n        # This part is not updated, it's just a example to use it without GUI.\n        condition_model = 'shgao/edit-anything-v0-1-1'\n        image_path = \"images/sa_309398.jpg\"\n        input_image = Image.open(image_path)\n        input_image = np.array(input_image, dtype=np.uint8)\n        prompt = \"\"\n        a_prompt = 'best quality, extremely detailed'\n        n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n        num_samples = 4\n        image_resolution = 512\n        detect_resolution = 512\n        ddim_steps = 100\n        guess_mode = False\n        strength = 1.0\n        scale = 9.0\n        seed = 10086\n        eta = 0.0\n\n        outputs, full_text = process(condition_model, input_image, prompt, a_prompt, n_prompt, num_samples,\n                                     image_resolution,\n                                     detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta)\n\n        image_list = []\n        input_image = resize_image(input_image, 512)\n        image_list.append(torch.tensor(input_image))\n        for i in range(len(outputs)):\n            each = outputs[i]\n            if type(each) is not np.ndarray:\n                each = np.array(each, dtype=np.uint8)\n            each = resize_image(each, 512)\n            print(i, each.shape)\n            image_list.append(torch.tensor(each))\n\n        image_list = torch.stack(image_list).permute(0, 3, 1, 2)\n\n        save_image(image_list, \"sample.jpg\", nrow=3,\n                   normalize=True, value_range=(0, 255))\n    else:\n        block = gr.Blocks()\n        with block as demo:\n            with gr.Row():\n                gr.Markdown(\n                    \"## Generate Anything\")\n            with gr.Row():\n                with gr.Column():\n                    input_image = gr.Image(source='upload', type=\"numpy\")\n                    prompt = gr.Textbox(label=\"Prompt (Optional)\")\n                    run_button = gr.Button(label=\"Run\")\n                    condition_model = gr.Dropdown(choices=list(config_dict.keys()),\n                                                  value=list(config_dict.keys())[0],\n                                                  label='Model',\n                                                  multiselect=False)\n                    num_samples = gr.Slider(\n                        label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n\n                    enable_auto_prompt = gr.Checkbox(label='Auto generated BLIP2 prompt', value=True)\n                    with gr.Accordion(\"Advanced options\", open=False):\n                        image_resolution = gr.Slider(\n                            label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                        strength = gr.Slider(\n                            label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                        guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                        detect_resolution = gr.Slider(\n                            label=\"SAM Resolution\", minimum=128, maximum=2048, value=1024, step=1)\n                        ddim_steps = gr.Slider(\n                            label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                        scale = gr.Slider(\n                            label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                        seed = gr.Slider(label=\"Seed\", minimum=-1,\n                                         maximum=2147483647, step=1, randomize=True)\n                        eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n                        a_prompt = gr.Textbox(\n                            label=\"Added Prompt\", value='best quality, extremely detailed')\n                        n_prompt = gr.Textbox(label=\"Negative Prompt\",\n                                              value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n\n                with gr.Column():\n                    result_gallery = gr.Gallery(\n                        label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n                    result_text = gr.Text(label='BLIP2+Human Prompt Text')\n            ips = [condition_model, input_image, enable_auto_prompt, prompt, a_prompt, n_prompt, num_samples,\n                   image_resolution,\n                   detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n            run_button.click(fn=process, inputs=ips, outputs=[result_gallery, result_text])\n            return demo\n\n\nif __name__ == '__main__':\n    demo = create_demo()\n    demo.queue().launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "sam2semantic.py",
          "type": "blob",
          "size": 6.51171875,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\n# pip install mmcv\n\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport subprocess\nfrom collections import OrderedDict\nimport numpy as np\nimport cv2\nimport textwrap\nimport torch\nimport os\nfrom annotator.util import resize_image, HWC3\nimport mmcv\nimport random\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # > 15GB GPU memory required\ndevice = \"cpu\"\nuse_blip = True\nuse_gradio = True\n\nif device == 'cpu':\n    data_type = torch.float32\nelse:\n    data_type = torch.float16\n# Diffusion init using diffusers.\n\n# diffusers==0.14.0 required.\nfrom diffusers.utils import load_image\n\nbase_model_path = \"stabilityai/stable-diffusion-2-inpainting\"\nconfig_dict = OrderedDict([('SAM Pretrained(v0-1): Good Natural Sense', 'shgao/edit-anything-v0-1-1'),\n                        ('LAION Pretrained(v0-3): Good Face', 'shgao/edit-anything-v0-3'),\n                        ('SD Inpainting: Not keep position', 'stabilityai/stable-diffusion-2-inpainting')\n                        ])\n\n# Segment-Anything init.\n# pip install git+https://github.com/facebookresearch/segment-anything.git\ntry:\n    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\nexcept ImportError:\n    print('segment_anything not installed')\n    result = subprocess.run(['pip', 'install', 'git+https://github.com/facebookresearch/segment-anything.git'], check=True)\n    print(f'Install segment_anything {result}')   \n    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\nif not os.path.exists('./models/sam_vit_h_4b8939.pth'):\n    result = subprocess.run(['wget', 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth', '-P', 'models'], check=True)\n    print(f'Download sam_vit_h_4b8939.pth {result}')   \nsam_checkpoint = \"models/sam_vit_h_4b8939.pth\"\nmodel_type = \"default\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\n\n\n# BLIP2 init.\nif use_blip:\n    # need the latest transformers\n    # pip install git+https://github.com/huggingface/transformers.git\n    from transformers import AutoProcessor, Blip2ForConditionalGeneration\n    processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n    blip_model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-opt-2.7b\", torch_dtype=data_type)\n\n\ndef region_classify_w_blip2(image):\n    inputs = processor(image, return_tensors=\"pt\").to(device, data_type)\n    generated_ids = blip_model.generate(**inputs, max_new_tokens=15)\n    generated_text = processor.batch_decode(\n        generated_ids, skip_special_tokens=True)[0].strip()\n    return generated_text\n\ndef region_level_semantic_api(image, topk=5):\n    \"\"\"\n    rank regions by area, and classify each region with blip2\n    Args:\n        image: numpy array\n        topk: int\n    Returns:\n        topk_region_w_class_label: list of dict with key 'class_label'\n    \"\"\"\n    topk_region_w_class_label = []\n    anns = mask_generator.generate(image)\n    if len(anns) == 0:\n        return []\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    for i in range(min(topk, len(sorted_anns))):\n        ann = anns[i]\n        m = ann['segmentation']\n        m_3c = m[:,:, np.newaxis]\n        m_3c = np.concatenate((m_3c,m_3c,m_3c), axis=2)\n        bbox = ann['bbox']\n        region = mmcv.imcrop(image*m_3c, np.array([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]), scale=1)\n        region_class_label = region_classify_w_blip2(region)\n        ann['class_label'] = region_class_label\n        print(ann['class_label'], str(bbox))\n        topk_region_w_class_label.append(ann)\n    return topk_region_w_class_label\n\ndef show_semantic_image_label(anns):\n    \"\"\"\n    show semantic image label for each region\n    Args:\n        anns: list of dict with key 'class_label'\n    Returns:\n        full_img: numpy array\n    \"\"\"\n    full_img = None\n    # generate mask image\n    for i in range(len(anns)):\n        m = anns[i]['segmentation']\n        if full_img is None:\n            full_img = np.zeros((m.shape[0], m.shape[1], 3))\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        full_img[m != 0] = color_mask\n    full_img = full_img*255\n    # add text on this mask image\n    for i in range(len(anns)):\n        m = anns[i]['segmentation']\n        class_label = anns[i]['class_label']\n        # add text to region\n        # Calculate the centroid of the region to place the text\n        y, x = np.where(m != 0)\n        x_center, y_center = int(np.mean(x)), int(np.mean(y))\n\n        # Split the text into multiple lines\n        max_width = 20  # Adjust this value based on your preferred maximum width\n        wrapped_text = textwrap.wrap(class_label, width=max_width)\n\n        # Add text to region\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        font_scale = 1.2\n        font_thickness = 2\n        font_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))  # red\n        line_spacing = 40  # Adjust this value based on your preferred line\n\n        for idx, line in enumerate(wrapped_text):\n            y_offset = y_center - (len(wrapped_text) - 1) * line_spacing // 2 + idx * line_spacing\n            text_size = cv2.getTextSize(line, font, font_scale, font_thickness)[0]\n            x_offset = x_center - text_size[0] // 2\n            # Draw the text multiple times with small offsets to create a bolder appearance\n            offsets = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n            for off_x, off_y in offsets:\n                cv2.putText(full_img, line, (x_offset + off_x, y_offset + off_y), font, font_scale, font_color, font_thickness, cv2.LINE_AA)\n\n    return full_img\n\n\n\nimage_path = \"images/sa_224577.jpg\"\ninput_image = Image.open(image_path)\ndetect_resolution=1024\ninput_image = resize_image(np.array(input_image, dtype=np.uint8), detect_resolution)\nregion_level_annots = region_level_semantic_api(input_image, topk=5)\noutput = show_semantic_image_label(region_level_annots)\n\nimage_list = []\ninput_image = resize_image(input_image, 512)\noutput = resize_image(output, 512)\ninput_image = np.array(input_image, dtype=np.uint8)\noutput = np.array(output, dtype=np.uint8)\nimage_list.append(torch.tensor(input_image).float())\nimage_list.append(torch.tensor(output).float())\nfor each in image_list:\n    print(each.shape, type(each))\n    print(each.max(), each.min())\n\n\nimage_list = torch.stack(image_list).permute(0, 3, 1, 2)\nprint(image_list.shape)\n\nsave_image(image_list, \"images/sample_semantic.jpg\", nrow=2,\n        normalize=True)\n\n"
        },
        {
          "name": "sam2vlpart_edit.py",
          "type": "blob",
          "size": 12.84375,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nfrom torchvision.utils import save_image\nfrom PIL import Image\nfrom cldm.ddim_hacked import DDIMSampler\nfrom cldm.model import create_model, load_state_dict\nfrom pytorch_lightning import seed_everything\nfrom share import *\nimport config\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\nimport os\nimport requests\nfrom io import BytesIO\nfrom annotator.util import resize_image, HWC3\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nuse_blip = False\nuse_gradio = False\n\n# Diffusion init using diffusers.\n\nfrom vlpart.vlpart import build_vlpart\nfrom segment_anything import build_sam, SamPredictor\nfrom segment_anything.utils.amg import remove_small_regions\nimport detectron2.data.transforms as T\n\n# diffusers==0.14.0 required.\nfrom diffusers import ControlNetModel, UniPCMultistepScheduler\nfrom utils.stable_diffusion_controlnet_inpaint import StableDiffusionControlNetInpaintPipeline\nfrom diffusers.utils import load_image\nimport torch\n\nbase_model_path = \"stabilityai/stable-diffusion-2-inpainting\"\ncontrolnet_path = \"shgao/edit-anything-v0-1-1\"\n\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n    base_model_path, controlnet=controlnet, torch_dtype=torch.float16\n)\n# speed up diffusion process with faster scheduler and memory optimization\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# remove following line if xformers is not installed\npipe.enable_xformers_memory_efficient_attention()\n\n# pipe.enable_model_cpu_offload() # disable for now because of unknow bug in accelerate\npipe.to(device)\n\n# Segment-Anything init.\n# pip install git+https://github.com/facebookresearch/segment-anything.git\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n\nsam_checkpoint = \"./models/sam_vit_h_4b8939.pth\"\nvlpart_checkpoint = \"./models/swinbase_part_0a0000.pth\"\nmodel_type = \"default\"\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\nmask_generator = SamAutomaticMaskGenerator(sam)\nvlpart = build_vlpart(checkpoint=vlpart_checkpoint)\nvlpart.to(device=device)\nsam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device=device))\n\n# BLIP2 init.\nif use_blip:\n    # need the latest transformers\n    # pip install git+https://github.com/huggingface/transformers.git\n    from transformers import AutoProcessor, Blip2ForConditionalGeneration\n\n    processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n    blip_model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n    blip_model.to(device)\n    blip_model.to(device)\n\n\ndef get_blip2_text(image):\n    inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n    generated_ids = blip_model.generate(**inputs, max_new_tokens=50)\n    generated_text = processor.batch_decode(\n        generated_ids, skip_special_tokens=True)[0].strip()\n    return generated_text\n\n\ndef show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    full_img = None\n\n    # for ann in sorted_anns:\n    for i in range(len(sorted_anns)):\n        ann = anns[i]\n        m = ann['segmentation']\n        if full_img is None:\n            full_img = np.zeros((m.shape[0], m.shape[1], 3))\n            map = np.zeros((m.shape[0], m.shape[1]), dtype=np.uint16)\n        map[m != 0] = i + 1\n        color_mask = np.random.random((1, 3)).tolist()[0]\n        full_img[m != 0] = color_mask\n    full_img = full_img * 255\n    # anno encoding from https://github.com/LUSSeg/ImageNet-S\n    res = np.zeros((map.shape[0], map.shape[1], 3))\n    res[:, :, 0] = map % 256\n    res[:, :, 1] = map // 256\n    res.astype(np.float32)\n    return full_img, res\n\n\ndef get_sam_control(image):\n    masks = mask_generator.generate(image)\n    full_img, res = show_anns(masks)\n    return full_img, res\n\n\ndef prompt2mask(original_image, text_prompt):\n    # original_image = original_image[:, :, :3]\n    preprocess = T.ResizeShortestEdge([800, 800], 1333)\n    height, width = original_image.shape[:2]\n    image = preprocess.get_transform(original_image).apply_image(original_image)\n    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n    inputs = {\"image\": image, \"height\": height, \"width\": width}\n    with torch.no_grad():\n        predictions = vlpart.inference([inputs], text_prompt=text_prompt)[0]\n    boxes, masks = None, None\n    filter_scores, filter_boxes, filter_classes = [], [], []\n\n    if \"instances\" in predictions:\n        instances = predictions['instances'].to('cpu')\n        boxes = instances.pred_boxes.tensor if instances.has(\"pred_boxes\") else None\n        scores = instances.scores if instances.has(\"scores\") else None\n        classes = instances.pred_classes.tolist() if instances.has(\"pred_classes\") else None\n\n        num_obj = len(scores)\n        for obj_ind in range(num_obj):\n            category_score = scores[obj_ind]\n            if category_score < 0.7:\n                continue\n            filter_scores.append(category_score)\n            filter_boxes.append(boxes[obj_ind])\n            filter_classes.append(classes[obj_ind])\n\n    final_m = torch.zeros((original_image.shape[0], original_image.shape[1]))\n\n    if len(filter_boxes) > 0:\n        # sam model inference\n        sam_predictor.set_image(original_image)\n\n        boxes_filter = torch.stack(filter_boxes)\n        transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_filter, original_image.shape[:2])\n        masks, _, _ = sam_predictor.predict_torch(\n            point_coords=None,\n            point_labels=None,\n            boxes=transformed_boxes.to(device),\n            multimask_output=False,\n        )\n\n        # remove small disconnected regions and holes\n        fine_masks = []\n        for mask in masks.to('cpu').numpy():  # masks: [num_masks, 1, h, w]\n            fine_masks.append(remove_small_regions(mask[0], 400, mode=\"holes\")[0])\n        masks = np.stack(fine_masks, axis=0)[:, np.newaxis]\n        masks = torch.from_numpy(masks)\n\n        num_obj = len(scores)\n        for obj_ind in range(num_obj):\n            # box = boxes[obj_ind]\n            score = scores[obj_ind]\n            if score < 0.5:\n                continue\n            m = masks[obj_ind][0]\n            final_m += m\n    final_m = (final_m > 0).to('cpu').numpy()\n    # print(final_m.max(), final_m.min())\n    return np.dstack((final_m, final_m, final_m)) * 255\n\n\ndef process(input_image, mask_prompt, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution,\n            ddim_steps, guess_mode, strength, scale, seed, eta):\n    with torch.no_grad():\n        mask_image = np.array(prompt2mask(input_image, mask_prompt), dtype=np.uint8)\n\n        if use_blip:\n            print(\"Generating text:\")\n            blip2_prompt = get_blip2_text(input_image)\n            print(\"Generated text:\", blip2_prompt)\n            if len(prompt) > 0:\n                prompt = blip2_prompt + ',' + prompt\n            else:\n                prompt = blip2_prompt\n            print(\"All text:\", prompt)\n\n        input_image = HWC3(input_image)\n\n        img = resize_image(input_image, image_resolution)\n        H, W, C = img.shape\n\n        print(\"Generating SAM seg:\")\n        # the default SAM model is trained with 1024 size.\n        full_segmask, detected_map = get_sam_control(\n            resize_image(input_image, detect_resolution))\n\n        detected_map = HWC3(detected_map.astype(np.uint8))\n        detected_map = cv2.resize(\n            detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n        control = torch.from_numpy(\n            detected_map.copy()).float().cuda()\n        control = torch.stack([control for _ in range(num_samples)], dim=0)\n        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n        mask_image = HWC3(mask_image.astype(np.uint8))\n        mask_image = cv2.resize(\n            mask_image, (W, H), interpolation=cv2.INTER_LINEAR)\n        mask_image = Image.fromarray(mask_image)\n\n        if seed == -1:\n            seed = random.randint(0, 65535)\n        seed_everything(seed)\n        generator = torch.manual_seed(seed)\n        x_samples = pipe(\n            image=img,\n            mask_image=mask_image,\n            prompt=[prompt + ', ' + a_prompt] * num_samples,\n            negative_prompt=[n_prompt] * num_samples,\n            num_images_per_prompt=num_samples,\n            num_inference_steps=ddim_steps,\n            generator=generator,\n            controlnet_conditioning_image=control.type(torch.float16),\n            height=H,\n            width=W,\n        ).images\n\n        results = [x_samples[i] for i in range(num_samples)]\n    return [full_segmask, mask_image] + results\n\n\ndef download_image(url):\n    response = requests.get(url)\n    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n\n# disable gradio when not using GUI.\nif not use_gradio:\n    image_path = \"assets/dog.png\"\n    input_image = Image.open(image_path)\n\n    input_image = np.array(input_image, dtype=np.uint8)[:, :, :3]\n\n    mask_prompt = 'dog head.'\n    # mask_image = np.array(prompt2mask(input_image, mask_prompt), dtype=np.uint8)\n    prompt = \"cute dogs\"\n    a_prompt = 'best quality, extremely detailed'\n    n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n    num_samples = 3\n    image_resolution = 512\n    detect_resolution = 512\n    ddim_steps = 30\n    guess_mode = False\n    strength = 1.0\n    scale = 9.0\n    seed = -1\n    eta = 0.0\n\n    outputs = process(input_image, mask_prompt, prompt, a_prompt, n_prompt, num_samples, image_resolution,\n                      detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta)\n\n    image_list = []\n    input_image = resize_image(input_image, 512)\n    image_list.append(torch.tensor(input_image))\n    for i in range(len(outputs)):\n        each = outputs[i]\n        if type(each) is not np.ndarray:\n            each = np.array(each, dtype=np.uint8)\n        each = resize_image(each, 512)\n        print(i, each.shape)\n        image_list.append(torch.tensor(each))\n\n    image_list = torch.stack(image_list).permute(0, 3, 1, 2)\n\n    save_image(image_list, \"sample.jpg\", nrow=3,\n               normalize=True, value_range=(0, 255))\nelse:\n    print(\"The GUI is not tested yet. Please open an issue if you find bugs.\")\n    block = gr.Blocks().queue()\n    with block:\n        with gr.Row():\n            gr.Markdown(\n                \"## Edit Anything powered by ControlNet+SAM+BLIP2+Stable Diffusion\")\n        with gr.Row():\n            with gr.Column():\n                input_image = gr.Image(source='upload', type=\"numpy\")\n                prompt = gr.Textbox(label=\"Prompt\")\n                run_button = gr.Button(label=\"Run\")\n                with gr.Accordion(\"Advanced options\", open=False):\n                    num_samples = gr.Slider(\n                        label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n                    image_resolution = gr.Slider(\n                        label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                    strength = gr.Slider(\n                        label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                    guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                    detect_resolution = gr.Slider(\n                        label=\"SAM Resolution\", minimum=128, maximum=2048, value=1024, step=1)\n                    ddim_steps = gr.Slider(\n                        label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                    scale = gr.Slider(\n                        label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                    seed = gr.Slider(label=\"Seed\", minimum=-1,\n                                     maximum=2147483647, step=1, randomize=True)\n                    eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n                    mask_prompt = gr.Textbox(\n                        label=\"Mask Prompt\", value='best quality, extremely detailed')\n                    a_prompt = gr.Textbox(\n                        label=\"Added Prompt\", value='best quality, extremely detailed')\n                    n_prompt = gr.Textbox(label=\"Negative Prompt\",\n                                          value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n            with gr.Column():\n                result_gallery = gr.Gallery(\n                    label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n        ips = [input_image, mask_prompt, prompt, a_prompt, n_prompt, num_samples, image_resolution,\n               detect_resolution, ddim_steps, guess_mode, strength, scale, seed, eta]\n        run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n\n    block.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "sam_train_sd21.py",
          "type": "blob",
          "size": 1.09375,
          "content": "from share import *\n\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nfrom utils.sam_dataset import SAMDataset\nfrom cldm.logger import ImageLogger\nfrom cldm.model import create_model, load_state_dict\nimport torch\n\n\n# Configs\nresume_path = './models/control_sd21_ini.ckpt'\nbatch_size = 4\nlogger_freq = 300\nlearning_rate = 1e-5\nsd_locked = True\nonly_mid_control = False\ndata_path = '../data/files'\ntxt_path = '../data/data_85616.txt'\n\n\n# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\nmodel = create_model('./models/cldm_v21.yaml').cpu()\nmodel.load_state_dict(load_state_dict(resume_path, location='cpu'))\nmodel.learning_rate = learning_rate\nmodel.sd_locked = sd_locked\nmodel.only_mid_control = only_mid_control\n\n\n# Misc\ndataset = SAMDataset(data_path=data_path, txt_path=txt_path)\ndataloader = DataLoader(dataset, num_workers=16,\n                        batch_size=batch_size, shuffle=True)\nlogger = ImageLogger(batch_frequency=logger_freq)\ntrainer = pl.Trainer(gpus=8, strategy=\"ddp\", precision=32, callbacks=[logger])\n\n\n# Train!\ntrainer.fit(model, dataloader)\n"
        },
        {
          "name": "share.py",
          "type": "blob",
          "size": 0.1513671875,
          "content": "import config\nfrom cldm.hack import disable_verbosity, enable_sliced_attention\n\n\ndisable_verbosity()\n\nif config.save_memory:\n    enable_sliced_attention()\n"
        },
        {
          "name": "sketch2image.py",
          "type": "blob",
          "size": 10.498046875,
          "content": "# Edit Anything trained with Stable Diffusion + ControlNet + SAM  + BLIP2\nfrom diffusers.utils import load_image\nfrom diffusers import UniPCMultistepScheduler\nfrom torchvision.utils import save_image\nfrom PIL import Image\nfrom pytorch_lightning import seed_everything\nimport subprocess\nfrom collections import OrderedDict\n\nimport cv2\nimport einops\nimport gradio as gr\nimport numpy as np\nimport torch\nimport random\nimport os\nfrom annotator.util import resize_image, HWC3\nimport base64\nfrom io import BytesIO\n\nfrom utils.stable_diffusion_controlnet import StableDiffusionControlNetPipeline2, ControlNetModel2\n\n\ndef create_demo():\n    MAX_COLORS = 12\n    canvas_html = \"<div id='canvas-root' style='max-width:400px; margin: 0 auto'></div>\"\n    load_js = \"\"\"\n    async () => {\n    const url = \"https://huggingface.co/datasets/radames/gradio-components/raw/main/sketch-canvas.js\"\n    fetch(url)\n      .then(res => res.text())\n      .then(text => {\n        const script = document.createElement('script');\n        script.type = \"module\"\n        script.src = URL.createObjectURL(new Blob([text], { type: 'application/javascript' }));\n        document.head.appendChild(script);\n      });\n    }\n    \"\"\"\n\n    get_js_colors = \"\"\"\n    async (canvasData) => {\n      const canvasEl = document.getElementById(\"canvas-root\");\n      return [canvasEl._data]\n    }\n    \"\"\"\n\n    set_canvas_size = \"\"\"\n    async (aspect) => {\n      if(aspect ==='square'){\n        _updateCanvas(512,512)\n      }\n      if(aspect ==='horizontal'){\n        _updateCanvas(768,512)\n      }\n      if(aspect ==='vertical'){\n        _updateCanvas(512,768)\n      }\n    }\n    \"\"\"\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # aspect = gr.Radio([\"square\", \"horizontal\", \"vertical\"], value=\"square\", label=\"Aspect Ratio\", visible=False if is_shared_ui else True)\n\n    # Diffusion init using diffusers.\n    # diffusers==0.14.0 required.\n\n    base_model_path = \"stabilityai/stable-diffusion-2-1\"\n\n    config_dict = OrderedDict([('SAM Pretrained(v0-1)', 'shgao/edit-anything-v0-1-1'),\n                               ('LAION Pretrained(v0-3)', 'shgao/edit-anything-v0-3'),\n                               ('LAION Pretrained(v0-4)', 'shgao/edit-anything-v0-4-sd21'),\n                               ])\n\n    def obtain_generation_model(controlnet_path):\n        controlnet = ControlNetModel2.from_pretrained(\n            controlnet_path, torch_dtype=torch.float16)\n        pipe = StableDiffusionControlNetPipeline2.from_pretrained(\n            base_model_path, controlnet=controlnet, torch_dtype=torch.float16\n        )\n        # speed up diffusion process with faster scheduler and memory optimization\n        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n        # remove following line if xformers is not installed\n        pipe.enable_xformers_memory_efficient_attention()\n        # pipe.enable_model_cpu_offload() # disable for now because of unknow bug in accelerate\n        pipe.to(device)\n        return pipe\n\n    global default_controlnet_path\n    default_controlnet_path = config_dict['LAION Pretrained(v0-4)']\n    pipe = obtain_generation_model(default_controlnet_path)\n\n    def get_sam_control(image):\n        im2arr = np.array(image)\n        colors_map, res = None, None\n        ptr = 0\n        for color in colors:\n            r, g, b = color\n            if any(c != 255 for c in (r, g, b)):\n                binary_matrix = np.all(im2arr == (r, g, b), axis=-1)\n                if colors_map is None:\n                    colors_map = np.zeros((im2arr.shape[0], im2arr.shape[1]), dtype=np.uint16)\n                    res = np.zeros((im2arr.shape[0], im2arr.shape[1], 3))\n                colors_map[binary_matrix != 0] = ptr + 1\n                ptr += 1\n        white = np.all(im2arr == (255, 255, 255), axis=-1)\n        scale_map = (white != 1).astype(np.float32)\n        res[:, :, 0] = colors_map % 256\n        res[:, :, 1] = colors_map // 256\n        res.astype(np.float32)\n        return image, res, scale_map\n\n    def process_sketch(canvas_data):\n        nonlocal colors\n        base64_img = canvas_data['image']\n        image_data = base64.b64decode(base64_img.split(',')[1])\n        image = Image.open(BytesIO(image_data)).convert(\"RGB\")\n        colors = [tuple(map(int, rgb[4:-1].split(','))) for rgb in canvas_data['colors']]\n        print(colors)\n        # binary_matrixes['sketch'] = res\n        return image, \"sketch loaded.\"\n\n    def process(condition_model, input_image, control_scale, prompt, a_prompt, n_prompt,\n                num_samples, image_resolution, ddim_steps, guess_mode, use_scale_map, strength, scale, seed, eta):\n\n        global default_controlnet_path\n        global pipe\n        print(\"To Use:\", config_dict[condition_model], \"Current:\", default_controlnet_path)\n        if default_controlnet_path != config_dict[condition_model]:\n            print(\"Change condition model to:\", config_dict[condition_model])\n            pipe = obtain_generation_model(config_dict[condition_model])\n            default_controlnet_path = config_dict[condition_model]\n\n        with torch.no_grad():\n            print(\"All text:\", prompt)\n\n            input_image = HWC3(input_image)\n\n            img = resize_image(input_image, image_resolution)\n            H, W, C = img.shape\n\n            # the default SAM model is trained with 1024 size.\n            fullseg, detected_map, scale_map = get_sam_control(input_image)\n\n            detected_map = HWC3(detected_map.astype(np.uint8))\n            detected_map = cv2.resize(\n                detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n\n            control = torch.from_numpy(\n                detected_map.copy()).float().cuda()\n            control = torch.stack([control for _ in range(num_samples)], dim=0)\n            control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n\n            scale_map = torch.from_numpy(scale_map).float().cuda() if use_scale_map else None\n\n            if seed == -1:\n                seed = random.randint(0, 65535)\n            seed_everything(seed)\n            print(\"control.shape\", control.shape)\n            generator = torch.manual_seed(seed)\n            x_samples = pipe(\n                prompt=[prompt + ', ' + a_prompt] * num_samples,\n                negative_prompt=[n_prompt] * num_samples,\n                num_images_per_prompt=num_samples,\n                num_inference_steps=ddim_steps,\n                generator=generator,\n                height=H,\n                width=W,\n                controlnet_conditioning_scale=float(control_scale),\n                controlnet_conditioning_scale_map=scale_map,\n                image=control.type(torch.float16),\n            ).images\n\n            results = [x_samples[i] for i in range(num_samples)]\n        return [fullseg] + results, prompt, \"waiting for sketch...\"\n\n    # disable gradio when not using GUI.\n    block = gr.Blocks()\n    with block as demo:\n        colors = []\n        with gr.Row():\n            gr.Markdown(\n                \"## Generate Anything\")\n        with gr.Row():\n            with gr.Column():\n                canvas_data = gr.JSON(value={}, visible=False)\n                canvas = gr.HTML(canvas_html)\n                aspect = gr.Radio([\"square\", \"horizontal\", \"vertical\"], value=\"square\", label=\"Aspect Ratio\",\n                                  visible=False)\n                button_run = gr.Button(\"I've finished my sketch\", elem_id=\"main_button\", interactive=True)\n                result_text1 = gr.Text(label='sketch status:')\n\n            with gr.Column(visible=True) as post_sketch:\n                input_image = gr.Image(type=\"numpy\", visible=False)\n                prompt = gr.Textbox(label=\"Prompt (Optional)\")\n                run_button = gr.Button(label=\"Run\")\n                condition_model = gr.Dropdown(choices=list(config_dict.keys()),\n                                              value=list(config_dict.keys())[0],\n                                              label='Model',\n                                              multiselect=False)\n                control_scale = gr.Slider(\n                    label=\"Mask Align strength\", info=\"Large value -> strict alignment with SAM mask\", minimum=0,\n                    maximum=1, value=1, step=0.1)\n                num_samples = gr.Slider(\n                    label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n\n                # enable_auto_prompt = True\n                with gr.Accordion(\"Advanced options\", open=False):\n                    image_resolution = gr.Slider(\n                        label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n                    strength = gr.Slider(\n                        label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                    guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n                    use_scale_map = gr.Checkbox(label='Use scale map', value=False)\n                    ddim_steps = gr.Slider(\n                        label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n                    scale = gr.Slider(\n                        label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                    seed = gr.Slider(label=\"Seed\", minimum=-1,\n                                     maximum=2147483647, step=1, randomize=True)\n                    eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n                    a_prompt = gr.Textbox(\n                        label=\"Added Prompt\", value='best quality, extremely detailed')\n                    n_prompt = gr.Textbox(label=\"Negative Prompt\",\n                                          value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n\n            with gr.Column():\n                result_gallery = gr.Gallery(\n                    label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n                result_text = gr.Text(label='BLIP2+Human Prompt Text')\n        aspect.change(None, inputs=[aspect], outputs=None, _js=set_canvas_size)\n        button_run.click(process_sketch, inputs=[canvas_data],\n                         outputs=[input_image, result_text1], _js=get_js_colors, queue=False)\n        ips = [condition_model, input_image, control_scale, prompt, a_prompt, n_prompt,\n               num_samples, image_resolution, ddim_steps, guess_mode, use_scale_map, strength, scale, seed, eta]\n        run_button.click(fn=process, inputs=ips, outputs=[result_gallery, result_text, result_text1])\n        demo.load(None, None, None, _js=load_js)\n        return demo\n\n\nif __name__ == '__main__':\n    demo = create_demo()\n    demo.queue().launch(server_name='0.0.0.0', share=True)\n"
        },
        {
          "name": "tool_transfer_control.py",
          "type": "blob",
          "size": 1.884765625,
          "content": "path_sd15 = './models/v1-5-pruned.ckpt'\npath_sd15_with_control = './models/control_sd15_openpose.pth'\npath_input = './models/anything-v3-full.safetensors'\npath_output = './models/control_any3_openpose.pth'\n\n\nimport os\n\n\nassert os.path.exists(path_sd15), 'Input path_sd15 does not exists!'\nassert os.path.exists(path_sd15_with_control), 'Input path_sd15_with_control does not exists!'\nassert os.path.exists(path_input), 'Input path_input does not exists!'\nassert os.path.exists(os.path.dirname(path_output)), 'Output folder not exists!'\n\n\nimport torch\nfrom share import *\nfrom cldm.model import load_state_dict\n\n\nsd15_state_dict = load_state_dict(path_sd15)\nsd15_with_control_state_dict = load_state_dict(path_sd15_with_control)\ninput_state_dict = load_state_dict(path_input)\n\n\ndef get_node_name(name, parent_name):\n    if len(name) <= len(parent_name):\n        return False, ''\n    p = name[:len(parent_name)]\n    if p != parent_name:\n        return False, ''\n    return True, name[len(parent_name):]\n\n\nkeys = sd15_with_control_state_dict.keys()\n\nfinal_state_dict = {}\nfor key in keys:\n    is_first_stage, _ = get_node_name(key, 'first_stage_model')\n    is_cond_stage, _ = get_node_name(key, 'cond_stage_model')\n    if is_first_stage or is_cond_stage:\n        final_state_dict[key] = input_state_dict[key]\n        continue\n    p = sd15_with_control_state_dict[key]\n    is_control, node_name = get_node_name(key, 'control_')\n    if is_control:\n        sd15_key_name = 'model.diffusion_' + node_name\n    else:\n        sd15_key_name = key\n    if sd15_key_name in input_state_dict:\n        p_new = p + input_state_dict[sd15_key_name] - sd15_state_dict[sd15_key_name]\n        # print(f'Offset clone from [{sd15_key_name}] to [{key}]')\n    else:\n        p_new = p\n        # print(f'Direct clone to [{key}]')\n    final_state_dict[key] = p_new\n\ntorch.save(final_state_dict, path_output)\nprint('Transferred model saved at ' + path_output)\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "vlpart",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}