{
  "metadata": {
    "timestamp": 1736559605905,
    "page": 244,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "sjvasquez/handwriting-synthesis",
      "stars": 4408,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0849609375,
          "content": "data/raw/ascii\ndata/raw/lineStrokes\ndata/raw/original\ndata/processed\n\nlogs\npredictions\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.8349609375,
          "content": "language: python\ncache: pip\npython:\n    - 2.7\n    - 3.6\n    #- nightly\n    #- pypy\n    #- pypy3\nmatrix:\n    allow_failures:\n        - python: nightly\n        - python: pypy\n        - python: pypy3\ninstall:\n    #- pip install -r requirements.txt\n    - pip install flake8  # pytest  # add another testing frameworks later\nbefore_script:\n    # stop the build if there are Python syntax errors or undefined names\n    - flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics\n    # exit-zero treats all errors as warnings.  The GitHub editor is 127 chars wide\n    - flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\nscript:\n    - true  # pytest --capture=sys  # add other tests here\nnotifications:\n    on_success: change\n    on_failure: change  # `always` will be the setting once code changes slow down\n"
        },
        {
          "name": "checkpoints",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "data_frame.py",
          "type": "blob",
          "size": 3.5625,
          "content": "import copy\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\nclass DataFrame(object):\n\n    \"\"\"Minimal pd.DataFrame analog for handling n-dimensional numpy matrices with additional\n    support for shuffling, batching, and train/test splitting.\n\n    Args:\n        columns: List of names corresponding to the matrices in data.\n        data: List of n-dimensional data matrices ordered in correspondence with columns.\n            All matrices must have the same leading dimension.  Data can also be fed a list of\n            instances of np.memmap, in which case RAM usage can be limited to the size of a\n            single batch.\n    \"\"\"\n\n    def __init__(self, columns, data):\n        assert len(columns) == len(data), 'columns length does not match data length'\n\n        lengths = [mat.shape[0] for mat in data]\n        assert len(set(lengths)) == 1, 'all matrices in data must have same first dimension'\n\n        self.length = lengths[0]\n        self.columns = columns\n        self.data = data\n        self.dict = dict(zip(self.columns, self.data))\n        self.idx = np.arange(self.length)\n\n    def shapes(self):\n        return pd.Series(dict(zip(self.columns, [mat.shape for mat in self.data])))\n\n    def dtypes(self):\n        return pd.Series(dict(zip(self.columns, [mat.dtype for mat in self.data])))\n\n    def shuffle(self):\n        np.random.shuffle(self.idx)\n\n    def train_test_split(self, train_size, random_state=np.random.randint(1000), stratify=None):\n        train_idx, test_idx = train_test_split(\n            self.idx,\n            train_size=train_size,\n            random_state=random_state,\n            stratify=stratify\n        )\n        train_df = DataFrame(copy.copy(self.columns), [mat[train_idx] for mat in self.data])\n        test_df = DataFrame(copy.copy(self.columns), [mat[test_idx] for mat in self.data])\n        return train_df, test_df\n\n    def batch_generator(self, batch_size, shuffle=True, num_epochs=10000, allow_smaller_final_batch=False):\n        epoch_num = 0\n        while epoch_num < num_epochs:\n            if shuffle:\n                self.shuffle()\n\n            for i in range(0, self.length + 1, batch_size):\n                batch_idx = self.idx[i: i + batch_size]\n                if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n                    break\n                yield DataFrame(\n                    columns=copy.copy(self.columns),\n                    data=[mat[batch_idx].copy() for mat in self.data]\n                )\n\n            epoch_num += 1\n\n    def iterrows(self):\n        for i in self.idx:\n            yield self[i]\n\n    def mask(self, mask):\n        return DataFrame(copy.copy(self.columns), [mat[mask] for mat in self.data])\n\n    def concat(self, other_df):\n        mats = []\n        for column in self.columns:\n            mats.append(np.concatenate([self[column], other_df[column]], axis=0))\n        return DataFrame(copy.copy(self.columns), mats)\n\n    def items(self):\n        return self.dict.items()\n\n    def __iter__(self):\n        return self.dict.items().__iter__()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            return self.dict[key]\n\n        elif isinstance(key, int):\n            return pd.Series(dict(zip(self.columns, [mat[self.idx[key]] for mat in self.data])))\n\n    def __setitem__(self, key, value):\n        assert value.shape[0] == len(self), 'matrix first dimension does not match'\n        if key not in self.columns:\n            self.columns.append(key)\n            self.data.append(value)\n        self.dict[key] = value\n"
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 6.8251953125,
          "content": "import os\nimport logging\n\nimport numpy as np\nimport svgwrite\n\nimport drawing\nimport lyrics\nfrom rnn import rnn\n\n\nclass Hand(object):\n\n    def __init__(self):\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n        self.nn = rnn(\n            log_dir='logs',\n            checkpoint_dir='checkpoints',\n            prediction_dir='predictions',\n            learning_rates=[.0001, .00005, .00002],\n            batch_sizes=[32, 64, 64],\n            patiences=[1500, 1000, 500],\n            beta1_decays=[.9, .9, .9],\n            validation_batch_size=32,\n            optimizer='rms',\n            num_training_steps=100000,\n            warm_start_init_step=17900,\n            regularization_constant=0.0,\n            keep_prob=1.0,\n            enable_parameter_averaging=False,\n            min_steps_to_checkpoint=2000,\n            log_interval=20,\n            logging_level=logging.CRITICAL,\n            grad_clip=10,\n            lstm_size=400,\n            output_mixture_components=20,\n            attention_mixture_components=10\n        )\n        self.nn.restore()\n\n    def write(self, filename, lines, biases=None, styles=None, stroke_colors=None, stroke_widths=None):\n        valid_char_set = set(drawing.alphabet)\n        for line_num, line in enumerate(lines):\n            if len(line) > 75:\n                raise ValueError(\n                    (\n                        \"Each line must be at most 75 characters. \"\n                        \"Line {} contains {}\"\n                    ).format(line_num, len(line))\n                )\n\n            for char in line:\n                if char not in valid_char_set:\n                    raise ValueError(\n                        (\n                            \"Invalid character {} detected in line {}. \"\n                            \"Valid character set is {}\"\n                        ).format(char, line_num, valid_char_set)\n                    )\n\n        strokes = self._sample(lines, biases=biases, styles=styles)\n        self._draw(strokes, lines, filename, stroke_colors=stroke_colors, stroke_widths=stroke_widths)\n\n    def _sample(self, lines, biases=None, styles=None):\n        num_samples = len(lines)\n        max_tsteps = 40*max([len(i) for i in lines])\n        biases = biases if biases is not None else [0.5]*num_samples\n\n        x_prime = np.zeros([num_samples, 1200, 3])\n        x_prime_len = np.zeros([num_samples])\n        chars = np.zeros([num_samples, 120])\n        chars_len = np.zeros([num_samples])\n\n        if styles is not None:\n            for i, (cs, style) in enumerate(zip(lines, styles)):\n                x_p = np.load('styles/style-{}-strokes.npy'.format(style))\n                c_p = np.load('styles/style-{}-chars.npy'.format(style)).tostring().decode('utf-8')\n\n                c_p = str(c_p) + \" \" + cs\n                c_p = drawing.encode_ascii(c_p)\n                c_p = np.array(c_p)\n\n                x_prime[i, :len(x_p), :] = x_p\n                x_prime_len[i] = len(x_p)\n                chars[i, :len(c_p)] = c_p\n                chars_len[i] = len(c_p)\n\n        else:\n            for i in range(num_samples):\n                encoded = drawing.encode_ascii(lines[i])\n                chars[i, :len(encoded)] = encoded\n                chars_len[i] = len(encoded)\n\n        [samples] = self.nn.session.run(\n            [self.nn.sampled_sequence],\n            feed_dict={\n                self.nn.prime: styles is not None,\n                self.nn.x_prime: x_prime,\n                self.nn.x_prime_len: x_prime_len,\n                self.nn.num_samples: num_samples,\n                self.nn.sample_tsteps: max_tsteps,\n                self.nn.c: chars,\n                self.nn.c_len: chars_len,\n                self.nn.bias: biases\n            }\n        )\n        samples = [sample[~np.all(sample == 0.0, axis=1)] for sample in samples]\n        return samples\n\n    def _draw(self, strokes, lines, filename, stroke_colors=None, stroke_widths=None):\n        stroke_colors = stroke_colors or ['black']*len(lines)\n        stroke_widths = stroke_widths or [2]*len(lines)\n\n        line_height = 60\n        view_width = 1000\n        view_height = line_height*(len(strokes) + 1)\n\n        dwg = svgwrite.Drawing(filename=filename)\n        dwg.viewbox(width=view_width, height=view_height)\n        dwg.add(dwg.rect(insert=(0, 0), size=(view_width, view_height), fill='white'))\n\n        initial_coord = np.array([0, -(3*line_height / 4)])\n        for offsets, line, color, width in zip(strokes, lines, stroke_colors, stroke_widths):\n\n            if not line:\n                initial_coord[1] -= line_height\n                continue\n\n            offsets[:, :2] *= 1.5\n            strokes = drawing.offsets_to_coords(offsets)\n            strokes = drawing.denoise(strokes)\n            strokes[:, :2] = drawing.align(strokes[:, :2])\n\n            strokes[:, 1] *= -1\n            strokes[:, :2] -= strokes[:, :2].min() + initial_coord\n            strokes[:, 0] += (view_width - strokes[:, 0].max()) / 2\n\n            prev_eos = 1.0\n            p = \"M{},{} \".format(0, 0)\n            for x, y, eos in zip(*strokes.T):\n                p += '{}{},{} '.format('M' if prev_eos == 1.0 else 'L', x, y)\n                prev_eos = eos\n            path = svgwrite.path.Path(p)\n            path = path.stroke(color=color, width=width, linecap='round').fill(\"none\")\n            dwg.add(path)\n\n            initial_coord[1] -= line_height\n\n        dwg.save()\n\n\nif __name__ == '__main__':\n    hand = Hand()\n\n    # usage demo\n    lines = [\n        \"Now this is a story all about how\",\n        \"My life got flipped turned upside down\",\n        \"And I'd like to take a minute, just sit right there\",\n        \"I'll tell you how I became the prince of a town called Bel-Air\",\n    ]\n    biases = [.75 for i in lines]\n    styles = [9 for i in lines]\n    stroke_colors = ['red', 'green', 'black', 'blue']\n    stroke_widths = [1, 2, 1, 2]\n\n    hand.write(\n        filename='img/usage_demo.svg',\n        lines=lines,\n        biases=biases,\n        styles=styles,\n        stroke_colors=stroke_colors,\n        stroke_widths=stroke_widths\n    )\n\n    # demo number 1 - fixed bias, fixed style\n    lines = lyrics.all_star.split(\"\\n\")\n    biases = [.75 for i in lines]\n    styles = [12 for i in lines]\n\n    hand.write(\n        filename='img/all_star.svg',\n        lines=lines,\n        biases=biases,\n        styles=styles,\n    )\n\n    # demo number 2 - fixed bias, varying style\n    lines = lyrics.downtown.split(\"\\n\")\n    biases = [.75 for i in lines]\n    styles = np.cumsum(np.array([len(i) for i in lines]) == 0).astype(int)\n\n    hand.write(\n        filename='img/downtown.svg',\n        lines=lines,\n        biases=biases,\n        styles=styles,\n    )\n\n    # demo number 3 - varying bias, fixed style\n    lines = lyrics.give_up.split(\"\\n\")\n    biases = .2*np.flip(np.cumsum([len(i) == 0 for i in lines]), 0)\n    styles = [7 for i in lines]\n\n    hand.write(\n        filename='img/give_up.svg',\n        lines=lines,\n        biases=biases,\n        styles=styles,\n    )\n"
        },
        {
          "name": "drawing.py",
          "type": "blob",
          "size": 5.8486328125,
          "content": "from __future__ import print_function\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.signal import savgol_filter\nfrom scipy.interpolate import interp1d\n\n\nalphabet = [\n    '\\x00', ' ', '!', '\"', '#', \"'\", '(', ')', ',', '-', '.',\n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';',\n    '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n    'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y',\n    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n    'y', 'z'\n]\nalphabet_ord = list(map(ord, alphabet))\nalpha_to_num = defaultdict(int, list(map(reversed, enumerate(alphabet))))\nnum_to_alpha = dict(enumerate(alphabet_ord))\n\nMAX_STROKE_LEN = 1200\nMAX_CHAR_LEN = 75\n\n\ndef align(coords):\n    \"\"\"\n    corrects for global slant/offset in handwriting strokes\n    \"\"\"\n    coords = np.copy(coords)\n    X, Y = coords[:, 0].reshape(-1, 1), coords[:, 1].reshape(-1, 1)\n    X = np.concatenate([np.ones([X.shape[0], 1]), X], axis=1)\n    offset, slope = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y).squeeze()\n    theta = np.arctan(slope)\n    rotation_matrix = np.array(\n        [[np.cos(theta), -np.sin(theta)],\n         [np.sin(theta), np.cos(theta)]]\n    )\n    coords[:, :2] = np.dot(coords[:, :2], rotation_matrix) - offset\n    return coords\n\n\ndef skew(coords, degrees):\n    \"\"\"\n    skews strokes by given degrees\n    \"\"\"\n    coords = np.copy(coords)\n    theta = degrees * np.pi/180\n    A = np.array([[np.cos(-theta), 0], [np.sin(-theta), 1]])\n    coords[:, :2] = np.dot(coords[:, :2], A)\n    return coords\n\n\ndef stretch(coords, x_factor, y_factor):\n    \"\"\"\n    stretches strokes along x and y axis\n    \"\"\"\n    coords = np.copy(coords)\n    coords[:, :2] *= np.array([x_factor, y_factor])\n    return coords\n\n\ndef add_noise(coords, scale):\n    \"\"\"\n    adds gaussian noise to strokes\n    \"\"\"\n    coords = np.copy(coords)\n    coords[1:, :2] += np.random.normal(loc=0.0, scale=scale, size=coords[1:, :2].shape)\n    return coords\n\n\ndef encode_ascii(ascii_string):\n    \"\"\"\n    encodes ascii string to array of ints\n    \"\"\"\n    return np.array(list(map(lambda x: alpha_to_num[x], ascii_string)) + [0])\n\n\ndef denoise(coords):\n    \"\"\"\n    smoothing filter to mitigate some artifacts of the data collection\n    \"\"\"\n    coords = np.split(coords, np.where(coords[:, 2] == 1)[0] + 1, axis=0)\n    new_coords = []\n    for stroke in coords:\n        if len(stroke) != 0:\n            x_new = savgol_filter(stroke[:, 0], 7, 3, mode='nearest')\n            y_new = savgol_filter(stroke[:, 1], 7, 3, mode='nearest')\n            xy_coords = np.hstack([x_new.reshape(-1, 1), y_new.reshape(-1, 1)])\n            stroke = np.concatenate([xy_coords, stroke[:, 2].reshape(-1, 1)], axis=1)\n            new_coords.append(stroke)\n\n    coords = np.vstack(new_coords)\n    return coords\n\n\ndef interpolate(coords, factor=2):\n    \"\"\"\n    interpolates strokes using cubic spline\n    \"\"\"\n    coords = np.split(coords, np.where(coords[:, 2] == 1)[0] + 1, axis=0)\n    new_coords = []\n    for stroke in coords:\n\n        if len(stroke) == 0:\n            continue\n\n        xy_coords = stroke[:, :2]\n\n        if len(stroke) > 3:\n            f_x = interp1d(np.arange(len(stroke)), stroke[:, 0], kind='cubic')\n            f_y = interp1d(np.arange(len(stroke)), stroke[:, 1], kind='cubic')\n\n            xx = np.linspace(0, len(stroke) - 1, factor*(len(stroke)))\n            yy = np.linspace(0, len(stroke) - 1, factor*(len(stroke)))\n\n            x_new = f_x(xx)\n            y_new = f_y(yy)\n\n            xy_coords = np.hstack([x_new.reshape(-1, 1), y_new.reshape(-1, 1)])\n\n        stroke_eos = np.zeros([len(xy_coords), 1])\n        stroke_eos[-1] = 1.0\n        stroke = np.concatenate([xy_coords, stroke_eos], axis=1)\n        new_coords.append(stroke)\n\n    coords = np.vstack(new_coords)\n    return coords\n\n\ndef normalize(offsets):\n    \"\"\"\n    normalizes strokes to median unit norm\n    \"\"\"\n    offsets = np.copy(offsets)\n    offsets[:, :2] /= np.median(np.linalg.norm(offsets[:, :2], axis=1))\n    return offsets\n\n\ndef coords_to_offsets(coords):\n    \"\"\"\n    convert from coordinates to offsets\n    \"\"\"\n    offsets = np.concatenate([coords[1:, :2] - coords[:-1, :2], coords[1:, 2:3]], axis=1)\n    offsets = np.concatenate([np.array([[0, 0, 1]]), offsets], axis=0)\n    return offsets\n\n\ndef offsets_to_coords(offsets):\n    \"\"\"\n    convert from offsets to coordinates\n    \"\"\"\n    return np.concatenate([np.cumsum(offsets[:, :2], axis=0), offsets[:, 2:3]], axis=1)\n\n\ndef draw(\n        offsets,\n        ascii_seq=None,\n        align_strokes=True,\n        denoise_strokes=True,\n        interpolation_factor=None,\n        save_file=None\n):\n    strokes = offsets_to_coords(offsets)\n\n    if denoise_strokes:\n        strokes = denoise(strokes)\n\n    if interpolation_factor is not None:\n        strokes = interpolate(strokes, factor=interpolation_factor)\n\n    if align_strokes:\n        strokes[:, :2] = align(strokes[:, :2])\n\n    fig, ax = plt.subplots(figsize=(12, 3))\n\n    stroke = []\n    for x, y, eos in strokes:\n        stroke.append((x, y))\n        if eos == 1:\n            coords = zip(*stroke)\n            ax.plot(coords[0], coords[1], 'k')\n            stroke = []\n    if stroke:\n        coords = zip(*stroke)\n        ax.plot(coords[0], coords[1], 'k')\n        stroke = []\n\n    ax.set_xlim(-50, 600)\n    ax.set_ylim(-40, 40)\n\n    ax.set_aspect('equal')\n    plt.tick_params(\n        axis='both',\n        left='off',\n        top='off',\n        right='off',\n        bottom='off',\n        labelleft='off',\n        labeltop='off',\n        labelright='off',\n        labelbottom='off'\n    )\n\n    if ascii_seq is not None:\n        if not isinstance(ascii_seq, str):\n            ascii_seq = ''.join(list(map(chr, ascii_seq)))\n        plt.title(ascii_seq)\n\n    if save_file is not None:\n        plt.savefig(save_file)\n        print('saved to {}'.format(save_file))\n    else:\n        plt.show()\n    plt.close('all')\n"
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "lyrics.py",
          "type": "blob",
          "size": 4.78125,
          "content": "\"\"\"lyrics taken from https://www.azlyrics.com/\"\"\"\n\nall_star = \"\"\"Somebody once told me the world is gonna roll me\nI ain't the sharpest tool in the shed\nShe was looking kind of dumb with her finger and her thumb\nIn the shape of an \"L\" on her forehead\n\nWell, the years start coming and they don't stop coming\nFed to the rules and I hit the ground running\nDidn't make sense not to live for fun\nYour brain gets smart but your head gets dumb\n\nSo much to do, so much to see\nSo what's wrong with taking the back streets?\nYou'll never know if you don't go\nYou'll never shine if you don't glow\n\nHey, now, you're an All Star, get your game on, go play\nHey, now, you're a Rock Star, get the show on, get paid\nAnd all that glitters is gold\nOnly shooting stars break the mold\n\nIt's a cool place and they say it gets colder\nYou're bundled up now wait 'til you get older\nBut the meteor men beg to differ\nJudging by the hole in the satellite picture\n\nThe ice we skate is getting pretty thin\nThe water's getting warm so you might as well swim\nMy world's on fire. How about yours?\nThat's the way I like it and I'll never get bored.\n\nSomebody once asked could I spare some change for gas\nI need to get myself away from this place\nI said yep, what a concept\nI could use a little fuel myself\nAnd we could all use a little change\n\nWell, the years start coming and they don't stop coming\nFed to the rules and I hit the ground running\nDidn't make sense not to live for fun\nYour brain gets smart but your head gets dumb\n\nSo much to do, so much to see\nSo what's wrong with taking the back streets?\nYou'll never know if you don't go\nYou'll never shine if you don't glow.\n\nAnd all that glitters is gold\nOnly shooting stars break the mold\"\"\"\n\ndowntown = \"\"\"Making my way downtown\nWalking fast\nFaces pass\nAnd I'm home-bound\n\nStaring blankly ahead\nJust making my way\nMaking a way\nThrough the crowd\n\nAnd I need you\nAnd I miss you\nAnd now I wonder\n\nIf I could fall into the sky\nDo you think time would pass me by?\n'Cause you know I'd walk a thousand miles\nIf I could just see you tonight\n\nIt's always times like these\nWhen I think of you\nAnd I wonder if you ever think of me\n'Cause everything's so wrong\nAnd I don't belong\nLiving in your precious memory\n\n'Cause I need you\nAnd I miss you\nAnd now I wonder\n\nIf I could fall into the sky\nDo you think time would pass me by?\n'Cause you know I'd walk a thousand miles\nIf I could just see you tonight\n\nAnd I, I don't wanna let you know\nI, I drown in your memory\nI, I don't wanna let this go\nI, I don't\n\nMaking my way downtown\nWalking fast\nFaces pass\nAnd I'm home-bound\n\nStaring blankly ahead\nJust making my way\nMaking a way\nThrough the crowd\n\nAnd I still need you\nAnd I still miss you\nAnd now I wonder\n\nIf I could fall into the sky\nDo you think time would pass us by?\n'Cause you know I'd walk a thousand miles\nIf I could just see you\n\nIf I could fall into the sky\nDo you think time would pass me by?\n'Cause you know I'd walk a thousand miles\nIf I could just see you\nIf I could just hold you tonight\"\"\"\n\ngive_up = \"\"\"We're no strangers to love\nYou know the rules and so do I\nA full commitment's what I'm thinking of\nYou wouldn't get this from any other guy\n\nI just wanna tell you how I'm feeling\nGotta make you understand\n\nNever gonna give you up\nNever gonna let you down\nNever gonna run around and desert you\nNever gonna make you cry\nNever gonna say goodbye\nNever gonna tell a lie and hurt you\n\nWe've known each other for so long\nYour heart's been aching, but\nYou're too shy to say it\nInside, we both know what's been going on\nWe know the game and we're gonna play it\n\nAnd if you ask me how I'm feeling\nDon't tell me you're too blind to see\n\nNever gonna give you up\nNever gonna let you down\nNever gonna run around and desert you\nNever gonna make you cry\nNever gonna say goodbye\nNever gonna tell a lie and hurt you\n\nNever gonna give you up\nNever gonna let you down\nNever gonna run around and desert you\nNever gonna make you cry\nNever gonna say goodbye\nNever gonna tell a lie and hurt you\n\n(Ooh, give you up)\n(Ooh, give you up)\nNever gonna give, never gonna give\n(Give you up)\nNever gonna give, never gonna give\n(Give you up)\n\nWe've known each other for so long\nYour heart's been aching, but\nYou're too shy to say it\nInside, we both know what's been going on\nWe know the game and we're gonna play it\n\nI just wanna tell you how I'm feeling\nGotta make you understand\n\nNever gonna give you up\nNever gonna let you down\nNever gonna run around and desert you\nNever gonna make you cry\nNever gonna say goodbye\nNever gonna tell a lie and hurt you\n\nNever gonna give you up\nNever gonna let you down\nNever gonna run around and desert you\nNever gonna make you cry\nNever gonna say goodbye\nNever gonna tell a lie and hurt you\n\nNever gonna give you up\nNever gonna let you down\nNever gonna run around and desert you\nNever gonna make you cry\nNever gonna say goodbye\nNever gonna tell a lie and hurt you\"\"\"\n"
        },
        {
          "name": "prepare_data.py",
          "type": "blob",
          "size": 4.634765625,
          "content": "from __future__ import print_function\nimport os\nfrom xml.etree import ElementTree\n\nimport numpy as np\n\nimport drawing\n\n\ndef get_stroke_sequence(filename):\n    tree = ElementTree.parse(filename).getroot()\n    strokes = [i for i in tree if i.tag == 'StrokeSet'][0]\n\n    coords = []\n    for stroke in strokes:\n        for i, point in enumerate(stroke):\n            coords.append([\n                int(point.attrib['x']),\n                -1*int(point.attrib['y']),\n                int(i == len(stroke) - 1)\n            ])\n    coords = np.array(coords)\n\n    coords = drawing.align(coords)\n    coords = drawing.denoise(coords)\n    offsets = drawing.coords_to_offsets(coords)\n    offsets = offsets[:drawing.MAX_STROKE_LEN]\n    offsets = drawing.normalize(offsets)\n    return offsets\n\n\ndef get_ascii_sequences(filename):\n    sequences = open(filename, 'r').read()\n    sequences = sequences.replace(r'%%%%%%%%%%%', '\\n')\n    sequences = [i.strip() for i in sequences.split('\\n')]\n    lines = sequences[sequences.index('CSR:') + 2:]\n    lines = [line.strip() for line in lines if line.strip()]\n    lines = [drawing.encode_ascii(line)[:drawing.MAX_CHAR_LEN] for line in lines]\n    return lines\n\n\ndef collect_data():\n    fnames = []\n    for dirpath, dirnames, filenames in os.walk('data/raw/ascii/'):\n        if dirnames:\n            continue\n        for filename in filenames:\n            if filename.startswith('.'):\n                continue\n            fnames.append(os.path.join(dirpath, filename))\n\n    # low quality samples (selected by collecting samples to\n    # which the trained model assigned very low likelihood)\n    blacklist = set(np.load('data/blacklist.npy'))\n\n    stroke_fnames, transcriptions, writer_ids = [], [], []\n    for i, fname in enumerate(fnames):\n        print(i, fname)\n        if fname == 'data/raw/ascii/z01/z01-000/z01-000z.txt':\n            continue\n\n        head, tail = os.path.split(fname)\n        last_letter = os.path.splitext(fname)[0][-1]\n        last_letter = last_letter if last_letter.isalpha() else ''\n\n        line_stroke_dir = head.replace('ascii', 'lineStrokes')\n        line_stroke_fname_prefix = os.path.split(head)[-1] + last_letter + '-'\n\n        if not os.path.isdir(line_stroke_dir):\n            continue\n        line_stroke_fnames = sorted([f for f in os.listdir(line_stroke_dir)\n                                     if f.startswith(line_stroke_fname_prefix)])\n        if not line_stroke_fnames:\n            continue\n\n        original_dir = head.replace('ascii', 'original')\n        original_xml = os.path.join(original_dir, 'strokes' + last_letter + '.xml')\n        tree = ElementTree.parse(original_xml)\n        root = tree.getroot()\n\n        general = root.find('General')\n        if general is not None:\n            writer_id = int(general[0].attrib.get('writerID', '0'))\n        else:\n            writer_id = int('0')\n\n        ascii_sequences = get_ascii_sequences(fname)\n        assert len(ascii_sequences) == len(line_stroke_fnames)\n\n        for ascii_seq, line_stroke_fname in zip(ascii_sequences, line_stroke_fnames):\n            if line_stroke_fname in blacklist:\n                continue\n\n            stroke_fnames.append(os.path.join(line_stroke_dir, line_stroke_fname))\n            transcriptions.append(ascii_seq)\n            writer_ids.append(writer_id)\n\n    return stroke_fnames, transcriptions, writer_ids\n\n\nif __name__ == '__main__':\n    print('traversing data directory...')\n    stroke_fnames, transcriptions, writer_ids = collect_data()\n\n    print('dumping to numpy arrays...')\n    x = np.zeros([len(stroke_fnames), drawing.MAX_STROKE_LEN, 3], dtype=np.float32)\n    x_len = np.zeros([len(stroke_fnames)], dtype=np.int16)\n    c = np.zeros([len(stroke_fnames), drawing.MAX_CHAR_LEN], dtype=np.int8)\n    c_len = np.zeros([len(stroke_fnames)], dtype=np.int8)\n    w_id = np.zeros([len(stroke_fnames)], dtype=np.int16)\n    valid_mask = np.zeros([len(stroke_fnames)], dtype=np.bool)\n\n    for i, (stroke_fname, c_i, w_id_i) in enumerate(zip(stroke_fnames, transcriptions, writer_ids)):\n        if i % 200 == 0:\n            print(i, '\\t', '/', len(stroke_fnames))\n        x_i = get_stroke_sequence(stroke_fname)\n        valid_mask[i] = ~np.any(np.linalg.norm(x_i[:, :2], axis=1) > 60)\n\n        x[i, :len(x_i), :] = x_i\n        x_len[i] = len(x_i)\n\n        c[i, :len(c_i)] = c_i\n        c_len[i] = len(c_i)\n\n        w_id[i] = w_id_i\n\n    if not os.path.isdir('data/processed'):\n        os.makedirs('data/processed')\n\n    np.save('data/processed/x.npy', x[valid_mask])\n    np.save('data/processed/x_len.npy', x_len[valid_mask])\n    np.save('data/processed/c.npy', c[valid_mask])\n    np.save('data/processed/c_len.npy', c_len[valid_mask])\n    np.save('data/processed/w_id.npy', w_id[valid_mask])\n"
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 3.064453125,
          "content": "![](img/banner.svg)\n# Handwriting Synthesis\nImplementation of the handwriting synthesis experiments in the paper <a href=\"https://arxiv.org/abs/1308.0850\">Generating Sequences with Recurrent Neural Networks</a> by Alex Graves.  The implementation closely follows the original paper, with a few slight deviations, and the generated samples are of similar quality to those presented in the paper.\n\nWeb demo is available <a href=\"https://seanvasquez.com/handwriting-generation/\">here</a>.\n\n## Usage\n```python\nlines = [\n    \"Now this is a story all about how\",\n    \"My life got flipped turned upside down\",\n    \"And I'd like to take a minute, just sit right there\",\n    \"I'll tell you how I became the prince of a town called Bel-Air\",\n]\nbiases = [.75 for i in lines]\nstyles = [9 for i in lines]\nstroke_colors = ['red', 'green', 'black', 'blue']\nstroke_widths = [1, 2, 1, 2]\n\nhand = Hand()\nhand.write(\n    filename='img/usage_demo.svg',\n    lines=lines,\n    biases=biases,\n    styles=styles,\n    stroke_colors=stroke_colors,\n    stroke_widths=stroke_widths\n)\n```\n![](img/usage_demo.svg)\n\nCurrently, the `Hand` class must be imported from `demo.py`.  If someone would like to package this project to make it more usable, please [contribute](#contribute).\n\nA pretrained model is included, but if you'd like to train your own, read <a href='https://github.com/sjvasquez/handwriting-synthesis/tree/master/data/raw'>these instructions</a>.\n\n## Demonstrations\nBelow are a few hundred samples from the model, including some samples demonstrating the effect of priming and biasing the model.  Loosely speaking, biasing controls the neatness of the samples and priming controls the style of the samples. The code for these demonstrations can be found in `demo.py`.\n\n### Demo #1:\nThe following samples were generated with a fixed style and fixed bias.\n\n**Smash Mouth – All Star (<a href=\"https://www.azlyrics.com/lyrics/smashmouth/allstar.html\">lyrics</a>)**\n![](img/all_star.svg)\n\n### Demo #2\nThe following samples were generated with varying style and fixed bias.  Each verse is generated in a different style.\n\n**Vanessa Carlton – A Thousand Miles (<a href=\"https://www.azlyrics.com/lyrics/vanessacarlton/athousandmiles.html\">lyrics</a>)**\n![](img/downtown.svg)\n\n### Demo #3\nThe following samples were generated with a fixed style and varying bias.  Each verse has a lower bias than the previous, with the last verse being unbiased.\n\n**Leonard Cohen – Hallelujah (<a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">lyrics</a>)**\n![](img/give_up.svg)\n\n## Contribute\nThis project was intended to serve as a reference implementation for a research paper, but since the results are of decent quality, it may be worthwile to make the project more broadly usable.  I plan to continue focusing on the machine learning side of things.  That said, I'd welcome contributors who can:\n\n  - Package this, and otherwise make it look more like a usable software project and less like research code.\n  - Add support for more sophisticated drawing, animations, or anything else in this direction.  Currently, the project only creates some simple svg files.\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1005859375,
          "content": "matplotlib>=2.1.0\npandas>= 0.22.0\nscikit-learn>=0.19.1\nscipy>=1.0.0\nsvgwrite>=1.1.12\ntensorflow==1.6.0\n"
        },
        {
          "name": "rnn.py",
          "type": "blob",
          "size": 8.189453125,
          "content": "from __future__ import print_function\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nimport drawing\nfrom data_frame import DataFrame\nfrom rnn_cell import LSTMAttentionCell\nfrom rnn_ops import rnn_free_run\nfrom tf_base_model import TFBaseModel\nfrom tf_utils import time_distributed_dense_layer\n\n\nclass DataReader(object):\n\n    def __init__(self, data_dir):\n        data_cols = ['x', 'x_len', 'c', 'c_len']\n        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i))) for i in data_cols]\n\n        self.test_df = DataFrame(columns=data_cols, data=data)\n        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.95, random_state=2018)\n\n        print('train size', len(self.train_df))\n        print('val size', len(self.val_df))\n        print('test size', len(self.test_df))\n\n    def train_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.train_df,\n            shuffle=True,\n            num_epochs=10000,\n            mode='train'\n        )\n\n    def val_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.val_df,\n            shuffle=True,\n            num_epochs=10000,\n            mode='val'\n        )\n\n    def test_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.test_df,\n            shuffle=False,\n            num_epochs=1,\n            mode='test'\n        )\n\n    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, mode='train'):\n        gen = df.batch_generator(\n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_epochs=num_epochs,\n            allow_smaller_final_batch=(mode == 'test')\n        )\n        for batch in gen:\n            batch['x_len'] = batch['x_len'] - 1\n            max_x_len = np.max(batch['x_len'])\n            max_c_len = np.max(batch['c_len'])\n            batch['y'] = batch['x'][:, 1:max_x_len + 1, :]\n            batch['x'] = batch['x'][:, :max_x_len, :]\n            batch['c'] = batch['c'][:, :max_c_len]\n            yield batch\n\n\nclass rnn(TFBaseModel):\n\n    def __init__(\n        self,\n        lstm_size,\n        output_mixture_components,\n        attention_mixture_components,\n        **kwargs\n    ):\n        self.lstm_size = lstm_size\n        self.output_mixture_components = output_mixture_components\n        self.output_units = self.output_mixture_components*6 + 1\n        self.attention_mixture_components = attention_mixture_components\n        super(rnn, self).__init__(**kwargs)\n\n    def parse_parameters(self, z, eps=1e-8, sigma_eps=1e-4):\n        pis, sigmas, rhos, mus, es = tf.split(\n            z,\n            [\n                1*self.output_mixture_components,\n                2*self.output_mixture_components,\n                1*self.output_mixture_components,\n                2*self.output_mixture_components,\n                1\n            ],\n            axis=-1\n        )\n        pis = tf.nn.softmax(pis, axis=-1)\n        sigmas = tf.clip_by_value(tf.exp(sigmas), sigma_eps, np.inf)\n        rhos = tf.clip_by_value(tf.tanh(rhos), eps - 1.0, 1.0 - eps)\n        es = tf.clip_by_value(tf.nn.sigmoid(es), eps, 1.0 - eps)\n        return pis, mus, sigmas, rhos, es\n\n    def NLL(self, y, lengths, pis, mus, sigmas, rho, es, eps=1e-8):\n        sigma_1, sigma_2 = tf.split(sigmas, 2, axis=2)\n        y_1, y_2, y_3 = tf.split(y, 3, axis=2)\n        mu_1, mu_2 = tf.split(mus, 2, axis=2)\n\n        norm = 1.0 / (2*np.pi*sigma_1*sigma_2 * tf.sqrt(1 - tf.square(rho)))\n        Z = tf.square((y_1 - mu_1) / (sigma_1)) + \\\n            tf.square((y_2 - mu_2) / (sigma_2)) - \\\n            2*rho*(y_1 - mu_1)*(y_2 - mu_2) / (sigma_1*sigma_2)\n\n        exp = -1.0*Z / (2*(1 - tf.square(rho)))\n        gaussian_likelihoods = tf.exp(exp) * norm\n        gmm_likelihood = tf.reduce_sum(pis * gaussian_likelihoods, 2)\n        gmm_likelihood = tf.clip_by_value(gmm_likelihood, eps, np.inf)\n\n        bernoulli_likelihood = tf.squeeze(tf.where(tf.equal(tf.ones_like(y_3), y_3), es, 1 - es))\n\n        nll = -(tf.log(gmm_likelihood) + tf.log(bernoulli_likelihood))\n        sequence_mask = tf.logical_and(\n            tf.sequence_mask(lengths, maxlen=tf.shape(y)[1]),\n            tf.logical_not(tf.is_nan(nll)),\n        )\n        nll = tf.where(sequence_mask, nll, tf.zeros_like(nll))\n        num_valid = tf.reduce_sum(tf.cast(sequence_mask, tf.float32), axis=1)\n\n        sequence_loss = tf.reduce_sum(nll, axis=1) / tf.maximum(num_valid, 1.0)\n        element_loss = tf.reduce_sum(nll) / tf.maximum(tf.reduce_sum(num_valid), 1.0)\n        return sequence_loss, element_loss\n\n    def sample(self, cell):\n        initial_state = cell.zero_state(self.num_samples, dtype=tf.float32)\n        initial_input = tf.concat([\n            tf.zeros([self.num_samples, 2]),\n            tf.ones([self.num_samples, 1]),\n        ], axis=1)\n        return rnn_free_run(\n            cell=cell,\n            sequence_length=self.sample_tsteps,\n            initial_state=initial_state,\n            initial_input=initial_input,\n            scope='rnn'\n        )[1]\n\n    def primed_sample(self, cell):\n        initial_state = cell.zero_state(self.num_samples, dtype=tf.float32)\n        primed_state = tf.nn.dynamic_rnn(\n            inputs=self.x_prime,\n            cell=cell,\n            sequence_length=self.x_prime_len,\n            dtype=tf.float32,\n            initial_state=initial_state,\n            scope='rnn'\n        )[1]\n        return rnn_free_run(\n            cell=cell,\n            sequence_length=self.sample_tsteps,\n            initial_state=primed_state,\n            scope='rnn'\n        )[1]\n\n    def calculate_loss(self):\n        self.x = tf.placeholder(tf.float32, [None, None, 3])\n        self.y = tf.placeholder(tf.float32, [None, None, 3])\n        self.x_len = tf.placeholder(tf.int32, [None])\n        self.c = tf.placeholder(tf.int32, [None, None])\n        self.c_len = tf.placeholder(tf.int32, [None])\n\n        self.sample_tsteps = tf.placeholder(tf.int32, [])\n        self.num_samples = tf.placeholder(tf.int32, [])\n        self.prime = tf.placeholder(tf.bool, [])\n        self.x_prime = tf.placeholder(tf.float32, [None, None, 3])\n        self.x_prime_len = tf.placeholder(tf.int32, [None])\n        self.bias = tf.placeholder_with_default(\n            tf.zeros([self.num_samples], dtype=tf.float32), [None])\n\n        cell = LSTMAttentionCell(\n            lstm_size=self.lstm_size,\n            num_attn_mixture_components=self.attention_mixture_components,\n            attention_values=tf.one_hot(self.c, len(drawing.alphabet)),\n            attention_values_lengths=self.c_len,\n            num_output_mixture_components=self.output_mixture_components,\n            bias=self.bias\n        )\n        self.initial_state = cell.zero_state(tf.shape(self.x)[0], dtype=tf.float32)\n        outputs, self.final_state = tf.nn.dynamic_rnn(\n            inputs=self.x,\n            cell=cell,\n            sequence_length=self.x_len,\n            dtype=tf.float32,\n            initial_state=self.initial_state,\n            scope='rnn'\n        )\n        params = time_distributed_dense_layer(outputs, self.output_units, scope='rnn/gmm')\n        pis, mus, sigmas, rhos, es = self.parse_parameters(params)\n        sequence_loss, self.loss = self.NLL(self.y, self.x_len, pis, mus, sigmas, rhos, es)\n\n        self.sampled_sequence = tf.cond(\n            self.prime,\n            lambda: self.primed_sample(cell),\n            lambda: self.sample(cell)\n        )\n        return self.loss\n\n\nif __name__ == '__main__':\n    dr = DataReader(data_dir='data/processed/')\n\n    nn = rnn(\n        reader=dr,\n        log_dir='logs',\n        checkpoint_dir='checkpoints',\n        prediction_dir='predictions',\n        learning_rates=[.0001, .00005, .00002],\n        batch_sizes=[32, 64, 64],\n        patiences=[1500, 1000, 500],\n        beta1_decays=[.9, .9, .9],\n        validation_batch_size=32,\n        optimizer='rms',\n        num_training_steps=100000,\n        warm_start_init_step=0,\n        regularization_constant=0.0,\n        keep_prob=1.0,\n        enable_parameter_averaging=False,\n        min_steps_to_checkpoint=2000,\n        log_interval=20,\n        grad_clip=10,\n        lstm_size=400,\n        output_mixture_components=20,\n        attention_mixture_components=10\n    )\n    nn.fit()\n"
        },
        {
          "name": "rnn_cell.py",
          "type": "blob",
          "size": 7.10546875,
          "content": "from collections import namedtuple\n\nimport tensorflow as tf\nimport tensorflow.contrib.distributions as tfd\nimport numpy as np\n\nfrom tf_utils import dense_layer, shape\n\n\nLSTMAttentionCellState = namedtuple(\n    'LSTMAttentionCellState',\n    ['h1', 'c1', 'h2', 'c2', 'h3', 'c3', 'alpha', 'beta', 'kappa', 'w', 'phi']\n)\n\n\nclass LSTMAttentionCell(tf.nn.rnn_cell.RNNCell):\n\n    def __init__(\n        self,\n        lstm_size,\n        num_attn_mixture_components,\n        attention_values,\n        attention_values_lengths,\n        num_output_mixture_components,\n        bias,\n        reuse=None,\n    ):\n        self.reuse = reuse\n        self.lstm_size = lstm_size\n        self.num_attn_mixture_components = num_attn_mixture_components\n        self.attention_values = attention_values\n        self.attention_values_lengths = attention_values_lengths\n        self.window_size = shape(self.attention_values, 2)\n        self.char_len = tf.shape(attention_values)[1]\n        self.batch_size = tf.shape(attention_values)[0]\n        self.num_output_mixture_components = num_output_mixture_components\n        self.output_units = 6*self.num_output_mixture_components + 1\n        self.bias = bias\n\n    @property\n    def state_size(self):\n        return LSTMAttentionCellState(\n            self.lstm_size,\n            self.lstm_size,\n            self.lstm_size,\n            self.lstm_size,\n            self.lstm_size,\n            self.lstm_size,\n            self.num_attn_mixture_components,\n            self.num_attn_mixture_components,\n            self.num_attn_mixture_components,\n            self.window_size,\n            self.char_len,\n        )\n\n    @property\n    def output_size(self):\n        return self.lstm_size\n\n    def zero_state(self, batch_size, dtype):\n        return LSTMAttentionCellState(\n            tf.zeros([batch_size, self.lstm_size]),\n            tf.zeros([batch_size, self.lstm_size]),\n            tf.zeros([batch_size, self.lstm_size]),\n            tf.zeros([batch_size, self.lstm_size]),\n            tf.zeros([batch_size, self.lstm_size]),\n            tf.zeros([batch_size, self.lstm_size]),\n            tf.zeros([batch_size, self.num_attn_mixture_components]),\n            tf.zeros([batch_size, self.num_attn_mixture_components]),\n            tf.zeros([batch_size, self.num_attn_mixture_components]),\n            tf.zeros([batch_size, self.window_size]),\n            tf.zeros([batch_size, self.char_len]),\n        )\n\n    def __call__(self, inputs, state, scope=None):\n        with tf.variable_scope(scope or type(self).__name__, reuse=tf.AUTO_REUSE):\n\n            # lstm 1\n            s1_in = tf.concat([state.w, inputs], axis=1)\n            cell1 = tf.contrib.rnn.LSTMCell(self.lstm_size)\n            s1_out, s1_state = cell1(s1_in, state=(state.c1, state.h1))\n\n            # attention\n            attention_inputs = tf.concat([state.w, inputs, s1_out], axis=1)\n            attention_params = dense_layer(attention_inputs, 3*self.num_attn_mixture_components, scope='attention')\n            alpha, beta, kappa = tf.split(tf.nn.softplus(attention_params), 3, axis=1)\n            kappa = state.kappa + kappa / 25.0\n            beta = tf.clip_by_value(beta, .01, np.inf)\n\n            kappa_flat, alpha_flat, beta_flat = kappa, alpha, beta\n            kappa, alpha, beta = tf.expand_dims(kappa, 2), tf.expand_dims(alpha, 2), tf.expand_dims(beta, 2)\n\n            enum = tf.reshape(tf.range(self.char_len), (1, 1, self.char_len))\n            u = tf.cast(tf.tile(enum, (self.batch_size, self.num_attn_mixture_components, 1)), tf.float32)\n            phi_flat = tf.reduce_sum(alpha*tf.exp(-tf.square(kappa - u) / beta), axis=1)\n\n            phi = tf.expand_dims(phi_flat, 2)\n            sequence_mask = tf.cast(tf.sequence_mask(self.attention_values_lengths, maxlen=self.char_len), tf.float32)\n            sequence_mask = tf.expand_dims(sequence_mask, 2)\n            w = tf.reduce_sum(phi*self.attention_values*sequence_mask, axis=1)\n\n            # lstm 2\n            s2_in = tf.concat([inputs, s1_out, w], axis=1)\n            cell2 = tf.contrib.rnn.LSTMCell(self.lstm_size)\n            s2_out, s2_state = cell2(s2_in, state=(state.c2, state.h2))\n\n            # lstm 3\n            s3_in = tf.concat([inputs, s2_out, w], axis=1)\n            cell3 = tf.contrib.rnn.LSTMCell(self.lstm_size)\n            s3_out, s3_state = cell3(s3_in, state=(state.c3, state.h3))\n\n            new_state = LSTMAttentionCellState(\n                s1_state.h,\n                s1_state.c,\n                s2_state.h,\n                s2_state.c,\n                s3_state.h,\n                s3_state.c,\n                alpha_flat,\n                beta_flat,\n                kappa_flat,\n                w,\n                phi_flat,\n            )\n\n            return s3_out, new_state\n\n    def output_function(self, state):\n        params = dense_layer(state.h3, self.output_units, scope='gmm', reuse=tf.AUTO_REUSE)\n        pis, mus, sigmas, rhos, es = self._parse_parameters(params)\n        mu1, mu2 = tf.split(mus, 2, axis=1)\n        mus = tf.stack([mu1, mu2], axis=2)\n        sigma1, sigma2 = tf.split(sigmas, 2, axis=1)\n\n        covar_matrix = [tf.square(sigma1), rhos*sigma1*sigma2,\n                        rhos*sigma1*sigma2, tf.square(sigma2)]\n        covar_matrix = tf.stack(covar_matrix, axis=2)\n        covar_matrix = tf.reshape(covar_matrix, (self.batch_size, self.num_output_mixture_components, 2, 2))\n\n        mvn = tfd.MultivariateNormalFullCovariance(loc=mus, covariance_matrix=covar_matrix)\n        b = tfd.Bernoulli(probs=es)\n        c = tfd.Categorical(probs=pis)\n\n        sampled_e = b.sample()\n        sampled_coords = mvn.sample()\n        sampled_idx = c.sample()\n\n        idx = tf.stack([tf.range(self.batch_size), sampled_idx], axis=1)\n        coords = tf.gather_nd(sampled_coords, idx)\n        return tf.concat([coords, tf.cast(sampled_e, tf.float32)], axis=1)\n\n    def termination_condition(self, state):\n        char_idx = tf.cast(tf.argmax(state.phi, axis=1), tf.int32)\n        final_char = char_idx >= self.attention_values_lengths - 1\n        past_final_char = char_idx >= self.attention_values_lengths\n        output = self.output_function(state)\n        es = tf.cast(output[:, 2], tf.int32)\n        is_eos = tf.equal(es, np.ones_like(es))\n        return tf.logical_or(tf.logical_and(final_char, is_eos), past_final_char)\n\n    def _parse_parameters(self, gmm_params, eps=1e-8, sigma_eps=1e-4):\n        pis, sigmas, rhos, mus, es = tf.split(\n            gmm_params,\n            [\n                1*self.num_output_mixture_components,\n                2*self.num_output_mixture_components,\n                1*self.num_output_mixture_components,\n                2*self.num_output_mixture_components,\n                1\n            ],\n            axis=-1\n        )\n        pis = pis*(1 + tf.expand_dims(self.bias, 1))\n        sigmas = sigmas - tf.expand_dims(self.bias, 1)\n\n        pis = tf.nn.softmax(pis, axis=-1)\n        pis = tf.where(pis < .01, tf.zeros_like(pis), pis)\n        sigmas = tf.clip_by_value(tf.exp(sigmas), sigma_eps, np.inf)\n        rhos = tf.clip_by_value(tf.tanh(rhos), eps - 1.0, 1.0 - eps)\n        es = tf.clip_by_value(tf.nn.sigmoid(es), eps, 1.0 - eps)\n        es = tf.where(es < .01, tf.zeros_like(es), es)\n\n        return pis, mus, sigmas, rhos, es\n"
        },
        {
          "name": "rnn_ops.py",
          "type": "blob",
          "size": 10.8193359375,
          "content": "from tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops.rnn_cell_impl import _concat, _like_rnncell\nfrom tensorflow.python.ops.rnn import _maybe_tensor_shape_from_tensor\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.eager import context\n\n\ndef raw_rnn(cell, loop_fn, parallel_iterations=None, swap_memory=False, scope=None):\n    \"\"\"\n    raw_rnn adapted from the original tensorflow implementation\n    (https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn.py)\n    to emit arbitrarily nested states for each time step (concatenated along the time axis)\n    in addition to the outputs at each timestep and the final state\n\n    returns (\n        states for all timesteps,\n        outputs for all timesteps,\n        final cell state,\n    )\n    \"\"\"\n    if not _like_rnncell(cell):\n        raise TypeError(\"cell must be an instance of RNNCell\")\n    if not callable(loop_fn):\n        raise TypeError(\"loop_fn must be a callable\")\n\n    parallel_iterations = parallel_iterations or 32\n\n    # Create a new scope in which the caching device is either\n    # determined by the parent scope, or is set to place the cached\n    # Variable using the same placement as for the rest of the RNN.\n    with vs.variable_scope(scope or \"rnn\") as varscope:\n        if context.in_graph_mode():\n            if varscope.caching_device is None:\n                varscope.set_caching_device(lambda op: op.device)\n\n        time = constant_op.constant(0, dtype=dtypes.int32)\n        (elements_finished, next_input, initial_state, emit_structure,\n         init_loop_state) = loop_fn(time, None, None, None)\n        flat_input = nest.flatten(next_input)\n\n        # Need a surrogate loop state for the while_loop if none is available.\n        loop_state = (init_loop_state if init_loop_state is not None\n                      else constant_op.constant(0, dtype=dtypes.int32))\n\n        input_shape = [input_.get_shape() for input_ in flat_input]\n        static_batch_size = input_shape[0][0]\n\n        for input_shape_i in input_shape:\n            # Static verification that batch sizes all match\n            static_batch_size.merge_with(input_shape_i[0])\n\n        batch_size = static_batch_size.value\n        const_batch_size = batch_size\n        if batch_size is None:\n            batch_size = array_ops.shape(flat_input[0])[0]\n\n        nest.assert_same_structure(initial_state, cell.state_size)\n        state = initial_state\n        flat_state = nest.flatten(state)\n        flat_state = [ops.convert_to_tensor(s) for s in flat_state]\n        state = nest.pack_sequence_as(structure=state,\n                                      flat_sequence=flat_state)\n\n        if emit_structure is not None:\n            flat_emit_structure = nest.flatten(emit_structure)\n            flat_emit_size = [emit.shape if emit.shape.is_fully_defined() else\n                              array_ops.shape(emit) for emit in flat_emit_structure]\n            flat_emit_dtypes = [emit.dtype for emit in flat_emit_structure]\n        else:\n            emit_structure = cell.output_size\n            flat_emit_size = nest.flatten(emit_structure)\n            flat_emit_dtypes = [flat_state[0].dtype] * len(flat_emit_size)\n\n        flat_state_size = [s.shape if s.shape.is_fully_defined() else\n                           array_ops.shape(s) for s in flat_state]\n        flat_state_dtypes = [s.dtype for s in flat_state]\n\n        flat_emit_ta = [\n            tensor_array_ops.TensorArray(\n                dtype=dtype_i,\n                dynamic_size=True,\n                element_shape=(tensor_shape.TensorShape([const_batch_size])\n                               .concatenate(_maybe_tensor_shape_from_tensor(size_i))),\n                size=0,\n                name=\"rnn_output_%d\" % i\n            )\n            for i, (dtype_i, size_i) in enumerate(zip(flat_emit_dtypes, flat_emit_size))\n        ]\n        emit_ta = nest.pack_sequence_as(structure=emit_structure, flat_sequence=flat_emit_ta)\n        flat_zero_emit = [\n            array_ops.zeros(_concat(batch_size, size_i), dtype_i)\n            for size_i, dtype_i in zip(flat_emit_size, flat_emit_dtypes)]\n\n        zero_emit = nest.pack_sequence_as(structure=emit_structure, flat_sequence=flat_zero_emit)\n\n        flat_state_ta = [\n            tensor_array_ops.TensorArray(\n                dtype=dtype_i,\n                dynamic_size=True,\n                element_shape=(tensor_shape.TensorShape([const_batch_size])\n                               .concatenate(_maybe_tensor_shape_from_tensor(size_i))),\n                size=0,\n                name=\"rnn_state_%d\" % i\n            )\n            for i, (dtype_i, size_i) in enumerate(zip(flat_state_dtypes, flat_state_size))\n        ]\n        state_ta = nest.pack_sequence_as(structure=state, flat_sequence=flat_state_ta)\n\n        def condition(unused_time, elements_finished, *_):\n            return math_ops.logical_not(math_ops.reduce_all(elements_finished))\n\n        def body(time, elements_finished, current_input, state_ta, emit_ta, state, loop_state):\n            (next_output, cell_state) = cell(current_input, state)\n\n            nest.assert_same_structure(state, cell_state)\n            nest.assert_same_structure(cell.output_size, next_output)\n\n            next_time = time + 1\n            (next_finished, next_input, next_state, emit_output,\n             next_loop_state) = loop_fn(next_time, next_output, cell_state, loop_state)\n\n            nest.assert_same_structure(state, next_state)\n            nest.assert_same_structure(current_input, next_input)\n            nest.assert_same_structure(emit_ta, emit_output)\n\n            # If loop_fn returns None for next_loop_state, just reuse the previous one.\n            loop_state = loop_state if next_loop_state is None else next_loop_state\n\n            def _copy_some_through(current, candidate):\n                \"\"\"Copy some tensors through via array_ops.where.\"\"\"\n                def copy_fn(cur_i, cand_i):\n                    # TensorArray and scalar get passed through.\n                    if isinstance(cur_i, tensor_array_ops.TensorArray):\n                        return cand_i\n                    if cur_i.shape.ndims == 0:\n                        return cand_i\n                    # Otherwise propagate the old or the new value.\n                    with ops.colocate_with(cand_i):\n                        return array_ops.where(elements_finished, cur_i, cand_i)\n                return nest.map_structure(copy_fn, current, candidate)\n\n            emit_output = _copy_some_through(zero_emit, emit_output)\n            next_state = _copy_some_through(state, next_state)\n\n            emit_ta = nest.map_structure(lambda ta, emit: ta.write(time, emit), emit_ta, emit_output)\n            state_ta = nest.map_structure(lambda ta, state: ta.write(time, state), state_ta, next_state)\n\n            elements_finished = math_ops.logical_or(elements_finished, next_finished)\n\n            return (next_time, elements_finished, next_input, state_ta,\n                    emit_ta, next_state, loop_state)\n\n        returned = control_flow_ops.while_loop(\n            condition, body, loop_vars=[\n                time, elements_finished, next_input, state_ta,\n                emit_ta, state, loop_state],\n            parallel_iterations=parallel_iterations,\n            swap_memory=swap_memory\n        )\n\n        (state_ta, emit_ta, final_state, final_loop_state) = returned[-4:]\n\n        flat_states = nest.flatten(state_ta)\n        flat_states = [array_ops.transpose(ta.stack(), (1, 0, 2)) for ta in flat_states]\n        states = nest.pack_sequence_as(structure=state_ta, flat_sequence=flat_states)\n\n        flat_outputs = nest.flatten(emit_ta)\n        flat_outputs = [array_ops.transpose(ta.stack(), (1, 0, 2)) for ta in flat_outputs]\n        outputs = nest.pack_sequence_as(structure=emit_ta, flat_sequence=flat_outputs)\n\n        return (states, outputs, final_state)\n\n\ndef rnn_teacher_force(inputs, cell, sequence_length, initial_state, scope='dynamic-rnn-teacher-force'):\n    \"\"\"\n    Implementation of an rnn with teacher forcing inputs provided.\n    Used in the same way as tf.dynamic_rnn.\n    \"\"\"\n    inputs = array_ops.transpose(inputs, (1, 0, 2))\n    inputs_ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=array_ops.shape(inputs)[0])\n    inputs_ta = inputs_ta.unstack(inputs)\n\n    def loop_fn(time, cell_output, cell_state, loop_state):\n        emit_output = cell_output\n        next_cell_state = initial_state if cell_output is None else cell_state\n\n        elements_finished = time >= sequence_length\n        finished = math_ops.reduce_all(elements_finished)\n\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros([array_ops.shape(inputs)[1], inputs.shape.as_list()[2]], dtype=dtypes.float32),\n            lambda: inputs_ta.read(time)\n        )\n\n        next_loop_state = None\n        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n\n    states, outputs, final_state = raw_rnn(cell, loop_fn, scope=scope)\n    return states, outputs, final_state\n\n\ndef rnn_free_run(cell, initial_state, sequence_length, initial_input=None, scope='dynamic-rnn-free-run'):\n    \"\"\"\n    Implementation of an rnn which feeds its feeds its predictions back to itself at the next timestep.\n\n    cell must implement two methods:\n\n        cell.output_function(state) which takes in the state at timestep t and returns\n        the cell input at timestep t+1.\n\n        cell.termination_condition(state) which returns a boolean tensor of shape\n        [batch_size] denoting which sequences no longer need to be sampled.\n    \"\"\"\n    with vs.variable_scope(scope, reuse=True):\n        if initial_input is None:\n            initial_input = cell.output_function(initial_state)\n\n    def loop_fn(time, cell_output, cell_state, loop_state):\n        next_cell_state = initial_state if cell_output is None else cell_state\n\n        elements_finished = math_ops.logical_or(\n            time >= sequence_length,\n            cell.termination_condition(next_cell_state)\n        )\n        finished = math_ops.reduce_all(elements_finished)\n\n        next_input = control_flow_ops.cond(\n            finished,\n            lambda: array_ops.zeros_like(initial_input),\n            lambda: initial_input if cell_output is None else cell.output_function(next_cell_state)\n        )\n        emit_output = next_input[0] if cell_output is None else next_input\n\n        next_loop_state = None\n        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n\n    states, outputs, final_state = raw_rnn(cell, loop_fn, scope=scope)\n    return states, outputs, final_state\n"
        },
        {
          "name": "styles",
          "type": "tree",
          "content": null
        },
        {
          "name": "tf_base_model.py",
          "type": "blob",
          "size": 18.2607421875,
          "content": "from __future__ import print_function\nfrom collections import deque\nfrom datetime import datetime\nimport logging\nimport os\nimport pprint as pp\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_utils import shape\n\n\nclass TFBaseModel(object):\n\n    \"\"\"Interface containing some boilerplate code for training tensorflow models.\n\n    Subclassing models must implement self.calculate_loss(), which returns a tensor for the batch loss.\n    Code for the training loop, parameter updates, checkpointing, and inference are implemented here and\n    subclasses are mainly responsible for building the computational graph beginning with the placeholders\n    and ending with the loss tensor.\n\n    Args:\n        reader: Class with attributes train_batch_generator, val_batch_generator, and test_batch_generator\n            that yield dictionaries mapping tf.placeholder names (as strings) to batch data (numpy arrays).\n        batch_size: Minibatch size.\n        learning_rate: Learning rate.\n        optimizer: 'rms' for RMSProp, 'adam' for Adam, 'sgd' for SGD\n        grad_clip: Clip gradients elementwise to have norm at most equal to grad_clip.\n        regularization_constant:  Regularization constant applied to all trainable parameters.\n        keep_prob: 1 - p, where p is the dropout probability\n        early_stopping_steps:  Number of steps to continue training after validation loss has\n            stopped decreasing.\n        warm_start_init_step:  If nonzero, model will resume training a restored model beginning\n            at warm_start_init_step.\n        num_restarts:  After validation loss plateaus, the best checkpoint will be restored and the\n            learning rate will be halved.  This process will repeat num_restarts times.\n        enable_parameter_averaging:  If true, model saves exponential weighted averages of parameters\n            to separate checkpoint file.\n        min_steps_to_checkpoint:  Model only saves after min_steps_to_checkpoint training steps\n            have passed.\n        log_interval:  Train and validation accuracies are logged every log_interval training steps.\n        loss_averaging_window:  Train/validation losses are averaged over the last loss_averaging_window\n            training steps.\n        num_validation_batches:  Number of batches to be used in validation evaluation at each step.\n        log_dir: Directory where logs are written.\n        checkpoint_dir: Directory where checkpoints are saved.\n        prediction_dir: Directory where predictions/outputs are saved.\n    \"\"\"\n\n    def __init__(\n        self,\n        reader=None,\n        batch_sizes=[128],\n        num_training_steps=20000,\n        learning_rates=[.01],\n        beta1_decays=[.99],\n        optimizer='adam',\n        grad_clip=5,\n        regularization_constant=0.0,\n        keep_prob=1.0,\n        patiences=[3000],\n        warm_start_init_step=0,\n        enable_parameter_averaging=False,\n        min_steps_to_checkpoint=100,\n        log_interval=20,\n        logging_level=logging.INFO,\n        loss_averaging_window=100,\n        validation_batch_size=64,\n        log_dir='logs',\n        checkpoint_dir='checkpoints',\n        prediction_dir='predictions',\n    ):\n\n        assert len(batch_sizes) == len(learning_rates) == len(patiences)\n        self.batch_sizes = batch_sizes\n        self.learning_rates = learning_rates\n        self.beta1_decays = beta1_decays\n        self.patiences = patiences\n        self.num_restarts = len(batch_sizes) - 1\n        self.restart_idx = 0\n        self.update_train_params()\n\n        self.reader = reader\n        self.num_training_steps = num_training_steps\n        self.optimizer = optimizer\n        self.grad_clip = grad_clip\n        self.regularization_constant = regularization_constant\n        self.warm_start_init_step = warm_start_init_step\n        self.keep_prob_scalar = keep_prob\n        self.enable_parameter_averaging = enable_parameter_averaging\n        self.min_steps_to_checkpoint = min_steps_to_checkpoint\n        self.log_interval = log_interval\n        self.loss_averaging_window = loss_averaging_window\n        self.validation_batch_size = validation_batch_size\n\n        self.log_dir = log_dir\n        self.logging_level = logging_level\n        self.prediction_dir = prediction_dir\n        self.checkpoint_dir = checkpoint_dir\n        if self.enable_parameter_averaging:\n            self.checkpoint_dir_averaged = checkpoint_dir + '_avg'\n\n        self.init_logging(self.log_dir)\n        logging.info('\\nnew run with parameters:\\n{}'.format(pp.pformat(self.__dict__)))\n\n        self.graph = self.build_graph()\n        self.session = tf.Session(graph=self.graph)\n        logging.info('built graph')\n\n    def update_train_params(self):\n        self.batch_size = self.batch_sizes[self.restart_idx]\n        self.learning_rate = self.learning_rates[self.restart_idx]\n        self.beta1_decay = self.beta1_decays[self.restart_idx]\n        self.early_stopping_steps = self.patiences[self.restart_idx]\n\n    def calculate_loss(self):\n        raise NotImplementedError('subclass must implement this')\n\n    def fit(self):\n        with self.session.as_default():\n\n            if self.warm_start_init_step:\n                self.restore(self.warm_start_init_step)\n                step = self.warm_start_init_step\n            else:\n                self.session.run(self.init)\n                step = 0\n\n            train_generator = self.reader.train_batch_generator(self.batch_size)\n            val_generator = self.reader.val_batch_generator(self.validation_batch_size)\n\n            train_loss_history = deque(maxlen=self.loss_averaging_window)\n            val_loss_history = deque(maxlen=self.loss_averaging_window)\n            train_time_history = deque(maxlen=self.loss_averaging_window)\n            val_time_history = deque(maxlen=self.loss_averaging_window)\n            if not hasattr(self, 'metrics'):\n                self.metrics = {}\n\n            metric_histories = {\n                metric_name: deque(maxlen=self.loss_averaging_window) for metric_name in self.metrics\n            }\n            best_validation_loss, best_validation_tstep = float('inf'), 0\n\n            while step < self.num_training_steps:\n\n                # validation evaluation\n                val_start = time.time()\n                val_batch_df = next(val_generator)\n                val_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in val_batch_df.items() if hasattr(self, placeholder_name)\n                }\n\n                val_feed_dict.update({self.learning_rate_var: self.learning_rate, self.beta1_decay_var: self.beta1_decay})\n                if hasattr(self, 'keep_prob'):\n                    val_feed_dict.update({self.keep_prob: 1.0})\n                if hasattr(self, 'is_training'):\n                    val_feed_dict.update({self.is_training: False})\n\n                results = self.session.run(\n                    fetches=[self.loss] + self.metrics.values(),\n                    feed_dict=val_feed_dict\n                )\n                val_loss = results[0]\n                val_metrics = results[1:] if len(results) > 1 else []\n                val_metrics = dict(zip(self.metrics.keys(), val_metrics))\n                val_loss_history.append(val_loss)\n                val_time_history.append(time.time() - val_start)\n                for key in val_metrics:\n                    metric_histories[key].append(val_metrics[key])\n\n                if hasattr(self, 'monitor_tensors'):\n                    for name, tensor in self.monitor_tensors.items():\n                        [np_val] = self.session.run([tensor], feed_dict=val_feed_dict)\n                        print(name)\n                        print('min', np_val.min())\n                        print('max', np_val.max())\n                        print('mean', np_val.mean())\n                        print('std', np_val.std())\n                        print('nans', np.isnan(np_val).sum())\n                        print()\n                    print()\n                    print()\n\n                # train step\n                train_start = time.time()\n                train_batch_df = next(train_generator)\n                train_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in train_batch_df.items() if hasattr(self, placeholder_name)\n                }\n\n                train_feed_dict.update({self.learning_rate_var: self.learning_rate, self.beta1_decay_var: self.beta1_decay})\n                if hasattr(self, 'keep_prob'):\n                    train_feed_dict.update({self.keep_prob: self.keep_prob_scalar})\n                if hasattr(self, 'is_training'):\n                    train_feed_dict.update({self.is_training: True})\n\n                train_loss, _ = self.session.run(\n                    fetches=[self.loss, self.step],\n                    feed_dict=train_feed_dict\n                )\n                train_loss_history.append(train_loss)\n                train_time_history.append(time.time() - train_start)\n\n                if step % self.log_interval == 0:\n                    avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n                    avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n                    avg_train_time = sum(train_time_history) / len(train_time_history)\n                    avg_val_time = sum(val_time_history) / len(val_time_history)\n                    metric_log = (\n                        \"[[step {:>8}]]     \"\n                        \"[[train {:>4}s]]     loss: {:<12}     \"\n                        \"[[val {:>4}s]]     loss: {:<12}     \"\n                    ).format(\n                        step,\n                        round(avg_train_time, 4),\n                        round(avg_train_loss, 8),\n                        round(avg_val_time, 4),\n                        round(avg_val_loss, 8),\n                    )\n                    early_stopping_metric = avg_val_loss\n                    for metric_name, metric_history in metric_histories.items():\n                        metric_val = sum(metric_history) / len(metric_history)\n                        metric_log += '{}: {:<4}     '.format(metric_name, round(metric_val, 4))\n                        if metric_name == self.early_stopping_metric:\n                            early_stopping_metric = metric_val\n\n                    logging.info(metric_log)\n\n                    if early_stopping_metric < best_validation_loss:\n                        best_validation_loss = early_stopping_metric\n                        best_validation_tstep = step\n                        if step > self.min_steps_to_checkpoint:\n                            self.save(step)\n                            if self.enable_parameter_averaging:\n                                self.save(step, averaged=True)\n\n                    if step - best_validation_tstep > self.early_stopping_steps:\n\n                        if self.num_restarts is None or self.restart_idx >= self.num_restarts:\n                            logging.info('best validation loss of {} at training step {}'.format(\n                                best_validation_loss, best_validation_tstep))\n                            logging.info('early stopping - ending training.')\n                            return\n\n                        if self.restart_idx < self.num_restarts:\n                            self.restore(best_validation_tstep)\n                            step = best_validation_tstep\n                            self.restart_idx += 1\n                            self.update_train_params()\n                            train_generator = self.reader.train_batch_generator(self.batch_size)\n\n                step += 1\n\n            if step <= self.min_steps_to_checkpoint:\n                best_validation_tstep = step\n                self.save(step)\n                if self.enable_parameter_averaging:\n                    self.save(step, averaged=True)\n\n            logging.info('num_training_steps reached - ending training')\n\n    def predict(self, chunk_size=256):\n        if not os.path.isdir(self.prediction_dir):\n            os.makedirs(self.prediction_dir)\n\n        if hasattr(self, 'prediction_tensors'):\n            prediction_dict = {tensor_name: [] for tensor_name in self.prediction_tensors}\n\n            test_generator = self.reader.test_batch_generator(chunk_size)\n            for i, test_batch_df in enumerate(test_generator):\n                if i % 10 == 0:\n                    print(i*len(test_batch_df))\n\n                test_feed_dict = {\n                    getattr(self, placeholder_name, None): data\n                    for placeholder_name, data in test_batch_df.items() if hasattr(self, placeholder_name)\n                }\n                if hasattr(self, 'keep_prob'):\n                    test_feed_dict.update({self.keep_prob: 1.0})\n                if hasattr(self, 'is_training'):\n                    test_feed_dict.update({self.is_training: False})\n\n                tensor_names, tf_tensors = zip(*self.prediction_tensors.items())\n                np_tensors = self.session.run(\n                    fetches=tf_tensors,\n                    feed_dict=test_feed_dict\n                )\n                for tensor_name, tensor in zip(tensor_names, np_tensors):\n                    prediction_dict[tensor_name].append(tensor)\n\n            for tensor_name, tensor in prediction_dict.items():\n                np_tensor = np.concatenate(tensor, 0)\n                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n                logging.info('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n                np.save(save_file, np_tensor)\n\n        if hasattr(self, 'parameter_tensors'):\n            for tensor_name, tensor in self.parameter_tensors.items():\n                np_tensor = tensor.eval(self.session)\n\n                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n                logging.info('saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n                np.save(save_file, np_tensor)\n\n    def save(self, step, averaged=False):\n        saver = self.saver_averaged if averaged else self.saver\n        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n        if not os.path.isdir(checkpoint_dir):\n            logging.info('creating checkpoint directory {}'.format(checkpoint_dir))\n            os.mkdir(checkpoint_dir)\n\n        model_path = os.path.join(checkpoint_dir, 'model')\n        logging.info('saving model to {}'.format(model_path))\n        saver.save(self.session, model_path, global_step=step)\n\n    def restore(self, step=None, averaged=False):\n        saver = self.saver_averaged if averaged else self.saver\n        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n        if not step:\n            model_path = tf.train.latest_checkpoint(checkpoint_dir)\n            logging.info('restoring model parameters from {}'.format(model_path))\n            saver.restore(self.session, model_path)\n        else:\n            model_path = os.path.join(\n                checkpoint_dir, 'model{}-{}'.format('_avg' if averaged else '', step)\n            )\n            logging.info('restoring model from {}'.format(model_path))\n            saver.restore(self.session, model_path)\n\n    def init_logging(self, log_dir):\n        if not os.path.isdir(log_dir):\n            os.makedirs(log_dir)\n\n        date_str = datetime.now().strftime('%Y-%m-%d_%H-%M')\n        log_file = 'log_{}.txt'.format(date_str)\n\n        try:                 # Python 2\n            reload(logging)  # bad\n        except NameError:    # Python 3\n            import logging\n        logging.basicConfig(\n            filename=os.path.join(log_dir, log_file),\n            level=self.logging_level,\n            format='[[%(asctime)s]] %(message)s',\n            datefmt='%m/%d/%Y %I:%M:%S %p'\n        )\n        logging.getLogger().addHandler(logging.StreamHandler())\n\n    def update_parameters(self, loss):\n        if self.regularization_constant != 0:\n            l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n            loss = loss + self.regularization_constant*l2_norm\n\n        optimizer = self.get_optimizer(self.learning_rate_var, self.beta1_decay_var)\n        grads = optimizer.compute_gradients(loss)\n        clipped = [(tf.clip_by_value(g, -self.grad_clip, self.grad_clip), v_) for g, v_ in grads]\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            step = optimizer.apply_gradients(clipped, global_step=self.global_step)\n\n        if self.enable_parameter_averaging:\n            maintain_averages_op = self.ema.apply(tf.trainable_variables())\n            with tf.control_dependencies([step]):\n                self.step = tf.group(maintain_averages_op)\n        else:\n            self.step = step\n\n        logging.info('all parameters:')\n        logging.info(pp.pformat([(var.name, shape(var)) for var in tf.global_variables()]))\n\n        logging.info('trainable parameters:')\n        logging.info(pp.pformat([(var.name, shape(var)) for var in tf.trainable_variables()]))\n\n        logging.info('trainable parameter count:')\n        logging.info(str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())))\n\n    def get_optimizer(self, learning_rate, beta1_decay):\n        if self.optimizer == 'adam':\n            return tf.train.AdamOptimizer(learning_rate, beta1=beta1_decay)\n        elif self.optimizer == 'gd':\n            return tf.train.GradientDescentOptimizer(learning_rate)\n        elif self.optimizer == 'rms':\n            return tf.train.RMSPropOptimizer(learning_rate, decay=beta1_decay, momentum=0.9)\n        else:\n            assert False, 'optimizer must be adam, gd, or rms'\n\n    def build_graph(self):\n        with tf.Graph().as_default() as graph:\n            self.ema = tf.train.ExponentialMovingAverage(decay=0.99)\n            self.global_step = tf.Variable(0, trainable=False)\n            self.learning_rate_var = tf.Variable(0.0, trainable=False)\n            self.beta1_decay_var = tf.Variable(0.0, trainable=False)\n\n            self.loss = self.calculate_loss()\n            self.update_parameters(self.loss)\n\n            self.saver = tf.train.Saver(max_to_keep=1)\n            if self.enable_parameter_averaging:\n                self.saver_averaged = tf.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n\n            self.init = tf.global_variables_initializer()\n            return graph\n"
        },
        {
          "name": "tf_utils.py",
          "type": "blob",
          "size": 2.970703125,
          "content": "import tensorflow as tf\n\n\ndef dense_layer(inputs, output_units, bias=True, activation=None, batch_norm=None,\n                dropout=None, scope='dense-layer', reuse=False):\n    \"\"\"\n    Applies a dense layer to a 2D tensor of shape [batch_size, input_units]\n    to produce a tensor of shape [batch_size, output_units].\n    Args:\n        inputs: Tensor of shape [batch size, input_units].\n        output_units: Number of output units.\n        activation: activation function.\n        dropout: dropout keep prob.\n    Returns:\n        Tensor of shape [batch size, output_units].\n    \"\"\"\n    with tf.variable_scope(scope, reuse=reuse):\n        W = tf.get_variable(\n            name='weights',\n            initializer=tf.contrib.layers.variance_scaling_initializer(),\n            shape=[shape(inputs, -1), output_units]\n        )\n        z = tf.matmul(inputs, W)\n        if bias:\n            b = tf.get_variable(\n                name='biases',\n                initializer=tf.constant_initializer(),\n                shape=[output_units]\n            )\n            z = z + b\n\n        if batch_norm is not None:\n            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n\n        z = activation(z) if activation else z\n        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n        return z\n\n\ndef time_distributed_dense_layer(\n        inputs, output_units, bias=True, activation=None, batch_norm=None,\n        dropout=None, scope='time-distributed-dense-layer', reuse=False):\n    \"\"\"\n    Applies a shared dense layer to each timestep of a tensor of shape\n    [batch_size, max_seq_len, input_units] to produce a tensor of shape\n    [batch_size, max_seq_len, output_units].\n\n    Args:\n        inputs: Tensor of shape [batch size, max sequence length, ...].\n        output_units: Number of output units.\n        activation: activation function.\n        dropout: dropout keep prob.\n\n    Returns:\n        Tensor of shape [batch size, max sequence length, output_units].\n    \"\"\"\n    with tf.variable_scope(scope, reuse=reuse):\n        W = tf.get_variable(\n            name='weights',\n            initializer=tf.contrib.layers.variance_scaling_initializer(),\n            shape=[shape(inputs, -1), output_units]\n        )\n        z = tf.einsum('ijk,kl->ijl', inputs, W)\n        if bias:\n            b = tf.get_variable(\n                name='biases',\n                initializer=tf.constant_initializer(),\n                shape=[output_units]\n            )\n            z = z + b\n\n        if batch_norm is not None:\n            z = tf.layers.batch_normalization(z, training=batch_norm, reuse=reuse)\n\n        z = activation(z) if activation else z\n        z = tf.nn.dropout(z, dropout) if dropout is not None else z\n        return z\n\n\ndef shape(tensor, dim=None):\n    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n    if dim is None:\n        return tensor.shape.as_list()\n    else:\n        return tensor.shape.as_list()[dim]\n\n\ndef rank(tensor):\n    \"\"\"Get tensor rank as python list\"\"\"\n    return len(tensor.shape.as_list())\n"
        }
      ]
    }
  ]
}