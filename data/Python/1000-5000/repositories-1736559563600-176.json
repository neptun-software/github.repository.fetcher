{
  "metadata": {
    "timestamp": 1736559563600,
    "page": 176,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hzwer/ECCV2022-RIFE",
      "stars": 4566,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0771484375,
          "content": "*.pyc\n*.py~\n*.py#\n\n*.pkl\noutput/*\ntrain_log/*\n*.mp4\n\ntest/\n.idea/\n*.npz\n\n*.zip\n"
        },
        {
          "name": "Colab_demo.ipynb",
          "type": "blob",
          "size": 3.1845703125,
          "content": "{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"colab_type\": \"text\",\n        \"id\": \"view-in-github\"\n      },\n      \"source\": [\n        \"<a href=\\\"https://colab.research.google.com/github/hzwer/arXiv2020-RIFE/blob/main/Colab_demo.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"FypCcZkNNt2p\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"!git clone https://github.com/hzwer/arXiv2020-RIFE\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"1wysVHxoN54f\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"!mkdir /content/arXiv2020-RIFE/train_log\\n\",\n        \"%cd /content/arXiv2020-RIFE/train_log\\n\",\n        \"!gdown --id 1APIzVeI-4ZZCEuIRE1m6WYfSCaOsi_7_\\n\",\n        \"!7z e RIFE_trained_model_v3.6.zip\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"AhbHfRBJRAUt\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"%cd /content/arXiv2020-RIFE/\\n\",\n        \"!gdown --id 1i3xlKb7ax7Y70khcTcuePi6E7crO_dFc\\n\",\n        \"!pip install scikit-video\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"rirngW5uRMdg\"\n      },\n      \"source\": [\n        \"Please upload your video to content/arXiv2020-RIFE/video.mp4, or use our demo video.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"dnLn4aHHPzN3\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"!nvidia-smi\\n\",\n        \"!python3 inference_video.py --exp=2 --video=demo.mp4 --montage\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"77KK6lxHgJhf\"\n      },\n      \"source\": [\n        \"Our demo.mp4 is 25FPS. You can adjust the parameters for your own perference.\\n\",\n        \"For example: \\n\",\n        \"--fps=60 --exp=1 --video=mydemo.avi --png\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"cellView\": \"code\",\n        \"id\": \"0zIBbVE3UfUD\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"from IPython.display import display, Image\\n\",\n        \"import moviepy.editor as mpy\\n\",\n        \"display(mpy.ipython_display('demo_4X_100fps.mp4', height=256, max_duration=100.))\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"id\": \"tWkJCNgP3zXA\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"!python3 inference_img.py --img demo/I0_0.png demo/I0_1.png\\n\",\n        \"ffmpeg -r 10 -f image2 -i output/img%d.png -s 448x256 -vf \\\"split[s0][s1];[s0]palettegen=stats_mode=single[p];[s1][p]paletteuse=new=1\\\" output/slomo.gif\\n\",\n        \"# Image interpolation\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"accelerator\": \"GPU\",\n    \"colab\": {\n      \"include_colab_link\": true,\n      \"name\": \"Untitled0.ipynb\",\n      \"provenance\": []\n    },\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"name\": \"python3\"\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright  (c)  Megvii  Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.7373046875,
          "content": "# Real-Time Intermediate Flow Estimation for Video Frame Interpolation\n## Introduction\nThis project is the implement of [Real-Time Intermediate Flow Estimation for Video Frame Interpolation](https://arxiv.org/abs/2011.06294). Currently, our model can run 30+FPS for 2X 720p interpolation on a 2080Ti GPU. It supports arbitrary-timestep interpolation between a pair of images.\n\n**2024.08 - We find that [4.22.lite](https://github.com/hzwer/Practical-RIFE/tree/main?tab=readme-ov-file#model-list) is quite suitable for post-processing of [some diffusion model generated videos](https://drive.google.com/drive/folders/1hSzUn10Era3JCaVz0Z5Eg4wT9R6eJ9U9?usp=sharing).**\n\n2023.11 - We recently release new [v4.7-4.10](https://github.com/hzwer/Practical-RIFE/tree/main#model-list) optimized for anime scenes! We draw from [SAFA](https://github.com/megvii-research/WACV2024-SAFA/tree/main)’s research.\n\n2022.7.4 - Our paper is accepted by ECCV2022. Thanks to all relevant authors, contributors and users!\n\nFrom 2020 to 2022, we submitted RIFE for five submissions（rejected by CVPR21 ICCV21 AAAI22 CVPR22). Thanks to all anonymous reviewers, your suggestions have helped to significantly improve the paper!\n\n[ECCV Poster](https://drive.google.com/file/d/1xCXuLUCSwhN61kvIF8jxDvQiUGtLK0kN/view?usp=sharing) | [ECCV 5-min presentation](https://youtu.be/qdp-NYqWQpA) | [论文中文介绍](https://zhuanlan.zhihu.com/p/568553080) | [rebuttal (2WA1WR->3WA)](https://drive.google.com/file/d/16IVjwRpwbTuJbYyTn4PizKX8I257QxY-/view?usp=sharing)\n\n## [YouTube](https://www.youtube.com/results?search_query=rife+interpolation&sp=CAM%253D) | [BiliBili](https://search.bilibili.com/all?keyword=SVFI&order=stow&duration=0&tids_1=0) | [Colab](https://colab.research.google.com/github/hzwer/ECCV2022-RIFE/blob/main/Colab_demo.ipynb) | [Tutorial](https://www.youtube.com/watch?v=gf_on-dbwyU&feature=emb_title) | [V2EX](https://www.v2ex.com/t/984548)\n\n**Pinned Software: [RIFE-App](https://grisk.itch.io/rife-app) | [FlowFrames](https://nmkd.itch.io/flowframes) | [SVFI (中文)](https://github.com/YiWeiHuang-stack/Squirrel-Video-Frame-Interpolation)**\n\n16X interpolation results from two input images: \n\n![Demo](./demo/I2_slomo_clipped.gif)\n![Demo](./demo/D2_slomo_clipped.gif)\n\n## Software\n[Flowframes](https://nmkd.itch.io/flowframes) | [SVFI(中文)](https://github.com/YiWeiHuang-stack/Squirrel-Video-Frame-Interpolation) | [Waifu2x-Extension-GUI](https://github.com/AaronFeng753/Waifu2x-Extension-GUI) | [Autodesk Flame](https://vimeo.com/505942142) | [SVP](https://www.svp-team.com/wiki/RIFE_AI_interpolation) | [MPV_lazy](https://github.com/hooke007/MPV_lazy) | [enhancr](https://github.com/mafiosnik777/enhancr)\n\n[RIFE-App(Paid)](https://grisk.itch.io/rife-app) | [Steam-VFI(Paid)](https://store.steampowered.com/app/1692080/SVFI/) \n\nWe are not responsible for and participating in the development of above software. According to the open source license, we respect the commercial behavior of other developers.\n\n[VapourSynth-RIFE](https://github.com/HolyWu/vs-rife) | [RIFE-ncnn-vulkan](https://github.com/nihui/rife-ncnn-vulkan) | [VapourSynth-RIFE-ncnn-Vulkan](https://github.com/styler00dollar/VapourSynth-RIFE-ncnn-Vulkan) | [vs-mlrt](https://github.com/AmusementClub/vs-mlrt)\n\n<img src=\"https://api.star-history.com/svg?repos=megvii-research/ECCV2022-RIFE,Justin62628/Squirrel-RIFE,n00mkrad/flowframes,nihui/rife-ncnn-vulkan,hzwer/Practical-RIFE&type=Date\" height=\"320\" width=\"480\" />\n\nIf you are a developer, welcome to follow [Practical-RIFE](https://github.com/hzwer/Practical-RIFE), which aims to make RIFE more practical for users by adding various features and design new models with faster speed.\n\nYou may check [this pull request](https://github.com/megvii-research/ECCV2022-RIFE/pull/300) for supporting macOS.\n## CLI Usage\n\n### Installation\n\n```\ngit clone git@github.com:megvii-research/ECCV2022-RIFE.git\ncd ECCV2022-RIFE\npip3 install -r requirements.txt\n```\n\n* Download the pretrained **HD** models from [here](https://drive.google.com/file/d/1APIzVeI-4ZZCEuIRE1m6WYfSCaOsi_7_/view?usp=sharing). (百度网盘链接:https://pan.baidu.com/share/init?surl=u6Q7-i4Hu4Vx9_5BJibPPA 密码:hfk3，把压缩包解开后放在 train_log/\\*)\n\n* Unzip and move the pretrained parameters to train_log/\\*\n\n* This model is not reported by our paper, for our paper model please refer to [evaluation](https://github.com/hzwer/ECCV2022-RIFE#evaluation).\n\n### Run\n\n**Video Frame Interpolation**\n\nYou can use our [demo video](https://drive.google.com/file/d/1i3xlKb7ax7Y70khcTcuePi6E7crO_dFc/view?usp=sharing) or your own video. \n```\npython3 inference_video.py --exp=1 --video=video.mp4 \n```\n(generate video_2X_xxfps.mp4)\n```\npython3 inference_video.py --exp=2 --video=video.mp4\n```\n(for 4X interpolation)\n```\npython3 inference_video.py --exp=1 --video=video.mp4 --scale=0.5\n```\n(If your video has very high resolution such as 4K, we recommend set --scale=0.5 (default 1.0). If you generate disordered pattern on your videos, try set --scale=2.0. This parameter control the process resolution for optical flow model.)\n```\npython3 inference_video.py --exp=2 --img=input/\n```\n(to read video from pngs, like input/0.png ... input/612.png, ensure that the png names are numbers)\n```\npython3 inference_video.py --exp=2 --video=video.mp4 --fps=60\n```\n(add slomo effect, the audio will be removed)\n```\npython3 inference_video.py --video=video.mp4 --montage --png\n```\n(if you want to montage the origin video and save the png format output)\n\n**Extended Application**\n\nYou may refer to [#278](https://github.com/megvii-research/ECCV2022-RIFE/issues/278#event-7199085190) for **Optical Flow Estimation** and refer to [#291](https://github.com/hzwer/ECCV2022-RIFE/issues/291#issuecomment-1328685348) for **Video Stitching**.\n\n**Image Interpolation**\n\n```\npython3 inference_img.py --img img0.png img1.png --exp=4\n```\n(2^4=16X interpolation results)\nAfter that, you can use pngs to generate mp4:\n```\nffmpeg -r 10 -f image2 -i output/img%d.png -s 448x256 -c:v libx264 -pix_fmt yuv420p output/slomo.mp4 -q:v 0 -q:a 0\n```\nYou can also use pngs to generate gif:\n```\nffmpeg -r 10 -f image2 -i output/img%d.png -s 448x256 -vf \"split[s0][s1];[s0]palettegen=stats_mode=single[p];[s1][p]paletteuse=new=1\" output/slomo.gif\n```\n\n### Run in docker\nPlace the pre-trained models in `train_log/\\*.pkl` (as above)\n\nBuilding the container:\n```\ndocker build -t rife -f docker/Dockerfile .\n```\n\nRunning the container:\n```\ndocker run --rm -it -v $PWD:/host rife:latest inference_video --exp=1 --video=untitled.mp4 --output=untitled_rife.mp4\n```\n```\ndocker run --rm -it -v $PWD:/host rife:latest inference_img --img img0.png img1.png --exp=4\n```\n\nUsing gpu acceleration (requires proper gpu drivers for docker):\n```\ndocker run --rm -it --gpus all -v /dev/dri:/dev/dri -v $PWD:/host rife:latest inference_video --exp=1 --video=untitled.mp4 --output=untitled_rife.mp4\n```\n\n## Evaluation\nDownload [RIFE model](https://drive.google.com/file/d/1h42aGYPNJn2q8j_GVkS_yDu__G_UZ2GX/view?usp=sharing) or [RIFE_m model](https://drive.google.com/file/d/147XVsDXBfJPlyct2jfo9kpbL944mNeZr/view?usp=sharing) reported by our paper.\n\n**UCF101**: Download [UCF101 dataset](https://liuziwei7.github.io/projects/VoxelFlow) at ./UCF101/ucf101_interp_ours/\n\n**Vimeo90K**: Download [Vimeo90K dataset](http://toflow.csail.mit.edu/) at ./vimeo_interp_test\n\n**MiddleBury**: Download [MiddleBury OTHER dataset](https://vision.middlebury.edu/flow/data/) at ./other-data and ./other-gt-interp\n\n**HD**: Download [HD dataset](https://github.com/baowenbo/MEMC-Net) at ./HD_dataset. We also provide a [google drive download link](https://drive.google.com/file/d/1iHaLoR2g1-FLgr9MEv51NH_KQYMYz-FA/view?usp=sharing).\n```\n# RIFE\npython3 benchmark/UCF101.py\n# \"PSNR: 35.282 SSIM: 0.9688\"\npython3 benchmark/Vimeo90K.py\n# \"PSNR: 35.615 SSIM: 0.9779\"\npython3 benchmark/MiddleBury_Other.py\n# \"IE: 1.956\"\npython3 benchmark/HD.py\n# \"PSNR: 32.14\"\n\n# RIFE_m\npython3 benchmark/HD_multi_4X.py\n# \"PSNR: 22.96(544*1280), 31.87(720p), 34.25(1080p)\"\n```\n\n## Training and Reproduction\nDownload [Vimeo90K dataset](http://toflow.csail.mit.edu/).\n\nWe use 16 CPUs, 4 GPUs and 20G memory for training: \n```\npython3 -m torch.distributed.launch --nproc_per_node=4 train.py --world_size=4\n```\n\n## Revision History\n\n2021.3.18 [arXiv](https://arxiv.org/pdf/2011.06294v5.pdf): Modify the main experimental data, especially the runtime related issues.\n\n2021.8.12 [arXiv](https://arxiv.org/pdf/2011.06294v6.pdf): Remove pre-trained model dependency and propose privileged distillation scheme for frame interpolation. Remove [census loss](https://github.com/hzwer/arXiv2021-RIFE/blob/0e241367847a0895748e64c6e1604c94db54d395/model/loss.py#L20) supervision.\n\n2021.11.17 [arXiv](https://arxiv.org/pdf/2011.06294v11.pdf): Support arbitrary-time frame interpolation, aka RIFEm and add more experiments.\n\n## Recommend\nWe sincerely recommend some related papers:\n\nCVPR22 - [Optimizing Video Prediction via Video Frame Interpolation](https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Optimizing_Video_Prediction_via_Video_Frame_Interpolation_CVPR_2022_paper.html)\n\nCVPR22 - [Video Frame Interpolation with Transformer](https://openaccess.thecvf.com/content/CVPR2022/html/Lu_Video_Frame_Interpolation_With_Transformer_CVPR_2022_paper.html)\n\nCVPR22 - [IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation](https://openaccess.thecvf.com/content/CVPR2022/html/Kong_IFRNet_Intermediate_Feature_Refine_Network_for_Efficient_Frame_Interpolation_CVPR_2022_paper.html)\n\nCVPR23 - [A Dynamic Multi-Scale Voxel Flow Network for Video Prediction](https://huxiaotaostasy.github.io/DMVFN/)\n\nCVPR23 - [Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation](https://arxiv.org/abs/2303.00440)\n\n## Citation\nIf you think this project is helpful, please feel free to leave a star or cite our paper:\n\n```\n@inproceedings{huang2022rife,\n  title={Real-Time Intermediate Flow Estimation for Video Frame Interpolation},\n  author={Huang, Zhewei and Zhang, Tianyuan and Heng, Wen and Shi, Boxin and Zhou, Shuchang},\n  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n  year={2022}\n}\n```\n\n## Reference\n\nOptical Flow:\n[ARFlow](https://github.com/lliuz/ARFlow)  [pytorch-liteflownet](https://github.com/sniklaus/pytorch-liteflownet)  [RAFT](https://github.com/princeton-vl/RAFT)  [pytorch-PWCNet](https://github.com/sniklaus/pytorch-pwc)\n\nVideo Interpolation: \n[DVF](https://github.com/lxx1991/pytorch-voxel-flow)  [TOflow](https://github.com/Coldog2333/pytoflow)  [SepConv](https://github.com/sniklaus/sepconv-slomo)  [DAIN](https://github.com/baowenbo/DAIN)  [CAIN](https://github.com/myungsub/CAIN)  [MEMC-Net](https://github.com/baowenbo/MEMC-Net)   [SoftSplat](https://github.com/sniklaus/softmax-splatting)  [BMBC](https://github.com/JunHeum/BMBC)  [EDSC](https://github.com/Xianhang/EDSC-pytorch)  [EQVI](https://github.com/lyh-18/EQVI)\n"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset.py",
          "type": "blob",
          "size": 4.2353515625,
          "content": "import os\nimport cv2\nimport ast\nimport torch\nimport numpy as np\nimport random\nfrom torch.utils.data import DataLoader, Dataset\n\ncv2.setNumThreads(1)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclass VimeoDataset(Dataset):\n    def __init__(self, dataset_name, batch_size=32):\n        self.batch_size = batch_size\n        self.dataset_name = dataset_name        \n        self.h = 256\n        self.w = 448\n        self.data_root = 'vimeo_triplet'\n        self.image_root = os.path.join(self.data_root, 'sequences')\n        train_fn = os.path.join(self.data_root, 'tri_trainlist.txt')\n        test_fn = os.path.join(self.data_root, 'tri_testlist.txt')\n        with open(train_fn, 'r') as f:\n            self.trainlist = f.read().splitlines()\n        with open(test_fn, 'r') as f:\n            self.testlist = f.read().splitlines()   \n        self.load_data()\n\n    def __len__(self):\n        return len(self.meta_data)\n\n    def load_data(self):\n        cnt = int(len(self.trainlist) * 0.95)\n        if self.dataset_name == 'train':\n            self.meta_data = self.trainlist[:cnt]\n        elif self.dataset_name == 'test':\n            self.meta_data = self.testlist\n        else:\n            self.meta_data = self.trainlist[cnt:]\n           \n    def crop(self, img0, gt, img1, h, w):\n        ih, iw, _ = img0.shape\n        x = np.random.randint(0, ih - h + 1)\n        y = np.random.randint(0, iw - w + 1)\n        img0 = img0[x:x+h, y:y+w, :]\n        img1 = img1[x:x+h, y:y+w, :]\n        gt = gt[x:x+h, y:y+w, :]\n        return img0, gt, img1\n\n    def getimg(self, index):\n        imgpath = os.path.join(self.image_root, self.meta_data[index])\n        imgpaths = [imgpath + '/im1.png', imgpath + '/im2.png', imgpath + '/im3.png']\n\n        # Load images\n        img0 = cv2.imread(imgpaths[0])\n        gt = cv2.imread(imgpaths[1])\n        img1 = cv2.imread(imgpaths[2])\n        timestep = 0.5\n        return img0, gt, img1, timestep\n    \n        # RIFEm with Vimeo-Septuplet\n        # imgpaths = [imgpath + '/im1.png', imgpath + '/im2.png', imgpath + '/im3.png', imgpath + '/im4.png', imgpath + '/im5.png', imgpath + '/im6.png', imgpath + '/im7.png']\n        # ind = [0, 1, 2, 3, 4, 5, 6]\n        # random.shuffle(ind)\n        # ind = ind[:3]\n        # ind.sort()\n        # img0 = cv2.imread(imgpaths[ind[0]])\n        # gt = cv2.imread(imgpaths[ind[1]])\n        # img1 = cv2.imread(imgpaths[ind[2]])        \n        # timestep = (ind[1] - ind[0]) * 1.0 / (ind[2] - ind[0] + 1e-6)\n            \n    def __getitem__(self, index):        \n        img0, gt, img1, timestep = self.getimg(index)\n        if self.dataset_name == 'train':\n            img0, gt, img1 = self.crop(img0, gt, img1, 224, 224)\n            if random.uniform(0, 1) < 0.5:\n                img0 = img0[:, :, ::-1]\n                img1 = img1[:, :, ::-1]\n                gt = gt[:, :, ::-1]\n            if random.uniform(0, 1) < 0.5:\n                img0 = img0[::-1]\n                img1 = img1[::-1]\n                gt = gt[::-1]\n            if random.uniform(0, 1) < 0.5:\n                img0 = img0[:, ::-1]\n                img1 = img1[:, ::-1]\n                gt = gt[:, ::-1]\n            if random.uniform(0, 1) < 0.5:\n                tmp = img1\n                img1 = img0\n                img0 = tmp\n                timestep = 1 - timestep\n            # random rotation\n            p = random.uniform(0, 1)\n            if p < 0.25:\n                img0 = cv2.rotate(img0, cv2.ROTATE_90_CLOCKWISE)\n                gt = cv2.rotate(gt, cv2.ROTATE_90_CLOCKWISE)\n                img1 = cv2.rotate(img1, cv2.ROTATE_90_CLOCKWISE)\n            elif p < 0.5:\n                img0 = cv2.rotate(img0, cv2.ROTATE_180)\n                gt = cv2.rotate(gt, cv2.ROTATE_180)\n                img1 = cv2.rotate(img1, cv2.ROTATE_180)\n            elif p < 0.75:\n                img0 = cv2.rotate(img0, cv2.ROTATE_90_COUNTERCLOCKWISE)\n                gt = cv2.rotate(gt, cv2.ROTATE_90_COUNTERCLOCKWISE)\n                img1 = cv2.rotate(img1, cv2.ROTATE_90_COUNTERCLOCKWISE)\n        img0 = torch.from_numpy(img0.copy()).permute(2, 0, 1)\n        img1 = torch.from_numpy(img1.copy()).permute(2, 0, 1)\n        gt = torch.from_numpy(gt.copy()).permute(2, 0, 1)\n        timestep = torch.tensor(timestep).reshape(1, 1, 1)\n        return torch.cat((img0, img1, gt), 0), timestep\n"
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference_img.py",
          "type": "blob",
          "size": 4.0888671875,
          "content": "import os\nimport cv2\nimport torch\nimport argparse\nfrom torch.nn import functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.set_grad_enabled(False)\nif torch.cuda.is_available():\n    torch.backends.cudnn.enabled = True\n    torch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description='Interpolation for a pair of images')\nparser.add_argument('--img', dest='img', nargs=2, required=True)\nparser.add_argument('--exp', default=4, type=int)\nparser.add_argument('--ratio', default=0, type=float, help='inference ratio between two images with 0 - 1 range')\nparser.add_argument('--rthreshold', default=0.02, type=float, help='returns image when actual ratio falls in given range threshold')\nparser.add_argument('--rmaxcycles', default=8, type=int, help='limit max number of bisectional cycles')\nparser.add_argument('--model', dest='modelDir', type=str, default='train_log', help='directory with trained model files')\n\nargs = parser.parse_args()\n\ntry:\n    try:\n        try:\n            from model.RIFE_HDv2 import Model\n            model = Model()\n            model.load_model(args.modelDir, -1)\n            print(\"Loaded v2.x HD model.\")\n        except:\n            from train_log.RIFE_HDv3 import Model\n            model = Model()\n            model.load_model(args.modelDir, -1)\n            print(\"Loaded v3.x HD model.\")\n    except:\n        from model.RIFE_HD import Model\n        model = Model()\n        model.load_model(args.modelDir, -1)\n        print(\"Loaded v1.x HD model\")\nexcept:\n    from model.RIFE import Model\n    model = Model()\n    model.load_model(args.modelDir, -1)\n    print(\"Loaded ArXiv-RIFE model\")\nmodel.eval()\nmodel.device()\n\nif args.img[0].endswith('.exr') and args.img[1].endswith('.exr'):\n    img0 = cv2.imread(args.img[0], cv2.IMREAD_COLOR | cv2.IMREAD_ANYDEPTH)\n    img1 = cv2.imread(args.img[1], cv2.IMREAD_COLOR | cv2.IMREAD_ANYDEPTH)\n    img0 = (torch.tensor(img0.transpose(2, 0, 1)).to(device)).unsqueeze(0)\n    img1 = (torch.tensor(img1.transpose(2, 0, 1)).to(device)).unsqueeze(0)\n\nelse:\n    img0 = cv2.imread(args.img[0], cv2.IMREAD_UNCHANGED)\n    img1 = cv2.imread(args.img[1], cv2.IMREAD_UNCHANGED)\n    img0 = (torch.tensor(img0.transpose(2, 0, 1)).to(device) / 255.).unsqueeze(0)\n    img1 = (torch.tensor(img1.transpose(2, 0, 1)).to(device) / 255.).unsqueeze(0)\n\nn, c, h, w = img0.shape\nph = ((h - 1) // 32 + 1) * 32\npw = ((w - 1) // 32 + 1) * 32\npadding = (0, pw - w, 0, ph - h)\nimg0 = F.pad(img0, padding)\nimg1 = F.pad(img1, padding)\n\n\nif args.ratio:\n    img_list = [img0]\n    img0_ratio = 0.0\n    img1_ratio = 1.0\n    if args.ratio <= img0_ratio + args.rthreshold / 2:\n        middle = img0\n    elif args.ratio >= img1_ratio - args.rthreshold / 2:\n        middle = img1\n    else:\n        tmp_img0 = img0\n        tmp_img1 = img1\n        for inference_cycle in range(args.rmaxcycles):\n            middle = model.inference(tmp_img0, tmp_img1)\n            middle_ratio = ( img0_ratio + img1_ratio ) / 2\n            if args.ratio - (args.rthreshold / 2) <= middle_ratio <= args.ratio + (args.rthreshold / 2):\n                break\n            if args.ratio > middle_ratio:\n                tmp_img0 = middle\n                img0_ratio = middle_ratio\n            else:\n                tmp_img1 = middle\n                img1_ratio = middle_ratio\n    img_list.append(middle)\n    img_list.append(img1)\nelse:\n    img_list = [img0, img1]\n    for i in range(args.exp):\n        tmp = []\n        for j in range(len(img_list) - 1):\n            mid = model.inference(img_list[j], img_list[j + 1])\n            tmp.append(img_list[j])\n            tmp.append(mid)\n        tmp.append(img1)\n        img_list = tmp\n\nif not os.path.exists('output'):\n    os.mkdir('output')\nfor i in range(len(img_list)):\n    if args.img[0].endswith('.exr') and args.img[1].endswith('.exr'):\n        cv2.imwrite('output/img{}.exr'.format(i), (img_list[i][0]).cpu().numpy().transpose(1, 2, 0)[:h, :w], [cv2.IMWRITE_EXR_TYPE, cv2.IMWRITE_EXR_TYPE_HALF])\n    else:\n        cv2.imwrite('output/img{}.png'.format(i), (img_list[i][0] * 255).byte().cpu().numpy().transpose(1, 2, 0)[:h, :w])\n"
        },
        {
          "name": "inference_video.py",
          "type": "blob",
          "size": 10.9462890625,
          "content": "import os\nimport cv2\nimport torch\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.nn import functional as F\nimport warnings\nimport _thread\nimport skvideo.io\nfrom queue import Queue, Empty\nfrom model.pytorch_msssim import ssim_matlab\n\nwarnings.filterwarnings(\"ignore\")\n\ndef transferAudio(sourceVideo, targetVideo):\n    import shutil\n    import moviepy.editor\n    tempAudioFileName = \"./temp/audio.mkv\"\n\n    # split audio from original video file and store in \"temp\" directory\n    if True:\n\n        # clear old \"temp\" directory if it exits\n        if os.path.isdir(\"temp\"):\n            # remove temp directory\n            shutil.rmtree(\"temp\")\n        # create new \"temp\" directory\n        os.makedirs(\"temp\")\n        # extract audio from video\n        os.system('ffmpeg -y -i \"{}\" -c:a copy -vn {}'.format(sourceVideo, tempAudioFileName))\n\n    targetNoAudio = os.path.splitext(targetVideo)[0] + \"_noaudio\" + os.path.splitext(targetVideo)[1]\n    os.rename(targetVideo, targetNoAudio)\n    # combine audio file and new video file\n    os.system('ffmpeg -y -i \"{}\" -i {} -c copy \"{}\"'.format(targetNoAudio, tempAudioFileName, targetVideo))\n\n    if os.path.getsize(targetVideo) == 0: # if ffmpeg failed to merge the video and audio together try converting the audio to aac\n        tempAudioFileName = \"./temp/audio.m4a\"\n        os.system('ffmpeg -y -i \"{}\" -c:a aac -b:a 160k -vn {}'.format(sourceVideo, tempAudioFileName))\n        os.system('ffmpeg -y -i \"{}\" -i {} -c copy \"{}\"'.format(targetNoAudio, tempAudioFileName, targetVideo))\n        if (os.path.getsize(targetVideo) == 0): # if aac is not supported by selected format\n            os.rename(targetNoAudio, targetVideo)\n            print(\"Audio transfer failed. Interpolated video will have no audio\")\n        else:\n            print(\"Lossless audio transfer failed. Audio was transcoded to AAC (M4A) instead.\")\n\n            # remove audio-less video\n            os.remove(targetNoAudio)\n    else:\n        os.remove(targetNoAudio)\n\n    # remove temp directory\n    shutil.rmtree(\"temp\")\n\nparser = argparse.ArgumentParser(description='Interpolation for a pair of images')\nparser.add_argument('--video', dest='video', type=str, default=None)\nparser.add_argument('--output', dest='output', type=str, default=None)\nparser.add_argument('--img', dest='img', type=str, default=None)\nparser.add_argument('--montage', dest='montage', action='store_true', help='montage origin video')\nparser.add_argument('--model', dest='modelDir', type=str, default='train_log', help='directory with trained model files')\nparser.add_argument('--fp16', dest='fp16', action='store_true', help='fp16 mode for faster and more lightweight inference on cards with Tensor Cores')\nparser.add_argument('--UHD', dest='UHD', action='store_true', help='support 4k video')\nparser.add_argument('--scale', dest='scale', type=float, default=1.0, help='Try scale=0.5 for 4k video')\nparser.add_argument('--skip', dest='skip', action='store_true', help='whether to remove static frames before processing')\nparser.add_argument('--fps', dest='fps', type=int, default=None)\nparser.add_argument('--png', dest='png', action='store_true', help='whether to vid_out png format vid_outs')\nparser.add_argument('--ext', dest='ext', type=str, default='mp4', help='vid_out video extension')\nparser.add_argument('--exp', dest='exp', type=int, default=1)\nargs = parser.parse_args()\nassert (not args.video is None or not args.img is None)\nif args.skip:\n    print(\"skip flag is abandoned, please refer to issue #207.\")\nif args.UHD and args.scale==1.0:\n    args.scale = 0.5\nassert args.scale in [0.25, 0.5, 1.0, 2.0, 4.0]\nif not args.img is None:\n    args.png = True\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.set_grad_enabled(False)\nif torch.cuda.is_available():\n    torch.backends.cudnn.enabled = True\n    torch.backends.cudnn.benchmark = True\n    if(args.fp16):\n        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n\ntry:\n    try:\n        try:\n            from model.RIFE_HDv2 import Model\n            model = Model()\n            model.load_model(args.modelDir, -1)\n            print(\"Loaded v2.x HD model.\")\n        except:\n            from train_log.RIFE_HDv3 import Model\n            model = Model()\n            model.load_model(args.modelDir, -1)\n            print(\"Loaded v3.x HD model.\")\n    except:\n        from model.RIFE_HD import Model\n        model = Model()\n        model.load_model(args.modelDir, -1)\n        print(\"Loaded v1.x HD model\")\nexcept:\n    from model.RIFE import Model\n    model = Model()\n    model.load_model(args.modelDir, -1)\n    print(\"Loaded ArXiv-RIFE model\")\nmodel.eval()\nmodel.device()\n\nif not args.video is None:\n    videoCapture = cv2.VideoCapture(args.video)\n    fps = videoCapture.get(cv2.CAP_PROP_FPS)\n    tot_frame = videoCapture.get(cv2.CAP_PROP_FRAME_COUNT)\n    videoCapture.release()\n    if args.fps is None:\n        fpsNotAssigned = True\n        args.fps = fps * (2 ** args.exp)\n    else:\n        fpsNotAssigned = False\n    videogen = skvideo.io.vreader(args.video)\n    lastframe = next(videogen)\n    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n    video_path_wo_ext, ext = os.path.splitext(args.video)\n    print('{}.{}, {} frames in total, {}FPS to {}FPS'.format(video_path_wo_ext, args.ext, tot_frame, fps, args.fps))\n    if args.png == False and fpsNotAssigned == True:\n        print(\"The audio will be merged after interpolation process\")\n    else:\n        print(\"Will not merge audio because using png or fps flag!\")\nelse:\n    videogen = []\n    for f in os.listdir(args.img):\n        if 'png' in f:\n            videogen.append(f)\n    tot_frame = len(videogen)\n    videogen.sort(key= lambda x:int(x[:-4]))\n    lastframe = cv2.imread(os.path.join(args.img, videogen[0]), cv2.IMREAD_UNCHANGED)[:, :, ::-1].copy()\n    videogen = videogen[1:]\nh, w, _ = lastframe.shape\nvid_out_name = None\nvid_out = None\nif args.png:\n    if not os.path.exists('vid_out'):\n        os.mkdir('vid_out')\nelse:\n    if args.output is not None:\n        vid_out_name = args.output\n    else:\n        vid_out_name = '{}_{}X_{}fps.{}'.format(video_path_wo_ext, (2 ** args.exp), int(np.round(args.fps)), args.ext)\n    vid_out = cv2.VideoWriter(vid_out_name, fourcc, args.fps, (w, h))\n\ndef clear_write_buffer(user_args, write_buffer):\n    cnt = 0\n    while True:\n        item = write_buffer.get()\n        if item is None:\n            break\n        if user_args.png:\n            cv2.imwrite('vid_out/{:0>7d}.png'.format(cnt), item[:, :, ::-1])\n            cnt += 1\n        else:\n            vid_out.write(item[:, :, ::-1])\n\ndef build_read_buffer(user_args, read_buffer, videogen):\n    try:\n        for frame in videogen:\n             if not user_args.img is None:\n                  frame = cv2.imread(os.path.join(user_args.img, frame), cv2.IMREAD_UNCHANGED)[:, :, ::-1].copy()\n             if user_args.montage:\n                  frame = frame[:, left: left + w]\n             read_buffer.put(frame)\n    except:\n        pass\n    read_buffer.put(None)\n\ndef make_inference(I0, I1, n):\n    global model\n    middle = model.inference(I0, I1, args.scale)\n    if n == 1:\n        return [middle]\n    first_half = make_inference(I0, middle, n=n//2)\n    second_half = make_inference(middle, I1, n=n//2)\n    if n%2:\n        return [*first_half, middle, *second_half]\n    else:\n        return [*first_half, *second_half]\n\ndef pad_image(img):\n    if(args.fp16):\n        return F.pad(img, padding).half()\n    else:\n        return F.pad(img, padding)\n\nif args.montage:\n    left = w // 4\n    w = w // 2\ntmp = max(32, int(32 / args.scale))\nph = ((h - 1) // tmp + 1) * tmp\npw = ((w - 1) // tmp + 1) * tmp\npadding = (0, pw - w, 0, ph - h)\npbar = tqdm(total=tot_frame)\nif args.montage:\n    lastframe = lastframe[:, left: left + w]\nwrite_buffer = Queue(maxsize=500)\nread_buffer = Queue(maxsize=500)\n_thread.start_new_thread(build_read_buffer, (args, read_buffer, videogen))\n_thread.start_new_thread(clear_write_buffer, (args, write_buffer))\n\nI1 = torch.from_numpy(np.transpose(lastframe, (2,0,1))).to(device, non_blocking=True).unsqueeze(0).float() / 255.\nI1 = pad_image(I1)\ntemp = None # save lastframe when processing static frame\n\nwhile True:\n    if temp is not None:\n        frame = temp\n        temp = None\n    else:\n        frame = read_buffer.get()\n    if frame is None:\n        break\n    I0 = I1\n    I1 = torch.from_numpy(np.transpose(frame, (2,0,1))).to(device, non_blocking=True).unsqueeze(0).float() / 255.\n    I1 = pad_image(I1)\n    I0_small = F.interpolate(I0, (32, 32), mode='bilinear', align_corners=False)\n    I1_small = F.interpolate(I1, (32, 32), mode='bilinear', align_corners=False)\n    ssim = ssim_matlab(I0_small[:, :3], I1_small[:, :3])\n\n    break_flag = False\n    if ssim > 0.996:        \n        frame = read_buffer.get() # read a new frame\n        if frame is None:\n            break_flag = True\n            frame = lastframe\n        else:\n            temp = frame\n        I1 = torch.from_numpy(np.transpose(frame, (2,0,1))).to(device, non_blocking=True).unsqueeze(0).float() / 255.\n        I1 = pad_image(I1)\n        I1 = model.inference(I0, I1, args.scale)\n        I1_small = F.interpolate(I1, (32, 32), mode='bilinear', align_corners=False)\n        ssim = ssim_matlab(I0_small[:, :3], I1_small[:, :3])\n        frame = (I1[0] * 255).byte().cpu().numpy().transpose(1, 2, 0)[:h, :w]\n    \n    if ssim < 0.2:\n        output = []\n        for i in range((2 ** args.exp) - 1):\n            output.append(I0)\n        '''\n        output = []\n        step = 1 / (2 ** args.exp)\n        alpha = 0\n        for i in range((2 ** args.exp) - 1):\n            alpha += step\n            beta = 1-alpha\n            output.append(torch.from_numpy(np.transpose((cv2.addWeighted(frame[:, :, ::-1], alpha, lastframe[:, :, ::-1], beta, 0)[:, :, ::-1].copy()), (2,0,1))).to(device, non_blocking=True).unsqueeze(0).float() / 255.)\n        '''\n    else:\n        output = make_inference(I0, I1, 2**args.exp-1) if args.exp else []\n\n    if args.montage:\n        write_buffer.put(np.concatenate((lastframe, lastframe), 1))\n        for mid in output:\n            mid = (((mid[0] * 255.).byte().cpu().numpy().transpose(1, 2, 0)))\n            write_buffer.put(np.concatenate((lastframe, mid[:h, :w]), 1))\n    else:\n        write_buffer.put(lastframe)\n        for mid in output:\n            mid = (((mid[0] * 255.).byte().cpu().numpy().transpose(1, 2, 0)))\n            write_buffer.put(mid[:h, :w])\n    pbar.update(1)\n    lastframe = frame\n    if break_flag:\n        break\n\nif args.montage:\n    write_buffer.put(np.concatenate((lastframe, lastframe), 1))\nelse:\n    write_buffer.put(lastframe)\n\nwrite_buffer.put(None)\n\nimport time\nwhile(not write_buffer.empty()):\n    time.sleep(0.1)\npbar.close()\nif not vid_out is None:\n    vid_out.release()\n\n# move audio to new video file if appropriate\nif args.png == False and fpsNotAssigned == True and not args.video is None:\n    try:\n        transferAudio(args.video, vid_out_name)\n    except:\n        print(\"Audio transfer failed. Interpolated video will have no audio\")\n        targetNoAudio = os.path.splitext(vid_out_name)[0] + \"_noaudio\" + os.path.splitext(vid_out_name)[1]\n        os.rename(targetNoAudio, vid_out_name)\n"
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1220703125,
          "content": "numpy>=1.16, <=1.23.5\ntqdm>=4.35.0 \nsk-video>=1.1.10 \ntorch>=1.6.0 \nopencv-python>=4.1.2 \nmoviepy>=1.0.3 \ntorchvision>=0.7.0\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 7.0478515625,
          "content": "import os\nimport cv2\nimport math\nimport time\nimport torch\nimport torch.distributed as dist\nimport numpy as np\nimport random\nimport argparse\n\nfrom model.RIFE import Model\nfrom dataset import *\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data.distributed import DistributedSampler\n\ndevice = torch.device(\"cuda\")\n\nlog_path = 'train_log'\n\ndef get_learning_rate(step):\n    if step < 2000:\n        mul = step / 2000.\n        return 3e-4 * mul\n    else:\n        mul = np.cos((step - 2000) / (args.epoch * args.step_per_epoch - 2000.) * math.pi) * 0.5 + 0.5\n        return (3e-4 - 3e-6) * mul + 3e-6\n\ndef flow2rgb(flow_map_np):\n    h, w, _ = flow_map_np.shape\n    rgb_map = np.ones((h, w, 3)).astype(np.float32)\n    normalized_flow_map = flow_map_np / (np.abs(flow_map_np).max())\n    \n    rgb_map[:, :, 0] += normalized_flow_map[:, :, 0]\n    rgb_map[:, :, 1] -= 0.5 * (normalized_flow_map[:, :, 0] + normalized_flow_map[:, :, 1])\n    rgb_map[:, :, 2] += normalized_flow_map[:, :, 1]\n    return rgb_map.clip(0, 1)\n\ndef train(model, local_rank):\n    if local_rank == 0:\n        writer = SummaryWriter('train')\n        writer_val = SummaryWriter('validate')\n    else:\n        writer = None\n        writer_val = None\n    step = 0\n    nr_eval = 0\n    dataset = VimeoDataset('train')\n    sampler = DistributedSampler(dataset)\n    train_data = DataLoader(dataset, batch_size=args.batch_size, num_workers=8, pin_memory=True, drop_last=True, sampler=sampler)\n    args.step_per_epoch = train_data.__len__()\n    dataset_val = VimeoDataset('validation')\n    val_data = DataLoader(dataset_val, batch_size=16, pin_memory=True, num_workers=8)\n    print('training...')\n    time_stamp = time.time()\n    for epoch in range(args.epoch):\n        sampler.set_epoch(epoch)\n        for i, data in enumerate(train_data):\n            data_time_interval = time.time() - time_stamp\n            time_stamp = time.time()\n            data_gpu, timestep = data\n            data_gpu = data_gpu.to(device, non_blocking=True) / 255.\n            timestep = timestep.to(device, non_blocking=True)\n            imgs = data_gpu[:, :6]\n            gt = data_gpu[:, 6:9]\n            learning_rate = get_learning_rate(step) * args.world_size / 4\n            pred, info = model.update(imgs, gt, learning_rate, training=True) # pass timestep if you are training RIFEm\n            train_time_interval = time.time() - time_stamp\n            time_stamp = time.time()\n            if step % 200 == 1 and local_rank == 0:\n                writer.add_scalar('learning_rate', learning_rate, step)\n                writer.add_scalar('loss/l1', info['loss_l1'], step)\n                writer.add_scalar('loss/tea', info['loss_tea'], step)\n                writer.add_scalar('loss/distill', info['loss_distill'], step)\n            if step % 1000 == 1 and local_rank == 0:\n                gt = (gt.permute(0, 2, 3, 1).detach().cpu().numpy() * 255).astype('uint8')\n                mask = (torch.cat((info['mask'], info['mask_tea']), 3).permute(0, 2, 3, 1).detach().cpu().numpy() * 255).astype('uint8')\n                pred = (pred.permute(0, 2, 3, 1).detach().cpu().numpy() * 255).astype('uint8')\n                merged_img = (info['merged_tea'].permute(0, 2, 3, 1).detach().cpu().numpy() * 255).astype('uint8')\n                flow0 = info['flow'].permute(0, 2, 3, 1).detach().cpu().numpy()\n                flow1 = info['flow_tea'].permute(0, 2, 3, 1).detach().cpu().numpy()\n                for i in range(5):\n                    imgs = np.concatenate((merged_img[i], pred[i], gt[i]), 1)[:, :, ::-1]\n                    writer.add_image(str(i) + '/img', imgs, step, dataformats='HWC')\n                    writer.add_image(str(i) + '/flow', np.concatenate((flow2rgb(flow0[i]), flow2rgb(flow1[i])), 1), step, dataformats='HWC')\n                    writer.add_image(str(i) + '/mask', mask[i], step, dataformats='HWC')\n                writer.flush()\n            if local_rank == 0:\n                print('epoch:{} {}/{} time:{:.2f}+{:.2f} loss_l1:{:.4e}'.format(epoch, i, args.step_per_epoch, data_time_interval, train_time_interval, info['loss_l1']))\n            step += 1\n        nr_eval += 1\n        if nr_eval % 5 == 0:\n            evaluate(model, val_data, step, local_rank, writer_val)\n        model.save_model(log_path, local_rank)    \n        dist.barrier()\n\ndef evaluate(model, val_data, nr_eval, local_rank, writer_val):\n    loss_l1_list = []\n    loss_distill_list = []\n    loss_tea_list = []\n    psnr_list = []\n    psnr_list_teacher = []\n    time_stamp = time.time()\n    for i, data in enumerate(val_data):\n        data_gpu, timestep = data\n        data_gpu = data_gpu.to(device, non_blocking=True) / 255.        \n        imgs = data_gpu[:, :6]\n        gt = data_gpu[:, 6:9]\n        with torch.no_grad():\n            pred, info = model.update(imgs, gt, training=False)\n            merged_img = info['merged_tea']\n        loss_l1_list.append(info['loss_l1'].cpu().numpy())\n        loss_tea_list.append(info['loss_tea'].cpu().numpy())\n        loss_distill_list.append(info['loss_distill'].cpu().numpy())\n        for j in range(gt.shape[0]):\n            psnr = -10 * math.log10(torch.mean((gt[j] - pred[j]) * (gt[j] - pred[j])).cpu().data)\n            psnr_list.append(psnr)\n            psnr = -10 * math.log10(torch.mean((merged_img[j] - gt[j]) * (merged_img[j] - gt[j])).cpu().data)\n            psnr_list_teacher.append(psnr)\n        gt = (gt.permute(0, 2, 3, 1).cpu().numpy() * 255).astype('uint8')\n        pred = (pred.permute(0, 2, 3, 1).cpu().numpy() * 255).astype('uint8')\n        merged_img = (merged_img.permute(0, 2, 3, 1).cpu().numpy() * 255).astype('uint8')\n        flow0 = info['flow'].permute(0, 2, 3, 1).cpu().numpy()\n        flow1 = info['flow_tea'].permute(0, 2, 3, 1).cpu().numpy()\n        if i == 0 and local_rank == 0:\n            for j in range(10):\n                imgs = np.concatenate((merged_img[j], pred[j], gt[j]), 1)[:, :, ::-1]\n                writer_val.add_image(str(j) + '/img', imgs.copy(), nr_eval, dataformats='HWC')\n                writer_val.add_image(str(j) + '/flow', flow2rgb(flow0[j][:, :, ::-1]), nr_eval, dataformats='HWC')\n    \n    eval_time_interval = time.time() - time_stamp\n\n    if local_rank != 0:\n        return\n    writer_val.add_scalar('psnr', np.array(psnr_list).mean(), nr_eval)\n    writer_val.add_scalar('psnr_teacher', np.array(psnr_list_teacher).mean(), nr_eval)\n        \nif __name__ == \"__main__\":    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epoch', default=300, type=int)\n    parser.add_argument('--batch_size', default=16, type=int, help='minibatch size')\n    parser.add_argument('--local_rank', default=0, type=int, help='local rank')\n    parser.add_argument('--world_size', default=4, type=int, help='world size')\n    args = parser.parse_args()\n    torch.distributed.init_process_group(backend=\"nccl\", world_size=args.world_size)\n    torch.cuda.set_device(args.local_rank)\n    seed = 1234\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = True\n    model = Model(args.local_rank)\n    train(model, args.local_rank)\n        \n"
        }
      ]
    }
  ]
}