{
  "metadata": {
    "timestamp": 1736559550392,
    "page": 155,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "instillai/deep-learning-roadmap",
      "stars": 4613,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".DS_Store",
          "type": "blob",
          "size": 8.00390625,
          "content": null
        },
        {
          "name": "CODE_OF_CONDUCT.rst",
          "type": "blob",
          "size": 3.2373046875,
          "content": "Contributor Covenant Code of Conduct\n====================================\n\nOur Pledge\n----------\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\nOur Standards\n-------------\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\nOur Responsibilities\n--------------------\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\nScope\n-----\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\nEnforcement\n-----------\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at amirsina.torfi@gmail.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\nAttribution\n------------\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
        },
        {
          "name": "CONTRIBUTING.rst",
          "type": "blob",
          "size": 1.35546875,
          "content": "\n*************\nContributing\n*************\n\n*For typos, please do not create a pull request. Instead, declare them in issues or email the repository owner*. Please note we have a code of conduct, please follow it in all your interactions with the project.\n\n====================\nPull Request Process\n====================\n\nPlease consider the following criterions in order to help us in a better way:\n\n1. The pull request is mainly expected to be a link suggestion.\n2. Please make sure your suggested resources are not obsolete or broken.\n3. Ensure any install or build dependencies are removed before the end of the layer when doing a\n   build and creating a pull request.\n4. Add comments with details of changes to the interface, this includes new environment\n   variables, exposed ports, useful file locations and container parameters.\n5. You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you\n   do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.\n\n============\nFinal Note\n============\n\nWe are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.\nFor contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate\nyour kind feedback and elaborate code inspections.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0546875,
          "content": "MIT License\n\nCopyright (c) 2019 Open Source for Science\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 36.4892578125,
          "content": "###################################################\nDeep Learning - All You Need to Know\n###################################################\n\n.. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\n    :target: https://github.com/osforscience/deep-learning-all-you-need/pulls\n.. image:: https://badges.frapsoft.com/os/v2/open-source.png?v=103\n    :target: https://github.com/ellerbrock/open-source-badge/\n.. image:: https://img.shields.io/pypi/l/ansicolortags.svg\n      :target: https://github.com/osforscience/deep-learning-all-you-need/blob/master/LICENSE\n.. image:: https://img.shields.io/twitter/follow/machinemindset.svg?label=Follow&style=social\n      :target: https://twitter.com/machinemindset\n      \n##########################################################################\nSponsorship\n##########################################################################\n\nTo support maintaining and upgrading this project, please kindly consider `Sponsoring the project developer <https://github.com/sponsors/astorfi/dashboard>`_.\n\nAny level of support is a great contribution here :heart:\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. raw:: html\n\n <a href=\"https://github.com/sponsors/astorfi/dashboard\" target=\"_blank\">\n  <img width=\"600\" height=\"500\" align=\"center\" src=\"https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/donation.jpg\"/>\n </a>\n\n.. raw:: html\n\n   </div>      \n      \n\n###################################################\nDownload Free Python Machine Learning Book\n###################################################\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. raw:: html\n\n <a href=\"http://www.machinelearningmindset.com/deep-learning-roadmap/\" target=\"_blank\">\n  <img width=\"900\" height=\"625\" align=\"center\" src=\"https://github.com/machinelearningmindset/deep-learning-roadmap/blob/master/_img/mainpage/deeplearningroadmapbook.png\"/>\n </a>\n\n.. raw:: html\n\n   </div>\n   \n   \n###################################################\nSlack Group\n###################################################\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. raw:: html\n\n <a href=\"https://www.machinelearningmindset.com/slack-group/\" target=\"_blank\">\n  <img width=\"1033\" height=\"350\" align=\"center\" src=\"https://github.com/machinelearningmindset/TensorFlow-Course/blob/master/_img/0-welcome/joinslack.png\"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n\n##################\nTable of Contents\n##################\n.. contents::\n  :local:\n  :depth: 4\n\n.. image:: _img/mainpage/logo.gif\n\n***************\nIntroduction\n***************\n\nThe purpose of this project is to introduce a shortcut to developers and researcher\nfor finding useful resources about Deep Learning.\n\n============\nMotivation\n============\n\nThere are different motivations for this open source project.\n\n.. --------------------\n.. Why Deep Learning?\n.. --------------------\n\n------------------------------------------------------------\nWhat's the point of this open source project?\n------------------------------------------------------------\n\nThere are other repositories similar to this repository that are very\ncomprehensive and useful and to be honest they made me ponder if there is\na necessity for this repository!\n\n**The point of this repository is that the resources are being targeted**. The organization\nof the resources is such that the user can easily find the things he/she is looking for.\nWe divided the resources to a large number of categories that in the beginning one may\nhave a headache!!! However, if someone knows what is being located, it is very easy to find the most related resources.\nEven if someone doesn't know what to look for, in the beginning, the general resources have\nbeen provided.\n\n\n.. ================================================\n.. How to make the most of this effort\n.. ================================================\n\n************\nPapers\n************\n\n.. image:: _img/mainpage/article.jpeg\n\nThis chapter is associated with the papers published in deep learning.\n\n====================\nModels\n====================\n\n-----------------------\nConvolutional Networks\n-----------------------\n\n  .. image:: _img/mainpage/convolutional.png\n\n.. For continuous lines, the lines must be start from the same locations.\n* **Imagenet classification with deep convolutional neural networks** :\n  [`Paper <http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks>`_][`Code <https://github.com/dontfollowmeimcrazy/imagenet>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Convolutional Neural Networks for Sentence Classification** :\n  [`Paper <https://arxiv.org/abs/1408.5882>`_][`Code <https://github.com/yoonkim/CNN_sentence>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Large-scale Video Classification with Convolutional Neural Networks** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html>`_][`Project Page <https://cs.stanford.edu/people/karpathy/deepvideo/>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n\n* **Deep convolutional neural networks for LVCSR** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/6639347/&hl=zh-CN&sa=T&oi=gsb&ct=res&cd=0&ei=KknXWYbGFMbFjwSsyICADQ&scisig=AAGBfm2F0Zlu0ciUwadzshNNm80IQQhuhA>`_]\n  \n  .. image:: _img/mainpage/star_3.png\n\n* **Face recognition: a convolutional neural-network approach** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/554195/>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n\n\n-----------------------\nRecurrent Networks\n-----------------------\n\n  .. image:: _img/mainpage/Recurrent_neural_network_unfold.svg\n\n\n.. For continuous lines, the lines must be start from the same locations.\n* **An empirical exploration of recurrent network architectures** :\n  [`Paper <http://proceedings.mlr.press/v37/jozefowicz15.pdf?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue>`_][`Code <https://github.com/debajyotidatta/RecurrentArchitectures>`_]\n\n\n  .. image:: _img/mainpage/star_4.png\n\n* **LSTM: A search space odyssey** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/7508408/>`_][`Code <https://github.com/fomorians/lstm-odyssey>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n\n* **On the difficulty of training recurrent neural networks** :\n  [`Paper <http://proceedings.mlr.press/v28/pascanu13.pdf>`_][`Code <https://github.com/pascanur/trainingRNNs>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Learning to forget: Continual prediction with LSTM** :\n  [`Paper <http://digital-library.theiet.org/content/conferences/10.1049/cp_19991218>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n-----------------------\nAutoencoders\n-----------------------\n\n.. image:: _img/mainpage/Autoencoder_structure.png\n\n* **Extracting and composing robust features with denoising autoencoders** :\n  [`Paper <https://dl.acm.org/citation.cfm?id=1390294>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion** :\n  [`Paper <http://www.jmlr.org/papers/v11/vincent10a.html>`_][`Code <https://github.com/rajarsheem/libsdae-autoencoder-tensorflow>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Adversarial Autoencoders** :\n  [`Paper <https://arxiv.org/abs/1511.05644>`_][`Code <https://github.com/conan7882/adversarial-autoencoders>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Autoencoders, Unsupervised Learning, and Deep Architectures** :\n  [`Paper <http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Reducing the Dimensionality of Data with Neural Networks** :\n  [`Paper <http://science.sciencemag.org/content/313/5786/504>`_][`Code <https://github.com/jordn/autoencoder>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n\n-----------------------\nGenerative Models\n-----------------------\n\n.. image:: _img/mainpage/generative.png\n\n* **Exploiting generative models discriminative classifiers** :\n  [`Paper <http://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Semi-supervised Learning with Deep Generative Models** :\n  [`Paper <http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models>`_][`Code <https://github.com/wohlert/semi-supervised-pytorch>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n\n* **Generative Adversarial Nets** :\n  [`Paper <http://papers.nips.cc/paper/5423-generative-adversarial-nets>`_][`Code <https://github.com/goodfeli/adversarial>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Generalized Denoising Auto-Encoders as Generative Models** :\n  [`Paper <http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models>`_]\n\n  .. image:: _img/mainpage/star_5.png\n  \n* **Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks** :\n  [`Paper <https://arxiv.org/abs/1511.06434>`_][`Code <https://github.com/carpedm20/DCGAN-tensorflow>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n\n-----------------------\nProbabilistic Models\n-----------------------\n\n* **Stochastic Backpropagation and Approximate Inference in Deep Generative Models** :\n  [`Paper <https://arxiv.org/abs/1401.4082>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Probabilistic models of cognition: exploring representations and inductive biases** :\n  [`Paper <https://www.sciencedirect.com/science/article/pii/S1364661310001129>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **On deep generative models with applications to recognition** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/5995710/>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n\n\n\n\n====================\nCore\n====================\n\n---------------------\nOptimization\n---------------------\n\n.. ################################################################################\n.. For continuous lines, the lines must be start from the same locations.\n* **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift** :\n  [`Paper <https://arxiv.org/abs/1502.03167>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Dropout: A Simple Way to Prevent Neural Networks from Overfitting** :\n  [`Paper <http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Training Very Deep Networks** :\n  [`Paper <http://papers.nips.cc/paper/5850-training-very-deep-networks>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Large Scale Distributed Deep Networks** :\n  [`Paper <http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n------------------------\nRepresentation Learning\n------------------------\n\n* **Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks** :\n  [`Paper <https://arxiv.org/abs/1511.06434>`_][`Code <https://github.com/Newmu/dcgan_code>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Representation Learning: A Review and New Perspectives** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/6472238/>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets** :\n  [`Paper <http://papers.nips.cc/paper/6399-infogan-interpretable-representation>`_][`Code <https://github.com/openai/InfoGAN>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n\n------------------------------------\nUnderstanding and Transfer Learning\n------------------------------------\n\n* **Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Distilling the Knowledge in a Neural Network** :\n  [`Paper <https://arxiv.org/abs/1503.02531>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition** :\n  [`Paper <http://proceedings.mlr.press/v32/donahue14.pdf>`_][\n\n  .. image:: _img/mainpage/star_5.png\n\n* **How transferable are features in deep neural networks?** :\n  [`Paper <http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-n%E2%80%A6>`_][`Code <https://github.com/yosinski/convnet_transfer>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n-----------------------\nReinforcement Learning\n-----------------------\n\n* **Human-level control through deep reinforcement learning** :\n  [`Paper <https://www.nature.com/articles/nature14236/>`_][`Code <https://github.com/devsisters/DQN-tensorflow>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Playing Atari with Deep Reinforcement Learning** :\n  [`Paper <https://arxiv.org/abs/1312.5602>`_][`Code <https://github.com/carpedm20/deep-rl-tensorflow>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Continuous control with deep reinforcement learning** :\n  [`Paper <https://arxiv.org/abs/1509.02971>`_][`Code <https://github.com/stevenpjg/ddpg-aigym>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Deep Reinforcement Learning with Double Q-Learning** :\n  [`Paper <http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847>`_][`Code <https://github.com/carpedm20/deep-rl-tensorflow>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Dueling Network Architectures for Deep Reinforcement Learning** :\n  [`Paper <https://arxiv.org/abs/1511.06581>`_][`Code <https://github.com/yoosan/deeprl>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n\n====================\nApplications\n====================\n\n--------------------\nImage Recognition\n--------------------\n\n* **Deep Residual Learning for Image Recognition** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html>`_][`Code <https://github.com/gcr/torch-residual-networks>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Very Deep Convolutional Networks for Large-Scale Image Recognition** :\n  [`Paper <https://arxiv.org/abs/1409.1556>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Multi-column Deep Neural Networks for Image Classification** :\n  [`Paper <https://arxiv.org/abs/1202.2745>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **DeepID3: Face Recognition with Very Deep Neural Networks** :\n  [`Paper <https://arxiv.org/abs/1502.00873>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps** :\n  [`Paper <https://arxiv.org/abs/1312.6034>`_][`Code <https://github.com/artvandelay/Deep_Inside_Convolutional_Networks>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Deep Image: Scaling up Image Recognition** :\n  [`Paper <https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Long-Term Recurrent Convolutional Networks for Visual Recognition and Description** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html>`_][`Code <https://github.com/JaggerYoung/LRCN-for-Activity-Recognition>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **3D Convolutional Neural Networks for Cross Audio-Visual Matching Recognition** :\n  [`Paper <https://ieeexplore.ieee.org/document/8063416>`_][`Code <https://github.com/astorfi/lip-reading-deeplearning>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n--------------------\nObject Recognition\n--------------------\n\n* **ImageNet Classification with Deep Convolutional Neural Networks** :\n  [`Paper <http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Learning Deep Features for Scene Recognition using Places Database** :\n  [`Paper <http://papers.nips.cc/paper/5349-learning-deep-features>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Scalable Object Detection using Deep Neural Networks** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Erhan_Scalable_Object_Detection_2014_CVPR_paper.html>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks** :\n  [`Paper <http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks>`_][`Code <https://github.com/rbgirshick/py-faster-rcnn>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks** :\n  [`Paper <https://arxiv.org/abs/1312.6229>`_][`Code <https://github.com/sermanet/OverFeat>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **CNN Features Off-the-Shelf: An Astounding Baseline for Recognition** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/html/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.html>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **What is the best multi-stage architecture for object recognition?** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/5459469/>`_]\n\n  .. image:: _img/mainpage/star_2.png\n\n\n--------------------\nAction Recognition\n--------------------\n\n* **Long-Term Recurrent Convolutional Networks for Visual Recognition and Description** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Learning Spatiotemporal Features With 3D Convolutional Networks** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html>`_][`Code <https://github.com/DavideA/c3d-pytorch>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Describing Videos by Exploiting Temporal Structure** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Yao_Describing_Videos_by_ICCV_2015_paper.html>`_][`Code <https://github.com/tsenghungchen/SA-tensorflow>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Convolutional Two-Stream Network Fusion for Video Action Recognition** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html>`_][`Code <https://github.com/feichtenhofer/twostreamfusion>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Temporal segment networks: Towards good practices for deep action recognition** :\n  [`Paper <https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2>`_][`Code <https://github.com/yjxiong/temporal-segment-networks>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n----------------------------\nCaption Generation\n----------------------------\n\n* **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention** :\n  [`Paper <http://proceedings.mlr.press/v37/xuc15.pdf>`_][`Code <https://github.com/yunjey/show-attend-and-tell>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Mind's Eye: A Recurrent Visual Representation for Image Caption Generation** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Chen_Minds_Eye_A_2015_CVPR_paper.html>`_]\n\n  .. image:: _img/mainpage/star_2.png\n\n* **Generative Adversarial Text to Image Synthesis** :\n  [`Paper <http://proceedings.mlr.press/v48/reed16.pdf>`_][`Code <https://github.com/zsdonghao/text-to-image>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Deep Visual-Semantic Al60ignments for Generating Image Descriptions** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html>`_][`Code <https://github.com/jonkuo/Deep-Learning-Image-Captioning>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Show and Tell: A Neural Image Caption Generator** :\n  [`Paper <https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html>`_][`Code <https://github.com/DeepRNN/image_captioning>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n\n----------------------------\nNatural Language Processing\n----------------------------\n\n* **Distributed Representations of Words and Phrases and their Compositionality** :\n  [`Paper <http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_][`Code <https://code.google.com/archive/p/word2vec/>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Efficient Estimation of Word Representations in Vector Space** :\n  [`Paper <https://arxiv.org/pdf/1301.3781.pdf>`_][`Code <https://code.google.com/archive/p/word2vec/>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Sequence to Sequence Learning with Neural Networks** :\n  [`Paper <https://arxiv.org/pdf/1409.3215.pdf>`_][`Code <https://github.com/farizrahman4u/seq2seq>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Neural Machine Translation by Jointly Learning to Align and Translate** :\n  [`Paper <https://arxiv.org/pdf/1409.0473.pdf>`_][`Code <https://github.com/tensorflow/nmt>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Get To The Point: Summarization with Pointer-Generator Networks** :\n  [`Paper <https://arxiv.org/abs/1704.04368>`_][`Code <https://github.com/abisee/pointer-generator>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Attention Is All You Need** :\n  [`Paper <https://arxiv.org/abs/1706.03762>`_][`Code <https://github.com/jadore801120/attention-is-all-you-need-pytorch>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Convolutional Neural Networks for Sentence Classification** :\n  [`Paper <https://arxiv.org/abs/1408.5882>`_][`Code <https://github.com/yoonkim/CNN_sentence>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n\n----------------------------\nSpeech Technology\n----------------------------\n\n* **Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/6296526/>`_]\n\n  .. image:: _img/mainpage/star_5.png\n\n* **Towards End-to-End Speech Recognition with Recurrent Neural Networks** :\n  [`Paper <http://proceedings.mlr.press/v32/graves14.pdf>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Speech recognition with deep recurrent neural networks** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/6638947/>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition** :\n  [`Paper <https://arxiv.org/abs/1507.06947>`_]\n\n  .. image:: _img/mainpage/star_3.png\n\n* **Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin** :\n  [`Paper <http://proceedings.mlr.press/v48/amodei16.html>`_][`Code <https://github.com/PaddlePaddle/DeepSpeech>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n* **A novel scheme for speaker recognition using a phonetically-aware deep neural network** :\n  [`Paper <https://ieeexplore.ieee.org/abstract/document/6853887/>`_]\n\n  .. image:: _img/mainpage/star_3.png\n \n* **Text-Independent Speaker Verification Using 3D Convolutional Neural Networks** :\n  [`Paper <https://arxiv.org/abs/1705.09422>`_][`Code <https://github.com/astorfi/3D-convolutional-speaker-recognition>`_]\n\n  .. image:: _img/mainpage/star_4.png\n\n\n************\nDatasets\n************\n\n====================\nImage\n====================\n\n\n----------------------------\nGeneral\n----------------------------\n\n* **MNIST** Handwritten digits:\n  [`Link <http://yann.lecun.com/exdb/mnist/>`_]\n\n\n----------------------------\nFace\n----------------------------\n\n* **Face Recognition Technology (FERET)** The goal of the FERET program was to develop automatic face recognition capabilities that could be employed to assist security, intelligence, and law enforcement personnel in the performance of their duties:\n  [`Link <https://www.nist.gov/programs-projects/face-recognition-technology-feret>`_]\n\n* **The CMU Pose, Illumination, and Expression (PIE) Database of Human Faces** Between October and December 2000 we collected a database of 41,368 images of 68 people:\n  [`Link <https://www.ri.cmu.edu/publications/the-cmu-pose-illumination-and-expression-pie-database-of-human-faces/>`_]\n\n* **YouTube Faces DB** The data set contains 3,425 videos of 1,595 different people. All the videos were downloaded from YouTube. An average of 2.15 videos are available for each subject:\n  [`Link <https://www.cs.tau.ac.il/~wolf/ytfaces/>`_]\n\n* **Grammatical Facial Expressions Data Set** Developed to assist the the automated analysis of facial expressions:\n  [`Link <https://archive.ics.uci.edu/ml/datasets/Grammatical+Facial+Expressions>`_]\n\n* **FaceScrub** A Dataset With Over 100,000 Face Images of 530 People:\n  [`Link <http://vintage.winklerbros.net/facescrub.html>`_]\n\n* **IMDB-WIKI** 500k+ face images with age and gender labels:\n  [`Link <https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/>`_]\n\n* **FDDB** Face Detection Data Set and Benchmark (FDDB):\n  [`Link <http://vis-www.cs.umass.edu/fddb/>`_]\n\n----------------------------\nObject Recognition\n----------------------------\n\n* **COCO** Microsoft COCO: Common Objects in Context:\n  [`Link <http://cocodataset.org/#home>`_]\n\n* **ImageNet** The famous ImageNet dataset:\n  [`Link <http://www.image-net.org/>`_]\n\n* **Open Images Dataset** Open Images is a dataset of ~9 million images that have been annotated with image-level labels and object bounding boxes:\n  [`Link <https://storage.googleapis.com/openimages/web/index.html>`_]\n\n* **Caltech-256 Object Category Dataset** A large dataset object classification:\n  [`Link <https://authors.library.caltech.edu/7694/>`_]\n\n* **Pascal VOC dataset** A large dataset for classification tasks:\n  [`Link <http://host.robots.ox.ac.uk/pascal/VOC/>`_]\n\n* **CIFAR 10 / CIFAR 100** The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes. CIFAR-100 is similar to CIFAR-10 but it has 100 classes containing 600 images each:\n  [`Link <https://www.cs.toronto.edu/~kriz/cifar.html>`_]\n\n\n----------------------------\nAction recognition\n----------------------------\n\n* **HMDB** a large human motion database:\n  [`Link <http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/>`_]\n\n* **MHAD** Berkeley Multimodal Human Action Database:\n  [`Link <http://tele-immersion.citris-uc.org/berkeley_mhad>`_]\n\n* **UCF101 - Action Recognition Data Set** UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. This data set is an extension of UCF50 data set which has 50 action categories:\n  [`Link <http://crcv.ucf.edu/data/UCF101.php>`_]\n\n* **THUMOS Dataset** A large dataset for action classification:\n  [`Link <http://crcv.ucf.edu/data/THUMOS.php>`_]\n\n* **ActivityNet** A Large-Scale Video Benchmark for Human Activity Understanding:\n  [`Link <http://activity-net.org/>`_]\n\n======================================\nText and Natural Language Processing\n======================================\n\n\n-----------------------\nGeneral\n-----------------------\n\n* **1 Billion Word Language Model Benchmark**: The purpose of the project is to make available a standard training and test setup for language modeling experiments:\n  [`Link <http://www.statmt.org/lm-benchmark/>`_]\n\n* **Common Crawl**: The Common Crawl corpus contains petabytes of data collected over the last 7 years. It contains raw web page data, extracted metadata and text extractions:\n  [`Link <http://commoncrawl.org/the-data/get-started/>`_]\n\n* **Yelp Open Dataset**: A subset of Yelp's businesses, reviews, and user data for use in personal, educational, and academic purposes:\n  [`Link <https://www.yelp.com/dataset>`_]\n\n\n-----------------------\nText classification\n-----------------------\n\n* **20 newsgroups** The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups:\n  [`Link <http://qwone.com/~jason/20Newsgroups/>`_]\n\n* **Broadcast News** The 1996 Broadcast News Speech Corpus contains a total of 104 hours of broadcasts from ABC, CNN and CSPAN television networks and NPR and PRI radio networks with corresponding transcripts:\n  [`Link <https://catalog.ldc.upenn.edu/LDC97S44>`_]\n\n* **The wikitext long term dependency language modeling dataset**: A collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. :\n  [`Link <https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset>`_]\n\n-----------------------\nQuestion Answering\n-----------------------\n\n* **Question Answering Corpus** by Deep Mind and Oxford which is two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites.\n  [`Link <https://github.com/deepmind/rc-data>`_]\n\n* **Stanford Question Answering Dataset (SQuAD)** consisting of questions posed by crowdworkers on a set of Wikipedia articles:\n  [`Link <https://rajpurkar.github.io/SQuAD-explorer/>`_]\n\n* **Amazon question/answer data** contains Question and Answer data from Amazon, totaling around 1.4 million answered questions:\n  [`Link <http://jmcauley.ucsd.edu/data/amazon/qa/>`_]\n\n\n\n-----------------------\nSentiment Analysis\n-----------------------\n\n* **Multi-Domain Sentiment Dataset** TThe Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains):\n  [`Link <http://www.cs.jhu.edu/~mdredze/datasets/sentiment/>`_]\n\n* **Stanford Sentiment Treebank Dataset** The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language:\n  [`Link <https://nlp.stanford.edu/sentiment/>`_]\n\n* **Large Movie Review Dataset**: This is a dataset for binary sentiment classification:\n  [`Link <http://ai.stanford.edu/~amaas/data/sentiment/>`_]\n\n\n-----------------------\nMachine Translation\n-----------------------\n\n* **Aligned Hansards of the 36th Parliament of Canada** dataset contains 1.3 million pairs of aligned text chunks:\n  [`Link <https://www.isi.edu/natural-language/download/hansard/>`_]\n\n* **Europarl: A Parallel Corpus for Statistical Machine Translation** dataset extracted from the proceedings of the European Parliament:\n  [`Link <http://www.statmt.org/europarl/>`_]\n\n\n-----------------------\nSummarization\n-----------------------\n\n* **Legal Case Reports Data Set** as a textual corpus of 4000 legal cases for automatic summarization and citation analysis.:\n  [`Link <https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports>`_]\n\n\n======================================\nSpeech Technology\n======================================\n\n* **TIMIT Acoustic-Phonetic Continuous Speech Corpus** The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems:\n  [`Link <https://catalog.ldc.upenn.edu/ldc93s1>`_]\n\n* **LibriSpeech** LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey:\n  [`Link <http://www.openslr.org/12/>`_]\n\n* **VoxCeleb** A large scale audio-visual dataset:\n  [`Link <http://www.robots.ox.ac.uk/~vgg/data/voxceleb/>`_]\n\n* **NIST Speaker Recognition**:\n  [`Link <https://www.nist.gov/itl/iad/mig/speaker-recognition>`_]\n\n\n\n\n\n\n************\nCourses\n************\n\n.. image:: _img/mainpage/online.png\n\n* **Machine Learning** by Stanford on Coursera :\n  [`Link <https://www.coursera.org/learn/machine-learning>`_]\n\n* **Neural Networks and Deep Learning** Specialization by Coursera:\n  [`Link <https://www.coursera.org/learn/neural-networks-deep-learning>`_]\n\n* **Intro to Deep Learning** by Google:\n  [`Link <https://www.udacity.com/course/deep-learning--ud730>`_]\n\n* **Introduction to Deep Learning** by CMU:\n  [`Link <http://deeplearning.cs.cmu.edu/>`_]\n\n* **NVIDIA Deep Learning Institute** by NVIDIA:\n  [`Link <https://www.nvidia.com/en-us/deep-learning-ai/education/>`_]\n\n* **Convolutional Neural Networks for Visual Recognition** by Stanford:\n  [`Link <http://cs231n.stanford.edu/>`_]\n\n* **Deep Learning for Natural Language Processing** by Stanford:\n  [`Link <http://cs224d.stanford.edu/>`_]\n\n* **Deep Learning** by fast.ai:\n  [`Link <http://www.fast.ai/>`_]\n\n* **Course on Deep Learning for Visual Computing** by IITKGP:\n  [`Link <https://www.youtube.com/playlist?list=PLuv3GM6-gsE1Biyakccxb3FAn4wBLyfWf>`_]\n\n\n\n\n************\nBooks\n************\n\n.. image:: _img/mainpage/books.jpg\n\n* **Deep Learning** by Ian Goodfellow:\n  [`Link <http://www.deeplearningbook.org/>`_]\n\n* **Neural Networks and Deep Learning** :\n  [`Link <http://neuralnetworksanddeeplearning.com/>`_]\n\n* **Deep Learning with Python**:\n  [`Link <https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=as_li_ss_tl?s=books&ie=UTF8&qid=1519989624&sr=1-4&keywords=deep+learning+with+python&linkCode=sl1&tag=trndingcom-20&linkId=ec7663329fdb7ace60f39c762e999683>`_]\n\n* **Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems**:\n  [`Link <https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/ref=as_li_ss_tl?ie=UTF8&qid=1519989725&sr=1-2-ent&linkCode=sl1&tag=trndingcom-20&linkId=71938c9398940c7b0a811dc1cfef7cc3>`_]\n\n\n************\nBlogs\n************\n\n.. image:: _img/mainpage/Blogger_icon.png\n\n* **Colah's blog**:\n  [`Link <http://colah.github.io/>`_]\n\n* **Andrej Karpathy blog**:\n  [`Link <http://karpathy.github.io/>`_]\n\n* **The Spectator** Shakir's Machine Learning Blog:\n  [`Link <http://blog.shakirm.com/>`_]\n\n* **WILDML**:\n  [`Link <http://www.wildml.com/about/>`_]\n\n* **Distill blog** It is more like a journal than a blog because it has a peer review process and only accepted articles will be published on that.:\n  [`Link <https://distill.pub/>`_]\n\n* **BAIR** Berkeley Artificial Inteliigent Research:\n  [`Link <http://bair.berkeley.edu/blog/>`_]\n\n* **Sebastian Ruder's blog**:\n  [`Link <http://ruder.io/>`_]\n\n* **inFERENCe**:\n  [`Link <https://www.inference.vc/page/2/>`_]\n\n* **i am trask** A Machine Learning Craftsmanship Blog:\n  [`Link <http://iamtrask.github.io>`_]\n\n\n************\nTutorials\n************\n\n.. image:: _img/mainpage/tutorial.png\n\n* **Deep Learning Tutorials**:\n  [`Link <http://deeplearning.net/tutorial/>`_]\n\n* **Deep Learning for NLP with Pytorch** by Pytorch:\n  [`Link <https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html>`_]\n\n* **Deep Learning for Natural Language Processing: Tutorials with Jupyter Notebooks** by Jon Krohn:\n  [`Link <https://insights.untapt.com/deep-learning-for-natural-language-processing-tutorials-with-jupyter-notebooks-ad67f336ce3f>`_]\n\n\n************\nFrameworks\n************\n\n* **Tensorflow**:\n  [`Link <https://www.tensorflow.org/>`_]\n\n* **Pytorch**:\n  [`Link <https://pytorch.org/>`_]\n\n* **CNTK**:\n  [`Link <https://docs.microsoft.com/en-us/cognitive-toolkit/>`_]\n\n* **MatConvNet**:\n  [`Link <http://www.vlfeat.org/matconvnet/>`_]\n\n* **Keras**:\n  [`Link <https://keras.io/>`_]\n\n* **Caffe**:\n  [`Link <http://caffe.berkeleyvision.org/>`_]\n\n* **Theano**:\n  [`Link <http://www.deeplearning.net/software/theano/>`_]\n\n* **CuDNN**:\n  [`Link <https://developer.nvidia.com/cudnn>`_]\n\n* **Torch**:\n  [`Link <https://github.com/torch/torch7>`_]\n\n* **Deeplearning4j**:\n  [`Link <https://deeplearning4j.org/>`_]\n\n\n************\nContributing\n************\n\n\n*For typos, unless significant changes, please do not create a pull request. Instead, declare them in issues or email the repository owner*. Please note we have a code of conduct, please follow it in all your interactions with the project.\n\n========================\nPull Request Process\n========================\n\nPlease consider the following criterions in order to help us in a better way:\n\n1. The pull request is mainly expected to be a link suggestion.\n2. Please make sure your suggested resources are not obsolete or broken.\n3. Ensure any install or build dependencies are removed before the end of the layer when doing a\n   build and creating a pull request.\n4. Add comments with details of changes to the interface, this includes new environment\n   variables, exposed ports, useful file locations and container parameters.\n5. You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you\n   do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.\n\n========================\nFinal Note\n========================\n\nWe are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.\nFor contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate\nyour kind feedback and support.\n"
        },
        {
          "name": "_img",
          "type": "tree",
          "content": null
        },
        {
          "name": "conf.py",
          "type": "blob",
          "size": 1.9697265625,
          "content": "import os\nimport sys\n\nimport sphinx_rtd_theme\nsys.path.insert(0, os.path.abspath('_ext'))\n# from recommonmark.parser import CommonMarkParser\n\nsys.path.insert(0, os.path.abspath('..'))\nsys.path.append(os.path.dirname(__file__))\n# os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"readthedocs.settings.dev\")\n\n# from django.conf import settings\n\n# import django\n# django.setup()\n\n\nsys.path.append(os.path.abspath('_ext'))\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.intersphinx',\n]\ntemplates_path = ['_templates']\n\nedit_on_github_project = 'astorfi/Deep-Learning-NLP'\n\nedit_on_github_branch = 'master'\n\nsource_suffix = ['.rst', '.md']\n# source_parsers = {\n#     '.md': CommonMarkParser,\n# }\n\nmaster_doc = 'index'\nproject = u'Deep-Learning-NLP'\ncopyright = u'2018, Amirsina Torfi'\nauthor = u'Amirsina Torfi'\nversion = '1.0'\nrelease = '1.0'\nexclude_patterns = ['_build']\ndefault_role = 'obj'\npygments_style = 'sphinx'\n# intersphinx_mapping = {\n#     'TensorFlow-World': ('https://github.com/astorfi/TensorFlow-World', None),\n# }\nhtmlhelp_basename = 'ReadTheDocsdoc'\nlatex_documents = [\n    (master_doc, 'Deep-Learning-NLP.tex', u'Deep-Learning-NLP Documentation',\n     u'Amirsina Torfi', 'manual'),\n]\nman_pages = [\n    (master_doc, 'Deep-Learning-NLP', u'Deep-Learning-NLP Documentation',\n     [author], 1)\n]\nexclude_patterns = [\n    # 'api' # needed for ``make gettext`` to not die.\n]\n\nlanguage = 'en'\n\nlocale_dirs = [\n    'locale/',\n]\ngettext_compact = False\n\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = ['_static']\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\nhtml_logo = 'logo/logo.png'\nhtml_theme_options = {\n    'logo_only': True,\n    'display_version': False,\n}\n\ngithub_url='https://github.com/astorfi/Deep-Learning-NLP'\n\nhtml_context = {\n\"display_github\": True, # Add 'Edit on Github' link instead of 'View page source'\n\"last_updated\": True,\n\"commit\": False,\n 'github_url': 'https://github.com/astorfi/Deep-Learning-NLP'\n}\n\n\n#def setup(app):\n#     app.add_stylesheet('custom.css')\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "source",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}