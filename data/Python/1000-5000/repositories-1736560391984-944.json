{
  "metadata": {
    "timestamp": 1736560391984,
    "page": 944,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "gpt-omni/mini-omni",
      "stars": 3256,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.12109375,
          "content": "# Byte-compiled / optimized / DLL files\n*pyc\n*pth\ncheckpoint/\ncheckpoint_bak/\noutput/\n.DS_Store\n\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control\n.pdm.toml\n.pdm-python\n.pdm-build/\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2024 gpt-omni\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.484375,
          "content": "\r\n# Mini-Omni\r\n\r\n<p align=\"center\"><strong style=\"font-size: 18px;\">\r\nMini-Omni: Language Models Can Hear, Talk While Thinking in Streaming\r\n</strong>\r\n</p>\r\n\r\n<p align=\"center\">\r\nðŸ¤— <a href=\"https://huggingface.co/gpt-omni/mini-omni\">Hugging Face</a>   | ðŸ“– <a href=\"https://github.com/gpt-omni/mini-omni\">Github</a> \r\n|     ðŸ“‘ <a href=\"https://arxiv.org/abs/2408.16725\">Technical report</a> |\r\nðŸ¤— <a href=\"https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K\">Datasets</a>\r\n</p>\r\n\r\nMini-Omni is an open-source multimodal large language model that can **hear, talk while thinking**. Featuring real-time end-to-end speech input and **streaming audio output** conversational capabilities.\r\n\r\n<p align=\"center\">\r\n    <img src=\"data/figures/frameworkv3.jpg\" width=\"100%\"/>\r\n</p>\r\n\r\n\r\n## Updates\r\n\r\n- **2024.10:** We released [Mini-Omni2](https://github.com/gpt-omni/mini-omni2) with vision and audio capabilities. \r\n- **2024.09:** Amazing online [interactive gradio demo](https://huggingface.co/spaces/gradio/omni-mini) by ðŸ¤— gradio team.\r\n- **2024.09:** **VoiceAssistant-400K** is uploaded to [Hugging Face](https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K).\r\n\r\n## Features\r\n\r\nâœ… **Real-time speech-to-speech** conversational capabilities. No extra ASR or TTS models required.\r\n\r\nâœ… **Talking while thinking**, with the ability to generate text and audio at the same time.\r\n\r\nâœ… **Streaming audio output** capabilities.\r\n\r\nâœ… With \"Audio-to-Text\" and \"Audio-to-Audio\" **batch inference** to further boost the performance.\r\n\r\n## Demo\r\n\r\nNOTE: need to unmute first.\r\n\r\nhttps://github.com/user-attachments/assets/03bdde05-9514-4748-b527-003bea57f118\r\n\r\n\r\n## Install\r\n\r\nCreate a new conda environment and install the required packages:\r\n\r\n```sh\r\nconda create -n omni python=3.10\r\nconda activate omni\r\n\r\ngit clone https://github.com/gpt-omni/mini-omni.git\r\ncd mini-omni\r\npip install -r requirements.txt\r\n```\r\n\r\n## Quick start\r\n\r\n**Interactive demo**\r\n\r\n- start server\r\n\r\nNOTE: you need to start the server before running the streamlit or gradio demo with API_URL set to the server address.\r\n\r\n```sh\r\nsudo apt-get install ffmpeg\r\nconda activate omni\r\ncd mini-omni\r\npython3 server.py --ip '0.0.0.0' --port 60808\r\n```\r\n\r\n\r\n- run streamlit demo\r\n\r\nNOTE: you need to run streamlit **locally** with PyAudio installed. For error: `ModuleNotFoundError: No module named 'utils.vad'`, please run `export PYTHONPATH=./` first.\r\n\r\n```sh\r\npip install PyAudio==0.2.14\r\nAPI_URL=http://0.0.0.0:60808/chat streamlit run webui/omni_streamlit.py\r\n```\r\n\r\n- run gradio demo\r\n```sh\r\nAPI_URL=http://0.0.0.0:60808/chat python3 webui/omni_gradio.py\r\n```\r\n\r\nexample:\r\n\r\nNOTE: need to unmute first. Gradio seems can not play audio stream instantly, so the latency feels a bit longer.\r\n\r\nhttps://github.com/user-attachments/assets/29187680-4c42-47ff-b352-f0ea333496d9\r\n\r\n\r\n**Local test**\r\n\r\n```sh\r\nconda activate omni\r\ncd mini-omni\r\n# test run the preset audio samples and questions\r\npython inference.py\r\n```\r\n\r\n## FAQ\r\n\r\n**1. Does the model support other languages?**\r\n\r\nNo, the model is only trained on English. However, as we use whisper as the audio encoder, the model can understand other languages which is supported by whisper (like chinese), but the output is only in English.\r\n\r\n**2. What is `post_adapter` in the code? does the open-source version support tts-adapter?**\r\n\r\nThe `post_adapter` is `tts-adapter` in the model.py, but the open-source version does not support `tts-adapter`.\r\n\r\n**3. Error: `ModuleNotFoundError: No module named 'utils.xxxx'`**\r\n\r\nRun `export PYTHONPATH=./` first. No need to run `pip install utils`, or just try: `pip uninstall utils`\r\n\r\n**4. Error: can not run streamlit in local browser, with remote streamlit server**, issue: https://github.com/gpt-omni/mini-omni/issues/37\r\n    \r\nYou need start streamlit **locally** with PyAudio installed.\r\n\r\n\r\n## Acknowledgements \r\n\r\n- [Qwen2](https://github.com/QwenLM/Qwen2/) as the LLM backbone.\r\n- [litGPT](https://github.com/Lightning-AI/litgpt/) for training and inference.\r\n- [whisper](https://github.com/openai/whisper/)  for audio encoding.\r\n- [snac](https://github.com/hubertsiuzdak/snac/)  for audio decoding.\r\n- [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) for generating synthetic speech.\r\n- [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) and [MOSS](https://github.com/OpenMOSS/MOSS/tree/main) for alignment.\r\n\r\n## Star History\r\n\r\n[![Star History Chart](https://api.star-history.com/svg?repos=gpt-omni/mini-omni&type=Date)](https://star-history.com/#gpt-omni/mini-omni&Date)\r\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0.0791015625,
          "content": "import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 23.203125,
          "content": "import os\nimport lightning as L\nimport torch\nimport time\nfrom snac import SNAC\nfrom litgpt import Tokenizer\nfrom litgpt.utils import (\n    num_parameters,\n)\nfrom litgpt.generate.base import (\n    generate_AA,\n    generate_ASR,\n    generate_TA,\n    generate_TT,\n    generate_AT,\n    generate_TA_BATCH,\n    next_token_batch\n)\nimport soundfile as sf\nfrom litgpt.model import GPT, Config\nfrom lightning.fabric.utilities.load import _lazy_load as lazy_load\nfrom utils.snac_utils import layershift, reconscruct_snac, reconstruct_tensors, get_time_str\nfrom utils.snac_utils import get_snac, generate_audio_data\nimport whisper\nfrom tqdm import tqdm\nfrom huggingface_hub import snapshot_download\n\n\ntorch.set_printoptions(sci_mode=False)\n\n\n# TODO\ntext_vocabsize = 151936\ntext_specialtokens = 64\naudio_vocabsize = 4096\naudio_specialtokens = 64\n\npadded_text_vocabsize = text_vocabsize + text_specialtokens\npadded_audio_vocabsize = audio_vocabsize + audio_specialtokens\n\n_eot = text_vocabsize\n_pad_t = text_vocabsize + 1\n_input_t = text_vocabsize + 2\n_answer_t = text_vocabsize + 3\n_asr = text_vocabsize + 4\n\n_eoa = audio_vocabsize\n_pad_a = audio_vocabsize + 1\n_input_a = audio_vocabsize + 2\n_answer_a = audio_vocabsize + 3\n_split = audio_vocabsize + 4\n\n\ndef get_input_ids_TA(text, text_tokenizer):\n    input_ids_item = [[] for _ in range(8)]\n    text_tokens = text_tokenizer.encode(text)\n    for i in range(7):\n        input_ids_item[i] = [layershift(_pad_a, i)] * (len(text_tokens) + 2) + [\n            layershift(_answer_a, i)\n        ]\n        input_ids_item[i] = torch.tensor(input_ids_item[i]).unsqueeze(0)\n    input_ids_item[-1] = [_input_t] + text_tokens.tolist() + [_eot] + [_answer_t]\n    input_ids_item[-1] = torch.tensor(input_ids_item[-1]).unsqueeze(0)\n    return input_ids_item\n\n\ndef get_input_ids_TT(text, text_tokenizer):\n    input_ids_item = [[] for i in range(8)]\n    text_tokens = text_tokenizer.encode(text).tolist()\n\n    for i in range(7):\n        input_ids_item[i] = torch.tensor(\n            [layershift(_pad_a, i)] * (len(text_tokens) + 3)\n        ).unsqueeze(0)\n    input_ids_item[-1] = [_input_t] + text_tokens + [_eot] + [_answer_t]\n    input_ids_item[-1] = torch.tensor(input_ids_item[-1]).unsqueeze(0)\n\n    return input_ids_item\n\n\ndef get_input_ids_whisper(\n    mel, leng, whispermodel, device, \n    special_token_a=_answer_a, special_token_t=_answer_t,\n):\n\n    with torch.no_grad():\n        mel = mel.unsqueeze(0).to(device)\n        # audio_feature = whisper.decode(whispermodel,mel, options).audio_features\n        audio_feature = whispermodel.embed_audio(mel)[0][:leng]\n\n    T = audio_feature.size(0)\n    input_ids = []\n    for i in range(7):\n        input_ids_item = []\n        input_ids_item.append(layershift(_input_a, i))\n        input_ids_item += [layershift(_pad_a, i)] * T\n        input_ids_item += [(layershift(_eoa, i)), layershift(special_token_a, i)]\n        input_ids.append(torch.tensor(input_ids_item).unsqueeze(0))\n    input_id_T = torch.tensor([_input_t] + [_pad_t] * T + [_eot, special_token_t])\n    input_ids.append(input_id_T.unsqueeze(0))\n    return audio_feature.unsqueeze(0), input_ids\n\n\ndef get_input_ids_whisper_ATBatch(mel, leng, whispermodel, device):\n    with torch.no_grad():\n        mel = mel.unsqueeze(0).to(device)\n        # audio_feature = whisper.decode(whispermodel,mel, options).audio_features\n        audio_feature = whispermodel.embed_audio(mel)[0][:leng]\n    T = audio_feature.size(0)\n    input_ids_AA = []\n    for i in range(7):\n        input_ids_item = []\n        input_ids_item.append(layershift(_input_a, i))\n        input_ids_item += [layershift(_pad_a, i)] * T\n        input_ids_item += [(layershift(_eoa, i)), layershift(_answer_a, i)]\n        input_ids_AA.append(torch.tensor(input_ids_item))\n    input_id_T = torch.tensor([_input_t] + [_pad_t] * T + [_eot, _answer_t])\n    input_ids_AA.append(input_id_T)\n\n    input_ids_AT = []\n    for i in range(7):\n        input_ids_item = []\n        input_ids_item.append(layershift(_input_a, i))\n        input_ids_item += [layershift(_pad_a, i)] * T\n        input_ids_item += [(layershift(_eoa, i)), layershift(_pad_a, i)]\n        input_ids_AT.append(torch.tensor(input_ids_item))\n    input_id_T = torch.tensor([_input_t] + [_pad_t] * T + [_eot, _answer_t])\n    input_ids_AT.append(input_id_T)\n\n    input_ids = [input_ids_AA, input_ids_AT]\n    stacked_inputids = [[] for _ in range(8)]\n    for i in range(2):\n        for j in range(8):\n            stacked_inputids[j].append(input_ids[i][j])\n    stacked_inputids = [torch.stack(tensors) for tensors in stacked_inputids]\n    return torch.stack([audio_feature, audio_feature]), stacked_inputids\n\n\ndef load_audio(path):\n    audio = whisper.load_audio(path)\n    duration_ms = (len(audio) / 16000) * 1000\n    audio = whisper.pad_or_trim(audio)\n    mel = whisper.log_mel_spectrogram(audio)\n    return mel, int(duration_ms / 20) + 1\n\n\ndef A1_A2_batch(fabric, audio_feature, input_ids, leng, model, text_tokenizer, step,\n                snacmodel, out_dir=None):\n    with fabric.init_tensor():\n        model.set_kv_cache(batch_size=2)\n    tokenlist = generate_TA_BATCH(\n        model,\n        audio_feature,\n        input_ids,\n        [leng, leng],\n        [\"A1A2\", \"A1T2\"],\n        max_returned_tokens=2048,\n        temperature=0.9,\n        top_k=1,\n        eos_id_a=_eoa,\n        eos_id_t=_eot,\n        pad_id_t=_pad_t,\n        shift=padded_text_vocabsize,\n        include_prompt=True,\n        generate_text=True,\n    )\n    text_tokenlist = tokenlist[-1]\n    if text_vocabsize in text_tokenlist:\n        text_tokenlist = text_tokenlist[: text_tokenlist.index(text_vocabsize)]\n    text = text_tokenizer.decode(torch.tensor(text_tokenlist)).strip()\n\n    audio_tokenlist = tokenlist[:-1]\n    audiolist = reconscruct_snac(audio_tokenlist)\n    audio = reconstruct_tensors(audiolist)\n    if out_dir is None:\n        out_dir = \"./output/default/A1-A2-batch\"\n    else:\n        out_dir = out_dir + \"/A1-A2-batch\"\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    with torch.inference_mode():\n        audio_hat = snacmodel.decode(audio)\n    sf.write(\n        f\"{out_dir}/{step:02d}.wav\",\n        audio_hat.squeeze().cpu().numpy(),\n        24000,\n    )\n    model.clear_kv_cache()\n    return text\n\n\ndef A1_T2(fabric, audio_feature, input_ids, leng, model, text_tokenizer, step):\n    with fabric.init_tensor():\n        model.set_kv_cache(batch_size=1)\n    tokenlist = generate_AT(\n        model,\n        audio_feature,\n        input_ids,\n        [leng],\n        [\"AT\"],\n        max_returned_tokens=2048,\n        temperature=0.9,\n        top_k=1,\n        eos_id_a=_eoa,\n        eos_id_t=_eot,\n        pad_id_t=_pad_t,\n        shift=padded_text_vocabsize,\n        include_prompt=True,\n        generate_text=True,\n    )\n    return text_tokenizer.decode(torch.tensor(tokenlist)).strip()\n\n\ndef A1_A2(fabric, audio_feature, input_ids, leng, model, text_tokenizer, step,\n          snacmodel, out_dir=None):\n    with fabric.init_tensor():\n        model.set_kv_cache(batch_size=1)\n    tokenlist = generate_AA(\n        model,\n        audio_feature,\n        input_ids,\n        [leng],\n        [\"A1T2\"],\n        max_returned_tokens=2048,\n        temperature=0.9,\n        top_k=1,\n        eos_id_a=_eoa,\n        eos_id_t=_eot,\n        pad_id_t=_pad_t,\n        shift=padded_text_vocabsize,\n        include_prompt=True,\n        generate_text=True,\n    )\n    audiolist = reconscruct_snac(tokenlist)\n    tokenlist = tokenlist[-1]\n    if text_vocabsize in tokenlist:\n        tokenlist = tokenlist[: tokenlist.index(text_vocabsize)]\n    if out_dir is None:\n        out_dir = \"./output/default/A1-A2\"\n    else:\n        out_dir = out_dir + \"/A1-A2\"\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n        \n    audio = reconstruct_tensors(audiolist)\n    with torch.inference_mode():\n        audio_hat = snacmodel.decode(audio)\n    sf.write(\n        f\"{out_dir}/{step:02d}.wav\",\n        audio_hat.squeeze().cpu().numpy(),\n        24000,\n    )\n    model.clear_kv_cache()\n    return text_tokenizer.decode(torch.tensor(tokenlist)).strip()\n\n\ndef A1_T1(fabric, audio_feature, input_ids, leng, model, text_tokenizer, step):\n    with fabric.init_tensor():\n        model.set_kv_cache(batch_size=1)\n    tokenlist = generate_ASR(\n        model,\n        audio_feature,\n        input_ids,\n        [leng],\n        [\"A1T1\"],\n        max_returned_tokens=2048,\n        temperature=0.9,\n        top_k=1,\n        eos_id_a=_eoa,\n        eos_id_t=_eot,\n        pad_id_t=_pad_t,\n        shift=padded_text_vocabsize,\n        include_prompt=True,\n        generate_text=True,\n    )\n    model.clear_kv_cache()\n    return text_tokenizer.decode(torch.tensor(tokenlist)).strip()\n\n\ndef T1_A2(fabric, input_ids, model, text_tokenizer, step,\n          snacmodel, out_dir=None):\n    with fabric.init_tensor():\n        model.set_kv_cache(batch_size=1)\n    tokenlist = generate_TA(\n        model,\n        None,\n        input_ids,\n        None,\n        [\"T1A2\"],\n        max_returned_tokens=2048,\n        temperature=0.9,\n        top_k=1,\n        eos_id_a=_eoa,\n        eos_id_t=_eot,\n        pad_id_t=_pad_t,\n        shift=padded_text_vocabsize,\n        include_prompt=True,\n        generate_text=True,\n    )\n\n    audiolist = reconscruct_snac(tokenlist)\n    tokenlist = tokenlist[-1]\n\n    if text_vocabsize in tokenlist:\n        tokenlist = tokenlist[: tokenlist.index(text_vocabsize)]\n    audio = reconstruct_tensors(audiolist)\n    if out_dir is None:\n        out_dir = \"./output/default/T1-A2\"\n    else:\n        out_dir = out_dir + \"/T1-A2\"\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    with torch.inference_mode():\n        audio_hat = snacmodel.decode(audio)\n    sf.write(\n        f\"{out_dir}/{step:02d}.wav\",\n        audio_hat.squeeze().cpu().numpy(),\n        24000,\n    )\n    model.clear_kv_cache()\n    return text_tokenizer.decode(torch.tensor(tokenlist)).strip()\n\n\ndef T1_T2(fabric, input_ids, model, text_tokenizer, step):\n\n    with fabric.init_tensor():\n        model.set_kv_cache(batch_size=1)\n    tokenlist = generate_TT(\n        model,\n        None,\n        input_ids,\n        None,\n        [\"T1T2\"],\n        max_returned_tokens=2048,\n        temperature=0.9,\n        top_k=1,\n        eos_id_a=_eoa,\n        eos_id_t=_eot,\n        pad_id_t=_pad_t,\n        shift=padded_text_vocabsize,\n        include_prompt=True,\n        generate_text=True,\n    )\n    model.clear_kv_cache()\n    return text_tokenizer.decode(torch.tensor(tokenlist)).strip()\n\n    \ndef load_model(ckpt_dir, device):\n    snacmodel = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").eval().to(device)\n    whispermodel = whisper.load_model(\"small\").to(device)\n    text_tokenizer = Tokenizer(ckpt_dir)\n    fabric = L.Fabric(devices=1, strategy=\"auto\")\n    config = Config.from_file(ckpt_dir + \"/model_config.yaml\")\n    config.post_adapter = False\n\n    with fabric.init_module(empty_init=False):\n        model = GPT(config)\n\n    model = fabric.setup(model)\n    state_dict = lazy_load(ckpt_dir + \"/lit_model.pth\")\n    model.load_state_dict(state_dict, strict=True)\n    model.to(device).eval()\n\n    return fabric, model, text_tokenizer, snacmodel, whispermodel\n\n    \ndef download_model(ckpt_dir):\n    repo_id = \"gpt-omni/mini-omni\"\n    snapshot_download(repo_id, local_dir=ckpt_dir, revision=\"main\")\n\n    \nclass OmniInference:\n\n    def __init__(self, ckpt_dir='./checkpoint', device='cuda:0'):\n        self.device = device\n        if not os.path.exists(ckpt_dir):\n            print(f\"checkpoint directory {ckpt_dir} not found, downloading from huggingface\")\n            download_model(ckpt_dir)\n        self.fabric, self.model, self.text_tokenizer, self.snacmodel, self.whispermodel = load_model(ckpt_dir, device)\n\n    def warm_up(self, sample='./data/samples/output1.wav'):\n        for _ in self.run_AT_batch_stream(sample):\n            pass\n\n    @torch.inference_mode()\n    def run_AT_batch_stream(self, \n                            audio_path, \n                            stream_stride=4,\n                            max_returned_tokens=2048, \n                            temperature=0.9, \n                            top_k=1, \n                            top_p=1.0,\n                            eos_id_a=_eoa,\n                            eos_id_t=_eot,\n        ):\n\n        assert os.path.exists(audio_path), f\"audio file {audio_path} not found\"\n        model = self.model\n\n        with self.fabric.init_tensor():\n            model.set_kv_cache(batch_size=2,device=self.device)\n\n        mel, leng = load_audio(audio_path)\n        audio_feature, input_ids = get_input_ids_whisper_ATBatch(mel, leng, self.whispermodel, self.device)\n        T = input_ids[0].size(1)\n        device = input_ids[0].device\n\n        assert max_returned_tokens > T, f\"max_returned_tokens {max_returned_tokens} should be greater than audio length {T}\"\n\n        if model.max_seq_length < max_returned_tokens - 1:\n            raise NotImplementedError(\n                f\"max_seq_length {model.max_seq_length} needs to be >= {max_returned_tokens - 1}\"\n            )\n\n        input_pos = torch.tensor([T], device=device)\n        list_output = [[] for i in range(8)]\n        tokens_A, token_T = next_token_batch(\n            model,\n            audio_feature.to(torch.float32).to(model.device),\n            input_ids,\n            [T - 3, T - 3],\n            [\"A1T2\", \"A1T2\"],\n            input_pos=torch.arange(0, T, device=device),\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n        )\n\n        for i in range(7):\n            list_output[i].append(tokens_A[i].tolist()[0])\n        list_output[7].append(token_T.tolist()[0])\n\n        model_input_ids = [[] for i in range(8)]\n        for i in range(7):\n            tokens_A[i] = tokens_A[i].clone() + padded_text_vocabsize + i * padded_audio_vocabsize\n            model_input_ids[i].append(tokens_A[i].clone().to(device).to(torch.int32))\n            model_input_ids[i].append(torch.tensor([layershift(4097, i)], device=device))\n            model_input_ids[i] = torch.stack(model_input_ids[i])\n\n        model_input_ids[-1].append(token_T.clone().to(torch.int32))\n        model_input_ids[-1].append(token_T.clone().to(torch.int32))\n        model_input_ids[-1] = torch.stack(model_input_ids[-1])\n\n        text_end = False\n        index = 1\n        nums_generate = stream_stride\n        begin_generate = False\n        current_index = 0\n        for _ in tqdm(range(2, max_returned_tokens - T + 1)):\n            tokens_A, token_T = next_token_batch(\n                model,\n                None,\n                model_input_ids,\n                None,\n                None,\n                input_pos=input_pos,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n            )\n\n            if text_end:\n                token_T = torch.tensor([_pad_t], device=device)\n\n            if tokens_A[-1] == eos_id_a:\n                break\n\n            if token_T == eos_id_t:\n                text_end = True\n\n            for i in range(7):\n                list_output[i].append(tokens_A[i].tolist()[0])\n            list_output[7].append(token_T.tolist()[0])\n\n            model_input_ids = [[] for i in range(8)]\n            for i in range(7):\n                tokens_A[i] = tokens_A[i].clone() +padded_text_vocabsize + i * padded_audio_vocabsize\n                model_input_ids[i].append(tokens_A[i].clone().to(device).to(torch.int32))\n                model_input_ids[i].append(\n                    torch.tensor([layershift(4097, i)], device=device)\n                )\n                model_input_ids[i] = torch.stack(model_input_ids[i])\n\n            model_input_ids[-1].append(token_T.clone().to(torch.int32))\n            model_input_ids[-1].append(token_T.clone().to(torch.int32))\n            model_input_ids[-1] = torch.stack(model_input_ids[-1])\n\n            if index == 7:\n                begin_generate = True\n\n            if begin_generate:\n                current_index += 1\n                if current_index == nums_generate:\n                    current_index = 0\n                    snac = get_snac(list_output, index, nums_generate)\n                    audio_stream = generate_audio_data(snac, self.snacmodel, self.device)\n                    yield audio_stream\n\n            input_pos = input_pos.add_(1)\n            index += 1\n        text = self.text_tokenizer.decode(torch.tensor(list_output[-1]))\n        print(f\"text output: {text}\")\n        model.clear_kv_cache()\n        return list_output\n\n\ndef test_infer():\n    device = \"cuda:0\"\n    out_dir = f\"./output/{get_time_str()}\"\n    ckpt_dir = f\"./checkpoint\"\n    if not os.path.exists(ckpt_dir):\n        print(f\"checkpoint directory {ckpt_dir} not found, downloading from huggingface\")\n        download_model(ckpt_dir)\n\n    fabric, model, text_tokenizer, snacmodel, whispermodel = load_model(ckpt_dir, device)\n\n    task = ['A1A2', 'asr', \"T1A2\", \"AA-BATCH\", 'T1T2', 'AT']\n\n    # prepare test data\n    # TODO\n    test_audio_list = sorted(os.listdir('./data/samples'))\n    test_audio_list = [os.path.join('./data/samples', path) for path in test_audio_list]\n    test_audio_transcripts = [\n        \"What is your name?\",\n        \"what are your hobbies?\",\n        \"Do you like beijing\",\n        \"How are you feeling today?\",\n        \"what is the weather like today?\",\n    ]\n    test_text_list = [\n        \"What is your name?\",\n        \"How are you feeling today?\",\n        \"Can you describe your surroundings?\",\n        \"What did you do yesterday?\",\n        \"What is your favorite book and why?\",\n        \"How do you make a cup of tea?\",\n        \"What is the weather like today?\",\n        \"Can you explain the concept of time?\",\n        \"Can you tell me a joke?\",\n    ]\n\n    # LOAD MODEL\n    with torch.no_grad():\n        if \"A1A2\" in task:\n            print(\"===============================================================\")\n            print(\"                       testing A1A2\")\n            print(\"===============================================================\")\n            step = 0\n            for path in test_audio_list:\n                try:\n                    mel, leng = load_audio(path)\n                    audio_feature, input_ids = get_input_ids_whisper(mel, leng, whispermodel, device)\n                    text = A1_A2(\n                        fabric,\n                        audio_feature,\n                        input_ids,\n                        leng,\n                        model,\n                        text_tokenizer,\n                        step,\n                        snacmodel,\n                        out_dir=out_dir,\n                    )\n                    print(f\"input: {test_audio_transcripts[step]}\")\n                    print(f\"output: {text}\")\n                    step += 1\n                    print(\n                        \"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n                    )\n                except:\n                    print(f\"[error] failed to process {path}\")\n            print(\"===============================================================\")\n\n        if 'asr' in task:\n            print(\"===============================================================\")\n            print(\"                       testing asr\")\n            print(\"===============================================================\")\n\n            index = 0\n            step = 0\n            for path in test_audio_list:\n                mel, leng = load_audio(path)\n                audio_feature, input_ids = get_input_ids_whisper(mel, leng, whispermodel, device, special_token_a=_pad_a, special_token_t=_asr)\n                output = A1_T1(fabric, audio_feature, input_ids ,leng, model, text_tokenizer, index).lower().replace(',','').replace('.','').replace('?','')\n                print(f\"audio_path: {path}\")\n                print(f\"audio transcript: {test_audio_transcripts[index]}\")\n                print(f\"asr output: {output}\")\n                print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n                index += 1\n\n        if \"T1A2\" in task:\n            step = 0\n            print(\"\\n\")\n            print(\"===============================================================\")\n            print(\"                       testing T1A2\")\n            print(\"===============================================================\")\n            for text in test_text_list:\n                input_ids = get_input_ids_TA(text, text_tokenizer)\n                text_output = T1_A2(fabric, input_ids, model, text_tokenizer, step,\n                                    snacmodel, out_dir=out_dir)\n                print(f\"input: {text}\")\n                print(f\"output: {text_output}\")\n                print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n                step += 1\n            print(\"===============================================================\")\n\n        if \"T1T2\" in task:\n            step = 0\n            print(\"\\n\")\n            print(\"===============================================================\")\n            print(\"                       testing T1T2\")\n            print(\"===============================================================\")\n\n            for text in test_text_list:\n                input_ids = get_input_ids_TT(text, text_tokenizer)\n                text_output = T1_T2(fabric, input_ids, model, text_tokenizer, step)\n                print(f\" Input: {text}\")\n                print(f\"Output: {text_output}\")\n                print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n            print(\"===============================================================\")\n\n        if \"AT\" in task:\n            print(\"===============================================================\")\n            print(\"                       testing A1T2\")\n            print(\"===============================================================\")\n            step = 0\n            for path in test_audio_list:\n                mel, leng = load_audio(path)\n                audio_feature, input_ids = get_input_ids_whisper(\n                    mel, leng, whispermodel, device, \n                    special_token_a=_pad_a, special_token_t=_answer_t\n                )\n                text = A1_T2(\n                    fabric, audio_feature, input_ids, leng, model, text_tokenizer, step\n                )\n                print(f\"input: {test_audio_transcripts[step]}\")\n                print(f\"output: {text}\")\n                step += 1\n                print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n            print(\"===============================================================\")\n\n        if \"AA-BATCH\" in task:\n            print(\"===============================================================\")\n            print(\"                       testing A1A2-BATCH\")\n            print(\"===============================================================\")\n            step = 0\n            for path in test_audio_list:\n                mel, leng = load_audio(path)\n                audio_feature, input_ids = get_input_ids_whisper_ATBatch(mel, leng, whispermodel, device)\n                text = A1_A2_batch(\n                    fabric, audio_feature, input_ids, leng, model, text_tokenizer, step,\n                    snacmodel, out_dir=out_dir\n                )\n                print(f\"input: {test_audio_transcripts[step]}\")\n                print(f\"output: {text}\")\n                step += 1\n                print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n            print(\"===============================================================\")\n\n        print(\"*********************** test end *****************************\")\n\n\n\nif __name__ == \"__main__\":\n    test_infer()\n"
        },
        {
          "name": "litgpt",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2646484375,
          "content": "torch==2.3.1\ntorchvision==0.18.1\ntorchaudio==2.3.1\nlitgpt==0.4.3\nsnac==1.2.0\nsoundfile==0.12.1\nopenai-whisper\ntokenizers==0.19.1 \nstreamlit==1.37.1\n# PyAudio==0.2.14\npydub==0.25.1\nonnxruntime==1.19.0\n# numpy==1.26.3\ngradio==4.42.0\nlibrosa==0.10.2.post1\nflask==3.0.3\nfire\n"
        },
        {
          "name": "server.py",
          "type": "blob",
          "size": 1.798828125,
          "content": "import sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\n\nfrom inference import OmniInference\nimport flask\nimport base64\nimport tempfile\nimport traceback\nfrom flask import Flask, Response, stream_with_context\n\n\nclass OmniChatServer(object):\n    def __init__(self, ip='0.0.0.0', port=60808, run_app=True,\n                 ckpt_dir='./checkpoint', device='cuda:0') -> None:\n        server = Flask(__name__)\n        # CORS(server, resources=r\"/*\")\n        # server.config[\"JSON_AS_ASCII\"] = False\n\n        self.client = OmniInference(ckpt_dir, device)\n        self.client.warm_up()\n\n        server.route(\"/chat\", methods=[\"POST\"])(self.chat)\n\n        if run_app:\n            server.run(host=ip, port=port, threaded=False)\n        else:\n            self.server = server\n\n    def chat(self) -> Response:\n\n        req_data = flask.request.get_json()\n        try:\n            data_buf = req_data[\"audio\"].encode(\"utf-8\")\n            data_buf = base64.b64decode(data_buf)\n            stream_stride = req_data.get(\"stream_stride\", 4)\n            max_tokens = req_data.get(\"max_tokens\", 2048)\n\n            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n                f.write(data_buf)\n                audio_generator = self.client.run_AT_batch_stream(f.name, stream_stride, max_tokens)\n                return Response(stream_with_context(audio_generator), mimetype=\"audio/wav\")\n        except Exception as e:\n            print(traceback.format_exc())\n\n\n# CUDA_VISIBLE_DEVICES=1 gunicorn -w 2 -b 0.0.0.0:60808 'server:create_app()'\ndef create_app():\n    server = OmniChatServer(run_app=False)\n    return server.server\n\n\ndef serve(ip='0.0.0.0', port=60808, device='cuda:0'):\n\n    OmniChatServer(ip, port=port,run_app=True, device=device)\n\n\nif __name__ == \"__main__\":\n    import fire\n    fire.Fire(serve)\n\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "webui",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}