{
  "metadata": {
    "timestamp": 1736559673419,
    "page": 342,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "layumi/Person_reID_baseline_pytorch",
      "stars": 4167,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.611328125,
          "content": "# Don't track content of these folders\noutputs/\nmodel/*\nlogs/\n__pycache__/\ncore\nGPU-Re-Ranking/extension/adjacency_matrix/build_adjacency_matrix.egg-info/\nGPU-Re-Ranking/extension/propagation/gnn_propagate.egg-info/\nGPU-Re-Ranking/extension/adjacency_matrix/dist\nGPU-Re-Ranking/extension/propagation/dist\n\n# But not \n!model/.gitkeep\n\n# Compiled source #\n###################\n*.com\n*.class\n*.dll\n*.exe\n*.o\n*.so\n*.pyc\n\n# Packages #\n############\n# it's better to unpack these files and commit the raw source\n# git has its own built in compression methods\n*.7z\n*.dmg\n*.gz\n*.zip\n*.iso\n*.jar\n*.rar\n*.tar\n*.mat\n*.jpg\n*.npy\n*.pt\n*.pth\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.6650390625,
          "content": "language: python\n\nmatrix:\n  include:\n    - python: \"3.6\"\n      env: IMAGE_BACKEND=Pillow-SIMD\n    - python: \"3.6\"\n\ninstall:\n  - sudo apt-get update\n  - wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;\n  - bash miniconda.sh -b -p $HOME/miniconda\n  - export PATH=\"$HOME/miniconda/bin:$PATH\"\n  - hash -r\n  - conda config --set always_yes yes --set changeps1 no\n  - conda update -q conda\n  # Useful for debugging any issues with conda\n  - conda info -a\n  - conda install numpy \n  - conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n  - conda install -c conda-forge pretrainedmodels\n\nscript:\n  # Simple Forward\n  - python model.py\n"
        },
        {
          "name": "DDP.sh",
          "type": "blob",
          "size": 0.0869140625,
          "content": "python -m torch.distributed.launch --nproc_per_node=2  --master_port=6005 train_DDP.py \n\n"
        },
        {
          "name": "GPU-Re-Ranking",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.044921875,
          "content": "MIT License\n\nCopyright (c) 2018 Zhedong Zheng\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "ODFA.py",
          "type": "blob",
          "size": 3.271484375,
          "content": "import torch\nfrom torch.autograd import Variable\nfrom copy import deepcopy\nimport torch.nn as nn\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n#   Online Adversarial Defense Trainnig via ODFA.\n#   https://github.com/layumi/U_turn/blob/master/README.md\ndef ODFA(model, img, rate = 16):\n            model = deepcopy(model)\n            model.eval()\n            model.classifier.return_f = False\n            n, c, h, w = img.size()\n            inputs = Variable(img.cuda(), requires_grad=True)\n            # ---------------------attack------------------\n            # The input has been whiten.\n            # So when we recover, we need to use a alpha\n            alpha = 1.0 / (0.226 * 255.0)\n            inputs_copy = Variable(inputs.data, requires_grad = False)\n            diff = torch.FloatTensor(inputs.shape).zero_()\n            diff = Variable(diff.cuda(), requires_grad = False)\n\n            model.model.fc = nn.Sequential() #nn.Sequential(*L2norm)\n            model.classifier.classifier = nn.Sequential()\n            #model.classifier = nn.Sequential() PCB\n            outputs = model(inputs)\n            fnorm = torch.norm(outputs, p=2, dim=1, keepdim=True)\n            outputs = outputs.div(fnorm.expand_as(outputs))\n            outputs = outputs.view(outputs.size(0), -1)\n            #print(outputs.shape)\n            #feature_dim = outputs.shape[1]\n            #batch_size = inputs.shape[0]\n            #zero_feature = torch.zeros(batch_size,feature_dim)\n            target = Variable(-outputs.data, requires_grad=False)\n            criterion2 = nn.MSELoss()\n            max_iter = round(min(1.25 * rate, rate+4))\n            for iter in range( max_iter ):\n                loss2 = criterion2(outputs, target)\n                loss2.backward()\n                diff += torch.sign(inputs.grad)\n                mask_diff = diff.abs() > rate\n                diff[mask_diff] = rate * torch.sign(diff[mask_diff])\n                inputs = inputs_copy - diff * 1.0 * alpha\n                inputs = clip(inputs,n)\n                inputs = Variable(inputs.data, requires_grad=True)\n                if iter == max_iter-1: break\n                outputs = model(inputs)\n                fnorm = torch.norm(outputs, p=2, dim=1, keepdim=True)\n                outputs = outputs.div(fnorm.expand_as(outputs))\n                outputs = outputs.view(outputs.size(0), -1)\n\n            return inputs.detach()\n\n\n\ndef clip(inputs, batch_size):\n    inputs = inputs.data\n    for i in range(batch_size):\n        inputs[i] = clip_single(inputs[i])\n    inputs = Variable(inputs.cuda())\n    return inputs\n\n#######################################################################\n# Creat Up bound and low bound\n# Clip\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nzeros = np.zeros((256,128,3),dtype=np.uint8)\nzeros = Image.fromarray(zeros)\nzeros = data_transforms(zeros)\n\nones = 255*np.ones((256,128,3), dtype=np.uint8)\nones = Image.fromarray(ones)\nones = data_transforms(ones)\n\nzeros,ones = zeros.cuda(),ones.cuda()\ndef clip_single(input):\n    low_mask = input<zeros\n    up_mask = input>ones\n    input[low_mask] = zeros[low_mask]\n    input[up_mask] = ones[up_mask]\n    return input\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 27.84765625,
          "content": "<h1 align=\"center\"> Pytorch ReID </h1>\n<h2 align=\"center\"> Strong, Small, Friendly </h2>\n\n![Python3.6+](https://img.shields.io/badge/python-3.6+-green.svg)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n\nA tiny, friendly, strong baseline code for Object-reID (based on [pytorch](https://pytorch.org)) since 2017.\n\n- **Strong.** It is consistent with the new baseline result in several top-conference works, e.g., [Joint Discriminative and Generative Learning for Person Re-identification(CVPR19)](https://arxiv.org/abs/1904.07223), [Beyond Part Models: Person Retrieval with Refined Part Pooling(ECCV18)](https://arxiv.org/abs/1711.09349), [Camera Style Adaptation for Person Re-identification(CVPR18)](https://arxiv.org/abs/1711.10295). We arrived Rank@1=88.24%, mAP=70.68% only with softmax loss. \n\n- **Small.** With fp16 (supported by Nvidia apex), our baseline could be trained with only 2GB GPU memory.\n\n- **Friendly.** You may use the off-the-shelf options to apply many state-of-the-art tricks in one line.\nBesides, if you are new to object re-ID, you may check out our **[Tutorial](https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/tutorial)** first (8 min read) :+1: .\n![Person ReID Demo](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/show.png)\n\nShare to\n    <!-- Facebook -->\n    <a href=\"http://www.facebook.com/sharer.php?u=https://github.com/layumi/Person_reID_baseline_pytorch/\" target=\"_blank\">\n        <img src=\"https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/1_Facebook_colored_svg_copy-64.webp\" alt=\"Facebook\" width=\"60\"/>\n    </a>\n    <!-- Twitter -->\n    <a href=\"https://twitter.com/share?url=https://github.com/layumi/Person_reID_baseline_pytorch/&amp;text=Strong,%20Small,%20Friendly%20Pytorch%20re-ID&amp;hashtags=pytorch re-ID\" target=\"_blank\">\n        <img src=\"https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/2018_social_media_popular_app_logo_twitter-64.webp\" alt=\"Twitter\" width=\"60\"/>\n    </a>\n    <!-- Weibo -->\n    <a href=\"https://service.weibo.com/share/share.php?url=https://github.com/layumi/Person_reID_baseline_pytorch/&amp;text=Strong,%20Small,%20Friendly%20Pytorch%20re-ID&amp;hashtags=pytorch re-ID\" target=\"_blank\">\n        <img src=\"https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/Weibo-64.webp\" alt=\"Weibo\" width=\"60\" />\n    </a>\n    <!-- LinkedIn -->\n    <a href=\"http://www.linkedin.com/shareArticle?mini=true&amp;url=https://github.com/layumi/Person_reID_baseline_pytorch/\" target=\"_blank\">\n        <img src=\"https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/circle-linkedin-64.webp\" alt=\"LinkedIn\" width=\"60\" />\n    </a>\n    <!-- Email -->\n    <a href=\"mailto:?Subject=Strong, Small, Friendly Pytorch Re-identification&amp;Body=I%20saw%20this%20and%20thought%20of%20you!%20 https://github.com/layumi/Person_reID_baseline_pytorch/\">\n        <img src=\"https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/Email-64.webp\" alt=\"Email\" width=\"60\"/>\n    </a>\n        \n\n## Tutorial\n* [8 min Tutorial](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/tutorial/README.md)，[8分钟教程](https://zhuanlan.zhihu.com/p/50387521)\n* [中文视频简介](https://www.bilibili.com/video/BV11K4y1f7eQ)\n* [Answer to Tutorial](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/tutorial/Answers_to_Quick_Questions.md)\n\n## Table of contents\n* [Features](#features)\n* [Some News](#some-news)\n* [Trained Model](#trained-model)\n* [Prerequisites](#prerequisites)\n* [Getting Started](#getting-started)\n    * [Installation](#installation)\n    * [Dataset Preparation](#dataset--preparation)\n    * [Train](#train)\n    * [Test](#test)\n    * [Evaluation](#evaluation)\n* [Tips for training with other datasets](#tips)\n* [How to Cite?](#citation)\n* [Related Repos](#related-repos)\n\n## Features\nNow we have supported:\n\n### Training \n- Running the code on Google Colab with Free GPU. Check [Here](https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/colab) (Thanks to @ronghao233)\n- [DG-Market](https://github.com/NVlabs/DG-Net#dg-market) (10x Large Synthetic Dataset from Market **CVPR 2019 Oral**)\n- [Swin Transformer](https://github.com/microsoft/Swin-Transformer) / [EfficientNet](https://github.com/lukemelas/EfficientNet-PyTorch) / [HRNet](https://github.com/HRNet)\n- ResNet/ResNet-ibn/DenseNet\n- Circle Loss, Triplet Loss, Contrastive Loss, Sphere Loss, Lifted Loss, Arcface, Cosface  and Instance Loss\n- Float16 to save GPU memory based on [apex](https://github.com/NVIDIA/apex)\n- Part-based Convolutional Baseline(PCB)\n- Random Erasing\n- Linear Warm-up \n- torch.compile (faster training)\n- DDP (Multiple GPUs)\n\n### Testing\n- TensorRT \n- Pytorch JIT \n- Fuse Conv and BN layer into one Conv layer\n- Multiple Query Evaluation\n- Re-Ranking (CPU & [GPU Version](https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/GPU-Re-Ranking))\n- Visualize Training Curves\n- Visualize Ranking Result\n- [Visualize Heatmap](https://github.com/layumi/Person_reID_baseline_pytorch/blob/dev/visual_heatmap.py)\n\n\nHere we provide hyperparameters and architectures, that were used to generate the result. \nSome of them (i.e. learning rate) are far from optimal. Do not hesitate to change them and see the effect. \n\nP.S. With similar structure, we arrived **Rank@1=87.74% mAP=69.46%** with [Matconvnet](http://www.vlfeat.org/matconvnet/). (batchsize=8, dropout=0.75) \nYou may refer to [Here](https://github.com/layumi/Person_reID_baseline_matconvnet).\nDifferent framework need to be tuned in a slightly different way.\n\n## Some News\n\n\n**22 Dec 2024** We are holding a workshop at ACM WWW 2025 on Multimedia Object Re-ID. You are welcome to show your insights. https://www.zdzheng.xyz/MORE2025/ \nSubmission DDL is **1 Jan 2025**.\n\n**12 Jan 2024** We are holding a workshop at ACM ICMR 2024 on  Multimedia Object Re-ID. You are welcome to show your insights. See you at Phuket, Thailand!😃 The workshop link is https://www.zdzheng.xyz/MORE2024/ . Submission DDL is **15 April 2024**. Good papers will be recommended to [ACM TOMM Special Issue](https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/ACM-SI_ToMM_MMGR-1708635711467.pdf ) (CCF-B). (Re-submission is needed.)\n\n<details>\n <summary><b>\n  2023 News\n</b></summary>\n    \n**12 Aug 2023** Large Person Langauge Model is currently available at [Here](https://github.com/Shuyu-XJTU/APTM)![GitHub stars](https://img.shields.io/github/stars/Shuyu-XJTU/APTM.svg?style=flat&label=Star) accepted by ACM MM'23. You are welcomed to check it.\n\n**19 Mar 2023** We host a special session on IEEE Intelligent Transportation Systems Conference (ITSC), covering the object re-identification & point cloud topic. The paper ddl is by May 15, 2023 and the paper notification is at June 30, 2023. Please select the session code ``w7r4a'' during submission. More details can be found at [Special Session Website](https://2023.ieee-itsc.org/wp-content/uploads/2023/03/IEEE-ITSC-2023-Special-Session-Proposal-Safe-Critical-Scenario-Understanding-in-Intelligent-Transportation-Systems-SCSU-ITS.pdf).  \n\n**9 Mar 2023** Market-1501 is in 3D. Please check our single 2D to 3D reconstruction work  https://github.com/layumi/3D-Magic-Mirror ![GitHub stars](https://img.shields.io/github/stars/layumi/3D-Magic-Mirror.svg?style=flat&label=Star).\n![Magic Mirror](https://github.com/layumi/3D-Magic-Mirror/raw/master/doc/rainbow_github.gif)\n</details>\n\n<details>\n <summary><b>\n  2022 News\n</b></summary>\n\n**7 Sep 2022** We support SwinV2. \n\n**24 Jul 2022** Market-HQ is released with super-resolution quality from 128\\*64 to 512\\*256. Please check at https://github.com/layumi/HQ-Market\n\n**14 Jul 2022** Add adversarial training by ``python train.py --name ftnet_adv --adv 0.1 --aiter 40``. \n\n**1 Feb 2022** Speed up the inference process about 10 seconds by removing the ``cat`` function in ``test.py``. \n   \n**1 Feb 2022** Add the demo with ``TensorRT`` (The fast inference speed may depend on the GPU with the latest RT Core).\n   \n</details>   \n\n<details>\n <summary><b>\n  2021 News\n </b></summary>\n\n**30 Dec 2021** We add supports for new losses, including arcface loss, cosface loss and instance loss. The hyper-parameters are still tunning.\n   \n**3 Dec 2021** We add supports for four losses, including triplet loss, contrastive loss, sphere loss and lifted loss. The hyper-parameters are still tunning.\n   \n**1 Dec 2021** We support EfficientNet/HRNet.\n   \n**15 Sep 2021** We support ResNet-ibn from ECCV2018 (https://github.com/XingangPan/IBN-Net). \n\n**17 Aug 2021** We support running code on Google Colab with free GPU. Please check it out at https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/colab .\n\n**14 Aug 2021** We have supported the training with [DG-Market](https://github.com/NVlabs/DG-Net#dg-market) for regularization via [Self-supervised Memory Learning](https://www.ijcai.org/proceedings/2020/150). You only neeed to download/unzip the dataset and add `--DG` to train model. \n\n**12 Aug 2021** We have supported the transformer-based model `Swin` by `--use_swin`. The basic performance is 92.73% Rank@1 and 79.71%mAP.\n\n**23 Jun 2021** Attack your re-ID model via Query! They are not robust as you expected! Check the code at [Here](https://github.com/layumi/A_reID).\n\n**5 Feb 2021** We have supported [Circle loss](https://arxiv.org/abs/2002.10857)(CVPR20 Oral). You can try it by simply adding `--circle`.  \n\n**11 January 2021** On the Market-1501 dataset, we accelerate the re-ranking processing from **89.2s** to **9.4ms** with one K40m GPU, facilitating the real-time post-processing. The pytorch implementation can be found in [GPU-Re-Ranking](GPU-Re-Ranking/).\n\n</details>\n\n<details>\n <summary><b>\n  2020 News\n </b></summary>\n   \n**11 June 2020** People live in the 3D world. We release one new person re-id code [Person Re-identification in the 3D Space](https://github.com/layumi/person-reid-3d), which conduct representation learning in the 3D space. You are welcomed to check out it.\n\n<img width=\"250\" height=\"150\" src=\"https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/pdf/3D-demo.png\"/>\n\n\n**30 April 2020** We have applied this code to the [AICity Challenge 2020](https://www.aicitychallenge.org/),  yielding the 1st Place Submission to the re-id track :red_car:. Check out [here](https://github.com/layumi/AICIty-reID-2020).\n\n**01 March 2020** We release one new image retrieval dataset, called [University-1652](https://github.com/layumi/University1652-Baseline), for drone-view target localization and drone navigation :helicopter:. It has a similar setting with the person re-ID. You are welcomed to check out it.\n</details>\n\n<details>\n <summary><b>\n  2019 News\n </b></summary>\n   \n**07 July 2019:** I added some new functions, such as `--resume`, auto-augmentation policy, acos loss, into [developing thread](https://github.com/layumi/Person_reID_baseline_pytorch/tree/dev) and rewrite the `save` and `load` functions. I haven't tested the functions throughly. Some new functions are worthy of having a try. If you are first to this repo, I suggest you stay with the master thread.\n\n**01 July 2019:** [My CVPR19 Paper](https://arxiv.org/abs/1904.07223) is online. It is based on this baseline repo as teacher model to provide pseudo label for the generated images to train a better student model. You are welcomed to check out the opensource code at [here](https://github.com/NVlabs/DG-Net).\n\n**03 Jun 2019:** Testing with multiple-scale inputs is added. You can use `--ms 1,0.9` when extracting the feature. It could slightly improve the final result.\n\n**20 May 2019:** Linear Warm Up is added. You also can set warm-up the first K epoch by `--warm_epoch K`. If K <=0, there will be no warm-up.\n\n </details>\n   \n<details>\n <summary><b>\n  2018 & 2017 News\n </b></summary> \n   \n**What's new:** FP16 has been added. It can be used by simply added `--fp16`. You need to install [apex](https://github.com/NVIDIA/apex) and update your pytorch to 1.0. \n\nFloat16 could save about 50% GPU memory usage without accuracy drop. **Our baseline could be trained with only 2GB GPU memory.** \n```bash\npython train.py --fp16\n```\n**What's new:** Visualizing ranking result is added.\n```bash\npython prepare.py\npython train.py\npython test.py\npython demo.py --query_index 777\n```\n\n**What's new:** Multiple-query Evaluation is added. The multiple-query result is about **Rank@1=91.95% mAP=78.06%**. \n```bash\npython prepare.py\npython train.py\npython test.py --multi\npython evaluate_gpu.py\n```\n\n**What's new:**  [PCB](https://arxiv.org/abs/1711.09349) is added. You may use '--PCB' to use this model. It can achieve around **Rank@1=92.73% mAP=78.16%**. I used a GPU (P40) with 24GB Memory. You may try apply smaller batchsize and choose the smaller learning rate (for stability) to run. (For example, `--batchsize 32 --lr 0.01 --PCB`)\n```bash\npython train.py --PCB --batchsize 64 --name PCB-64\npython test.py --PCB --name PCB-64\n```\n**What's new:** You may try `evaluate_gpu.py` to conduct a faster evaluation with GPU.\n\n**What's new:** You may apply '--use_dense' to use `DenseNet-121`. It can arrive around Rank@1=89.91% mAP=73.58%. \n\n**What's new:** Re-ranking is added to evaluation. The re-ranked result is about **Rank@1=90.20% mAP=84.76%**.\n\n**What's new:** Random Erasing is added to train.\n\n**What's new:** I add some code to generate training curves. The figure will be saved into the model folder when training.\n\n![](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/train.jpg)\n </details>\n   \n## Trained Model\nI re-trained several models, and the results may be different with the original one. Just for a quick reference, you may directly use these models. \nThe download link is [Here](https://drive.google.com/open?id=1XVEYb0TN2SbBYOqf8SzazfYZlpH9CxyE).\n\n|Methods | Rank@1 | mAP| Reference|\n| -------- | ----- | ---- | ---- |\n| [EfficientNet-b4] | 85.78% | 66.80% |  `python train.py --use_efficient --name eff; python test.py --name eff` |\n| [ResNet-50 + adv defense] | 87.77% | 69.83% |  `python train.py  --name adv0.1_40_w10_all --adv 0.1 --aiter 40 --warm 10 --train_all; python test.py  --name adv0.1_40_w10_all` |\n| [ConvNeXt] | 88.98% | 71.35% |  `python train.py --use_convnext --name convnext; python test.py --name convnext` |\n| [ResNet-50 (fp16)] | 88.03% | 71.40% | `python train.py --name fp16 --fp16 --train_all` |\n| [ResNet-50] | 88.84% | 71.59% |  `python train.py --train_all` |\n| [ResNet-50-ibn] | 89.13% | 73.40% | `python train.py --train_all --name res-ibn --ibn` |\n| [DenseNet-121] | 90.17% | 74.02% | `python train.py --name ft_net_dense --use_dense --train_all` |\n| [DenseNet-121 (Circle)] | 91.00% | 76.54% | `python train.py --name ft_net_dense_circle_w5 --circle --use_dense --train_all --warm_epoch 5` |\n| [HRNet-18] | 90.83% | 76.65% |  `python train.py --use_hr --name hr18; python test.py --name hr18` |\n| [PCB] | 92.64% | 77.47% | `python train.py --name PCB --PCB --train_all --lr 0.02` |\n| [PCB + DG] | 92.70% | 78.31% | `python train.py --name PCB_DG --PCB --train_all --lr 0.02 --DG; python test.py --name PCB_DG` |\n| [ResNet-50 (all tricks)] | 91.83% | 78.32% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5` |\n| [ResNet-50 (all tricks+Circle)] | 92.13% | 79.84% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5_circle  --circle` |\n| [ResNet-50 (all tricks+Circle+DG)] | 92.13% | 80.13% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5_circle_DG --circle --DG; python test.py --name warm5_s1_b8_lr2_p0.5_circle_DG` |\n| [DenseNet-121 (all tricks+Circle)] | 92.61% | 80.24% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name dense_warm5_s1_b8_lr2_p0.5_circle --circle --use_dense; python test.py --name  dense_warm5_s1_b8_lr2_p0.5_circle` |\n| [HRNet-18 (all tricks+Circle+DG)]| 92.19% | 81.00% | `python train.py --use_hr --name  hr18_p0.5_circle_w5_b16_lr0.01_DG --lr 0.01 --batch 16 --DG --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name  hr18_p0.5_circle_w5_b16_lr0.01_DG` |\n| [Swin] (224x224) | 92.75% | 79.70% | `python train.py --use_swin --name swin; python test.py --name swin`|\n| [SwinV2 (all tricks+Circle 256x128)] | 92.93% | 82.99% | `python train.py --use_swinv2 --name swinv2_p0.5_circle_w5_b16_lr0.03  --lr 0.03 --batch 16 --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name   swinv2_p0.5_circle_w5_b16_lr0.03 --batch 32`|\n| [Swin (all tricks+Circle 224x224)] | 94.12% | 84.39% | `python train.py --use_swin --name swin_p0.5_circle_w5  --erasing_p 0.5 --circle --warm_epoch 5;  python test.py --name swin_p0.5_circle_w5`|\n| [Swin (all tricks+Circle+b16 224x224)] | 94.00% | 85.21% | `python train.py --use_swin --name swin_p0.5_circle_w5_b16_lr0.01 --lr 0.01 --batch 16  --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name swin_p0.5_circle_w5_b16_lr0.01`|\n| [Swin (all tricks+Circle+b16+DG 224x224)] | 94.00% | 85.36% | `python train.py --use_swin --name swin_p0.5_circle_w5_b16_lr0.01_DG --lr 0.01 --batch 16 --DG --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name swin_p0.5_circle_w5_b16_lr0.01_DG`|\n\n* More training iterations may lead to better results. \n* Swin costs more GPU memory (11G GPU is needed) to run. \n* The hyper-parameter of [DG-Market](https://github.com/NVlabs/DG-Net#dg-market) `--DG` is not tuned. Better hyper-parameter may lead to better results.\n\n### Different Losses \n   \nI do not optimize the hyper-parameters. You are free to tune them for better performance.\n\n|Methods | Rank@1 | mAP| Reference|\n| -------- | ----- | ---- | ---- |\n| CE | 92.01% | 79.31% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_100 --total 100 ; python test.py  --name  warm5_s1_b32_lr8_p0.5_100`|\n| CE + Sphere [[Paper]](https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.pdf) | 92.01% | 79.39% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_sphere100 --sphere --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_sphere100` |\n| CE + Triplet [[Paper]](https://arxiv.org/pdf/1703.07737) | 92.40%\t| 79.71% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_triplet100 --triplet --total 100; python test.py  --name warm5_s1_b32_lr8_p0.5_triplet100` |\n| CE + Lifted [[Paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Song_Deep_Metric_Learning_CVPR_2016_paper.pdf)|  91.78% | 79.77% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_lifted100 --lifted --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_lifted100` |\n| CE + Instance [[Paper]](https://zdzheng.xyz/files/TOMM20.pdf) | 92.73% | 81.11% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_instance100_gamma64 --instance --ins_gamma 64 --total 100 ; python test.py  --name  warm5_s1_b32_lr8_p0.5_instance100_gamma64`|\n| CE + Contrast [[Paper]](https://zdzheng.xyz/files/TOMM18.pdf) | 92.28% | 81.42% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_contrast100 --contrast  --total 100; python test.py  --name warm5_s1_b32_lr8_p0.5_contrast100`|\n| CE + Circle [[Paper]](https://arxiv.org/abs/2002.10857) | 92.46% | 81.70% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_circle100 --circle --total 100 ; python test.py  --name  warm5_s1_b32_lr8_p0.5_circle100` |\n| CE + Contrast + Sphere | 92.79% | 82.02% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_cs100 --contrast --sphere --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_cs100`|\n| CE + Contrast + Triplet (Long) | 92.61% | 82.01% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.062 --name warm5_s1_b24_lr6.2_p0.5_contrast_triplet_133 --contrast --triplet --total 133 ; python test.py  --name  warm5_s1_b24_lr6.2_p0.5_contrast_triplet_133` |\n| CE + Contrast + Circle (Long) | 92.19% | 82.07% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.08 --name warm5_s1_b24_lr8_p0.5_contrast_circle133 --contrast --circle --total 133 ; python test.py  --name  warm5_s1_b24_lr8_p0.5_contrast_circle133` |\n| CE + Contrast + Sphere (Long) | 92.84% | 82.37% | `python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.06 --name warm5_s1_b24_lr6_p0.5_contrast_sphere133 --contrast --sphere --total 133 ; python test.py  --name  warm5_s1_b24_lr6_p0.5_contrast_sphere133` |\n\n\n### Model Structure\nYou may learn more from `model.py`. \nWe add one linear layer(bottleneck), one batchnorm layer and relu.\n\n## Prerequisites\n\n- Python 3.6+\n- GPU Memory >= 6G\n- Numpy\n- Pytorch 0.3+\n- timm `pip install timm` for Swin-Transformer with Pytorch >1.7.0\n- pretrainedmodels via `pip install pretrainedmodels`\n- [Optional] apex (for float16) \n- [Optional] [pretrainedmodels](https://github.com/Cadene/pretrained-models.pytorch)\n\n**(Some reports found that updating numpy can arrive the right accuracy. If you only get 50~80 Top1 Accuracy, just try it.)**\nWe have successfully run the code based on numpy 1.12.1 and 1.13.1 .\n\n## Getting started\n### Installation\n- Install Pytorch from http://pytorch.org/\n- Install required packages\n```bash\npip install -r requirements.txt\n```\n- [Optional] You may skip it. Usually it comes with pytorch. Install Torchvision from the source\n```bash\ngit clone https://github.com/pytorch/vision\ncd vision\npython setup.py install\n```\n- [Optional] You may skip it. Install apex from the source\n```bash\ngit clone https://github.com/NVIDIA/apex.git\ncd apex\npython setup.py install --cuda_ext --cpp_ext\n```\nBecause pytorch and torchvision are ongoing projects.\n\nHere we noted that our code is tested based on Pytorch 0.3.0/0.4.0/0.5.0/1.0.0 and Torchvision 0.2.0/0.2.1 .\n\n### Dataset & Preparation\n\nDownload [Market1501 Dataset](https://zheng-lab.cecs.anu.edu.au/Project/project_reid.html) [[Google]](https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view) [[Baidu]](https://pan.baidu.com/s/1ntIi2Op) Or use command line:\n```bash\npip install gdown \npip install --upgrade gdown #!!important!!\ngdown 0B8-rUzbwVRk0c054eEozWG9COHM\n```\n\nPreparation: Put the images with the same id in one folder. You may use \n```bash\npython prepare.py\n```\nRemember to change the dataset path to your own path.\n\nFuthermore, you also can test our code on [DukeMTMC-reID Dataset]( [GoogleDriver](https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O) or ([BaiduYun](https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw) password: bhbh)) Or use command line:\n```bash \ngdown 1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O\n```\nOur baseline code is not such high on DukeMTMC-reID **Rank@1=64.23%, mAP=43.92%**. Hyperparameters are need to be tuned.\n\n- [Optional] [DG-Market](https://github.com/NVlabs/DG-Net#dg-market) is a generated pedestrian dataset of 128,307 images for training a robust model.\n\n### Train\nTrain a model by\n```bash\npython train.py --gpu_ids 0 --name ft_ResNet50 --train_all --batchsize 32  --data_dir your_data_path\n```\n`--gpu_ids` which gpu to run.\n\n`--name` the name of model.\n\n`--data_dir` the path of the training data, e.g., `/home/yourname/Market/pytorch`\n\n`--train_all` using all images to train. \n\n`--batchsize` batch size.\n\n`--erasing_p` random erasing probability.\n\nTrain a model with random erasing by\n```bash\npython train.py --gpu_ids 0 --name ft_ResNet50 --train_all --batchsize 32  --data_dir your_data_path --erasing_p 0.5\n```\n\nIf you want to use **multiple GPUs**, you are suggested to use DDP (`train_DDP.py`) instead of DP (`train.py`). It is because DP lacks the torch supports and may face some [NaN](https://discuss.pytorch.org/t/nan-loss-with-dataparallel/26501).\nYou could call `train_DDP.py` by running `DDP.sh`. \n```bash\nbash DDP.sh \n```\n\n### Test\nUse trained model to extract feature by\n```bash\npython test.py --gpu_ids 0 --name ft_ResNet50 --test_dir your_data_path  --batchsize 32 --which_epoch 59\n```\n`--gpu_ids` which gpu to run.\n\n`--batchsize` batch size.\n\n`--name` the dir name of trained model.\n\n`--which_epoch` select the i-th model.\n\n`--data_dir` the path of the testing data.\n\n\n### Evaluation\n```bash\npython evaluate.py\n```\nIt will output Rank@1, Rank@5, Rank@10 and mAP results.\nYou may also try `evaluate_gpu.py` to conduct a faster evaluation with GPU.\n\nFor mAP calculation, you also can refer to the [C++ code for Oxford Building](http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/compute_ap.cpp). We use the triangle mAP calculation (consistent with the Market1501 original code).\n\n### re-ranking\n```bash\npython evaluate_rerank.py\n```\n**It may take more than 10G Memory to run.** So run it on a powerful machine if possible. \n\nIt will output Rank@1, Rank@5, Rank@10 and mAP results.\n\n### Tips\nNotes the format of the camera id and the number of cameras.\n\nFor some dataset, e.g., MSMT17, there are more than 10 cameras. You need to modify the `prepare.py` and `test.py` to read the double-digit camera ID.\n\nFor some vehicle re-ID datasets. e.g. VeRi, you also need to modify the `prepare.py` and `test.py`.  It has different naming rules.\nhttps://github.com/layumi/Person_reID_baseline_pytorch/issues/107 (Sorry. It is in Chinese)\n\n\n## Citation\nThe following paper uses and reports the result of the baseline model. You may cite it in your paper.\n```bib\n@article{zheng2019joint,\n  title={Joint discriminative and generative learning for person re-identification},\n  author={Zheng, Zhedong and Yang, Xiaodong and Yu, Zhiding and Zheng, Liang and Yang, Yi and Kautz, Jan},\n  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2019}\n}\n```\n\nThe following papers may be the first two to use the bottleneck baseline. You may cite them in your paper.\n```bib\n@article{DBLP:journals/corr/SunZDW17,\n  author    = {Yifan Sun and\n               Liang Zheng and\n               Weijian Deng and\n               Shengjin Wang},\n  title     = {SVDNet for Pedestrian Retrieval},\n  booktitle   = {ICCV},\n  year      = {2017},\n}\n\n@article{hermans2017defense,\n  title={In Defense of the Triplet Loss for Person Re-Identification},\n  author={Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},\n  journal={arXiv preprint arXiv:1703.07737},\n  year={2017}\n}\n```\n\nBasic Model\n```bib\n@article{zheng2018discriminatively,\n  title={A discriminatively learned CNN embedding for person reidentification},\n  author={Zheng, Zhedong and Zheng, Liang and Yang, Yi},\n  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},\n  volume={14},\n  number={1},\n  pages={13},\n  year={2018},\n  publisher={ACM}\n}\n\n@article{zheng2020vehiclenet,\n  title={VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification},\n  author={Zheng, Zhedong and Ruan, Tao and Wei, Yunchao and Yang, Yi and Mei, Tao},\n  journal={IEEE Transaction on Multimedia (TMM)},\n  year={2020}\n}\n```\n\n## Related Repos\n1. [Pedestrian Alignment Network](https://github.com/layumi/Pedestrian_Alignment) ![GitHub stars](https://img.shields.io/github/stars/layumi/Pedestrian_Alignment.svg?style=flat&label=Star)\n2. [2stream Person re-ID](https://github.com/layumi/2016_person_re-ID) ![GitHub stars](https://img.shields.io/github/stars/layumi/2016_person_re-ID.svg?style=flat&label=Star)\n3. [Pedestrian GAN](https://github.com/layumi/Person-reID_GAN) ![GitHub stars](https://img.shields.io/github/stars/layumi/Person-reID_GAN.svg?style=flat&label=Star)\n4. [Language Person Search](https://github.com/layumi/Image-Text-Embedding) ![GitHub stars](https://img.shields.io/github/stars/layumi/Image-Text-Embedding.svg?style=flat&label=Star)\n5. [DG-Net](https://github.com/NVlabs/DG-Net) ![GitHub stars](https://img.shields.io/github/stars/NVlabs/DG-Net.svg?style=flat&label=Star)\n6. [3D Person re-ID](https://github.com/layumi/person-reid-3d) ![GitHub stars](https://img.shields.io/github/stars/layumi/person-reid-3d.svg?style=flat&label=Star)\n"
        },
        {
          "name": "circle_loss.py",
          "type": "blob",
          "size": 1.685546875,
          "content": "#from https://github.com/TinyZeaMays/CircleLoss/blob/master/circle_loss.py \n\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn, Tensor\n\n\ndef convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\n    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\n    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\n\n    positive_matrix = label_matrix.triu(diagonal=1)\n    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\n\n    similarity_matrix = similarity_matrix.view(-1)\n    positive_matrix = positive_matrix.view(-1)\n    negative_matrix = negative_matrix.view(-1)\n    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]\n\n\nclass CircleLoss(nn.Module):\n    def __init__(self, m: float, gamma: float) -> None:\n        super(CircleLoss, self).__init__()\n        self.m = m\n        self.gamma = gamma\n        self.soft_plus = nn.Softplus()\n\n    def forward(self, sp: Tensor, sn: Tensor) -> Tensor:\n        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n\n        delta_p = 1 - self.m\n        delta_n = self.m\n\n        logit_p = - ap * (sp - delta_p) * self.gamma\n        logit_n = an * (sn - delta_n) * self.gamma\n\n        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n\n        return loss\n\n\nif __name__ == \"__main__\":\n    feat = nn.functional.normalize(torch.rand(256, 64, requires_grad=True))\n    lbl = torch.randint(high=10, size=(256,))\n\n    inp_sp, inp_sn = convert_label_to_similarity(feat, lbl)\n\n    criterion = CircleLoss(m=0.25, gamma=256)\n    circle_loss = criterion(inp_sp, inp_sn)\n\n    print(circle_loss)\n"
        },
        {
          "name": "colab",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 3.6435546875,
          "content": "import argparse\nimport scipy.io\nimport torch\nimport numpy as np\nimport os\nfrom torchvision import datasets\nimport matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\n#######################################################################\n# Evaluate\nparser = argparse.ArgumentParser(description='Demo')\nparser.add_argument('--query_index', default=777, type=int, help='test_image_index')\nparser.add_argument('--test_dir',default='../Market/pytorch',type=str, help='./test_data')\nopts = parser.parse_args()\n\ndata_dir = opts.test_dir\nimage_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ) for x in ['gallery','query']}\n\n#####################################################################\n#Show result\ndef imshow(path, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    im = plt.imread(path)\n    plt.imshow(im)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = torch.FloatTensor(result['query_f'])\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = torch.FloatTensor(result['gallery_f'])\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nmulti = os.path.isfile('multi_query.mat')\n\nif multi:\n    m_result = scipy.io.loadmat('multi_query.mat')\n    mquery_feature = torch.FloatTensor(m_result['mquery_f'])\n    mquery_cam = m_result['mquery_cam'][0]\n    mquery_label = m_result['mquery_label'][0]\n    mquery_feature = mquery_feature.cuda()\n\nquery_feature = query_feature.cuda()\ngallery_feature = gallery_feature.cuda()\n\n#######################################################################\n# sort the images\ndef sort_img(qf, ql, qc, gf, gl, gc):\n    query = qf.view(-1,1)\n    # print(query.shape)\n    score = torch.mm(gf,query)\n    score = score.squeeze(1).cpu()\n    score = score.numpy()\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    # index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    #same camera\n    camera_index = np.argwhere(gc==qc)\n\n    #good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) \n\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n    return index\n\ni = opts.query_index\nindex = sort_img(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n\n########################################################################\n# Visualize the rank result\n\nquery_path, _ = image_datasets['query'].imgs[i]\nquery_label = query_label[i]\nprint(query_path)\nprint('Top 10 images are as follow:')\ntry: # Visualize Ranking Result \n    # Graphical User Interface is needed\n    fig = plt.figure(figsize=(16,4))\n    ax = plt.subplot(1,11,1)\n    ax.axis('off')\n    imshow(query_path,'query')\n    for i in range(10):\n        ax = plt.subplot(1,11,i+2)\n        ax.axis('off')\n        img_path, _ = image_datasets['gallery'].imgs[index[i]]\n        label = gallery_label[index[i]]\n        imshow(img_path)\n        if label == query_label:\n            ax.set_title('%d'%(i+1), color='green')\n        else:\n            ax.set_title('%d'%(i+1), color='red')\n        print(img_path)\nexcept RuntimeError:\n    for i in range(10):\n        img_path = image_datasets.imgs[index[i]]\n        print(img_path[0])\n    print('If you want to see the visualization of the ranking result, graphical user interface is needed.')\n\nfig.savefig(\"show.png\")\n"
        },
        {
          "name": "dgfolder.py",
          "type": "blob",
          "size": 0.7421875,
          "content": "from torchvision import datasets\nimport numpy as np\n\nclass DGFolder(datasets.ImageFolder):\n\n    def __init__(self, root, transform):\n        super(DGFolder, self).__init__(root, transform)\n        targets = np.asarray([s[1] for s in self.samples])\n        self.targets = targets\n        self.img_num = len(self.samples)\n        print(self.img_num)\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample1 = self.loader(path)\n        sample2 = sample1\n        if self.transform is not None:\n            sample1 = self.transform(sample1)\n            sample2 = self.transform(sample2)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample1, sample2, target\n\n\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 3.3447265625,
          "content": "import scipy.io\nimport torch\nimport numpy as np\n#import time\nimport os\n\n#######################################################################\n# Evaluate\ndef evaluate(qf,ql,qc,gf,gl,gc):\n    query = qf\n    score = np.dot(gf,query)\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    #index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = result['query_f']\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = result['gallery_f']\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nmulti = os.path.isfile('multi_query.mat')\n\nif multi:\n    m_result = scipy.io.loadmat('multi_query.mat')\n    mquery_feature = m_result['mquery_f']\n    mquery_cam = m_result['mquery_cam'][0]\n    mquery_label = m_result['mquery_label'][0]\n    \nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#print(query_label)\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n\n# multiple-query\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\nif multi:\n    for i in range(len(query_label)):\n        mquery_index1 = np.argwhere(mquery_label==query_label[i])\n        mquery_index2 = np.argwhere(mquery_cam==query_cam[i])\n        mquery_index =  np.intersect1d(mquery_index1, mquery_index2)\n        mq = np.mean(mquery_feature[mquery_index,:], axis=0)\n        ap_tmp, CMC_tmp = evaluate(mq,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n        if CMC_tmp[0]==-1:\n            continue\n        CMC = CMC + CMC_tmp\n        ap += ap_tmp\n        #print(i, CMC_tmp[0])\n    CMC = CMC.float()\n    CMC = CMC/len(query_label) #average CMC\n    print('multi Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"
        },
        {
          "name": "evaluate_gpu.py",
          "type": "blob",
          "size": 3.642578125,
          "content": "import scipy.io\nimport torch\nimport numpy as np\n#import time\nimport os\n\n#######################################################################\n# Evaluate\ndef evaluate(qf,ql,qc,gf,gl,gc):\n    query = qf.view(-1,1)\n    # print(query.shape)\n    score = torch.mm(gf,query)\n    score = score.squeeze(1).cpu()\n    score = score.numpy()\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    # index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = torch.FloatTensor(result['query_f'])\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = torch.FloatTensor(result['gallery_f'])\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nmulti = os.path.isfile('multi_query.mat')\n\nif multi:\n    m_result = scipy.io.loadmat('multi_query.mat')\n    mquery_feature = torch.FloatTensor(m_result['mquery_f'])\n    mquery_cam = m_result['mquery_cam'][0]\n    mquery_label = m_result['mquery_label'][0]\n    mquery_feature = mquery_feature.cuda()\n\nquery_feature = query_feature.cuda()\ngallery_feature = gallery_feature.cuda()\n\nprint(query_feature.shape)\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#print(query_label)\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    #print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n\n# multiple-query\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\nif multi:\n    for i in range(len(query_label)):\n        mquery_index1 = np.argwhere(mquery_label==query_label[i])\n        mquery_index2 = np.argwhere(mquery_cam==query_cam[i])\n        mquery_index =  np.intersect1d(mquery_index1, mquery_index2)\n        mq = torch.mean(mquery_feature[mquery_index,:], dim=0)\n        ap_tmp, CMC_tmp = evaluate(mq,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n        if CMC_tmp[0]==-1:\n            continue\n        CMC = CMC + CMC_tmp\n        ap += ap_tmp\n        #print(i, CMC_tmp[0])\n    CMC = CMC.float()\n    CMC = CMC/len(query_label) #average CMC\n    print('multi Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"
        },
        {
          "name": "evaluate_rerank.py",
          "type": "blob",
          "size": 2.6708984375,
          "content": "import scipy.io\nimport torch\nimport numpy as np\nimport time\nfrom  re_ranking import re_ranking\n#######################################################################\n# Evaluate\ndef evaluate(score,ql,qc,gl,gc):\n    index = np.argsort(score)  #from small to large\n    #index = index[::-1]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = result['query_f']\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = result['gallery_f']\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#re-ranking\nprint('calculate initial distance')\nq_g_dist = np.dot(query_feature, np.transpose(gallery_feature))\nq_q_dist = np.dot(query_feature, np.transpose(query_feature))\ng_g_dist = np.dot(gallery_feature, np.transpose(gallery_feature))\nsince = time.time()\nre_rank = re_ranking(q_g_dist, q_q_dist, g_g_dist)\ntime_elapsed = time.time() - since\nprint('Reranking complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(re_rank[i,:],query_label[i],query_cam[i],gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    #print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('top1:%f top5:%f top10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"
        },
        {
          "name": "instance_loss.py",
          "type": "blob",
          "size": 1.283203125,
          "content": "import torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\n\ndef l2_norm(v):\n    fnorm = torch.norm(v, p=2, dim=1, keepdim=True) + 1e-6\n    v = v.div(fnorm.expand_as(v))\n    return v\n\nclass InstanceLoss(nn.Module):\n    def __init__(self, gamma = 1) -> None:\n        super(InstanceLoss, self).__init__()\n        self.gamma = gamma\n\n    def forward(self, feature, label = None) -> Tensor:\n        # Dual-Path Convolutional Image-Text Embeddings with Instance Loss, ACM TOMM 2020 \n        # https://zdzheng.xyz/files/TOMM20.pdf \n        # using cross-entropy loss for every sample if label is not available. else use given label.\n        normed_feature = l2_norm(feature)\n        sim1 = torch.mm(normed_feature*self.gamma, torch.t(normed_feature)) \n        #sim2 = sim1.t()\n        if label is None:\n            sim_label = torch.arange(sim1.size(0)).cuda().detach()\n        else:\n            _, sim_label = torch.unique(label, return_inverse=True)\n        loss = F.cross_entropy(sim1, sim_label) #+ F.cross_entropy(sim2, sim_label)\n        return loss\n\n\nif __name__ == \"__main__\":\n    feat = nn.functional.normalize(torch.rand(256, 64, requires_grad=True))\n    lbl = torch.randint(high=10, size=(256,))\n\n    criterion = InstanceLoss()\n    instance_loss = criterion(feat, lbl)\n\n    print(instance_loss)\n"
        },
        {
          "name": "leaderboard-3D",
          "type": "tree",
          "content": null
        },
        {
          "name": "leaderboard-RGB-Infrared",
          "type": "tree",
          "content": null
        },
        {
          "name": "leaderboard",
          "type": "tree",
          "content": null
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 14.2216796875,
          "content": "import torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torchvision import models\nfrom torch.autograd import Variable\nimport pretrainedmodels\nimport timm\nfrom utils import load_state_dict_mute\n######################################################################\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find('Conv') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in') # For old pytorch, you may use kaiming_normal.\n    elif classname.find('Linear') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n    elif classname.find('BatchNorm1d') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n    if hasattr(m, 'bias') and m.bias is not None:\n        init.constant_(m.bias.data, 0.0)\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        init.normal_(m.weight.data, std=0.001)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef activate_drop(m):\n    classname = m.__class__.__name__\n    if classname.find('Drop') != -1:\n        m.p = 0.1\n        m.inplace = True\n\n# Defines the new fc layer and classification layer\n# |--Linear--|--bn--|--relu--|--Linear--|\nclass ClassBlock(nn.Module):\n    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, linear=512, return_f = False):\n        super(ClassBlock, self).__init__()\n        self.return_f = return_f\n        add_block = []\n        if linear>0:\n            add_block += [nn.Linear(input_dim, linear)]\n        else:\n            linear = input_dim\n        if bnorm:\n            add_block += [nn.BatchNorm1d(linear)]\n        if relu:\n            add_block += [nn.LeakyReLU(0.1)]\n        if droprate>0:\n            add_block += [nn.Dropout(p=droprate)]\n        add_block = nn.Sequential(*add_block)\n        add_block.apply(weights_init_kaiming)\n\n        classifier = []\n        classifier += [nn.Linear(linear, class_num)]\n        classifier = nn.Sequential(*classifier)\n        classifier.apply(weights_init_classifier)\n\n        self.linear_num = linear\n        self.add_block = add_block\n        self.classifier = classifier\n    def forward(self, x):\n        x = self.add_block(x)\n        if self.return_f:\n            f = x\n            x = self.classifier(x)\n            return [x,f]\n        else:\n            x = self.classifier(x)\n            return x\n\n# Define the ResNet50-based Model\nclass ft_net(nn.Module):\n\n    def __init__(self, class_num=751, droprate=0.5, stride=2, circle=False, ibn=False, linear_num=512):\n        super(ft_net, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        if ibn==True:\n            model_ft = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\n        # avg pooling to global pooling\n        if stride == 1:\n            model_ft.layer4[0].downsample[0].stride = (1,1)\n            model_ft.layer4[0].conv2.stride = (1,1)\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.model = model_ft\n        self.circle = circle\n        self.classifier = ClassBlock(2048, class_num, droprate, linear=linear_num, return_f = circle)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.model.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the swin_base_patch4_window7_224 Model\n# pytorch > 1.6\nclass ft_net_swin(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5, stride=2, circle=False, linear_num=512):\n        super(ft_net_swin, self).__init__()\n        model_ft = timm.create_model('swin_base_patch4_window7_224', pretrained=True, drop_path_rate = 0.2)\n        # avg pooling to global pooling\n        #model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.head = nn.Sequential() # save memory\n        self.model = model_ft\n        self.circle = circle\n        self.avgpool1d = nn.AdaptiveAvgPool1d(1)\n        self.avgpool2d = nn.AdaptiveAvgPool2d((1,1))\n        self.classifier = ClassBlock(1024, class_num, droprate, linear=linear_num, return_f = circle)\n        print('Make sure timm > 0.6.0 and you can install latest timm version by pip install git+https://github.com/rwightman/pytorch-image-models.git')\n    def forward(self, x):\n        x = self.model.forward_features(x)\n        # swin is update in latest timm>0.6.0, so I add the following two lines.\n        if x.dim()==3:\n            x = self.avgpool1d(x.permute((0,2,1)))\n        else: \n            x = self.avgpool2d(x.permute((0,3,1,2)))\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\nclass ft_net_swinv2(nn.Module):\n\n    def __init__(self, class_num, input_size=(256, 128), droprate=0.5, stride=2, circle=False, linear_num=512):\n        super(ft_net_swinv2, self).__init__()\n        model_ft = timm.create_model('swinv2_base_window8_256', pretrained=False, img_size = input_size, drop_path_rate = 0.2)\n        model_full = timm.create_model('swinv2_base_window8_256', pretrained=True)\n        load_state_dict_mute(model_ft, model_full.state_dict(), strict=False)\n        #model_ft = timm.create_model('swinv2_cr_small_224', pretrained=True, img_size = input_size, drop_path_rate = 0.2)\n        # avg pooling to global pooling\n        model_ft.head = nn.Sequential() # save memory\n        self.model = model_ft\n        self.circle = circle\n        self.avgpool1d = nn.AdaptiveAvgPool1d(1)\n        self.avgpool2d = nn.AdaptiveAvgPool2d((1,1))\n        self.classifier = ClassBlock(1024, class_num, droprate, linear=linear_num, return_f = circle)\n        print('Make sure timm > 0.6.0 and you can install latest timm version by pip install git+https://github.com/rwightman/pytorch-image-models.git')\n    def forward(self, x):\n        x = self.model.forward_features(x)\n        if x.dim()==3:\n            x = self.avgpool1d(x.permute((0,2,1)))\n        else:\n            x = self.avgpool2d(x.permute((0,3,1,2)))\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\nclass ft_net_convnext(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5, stride=2, circle=False, linear_num=512):\n        super(ft_net_convnext, self).__init__()\n        model_ft = timm.create_model('convnext_base', pretrained=True, drop_path_rate = 0.2)\n        # avg pooling to global pooling\n        #model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.head = nn.Sequential() # save memory\n        self.model = model_ft\n        #self.model.apply(activate_drop)\n        self.circle = circle\n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.classifier = ClassBlock(1024, class_num, droprate, linear=linear_num, return_f = circle)\n\n    def forward(self, x):\n        x = self.model.forward_features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the HRNet18-based Model\nclass ft_net_hr(nn.Module):\n    def __init__(self, class_num, droprate=0.5, circle=False, linear_num=512):\n        super().__init__()\n        model_ft = timm.create_model('hrnet_w18', pretrained=True)\n        # avg pooling to global pooling\n        #model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.classifier = nn.Sequential() # save memory\n        self.model = model_ft\n        self.circle = circle\n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.classifier = ClassBlock(2048, class_num, droprate, linear=linear_num, return_f = circle)\n\n    def forward(self, x):\n        x = self.model.forward_features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the DenseNet121-based Model\nclass ft_net_dense(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5, stride = 2, circle=False, linear_num=512):\n        super().__init__()\n        model_ft = models.densenet121(pretrained=True)\n        model_ft.features.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.fc = nn.Sequential()\n        if stride == 1:\n            model_ft.features.transition3.pool.stride = 1\n        self.model = model_ft\n        self.circle = circle\n        # For DenseNet, the feature dim is 1024 \n        self.classifier = ClassBlock(1024, class_num, droprate, linear=linear_num, return_f=circle)\n\n    def forward(self, x):\n        x = self.model.features(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n# Define the Efficient-b4-based Model\nclass ft_net_efficient(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5, circle=False, linear_num=512):\n        super().__init__()\n        #model_ft = timm.create_model('tf_efficientnet_b4', pretrained=True)\n        try:\n            from efficientnet_pytorch import EfficientNet\n        except ImportError:\n            print('Please pip install efficientnet_pytorch')\n        model_ft = EfficientNet.from_pretrained('efficientnet-b4')\n        # avg pooling to global pooling\n        #model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.head = nn.Sequential() # save memory\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.classifier = nn.Sequential()\n        self.model = model_ft\n        self.circle = circle\n        # For EfficientNet, the feature dim is not fixed\n        # for efficientnet_b2 1408\n        # for efficientnet_b4 1792\n        self.classifier = ClassBlock(1792, class_num, droprate, linear=linear_num, return_f=circle)\n    def forward(self, x):\n        #x = self.model.forward_features(x)\n        x = self.model.extract_features(x)\n        x = self.model.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the NAS-based Model\nclass ft_net_NAS(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5, linear_num=512):\n        super().__init__()  \n        model_name = 'nasnetalarge' \n        # pip install pretrainedmodels\n        model_ft = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n        model_ft.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.dropout = nn.Sequential()\n        model_ft.last_linear = nn.Sequential()\n        self.model = model_ft\n        # For DenseNet, the feature dim is 4032\n        self.classifier = ClassBlock(4032, class_num, droprate, linear=linear_num)\n\n    def forward(self, x):\n        x = self.model.features(x)\n        x = self.model.avg_pool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n    \n# Define the ResNet50-based Model (Middle-Concat)\n# In the spirit of \"The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching.\" Yu, Qian, et al. arXiv:1711.08106 (2017).\nclass ft_net_middle(nn.Module):\n\n    def __init__(self, class_num=751, droprate=0.5):\n        super(ft_net_middle, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.model.avgpool(x)\n        x = torch.squeeze(x)\n        x = self.classifier(x) #use our classifier.\n        return x\n\n# Part Model proposed in Yifan Sun etal. (2018)\nclass PCB(nn.Module):\n    def __init__(self, class_num ):\n        super(PCB, self).__init__()\n\n        self.part = 6 # We cut the pool5 to 6 parts\n        model_ft = models.resnet50(pretrained=True)\n        self.model = model_ft\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part,1))\n        self.dropout = nn.Dropout(p=0.5)\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1,1)\n        self.model.layer4[0].conv2.stride = (1,1)\n        # define 6 classifiers\n        for i in range(self.part):\n            name = 'classifier'+str(i)\n            setattr(self, name, ClassBlock(2048, class_num, droprate=0.5, linear=256, relu=False, bnorm=True))\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        \n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        part = {}\n        predict = {}\n        # get six part feature batchsize*2048*6\n        for i in range(self.part):\n            part[i] = x[:,:,i].view(x.size(0), x.size(1))\n            name = 'classifier'+str(i)\n            c = getattr(self,name)\n            predict[i] = c(part[i])\n\n        # sum prediction\n        #y = predict[0]\n        #for i in range(self.part-1):\n        #    y += predict[i+1]\n        y = []\n        for i in range(self.part):\n            y.append(predict[i])\n        return y\n\nclass PCB_test(nn.Module):\n    def __init__(self,model):\n        super(PCB_test,self).__init__()\n        self.part = 6\n        self.model = model.model\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part,1))\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1,1)\n        self.model.layer4[0].conv2.stride = (1,1)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        y = x.view(x.size(0),x.size(1),x.size(2))\n        return y\n'''\n# debug model structure\n# Run this code with:\npython model.py\n'''\nif __name__ == '__main__':\n# Here I left a simple forward function.\n# Test the model, before you train it. \n    net = ft_net_hr(751)\n    #net = ft_net_swin(751, stride=1)\n    net.classifier = nn.Sequential()\n    print(net)\n    input = Variable(torch.FloatTensor(8, 3, 224, 224))\n    output = net(input)\n    print('net output size:')\n    print(output.shape)\n"
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "pdf",
          "type": "tree",
          "content": null
        },
        {
          "name": "prepare.py",
          "type": "blob",
          "size": 3.76171875,
          "content": "import os\nfrom shutil import copyfile\n\ndownload_path = '../Market' # Please not change.\ndownload_path2 = '../Market-1501-v15.09.15' # You only need to change this line to your dataset download path\n\nif not os.path.isdir(download_path):\n    if os.path.isdir(download_path2):\n        os.system('mv %s %s'%(download_path2, download_path)) # rename\n    else:\n        print('please change the download_path')\n\nsave_path = download_path + '/pytorch'\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n#-----------------------------------------\n#query\nquery_path = download_path + '/query'\nquery_save_path = download_path + '/pytorch/query'\nif not os.path.isdir(query_save_path):\n    os.mkdir(query_save_path)\n\nfor root, dirs, files in os.walk(query_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = query_path + '/' + name\n        dst_path = query_save_path + '/' + ID[0] \n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n#-----------------------------------------\n#multi-query\nquery_path = download_path + '/gt_bbox'\n# for dukemtmc-reid, we do not need multi-query\nif os.path.isdir(query_path):\n    query_save_path = download_path + '/pytorch/multi-query'\n    if not os.path.isdir(query_save_path):\n        os.mkdir(query_save_path)\n\n    for root, dirs, files in os.walk(query_path, topdown=True):\n        for name in files:\n            if not name[-3:]=='jpg':\n                continue\n            ID  = name.split('_')\n            src_path = query_path + '/' + name\n            dst_path = query_save_path + '/' + ID[0]\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            copyfile(src_path, dst_path + '/' + name)\n\n#-----------------------------------------\n#gallery\ngallery_path = download_path + '/bounding_box_test'\ngallery_save_path = download_path + '/pytorch/gallery'\nif not os.path.isdir(gallery_save_path):\n    os.mkdir(gallery_save_path)\n\nfor root, dirs, files in os.walk(gallery_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = gallery_path + '/' + name\n        dst_path = gallery_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n#---------------------------------------\n#train_all\ntrain_path = download_path + '/bounding_box_train'\ntrain_save_path = download_path + '/pytorch/train_all'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n\n#---------------------------------------\n#train_val\ntrain_path = download_path + '/bounding_box_train'\ntrain_save_path = download_path + '/pytorch/train'\nval_save_path = download_path + '/pytorch/val'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n    os.mkdir(val_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n            dst_path = val_save_path + '/' + ID[0]  #first image is used as val image\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n"
        },
        {
          "name": "prepare_CUB.py",
          "type": "blob",
          "size": 1.3857421875,
          "content": "import os\nfrom shutil import copyfile\n\n# 1-100 train\n# 101-200 test\nos.system('wget http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz')\nos.system('tar -zxvf CUB_200_2011.tgz')\nos.system('cd ./CUB_200_2011/images')\nos.system('mkdir train_all')\nos.system('mkdir test')\nos.system('mv 0* train_all')\nos.system('mv ./100.Brown_Pelican train_all')\nos.system('mv 1* test')\nos.system('mv ./200.Common_Yellowthroat/ test')\n\n#---------------------------------------\n#train_val\ndownload_path = './CUB_200_2011'\ntrain_path = download_path + '/images/train_all'\ntrain_save_path = download_path + '/images/train'\nval_save_path = download_path + '/images/val'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n    os.mkdir(val_save_path)\n\nfor r, subdirs,f in os.walk(train_path, topdown=True):\n    for sub in subdirs:\n        for root, dirs, files in os.walk(train_path+'/'+sub, topdown=True):\n            for name in files:\n                if not name[-3:]=='jpg':\n                    continue\n                src_path = train_path + '/' + sub  + '/' + name\n                dst_path = train_save_path + '/' + sub\n                if not os.path.isdir(dst_path):\n                    os.mkdir(dst_path)\n                    dst_path = val_save_path + '/'+ sub   #first image is used as val image\n                    os.mkdir(dst_path)\n                copyfile(src_path, dst_path + '/' + name)\n"
        },
        {
          "name": "prepare_MSMT.py",
          "type": "blob",
          "size": 2.681640625,
          "content": "import os\nfrom shutil import copyfile\n\n# You only need to change this line to your dataset download path\ndownload_path = '/home/zzd/MSMT17_V1/'\n\nif not os.path.isdir(download_path):\n    print('please change the download_path')\n\nsave_path = download_path + '/pytorch'\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n#-----------------------------------------\n#query\nquery_path = download_path + 'test/'\nquery_save_path = download_path + '/pytorch/query'\nif not os.path.isdir(query_save_path):\n    os.mkdir(query_save_path)\n\nfor name in open(download_path+'list_query.txt'):\n    name = name.split(' ')[0]\n    ID = name.split('/')\n    src_path = query_path  + name\n    dst_path = query_save_path + '/' + ID[0] \n    if not os.path.isdir(dst_path):\n        os.mkdir(dst_path)\n    copyfile(src_path, dst_path + '/' + os.path.basename(name))\n\n#-----------------------------------------\n#gallery\ngallery_path = download_path + 'test/'\ngallery_save_path = download_path + '/pytorch/gallery'\nif not os.path.isdir(gallery_save_path):\n    os.mkdir(gallery_save_path)\n\nfor name in open(download_path+'list_gallery.txt'):\n    name = name.split(' ')[0]\n    ID = name.split('/')\n    src_path = gallery_path  + name\n    dst_path = gallery_save_path + '/' + ID[0]\n    if not os.path.isdir(dst_path):\n        os.mkdir(dst_path)\n    copyfile(src_path, dst_path + '/' + os.path.basename(name))\n\n#---------------------------------------\n#train_val\ntrain_path = download_path + 'train/'\nval_path = download_path + 'train/'\ntrain_save_path = download_path + '/pytorch/train'\ntrain_all_save_path = download_path + '/pytorch/train_all'\nval_save_path = download_path + '/pytorch/val'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n    os.mkdir(train_all_save_path)\n    os.mkdir(val_save_path)\n\nfor name in open(download_path+'list_train.txt'):\n    name = name.split(' ')[0]\n    ID = name.split('/')\n    src_path = train_path  + name\n    dst_path = train_save_path + '/' + ID[0]\n    dst_all_path = train_all_save_path + '/' + ID[0]\n    if not os.path.isdir(dst_path):\n        os.mkdir(dst_path)\n    if not os.path.isdir(dst_all_path):\n        os.mkdir(dst_all_path)\n    copyfile(src_path, dst_path + '/' + os.path.basename(name))\n    copyfile(src_path, dst_all_path + '/' + os.path.basename(name))\n\nfor name in open(download_path+'list_val.txt'):\n    name = name.split(' ')[0]\n    ID = name.split('/')\n    src_path = val_path  + name\n    dst_path = val_save_path + '/' + ID[0]\n    if not os.path.isdir(dst_path):\n        os.mkdir(dst_path)\n    if not os.path.isdir(dst_all_path):\n        os.mkdir(dst_all_path)\n    copyfile(src_path, dst_path + '/' + os.path.basename(name))\n    copyfile(src_path, dst_all_path + '/' + os.path.basename(name))\n"
        },
        {
          "name": "prepare_VeRi.py",
          "type": "blob",
          "size": 3.4619140625,
          "content": "import os\nfrom shutil import copyfile\n\ndef copy_file(s, t):\n    for root, dirs, files in os.walk(s):\n        for name in files:\n            copyfile(root+'/'+name,t+'/'+name)\n\n# You only need to change this line to your dataset download path\ndownload_path = './data/VeRi'\n\nif not os.path.isdir(download_path):\n    print('please change the download_path')\n\nsave_path = download_path + '/image_all'\ntrain_path = download_path + '/image_train'\ngallery_path = download_path + '/image_test'\nquery_path  = download_path + '/image_query'\n\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n\n    copy_file(train_path, save_path)\n    copy_file(query_path, save_path)\n    copy_file(gallery_path,  save_path)\n\n#---------------------------------------\n#train\nos.mkdir(download_path + '/pytorch')\ntrain_save_path = download_path + '/pytorch/train'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\n    for root, dirs, files in os.walk(train_path, topdown=True):\n        for name in files:\n            if not name[-3:]=='jpg':\n                continue\n            ID  = name.split('_')\n            src_path = train_path + '/' + name\n            dst_path = train_save_path + '/v' + ID[0]\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            copyfile(src_path, dst_path + '/' + name)\n\n#---------------------------------------\n#query\ntrain_path = query_path\ntrain_save_path = download_path + '/pytorch/query'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\n    for root, dirs, files in os.walk(train_path, topdown=True):\n        for name in files:\n            if not name[-3:]=='jpg':\n                continue\n            ID  = name.split('_')\n            src_path = train_path + '/' + name\n            dst_path = train_save_path + '/v' + ID[0]\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            copyfile(src_path, dst_path + '/' + name)\n\n#---------------------------------------\n#gallery\ntrain_path = gallery_path\ntrain_save_path = download_path + '/pytorch/gallery'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\n    for root, dirs, files in os.walk(train_path, topdown=True):\n        for name in files:\n            if not name[-3:]=='jpg':\n                continue\n            ID  = name.split('_')\n            src_path = train_path + '/' + name\n            dst_path = train_save_path + '/v' + ID[0]\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            copyfile(src_path, dst_path + '/' + name)\n\n#---------------------------------------\n#train_all\ntrain_path = save_path\ntrain_save_path = download_path + '/train_all'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\n    for root, dirs, files in os.walk(train_path, topdown=True):\n        for name in files:\n            if not name[-3:]=='jpg':\n                continue\n            ID  = name.split('_')\n            src_path = train_path + '/' + name\n            dst_path = train_save_path + '/v' + ID[0]\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            copyfile(src_path, dst_path + '/' + name)\n\n#train_veri_path =  './data/pytorch/train+veri'\n#original_train_save_path = './data/pytorch/train'\n#if not os.path.isdir(train_veri_path):\n#    os.system('rsync -r %s/ %s/'%(os.path.abspath(train_save_path) , os.path.abspath(train_veri_path) ) )\n#    os.system('rsync -r %s/ %s/'%(os.path.abspath(original_train_save_path) , os.path.abspath(train_veri_path) ) )\n"
        },
        {
          "name": "prepare_VehicleID.py",
          "type": "blob",
          "size": 2.921875,
          "content": "import os\nfrom shutil import copyfile\n\ndef copy_file(s, t):\n    for root, dirs, files in os.walk(s):\n        for name in files:\n            copyfile(root+'/'+name,t+'/'+name)\n\n# You only need to change this line to your dataset download path\ndownload_path = './data/VehicleID_V1.0/'\n\nif not os.path.isdir(download_path):\n    print('please change the download_path')\n\n#---------------------------------------\n#train_all\ntrain_path = download_path + '/image'\ntrain_save_path = download_path + '/pytorch/train_test'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\n    fname = './data/VehicleID_V1.0/attribute/img2vid.txt'\n    with open(fname) as fp:\n        for i, line in enumerate(fp):\n            name, label = line.split(' ')\n            name = name + '.jpg'\n            ID  = int(label)\n            src_path = train_path + '/' + name\n            dst_path = train_save_path + '/p%d'%ID\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            print(src_path, dst_path)\n            copyfile( src_path, dst_path+'/'+name)\n\n#---------------------------------------\n#train\ntrain_list = []\ntrain_only_save_path = download_path + '/pytorch/train'\nif not os.path.isdir(train_only_save_path):\n    os.mkdir(train_only_save_path)\n    with open(download_path+'train_test_split/train_list.txt', 'r') as f:\n        for name in f:\n            name = name.replace('\\n','')\n            train_ID = name.split(' ')\n            train_ID = int(train_ID[1])\n            if not train_ID in train_list:\n                train_list.append(train_ID)\n\n        print(len(train_list))\n        for ID in train_list:\n            os.system('rsync -r %s/p%d %s'%( train_save_path, ID, train_only_save_path))\n\n#---------------------------------------\n#val800\nfor num in [800,1600,2400]:\n    val_list = []\n    query_save_path = download_path + '/pytorch/query%d'%num\n    gallery_save_path = download_path + '/pytorch/gallery%d'%num\n    if not os.path.isdir(query_save_path):\n        os.mkdir(query_save_path)\n        os.mkdir(gallery_save_path)\n    with open(download_path+'train_test_split/test_list_%d.txt'%num, 'r') as f:\n            for name in f:\n                name = name.replace('\\n','')\n                val_ID = name.split(' ')\n                val_name = val_ID[0] + '.jpg'\n                val_ID = int(val_ID[1])\n                src_path = train_path + '/' + val_name\n                if val_ID not in val_list:\n                    val_list.append(val_ID)\n                    dst_path = gallery_save_path + '/p%d'%val_ID #For VehicleID QueryNumber > Gallery\n                    if not os.path.isdir(dst_path):\n                        os.mkdir(dst_path)\n                    copyfile( src_path, dst_path+'/'+val_name)\n                else:\n                    dst_path = query_save_path + '/p%d'%val_ID\n                    if not os.path.isdir(dst_path):\n                        os.mkdir(dst_path)\n                    copyfile( src_path, dst_path+'/'+val_name)\n\n"
        },
        {
          "name": "prepare_static.py",
          "type": "blob",
          "size": 3.6708984375,
          "content": "# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nfrom torchvision import datasets, transforms\nimport time\nimport os\n\nversion =  torch.__version__\n\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description='Training')\nparser.add_argument('--data_dir',default='/home/zzd/Market/pytorch',type=str, help='training dir path')\nparser.add_argument('--train_all', action='store_true', help='use all training data' )\nparser.add_argument('--color_jitter', action='store_true', help='use color jitter in training' )\nparser.add_argument('--batchsize', default=128, type=int, help='batchsize')\nopt = parser.parse_args()\n\ndata_dir = opt.data_dir\n\n######################################################################\n# Load Data\n# ---------\n#\n\ntransform_train_list = [\n        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n        transforms.Resize((288,144), interpolation=3),\n        #transforms.RandomCrop((256,128)),\n        #transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\ntransform_val_list = [\n        transforms.Resize(size=(256,128),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\n\nprint(transform_train_list)\ndata_transforms = {\n    'train': transforms.Compose( transform_train_list ),\n    'val': transforms.Compose(transform_val_list),\n}\n\n\ntrain_all = ''\nif opt.train_all:\n     train_all = '_all'\n\nimage_datasets = {}\nimage_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train' + train_all),\n                                          data_transforms['train'])\nimage_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n                                          data_transforms['val'])\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=True, num_workers=16)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\nuse_gpu = torch.cuda.is_available()\n\n######################################################################\n# prepare_dataset\n# ------------------\n#\n# Now, let's write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\ndef prepare_model():\n    since = time.time()\n\n    num_epochs = 1\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train']:\n\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            # Iterate over data.\n            for data in dataloaders[phase]:\n                # get the inputs\n                inputs, labels = data\n                now_batch_size,c,h,w = inputs.shape\n                mean += torch.sum(torch.mean(torch.mean(inputs,dim=3),dim=2),dim=0)\n                std += torch.sum(torch.std(inputs.view(now_batch_size,c,h*w),dim=2),dim=0)\n                \n            print(mean/dataset_sizes['train'])\n            print(std/dataset_sizes['train'])\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    return \n\n\n\nprepare_model()\n"
        },
        {
          "name": "prepare_viper.py",
          "type": "blob",
          "size": 2.4423828125,
          "content": "import os\nfrom shutil import copyfile\nimport numpy as np\n\n\n#http://users.soe.ucsc.edu/~manduchi/VIPeR.v1.0.zip\ndownload_path = '../VIPeR'\n\nif not os.path.isdir(download_path):\n    print('please change the download_path')\n\nsave_path = download_path + '/pytorch'\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n\n\n#train_all\nID_list = []\ntrain_path = download_path + '/cam_a'\ntrain_save_path = download_path + '/pytorch/all'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='bmp':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n            ID_list.append(ID[0])\n        copyfile(src_path, dst_path + '/' + 'ca_'+name)\n\n\ntrain_path = download_path + '/cam_b'\ntrain_save_path = download_path + '/pytorch/all'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='bmp':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + 'cb_'+name)\n\nnp.random.seed(0)\n\nfor ii in range(1):\n    index = np.random.permutation(632)\n    index = index[0:316]\n    test_save_path = download_path + '/pytorch/' #'/test%d'%ii\n    if not os.path.isdir(test_save_path):\n        os.mkdir(test_save_path)\n    os.mkdir(test_save_path + '/train')\n    os.mkdir(test_save_path + '/query')\n    os.mkdir(test_save_path + '/gallery')\n\n    for i in range(632):\n        dir_name = ID_list[i]\n        if i in index:\n            src_path_1 = train_save_path + '/' + dir_name\n            dst_path_1 = test_save_path+'/train/'\n            os.system('cp -r  %s %s'%(src_path_1, dst_path_1))\n        else:\n            src_path_1 = train_save_path + '/' + dir_name #+ '/ca_*.bmp'\n            #src_path_2 = train_save_path + '/' + dir_name + '/cb_*.bmp'\n            dst_path_1 = test_save_path+'/query/'\n            #dst_path_2 = test_save_path+'/gallery/' + dir_name\n            os.system('cp -r %s %s'%(src_path_1, dst_path_1))\n\nos.system('cp -r %s  %s'%(test_save_path+'/query/*', test_save_path+'/gallery/'))\n\n\n"
        },
        {
          "name": "random_erasing.py",
          "type": "blob",
          "size": 3.9345703125,
          "content": "from __future__ import absolute_import\n\n#from torchvision.transforms import *\n\n#from PIL import Image\nimport random\nimport math\n#import numpy as np\n\nclass RandomErasing(object):\n    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n    Args:\n         probability: The probability that the Random Erasing operation will be performed.\n         sl: Minimum proportion of erased area against input image.\n         sh: Maximum proportion of erased area against input image.\n         r1: Minimum aspect ratio of erased area.\n         mean: Erasing value. \n    \"\"\"\n    \n    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n       \n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                return img\n\n        return img\n\n\nclass RandomGrayscaleErasing(object):\n    \"\"\" Randomly selects a rectangle region in an image and use grayscale image\n        instead of its pixels.\n        'Local Grayscale Transfomation' by Yunpeng Gong.\n        See https://arxiv.org/pdf/2101.08533.pdf\n    Args:\n         probability: The probability that the Random Grayscale Erasing operation will be performed.\n         sl: Minimum proportion of erased area against input image.\n         sh: Maximum proportion of erased area against input image.\n         r1: Minimum aspect ratio of erased area.\n    \"\"\"\n\n    def __init__(self, probability: float = 0.2, sl: float = 0.02, sh: float = 0.4, r1: float = 0.3):\n        self.probability = probability\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img: after ToTensor() and Normalize([...]), img's type is Tensor\n        \"\"\"\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        height, width = img.size()[-2], img.size()[-1]\n        area = height * width\n\n        for _ in range(100):\n\n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)  # height / width\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < width and h < height:\n                # tl\n                x = random.randint(0, height - h)\n                y = random.randint(0, width - w)\n                # unbind channel dim\n                r, g, b = img.unbind(dim=-3)\n                # Weighted average method -> grayscale patch\n                l_img = (0.2989 * r + 0.587 * g + 0.114 * b).to(img.dtype)\n                l_img = l_img.unsqueeze(dim=-3)  # rebind channel\n                # erasing\n                img[0, y:y + h, x:x + w] = l_img[0, y:y + h, x:x + w]\n                img[1, y:y + h, x:x + w] = l_img[0, y:y + h, x:x + w]\n                img[2, y:y + h, x:x + w] = l_img[0, y:y + h, x:x + w]\n\n                return img\n\n        return img\n"
        },
        {
          "name": "re_ranking.py",
          "type": "blob",
          "size": 4.1865234375,
          "content": "#!/usr/bin/env python2/python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Jun 26 14:46:56 2017\n@author: luohao\nModified by Houjing Huang, 2017-12-22. \n- This version accepts distance matrix instead of raw features. \n- The difference of `/` division between python 2 and 3 is handled.\n- numpy.float16 is replaced by numpy.float32 for numerical precision.\n\nModified by Zhedong Zheng, 2018-1-12.\n- replace sort with topK, which save about 30s.\n\"\"\"\n\n\"\"\"\nCVPR2017 paper:Zhong Z, Zheng L, Cao D, et al. Re-ranking Person Re-identification with k-reciprocal Encoding[J]. 2017.\nurl:http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf\nMatlab version: https://github.com/zhunzhong07/person-re-ranking\n\"\"\"\n\n\"\"\"\nAPI\nq_g_dist: query-gallery distance matrix, numpy array, shape [num_query, num_gallery]\nq_q_dist: query-query distance matrix, numpy array, shape [num_query, num_query]\ng_g_dist: gallery-gallery distance matrix, numpy array, shape [num_gallery, num_gallery]\nk1, k2, lambda_value: parameters, the original paper is (k1=20, k2=6, lambda_value=0.3)\nReturns:\n  final_dist: re-ranked distance, numpy array, shape [num_query, num_gallery]\n\"\"\"\n\n\nimport numpy as np\n\ndef k_reciprocal_neigh( initial_rank, i, k1):\n    forward_k_neigh_index = initial_rank[i,:k1+1]\n    backward_k_neigh_index = initial_rank[forward_k_neigh_index,:k1+1]\n    fi = np.where(backward_k_neigh_index==i)[0]\n    return forward_k_neigh_index[fi]\n\ndef re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=20, k2=6, lambda_value=0.3):\n    # The following naming, e.g. gallery_num, is different from outer scope.\n    # Don't care about it.\n    original_dist = np.concatenate(\n      [np.concatenate([q_q_dist, q_g_dist], axis=1),\n       np.concatenate([q_g_dist.T, g_g_dist], axis=1)],\n      axis=0)\n    original_dist = 2. - 2 * original_dist   # change the cosine similarity metric to euclidean similarity metric\n    original_dist = np.power(original_dist, 2).astype(np.float32)\n    original_dist = np.transpose(1. * original_dist/np.max(original_dist,axis = 0))\n    V = np.zeros_like(original_dist).astype(np.float32)\n    #initial_rank = np.argsort(original_dist).astype(np.int32)\n    # top K1+1\n    initial_rank = np.argpartition( original_dist, range(1,k1+1) )\n\n    query_num = q_g_dist.shape[0]\n    all_num = original_dist.shape[0]\n\n    for i in range(all_num):\n        # k-reciprocal neighbors\n        k_reciprocal_index = k_reciprocal_neigh( initial_rank, i, k1)\n        k_reciprocal_expansion_index = k_reciprocal_index\n        for j in range(len(k_reciprocal_index)):\n            candidate = k_reciprocal_index[j]\n            candidate_k_reciprocal_index = k_reciprocal_neigh( initial_rank, candidate, int(np.around(k1/2)))\n            if len(np.intersect1d(candidate_k_reciprocal_index,k_reciprocal_index))> 2./3*len(candidate_k_reciprocal_index):\n                k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index,candidate_k_reciprocal_index)\n\n        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\n        weight = np.exp(-original_dist[i,k_reciprocal_expansion_index])\n        V[i,k_reciprocal_expansion_index] = 1.*weight/np.sum(weight)\n\n    original_dist = original_dist[:query_num,]\n    if k2 != 1:\n        V_qe = np.zeros_like(V,dtype=np.float32)\n        for i in range(all_num):\n            V_qe[i,:] = np.mean(V[initial_rank[i,:k2],:],axis=0)\n        V = V_qe\n        del V_qe\n    del initial_rank\n    invIndex = []\n    for i in range(all_num):\n        invIndex.append(np.where(V[:,i] != 0)[0])\n\n    jaccard_dist = np.zeros_like(original_dist,dtype = np.float32)\n\n    for i in range(query_num):\n        temp_min = np.zeros(shape=[1,all_num],dtype=np.float32)\n        indNonZero = np.where(V[i,:] != 0)[0]\n        indImages = []\n        indImages = [invIndex[ind] for ind in indNonZero]\n        for j in range(len(indNonZero)):\n            temp_min[0,indImages[j]] = temp_min[0,indImages[j]]+ np.minimum(V[i,indNonZero[j]],V[indImages[j],indNonZero[j]])\n        jaccard_dist[i] = 1-temp_min/(2.-temp_min)\n\n    final_dist = jaccard_dist*(1-lambda_value) + original_dist*lambda_value\n    del original_dist\n    del V\n    del jaccard_dist\n    final_dist = final_dist[:query_num,query_num:]\n    return final_dist\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1083984375,
          "content": "pyyaml\npretrainedmodels\ntimm >= 0.6.0\nscipy\nefficientnet_pytorch\npytorch_metric_learning\ntqdm\ngdown\nmatplotlib\n"
        },
        {
          "name": "show.png",
          "type": "blob",
          "size": 213.5029296875,
          "content": null
        },
        {
          "name": "sitemap.xml",
          "type": "blob",
          "size": 79.904296875,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<urlset\r\n      xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\r\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n      xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9\r\n            http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\">\r\n<!-- created with Free Online Sitemap Generator www.xml-sitemaps.com -->\r\n\r\n\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>1.00</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/f02a01f902a0df5de064d440e18e3674f77966fc</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e4d1b66cf611fc2aae53b398a5f02fa22389916a</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/3065c3a157c852b6b03aaf3193003d37d5d75c00</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/211752580b6fce638baf04f7681e69de6f50001c</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/.gitignore</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/9cfde137ee8061b60084531c859580b7b8bdff15</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/.travis.yml</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/6cbc2e6de7dbea526b23380c691938a9350a6b71</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/93ff4bf73a751e501f04a27955d15d643e38d413</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/2518daf0f6f7e194ee081349c29d02a8a86edece</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/circle_loss.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e9613bb1f814aafb8702d0ae5f9e16d6c466d52a</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/demo.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/629f1ca9ef22b297f3f1c3f03476d10dbac5f349</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/evaluate.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/7f8c3c1cb94d45e1b3a0014c3bd2bb58e7f6f9b9</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/evaluate_gpu.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/evaluate_rerank.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/a112bd746ced1f9dd43f7ca5b6eb1aaa214808ab</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/model.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/f5aa37f53b4a85fc728bf776114b9807a57efe5f</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/prepare.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/d3d8d84162add989496e208af4a3bb7c8c2faf37</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/prepare_static.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e9cc9a1eb25690b5582346e8797445d860c9f5ba</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/prepare_viper.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/394db7b4ef0a2801be7b7c5bb6df022498461296</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/random_erasing.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/re_ranking.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/63bff64ef182f5737b58c7fa165641a22cd22ef8</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/abac99ce450faca6966c065a934f861990291566</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/test.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/10c515887abc84b1ece01f8227c4b7ef8b5b96c4</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/train.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/a17698464d255d9c451bf70fa52a990fa402ffda</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/dev/visual_heatmap.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/GPU-Re-Ranking</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/107</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.80</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/259</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/258</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/257</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/256</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/254</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/253</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/248</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/246</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/245</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/243</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/242</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/241</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/240</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/239</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/238</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/237</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/236</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/234</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/233</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/232</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/231</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/230</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/226</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/225</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/224</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/compare</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/pull/244</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/security/policy</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/security/advisories</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/f02a01f902a0df5de064d440e18e3674f77966fc?branch=f02a01f902a0df5de064d440e18e3674f77966fc&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/f02a01f902a0df5de064d440e18e3674f77966fc?branch=f02a01f902a0df5de064d440e18e3674f77966fc&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/c346969a9f8c7e8ef2fb4cf50ec149cb450eeeb6</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e4d1b66cf611fc2aae53b398a5f02fa22389916a?branch=e4d1b66cf611fc2aae53b398a5f02fa22389916a&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e4d1b66cf611fc2aae53b398a5f02fa22389916a?branch=e4d1b66cf611fc2aae53b398a5f02fa22389916a&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/4924e16bf8f3f0f0e0f2da4853d31968262a260b</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/3065c3a157c852b6b03aaf3193003d37d5d75c00?branch=3065c3a157c852b6b03aaf3193003d37d5d75c00&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/3065c3a157c852b6b03aaf3193003d37d5d75c00?branch=3065c3a157c852b6b03aaf3193003d37d5d75c00&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/76cadb8b32332ac19611d748780dbcb426792bdf</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/211752580b6fce638baf04f7681e69de6f50001c?branch=211752580b6fce638baf04f7681e69de6f50001c&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/211752580b6fce638baf04f7681e69de6f50001c?branch=211752580b6fce638baf04f7681e69de6f50001c&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/.gitignore</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/ba979ab1888799e79fa8684427865a1813334066</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/9cfde137ee8061b60084531c859580b7b8bdff15?branch=9cfde137ee8061b60084531c859580b7b8bdff15&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/9cfde137ee8061b60084531c859580b7b8bdff15?branch=9cfde137ee8061b60084531c859580b7b8bdff15&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/.travis.yml</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/ea4b65ef3214eb5e845b5771d4d452753d18b44a</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/6cbc2e6de7dbea526b23380c691938a9350a6b71?branch=6cbc2e6de7dbea526b23380c691938a9350a6b71&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/6cbc2e6de7dbea526b23380c691938a9350a6b71?branch=6cbc2e6de7dbea526b23380c691938a9350a6b71&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/1bea13e2739777b8f0510eec99460bd988689ed9</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/93ff4bf73a751e501f04a27955d15d643e38d413?branch=93ff4bf73a751e501f04a27955d15d643e38d413&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/93ff4bf73a751e501f04a27955d15d643e38d413?branch=93ff4bf73a751e501f04a27955d15d643e38d413&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/77a10a2c88fade1272e52f4666e51ca31225d5de</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/2518daf0f6f7e194ee081349c29d02a8a86edece?branch=2518daf0f6f7e194ee081349c29d02a8a86edece&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/2518daf0f6f7e194ee081349c29d02a8a86edece?branch=2518daf0f6f7e194ee081349c29d02a8a86edece&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/circle_loss.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e9613bb1f814aafb8702d0ae5f9e16d6c466d52a?branch=e9613bb1f814aafb8702d0ae5f9e16d6c466d52a&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e9613bb1f814aafb8702d0ae5f9e16d6c466d52a?branch=e9613bb1f814aafb8702d0ae5f9e16d6c466d52a&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/demo.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/2d5b5b9d3901e61e85fe4295a76b52134808c5bb</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/629f1ca9ef22b297f3f1c3f03476d10dbac5f349?branch=629f1ca9ef22b297f3f1c3f03476d10dbac5f349&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/629f1ca9ef22b297f3f1c3f03476d10dbac5f349?branch=629f1ca9ef22b297f3f1c3f03476d10dbac5f349&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/evaluate.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/cc35dcbd19f6cdc1f37c681b57701ee1c98119b4</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/7f8c3c1cb94d45e1b3a0014c3bd2bb58e7f6f9b9?branch=7f8c3c1cb94d45e1b3a0014c3bd2bb58e7f6f9b9&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/7f8c3c1cb94d45e1b3a0014c3bd2bb58e7f6f9b9?branch=7f8c3c1cb94d45e1b3a0014c3bd2bb58e7f6f9b9&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/evaluate_gpu.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/evaluate_rerank.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/ad8c84d44b43c324a7f5c388bec11c102ab03407</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/a112bd746ced1f9dd43f7ca5b6eb1aaa214808ab?branch=a112bd746ced1f9dd43f7ca5b6eb1aaa214808ab&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/a112bd746ced1f9dd43f7ca5b6eb1aaa214808ab?branch=a112bd746ced1f9dd43f7ca5b6eb1aaa214808ab&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/model.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/0cd1d712a635c625c543a0b456391fb383ea76ec</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/f5aa37f53b4a85fc728bf776114b9807a57efe5f?branch=f5aa37f53b4a85fc728bf776114b9807a57efe5f&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/f5aa37f53b4a85fc728bf776114b9807a57efe5f?branch=f5aa37f53b4a85fc728bf776114b9807a57efe5f&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/prepare.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e11c9248321c0fb311d40575057abffa1224de9f</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/d3d8d84162add989496e208af4a3bb7c8c2faf37?branch=d3d8d84162add989496e208af4a3bb7c8c2faf37&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/d3d8d84162add989496e208af4a3bb7c8c2faf37?branch=d3d8d84162add989496e208af4a3bb7c8c2faf37&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/prepare_static.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/f0475048d6d59fa83e9d7319b222c6af17822b37</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e9cc9a1eb25690b5582346e8797445d860c9f5ba?branch=e9cc9a1eb25690b5582346e8797445d860c9f5ba&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/e9cc9a1eb25690b5582346e8797445d860c9f5ba?branch=e9cc9a1eb25690b5582346e8797445d860c9f5ba&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/prepare_viper.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/f37bde8a86c859e9f9a57f388bba005f65bcfaa2</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/394db7b4ef0a2801be7b7c5bb6df022498461296?branch=394db7b4ef0a2801be7b7c5bb6df022498461296&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/394db7b4ef0a2801be7b7c5bb6df022498461296?branch=394db7b4ef0a2801be7b7c5bb6df022498461296&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/random_erasing.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/re_ranking.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/104</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/a253943e16abd94277ee832a950b975df75b11e3</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/63bff64ef182f5737b58c7fa165641a22cd22ef8?branch=63bff64ef182f5737b58c7fa165641a22cd22ef8&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/63bff64ef182f5737b58c7fa165641a22cd22ef8?branch=63bff64ef182f5737b58c7fa165641a22cd22ef8&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/b24b82c946187124196c68bcb50ff6e16b6000be</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/abac99ce450faca6966c065a934f861990291566?branch=abac99ce450faca6966c065a934f861990291566&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/abac99ce450faca6966c065a934f861990291566?branch=abac99ce450faca6966c065a934f861990291566&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/test.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/4856c8e109003f10aafb43608cfa544607586ff2</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/10c515887abc84b1ece01f8227c4b7ef8b5b96c4?branch=10c515887abc84b1ece01f8227c4b7ef8b5b96c4&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/10c515887abc84b1ece01f8227c4b7ef8b5b96c4?branch=10c515887abc84b1ece01f8227c4b7ef8b5b96c4&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/f02a01f902a0df5de064d440e18e3674f77966fc/train.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/9a649c21448203d893a5edee01fc6a3172af3cc6</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/a17698464d255d9c451bf70fa52a990fa402ffda?branch=a17698464d255d9c451bf70fa52a990fa402ffda&amp;diff=unified</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/a17698464d255d9c451bf70fa52a990fa402ffda?branch=a17698464d255d9c451bf70fa52a990fa402ffda&amp;diff=split</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/36fd6dc247650efb2c3b67679546656615232aa6/visual_heatmap.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/find/dev</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/3248a07210043a08659ff03baad9a6934e229f66</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/blob/master/gpu_compile.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/blob/master/prepare_data.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/blob/master/resnet52_market_2.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_matconvnet/blob/master/train_id_net_res_market_baseline.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/GPU-Re-Ranking/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/GPU-Re-Ranking/evaluate_rerank_gpu.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/94e3c7fd42466816b6c5c49cc83214aa20ecd008</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/GPU-Re-Ranking/gnn_reranking.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/a8737ded2dfe3eb29e7c1afa64d41e7bffca0af3</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/GPU-Re-Ranking/utils.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/commit/93bc1a12b9b9ca0a23f5e98739373ed35f693e64</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/commit/46004576f9674a53465fe164b30a98c261523fd5</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/DGCNN.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/KNNGraphE.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/evaluate_gpu.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/gated_gcn_layer.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/market3d.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/model.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/model_efficient.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/model_efficient2.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/pointnet2_model.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/prepare_MSMT.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/prepare_duke.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/prepare_market.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/ptflops.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/requirements.txt</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/swa_utils.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/test_M.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/test_MSMT.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/test_P.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/test_vip.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/train_M.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/person-reid-3d/blob/master/utils.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/hmr</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/dgcnn</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/blob/master/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/commit/bbeacd8778b7c7b09018a82e59d8831358e7f39f</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/commit/96dc0e52177ab67fabb73424cdaebde5f6b6c76e</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/commit/745cf74bef1baa54c8fe2de4f649566e73e418f8</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/commit/13bd3d0040ea579686ff4e5e029067caefde38fd</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/commit/d5d968fc1d5529df6899bc00cb996b667a9d5543</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/blob/master/paper.pdf</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/AICIty-reID-2020/commit/6df0a32ebb820d3a8151e47f5907ab888fa91442</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Vehicle_reID-Collection</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/d80c7d8fbf7a1dfaf61e1b7df9fdf0b7446a59dd</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/pull/15</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/30f7ce3c5307c549e42a7392f7a7859cfbcb0adf</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/8b501719393b0ec0b33ddf0805dd92144c3bf16b</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/3421c4f14ac4f814cfa86ecc2ed2a2b6b2bed36f</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/d0105155c0e74c00689abe1679c98070e3d61cb7</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/56b3491933ac0e31ec5481d37c9472125cec0ffa</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/8c7ac25a1703e2f8c3440d72fd4346d6741ee161</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/.gitignore</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/f95776d01aac616a493dbe1681a21641f7a5a306</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/4f2038d1f99ef96b5ebf5a1f569c18ab1c3928a7</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/Request.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/3eb604a65388e1bbd118df00cae156ee8eb607f9</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/autoaugment.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/a9eb831a5c36c134cd8403c295b52e786afe39db</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/demo.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/demo_4K.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/evaluate_gpu.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/b5fe72db4b7c52dca4ee40a45e9f52afd6b3d908</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/gallery_name.txt</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/model.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/prepare_cvusa.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/prepare_limit_view.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/query_name.txt</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/random_erasing.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/dec0e181553bf3c50fbbf421eb6636b76423826d</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/show_data.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/test.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/test_4K.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/test_ImageNet.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/cf73c42f976479fdb20ba4633b9689f0da0feb0e</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/test_Place365.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/test_cvusa.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/test_train.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/commit/92976524cb2d66ba16331da668cf258c4c5b34ea</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/train.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/train_18_sample.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/train_1_sample.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/train_27_sample.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/train_3_sample.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/train_9_sample.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/train_cvusa.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/train_no_street.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/blob/master/utils.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-Baseline/tree/master/GPU-Re-Ranking</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/cnnimageretrieval-pytorch/blob/master/cirtorch/examples/test_My1652model.py</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/University1652-triplet-loss</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/issues/142</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/.DS_Store</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/concat_2net.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/gpu_compile.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/prepare_data.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/resnet52_market.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/resnet52_market_stn_alignment.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/train_id_net_res_market_align.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Pedestrian_Alignment/blob/master/train_id_net_res_market_new.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/.DS_Store</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/Makefile</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/concat_2net.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/gpu_compile.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/market1501-train-curve.pdf</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/prepare_data.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/rand_diff_class.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/rand_same_class.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/resnet52_2stream.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/resnet52_market.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/start-zzd.sh</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/2016_person_re-ID/blob/master/train_id_net_res_2stream.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID-verification</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/0d8b27ac63d8760d545cf9e50fc3c71ea1e4246a</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/06eaa8941ff11eeb2059a816121c7fd4ce47cd76</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/21b8d2d1835f06d950fc044ae232ed06bae22e1c</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/cc264dea7e3bbd5627612aff439a08d6604952f8</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/17607bbd7bf11bf83a6ff7ecded3c5534e6a2bdd</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/b5ae504ab4c01b40aff62ad0d868ec22cc534ce3</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/concat_2net.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/170c172480ed5a00bd6441d56c1f3e20161dda29</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/1f1ab3c00e0ca7b5226234e148fdd7dad4824980</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/gpu_compile.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/prepare_data.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/96cef2824ada840cf6269dc99136681410212319</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/prepare_gan_data.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/0a643b979fb5986b29d6ec165536f9df405380ec</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/resnet52_2stream_gan.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/e17ae2b6f69ee71dcc15845be2a24c0f2d85ed31</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/resnet52_market.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/0784ee0fcd46296f43c2c7f03214f52d2c2bcb29</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/resnet52_market_K_1.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/3febff01b7d5a8c2963c1cea1372762780df5676</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/resnet52_market_gan.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/f3b083454695b172b65a9690fdd89bf90a53828a</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/resnet52_market_pseudo.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/support.pdf</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/cd9178b532e5cd4279172b4a5275def1f8d07564</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/train_id_net_res_2stream_gan.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/a921b5e6361856fbe2f67aabd9a1e88666484e31</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/train_id_net_res_market_K_1.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/1755986dbb0e47e37498c1ee4cd4e051d4406c06</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/train_id_net_res_market_lsro.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/train_id_net_res_market_new.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/commit/2bc18d1251bb54b1a45fc4dde726894e6ae70d9f</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/train_id_net_res_market_pseudo.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/update_pseudo.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/DCGAN-tensorflow</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/matlab/vl_nnloss.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID_GAN/blob/master/matlab/%2Bdagnn/Pseudo_Loss.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/LICENSE</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/issues</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/pulls</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/actions</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/projects</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/security</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/find/master</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/commit/e3d844490dc7d6f1075b0ae0c5f95ef10ac47509</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/README.md</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/coco_word2_Rankloss.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/coco_word2_Rankloss_vgg19.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/coco_word2_pool.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/coco_word2_pool_vgg19.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/concat_2net.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/cuhk_word2_Rankloss.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/cuhk_word2_Rankloss_vgg16.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/cuhk_word2_pool.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/cuhk_word2_pool_vgg16.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/gpu_compile.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/rand_diff_class.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/rand_diff_class2.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/rand_diff_class3.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/rand_same_class.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/rand_same_class_coco.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/resnet52_new_hope_word2_pool.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/resnet52_new_hope_word2_pool_vgg19.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/resnet52_new_hope_word_Rankloss.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/resnet52_new_hope_word_Rankloss_vgg19.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_coco_Rankloss_shift_hard.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_coco_Rankloss_shift_hard_vgg19.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_coco_word2_1_pool.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_coco_word2_1_pool_vgg19.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_cuhk_Rankloss_shift.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_cuhk_Rankloss_shift_vgg16.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_cuhk_word2_1_pool.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_cuhk_word2_1_pool_vgg16.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_flickr_word2_1_pool.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_flickr_word2_1_pool_vgg19.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_flickr_word_Rankloss_shift_hard.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Image-Text-Embedding/blob/master/train_flickr_word_Rankloss_shift_hard_vgg19.m</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.64</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/find/f02a01f902a0df5de064d440e18e3674f77966fc</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels?sort=name-asc</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels?sort=name-desc</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels?sort=count-desc</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels?sort=count-asc</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels/bug</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels/duplicate</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels/enhancement</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels/good%20first%20issue</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels/help%20wanted</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels/invalid</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels/question</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/labels/wontfix</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?state=closed</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?direction=desc&amp;sort=due_date&amp;state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?direction=asc&amp;sort=due_date&amp;state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?direction=asc&amp;sort=completeness&amp;state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?direction=desc&amp;sort=completeness&amp;state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?direction=asc&amp;sort=title&amp;state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?direction=desc&amp;sort=title&amp;state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?direction=desc&amp;sort=count&amp;state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person_reID_baseline_pytorch/milestones?direction=asc&amp;sort=count&amp;state=open</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\n<url>\n  <loc>https://github.com/layumi/Person-reID-triplet-loss</loc>\n  <lastmod>2021-03-15T02:00:35+00:00</lastmod>\n  <priority>0.51</priority>\n</url>\r\n\r\n\r\n</urlset>"
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 12.314453125,
          "content": "# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport time\nimport os\nimport scipy.io\nimport yaml\nimport math\nfrom tqdm import tqdm\nfrom model import ft_net, ft_net_dense, ft_net_hr, ft_net_swin, ft_net_swinv2, ft_net_efficient, ft_net_NAS, ft_net_convnext, PCB, PCB_test\nfrom utils import fuse_all_conv_bn\nversion =  torch.__version__\n#fp16\ntry:\n    from apex.fp16_utils import *\nexcept ImportError: # will be 3.x series\n    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n\n######################################################################\n# Options\n# --------\n\nparser = argparse.ArgumentParser(description='Test')\nparser.add_argument('--gpu_ids',default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\nparser.add_argument('--which_epoch',default='last', type=str, help='0,1,2,3...or last')\nparser.add_argument('--test_dir',default='../Market/pytorch',type=str, help='./test_data')\nparser.add_argument('--name', default='ft_ResNet50', type=str, help='save model path')\nparser.add_argument('--batchsize', default=256, type=int, help='batchsize')\nparser.add_argument('--linear_num', default=512, type=int, help='feature dimension: 512 or default or 0 (linear=False)')\nparser.add_argument('--use_dense', action='store_true', help='use densenet121' )\nparser.add_argument('--use_efficient', action='store_true', help='use efficient-b4' )\nparser.add_argument('--use_hr', action='store_true', help='use hr18 net' )\nparser.add_argument('--PCB', action='store_true', help='use PCB' )\nparser.add_argument('--multi', action='store_true', help='use multiple query' )\nparser.add_argument('--fp16', action='store_true', help='use fp16.' )\nparser.add_argument('--ibn', action='store_true', help='use ibn.' )\nparser.add_argument('--ms',default='1', type=str,help='multiple_scale: e.g. 1 1,1.1  1,1.1,1.2')\n\nopt = parser.parse_args()\n###load config###\n# load the training config\nconfig_path = os.path.join('./model',opt.name,'opts.yaml')\nwith open(config_path, 'r') as stream:\n        config = yaml.load(stream, Loader=yaml.FullLoader) # for the new pyyaml via 'conda install pyyaml'\nopt.fp16 = config['fp16'] \nopt.PCB = config['PCB']\nopt.use_dense = config['use_dense']\nopt.use_NAS = config['use_NAS']\nopt.stride = config['stride']\nif 'use_swin' in config:\n    opt.use_swin = config['use_swin']\nif 'use_swinv2' in config:\n    opt.use_swinv2 = config['use_swinv2']\nif 'use_convnext' in config:\n    opt.use_convnext = config['use_convnext']\nif 'use_efficient' in config:\n    opt.use_efficient = config['use_efficient']\nif 'use_hr' in config:\n    opt.use_hr = config['use_hr']\n\nif 'nclasses' in config: # tp compatible with old config files\n    opt.nclasses = config['nclasses']\nelse: \n    opt.nclasses = 751 \n\nif 'ibn' in config:\n    opt.ibn = config['ibn']\nif 'linear_num' in config:\n    opt.linear_num = config['linear_num']\n\nstr_ids = opt.gpu_ids.split(',')\n#which_epoch = opt.which_epoch\nname = opt.name\ntest_dir = opt.test_dir\n\ngpu_ids = []\nfor str_id in str_ids:\n    id = int(str_id)\n    if id >=0:\n        gpu_ids.append(id)\n\nprint('We use the scale: %s'%opt.ms)\nstr_ms = opt.ms.split(',')\nms = []\nfor s in str_ms:\n    s_f = float(s)\n    ms.append(math.sqrt(s_f))\n\n# set gpu ids\nif len(gpu_ids)>0:\n    torch.cuda.set_device(gpu_ids[0])\n    cudnn.benchmark = True\n\n######################################################################\n# Load Data\n# ---------\n#\n# We will use torchvision and torch.utils.data packages for loading the\n# data.\n#\nif opt.use_swin:\n    h, w = 224, 224\nelse:\n    h, w = 256, 128\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((h, w), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ############### Ten Crop        \n        #transforms.TenCrop(224),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.ToTensor()(crop) \n          #      for crop in crops]\n           # )),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(crop)\n          #       for crop in crops]\n          # ))\n])\n\nif opt.PCB:\n    data_transforms = transforms.Compose([\n        transforms.Resize((384,192), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n    ])\n    h, w = 384, 192\n\n\ndata_dir = test_dir\n\nif opt.multi:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query','multi-query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query','multi-query']}\nelse:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query']}\nclass_names = image_datasets['query'].classes\nuse_gpu = torch.cuda.is_available()\n\n######################################################################\n# Load model\n#---------------------------\ndef load_network(network):\n    save_path = os.path.join('./model',name,'net_%s.pth'%opt.which_epoch)\n    try:\n        network.load_state_dict(torch.load(save_path))\n    except: \n        if torch.cuda.get_device_capability()[0]>6 and len(opt.gpu_ids)==1 and int(version[0])>1: # should be >=7\n            print(\"Compiling model...\")\n            # https://huggingface.co/docs/diffusers/main/en/optimization/torch2.0\n            torch.set_float32_matmul_precision('high')\n            network = torch.compile(network, mode=\"default\", dynamic=True) # pytorch 2.0\n        network.load_state_dict(torch.load(save_path))\n\n    return network\n\n\n######################################################################\n# Extract feature\n# ----------------------\n#\n# Extract feature from  a trained model.\n#\ndef fliplr(img):\n    '''flip horizontal'''\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\ndef extract_feature(model,dataloaders):\n    #features = torch.FloatTensor()\n    # count = 0\n    pbar = tqdm()\n    if opt.linear_num <= 0:\n        if opt.use_swin or opt.use_swinv2 or opt.use_dense or opt.use_convnext:\n            opt.linear_num = 1024\n        elif opt.use_efficient:\n            opt.linear_num = 1792\n        elif opt.use_NAS:\n            opt.linear_num = 4032\n        else:\n            opt.linear_num = 2048\n\n    for iter, data in enumerate(dataloaders):\n        img, label = data\n        n, c, h, w = img.size()\n        # count += n\n        # print(count)\n        pbar.update(n)\n        ff = torch.FloatTensor(n,opt.linear_num).zero_().cuda()\n\n        if opt.PCB:\n            ff = torch.FloatTensor(n,2048,6).zero_().cuda() # we have six parts\n\n        for i in range(2):\n            if(i==1):\n                img = fliplr(img)\n            input_img = Variable(img.cuda())\n            for scale in ms:\n                if scale != 1:\n                    # bicubic is only  available in pytorch>= 1.1\n                    input_img = nn.functional.interpolate(input_img, scale_factor=scale, mode='bicubic', align_corners=False)\n                outputs = model(input_img) \n                ff += outputs\n        # norm feature\n        if opt.PCB:\n            # feature size (n,2048,6)\n            # 1. To treat every part equally, I calculate the norm for every 2048-dim part feature.\n            # 2. To keep the cosine score==1, sqrt(6) is added to norm the whole feature (2048*6).\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True) * np.sqrt(6) \n            ff = ff.div(fnorm.expand_as(ff))\n            ff = ff.view(ff.size(0), -1)\n        else:\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n            ff = ff.div(fnorm.expand_as(ff))\n\n        \n        if iter == 0:\n            features = torch.FloatTensor( len(dataloaders.dataset), ff.shape[1])\n        #features = torch.cat((features,ff.data.cpu()), 0)\n        start = iter*opt.batchsize\n        end = min( (iter+1)*opt.batchsize, len(dataloaders.dataset))\n        features[ start:end, :] = ff\n    pbar.close()\n    return features\n\ndef get_id(img_path):\n    camera_id = []\n    labels = []\n    for path, v in img_path:\n        #filename = path.split('/')[-1]\n        filename = os.path.basename(path)\n        label = filename[0:4]\n        camera = filename.split('c')[1]\n        if label[0:2]=='-1':\n            labels.append(-1)\n        else:\n            labels.append(int(label))\n        camera_id.append(int(camera[0]))\n    return camera_id, labels\n\ngallery_path = image_datasets['gallery'].imgs\nquery_path = image_datasets['query'].imgs\n\ngallery_cam,gallery_label = get_id(gallery_path)\nquery_cam,query_label = get_id(query_path)\n\nif opt.multi:\n    mquery_path = image_datasets['multi-query'].imgs\n    mquery_cam,mquery_label = get_id(mquery_path)\n\n######################################################################\n# Load Collected data Trained model\nprint('-------test-----------')\nif opt.use_dense:\n    model_structure = ft_net_dense(opt.nclasses, stride = opt.stride, linear_num=opt.linear_num)\nelif opt.use_NAS:\n    model_structure = ft_net_NAS(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_swin:\n    model_structure = ft_net_swin(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_swinv2:\n    model_structure = ft_net_swinv2(opt.nclasses, (h,w),  linear_num=opt.linear_num)\nelif opt.use_convnext:\n    model_structure = ft_net_convnext(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_efficient:\n    model_structure = ft_net_efficient(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_hr:\n    model_structure = ft_net_hr(opt.nclasses, linear_num=opt.linear_num)\nelse:\n    model_structure = ft_net(opt.nclasses, stride = opt.stride, ibn = opt.ibn, linear_num=opt.linear_num)\n\nif opt.PCB:\n    model_structure = PCB(opt.nclasses)\n\n#if opt.fp16:\n#    model_structure = network_to_half(model_structure)\n\n\nmodel = load_network(model_structure)\n\n# Remove the final fc layer and classifier layer\nif opt.PCB:\n    #if opt.fp16:\n    #    model = PCB_test(model[1])\n    #else:\n        model = PCB_test(model)\nelse:\n    #if opt.fp16:\n        #model[1].model.fc = nn.Sequential()\n        #model[1].classifier = nn.Sequential()\n    #else:\n        model.classifier.classifier = nn.Sequential()\n\n# Change to test mode\nmodel = model.eval()\nif use_gpu:\n    model = model.cuda()\n\n\nprint('Here I fuse conv and bn for faster inference, and it does not work for transformers. Comment out this following line if you do not want to fuse conv&bn.')\nmodel = fuse_all_conv_bn(model)\n\n# We can optionally trace the forward method with PyTorch JIT so it runs faster.\n# To do so, we can call `.trace` on the reparamtrized module with dummy inputs\n# expected by the module.\n# Comment out this following line if you do not want to trace.\n#dummy_forward_input = torch.rand(opt.batchsize, 3, h, w).cuda()\n#model = torch.jit.trace(model, dummy_forward_input)\n\nprint(model)\n# Extract feature\nsince = time.time()\nwith torch.no_grad():\n    gallery_feature = extract_feature(model,dataloaders['gallery'])\n    query_feature = extract_feature(model,dataloaders['query'])\n    if opt.multi:\n        mquery_feature = extract_feature(model,dataloaders['multi-query'])\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.2f}s'.format(\n            time_elapsed // 60, time_elapsed % 60))\n# Save to Matlab for check\nresult = {'gallery_f':gallery_feature.numpy(),'gallery_label':gallery_label,'gallery_cam':gallery_cam,'query_f':query_feature.numpy(),'query_label':query_label,'query_cam':query_cam}\nscipy.io.savemat('pytorch_result.mat',result)\n\nprint(opt.name)\nresult = './model/%s/result.txt'%opt.name\nos.system('python evaluate_gpu.py | tee -a %s'%result)\n\nif opt.multi:\n    result = {'mquery_f':mquery_feature.numpy(),'mquery_label':mquery_label,'mquery_cam':mquery_cam}\n    scipy.io.savemat('multi_query.mat',result)\n"
        },
        {
          "name": "test_MSMT.py",
          "type": "blob",
          "size": 11.7490234375,
          "content": "# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport time\nimport os\nimport scipy.io\nimport yaml\nimport math\nfrom model import ft_net, ft_net_dense, ft_net_hr, ft_net_swin, ft_net_swinv2, ft_net_efficient, ft_net_NAS, ft_net_convnext, PCB, PCB_test\nfrom utils import fuse_all_conv_bn\n#fp16\ntry:\n    from apex.fp16_utils import *\nexcept ImportError: # will be 3.x series\n    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n######################################################################\n# Options\n# --------\n\nparser = argparse.ArgumentParser(description='Test')\nparser.add_argument('--gpu_ids',default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\nparser.add_argument('--which_epoch',default='last', type=str, help='0,1,2,3...or last')\nparser.add_argument('--test_dir',default='../MSMT/pytorch',type=str, help='./test_data')\nparser.add_argument('--name', default='ft_ResNet50', type=str, help='save model path')\nparser.add_argument('--batchsize', default=256, type=int, help='batchsize')\nparser.add_argument('--linear_num', default=512, type=int, help='feature dimension: 512 or default or 0 (linear=False)')\nparser.add_argument('--use_dense', action='store_true', help='use densenet121' )\nparser.add_argument('--use_efficient', action='store_true', help='use efficient-b4' )\nparser.add_argument('--use_hr', action='store_true', help='use hr18 net' )\nparser.add_argument('--PCB', action='store_true', help='use PCB' )\nparser.add_argument('--multi', action='store_true', help='use multiple query' )\nparser.add_argument('--fp16', action='store_true', help='use fp16.' )\nparser.add_argument('--ibn', action='store_true', help='use ibn.' )\nparser.add_argument('--ms',default='1', type=str,help='multiple_scale: e.g. 1 1,1.1  1,1.1,1.2')\n\nopt = parser.parse_args()\n###load config###\n# load the training config\nconfig_path = os.path.join('./model',opt.name,'opts.yaml')\nwith open(config_path, 'r') as stream:\n        config = yaml.load(stream, Loader=yaml.FullLoader) # for the new pyyaml via 'conda install pyyaml'\nopt.fp16 = config['fp16'] \nopt.PCB = config['PCB']\nopt.use_dense = config['use_dense']\nopt.use_NAS = config['use_NAS']\nopt.stride = config['stride']\nif 'use_swin' in config:\n    opt.use_swin = config['use_swin']\nif 'use_swinv2' in config:\n    opt.use_swinv2 = config['use_swinv2']\nif 'use_convnext' in config:\n    opt.use_convnext = config['use_convnext']\nif 'use_efficient' in config:\n    opt.use_efficient = config['use_efficient']\nif 'use_hr' in config:\n    opt.use_hr = config['use_hr']\n\nif 'nclasses' in config: # tp compatible with old config files\n    opt.nclasses = config['nclasses']\nelse: \n    opt.nclasses = 751 \n\nif 'ibn' in config:\n    opt.ibn = config['ibn']\nif 'linear_num' in config:\n    opt.linear_num = config['linear_num']\n\nstr_ids = opt.gpu_ids.split(',')\n#which_epoch = opt.which_epoch\nname = opt.name\ntest_dir = opt.test_dir\n\ngpu_ids = []\nfor str_id in str_ids:\n    id = int(str_id)\n    if id >=0:\n        gpu_ids.append(id)\n\nprint('We use the scale: %s'%opt.ms)\nstr_ms = opt.ms.split(',')\nms = []\nfor s in str_ms:\n    s_f = float(s)\n    ms.append(math.sqrt(s_f))\n\n# set gpu ids\nif len(gpu_ids)>0:\n    torch.cuda.set_device(gpu_ids[0])\n    cudnn.benchmark = True\n\n######################################################################\n# Load Data\n# ---------\n#\n# We will use torchvision and torch.utils.data packages for loading the\n# data.\n#\nif opt.use_swin:\n    h, w = 224, 224\nelse:\n    h, w = 256, 128\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((h, w), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ############### Ten Crop        \n        #transforms.TenCrop(224),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.ToTensor()(crop) \n          #      for crop in crops]\n           # )),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(crop)\n          #       for crop in crops]\n          # ))\n])\n\nif opt.PCB:\n    data_transforms = transforms.Compose([\n        transforms.Resize((384,192), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n    ])\n    h, w = 384, 192\n\n\ndata_dir = test_dir\n\nif opt.multi:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query','multi-query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query','multi-query']}\nelse:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query']}\nclass_names = image_datasets['query'].classes\nuse_gpu = torch.cuda.is_available()\n\n######################################################################\n# Load model\n#---------------------------\ndef load_network(network):\n    save_path = os.path.join('./model',name,'net_%s.pth'%opt.which_epoch)\n    network.load_state_dict(torch.load(save_path))\n    return network\n\n\n######################################################################\n# Extract feature\n# ----------------------\n#\n# Extract feature from  a trained model.\n#\ndef fliplr(img):\n    '''flip horizontal'''\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\ndef extract_feature(model,dataloaders):\n    #features = torch.FloatTensor()\n    count = 0\n    if opt.linear_num <= 0:\n        if opt.use_swin or opt.use_swinv2 or opt.use_dense or opt.use_convnext:\n            opt.linear_num = 1024\n        elif opt.use_efficient:\n            opt.linear_num = 1792\n        elif opt.use_NAS:\n            opt.linear_num = 4032\n        else:\n            opt.linear_num = 2048\n\n    for iter, data in enumerate(dataloaders):\n        img, label = data\n        n, c, h, w = img.size()\n        count += n\n        print(count)\n        ff = torch.FloatTensor(n,opt.linear_num).zero_().cuda()\n\n        if opt.PCB:\n            ff = torch.FloatTensor(n,2048,6).zero_().cuda() # we have six parts\n\n        for i in range(2):\n            if(i==1):\n                img = fliplr(img)\n            input_img = Variable(img.cuda())\n            for scale in ms:\n                if scale != 1:\n                    # bicubic is only  available in pytorch>= 1.1\n                    input_img = nn.functional.interpolate(input_img, scale_factor=scale, mode='bicubic', align_corners=False)\n                outputs = model(input_img) \n                ff += outputs\n        # norm feature\n        if opt.PCB:\n            # feature size (n,2048,6)\n            # 1. To treat every part equally, I calculate the norm for every 2048-dim part feature.\n            # 2. To keep the cosine score==1, sqrt(6) is added to norm the whole feature (2048*6).\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True) * np.sqrt(6) \n            ff = ff.div(fnorm.expand_as(ff))\n            ff = ff.view(ff.size(0), -1)\n        else:\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n            ff = ff.div(fnorm.expand_as(ff))\n\n        \n        if iter == 0:\n            features = torch.FloatTensor( len(dataloaders.dataset), ff.shape[1])\n        #features = torch.cat((features,ff.data.cpu()), 0)\n        start = iter*opt.batchsize\n        end = min( (iter+1)*opt.batchsize, len(dataloaders.dataset))\n        features[ start:end, :] = ff\n    return features\n\ndef get_id(img_path):\n    camera_id = []\n    labels = []\n    for path, v in img_path:\n        #filename = path.split('/')[-1]\n        filename = os.path.basename(path)\n        label = filename[0:4]\n        camera = filename.split('_')[2][0:2]\n        if label[0:2]=='-1':\n            labels.append(-1)\n        else:\n            labels.append(int(label))\n        camera_id.append(int(camera))\n    return camera_id, labels\n\ngallery_path = image_datasets['gallery'].imgs\nquery_path = image_datasets['query'].imgs\n\ngallery_cam,gallery_label = get_id(gallery_path)\nquery_cam,query_label = get_id(query_path)\n\nif opt.multi:\n    mquery_path = image_datasets['multi-query'].imgs\n    mquery_cam,mquery_label = get_id(mquery_path)\n\n######################################################################\n# Load Collected data Trained model\nprint('-------test-----------')\nif opt.use_dense:\n    model_structure = ft_net_dense(opt.nclasses, stride = opt.stride, linear_num=opt.linear_num)\nelif opt.use_NAS:\n    model_structure = ft_net_NAS(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_swin:\n    model_structure = ft_net_swin(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_swinv2:\n    model_structure = ft_net_swinv2(opt.nclasses, (h,w),  linear_num=opt.linear_num)\nelif opt.use_convnext:\n    model_structure = ft_net_convnext(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_efficient:\n    model_structure = ft_net_efficient(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_hr:\n    model_structure = ft_net_hr(opt.nclasses, linear_num=opt.linear_num)\nelse:\n    model_structure = ft_net(opt.nclasses, stride = opt.stride, ibn = opt.ibn, linear_num=opt.linear_num)\n\nif opt.PCB:\n    model_structure = PCB(opt.nclasses)\n\n#if opt.fp16:\n#    model_structure = network_to_half(model_structure)\n\nmodel = load_network(model_structure)\n\n# Remove the final fc layer and classifier layer\nif opt.PCB:\n    #if opt.fp16:\n    #    model = PCB_test(model[1])\n    #else:\n        model = PCB_test(model)\nelse:\n    #if opt.fp16:\n        #model[1].model.fc = nn.Sequential()\n        #model[1].classifier = nn.Sequential()\n    #else:\n        model.classifier.classifier = nn.Sequential()\n\n# Change to test mode\nmodel = model.eval()\nif use_gpu:\n    model = model.cuda()\n\n\nprint('Here I fuse conv and bn for faster inference, and it does not work for transformers. Comment out this following line if you do not want to fuse conv&bn.')\nmodel = fuse_all_conv_bn(model)\n\n# We can optionally trace the forward method with PyTorch JIT so it runs faster.\n# To do so, we can call `.trace` on the reparamtrized module with dummy inputs\n# expected by the module.\n# Comment out this following line if you do not want to trace.\n#dummy_forward_input = torch.rand(opt.batchsize, 3, h, w).cuda()\n#model = torch.jit.trace(model, dummy_forward_input)\n\nprint(model)\n# Extract feature\nsince = time.time()\nwith torch.no_grad():\n    query_feature = extract_feature(model,dataloaders['query'])\n    gallery_feature = extract_feature(model,dataloaders['gallery'])\n    if opt.multi:\n        mquery_feature = extract_feature(model,dataloaders['multi-query'])\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.2f}s'.format(\n            time_elapsed // 60, time_elapsed % 60))\n# Save to Matlab for check\nresult = {'gallery_f':gallery_feature.numpy(),'gallery_label':gallery_label,'gallery_cam':gallery_cam,'query_f':query_feature.numpy(),'query_label':query_label,'query_cam':query_cam}\nscipy.io.savemat('pytorch_result.mat',result)\n\nprint(opt.name)\nresult = './model/%s/result.txt'%opt.name\nos.system('python evaluate_gpu.py | tee -a %s'%result)\n\nif opt.multi:\n    result = {'mquery_f':mquery_feature.numpy(),'mquery_label':mquery_label,'mquery_cam':mquery_cam}\n    scipy.io.savemat('multi_query.mat',result)\n"
        },
        {
          "name": "test_with_TensorRT.py",
          "type": "blob",
          "size": 11.1103515625,
          "content": "# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nfrom torch2trt import torch2trt\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport time\nimport os\nimport scipy.io\nimport yaml\nimport math\nfrom model import ft_net, ft_net_dense, ft_net_hr, ft_net_swin, ft_net_efficient, ft_net_NAS, PCB, PCB_test\n\n#fp16\ntry:\n    from apex.fp16_utils import *\nexcept ImportError: # will be 3.x series\n    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n######################################################################\n# Options\n# --------\n\nparser = argparse.ArgumentParser(description='Test')\nparser.add_argument('--gpu_ids',default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\nparser.add_argument('--which_epoch',default='last', type=str, help='0,1,2,3...or last')\nparser.add_argument('--test_dir',default='../Market/pytorch',type=str, help='./test_data')\nparser.add_argument('--name', default='ft_ResNet50', type=str, help='save model path')\nparser.add_argument('--batchsize', default=256, type=int, help='batchsize')\nparser.add_argument('--linear_num', default=512, type=int, help='feature dimension: 512 or default or 0 (linear=False)')\nparser.add_argument('--use_dense', action='store_true', help='use densenet121' )\nparser.add_argument('--use_efficient', action='store_true', help='use efficient-b4' )\nparser.add_argument('--use_hr', action='store_true', help='use hr18 net' )\nparser.add_argument('--PCB', action='store_true', help='use PCB' )\nparser.add_argument('--multi', action='store_true', help='use multiple query' )\nparser.add_argument('--fp16', action='store_true', help='use fp16.' )\nparser.add_argument('--ibn', action='store_true', help='use ibn.' )\nparser.add_argument('--ms',default='1', type=str,help='multiple_scale: e.g. 1 1,1.1  1,1.1,1.2')\n\nopt = parser.parse_args()\n###load config###\n# load the training config\nconfig_path = os.path.join('./model',opt.name,'opts.yaml')\nwith open(config_path, 'r') as stream:\n        config = yaml.load(stream, Loader=yaml.FullLoader) # for the new pyyaml via 'conda install pyyaml'\nopt.fp16 = config['fp16'] \nopt.PCB = config['PCB']\nopt.use_dense = config['use_dense']\nopt.use_NAS = config['use_NAS']\nopt.stride = config['stride']\nif 'use_swin' in config:\n    opt.use_swin = config['use_swin']\nif 'use_efficient' in config:\n    opt.use_efficient = config['use_efficient']\nif 'use_hr' in config:\n    opt.use_hr = config['use_hr']\n\nif 'nclasses' in config: # tp compatible with old config files\n    opt.nclasses = config['nclasses']\nelse: \n    opt.nclasses = 751 \n\nif 'ibn' in config:\n    opt.ibn = config['ibn']\nif 'linear_num' in config:\n    opt.linear_num = config['linear_num']\n\nstr_ids = opt.gpu_ids.split(',')\n#which_epoch = opt.which_epoch\nname = opt.name\ntest_dir = opt.test_dir\n\ngpu_ids = []\nfor str_id in str_ids:\n    id = int(str_id)\n    if id >=0:\n        gpu_ids.append(id)\n\nprint('We use the scale: %s'%opt.ms)\nstr_ms = opt.ms.split(',')\nms = []\nfor s in str_ms:\n    s_f = float(s)\n    ms.append(math.sqrt(s_f))\n\n# set gpu ids\nif len(gpu_ids)>0:\n    torch.cuda.set_device(gpu_ids[0])\n    cudnn.benchmark = True\n\n######################################################################\n# Load Data\n# ---------\n#\n# We will use torchvision and torch.utils.data packages for loading the\n# data.\n#\nif opt.use_swin:\n    h, w = 224, 224\nelse:\n    h, w = 256, 128\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((h, w), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n############### Ten Crop        \n        #transforms.TenCrop(224),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.ToTensor()(crop) \n          #      for crop in crops]\n           # )),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(crop)\n          #       for crop in crops]\n          # ))\n])\n\nif opt.PCB:\n    data_transforms = transforms.Compose([\n        transforms.Resize((384,192), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n    ])\n    h, w = 384, 192\n\ndata_dir = test_dir\n\nif opt.multi:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query','multi-query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query','multi-query']}\nelse:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query']}\nclass_names = image_datasets['query'].classes\nuse_gpu = torch.cuda.is_available()\n\n######################################################################\n# Load model\n#---------------------------\ndef load_network(network):\n    save_path = os.path.join('./model',name,'net_%s.pth'%opt.which_epoch)\n    network.load_state_dict(torch.load(save_path))\n    return network\n\n\n######################################################################\n# Extract feature\n# ----------------------\n#\n# Extract feature from  a trained model.\n#\ndef fliplr(img):\n    '''flip horizontal'''\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\ndef extract_feature(model,dataloaders):\n    #features = torch.FloatTensor()\n    count = 0\n    if opt.linear_num <= 0:\n        if opt.use_swin or opt.use_dense:\n            opt.linear_num = 1024\n        elif opt.use_efficient:\n            opt.linear_num = 1792\n        elif opt.use_NAS:\n            opt.linear_num = 4032\n        else:\n            opt.linear_num = 2048\n\n    for iter, data in enumerate(dataloaders):\n        img, label = data\n        n, c, h, w = img.size()\n        count += n\n        print(count)\n        ff = torch.FloatTensor(n,opt.linear_num).zero_().cuda()\n\n        if opt.PCB:\n            ff = torch.FloatTensor(n,2048,6).zero_().cuda() # we have six parts\n\n        for i in range(2):\n            if(i==1):\n                img = fliplr(img)\n            input_img = Variable(img.cuda())\n            for scale in ms:\n                if scale != 1:\n                    # bicubic is only  available in pytorch>= 1.1\n                    input_img = nn.functional.interpolate(input_img, scale_factor=scale, mode='bicubic', align_corners=False)\n                outputs = model(input_img) \n                ff += outputs\n        # norm feature\n        if opt.PCB:\n            # feature size (n,2048,6)\n            # 1. To treat every part equally, I calculate the norm for every 2048-dim part feature.\n            # 2. To keep the cosine score==1, sqrt(6) is added to norm the whole feature (2048*6).\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True) * np.sqrt(6) \n            ff = ff.div(fnorm.expand_as(ff))\n            ff = ff.view(ff.size(0), -1)\n        else:\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n            ff = ff.div(fnorm.expand_as(ff))\n\n        \n        if iter == 0:\n            features = torch.FloatTensor( len(dataloaders.dataset), ff.shape[1])\n        #features = torch.cat((features,ff.data.cpu()), 0)\n        start = iter*opt.batchsize\n        end = min( (iter+1)*opt.batchsize, len(dataloaders.dataset))\n        features[ start:end, :] = ff\n    return features\n\ndef get_id(img_path):\n    camera_id = []\n    labels = []\n    for path, v in img_path:\n        #filename = path.split('/')[-1]\n        filename = os.path.basename(path)\n        label = filename[0:4]\n        camera = filename.split('c')[1]\n        if label[0:2]=='-1':\n            labels.append(-1)\n        else:\n            labels.append(int(label))\n        camera_id.append(int(camera[0]))\n    return camera_id, labels\n\ngallery_path = image_datasets['gallery'].imgs\nquery_path = image_datasets['query'].imgs\n\ngallery_cam,gallery_label = get_id(gallery_path)\nquery_cam,query_label = get_id(query_path)\n\nif opt.multi:\n    mquery_path = image_datasets['multi-query'].imgs\n    mquery_cam,mquery_label = get_id(mquery_path)\n\n######################################################################\n# Load Collected data Trained model\nif opt.use_dense:\n    model_structure = ft_net_dense(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_NAS:\n    model_structure = ft_net_NAS(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_swin:\n    model_structure = ft_net_swin(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_efficient:\n    model_structure = ft_net_efficient(opt.nclasses, linear_num=opt.linear_num)\nelif opt.use_hr:\n    model_structure = ft_net_hr(opt.nclasses, linear_num=opt.linear_num)\nelse:\n    model_structure = ft_net(opt.nclasses, stride = opt.stride, ibn = opt.ibn, linear_num=opt.linear_num)\n\nif opt.PCB:\n    model_structure = PCB(opt.nclasses)\n\n#if opt.fp16:\n#    model_structure = network_to_half(model_structure)\n\nmodel = load_network(model_structure)\n\n# Remove the final fc layer and classifier layer\nif opt.PCB:\n    #if opt.fp16:\n    #    model = PCB_test(model[1])\n    #else:\n        model = PCB_test(model)\nelse:\n    #if opt.fp16:\n        #model[1].model.fc = nn.Sequential()\n        #model[1].classifier = nn.Sequential()\n    #else:\n        model.classifier.classifier = nn.Sequential()\n\n# Change to test mode\nmodel = model.eval()\nif use_gpu:\n    model = model.cuda()\n\n# create example data\n# batchsize= 1\nx = torch.ones((opt.batchsize, 3, h, w)).cuda()\n# convert to TensorRT feeding sample data as input\nprint('Please pip install nvidia-pyindex; pip install nvidia-tensorrt; git clone https://github.com/NVIDIA-AI-IOT/torch2trt; cd torch2trt; python setup.py install')\nmodel = torch2trt(model, [x], fp16_mode=True, max_batch_size=opt.batchsize)\n\nprint('-------test-----------')\n# Extract feature\nsince = time.time()\nwith torch.no_grad():\n    gallery_feature = extract_feature(model,dataloaders['gallery'])\n    query_feature = extract_feature(model,dataloaders['query'])\n    if opt.multi:\n        mquery_feature = extract_feature(model,dataloaders['multi-query'])\ntime_elapsed = time.time() - since\nprint('Testing complete in {:.0f}m {:.2f}s'.format(\n            time_elapsed // 60, time_elapsed % 60))\n# Save to Matlab for check\nresult = {'gallery_f':gallery_feature.numpy(),'gallery_label':gallery_label,'gallery_cam':gallery_cam,'query_f':query_feature.numpy(),'query_label':query_label,'query_cam':query_cam}\nscipy.io.savemat('pytorch_result.mat',result)\n\nprint(opt.name)\nresult = './model/%s/result.txt'%opt.name\nos.system('python evaluate_gpu.py | tee -a %s'%result)\n\nif opt.multi:\n    result = {'mquery_f':mquery_feature.numpy(),'mquery_label':mquery_label,'mquery_cam':mquery_cam}\n    scipy.io.savemat('multi_query.mat',result)\n"
        },
        {
          "name": "tool",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 26.6455078125,
          "content": "# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nimport torch.backends.cudnn as cudnn\nimport matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\n#from PIL import Image\nimport time\nimport os\nimport collections\nfrom tqdm import tqdm\nfrom model import ft_net, ft_net_dense, ft_net_hr, ft_net_swin, ft_net_swinv2, ft_net_convnext, ft_net_efficient, ft_net_NAS, PCB\nfrom random_erasing import RandomErasing\nfrom dgfolder import DGFolder\nimport yaml\nfrom shutil import copyfile\nfrom circle_loss import CircleLoss, convert_label_to_similarity\nfrom instance_loss import InstanceLoss\nfrom ODFA import ODFA\nfrom utils import save_network\nversion =  torch.__version__\n#fp16\ntry:\n    from apex.fp16_utils import *\n    from apex import amp\n    from apex.optimizers import FusedSGD\nexcept ImportError: # will be 3.x series\n    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n\nfrom pytorch_metric_learning import losses, miners #pip install pytorch-metric-learning\n\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description='Training')\nparser.add_argument('--gpu_ids',default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\nparser.add_argument('--name',default='ft_ResNet50', type=str, help='output model name')\n# data\nparser.add_argument('--data_dir',default='../Market/pytorch',type=str, help='training dir path')\nparser.add_argument('--train_all', action='store_true', help='use all training data' )\nparser.add_argument('--batchsize', default=32, type=int, help='batchsize')\nparser.add_argument('--color_jitter', action='store_true', help='use color jitter in training' )\nparser.add_argument('--erasing_p', default=0, type=float, help='Random Erasing probability, in [0,1]')\nparser.add_argument('--DG', action='store_true', help='use extra DG-Market Dataset for training. Please download it from https://github.com/NVlabs/DG-Net#dg-market.' )\n# optimizer\nparser.add_argument('--lr', default=0.05, type=float, help='learning rate')\nparser.add_argument('--weight_decay', default=5e-4, type=float, help='Weight decay. More Regularization Smaller Weight.')\nparser.add_argument('--total_epoch', default=60, type=int, help='total training epoch')\nparser.add_argument('--fp16', action='store_true', help='use float16 instead of float32, which will save about 50%% memory' )\nparser.add_argument('--cosine', action='store_true', help='use cosine lrRate' )\nparser.add_argument('--FSGD', action='store_true', help='use fused sgd, which will speed up trainig slightly. apex is needed.' )\n# backbone\nparser.add_argument('--linear_num', default=512, type=int, help='feature dimension: 512 or default or 0 (linear=False)')\nparser.add_argument('--stride', default=2, type=int, help='stride')\nparser.add_argument('--droprate', default=0.5, type=float, help='drop rate')\nparser.add_argument('--use_dense', action='store_true', help='use densenet121' )\nparser.add_argument('--use_swin', action='store_true', help='use swin transformer 224x224' )\nparser.add_argument('--use_swinv2', action='store_true', help='use swin transformerv2' )\nparser.add_argument('--use_efficient', action='store_true', help='use efficientnet-b4' )\nparser.add_argument('--use_NAS', action='store_true', help='use NAS' )\nparser.add_argument('--use_hr', action='store_true', help='use hrNet' )\nparser.add_argument('--use_convnext', action='store_true', help='use ConvNext' )\nparser.add_argument('--ibn', action='store_true', help='use resnet+ibn' )\nparser.add_argument('--PCB', action='store_true', help='use PCB+ResNet50' )\n# loss\nparser.add_argument('--warm_epoch', default=0, type=int, help='the first K epoch that needs warm up')\nparser.add_argument('--arcface', action='store_true', help='use ArcFace loss' )\nparser.add_argument('--circle', action='store_true', help='use Circle loss' )\nparser.add_argument('--cosface', action='store_true', help='use CosFace loss' )\nparser.add_argument('--contrast', action='store_true', help='use contrast loss' )\nparser.add_argument('--instance', action='store_true', help='use instance loss' )\nparser.add_argument('--ins_gamma', default=32, type=int, help='gamma for instance loss')\nparser.add_argument('--triplet', action='store_true', help='use triplet loss' )\nparser.add_argument('--lifted', action='store_true', help='use lifted loss' )\nparser.add_argument('--sphere', action='store_true', help='use sphere loss' )\nparser.add_argument('--adv', default=0.0, type=float, help='use adv loss as 1.0' )\nparser.add_argument('--aiter', default=10, type=float, help='use adv loss with iter' )\n\nopt = parser.parse_args()\n\nfp16 = opt.fp16\ndata_dir = opt.data_dir\nname = opt.name\nstr_ids = opt.gpu_ids.split(',')\ngpu_ids = []\nfor str_id in str_ids:\n    gid = int(str_id)\n    if gid >=0:\n        gpu_ids.append(gid)\nopt.gpu_ids = gpu_ids\n# set gpu ids\nif len(gpu_ids)>0:\n    #torch.cuda.set_device(gpu_ids[0])\n    cudnn.enabled = True\n    cudnn.benchmark = True\n######################################################################\n# Load Data\n# ---------\n#\n\nif opt.use_swin:\n    h, w = 224, 224\nelse:\n    h, w = 256, 128\n\ntransform_train_list = [\n        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n        transforms.Resize((h, w), interpolation=3),\n        transforms.Pad(10),\n        transforms.RandomCrop((h, w)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\ntransform_val_list = [\n        transforms.Resize(size=(h, w),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.PCB:\n    transform_train_list = [\n        transforms.Resize((384,192), interpolation=3),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n    transform_val_list = [\n        transforms.Resize(size=(384,192),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.erasing_p>0:\n    transform_train_list = transform_train_list +  [RandomErasing(probability = opt.erasing_p, mean=[0.0, 0.0, 0.0])]\n\nif opt.color_jitter:\n    transform_train_list = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0)] + transform_train_list\n\nprint(transform_train_list)\ndata_transforms = {\n    'train': transforms.Compose( transform_train_list ),\n    'val': transforms.Compose(transform_val_list),\n}\n\n\ntrain_all = ''\nif opt.train_all:\n     train_all = '_all'\n\nimage_datasets = {}\nimage_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train' + train_all),\n                                          data_transforms['train'])\nimage_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n                                          data_transforms['val'])\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=True, num_workers=2, pin_memory=True,\n                                             prefetch_factor=2, persistent_workers=True) # 8 workers may work faster\n              for x in ['train', 'val']}\n# Use extra DG-Market Dataset for training. Please download it from https://github.com/NVlabs/DG-Net#dg-market.\nif opt.DG:\n    if not os.path.isdir('../DG-Market'):\n        os.system('gdown 126Gn90Tzpk3zWp2c7OBYPKc-ZjhptKDo')\n        os.system('unzip DG-Market.zip -d ../')\n        os.system('rm DG-Market.zip')\n        \n    image_datasets['DG'] = DGFolder(os.path.join('../DG-Market' ),\n                                          data_transforms['train'])\n    dataloaders['DG'] = torch.utils.data.DataLoader(image_datasets['DG'], batch_size = max(8, opt.batchsize//2),\n                                             shuffle=True, num_workers=2, pin_memory=True)\n    DGloader_iter = enumerate(dataloaders['DG'])\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\nuse_gpu = torch.cuda.is_available()\n\nsince = time.time()\ninputs, classes = next(iter(dataloaders['train']))\nprint(time.time()-since)\n######################################################################\n# Training the model\n# ------------------\n#\n# Now, let's write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\ny_loss = {} # loss history\ny_loss['train'] = []\ny_loss['val'] = []\ny_err = {}\ny_err['train'] = []\ny_err['val'] = []\n\ndef fliplr(img):\n    '''flip horizontal'''\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long().cuda()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    #best_model_wts = model.state_dict()\n    #best_acc = 0.0\n    warm_up = 0.1 # We start from the 0.1*lrRate\n    warm_iteration = round(dataset_sizes['train']/opt.batchsize)*opt.warm_epoch # first 5 epoch\n    embedding_size = model.classifier.linear_num\n    if opt.arcface:\n        criterion_arcface = losses.ArcFaceLoss(num_classes=opt.nclasses, embedding_size=embedding_size)\n    if opt.cosface: \n        criterion_cosface = losses.CosFaceLoss(num_classes=opt.nclasses, embedding_size=embedding_size)\n    if opt.circle:\n        criterion_circle = CircleLoss(m=0.25, gamma=32) # gamma = 64 may lead to a better result.\n    if opt.triplet:\n        miner = miners.MultiSimilarityMiner()\n        criterion_triplet = losses.TripletMarginLoss(margin=0.3)\n    if opt.lifted:\n        criterion_lifted = losses.GeneralizedLiftedStructureLoss(neg_margin=1, pos_margin=0)\n    if opt.contrast: \n        criterion_contrast = losses.ContrastiveLoss(pos_margin=0, neg_margin=1)\n    if opt.instance:\n        criterion_instance = InstanceLoss(gamma = opt.ins_gamma)\n    if opt.sphere:\n        criterion_sphere = losses.SphereFaceLoss(num_classes=opt.nclasses, embedding_size=embedding_size, margin=4)\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        # print('-' * 10)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            # Phases 'train' and 'val' are visualized in two separate progress bars\n            pbar = tqdm()\n            pbar.reset(total=len(dataloaders[phase].dataset))\n            ordered_dict = collections.OrderedDict(phase=\"\", Loss=\"\", Acc=\"\")\n\n            running_loss = 0.0\n            running_corrects = 0.0\n            # Iterate over data.\n            for iter, data in enumerate(dataloaders[phase]):\n                # get the inputs\n                inputs, labels = data\n                now_batch_size,c,h,w = inputs.shape\n                pbar.update(now_batch_size)  # update the pbar even in the last batch\n                if now_batch_size<opt.batchsize: # skip the last batch\n                    continue\n                #print(inputs.shape)\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = inputs.cuda().detach()\n                    labels = labels.cuda().detach()\n                # if we use low precision, input also need to be fp16\n                #if fp16:\n                #    inputs = inputs.half()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                if phase == 'val':\n                    with torch.no_grad():\n                        outputs = model(inputs)\n                else:\n                    outputs = model(inputs)\n\n\n\n                if opt.adv>0 and iter%opt.aiter==0: \n                    inputs_adv = ODFA(model, inputs)\n                    outputs_adv = model(inputs_adv)\n\n                sm = nn.Softmax(dim=1)\n                log_sm = nn.LogSoftmax(dim=1)\n                return_feature = opt.arcface or opt.cosface or opt.circle or opt.triplet or opt.contrast or opt.instance or opt.lifted or opt.sphere\n                if return_feature: \n                    logits, ff = outputs\n                    fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n                    ff = ff.div(fnorm.expand_as(ff))\n                    loss = criterion(logits, labels) \n                    _, preds = torch.max(logits.data, 1)\n                    if opt.adv>0  and iter%opt.aiter==0:\n                        logits_adv, _ = outputs_adv\n                        loss += opt.adv * criterion(logits_adv, labels)\n                    if opt.arcface:\n                        loss +=  criterion_arcface(ff, labels)/now_batch_size\n                    if opt.cosface:\n                        loss +=  criterion_cosface(ff, labels)/now_batch_size\n                    if opt.circle:\n                        loss +=  criterion_circle(*convert_label_to_similarity( ff, labels))/now_batch_size\n                    if opt.triplet:\n                        hard_pairs = miner(ff, labels)\n                        loss +=  criterion_triplet(ff, labels, hard_pairs) #/now_batch_size\n                    if opt.lifted:\n                        loss +=  criterion_lifted(ff, labels) #/now_batch_size\n                    if opt.contrast:\n                        loss +=  criterion_contrast(ff, labels) #/now_batch_size\n                    if opt.instance:\n                        loss += criterion_instance(ff) /now_batch_size\n                    if opt.sphere:\n                        loss +=  criterion_sphere(ff, labels)/now_batch_size\n                elif opt.PCB:  #  PCB\n                    part = {}\n                    num_part = 6\n                    for i in range(num_part):\n                        part[i] = outputs[i]\n\n                    score = sm(part[0]) + sm(part[1]) +sm(part[2]) + sm(part[3]) +sm(part[4]) +sm(part[5])\n                    _, preds = torch.max(score.data, 1)\n\n                    loss = criterion(part[0], labels)\n                    for i in range(num_part-1):\n                        loss += criterion(part[i+1], labels)\n                else:  #  norm\n                    _, preds = torch.max(outputs.data, 1)\n                    loss = criterion(outputs, labels)\n                    if opt.adv>0 and iter%opt.aiter==0:\n                        loss += opt.adv * criterion(outputs_adv, labels)\n\n                del inputs\n                # use extra DG Dataset (https://github.com/NVlabs/DG-Net#dg-market)\n                if opt.DG and phase == 'train' and epoch > num_epochs*0.1:\n                    # print(\"DG-Market is involved. It will double the training time.\")\n                    try:\n                        _, batch = DGloader_iter.__next__()\n                    except StopIteration: \n                        DGloader_iter = enumerate(dataloaders['DG'])\n                        _, batch = DGloader_iter.__next__()\n                    except UnboundLocalError:  # first iteration\n                        DGloader_iter = enumerate(dataloaders['DG'])\n                        _, batch = DGloader_iter.__next__()\n                        \n                    inputs1, inputs2, _ = batch\n                    inputs1 = inputs1.cuda().detach()\n                    inputs2 = inputs2.cuda().detach()\n                    # use memory in vivo loss (https://arxiv.org/abs/1912.11164)\n                    outputs1 = model(inputs1)\n                    if return_feature:\n                        outputs1, _ = outputs1\n                    elif opt.PCB:\n                        for i in range(num_part):\n                            part[i] = outputs1[i]\n                        outputs1 = part[0] + part[1] + part[2] + part[3] + part[4] + part[5]\n                    outputs2 = model(inputs2)\n                    if return_feature:\n                        outputs2, _ = outputs2\n                    elif opt.PCB:\n                        for i in range(num_part):\n                            part[i] = outputs2[i]\n                        outputs2 = part[0] + part[1] + part[2] + part[3] + part[4] + part[5]\n\n                    mean_pred = sm(outputs1 + outputs2)\n                    kl_loss = nn.KLDivLoss(reduction='batchmean')\n                    reg= (kl_loss(log_sm(outputs2) , mean_pred)  + kl_loss(log_sm(outputs1) , mean_pred))/2\n                    loss += 0.01*reg\n                    del inputs1, inputs2\n                    #print(0.01*reg)\n                # backward + optimize only if in training phase\n                if epoch<opt.warm_epoch and phase == 'train': \n                    warm_up = min(1.0, warm_up + 0.9 / warm_iteration)\n                    loss = loss*warm_up\n                    print(loss, warm_up)\n\n                if phase == 'train':\n                    if fp16: # we use optimier to backward loss\n                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n                            scaled_loss.backward()\n                    else:\n                        loss.backward()\n                    optimizer.step()\n                # statistics\n                if int(version[0])>0 or int(version[2]) > 3: # for the new version like 0.4.0, 0.5.0 and 1.0.0\n                    running_loss += loss.item() * now_batch_size\n                    ordered_dict[\"Loss\"] = f\"{loss.item():.4f}\"\n                else :  # for the old version like 0.3.0 and 0.3.1\n                    running_loss += loss.data[0] * now_batch_size\n                    ordered_dict[\"Loss\"] = f\"{loss.data[0]:.4f}\"\n                del loss\n                running_corrects += float(torch.sum(preds == labels.data))\n                # Refresh the progress bar in every batch\n                ordered_dict[\"phase\"] = phase\n                ordered_dict[\n                    \"Acc\"\n                ] = f\"{(float(torch.sum(preds == labels.data)) / now_batch_size):.4f}\"\n                pbar.set_postfix(ordered_dict=ordered_dict)\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            \n            # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n            #     phase, epoch_loss, epoch_acc))\n            ordered_dict[\"phase\"] = phase\n            ordered_dict[\"Loss\"] = f\"{epoch_loss:.4f}\"\n            ordered_dict[\"Acc\"] = f\"{epoch_acc:.4f}\"\n            pbar.set_postfix(ordered_dict=ordered_dict)\n            pbar.close()\n            \n            y_loss[phase].append(epoch_loss)\n            y_err[phase].append(1.0-epoch_acc)            \n            # deep copy the model\n            if phase == 'val' and epoch%10 == 9:\n                last_model_wts = model.state_dict()\n                if len(opt.gpu_ids)>1:\n                    save_network(model.module, opt.name, epoch+1)\n                else:\n                    save_network(model, opt.name, epoch+1)\n            if phase == 'val':\n                draw_curve(epoch)\n            if phase == 'train':\n                scheduler.step()\n        time_elapsed = time.time() - since\n        print('Training complete in {:.0f}m {:.0f}s'.format(\n            time_elapsed // 60, time_elapsed % 60))\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    #print('Best val Acc: {:4f}'.format(best_acc)\n\n    # load best model weights\n    model.load_state_dict(last_model_wts)\n    if len(opt.gpu_ids)>1:\n        save_network(model.module, opt.name, 'last')\n    else:\n        save_network(model, opt.name, 'last')\n\n    return model\n\n\n######################################################################\n# Draw Curve\n#---------------------------\nx_epoch = []\nfig = plt.figure()\nax0 = fig.add_subplot(121, title=\"loss\")\nax1 = fig.add_subplot(122, title=\"top1err\")\ndef draw_curve(current_epoch):\n    x_epoch.append(current_epoch)\n    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='train')\n    ax0.plot(x_epoch, y_loss['val'], 'ro-', label='val')\n    ax1.plot(x_epoch, y_err['train'], 'bo-', label='train')\n    ax1.plot(x_epoch, y_err['val'], 'ro-', label='val')\n    if current_epoch == 0:\n        ax0.legend()\n        ax1.legend()\n    fig.savefig( os.path.join('./model',name,'train.jpg'))\n\n\n######################################################################\n# Finetuning the convnet\n# ----------------------\n#\n# Load a pretrainied model and reset final fully connected layer.\n#\n\nreturn_feature = opt.arcface or opt.cosface or opt.circle or opt.triplet or opt.contrast or opt.instance or opt.lifted or opt.sphere\n\nif opt.use_dense:\n    model = ft_net_dense(len(class_names), opt.droprate, opt.stride, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_NAS:\n    model = ft_net_NAS(len(class_names), opt.droprate, linear_num=opt.linear_num)\nelif opt.use_swin:\n    model = ft_net_swin(len(class_names), opt.droprate, opt.stride, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_swinv2:\n    model = ft_net_swinv2(len(class_names), (h, w), opt.droprate, opt.stride, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_efficient:\n    model = ft_net_efficient(len(class_names), opt.droprate, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_hr:\n    model = ft_net_hr(len(class_names), opt.droprate, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_convnext:\n    model = ft_net_convnext(len(class_names), opt.droprate, circle = return_feature, linear_num=opt.linear_num)\nelse:\n    model = ft_net(len(class_names), opt.droprate, opt.stride, circle = return_feature, ibn=opt.ibn, linear_num=opt.linear_num)\n\nif opt.PCB:\n    model = PCB(len(class_names))\n\nopt.nclasses = len(class_names)\nprint(model)\n# model to gpu\nmodel = model.cuda()\n\noptim_name = optim.SGD #apex.optimizers.FusedSGD\nif opt.FSGD: # apex is needed\n    optim_name = FusedSGD\n\nif torch.cuda.get_device_capability()[0]>6 and len(opt.gpu_ids)==1 and int(version[0])>1: # should be >=7 and one gpu\n    torch.set_float32_matmul_precision('high')\n    print(\"Compiling model... The first epoch may be slow, which is expected!\")\n    # https://huggingface.co/docs/diffusers/main/en/optimization/torch2.0\n    model = torch.compile(model, mode=\"reduce-overhead\", dynamic = True) # pytorch 2.0\n\nif len(opt.gpu_ids)>1:\n    model = torch.nn.DataParallel(model, device_ids=opt.gpu_ids) \n    if not opt.PCB:\n        ignored_params = list(map(id, model.module.classifier.parameters() ))\n        base_params = filter(lambda p: id(p) not in ignored_params, model.module.parameters())\n        classifier_params = model.module.classifier.parameters()\n        optimizer_ft = optim_name([\n             {'params': base_params, 'lr': 0.1*opt.lr},\n             {'params': classifier_params, 'lr': opt.lr}\n         ], weight_decay=opt.weight_decay, momentum=0.9, nesterov=True)\n    else:\n        ignored_params = list(map(id, model.module.model.fc.parameters() ))\n        ignored_params += (list(map(id, model.module.classifier0.parameters() ))\n                     +list(map(id, model.module.classifier1.parameters() ))\n                     +list(map(id, model.module.classifier2.parameters() ))\n                     +list(map(id, model.module.classifier3.parameters() ))\n                     +list(map(id, model.module.classifier4.parameters() ))\n                     +list(map(id, model.module.classifier5.parameters() ))\n                     #+list(map(id, model.module.classifier6.parameters() ))\n                     #+list(map(id, model.module.classifier7.parameters() ))\n                      )\n        base_params = filter(lambda p: id(p) not in ignored_params, model.module.parameters())\n        classifier_params = filter(lambda p: id(p) in ignored_params, model.module.parameters())\n        optimizer_ft = optim_name([\n             {'params': base_params, 'lr': 0.1*opt.lr},\n             {'params': classifier_params, 'lr': opt.lr}\n         ], weight_decay=opt.weight_decay, momentum=0.9, nesterov=True)\nelse:\n    if not opt.PCB:\n        ignored_params = list(map(id, model.classifier.parameters() ))\n        base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n        classifier_params = model.classifier.parameters()\n        optimizer_ft = optim_name([\n             {'params': base_params, 'lr': 0.1*opt.lr},\n             {'params': classifier_params, 'lr': opt.lr}\n         ], weight_decay=opt.weight_decay, momentum=0.9, nesterov=True)\n    else:\n        ignored_params = list(map(id, model.model.fc.parameters() ))\n        ignored_params += (list(map(id, model.classifier0.parameters() )) \n                     +list(map(id, model.classifier1.parameters() ))\n                     +list(map(id, model.classifier2.parameters() ))\n                     +list(map(id, model.classifier3.parameters() ))\n                     +list(map(id, model.classifier4.parameters() ))\n                     +list(map(id, model.classifier5.parameters() ))\n                     #+list(map(id, model.classifier6.parameters() ))\n                     #+list(map(id, model.classifier7.parameters() ))\n                      )\n        base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n        classifier_params = filter(lambda p: id(p) in ignored_params, model.parameters())\n        optimizer_ft = optim_name([\n             {'params': base_params, 'lr': 0.1*opt.lr},\n             {'params': classifier_params, 'lr': opt.lr}\n         ], weight_decay=opt.weight_decay, momentum=0.9, nesterov=True)\n\n# Decay LR by a factor of 0.1 every 40 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=opt.total_epoch*2//3, gamma=0.1)\nif opt.cosine:\n    exp_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, opt.total_epoch, eta_min=0.01*opt.lr)\n\n######################################################################\n# Train and evaluate\n# ^^^^^^^^^^^^^^^^^^\n#\n# It should take around 1-2 hours on GPU. \n#\ndir_name = os.path.join('./model',name)\nif not os.path.isdir(dir_name):\n    os.mkdir(dir_name)\n#record every run\ncopyfile('./train.py', dir_name+'/train.py')\ncopyfile('./model.py', dir_name+'/model.py')\n\n# save opts\nwith open('%s/opts.yaml'%dir_name,'w') as fp:\n    yaml.dump(vars(opt), fp, default_flow_style=False)\n\ncriterion = nn.CrossEntropyLoss()\n\nif fp16:\n    #model = network_to_half(model)\n    #optimizer_ft = FP16_Optimizer(optimizer_ft, static_loss_scale = 128.0)\n    model, optimizer_ft = amp.initialize(model, optimizer_ft, opt_level = \"O1\")\n\n\nmodel = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=opt.total_epoch)\n\n"
        },
        {
          "name": "train_DDP.py",
          "type": "blob",
          "size": 25.6904296875,
          "content": "# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nimport torch.backends.cudnn as cudnn\nimport matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\n#from PIL import Image\nimport time\nimport os\nimport collections\nfrom tqdm import tqdm\nfrom model import ft_net, ft_net_dense, ft_net_hr, ft_net_swin, ft_net_swinv2, ft_net_convnext, ft_net_efficient, ft_net_NAS, PCB\nfrom random_erasing import RandomErasing\nfrom dgfolder import DGFolder\nimport yaml\nfrom shutil import copyfile\nfrom circle_loss import CircleLoss, convert_label_to_similarity\nfrom instance_loss import InstanceLoss\nfrom ODFA import ODFA\nfrom utils import save_network\nversion =  torch.__version__\n#fp16\ntry:\n    from apex.fp16_utils import *\n    from apex import amp\n    from apex.optimizers import FusedSGD\nexcept ImportError: # will be 3.x series\n    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n\nfrom pytorch_metric_learning import losses, miners #pip install pytorch-metric-learning\n\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description='Training')\nparser.add_argument('--gpu_ids',default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\n# in new pytorch, local_rank -> local-rank\nparser.add_argument('--local-rank', type=int,help='gpu_ids: e.g. 0  0,1,2  0,2')\nparser.add_argument('--name',default='ft_ResNet50', type=str, help='output model name')\n# data\nparser.add_argument('--data_dir',default='../Market/pytorch',type=str, help='training dir path')\nparser.add_argument('--train_all', action='store_true', help='use all training data' )\nparser.add_argument('--batchsize', default=32, type=int, help='batchsize')\nparser.add_argument('--color_jitter', action='store_true', help='use color jitter in training' )\nparser.add_argument('--erasing_p', default=0, type=float, help='Random Erasing probability, in [0,1]')\nparser.add_argument('--DG', action='store_true', help='use extra DG-Market Dataset for training. Please download it from https://github.com/NVlabs/DG-Net#dg-market.' )\n# optimizer\nparser.add_argument('--lr', default=0.05, type=float, help='learning rate')\nparser.add_argument('--weight_decay', default=5e-4, type=float, help='Weight decay. More Regularization Smaller Weight.')\nparser.add_argument('--total_epoch', default=60, type=int, help='total training epoch')\nparser.add_argument('--fp16', action='store_true', help='use float16 instead of float32, which will save about 50%% memory' )\nparser.add_argument('--cosine', action='store_true', help='use cosine lrRate' )\nparser.add_argument('--FSGD', action='store_true', help='use fused sgd, which will speed up trainig slightly. apex is needed.' )\n# backbone\nparser.add_argument('--linear_num', default=512, type=int, help='feature dimension: 512 or default or 0 (linear=False)')\nparser.add_argument('--stride', default=2, type=int, help='stride')\nparser.add_argument('--droprate', default=0.5, type=float, help='drop rate')\nparser.add_argument('--use_dense', action='store_true', help='use densenet121' )\nparser.add_argument('--use_swin', action='store_true', help='use swin transformer 224x224' )\nparser.add_argument('--use_swinv2', action='store_true', help='use swin transformerv2' )\nparser.add_argument('--use_efficient', action='store_true', help='use efficientnet-b4' )\nparser.add_argument('--use_NAS', action='store_true', help='use NAS' )\nparser.add_argument('--use_hr', action='store_true', help='use hrNet' )\nparser.add_argument('--use_convnext', action='store_true', help='use ConvNext' )\nparser.add_argument('--ibn', action='store_true', help='use resnet+ibn' )\nparser.add_argument('--PCB', action='store_true', help='use PCB+ResNet50' )\n# loss\nparser.add_argument('--warm_epoch', default=0, type=int, help='the first K epoch that needs warm up')\nparser.add_argument('--arcface', action='store_true', help='use ArcFace loss' )\nparser.add_argument('--circle', action='store_true', help='use Circle loss' )\nparser.add_argument('--cosface', action='store_true', help='use CosFace loss' )\nparser.add_argument('--contrast', action='store_true', help='use contrast loss' )\nparser.add_argument('--instance', action='store_true', help='use instance loss' )\nparser.add_argument('--ins_gamma', default=32, type=int, help='gamma for instance loss')\nparser.add_argument('--triplet', action='store_true', help='use triplet loss' )\nparser.add_argument('--lifted', action='store_true', help='use lifted loss' )\nparser.add_argument('--sphere', action='store_true', help='use sphere loss' )\nparser.add_argument('--adv', default=0.0, type=float, help='use adv loss as 1.0' )\nparser.add_argument('--aiter', default=10, type=float, help='use adv loss with iter' )\n\nopt = parser.parse_args()\n\nfp16 = opt.fp16\ndata_dir = opt.data_dir\nname = opt.name\nstr_ids = opt.gpu_ids.split(',')\ngpu_ids = []\nfor str_id in str_ids:\n    gid = int(str_id)\n    if gid >=0:\n        gpu_ids.append(gid)\nopt.gpu_ids = gpu_ids\n# set gpu ids\ncudnn.enabled = True\ncudnn.benchmark = True\ntorch.distributed.init_process_group(backend='gloo',\n                                             init_method='env://')\nopt.world_size = torch.distributed.get_world_size()\n\n######################################################################\n# Load Data\n# ---------\n#\n\nif opt.use_swin:\n    h, w = 224, 224\nelse:\n    h, w = 256, 128\n\ntransform_train_list = [\n        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n        transforms.Resize((h, w), interpolation=3),\n        transforms.Pad(10),\n        transforms.RandomCrop((h, w)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\ntransform_val_list = [\n        transforms.Resize(size=(h, w),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.PCB:\n    transform_train_list = [\n        transforms.Resize((384,192), interpolation=3),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n    transform_val_list = [\n        transforms.Resize(size=(384,192),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.erasing_p>0:\n    transform_train_list = transform_train_list +  [RandomErasing(probability = opt.erasing_p, mean=[0.0, 0.0, 0.0])]\n\nif opt.color_jitter:\n    transform_train_list = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0)] + transform_train_list\n\nprint(transform_train_list)\ndata_transforms = {\n    'train': transforms.Compose( transform_train_list ),\n    'val': transforms.Compose(transform_val_list),\n}\n\n\ntrain_all = ''\nif opt.train_all:\n     train_all = '_all'\n\nimage_datasets = {}\nimage_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train' + train_all),\n                                          data_transforms['train'])\nimage_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n                                          data_transforms['val'])\n\nimport multiprocessing\ncpu_count = multiprocessing.cpu_count()\nopt.workers = 4\nopt.prefetch_factor = 2\nif cpu_count>=32:\n    opt.workers = 8\n    opt.prefetch_factor = 4\n\nimg_sampler ={x: torch.utils.data.distributed.DistributedSampler(image_datasets[x]) for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, sampler=img_sampler[x],\n                                             num_workers=opt.workers, pin_memory=True, drop_last=True,\n                                             prefetch_factor=opt.prefetch_factor, persistent_workers=True) # 2 workers may work faster\n                                             for x in ['train', 'val']}\n\n# Use extra DG-Market Dataset for training. Please download it from https://github.com/NVlabs/DG-Net#dg-market.\nif opt.DG:\n    if not os.path.isdir('../DG-Market'):\n        os.system('gdown 126Gn90Tzpk3zWp2c7OBYPKc-ZjhptKDo')\n        os.system('unzip DG-Market.zip -d ../')\n        os.system('rm DG-Market.zip')\n        \n    image_datasets['DG'] = DGFolder(os.path.join('../DG-Market' ),\n                                          data_transforms['train'])\n    dataloaders['DG'] = torch.utils.data.DataLoader(image_datasets['DG'], batch_size = max(8, opt.batchsize//2),\n                                             shuffle=True, num_workers=2, pin_memory=True)\n    DGloader_iter = enumerate(dataloaders['DG'])\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\nuse_gpu = torch.cuda.is_available()\n\nsince = time.time()\ninputs, classes = next(iter(dataloaders['train']))\nprint(time.time()-since)\n######################################################################\n# Training the model\n# ------------------\n#\n# Now, let's write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\ny_loss = {} # loss history\ny_loss['train'] = []\ny_loss['val'] = []\ny_err = {}\ny_err['train'] = []\ny_err['val'] = []\n\ndef fliplr(img):\n    '''flip horizontal'''\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long().cuda()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    #best_model_wts = model.state_dict()\n    #best_acc = 0.0\n    warm_up = 0.1 # We start from the 0.1*lrRate\n    warm_iteration = round(dataset_sizes['train']/opt.batchsize)*opt.warm_epoch # first 5 epoch\n    if opt.arcface:\n        criterion_arcface = losses.ArcFaceLoss(num_classes=opt.nclasses, embedding_size=512)\n    if opt.cosface: \n        criterion_cosface = losses.CosFaceLoss(num_classes=opt.nclasses, embedding_size=512)\n    if opt.circle:\n        criterion_circle = CircleLoss(m=0.25, gamma=32) # gamma = 64 may lead to a better result.\n    if opt.triplet:\n        miner = miners.MultiSimilarityMiner()\n        criterion_triplet = losses.TripletMarginLoss(margin=0.3)\n    if opt.lifted:\n        criterion_lifted = losses.GeneralizedLiftedStructureLoss(neg_margin=1, pos_margin=0)\n    if opt.contrast: \n        criterion_contrast = losses.ContrastiveLoss(pos_margin=0, neg_margin=1)\n    if opt.instance:\n        criterion_instance = InstanceLoss(gamma = opt.ins_gamma)\n    if opt.sphere:\n        criterion_sphere = losses.SphereFaceLoss(num_classes=opt.nclasses, embedding_size=512, margin=4)\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        # print('-' * 10)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            # Phases 'train' and 'val' are visualized in two separate progress bars\n            pbar = tqdm()\n            pbar.reset(total=len(dataloaders[phase].dataset))\n            ordered_dict = collections.OrderedDict(phase=\"\", Loss=\"\", Acc=\"\")\n\n            running_loss = 0.0\n            running_corrects = 0.0\n            # Iterate over data.\n            for iter, data in enumerate(dataloaders[phase]):\n                # get the inputs\n                inputs, labels = data\n                now_batch_size,c,h,w = inputs.shape\n                pbar.update(now_batch_size)  # update the pbar even in the last batch\n                if now_batch_size<opt.batchsize: # skip the last batch\n                    continue\n                #print(inputs.shape)\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = inputs.cuda(opt.local_rank, non_blocking=True)\n                    labels = labels.cuda(opt.local_rank, non_blocking=True)\n                # if we use low precision, input also need to be fp16\n                #if fp16:\n                #    inputs = inputs.half()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                if phase == 'val':\n                    with torch.no_grad():\n                        outputs = model(inputs)\n                else:\n                    outputs = model(inputs)\n\n\n\n                if opt.adv>0 and iter%opt.aiter==0: \n                    inputs_adv = ODFA(model, inputs)\n                    outputs_adv = model(inputs_adv)\n\n                sm = nn.Softmax(dim=1)\n                log_sm = nn.LogSoftmax(dim=1)\n                return_feature = opt.arcface or opt.cosface or opt.circle or opt.triplet or opt.contrast or opt.instance or opt.lifted or opt.sphere\n                if return_feature: \n                    logits, ff = outputs\n                    fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n                    ff = ff.div(fnorm.expand_as(ff))\n                    loss = criterion(logits, labels) \n                    _, preds = torch.max(logits.data, 1)\n                    if opt.adv>0  and iter%opt.aiter==0:\n                        logits_adv, _ = outputs_adv\n                        loss += opt.adv * criterion(logits_adv, labels)\n                    if opt.arcface:\n                        loss +=  criterion_arcface(ff, labels)/now_batch_size\n                    if opt.cosface:\n                        loss +=  criterion_cosface(ff, labels)/now_batch_size\n                    if opt.circle:\n                        loss +=  criterion_circle(*convert_label_to_similarity( ff, labels))/now_batch_size\n                    if opt.triplet:\n                        hard_pairs = miner(ff, labels)\n                        loss +=  criterion_triplet(ff, labels, hard_pairs) #/now_batch_size\n                    if opt.lifted:\n                        loss +=  criterion_lifted(ff, labels) #/now_batch_size\n                    if opt.contrast:\n                        loss +=  criterion_contrast(ff, labels) #/now_batch_size\n                    if opt.instance:\n                        loss += criterion_instance(ff) /now_batch_size\n                    if opt.sphere:\n                        loss +=  criterion_sphere(ff, labels)/now_batch_size\n                elif opt.PCB:  #  PCB\n                    part = {}\n                    num_part = 6\n                    for i in range(num_part):\n                        part[i] = outputs[i]\n\n                    score = sm(part[0]) + sm(part[1]) +sm(part[2]) + sm(part[3]) +sm(part[4]) +sm(part[5])\n                    _, preds = torch.max(score.data, 1)\n\n                    loss = criterion(part[0], labels)\n                    for i in range(num_part-1):\n                        loss += criterion(part[i+1], labels)\n                else:  #  norm\n                    _, preds = torch.max(outputs.data, 1)\n                    loss = criterion(outputs, labels)\n                    if opt.adv>0 and iter%opt.aiter==0:\n                        loss += opt.adv * criterion(outputs_adv, labels)\n\n                del inputs\n                # use extra DG Dataset (https://github.com/NVlabs/DG-Net#dg-market)\n                if opt.DG and phase == 'train' and epoch > num_epochs*0.1:\n                    # print(\"DG-Market is involved. It will double the training time.\")\n                    try:\n                        _, batch = DGloader_iter.__next__()\n                    except StopIteration: \n                        DGloader_iter = enumerate(dataloaders['DG'])\n                        _, batch = DGloader_iter.__next__()\n                    except UnboundLocalError:  # first iteration\n                        DGloader_iter = enumerate(dataloaders['DG'])\n                        _, batch = DGloader_iter.__next__()\n                        \n                    inputs1, inputs2, _ = batch\n                    inputs1 = inputs1.cuda().detach()\n                    inputs2 = inputs2.cuda().detach()\n                    # use memory in vivo loss (https://arxiv.org/abs/1912.11164)\n                    outputs1 = model(inputs1)\n                    if return_feature:\n                        outputs1, _ = outputs1\n                    elif opt.PCB:\n                        for i in range(num_part):\n                            part[i] = outputs1[i]\n                        outputs1 = part[0] + part[1] + part[2] + part[3] + part[4] + part[5]\n                    outputs2 = model(inputs2)\n                    if return_feature:\n                        outputs2, _ = outputs2\n                    elif opt.PCB:\n                        for i in range(num_part):\n                            part[i] = outputs2[i]\n                        outputs2 = part[0] + part[1] + part[2] + part[3] + part[4] + part[5]\n\n                    mean_pred = sm(outputs1 + outputs2)\n                    kl_loss = nn.KLDivLoss(reduction='batchmean')\n                    reg= (kl_loss(log_sm(outputs2) , mean_pred)  + kl_loss(log_sm(outputs1) , mean_pred))/2\n                    loss += 0.01*reg\n                    del inputs1, inputs2\n                    #print(0.01*reg)\n                # backward + optimize only if in training phase\n                if epoch<opt.warm_epoch and phase == 'train': \n                    warm_up = min(1.0, warm_up + 0.9 / warm_iteration)\n                    loss = loss*warm_up\n                    print(loss, warm_up)\n\n                if phase == 'train':\n                    if fp16: # we use optimier to backward loss\n                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n                            scaled_loss.backward()\n                    else:\n                        loss.backward()\n                    optimizer.step()\n                # statistics\n                if int(version[0])>0 or int(version[2]) > 3: # for the new version like 0.4.0, 0.5.0 and 1.0.0\n                    running_loss += loss.item() * now_batch_size\n                    ordered_dict[\"Loss\"] = f\"{loss.item():.4f}\"\n                else :  # for the old version like 0.3.0 and 0.3.1\n                    running_loss += loss.data[0] * now_batch_size\n                    ordered_dict[\"Loss\"] = f\"{loss.data[0]:.4f}\"\n                del loss\n                running_corrects += float(torch.sum(preds == labels.data))\n                # Refresh the progress bar in every batch\n                ordered_dict[\"phase\"] = phase\n                ordered_dict[\n                    \"Acc\"\n                ] = f\"{(float(torch.sum(preds == labels.data)) / now_batch_size):.4f}\"\n                pbar.set_postfix(ordered_dict=ordered_dict)\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            \n            # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n            #     phase, epoch_loss, epoch_acc))\n            ordered_dict[\"phase\"] = phase\n            ordered_dict[\"Loss\"] = f\"{epoch_loss:.4f}\"\n            ordered_dict[\"Acc\"] = f\"{epoch_acc:.4f}\"\n            pbar.set_postfix(ordered_dict=ordered_dict)\n            pbar.close()\n            \n            y_loss[phase].append(epoch_loss)\n            y_err[phase].append(1.0-epoch_acc)            \n            # deep copy the model\n            if opt.local_rank == 0 and phase == 'val' and ( (epoch+1)%10 == 0 or epoch == num_epochs - 1):\n                print('saving...')\n                last_model_wts = model.state_dict()\n                save_network(model.module, opt.name, epoch+1, opt.local_rank)\n            if phase == 'val':\n                draw_curve(epoch)\n            if phase == 'train':\n                scheduler.step()\n        time_elapsed = time.time() - since\n        print('Training complete in {:.0f}m {:.0f}s'.format(\n            time_elapsed // 60, time_elapsed % 60))\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    #print('Best val Acc: {:4f}'.format(best_acc)\n\n    # load best model weights\n    if opt.local_rank == 0:\n        model.load_state_dict(last_model_wts)\n        save_network(model.module, opt.name, 'last', opt.local_rank)\n\n    return model\n\n\n######################################################################\n# Draw Curve\n#---------------------------\nx_epoch = []\nfig = plt.figure()\nax0 = fig.add_subplot(121, title=\"loss\")\nax1 = fig.add_subplot(122, title=\"top1err\")\ndef draw_curve(current_epoch):\n    x_epoch.append(current_epoch)\n    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='train')\n    ax0.plot(x_epoch, y_loss['val'], 'ro-', label='val')\n    ax1.plot(x_epoch, y_err['train'], 'bo-', label='train')\n    ax1.plot(x_epoch, y_err['val'], 'ro-', label='val')\n    if current_epoch == 0:\n        ax0.legend()\n        ax1.legend()\n    fig.savefig( os.path.join('./model',name,'train.jpg'))\n\n\n\n######################################################################\n# Finetuning the convnet\n# ----------------------\n#\n# Load a pretrainied model and reset final fully connected layer.\n#\n\nreturn_feature = opt.arcface or opt.cosface or opt.circle or opt.triplet or opt.contrast or opt.instance or opt.lifted or opt.sphere\n\nif opt.use_dense:\n    model = ft_net_dense(len(class_names), opt.droprate, opt.stride, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_NAS:\n    model = ft_net_NAS(len(class_names), opt.droprate, linear_num=opt.linear_num)\nelif opt.use_swin:\n    model = ft_net_swin(len(class_names), opt.droprate, opt.stride, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_swinv2:\n    model = ft_net_swinv2(len(class_names), (h, w), opt.droprate, opt.stride, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_efficient:\n    model = ft_net_efficient(len(class_names), opt.droprate, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_hr:\n    model = ft_net_hr(len(class_names), opt.droprate, circle = return_feature, linear_num=opt.linear_num)\nelif opt.use_convnext:\n    model = ft_net_convnext(len(class_names), opt.droprate, circle = return_feature, linear_num=opt.linear_num)\nelse:\n    model = ft_net(len(class_names), opt.droprate, opt.stride, circle = return_feature, ibn=opt.ibn, linear_num=opt.linear_num)\n\nif opt.PCB:\n    model = PCB(len(class_names))\n\nopt.nclasses = len(class_names)\nprint(model)\n# model to gpu\nmodel = model.cuda()\n\noptim_name = optim.SGD #apex.optimizers.FusedSGD\nif opt.FSGD: # apex is needed\n    optim_name = FusedSGD\n\nif not opt.PCB:\n        ignored_params = list(map(id, model.classifier.parameters() ))\n        base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n        classifier_params = model.classifier.parameters()\n        optimizer_ft = optim_name([\n             {'params': base_params, 'lr': 0.1*opt.lr},\n             {'params': classifier_params, 'lr': opt.lr}\n         ], weight_decay=opt.weight_decay, momentum=0.9, nesterov=True)\nelse:\n        ignored_params = list(map(id, model.model.fc.parameters() ))\n        ignored_params += (list(map(id, model.classifier0.parameters() )) \n                     +list(map(id, model.classifier1.parameters() ))\n                     +list(map(id, model.classifier2.parameters() ))\n                     +list(map(id, model.classifier3.parameters() ))\n                     +list(map(id, model.classifier4.parameters() ))\n                     +list(map(id, model.classifier5.parameters() ))\n                     #+list(map(id, model.classifier6.parameters() ))\n                     #+list(map(id, model.classifier7.parameters() ))\n                      )\n        base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n        classifier_params = filter(lambda p: id(p) in ignored_params, model.parameters())\n        optimizer_ft = optim_name([\n             {'params': base_params, 'lr': 0.1*opt.lr},\n             {'params': classifier_params, 'lr': opt.lr}\n         ], weight_decay=opt.weight_decay, momentum=0.9, nesterov=True)\n\n# Decay LR by a factor of 0.1 every 40 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=opt.total_epoch*2//3, gamma=0.1)\nif opt.cosine:\n    exp_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, opt.total_epoch, eta_min=0.01*opt.lr)\n\n######################################################################\n# Train and evaluate\n# ^^^^^^^^^^^^^^^^^^\n#\n# It should take around 1-2 hours on GPU. \n#\ndir_name = os.path.join('./model',name)\nif not os.path.isdir(dir_name):\n    os.mkdir(dir_name)\n#record every run\ncopyfile('./train.py', dir_name+'/train.py')\ncopyfile('./model.py', dir_name+'/model.py')\n\n# save opts\nwith open('%s/opts.yaml'%dir_name,'w') as fp:\n    yaml.dump(vars(opt), fp, default_flow_style=False)\n\ncriterion = nn.CrossEntropyLoss()\n\nif fp16:\n    #model = network_to_half(model)\n    #optimizer_ft = FP16_Optimizer(optimizer_ft, static_loss_scale = 128.0)\n    model, optimizer_ft = amp.initialize(model, optimizer_ft, opt_level = \"O1\")\n\n#if torch.cuda.get_device_capability()[0]>6 and len(opt.gpu_ids)==1 and int(version[0])>1: # should be >=7 and one gpu\n#    torch.set_float32_matmul_precision('high')\n#    print(\"Compiling model... The first epoch may be slow, which is expected!\")\n    # https://huggingface.co/docs/diffusers/main/en/optimization/torch2.0\n#    model = torch.compile(model, mode=\"reduce-overhead\", dynamic = True) # pytorch 2.0\nmodel = model.to(opt.local_rank)\nmodel = torch.nn.parallel.DistributedDataParallel(model, device_ids=[opt.local_rank], output_device=0, find_unused_parameters=True)\nmodel = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=opt.total_epoch)\n\n"
        },
        {
          "name": "tutorial",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 4.609375,
          "content": "import os\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import fuse_conv_bn_eval\n\nclass CrossEntropyLabelSmooth(nn.Module):\n    \"\"\"Cross entropy loss with label smoothing regularizer.\n    Reference:\n    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\n    Equation: y = (1 - epsilon) * y + epsilon / K.\n    Args:\n        epsilon (float): weight.\n    \"\"\"\n    def __init__(self, epsilon=0.1, use_gpu=True):\n        super(CrossEntropyLabelSmooth, self).__init__()\n        self.epsilon = epsilon\n        self.use_gpu = use_gpu\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, inputs, targets):\n        \"\"\"\n        Args:\n            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n            targets: ground truth labels with shape (num_classes)\n        \"\"\"\n        _, num_classes = inputs.shape\n        log_probs = self.logsoftmax(inputs)\n        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n        if self.use_gpu: targets = targets.cuda()\n        targets = (1 - self.epsilon) * targets + self.epsilon / num_classes\n        loss = (- targets * log_probs).mean(0).sum()\n        return loss\n\ndef fuse_all_conv_bn(model):\n    stack = []\n    for name, module in model.named_children():\n        if list(module.named_children()):\n            fuse_all_conv_bn(module)\n            \n        if isinstance(module, nn.BatchNorm2d):\n            if not stack:\n                continue\n            if isinstance(stack[-1][1], nn.Conv2d):\n                setattr(model, stack[-1][0], fuse_conv_bn_eval(stack[-1][1], module))\n                setattr(model, name, nn.Identity())\n        else:\n            stack.append((name, module))\n    return model\n\ndef save_network(network, dirname, epoch_label, local_rank=-1):\n    if isinstance(epoch_label, int):\n        save_filename = 'net_%03d.pth'% epoch_label\n    else:\n        save_filename = 'net_%s.pth'% epoch_label\n    save_path = os.path.join('./model',dirname,save_filename)\n\n    if local_rank>-1:\n        if local_rank == 0: # save the main process model\n            torch.save(network.state_dict(), save_path)\n            network.cuda(local_rank)\n    else:\n        torch.save(network.state_dict(), save_path)\n        network.cuda()\n\n\ndef load_state_dict_mute(self, state_dict: 'OrderedDict[str, Tensor]',\n                        strict: bool = True):\n        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n        this module and its descendants. If :attr:`strict` is ``True``, then\n        the keys of :attr:`state_dict` must exactly match the keys returned\n        by this module's :meth:`~torch.nn.Module.state_dict` function.\n\n        Args:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            strict (bool, optional): whether to strictly enforce that the keys\n                in :attr:`state_dict` match the keys returned by this module's\n                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n\n        Returns:\n            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n                * **missing_keys** is a list of str containing the missing keys\n                * **unexpected_keys** is a list of str containing the unexpected keys\n        \"\"\"\n        missing_keys: List[str] = []\n        unexpected_keys: List[str] = []\n        error_msgs: List[str] = []\n\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, '_metadata', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            # mypy isn't aware that \"_metadata\" exists in state_dict\n            state_dict._metadata = metadata  # type: ignore[attr-defined]\n\n        def load(module, prefix=''):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + '.')\n\n        load(self)\n        del load\n\n        if strict:\n            if len(unexpected_keys) > 0:\n                error_msgs.insert(\n                    0, 'Unexpected key(s) in state_dict: {}. '.format(\n                        ', '.join('\"{}\"'.format(k) for k in unexpected_keys)))\n            if len(missing_keys) > 0:\n                error_msgs.insert(\n                    0, 'Missing key(s) in state_dict: {}. '.format(\n                        ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n\n"
        }
      ]
    }
  ]
}