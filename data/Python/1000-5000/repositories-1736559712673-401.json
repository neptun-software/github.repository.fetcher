{
  "metadata": {
    "timestamp": 1736559712673,
    "page": 401,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "sktime/pytorch-forecasting",
      "stars": 4063,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9423828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/source/api/\ndocs/source/CHANGELOG.md\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pycharm\n.idea\n\n# vscode\n.vscode\n\n# logs\nlightning_logs\n.history\n\n# checkpoints\n*.ckpt\n*.pkl\n.DS_Store\n\n# data\npytorch_forecasting/data/*.parquet\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.6337890625,
          "content": "# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-ast\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.6.9\n    hooks:\n      - id: ruff\n        args: [--fix]\n  - repo: https://github.com/psf/black\n    rev: 24.8.0\n    hooks:\n      - id: black\n  - repo: https://github.com/nbQA-dev/nbQA\n    rev: 1.8.7\n    hooks:\n      - id: nbqa-black\n      - id: nbqa-ruff\n      - id: nbqa-check-ast\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.6591796875,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Build documentation in the docs/ directory with Sphinx\n# reference: https://docs.readthedocs.io/en/stable/config-file/v2.html#sphinx\nsphinx:\n  configuration: docs/source/conf.py\n  # fail_on_warning: true\n\n# Build documentation with MkDocs\n#mkdocs:\n#  configuration: mkdocs.yml\n\n# Optionally build your docs in additional formats such as PDF and ePub\nformats:\n  - htmlzip\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.12\"\n\npython:\n  install:\n    - method: pip\n      path: .\n      extra_requirements:\n        - docs\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 20.9609375,
          "content": "# Release Notes\n\n## v1.2.0\n\nMaintenance update, minor feature additions and bugfixes.\n\n* support for `numpy 2.X`\n* end of life for `python 3.8`\n* fixed documentation build\n* bugfixes\n\n### Dependency changes\n\n* `pytorch-forecasting` is now compatible with `numpy 2.X` (core dependency)\n* `optuna` (tuning soft dependency) bounds have been update to `>=3.1.0,<5.0.0`\n\n### Fixes\n\n* [BUG] fix `AttributeError: 'ExperimentWriter' object has no attribute 'add_figure'` by @ewth in https://github.com/sktime/pytorch-forecasting/pull/1694\n\n### Documentation\n\n* [DOC] typo fixes in changelog by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1660\n* [DOC] update URLs to `sktime` org by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1674\n\n### Maintenance\n\n* [MNT] handle `mps backend` for lower versions of pytorch and fix `mps` failure on `macOS-latest` runner by @fnhirwa in https://github.com/sktime/pytorch-forecasting/pull/1648\n* [MNT] updates the actions in the doc build CI by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1673\n* [MNT] fixes to `readthedocs.yml` by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1676\n* [MNT] updates references in CI and doc locations to `main` by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1677\n* [MNT] `show_versions` utility by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1688\n* [MNT] Relax `numpy` bound to `numpy<3.0.0` by @XinyuWuu in https://github.com/sktime/pytorch-forecasting/pull/1624\n* [MNT] fix `pre-commit` failures on `main` by @ewth in https://github.com/sktime/pytorch-forecasting/pull/1696\n* [MNT] Move linting to ruff by @airookie17 in https://github.com/sktime/pytorch-forecasting/pull/1692\n1693\n* [MNT] `ruff` linting - allow use of assert (S101) by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1701\n* [MNT] `ruff` - fix list related linting failures C416 and C419 by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1702\n* [MNT] Delete poetry.lock by @benHeid in https://github.com/sktime/pytorch-forecasting/pull/1704\n* [MNT] fix `black` doesn't have `extras` dependency by @fnhirwa in https://github.com/sktime/pytorch-forecasting/pull/1697\n* [MNT] Remove mutable objects from defaults by @eugenio-mercuriali in https://github.com/sktime/pytorch-forecasting/pull/1699\n* [MNT] remove docs build in ci for all pr by @yarnabrina in https://github.com/sktime/pytorch-forecasting/pull/1712\n* [MNT] EOL for python 3.8 by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1661\n* [MNT] remove `poetry.lock` by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1651\n* [MNT] update `pre-commit` requirement from `<4.0.0,>=3.2.0` to `>=3.2.0,<5.0.0` by @dependabot in https://github.com/sktime/pytorch-forecasting/pull/\n* [MNT] update optuna requirement from `<4.0.0,>=3.1.0` to `>=3.1.0,<5.0.0` by @dependabot in https://github.com/sktime/pytorch-forecasting/pull/1715\n* [MNT] CODEOWNERS file by @fkiraly in https://github.com/sktime/pytorch-forecasting/pull/1710\n\n### All Contributors\n\n@airookie17,\n@benHeid,\n@eugenio-mercuriali,\n@ewth,\n@fkiraly,\n@fnhirwa,\n@XinyuWuu,\n@yarnabrina\n\n## v1.1.1\n\nHotfix for accidental package name change in `pyproject.toml`.\n\nThe package name is now corrected to `pytorch-forecasting`.\n\n\n## v1.1.0\n\nMaintenance update widening compatibility ranges and consolidating dependencies:\n\n* support for python 3.11 and 3.12, added CI testing\n* support for MacOS, added CI testing\n* core dependencies have been minimized to `numpy`, `torch`, `lightning`, `scipy`, `pandas`, and `scikit-learn`.\n* soft dependencies are available in soft dependency sets: `all_extras` for all soft dependencies, and `tuning` for `optuna` based optimization.\n\n### Dependency changes\n\n* the following are no longer core dependencies and have been changed to optional dependencies : `optuna`, `statsmodels`, `pytorch-optimize`, `matplotlib`. Environments relying on functionality requiring these dependencies need to be updated to install these explicitly.\n* `optuna` bounds have been updated to `optuna >=3.1.0,<4.0.0`\n* `optuna-integrate` is now an additional soft dependency, in case of `optuna >=3.3.0`\n\n### Deprecations and removals\n\n* from 1.2.0, the default optimizer will be changed from `\"ranger\"` to `\"adam\"` to avoid non-`torch` dependencies in defaults. `pytorch-optimize` optimizers can still be used. Users should set the optimizer explicitly to continue using `\"ranger\"`.\n*  from 1.1.0, the loggers do not log figures if soft dependency `matplotlib` is not present, but will raise no exceptions in this case. To log figures, ensure that `matplotlib` is installed.\n\n## v1.0.0 Update to pytorch 2.0 (10/04/2023)\n\n\n### Breaking Changes\n\n- Upgraded to pytorch 2.0 and lightning 2.0. This brings a couple of changes, such as configuration of trainers. See the [lightning upgrade guide](https://lightning.ai/docs/pytorch/latest/upgrade/migration_guide.html). For PyTorch Forecasting, this particularly means if you are developing own models, the class method `epoch_end` has been renamed to `on_epoch_end` and replacing `model.summarize()` with `ModelSummary(model, max_depth=-1)` and `Tuner(trainer)` is its own class, so `trainer.tuner` needs replacing. (#1280)\n- Changed the `predict()` interface returning named tuple - see tutorials.\n\n### Changes\n\n- The predict method is now using the lightning predict functionality and allows writing results to disk (#1280).\n\n### Fixed\n\n- Fixed robust scaler when quantiles are 0.0, and 1.0, i.e. minimum and maximum (#1142)\n\n## v0.10.3 Poetry update (07/09/2022)\n\n### Fixed\n\n- Removed pandoc from dependencies as issue with poetry install (#1126)\n- Added metric attributes for torchmetric resulting in better multi-GPU performance (#1126)\n\n### Added\n\n- \"robust\" encoder method can be customized by setting \"center\", \"lower\" and \"upper\" quantiles (#1126)\n\n## v0.10.2 Multivariate networks (23/05/2022)\n\n### Added\n\n- DeepVar network (#923)\n- Enable quantile loss for N-HiTS (#926)\n- MQF2 loss (multivariate quantile loss) (#949)\n- Non-causal attention for TFT (#949)\n- Tweedie loss (#949)\n- ImplicitQuantileNetworkDistributionLoss (#995)\n\n### Fixed\n\n- Fix learning scale schedule (#912)\n- Fix TFT list/tuple issue at interpretation (#924)\n- Allowed encoder length down to zero for EncoderNormalizer if transformation is not needed (#949)\n- Fix Aggregation and CompositeMetric resets (#949)\n\n### Changed\n\n- Dropping Python 3.6 suppport, adding 3.10 support (#479)\n- Refactored dataloader sampling - moved samplers to pytorch_forecasting.data.samplers module (#479)\n- Changed transformation format for Encoders to dict from tuple (#949)\n\n### Contributors\n\n- jdb78\n\n## v0.10.1 Bugfixes (24/03/2022)\n\n### Fixed\n\n- Fix with creating tensors on correct devices (#908)\n- Fix with MultiLoss when calculating gradient (#908)\n\n### Contributors\n\n- jdb78\n\n## v0.10.0 Adding N-HiTS network (N-BEATS successor) (23/03/2022)\n\n### Added\n\n- Added new `N-HiTS` network that has consistently beaten `N-BEATS` (#890)\n- Allow using [torchmetrics](https://torchmetrics.readthedocs.io/) as loss metrics (#776)\n- Enable fitting `EncoderNormalizer()` with limited data history using `max_length` argument (#782)\n- More flexible `MultiEmbedding()` with convenience `output_size` and `input_size` properties (#829)\n- Fix concatentation of attention (#902)\n\n### Fixed\n\n- Fix pip install via github (#798)\n\n### Contributors\n\n- jdb78\n- christy\n- lukemerrick\n- Seon82\n\n## v0.9.2 Maintenance Release (30/11/2021)\n\n### Added\n\n- Added support for running `lightning.trainer.test` (#759)\n\n### Fixed\n\n- Fix inattention mutation to `x_cont` (#732).\n- Compatability with pytorch-lightning 1.5 (#758)\n\n### Contributors\n\n- eavae\n- danielgafni\n- jdb78\n\n## v0.9.1 Maintenance Release (26/09/2021)\n\n### Added\n\n- Use target name instead of target number for logging metrics (#588)\n- Optimizer can be initialized by passing string, class or function (#602)\n- Add support for multiple outputs in Baseline model (#603)\n- Added Optuna pruner as optional parameter in `TemporalFusionTransformer.optimize_hyperparameters` (#619)\n- Dropping support for Python 3.6 and starting support for Python 3.9 (#639)\n\n### Fixed\n\n- Initialization of TemporalFusionTransformer with multiple targets but loss for only one target (#550)\n- Added missing transformation of prediction for MLP (#602)\n- Fixed logging hyperparameters (#688)\n- Ensure MultiNormalizer fit state is detected (#681)\n- Fix infinite loop in TimeDistributedEmbeddingBag (#672)\n\n### Contributors\n\n- jdb78\n- TKlerx\n- chefPony\n- eavae\n- L0Z1K\n\n## v0.9.0 Simplified API (04/06/2021)\n\n### Breaking changes\n\n- Removed `dropout_categoricals` parameter from `TimeSeriesDataSet`.\n  Use `categorical_encoders=dict(<variable_name>=NaNLabelEncoder(add_nan=True)`) instead (#518)\n- Rename parameter `allow_missings` for `TimeSeriesDataSet` to `allow_missing_timesteps` (#518)\n- Transparent handling of transformations. Forward methods should now call two new methods (#518):\n\n  - `transform_output` to explicitly rescale the network outputs into the de-normalized space\n  - `to_network_output` to create a dict-like named tuple. This allows tracing the modules with PyTorch's JIT. Only `prediction` is still required which is the main network output.\n\n  Example:\n\n  ```python\n  def forward(self, x):\n      normalized_prediction = self.module(x)\n      prediction = self.transform_output(prediction=normalized_prediction, target_scale=x[\"target_scale\"])\n      return self.to_network_output(prediction=prediction)\n  ```\n\n### Fixed\n\n- Fix quantile prediction for tensors on GPUs for distribution losses (#491)\n- Fix hyperparameter update for RecurrentNetwork.from_dataset method (#497)\n\n### Added\n\n- Improved validation of input parameters of TimeSeriesDataSet (#518)\n\n## v0.8.5 Generic distribution loss(es) (27/04/2021)\n\n### Added\n\n- Allow lists for multiple losses and normalizers (#405)\n- Warn if normalization is with scale `< 1e-7` (#429)\n- Allow usage of distribution losses in all settings (#434)\n\n### Fixed\n\n- Fix issue when predicting and data is on different devices (#402)\n- Fix non-iterable output (#404)\n- Fix problem with moving data to CPU for multiple targets (#434)\n\n### Contributors\n\n- jdb78\n- domplexity\n\n## v0.8.4 Simple models (07/03/2021)\n\n### Added\n\n- Adding a filter functionality to the timeseries datasset (#329)\n- Add simple models such as LSTM, GRU and a MLP on the decoder (#380)\n- Allow usage of any torch optimizer such as SGD (#380)\n\n### Fixed\n\n- Moving predictions to CPU to avoid running out of memory (#329)\n- Correct determination of `output_size` for multi-target forecasting with the TemporalFusionTransformer (#328)\n- Tqdm autonotebook fix to work outside of Jupyter (#338)\n- Fix issue with yaml serialization for TensorboardLogger (#379)\n\n### Contributors\n\n- jdb78\n- JakeForsey\n- vakker\n\n## v0.8.3 Bugfix release (31/01/2021)\n\n### Added\n\n- Make tuning trainer kwargs overwritable (#300)\n- Allow adding categories to NaNEncoder (#303)\n\n### Fixed\n\n- Underlying data is copied if modified. Original data is not modified inplace (#263)\n- Allow plotting of interpretation on passed figure for NBEATS (#280)\n- Fix memory leak for plotting and logging interpretation (#311)\n- Correct shape of `predict()` method output for multi-targets (#268)\n- Remove cloudpickle to allow GPU trained models to be loaded on CPU devices from checkpoints (#314)\n\n### Contributors\n\n- jdb78\n- kigawas\n- snumumrik\n\n## v0.8.2 Fix for output transformer (12/01/2021)\n\n- Added missing output transformation which was switched off by default (#260)\n\n## v0.8.1 Adding support for lag variables (10/01/2021)\n\n### Added\n\n- Add \"Release Notes\" section to docs (#237)\n- Enable usage of lag variables for any model (#252)\n\n### Changed\n\n- Require PyTorch>=1.7 (#245)\n\n### Fixed\n\n- Fix issue for multi-target forecasting when decoder length varies in single batch (#249)\n- Enable longer subsequences for min_prediction_idx that were previously wrongfully excluded (#250)\n\n### Contributors\n\n- jdb78\n\n---\n\n## v0.8.0 Adding multi-target support (03/01/2021)\n\n### Added\n\n- Adding support for multiple targets in the TimeSeriesDataSet (#199) and amended tutorials.\n- Temporal fusion transformer and DeepAR with support for multiple tagets (#199)\n- Check for non-finite values in TimeSeriesDataSet and better validate scaler argument (#220)\n- LSTM and GRU implementations that can handle zero-length sequences (#235)\n- Helpers for implementing auto-regressive models (#236)\n\n### Changed\n\n- TimeSeriesDataSet's `y` of the dataloader is a tuple of (target(s), weight) - potentially breaking for model or metrics implementation\n  Most implementations will not be affected as hooks in BaseModel and MultiHorizonMetric were modified. (#199)\n\n### Fixed\n\n- Fixed autocorrelation for pytorch 1.7 (#220)\n- Ensure reproducibility by replacing python `set()` with `dict.fromkeys()` (mostly TimeSeriesDataSet) (#221)\n- Ensures BetaDistributionLoss does not lead to infinite loss if actuals are 0 or 1 (#233)\n- Fix for GroupNormalizer if scaling by group (#223)\n- Fix for TimeSeriesDataSet when using `min_prediction_idx` (#226)\n\n### Contributors\n\n- jdb78\n- JustinNeumann\n- reumar\n- rustyconover\n\n---\n\n## v0.7.1 Tutorial on how to implement a new architecture (07/12/2020)\n\n### Added\n\n- Tutorial on how to implement a new architecture covering basic and advanced use cases (#188)\n- Additional and improved documentation - particularly of implementation details (#188)\n\n### Changed (breaking for new model implementations)\n\n- Moved multiple private methods to public methods (particularly logging) (#188)\n- Moved `get_mask` method from BaseModel into utils module (#188)\n- Instead of using label to communicate if model is training or validating, using `self.training` attribute (#188)\n- Using `sample((n,))` of pytorch distributions instead of deprecated `sample_n(n)` method (#188)\n\n---\n\n## v0.7.0 New API for transforming inputs and outputs with encoders (03/12/2020)\n\n### Added\n\n- Beta distribution loss for probabilistic models such as DeepAR (#160)\n\n### Changed\n\n- BREAKING: Simplifying how to apply transforms (such as logit or log) before and after applying encoder. Some transformations are included by default but a tuple of a forward and reverse transform function can be passed for arbitrary transformations. This requires to use a `transformation` keyword in target normalizers instead of, e.g. `log_scale` (#185)\n\n### Fixed\n\n- Incorrect target position if `len(static_reals) > 0` leading to leakage (#184)\n- Fixing predicting completely unseen series (#172)\n\n### Contributors\n\n- jdb78\n- JakeForsey\n\n---\n\n## v0.6.1 Bugfixes and DeepAR improvements (24/11/2020)\n\n### Added\n\n- Using GRU cells with DeepAR (#153)\n\n### Fixed\n\n- GPU fix for variable sequence length (#169)\n- Fix incorrect syntax for warning when removing series (#167)\n- Fix issue when using unknown group ids in validation or test dataset (#172)\n- Run non-failing CI on PRs from forks (#166, #156)\n\n### Docs\n\n- Improved model selection guidance and explanations on how TimeSeriesDataSet works (#148)\n- Clarify how to use with conda (#168)\n\n### Contributors\n\n- jdb78\n- JakeForsey\n\n---\n\n## v0.6.0 Adding DeepAR (10/11/2020)\n\n### Added\n\n- DeepAR by Amazon (#115)\n  - First autoregressive model in PyTorch Forecasting\n  - Distribution loss: normal, negative binomial and log-normal distributions\n  - Currently missing: handling lag variables and tutorial (planned for 0.6.1)\n- Improved documentation on TimeSeriesDataSet and how to implement a new network (#145)\n\n### Changed\n\n- Internals of encoders and how they store center and scale (#115)\n\n### Fixed\n\n- Update to PyTorch 1.7 and PyTorch Lightning 1.0.5 which came with breaking changes for CUDA handling and with optimizers (PyTorch Forecasting Ranger version) (#143, #137, #115)\n\n### Contributors\n\n- jdb78\n- JakeForesey\n\n---\n\n## v0.5.3 Bug fixes (31/10/2020)\n\n### Fixes\n\n- Fix issue where hyperparameter verbosity controlled only part of output (#118)\n- Fix occasional error when `.get_parameters()` from `TimeSeriesDataSet` failed (#117)\n- Remove redundant double pass through LSTM for temporal fusion transformer (#125)\n- Prevent installation of pytorch-lightning 1.0.4 as it breaks the code (#127)\n- Prevent modification of model defaults in-place (#112)\n\n---\n\n## v0.5.2 Fixes to interpretation and more control over hyperparameter verbosity (18/10/2020)\n\n### Added\n\n- Hyperparameter tuning with optuna to tutorial\n- Control over verbosity of hyper parameter tuning\n\n### Fixes\n\n- Interpretation error when different batches had different maximum decoder lengths\n- Fix some typos (no changes to user API)\n\n---\n\n## v0.5.1 PyTorch Lightning 1.0 compatibility (14/10/2020)\n\nThis release has only one purpose: Allow usage of PyTorch Lightning 1.0 - all tests have passed.\n\n---\n\n## v0.5.0 PyTorch Lightning 0.10 compatibility and classification (12/10/2020)\n\n### Added\n\n- Additional checks for `TimeSeriesDataSet` inputs - now flagging if series are lost due to high `min_encoder_length` and ensure parameters are integers\n- Enable classification - simply change the target in the `TimeSeriesDataSet` to a non-float variable, use the `CrossEntropy` metric to optimize and output as many classes as you want to predict\n\n### Changed\n\n- Ensured PyTorch Lightning 0.10 compatibility\n  - Using `LearningRateMonitor` instead of `LearningRateLogger`\n  - Use `EarlyStopping` callback in trainer `callbacks` instead of `early_stopping` argument\n  - Update metric system `update()` and `compute()` methods\n  - Use `Tuner(trainer).lr_find()` instead of `trainer.lr_find()` in tutorials and examples\n- Update poetry to 1.1.0\n\n---\n\n## v0.4.1 Various fixes models and data (01/10/2020)\n\n### Fixes\n\n#### Model\n\n- Removed attention to current datapoint in TFT decoder to generalise better over various sequence lengths\n- Allow resuming optuna hyperparamter tuning study\n\n#### Data\n\n- Fixed inconsistent naming and calculation of `encoder_length`in TimeSeriesDataSet when added as feature\n\n### Contributors\n\n- jdb78\n\n---\n\n## v0.4.0 Metrics, performance, and subsequence detection (28/09/2020)\n\n### Added\n\n#### Models\n\n- Backcast loss for N-BEATS network for better regularisation\n- logging_metrics as explicit arguments to models\n\n#### Metrics\n\n- MASE (Mean absolute scaled error) metric for training and reporting\n- Metrics can be composed, e.g. `0.3* metric1 + 0.7 * metric2`\n- Aggregation metric that is computed on mean prediction over all samples to reduce mean-bias\n\n#### Data\n\n- Increased speed of parsing data with missing datapoints. About 2s for 1M data points. If `numba` is installed, 0.2s for 1M data points\n- Time-synchronize samples in batches: ensure that all samples in each batch have with same time index in decoder\n\n### Breaking changes\n\n- Improved subsequence detection in TimeSeriesDataSet ensures that there exists a subsequence starting and ending on each point in time.\n- Fix `min_encoder_length = 0` being ignored and processed as `min_encoder_length = max_encoder_length`\n\n### Contributors\n\n- jdb78\n- dehoyosb\n\n---\n\n## v0.3.1 More tests and better docs (13/09/2020)\n\n- More tests driving coverage to ~90%\n- Performance tweaks for temporal fusion transformer\n- Reformatting with sort\n- Improve documentation - particularly expand on hyper parameter tuning\n\n### Fixed\n\n- Fix PoissonLoss quantiles calculation\n- Fix N-Beats visualisations\n\n---\n\n## v0.3.0 More testing and interpretation features (02/09/2020)\n\n### Added\n\n- Calculating partial dependency for a variable\n- Improved documentation - in particular added FAQ section and improved tutorial\n- Data for examples and tutorials can now be downloaded. Cloning the repo is not a requirement anymore\n- Added Ranger Optimizer from `pytorch_ranger` package and fixed its warnings (part of preparations for conda package release)\n- Use GPU for tests if available as part of preparation for GPU tests in CI\n\n### Changes\n\n- **BREAKING**: Fix typo \"add_decoder_length\" to \"add_encoder_length\" in TimeSeriesDataSet\n\n### Bugfixes\n\n- Fixing plotting predictions vs actuals by slicing variables\n\n---\n\n## v0.2.4 Fix edge case in prediction logging (26/08/2020)\n\n### Fixed\n\nFix bug where predictions were not correctly logged in case of `decoder_length == 1`.\n\n### Added\n\n- Add favicon to docs page\n\n---\n\n## v0.2.3 Make pip installable from master branch (23/08/2020)\n\nUpdate build system requirements to be parsed correctly when installing with `pip install git+https://github.com/jdb78/pytorch-forecasting`\n\n---\n\n## v0.2.2 Improving tests (23/08/2020)\n\n- Add tests for MacOS\n- Automatic releases\n- Coverage reporting\n\n---\n\n## v0.2.1 Patch release (23/08/2020)\n\nThis release improves robustness of the code.\n\n- Fixing bug across code, in particularly\n\n  - Ensuring that code works on GPUs\n  - Adding tests for models, dataset and normalisers\n  - Test using GitHub Actions (tests on GPU are still missing)\n\n- Extend documentation by improving docstrings and adding two tutorials.\n- Improving default arguments for TimeSeriesDataSet to avoid surprises\n\n---\n\n## v0.2.0 Minor release (16/08/2020)\n\n### Added\n\n- Basic tests for data and model (mostly integration tests)\n- Automatic target normalization\n- Improved visualization and logging of temporal fusion transformer\n- Model bugfixes and performance improvements for temporal fusion transformer\n\n### Modified\n\n- Metrics are reduced to calculating loss. Target transformations are done by new target transformer\n"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 0.16015625,
          "content": "# The file specifies framework level core developers for automated review requests\n\n* @benheid @fkiraly @fnhirwa @geetu040 @jdb78 @pranavvp16 @XinyuWuu @yarnabrina\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1103515625,
          "content": "THE MIT License\n\nCopyright (c) 2020 - present, the pytorch-forecasting developers\nCopyright (c) 2020 Jan Beitner\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.724609375,
          "content": "![PyTorch Forecasting](./docs/source/_static/logo.svg)\n\n_PyTorch Forecasting_ is a PyTorch-based package for forecasting with state-of-the-art deep learning architectures. It provides a high-level API and uses [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/) to scale training on GPU or CPU, with automatic logging.\n\n\n|  | **[Documentation](https://pytorch-forecasting.readthedocs.io)** · **[Tutorials](https://pytorch-forecasting.readthedocs.io/en/latest/tutorials.html)** · **[Release Notes](https://pytorch-forecasting.readthedocs.io/en/latest/CHANGELOG.html)** |\n|---|---|\n| **Open&#160;Source** | [![MIT](https://img.shields.io/github/license/sktime/pytorch-forecasting)](https://github.com/sktime/pytorch-forecasting/blob/master/LICENSE) |\n| **Community** | [![!discord](https://img.shields.io/static/v1?logo=discord&label=discord&message=chat&color=lightgreen)](https://discord.com/invite/54ACzaFsn7) [![!slack](https://img.shields.io/static/v1?logo=linkedin&label=LinkedIn&message=news&color=lightblue)](https://www.linkedin.com/company/scikit-time/) |\n| **CI/CD** | [![github-actions](https://img.shields.io/github/actions/workflow/status/sktime/pytorch-forecasting/pypi_release.yml?logo=github)](https://github.com/sktime/pytorch-forecasting/actions/workflows/pypi_release.yml) [![readthedocs](https://img.shields.io/readthedocs/pytorch-forecasting?logo=readthedocs)](https://pytorch-forecasting.readthedocs.io) [![platform](https://img.shields.io/conda/pn/conda-forge/pytorch-forecasting)](https://github.com/sktime/pytorch-forecasting) [![Code Coverage][coverage-image]][coverage-url] |\n| **Code** | [![!pypi](https://img.shields.io/pypi/v/pytorch-forecasting?color=orange)](https://pypi.org/project/pytorch-forecasting/) [![!conda](https://img.shields.io/conda/vn/conda-forge/pytorch-forecasting)](https://anaconda.org/conda-forge/pytorch-forecasting) [![!python-versions](https://img.shields.io/pypi/pyversions/pytorch-forecasting)](https://www.python.org/) [![!black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  |\n\n[coverage-image]: https://codecov.io/gh/sktime/pytorch-forecasting/branch/main/graph/badge.svg\n[coverage-url]: https://codecov.io/github/sktime/pytorch-forecasting?branch=main\n\n---\n\nOur article on [Towards Data Science](https://towardsdatascience.com/introducing-pytorch-forecasting-64de99b9ef46) introduces the package and provides background information.\n\nPyTorch Forecasting aims to ease state-of-the-art timeseries forecasting with neural networks for real-world cases and research alike. The goal is to provide a high-level API with maximum flexibility for professionals and reasonable defaults for beginners.\nSpecifically, the package provides\n\n- A timeseries dataset class which abstracts handling variable transformations, missing values,\n  randomized subsampling, multiple history lengths, etc.\n- A base model class which provides basic training of timeseries models along with logging in tensorboard\n  and generic visualizations such actual vs predictions and dependency plots\n- Multiple neural network architectures for timeseries forecasting that have been enhanced\n  for real-world deployment and come with in-built interpretation capabilities\n- Multi-horizon timeseries metrics\n- Hyperparameter tuning with [optuna](https://optuna.readthedocs.io/)\n\nThe package is built on [pytorch-lightning](https://pytorch-lightning.readthedocs.io/) to allow training on CPUs, single and multiple GPUs out-of-the-box.\n\n# Installation\n\nIf you are working on windows, you need to first install PyTorch with\n\n`pip install torch -f https://download.pytorch.org/whl/torch_stable.html`.\n\nOtherwise, you can proceed with\n\n`pip install pytorch-forecasting`\n\nAlternatively, you can install the package via conda\n\n`conda install pytorch-forecasting pytorch -c pytorch>=1.7 -c conda-forge`\n\nPyTorch Forecasting is now installed from the conda-forge channel while PyTorch is install from the pytorch channel.\n\nTo use the MQF2 loss (multivariate quantile loss), also install\n`pip install pytorch-forecasting[mqf2]`\n\n# Documentation\n\nVisit [https://pytorch-forecasting.readthedocs.io](https://pytorch-forecasting.readthedocs.io) to read the\ndocumentation with detailed tutorials.\n\n# Available models\n\nThe documentation provides a [comparison of available models](https://pytorch-forecasting.readthedocs.io/en/latest/models.html).\n\n- [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/pdf/1912.09363.pdf)\n  which outperforms DeepAR by Amazon by 36-69% in benchmarks\n- [N-BEATS: Neural basis expansion analysis for interpretable time series forecasting](http://arxiv.org/abs/1905.10437)\n  which has (if used as ensemble) outperformed all other methods including ensembles of traditional statical\n  methods in the M4 competition. The M4 competition is arguably the most important benchmark for univariate time series forecasting.\n- [N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting](http://arxiv.org/abs/2201.12886) which supports covariates and has consistently beaten N-BEATS. It is also particularly well-suited for long-horizon forecasting.\n- [DeepAR: Probabilistic forecasting with autoregressive recurrent networks](https://www.sciencedirect.com/science/article/pii/S0169207019301888)\n  which is the one of the most popular forecasting algorithms and is often used as a baseline\n- Simple standard networks for baselining: LSTM and GRU networks as well as a MLP on the decoder\n- A baseline model that always predicts the latest known value\n\nTo implement new models or other custom components, see the [How to implement new models tutorial](https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/building.html). It covers basic as well as advanced architectures.\n\n# Usage example\n\nNetworks can be trained with the [PyTorch Lighning Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html) on [pandas Dataframes](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe) which are first converted to a [TimeSeriesDataSet](https://pytorch-forecasting.readthedocs.io/en/latest/data.html).\n\n```python\n# imports for training\nimport lightning.pytorch as pl\nfrom lightning.pytorch.loggers import TensorBoardLogger\nfrom lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n# import dataset, network to train and metric to optimize\nfrom pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\nfrom lightning.pytorch.tuner import Tuner\n\n# load data: this is pandas dataframe with at least a column for\n# * the target (what you want to predict)\n# * the timeseries ID (which should be a unique string to identify each timeseries)\n# * the time of the observation (which should be a monotonically increasing integer)\ndata = ...\n\n# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\nmax_encoder_length = 36\nmax_prediction_length = 6\ntraining_cutoff = \"YYYY-MM-DD\"  # day for cutoff\n\ntraining = TimeSeriesDataSet(\n    data[lambda x: x.date <= training_cutoff],\n    time_idx= ...,  # column name of time of observation\n    target= ...,  # column name of target to predict\n    group_ids=[ ... ],  # column name(s) for timeseries IDs\n    max_encoder_length=max_encoder_length,  # how much history to use\n    max_prediction_length=max_prediction_length,  # how far to predict into future\n    # covariates static for a timeseries ID\n    static_categoricals=[ ... ],\n    static_reals=[ ... ],\n    # covariates known and unknown in the future to inform prediction\n    time_varying_known_categoricals=[ ... ],\n    time_varying_known_reals=[ ... ],\n    time_varying_unknown_categoricals=[ ... ],\n    time_varying_unknown_reals=[ ... ],\n)\n\n# create validation dataset using the same normalization techniques as for the training dataset\nvalidation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training.index.time.max() + 1, stop_randomization=True)\n\n# convert datasets to dataloaders for training\nbatch_size = 128\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n\n# create PyTorch Lighning Trainer with early stopping\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\nlr_logger = LearningRateMonitor()\ntrainer = pl.Trainer(\n    max_epochs=100,\n    accelerator=\"auto\",  # run on CPU, if on multiple GPUs, use strategy=\"ddp\"\n    gradient_clip_val=0.1,\n    limit_train_batches=30,  # 30 batches per epoch\n    callbacks=[lr_logger, early_stop_callback],\n    logger=TensorBoardLogger(\"lightning_logs\")\n)\n\n# define network to train - the architecture is mostly inferred from the dataset, so that only a few hyperparameters have to be set by the user\ntft = TemporalFusionTransformer.from_dataset(\n    # dataset\n    training,\n    # architecture hyperparameters\n    hidden_size=32,\n    attention_head_size=1,\n    dropout=0.1,\n    hidden_continuous_size=16,\n    # loss metric to optimize\n    loss=QuantileLoss(),\n    # logging frequency\n    log_interval=2,\n    # optimizer parameters\n    learning_rate=0.03,\n    reduce_on_plateau_patience=4\n)\nprint(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n\n# find the optimal learning rate\nres = Tuner(trainer).lr_find(\n    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=1000.0, max_lr=0.3,\n)\n# and plot the result - always visually confirm that the suggested learning rate makes sense\nprint(f\"suggested learning rate: {res.suggestion()}\")\nfig = res.plot(show=True, suggest=True)\nfig.show()\n\n# fit the model on the data - redefine the model with the correct learning rate if necessary\ntrainer.fit(\n    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,\n)\n```\n"
        },
        {
          "name": "build_tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.1708984375,
          "content": "coverage:\n  precision: 2\n  round: down\n  range: \"70...100\"\n  status:\n    project:\n      default:\n        threshold: 0.2%\n    patch:\n      default:\n        informational: true\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 4.5283203125,
          "content": "[project]\nname = \"pytorch-forecasting\"\nreadme = \"README.md\"         # Markdown files are supported\nversion = \"1.2.0\"            # is being replaced automatically\n\nauthors = [\n  {name = \"Jan Beitner\"},\n]\nrequires-python = \">=3.9,<3.13\"\nclassifiers = [\n  \"Intended Audience :: Developers\",\n  \"Intended Audience :: Science/Research\",\n  \"Programming Language :: Python :: 3\",\n  \"Programming Language :: Python :: 3.9\",\n  \"Programming Language :: Python :: 3.10\",\n  \"Programming Language :: Python :: 3.11\",\n  \"Programming Language :: Python :: 3.12\",\n  \"Topic :: Scientific/Engineering\",\n  \"Topic :: Scientific/Engineering :: Mathematics\",\n  \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n  \"Topic :: Software Development\",\n  \"Topic :: Software Development :: Libraries\",\n  \"Topic :: Software Development :: Libraries :: Python Modules\",\n  \"License :: OSI Approved :: MIT License\",\n]\ndescription = \"Forecasting timeseries with PyTorch - dataloaders, normalizers, metrics and models\"\n\ndependencies = [\n  \"numpy<=3.0.0\",\n  \"torch >=2.0.0,!=2.0.1,<3.0.0\",\n  \"lightning >=2.0.0,<3.0.0\",\n  \"scipy >=1.8,<2.0\",\n  \"pandas >=1.3.0,<3.0.0\",\n  \"scikit-learn >=1.2,<2.0\",\n]\n\n[project.optional-dependencies]\n# there are the following dependency sets:\n# - all_extras - all soft dependencies\n# - granular dependency sets:\n#     - tuning - dependencies for tuning hyperparameters via optuna\n#     - mqf2 - dependencies for multivariate quantile loss\n#     - graph - dependencies for graph based forecasting\n# - dev - the developer dependency set, for contributors to pytorch-forecasting\n# - CI related: e.g., dev, github-actions. Not for users of sktime.\n#\n# soft dependencies are not required for the core functionality of sktime\n# but are required by popular estimators, e.g., prophet, tbats, etc.\n\n# all soft dependencies\n#\n# users can install via \"pip install pytorch-forecasting[all_extras]\"\n#\nall_extras = [\n  \"cpflows\",\n  \"matplotlib\",\n  \"optuna >=3.1.0,<5.0.0\",\n  \"optuna-integration\",\n  \"pytorch_optimizer >=2.5.1,<4.0.0\",\n  \"statsmodels\",\n]\n\ntuning = [\n  \"optuna >=3.1.0,<5.0.0\",\n  \"optuna-integration\",\n  \"statsmodels\",\n]\n\nmqf2 = [\"cpflows\"]\n\n# the graph set is not currently used within pytorch-forecasting\n# but is kept for future development, as it has already been released\ngraph = [\"networkx\"]\n\n# dev - the developer dependency set, for contributors to pytorch-forecasting\ndev = [\n  \"pydocstyle >=6.1.1,<7.0.0\",\n  # checks and make tools\n  \"pre-commit >=3.2.0,<5.0.0\",\n  \"invoke\",\n  \"mypy\",\n  \"pylint\",\n  \"ruff\",\n  # pytest\n  \"pytest\",\n  \"pytest-xdist\",\n  \"pytest-cov\",\n  \"pytest-sugar\",\n  \"coverage\",\n  \"pyarrow\",\n  # jupyter notebook\n  \"ipykernel\",\n  \"nbconvert\",\n  \"black[jupyter]\",\n  # documentatation\n  \"sphinx\",\n  \"pydata-sphinx-theme\",\n  \"nbsphinx\",\n  \"recommonmark\",\n  \"ipywidgets>=8.0.1,<9.0.0\",\n  \"pytest-dotenv>=0.5.2,<1.0.0\",\n  \"tensorboard>=2.12.1,<3.0.0\",\n  \"pandoc>=2.3,<3.0.0\",\n]\n\n# docs - dependencies for building the documentation\ndocs = [\n  \"sphinx>3.2\",\n  \"pydata-sphinx-theme\",\n  \"nbsphinx\",\n  \"pandoc\",\n  \"nbconvert\",\n  \"recommonmark\",\n  \"docutils\",\n]\n\ngithub-actions = [\"pytest-github-actions-annotate-failures\"]\n\n[tool.setuptools.packages.find]\nexclude = [\"build_tools\"]\n\n[build-system]\nbuild-backend = \"setuptools.build_meta\"\nrequires = [\n  \"setuptools>=70.0.0\",\n]\n\n[tool.ruff]\nline-length = 88\nexclude = [\n  \"docs/build/\",\n  \"node_modules/\",\n  \".eggs/\",\n  \"versioneer.py\",\n  \"venv/\",\n  \".venv/\",\n  \".git/\",\n  \".history/\",\n]\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"W\", \"C4\", \"S\"]\nextend-select = [\n  \"I\", # isort\n  \"C4\", # https://pypi.org/project/flake8-comprehensions\n]\nextend-ignore = [\n  \"E203\", # space before : (needed for how black formats slicing)\n  \"E402\", # module level import not at top of file\n  \"E731\", # do not assign a lambda expression, use a def\n  \"E741\", # ignore not easy to read variables like i l I etc.\n  \"C406\", # Unnecessary list literal - rewrite as a dict literal.\n  \"C408\", # Unnecessary dict call - rewrite as a literal.\n  \"C409\", # Unnecessary list passed to tuple() - rewrite as a tuple literal.\n  \"F401\", # unused imports\n  \"S101\", # use of assert\n]\n\n[tool.ruff.lint.isort]\nknown-first-party = [\"pytorch_forecasting\"]\ncombine-as-imports = true\nforce-sort-within-sections = true\n\n[tool.black]\nline-length = 88\ninclude = '\\.pyi?$'\nexclude = '''\n(\n  /(\n      \\.eggs         # exclude a few common directories in the\n    | \\.git          # root of the project\n    | \\.hg\n    | \\.mypy_cache\n    | \\.tox\n    | \\.venv\n    | _build\n    | buck-out\n    | build\n    | dist\n  )/\n  | docs/build/\n  | node_modules/\n  | venve/\n  | .venv/\n)\n'''\n\n[tool.nbqa.mutate]\nruff = 1\nblack = 1\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 1.1279296875,
          "content": "[pytest]\naddopts =\n    -rsxX\n    -vv\n\n    --cov-config=.coveragerc\n    --cov=pytorch_forecasting\n    --cov-report=html\n    --cov-report=term-missing:skip-covered\n    --no-cov-on-fail\n\nmarkers =\ntestpaths = tests/\nlog_cli_level = ERROR\nlog_format = %(asctime)s %(levelname)s %(message)s\nlog_date_format = %Y-%m-%d %H:%M:%S\ncache_dir = .cache\n# suppress warnings that are expected given \"small\" data for testing\nfilterwarnings =\n    ignore:Found \\d+ unknown classes which were set to NaN:UserWarning\n    ignore:Less than \\d+ samples available for \\d+ prediction times. Use ba:UserWarning\n    ignore:scale is below 1e-7 - consider not centering the data or using data with:UserWarning\n    ignore:You defined a `validation_step` but have no `val_dataloader`:UserWarning\n    ignore:ReduceLROnPlateau conditioned on metric:RuntimeWarning\n    ignore:The number of training samples \\(\\d+\\) is smaller than the logging interval Trainer\\(:UserWarning\n    ignore:The dataloader, [\\_\\s]+ \\d+, does not have many workers which may be a bottleneck.:UserWarning\n    ignore:Consider increasing the value of the `num_workers` argument`:UserWarning\n    ignore::UserWarning\n"
        },
        {
          "name": "pytorch_forecasting",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.1748046875,
          "content": "[coverage:report]\nignore_errors = False\nshow_missing = true\n\n\n[mypy]\nignore_missing_imports = true\nno_implicit_optional = true\ncheck_untyped_defs = true\n\ncache_dir = .cache/mypy/\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}