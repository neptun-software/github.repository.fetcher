{
  "metadata": {
    "timestamp": 1736560011225,
    "page": 820,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ClusterHQ/flocker",
      "stars": 3388,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.0380859375,
          "content": "[run]\nomit =\n    */flocker/_version.py\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0322265625,
          "content": "flocker/_version.py export-subst\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.79296875,
          "content": "# Floobits\n.floo\n.flooignore\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n\n# Editor artifacts\n*~\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbin/\nbuild/\ndevelop-eggs/\ndist/\neggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\ntwisted/plugins/dropin.cache\nMANIFEST\nflocker.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.cache\nnosetests.xml\ncoverage.xml\n_trial_temp*\n.hypothesis\n\n# Translations\n*.mo\n\n# Mr Developer\n.mr.developer.cfg\n.project\n.pydevproject\n\n# Rope\n.ropeproject\n\n# Django stuff:\n*.log\n*.pot\n\n# Sphinx documentation\ndocs/_build/\n\n# Mac stuff\n.DS_Store\n.DS_Store?\n._*\n.Spotlight-V100\n.Trashes\nehthumbs.db\nThumbs.db\njobs.groovy\n\n# Vagrant dev items\nVagrantfile\n.vagrant/\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 17.0009765625,
          "content": "[MASTER]\n\n# Specify a configuration file.\n#rcfile=\n\n# Python code to execute, usually for sys.path manipulation such as\n# pygtk.require().\ninit-hook=\"import flocker\"\n\n# Add files or directories to the blacklist. They should be base names, not\n# paths.\nignore=\n\n# Pickle collected data for later comparisons.\npersistent=no\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Use multiple processes to speed up Pylint.\n# DO NOT CHANGE THIS VALUES >1 HIDE RESULTS!!!!!\njobs=1\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n# A comma-separated list of package or module names from where C extensions may\n# be loaded. Extensions are loading into the active Python interpreter and may\n# run arbitrary code\nextension-pkg-whitelist=\n\n# Allow optimization of some AST trees. This will activate a peephole AST\n# optimizer, which will apply various small optimizations. For instance, it can\n# be used to obtain the result of joining multiple strings with the addition\n# operator. Joining a lot of strings can lead to a maximum recursion error in\n# Pylint and this flag can prevent that. It has one side effect, the resulting\n# AST will be different than the one from reality.\noptimize-ast=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED\nconfidence=\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time. See also the \"--disable\" option for examples.\ndisable=all\n\nenable=import-error,\n       import-self,\n       reimported,\n       wildcard-import,\n       misplaced-future,\n       relative-import,\n       deprecated-module,\n       unpacking-non-sequence,\n       invalid-all-object,\n       undefined-all-variable,\n       used-before-assignment,\n       cell-var-from-loop,\n       global-variable-undefined,\n       redefined-builtin,\n       redefine-in-handler,\n       unused-import,\n       unused-wildcard-import,\n       global-variable-not-assigned,\n       undefined-loop-variable,\n       global-statement,\n       global-at-module-level,\n       bad-open-mode,\n       redundant-unittest-assert,\n       boolean-datetime,\n       # Has common issues with our style due to\n       # https://github.com/PyCQA/pylint/issues/210\n       unused-variable\n\n# Things we'd like to enable someday:\n# redefined-outer-name (requires a bunch of work to clean up our code first)\n# undefined-variable (re-enable when pylint fixes https://github.com/PyCQA/pylint/issues/760)\n# no-name-in-module (giving us spurious warnings https://github.com/PyCQA/pylint/issues/73)\n# unused-argument (need to clean up or code a lot, e.g. prefix unused_?)\n\n# Things we'd like to try.\n# Procedure:\n# 1. Enable a bunch.\n# 2. See if there's spurious ones; if so disable.\n# 3. Record above.\n# 4. Remove from this list.\n       # deprecated-method,\n       # anomalous-unicode-escape-in-string,\n       # anomalous-backslash-in-string,\n       # not-in-loop,\n       # function-redefined,\n       # continue-in-finally,\n       # abstract-class-instantiated,\n       # star-needs-assignment-target,\n       # duplicate-argument-name,\n       # return-in-init,\n       # too-many-star-expressions,\n       # nonlocal-and-global,\n       # return-outside-function,\n       # return-arg-in-generator,\n       # invalid-star-assignment-target,\n       # bad-reversed-sequence,\n       # nonexistent-operator,\n       # yield-outside-function,\n       # init-is-generator,\n       # nonlocal-without-binding,\n       # lost-exception,\n       # assert-on-tuple,\n       # dangerous-default-value,\n       # duplicate-key,\n       # useless-else-on-loop,\n       # expression-not-assigned,\n       # confusing-with-statement,\n       # unnecessary-lambda,\n       # pointless-statement,\n       # pointless-string-statement,\n       # unnecessary-pass,\n       # unreachable,\n       # eval-used,\n       # exec-used,\n       # bad-builtin,\n       # using-constant-test,\n       # deprecated-lambda,\n       # bad-super-call,\n       # missing-super-argument,\n       # slots-on-old-class,\n       # super-on-old-class,\n       # property-on-old-class,\n       # not-an-iterable,\n       # not-a-mapping,\n       # format-needs-mapping,\n       # truncated-format-string,\n       # missing-format-string-key,\n       # mixed-format-string,\n       # too-few-format-args,\n       # bad-str-strip-call,\n       # too-many-format-args,\n       # bad-format-character,\n       # format-combined-specification,\n       # bad-format-string-key,\n       # bad-format-string,\n       # missing-format-attribute,\n       # missing-format-argument-key,\n       # unused-format-string-argument,\n       # unused-format-string-key,\n       # invalid-format-index,\n       # bad-indentation,\n       # mixed-indentation,\n       # unnecessary-semicolon,\n       # lowercase-l-suffix,\n       # fixme,\n       # invalid-encoded-data,\n       # unpacking-in-except,\n       # import-star-module-level,\n       # parameter-unpacking,\n       # long-suffix,\n       # old-octal-literal,\n       # old-ne-operator,\n       # backtick,\n       # old-raise-syntax,\n       # print-statement,\n       # metaclass-assignment,\n       # next-method-called,\n       # dict-iter-method,\n       # dict-view-method,\n       # indexing-exception,\n       # raising-string,\n       # standarderror-builtin,\n       # using-cmp-argument,\n       # cmp-method,\n       # coerce-method,\n       # delslice-method,\n       # getslice-method,\n       # hex-method,\n       # nonzero-method,\n       # oct-method,\n       # setslice-method,\n       # apply-builtin,\n       # basestring-builtin,\n       # buffer-builtin,\n       # cmp-builtin,\n       # coerce-builtin,\n       # old-division,\n       # execfile-builtin,\n       # file-builtin,\n       # filter-builtin-not-iterating,\n       # no-absolute-import,\n       # input-builtin,\n       # intern-builtin,\n       # long-builtin,\n       # map-builtin-not-iterating,\n       # range-builtin-not-iterating,\n       # raw_input-builtin,\n       # reduce-builtin,\n       # reload-builtin,\n       # round-builtin,\n       # unichr-builtin,\n       # unicode-builtin,\n       # xrange-builtin,\n       # zip-builtin-not-iterating,\n       # logging-format-truncated,\n       # logging-too-few-args,\n       # logging-too-many-args,\n       # logging-unsupported-format,\n       # logging-not-lazy,\n       # logging-format-interpolation,\n       # invalid-unary-operand-type,\n       # unsupported-binary-operation,\n       # no-member,\n       # not-callable,\n       # redundant-keyword-arg,\n       # assignment-from-no-return,\n       # assignment-from-none,\n       # not-context-manager,\n       # repeated-keyword,\n       # missing-kwoa,\n       # no-value-for-parameter,\n       # invalid-sequence-index,\n       # invalid-slice-index,\n       # too-many-function-args,\n       # unexpected-keyword-arg,\n       # unsupported-membership-test,\n       # unsubscriptable-object,\n       # access-member-before-definition,\n       # method-hidden,\n       # assigning-non-slot,\n       # duplicate-bases,\n       # inconsistent-mro,\n       # inherit-non-class,\n       # invalid-slots,\n       # invalid-slots-object,\n       # no-method-argument,\n       # no-self-argument,\n       # unexpected-special-method-signature,\n       # non-iterator-returned,\n       # protected-access,\n       # arguments-differ,\n       # attribute-defined-outside-init,\n       # no-init,\n       # abstract-method,\n       # signature-differs,\n       # bad-staticmethod-argument,\n       # non-parent-init-called,\n       # super-init-not-called,\n       # bad-except-order,\n       # catching-non-exception,\n       # bad-exception-context,\n       # notimplemented-raised,\n       # raising-bad-type,\n       # raising-non-exception,\n       # misplaced-bare-raise,\n       # duplicate-except,\n       # broad-except,\n       # nonstandard-exception,\n       # binary-op-exception,\n       # bare-except,\n       # not-async-context-manager,\n       # yield-inside-async-function,\n\n# ...\n[REPORTS]\n\n# Set the output format. Available formats are text, parseable, colorized, msvs\n# (visual studio) and html. You can also give a reporter class, eg\n# mypackage.mymodule.MyReporterClass.\noutput-format=parseable\n\n# Put messages in a separate file for each module / package specified on the\n# command line instead of printing them on stdout. Reports (if any) will be\n# written in a file name \"pylint_global.[txt|html]\".\nfiles-output=no\n\n# Tells whether to display a full report or only the messages\nreports=no\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\n#msg-template=\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format\nlogging-modules=logging\n\n\n[FORMAT]\n\n# Maximum number of characters on a single line.\nmax-line-length=100\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=^\\s*(# )?<?https?://\\S+>?$\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=no\n\n# List of optional constructs for which whitespace checking is disabled. `dict-\n# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\\n222: 2}.\n# `trailing-comma` allows a space between comma and closing bracket: (a, ).\n# `empty-line` allows space-only lines.\nno-space-check=trailing-comma,dict-separator\n\n# Maximum number of lines in a module\nmax-module-lines=1000\n\n# String used as indentation unit. This is usually \"    \" (4 spaces) or \"\\t\" (1\n# tab).\nindent-string='    '\n\n# Number of spaces of indent required inside a hanging  or continued line.\nindent-after-paren=4\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n\n[TYPECHECK]\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis. It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# List of classes names for which member attributes should not be checked\n# (useful for classes with attributes dynamically set). This supports can work\n# with qualified names.\nignored-classes=\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n\n[VARIABLES]\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# A regular expression matching the name of dummy variables (i.e. expectedly\n# not used).\ndummy-variables-rgx=^_|^dummy\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,_cb\n\n\n[SIMILARITIES]\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n\n[SPELLING]\n\n# Spelling dictionary name. Available dictionaries: none. To make it working\n# install python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=FIXME,XXX,TODO\n\n\n[BASIC]\n\n# List of builtins function names that should not be used, separated by a comma\nbad-functions=map,filter,input\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=i,j,k,ex,Run,_\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=foo,bar,baz,toto,tutu,tata\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Include a hint for the correct naming format with invalid-name\ninclude-naming-hint=no\n\n# Regular expression matching correct function names\nfunction-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for function names\nfunction-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression matching correct variable names\nvariable-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for variable names\nvariable-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression matching correct constant names\nconst-rgx=(([A-Z_][A-Z0-9_]*)|(__.*__))$\n\n# Naming hint for constant names\nconst-name-hint=(([A-Z_][A-Z0-9_]*)|(__.*__))$\n\n# Regular expression matching correct attribute names\nattr-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for attribute names\nattr-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression matching correct argument names\nargument-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for argument names\nargument-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression matching correct class attribute names\nclass-attribute-rgx=([A-Za-z_][A-Za-z0-9_]{2,30}|(__.*__))$\n\n# Naming hint for class attribute names\nclass-attribute-name-hint=([A-Za-z_][A-Za-z0-9_]{2,30}|(__.*__))$\n\n# Regular expression matching correct inline iteration names\ninlinevar-rgx=[A-Za-z_][A-Za-z0-9_]*$\n\n# Naming hint for inline iteration names\ninlinevar-name-hint=[A-Za-z_][A-Za-z0-9_]*$\n\n# Regular expression matching correct class names\nclass-rgx=[A-Z_][a-zA-Z0-9]+$\n\n# Naming hint for class names\nclass-name-hint=[A-Z_][a-zA-Z0-9]+$\n\n# Regular expression matching correct module names\nmodule-rgx=(([a-z_][a-z0-9_]*)|([A-Z][a-zA-Z0-9]+))$\n\n# Naming hint for module names\nmodule-name-hint=(([a-z_][a-z0-9_]*)|([A-Z][a-zA-Z0-9]+))$\n\n# Regular expression matching correct method names\nmethod-rgx=[a-z_][a-z0-9_]{2,30}$\n\n# Naming hint for method names\nmethod-name-hint=[a-z_][a-z0-9_]{2,30}$\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=^_\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=-1\n\n\n[ELIF]\n\n# Maximum number of nested blocks for function / method body\nmax-nested-blocks=5\n\n\n[IMPORTS]\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=regsub,TERMIOS,Bastion,rexec\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n\n[DESIGN]\n\n# Maximum number of arguments for function / method\nmax-args=5\n\n# Argument names that match this expression will be ignored. Default to name\n# with leading underscore\nignored-argument-names=_.*\n\n# Maximum number of locals for function / method body\nmax-locals=15\n\n# Maximum number of return / yield for function / method body\nmax-returns=6\n\n# Maximum number of branch for function / method body\nmax-branches=12\n\n# Maximum number of statements in function / method body\nmax-statements=50\n\n# Maximum number of parents for a class (see R0901).\nmax-parents=7\n\n# Maximum number of attributes for a class (see R0902).\nmax-attributes=7\n\n# Minimum number of public methods for a class (see R0903).\nmin-public-methods=2\n\n# Maximum number of public methods for a class (see R0904).\nmax-public-methods=20\n\n# Maximum number of boolean expressions in a if statement\nmax-bool-expr=5\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,__new__,setUp\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,_fields,_replace,_source,_make\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=Exception\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.818359375,
          "content": "# Copyright ClusterHQ Inc.  See LICENSE file for details.\n#\n# Run a subset of Flocker tests on Travis-CI.\nsudo: false\n\ndist: trusty\n\nlanguage: python\n\npython: 2.7\n\ncache:\n  pip: true\n  apt: true\n\ninstall:\n  - pip install tox awscli\n\nenv:\n  - FLOCKER_BUILDER=test-admin\n  - FLOCKER_BUILDER=test-benchmark\n  - FLOCKER_BUILDER=test-flocker\n  - FLOCKER_BUILDER=lint\n  - FLOCKER_BUILDER=docs-lint\n  - FLOCKER_BUILDER=docs-spelling\n  - FLOCKER_BUILDER=docs-linkcheck\n  - FLOCKER_BUILDER=docs-html\n\nmatrix:\n  allow_failures:\n  - env: FLOCKER_BUILDER=docs-linkcheck\n\nscript:\n  - .travis/script\n\nafter_script:\n  - .travis/after_script\n\naddons:\n  apt:\n    packages:\n    - python-enchant\n\n# Only run tests on push on a few branches.\n# Test on PR should be execute for all branches and forks.\nbranches:\n  only:\n  - master\n  - /^release\\/flocker-.*$/\n"
        },
        {
          "name": ".travis",
          "type": "tree",
          "content": null
        },
        {
          "name": "AUTHORS.rst",
          "type": "blob",
          "size": 0.4912109375,
          "content": "Flocker is maintained by ClusterHQ and is licensed under the Apache Software License 2.0.\n\nThe following people and organizations contributed to its development; please add your name in alphabetical order with your first pull request:\n\n* ClusterHQ Inc. (formerly Hybrid Logic Ltd.)\n* Scotch Media (Base for documentation theme)\n* Wentao Zhang (zhangwentao234@huaewi.com, Huawei Ltd.)\n* Sean McGinnis (Dell SC Series storage configuration documentation)\n* Huai Cheng Zheng <huaicheng.zheng@transwarp.io>\n"
        },
        {
          "name": "CONTRIBUTING.rst",
          "type": "blob",
          "size": 8.193359375,
          "content": ".. _contribute:\n\n=======================\nContributing to Flocker\n=======================\n\nIntroduction\n============\n\nClusterHQ develops software using test-driven development, code reviews and per-issue branches.\n\n* Each unit of work is defined in an issue in the issue tracker and developed on a branch.\n\n* Code is written using test-driven development.\n\n* The issue is closed by merging the branch (via a GitHub pull request).\n\n* Before a branch is merged it must pass code review.\n\n* The code reviewer ensures that the pull request:\n    * Follows the coding standard (Python's `PEP 8`_).\n\n    * Includes appropriate documentation.\n\n    * Has full test coverage (unit tests and functional tests).\n\n    * The tests pass in the continuous integration system (`Buildbot`_).\n\n    * Resolves the issue.\n\n* The code reviewer can approve the pull request for merging as is, with some changes, or request changes and an additional review.\n\n.. _PEP 8: http://legacy.python.org/dev/peps/pep-0008/\n.. _Buildbot: http://build.clusterhq.com/\n\n\n.. _talk-to-us:\n\nTalk to Us\n==========\n\nHave questions or need help?\n\n* If you have problems running Flocker, please read our `debugging documentation`_.\n* If you want to follow our development plans, our main issue tracker is `JIRA`_.\n* You can open an account there to file issues, but we're also happy to accept a `GitHub issue`_ with feature requests or bug reports. `Security issues`_  should be reported directly to our security team.\n* You can also join us on the ``#clusterhq`` channel on the ``irc.freenode.net`` IRC network or on the `flocker-users Google Group`_.\n\n.. _debugging documentation: https://flocker-docs.clusterhq.com/en/latest/administering/debugging.html\n.. _Security issues: https://flocker-docs.clusterhq.com/en/latest/gettinginvolved/contributing.html#reporting-security-issues\n.. _flocker-users Google Group: https://groups.google.com/forum/?hl=en#!forum/flocker-users\n\n\nDevelopment Environment\n=======================\n\nYou will need Python 2.7 installed on your development machine.\nTo run the complete test suite you will also need `Docker`_ installed.\n\nInstall Flocker's development dependencies in a ``virtualenv`` by running the following commands:\n\n.. prompt:: bash $\n\n   mkvirtualenv flocker\n   pip install --requirement dev-requirements.txt\n\n.. _Docker: https://www.docker.com/\n\n\nCentOS 7\n^^^^^^^^\n\n.. prompt:: bash $\n\n   sudo yum install git python-virtualenv libffi-devel openssl-devel gcc enchant-devel\n\nUbuntu\n^^^^^^\n\n.. prompt:: bash $\n\n   sudo apt-get install git virtualenvwrapper python-dev libffi-dev libssl-dev enchant\n\nRunning Tests\n=============\n\nYou can run all unit tests by doing:\n\n.. prompt:: bash $\n\n   tox\n\nYou can also run specific tests in a specific environment:\n\n.. prompt:: bash $\n\n   tox -e py27 flocker.control.test.test_httpapi\n\nFunctional tests require ``Docker`` to be installed and, in the case of Docker, running.\nIn addition, ``tox`` needs to be run as root:\n\n.. prompt:: bash $\n\n   sudo tox\n\nSince these tests involve global state on your machine (filesystems, ``iptables``, Docker containers, etc.) we recommend running them in a virtual machine.\n\n\nRunning Lint Tests\n==================\n\nYou can run ``flake8`` and ``pylint`` tests by doing:\n\n.. prompt:: bash $\n\n   tox -e lint\n\n\nDocumentation\n=============\n\nDocumentation is generated using `Sphinx`_ and stored in the ``docs/`` directory.\nYou can build it individually by running:\n\n.. prompt:: bash $\n\n   tox -e sphinx\n\nYou can view the result by opening ``docs/_build/html/index.html`` in your browser.\n\n.. _Sphinx: http://sphinx-doc.org/\n\n\nContributing to Flocker\n=======================\n\nIf you have any feature requests or suggestions, we would love to hear about them.\n\nAt a minimum you can simply submit a GitHub Pull Request with your changes.\nIn order to maximize your chances of getting your code accepted, and to keep you from wasting time:\n\n* Discuss your ideas with us in advance by filing a `GitHub issue`_.\n* Explain the purpose of your PR, and why these changes are necessary.\n* Limit your PR to fixing a single problem or adding a single feature.\n* See the merge requirements below for details about our testing and documentation requirements.\n\nMake sure your PR adds your name to ``AUTHORS.rst`` if you've never contributed to Flocker before.\n\nOnce your pull request is merged, as a small thank you for contributing to Flocker we'd like to send you some ClusterHQ swag.\nJust send an email to thankyou@clusterhq.com with your t-shirt size, mailing address and a phone number to be used only for filling out the shipping form.\nWe'll get something in the mail to you.\n\nMerge Requirements\n^^^^^^^^^^^^^^^^^^\n\nWhile we're happy to look at contributions in any state as GitHub PRs, the requirements below will need to be met before code is merged.\n\n1. All code must have unit test coverage and to the extent possible functional test coverage.\n\n   Use the ``coverage.py`` tool with the ``--branch`` option to generate line and branch coverage reports.\n   This report can tell you if you missed anything.\n   It does not necessarily catch everything though.\n   Treat it as a helper but not the definitive indicator of success.\n   You can also see coverage output in the Buildbot details link of your pull request.\n   Practice test-driven development to ensure all code has test coverage.\n\n2. All code must have documentation.\n\n   Modules, functions, classes, and methods must be documented (even if they are private).\n   Function parameters and object attributes must be documented (even if they are private).\n\n3. All user-facing tools must have documentation.\n\n   Document tool usage as part of big-picture documentation.\n   Identify useful goals the user may want to accomplish and document tools within the context of accomplishing those goals.\n   Documentation should be as accessible and inclusive as possible.\n   Avoid language and markup which assumes the ability to precisely use a mouse and keyboard, or that the reader has perfect vision.\n   Create alternative but equal documentation for the visually impaired, for example, by using alternative text on all images.\n   If in doubt, particularly about markup changes, use http://achecker.ca/checker/index.php and fix any \"Known Problems\" and \"Likely Problems\".\n\n\nProject Development Process\n===========================\n\nThe core development team uses a `JIRA`_ workflow to track planned work.\nIssues are organized by sprints, and can reside in various states:\n\nBacklog\n    All issues start in the backlog when they are filed.\n\nDesign Backlog\n    The issue requires a design, and will be worked on soon.\n\nDesign\n    The issue is currently being designed.\n\nDesign Review Ready\n    The design is ready for review.\n    This often involves submitting a GitHub pull request with a sketch of the code.\n\nCode Backlog\n    The design has been approved and is ready to code.\n\nCoding\n    The issue is currently being coded.\n\nCode Review Ready\n    The code is ready for review.\n    This typically involves submitting a GitHub pull request.\n\nCode Review\n    The code is being reviewed.\n\nDone\n    The issue has been closed.\n    Some final work may remain to address review comments; once this is done and the branch is merged the GitHub PR will be closed.\n\n\n.. _reporting-security-issues:\n\nReporting Security Issues\n=========================\n\nPlease report security issues by emailing security@clusterhq.com.\n\nFlocker bugs should normally be `reported publicly`_, but due to the sensitive nature of security issues, we ask that they not be publicly reported in this fashion.\n\nInstead, if you believe you have found something in Flocker (or any other ClusterHQ software) which has security implications, please send a description of the issue via email to security@clusterhq.com.\nYour message will be forwarded to the ClusterHQ security team (a small group of trusted developers) for triage and it will not be publicly readable.\nOnce you have submitted an issue via email, you should receive an acknowledgment from a member of the security team within 48 hours, and depending on the action to be taken, you may receive further follow up emails.\n\n.. _JIRA: https://clusterhq.atlassian.net/secure/Dashboard.jspa\n.. _GitHub issue: https://github.com/ClusterHQ/flocker/issues\n.. _reported publicly: https://flocker-docs.clusterhq.com/en/latest/gettinginvolved/contributing.html#talk-to-us\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 10.505859375,
          "content": "                                Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   Copyright 2014-2016 ClusterHQ\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.3798828125,
          "content": "include README.rst\ninclude versioneer.py\ninclude flocker/_version.py\ngraft requirements\ninclude dev-requirements.txt\nrecursive-include admin *.py\n\n# These data files are used by the volumes API to define input and output\n# schemas.\nrecursive-include flocker/control/schema *.yml\n\n# JSON files used by persistence service tests.\nrecursive-include flocker/control/test/configurations *.json\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 2.5634765625,
          "content": "Flocker\n=======\n\nFlocker is an open-source Container Data Volume Manager for your Dockerized applications.\n\nBy providing tools for data migrations, Flocker gives ops teams the tools they need to run containerized stateful services like databases in production.\n\nUnlike a Docker data volume which is tied to a single server, a Flocker data volume, called a dataset, is portable and can be used with any container, no matter where that container is running.\n\nFlocker manages Docker containers and data volumes together.\nWhen you use Flocker to manage your stateful microservice, your volumes will follow your containers when they move between different hosts in your cluster.\n\nYou can also use Flocker to manage only your volumes, while continuing to manage your containers however you choose.\n\n\nAbout Us\n--------\n\nFlocker is being developed by `ClusterHQ`_.\nWe are a small team of engineers with experience running distributed systems and many of us are core contributors to the `Twisted`_ project.\n\nThis project is under active development; version 1.0 was released on June 17th, 2015.\nContributions are welcome.\nIf you have any issues or feedback, you can `talk to us`_.\nWe're looking forward to working on this project with you.\n\n\nDocumentation\n-------------\n\nYou can read more about installing Flocker, follow a tutorial and learn about the features of Flocker and its architecture in the `Flocker docs`_.\n\n\nFeature Requests\n----------------\n\nIf you have any feature requests or suggestions, we would love to hear about them.\nPlease send us your ideas by filing a `GitHub issue`_.\n\n\nTests\n-----\n\nFlocker's test suite is based on `unittest`_ and `Twisted Trial`_.\nThe preferred way to run the test suite is using the command ``trial flocker``.\nFlocker also includes a `tox`_ configuration to run the test suite in multiple environments and to run additional checks\n(such as `flake8`_) and build the documentation with Sphinx.\nYou can run all of the tox environments using the command ``tox``.\n\nFlocker is also tested using `continuous integration`_.\n\n.. _ClusterHQ: https://clusterhq.com/\n.. _Twisted: https://twistedmatrix.com/trac/\n.. _Flocker docs: https://flocker.readthedocs.io/\n.. _unittest: https://docs.python.org/2/library/unittest.html\n.. _Twisted Trial: https://twistedmatrix.com/trac/wiki/TwistedTrial\n.. _tox: https://tox.readthedocs.org/\n.. _continuous integration: http://build.clusterhq.com/\n.. _talk to us: http://flocker-docs.clusterhq.com/en/latest/gettinginvolved/contributing.html#talk-to-us\n.. _flake8: https://pypi.python.org/pypi/flake8\n.. _GitHub issue: https://github.com/clusterhq/flocker/issues\n"
        },
        {
          "name": "admin",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.yaml",
          "type": "blob",
          "size": 66.853515625,
          "content": "# vim: ai ts=4 sts=4 et sw=4 ft=yaml et\n#\n# build.yaml\n#\n# This file contains the Jenkins job definitions for the Flocker project.\n#\n# We add new or reconfigure existing jobs in Jenkins using the Job DSL plugin.\n# https://github.com/jenkinsci/job-dsl-plugin\n#\n# That plugin consumes 'groovy Job DSL' code through FreeStyle Jenkins job\n# types which contain a 'Process Job DSLs' step.\n#\n# As part of the provisioning process for the Jenkins Master, we configure a\n# job called 'setup_ClusterHQ_Flocker' which is responsible for querying the\n# github clusterhq/flocker respository and retrieving this build.yaml file, as\n# well as the jobs.groovy.j2 jinja file.\n#\n# As part of the provisioning process for the Jenkins Master, we also deploy\n# a small python script '/usr/local/bin/render.py', this script simply contains\n# code to read a YAML file into a dictionary and expand a Jinja2 template using\n# the k,v in that dict.\n#\n# Our job 'setup_ClusterHQ_Flocker' when running, will run the 'render.py' with\n# this build.yaml file and produce a jobs.groovy file.\n# We do this, because we'd rather configure our jobs with YAML than with Groovy\n#\n# The next step in the job is a 'Process JOB DSL's' step which consumes the\n# jobs.groovy file, and generates all the jenkins folders and jobs.\n#\n#\n# We pass the branch name as a parameter to the setup_clusterhq_flocker job,\n# the parameter is shown as 'RECONFIGURE_BRANCH'.\n#\n# The setup job only produces jobs for a single branch . We don't produce jobs\n# for every branch due to the large number of branches in the repository, which\n# would generate over 16000 jobs and take over an hour to run.\n#\n# The workflow is that, when a developer is working on a feature branch and is\n# happy to start testing some of his code they will execute the job\n# setup_clusterhq_flocker passing his branch as the parameter.\n#\n# Their jobs will then be available under the path:\n# /ClusterHQ-Flocker/<branch>/\n#\n# Inside that folder there will be a large number of jobs, where at the top\n# she/he can see a job called '_main_multijob'. This job is responsible for\n# executing all other jobs in parallel and collecting the produced artifacts\n# from each job after its execution.\n#\n# The artifacts in this case are trial logs, coverage xml reports, subunit\n# reports.\n#\n# Those artifacts are consumed by the _main_multijob to produce an overall\n# coverage report, and an aggregated summary of all the executed tests and\n# their failures/skips/successes.\n#\n\n# The project contains the github owner and repository to build\nproject: 'ClusterHQ/flocker'\n\n# git_url, contains the full HTTPS url for the repository to build\ngit_url: 'https://github.com/ClusterHQ/flocker.git'\n\n\n# We use a set of YAML aliases and anchors to define the different steps\n# in our jobs.\n# This helps us to keep some of the code DRY, we are not forced to use YAML\n# operators, a different approach could be used:\n#  - bash functions\n#  - python functions\n#  - Rust or D code\n#\ncommon_cli:\n  hashbang: &hashbang |\n    #!/bin/bash -l\n    # don't leak secrets\n    set +x\n    set -e\n\n\n  # add_shell_functions, contains our the bash functions consumed by the\n  # the build script.\n  # We set the shebang to /bin/bash. Jenkins 'should' respect this.\n  # Note:\n  # We noticed that our Ubuntu /bin/bash call were being executed as /bin/sh.\n  # So we as part of the slave image build process symlinked\n  # /bin/sh -> /bin/bash.\n  # TODO: https://clusterhq.atlassian.net/browse/FLOC-2986\n  add_shell_functions: &add_shell_functions |\n\n    # set default AWS region for S3 operations\n    export S3_REGION=us-west-2\n    # Docs buckets (clusterhq-staging-docs, doc-dev.clusterhq.com) are hosted in us-east-1\n    export S3_DOCS_REGION=us-east-1\n\n    # The long directory names where we build our code cause pip to fail.\n    # https://gitlab.com/gitlab-org/gitlab-ci-multi-runner/issues/20\n    # http://stackoverflow.com/questions/10813538/shebang-line-limit-in-bash-and-linux-kernel\n    # https://github.com/spotify/dh-virtualenv/issues/10\n    # so we set our virtualenv dir to live in /tmp/<random number>\n    #\n    export venv=/tmp/${RANDOM}\n    # make sure the virtualenv doesn't already exist\n    while [ -e ${venv} ]\n    do\n      export venv=/tmp/${RANDOM}\n    done\n\n    # fix the docker permission issue, the base image doesn't have the correct\n    # permissions/owners.\n    # This is to be tackled as part of:\n    # https://clusterhq.atlassian.net/browse/FLOC-2689\n    test -e /var/run/docker.sock && sudo chmod 777 /var/run/docker.sock\n\n    # Returns the ip address for eth0\n    # We consume this as part or the vagrant build tests.\n    # Those tests run on our Mesos Cluster, the job connects to a local nginx\n    # instance running on the Mesos Slave where the job is running.\n    export eth0_ip=$( ip -o -4 addr show eth0 |awk '{ print $4 '} | cut -f 1 -d \"/\")\n\n    # pass the exit code from the previous and return the the aggregated\n    # exit status for the whole job.\n    function updateExitStatus {\n      # if we had previous failures, we just fail here.\n      if [ \"${JOB_EXIT_STATUS}\" != \"0\" ] && [ \"${JOB_EXIT_STATUS}\" != \"\" ]; then\n        echo 1\n      else\n         if [ \"$1\" !=  \"0\" ]; then\n         # first failure on the job, lets return 1, which will set\n         # JOB_EXIT_STATUS to 1 and mark the job as failed.\n             echo 1\n         fi\n      fi\n    }\n\n    # Retries a command if it fails, using the exponential backoff algorithm.\n    # Usage:\n    # retry_n_times_with_timeout <max retries> <timeout in seconds> command\n    #\n    function retry_n_times_with_timeout {\n      set +e\n      max_tries=${1}\n      seconds=${2}\n      shift 2\n\n      let delay=15\n      let tries=0\n      while [ ${tries} -lt ${max_tries} ]; do\n        echo \"executing...  ${*}\"\n        if (timeout --foreground --signal=SIGKILL ${seconds} bash -c \"${*}\"); then\n            break\n        fi\n        echo Command failed, waiting ${delay}...\n        sleep ${delay}\n        let delay=delay*2\n        let tries=tries+1\n\n        # abort on too many failures\n        if [ ${tries} -eq ${max_tries} ]; then\n          echo \"too many failed retries... aborting\"\n          exit 1\n        fi\n\n      done\n    }\n\n    # uploads a file to S3, retrying on failure.\n    # usage:\n    # s3_upload <bucket> <filename>\n    function s3_upload() {\n      echo \"uploading ${2} to S3 bucket ${1} ...\"\n      retry_n_times_with_timeout 3 900 \\\n        aws --region ${S3_REGION} s3 cp \"${2}\" s3://${1}/\n    }\n\n    # creates a metadata file to be used with vagrant up; vagrant box update\n    # usage: create_vagrant_metadata_file\n    #   <filename> <box_name> <description> <version> <url> <sha1>\n    function create_vagrant_metadata_file() {\n      cat <<EOF_METADATA > ${1}\n      {\n        \"name\": \"${2}\",\n        \"description\": \"${3}\",\n        \"versions\": [{\n          \"version\": \"${4}\",\n          \"providers\": [{\n            \"name\": \"virtualbox\",\n            \"url\": \"${5}\",\n            \"checksum_type\": \"sha1\",\n            \"checksum\": \"${6}\"\n          }]\n        }]\n      }\n    EOF_METADATA\n    }\n\n    # destroy the vagrant vm and aborts the build\n    # this is used when one the of the intermediate steps has failed\n    # usage:\n    # <some code> || abort_build\n    function abort_build() {\n      echo \"aborting ...\"\n      VAGRANT_LOG=info vagrant_destroy\n      exit 1\n    }\n\n    # attempts a vagrant up, and aborts the build on failure\n    # usage:\n    # vagrant_up\n    function vagrant_up() {\n      (\n        retry_n_times_with_timeout 3 1200 \\\n          VAGRANT_LOG=info vagrant up\n      ) || abort_build\n    }\n\n    # attempts to refresh the local vagrant box, and aborts the build on error\n    # make sure we are using the latest available version of this vagrant box\n    # usage:\n    # vagrant_box_update\n    function vagrant_box_update() {\n      (\n        retry_n_times_with_timeout 2 1800 \\\n          VAGRANT_LOG=info vagrant box update\n      ) || abort_build\n    }\n\n    # destroys the vagrant vm\n    # usage:\n    # vagrant_destroy\n    function vagrant_destroy() {\n      VAGRANT_LOG=info vagrant destroy -f\n    }\n\n  # TODO: do we need to clean up old files on ubuntu and centos or\n  # does the pre-scm plugin does this correctly for us ?\n  # https://clusterhq.atlassian.net/browse/FLOC-3139\n  cleanup: &cleanup |\n    export PATH=/usr/local/bin:${PATH}\n    # clean up the stuff from previous runs\n    # due to the length of the jobname workspace, we are hitting limits in\n    # our sheebang path name in pip.\n    # https://github.com/spotify/dh-virtualenv/issues/10\n    # http://stackoverflow.com/questions/10813538/shebang-line-limit-in-bash-and-linux-kernel\n    # https://gitlab.com/gitlab-org/gitlab-ci-multi-runner/issues/20\n    # So we will place the virtualenv in /tmp/v instead\n    #\n    rm -rf ${venv}\n\n  setup_venv: &setup_venv |\n    # Set up the new venv.\n    virtualenv -p python2.7 --clear ${venv}\n    . ${venv}/bin/activate\n    # Upgrade Python packaging tools.\n    ${venv}/bin/pip install pip==8.1.2 setuptools==23.0.0 tox==2.3.1\n    # Report the versions to aid debugging.\n    ${venv}/bin/python --version\n    ${venv}/bin/pip --version\n    ${venv}/bin/python \\\n        -c 'import setuptools; print \"SETUPTOOLS_VERSION:\", setuptools.__version__'\n    ${venv}/bin/tox --version\n\n  setup_flocker_modules: &setup_flocker_modules |\n    # installs all the required python modules as well as the flocker code.\n    pip install --requirement dev-requirements.txt\n    # install junix for our coverage report\n    pip install python-subunit junitxml\n\n  setup_aws_env_vars: &setup_aws_env_vars |\n    # set vars and run tests\n    export FLOCKER_FUNCTIONAL_TEST_CLOUD_CONFIG_FILE=/tmp/acceptance.yaml\n    export FLOCKER_FUNCTIONAL_TEST_CLOUD_CONFIG_SECTION=storage-drivers.aws\n    AWS_ZONE=$(\\\n        wget -q -O - \\\n        http://169.254.169.254/latest/meta-data/placement/availability-zone\\\n    )\n    export FLOCKER_FUNCTIONAL_TEST_CONFIG_OVERRIDE_zone=\"${AWS_ZONE}\"\n    # There isn't a separate meta-data endpoint to get the instance region.\n    # So assume that region is always ZONE without the trailing character.\n    export FLOCKER_FUNCTIONAL_TEST_CONFIG_OVERRIDE_region=\"${AWS_ZONE:0:-1}\"\n    export FLOCKER_FUNCTIONAL_TEST_CLOUD_PROVIDER=aws\n    # The minimum size of an EBS volume is 1GiB.\n    export FLOCKER_FUNCTIONAL_TEST_MINIMUM_ALLOCATABLE_SIZE=$((1024 ** 3 * 1))\n\n  setup_rackspace_env_vars: &setup_rackspace_env_vars |\n    # set vars and run tests\n    # The /tmp/acceptance.yaml file is deployed to the jenkins slave during\n    # bootstrapping. These are copied from the Jenkins Master /etc/slave_config\n    # directory.\n    export FLOCKER_FUNCTIONAL_TEST_CLOUD_CONFIG_FILE=/tmp/acceptance.yaml\n    export FLOCKER_FUNCTIONAL_TEST_CLOUD_CONFIG_SECTION=storage-drivers.rackspace\n    export FLOCKER_FUNCTIONAL_TEST_CLOUD_PROVIDER=rackspace\n    export FLOCKER_FUNCTIONAL_TEST_CONFIG_OVERRIDE_region=\"DFW\"\n    # The minimum size of a Rackspace SATA Cinder volume is 75GiB. See:\n    # * http://www.rackspace.com/knowledge_center/product-faq/cloud-block-storage\n    export FLOCKER_FUNCTIONAL_TEST_MINIMUM_ALLOCATABLE_SIZE=$((1024 ** 3 * 75))\n\n  setup_gce_env_vars: &setup_gce_env_vars |\n    # set vars and run tests\n    # The /tmp/acceptance.yaml file is deployed to the jenkins slave during\n    # bootstrapping. These are copied from the Jenkins Master /etc/slave_config\n    # directory.\n    export FLOCKER_FUNCTIONAL_TEST_CLOUD_CONFIG_FILE=/tmp/acceptance.yaml\n    export FLOCKER_FUNCTIONAL_TEST_CLOUD_CONFIG_SECTION=storage-drivers.gce\n    # Segredos has project set to clusterhq-acceptance so override it.\n    export FLOCKER_FUNCTIONAL_TEST_CONFIG_OVERRIDE_project=\"clusterhqjenkins\"\n    # The minimum size of a GCE PD volume is 10GiB.\n    export FLOCKER_FUNCTIONAL_TEST_MINIMUM_ALLOCATABLE_SIZE=$((1024 ** 3 * 10))\n\n  setup_coverage: &setup_coverage |\n    # we install the python coverage module so generate coverage.xml files\n    # which Jenkins will process through the Jenkins Cobertura plugin.\n    # https://wiki.jenkins-ci.org/display/JENKINS/Cobertura+Plugin\n    # This plugin allows for producing coverage reports over time for a\n    # particular job, or aggregated reports across a set of related jobs.\n    # This is achieved by downloading the coverage.xml artifacts from the\n    # downstream child jobs to the parent job and processing those files\n    # one last time through the cobertura plugin.\n    # The resulting report wil contain stats from every single job.\n    pip install coverage\n\n  run_coverage: &run_coverage |\n    # run coverage and produce a report\n    coverage xml --include=flocker*\n\n  convert_results_to_junit: &convert_results_to_junit |\n    # pip the trial.log results through subunit and export them as junit in xml\n    cat trial.log | subunit-1to2 | subunit2junitxml \\\n      --no-passthrough --output-to=results.xml\n\n  # flocker artifacts contains the list of files we want to collect from our\n  # _main_multijob. These are used to produce the coverage, test reports.\n  flocker_artifacts: &flocker_artifacts\n    - results.xml\n    - _trial_temp/test.log\n    - coverage.xml\n\n  # acceptance-test-artifacts contains the list of files we want to collect\n  # from our _main_multijob.\n  # These are the remote logs from the acceptance tests.\n  acceptance_tests_artifacts: &acceptance_tests_artifacts\n    - results.xml\n    - run-acceptance-tests.log\n    - _trial_temp/test.log\n    - remote_logs.log\n\n  # Ubuntu acceptance tests do not collect the logs from the remote nodes\n  # https://clusterhq.atlassian.net/browse/FLOC-2560\n  acceptance_tests_artifacts_ubuntu_special_case: &acceptance_tests_artifacts_ubuntu_special_case\n    - results.xml\n    - run-acceptance-tests.log\n    - remote_logs.log\n    - _trial_temp/test.log\n\n  run_trial_directories_to_delete: &run_trial_directories_to_delete\n    - ${WORKSPACE}/_trial_temp\n    - ${WORKSPACE}/.hypothesis\n\n  run_acceptance_directories_to_delete: &run_acceptance_directories_to_delete\n    - ${WORKSPACE}/dist\n\n  run_trial_with_coverage: &run_trial_with_coverage |\n    # The jobs.groovy.j2 file produces jobs that contain a parameterized job\n    # type. These type of jobs always require a parameter to be passed on in\n    # order for they to be executed.\n    # We grab the value from the 'with_modules:' dictionary in the yaml job\n    # defintion, and feed it to the job configuration as the default value for\n    # the 'MODULE' parameter.\n    # This is how we tell trial which flocker module to call.\n    coverage run ${venv}/bin/trial \\\n      --debug-stacktraces --reporter=subunit \\\n      ${MODULE} 2>&1 | tee trial.log\n\n  run_trial_with_coverage_as_root: &run_trial_with_coverage_as_root |\n    # The jobs.groovy.j2 file produces jobs that contain a parameterized job\n    # type. These type of jobs always require a parameter to be passed on in\n    # order for they to be executed.\n    # We grab the value from the 'with_modules:' dictionary in the yaml job\n    # defintion, and feed it to the job configuration as the default value for\n    # the 'MODULE' parameter.\n    # This is how we tell trial which flocker module to call.\n    sudo ${venv}/bin/coverage run ${venv}/bin/trial \\\n      --debug-stacktraces --reporter=subunit \\\n      ${MODULE} 2>&1 | tee trial.log\n\n  run_trial_for_storage_drivers_with_coverage: &run_trial_for_storage_drivers_with_coverage |\n    # The jobs.groovy.j2 file produces jobs that contain a parameterized job\n    # type. These type of jobs always require a parameter to be passed on in\n    # order for they to be executed.\n    # We grab the value from the 'with_modules:' dictionary in the yaml job\n    # defintion, and feed it to the job configuration as the default value for\n    # Consume the MODULE parameter set in the job configuration\n    sudo -E ${venv}/bin/coverage run ${venv}/bin/trial \\\n      --debug-stacktraces --reporter=subunit \\\n      ${MODULE} 2>&1 | tee trial.log\n\n  setup_authentication: &setup_authentication |\n    # acceptance tests rely on this file existing\n    touch ${HOME}/.ssh/known_hosts\n    chmod 0600 /tmp/id_rsa\n    eval `ssh-agent -s`\n    ssh-add /tmp/id_rsa\n\n  run_acceptance_tests: &run_acceptance_tests |\n    # We gather the return code but make sure we come out of these tests with 0\n    # we store that code and pass it to the end of the job execution,\n    # as part of the JOB_EXIT_STATUS variable.\n    #\n    # The admin/run-acceptance-tests will provision a flocker cluster of\n    # several nodes. These nodes will install the flocker packages (RPM/DEB)\n    # during the provisioning process by that tool. These packages are fetched\n    # from a repository on the network through a common apt-get/yum install.\n    # The jenkins slave will be the repository host containing those packages\n    # which are made available through a webserver running on port 80.\n    # We pass the URL of our Jenkins Slave to the acceptance test nodes\n    # through the --build-server parameter below.\n    #\n    EXTERNAL_IP=$(\\\n        curl \\\n            --silent \\\n            --header \"Metadata-Flavor: Google\" \\\n            http://metadata/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip \\\n    )\n    BUILD_SERVER=\"http://${EXTERNAL_IP}/${BUILD_TAG}/\"\n    ${venv}/bin/python admin/run-acceptance-tests \\\n    --distribution ${DISTRIBUTION_NAME} \\\n    --provider ${ACCEPTANCE_TEST_PROVIDER} \\\n    --dataset-backend ${ACCEPTANCE_TEST_PROVIDER} \\\n    --branch ${TRIGGERED_BRANCH} \\\n    --build-server ${BUILD_SERVER} \\\n    --config-file /tmp/acceptance.yaml \\\n    -- --reporter=subunit ${ACCEPTANCE_TEST_MODULE} 2>&1 | tee trial.log\n    JOB_EXIT_STATUS=\"$( updateExitStatus $? )\"\n\n  run_acceptance_loopback_tests: &run_acceptance_loopback_tests |\n    # We gather the return code but make sure we come out of these tests with 0\n    # we store that code and pass it to the end of the job execution,\n    # as part of the JOB_EXIT_STATUS variable.\n    #\n    # The admin/run-acceptance-tests will provision a flocker cluster with a\n    # single node since the loopback backend doesn't support moving data across\n    # hosts.\n    EXTERNAL_IP=$(\\\n        curl \\\n            --silent \\\n            --header \"Metadata-Flavor: Google\" \\\n            http://metadata/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip \\\n    )\n    BUILD_SERVER=\"http://${EXTERNAL_IP}/${BUILD_TAG}/\"\n    ${venv}/bin/python admin/run-acceptance-tests \\\n    --distribution ${DISTRIBUTION_NAME} --number-of-nodes 1 \\\n    --provider ${ACCEPTANCE_TEST_PROVIDER} --dataset-backend loopback \\\n    --branch ${TRIGGERED_BRANCH} \\\n    --build-server $BUILD_SERVER \\\n    --config-file /tmp/acceptance.yaml \\\n    -- --reporter=subunit ${ACCEPTANCE_TEST_MODULE} 2>&1 | tee trial.log\n    JOB_EXIT_STATUS=\"$( updateExitStatus $? )\"\n\n  run_client_tests: &run_client_tests |\n    # We gather the return code but make sure we come out of these tests with 0\n    # we store that code and pass it to the end of the job execution,\n    # as part of the JOB_EXIT_STATUS variable.\n    EXTERNAL_IP=$(\\\n        curl \\\n            --silent \\\n            --header \"Metadata-Flavor: Google\" \\\n            http://metadata/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip \\\n    )\n    BUILD_SERVER=\"http://${EXTERNAL_IP}/${BUILD_TAG}/\"\n    ${venv}/bin/python admin/run-client-tests \\\n    --distribution ${DISTRIBUTION_NAME} \\\n    --branch ${TRIGGERED_BRANCH} \\\n    --build-server $BUILD_SERVER\n    JOB_EXIT_STATUS=\"$( updateExitStatus $? )\"\n\n  disable_selinux: &disable_selinux |\n    sudo /usr/sbin/setenforce 0\n\n  check_version: &check_version |\n    export FLOCKER_VERSION=$(${venv}/bin/python setup.py --version)\n\n  build_sdist: &build_sdist  |\n    # package the goodies\n    ${venv}/bin/python setup.py sdist\n\n  build_package: &build_package  |\n    # Build an RPM / DEB package using docker.\n    # The packages are served by a local nginx server.\n    REPO_BASE=/var/www/html/${BUILD_TAG}\n    REPO_PATH=${REPO_BASE}/results/omnibus/${TRIGGERED_BRANCH}/${DISTRIBUTION_NAME}\n    # Export for subsequent functions that perform operations on the REPO_BASE\n    # and REPO_PATH.\n    export REPO_BASE REPO_PATH\n    sudo mkdir -p ${REPO_PATH}\n    ${venv}/bin/python admin/build-package \\\n    --destination-path ${REPO_PATH} \\\n    --distribution ${DISTRIBUTION_NAME_PACKAGE:-$DISTRIBUTION_NAME} \\\n    /flocker/dist/Flocker-${FLOCKER_VERSION}.tar.gz\n\n  build_repo_metadata: &build_repo_metadata |\n    # the acceptance tests look for a package in a yum repository,\n    # we provide one by starting a webserver and pointing the tests\n    # to look over there\n    cd ${REPO_PATH}\n    distribution=\"${DISTRIBUTION_NAME_PACKAGE:-$DISTRIBUTION_NAME}\"\n    # create a repo on either centos or ubuntu\n    case \"${distribution}\" in\n        ubuntu*)\n            sudo sh -c 'dpkg-scanpackages --multiversion . | gzip > Packages.gz'\n            ;;\n        centos*)\n            sudo createrepo .\n            ;;\n        *)\n            echo \"ERROR: Unsupported distribution '${distribution}'.\" >&2\n            exit 1\n            ;;\n    esac\n    cd -\n\n  clean_packages: &clean_packages |\n    # Delete the repo files we created\n    sudo rm -rf $REPO_BASE\n\n  exit_with_return_code_from_test: &exit_with_return_code_from_test |\n    # this is where we make sure we exit with the correct return code\n    # from the tests we executed above.\n    exit ${JOB_EXIT_STATUS}\n\n  push_image_to_dockerhub: &push_image_to_dockerhub |\n    # the /tmp/dockerhub_creds is copied from the Jenkins Master on\n    # /etc/slave_config to the slave during the bootstrap process of the slave.\n    # This contains the login details for our dockerhub instance, which is\n    # deployed as part of our caching platform.\n    #\n    export D_USER=$( cat /tmp/dockerhub_creds | cut -f 1 -d \":\" )\n    export D_PASSWORD=$( cat /tmp/dockerhub_creds | cut -f 2 -d \":\" )\n    export D_EMAIL=$( cat /tmp/dockerhub_creds | cut -f 3 -d \":\" )\n    docker login -u ${D_USER} -p ${D_PASSWORD} -e ${D_EMAIL}\n    echo y | docker push ${DOCKER_IMAGE}\n\n  build_docker_image: &build_docker_image |\n    # we want to make sure we are fetching the latest OS updates every time\n    # so we build the docker image using --no-cache, this way apt-get updates\n    # are actually executed.\n    # See: https://github.com/docker/docker/issues/3313\n    docker build --no-cache -t $DOCKER_IMAGE .\n\n  build_docker_image_flocker: &build_docker_image_flocker |\n    # Build Flocker Docker image\n    BUILD_SERVER_IP=\"$(wget -qO- http://instance-data/latest/meta-data/public-ipv4)\"\n    REPOSITORY_PATH=\"${BUILD_TAG}/results/omnibus/${TRIGGERED_BRANCH}/${DISTRIBUTION_NAME}\"\n    docker build \\\n        --rm \\\n        --tag \"${DOCKER_IMAGE}\" \\\n        --label \"FLOCKER_VERSION=${FLOCKER_VERSION}\" \\\n        --label \"GIT_COMMIT=${GIT_COMMIT}\" \\\n        --label \"TRIGGERED_BRANCH=${TRIGGERED_BRANCH}\" \\\n        --label \"BUILD_SERVER_IP=${BUILD_SERVER_IP}\" \\\n        --build-arg \"FLOCKER_REPOSITORY=http://${BUILD_SERVER_IP}/${REPOSITORY_PATH}\" \\\n        dockerfiles/dataset\n    JOB_EXIT_STATUS=\"$( updateExitStatus $? )\"\n\n\n  build_docker_image_flocker_control: &build_docker_image_flocker_control |\n    # Build Flocker-control Docker image\n    BUILD_SERVER_IP=\"$(wget -qO- http://instance-data/latest/meta-data/public-ipv4)\"\n    REPOSITORY_PATH=\"${BUILD_TAG}/results/omnibus/${TRIGGERED_BRANCH}/${DISTRIBUTION_NAME}\"\n    docker build \\\n        --rm \\\n        --tag \"${DOCKER_IMAGE}\" \\\n        --label \"FLOCKER_VERSION=${FLOCKER_VERSION}\" \\\n        --label \"GIT_COMMIT=${GIT_COMMIT}\" \\\n        --label \"TRIGGERED_BRANCH=${TRIGGERED_BRANCH}\" \\\n        --label \"BUILD_SERVER_IP=${BUILD_SERVER_IP}\" \\\n        --build-arg \"FLOCKER_REPOSITORY=http://${BUILD_SERVER_IP}/${REPOSITORY_PATH}\" \\\n        dockerfiles/control\n    JOB_EXIT_STATUS=\"$( updateExitStatus $? )\"\n\n  build_docker_image_flocker_docker_plugin: &build_docker_image_flocker_docker_plugin |\n    # Build Flocker-docker-plugin Docker image\n    BUILD_SERVER_IP=\"$(wget -qO- http://instance-data/latest/meta-data/public-ipv4)\"\n    REPOSITORY_PATH=\"${BUILD_TAG}/results/omnibus/${TRIGGERED_BRANCH}/${DISTRIBUTION_NAME}\"\n    docker build \\\n        --rm \\\n        --tag \"${DOCKER_IMAGE}\" \\\n        --label \"FLOCKER_VERSION=${FLOCKER_VERSION}\" \\\n        --label \"GIT_COMMIT=${GIT_COMMIT}\" \\\n        --label \"TRIGGERED_BRANCH=${TRIGGERED_BRANCH}\" \\\n        --label \"BUILD_SERVER_IP=${BUILD_SERVER_IP}\" \\\n        --build-arg \"FLOCKER_REPOSITORY=http://${BUILD_SERVER_IP}/${REPOSITORY_PATH}\" \\\n        dockerfiles/docker-plugin\n    JOB_EXIT_STATUS=\"$( updateExitStatus $? )\"\n\n  # These are the docker images we will be using during our tests.\n  # We build them every 24 hours, making sure we have the latest OS updates\n  # installed on those images.\n  # By doing this we speed up the bootstrapping of our client/acceptance tests.\n  #\n  build_dockerfile_centos7: &build_dockerfile_centos7 |\n    # don't waste time installing ruby or fpm, use an image containing fpm\n    # https://github.com/alanfranz/fpm-within-docker\n    echo \"FROM alanfranz/fwd-centos-7:latest\" > Dockerfile\n    echo \"MAINTAINER ClusterHQ <contact@clusterhq.com>\" >> Dockerfile\n    echo \"# URLGRABBER_DEBUG=1 to log low-level network info \\\n          - see FLOC-2640\" >> Dockerfile\n    echo \"RUN env URLGRABBER_DEBUG=1 yum groupinstall \\\n          --assumeyes 'Development Tools'\" >> Dockerfile\n    echo \"RUN env URLGRABBER_DEBUG=1 yum install \\\n          --assumeyes epel-release\" >> Dockerfile\n    echo \"RUN env URLGRABBER_DEBUG=1 yum install --assumeyes \\\n          git ruby-devel python-devel libffi-devel openssl-devel enchant-devel \\\n          rpmlint\" >> Dockerfile\n    echo \"RUN env URLGRABBER_DEBUG=1 yum update --assumeyes\" >> Dockerfile\n    echo \"ADD https://bootstrap.pypa.io/get-pip.py /tmp/\" >> Dockerfile\n    echo 'RUN [\"python\", \"/tmp/get-pip.py\", \"pip==8.1.2\", \"setuptools==23.0.0\"]' >> Dockerfile\n\n  build_dockerfile_ubuntu_trusty: &build_dockerfile_ubuntu_trusty |\n    # don't waste time installing ruby or fpm, use an image containing fpm\n    # https://github.com/alanfranz/fpm-within-docker\n    echo \"FROM alanfranz/fwd-ubuntu-trusty:latest\" > Dockerfile\n    echo \"MAINTAINER ClusterHQ <contact@clusterhq.com>\" >> Dockerfile\n    echo \"RUN apt-get update\" >> Dockerfile\n    echo \"RUN apt-get install --no-install-recommends -y git ruby-dev \\\n          libffi-dev libssl-dev libenchant-dev build-essential \\\n          python2.7-dev lintian python\" >> Dockerfile\n    echo \"ADD https://bootstrap.pypa.io/get-pip.py /tmp/\" >> Dockerfile\n    echo 'RUN [\"python\", \"/tmp/get-pip.py\", \"pip==8.1.2\", \"setuptools==23.0.0\"]' >> Dockerfile\n\n  build_dockerfile_ubuntu_xenial: &build_dockerfile_ubuntu_xenial |\n    # don't waste time installing ruby or fpm, use an image containing fpm\n    # https://github.com/alanfranz/fpm-within-docker\n    echo \"FROM alanfranz/fwd-ubuntu-xenial:latest\" > Dockerfile\n    echo \"MAINTAINER ClusterHQ <contact@clusterhq.com>\" >> Dockerfile\n    echo \"RUN apt-get update\" >> Dockerfile\n    echo \"RUN apt-get install --no-install-recommends -y \\\n          git ruby-dev libffi-dev libssl-dev libenchant-dev build-essential \\\n          python2.7-dev lintian python\" >> Dockerfile\n    echo \"ADD https://bootstrap.pypa.io/get-pip.py /tmp/\" >> Dockerfile\n    echo 'RUN [\"python\", \"/tmp/get-pip.py\", \"pip==8.1.2\", \"setuptools==23.0.0\"]' >> Dockerfile\n\n  do_not_abort_on_errors: &do_not_abort_on_errors |\n    # make sure we don't abort the job on the first error we find.\n    #\n    # jenkins will execute our shellscript with -e, which will cause the\n    # script to terminate at the first error and mark the job as failed.\n    # This is fine for the majority of the build cases, but in some situations\n    # we don't want to terminate the job straight away. For example, a job\n    # which generates a new Virtual Machine where the tests are executed.\n    # With '-e' defined, jenkins would abort the job and leave an orphan VM\n    # behind.\n    # To avoid it, we set '+e' on this shell.\n    set +e\n\n  cleanup_cloud_resources: &cleanup_cloud_resources |\n    # Cleanup leaked or stale cloud instances and volumes.\n    ${venv}/bin/python setup.py --version\n    ${venv}/bin/python admin/cleanup_cloud_resources \\\n    --config-file /tmp/acceptance.yaml \\\n    -- 1>cleanup_cloud_resources.stdout 2>cleanup_cloud_resources.stderr\n\n  source_git_commit: &source_git_commit |\n    # we use a script 'git-commit.sh' to pass any the git commit sha1\n    # to our vagrant machine. This file is copied and sourced as part of our\n    # build.sh run inside the virtual machine.\n    chmod 755 git-commit.sh\n    . git-commit.sh\n\n  set_home_variable_on_mesos: &set_home_variable_on_mesos |\n    # mesos is not setting HOME, so we set it here.\n    # it needs to be set to /root, so that we can re-use the existing\n    # vagrant base images which live under /root/.vagrant.d\n    export HOME=/root\n\n  # this is a wrapper that we can use for building a script that will be\n  # populated with different cli actions.\n  # we copy the resulting build.sh file to the vagrant box for execution.\n  begin_build_sh_EOF: &begin_build_sh_EOF cat <<'EOF_BUILD_SH' > build.sh\n\n  end_build_sh_EOF: &end_build_sh_EOF |\n     EOF_BUILD_SH\n     chmod 755 build.sh\n\n  run_build_sh_script_on_vagrant_box: &run_build_sh_script_on_vagrant_box |\n    # copy project files to the vagrant box\n    vagrant ssh-config > ssh-config\n    rsync -ave 'ssh -F ssh-config' . default:.\n\n    # this sets some variables used by the tests, since the tests run inside\n    # virtualbox we store them to a file and copy them to the vagrant box.\n    echo \"export GIT_COMMIT=${GIT_COMMIT}\" > git-commit.sh\n    for item in /tmp/pip.sh build.sh git-commit.sh\n    do\n      vagrant scp ${item} default:${item}\n      vagrant ssh -c \"chmod 755 ${item}\"\n    done\n\n    # make sure we don't abort on the first error\n    set +e\n    # run the build.sh script inside our vagrant box\n    VAGRANT_LOG=info vagrant ssh -c 'bash build.sh' || abort_build\n\n    # this removes the secrets files we used during provisioning\n    for item in \"/tmp/pip.sh build.sh git-commit.sh\"\n    do\n      vagrant ssh -c \"rm -f ${item}\"\n    done\n\n#-----------------------------------------------------------------------------#\n# Job Definitions below this point\n#-----------------------------------------------------------------------------#\n# Job Types:\n#\n# * run_trial\n# * run_trial_for_storage_driver (ebs/cinder)\n# * run_acceptance (tests)\n# * cronly_jobs (builds docker images every 24 hours)\n#\n# Toggles:\n#   * archive_artifacts: ( define if there are files to be archived)\n#   * coverage_report: ( enable if this job produces a coverage report file)\n#   * clean_repo: (enable if we need to clean old files owned by root)\n#\n\n# run_trial_modules contains a list of all the modules we want to execute\n# through trial.\nrun_trial_modules: &run_trial_modules\n  - admin\n  - benchmark\n  - flocker\nrun_trial_as_root_modules: &run_trial_as_root_modules\n  # journald access requires root\n  - flocker.common.functional\n  # flocker.volume needs to run as root due to ZFS calls\n  # so we run it as part of the run_trial_on_<Cloud>_<OS>_as_root jobs\n  - flocker.volume\n  # flocker.node.agent needs to run as root due to mounting filesystems\n  - flocker.node.agents\nrun_trial_functional_agent_modules: &run_trial_functional_agent_modules\n  # The modules have appropriate skips so only EBS tests run on EBS environment,\n  # etc.\n  - flocker.node.agents.functional\n\n# run_trial_cli contains a list of all the CLI yaml anchors we want to\n# execute as part of our run_trial_tasks\nrun_trial_cli: &run_trial_cli [\n  *hashbang,\n  *add_shell_functions,\n  *cleanup,\n  *setup_venv,\n  *setup_flocker_modules,\n  *setup_coverage,\n  *run_trial_with_coverage,\n  *run_coverage,\n  *convert_results_to_junit ]\n\nrun_trial_cli_as_root: &run_trial_cli_as_root [\n  *hashbang,\n  *add_shell_functions,\n  *cleanup,\n  *setup_venv,\n  *setup_flocker_modules,\n  *setup_coverage,\n  *run_trial_with_coverage_as_root,\n  *run_coverage,\n  *convert_results_to_junit ]\n\n# When we have acceptance tests that run quickly enough (as they do on loopback)\n# we just run them all in one builder:\nrun_full_acceptance_modules: &run_full_acceptance_modules\n  - flocker.acceptance\n\n# flocker.node.functional is hanging, so we don't run it\njob_type:\n  run_trial:\n    # http://build.clusterhq.com/builders/flocker-centos-7\n    run_trial_on_AWS_CentOS_7:\n      on_nodes_with_labels: 'aws-centos-7-SELinux-T2Medium'\n      with_modules: *run_trial_modules\n      with_steps:\n        - { type: 'shell', cli: *run_trial_cli }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_on_AWS_CentOS_7_as_root:\n      on_nodes_with_labels: 'aws-centos-7-SELinux-T2Medium'\n      with_modules: *run_trial_as_root_modules\n      with_steps:\n        - { type: 'shell', cli: *run_trial_cli_as_root }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_trial_directories_to_delete\n\n    # Ubuntu 14.04 non-root tests are run on Travis-CI container infrastructure.\n    # See .travis.yml\n\n    run_trial_on_AWS_Ubuntu_Trusty_as_root:\n      on_nodes_with_labels: 'aws-ubuntu-trusty-T2Medium'\n      with_modules: *run_trial_as_root_modules\n      with_steps:\n        - { type: 'shell', cli: *run_trial_cli_as_root }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_on_AWS_Ubuntu_Xenial:\n      on_nodes_with_labels: 'aws-ubuntu-xenial-T2Medium'\n      with_modules: *run_trial_modules\n      with_steps:\n        - { type: 'shell', cli: *run_trial_cli }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_on_AWS_Ubuntu_Xenial_as_root:\n      on_nodes_with_labels: 'aws-ubuntu-xenial-T2Medium'\n      with_modules: *run_trial_as_root_modules\n      with_steps:\n        - { type: 'shell', cli: *run_trial_cli_as_root }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_on_AWS_CentOS_7_flocker.node.functional.test_docker:\n      # FLOC-3903: docker on centos use loop-devmapper\n      # by default. That makes it much slower than Ubuntu\n      # with aufs. It leads to timeouts, but seems to do\n      # a bit better on the medium instance\n      on_nodes_with_labels: 'aws-centos-7-SELinux-T2Medium'\n      module: flocker.node.functional.test_docker\n      with_steps:\n        - { type: 'shell', cli: *run_trial_cli }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      # Increase the timeout due to FLOC-3903\n      timeout: 45\n      directories_to_delete: *run_trial_directories_to_delete\n\n    # Split out just to do the CentOS version above,\n    # for the reasons outlined in that section\n    run_trial_on_AWS_Ubuntu_Trusty_flocker.node.functional.test_docker:\n      on_nodes_with_labels: 'aws-ubuntu-trusty-T2Medium'\n      module: flocker.node.functional.test_docker\n      with_steps:\n        - { type: 'shell', cli: *run_trial_cli }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_trial_directories_to_delete\n\n    # Split out just to do the CentOS version above,\n    # for the reasons outlined in that section\n    run_trial_on_AWS_Ubuntu_Xenial_flocker.node.functional.test_docker:\n      on_nodes_with_labels: 'aws-ubuntu-xenial-T2Medium'\n      module: flocker.node.functional.test_docker\n      with_steps:\n        - { type: 'shell', cli: *run_trial_cli }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_trial_directories_to_delete\n\n  run_trial_for_storage_driver:\n    run_trial_for_ebs_storage_driver_on_CentOS_7:\n      on_nodes_with_labels: 'aws-centos-7-SELinux-T2Medium'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_aws_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 45\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_for_ebs_storage_driver_on_Ubuntu_trusty:\n      on_nodes_with_labels: 'aws-ubuntu-trusty-T2Medium'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_aws_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 45\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_for_ebs_storage_driver_on_Ubuntu_xenial:\n      on_nodes_with_labels: 'aws-ubuntu-xenial-T2Medium'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_aws_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 45\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_for_cinder_storage_driver_on_CentOS_7:\n      on_nodes_with_labels: 'rackspace-jenkins-slave-centos7-selinux-standard-4-dfw'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_rackspace_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      # Give this job a slightly longer timeout because it has to interact with\n      # the cloud-supplied Cinder which can be a bit sluggish even in the best\n      # of times.\n      timeout: 90\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_for_cinder_storage_driver_on_Ubuntu_trusty:\n      on_nodes_with_labels: 'rackspace-jenkins-slave-ubuntu14-standard-4-dfw'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_rackspace_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      # Reasoning for run_trial_for_cinder_storage_driver_on_CentOS_7's timeout\n      # applies here as well.\n      timeout: 90\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_for_cinder_storage_driver_on_Ubuntu_xenial:\n      on_nodes_with_labels: 'rackspace-jenkins-slave-ubuntu16-standard-4-dfw'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_rackspace_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      # Reasoning for run_trial_for_cinder_storage_driver_on_CentOS_7's timeout\n      # applies here as well.\n      timeout: 90\n      directories_to_delete: *run_trial_directories_to_delete\n\n  # http://build.clusterhq.com/builders/flocker%2Facceptance%2Faws%2Fcentos-7%2Faws\n  run_acceptance:\n    run_acceptance_loopback_on_AWS_CentOS_7_for:\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_modules: *run_full_acceptance_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   'export DISTRIBUTION_NAME=centos-7',\n                   *check_version,\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=${MODULE}',\n                   'export ACCEPTANCE_TEST_PROVIDER=aws',\n                   *run_acceptance_loopback_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts\n      publish_test_results: true\n      timeout: 45\n      directories_to_delete: *run_acceptance_directories_to_delete\n\n    run_acceptance_loopback_on_AWS_RHEL_7_for:\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_modules: *run_full_acceptance_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   # For RHEL7 we build Centos-7 packages but put them in a repo\n                   # named RHEL7.\n                   # XXX We should have a way to build RHEL7 packages.\n                   'export DISTRIBUTION_NAME=rhel-7',\n                   'export DISTRIBUTION_NAME_PACKAGE=centos-7',\n                   *check_version,\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=${MODULE}',\n                   'export ACCEPTANCE_TEST_PROVIDER=aws',\n                   *run_acceptance_loopback_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts\n      publish_test_results: true\n      timeout: 45\n      directories_to_delete: *run_acceptance_directories_to_delete\n\n    run_acceptance_loopback_on_AWS_Ubuntu_Trusty_for:\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_modules: *run_full_acceptance_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *check_version,\n                   'export DISTRIBUTION_NAME=ubuntu-14.04',\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=${MODULE}',\n                   'export ACCEPTANCE_TEST_PROVIDER=aws',\n                   *run_acceptance_loopback_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts_ubuntu_special_case\n      publish_test_results: true\n      timeout: 45\n      directories_to_delete: *run_acceptance_directories_to_delete\n\n    run_acceptance_loopback_on_AWS_Ubuntu_Xenial_for:\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_modules: *run_full_acceptance_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *check_version,\n                   'export DISTRIBUTION_NAME=ubuntu-16.04',\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=${MODULE}',\n                   'export ACCEPTANCE_TEST_PROVIDER=aws',\n                   *run_acceptance_loopback_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts_ubuntu_special_case\n      publish_test_results: true\n      timeout: 45\n      directories_to_delete: *run_acceptance_directories_to_delete\n\n  run_client:\n    run_client_installation_on_Ubuntu_Trusty:\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   'export DISTRIBUTION_NAME=ubuntu-14.04',\n                   *check_version,\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *run_client_tests,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_acceptance_directories_to_delete\n\n    run_client_installation_on_Ubuntu_Xenial:\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   'export DISTRIBUTION_NAME=ubuntu-16.04',\n                   *check_version,\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *run_client_tests,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_acceptance_directories_to_delete\n\n  run_docker_build:\n    run_docker_build_on_Ubuntu_Xenial:\n      on_nodes_with_labels: 'aws-ubuntu-xenial-T2Medium'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   'export DISTRIBUTION_NAME=ubuntu-16.04',\n                   *check_version,\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   'export DOCKER_IMAGE=clusterhqci/flocker-dataset-agent:${GIT_COMMIT}',\n                   *build_docker_image_flocker,\n                   *push_image_to_dockerhub,\n                   'export DOCKER_IMAGE=clusterhqci/flocker-control:${GIT_COMMIT}',\n                   *build_docker_image_flocker_control,\n                   *push_image_to_dockerhub,\n                   'export DOCKER_IMAGE=clusterhqci/flocker-docker-plugin:${GIT_COMMIT}',\n                   *build_docker_image_flocker_docker_plugin,\n                   *push_image_to_dockerhub,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      timeout: 30\n      directories_to_delete: *run_acceptance_directories_to_delete\n\n  cronly_jobs:\n    run_docker_build_centos7_fpm:\n      at: '0 0 * * *'\n      on_nodes_with_labels: 'aws-centos-7-SELinux-T2Medium'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   'export DOCKER_IMAGE=clusterhqci/fpm-centos-7',\n                   *build_dockerfile_centos7,\n                   *build_docker_image,\n                   *push_image_to_dockerhub ]\n          }\n      timeout: 30\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_docker_build_ubuntu_trusty_fpm:\n      at: '0 1 * * *'\n      on_nodes_with_labels: 'aws-centos-7-SELinux-T2Medium'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   'export DOCKER_IMAGE=clusterhqci/fpm-ubuntu-trusty',\n                   *build_dockerfile_ubuntu_trusty,\n                   *build_docker_image,\n                   *push_image_to_dockerhub ]\n          }\n      timeout: 30\n      directories_to_delete: []\n\n    run_docker_build_ubuntu_xenial_fpm:\n      at: '0 3 * * *'\n      on_nodes_with_labels: 'aws-centos-7-SELinux-T2Medium'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   'export DOCKER_IMAGE=clusterhqci/fpm-ubuntu-xenial',\n                   *build_dockerfile_ubuntu_xenial,\n                   *build_docker_image,\n                   *push_image_to_dockerhub ]\n          }\n      timeout: 30\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_trial_for_gce_storage_driver_on_Ubuntu_trusty:\n      at: '0 5 * * *'\n      on_nodes_with_labels: 'gce-ubuntu14'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_gce_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 45\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_for_gce_storage_driver_on_Ubuntu_xenial:\n      at: '0 5 * * *'\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_gce_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 45\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_trial_for_gce_storage_driver_on_CentOS_7:\n      at: '0 5 * * *'\n      on_nodes_with_labels: 'gce-centos7'\n      with_modules: *run_trial_functional_agent_modules\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *setup_coverage, *setup_gce_env_vars,\n                   'export FLOCKER_FUNCTIONAL_TEST=TRUE',\n                   *run_trial_for_storage_drivers_with_coverage, *run_coverage,\n                   *convert_results_to_junit ]\n          }\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      coverage_report: true\n      clean_repo: true\n      timeout: 45\n      directories_to_delete: *run_trial_directories_to_delete\n\n    run_acceptance_on_AWS_CentOS_7_with_EBS:\n      at: '0 5 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   'export DISTRIBUTION_NAME=centos-7',\n                   *check_version,\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=aws',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts\n      publish_test_results: true\n      # Give the acceptance test suite a nice long time to run.  Give it even\n      # longer on CentOS than Ubuntu because Docker configuration on CentOS\n      # causes some things to be particularly slow.  This value is just a guess\n      # at what a reasonable upper-bound for the runtime of the suite might be\n      # as of Dec 2015.\n      timeout: 120\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_acceptance_on_AWS_Ubuntu_Trusty_with_EBS:\n      at: '0 6 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *check_version,\n                   'export DISTRIBUTION_NAME=ubuntu-14.04',\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=aws',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts_ubuntu_special_case\n      publish_test_results: true\n      # Similar to the reasoning for run_acceptance_on_AWS_CentOS_7_with_EBS\n      # but slightly shorter since Ubuntu runs the tests faster.\n      timeout: 90\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_acceptance_on_AWS_Ubuntu_Xenial_with_EBS:\n      at: '0 6 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *check_version,\n                   'export DISTRIBUTION_NAME=ubuntu-16.04',\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=aws',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts_ubuntu_special_case\n      publish_test_results: true\n      # Similar to the reasoning for run_acceptance_on_AWS_CentOS_7_with_EBS\n      # but slightly shorter since Ubuntu runs the tests faster.\n      timeout: 90\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_acceptance_on_GCE_CentOS_7_with_GCE:\n      at: '0 5 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   'export DISTRIBUTION_NAME=centos-7',\n                   *check_version,\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=gce',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts\n      publish_test_results: true\n      # Reasoning as for run_acceptance_on_AWS_CentOS_7_with_EBS\n      timeout: 120\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_acceptance_on_GCE_Ubuntu_Trusty_with_GCE:\n      at: '0 6 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *check_version,\n                   'export DISTRIBUTION_NAME=ubuntu-14.04',\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=gce',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts_ubuntu_special_case\n      publish_test_results: true\n      # Reasoning as for run_acceptance_on_AWS_Ubuntu_Trusty_with_EBS\n      timeout: 90\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_acceptance_on_GCE_Ubuntu_Xenial_with_GCE:\n      at: '0 6 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *check_version,\n                   'export DISTRIBUTION_NAME=ubuntu-16.04',\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=gce',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts_ubuntu_special_case\n      publish_test_results: true\n      # Reasoning as for run_acceptance_on_AWS_Ubuntu_Trusty_with_EBS\n      timeout: 90\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_acceptance_on_Rackspace_CentOS_7_with_Cinder:\n      at: '0 5 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   'export DISTRIBUTION_NAME=centos-7',\n                   *check_version,\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=rackspace',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts\n      publish_test_results: true\n      # Reasoning as for run_acceptance_on_AWS_CentOS_7_with_EBS\n      timeout: 120\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_acceptance_on_Rackspace_Ubuntu_Trusty_with_Cinder:\n      at: '0 6 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *check_version,\n                   'export DISTRIBUTION_NAME=ubuntu-14.04',\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=rackspace',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts_ubuntu_special_case\n      publish_test_results: true\n      # Reasoning as for run_acceptance_on_AWS_Ubuntu_Trusty_with_EBS\n      timeout: 90\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    run_acceptance_on_Rackspace_Ubuntu_Xenial_with_Cinder:\n      at: '0 6 * * *'\n      # flocker.provision is responsible for creating the test nodes on\n      # so we can actually run run-acceptance-tests from GCE\n      on_nodes_with_labels: 'gce-ubuntu16'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *check_version,\n                   'export DISTRIBUTION_NAME=ubuntu-16.04',\n                   *build_sdist, *build_package,\n                   *build_repo_metadata,\n                   *setup_authentication,\n                   'export ACCEPTANCE_TEST_MODULE=flocker.acceptance',\n                   'export ACCEPTANCE_TEST_PROVIDER=rackspace',\n                   *run_acceptance_tests,\n                   *convert_results_to_junit,\n                   *clean_packages,\n                   *exit_with_return_code_from_test ]\n          }\n      clean_repo: true\n      archive_artifacts: *acceptance_tests_artifacts_ubuntu_special_case\n      publish_test_results: true\n      # Reasoning as for run_acceptance_on_AWS_Ubuntu_Trusty_with_EBS\n      timeout: 90\n      directories_to_delete: []\n      notify_slack: '#nightly-builds'\n\n    cleanup_cloud_resources:\n      at: '0 8 * * *'\n      on_nodes_with_labels: 'aws-ubuntu-trusty-T2Medium'\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang, *add_shell_functions,\n                   *cleanup, *setup_venv, *setup_flocker_modules,\n                   *cleanup_cloud_resources ]\n          }\n      timeout: 10\n      archive_artifacts: [\n          \"cleanup_cloud_resources.stdout\",\n          \"cleanup_cloud_resources.stderr\"\n      ]\n\n    run_admin_test_test_installer:\n      at: '0 8 * * *'\n      on_nodes_with_labels: 'aws-ubuntu-trusty-T2Medium'\n      module: admin.test.test_installer\n      with_steps:\n        - { type: 'shell',\n            cli: [ *hashbang,\n                   *add_shell_functions,\n                   *cleanup,\n                   *setup_venv,\n                   *setup_flocker_modules,\n                   *setup_coverage,\n                   *check_version,\n                   *setup_authentication,\n                   'export ACCEPTANCE_YAML=/tmp/acceptance.yaml',\n                   *run_trial_with_coverage,\n                   *run_coverage,\n                   *convert_results_to_junit ]\n        }\n      clean_repo: true\n      archive_artifacts: *flocker_artifacts\n      publish_test_results: true\n      # CloudFormation stack creation takes 5-10 minutes and the test itself\n      # may take another 10. Give 30 minutes to be sure.\n      timeout: 30\n      directories_to_delete: []\n\n\n\n#-----------------------------------------------------------------------------#\n# View definitions below this point\n#-----------------------------------------------------------------------------#\n\n# Use the section below to define multiple 'views/tabs' that appear at the top\n# of the jenkins build page for every branch.\n# These views allow for filtering of particular jobs according to its type,\n# cloud, operating system, ...\n#\n#\n# The views are built from :\n# * the dictionary 'key', which is the view name.\n# * description, containing a description of the jobs in this view\n# * regex, containing a regular expression that is applied to build the view.\n#\n# the site below can help with regular expressions:\n# http://pythex.org/\n\nviews:\n  trial:\n    description: 'All jobs that are executed directly using trial'\n    regex: 'run_(?i)trial_.*.*'\n  acceptance:\n    description: 'All Acceptance jobs'\n    regex: 'run_(?i)acceptance_.*.*'\n  client:\n    description: 'All client installation jobs'\n    regex: 'run_(?i)client_.*.*'\n  cinder:\n    description: 'cinder jobs'\n    regex: '.*_(?i)cinder_.*.*'\n  ebs:\n    description: 'ebs jobs'\n    regex: '.*_(?i)ebs_.*.*'\n  storage_drivers:\n    description: 'All Storage Driver Jobs'\n    regex: '.*_(?i)storage_(?i)driver_.*.*'\n  on_AWS:\n    description: 'All AWS Jobs'\n    regex: '(.*_(?i)ebs_.*.*|.*_(?i)aws_.*.*)'\n  on_Rackspace:\n    description: 'All Rackspace Jobs'\n    regex: '(.*_(?i)cinder_.*.*|.*_(?i)rackspace_.*.*)'\n  CentOS_7:\n    description: 'All CentOS jobs'\n    regex: '.*_(?i)centos_.*.*'\n  Ubuntu_Trusty_14_04_LTS:\n    description: 'All Ubuntu Trusty LTS (14.04) jobs'\n    regex: '.*_(?i)trusty_.*.*'\n  Ubuntu_Xenial_16_04:\n    description: 'All Ubuntu Xenial (16.04) jobs'\n    regex: '.*_(?i)xenial_.*.*'\n  cron:\n    description: 'All Nightly Cron jobs'\n    regex: '^_[^_](?i)[a-z]*.*'\n"
        },
        {
          "name": "dev-requirements.txt",
          "type": "blob",
          "size": 0.0986328125,
          "content": "# Install everything required for local development.\n--requirement requirements/all.txt\n--editable .\n"
        },
        {
          "name": "dockerfiles",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "flocker",
          "type": "tree",
          "content": null
        },
        {
          "name": "jobs.groovy",
          "type": "blob",
          "size": 31.2900390625,
          "content": "// vim: ai ts=2 sts=2 et sw=2 ft=groovy et\n/* jobs.groovy\n\n  This Groovy file contains the jenkins job DSL plugin code to build all the\n  required views, folders, and jobs for a particular project in the Jenkins\n  interface.\n\n  The easiest way to debug or experiment with changes to this is:\n    * check them in to a branch\n    * trigger a build of that branch on the setup_ClusterHQ-flocker job\n    * ssh into the master (same hostname as the web ui)\n    * compare the master configuration xml with the branch configuration xml:\n\n        BASE=/var/lib/jenkins/jobs/ClusterHQ-flocker/jobs/\n        A=master\n        B=copy-artifacts-for-all-jobs-FLOC-3799\n        for config in $(find ${BASE}/${A} -name config.xml | cut -d '/' -f 10- | sort); do\n            if [ -e ${BASE}/${A}/${config} ] && [ -e ${BASE}/${B}/${config} ]; then\n                diff -u ${BASE}/${A}/${config} <(sed -e \"s,${B},${A},\" ${BASE}/${B}/${config})\n            fi\n        done | less\n\n      Certain changes will always be present.  For example, triggers for the\n      cron-style jobs are only created for the master branch.\n\n  The full Jenkins job DSL reference can be found on the URL below:\n  https://github.com/jenkinsci/job-dsl-plugin/wiki/Job-reference\n\n  The file is consumed by the Jenkins job setup_ClusterHQ_Flocker. This file\n  is read by a build step of type 'Process Job DSL' which produces all the\n  jenkins objects.\n\n  A typical jenkins job contains the following sections, this is just an\n  example of possible actions within a particular section:\n\n   * Environment variables:\n        - These are used to pass environment variables to the build steps or\n          any other step in the job.\n\n   * Source Control defintions:\n        - repository URL\n        - what SCM tool git/svn/mercurial\n        - operations to do prior/after a git update/close\n\n    * Triggers:\n        - When to execute the build ?\n        - On a crontab style\n        - by pooling the repository for changes\n        - triggered by GitHub or others\n\n   * Wrappers which 'decorate' the build steps of the job\n        - adds stuff like timestamps to the log output\n        - sets a timeout and aborts the build when exceeded\n\n    * Build steps\n        - Commonly shell based steps, as in the cli to execute\n\n    * Post-Build actions also called Publishers:\n        - Archiving produced files so that they are made available in other\n          jobs\n        - Running coverage or test reports\n        - Triggering other jobs\n\n\n    The structure of this file is as follows:\n\n    A set of functions which implement the different sections of the job:\n\n        - folder (creates a folder view in Jenkins to store the jobs)\n        - wrappers\n        - triggers\n        - scm (sets the Source Control section)\n        - publishers (set the post-build steps)\n        - steps (consumes the contentes of the build.yaml build steps, and\n            expands those as shell cli text'\n\n    A full set of job blocks follows, these iterate over the build.yaml file for\n    each job definition of a particular 'job_type' and create all the jobs\n    related to that 'job_type':\n\n        - run_trial\n        - run_trial_for_storage_driver\n        - run_sphinx\n        - run_acceptance\n        - run_client\n        - omnibus\n\n    Then we have the multijob block, which collects all the defined jobs based\n    on 'job_type' and adds them to a 'multijob phase' that executes all the\n    jobs in parallel.\n    When a new 'job_type' is added to the configuration above, that job_type\n    configuration should be added into the multijob block so that it gets run\n    as part of the parallel execution phase.\n    This multijob after executing the different jobs, collects all the produced\n    artifacts (test reports, coverage reports) from the different jobs.\n    So that a aggregated view of all the results is made available within the\n    multijob details page for a completed job.\n\n    The last section in this file defines cron style type jobs which are not\n    executed as part of the main multijob.  These follow a similar structure to\n    the common 'job_type' definitions above, except they include a trigger\n    section specifying an daily schedulle.\n*/\n\n// Load the build.yaml, convert to JSON:\ndef processbuilder = new ProcessBuilder('python', '-c', 'import json, yaml, sys; sys.stdout.write(json.dumps(yaml.safe_load(sys.stdin.read()))); sys.stdout.flush()')\ndef process = processbuilder.redirectInput(ProcessBuilder.Redirect.PIPE).redirectOutput(\n    ProcessBuilder.Redirect.PIPE).start()\n// Aren't Java APIs awesome? Process.getOutputStream() is stdin.\ndef stdin = new java.io.PrintWriter(process.getOutputStream())\nstdin.write(readFileFromWorkspace('build.yaml'))\nstdin.flush()\nstdin.close()\nprocess.getOutputStream().close()\ndef jsonSlurper = new groovy.json.JsonSlurper()\n// Java is the best! This is how you read file into a string:\nGLOBAL_CONFIG = jsonSlurper.parseText(\n    new Scanner(process.getInputStream(),\"UTF-8\").useDelimiter(\"\\\\A\").next())\n\n// --------------------------------------------------------------------------\n// UTILITIES DEFINED BELOW\n// --------------------------------------------------------------------------\n\ndef updateGitHubStatus(status, description, branch, dashProject,\n                       dashBranchName, jobName, buildNumber=true) {\n    if (buildNumber) {\n      buildURL = \"\\${BUILD_URL}\"\n    } else {\n      buildURL = \"\\$JENKINS_URL/job/${dashProject}/job/${dashBranchName}/job/${jobName}/\"\n    }\n    return \"\"\"#!/bin/bash\n# Jenkins jobs are usually run using 'set -x' however we use 'set +x' here\n# to avoid leaking credentials into the build logs.\nset +x\n# Set the GitHub credentials\n\"\"\" + new File('/etc/slave_config/github_status.sh').text + \"\"\"\n# Send a request to Github to update the commit with the build status\nREMOTE_GIT_COMMIT=`git rev-list --max-count=1 upstream/${branch}`\nBUILD_URL=\"${buildURL}\"\nPAYLOAD=\"{\\\\\"state\\\\\": \\\\\"${status}\\\\\", \\\\\"target_url\\\\\": \\\\\"\\$BUILD_URL\\\\\", \\\\\"description\\\\\": \\\\\"${description}\\\\\", \\\\\"context\\\\\": \\\\\"jenkins-${jobName}\\\\\" }\"\ncurl \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Authorization: token \\$GITHUB_STATUS_API_TOKEN\" \\\\\n  -X POST \\\\\n  --data \"\\$PAYLOAD\" \\\\\n  https://api.github.com/repos/${GLOBAL_CONFIG.project}/statuses/\\$REMOTE_GIT_COMMIT\necho \"Updating commit \\$REMOTE_GIT_COMMIT with ${status} status.\"\n\"\"\"\n}\n\ndef folder_name(dashProject, dashBranchName) {\n    /*\n        Return the folder name (url path) given the constituent parts\n\n        :param unicode dashProject: the name of the top-level project,\n            escaped for use in a path.\n        :param unicode dashBranchName: the name of the branch,\n            escaped for use in a path.\n        :return unicode: the folder name for the branch\n    */\n    return \"${dashProject}/${dashBranchName}\"\n}\n\n\ndef full_job_name(dashProject, dashBranchName, job_name) {\n    /*\n        Return the full job name (url path) given the constituent parts\n\n        :param unicode dashProject: the name of the top-level project,\n            escaped for use in a path.\n        :param unicode dashBranchName: the name of the branch,\n            escaped for use in a path.\n        :param unicode job_name: the sub-task name,\n            escaped for use in a path.\n        :return unicode: the full name\n    */\n    return folder_name(dashProject, dashBranchName) + \"/${job_name}\"\n}\n\n\ndef escape_name(name) {\n    /*\n        Escape a name to make it suitable for use in a path.\n\n        :param unicode name: the name to escape.\n        :return unicode: the escaped name.\n    */\n    return name.replace('/', '-')\n}\n\n\ndef make_view(path, description, make_jobs) {\n    /*\n        Make a view tab.\n\n        :param unicode path: the url segment of the view, including project etc.\n            e.g. \"flocker/master/acceptance\"\n        :param unicode description: the description of the view.\n        :param callable make_jobs: a callable that will return the\n                the list of jobs to include when passed the view context,\n                something like:\n                    { context ->\n                        context.jobs {\n                            name(\"foo\")\n                            regex(\"bar.*\")\n                        }\n                    }\n    */\n\n    listView(path) {\n        delegate.description(description)\n        filterBuildQueue()\n        filterExecutors()\n        make_jobs(delegate)\n        columns {\n            status()\n            weather()\n            name()\n            lastSuccess()\n            lastFailure()\n            lastDuration()\n            buildButton()\n        }\n    }\n}\n\n\n/* adds a list of common wrappers to the build jobs.\n\n   param v:  dictionary containing the values from the job\n   param list directories_to_delete: directory to clean up\n*/\ndef build_wrappers(v) {\n    return {\n        //    adds timestamps to the job log output\n        timestamps()\n        // colorizeOuptut allows for ascii coloured output in the logs of a job\n        colorizeOutput()\n        /* define the max duration a running job should take, this prevents stuck\n           jobs for reserving jenkins slaves and preventing other jobs from running.\n           These timeouts will have to be adjusted as we work on improving the\n           execution time of the tests so that we can enforce a SLA of a maximum of\n           'n' minutes.\n           An improvement here (TODO) would be to receibe the timeout as a parameter\n           so that we can have different timeouts for different job types. */\n        timeout {\n            absolute(v.timeout)\n            failBuild()\n        }\n        /* Jobs that are executed with sudo can leave files behind that prevents\n           Jenkins from cleaning the git repository before a git merge.\n           The git cleanup process is run as the jenkins execution user which lacks\n           the priviledges to remove the root owned files created by the sudo tests.\n           To fix this issue, we use a preSCM plugin which allows us to execute a\n           step before cloning/checking the git reposity.\n           if the build.yaml contains a 'clean_repo' flag, then we will clean up\n           old root files from the repo. */\n        if (v.clean_repo) {\n            preScmSteps {\n                steps {\n                    directories_to_delete = v.directories_to_delete.join(\" \")\n                    shell(\"sudo rm -rf ${directories_to_delete}\")\n                }\n            }\n        }\n    }\n}\n\n\n/*\n    Is the specified branch the master branch?\n\n    :param unicode branchName: the name of the branch.\n    :return bool: whether the branch is the master branch.\n*/\ndef is_master(branchName) {\n    return branchName == \"master\"\n}\n\n\n/* adds a list of triggers to the build job\n\n   param  _type: type of job\n   param _value: the cron string\n\n*/\ndef build_triggers(_type, _value, _branch ) {\n    return {\n        /* the job_type 'cronly_jobs' is used by the docker_build jobs running every 24h\n           but we only configure the scheduler if the jobs is for the master branch.\n           If we were to schedule the job on every branch we would have multiple jobs\n           running at the same time. */\n        if (_type == \"cronly_jobs\" && is_master(_branch)) {\n            //  the cron  string below is a common crontab style string\n            cron(_value)\n        }\n        /*  the job_type 'multijob' is used by the multijob, we use it to\n            configure that job so that builds on master are triggered automatically\n            this block enables 'Build when a change is pushed to GitHub' */\n        if (_type == \"multijob\" && is_master(_branch)) {\n            githubPush()\n        }\n    }\n}\n\n\n/* configures a remote git repository, and merges 'branch' before build\n\n    param: git_url - example: https://github.com/clusterhq/flocker\n    param: branch - remote branch name to configure\n*/\ndef build_scm(git_url, branchName, isReleaseBuild) {\n    return {\n        git {\n            // The default clone timeout is 10 minutes.  Lower this a bit\n            // because checking out Flocker takes less than a minute (Jan 2016)\n            // but completely stalls from time to time.  We want to hit the\n            // timeout for the stalled case as quickly as we can so a retry\n            // (which will hopefully succeed) can be attempted.  See the\n            // checkoutRetryCount setting elsewhere.\n            cloneTimeout(2)\n\n            remote {\n                // our remote will be called 'upstream'\n                name(\"upstream\")\n                // the project name from the build yaml 'ClusterHQ/Flocker'\n                github(GLOBAL_CONFIG.project)\n            }\n            //  configure the git user merging the branches.\n            // the job dsl scm/git doesn't contain a method to specify the local git user\n            // or email address, so we use a configure/node block to insert the XML block\n            // into the jenkins job config\n            configure { node ->\n                node / gitConfigName('Jenkins')\n                node / gitConfigEmail('jenkins@clusterhq.com')\n            }\n            // the branch to be built\n            branch(branchName)\n            // clean the repository before merging (git reset --hard)\n            clean(true)\n            createTag(false)\n            if (!isReleaseBuild) {\n                // merge our branch with the master branch\n                mergeOptions {\n                    remote('upstream')\n                    branch('master')\n                    // there are a few merge strategies available, recursive is the default one\n                    strategy('recursive')\n                }\n            }\n        }\n    }\n}\n\n\n/* adds a publishers block to the jenkins job configuration, containing:\n   an action for archiving artifacts\n   an action for archiving junit results\n   an action for publishing coverate reports\n\n   param v: dictionary containing the job keys\n*/\ndef build_publishers(v, branchName, dashProject, dashBranchName, job_name) {\n    return {\n        if (v.archive_artifacts) {\n            for (artifact in v.archive_artifacts) {\n                // v.archive_artifacts typically contain:\n                // 'results.xml', 'coverage.xml', '_trial_temp/trial.log'\n                archiveArtifacts(artifact)\n            }\n        }\n\n        if (v.publish_test_results) {\n            // archives the junit results and publish the test results\n            archiveJunit('results.xml') {\n                retainLongStdout(true)\n                testDataPublishers {\n                    allowClaimingOfFailedTests()\n                    publishTestAttachments()\n                    publishTestStabilityData()\n                    publishFlakyTestsReport()\n                }\n            }\n        }\n        if (v.coverage_report) {\n            // publishes a coverage report, using junit and the cobertura plugin\n            cobertura('coverage.xml') {\n                // don't publish coverage reports if the build is not stable.\n                onlyStable(false)\n                failUnhealthy(true)\n                failUnstable(true)\n                // fail the build if we were expecating a coverage report from a build and\n                // that report is not available\n                failNoReports(true)\n            }\n        }\n        if (v.publish_lint) {\n          violations {\n            sourcePathPattern('**/*.py')\n            pylint(1, null, null, '*.lint')\n          }\n        }\n        if (job_name) {\n            /* Update the commit status on GitHub with the build result We use the\n               flexible-publish plugin combined with the the any-buildstep plugin to\n               allow us to run shell commands as part of a post-build step.  We also\n               use the run-condition plugin to inspect the status of the build\n               https://wiki.jenkins-ci.org/display/JENKINS/Flexible+Publish+Plugin\n               https://wiki.jenkins-ci.org/display/JENKINS/Any+Build+Step+Plugin\n               https://wiki.jenkins-ci.org/display/JENKINS/Run+Condition+Plugin */\n            flexiblePublish {\n                condition {\n                    status('SUCCESS', 'SUCCESS')\n                }\n                step {\n                    shell(updateGitHubStatus(\n                              'success', 'Build succeeded',\n                              branchName, dashProject, dashBranchName, job_name))\n                }\n            }\n            flexiblePublish {\n                condition {\n                    status('FAILURE', 'FAILURE')\n                }\n                step {\n                    shell(updateGitHubStatus(\n                          'failure', 'Build failed',\n                          branchName, dashProject, dashBranchName, job_name))\n                }\n            }\n        }\n    }\n}\n\n\n/*  builds a list of job steps based on the type of the job:\n    ( 'shell', others )\n    currently only shell has been implemented.\n\n    params job: dictionary the job steps (``with_steps``) */\ndef build_steps(job) {\n    return {\n        for (_step in job.with_steps) {\n            if (_step.type == 'shell') {\n                shell(_step.cli.join(\"\\n\") + \"\\n\")\n            }\n        }\n    }\n}\n\n\n/*\n    Closure that configures the current project to notify slack on failure.\n    The specified channel will be notified.\n*/\nClosure notify_slack(String channel) {\n    return {\n        // I'm not sure how to put these magic strings in a variable\n        // or avoid them altogether.\n        it / 'properties' << 'jenkins.plugins.slack.SlackNotifier_-SlackJobProperty'(plugin: 'slack@1.8.1') {\n            room(channel)\n            startNotification(false)\n            notifySuccess(false)\n            notifyAborted(false)\n            notifyNotBuilt(true)\n            notifyUnstable(true)\n            notifyFailure(true)\n            notifyBackToNormal(false)\n            notifyRepeatedFailure(true)\n            includeTestSummary(true)\n            showCommitList(true)\n            includeCustomMessage(false)\n        }\n        it / 'publishers' << 'jenkins.plugins.slack.SlackNotifier'(plugin: 'slack@1.8.1')\n    }\n}\n\n\ndef list_jobs(dashProject, dashBranchName) {\n    /*\n        List all the jobs, accounting for modules to run etc.\n\n        A common pattern is to loop over the jobs and\n        extract particular information and act on it.\n\n        The way the information is stored in build.yaml,\n        and the varied rules for processing it mean that\n        it's not a simple loop.\n\n        This function performs the transformations and\n        provides a simple list of all jobs with their\n        basic information. It can be filtered and processed\n        as needed.\n\n        :param unicode dashProject: the name of the project, escaped\n            for use in paths.\n        :param unicode dashBranchName: the name of the branch, escaped\n            for use in paths.\n        :return List: a list of maps of job info, with keys:\n            unicode type: the type of the job\n            unicode name: the short name of the job\n            unicode full_name: the full path of the job, including\n                               project and branch\n            unicode module: the test module to run, or null if this\n                            doesn't apply to this job type\n            Map values: the rest of the key/value pairs from the\n                        build.yaml job definition\n    */\n    results = []\n    GLOBAL_CONFIG.job_type.each { job_type, job_type_values ->\n        job_type_values.each { job_name, job_values ->\n            // Cron jobs are prefixed with an underscore\n            if (job_type == 'cronly_jobs') {\n                job_name = \"_${job_name}\"\n            }\n            if (job_values.with_modules) {\n                for (module in job_values.with_modules) {\n                    _job_name = job_name + '_' + escape_name(module)\n                    full_name = full_job_name(\n                        dashProject, dashBranchName, _job_name)\n                    results.add([type: job_type, name: _job_name,\n                                 module: module, values: job_values,\n                                 full_name: full_name])\n                }\n            } else if (job_values.module) {\n                full_name = full_job_name(\n                    dashProject, dashBranchName, job_name)\n                results.add([type: job_type, name: job_name,\n                             module: job_values.module, values: job_values,\n                             full_name: full_name])\n            }else {\n                full_name = full_job_name(\n                    dashProject, dashBranchName, job_name)\n                results.add([type: job_type, name: job_name, module: null,\n                             values: job_values, full_name: full_name])\n            }\n        }\n    }\n    return results\n}\n\n\n// the project name from the build yaml 'ClusterHQ/Flocker'\nproject = GLOBAL_CONFIG.project\n// the github https url for the project\ngit_url = GLOBAL_CONFIG.git_url\nbranches = []\n/* grab the GitHub token from the jenkins homedir.\n   this is the github api token for jenkins. We need to authenticate as github\n   limits the number of API calls that can be done withouth authenticating   */\nString token = new File('/var/lib/jenkins/.github_token').text.trim()\n\n// Lets's call it ClusterHQ-Flocker instead of ClusterHQ/Flocker\ndashProject = escape_name(project)\n\n// Create a basefolder for our project, it should look like:\n//   '<github username>-<git repository>'\nprintln(\"Creating folder ${dashProject}...\")\nfolder(dashProject) { displayName(dashProject) }\n\nmake_view(\n    \"${dashProject}/master\", \"Builds of master\",\n    { context ->\n        context.jobs {\n            name(\"master\")\n        }\n    }\n)\n\nmake_view(\n    \"${dashProject}/releases\", \"Release build\",\n    { context ->\n        context.jobs {\n            regex(\"release-.*\")\n        }\n    }\n)\n\n// branches contains the passed parameter RECONFIGURE_BRANCH from jenkins\nbranches.add(\"${RECONFIGURE_BRANCH}\")\n\n// create tabs inside the branch folder\ndef build_tabs(dashBranchName) {\n    for (entry in GLOBAL_CONFIG.views) {\n        name = \"${dashProject}/${dashBranchName}/${entry.key}\"\n        println(\"creating tab ${name}...\")\n        make_view(\n            name, entry.value.description,\n            { context ->\n                context.jobs {\n                    regex(entry.value.regex)\n                }\n            }\n        )\n    }\n}\n\n\ndef define_job(dashProject, dashBranchName, branchName, job_type, job_name,\n               full_name, job_values, module, isReleaseBuild) {\n\n    job(full_name) {\n        environmentVariables {\n            // we pass the 'MODULE' parameter as the flocker module to test with trial\n            if (module != null) {\n                env(\"MODULE\", module)\n            }\n            env(\"TRIGGERED_BRANCH\", branchName)\n        }\n        // Allow some attempts to checkout the source to fail, since\n        // doing so depends on a third-party, network-accessible resource.\n        //\n        // Unfortunately, this *may* not actually work due to bugs in\n        // Jenkins or the Git SCM plugin:\n        //\n        //     https://issues.jenkins-ci.org/browse/JENKINS-14575\n        //\n        checkoutRetryCount(5)\n        label(job_values.on_nodes_with_labels)\n        wrappers build_wrappers(job_values)\n        triggers build_triggers(job_type, job_values.at, branchName)\n        scm build_scm(git_url, branchName, isReleaseBuild)\n        steps {\n          shell(updateGitHubStatus(\n                'pending', 'Build started',\n                branchName, dashProject, dashBranchName, job_name))\n        }\n        steps build_steps(job_values)\n        publishers build_publishers(job_values, branchName, dashProject, dashBranchName, job_name)\n        if (job_values.notify_slack && is_master(branchName)) {\n            configure notify_slack(job_values.notify_slack)\n        }\n    }\n\n}\n\n\ndef trigger_sub_job(context, name) {\n    /*\n        Trigger a sub-job in a multijob without killing the parent on failure\n\n        :param context: the multijob context to add the job to\n        :param name: the name of the sub-job to trigger\n    */\n    context.phaseJob(name) {\n        /* make sure we don't kill the parent multijob when we\n           fail */\n        killPhaseCondition(\"NEVER\")\n    }\n}\n\n\ndef copy_artifacts_from(context, full_job_name, short_job_name, archive_artifacts) {\n    /*\n        Copy artifacts from another job to this one.\n\n        :param context: the context of the job that receives the copied\n            artifacts.\n        :param unicode full_job_name: the full path of the job.\n        :param unicode short_job_name: the name of the job without the full path.\n        :param list[str] archive_artifacts: the artifacts to archive as a\n             string glob patter.\n    */\n    for (artifact in archive_artifacts) {\n        context.copyArtifacts(full_job_name) {\n            optional(true)\n            includePatterns(artifact)\n            /* and place them under short_job_name/artifact on\n               the multijob workspace, so that we don't\n               overwrite them.  */\n            targetDirectory(short_job_name)\n            fingerprintArtifacts(true)\n            buildSelector {\n                workspace()\n            }\n        }\n    }\n}\n\nPULL_REQUEST_JOB_TYPES = ['run_trial', 'run_trial_for_storage_driver',\n                          'run_acceptance', 'run_sphinx', 'omnibus',\n                          'run_lint', 'run_client']\n\ndef build_multijob(dashProject, dashBranchName, branchName, isReleaseBuild) {\n    // -------------------------------------------------------------------------\n    // MULTIJOB CONFIGURATION BELOW\n    // --------------------------------------------------------------------------\n\n    // the multijob is responsible for running all configured jobs in parallel\n    multiJob(full_job_name(dashProject, dashBranchName, \"__main_multijob\")) {\n        /* we don't execute any code from the mulitjob run, but we need to fetch\n           the git repository so that we can track when changes are pushed upstream.\n           We add a SCM block pointing to our flocker code base.\n        */\n        scm build_scm(git_url, branchName, isReleaseBuild)\n        /* By adding a trigger of type 'github' we enable automatic builds when\n           changes are pushed to the repository.\n           This however only happens for the master branch, no other branches are\n           automatically built when new commits are pushed to those branches.\n        */\n        triggers build_triggers('multijob', 'none', branchName)\n        wrappers {\n            timestamps()\n            colorizeOutput()\n            /* lock down the multijob to 60 minutes so that 'stuck' jobs won't\n               block future runs                                                  */\n            timeout {\n                absolute(60)\n            }\n        }\n\n        /* the multijob runs on the jenkins-master only                             */\n        label(\"master\")\n        steps {\n            /* Set the commit status on GitHub to pending for each subtask we will run */\n            list_jobs(dashProject, dashBranchName).findAll { it.type in PULL_REQUEST_JOB_TYPES }.each {\n                shell(updateGitHubStatus('pending', 'Build triggered', branchName, dashProject, dashBranchName, it.name, false))\n            }\n            /* make sure we are starting with a clean workspace  */\n            shell('rm -rf *')\n            /* build 'parallel_tests' phase that will run all our jobs in parallel  */\n            phase('parallel_tests') {\n                /* and don't fail when a child job fails, as we want to collect the artifacts\n                   from the jobs, especially those that have failed   */\n                continuationCondition('ALWAYS')\n                /*\n                  in order for the different job_types to be executed as part of the multijob\n                  a block entry for that particular job_type needs to be added to the\n                  multijob configuration below.\n                */\n                context = delegate\n                list_jobs(dashProject, dashBranchName).findAll { it.type in PULL_REQUEST_JOB_TYPES }.each {\n                    trigger_sub_job(context, it.full_name)\n                }\n            } /* ends parallel phase */\n\n            /* we've added the jobs to the multijob, we now need to fetch and\n               archive all the artifacts produced by the different jobs\n               Skip the cron jobs as they don't run from this multijob,\n               so the copied artifacts could be from a build of a previous\n               revision, which may confuse things. */\n            list_jobs(dashProject, dashBranchName).findAll { it.type != 'cronly_jobs' }.each {\n                if (it.values.archive_artifacts) {\n                    copy_artifacts_from(\n                        delegate, it.full_name,\n                        it.name, it.values.archive_artifacts\n                    )\n                }\n            }\n        }\n\n        /* do an aggregation of all the test results */\n        publishers {\n            archiveJunit('**/results.xml') {\n                retainLongStdout(true)\n                testDataPublishers {\n                    /* allows a jenkins user to 'claim' a failed test, indicating\n                       that user is 'looking into it' */\n                    allowClaimingOfFailedTests()\n                    publishTestAttachments()\n                    /* publish a percentage of failures for a particular test */\n                    publishTestStabilityData()\n                    /* publish a report of tests that fail every so often */\n                    publishFlakyTestsReport()\n                }\n            }\n            /* do an aggregation of all the coverage results */\n            cobertura('**/coverage.xml') {\n                /* only produce coverage reports for stable builds */\n                onlyStable(false)\n                failUnhealthy(true)\n                failUnstable(true)\n                failNoReports(false)\n            }\n        }\n\n    }\n}\n\n\ndef generate_jobs_for_branch(dashProject, dashBranchName, displayFolderName, branchName, isReleaseBuild) {\n    /*\n        Generate all the jobs for the specific branch.\n\n        :param unicode dashProject: the project name escaped with dashes\n        :param unicode dashBranchName: the branch name escaped with dashes\n        :param unicode displayFolderName: The display name of the folder.\n        :param unicode branchName: the name of the branch for display - spaces\n             etc. allowed\n        :param bool isReleaseBuild: whether to generate a release build that\n             doesn't merge to master before performing operations.\n    */\n\n    // create a folder for every branch: /git-username/git-repo/branch\n    folder(folder_name(dashProject, dashBranchName)) {\n        displayName(displayFolderName)\n    }\n\n    // create tabs\n    build_tabs(dashBranchName)\n\n    // create individual jobs\n    list_jobs(dashProject, dashBranchName).each {\n        define_job(\n            dashProject, dashBranchName, branchName, it.type, it.name,\n            it.full_name, it.values, it.module, isReleaseBuild\n        )\n    }\n\n    // create multijob that aggregates the individual jobs\n    build_multijob(dashProject, dashBranchName, branchName, isReleaseBuild)\n}\n\n\n/* --------------------\n   MAIN ACTION:\n   --------------------\n*/\n// Iterate over every branch, and create folders, jobs\nbranches.each { branchName ->\n    println(\"iterating over branch... ${branchName}\")\n    dashBranchName = escape_name(branchName)\n    // our convention for release branches is release/flocker-<version>\n    isReleaseBuild = branchName.startsWith(\"release/\")\n\n    generate_jobs_for_branch(dashProject, dashBranchName, branchName, branchName, false)\n\n    if (isReleaseBuild) {\n        generate_jobs_for_branch(\n            dashProject, \"release-\" + dashBranchName, \"Release \" + branchName,\n            branchName, true\n        )\n    }\n}\n"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.5263671875,
          "content": "# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\nVCS = git\nstyle = pep440\nversionfile_source = flocker/_version.py\nversionfile_build = flocker/_version.py\ntag_prefix =\nparentdir_prefix = Flocker-\n\n[flake8]\nexclude=flocker/_version.py,flocker/restapi/docs/hidden_code_block.py\n# Ignore defaults + E731, E305\n# http://pep8.readthedocs.io/en/latest/intro.html#error-codes\nignore=E123,E226,E241,E731,E305,W503\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 5.7578125,
          "content": "# Copyright ClusterHQ Inc.  See LICENSE file for details.\n\n\"\"\"\nGenerate a Flocker package that can be deployed onto cluster nodes.\n\"\"\"\n\nfrom os import environ\nfrom pkg_resources import parse_requirements, RequirementParseError\nfrom setuptools import setup, find_packages\nimport versioneer\n\nwith open(\"README.rst\") as readme:\n    description = readme.read()\n\n\ndef requirements_list_from_file(requirements_file, dependency_links):\n    \"\"\"\n    Parse a requirements file.\n\n    Requirements that have an environment marker will only be included\n    in the list if the marker evaluates True.\n\n    ``--find-links`` lines will be added to the supplied ``dependency_links``\n    list.\n\n    XXX There's a package called ``pbr`` which is also supposed to do this\n    job. I couldn't get it to work --RichardW.\n    \"\"\"\n    requirements = []\n    with open(requirements_file) as f:\n        for line in f:\n            line = line.rstrip()\n            if line.startswith('#'):\n                continue\n            elif line.startswith('--find-links'):\n                link = line.split(None, 1)[1]\n                dependency_links.append(link)\n            else:\n                parsed_requirements = parse_requirements(line)\n                try:\n                    (req,) = list(parsed_requirements)\n                except RequirementParseError as original_error:\n                    # XXX Buildbot has an old version of setuptools /\n                    # pkg_resources which can't parse environment markers.\n                    message = unicode(original_error)\n                    if environ['HOME'] != \"/srv/buildslave\":\n                        raise\n                    if not message.startswith(\"Expected version spec in \"):\n                        raise\n                    if \";\" not in line:\n                        raise\n                    continue\n                if getattr(req, \"marker\", None) and not req.marker.evaluate():\n                    continue\n                requirements.append(unicode(req))\n    return requirements\n\n# Parse the ``.in`` files. This will allow the dependencies to float when\n# Flocker is installed using ``pip install .``.\n# It also allows Flocker to be imported as a package alongside other Python\n# libraries that may require different versions than those specified in\n# Flocker's pinned dependency files.\ndependency_links = []\ninstall_requires = requirements_list_from_file(\n    \"requirements/flocker.txt.in\",\n    dependency_links,\n)\ndev_requires = requirements_list_from_file(\n    \"requirements/flocker-dev.txt.in\",\n    dependency_links,\n)\n\nsetup(\n    # This is the human-targetted name of the software being packaged.\n    name=\"Flocker\",\n    # This is a string giving the version of the software being packaged.  For\n    # simplicity it should be something boring like X.Y.Z.\n    version=versioneer.get_version(),\n    # This identifies the creators of this software.  This is left symbolic for\n    # ease of maintenance.\n    author=\"ClusterHQ Team\",\n    # This is contact information for the authors.\n    author_email=\"support@clusterhq.com\",\n    # Here is a website where more information about the software is available.\n    url=\"https://clusterhq.com/\",\n\n    # A short identifier for the license under which the project is released.\n    license=\"Apache License, Version 2.0\",\n\n    # Some details about what Flocker is.  Synchronized with the README.rst to\n    # keep it up to date more easily.\n    long_description=description,\n\n    # This setuptools helper will find everything that looks like a *Python*\n    # package (in other words, things that can be imported) which are part of\n    # the Flocker package.\n    packages=find_packages(exclude=('admin', 'admin.*')),\n\n    package_data={\n        'flocker.node.functional': [\n            'sendbytes-docker/*',\n            'env-docker/*',\n            'retry-docker/*'\n        ],\n        # These data files are used by the volumes API to define input and\n        # output schemas.\n        'flocker.control': ['schema/*.yml'],\n        # These files are used by the Docker plugin API:\n        'flocker.dockerplugin': ['schema/*.yml'],\n        # Configuration schema, used to detect need for upgrade code:\n        'flocker.control.test': [\n            'persisted_model.json', 'configurations/*.json'\n        ],\n    },\n\n    entry_points={\n        # These are the command-line programs we want setuptools to install.\n        # Don't forget to modify the omnibus packaging tool\n        # (admin/packaging.py) if you make changes here.\n        'console_scripts': [\n            'flocker-volume = flocker.volume.script:flocker_volume_main',\n            'flocker-container-agent = flocker.node.script:flocker_container_agent_main',  # noqa\n            'flocker-dataset-agent = flocker.node.script:flocker_dataset_agent_main',  # noqa\n            'flocker-control = flocker.control.script:flocker_control_main',\n            'flocker-ca = flocker.ca._script:flocker_ca_main',\n            'flocker = flocker.cli.script:flocker_cli_main',\n            'flocker-docker-plugin = ' +\n            'flocker.dockerplugin._script:docker_plugin_main',\n            'flocker-diagnostics = ' +\n            'flocker.node.script:flocker_diagnostics_main',\n            'flocker-benchmark = ' +\n            'flocker.node.benchmark:flocker_benchmark_main',\n            'flocker-node-era = flocker.common._era:era_main',\n        ],\n    },\n\n    install_requires=install_requires,\n\n    extras_require={\n        # This extra is for developers who need to work on Flocker itself.\n        \"dev\": dev_requires,\n    },\n\n    cmdclass=versioneer.get_cmdclass(),\n\n    # Duplicate dependency links may have been added from different\n    # requirements files.\n    dependency_links=list(set(dependency_links)),\n\n    # Some \"trove classifiers\" which are relevant.\n    classifiers=[\n        \"License :: OSI Approved :: Apache Software License\",\n        ],\n    )\n"
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 1.1904296875,
          "content": "[tox]\nminversion = 1.6\nskipsdist=True\n\n[testenv]\nusedevelop=True\nbasepython = python2.7\nchangedir = {toxinidir}\ndeps =\n    --requirement=requirements/all.txt\n\n[testenv:test-admin]\ncommands =\n    trial --rterrors {posargs:admin}\n\n[testenv:test-benchmark]\ncommands =\n    trial --rterrors {posargs:benchmark}\n\n[testenv:test-flocker]\ncommands =\n    trial --rterrors {posargs:flocker}\n\n[testenv:lint]\n# ``ignore_errors`` allows both commands to run.\n# tox will still exit with failure code if either command fails.\nignore_errors=True\ncommands =\n    flake8 {posargs:admin benchmark flocker}\n    pylint {posargs:admin benchmark flocker}\n\n[testenv:docs-lint]\ncommands =\n    sphinx-build -d {envtmpdir}/doctrees -q -W -n -b dummy docs build/{envname}\n\n[testenv:docs-spelling]\nignore_errors = True\ncommands =\n    sphinx-build -d {envtmpdir}/doctrees -Q -b spelling docs build/{envname}\n    python -m fileinput build/{envname}/output.txt\n\n[testenv:docs-linkcheck]\nignore_errors = True\ncommands =\n    sphinx-build -d {envtmpdir}/doctrees -Q -b linkcheck docs build/{envname}\n    python -m fileinput build/{envname}/output.txt\n\n[testenv:docs-html]\ncommands =\n    sphinx-build -d {envtmpdir}/doctrees -q -b html docs build/{envname}\n"
        },
        {
          "name": "versioneer.py",
          "type": "blob",
          "size": 61.009765625,
          "content": "\n# Version: 0.15\n\n\"\"\"\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone \"update\nthe embedded version string\" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github's\n  \"tarball from tag\" feature\n* a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n  about recent \"tags\" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n\"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a 'setup.py sdist' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nFirst, decide on values for the following configuration variables:\n\n* `VCS`: the version control system you use. Currently accepts \"git\".\n\n* `style`: the style of version string to be produced. See \"Styles\" below for\n  details. Defaults to \"pep440\", which looks like\n  `TAG[+DISTANCE.gSHORTHASH[.dirty]]`.\n\n* `versionfile_source`:\n\n  A project-relative pathname into which the generated version strings should\n  be written. This is usually a `_version.py` next to your project's main\n  `__init__.py` file, so it can be imported at runtime. If your project uses\n  `src/myproject/__init__.py`, this should be `src/myproject/_version.py`.\n  This file should be checked in to your VCS as usual: the copy created below\n  by `setup.py setup_versioneer` will include code that parses expanded VCS\n  keywords in generated tarballs. The 'build' and 'sdist' commands will\n  replace it with a copy that has just the calculated version string.\n\n  This must be set even if your project does not have any modules (and will\n  therefore never import `_version.py`), since \"setup.py sdist\" -based trees\n  still need somewhere to record the pre-calculated version strings. Anywhere\n  in the source tree should do. If there is a `__init__.py` next to your\n  `_version.py`, the `setup.py setup_versioneer` command (described below)\n  will append some `__version__`-setting assignments, if they aren't already\n  present.\n\n* `versionfile_build`:\n\n  Like `versionfile_source`, but relative to the build directory instead of\n  the source directory. These will differ when your setup.py uses\n  'package_dir='. If you have `package_dir={'myproject': 'src/myproject'}`,\n  then you will probably have `versionfile_build='myproject/_version.py'` and\n  `versionfile_source='src/myproject/_version.py'`.\n\n  If this is set to None, then `setup.py build` will not attempt to rewrite\n  any `_version.py` in the built tree. If your project does not have any\n  libraries (e.g. if it only builds a script), then you should use\n  `versionfile_build = None` and override `distutils.command.build_scripts`\n  to explicitly insert a copy of `versioneer.get_version()` into your\n  generated script.\n\n* `tag_prefix`:\n\n  a string, like 'PROJECTNAME-', which appears at the start of all VCS tags.\n  If your tags look like 'myproject-1.2.0', then you should use\n  tag_prefix='myproject-'. If you use unprefixed tags like '1.2.0', this\n  should be an empty string.\n\n* `parentdir_prefix`:\n\n  a optional string, frequently the same as tag_prefix, which appears at the\n  start of all unpacked tarball filenames. If your tarball unpacks into\n  'myproject-1.2.0', this should be 'myproject-'. To disable this feature,\n  just omit the field from your `setup.cfg`.\n\nThis tool provides one script, named `versioneer`. That script has one mode,\n\"install\", which writes a copy of `versioneer.py` into the current directory\nand runs `versioneer.py setup` to finish the installation.\n\nTo versioneer-enable your project:\n\n* 1: Modify your `setup.cfg`, adding a section named `[versioneer]` and\n  populating it with the configuration values you decided earlier (note that\n  the option names are not case-sensitive):\n\n  ````\n  [versioneer]\n  VCS = git\n  style = pep440\n  versionfile_source = src/myproject/_version.py\n  versionfile_build = myproject/_version.py\n  tag_prefix = \"\"\n  parentdir_prefix = myproject-\n  ````\n\n* 2: Run `versioneer install`. This will do the following:\n\n  * copy `versioneer.py` into the top of your source tree\n  * create `_version.py` in the right place (`versionfile_source`)\n  * modify your `__init__.py` (if one exists next to `_version.py`) to define\n    `__version__` (by calling a function from `_version.py`)\n  * modify your `MANIFEST.in` to include both `versioneer.py` and the\n    generated `_version.py` in sdist tarballs\n\n  `versioneer install` will complain about any problems it finds with your\n  `setup.py` or `setup.cfg`. Run it multiple times until you have fixed all\n  the problems.\n\n* 3: add a `import versioneer` to your setup.py, and add the following\n  arguments to the setup() call:\n\n        version=versioneer.get_version(),\n        cmdclass=versioneer.get_cmdclass(),\n\n* 4: commit these changes to your VCS. To make sure you won't forget,\n  `versioneer install` will mark everything it touched for addition using\n  `git add`. Don't forget to add `setup.py` and `setup.cfg` too.\n\n## Post-Installation Usage\n\nOnce established, all uses of your tree from a VCS checkout should get the\ncurrent version string. All generated tarballs should include an embedded\nversion string (so users who unpack them will not need a VCS tool installed).\n\nIf you distribute your project through PyPI, then the release process should\nboil down to two steps:\n\n* 1: git tag 1.0\n* 2: python setup.py register sdist upload\n\nIf you distribute it through github (i.e. users use github to generate\ntarballs with `git archive`), the process is:\n\n* 1: git tag 1.0\n* 2: git push; git push --tags\n\nVersioneer will report \"0+untagged.NUMCOMMITS.gHASH\" until your tree has at\nleast one tag in its history.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `['version']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project's version\n  string. The default \"pep440\" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n  below for alternative styles.\n\n* `['full-revisionid']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n\n* `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `['error']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of \"unknown\".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an \"about\" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()['version']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, \"pep440\", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional \"local\nversion\" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\ntree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\nthat this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. \"0.11\".\n\nOther styles are available. See details.md in the Versioneer source tree for\ndescriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of \"0+unknown\". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n### Upgrading to 0.15\n\nStarting with this version, Versioneer is configured with a `[versioneer]`\nsection in your `setup.cfg` file. Earlier versions required the `setup.py` to\nset attributes on the `versioneer` module immediately after import. The new\nversion will refuse to run (raising an exception during import) until you\nhave provided the necessary `setup.cfg` section.\n\nIn addition, the Versioneer package provides an executable named\n`versioneer`, and the installation process is driven by running `versioneer\ninstall`. In 0.14 and earlier, the executable was named\n`versioneer-installer` and was run without an argument.\n\n### Upgrading to 0.14\n\n0.14 changes the format of the version string. 0.13 and earlier used\nhyphen-separated strings like \"0.11-2-g1076c97-dirty\". 0.14 and beyond use a\nplus-separated \"local version\" section strings, with dot-separated\ncomponents, like \"0.11+2.g1076c97\". PEP440-strict tools did not like the old\nformat, but should be ok with the new one.\n\n### Upgrading from 0.11 to 0.12\n\nNothing special.\n\n### Upgrading from 0.10 to 0.11\n\nYou must add a `versioneer.VCS = \"git\"` to your `setup.py` before re-running\n`setup.py setup_versioneer`. This will enable the use of additional\nversion-control systems (SVN, etc) in the future.\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is hereby released into the\npublic domain. The `_version.py` that it creates is also in the public\ndomain.\n\n\"\"\"\n\nfrom __future__ import print_function\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    pass\n\n\ndef get_root():\n    # we require that all commands are run from the project root, i.e. the\n    # directory that contains setup.py, setup.cfg, and versioneer.py .\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, \"setup.py\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow 'python path/to/setup.py COMMAND'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, \"setup.py\")\n        versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\"Versioneer was unable to run the project root directory. \"\n               \"Versioneer requires setup.py to be executed from \"\n               \"its immediate directory (like 'python setup.py COMMAND'), \"\n               \"or in a way that lets it use sys.argv[0] to find the root \"\n               \"(like 'python path/to/setup.py COMMAND').\")\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # \"versioneer\" may be imported multiple times, and python's shared\n        # module-import table will cache the first one. So we can't use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        if os.path.splitext(me)[0] != os.path.splitext(versioneer_py)[0]:\n            print(\"Warning: build in %s is using versioneer.py from %s\"\n                  % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, \"setup.cfg\")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, \"r\") as f:\n        parser.readfp(f)\n    VCS = parser.get(\"versioneer\", \"VCS\")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(\"versioneer\", name):\n            return parser.get(\"versioneer\", name)\n        return None\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, \"style\") or \"\"\n    cfg.versionfile_source = get(parser, \"versionfile_source\")\n    cfg.versionfile_build = get(parser, \"versionfile_build\")\n    cfg.tag_prefix = get(parser, \"tag_prefix\")\n    cfg.parentdir_prefix = get(parser, \"parentdir_prefix\")\n    cfg.verbose = get(parser, \"verbose\")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    pass\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n        return None\n    return stdout\nLONG_VERSION_PY['git'] = '''\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.15 (https://github.com/warner/python-versioneer)\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n    git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full}\n    return keywords\n\n\nclass VersioneerConfig:\n    pass\n\n\ndef get_config():\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"%(STYLE)s\"\n    cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n    cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n    cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    pass\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %%s\" %% dispcmd)\n                print(e)\n            return None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %%s\" %% (commands,))\n        return None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %%s (error)\" %% dispcmd)\n        return None\n    return stdout\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    # Source tarballs conventionally unpack into a directory that includes\n    # both the project name and a version string.\n    dirname = os.path.basename(root)\n    if not dirname.startswith(parentdir_prefix):\n        if verbose:\n            print(\"guessing rootdir is '%%s', but '%%s' doesn't start with \"\n                  \"prefix '%%s'\" %% (root, dirname, parentdir_prefix))\n        raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n    return {\"version\": dirname[len(parentdir_prefix):],\n            \"full-revisionid\": None,\n            \"dirty\": False, \"error\": None}\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"full\"] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    if not keywords:\n        raise NotThisMethod(\"no keywords at all, weird\")\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = set([r for r in refs if re.search(r'\\d', r)])\n        if verbose:\n            print(\"discarding '%%s', no digits\" %% \",\".join(refs-tags))\n    if verbose:\n        print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(\"picking %%s\" %% r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None\n                    }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\"}\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    # this runs 'git' from the root of the source tree. This only gets called\n    # if the git-archive 'subst' keywords were *not* expanded, and\n    # _version.py hasn't already been rewritten with a short version string,\n    # meaning we're inside a checked out source tree.\n\n    if not os.path.exists(os.path.join(root, \".git\")):\n        if verbose:\n            print(\"no .git in %%s\" %% root)\n        raise NotThisMethod(\"no .git directory\")\n\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    # if there is a tag, this yields TAG-NUM-gHEX[-dirty]\n    # if there are no tags, this yields HEX[-dirty] (no NUM)\n    describe_out = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                      \"--always\", \"--long\"],\n                               cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    # now build up version string, with post-release \"local version\n    # identifier\". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    # get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    # exceptions:\n    # 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    # TAG[.post.devDISTANCE] . No -dirty\n\n    # exceptions:\n    # 1: no tags. 0.post.devDISTANCE\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post.dev%%d\" %% pieces[\"distance\"]\n    else:\n        # exception #1\n        rendered = \"0.post.dev%%d\" %% pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    # TAG[.postDISTANCE[.dev0]+gHEX] . The \".dev0\" means dirty. Note that\n    # .dev0 sorts backwards (a dirty tree will appear \"older\" than the\n    # corresponding clean one), but you shouldn't be releasing software with\n    # -dirty anyways.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    # TAG[.postDISTANCE[.dev0]] . The \".dev0\" means dirty.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    # TAG[-DISTANCE-gHEX][-dirty], like 'git describe --tags --dirty\n    # --always'\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    # TAG-DISTANCE-gHEX[-dirty], like 'git describe --tags --dirty\n    # --always -long'. The distance/hash is unconditional.\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"]}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%%s'\" %% style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None}\n\n\ndef get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n                \"dirty\": None,\n                \"error\": \"unable to find root of source tree\"}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to compute version\"}\n'''\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"full\"] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    if not keywords:\n        raise NotThisMethod(\"no keywords at all, weird\")\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = set([r for r in refs if re.search(r'\\d', r)])\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs-tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None\n                    }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\"}\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    # this runs 'git' from the root of the source tree. This only gets called\n    # if the git-archive 'subst' keywords were *not* expanded, and\n    # _version.py hasn't already been rewritten with a short version string,\n    # meaning we're inside a checked out source tree.\n\n    if not os.path.exists(os.path.join(root, \".git\")):\n        if verbose:\n            print(\"no .git in %s\" % root)\n        raise NotThisMethod(\"no .git directory\")\n\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    # if there is a tag, this yields TAG-NUM-gHEX[-dirty]\n    # if there are no tags, this yields HEX[-dirty] (no NUM)\n    describe_out = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                      \"--always\", \"--long\"],\n                               cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith(\".pyc\") or me.endswith(\".pyo\"):\n            me = os.path.splitext(me)[0] + \".py\"\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = \"versioneer.py\"\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open(\".gitattributes\", \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if \"export-subst\" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open(\".gitattributes\", \"a+\")\n        f.write(\"%s export-subst\\n\" % versionfile_source)\n        f.close()\n        files.append(\".gitattributes\")\n    run_command(GITS, [\"add\", \"--\"] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    # Source tarballs conventionally unpack into a directory that includes\n    # both the project name and a version string.\n    dirname = os.path.basename(root)\n    if not dirname.startswith(parentdir_prefix):\n        if verbose:\n            print(\"guessing rootdir is '%s', but '%s' doesn't start with \"\n                  \"prefix '%s'\" % (root, dirname, parentdir_prefix))\n        raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n    return {\"version\": dirname[len(parentdir_prefix):],\n            \"full-revisionid\": None,\n            \"dirty\": False, \"error\": None}\n\nSHORT_VERSION_PY = \"\"\"\n# This file was generated by 'versioneer.py' (0.15) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\nimport sys\n\nversion_json = '''\n%s\n'''  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n\"\"\"\n\n\ndef versions_from_file(filename):\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(\"unable to read _version.py\")\n    mo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\",\n                   contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(\"no version_json in _version.py\")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True,\n                          indent=1, separators=(\",\", \": \"))\n    with open(filename, \"w\") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))\n\n\ndef plus_or_dot(pieces):\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    # now build up version string, with post-release \"local version\n    # identifier\". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    # get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    # exceptions:\n    # 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    # TAG[.post.devDISTANCE] . No -dirty\n\n    # exceptions:\n    # 1: no tags. 0.post.devDISTANCE\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post.dev%d\" % pieces[\"distance\"]\n    else:\n        # exception #1\n        rendered = \"0.post.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    # TAG[.postDISTANCE[.dev0]+gHEX] . The \".dev0\" means dirty. Note that\n    # .dev0 sorts backwards (a dirty tree will appear \"older\" than the\n    # corresponding clean one), but you shouldn't be releasing software with\n    # -dirty anyways.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    # TAG[.postDISTANCE[.dev0]] . The \".dev0\" means dirty.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    # TAG[-DISTANCE-gHEX][-dirty], like 'git describe --tags --dirty\n    # --always'\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    # TAG-DISTANCE-gHEX[-dirty], like 'git describe --tags --dirty\n    # --always -long'. The distance/hash is unconditional.\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"]}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None}\n\n\nclass VersioneerBadRootError(Exception):\n    pass\n\n\ndef get_versions(verbose=False):\n    # returns dict with two keys: 'version' and 'full'\n\n    if \"versioneer\" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[\"versioneer\"]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \\\n        \"please set versioneer.versionfile_source\"\n    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. 'git\n    # describe'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by 'setup.py sdist',\n    # and for users of a tarball/zipball created by 'git archive' or github's\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(\"get_keywords\")\n    from_keywords_f = handlers.get(\"keywords\")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(\"got version from expanded keyword %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(\"got version from file %s %s\" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(\"got version from VCS %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(\"got version from parentdir %s\" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(\"unable to compute version\")\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None, \"error\": \"unable to compute version\"}\n\n\ndef get_version():\n    return get_versions()[\"version\"]\n\n\ndef get_cmdclass():\n    if \"versioneer\" in sys.modules:\n        del sys.modules[\"versioneer\"]\n        # this fixes the \"python setup.py develop\" case (also 'install' and\n        # 'easy_install .'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n        # sys.modules by the time B's setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it's pre-build state, so the\n        # parent is protected against the child's \"import versioneer\". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent's versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add \"version\" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = \"report generated version string\"\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(\"Version: %s\" % vers[\"version\"])\n            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n            print(\" dirty: %s\" % vers.get(\"dirty\"))\n            if vers[\"error\"]:\n                print(\" error: %s\" % vers[\"error\"])\n    cmds[\"version\"] = cmd_version\n\n    # we override \"build_py\" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n\n    from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib,\n                                                  cfg.versionfile_build)\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n    cmds[\"build_py\"] = cmd_build_py\n\n    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {\"DOLLAR\": \"$\",\n                             \"STYLE\": cfg.style,\n                             \"TAG_PREFIX\": cfg.tag_prefix,\n                             \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                             \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                             })\n        cmds[\"build_exe\"] = cmd_build_exe\n        del cmds[\"build_py\"]\n\n    # we override different \"sdist\" commands for both environments\n    if \"setuptools\" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[\"version\"]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile,\n                                  self._versioneer_generated_versions)\n    cmds[\"sdist\"] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = \"\"\"\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix = \"\"\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n\"\"\"\n\nSAMPLE_CONFIG = \"\"\"\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n\"\"\"\n\nINIT_PY_SNIPPET = \"\"\"\nfrom ._version import get_versions\n__version__ = get_versions()['version']\ndel get_versions\n\"\"\"\n\n\ndef do_setup():\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError,\n            configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\",\n                  file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print(\" creating %s\" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, \"w\") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(LONG % {\"DOLLAR\": \"$\",\n                        \"STYLE\": cfg.style,\n                        \"TAG_PREFIX\": cfg.tag_prefix,\n                        \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                        \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        })\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                       \"__init__.py\")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, \"r\") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = \"\"\n        if INIT_PY_SNIPPET not in old:\n            print(\" appending to %s\" % ipy)\n            with open(ipy, \"a\") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print(\" %s unmodified\" % ipy)\n    else:\n        print(\" %s doesn't exist, ok\" % ipy)\n        ipy = None\n\n    # Make sure both the top-level \"versioneer.py\" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they'll be copied into source distributions. Pip won't be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, \"MANIFEST.in\")\n    simple_includes = set()\n    try:\n        with open(manifest_in, \"r\") as f:\n            for line in f:\n                if line.startswith(\"include \"):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn't cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant 'include'\n    # lines is safe, though.\n    if \"versioneer.py\" not in simple_includes:\n        print(\" appending 'versioneer.py' to MANIFEST.in\")\n        with open(manifest_in, \"a\") as f:\n            f.write(\"include versioneer.py\\n\")\n    else:\n        print(\" 'versioneer.py' already in MANIFEST.in\")\n    if cfg.versionfile_source not in simple_includes:\n        print(\" appending versionfile_source ('%s') to MANIFEST.in\" %\n              cfg.versionfile_source)\n        with open(manifest_in, \"a\") as f:\n            f.write(\"include %s\\n\" % cfg.versionfile_source)\n    else:\n        print(\" versionfile_source already in MANIFEST.in\")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-time keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    found = set()\n    setters = False\n    errors = 0\n    with open(\"setup.py\", \"r\") as f:\n        for line in f.readlines():\n            if \"import versioneer\" in line:\n                found.add(\"import\")\n            if \"versioneer.get_cmdclass()\" in line:\n                found.add(\"cmdclass\")\n            if \"versioneer.get_version()\" in line:\n                found.add(\"get_version\")\n            if \"versioneer.VCS\" in line:\n                setters = True\n            if \"versioneer.versionfile_source\" in line:\n                setters = True\n    if len(found) != 3:\n        print(\"\")\n        print(\"Your setup.py appears to be missing some important items\")\n        print(\"(but I might be wrong). Please make sure it has something\")\n        print(\"roughly like the following:\")\n        print(\"\")\n        print(\" import versioneer\")\n        print(\" setup( version=versioneer.get_version(),\")\n        print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n        print(\"\")\n        errors += 1\n    if setters:\n        print(\"You should remove lines like 'versioneer.VCS = ' and\")\n        print(\"'versioneer.versionfile_source = ' . This configuration\")\n        print(\"now lives in setup.cfg, and should be removed from setup.py\")\n        print(\"\")\n        errors += 1\n    return errors\n\nif __name__ == \"__main__\":\n    cmd = sys.argv[1]\n    if cmd == \"setup\":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n"
        }
      ]
    }
  ]
}