{
  "metadata": {
    "timestamp": 1736559576164,
    "page": 195,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "TachibanaYoshino/AnimeGAN",
      "stars": 4510,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "AnimeGAN.py",
          "type": "blob",
          "size": 14.3759765625,
          "content": "from tools.ops import *\nfrom tools.utils import *\nfrom glob import glob\nimport time\nimport numpy as np\nfrom net import generator\nfrom net.discriminator import D_net\nfrom tools.data_loader import ImageGenerator\nfrom tools.vgg19 import Vgg19\n\nclass AnimeGAN(object) :\n    def __init__(self, sess, args):\n        self.model_name = 'AnimeGAN'\n\n        self.sess = sess\n        self.checkpoint_dir = args.checkpoint_dir\n        self.log_dir = args.log_dir\n        self.dataset_name = args.dataset\n\n        self.epoch = args.epoch\n        self.init_epoch = args.init_epoch # args.epoch // 20\n\n        self.gan_type = args.gan_type\n\n        self.batch_size = args.batch_size\n\n        self.save_freq = args.save_freq\n\n        self.init_lr = args.init_lr\n        self.d_lr = args.d_lr\n        self.g_lr = args.g_lr\n\n\n        \"\"\" Weight \"\"\"\n        self.g_adv_weight = args.g_adv_weight\n        self.d_adv_weight = args.d_adv_weight\n        self.con_weight = args.con_weight\n        self.sty_weight = args.sty_weight\n        self.color_weight = args.color_weight\n\n        self.training_rate = args.training_rate\n        self.ld = args.ld\n\n        self.img_size = args.img_size\n        self.img_ch = args.img_ch\n\n        \"\"\" Discriminator \"\"\"\n        self.n_dis = args.n_dis\n        self.ch = args.ch\n        self.sn = args.sn\n\n        self.sample_dir = os.path.join(args.sample_dir, self.model_dir)\n        check_folder(self.sample_dir)\n\n        self.real = tf.placeholder(tf.float32, [self.batch_size, self.img_size[0], self.img_size[1], self.img_ch], name='real_A')\n        self.anime = tf.placeholder(tf.float32, [self.batch_size, self.img_size[0], self.img_size[1], self.img_ch], name='anime_A')\n        self.anime_smooth = tf.placeholder(tf.float32, [self.batch_size, self.img_size[0], self.img_size[1], self.img_ch], name='anime_smooth_A')\n        self.test_real = tf.placeholder(tf.float32, [1, None, None, self.img_ch], name='test_input')\n\n        self.anime_gray = tf.placeholder(tf.float32, [self.batch_size, self.img_size[0], self.img_size[1], self.img_ch],name='anime_B')\n\n\n        self.real_image_generator = ImageGenerator('./dataset/train_photo', self.img_size, self.batch_size)\n        self.anime_image_generator = ImageGenerator('./dataset/{}'.format(self.dataset_name + '/style'), self.img_size, self.batch_size)\n        self.anime_smooth_generator = ImageGenerator('./dataset/{}'.format(self.dataset_name + '/smooth'), self.img_size, self.batch_size)\n        self.dataset_num = max(self.real_image_generator.num_images, self.anime_image_generator.num_images)\n\n        self.vgg = Vgg19()\n\n        print()\n        print(\"##### Information #####\")\n        print(\"# gan type : \", self.gan_type)\n        print(\"# dataset : \", self.dataset_name)\n        print(\"# max dataset number : \", self.dataset_num)\n        print(\"# batch_size : \", self.batch_size)\n        print(\"# epoch : \", self.epoch)\n        print(\"# init_epoch : \", self.init_epoch)\n        print(\"# training image size [H, W] : \", self.img_size)\n        print(\"# g_adv_weight,d_adv_weight,con_weight,sty_weight,color_weight : \", self.g_adv_weight,self.d_adv_weight,self.con_weight,self.sty_weight,self.color_weight)\n        print(\"# init_lr,g_lr,d_lr : \", self.init_lr,self.g_lr,self.d_lr)\n        print(f\"# training_rate G -- D: {self.training_rate} : 1\" )\n        print()\n\n    ##################################################################################\n    # Generator\n    ##################################################################################\n\n    def generator(self,x_init, reuse=False, scope=\"generator\"):\n\n        with tf.variable_scope(scope, reuse=reuse) :\n            G = generator.G_net(x_init)\n            return G.fake\n\n    ##################################################################################\n    # Discriminator\n    ##################################################################################\n\n    def discriminator(self, x_init, reuse=False, scope=\"discriminator\"):\n\n            D = D_net(x_init, self.ch, self.n_dis, self.sn, reuse=reuse, scope=scope)\n\n            return D\n\n    ##################################################################################\n    # Model\n    ##################################################################################\n    def gradient_panalty(self, real, fake, scope=\"discriminator\"):\n        if self.gan_type.__contains__('dragan') :\n            eps = tf.random_uniform(shape=tf.shape(real), minval=0., maxval=1.)\n            _, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3])\n            x_std = tf.sqrt(x_var)  # magnitude of noise decides the size of local region\n\n            fake = real + 0.5 * x_std * eps\n\n        alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\n        interpolated = real + alpha * (fake - real)\n\n        logit, _= self.discriminator(interpolated, reuse=True, scope=scope)\n\n\n        grad = tf.gradients(logit, interpolated)[0] # gradient of D(interpolated)\n        grad_norm = tf.norm(flatten(grad), axis=1) # l2 norm\n\n        GP = 0\n        # WGAN - LP\n        if self.gan_type.__contains__('lp'):\n            GP = self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.)))\n\n        elif self.gan_type.__contains__('gp') or self.gan_type == 'dragan' :\n            GP = self.ld * tf.reduce_mean(tf.square(grad_norm - 1.))\n\n        return GP\n\n    def build_model(self):\n\n        \"\"\" Define Generator, Discriminator \"\"\"\n        self.generated = self.generator(self.real)\n        self.test_generated = self.generator(self.test_real, reuse=True)\n\n        anime_logit = self.discriminator(self.anime)\n        anime_gray_logit = self.discriminator(self.anime_gray, reuse=True)\n\n        generated_logit = self.discriminator(self.generated, reuse=True)\n        smooth_logit = self.discriminator(self.anime_smooth, reuse=True)\n\n        \"\"\" Define Loss \"\"\"\n        if self.gan_type.__contains__('gp') or self.gan_type.__contains__('lp') or self.gan_type.__contains__('dragan') :\n            GP = self.gradient_panalty(real=self.anime, fake=self.generated)\n        else :\n            GP = 0.0\n\n        # init pharse\n        init_c_loss = con_loss(self.vgg, self.real, self.generated)\n        init_loss = self.con_weight * init_c_loss\n        \n        self.init_loss = init_loss\n\n        # gan\n        c_loss, s_loss = con_sty_loss(self.vgg, self.real, self.anime_gray, self.generated)\n        t_loss = self.con_weight * c_loss + self.sty_weight * s_loss + color_loss(self.real,self.generated) * self.color_weight\n\n        g_loss = self.g_adv_weight * generator_loss(self.gan_type, generated_logit)\n        d_loss = self.d_adv_weight * discriminator_loss(self.gan_type, anime_logit, anime_gray_logit, generated_logit, smooth_logit) + GP\n\n        self.Generator_loss =  t_loss + g_loss\n        self.Discriminator_loss = d_loss\n\n        \"\"\" Training \"\"\"\n        t_vars = tf.trainable_variables()\n        G_vars = [var for var in t_vars if 'generator' in var.name]\n        D_vars = [var for var in t_vars if 'discriminator' in var.name]\n\n        self.init_optim = tf.train.AdamOptimizer(self.init_lr, beta1=0.5, beta2=0.999).minimize(self.init_loss, var_list=G_vars)\n        self.G_optim = tf.train.AdamOptimizer(self.g_lr , beta1=0.5, beta2=0.999).minimize(self.Generator_loss, var_list=G_vars)\n        self.D_optim = tf.train.AdamOptimizer(self.d_lr , beta1=0.5, beta2=0.999).minimize(self.Discriminator_loss, var_list=D_vars)\n\n\n        \"\"\"\" Summary \"\"\"\n        self.G_loss = tf.summary.scalar(\"Generator_loss\", self.Generator_loss)\n        self.D_loss = tf.summary.scalar(\"Discriminator_loss\", self.Discriminator_loss)\n\n        self.G_gan = tf.summary.scalar(\"G_gan\", g_loss)\n        self.G_vgg = tf.summary.scalar(\"G_vgg\", t_loss)\n        self.G_init_loss = tf.summary.scalar(\"G_init\", init_loss)\n\n        self.V_loss_merge = tf.summary.merge([self.G_init_loss])\n        self.G_loss_merge = tf.summary.merge([self.G_loss, self.G_gan, self.G_vgg, self.G_init_loss])\n        self.D_loss_merge = tf.summary.merge([self.D_loss])\n\n\n    def train(self):\n        # initialize all variables\n        self.sess.run(tf.global_variables_initializer())\n\n        # saver to save model\n        self.saver = tf.train.Saver(max_to_keep=self.epoch)\n\n        # summary writer\n        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n\n        \"\"\" Input Image\"\"\"\n        real_img_op, anime_img_op, anime_smooth_op  = self.real_image_generator.load_images(), self.anime_image_generator.load_images(), self.anime_smooth_generator.load_images()\n\n\n        # restore check-point if it exits\n        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n        if could_load:\n            start_epoch = checkpoint_counter + 1\n\n            print(\" [*] Load SUCCESS\")\n        else:\n            start_epoch = 0\n\n            print(\" [!] Load failed...\")\n\n        # loop for epoch\n        init_mean_loss = []\n        mean_loss = []\n        # training times , G : D = self.training_rate : 1\n        j = self.training_rate\n        for epoch in range(start_epoch, self.epoch):\n\n            for idx in range(int(self.dataset_num / self.batch_size)):\n\n                anime, anime_smooth, real = self.sess.run([anime_img_op, anime_smooth_op, real_img_op])\n\n                train_feed_dict = {\n                    self.real:real[0],\n                    self.anime:anime[0],\n                    self.anime_gray:anime[1],\n                    self.anime_smooth:anime_smooth[1]\n                }\n\n                if epoch < self.init_epoch :\n                    # Init G\n                    start_time = time.time()\n\n                    real_images, generator_images, _, v_loss, summary_str = self.sess.run([self.real, self.generated,\n                                                                             self.init_optim,\n                                                                             self.init_loss, self.V_loss_merge], feed_dict = train_feed_dict)\n                    self.writer.add_summary(summary_str, epoch)\n                    init_mean_loss.append(v_loss)\n\n                    print(\"Epoch: %3d Step: %5d / %5d  time: %f s init_v_loss: %.8f  mean_v_loss: %.8f\" % (epoch, idx,int(self.dataset_num / self.batch_size), time.time() - start_time, v_loss, np.mean(init_mean_loss)))\n                    if (idx+1)%200 ==0:\n                        init_mean_loss.clear()\n                else :\n                    start_time = time.time()\n\n                    if j == self.training_rate:\n                        # Update D\n                        _, d_loss, summary_str = self.sess.run([self.D_optim, self.Discriminator_loss, self.D_loss_merge],\n                                                            feed_dict=train_feed_dict)\n                        self.writer.add_summary(summary_str, epoch)\n\n                    # Update G\n                    real_images, generator_images, _, g_loss, summary_str = self.sess.run([self.real, self.generated,self.G_optim,\n                                                                                              self.Generator_loss, self.G_loss_merge], feed_dict = train_feed_dict)\n                    self.writer.add_summary(summary_str, epoch)\n\n                    mean_loss.append([d_loss, g_loss])\n                    if j == self.training_rate:\n\n                        print(\n                            \"Epoch: %3d Step: %5d / %5d  time: %f s d_loss: %.8f, g_loss: %.8f -- mean_d_loss: %.8f, mean_g_loss: %.8f\" % (\n                                epoch, idx, int(self.dataset_num / self.batch_size), time.time() - start_time, d_loss, g_loss, np.mean(mean_loss, axis=0)[0],\n                                np.mean(mean_loss, axis=0)[1]))\n                    else:\n                        print(\n                            \"Epoch: %3d Step: %5d / %5d time: %f s , g_loss: %.8f --  mean_g_loss: %.8f\" % (\n                                epoch, idx, int(self.dataset_num / self.batch_size), time.time() - start_time, g_loss, np.mean(mean_loss, axis=0)[1]))\n\n                    if (idx + 1) % 200 == 0:\n                        mean_loss.clear()\n\n                    j = j - 1\n                    if j < 1:\n                        j = self.training_rate\n\n\n            if (epoch + 1) >= self.init_epoch and np.mod(epoch + 1, self.save_freq) == 0:\n                self.save(self.checkpoint_dir, epoch)\n\n            if epoch >= self.init_epoch -1:\n                \"\"\" Result Image \"\"\"\n                val_files = glob('./dataset/{}/*.*'.format('val'))\n                save_path = './{}/{:03d}/'.format(self.sample_dir, epoch)\n                check_folder(save_path)\n                for i, sample_file in enumerate(val_files):\n                    print('val: '+ str(i) + sample_file)\n                    sample_image = np.asarray(load_test_data(sample_file, self.img_size))\n                    test_real,test_generated = self.sess.run([self.test_real,self.test_generated],feed_dict = {self.test_real:sample_image} )\n\n                    save_images(test_real, save_path+'{:03d}_a.jpg'.format(i), None)\n                    save_images(test_generated, save_path + '{:03d}_b.jpg'.format(i), None)\n                    # adjust_brightness_from_photo_to_fake\n                    # save_images(test_generated, save_path+'{:03d}_b.jpg'.format(i), sample_file)\n\n\n    @property\n    def model_dir(self):\n        return \"{}_{}_{}_{}_{}_{}_{}_{}\".format(self.model_name, self.dataset_name,\n                                                   self.gan_type,\n                                                   int(self.g_adv_weight), int(self.d_adv_weight), int(self.con_weight), int(self.sty_weight), int(self.color_weight))\n\n    def save(self, checkpoint_dir, step):\n        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)\n\n    def load(self, checkpoint_dir):\n        print(\" [*] Reading checkpoints...\")\n        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir) # checkpoint file information\n\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path) # first line\n            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n            counter = int(ckpt_name.split('-')[-1])\n            print(\" [*] Success to read {}\".format(ckpt_name))\n            return True, counter\n        else:\n            print(\" [*] Failed to find a checkpoint\")\n            return False, 0\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.4189453125,
          "content": "# AnimeGAN    \nA Tensorflow implementation of AnimeGAN for fast photo animation ! &ensp;&ensp;&ensp;&ensp;[日本語](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/doc/Japanese_README.md)  \nThe paper can be accessed [here](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/doc/Chen2020_Chapter_AnimeGAN.pdf) or on the [website](https://link.springer.com/chapter/10.1007/978-981-15-5577-0_18).  \n    \n### If you like what I'm doing you can tip me on [*patreon*](https://www.patreon.com/Asher_Chan).    \n   \nPhotos [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1PbBkmj1EhULvEE8AXr2z84pZ2DQJN4hc/view?usp=sharing)   \n    \nVideos [![Colab for videos](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1qhBxA72Wxbh6Eyhd-V0zY_jTIblP9rHz/view?usp=sharing)    \n____\n* Added the AnimeGAN [Colab](https://drive.google.com/file/d/1PbBkmj1EhULvEE8AXr2z84pZ2DQJN4hc/view?usp=sharing).  \n* [**AnimeGANv3**](https://github.com/TachibanaYoshino/AnimeGANv3) has been released.   \n* [**AnimeGANv2, the improved version of AnimeGAN.**](https://github.com/TachibanaYoshino/AnimeGANv2)   \n* **Online access**:  Be grateful to [@TonyLianLong](https://github.com/TonyLianLong/AnimeGAN.js) for developing an online access project, you can implement photo animation through a browser without installing anything, [click here to have a try](https://animegan.js.org/).     \n* **Pytorch version**:  [pytorch-animeGAN](https://github.com/ptran1203/pytorch-animeGAN). Be grateful to @ptran1203.   \n   \n-----\n**Some suggestions:**   \n1. since the real photos in the training set are all landscape photos, if you want to stylize the photos with people as the main body, you may as well add at least 3000 photos of people in the training set and retrain to obtain a new model.  \n2. In order to obtain a better face animation effect, when using 2 images as data pairs for training, it is suggested that the faces in the photos and the faces in the anime style data should be consistent in terms of gender as much as possible.  \n3. The generated stylized images will be affected by the overall brightness and tone of the style data, so try not to select the anime images of night as the style data, and it is necessary to make an exposure compensation for the overall style data to promote the consistency of brightness and darkness of the entire style data.  \n  \n**News:**   \n&ensp;&ensp;&ensp;&ensp;&ensp;  ***AnimeGANv2*** has been released and can be accessed [here](https://github.com/TachibanaYoshino/AnimeGANv2).  \n```yaml\nThe improvement directions of AnimeGANv2 mainly include the following 4 points:  \n```\n&ensp;&ensp; 1. Solve the problem of high-frequency artifacts in the generated image.  \n&ensp;&ensp; 2. It is easy to train and directly achieve the effects in the paper.  \n&ensp;&ensp; 3. Further reduce the number of parameters of the generator network.  \n&ensp;&ensp; 4. Use new high-quality style data, which come from BD movies as much as possible.  \n  \n___  \n\n## Requirements  \n- python 3.7  \n- tensorflow-gpu 1.15.0 (ubuntu, GPU 2080Ti, cuda 10.0.130, cudnn 7.6.0)  \n- opencv  \n- tqdm  \n- numpy  \n- glob  \n- argparse  \n  \n## Usage  \n### 1. Inference  \n  e.g.  `python test.py --checkpoint_dir checkpoint/generator_Hayao_weight --test_dir dataset/test/real --style_name H`\n    \n### 2. Convert video to anime  \n  e.g. `python video2anime.py  --video video/input/お花見.mp4  --checkpoint_dir  ./checkpoint/generator_Hayao_weight`  \n    \n### 3. Train \n#### 1. Download vgg19 or Pretrained model  \n> [vgg19.npy](https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/vgg16%2F19.npy)  \n  \n> [Pretrained model](https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/Haoyao-style_V1.0)  \n  \n#### 2. Download dataset  \n> [Link](https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/dataset-1)  \n  \n#### 3. Do edge_smooth  \n   e.g. `python edge_smooth.py --dataset Hayao --img_size 256`  \n  \n#### 4. Train\n   e.g. `python train.py --dataset Hayao  --epoch 101 --init_epoch 5`  \n  \n#### 5. Extract the weights of the generator  \n   e.g. `python get_generator_ckpt.py --checkpoint_dir  ../checkpoint/AnimeGAN_Hayao_lsgan_300_300_1_1_10  --style_name Hayao`  \n    \n   \n____  \n## Results  \n:blush:  pictures from the paper - *AnimeGAN: a novel lightweight GAN for photo animation*  \n  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/doc/sota.png)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/doc/e2.png)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/doc/e3.png)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/doc/e4.png)  \n  \n:heart_eyes:  Photo  to  Hayao  Style  \n  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/AE86.png) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/AE86.png)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2037.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2037.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%201.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%201.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2031.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2031.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2021.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2021.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2022.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2022.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2024.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2024.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2046.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2046.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2030.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2030.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2028.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2028.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo/%2044.jpg) ![](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/result/Hayao/photo_result/%2044.jpg)  \n____  \n## License   \nThis repo is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications. Permission is granted to use the AnimeGAN given that you agree to my license terms. Regarding the request for commercial use, please contact us via email to help you obtain the authorization letter.  \n## Author  \nXin Chen, Gang Liu, Jie Chen  \n## Acknowledgment  \nThis code is based on the [CartoonGAN-Tensorflow](https://github.com/taki0112/CartoonGAN-Tensorflow/blob/master/CartoonGAN.py) and [Anime-Sketch-Coloring-with-Swish-Gated-Residual-UNet](https://github.com/pradeeplam/Anime-Sketch-Coloring-with-Swish-Gated-Residual-UNet). Thanks to the contributors of this project.  \n\n"
        },
        {
          "name": "Requirements installation tutorial.md",
          "type": "blob",
          "size": 0.6357421875,
          "content": "1. Linux os (Ubuntu 18.04 or 20.04), Nvidia GPU (e.g. 2080ti) , Install GPU Driver  \n2. Use Anaconda or Miniconda to build python environment. The Miniconda version I used can be downloaded [here](https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh).  \n3. Install CUDA in conda command window : `conda install cudatoolkit=10` , Then install cudnn : `conda install cudnn=7.6.0`  \n4. Install other requirements by pip command, such as `pip install tensorflow-gpu==1.15.0`, `pip install tqdm`, `pip install glob` and so on.  \n5. Place the downloaded dataset in the corresponding directory, and then perform training and testing.  \n\n"
        },
        {
          "name": "checkpoint",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "net",
          "type": "tree",
          "content": null
        },
        {
          "name": "result",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 3.50390625,
          "content": "import argparse\r\nfrom tools.utils import *\r\nimport os\r\nfrom tqdm import tqdm\r\nfrom glob import glob\r\nimport time\r\nimport numpy as np\r\nfrom net import generator\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\ndef parse_args():\r\n    desc = \"AnimeGAN\"\r\n    parser = argparse.ArgumentParser(description=desc)\r\n\r\n    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint/'+'generator_Hayao_weight/',\r\n                        help='Directory name to save the checkpoints')\r\n    parser.add_argument('--test_dir', type=str, default='dataset/test/HR_photo',\r\n                        help='Directory name of test photos')\r\n    parser.add_argument('--style_name', type=str, default='Hayao',\r\n                        help='what style you want to get')\r\n    parser.add_argument('--if_adjust_brightness', type=bool, default=False,\r\n                        help='adjust brightness by the real photo')\r\n\r\n    return parser.parse_args()\r\n\r\ndef stats_graph(graph):\r\n    flops = tf.profiler.profile(graph, options=tf.profiler.ProfileOptionBuilder.float_operation())\r\n    # params = tf.profiler.profile(graph, options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())\r\n    print('FLOPs: {}'.format(flops.total_float_ops))\r\n\r\ndef test(checkpoint_dir, style_name, test_dir, if_adjust_brightness, img_size=[256,256]):\r\n    # tf.reset_default_graph()\r\n    test_files = glob('{}/*.*'.format(test_dir))\r\n\r\n    result_dir = 'results/'+style_name\r\n    check_folder(result_dir)\r\n\r\n    # test_real = tf.placeholder(tf.float32, [1, 256, 256, 3], name='test')\r\n    test_real = tf.placeholder(tf.float32, [1, None, None, 3], name='test')\r\n\r\n    with tf.variable_scope(\"generator\", reuse=False):\r\n        test_generated = generator.G_net(test_real).fake\r\n\r\n    generator_var = [var for var in tf.trainable_variables() if var.name.startswith('generator')]\r\n    saver = tf.train.Saver(generator_var)\r\n\r\n    gpu_options = tf.GPUOptions(allow_growth=True)\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)) as sess:\r\n        # tf.global_variables_initializer().run()\r\n        # load model\r\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)  # checkpoint file information\r\n        if ckpt and ckpt.model_checkpoint_path:\r\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)  # first line\r\n            saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\r\n            print(\" [*] Success to read {}\".format(ckpt_name))\r\n        else:\r\n            print(\" [*] Failed to find a checkpoint\")\r\n            return\r\n        \r\n        # FLOPs\r\n        # stats_graph(tf.get_default_graph())\r\n\r\n        begin = time.time()\r\n        for sample_file  in tqdm(test_files) :\r\n            # print('Processing image: ' + sample_file)\r\n            sample_image = np.asarray(load_test_data(sample_file, img_size))\r\n            image_path = os.path.join(result_dir,'{0}'.format(os.path.basename(sample_file)))\r\n            fake_img = sess.run(test_generated, feed_dict = {test_real : sample_image})\r\n            if if_adjust_brightness:\r\n                save_images(fake_img, image_path, sample_file)\r\n            else:\r\n                save_images(fake_img, image_path, None)\r\n        end = time.time()\r\n        print(f'test-time: {end-begin} s')\r\n        print(f'one image test time : {(end-begin)/len(test_files)} s')\r\n        print(f'result path: {result_dir}')\r\nif __name__ == '__main__':\r\n    arg = parse_args()\r\n    print(arg.checkpoint_dir)\r\n    test(arg.checkpoint_dir, arg.style_name, arg.test_dir, arg.if_adjust_brightness)\r\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.919921875,
          "content": "from AnimeGAN import AnimeGAN\nimport argparse\nfrom tools.utils import *\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\n\"\"\"parsing and configuration\"\"\"\n\ndef parse_args():\n    desc = \"Tensorflow implementation of AnimeGAN\"\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('--dataset', type=str, default='Hayao', help='dataset_name')\n\n    parser.add_argument('--epoch', type=int, default=101, help='The number of epochs to run')\n    parser.add_argument('--init_epoch', type=int, default=5, help='The number of epochs for weight initialization')\n    parser.add_argument('--batch_size', type=int, default=6, help='The size of batch size')\n    parser.add_argument('--save_freq', type=int, default=1, help='The number of ckpt_save_freq')\n\n    parser.add_argument('--init_lr', type=float, default=1e-4, help='The learning rate')\n    parser.add_argument('--g_lr', type=float, default=8e-5, help='The learning rate')\n    parser.add_argument('--d_lr', type=float, default=16e-5, help='The learning rate')\n    parser.add_argument('--ld', type=float, default=10.0, help='The gradient penalty lambda')\n\n    parser.add_argument('--g_adv_weight', type=float, default=300.0, help='Weight about GAN')\n    parser.add_argument('--d_adv_weight', type=float, default=300.0, help='Weight about GAN')\n    parser.add_argument('--con_weight', type=float, default=1.5, help='Weight about VGG19') # 1.1 for Shinkai\n    # ------ the follow weight used in AnimeGAN\n    parser.add_argument('--sty_weight', type=float, default=3.0, help='Weight about style')\n    parser.add_argument('--color_weight', type=float, default=10.0, help='Weight about color')\n    # ---------------------------------------------\n    parser.add_argument('--training_rate', type=int, default=1, help='training rate about G & D')\n    parser.add_argument('--gan_type', type=str, default='lsgan', help='[gan / lsgan / wgan-gp / wgan-lp / dragan / hinge')\n\n    parser.add_argument('--img_size', type=list, default=[256,256], help='The size of image: H and W')\n    parser.add_argument('--img_ch', type=int, default=3, help='The size of image channel')\n\n    parser.add_argument('--ch', type=int, default=64, help='base channel number per layer')\n    parser.add_argument('--n_dis', type=int, default=3, help='The number of discriminator layer')\n    parser.add_argument('--sn', type=str2bool, default=True, help='using spectral norm')\n\n    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n                        help='Directory name to save the checkpoints')\n    parser.add_argument('--log_dir', type=str, default='logs',\n                        help='Directory name to save training logs')\n    parser.add_argument('--sample_dir', type=str, default='samples',\n                        help='Directory name to save the samples on training')\n\n    return check_args(parser.parse_args())\n\n\"\"\"checking arguments\"\"\"\ndef check_args(args):\n    # --checkpoint_dir\n    check_folder(args.checkpoint_dir)\n\n    # --log_dir\n    check_folder(args.log_dir)\n\n    # --sample_dir\n    check_folder(args.sample_dir)\n\n    # --epoch\n    try:\n        assert args.epoch >= 1\n    except:\n        print('number of epochs must be larger than or equal to one')\n\n    # --batch_size\n    try:\n        assert args.batch_size >= 1\n    except:\n        print('batch size must be larger than or equal to one')\n    return args\n\n\n\"\"\"main\"\"\"\ndef main():\n    # parse arguments\n    args = parse_args()\n    if args is None:\n      exit()\n\n    # open session\n    gpu_options = tf.GPUOptions(allow_growth=True)\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,inter_op_parallelism_threads=8,\n                               intra_op_parallelism_threads=8,gpu_options=gpu_options)) as sess:\n        gan = AnimeGAN(sess, args)\n\n        # build graph\n        gan.build_model()\n      \n        # show network architecture\n        show_all_variables()\n\n        gan.train()\n        print(\" [*] Training finished!\")\n\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "vgg19_weight",
          "type": "tree",
          "content": null
        },
        {
          "name": "video",
          "type": "tree",
          "content": null
        },
        {
          "name": "video2anime.py",
          "type": "blob",
          "size": 4.642578125,
          "content": "'''\n   made by @finnkso (github)\n   2020.04.09\n'''\nimport argparse\nimport os\nimport cv2\nfrom tqdm import tqdm\nimport numpy as np\nimport tensorflow as tf\n\nfrom net import generator\nfrom tools.utils import preprocessing, check_folder\nfrom tools.adjust_brightness import adjust_brightness_from_src_to_dst\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndef parse_args():\n    desc = \"Tensorflow implementation of AnimeGAN\"\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('--video', type=str, default='video/input/'+ 'お花見.mp4',\n                        help='video file or number for webcam')\n    parser.add_argument('--checkpoint_dir', type=str, default='../checkpoint/generator_Hayao_weight',\n                        help='Directory name to save the checkpoints')\n    parser.add_argument('--output', type=str, default='video/output',\n                        help='output path')\n    parser.add_argument('--output_format', type=str, default='MP4V',\n                        help='codec used in VideoWriter when saving video to file')\n    parser.add_argument('--if_adjust_brightness', type=bool, default=False,\n                        help='adjust brightness by the real photo')\n    return parser.parse_args()\n\n\ndef convert_image(img, img_size):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = preprocessing(img, img_size)\n    img = np.expand_dims(img, axis=0)\n    img = np.asarray(img)\n    return img\n\ndef inverse_image(img):\n    img = (img.squeeze()+1.) / 2 * 255\n    img = img.astype(np.uint8)\n    return img\n\ndef cvt2anime_video(video, output, checkpoint_dir, output_format='MP4V', if_adjust_brightness=False, img_size=(256,256)):\n    '''\n    output_format: 4-letter code that specify codec to use for specific video type. e.g. for mp4 support use \"H264\", \"MP4V\", or \"X264\"\n    '''\n    # tf.reset_default_graph()\n    # check_folder(result_dir)\n    gpu_stat = bool(len(tf.config.experimental.list_physical_devices('GPU')))\n    if gpu_stat:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n    gpu_options = tf.GPUOptions(allow_growth=gpu_stat)\n\n    test_real = tf.placeholder(tf.float32, [1, None, None, 3], name='test')\n\n    with tf.variable_scope(\"generator\", reuse=False):\n        test_generated = generator.G_net(test_real).fake\n\n    # load video\n    vid = cv2.VideoCapture(video)\n    vid_name = os.path.basename(video)\n    total = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = vid.get(cv2.CAP_PROP_FPS)\n    # codec = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n    codec = cv2.VideoWriter_fourcc(*output_format)\n\n    tfconfig = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n    with tf.Session(config=tfconfig) as sess:\n        # tf.global_variables_initializer().run()\n        # load model\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)  # checkpoint file information\n        saver = tf.train.Saver()\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)  # first line\n            saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\n            print(\" [*] Success to read {}\".format(ckpt_name))\n        else:\n            print(\" [*] Failed to find a checkpoint\")\n            return\n\n        # determine output width and height\n        ret, img = vid.read()\n        if img is None:\n            print('Error! Failed to determine frame size: frame empty.')\n            return\n        img = preprocessing(img, img_size)\n        height, width = img.shape[:2]\n        out = cv2.VideoWriter(os.path.join(output, vid_name), codec, fps, (width, height))\n\n        pbar = tqdm(total=total)\n        vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n        while ret:\n            ret, frame = vid.read()\n            if frame is None:\n                print('Warning: got empty frame.')\n                continue\n\n            img = convert_image(frame, img_size)\n            fake_img = sess.run(test_generated, feed_dict={test_real: img})\n            fake_img = inverse_image(fake_img)\n            if if_adjust_brightness:\n                fake_img = cv2.cvtColor(adjust_brightness_from_src_to_dst(fake_img, frame), cv2.COLOR_BGR2RGB)\n            else:\n                fake_img = cv2.cvtColor(fake_img, cv2.COLOR_BGR2RGB)\n            fake_img = cv2.resize(fake_img, (width, height))\n            out.write(fake_img)\n            pbar.update(1)\n\n        pbar.close()\n        vid.release()\n        # cv2.destroyAllWindows()\n        return os.path.join(output, vid_name)\n\n\nif __name__ == '__main__':\n    arg = parse_args()\n    check_folder(arg.output)\n    info = cvt2anime_video(arg.video, arg.output, arg.checkpoint_dir, output_format=arg.output_format, if_adjust_brightness=arg.if_adjust_brightness)\n    print(f'output video: {info}')\n"
        }
      ]
    }
  ]
}