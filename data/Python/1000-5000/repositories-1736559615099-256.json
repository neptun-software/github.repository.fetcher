{
  "metadata": {
    "timestamp": 1736559615099,
    "page": 256,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "KaiyangZhou/deep-person-reid",
      "stars": 4376,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.484375,
          "content": "[flake8]\nignore =\n    # At least two spaces before inline comment\n    E261,\n    # Line lengths are recommended to be no greater than 79 characters\n    E501,\n    # Missing whitespace around arithmetic operator \n    E226,\n    # Blank line contains whitespace\n    W293,\n    # Do not use bare 'except'\n    E722,\n    # Line break after binary operator\n    W504,\n    # isort found an import in the wrong position\n    I001\nmax-line-length = 79\nexclude = __init__.py, build, torchreid/metrics/rank_cylib/"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.7919921875,
          "content": ".idea/\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# Cython eval code\n*.c\n*.html\n\n# OS X\n.DS_Store\n.Spotlight-V100\n.Trashes\n._*\n\n# ReID\nreid-data/\nlog/\nsaved-models/\nmodel-zoo/\ndebug*\n"
        },
        {
          "name": ".isort.cfg",
          "type": "blob",
          "size": 0.2998046875,
          "content": "[isort]\nline_length=79\nmulti_line_output=3\nlength_sort=true\nknown_standard_library=numpy,setuptools\nknown_myself=torchreid\nknown_third_party=matplotlib,cv2,torch,torchvision,PIL,yacs\nno_lines_before=STDLIB,THIRDPARTY\nsections=FUTURE,STDLIB,THIRDPARTY,myself,FIRSTPARTY,LOCALFOLDER\ndefault_section=FIRSTPARTY"
        },
        {
          "name": ".style.yapf",
          "type": "blob",
          "size": 0.216796875,
          "content": "[style]\nBASED_ON_STYLE = pep8\nBLANK_LINE_BEFORE_NESTED_CLASS_OR_DEF = true\nSPLIT_BEFORE_EXPRESSION_AFTER_OPENING_PAREN = true\nDEDENT_CLOSING_BRACKETS = true\nSPACES_BEFORE_COMMENT = 1\nARITHMETIC_PRECEDENCE_INDICATION = true"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.787109375,
          "content": "FROM nvidia/cuda:11.1.1-cudnn8-devel-ubuntu18.04\n\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update && apt-get install -y \\\n\tpython3-opencv ca-certificates python3-dev git wget sudo ninja-build\nRUN ln -sv /usr/bin/python3 /usr/bin/python\n\nCOPY . /home/appuser\nWORKDIR /home/appuser\n\n# https://github.com/facebookresearch/detectron2/issues/3933\nENV PATH=\"/home/appuser/.local/bin:${PATH}\"\nRUN wget https://bootstrap.pypa.io/pip/3.6/get-pip.py && \\\n\tpython3 get-pip.py && \\\n\trm get-pip.py\n\n# install dependencies\n# See https://pytorch.org/ for other options if you use a different version of CUDA\nRUN pip install -r requirements.txt\nRUN pip install torch==1.10 torchvision==0.11.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html\nRUN pip install setuptools==59.5.0\nRUN python setup.py develop\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2018 Kaiyang Zhou\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.185546875,
          "content": "clean:\n\tdocker rm -f $$(docker ps -qa)\n\nbuild-image:\n\tdocker build -t=deeppreid:v0 .\n\nrun:\n\tnvidia-docker run -v ${PWD}:/home/appuser --name=deeppreid --net=host --ipc=host -it deeppreid:v0\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 16.8779296875,
          "content": "Torchreid\n===========\nTorchreid is a library for deep-learning person re-identification, written in `PyTorch <https://pytorch.org/>`_ and developed for our ICCV'19 project, `Omni-Scale Feature Learning for Person Re-Identification <https://arxiv.org/abs/1905.00953>`_.\n\nIt features:\n\n- multi-GPU training\n- support both image- and video-reid\n- end-to-end training and evaluation\n- incredibly easy preparation of reid datasets\n- multi-dataset training\n- cross-dataset evaluation\n- standard protocol used by most research papers\n- highly extensible (easy to add models, datasets, training methods, etc.)\n- implementations of state-of-the-art deep reid models\n- access to pretrained reid models\n- advanced training techniques\n- visualization tools (tensorboard, ranks, etc.)\n\n\nCode: https://github.com/KaiyangZhou/deep-person-reid.\n\nDocumentation: https://kaiyangzhou.github.io/deep-person-reid/.\n\nHow-to instructions: https://kaiyangzhou.github.io/deep-person-reid/user_guide.\n\nModel zoo: https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO.\n\nTech report: https://arxiv.org/abs/1910.10093.\n\nYou can find some research projects that are built on top of Torchreid `here <https://github.com/KaiyangZhou/deep-person-reid/tree/master/projects>`_.\n\n\nWhat's new\n------------\n- [Aug 2022] We have added model export capabilities to the following frameworks: ONNX, OpenVINO and TFLite. The export script can be found `here <https://github.com/KaiyangZhou/deep-person-reid/blob/master/tools/export.py>`_\n- [Aug 2021] We have released the ImageNet-pretrained models of ``osnet_ain_x0_75``, ``osnet_ain_x0_5`` and ``osnet_ain_x0_25``. The pretraining setup follows `pycls <https://github.com/facebookresearch/pycls/blob/master/configs/archive/imagenet/resnet/R-50-1x64d_step_8gpu.yaml>`_.\n- [Apr 2021] We have updated the appendix in the `TPAMI version of OSNet <https://arxiv.org/abs/1910.06827v5>`_ to include results in the multi-source domain generalization setting. The trained models can be found in the `Model Zoo <https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO.html>`_.\n- [Apr 2021] We have added a script to automate the process of calculating average results over multiple splits. For more details please see ``tools/parse_test_res.py``.\n- [Apr 2021] ``v1.4.0``: We added the person search dataset, `CUHK-SYSU <http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html>`_.  Please see the `documentation <https://kaiyangzhou.github.io/deep-person-reid/>`_ regarding how to download the dataset (it contains cropped person images).\n- [Apr 2021] All models in the model zoo have been moved to google drive. Please raise an issue if any model's performance is inconsistent with the numbers shown in the model zoo page (could be caused by wrong links).\n- [Mar 2021] `OSNet <https://arxiv.org/abs/1910.06827>`_ will appear in the TPAMI journal! Compared with the conference version, which focuses on discriminative feature learning using the omni-scale building block, this journal extension further considers generalizable feature learning by integrating `instance normalization layers <https://arxiv.org/abs/1607.08022>`_ with the OSNet architecture. We hope this journal paper can motivate more future work to taclke the generalization issue in cross-dataset re-ID.\n- [Mar 2021] Generalization across domains (datasets) in person re-ID is crucial in real-world applications, which is closely related to the topic of *domain generalization*. Interested in learning how the field of domain generalization has developed over the last decade? Check our recent survey in this topic at https://arxiv.org/abs/2103.02503, with coverage on the history, datasets, related problems, methodologies, potential directions, and so on (*methods designed for generalizable re-ID are also covered*!).\n- [Feb 2021] ``v1.3.6`` Added `University-1652 <https://dl.acm.org/doi/abs/10.1145/3394171.3413896>`_, a new dataset for multi-view multi-source geo-localization (credit to `Zhedong Zheng <https://github.com/layumi>`_).\n- [Feb 2021] ``v1.3.5``: Now the `cython code <https://github.com/KaiyangZhou/deep-person-reid/pull/412>`_ works on Windows (credit to `lablabla <https://github.com/lablabla>`_).\n- [Jan 2021] Our recent work, `MixStyle <https://openreview.net/forum?id=6xHJ37MVxxp>`_ (mixing instance-level feature statistics of samples of different domains for improving domain generalization), has been accepted to ICLR'21. The code has been released at https://github.com/KaiyangZhou/mixstyle-release where the person re-ID part is based on Torchreid.\n- [Jan 2021] A new evaluation metric called `mean Inverse Negative Penalty (mINP)` for person re-ID has been introduced in `Deep Learning for Person Re-identification: A Survey and Outlook (TPAMI 2021) <https://arxiv.org/abs/2001.04193>`_. Their code can be accessed at `<https://github.com/mangye16/ReID-Survey>`_.\n- [Aug 2020] ``v1.3.3``: Fixed bug in ``visrank`` (caused by not unpacking ``dsetid``).\n- [Aug 2020] ``v1.3.2``: Added ``_junk_pids`` to ``grid`` and ``prid``. This avoids using mislabeled gallery images for training when setting ``combineall=True``.\n- [Aug 2020] ``v1.3.0``: (1) Added ``dsetid`` to the existing 3-tuple data source, resulting in ``(impath, pid, camid, dsetid)``. This variable denotes the dataset ID and is useful when combining multiple datasets for training (as a dataset indicator). E.g., when combining ``market1501`` and ``cuhk03``, the former will be assigned ``dsetid=0`` while the latter will be assigned ``dsetid=1``. (2) Added ``RandomDatasetSampler``. Analogous to ``RandomDomainSampler``, ``RandomDatasetSampler`` samples a certain number of images (``batch_size // num_datasets``) from each of specified datasets (the amount is determined by ``num_datasets``).\n- [Aug 2020] ``v1.2.6``: Added ``RandomDomainSampler`` (it samples ``num_cams`` cameras each with ``batch_size // num_cams`` images to form a mini-batch).\n- [Jun 2020] ``v1.2.5``: (1) Dataloader's output from ``__getitem__`` has been changed from ``list`` to ``dict``. Previously, an element, e.g. image tensor, was fetched with ``imgs=data[0]``. Now it should be obtained by ``imgs=data['img']``. See this `commit <https://github.com/KaiyangZhou/deep-person-reid/commit/aefe335d68f39a20160860e6d14c2d34f539b8a5>`_ for detailed changes. (2) Added ``k_tfm`` as an option to image data loader, which allows data augmentation to be applied ``k_tfm`` times *independently* to an image. If ``k_tfm > 1``, ``imgs=data['img']`` returns a list with ``k_tfm`` image tensors.\n- [May 2020] Added the person attribute recognition code used in `Omni-Scale Feature Learning for Person Re-Identification (ICCV'19) <https://arxiv.org/abs/1905.00953>`_. See ``projects/attribute_recognition/``.\n- [May 2020] ``v1.2.1``: Added a simple API for feature extraction (``torchreid/utils/feature_extractor.py``). See the `documentation <https://kaiyangzhou.github.io/deep-person-reid/user_guide.html>`_ for the instruction.\n- [Apr 2020] Code for reproducing the experiments of `deep mutual learning <https://zpascal.net/cvpr2018/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf>`_ in the `OSNet paper <https://arxiv.org/pdf/1905.00953v6.pdf>`__ (Supp. B) has been released at ``projects/DML``.\n- [Apr 2020] Upgraded to ``v1.2.0``. The engine class has been made more model-agnostic to improve extensibility. See `Engine <torchreid/engine/engine.py>`_ and `ImageSoftmaxEngine <torchreid/engine/image/softmax.py>`_ for more details. Credit to `Dassl.pytorch <https://github.com/KaiyangZhou/Dassl.pytorch>`_.\n- [Dec 2019] Our `OSNet paper <https://arxiv.org/pdf/1905.00953v6.pdf>`_ has been updated, with additional experiments (in section B of the supplementary) showing some useful techniques for improving OSNet's performance in practice.\n- [Nov 2019] ``ImageDataManager`` can load training data from target datasets by setting ``load_train_targets=True``, and the train-loader can be accessed with ``train_loader_t = datamanager.train_loader_t``. This feature is useful for domain adaptation research.\n\n\nInstallation\n---------------\n\nMake sure `conda <https://www.anaconda.com/distribution/>`_ is installed.\n\n\n.. code-block:: bash\n\n    # cd to your preferred directory and clone this repo\n    git clone https://github.com/KaiyangZhou/deep-person-reid.git\n\n    # create environment\n    cd deep-person-reid/\n    conda create --name torchreid python=3.7\n    conda activate torchreid\n\n    # install dependencies\n    # make sure `which python` and `which pip` point to the correct path\n    pip install -r requirements.txt\n\n    # install torch and torchvision (select the proper cuda version to suit your machine)\n    conda install pytorch torchvision cudatoolkit=9.0 -c pytorch\n\n    # install torchreid (don't need to re-build it if you modify the source code)\n    python setup.py develop\n\nAnother way to install is to run everything inside docker container:\n\n- build: ``make build-image``\n- run: ``make run``\n\nGet started: 30 seconds to Torchreid\n-------------------------------------\n1. Import ``torchreid``\n\n.. code-block:: python\n    \n    import torchreid\n\n2. Load data manager\n\n.. code-block:: python\n    \n    datamanager = torchreid.data.ImageDataManager(\n        root=\"reid-data\",\n        sources=\"market1501\",\n        targets=\"market1501\",\n        height=256,\n        width=128,\n        batch_size_train=32,\n        batch_size_test=100,\n        transforms=[\"random_flip\", \"random_crop\"]\n    )\n\n3 Build model, optimizer and lr_scheduler\n\n.. code-block:: python\n    \n    model = torchreid.models.build_model(\n        name=\"resnet50\",\n        num_classes=datamanager.num_train_pids,\n        loss=\"softmax\",\n        pretrained=True\n    )\n\n    model = model.cuda()\n\n    optimizer = torchreid.optim.build_optimizer(\n        model,\n        optim=\"adam\",\n        lr=0.0003\n    )\n\n    scheduler = torchreid.optim.build_lr_scheduler(\n        optimizer,\n        lr_scheduler=\"single_step\",\n        stepsize=20\n    )\n\n4. Build engine\n\n.. code-block:: python\n    \n    engine = torchreid.engine.ImageSoftmaxEngine(\n        datamanager,\n        model,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        label_smooth=True\n    )\n\n5. Run training and test\n\n.. code-block:: python\n    \n    engine.run(\n        save_dir=\"log/resnet50\",\n        max_epoch=60,\n        eval_freq=10,\n        print_freq=10,\n        test_only=False\n    )\n\n\nA unified interface\n-----------------------\nIn \"deep-person-reid/scripts/\", we provide a unified interface to train and test a model. See \"scripts/main.py\" and \"scripts/default_config.py\" for more details. The folder \"configs/\" contains some predefined configs which you can use as a starting point.\n\nBelow we provide an example to train and test `OSNet (Zhou et al. ICCV'19) <https://arxiv.org/abs/1905.00953>`_. Assume :code:`PATH_TO_DATA` is the directory containing reid datasets. The environmental variable :code:`CUDA_VISIBLE_DEVICES` is omitted, which you need to specify if you have a pool of gpus and want to use a specific set of them.\n\nConventional setting\n^^^^^^^^^^^^^^^^^^^^^\n\nTo train OSNet on Market1501, do\n\n.. code-block:: bash\n\n    python scripts/main.py \\\n    --config-file configs/im_osnet_x1_0_softmax_256x128_amsgrad_cosine.yaml \\\n    --transforms random_flip random_erase \\\n    --root $PATH_TO_DATA\n\n\nThe config file sets Market1501 as the default dataset. If you wanna use DukeMTMC-reID, do\n\n.. code-block:: bash\n\n    python scripts/main.py \\\n    --config-file configs/im_osnet_x1_0_softmax_256x128_amsgrad_cosine.yaml \\\n    -s dukemtmcreid \\\n    -t dukemtmcreid \\\n    --transforms random_flip random_erase \\\n    --root $PATH_TO_DATA \\\n    data.save_dir log/osnet_x1_0_dukemtmcreid_softmax_cosinelr\n\nThe code will automatically (download and) load the ImageNet pretrained weights. After the training is done, the model will be saved as \"log/osnet_x1_0_market1501_softmax_cosinelr/model.pth.tar-250\". Under the same folder, you can find the `tensorboard <https://pytorch.org/docs/stable/tensorboard.html>`_ file. To visualize the learning curves using tensorboard, you can run :code:`tensorboard --logdir=log/osnet_x1_0_market1501_softmax_cosinelr` in the terminal and visit :code:`http://localhost:6006/` in your web browser.\n\nEvaluation is automatically performed at the end of training. To run the test again using the trained model, do\n\n.. code-block:: bash\n\n    python scripts/main.py \\\n    --config-file configs/im_osnet_x1_0_softmax_256x128_amsgrad_cosine.yaml \\\n    --root $PATH_TO_DATA \\\n    model.load_weights log/osnet_x1_0_market1501_softmax_cosinelr/model.pth.tar-250 \\\n    test.evaluate True\n\n\nCross-domain setting\n^^^^^^^^^^^^^^^^^^^^^\n\nSuppose you wanna train OSNet on DukeMTMC-reID and test its performance on Market1501, you can do\n\n.. code-block:: bash\n\n    python scripts/main.py \\\n    --config-file configs/im_osnet_x1_0_softmax_256x128_amsgrad.yaml \\\n    -s dukemtmcreid \\\n    -t market1501 \\\n    --transforms random_flip color_jitter \\\n    --root $PATH_TO_DATA\n\nHere we only test the cross-domain performance. However, if you also want to test the performance on the source dataset, i.e. DukeMTMC-reID, you can set :code:`-t dukemtmcreid market1501`, which will evaluate the model on the two datasets separately.\n\nDifferent from the same-domain setting, here we replace :code:`random_erase` with :code:`color_jitter`. This can improve the generalization performance on the unseen target dataset.\n\nPretrained models are available in the `Model Zoo <https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO.html>`_.\n\n\nDatasets\n--------\n\nImage-reid datasets\n^^^^^^^^^^^^^^^^^^^^^\n- `Market1501 <https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf>`_\n- `CUHK03 <https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Li_DeepReID_Deep_Filter_2014_CVPR_paper.pdf>`_\n- `DukeMTMC-reID <https://arxiv.org/abs/1701.07717>`_\n- `MSMT17 <https://arxiv.org/abs/1711.08565>`_\n- `VIPeR <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.7285&rep=rep1&type=pdf>`_\n- `GRID <http://www.eecs.qmul.ac.uk/~txiang/publications/LoyXiangGong_cvpr_2009.pdf>`_\n- `CUHK01 <http://www.ee.cuhk.edu.hk/~xgwang/papers/liZWaccv12.pdf>`_\n- `SenseReID <http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf>`_\n- `QMUL-iLIDS <http://www.eecs.qmul.ac.uk/~sgg/papers/ZhengGongXiang_BMVC09.pdf>`_\n- `PRID <https://pdfs.semanticscholar.org/4c1b/f0592be3e535faf256c95e27982db9b3d3d3.pdf>`_\n\nGeo-localization datasets\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n- `University-1652 <https://dl.acm.org/doi/abs/10.1145/3394171.3413896>`_\n\nVideo-reid datasets\n^^^^^^^^^^^^^^^^^^^^^^^\n- `MARS <http://www.liangzheng.org/1320.pdf>`_\n- `iLIDS-VID <https://www.eecs.qmul.ac.uk/~sgg/papers/WangEtAl_ECCV14.pdf>`_\n- `PRID2011 <https://pdfs.semanticscholar.org/4c1b/f0592be3e535faf256c95e27982db9b3d3d3.pdf>`_\n- `DukeMTMC-VideoReID <http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf>`_\n\n\nModels\n-------\n\nImageNet classification models\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n- `ResNet <https://arxiv.org/abs/1512.03385>`_\n- `ResNeXt <https://arxiv.org/abs/1611.05431>`_\n- `SENet <https://arxiv.org/abs/1709.01507>`_\n- `DenseNet <https://arxiv.org/abs/1608.06993>`_\n- `Inception-ResNet-V2 <https://arxiv.org/abs/1602.07261>`_\n- `Inception-V4 <https://arxiv.org/abs/1602.07261>`_\n- `Xception <https://arxiv.org/abs/1610.02357>`_\n- `IBN-Net <https://arxiv.org/abs/1807.09441>`_\n\nLightweight models\n^^^^^^^^^^^^^^^^^^^\n- `NASNet <https://arxiv.org/abs/1707.07012>`_\n- `MobileNetV2 <https://arxiv.org/abs/1801.04381>`_\n- `ShuffleNet <https://arxiv.org/abs/1707.01083>`_\n- `ShuffleNetV2 <https://arxiv.org/abs/1807.11164>`_\n- `SqueezeNet <https://arxiv.org/abs/1602.07360>`_\n\nReID-specific models\n^^^^^^^^^^^^^^^^^^^^^^\n- `MuDeep <https://arxiv.org/abs/1709.05165>`_\n- `ResNet-mid <https://arxiv.org/abs/1711.08106>`_\n- `HACNN <https://arxiv.org/abs/1802.08122>`_\n- `PCB <https://arxiv.org/abs/1711.09349>`_\n- `MLFN <https://arxiv.org/abs/1803.09132>`_\n- `OSNet <https://arxiv.org/abs/1905.00953>`_\n- `OSNet-AIN <https://arxiv.org/abs/1910.06827>`_\n\n\nUseful links\n-------------\n- `OSNet-IBN1-Lite (test-only code with lite docker container) <https://github.com/RodMech/OSNet-IBN1-Lite>`_\n- `Deep Learning for Person Re-identification: A Survey and Outlook <https://github.com/mangye16/ReID-Survey>`_\n\n\nCitation\n---------\nIf you use this code or the models in your research, please give credit to the following papers:\n\n.. code-block:: bash\n\n    @article{torchreid,\n      title={Torchreid: A Library for Deep Learning Person Re-Identification in Pytorch},\n      author={Zhou, Kaiyang and Xiang, Tao},\n      journal={arXiv preprint arXiv:1910.10093},\n      year={2019}\n    }\n    \n    @inproceedings{zhou2019osnet,\n      title={Omni-Scale Feature Learning for Person Re-Identification},\n      author={Zhou, Kaiyang and Yang, Yongxin and Cavallaro, Andrea and Xiang, Tao},\n      booktitle={ICCV},\n      year={2019}\n    }\n\n    @article{zhou2021osnet,\n      title={Learning Generalisable Omni-Scale Representations for Person Re-Identification},\n      author={Zhou, Kaiyang and Yang, Yongxin and Cavallaro, Andrea and Xiang, Tao},\n      journal={TPAMI},\n      year={2021}\n    }\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "linter.sh",
          "type": "blob",
          "size": 0.146484375,
          "content": "echo \"Running isort\"\nisort -y -sp .\necho \"Done\"\n\necho \"Running yapf\"\nyapf -i -r -vv -e build .\necho \"Done\"\n\necho \"Running flake8\"\nflake8 .\necho \"Done\""
        },
        {
          "name": "projects",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.26953125,
          "content": "numpy\nCython\nh5py\nPillow\nsix\nscipy\nopencv-python\nmatplotlib\ntb-nightly\nfuture\nyacs\ngdown\nflake8\nyapf\nisort==4.3.21\nimageio\nchardet\n# Export --------------------------------------\n# onnx\n# onnx-simplified\n# openvino-dev\n# openvino2tensorflow\n# tensorflow\n# tensorflow_datasets\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.46875,
          "content": "import numpy as np\nimport os.path as osp\nfrom setuptools import setup, find_packages\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\n\n\ndef readme():\n    with open('README.rst') as f:\n        content = f.read()\n    return content\n\n\ndef find_version():\n    version_file = 'torchreid/__init__.py'\n    with open(version_file, 'r') as f:\n        exec(compile(f.read(), version_file, 'exec'))\n    return locals()['__version__']\n\n\ndef numpy_include():\n    try:\n        numpy_include = np.get_include()\n    except AttributeError:\n        numpy_include = np.get_numpy_include()\n    return numpy_include\n\n\next_modules = [\n    Extension(\n        'torchreid.metrics.rank_cylib.rank_cy',\n        ['torchreid/metrics/rank_cylib/rank_cy.pyx'],\n        include_dirs=[numpy_include()],\n    )\n]\n\n\ndef get_requirements(filename='requirements.txt'):\n    here = osp.dirname(osp.realpath(__file__))\n    with open(osp.join(here, filename), 'r') as f:\n        requires = [line.replace('\\n', '') for line in f.readlines()]\n    return requires\n\n\nsetup(\n    name='torchreid',\n    version=find_version(),\n    description='A library for deep learning person re-ID in PyTorch',\n    author='Kaiyang Zhou',\n    license='MIT',\n    long_description=readme(),\n    url='https://github.com/KaiyangZhou/deep-person-reid',\n    packages=find_packages(),\n    install_requires=get_requirements(),\n    keywords=['Person Re-Identification', 'Deep Learning', 'Computer Vision'],\n    ext_modules=cythonize(ext_modules)\n)\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "torchreid",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}