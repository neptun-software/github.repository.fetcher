{
  "metadata": {
    "timestamp": 1736559695452,
    "page": 376,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "mosaicml/llm-foundry",
      "stars": 4101,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.1240234375,
          "content": "# Data (created in readme examples)\nmy-copy-c4*/\nmy-copy-arxiv*/\n*.jsonl*\n!tests/eval/local_data/*.jsonl\n\n# WandB\nwandb/\n\n# Makefile envs\nenv*/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# python venv installed in the dir, llmfoundry-venv\n*-venv\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# VSCode settings\n.vscode\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# macOS\n.DS_Store\n\n# notebooks\nnotebooks/\n\n# artifacts from training\n**/*.pt\n**/mlruns/*\n**/tokenizer-save-dir-*/**\n**/.downloaded_finetuning/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.56640625,
          "content": "default_language_version:\n  python: python3\nrepos:\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  # Ruff version.\n  rev: v0.2.2\n  hooks:\n  - id: ruff\n    args: [--fix, --exit-non-zero-on-fix]\n- repo: https://github.com/google/yapf\n  rev: v0.32.0\n  hooks:\n  - id: yapf\n    name: yapf\n    exclude: tests/horrible_strings.py\n    description: A formatter for Python files.\n    entry: yapf\n    args: [-i, -vv, -p]       # inplace\n    language: python\n    types: [python]\n    additional_dependencies:\n    - toml\n- repo: https://github.com/hadialqattan/pycln\n  rev: v2.5.0\n  hooks:\n  - id: pycln\n    args: [. --all]\n- repo: https://github.com/pycqa/isort\n  hooks:\n  - id: isort\n  rev: 5.12.0\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v4.3.0\n  hooks:\n  - id: check-added-large-files\n  - id: check-ast\n  - id: check-builtin-literals\n    args:\n    - --no-allow-dict-kwargs\n  - id: check-case-conflict\n  - id: check-docstring-first\n  - id: check-executables-have-shebangs\n  - id: check-json\n  - id: check-shebang-scripts-are-executable\n  - id: pretty-format-json\n    args:\n    - --autofix\n    - --no-sort-keys\n    - --indent=4\n  - id: check-merge-conflict\n  - id: check-symlinks\n  - id: check-toml\n  - id: check-vcs-permalinks\n  - id: check-xml\n  - id: check-yaml\n  - id: debug-statements\n  - id: destroyed-symlinks\n  - id: double-quote-string-fixer\n    exclude: tests/horrible_strings.py\n  - id: end-of-file-fixer\n  - id: fix-byte-order-marker\n  - id: mixed-line-ending\n  - id: trailing-whitespace\n- repo: https://github.com/Lucas-C/pre-commit-hooks\n  rev: v1.5.4\n  hooks:\n  - id: insert-license\n    args:\n    - --license-filepath\n    - .pre-commit/FILE_HEADER\n    - --comment-style\n    - \"#\"\n    - --allow-past-years\n    types: [python]\n- repo: https://github.com/PyCQA/docformatter\n  rev: v1.5.0\n  hooks:\n  - id: docformatter\n    args: [--in-place, --wrap-summaries=80, --wrap-descriptions=80]\n- repo: https://github.com/adrienverge/yamllint.git\n  rev: v1.28.0\n  hooks:\n  - id: yamllint\n    name: yamllint\n    description: This hook runs yamllint.\n    entry: yamllint\n    language: python\n    types: [file, yaml]\n- repo: local\n  hooks:\n  - id: pyright\n    name: pyright\n    exclude: tests/horrible_strings.py\n    entry: pyright\n    language: node\n    types: [python]\n    pass_filenames: false\n    args: [--warnings]\n    additional_dependencies: [\"pyright@1.1.256\"]\n- repo: https://github.com/trufflesecurity/trufflehog.git\n  rev: v3.40.0\n  hooks:\n  - id: trufflehog\n    name: secret scan\n    exclude: tests/horrible_strings.py\n    entry: trufflehog filesystem ./\n    args:\n    - --only-verified\n    - --fail\n"
        },
        {
          "name": ".pre-commit",
          "type": "tree",
          "content": null
        },
        {
          "name": ".yamllint.yaml",
          "type": "blob",
          "size": 0.76953125,
          "content": "yaml-files:\n- \"*.yaml\"\n- \"*.yml\"\n- .yamllint\n\nignore: |\n  wandb\n\nrules:\n  braces:\n    forbid: non-empty\n  brackets:\n    forbid: false\n  colons: enable\n  commas: enable\n  comments: enable\n  comments-indentation: enable\n  document-end:\n    present: false\n  document-start:\n    present: false\n  empty-lines: enable\n  empty-values: disable\n  hyphens: enable\n  indentation:\n    spaces: 2\n    indent-sequences: false\n    check-multi-line-strings: false\n  key-duplicates: enable\n  key-ordering: disable\n  line-length:\n    max: 120\n    allow-non-breakable-words: true\n    allow-non-breakable-inline-mappings: true\n  new-line-at-end-of-file: enable\n  new-lines: enable\n  octal-values: enable\n  quoted-strings:\n    quote-type: double\n    required: false\n  trailing-spaces: enable\n  truthy: disable\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.8212890625,
          "content": "# Copyright 2022 MosaicML LLM Foundry authors\n# SPDX-License-Identifier: Apache-2.0\n\nARG BASE_IMAGE\nFROM $BASE_IMAGE\n\nARG BRANCH_NAME\nARG DEP_GROUPS\nARG KEEP_FOUNDRY=false\n\nENV TORCH_CUDA_ARCH_LIST=\"8.0 8.6 8.7 8.9 9.0\"\n\n# Check for changes in setup.py.\n# If there are changes, the docker cache is invalidated and a fresh pip installation is triggered.\nADD https://raw.githubusercontent.com/mosaicml/llm-foundry/$BRANCH_NAME/setup.py setup.py\nRUN rm setup.py\n\n# Install and uninstall foundry to cache foundry requirements\nRUN git clone -b $BRANCH_NAME https://github.com/mosaicml/llm-foundry.git\nRUN pip install --no-cache-dir \"./llm-foundry${DEP_GROUPS}\"\n\n# Conditionally uninstall llm-foundry and remove its directory\nRUN if [ \"$KEEP_FOUNDRY\" != \"true\" ]; then \\\n      pip uninstall -y llm-foundry && \\\n      rm -rf /llm-foundry; \\\n    fi\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0888671875,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2022 MosaicML Examples authors\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.9150390625,
          "content": "# several pytest settings\nWORLD_SIZE ?= 1  # world size for launcher tests\nMASTER_PORT ?= 26000 # port for distributed tests\nPYTHON ?= python3  # Python command\nPYTEST ?= pytest  # Pytest command\nPYRIGHT ?= pyright  # Pyright command. Pyright must be installed seperately -- e.g. `node install -g pyright`\nEXTRA_ARGS ?=  # extra arguments for pytest\nEXTRA_LAUNCHER_ARGS ?= # extra arguments for the composer cli launcher\n\ntest:\n\tLOCAL_WORLD_SIZE=1 $(PYTHON) -m $(PYTEST) $(EXTRA_ARGS)\n\ntest-gpu:\n\tLOCAL_WORLD_SIZE=1 $(PYTHON) -m $(PYTEST) -m gpu $(EXTRA_ARGS)\n\n# runs tests with the launcher\ntest-dist:\n\t$(PYTHON) -m composer.cli.launcher -n $(WORLD_SIZE) --master_port $(MASTER_PORT) $(EXTRA_LAUNCHER_ARGS) -m $(PYTEST) $(EXTRA_ARGS)\n\ntest-dist-gpu:\n\t$(PYTHON) -m composer.cli.launcher -n $(WORLD_SIZE) --master_port $(MASTER_PORT) $(EXTRA_LAUNCHER_ARGS) -m $(PYTEST) -m gpu $(EXTRA_ARGS)\n\n.PHONY: test test-gpu test-dist test-dist-gpu\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.4599609375,
          "content": "<!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_BEGIN -->\n<p align=\"center\">\n  <a href=\"https://github.com/mosaicml/llm-foundry\">\n    <picture>\n      <img alt=\"LLM Foundry\" src=\"./assets/llm-foundry.png\" width=\"95%\">\n    </picture>\n  </a>\n</p>\n<!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_END -->\n\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/llm-foundry/\">\n        <img alt=\"PyPi Version\" src=\"https://img.shields.io/pypi/pyversions/llm-foundry\">\n    </a>\n    <a href=\"https://pypi.org/project/llm-foundry/\">\n        <img alt=\"PyPi Package Version\" src=\"https://img.shields.io/pypi/v/llm-foundry\">\n    </a>\n    <a href=\"https://mosaicml.me/slack\">\n        <img alt=\"Chat @ Slack\" src=\"https://img.shields.io/badge/slack-chat-2eb67d.svg?logo=slack\">\n    </a>\n    <a href=\"https://github.com/mosaicml/llm-foundry/blob/main/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache%202.0-green.svg\">\n    </a>\n</p>\n<br />\n\n# LLM Foundry\n\nThis repository contains code for training, finetuning, evaluating, and deploying LLMs for inference with [Composer](https://github.com/mosaicml/composer) and the [MosaicML platform](https://forms.mosaicml.com/demo?utm_source=github.com&utm_medium=referral&utm_campaign=llm-foundry). Designed to be easy-to-use, efficient _and_ flexible, this codebase enables rapid experimentation with the latest techniques.\n\nYou'll find in this repo:\n* `llmfoundry/` - source code for models, datasets, callbacks, utilities, etc.\n* `scripts/` - scripts to run LLM workloads\n  * `data_prep/` - convert text data from original sources to StreamingDataset format\n  * `train/` - train or finetune HuggingFace and MPT models from 125M - 70B parameters\n    * `train/benchmarking` - profile training throughput and MFU\n  * `inference/` - convert models to HuggingFace or ONNX format, and generate responses\n    * `inference/benchmarking` - profile inference latency and throughput\n  * `eval/` - evaluate LLMs on academic (or custom) in-context-learning tasks\n* `mcli/` - launch any of these workloads using [MCLI](https://docs.mosaicml.com/projects/mcli/en/latest/) and the [MosaicML platform](https://www.mosaicml.com/platform)\n* `TUTORIAL.md` - a deeper dive into the repo, example workflows, and FAQs\n\n# DBRX\n\nDBRX is a state-of-the-art open source LLM trained by Databricks Mosaic team. It uses the Mixture-of-Experts (MoE) architecture and was trained with optimized versions of [Composer](https://github.com/mosaicml/composer), LLM Foundry, and [MegaBlocks](https://github.com/databricks/megablocks). The model has 132B total parameters and 36B active parameters. We have released two DBRX models:\n\n\n| Model              | Context Length | Download                                           |\n| ------------------ | -------------- | -------------------------------------------------- |\n| DBRX Base          | 32768          | https://huggingface.co/databricks/dbrx-base        |\n| DBRX Instruct      | 32768          | https://huggingface.co/databricks/dbrx-instruct    |\n\nOur model weights and code are licensed for both researchers and commercial entities. The Databricks Open Source License can be found at [LICENSE](https://github.com/databricks/dbrx/blob/main/LICENSE), and our Acceptable Use Policy can be found [here](https://www.databricks.com/legal/acceptable-use-policy-open-model).\n\nFor more information about the DBRX models, see https://github.com/databricks/dbrx.\n\n# MPT\n\nMosaic Pretrained Transformers (MPT) are GPT-style models with some special features -- Flash Attention for efficiency, ALiBi for context length extrapolation, and stability improvements to mitigate loss spikes. As part of MosaicML's Foundation series, we have open-sourced several MPT models:\n\n\n| Model              | Context Length | Download                                           | Commercial use? |\n| ------------------ | -------------- | -------------------------------------------------- | --------------- |\n| MPT-30B            | 8192           | https://huggingface.co/mosaicml/mpt-30b            | Yes             |\n| MPT-30B-Instruct   | 8192           | https://huggingface.co/mosaicml/mpt-30b-instruct   | Yes             |\n| MPT-30B-Chat       | 8192           | https://huggingface.co/mosaicml/mpt-30b-chat       | No              |\n| MPT-7b-8k          | 8192           | https://huggingface.co/mosaicml/mpt-7b-8k          | Yes             |\n| MPT-7b-8k-Chat | 8192           | https://huggingface.co/mosaicml/mpt-7b-8k-chat         | No              |\n| MPT-7B             | 2048           | https://huggingface.co/mosaicml/mpt-7b             | Yes             |\n| MPT-7B-Instruct    | 2048           | https://huggingface.co/mosaicml/mpt-7b-instruct    | Yes             |\n| MPT-7B-Chat        | 2048           | https://huggingface.co/mosaicml/mpt-7b-chat        | No              |\n| MPT-7B-StoryWriter | 65536          | https://huggingface.co/mosaicml/mpt-7b-storywriter | Yes             |\n\nTo try out these models locally, [follow the instructions](https://github.com/mosaicml/llm-foundry/tree/main/scripts/inference#interactive-generation-with-modelgenerate) in `scripts/inference/README.md` to prompt HF models using our [hf_generate.py](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_generate.py) or [hf_chat.py](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_chat.py) scripts.\n\n# MPT Community\n\nWe've been overwhelmed by all the amazing work the community has put into MPT! Here we provide a few links to some of them:\n* [ReplitLM](https://github.com/replit/replitLM): `replit-code-v1-3b` is a 2.7B Causal Language Model focused on Code Completion. The model has been trained on a subset of the Stack Dedup v1.2 dataset covering 20 languages such as Java, Python, and C++\n* [LLaVa-MPT](https://github.com/haotian-liu/LLaVA#LLaVA-MPT-7b): Visual instruction tuning to get MPT multimodal capabilities\n* [ggml](https://github.com/ggerganov/ggml/tree/master): Optimized MPT version for efficient inference on consumer hardware\n* [GPT4All](https://gpt4all.io/index.html): locally running chat system, now with MPT support!\n* [Q8MPT-Chat](https://huggingface.co/spaces/Intel/Q8-Chat): 8-bit optimized MPT for CPU by our friends at Intel\n\nTutorial videos from the community:\n* [Using MPT-7B with Langchain](https://www.youtube.com/watch?v=DXpk9K7DgMo&t=3s) by [@jamesbriggs](https://www.youtube.com/@jamesbriggs)\n* [MPT-7B StoryWriter Intro](https://www.youtube.com/watch?v=O9Y_ZdsuKWQ) by [AItrepreneur](https://www.youtube.com/@Aitrepreneur)\n* [Fine-tuning MPT-7B on a single GPU](https://www.youtube.com/watch?v=KSlWkrByc0o&t=9s) by [@AIology2022](https://www.youtube.com/@AIology2022)\n* [How to Fine-tune MPT-7B-Instruct on Google Colab](https://youtu.be/3de0Utr9XnI) by [@VRSEN](https://www.youtube.com/@vrsen)\n\nSomething missing? Contribute with a PR!\n\n# Latest News\n* [Blog: Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n* [Blog: LLM Training and Inference with Intel Gaudi2 AI Accelerators](https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators)\n* [Blog: Training LLMs at Scale with AMD MI250 GPUs](https://www.databricks.com/blog/training-llms-scale-amd-mi250-gpus)\n* [Blog: Training LLMs with AMD MI250 GPUs and MosaicML](https://www.mosaicml.com/blog/amd-mi250)\n* [Blog: Announcing MPT-7B-8K: 8K Context Length for Document Understanding](https://www.mosaicml.com/blog/long-context-mpt-7b-8k)\n* [Blog: Training LLMs with AMD MI250 GPUs and MosaicML](https://www.mosaicml.com/blog/amd-mi250)\n* [Blog: MPT-30B: Raising the bar for open-source foundation models](https://www.mosaicml.com/blog/mpt-30b)\n* [Blog: Introducing MPT-7B](https://www.mosaicml.com/blog/mpt-7b)\n* [Blog: Benchmarking LLMs on H100](https://www.mosaicml.com/blog/coreweave-nvidia-h100-part-1)\n* [Blog: Blazingly Fast LLM Evaluation](https://www.mosaicml.com/blog/llm-evaluation-for-icl)\n* [Blog: GPT3 Quality for $500k](https://www.mosaicml.com/blog/gpt-3-quality-for-500k)\n* [Blog: Billion parameter GPT training made easy](https://www.mosaicml.com/blog/billion-parameter-gpt-training-made-easy)\n\n\n\n# Hardware and Software Requirements\nThis codebase has been tested with PyTorch 2.4 with NVIDIA A100s and H100s.\nThis codebase may also work on systems with other devices, such as consumer NVIDIA cards and AMD cards, but we are not actively testing these systems.\nIf you have success/failure using LLM Foundry on other systems, please let us know in a Github issue and we will update the support matrix!\n\n| Device         | Torch Version | Cuda Version | Status                       |\n| -------------- | ------------- | ------------ | ---------------------------- |\n| A100-40GB/80GB | 2.5.1         | 12.4         | :white_check_mark: Supported |\n| H100-80GB      | 2.5.1         | 12.4         | :white_check_mark: Supported |\n\n## MosaicML Docker Images\nWe highly recommend using our prebuilt Docker images. You can find them here: https://hub.docker.com/orgs/mosaicml/repositories.\n\nThe `mosaicml/pytorch` images are pinned to specific PyTorch and CUDA versions, and are stable and rarely updated.\n\nThe `mosaicml/llm-foundry` images are built with new tags upon every commit to the `main` branch.\nYou can select a specific commit hash such as `mosaicml/llm-foundry:2.5.1_cu124-9867a7b` or take the latest one using `mosaicml/llm-foundry:2.5.1_cu124-latest`.\n\n**Please Note:** The `mosaicml/llm-foundry` images do not come with the `llm-foundry` package preinstalled, just the dependencies. You will still need to `pip install llm-foundry` either from PyPi or from source.\n\n| Docker Image                                           | Torch Version | Cuda Version      | LLM Foundry dependencies installed? |\n| ------------------------------------------------------ | ------------- | ----------------- | ----------------------------------- |\n| `mosaicml/pytorch:2.5.1_cu124-python3.11-ubuntu22.04`  | 2.5.1         | 12.4 (Infiniband) | No                                  |\n| `mosaicml/llm-foundry:2.5.1_cu124-latest`              | 2.5.1         | 12.4 (Infiniband) | Yes                                 |\n| `mosaicml/llm-foundry:2.5.1_cu124_aws-latest`          | 2.5.1         | 12.4 (EFA)        | Yes                                 |\n\n\n# Installation\n\nThis assumes you already have PyTorch, CMake, and packaging installed. If not, you can install them with `pip install cmake packaging torch`.\n\nTo get started, clone the repo and set up your environment. Instructions to do so differ slightly depending on whether you're using Docker.\n\n### With Docker (recommended)\n\nWe *strongly* recommend working with LLM Foundry inside a Docker container (see our recommended Docker image above). If you are doing so, follow these steps to clone the repo and install the requirements.\n\n<!--pytest.mark.skip-->\n```bash\ngit clone https://github.com/mosaicml/llm-foundry.git\ncd llm-foundry\npip install -e \".[gpu]\"  # or `pip install -e .` if no NVIDIA GPU.\n```\n\n### Without Docker (not recommended)\n\nIf you choose not to use Docker, you should create and use a virtual environment.\n\n<!--pytest.mark.skip-->\n```bash\ngit clone https://github.com/mosaicml/llm-foundry.git\ncd llm-foundry\n\n# Creating and activate a virtual environment\npython3 -m venv llmfoundry-venv\nsource llmfoundry-venv/bin/activate\n\npip install cmake packaging torch  # setup.py requires these be installed\n\npip install -e \".[gpu]\"  # or `pip install -e .` if no NVIDIA GPU.\n```\n\n### TransformerEngine and amp_fp8 support\nNVIDIA H100 GPUs have FP8 support; we have installed Flash Attention and Transformer in our Docker images already (see above). If you are not using our Docker images, you can install these packages with:\n<!--pytest.mark.skip-->\n```bash\npip install flash-attn --no-build-isolation\npip install git+https://github.com/NVIDIA/TransformerEngine.git@stable\n```\n\nSee [here](https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md#TransformerEngine-and-amp_fp8-support) for more details on enabling TransformerEngine layers and amp_fp8.\n\n### AMD (BETA support)\n\nIn [our testing of AMD GPUs](https://www.mosaicml.com/blog/amd-mi250), the env setup includes:\n\n<!--pytest.mark.skip-->\n```bash\ngit clone https://github.com/mosaicml/llm-foundry.git\ncd llm-foundry\n\n# Creating and activate a virtual environment\npython3 -m venv llmfoundry-venv-amd\nsource llmfoundry-venv-amd/bin/activate\n\n# installs\npip install cmake packaging torch\npip install -e .  # This installs some things that are not needed but they don't hurt\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2\n```\n**Lastly**, install the ROCm enabled flash attention (instructions [here](https://github.com/ROCmSoftwarePlatform/flash-attention/tree/flash_attention_for_rocm2#amd-gpurocm-support)).\n\nNotes:\n1. We don't yet have a Docker image where everything works perfectly. You might need to up/downgrade some packages (in our case, we needed to downgrade to `numpy==1.23.5`) before everything works without issue.\n\n### Intel Gaudi\nSupport for LLM Foundry on Intel Gaudi devices is experimental, please use the branch `habana_alpha` and see the [README on that branch](https://github.com/mosaicml/llm-foundry/blob/habana_alpha) which has [install instructions and known issues.](https://github.com/mosaicml/llm-foundry/tree/habana_alpha?tab=readme-ov-file#intel-gaudi)\n\nFor training and inference performance results on Intel Gaudi2 accelerators, see our blog: https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators\n\n\n# Quickstart\n\n> **Note**\n> Make sure to go through the installation steps above before trying the quickstart!\n\nHere is an end-to-end workflow for preparing a subset of the C4 dataset, training an MPT-125M model for 10 batches,\nconverting the model to HuggingFace format, evaluating the model on the Winograd challenge, and generating responses to prompts.\n\n**(Remember this is a quickstart just to demonstrate the tools -- To get good quality, the LLM must be trained for longer than 10 batches üòÑ)**\n\n<!--pytest.mark.skip-->\n```bash\ncd scripts\n\n# Convert C4 dataset to StreamingDataset format\npython data_prep/convert_dataset_hf.py \\\n  --dataset allenai/c4 --data_subset en \\\n  --out_root my-copy-c4 --splits train_small val_small \\\n  --concat_tokens 2048 --tokenizer EleutherAI/gpt-neox-20b --eos_text '<|endoftext|>'\n\n# Train an MPT-125m model for 10 batches\ncomposer train/train.py \\\n  train/yamls/pretrain/mpt-125m.yaml \\\n  variables.data_local=my-copy-c4 \\\n  train_loader.dataset.split=train_small \\\n  eval_loader.dataset.split=val_small \\\n  max_duration=10ba \\\n  eval_interval=0 \\\n  save_folder=mpt-125m\n\n# Convert the model to HuggingFace format\npython inference/convert_composer_to_hf.py \\\n  --composer_path mpt-125m/ep0-ba10-rank0.pt \\\n  --hf_output_path mpt-125m-hf \\\n  --output_precision bf16 \\\n  # --hf_repo_for_upload user-org/repo-name\n\n# Evaluate the model on a subset of tasks\ncomposer eval/eval.py \\\n  eval/yamls/hf_eval.yaml \\\n  icl_tasks=eval/yamls/copa.yaml \\\n  model_name_or_path=mpt-125m-hf\n\n# Generate responses to prompts\npython inference/hf_generate.py \\\n  --name_or_path mpt-125m-hf \\\n  --max_new_tokens 256 \\\n  --prompts \\\n    \"The answer to life, the universe, and happiness is\" \\\n    \"Here's a quick recipe for baking chocolate chip cookies: Start by\"\n```\n\nNote: the `composer` command used above to train the model refers to the [Composer](https://github.com/mosaicml/composer) library's distributed launcher.\n\nIf you have a write-enabled [HuggingFace auth token](https://huggingface.co/docs/hub/security-tokens), you can optionally upload your model to the Hub! Just export your token like this:\n\n```bash\nexport HF_TOKEN=your-auth-token\n```\n\nand uncomment the line containing `--hf_repo_for_upload ...` in the above call to `inference/convert_composer_to_hf.py`.\n\n# Registry\n\nYou can use the registry to customize your workflows without forking the library. Some components of LLM Foundry are registrable, such as models, loggers, and callbacks. This means that you can register new options for these components, and then use them in your yaml config.\n\n## Discovering registrable components\nTo help find and understand registrable components, you can use the `llmfoundry registry` cli command.\n\nWe provide two commands currently:\n- `llmfoundry registry get [--group]`: List all registries, and their components, optionally specifying a specific registry. Example usage: `llmfoundry registry get --group loggers` or `llmfoundry registry get`\n- `llmfoundry registry find <group> <name>`: Get information about a specific registered component. Example usage: `llmfoundry registry find loggers wandb`\n\nUse `--help` on any of these commands for more information.\n\nThese commands can also help you understand what each registry is composed of, as each registry contains a docstring that will be printed out. The general concept is that each registry defines an interface, and components registered to that registry must implement that interface. If there is a part of the library that is not currently extendable, but you think it should be, please open an issue!\n\n## How to register\n\nThere are a few ways to register a new component:\n\n### Python entrypoints\n\nYou can specify registered components via a Python entrypoint if you are building your own package with registered components.\nThis would be the expected usage if you are building a large extension to LLM Foundry, and going to be overriding many components. Note that things registered via entrypoints will override components registered directly in code.\n\nFor example, the following would register the `MyLogger` class, under the key `my_logger`, in the `llm_foundry.loggers` registry:\n\n<!--pytest.mark.skip-->\n```yaml\n[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"foundry_registry\"\nversion = \"0.1.0\"\ndependencies = [\n    \"mosaicml\",\n    \"llm-foundry\",\n]\n\n# Note: Even though in python code, this would be llmfoundry.registry.loggers,\n# when specified in the entry_points, it has to be \"llmfoundry_loggers\". That is,\n# the segments of the name should be joined by an _ in the entry_points section.\n[project.entry-points.\"llmfoundry_loggers\"]\nmy_logger = \"foundry_registry.loggers:MyLogger\"\n```\n\nIf developing new components via entrypoints, it is important to note that Python entrypoints are global to the Python environment. This means that if you have multiple packages that register components with the same key, the last one installed will be the one used. This can be useful for overriding components in LLM Foundry, but can also lead to unexpected behavior if not careful. Additionally, if you change the pyproject.toml, you will need to reinstall the package for the changes to take effect. You can do this quickly by installing with `pip install -e . --no-deps` to avoid reinstalling dependencies.\n\n### Direct call to register\n\nYou can also register a component directly in your code:\n\n<!--pytest.mark.skip-->\n```python\nfrom composer.loggers import LoggerDestination\nfrom llmfoundry.registry import loggers\n\nclass MyLogger(LoggerDestination):\n    pass\n\nloggers.register(\"my_logger\", func=MyLogger)\n```\n\n### Decorators\n\nYou can also use decorators to register components directly from your code:\n\n<!--pytest.mark.skip-->\n```python\nfrom composer.loggers import LoggerDestination\nfrom llmfoundry.registry import loggers\n\n@loggers.register(\"my_logger\")\nclass MyLogger(LoggerDestination):\n    pass\n```\n\nFor both the direct call and decorator approaches, if using the LLM Foundry train/eval scripts, you will need to provide the `code_paths` argument, which is a list of files need to execute in order to register your components. For example, you may have a file called `foundry_imports.py` that contains the following:\n\n<!--pytest.mark.skip-->\n```python\nfrom foundry_registry.loggers import MyLogger\nfrom llmfoundry.registry import loggers\n\nloggers.register(\"my_logger\", func=MyLogger)\n```\n\nYou would then provide `code_paths` to the train/eval scripts in your yaml config:\n\n<!--pytest.mark.skip-->\n```yaml\n...\ncode_paths:\n  - foundry_imports.py\n...\n```\n\nOne of these would be the expected usage if you are building a small extension to LLM Foundry, only overriding a few components, and thus don't want to create an entire package.\n\n# Learn more about LLM Foundry!\n\nCheck out [TUTORIAL.md](https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md) to keep learning about working with LLM Foundry. The tutorial highlights example workflows, points you to other resources throughout the repo, and answers frequently asked questions!\n\n# Contact Us\n\nIf you run into any problems with the code, please file Github issues directly to this repo.\n\nIf you want to train LLMs on the MosaicML platform, reach out to us at [demo@mosaicml.com](mailto:demo@mosaicml.com)!\n"
        },
        {
          "name": "TUTORIAL.md",
          "type": "blob",
          "size": 31.291015625,
          "content": "# LLM Foundry Tutorial\n\n\n\n**Hello!** We‚Äôve put together this ‚Äútutorial‚Äù to help you develop a stronger familiarity with `llm-foundry`, the kinds of things you can do with it, and how to use it to meet your needs.\n\nForging LLMs can be quite complicated ‚Äî you have to get your data prepared, set up your environment correctly with adequate hardware in order to train, evaluate your model once it‚Äôs trained, and finally get it ready to be served. That‚Äôs a lot of moving parts! And that‚Äôs exactly why we decided to create and release `llm-foundry`. This repo aims to simplify each of those pieces of the pipeline into an easy-to-use toolkit.\n\nThis tutorial will provide a brief intro to the repo‚Äôs structure and underlying tools (all courtesy of MosaicML, of course), will go over a few example workflows and point you to the related resources within the repo, and will finally cover a number of FAQs that we have encountered since release.\n\n- [LLM Foundry Tutorial](#llm-foundry-tutorial)\n- [Intro](#intro)\n  - [How this repo is structured](#how-this-repo-is-structured)\n  - [Key components](#key-components)\n    - [Composer](#composer)\n    - [StreamingDataset](#streamingdataset)\n    - [MCLI](#mcli)\n  - [How the YAMLs work](#how-the-yamls-work)\n- [Example Workflows](#example-workflows)\n  - [Workflow 1: I want to play with a HF model like MPT-7B locally](#workflow-1-i-want-to-play-with-a-hf-model-like-mpt-7b-locally)\n  - [Workflow 2: I want to deploy an inference endpoint with a HF model like MPT-7B](#workflow-2-i-want-to-deploy-an-inference-endpoint-with-a-hf-model-like-mpt-7b)\n  - [Workflow 3: I want to finetune a HF model like MPT-7B](#workflow-3-i-want-to-finetune-a-hf-model-like-mpt-7b)\n    - [Supervised FineTuning and Instruction FineTuning](#supervised-finetuning-and-instruction-finetuning)\n    - [Domain Adaptation and Sequence Length Adaptation](#domain-adaptation-and-sequence-length-adaptation)\n      - [Data](#data)\n      - [Modeling](#modeling)\n  - [Workflow 4: I want to train a new HF model from scratch](#workflow-4-i-want-to-train-a-new-hf-model-from-scratch)\n- [FAQs](#faqs)\n    - [Why is the script only using 1 out of N GPUs?](#why-is-the-script-only-using-1-out-of-n-gpus)\n    - [I‚Äôm running into an Out-Of-Memory (OOM) error. What do I do?](#im-running-into-an-out-of-memory-oom-error-what-do-i-do)\n    - [What hardware can I train on?](#what-hardware-can-i-train-on)\n    - [What hardware can I run eval on?](#what-hardware-can-i-run-eval-on)\n    - [What hardware can I run inference on?](#what-hardware-can-i-run-inference-on)\n    - [What is FSDP?](#what-is-fsdp)\n    - [What are the different attention options `torch` / `flash`  for MPT and which one should I use?](#what-are-the-different-attention-options-torch--flash--for-mpt-and-which-one-should-i-use)\n      - [Limitations](#limitations)\n      - [Support for FlashAttention-2](#support-for-flashattention-2)\n    - [What kinds of positional embeddings does LLM Foundry support?](#what-kinds-of-positional-embeddings-does-llm-foundry-support)\n    - [Can I finetune using PEFT / LoRA?](#can-i-finetune-using-peft--lora)\n    - [Can I quantize these models and/or run on CPU?](#can-i-quantize-these-models-andor-run-on-cpu)\n    - [How do I deploy with ONNX/FasterTransformer?](#how-do-i-deploy-with-onnxfastertransformer)\n    - [TransformerEngine and amp\\_fp8 support](#transformerengine-and-amp_fp8-support)\n    - [How expensive is it to build LLMs?](#how-expensive-is-it-to-build-llms)\n    - [Common installation issues](#common-installation-issues)\n\nLet‚Äôs get started!\n\n\n\n# Intro\n\nThis section establishes some basics that will provide useful context when navigating `llm-foundry` and digging into the provided scripts, YAMLs, etc. The goals here are to give you a clear sense of the general layout, orient you to the core MosaicML tools that this repo builds on, and introduce the way we use YAMLs to configure some of the more complex scripts.\n\n## How this repo is structured\n\n`llm-foundry` is divided broadly into source code and the scripts that use the source code. Here is an overview:\n\n- `llmfoundry/` - The pip installable source code\n  - `data/` - Dataloaders used throughout training and evaluation\n  - `models/` - Model source code and classes for training, evaluation, and inference\n  - `callbacks/` - A collection of useful Composer callbacks\n  - `optim/` - Custom optimizers, e.g., Decoupled LionW\n- `scripts/` - Scripts for each stage of the model forging lifecycle\n  - `data_prep/` - Convert local and HuggingFace datasets to our Streaming MDS format with support for remote uploading\n  - `train/` - Contains the main training script for pretraining/finetuning your model, as well as example workloads\n  - `eval/` - Contains a dedicated script for evaluating your trained model, as well as example workloads\n  - `inference/` - Scripts to load and query trained, HuggingFace-formatted models\n- `mcli/` - A collection of example workload configurations that could be launched on the MosaicML Platform via the `mcli` command line interface.\n\nEach of the above directories has its own `README.md` which goes into more depth about how to use the code it contains. To get the fullest picture on how `llm-foundry` works, make sure to give those a read too.\n\n## Key components\n\nThere are 3 key libraries (all from MosaicML) that power `llm-foundry` and which you'll see throughout. These are worth covering a bit more, so in this section we'll briefly go over [Composer](https://docs.mosaicml.com/projects/composer/en/latest/), our distributed training engine, [Streaming](https://docs.mosaicml.com/projects/streaming/en/stable/), which enables streaming datasets, and [MCLI](https://docs.mosaicml.com/projects/mcli/en/latest/), which you can use to train on the MosaicML Platform.\n\n### Composer\n\nThe Composer library is the workhorse of our training and evaluation scripts.\nIf you dig into those scripts, you'll notice that they are basically just very configurable wrappers around the Composer [Trainer](https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html).\nThe Trainer is a pytorch-native object that composes your model, dataset(s), optimizer, and more into a cohesive training pipeline with all the bells and whistles.\nSpending some time understanding the Composer Trainer is a great way to form a deeper understanding of what the train and eval scripts are doing under the hood.\n\nComposer also comes packaged with the `composer` launcher.\nIf you go through our docs, you'll notice that we instruct you to launch the training script (`scripts/train/train.py`) and eval script (`scripts/eval/eval.py`) using the launcher, like so,\n\n<!--pytest.mark.skip-->\n```bash\ncd scripts/train\ncomposer train.py <path/to/your/training/yaml>\n```\n\nThe `composer` launcher puts all your GPUs to work by launching the script on a separate process for each device. The Trainer handles the rest.\n\n### StreamingDataset\n\nThe training script contains logic for building a few different types of dataloaders used for different training tasks.\nEach of these dataloaders is built to work with **streaming datasets**.\nThere are a number of benefits that come from using streaming datasets, from fast, deterministic resumption to easily loading from a mixture of streams at once.\n\nThe scripts in `scripts/data_prep/` are your one-stop-shop for converting a local dataset or a dataset on the Hugging Face Hub to our streaming MDS format.\nThese conversion scripts also allow you to upload your converted datasets directly to remote storage like S3, which our streaming datasets can read from.\n\n### MCLI\n\n`mcli` (short for MosaicML platform's Command Line Interface) is your gateway to scaling up training, eval, and inference on the MosaicML Platform. Access to the Platform is available to MosaicML customers (which you will need to set up separately). The `mcli/` directory includes several example YAMLs that demonstrate running various `llm-foundry` workloads on a remote cluster using `mcli`.\n\n## How the YAMLs work\n\nYou'll find a lot of YAMLs in this repo. That's because they are a convenient tool for managing configs, which is what we use them for.\n\nConfig YAMLs are used as inputs to `scripts/train/train.py` and `scripts/eval/eval.py`, and are the main way we configure runs launched with `mcli`.\n\nBoth of the above scripts, `train.py` and `eval.py`, wrap a Composer Trainer in an opinionated way to make it easy to train and evaluate (respectively) LLMs. The bulk of each script essentially just interprets the config YAML it receives to build the appropriate inputs to the Trainer.\n\n**We strive to keep the names of the YAML fields as closely related as possible to the kwargs of the function/class they direct to.** For instance, here's an example snippet for the `model` portion:\n```yaml\nmodel:\n  name: hf_causal_lm\n  pretrained: true\n  pretrained_model_name_or_path: mosaicml/mpt-7b\n```\nIf you dig into `train.py`, you'll find that `model.name: hf_causal_lm` instructs the model builder to create a [ComposerHFCausalLM](https://github.com/mosaicml/llm-foundry/blob/main/llmfoundry/models/hf/hf_causal_lm.py) object. The fields `pretrained` and `pretrained_model_name_or_path` correspond to the same kwargs used by the Hugging Face constructors that class builds from.\n\nThe YAMLS in `mcli/` are used to submit a training job to the MosaicML platform using our MosaicML CLI.\nSign up [here](https://forms.mosaicml.com/demo?utm_source=home&utm_medium=mosaicml.com&utm_campaign=always-on).\nYou can find more info about how to configure mcli YAMLs [here](https://docs.mosaicml.com/projects/mcli/en/latest/).\n\n# Example Workflows\n\nIn this section, we‚Äôll give a brief overview of 4 different workflows. You can treat them as independent ‚Äî that is, you don‚Äôt need to go through each in any particular order. Instead, the goal here is to give you a sense of how you might approach each of these different situations using `llm-foundry` and related tooling.\n\n## Workflow 1: I want to play with a HF model like MPT-7B locally\n\nThe quickest way to get started is to use the `transformers` library to download one of our MPT-7B models ([base](https://huggingface.co/mosaicml/mpt-7b), [chat](https://huggingface.co/mosaicml/mpt-7b-chat), [instruct](https://huggingface.co/mosaicml/mpt-7b-instruct)) and running a `text-generation` pipeline. You may see some UserWarnings appear due to MPT being a custom model, but those warnings can be safely ignored.\n\n<!--pytest.mark.skip-->\n```python\nimport torch\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n\nname = 'mosaicml/mpt-7b'\n\n# Download config\nconfig = AutoConfig.from_pretrained(name, trust_remote_code=True)\n# (Optional) Use `flash` (preferred) backend for fast attention. Defaults to `torch`.\n# config.attn_config['attn_impl'] = 'flash'\n# (Optional) Change the `max_seq_len` allowed for inference\n# config.max_seq_len = 4096\n\ndtype = torch.bfloat16  # or torch.float32\n\n# Download model source and weights\nmodel = AutoModelForCausalLM.from_pretrained(\n    name,\n    config=config,\n    torch_dtype=dtype,\n    trust_remote_code=True)\n\n# Download tokenizer\ntokenizer = AutoTokenizer.from_pretrained(name)\n\n# Run text-generation pipeline\npipe = pipeline(\n    'text-generation',\n    model=model,\n    tokenizer=tokenizer,\n    device='cuda:0',  # (Optional) to run on GPU 0\n)\nwith torch.autocast('cuda', dtype=dtype):\n    print(\n        pipe('Here is a recipe for vegan banana bread:\\n',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n\n```\n\nNote: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n\nTo play with more features like batching and multi-turn chat, check out our example scripts `scripts/inference/hf_generate.py` and `scripts/inference/hf_chat.py`, with instructions in the [inference README](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/README.md).\n\n## Workflow 2: I want to deploy an inference endpoint with a HF model like MPT-7B\n\nThis site is under construction :)\n\nPlease check back soon for more info.\n\n## Workflow 3: I want to finetune a HF model like MPT-7B\n\nWe address two possible versions of ‚Äúfinetuning‚Äù here. For both, you‚Äôll want to be familiar with the material covered in `scripts/train/README.md`. The first finetuning workflow applies if you have **labeled data** ‚Äî where you want to train the model to produce a target output given some input. The second workflow applies if you have additional **unlabeled data** that you want to adapt the model to.\n\n### Supervised FineTuning and Instruction FineTuning\n\n`scripts/train/` already includes some resources for supervised finetuning. If that‚Äôs what you‚Äôre interested in check out\n\n1. [**LLM Finetuning from a Local Dataset: A Concrete Example**](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/finetune_example/README.md)\n2. [The YAML which should replicate the process of creating MPT-7B-Instruct from MPT-7b](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml) ‚Äî You can point this at your own dataset by [following these instructions](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/README.md#Usage)\n\n### Domain Adaptation and Sequence Length Adaptation\n\n> **Note**\n> Finetuning MPT-7B requires ‚â• 4x40GB A100s, and a similarly sized model without flash attention may take 8 or more, depending on your sequence length. Use a smaller model if you do not have enough GPUs.\n\nDomain and Sequence Length Adaptation are two similar cases that do not fit neatly into the pretraining/finetuning taxonomy. For the purposes of LLM Foundry, it is more instructive to consider them ‚Äúcontinued pretraining‚Äù, as our setup will more resemble pretraining than it does Supervised Fine Tuning. In particular, we will employ the same dataloader and data preparation strategy as used in pretraining.\n\nFor the purposes of this example, we will assume you are \"fine-tuning\" MPT-7B on a longer sequence length, but the same process would work for a new style of text (e.g. getting MPT-7B to work on, say, legal text). Note that the bigger the change, the more tokens you want to continue training on: extending the sequences to 4,096 does not require as many training steps as extending to 65,536. Similarly, adapting MPT-7B to code (which made up a significant fraction of its training data) does not require as many steps as adapting to legal documents in Hindi (which made up ~0% of its training data).\n\n#### Data\n\nFirst we need to pre-tokenize our data and concatenate it to fill up each sequence, as this keeps us from wasting any compute on pad tokens. The canonical reference for this is `scripts/data_prep/README.md`.\n\nIf you are doing Sequence Length Adaptation, remember to adapt the above example to use your longer sequence length. Since we are using ALiBi, you can train on shorter sequences than you plan to use for evaluation; you can go somewhere between 20% and 100% longer, depending on how long your sequences are and the nuances of your data. For this example, suppose you want to do inference on sequences that are around 6,000 tokens long; for this it makes sense to train on 4,096 and then rely on ALiBi‚Äôs zero-shot length extrapolation at inference time.\n\nOutput the processed data to `./my-adaptation-data`. Note that we use smaller subsets of C4 as an example; you may have different data you want to use. Following [the data preparation README](https://github.com/mosaicml/llm-foundry/blob/main/scripts/data_prep/README.md), we convert C4 as follows:\n\n<!--pytest.mark.skip-->\n```bash\npython scripts/data_prep/convert_dataset_hf.py \\\n  --dataset allenai/c4 --data_subset en \\\n  --out_root my-adaptation-data --splits train_small val_small \\\n  --concat_tokens 4096 --tokenizer EleutherAI/gpt-neox-20b --eos_text '<|endoftext|>' \\\n  --compression zstd\n```\n\n#### Modeling\n\nNow that we have our data ready, we can slightly modify `scripts/train/yamls/finetune/mpt-7b_domain_adapt.yaml` to fit our purposes, changing `max_seq_len` to 4096 and the directory data_local to `./my-adaptation-data`. We *could* create a new YAML to do this, then point the trainer to it, but there is no need to. We can change these values as we kick off the training by supplying the override values as additional arguments:\n\n<!--pytest.mark.skip-->\n```bash\ncomposer scripts/train/train.py scripts/train/yamls/finetune/mpt-7b_domain_adapt.yaml max_seq_len=4096 ...\n```\n> Note that this override where we set `max_seq_len=4096` in the above command works because of how the whole YAML is set up. Importantly, the YAML is configured with `model.config_overrides.max_seq_len: ${max_seq_len}`, which tells the MPT model to override its default max sequence length with the value set for `max_seq_len`.\n\nYou will see some info logs including your configs, and then training will start.\n\nAfter you're done training, you probably want to convert your Composer checkpoint to HuggingFace/ONNX/FasterTransformer format. To do that, check out the [inference README](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/README.md).\n\n## Workflow 4: I want to train a new HF model from scratch\n\n> **Note**\n> Pretraining for 10s of billions of tokens is a large job even for a smaller model; you‚Äôll want multiple A100s for this example.\n\nIt is conceivable that you would like to train a model *with the same architecture* as a model available in HuggingFace `transformers` but without using those same weights; for example, if you have a large amount of proprietary data, or want to change something about the model that is hard to change after the fact. So, as an example, let‚Äôs say you want a version of `gpt2`  but with a longer sequence length, say 2048. Using the MPT architecture would give us Flash Attention and ALiBi, allowing us to go much longer; but for this example we stick with 2048. And of course, let‚Äôs use 150 tokens/parameter, which is the ratio that MPT-7B used, getting us to 17.55B tokens for our 117M param model.\n\nThe first step to training from scratch is to get your pretraining data prepared.  Following [the data preparation README](https://github.com/mosaicml/llm-foundry/blob/main/scripts/data_prep/README.md), we convert C4 as follows:\n\n<!--pytest.mark.skip-->\n```bash\npython scripts/data_prep/convert_dataset_hf.py \\\n  --dataset allenai/c4 --data_subset en \\\n  --out_root my-copy-c4 --splits train_small val_small \\\n  --concat_tokens 2048 --tokenizer gpt2 \\\n  --eos_text '<|endoftext|>' \\\n  --compression zstd\n```\n\nNow we kick off a training using the configuration located at `scripts/train/yamls/pretrain/gpt2-small.yaml`:\n\n<!--pytest.mark.skip-->\n```bash\ncomposer scripts/train/train.py scripts/train/yamls/pretrain/gpt2-small.yaml \\\n    max_seq_len=2048 \\\n    train_loader.dataset.split=train_small \\\n    eval_loader.dataset.split=val_small \\\n```\n\nAfter you're done training, you probably want to convert your Composer checkpoint to HuggingFace/ONNX/FasterTransformer format. To do that, check out the [inference README](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/README.md).\n\n# FAQs\n\nThe purpose of this section is probably pretty self-evident. You‚Äôve got questions and we‚Äôve (hopefully) got answers. Here are some of the more common ones we‚Äôve seen. Before filing an issue, please see if your question is addressed in one of these FAQs or in the READMEs.\n\n### Why is the script only using 1 out of N GPUs?\n- Make sure you are using the `composer` launcher instead of the `python` launcher:\n\n   ‚úÖ `composer train/train.py ...`\n\n   ‚ùå `python train/train.py ...`\n\n  The `composer` launcher is responsible for detecting the available GPUs and launching N processes with the correct distributed environment details. See [the launcher script here](https://github.com/mosaicml/composer/blob/dev/composer/cli/launcher.py) for more details.\n\n### I‚Äôm running into an Out-Of-Memory (OOM) error. What do I do?\n- Hardware limitations may simply prevent some training/inference configurations, but here are some steps to troubleshooting OOMs.\n- First, confirm that you are running with the `composer` launcher, e.g. `composer train/train.py ...`, and using all N GPUs? If not, you may be running into OOMs because your model is not being FSDP-sharded across N devices.\n- Second, confirm that you have turned on FSDP for model sharding. For example, YAMLs for the `train.py` script should have a `fsdp_config` section. And you need to use `fsdp_config.sharding_strategy: FULL_SHARD` to enable parameter sharding.\n- Third, confirm that you are using mixed precision, for example by setting `precision: amp_bf16`.\n- If you are still seeing OOMs, reduce the `device_train_microbatch_size` or `device_eval_batch_size` which will reduce the live activation memory.\n- If OOMs persist with `device_train_microbatch_size: 1` and `device_eval_batch_size: 1`, you may need to use activation checkpointing `fsdp_config.activation_checkpointing: true` (if you are not already) and, as a last resort, activation CPU offloading `fsdp_config.activation_cpu_offload: true`.\n\n### What hardware can I train on?\n- In general, this repo should work on any system with NVIDIA GPUs. Checkout the `scripts/train/README.md` for more [details on GPU memory requirements]([https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm](https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm)). We recommend using `Flash` attention. Keep in mind you may run into issues with `Flash` support on some GPU types. In that situation, you can fall back to `attn_impl: torch`, or raise an issue in the [Flash Attention github repo](https://github.com/Dao-AILab/flash-attention).\n\n### What hardware can I run eval on?\n- Similar to above‚Ä¶\n\n### What hardware can I run inference on?\n- Similar to above‚Ä¶\n\n### What is FSDP?\n- [Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a PyTorch implementation of the [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/abs/1910.02054). FSDP shards networks parameters and the optimizer state across all GPUs. This enables users to train models with large parameter counts which do not fit into a single GPUs memory.\n\n### What are the different attention options `torch` / `flash`  for MPT and which one should I use?\n- **Short answer:** `torch` is the native pytorch attention implementation, and `flash` is an implementation of the much more optimized [Flash Attention](https://arxiv.org/abs/2205.14135) method. `flash` will be faster (and use less GPU memory) than `torch`, but they might not work with all hardware and environment setups.\n\n  Our training setups typically use `flash`.\n\n- **Long answer:** In NLP, Softmax Attention operates on a sequence. It is an all to all graph operation where, during training, the memory complexity is quadratic with respect to the length of the sequence. Furthermore, on GPUs, naive implementations of Softmax Attention are bandwidth (BW) limited.\n[Rabe et al. (2021)](https://arxiv.org/abs/2112.05682) and [Dao et al. (2022)](https://arxiv.org/abs/2205.14135) showed that fusing all operations in Softmax Attention can make the operation much less BW limited.\nFurthermore, integrating a recomputation schema decreases the sequence length memory complexity from *quadratic* to *linear*, thereby supporting much longer sequence lengths.\n\n  - Setting `attn_config.attn_impl=torch` enables a naive Softmax Attention written using base torch operations.\n  - Setting `attn_config.attn_impl=flash` enables Flash Attention [implemented by Dao et al in the Dao-AILab repo using CUDA](https://github.com/Dao-AILab/flash-attention). This will have linear memory complexity (enabling larger batch sizes) and will run much faster.\n\n<!-- In NLP, Softmax Attention operates on a sequence. It is an all to all graph operation where, during training, the memory complexity is quadratic with respect to the length of the sequence. Furthermore, on GPUs, naive implementations of Softmax Attention are BW limited.\n[Rabe et al. (2021)](https://arxiv.org/abs/2112.05682) and [Dao et al. (2022)](https://arxiv.org/abs/2205.14135) noted that fusing all operations in Softmax Attention can make the operation much less BW limited.\nFurthermore, integrating a recomputation schema decreases the sequence length memory complexity from quadratic to linear enabling practitioners to train transformer networks using much longer sequence lengths.\n\nSetting `attn_config.attn_impl=torch` enables a naive Softmax Attention written using base torch operations.\nSetting `attn_config.attn_impl=flash` enables flash attention [implemented by Dao et al in the HazyResearch repo using CUDA](https://github.com/HazyResearch/flash-attention). This will have linear memory complexity (enabling larger batch sizes) and will run much faster.\nThe majority of our training setups use `flash`. -->\n\n#### Limitations\n- For training, `torch` uses a lot of memory and is slow.\n- `flash` cannot return attention weights and therefore cannot be used with methods that require it.\n- `flash` cannot accept an attention bias. However, it still allows the use of ALiBi positional bias.\n\n#### Support for FlashAttention-2\n- [FlashAttention-2](https://arxiv.org/pdf/2307.08691.pdf) improves upon FlashAttention to get even faster attention computation. LLM Foundry supports FlashAttention-2. Please follow the instructions [here](https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#flashattention).\n\n### What kinds of positional embeddings does LLM Foundry support?\nCurrently we support [Learned Positional Embeddings](https://arxiv.org/pdf/1706.03762.pdf), [Attention with Linear Biases (ALiBi)](https://arxiv.org/pdf/2108.12409.pdf), and [Rotary Positional Embeddings (RoPE)](https://arxiv.org/pdf/2104.09864.pdf). There is also an option to switch off all of these embeddings to get [No Positional Embedding](https://arxiv.org/pdf/2203.16634.pdf).\n\n| Name                               | YAML Config                                                       | Training MFU on MPT-7B trained on 8 A100 80GB GPUs | Notes                                                                                                                                                                       |\n|:-----------------------------------|:------------------------------------------------------------------|:---------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Learned Positional Embeddings      | <pre>model:<br>     learned_pos_emb:&nbsp;True</pre>| 65.7                                                |                                                                                                                                                                             |\n| ALiBi                              | <pre>model:<br>     attn_config:<br>         alibi:&nbsp;True</pre>| 64.5                                                |  Requires Flash (v2.4.2 or higher) or Torch attention.                                                                                                                                        |\n| RoPE (Dao-AILab Implementation)    | <pre>model:<br>     attn_config:<br>         rope:&nbsp;True<br>         rope_impl:&nbsp;dail</pre>| 64.5                                                | Requires a CUDA GPU and the [flash-attn library](https://github.com/Dao-AILab/flash-attention) v2.0.1 or higher to be installed. Please see the instructions in the [paragraph above](#support-for-flashattention-2) on how to install flash-attn v2. Note that the attention implementation can still be `torch` or `flash`. |\n| RoPE (Hugging<code>&nbsp;</code>Face Implementation)  | <pre>model:<br>     attn_config:<br>         rope:&nbsp;True<br>         rope_impl:&nbsp;hf</pre>| 62.3                                                |                                                                                                                                                                             |\n\n### Can I finetune using PEFT / LoRA?\n- LLM Foundry does support LoRA via an integration with the [PEFT](https://github.com/huggingface/peft) library. Within LLM Foundry, run (`scripts/train/train.py`), adding `peft_config` arguments to the `model` section of the config `.yaml`, like so:\n<!--pytest.mark.skip-->\n```yaml\nmodel:\n  ...\n  peft_config:\n      r: 16\n      peft_type: LORA\n      task_type: CAUSAL_LM\n      lora_alpha: 32\n      lora_dropout: 0.05\n      target_modules:\n      - q_proj\n      - k_proj\n```\n- For efficiency, The MPT model concatenates the `Q`, `K`, and `V` matrices in each attention block into a single `Wqkv` matrix that is three times wider. Currently, LoRA supports a low-rank approximation to this `Wqkv` matrix.\n- When evaluating with PEFT / LoRA separated weight, just set `pretrained_lora_id_or_path` in `model`(Find an example [here](scripts/eval/yamls/hf_lora_eval.yml#L19)).\n\n### Can I quantize these models and/or run on CPU?\n- The LLM Foundry codebase does not directly have examples of quantization or limited-resource inference. But you can check out [GGML](https://github.com/ggerganov/ggml) (same library that powers llama.cpp) which has built support for efficiently running MPT models on CPU! You _can_ load your model in 8-bit precision for inference using the [bitsandbytes library](https://github.com/TimDettmers/bitsandbytes) and Hugging Face's [accelerate](https://huggingface.co/docs/accelerate/index) via `load model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)`, although we have not extensively benchmarked the performance (see the Hugging Face [quantization documentation](https://huggingface.co/docs/transformers/main/main_classes/quantization) for more detail).\n\n### How do I deploy with ONNX/FasterTransformer?\n- Check out the `scripts/inference` directory for instructions and scripts.\n\n### TransformerEngine and amp_fp8 support\nOnce [installed](https://github.com/mosaicml/llm-foundry/tree/main#TransformerEngine-and-amp_fp8-support), if you are using an H100, you can use fp8 with te layers by setting eg:\n<!--pytest.mark.skip-->\n```yaml\nprecision: amp_fp8\n\nmodel:\n  fc_type: te\n```\nin the training yaml.\n\nSetting\n<!--pytest.mark.skip-->\n```yaml\nmodel:\n  ffn_config_defaults:\n    ffn_type: te_ln_mlp\n```\nenables [TransformerEngine's LayerNormMLP](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/pytorch.html#transformer_engine.pytorch.LayerNormMLP) layer which enables sequence parallelism if configured correctly.\n\nWARNING: `state_dicts` generated with `ffn_type: te_ln_mlp` will NOT directly map to `state_dicts` generated using the default network configurations. We do not have control over how `te.LayerNormMLP` is implemented and therefore cannot readily reconcile it with the default implementation (or any other implementation).\n\n### How expensive is it to build LLMs?\n- Check out our blog post [GPT3-Quality for <$500k](https://www.mosaicml.com/blog/gpt-3-quality-for-500k) for guidance on LLM training times and costs.\n\n  You can also check out our `scripts/train/benchmarking` folder for up-to-date information on the training throughput of MPT models using LLM Foundry. This datasheet can be used to answer questions like: ‚ÄúIf I want to train an MPT-13B with context length 8k on 128xA100-40GB, what training throughput in tokens/sec should I expect?‚Äù\n\n### Common installation issues\n- We're still putting this section together. In the meantime, please see the top-level README for our recommended installation.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "llmfoundry",
          "type": "tree",
          "content": null
        },
        {
          "name": "mcli",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 15.2255859375,
          "content": "# build requirements\n[build-system]\nrequires = [\"setuptools < 76.0.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n# iSort\n[tool.isort]\nmulti_line_output = 0\nline_length = 80\nskip = [ \"env\", \"wandb\", \"runs\", \"build\", \"node_modules\" ]\ninclude_trailing_comma = true\nsplit_on_trailing_comma = true\n\n# Ruff global\n[tool.ruff]\ntarget-version = \"py39\"\nexclude = [\n    \"build/**\",\n    \"docs/**\",\n    \"node_modules/**\",\n]\n\n# Ruff linter\n[tool.ruff.lint]\nselect = [\n    \"C4\",\n    \"LOG\",\n    \"PERF\",\n    \"PLE\",\n    \"COM812\",\n    \"D\", # pydocstyle\n    \"UP006\"\n]\nextend-safe-fixes = [\n    \"UP006\",\n]\nextend-select = [\"D404\"] # pydocstyle\nignore = [\n    \"D100\",\n    \"D101\",\n    \"D102\",\n    \"D103\",\n    \"D104\",\n    \"D105\",\n    \"D107\",\n    \"D400\",\n    \"D401\",\n    \"D415\",\n]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n# Coverage\n[tool.coverage.run]\nparallel = true\nbranch = true\nrelative_files = true\nconcurrency = [\"thread\"]\ninclude = [\n    \"llmfoundry/*\"\n]\n\n\n# Pyright\n[tool.pyright]\nexclude = ['env-**', 'venv*', '.venv']\nstubPath = \"\"  # suppress useless 'stubPath is not a valid directory' errors\n\nreportUnnecessaryIsInstance = \"none\" # it is ok to do this for clarity or safety\nreportMissingTypeStubs = \"none\"\nreportIncompatibleMethodOverride = \"none\"\nreportIncompatibleVariableOverride = \"error\"\nreportUnusedImport = \"error\"\nreportUnusedClass = \"warning\"\nreportUnusedFunction = \"warning\"\nreportUnusedVariable = \"error\"\nreportDuplicateImport = \"error\"\nreportWildcardImportFromLibrary = \"error\"\nreportUntypedFunctionDecorator = \"warning\"\nreportPrivateImportUsage = \"none\"\nreportUndefinedVariable = \"error\"\nstrictParameterNoneValue = true\nreportPropertyTypeMismatch = \"error\"\nreportUntypedNamedTuple = \"error\"\nreportUnnecessaryCast = \"error\"\nreportInvalidTypeVarUse = \"error\"\nreportOverlappingOverload = \"error\"\nreportUninitializedInstanceVariable = \"error\"\nreportInvalidStringEscapeSequence = \"error\"\nreportMissingParameterType = \"error\"\nreportCallInDefaultInitializer = \"error\"\nreportUnnecessaryComparison = \"error\"\nreportSelfClsParameterName = \"error\"\nreportImplicitStringConcatenation = \"warning\"  # TODO: make this an error\nreportInvalidStubStatement = \"error\"\nreportIncompleteStub = \"error\"\nreportUnsupportedDunderAll = \"error\"\nreportUnusedCoroutine = \"error\"\nreportMissingImports = \"none\"\n\n# Pytest\n[tool.pytest.ini_options]\n# By default, skip gpu tests\naddopts = \"--tb=short -m 'not gpu' --color=yes\"\n\nmarkers = [\n    # For distributed testing\n    \"world_size(val)\",\n    # Should be run during daily regression\n    \"daily\",\n    # Whether the test will be reading data from a remote source, and may require credentials\n    \"remote\",\n    # whether the test requires a gpu\n    \"gpu\",\n]\n\nfilterwarnings = [\n    # \"error\",  # warnings should be treated like errors, but still need to fix some warnings\n    'ignore:ExtraArgumentWarning',  # extra arguments originate from pytest-specific CLI args\n    'ignore:DistributedDefaultValueWarning',  # default distributed values are fine\n    'ignore:NoDistributedWarning',  # running without distributed is fine\n    'ignore:Deterministic mode is activated:UserWarning',  # all tests run with deterministic mode\n    'ignore:SubsetNumBatchesWarning',  # different subsets OK for testing\n    'ignore:No optimizer:UserWarning',  # testing defaults\n    'ignore:No scheduler:UserWarning',  # testing defaults\n    'ignore::DeprecationWarning:tensorboard',  # ignore tensorboard\n]\n\n# Yapf\n[tool.yapf]\n# Align closing bracket with visual indentation.\nalign_closing_bracket_with_visual_indent = false\n\n# Allow dictionary keys to exist on multiple lines. For example:\n#\n#   x = {\n#       ('this is the first element of a tuple',\n#        'this is the second element of a tuple'):\n#            value,\n#   }\nallow_multiline_dictionary_keys = false\n\n# Allow lambdas to be formatted on more than one line.\nallow_multiline_lambdas = false\n\n# Allow splitting before a default / named assignment in an argument list.\nallow_split_before_default_or_named_assigns = true\n\n# Allow splits before the dictionary value.\nallow_split_before_dict_value = true\n\n#   Let spacing indicate operator precedence. For example:\n#\n#     a = 1 * 2 + 3 / 4\n#     b = 1 / 2 - 3 * 4\n#     c = (1 + 2) * (3 - 4)\n#     d = (1 - 2) / (3 + 4)\n#     e = 1 * 2 - 3\n#     f = 1 + 2 + 3 + 4\n#\n# will be formatted as follows to indicate precedence:\n#\n#     a = 1*2 + 3/4\n#     b = 1/2 - 3*4\n#     c = (1+2) * (3-4)\n#     d = (1-2) / (3+4)\n#     e = 1*2 - 3\n#     f = 1 + 2 + 3 + 4\n#\narithmetic_precedence_indication = false\n\n# Number of blank lines surrounding top-level function and class\n# definitions.\nblank_lines_around_top_level_definition = 2\n\n# Insert a blank line before a class-level docstring.\nblank_line_before_class_docstring = false\n\n# Insert a blank line before a module docstring.\nblank_line_before_module_docstring = true\n\n# Insert a blank line before a 'def' or 'class' immediately nested\n# within another 'def' or 'class'. For example:\n#\n#   class Foo:\n#                      # <------ this blank line\n#     def method():\n#       ...\nblank_line_before_nested_class_or_def = true\n\n# Do not split consecutive brackets. Only relevant when\n# dedent_closing_brackets is set. For example:\n#\n#    call_func_that_takes_a_dict(\n#        {\n#            'key1': 'value1',\n#            'key2': 'value2',\n#        }\n#    )\n#\n# would reformat to:\n#\n#    call_func_that_takes_a_dict({\n#        'key1': 'value1',\n#        'key2': 'value2',\n#    })\ncoalesce_brackets = true\n\n# The column limit.\ncolumn_limit = 80\n\n# The style for continuation alignment. Possible values are:\n#\n# - SPACE: Use spaces for continuation alignment. This is default behavior.\n# - FIXED: Use fixed number (CONTINUATION_INDENT_WIDTH) of columns\n#   (ie: CONTINUATION_INDENT_WIDTH/INDENT_WIDTH tabs or\n#   CONTINUATION_INDENT_WIDTH spaces) for continuation alignment.\n# - VALIGN-RIGHT: Vertically align continuation lines to multiple of\n#   INDENT_WIDTH columns. Slightly right (one tab or a few spaces) if\n#   cannot vertically align continuation lines with indent characters.\ncontinuation_align_style = 'SPACE'\n\n# Indent width used for line continuations.\ncontinuation_indent_width = 4\n\n# Put closing brackets on a separate line, dedented, if the bracketed\n# expression can't fit in a single line. Applies to all kinds of brackets,\n# including function definitions and calls. For example:\n#\n#   config = {\n#       'key1': 'value1',\n#       'key2': 'value2',\n#   }        # <--- this bracket is dedented and on a separate line\n#\n#   time_series = self.remote_client.query_entity_counters(\n#       entity='dev3246.region1',\n#       key='dns.query_latency_tcp',\n#       transform=Transformation.AVERAGE(window=timedelta(seconds=60)),\n#       start_ts=now()-timedelta(days=3),\n#       end_ts=now(),\n#   )        # <--- this bracket is dedented and on a separate line\ndedent_closing_brackets = true\n\n# Disable the heuristic which places each list element on a separate line\n# if the list is comma-terminated.\ndisable_ending_comma_heuristic = false\n\n# Place each dictionary entry onto its own line.\neach_dict_entry_on_separate_line = true\n\n# Require multiline dictionary even if it would normally fit on one line.\n# For example:\n#\n#   config = {\n#       'key1': 'value1'\n#   }\nforce_multiline_dict = false\n\n# The regex for an i18n comment. The presence of this comment stops\n# reformatting of that line, because the comments are required to be\n# next to the string they translate.\ni18n_comment = '#\\..*'\n\n# The i18n function call names. The presence of this function stops\n# reformattting on that line, because the string it has cannot be moved\n# away from the i18n comment.\ni18n_function_call = 'N_, _'\n\n# Indent blank lines.\nindent_blank_lines = false\n\n# Put closing brackets on a separate line, indented, if the bracketed\n# expression can't fit in a single line. Applies to all kinds of brackets,\n# including function definitions and calls. For example:\n#\n#   config = {\n#       'key1': 'value1',\n#       'key2': 'value2',\n#       }        # <--- this bracket is indented and on a separate line\n#\n#   time_series = self.remote_client.query_entity_counters(\n#       entity='dev3246.region1',\n#       key='dns.query_latency_tcp',\n#       transform=Transformation.AVERAGE(window=timedelta(seconds=60)),\n#       start_ts=now()-timedelta(days=3),\n#       end_ts=now(),\n#       )        # <--- this bracket is indented and on a separate line\nindent_closing_brackets = false\n\n# Indent the dictionary value if it cannot fit on the same line as the\n# dictionary key. For example:\n#\n#   config = {\n#       'key1':\n#           'value1',\n#       'key2': value1 +\n#               value2,\n#   }\nindent_dictionary_value = true\n\n# The number of columns to use for indentation.\nindent_width = 4\n\n# Join short lines into one line. E.g., single line 'if' statements.\njoin_multiple_lines = false\n\n# Do not include spaces around selected binary operators. For example:\n#\n#   1 + 2 * 3 - 4 / 5\n#\n# will be formatted as follows when configured with \"*,/\":\n#\n#   1 + 2*3 - 4/5\nno_spaces_around_selected_binary_operators = ''\n\n# Use spaces around default or named assigns.\nspaces_around_default_or_named_assign = false\n\n# Adds a space after the opening '{' and before the ending '}' dict delimiters.\n#\n#   {1: 2}\n#\n# will be formatted as:\n#\n#   { 1: 2 }\nspaces_around_dict_delimiters = false\n\n# Adds a space after the opening '[' and before the ending ']' list delimiters.\n#\n#   [1, 2]\n#\n# will be formatted as:\n#\n#   [ 1, 2 ]\nspaces_around_list_delimiters = false\n\n# Use spaces around the power operator.\nspaces_around_power_operator = false\n\n# Use spaces around the subscript / slice operator.  For example:\n#\n#   my_list[1 : 10 : 2]\nspaces_around_subscript_colon = false\n\n# Adds a space after the opening '(' and before the ending ')' tuple delimiters.\n#\n#   (1, 2, 3)\n#\n# will be formatted as:\n#\n#   ( 1, 2, 3 )\nspaces_around_tuple_delimiters = false\n\n# The number of spaces required before a trailing comment.\n# This can be a single value (representing the number of spaces\n# before each trailing comment) or list of values (representing\n# alignment column values; trailing comments within a block will\n# be aligned to the first column value that is greater than the maximum\n# line length within the block). For example:\n#\n# With spaces_before_comment=5:\n#\n#   1 + 1 # Adding values\n#\n# will be formatted as:\n#\n#   1 + 1     # Adding values <-- 5 spaces between the end of the statement and comment\n#\n# With spaces_before_comment = '15, 20:'\n#\n#   1 + 1 # Adding values\n#   two + two # More adding\n#\n#   longer_statement # This is a longer statement\n#   short # This is a shorter statement\n#\n#   a_very_long_statement_that_extends_beyond_the_final_column # Comment\n#   short # This is a shorter statement\n#\n# will be formatted as:\n#\n#   1 + 1          # Adding values <-- end of line comments in block aligned to col 15\n#   two + two      # More adding\n#\n#   longer_statement    # This is a longer statement <-- end of line comments in block aligned to col 20\n#   short               # This is a shorter statement\n#\n#   a_very_long_statement_that_extends_beyond_the_final_column  # Comment <-- the end of line comments are aligned based on the line length\n#   short                                                       # This is a shorter statement\n#\nspaces_before_comment = 2\n\n# Insert a space between the ending comma and closing bracket of a list,\n# etc.\nspace_between_ending_comma_and_closing_bracket = false\n\n# Use spaces inside brackets, braces, and parentheses.  For example:\n#\n#   method_call( 1 )\n#   my_dict[ 3 ][ 1 ][ get_index( *args, **kwargs ) ]\n#   my_set = { 1, 2, 3 }\nspace_inside_brackets = false\n\n# Split before arguments\nsplit_all_comma_separated_values = false\n\n# Split before arguments, but do not split all subexpressions recursively\n# (unless needed).\nsplit_all_top_level_comma_separated_values = false\n\n# Split before arguments if the argument list is terminated by a\n# comma.\nsplit_arguments_when_comma_terminated = true\n\n# Set to True to prefer splitting before '+', '-', '*', '/', '//', or '@'\n# rather than after.\nsplit_before_arithmetic_operator = false\n\n# Set to True to prefer splitting before '&', '|' or '^' rather than\n# after.\nsplit_before_bitwise_operator = false\n\n# Split before the closing bracket if a list or dict literal doesn't fit on\n# a single line.\nsplit_before_closing_bracket = true\n\n# Split before a dictionary or set generator (comp_for). For example, note\n# the split before the 'for':\n#\n#   foo = {\n#       variable: 'Hello world, have a nice day!'\n#       for variable in bar if variable != 42\n#   }\nsplit_before_dict_set_generator = false\n\n# Split before the '.' if we need to split a longer expression:\n#\n#   foo = ('This is a really long string: {}, {}, {}, {}'.format(a, b, c, d))\n#\n# would reformat to something like:\n#\n#   foo = ('This is a really long string: {}, {}, {}, {}'\n#          .format(a, b, c, d))\nsplit_before_dot = false\n\n# Split after the opening paren which surrounds an expression if it doesn't\n# fit on a single line.\nsplit_before_expression_after_opening_paren = false\n\n# If an argument / parameter list is going to be split, then split before\n# the first argument.\nsplit_before_first_argument = false\n\n# Set to True to prefer splitting before 'and' or 'or' rather than\n# after.\nsplit_before_logical_operator = false\n\n# Split named assignments onto individual lines.\nsplit_before_named_assigns = true\n\n# Set to True to split list comprehensions and generators that have\n# non-trivial expressions and multiple clauses before each of these\n# clauses. For example:\n#\n#   result = [\n#       a_long_var + 100 for a_long_var in xrange(1000)\n#       if a_long_var % 10]\n#\n# would reformat to something like:\n#\n#   result = [\n#       a_long_var + 100\n#       for a_long_var in xrange(1000)\n#       if a_long_var % 10]\nsplit_complex_comprehension = true\n\n# The penalty for splitting right after the opening bracket.\nsplit_penalty_after_opening_bracket = 300\n\n# The penalty for splitting the line after a unary operator.\nsplit_penalty_after_unary_operator = 10000\n\n# The penalty of splitting the line around the '+', '-', '*', '/', '//',\n# ``%``, and '@' operators.\nsplit_penalty_arithmetic_operator = 300\n\n# The penalty for splitting right before an if expression.\nsplit_penalty_before_if_expr = 0\n\n# The penalty of splitting the line around the '&', '|', and '^'\n# operators.\nsplit_penalty_bitwise_operator = 300\n\n# The penalty for splitting a list comprehension or generator\n# expression.\nsplit_penalty_comprehension = 2100\n\n# The penalty for characters over the column limit.\nsplit_penalty_excess_character = 7000\n\n# The penalty incurred by adding a line split to the unwrapped line. The\n# more line splits added the higher the penalty.\nsplit_penalty_for_added_line_split = 20\n\n# The penalty of splitting a list of \"import as\" names. For example:\n#\n#   from a_very_long_or_indented_module_name_yada_yad import (long_argument_1,\n#                                                             long_argument_2,\n#                                                             long_argument_3)\n#\n# would reformat to something like:\n#\n#   from a_very_long_or_indented_module_name_yada_yad import (\n#       long_argument_1, long_argument_2, long_argument_3)\nsplit_penalty_import_names = 0\n\n# The penalty of splitting the line around the 'and' and 'or'\n# operators.\nsplit_penalty_logical_operator = 300\n\n# Use the Tab character for indentation.\nuse_tabs = false\n\n# Ignore directories\n[tool.yapfignore]\nignore_patterns = [\n    \"runs/**/*.py\",\n    \"wandb/**/*.py\",\n    \"build/**/*.py\",\n]\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.8623046875,
          "content": "# Copyright 2024 MosaicML LLM Foundry authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"MosaicML LLM Foundry package setup.\"\"\"\n\nimport copy\nimport os\nfrom typing import Any, Mapping\n\nimport setuptools\nfrom setuptools import setup\n\n_PACKAGE_NAME = 'llm-foundry'\n_PACKAGE_DIR = 'llmfoundry'\n_REPO_REAL_PATH = os.path.dirname(os.path.realpath(__file__))\n_PACKAGE_REAL_PATH = os.path.join(_REPO_REAL_PATH, _PACKAGE_DIR)\n\n# Read the llm-foundry version\n# We can't use `.__version__` from the library since it's not installed yet\nversion_path = os.path.join(_PACKAGE_REAL_PATH, '_version.py')\nwith open(version_path, encoding='utf-8') as f:\n    version_globals: dict[str, Any] = {}\n    version_locals: Mapping[str, object] = {}\n    content = f.read()\n    exec(content, version_globals, version_locals)\n    repo_version = str(version_locals['__version__'])\n\n# Use repo README for PyPi description\nwith open('README.md', 'r', encoding='utf-8') as fh:\n    long_description = fh.read()\n\n# Hide the content between <!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_BEGIN --> and\n# <!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_END --> tags in the README\nwhile True:\n    start_tag = '<!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_BEGIN -->'\n    end_tag = '<!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_END -->'\n    start = long_description.find(start_tag)\n    end = long_description.find(end_tag)\n    if start == -1:\n        assert end == -1, 'there should be a balanced number of start and ends'\n        break\n    else:\n        assert end != -1, 'there should be a balanced number of start and ends'\n        long_description = long_description[:start] + long_description[\n            end + len(end_tag):]\n\nclassifiers = [\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n]\n\ninstall_requires = [\n    'mosaicml[libcloud,wandb,oci,gcs,mlflow]>=0.28.0,<0.29',\n    'mlflow>=2.14.1,<2.19',\n    'accelerate>=0.25,<1.2',  # for HF inference `device_map`\n    'transformers>=4.43.2,<4.47',\n    'mosaicml-streaming>=0.10.0,<0.11',\n    'torch>=2.5.1,<2.5.2',\n    'datasets>=2.20.0,<3.3',\n    'fsspec==2023.6.0',  # newer version results in a bug in datasets that duplicates data\n    'sentencepiece==0.2.0',\n    'einops==0.8.0',\n    'omegaconf>=2.2.3,<3',\n    'slack-sdk<4',\n    'mosaicml-cli>=0.6.10,<1',\n    'onnx==1.17.0',\n    'onnxruntime==1.19.2',\n    'boto3>=1.21.45,<2',\n    'huggingface-hub>=0.19.0,<0.27',\n    'beautifulsoup4>=4.12.2,<5',  # required for model download utils\n    'tenacity>=8.2.3,<10',\n    'catalogue>=2,<3',\n    'typer<1',\n    'GitPython==3.1.43',\n]\n\nextra_deps = {}\n\nextra_deps['dev'] = [\n    'coverage[toml]==7.6.10',\n    'pre-commit>=3.4.0,<4',\n    'pytest>=7.2.1,<9',\n    'pytest_codeblocks>=0.16.1,<0.18',\n    'pytest-cov>=4,<7',\n    'pyright==1.1.256',\n    'toml>=0.10.2,<0.11',\n    'packaging>=21,<25',\n    'hf_transfer==0.1.8',\n]\n\nextra_deps['databricks'] = [\n    'mosaicml[databricks]>=0.28.0,<0.29',\n    'numpy<2',\n    'databricks-sql-connector>=3,<4',\n    'databricks-connect==14.1.0',\n    'lz4>=4,<5',\n]\n\nextra_deps['tensorboard'] = [\n    'mosaicml[tensorboard]>=0.28.0,<0.29',\n]\n\n# Flash 2 group kept for backwards compatibility\nextra_deps['gpu-flash2'] = [\n    'flash-attn==2.6.3',\n]\n\nextra_deps['gpu'] = copy.deepcopy(extra_deps['gpu-flash2'])\n\nextra_deps['peft'] = [\n    'mosaicml[peft]>=0.28.0,<0.29',\n]\n\nextra_deps['openai'] = [\n    'openai>=1.56.0,<2.0',\n    'tiktoken>=0.4,<0.8.1',\n]\n\nextra_deps['megablocks'] = [\n    'megablocks<1.0',\n    'grouped-gemm==0.1.6',\n]\n\nextra_deps['te'] = [\n    'transformer-engine[pytorch]>=1.11.0,<1.12',\n]\n\nextra_deps['databricks-serverless'] = {\n    dep for key, deps in extra_deps.items() for dep in deps\n    if 'gpu' not in key and 'megablocks' not in key and 'te' not in key and\n    'databricks-connect' not in dep\n}\nextra_deps['all-cpu'] = {\n    dep for key, deps in extra_deps.items() for dep in deps\n    if 'gpu' not in key and 'megablocks' not in key and 'te' not in key\n}\nextra_deps['all'] = {\n    dep for key, deps in extra_deps.items() for dep in deps\n    if key not in {'gpu-flash2', 'all-cpu'}\n}\nextra_deps['all-flash2'] = {\n    dep for key, deps in extra_deps.items() for dep in deps\n    if key not in {'gpu', 'all', 'all-cpu'}\n}\n\nsetup(\n    name=_PACKAGE_NAME,\n    version=repo_version,\n    author='MosaicML',\n    author_email='team@mosaicml.com',\n    description='LLM Foundry',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://github.com/mosaicml/llm-foundry/',\n    package_data={\n        'llmfoundry': ['py.typed'],\n    },\n    packages=setuptools.find_packages(\n        exclude=['.github*', 'mcli*', 'scripts*', 'tests*'],\n    ),\n    classifiers=classifiers,\n    install_requires=install_requires,\n    extras_require=extra_deps,\n    python_requires='>=3.9',\n    entry_points={\n        'console_scripts': ['llmfoundry = llmfoundry.cli.cli:app'],\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}