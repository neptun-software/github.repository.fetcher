{
  "metadata": {
    "timestamp": 1736559591071,
    "page": 221,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "tyxsspa/AnyText",
      "stars": 4465,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.92578125,
          "content": ".idea/\n\ntraining/\nlightning_logs/\nimage_log/\n\n*.pth\n*.pt\n*.ckpt\n*.safetensors\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n.vscode\n/show_results/\n*-ori.py\n*.tar.gz\n/tmp_dir/\n/tmp_files/\n/SaveImages/\n/*.png\nfont/*.ttf\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0576171875,
          "content": "Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.5712890625,
          "content": "# AnyText: Multilingual Visual Text Generation And Editing\n\n<a href='https://arxiv.org/abs/2311.03054'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> <a href='https://github.com/tyxsspa/AnyText'><img src='https://img.shields.io/badge/Code-Github-green'></a> <a href='https://modelscope.cn/studios/damo/studio_anytext'><img src='https://img.shields.io/badge/Demo-ModelScope-lightblue'></a> <a href='https://huggingface.co/spaces/modelscope/AnyText'><img src='https://img.shields.io/badge/Demo-HuggingFace-yellow'></a> <a href='https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-wanxiang-api-for-anytext'><img src='https://img.shields.io/badge/API-DashScope-orange'></a>\n\n![sample](docs/sample.jpg \"sample\")\n\n## üìåNews\n[2024.04.18] - üëèüëèüëèThe training code and dataset([**AnyWord-3M**](https://modelscope.cn/datasets/iic/AnyWord-3M/summary)) are released!  \n[2024.04.18] - You can merge weights from self-trained or community models into AnyText now, including all base models and LoRA models based on SD1.5. Have fun!  \n[2024.02.21] - The evaluation code and dataset(**AnyText-benchmark**) are released.  \n[2024.02.06] - Happy Lunar New Year Everyone! We've launched a fun app(Ë°®ÊÉÖÂåÖÂ§ßÂ∏à/MeMeMaster) on [ModelScope](https://modelscope.cn/studios/iic/MemeMaster/summary) and [HuggingFace](https://huggingface.co/spaces/martinxm/MemeMaster) to create cute meme stickers. Come and have fun with it!   \n[2024.01.17] - üéâAnyText has been accepted by ICLR 2024(**Spotlight**)!  \n[2024.01.04] - FP16 inference is available, 3x faster! Now the demo can be deployed on GPU with >8GB memory. Enjoy!  \n[2024.01.04] - HuggingFace Online demo is available [here](https://huggingface.co/spaces/modelscope/AnyText)!  \n[2023.12.28] - ModelScope Online demo is available [here](https://modelscope.cn/studios/damo/studio_anytext/summary)!  \n[2023.12.27] - üß®We released the latest checkpoint(v1.1) and inference code, check on [ModelScope](https://modelscope.cn/models/damo/cv_anytext_text_generation_editing/summary) in Chinese.  \n[2023.12.05] - The paper is available at [here](https://arxiv.org/abs/2311.03054).  \n\nFor more AIGC related works of our group, please visit [here](https://github.com/AIGCDesignGroup), and we are seeking collaborators and research interns([Email us](mailto:cangyu.gyf@alibaba-inc.com)).\n\n## ‚è∞TODOs\n- [x] Release the model and inference code\n- [x] Provide publicly accessible demo link\n- [ ] Provide a free font file(ü§î)\n- [x] Release tools for merging weights from community models or LoRAs\n- [ ] Support AnyText in stable-diffusion-webui(ü§î)\n- [x] Release AnyText-benchmark dataset and evaluation code\n- [x] Release AnyWord-3M dataset and training code\n \n\n## üí°Methodology\nAnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy.\n\n![framework](docs/framework.jpg \"framework\")\n\n## üõ†Installation\n```bash\n# Install git (skip if already done)\nconda install -c anaconda git\n# Clone anytext code\ngit clone https://github.com/tyxsspa/AnyText.git\ncd AnyText\n# Prepare a font file; Arial Unicode MS is recommended, **you need to download it on your own**\nmv your/path/to/arialuni.ttf ./font/Arial_Unicode.ttf\n# Create a new environment and install packages as follows:\nconda env create -f environment.yaml\nconda activate anytext\n```\n\n## üîÆInference\n**[Recommend]**Ôºö We release a demo on [ModelScope](https://modelscope.cn/studios/damo/studio_anytext/summary) and [HuggingFace](https://huggingface.co/spaces/modelscope/AnyText)! You can also try AnyText through our [API](https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-wanxiang-api-for-anytext?spm=a2c4g.11186623.0.0.31f5e0f6B8s4rF) service.\n\nAnyText include two modes: Text Generation and Text Editing. Running the simple code below to perform inference in both modes and verify whether the environment is correctly installed.\n```bash\npython inference.py\n```\nIf you have advanced GPU (with at least 8G memory), it is recommended to deploy our demo as below, which includes usage instruction, user interface and abundant examples.\n```bash\nexport CUDA_VISIBLE_DEVICES=0 && python demo.py\n```\nFP16 inference is used as default, and a Chinese-to-English translation model is loaded for direct input of Chinese prompt (occupying ~4GB of GPU memory). The default behavior can be modified, as the following command enables FP32 inference and disables the translation model:\n```bash\nexport CUDA_VISIBLE_DEVICES=0 && python demo.py --use_fp32 --no_translator\n```\nIf FP16 is used and the translation model not used(or load it on CPU, [see here](https://github.com/tyxsspa/AnyText/issues/33)), generation of one single 512x512 image will occupy ~7.5GB of GPU memory.  \nIn addition, other font file can be used by(although the result may not be optimal):\n```bash\nexport CUDA_VISIBLE_DEVICES=0 && python demo.py --font_path your/path/to/font/file.ttf\n```\nYou can also load a specified AnyText checkpoint:\n```bash\nexport CUDA_VISIBLE_DEVICES=0 && python demo.py --model_path your/path/to/your/own/anytext.ckpt\n```\n![demo](docs/demo.jpg \"demo\")\n**Please note** that when executing inference for the first time, the model files will be downloaded to: `~/.cache/modelscope/hub`. If you need to modify the download directory, you can manually specify the environment variable: `MODELSCOPE_CACHE`.\n\nIn this demo, you can change the style during inference by either change the base model or loading LoRA models(must based on SD1.5):  \n- Change base model: Simply fill in your local base model's path in the [Base Model Path].  \n- Load LoRA models: Input your LoRA model's path and weight ratio into the [LoRA Path and Ratio]. For example: `/path/of/lora1.pth 0.3 /path/of/lora2.safetensors 0.6`.\n\n## üìàEvaluation\n### 1. Data Preparation\n\nDownload the AnyText-benchmark dataset from [ModelScope](https://modelscope.cn/datasets/iic/AnyText-benchmark/summary) or [GoogleDrive](https://drive.google.com/drive/folders/1Eesj6HTqT1kCi6QLyL5j0mL_ELYRp3GV) and unzip the files. In *benchmark* folder, *laion_word* and *wukong_word* are datasets for English and Chinese evaluation, respectively. Open each *test1k.json* and modify the `data_root` with your own path of *imgs* folder. The *FID* directory contains images that are used for calculating the FID (Fr√©chet Inception Distance) score.\n\n### 2. Generate Images\n\nBefore evaluation, we need to generate corresponding images for each method based on the evaluation set. We have also provided [pre-generated images](https://drive.google.com/file/d/1pGN35myilYY04ChFtgAosYr0oqeBy4NU/view?usp=drive_link) for all methods.  Follow the instructions below to generate images on you own. Note that you need modify the paths and other parameters in the bash script accordingly.\n- AnyText\n```bash\nbash ./eval/gen_imgs_anytext.sh\n```\nÔºàIf you encounter an error caused by huggingface being blocked, please uncomment line 98 of ./models_yaml/anytext_sd15.yaml, and replace the path of the *clip-vit-large-patch14* folder with a local oneÔºâ  \n- ControlNet, Textdiffuser, GlyphControl  \nWe use glyph images rendered from AnyText-benchmark dataset as conditional input for these methods:\n```bash\nbash eval/gen_glyph.sh\n```\nNext, please clone the official repositories of **ControlNet**, **Textdiffuser**, and **GlyphControl**, and follow their documentation to set up the environment, download the respective checkpoints, and ensure that inference can be executed normally. Then, copy the three files `<method>_singleGPU.py`, `<method>_multiGPUs.py`, and `gen_imgs_<method>.sh` from the *./eval* folder to the root directory of the corresponding codebases, and run:\n```bash\nbash gen_imgs_<method>.sh\n```\n\n### 3. Evaluate\n\nWe use Sentence Accuracy (Sen. ACC) and Normalized Edit Distance (NED) to evaluate the accuracy of generated text. Please run:\n```bash\nbash eval/eval_ocr.sh\n```\nWe use the FID metric to assess the quality of generated images. Please run:\n```bash\nbash eval/eval_fid.sh\n```\n\nCompared to existing methods, AnyText has a significant advantage in both English and Chinese text generation.  \n![eval](docs/eval.jpg \"eval\")\nPlease note that we have reorganized the code and have further aligned the configuration for each method under evaluation. As a result, there may be minor numerical differences compared to those reported in the original paper.\n\n## üöÇTraining\n1. It is highly recommended to create and activate the `anytext` virtual environment from previous instructions, where the versions of libraries are verified. Otherwise, if you encounter an environmental dependency or training issue, please check if it matches with the versions as listed in `environment.yaml`.  \n2. Download training dataset [**AnyWord-3M**](https://modelscope.cn/datasets/iic/AnyWord-3M/summary) from ModelScope, unzip all \\*.zip files in each subfolder, then open *\\*.json* and modify the `data_root` with your own path of *imgs* folder for each sub dataset.  \n3. Download SD1.5 checkpoint from [HuggingFace](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main), then run `python tool_add_anytext.py` to get an anytext pretrained model.  \n4. Run `python train.py`.  \n**Note**: Training AnyText on 8xA100 (80GB) takes ~312 hours, you can also quickly reproduce it with 200k images, which takes ~60 hours on 8xV100(32GB). Enable Perceptual Loss will consume a significant amount of VRAM and reduce the training speed. Additionally, filtering out images containing watermarks will lower the likelihood of generating watermarks. These two steps are usually activated in the last 1 or 2 epochs. Relevant parameters can be found in the `Configs` within train.py, please check carefully.  \nThe metrics we achieved using 200k images for reproduce are as follows. Note that these metrics are significantly higher than those reported in the original paper because we used V1.1 data and code. For more details, see Appendix 7 of the original paper.  \n![reproduce](docs/reproduce.jpg \"reproduce\")\n\n## üåÑGallery\n![gallery](docs/gallery.png \"gallery\")\n\n## Citation\n```\n@article{tuo2023anytext,\n      title={AnyText: Multilingual Visual Text Generation And Editing}, \n      author={Yuxiang Tuo and Wangmeng Xiang and Jun-Yan He and Yifeng Geng and Xuansong Xie},\n      year={2023},\n      eprint={2311.03054},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n"
        },
        {
          "name": "bert_tokenizer.py",
          "type": "blob",
          "size": 14.146484375,
          "content": "# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tokenization classes.\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\nimport collections\nimport re\nimport unicodedata\n\nimport six\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n    \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n\n    # The casing has to be passed in by the user and there is no explicit check\n    # as to whether it matches the checkpoint. The casing information probably\n    # should have been stored in the bert_config.json file, but it's not, so\n    # we have to heuristically detect it to validate.\n\n    if not init_checkpoint:\n        return\n\n    m = re.match('^.*?([A-Za-z0-9_-]+)/bert_model.ckpt', init_checkpoint)\n    if m is None:\n        return\n\n    model_name = m.group(1)\n\n    lower_models = [\n        'uncased_L-24_H-1024_A-16', 'uncased_L-12_H-768_A-12',\n        'multilingual_L-12_H-768_A-12', 'chinese_L-12_H-768_A-12'\n    ]\n\n    cased_models = [\n        'cased_L-12_H-768_A-12', 'cased_L-24_H-1024_A-16',\n        'multi_cased_L-12_H-768_A-12'\n    ]\n\n    is_bad_config = False\n    if model_name in lower_models and not do_lower_case:\n        is_bad_config = True\n        actual_flag = 'False'\n        case_name = 'lowercased'\n        opposite_flag = 'True'\n\n    if model_name in cased_models and do_lower_case:\n        is_bad_config = True\n        actual_flag = 'True'\n        case_name = 'cased'\n        opposite_flag = 'False'\n\n    if is_bad_config:\n        raise ValueError(\n            'You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. '\n            'However, `%s` seems to be a %s model, so you '\n            'should pass in `--do_lower_case=%s` so that the fine-tuning matches '\n            'how the model was pre-training. If this error is wrong, please '\n            'just comment out this check.' %\n            (actual_flag, init_checkpoint, model_name, case_name,\n             opposite_flag))\n\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode('utf-8', 'ignore')\n        else:\n            raise ValueError('Unsupported string type: %s' % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode('utf-8', 'ignore')\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError('Unsupported string type: %s' % (type(text)))\n    else:\n        raise ValueError('Not running on Python2 or Python 3?')\n\n\ndef printable_text(text):\n    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it's a Unicode string and in the other it's a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode('utf-8', 'ignore')\n        else:\n            raise ValueError('Unsupported string type: %s' % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode('utf-8')\n        else:\n            raise ValueError('Unsupported string type: %s' % (type(text)))\n    else:\n        raise ValueError('Not running on Python2 or Python 3?')\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, 'r', encoding='utf-8') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_by_vocab(vocab, items):\n    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    \"\"\"Runs end-to-end tokenization.\"\"\"\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n    @staticmethod\n    def convert_tokens_to_string(tokens, clean_up_tokenization_spaces=True):\n        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n\n        def clean_up_tokenization(out_string):\n            \"\"\" Clean up a list of simple English tokenization artifacts\n            like spaces before punctuations and abreviated forms.\n            \"\"\"\n            out_string = (\n                out_string.replace(' .', '.').replace(' ?', '?').replace(\n                    ' !', '!').replace(' ,', ',').replace(\" ' \", \"'\").replace(\n                        \" n't\", \"n't\").replace(\" 'm\", \"'m\").replace(\n                            \" 's\", \"'s\").replace(\" 've\",\n                                                 \"'ve\").replace(\" 're\", \"'re\"))\n            return out_string\n\n        text = ' '.join(tokens).replace(' ##', '').strip()\n        if clean_up_tokenization_spaces:\n            clean_text = clean_up_tokenization(text)\n            return clean_text\n        else:\n            return text\n\n    def vocab_size(self):\n        return len(self.vocab)\n\n\nclass BasicTokenizer(object):\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n    def __init__(self, do_lower_case=True):\n        \"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text.\"\"\"\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize(' '.join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize('NFD', text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == 'Mn':\n                continue\n            output.append(char)\n        return ''.join(output)\n\n    def _run_split_on_punc(self, text):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [''.join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append(' ')\n                output.append(char)\n                output.append(' ')\n            else:\n                output.append(char)\n        return ''.join(output)\n\n    def _is_chinese_char(self, cp):\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or (cp >= 0x3400 and cp <= 0x4DBF)\n                or (cp >= 0x20000 and cp <= 0x2A6DF)\n                or (cp >= 0x2A700 and cp <= 0x2B73F)\n                or (cp >= 0x2B740 and cp <= 0x2B81F)\n                or (cp >= 0x2B820 and cp <= 0x2CEAF)\n                or (cp >= 0xF900 and cp <= 0xFAFF)\n                or (cp >= 0x2F800 and cp <= 0x2FA1F)):\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(' ')\n            else:\n                output.append(char)\n        return ''.join(output)\n\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenization.\"\"\"\n\n    def __init__(self, vocab, unk_token='[UNK]', max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = ''.join(chars[start:end])\n                    if start > 0:\n                        substr = '##' + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == ' ' or char == '\\t' or char == '\\n' or char == '\\r':\n        return True\n    cat = unicodedata.category(char)\n    if cat == 'Zs':\n        return True\n    return False\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == '\\t' or char == '\\n' or char == '\\r':\n        return False\n    cat = unicodedata.category(char)\n    if cat in ('Cc', 'Cf'):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64)\n            or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith('P'):\n        return True\n    return False\n"
        },
        {
          "name": "cldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset_util.py",
          "type": "blob",
          "size": 2.2998046875,
          "content": "import ujson\nimport json\nimport pathlib\n\n__all__ = ['load', 'save', 'show_bbox_on_image']\n\n\ndef load(file_path: str):\n    file_path = pathlib.Path(file_path)\n    func_dict = {'.txt': load_txt, '.json': load_json, '.list': load_txt}\n    assert file_path.suffix in func_dict\n    return func_dict[file_path.suffix](file_path)\n\n\ndef load_txt(file_path: str):\n    with open(file_path, 'r', encoding='utf8') as f:\n        content = [x.strip().strip('\\ufeff').strip('\\xef\\xbb\\xbf') for x in f.readlines()]\n    return content\n\n\ndef load_json(file_path: str):\n    with open(file_path, 'rb') as f:\n        content = f.read()\n    return ujson.loads(content)\n\n\ndef save(data, file_path):\n    file_path = pathlib.Path(file_path)\n    func_dict = {'.txt': save_txt, '.json': save_json}\n    assert file_path.suffix in func_dict\n    return func_dict[file_path.suffix](data, file_path)\n\n\ndef save_txt(data, file_path):\n    if not isinstance(data, list):\n        data = [data]\n    with open(file_path, mode='w', encoding='utf8') as f:\n        f.write('\\n'.join(data))\n\n\ndef save_json(data, file_path):\n    with open(file_path, 'w', encoding='utf-8') as json_file:\n        json.dump(data, json_file, ensure_ascii=False, indent=4)\n\n\ndef show_bbox_on_image(image, polygons=None, txt=None, color=None, font_path='./font/Arial_Unicode.ttf'):\n    from PIL import ImageDraw, ImageFont\n    image = image.convert('RGB')\n    draw = ImageDraw.Draw(image)\n    if len(txt) == 0:\n        txt = None\n    if color is None:\n        color = (255, 0, 0)\n    if txt is not None:\n        font = ImageFont.truetype(font_path, 20)\n    for i, box in enumerate(polygons):\n        box = box[0]\n        if txt is not None:\n            draw.text((int(box[0][0]) + 20, int(box[0][1]) - 20), str(txt[i]), fill='red', font=font)\n        for j in range(len(box) - 1):\n            draw.line((box[j][0], box[j][1], box[j + 1][0], box[j + 1][1]), fill=color, width=2)\n        draw.line((box[-1][0], box[-1][1], box[0][0], box[0][1]), fill=color, width=2)\n    return image\n\n\ndef show_glyphs(glyphs, name):\n    import numpy as np\n    import cv2\n    size = 64\n    gap = 5\n    n_char = 20\n    canvas = np.ones((size, size*n_char + gap*(n_char-1), 1))*0.5\n    x = 0\n    for i in range(glyphs.shape[-1]):\n        canvas[:, x:x + size, :] = glyphs[..., i:i+1]\n        x += size+gap\n    cv2.imwrite(name, canvas*255)\n"
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 32.462890625,
          "content": "'''\nAnyText: Multilingual Visual Text Generation And Editing\nPaper: https://arxiv.org/abs/2311.03054\nCode: https://github.com/tyxsspa/AnyText\nCopyright (c) Alibaba, Inc. and its affiliates.\n'''\nimport os\nfrom modelscope.pipelines import pipeline\nimport cv2\nimport gradio as gr\nimport numpy as np\nimport re\nfrom gradio.components import Component\nfrom util import check_channels, resize_image, save_images\nimport json\nimport argparse\n\n\nBBOX_MAX_NUM = 8\nimg_save_folder = 'SaveImages'\nload_model = True\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--use_fp32\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether or not to use fp32 during inference.\"\n    )\n    parser.add_argument(\n        \"--no_translator\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether or not to use the CH->EN translator, which enable input Chinese prompt and cause ~4GB VRAM.\"\n    )\n    parser.add_argument(\n        \"--font_path\",\n        type=str,\n        default='font/Arial_Unicode.ttf',\n        help=\"path of a font file\"\n    )\n    parser.add_argument(\n        \"--model_path\",\n        type=str,\n        default=None,\n        help=\"load a specified anytext checkpoint\"\n    )\n    args = parser.parse_args()\n    return args\n\n\nargs = parse_args()\ninfer_params = {\n    \"model\": 'damo/cv_anytext_text_generation_editing',\n    \"model_revision\": 'v1.1.3',\n    \"use_fp16\": not args.use_fp32,\n    \"use_translator\": not args.no_translator,\n    \"font_path\": args.font_path,\n}\nif args.model_path:\n    infer_params['model_path'] = args.model_path\nif load_model:\n    inference = pipeline('my-anytext-task', **infer_params)\n\n\ndef count_lines(prompt):\n    prompt = prompt.replace('‚Äú', '\"')\n    prompt = prompt.replace('‚Äù', '\"')\n    p = '\"(.*?)\"'\n    strs = re.findall(p, prompt)\n    if len(strs) == 0:\n        strs = [' ']\n    return len(strs)\n\n\ndef generate_rectangles(w, h, n, max_trys=200):\n    img = np.zeros((h, w, 1), dtype=np.uint8)\n    rectangles = []\n    attempts = 0\n    n_pass = 0\n    low_edge = int(max(w, h)*0.3 if n <= 3 else max(w, h)*0.2)  # ~150, ~100\n    while attempts < max_trys:\n        rect_w = min(np.random.randint(max((w*0.5)//n, low_edge), w), int(w*0.8))\n        ratio = np.random.uniform(4, 10)\n        rect_h = max(low_edge, int(rect_w/ratio))\n        rect_h = min(rect_h, int(h*0.8))\n        # gen rotate angle\n        rotation_angle = 0\n        rand_value = np.random.rand()\n        if rand_value < 0.7:\n            pass\n        elif rand_value < 0.8:\n            rotation_angle = np.random.randint(0, 40)\n        elif rand_value < 0.9:\n            rotation_angle = np.random.randint(140, 180)\n        else:\n            rotation_angle = np.random.randint(85, 95)\n        # rand position\n        x = np.random.randint(0, w - rect_w)\n        y = np.random.randint(0, h - rect_h)\n        # get vertex\n        rect_pts = cv2.boxPoints(((rect_w/2, rect_h/2), (rect_w, rect_h), rotation_angle))\n        rect_pts = np.int32(rect_pts)\n        # move\n        rect_pts += (x, y)\n        # check boarder\n        if np.any(rect_pts < 0) or np.any(rect_pts[:, 0] >= w) or np.any(rect_pts[:, 1] >= h):\n            attempts += 1\n            continue\n        # check overlap\n        if any(check_overlap_polygon(rect_pts, rp) for rp in rectangles):\n            attempts += 1\n            continue\n        n_pass += 1\n        cv2.fillPoly(img, [rect_pts], 255)\n        rectangles.append(rect_pts)\n        if n_pass == n:\n            break\n    print(\"attempts:\", attempts)\n    if len(rectangles) != n:\n        raise gr.Error(f'Failed in auto generate positions after {attempts} attempts, try again!')\n    return img\n\n\ndef check_overlap_polygon(rect_pts1, rect_pts2):\n    poly1 = cv2.convexHull(rect_pts1)\n    poly2 = cv2.convexHull(rect_pts2)\n    rect1 = cv2.boundingRect(poly1)\n    rect2 = cv2.boundingRect(poly2)\n    if rect1[0] + rect1[2] >= rect2[0] and rect2[0] + rect2[2] >= rect1[0] and rect1[1] + rect1[3] >= rect2[1] and rect2[1] + rect2[3] >= rect1[1]:\n        return True\n    return False\n\n\ndef draw_rects(width, height, rects):\n    img = np.zeros((height, width, 1), dtype=np.uint8)\n    for rect in rects:\n        x1 = int(rect[0] * width)\n        y1 = int(rect[1] * height)\n        w = int(rect[2] * width)\n        h = int(rect[3] * height)\n        x2 = x1 + w\n        y2 = y1 + h\n        cv2.rectangle(img, (x1, y1), (x2, y2), 255, -1)\n    return img\n\n\ndef process(mode, prompt, pos_radio, sort_radio, revise_pos, base_model_path, lora_path_ratio, show_debug, draw_img, rect_img, ref_img, ori_img, img_count, ddim_steps, w, h, strength, cfg_scale, seed, eta, a_prompt, n_prompt, *rect_list):\n    n_lines = count_lines(prompt)\n    # Text Generation\n    if mode == 'gen':\n        # create pos_imgs\n        if pos_radio == 'Manual-draw(ÊâãÁªò)':\n            if draw_img is not None:\n                pos_imgs = 255 - draw_img['image']\n                if 'mask' in draw_img:\n                    pos_imgs = pos_imgs.astype(np.float32) + draw_img['mask'][..., 0:3].astype(np.float32)\n                    pos_imgs = pos_imgs.clip(0, 255).astype(np.uint8)\n            else:\n                pos_imgs = np.zeros((w, h, 1))\n        elif pos_radio == 'Manual-rect(ÊãñÊ°Ü)':\n            rect_check = rect_list[:BBOX_MAX_NUM]\n            rect_xywh = rect_list[BBOX_MAX_NUM:]\n            checked_rects = []\n            for idx, c in enumerate(rect_check):\n                if c:\n                    _xywh = rect_xywh[4*idx:4*(idx+1)]\n                    checked_rects += [_xywh]\n            pos_imgs = draw_rects(w, h, checked_rects)\n        elif pos_radio == 'Auto-rand(ÈöèÊú∫)':\n            pos_imgs = generate_rectangles(w, h, n_lines, max_trys=500)\n    # Text Editing\n    elif mode == 'edit':\n        revise_pos = False  # disable pos revise in edit mode\n        if ref_img is None or ori_img is None:\n            raise gr.Error('No reference image, please upload one for edit!')\n        edit_image = ori_img.clip(1, 255)  # for mask reason\n        edit_image = check_channels(edit_image)\n        edit_image = resize_image(edit_image, max_length=768)\n        h, w = edit_image.shape[:2]\n        if isinstance(ref_img, dict) and 'mask' in ref_img and ref_img['mask'].mean() > 0:\n            pos_imgs = 255 - edit_image\n            edit_mask = cv2.resize(ref_img['mask'][..., 0:3], (w, h))\n            pos_imgs = pos_imgs.astype(np.float32) + edit_mask.astype(np.float32)\n            pos_imgs = pos_imgs.clip(0, 255).astype(np.uint8)\n        else:\n            if isinstance(ref_img, dict) and 'image' in ref_img:\n                ref_img = ref_img['image']\n            pos_imgs = 255 - ref_img  # example input ref_img is used as pos\n    cv2.imwrite('pos_imgs.png', 255-pos_imgs[..., ::-1])\n    params = {\n        \"mode\": mode,\n        \"sort_priority\": sort_radio,\n        \"show_debug\": show_debug,\n        \"revise_pos\": revise_pos,\n        \"image_count\": img_count,\n        \"ddim_steps\": ddim_steps,\n        \"image_width\": w,\n        \"image_height\": h,\n        \"strength\": strength,\n        \"cfg_scale\": cfg_scale,\n        \"eta\": eta,\n        \"a_prompt\": a_prompt,\n        \"n_prompt\": n_prompt,\n        \"base_model_path\": base_model_path,\n        \"lora_path_ratio\": lora_path_ratio\n    }\n    input_data = {\n        \"prompt\": prompt,\n        \"seed\": seed,\n        \"draw_pos\": pos_imgs,\n        \"ori_image\": ori_img,\n    }\n\n    results, rtn_code, rtn_warning, debug_info = inference(input_data, **params)\n    if rtn_code >= 0:\n        save_images(results, img_save_folder)\n        print(f'Done, result images are saved in: {img_save_folder}')\n        if rtn_warning:\n            gr.Warning(rtn_warning)\n    else:\n        raise gr.Error(rtn_warning)\n    return results, gr.Markdown(debug_info, visible=show_debug)\n\n\ndef create_canvas(w=512, h=512, c=3, line=5):\n    image = np.full((h, w, c), 200, dtype=np.uint8)\n    for i in range(h):\n        if i % (w//line) == 0:\n            image[i, :, :] = 150\n    for j in range(w):\n        if j % (w//line) == 0:\n            image[:, j, :] = 150\n    image[h//2-8:h//2+8, w//2-8:w//2+8, :] = [200, 0, 0]\n    return image\n\n\ndef resize_w(w, img1, img2):\n    if isinstance(img2, dict):\n        img2 = img2['image']\n    return [cv2.resize(img1, (w, img1.shape[0])), cv2.resize(img2, (w, img2.shape[0]))]\n\n\ndef resize_h(h, img1, img2):\n    if isinstance(img2, dict):\n        img2 = img2['image']\n    return [cv2.resize(img1, (img1.shape[1], h)), cv2.resize(img2, (img2.shape[1], h))]\n\n\nis_t2i = 'true'\nblock = gr.Blocks(css='style.css', theme=gr.themes.Soft()).queue()\n\nwith open('javascript/bboxHint.js', 'r') as file:\n    value = file.read()\nescaped_value = json.dumps(value)\n\nwith block:\n    block.load(fn=None,\n               _js=f\"\"\"() => {{\n               const script = document.createElement(\"script\");\n               const text =  document.createTextNode({escaped_value});\n               script.appendChild(text);\n               document.head.appendChild(script);\n               }}\"\"\")\n    gr.HTML('<div style=\"text-align: center; margin: 20px auto;\"> \\\n            <img id=\"banner\" src=\"file/example_images/banner.png\" alt=\"anytext\"> <br>  \\\n            [<a href=\"https://arxiv.org/abs/2311.03054\" style=\"color:blue; font-size:18px;\">arXiv</a>] \\\n            [<a href=\"https://github.com/tyxsspa/AnyText\" style=\"color:blue; font-size:18px;\">Code</a>] \\\n            [<a href=\"https://modelscope.cn/models/damo/cv_anytext_text_generation_editing/summary\" style=\"color:blue; font-size:18px;\">ModelScope</a>]\\\n            [<a href=\"https://huggingface.co/spaces/modelscope/AnyText\" style=\"color:blue; font-size:18px;\">HuggingFace</a>]\\\n            version: 1.1.3 </div>')\n    with gr.Row(variant='compact'):\n        with gr.Column() as left_part:\n            pass\n        with gr.Column():\n            result_gallery = gr.Gallery(label='Result(ÁªìÊûú)', show_label=True, preview=True, columns=2, allow_preview=True, height=600)\n            result_info = gr.Markdown('', visible=False)\n        with left_part:\n            with gr.Accordion('üïπInstructions(ËØ¥Êòé)', open=False,):\n                with gr.Tabs():\n                    with gr.Tab(\"English\"):\n                        gr.Markdown('<span style=\"color:#3B5998;font-size:20px\">Run Examples</span>')\n                        gr.Markdown('<span style=\"color:#575757;font-size:16px\">AnyText has two modes: Text Generation and Text Editing, and we provides a variety of examples. Select one, click on [Run!] button to run.</span>')\n                        gr.Markdown('<span style=\"color:gray;font-size:12px\">Please note, before running examples, ensure the manual draw area is empty, otherwise may get wrong results. Additionally, different examples use \\\n                                     different parameters (such as resolution, seed, etc.). When generate your own, please pay attention to the parameter changes, or refresh the page to restore the default parameters.</span>')\n                        gr.Markdown('<span style=\"color:#3B5998;font-size:20px\">Text Generation</span>')\n                        gr.Markdown('<span style=\"color:#575757;font-size:16px\">Enter the textual description (in Chinese or English) of the image you want to generate in [Prompt]. Each text line that needs to be generated should be \\\n                                     enclosed in double quotes. Then, manually draw the specified position for each text line to generate the image.</span>\\\n                                     <span style=\"color:red;font-size:16px\">The drawing of text positions is crucial to the quality of the resulting image</span>, \\\n                                     <span style=\"color:#575757;font-size:16px\">please do not draw too casually or too small. The number of positions should match the number of text lines, and the size of each position should be matched \\\n                                     as closely as possible to the length or width of the corresponding text line. If [Manual-draw] is inconvenient, you can try dragging rectangles [Manual-rect] or random positions [Auto-rand].</span>')\n                        gr.Markdown('<span style=\"color:gray;font-size:12px\">When generating multiple lines, each position is matched with the text line according to a certain rule. The [Sort Position] option is used to \\\n                                     determine whether to prioritize sorting from top to bottom or from left to right. You can open the [Show Debug] option in the parameter settings to observe the text position and glyph image \\\n                                     in the result. You can also select the [Revise Position] which uses the bounding box of the rendered text as the revised position. However, it is occasionally found that the creativity of the \\\n                                     generated text is slightly lower using this method.</span>')\n                        gr.Markdown('<span style=\"color:#3B5998;font-size:20px\">Text Editing</span>')\n                        gr.Markdown('<span style=\"color:#575757;font-size:16px\">Please upload an image in [Ref] as a reference image, then adjust the brush size, and mark the area(s) to be edited. Input the textual description and \\\n                                     the new text to be modified in [Prompt], then generate the image.</span>')\n                        gr.Markdown('<span style=\"color:gray;font-size:12px\">The reference image can be of any resolution, but it will be internally processed with a limit that the longer side cannot exceed 768 pixels, and the \\\n                                     width and height will both be scaled to multiples of 64.</span>')\n                    with gr.Tab(\"ÁÆÄ‰Ωì‰∏≠Êñá\"):\n                        gr.Markdown('<span style=\"color:#3B5998;font-size:20px\">ËøêË°åÁ§∫‰æã</span>')\n                        gr.Markdown('<span style=\"color:#575757;font-size:16px\">AnyTextÊúâ‰∏§ÁßçËøêË°åÊ®°ÂºèÔºöÊñáÂ≠óÁîüÊàêÂíåÊñáÂ≠óÁºñËæëÔºåÊØèÁßçÊ®°Âºè‰∏ãÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑÁ§∫‰æãÔºåÈÄâÊã©‰∏Ä‰∏™ÔºåÁÇπÂáª[Run!]Âç≥ÂèØ„ÄÇ</span>')\n                        gr.Markdown('<span style=\"color:gray;font-size:12px\">ËØ∑Ê≥®ÊÑèÔºåËøêË°åÁ§∫‰æãÂâçÁ°Æ‰øùÊâãÁªò‰ΩçÁΩÆÂå∫ÂüüÊòØÁ©∫ÁöÑÔºåÈò≤Ê≠¢ÂΩ±ÂìçÁ§∫‰æãÁªìÊûúÔºåÂè¶Â§ñ‰∏çÂêåÁ§∫‰æã‰ΩøÁî®‰∏çÂêåÁöÑÂèÇÊï∞ÔºàÂ¶ÇÂàÜËæ®ÁéáÔºåÁßçÂ≠êÊï∞Á≠âÔºâÔºåÂ¶ÇÊûúË¶ÅËá™Ë°åÁîüÊàêÊó∂ÔºåËØ∑ÁïôÊÑèÂèÇÊï∞ÂèòÂåñÔºåÊàñÂà∑Êñ∞È°µÈù¢ÊÅ¢Â§çÂà∞ÈªòËÆ§ÂèÇÊï∞„ÄÇ</span>')\n                        gr.Markdown('<span style=\"color:#3B5998;font-size:20px\">ÊñáÂ≠óÁîüÊàê</span>')\n                        gr.Markdown('<span style=\"color:#575757;font-size:16px\">Âú®Prompt‰∏≠ËæìÂÖ•ÊèèËø∞ÊèêÁ§∫ËØçÔºàÊîØÊåÅ‰∏≠Ëã±ÊñáÔºâÔºåÈúÄË¶ÅÁîüÊàêÁöÑÊØè‰∏ÄË°åÊñáÂ≠óÁî®ÂèåÂºïÂè∑ÂåÖË£πÔºåÁÑ∂Âêé‰æùÊ¨°ÊâãÁªòÊåáÂÆöÊØèË°åÊñáÂ≠óÁöÑ‰ΩçÁΩÆÔºåÁîüÊàêÂõæÁâá„ÄÇ</span>\\\n                                     <span style=\"color:red;font-size:16px\">ÊñáÂ≠ó‰ΩçÁΩÆÁöÑÁªòÂà∂ÂØπÊàêÂõæË¥®ÈáèÂæàÂÖ≥ÈîÆ</span>, \\\n                                     <span style=\"color:#575757;font-size:16px\">ËØ∑‰∏çË¶ÅÁîªÁöÑÂ§™ÈöèÊÑèÊàñÂ§™Â∞èÔºå‰ΩçÁΩÆÁöÑÊï∞ÈáèË¶Å‰∏éÊñáÂ≠óË°åÊï∞Èáè‰∏ÄËá¥ÔºåÊØè‰∏™‰ΩçÁΩÆÁöÑÂ∞∫ÂØ∏Ë¶Å‰∏éÂØπÂ∫îÁöÑÊñáÂ≠óË°åÁöÑÈïøÁü≠ÊàñÂÆΩÈ´òÂ∞ΩÈáèÂåπÈÖç„ÄÇÂ¶ÇÊûúÊâãÁªòÔºàManual-drawÔºâ‰∏çÊñπ‰æøÔºå\\\n                                     ÂèØ‰ª•Â∞ùËØïÊãñÊ°ÜÁü©ÂΩ¢ÔºàManual-rectÔºâÊàñÈöèÊú∫ÁîüÊàêÔºàAuto-randÔºâ„ÄÇ</span>')\n                        gr.Markdown('<span style=\"color:gray;font-size:12px\">Â§öË°åÁîüÊàêÊó∂ÔºåÊØè‰∏™‰ΩçÁΩÆÊåâÁÖß‰∏ÄÂÆöËßÑÂàôÊéíÂ∫èÂêé‰∏éÊñáÂ≠óË°åÂÅöÂØπÂ∫îÔºåSort PositionÈÄâÈ°πÁî®‰∫éÁ°ÆÂÆöÊéíÂ∫èÊó∂‰ºòÂÖà‰ªé‰∏äÂà∞‰∏ãËøòÊòØ‰ªéÂ∑¶Âà∞Âè≥„ÄÇ\\\n                                     ÂèØ‰ª•Âú®ÂèÇÊï∞ËÆæÁΩÆ‰∏≠ÊâìÂºÄShow DebugÈÄâÈ°πÔºåÂú®ÁªìÊûúÂõæÂÉè‰∏≠ËßÇÂØüÊñáÂ≠ó‰ΩçÁΩÆÂíåÂ≠óÂΩ¢Âõæ„ÄÇ‰πüÂèØ‰ª•ÂãæÈÄâRevise PositionÈÄâÈ°πÔºåËøôÊ†∑‰ºöÁî®Ê∏≤ÊüìÊñáÂ≠óÁöÑÂ§ñÊé•Áü©ÂΩ¢‰Ωú‰∏∫‰øÆÊ≠£ÂêéÁöÑ‰ΩçÁΩÆÔºå‰∏çËøáÂÅ∂Â∞îÂèëÁé∞ËøôÊ†∑ÁîüÊàêÁöÑÊñáÂ≠óÂàõÈÄ†ÊÄßÁï•‰Ωé„ÄÇ</span>')\n                        gr.Markdown('<span style=\"color:#3B5998;font-size:20px\">ÊñáÂ≠óÁºñËæë</span>')\n                        gr.Markdown('<span style=\"color:#575757;font-size:16px\">ËØ∑‰∏ä‰º†‰∏ÄÂº†ÂæÖÁºñËæëÁöÑÂõæÁâá‰Ωú‰∏∫ÂèÇËÄÉÂõæ(Ref)ÔºåÁÑ∂ÂêéË∞ÉÊï¥Á¨îËß¶Â§ßÂ∞èÂêéÔºåÂú®ÂèÇËÄÉÂõæ‰∏äÊ∂ÇÊäπË¶ÅÁºñËæëÁöÑ‰ΩçÁΩÆÔºåÂú®Prompt‰∏≠ËæìÂÖ•ÊèèËø∞ÊèêÁ§∫ËØçÂíåË¶Å‰øÆÊîπÁöÑÊñáÂ≠óÂÜÖÂÆπÔºåÁîüÊàêÂõæÁâá„ÄÇ</span>')\n                        gr.Markdown('<span style=\"color:gray;font-size:12px\">ÂèÇËÄÉÂõæÂèØ‰ª•‰∏∫‰ªªÊÑèÂàÜËæ®ÁéáÔºå‰ΩÜÂÜÖÈÉ®Â§ÑÁêÜÊó∂‰ºöÈôêÂà∂ÈïøËæπ‰∏çËÉΩË∂ÖËøá768ÔºåÂπ∂‰∏îÂÆΩÈ´òÈÉΩË¢´Áº©Êîæ‰∏∫64ÁöÑÊï¥Êï∞ÂÄç„ÄÇ</span>')\n            with gr.Accordion('üõ†Parameters(ÂèÇÊï∞)', open=False):\n                with gr.Row(variant='compact'):\n                    img_count = gr.Slider(label=\"Image Count(ÂõæÁâáÊï∞)\", minimum=1, maximum=12, value=4, step=1)\n                    ddim_steps = gr.Slider(label=\"Steps(Ê≠•Êï∞)\", minimum=1, maximum=100, value=20, step=1)\n                with gr.Row(variant='compact'):\n                    image_width = gr.Slider(label=\"Image Width(ÂÆΩÂ∫¶)\", minimum=256, maximum=768, value=512, step=64)\n                    image_height = gr.Slider(label=\"Image Height(È´òÂ∫¶)\", minimum=256, maximum=768, value=512, step=64)\n                with gr.Row(variant='compact'):\n                    strength = gr.Slider(label=\"Strength(ÊéßÂà∂ÂäõÂ∫¶)\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n                    cfg_scale = gr.Slider(label=\"CFG-Scale(CFGÂº∫Â∫¶)\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n                with gr.Row(variant='compact'):\n                    seed = gr.Slider(label=\"Seed(ÁßçÂ≠êÊï∞)\", minimum=-1, maximum=99999999, step=1, randomize=False, value=-1)\n                    eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n                with gr.Row(variant='compact'):\n                    show_debug = gr.Checkbox(label='Show Debug(Ë∞ÉËØï‰ø°ÊÅØ)', value=False)\n                    gr.Markdown('<span style=\"color:silver;font-size:12px\">whether show glyph image and debug information in the result(ÊòØÂê¶Âú®ÁªìÊûú‰∏≠ÊòæÁ§∫glyphÂõæ‰ª•ÂèäË∞ÉËØï‰ø°ÊÅØ)</span>')\n                a_prompt = gr.Textbox(label=\"Added Prompt(ÈôÑÂä†ÊèêÁ§∫ËØç)\", value='best quality, extremely detailed,4k, HD, supper legible text,  clear text edges,  clear strokes, neat writing, no watermarks')\n                n_prompt = gr.Textbox(label=\"Negative Prompt(Ë¥üÂêëÊèêÁ§∫ËØç)\", value='low-res, bad anatomy, extra digit, fewer digits, cropped, worst quality, low quality, watermark, unreadable text, messy words, distorted text, disorganized writing, advertising picture')\n            base_model_path = gr.Textbox(label='Base Model Path(Âü∫Ê®°Âú∞ÂùÄ)')\n            lora_path_ratio = gr.Textbox(label='LoRA Path and Ratio(loraÂú∞ÂùÄÂíåÊØî‰æã)')\n            prompt = gr.Textbox(label=\"Prompt(ÊèêÁ§∫ËØç)\")\n            with gr.Tabs() as tab_modes:\n                with gr.Tab(\"üñºText Generation(ÊñáÂ≠óÁîüÊàê)\", elem_id='MD-tab-t2i') as mode_gen:\n                    pos_radio = gr.Radio([\"Manual-draw(ÊâãÁªò)\", \"Manual-rect(ÊãñÊ°Ü)\", \"Auto-rand(ÈöèÊú∫)\"], value='Manual-draw(ÊâãÁªò)', label=\"Pos-Method(‰ΩçÁΩÆÊñπÂºè)\", info=\"choose a method to specify text positions(ÈÄâÊã©ÊñπÊ≥ïÁî®‰∫éÊåáÂÆöÊñáÂ≠ó‰ΩçÁΩÆ).\")\n                    with gr.Row():\n                        sort_radio = gr.Radio([\"‚Üï\", \"‚Üî\"], value='‚Üï', label=\"Sort Position(‰ΩçÁΩÆÊéíÂ∫è)\", info=\"position sorting priority(‰ΩçÁΩÆÊéíÂ∫èÊó∂ÁöÑ‰ºòÂÖàÁ∫ß)\")\n                        revise_pos = gr.Checkbox(label='Revise Position(‰øÆÊ≠£‰ΩçÁΩÆ)', value=False)\n                        # gr.Markdown('<span style=\"color:silver;font-size:12px\">try to revise according to text\\'s bounding rectangle(Â∞ùËØïÈÄöËøáÊ∏≤ÊüìÂêéÁöÑÊñáÂ≠óË°åÁöÑÂ§ñÊé•Áü©ÂΩ¢Ê°Ü‰øÆÊ≠£‰ΩçÁΩÆ)</span>')\n                    with gr.Row(variant='compact'):\n                        rect_cb_list: list[Component] = []\n                        rect_xywh_list: list[Component] = []\n                        for i in range(BBOX_MAX_NUM):\n                            e = gr.Checkbox(label=f'{i}', value=False, visible=False, min_width='10')\n                            x = gr.Slider(label='x', value=0.4, minimum=0.0, maximum=1.0, step=0.0001, elem_id=f'MD-t2i-{i}-x', visible=False)\n                            y = gr.Slider(label='y', value=0.4, minimum=0.0, maximum=1.0, step=0.0001, elem_id=f'MD-t2i-{i}-y',  visible=False)\n                            w = gr.Slider(label='w', value=0.2, minimum=0.0, maximum=1.0, step=0.0001, elem_id=f'MD-t2i-{i}-w',  visible=False)\n                            h = gr.Slider(label='h', value=0.2, minimum=0.0, maximum=1.0, step=0.0001, elem_id=f'MD-t2i-{i}-h',  visible=False)\n                            x.change(fn=None, inputs=x, outputs=x, _js=f'v => onBoxChange({is_t2i}, {i}, \"x\", v)', show_progress=False, queue=False)\n                            y.change(fn=None, inputs=y, outputs=y, _js=f'v => onBoxChange({is_t2i}, {i}, \"y\", v)', show_progress=False, queue=False)\n                            w.change(fn=None, inputs=w, outputs=w, _js=f'v => onBoxChange({is_t2i}, {i}, \"w\", v)', show_progress=False, queue=False)\n                            h.change(fn=None, inputs=h, outputs=h, _js=f'v => onBoxChange({is_t2i}, {i}, \"h\", v)', show_progress=False, queue=False)\n\n                            e.change(fn=None, inputs=e, outputs=e, _js=f'e => onBoxEnableClick({is_t2i}, {i}, e)', queue=False)\n                            rect_cb_list.extend([e])\n                            rect_xywh_list.extend([x, y, w, h])\n\n                    rect_img = gr.Image(value=create_canvas(), label=\"Rext Position(ÊñπÊ°Ü‰ΩçÁΩÆ)\", elem_id=\"MD-bbox-rect-t2i\", show_label=False, visible=False)\n                    draw_img = gr.Image(value=create_canvas(), label=\"Draw Position(ÁªòÂà∂‰ΩçÁΩÆ)\", visible=True, tool='sketch', show_label=False, brush_radius=100)\n\n                    def re_draw():\n                        return [gr.Image(value=create_canvas(), tool='sketch'), gr.Slider(value=512), gr.Slider(value=512)]\n                    draw_img.clear(re_draw, None, [draw_img, image_width, image_height])\n                    image_width.release(resize_w, [image_width, rect_img, draw_img], [rect_img, draw_img])\n                    image_height.release(resize_h, [image_height, rect_img, draw_img], [rect_img, draw_img])\n\n                    def change_options(selected_option):\n                        return [gr.Checkbox(visible=selected_option == 'Manual-rect(ÊãñÊ°Ü)')] * BBOX_MAX_NUM + \\\n                                [gr.Image(visible=selected_option == 'Manual-rect(ÊãñÊ°Ü)'),\n                                 gr.Image(visible=selected_option == 'Manual-draw(ÊâãÁªò)'),\n                                 gr.Radio(visible=selected_option != 'Auto-rand(ÈöèÊú∫)'),\n                                 gr.Checkbox(value=selected_option == 'Auto-rand(ÈöèÊú∫)')]\n                    pos_radio.change(change_options, pos_radio, rect_cb_list + [rect_img, draw_img, sort_radio, revise_pos], show_progress=False, queue=False)\n                    with gr.Row():\n                        gr.Markdown(\"\")\n                        run_gen = gr.Button(value=\"Run(ËøêË°å)!\", scale=0.3, elem_classes='run')\n                        gr.Markdown(\"\")\n\n                    def exp_gen_click():\n                        return [gr.Slider(value=512), gr.Slider(value=512)]  # all examples are 512x512, refresh draw_img\n                    with gr.Tab(\"English Examples\"):\n                        exp_gen_en = gr.Examples(\n                            [\n                                ['A raccoon stands in front of the blackboard with the words \"Deep Learning\" written on it', \"example_images/gen17.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 33789703],\n                                ['A crayon drawing by child,  a snowman with a Santa hat, pine trees, outdoors in heavy snowfall, titled \"Snowman\"', \"example_images/gen18.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 35621187],\n                                ['A meticulously designed logo, a minimalist brain, stick drawing style, simplistic style,  refined with minimal strokes, black and white color, white background,  futuristic sense, exceptional design, logo name is \"NextAI\"', \"example_images/gen19.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 2563689],\n                                ['A photograph of the colorful graffiti art on the wall with the words \"Hi~\" \"Get Ready\" \"to\" \"Party\"', \"example_images/gen21.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 88952132],\n                                ['photo of caramel macchiato coffee on the table, top-down perspective, with \"Any\" \"Text\" written on it using cream', \"example_images/gen9.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 66273235],\n                                ['A fine sweater with knitted text: \"Have\" \"A\" \"Good Day\"', \"example_images/gen20.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 35107824],\n                                ['Sign on the clean building that reads \"ÁßëÂ≠¶\" and \"Í≥ºÌïô\"  and \"„Çπ„ÉÜ„ÉÉ„Éó\" and \"SCIENCE\"', \"example_images/gen6.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", True, 4, 13246309],\n                                ['A delicate square cake, cream and fruit, with \"CHEERS\" \"to the\" and \"GRADUATE\" written in chocolate', \"example_images/gen8.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 93424638],\n                                ['A nice drawing in pencil of Michael Jackson,  with the words \"Micheal\" and \"Jackson\" written on it', \"example_images/gen7.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 83866922],\n                                ['a well crafted ice sculpture that made with \"Happy\" and \"Holidays\". Dslr photo, perfect illumination', \"example_images/gen11.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", True, 4, 64901362],\n                            ],\n                            [prompt, draw_img, pos_radio, sort_radio, revise_pos, img_count, seed],\n                            examples_per_page=5,\n                            label=''\n                        )\n                        exp_gen_en.dataset.click(exp_gen_click, None, [image_width, image_height])\n                    with gr.Tab(\"‰∏≠ÊñáÁ§∫‰æã\"):\n                        exp_gen_ch = gr.Examples(\n                            [\n                                ['‰∏ÄÂè™Êµ£ÁÜäÁ´ôÂú®ÈªëÊùøÂâçÔºå‰∏äÈù¢ÂÜôÁùÄ\"Ê∑±Â∫¶Â≠¶‰π†\"', \"example_images/gen1.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 81808278],\n                                ['‰∏Ä‰∏™ÂÑøÁ´•Ëú°Á¨îÁîªÔºåÊ£ÆÊûóÈáåÊúâ‰∏Ä‰∏™ÂèØÁà±ÁöÑËòëËèáÂΩ¢Áä∂ÁöÑÊàøÂ≠êÔºåÊ†áÈ¢òÊòØ\"Ê£ÆÊûóÂ∞èÂ±ã\"', \"example_images/gen16.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 40173333],\n                                ['‰∏Ä‰∏™Á≤æÁæéËÆæËÆ°ÁöÑlogoÔºåÁîªÁöÑÊòØ‰∏Ä‰∏™ÈªëÁôΩÈ£éÊ†ºÁöÑÂé®Â∏àÔºåÂ∏¶ÁùÄÂé®Â∏àÂ∏ΩÔºålogo‰∏ãÊñπÂÜôÁùÄ‚ÄúÊ∑±Â§úÈ£üÂ†Ç‚Äù', \"example_images/gen14.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 6970544],\n                                ['‰∏ÄÂº†Êà∑Â§ñÈõ™Âú∞Èù¥ÁöÑÁîµÂïÜÂπøÂëäÔºå‰∏äÈù¢ÂÜôÁùÄ ‚ÄúÂèå12Â§ß‰øÉÔºÅ‚ÄùÔºå‚ÄúÁ´ãÂáè50‚ÄùÔºå‚ÄúÂä†ÁªíÂä†Âéö‚ÄùÔºå‚ÄúÁ©øËÑ±Êñπ‰æø‚ÄùÔºå‚ÄúÊ∏©Êöñ24Â∞èÊó∂ÈÄÅËææ‚ÄùÔºå ‚ÄúÂåÖÈÇÆ‚ÄùÔºåÈ´òÁ∫ßËÆæËÆ°ÊÑüÔºåÁ≤æÁæéÊûÑÂõæ', \"example_images/gen15.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 66980376],\n                                ['‰∏Ä‰∏™Á≤æËá¥ÁöÑÈ©¨ÂÖãÊùØÔºå‰∏äÈù¢ÈõïÂàªÁùÄ‰∏ÄÈ¶ñ‰∏≠ÂõΩÂè§ËØóÔºåÂÜÖÂÆπÊòØ \"Ëä±ËêΩÁü•Â§öÂ∞ë\" \"Â§úÊù•È£éÈõ®Â£∞\" \"Â§ÑÂ§ÑÈóªÂïºÈ∏ü\" \"Êò•Áú†‰∏çËßâÊôì\"', \"example_images/gen3.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üî\", False, 4, 60358279],\n                                ['‰∏Ä‰ª∂Á≤æÁæéÁöÑÊØõË°£Ôºå‰∏äÈù¢ÊúâÈíàÁªáÁöÑÊñáÂ≠óÔºö\"ÈÄö‰πâ‰∏πÈùí\"', \"example_images/gen4.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 48769450],\n                                ['‰∏Ä‰∏™ÂèåËÇ©ÂåÖÁöÑÁâπÂÜôÁÖßÔºå‰∏äÈù¢Áî®ÈíàÁªáÊñáÂ≠óÂÜôÁùÄ‚Äù‰∏∫‰∫ÜÊó†Ê≥ï‚Äú ‚ÄùËÆ°ÁÆóÁöÑ‰ª∑ÂÄº‚Äú', \"example_images/gen12.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 35552323],\n                                ['‰∏Ä‰∏™ÊºÇ‰∫ÆÁöÑËú°Á¨îÁîªÔºåÊúâË°åÊòüÔºåÂÆáËà™ÂëòÔºåËøòÊúâÂÆáÂÆôÈ£ûËàπÔºå‰∏äÈù¢ÂÜôÁöÑÊòØ\"ÂéªÁÅ´ÊòüÊóÖË°å\", \"ÁéãÂ∞èÊòé\", \"11Êúà1Êó•\"', \"example_images/gen5.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 42328250],\n                                ['‰∏Ä‰∏™Ë£ÖÈ•∞Âçé‰∏ΩÁöÑËõãÁ≥ïÔºå‰∏äÈù¢Áî®Â•∂Ê≤πÂÜôÁùÄ‚ÄúÈòøÈáå‰∫ë‚ÄùÂíå\"APSARA\"', \"example_images/gen13.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 62357019],\n                                ['‰∏ÄÂº†ÂÖ≥‰∫éÂ¢ô‰∏äÁöÑÂΩ©Ëâ≤Ê∂ÇÈ∏¶Ëâ∫ÊúØÁöÑÊëÑÂΩ±‰ΩúÂìÅÔºå‰∏äÈù¢ÂÜôÁùÄ‚Äú‰∫∫Â∑•Êô∫ËÉΩ\" Âíå \"Á•ûÁªèÁΩëÁªú\"', \"example_images/gen10.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 64722007],\n                                ['‰∏ÄÊûö‰∏≠ÂõΩÂè§‰ª£ÈìúÈí±,  ‰∏äÈù¢ÁöÑÊñáÂ≠óÊòØ \"Â∫∑\"  \"ÂØ∂\" \"ÈÄö\" \"ÁÜô\"', \"example_images/gen2.png\", \"Manual-draw(ÊâãÁªò)\", \"‚Üï\", False, 4, 24375031],\n                            ],\n                            [prompt, draw_img, pos_radio, sort_radio, revise_pos, img_count, seed],\n                            examples_per_page=5,\n                            label=''\n                        )\n                        exp_gen_ch.dataset.click(exp_gen_click, None, [image_width, image_height])\n\n                with gr.Tab(\"üé®Text Editing(ÊñáÂ≠óÁºñËæë)\") as mode_edit:\n                    with gr.Row(variant='compact'):\n                        ref_img = gr.Image(label='Ref(ÂèÇËÄÉÂõæ)', source='upload')\n                        ori_img = gr.Image(label='Ori(ÂéüÂõæ)', scale=0.4)\n\n                    def upload_ref(x):\n                        return [gr.Image(type=\"numpy\", brush_radius=100, tool='sketch'),\n                                gr.Image(value=x)]\n\n                    def clear_ref(x):\n                        return gr.Image(source='upload', tool=None)\n                    ref_img.upload(upload_ref, ref_img, [ref_img, ori_img])\n                    ref_img.clear(clear_ref, ref_img, ref_img)\n                    with gr.Row():\n                        gr.Markdown(\"\")\n                        run_edit = gr.Button(value=\"Run(ËøêË°å)!\", scale=0.3, elem_classes='run')\n                        gr.Markdown(\"\")\n                    with gr.Tab(\"English Examples\"):\n                        gr.Examples(\n                            [\n                                ['A Minion meme that says \"wrong\"', \"example_images/ref15.jpeg\", \"example_images/edit15.png\", 4, 39934684],\n                                ['A pile of fruit with \"UIT\" written in the middle', \"example_images/ref13.jpg\", \"example_images/edit13.png\", 4, 54263567],\n                                ['Characters written in chalk on the blackboard that says \"DADDY\"', \"example_images/ref8.jpg\", \"example_images/edit8.png\", 4, 73556391],\n                                ['The blackboard says \"Here\"', \"example_images/ref11.jpg\", \"example_images/edit11.png\", 2, 15353513],\n                                ['A letter picture that says \"THER\"', \"example_images/ref6.jpg\", \"example_images/edit6.png\", 4, 72321415],\n                                ['A cake with colorful characters that reads \"EVERYDAY\"', \"example_images/ref7.jpg\", \"example_images/edit7.png\", 4, 8943410],\n                                ['photo of clean sandy beach,\" \" \" \"', \"example_images/ref16.jpeg\", \"example_images/edit16.png\", 4, 85664100],\n                            ],\n                            [prompt, ori_img, ref_img, img_count, seed],\n                            examples_per_page=5,\n                            label=''\n                        )\n                    with gr.Tab(\"‰∏≠ÊñáÁ§∫‰æã\"):\n                        gr.Examples(\n                            [\n                                ['Á≤æÁæéÁöÑ‰π¶Ê≥ï‰ΩúÂìÅÔºå‰∏äÈù¢ÂÜôÁùÄ‚ÄúÂøó‚Äù ‚ÄúÂ≠ò‚Äù ‚ÄúÈ´ò‚Äù ‚ÄùËøú‚Äú', \"example_images/ref10.jpg\", \"example_images/edit10.png\", 4, 98053044],\n                                ['‰∏Ä‰∏™Ë°®ÊÉÖÂåÖÔºåÂ∞èÁå™ËØ¥ \"‰∏ãÁè≠\"', \"example_images/ref2.jpg\", \"example_images/edit2.png\", 2, 43304008],\n                                ['‰∏Ä‰∏™‰∏≠ÂõΩÂè§‰ª£ÈìúÈí±Ôºå‰∏äÈù¢ÂÜôÁùÄ\"‰πæ\" \"ÈöÜ\"', \"example_images/ref12.png\", \"example_images/edit12.png\", 4, 89159482],\n                                ['‰∏Ä‰∏™Êº´ÁîªÔºå‰∏äÈù¢ÂÜôÁùÄ\" \"', \"example_images/ref14.png\", \"example_images/edit14.png\", 4, 94081527],\n                                ['‰∏Ä‰∏™ÈªÑËâ≤Ê†áÂøóÁâåÔºå‰∏äËæπÂÜôÁùÄ\"‰∏çË¶Å\" Âíå \"Â§ßÊÑè\"', \"example_images/ref3.jpg\", \"example_images/edit3.png\", 2, 64010349],\n                                ['‰∏Ä‰∏™ÈùíÈìúÈºéÔºå‰∏äÈù¢ÂÜôÁùÄ\"  \"Âíå\"  \"', \"example_images/ref4.jpg\", \"example_images/edit4.png\", 4, 71139289],\n                                ['‰∏Ä‰∏™Âª∫Á≠ëÁâ©ÂâçÈù¢ÁöÑÂ≠óÊØçÊ†áÁâåÔºå ‰∏äÈù¢ÂÜôÁùÄ \" \"', \"example_images/ref5.jpg\", \"example_images/edit5.png\", 4, 50416289],\n                            ],\n                            [prompt, ori_img, ref_img, img_count, seed],\n                            examples_per_page=5,\n                            label=''\n                        )\n    ips = [prompt, pos_radio, sort_radio, revise_pos, base_model_path, lora_path_ratio, show_debug, draw_img, rect_img, ref_img, ori_img, img_count, ddim_steps, image_width, image_height, strength, cfg_scale, seed, eta, a_prompt, n_prompt, *(rect_cb_list+rect_xywh_list)]\n    run_gen.click(fn=process, inputs=[gr.State('gen')] + ips, outputs=[result_gallery, result_info])\n    run_edit.click(fn=process, inputs=[gr.State('edit')] + ips, outputs=[result_gallery, result_info])\n\n\nblock.launch(\n    server_name='0.0.0.0' if os.getenv('GRADIO_LISTEN', '') != '' else \"127.0.0.1\",\n    share=False,\n    root_path=f\"/{os.getenv('GRADIO_PROXY_PATH')}\" if os.getenv('GRADIO_PROXY_PATH') else \"\"\n)\n# block.launch(server_name='0.0.0.0')\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yaml",
          "type": "blob",
          "size": 0.9912109375,
          "content": "name: anytext\nchannels:\n  - pytorch\n  - defaults\ndependencies:\n  - python=3.10.6\n  - pip=23.0.1\n  - cudatoolkit=11.7\n  - numpy=1.23.3\n  - cython==0.29.33\n  - pip:\n      - Pillow==9.5.0\n      - gradio==3.50.0\n      - albumentations==0.4.3\n      - opencv-python==4.7.0.72\n      - imageio==2.9.0\n      - imageio-ffmpeg==0.4.2\n      - pytorch-lightning==1.5.0\n      - omegaconf==2.2.3\n      - test-tube==0.7.5\n      - streamlit==1.20.0\n      - einops==0.4.1\n      - transformers==4.30.2\n      - webdataset==0.2.5\n      - kornia==0.6.7\n      - open_clip_torch==2.7.0\n      - torchmetrics==0.11.4\n      - timm==0.6.7\n      - addict==2.4.0\n      - yapf==0.32.0\n      - safetensors==0.4.0\n      - basicsr==1.4.2\n      - jieba==0.42.1\n      - modelscope==1.10.0\n      - tensorflow==2.13.0\n      - torch==2.0.1\n      - torchvision==0.15.2\n      - easydict==1.10\n      - xformers==0.0.20\n      - subword-nmt==0.3.8\n      - sacremoses==0.0.53\n      - sentencepiece==0.1.99\n      - fsspec\n      - diffusers==0.10.2\n      - ujson"
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "example_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "font",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 1.2529296875,
          "content": "from modelscope.pipelines import pipeline\nfrom util import save_images\npipe = pipeline('my-anytext-task', model='damo/cv_anytext_text_generation_editing', model_revision='v1.1.3')\nimg_save_folder = \"SaveImages\"\nparams = {\n    \"show_debug\": True,\n    \"image_count\": 2,\n    \"ddim_steps\": 20,\n}\n\n# 1. text generation\nmode = 'text-generation'\ninput_data = {\n    \"prompt\": 'photo of caramel macchiato coffee on the table, top-down perspective, with \"Any\" \"Text\" written on it using cream',\n    \"seed\": 66273235,\n    \"draw_pos\": 'example_images/gen9.png'\n}\nresults, rtn_code, rtn_warning, debug_info = pipe(input_data, mode=mode, **params)\nif rtn_code >= 0:\n    save_images(results, img_save_folder)\n    print(f'Done, result images are saved in: {img_save_folder}')\nif rtn_warning:\n    print(rtn_warning)\n# 2. text editing\nmode = 'text-editing'\ninput_data = {\n    \"prompt\": 'A cake with colorful characters that reads \"EVERYDAY\"',\n    \"seed\": 8943410,\n    \"draw_pos\": 'example_images/edit7.png',\n    \"ori_image\": 'example_images/ref7.jpg'\n}\nresults, rtn_code, rtn_warning, debug_info = pipe(input_data, mode=mode, **params)\nif rtn_code >= 0:\n    save_images(results, img_save_folder)\n    print(f'Done, result images are saved in: {img_save_folder}')\nif rtn_warning:\n    print(rtn_warning)\n"
        },
        {
          "name": "javascript",
          "type": "tree",
          "content": null
        },
        {
          "name": "ldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "lora_util.py",
          "type": "blob",
          "size": 18.708984375,
          "content": "'''\nBorrowed and modified from sd-scripts, publicly available at\nhttps://github.com/kohya-ss/sd-scripts/blob/main/library/model_util.py\n'''\n\nfrom diffusers import UNet2DConditionModel\n\n# Model paras of stable diffusion in diffUsers\nNUM_TRAIN_TIMESTEPS = 1000\nBETA_START = 0.00085\nBETA_END = 0.0120\n\nUNET_PARAMS_MODEL_CHANNELS = 320\nUNET_PARAMS_CHANNEL_MULT = [1, 2, 4, 4]\nUNET_PARAMS_ATTENTION_RESOLUTIONS = [4, 2, 1]\nUNET_PARAMS_IMAGE_SIZE = 64  # fixed from old invalid value `32`\nUNET_PARAMS_IN_CHANNELS = 4\nUNET_PARAMS_OUT_CHANNELS = 4\nUNET_PARAMS_NUM_RES_BLOCKS = 2\nUNET_PARAMS_CONTEXT_DIM = 768\nUNET_PARAMS_NUM_HEADS = 8\n# UNET_PARAMS_USE_LINEAR_PROJECTION = False\n\nVAE_PARAMS_Z_CHANNELS = 4\nVAE_PARAMS_RESOLUTION = 256\nVAE_PARAMS_IN_CHANNELS = 3\nVAE_PARAMS_OUT_CH = 3\nVAE_PARAMS_CH = 128\nVAE_PARAMS_CH_MULT = [1, 2, 4, 4]\nVAE_PARAMS_NUM_RES_BLOCKS = 2\n\n# V2\nV2_UNET_PARAMS_ATTENTION_HEAD_DIM = [5, 10, 20, 20]\nV2_UNET_PARAMS_CONTEXT_DIM = 1024\n# V2_UNET_PARAMS_USE_LINEAR_PROJECTION = True\n\n\ndef shave_segments(path, n_shave_prefix_segments=1):\n    \"\"\"\n    Removes segments. Positive values shave the first segments, negative shave the last segments.\n    \"\"\"\n    if n_shave_prefix_segments >= 0:\n        return \".\".join(path.split(\".\")[n_shave_prefix_segments:])\n    else:\n        return \".\".join(path.split(\".\")[:n_shave_prefix_segments])\n\n\ndef renew_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item.replace(\"in_layers.0\", \"norm1\")\n        new_item = new_item.replace(\"in_layers.2\", \"conv1\")\n\n        new_item = new_item.replace(\"out_layers.0\", \"norm2\")\n        new_item = new_item.replace(\"out_layers.3\", \"conv2\")\n\n        new_item = new_item.replace(\"emb_layers.1\", \"time_emb_proj\")\n        new_item = new_item.replace(\"skip_connection\", \"conv_shortcut\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping\n\n\ndef renew_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping\n\n\ndef assign_to_checkpoint(\n    paths, checkpoint, old_checkpoint, attention_paths_to_split=None, additional_replacements=None, config=None\n):\n    \"\"\"\n    This does the final conversion step: take locally converted weights and apply a global renaming\n    to them. It splits attention layers, and takes into account additional replacements\n    that may arise.\n\n    Assigns the weights to the new checkpoint.\n    \"\"\"\n    assert isinstance(paths, list), \"Paths should be a list of dicts containing 'old' and 'new' keys.\"\n\n    # Splits the attention layers into three variables.\n    if attention_paths_to_split is not None:\n        for path, path_map in attention_paths_to_split.items():\n            old_tensor = old_checkpoint[path]\n            channels = old_tensor.shape[0] // 3\n\n            target_shape = (-1, channels) if len(old_tensor.shape) == 3 else (-1)\n\n            num_heads = old_tensor.shape[0] // config[\"num_head_channels\"] // 3\n\n            old_tensor = old_tensor.reshape((num_heads, 3 * channels // num_heads) + old_tensor.shape[1:])\n            query, key, value = old_tensor.split(channels // num_heads, dim=1)\n\n            checkpoint[path_map[\"query\"]] = query.reshape(target_shape)\n            checkpoint[path_map[\"key\"]] = key.reshape(target_shape)\n            checkpoint[path_map[\"value\"]] = value.reshape(target_shape)\n\n    for path in paths:\n        new_path = path[\"new\"]\n\n        # These have already been assigned\n        if attention_paths_to_split is not None and new_path in attention_paths_to_split:\n            continue\n\n        # Global renaming happens here\n        new_path = new_path.replace(\"middle_block.0\", \"mid_block.resnets.0\")\n        new_path = new_path.replace(\"middle_block.1\", \"mid_block.attentions.0\")\n        new_path = new_path.replace(\"middle_block.2\", \"mid_block.resnets.1\")\n\n        if additional_replacements is not None:\n            for replacement in additional_replacements:\n                new_path = new_path.replace(replacement[\"old\"], replacement[\"new\"])\n\n        # proj_attn.weight has to be converted from conv 1D to linear\n        if \"proj_attn.weight\" in new_path:\n            checkpoint[new_path] = old_checkpoint[path[\"old\"]][:, :, 0]\n        else:\n            checkpoint[new_path] = old_checkpoint[path[\"old\"]]\n\n\ndef linear_transformer_to_conv(checkpoint):\n    keys = list(checkpoint.keys())\n    tf_keys = [\"proj_in.weight\", \"proj_out.weight\"]\n    for key in keys:\n        if \".\".join(key.split(\".\")[-2:]) in tf_keys:\n            if checkpoint[key].ndim == 2:\n                checkpoint[key] = checkpoint[key].unsqueeze(2).unsqueeze(2)\n\n\ndef conv_transformer_to_linear(checkpoint):\n    keys = list(checkpoint.keys())\n    tf_keys = [\"proj_in.weight\", \"proj_out.weight\"]\n    for key in keys:\n        if \".\".join(key.split(\".\")[-2:]) in tf_keys:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0, 0]\n\n\ndef create_unet_diffusers_config(v2, use_linear_projection_in_v2=False):\n    \"\"\"\n    Creates a config for the diffusers based on the config of the LDM model.\n    \"\"\"\n    # unet_params = original_config.model.params.unet_config.params\n\n    block_out_channels = [UNET_PARAMS_MODEL_CHANNELS * mult for mult in UNET_PARAMS_CHANNEL_MULT]\n\n    down_block_types = []\n    resolution = 1\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnDownBlock2D\" if resolution in UNET_PARAMS_ATTENTION_RESOLUTIONS else \"DownBlock2D\"\n        down_block_types.append(block_type)\n        if i != len(block_out_channels) - 1:\n            resolution *= 2\n\n    up_block_types = []\n    for i in range(len(block_out_channels)):\n        block_type = \"CrossAttnUpBlock2D\" if resolution in UNET_PARAMS_ATTENTION_RESOLUTIONS else \"UpBlock2D\"\n        up_block_types.append(block_type)\n        resolution //= 2\n\n    config = dict(\n        sample_size=UNET_PARAMS_IMAGE_SIZE,\n        in_channels=UNET_PARAMS_IN_CHANNELS,\n        out_channels=UNET_PARAMS_OUT_CHANNELS,\n        down_block_types=tuple(down_block_types),\n        up_block_types=tuple(up_block_types),\n        block_out_channels=tuple(block_out_channels),\n        layers_per_block=UNET_PARAMS_NUM_RES_BLOCKS,\n        cross_attention_dim=UNET_PARAMS_CONTEXT_DIM if not v2 else V2_UNET_PARAMS_CONTEXT_DIM,\n        attention_head_dim=UNET_PARAMS_NUM_HEADS if not v2 else V2_UNET_PARAMS_ATTENTION_HEAD_DIM,\n        # use_linear_projection=UNET_PARAMS_USE_LINEAR_PROJECTION if not v2 else V2_UNET_PARAMS_USE_LINEAR_PROJECTION,\n    )\n    if v2 and use_linear_projection_in_v2:\n        config[\"use_linear_projection\"] = True\n\n    return config\n\n\ndef convert_ldm_unet_checkpoint(v2, checkpoint, config):\n    \"\"\"\n    Takes a state dict and a config, and returns a converted checkpoint.\n    \"\"\"\n\n    # extract state_dict for UNet\n    unet_state_dict = {}\n    unet_key = \"model.diffusion_model.\"\n    keys = list(checkpoint.keys())\n    for key in keys:\n        if key.startswith(unet_key):\n            unet_state_dict[key.replace(unet_key, \"\")] = checkpoint.pop(key)\n\n    new_checkpoint = {}\n\n    new_checkpoint[\"time_embedding.linear_1.weight\"] = unet_state_dict[\"time_embed.0.weight\"]\n    new_checkpoint[\"time_embedding.linear_1.bias\"] = unet_state_dict[\"time_embed.0.bias\"]\n    new_checkpoint[\"time_embedding.linear_2.weight\"] = unet_state_dict[\"time_embed.2.weight\"]\n    new_checkpoint[\"time_embedding.linear_2.bias\"] = unet_state_dict[\"time_embed.2.bias\"]\n\n    new_checkpoint[\"conv_in.weight\"] = unet_state_dict[\"input_blocks.0.0.weight\"]\n    new_checkpoint[\"conv_in.bias\"] = unet_state_dict[\"input_blocks.0.0.bias\"]\n\n    new_checkpoint[\"conv_norm_out.weight\"] = unet_state_dict[\"out.0.weight\"]\n    new_checkpoint[\"conv_norm_out.bias\"] = unet_state_dict[\"out.0.bias\"]\n    new_checkpoint[\"conv_out.weight\"] = unet_state_dict[\"out.2.weight\"]\n    new_checkpoint[\"conv_out.bias\"] = unet_state_dict[\"out.2.bias\"]\n\n    # Retrieves the keys for the input blocks only\n    num_input_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"input_blocks\" in layer})\n    input_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"input_blocks.{layer_id}.\" in key] for layer_id in range(num_input_blocks)\n    }\n\n    # Retrieves the keys for the middle blocks only\n    num_middle_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"middle_block\" in layer})\n    middle_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"middle_block.{layer_id}.\" in key] for layer_id in range(num_middle_blocks)\n    }\n\n    # Retrieves the keys for the output blocks only\n    num_output_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"output_blocks\" in layer})\n    output_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"output_blocks.{layer_id}.\" in key] for layer_id in range(num_output_blocks)\n    }\n\n    for i in range(1, num_input_blocks):\n        block_id = (i - 1) // (config[\"layers_per_block\"] + 1)\n        layer_in_block_id = (i - 1) % (config[\"layers_per_block\"] + 1)\n\n        resnets = [key for key in input_blocks[i] if f\"input_blocks.{i}.0\" in key and f\"input_blocks.{i}.0.op\" not in key]\n        attentions = [key for key in input_blocks[i] if f\"input_blocks.{i}.1\" in key]\n\n        if f\"input_blocks.{i}.0.op.weight\" in unet_state_dict:\n            new_checkpoint[f\"down_blocks.{block_id}.downsamplers.0.conv.weight\"] = unet_state_dict.pop(\n                f\"input_blocks.{i}.0.op.weight\"\n            )\n            new_checkpoint[f\"down_blocks.{block_id}.downsamplers.0.conv.bias\"] = unet_state_dict.pop(f\"input_blocks.{i}.0.op.bias\")\n\n        paths = renew_resnet_paths(resnets)\n        meta_path = {\"old\": f\"input_blocks.{i}.0\", \"new\": f\"down_blocks.{block_id}.resnets.{layer_in_block_id}\"}\n        assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)\n\n        if len(attentions):\n            paths = renew_attention_paths(attentions)\n            meta_path = {\"old\": f\"input_blocks.{i}.1\", \"new\": f\"down_blocks.{block_id}.attentions.{layer_in_block_id}\"}\n            assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)\n\n    resnet_0 = middle_blocks[0]\n    attentions = middle_blocks[1]\n    resnet_1 = middle_blocks[2]\n\n    resnet_0_paths = renew_resnet_paths(resnet_0)\n    assign_to_checkpoint(resnet_0_paths, new_checkpoint, unet_state_dict, config=config)\n\n    resnet_1_paths = renew_resnet_paths(resnet_1)\n    assign_to_checkpoint(resnet_1_paths, new_checkpoint, unet_state_dict, config=config)\n\n    attentions_paths = renew_attention_paths(attentions)\n    meta_path = {\"old\": \"middle_block.1\", \"new\": \"mid_block.attentions.0\"}\n    assign_to_checkpoint(attentions_paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)\n\n    for i in range(num_output_blocks):\n        block_id = i // (config[\"layers_per_block\"] + 1)\n        layer_in_block_id = i % (config[\"layers_per_block\"] + 1)\n        output_block_layers = [shave_segments(name, 2) for name in output_blocks[i]]\n        output_block_list = {}\n\n        for layer in output_block_layers:\n            layer_id, layer_name = layer.split(\".\")[0], shave_segments(layer, 1)\n            if layer_id in output_block_list:\n                output_block_list[layer_id].append(layer_name)\n            else:\n                output_block_list[layer_id] = [layer_name]\n\n        if len(output_block_list) > 1:\n            resnets = [key for key in output_blocks[i] if f\"output_blocks.{i}.0\" in key]\n            attentions = [key for key in output_blocks[i] if f\"output_blocks.{i}.1\" in key]\n\n            resnet_0_paths = renew_resnet_paths(resnets)\n            paths = renew_resnet_paths(resnets)\n\n            meta_path = {\"old\": f\"output_blocks.{i}.0\", \"new\": f\"up_blocks.{block_id}.resnets.{layer_in_block_id}\"}\n            assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)\n\n            # „Ç™„É™„Ç∏„Éä„É´Ôºö\n            # if [\"conv.weight\", \"conv.bias\"] in output_block_list.values():\n            #   index = list(output_block_list.values()).index([\"conv.weight\", \"conv.bias\"])\n\n            # bias„Å®weight„ÅÆÈ†ÜÁï™„Å´‰æùÂ≠ò„Åó„Å™„ÅÑ„Çà„ÅÜ„Å´„Åô„ÇãÔºö„ÇÇ„Å£„Å®„ÅÑ„ÅÑ„ÇÑ„ÇäÊñπ„Åå„ÅÇ„Çä„Åù„ÅÜ„Å†„Åå\n            for l in output_block_list.values():\n                l.sort()\n\n            if [\"conv.bias\", \"conv.weight\"] in output_block_list.values():\n                index = list(output_block_list.values()).index([\"conv.bias\", \"conv.weight\"])\n                new_checkpoint[f\"up_blocks.{block_id}.upsamplers.0.conv.bias\"] = unet_state_dict[\n                    f\"output_blocks.{i}.{index}.conv.bias\"\n                ]\n                new_checkpoint[f\"up_blocks.{block_id}.upsamplers.0.conv.weight\"] = unet_state_dict[\n                    f\"output_blocks.{i}.{index}.conv.weight\"\n                ]\n\n                # Clear attentions as they have been attributed above.\n                if len(attentions) == 2:\n                    attentions = []\n\n            if len(attentions):\n                paths = renew_attention_paths(attentions)\n                meta_path = {\n                    \"old\": f\"output_blocks.{i}.1\",\n                    \"new\": f\"up_blocks.{block_id}.attentions.{layer_in_block_id}\",\n                }\n                assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)\n        else:\n            resnet_0_paths = renew_resnet_paths(output_block_layers, n_shave_prefix_segments=1)\n            for path in resnet_0_paths:\n                old_path = \".\".join([\"output_blocks\", str(i), path[\"old\"]])\n                new_path = \".\".join([\"up_blocks\", str(block_id), \"resnets\", str(layer_in_block_id), path[\"new\"]])\n\n                new_checkpoint[new_path] = unet_state_dict[old_path]\n\n    # SD„ÅÆv2„Åß„ÅØ1*1„ÅÆconv2d„Åålinear„Å´Â§â„Çè„Å£„Å¶„ÅÑ„Çã\n    # Ë™§„Å£„Å¶ Diffusers ÂÅ¥„Çí conv2d „ÅÆ„Åæ„Åæ„Å´„Åó„Å¶„Åó„Åæ„Å£„Åü„ÅÆ„Åß„ÄÅÂ§âÊèõÂøÖË¶Å\n    if v2 and not config.get('use_linear_projection', False):\n        linear_transformer_to_conv(new_checkpoint)\n\n    return new_checkpoint\n\n\ndef convert_unet_state_dict_to_sd(unet_state_dict, v2=False):\n    unet_conversion_map = [\n        # (stable-diffusion, HF Diffusers)\n        (\"time_embed.0.weight\", \"time_embedding.linear_1.weight\"),\n        (\"time_embed.0.bias\", \"time_embedding.linear_1.bias\"),\n        (\"time_embed.2.weight\", \"time_embedding.linear_2.weight\"),\n        (\"time_embed.2.bias\", \"time_embedding.linear_2.bias\"),\n        (\"input_blocks.0.0.weight\", \"conv_in.weight\"),\n        (\"input_blocks.0.0.bias\", \"conv_in.bias\"),\n        (\"out.0.weight\", \"conv_norm_out.weight\"),\n        (\"out.0.bias\", \"conv_norm_out.bias\"),\n        (\"out.2.weight\", \"conv_out.weight\"),\n        (\"out.2.bias\", \"conv_out.bias\"),\n    ]\n\n    unet_conversion_map_resnet = [\n        # (stable-diffusion, HF Diffusers)\n        (\"in_layers.0\", \"norm1\"),\n        (\"in_layers.2\", \"conv1\"),\n        (\"out_layers.0\", \"norm2\"),\n        (\"out_layers.3\", \"conv2\"),\n        (\"emb_layers.1\", \"time_emb_proj\"),\n        (\"skip_connection\", \"conv_shortcut\"),\n    ]\n\n    unet_conversion_map_layer = []\n    for i in range(4):\n        # loop over downblocks/upblocks\n\n        for j in range(2):\n            # loop over resnets/attentions for downblocks\n            hf_down_res_prefix = f\"down_blocks.{i}.resnets.{j}.\"\n            sd_down_res_prefix = f\"input_blocks.{3*i + j + 1}.0.\"\n            unet_conversion_map_layer.append((sd_down_res_prefix, hf_down_res_prefix))\n\n            if i < 3:\n                # no attention layers in down_blocks.3\n                hf_down_atn_prefix = f\"down_blocks.{i}.attentions.{j}.\"\n                sd_down_atn_prefix = f\"input_blocks.{3*i + j + 1}.1.\"\n                unet_conversion_map_layer.append((sd_down_atn_prefix, hf_down_atn_prefix))\n\n        for j in range(3):\n            # loop over resnets/attentions for upblocks\n            hf_up_res_prefix = f\"up_blocks.{i}.resnets.{j}.\"\n            sd_up_res_prefix = f\"output_blocks.{3*i + j}.0.\"\n            unet_conversion_map_layer.append((sd_up_res_prefix, hf_up_res_prefix))\n\n            if i > 0:\n                # no attention layers in up_blocks.0\n                hf_up_atn_prefix = f\"up_blocks.{i}.attentions.{j}.\"\n                sd_up_atn_prefix = f\"output_blocks.{3*i + j}.1.\"\n                unet_conversion_map_layer.append((sd_up_atn_prefix, hf_up_atn_prefix))\n\n        if i < 3:\n            # no downsample in down_blocks.3\n            hf_downsample_prefix = f\"down_blocks.{i}.downsamplers.0.conv.\"\n            sd_downsample_prefix = f\"input_blocks.{3*(i+1)}.0.op.\"\n            unet_conversion_map_layer.append((sd_downsample_prefix, hf_downsample_prefix))\n\n            # no upsample in up_blocks.3\n            hf_upsample_prefix = f\"up_blocks.{i}.upsamplers.0.\"\n            sd_upsample_prefix = f\"output_blocks.{3*i + 2}.{1 if i == 0 else 2}.\"\n            unet_conversion_map_layer.append((sd_upsample_prefix, hf_upsample_prefix))\n\n    hf_mid_atn_prefix = \"mid_block.attentions.0.\"\n    sd_mid_atn_prefix = \"middle_block.1.\"\n    unet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))\n\n    for j in range(2):\n        hf_mid_res_prefix = f\"mid_block.resnets.{j}.\"\n        sd_mid_res_prefix = f\"middle_block.{2*j}.\"\n        unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))\n\n    # buyer beware: this is a *brittle* function,\n    # and correct output requires that all of these pieces interact in\n    # the exact order in which I have arranged them.\n    mapping = {k: k for k in unet_state_dict.keys()}\n    for sd_name, hf_name in unet_conversion_map:\n        mapping[hf_name] = sd_name\n    for k, v in mapping.items():\n        if \"resnets\" in k:\n            for sd_part, hf_part in unet_conversion_map_resnet:\n                v = v.replace(hf_part, sd_part)\n            mapping[k] = v\n    for k, v in mapping.items():\n        for sd_part, hf_part in unet_conversion_map_layer:\n            v = v.replace(hf_part, sd_part)\n        mapping[k] = v\n    new_state_dict = {v: unet_state_dict[k] for k, v in mapping.items()}\n\n    if v2:\n        conv_transformer_to_linear(new_state_dict)\n\n    return new_state_dict\n\n\ndef get_diffusers_unet(unet=None, state_dict=None, v2=False):\n    unet_config = create_unet_diffusers_config(v2, use_linear_projection_in_v2=False)\n    if unet is None:\n        unet = UNet2DConditionModel(**unet_config).to(\"cpu\")\n    if state_dict:\n        converted_unet_checkpoint = convert_ldm_unet_checkpoint(v2, state_dict, unet_config)\n        info = unet.load_state_dict(converted_unet_checkpoint)\n        print(\"loading diffusers u-net:\", info)\n    return unet\n"
        },
        {
          "name": "models_yaml",
          "type": "tree",
          "content": null
        },
        {
          "name": "ocr_recog",
          "type": "tree",
          "content": null
        },
        {
          "name": "ocr_weights",
          "type": "tree",
          "content": null
        },
        {
          "name": "style.css",
          "type": "blob",
          "size": 0.7333984375,
          "content": "#banner {\n    max-width: 400px;\n    margin: auto;\n    box-shadow: 0 2px 20px rgba(0, 0, 0, 0.5) !important;\n    border-radius: 20px;\n}\n\n.run {\n    background-color: #624AFF !important;\n    color: #FFFFFF !important;\n    border-radius: 2px !important;\n    box-shadow: 0 3px 5px rgba(0, 0, 0, 0.5) !important;\n}\n.run:active {\n    background-color: #d96565 !important; \n}\n.run:hover {\n    background-color: #a079f5 !important; \n}\n/* tab button style */\nbutton.svelte-kqij2n {\n    margin-bottom: -1px;\n    border: 1px solid transparent;\n    border-color: transparent;\n    border-bottom: none;\n    color: #9CA3AF !important;\n    font-size: 16px;\n}\nbutton.selected.svelte-kqij2n {\n    background: #ddd8f9 !important;\n    color: rgb(62, 7, 240) !important;\n}"
        },
        {
          "name": "t3_dataset.py",
          "type": "blob",
          "size": 17.265625,
          "content": "import os\nimport numpy as np\nimport cv2\nimport random\nimport math\nimport time\nfrom PIL import Image, ImageDraw, ImageFont\nfrom torch.utils.data import Dataset, DataLoader\nfrom dataset_util import load, show_bbox_on_image\n\n\nphrase_list = [\n    ', content and position of the texts are ',\n    ', textual material depicted in the image are ',\n    ', texts that says ',\n    ', captions shown in the snapshot are ',\n    ', with the words of ',\n    ', that reads ',\n    ', the written materials on the picture: ',\n    ', these texts are written on it: ',\n    ', captions are ',\n    ', content of the text in the graphic is '\n]\n\n\ndef insert_spaces(string, nSpace):\n    if nSpace == 0:\n        return string\n    new_string = \"\"\n    for char in string:\n        new_string += char + \" \" * nSpace\n    return new_string[:-nSpace]\n\n\ndef draw_glyph(font, text):\n    g_size = 50\n    W, H = (512, 80)\n    new_font = font.font_variant(size=g_size)\n    img = Image.new(mode='1', size=(W, H), color=0)\n    draw = ImageDraw.Draw(img)\n    left, top, right, bottom = new_font.getbbox(text)\n    text_width = max(right-left, 5)\n    text_height = max(bottom - top, 5)\n    ratio = min(W*0.9/text_width, H*0.9/text_height)\n    new_font = font.font_variant(size=int(g_size*ratio))\n\n    text_width, text_height = new_font.getsize(text)\n    offset_x, offset_y = new_font.getoffset(text)\n    x = (img.width - text_width) // 2\n    y = (img.height - text_height) // 2 - offset_y//2\n    draw.text((x, y), text, font=new_font, fill='white')\n    img = np.expand_dims(np.array(img), axis=2).astype(np.float64)\n    return img\n\n\ndef draw_glyph2(font, text, polygon, vertAng=10, scale=1, width=512, height=512, add_space=True):\n    enlarge_polygon = polygon*scale\n    rect = cv2.minAreaRect(enlarge_polygon)\n    box = cv2.boxPoints(rect)\n    box = np.int0(box)\n    w, h = rect[1]\n    angle = rect[2]\n    if angle < -45:\n        angle += 90\n    angle = -angle\n    if w < h:\n        angle += 90\n\n    vert = False\n    if (abs(angle) % 90 < vertAng or abs(90-abs(angle) % 90) % 90 < vertAng):\n        _w = max(box[:, 0]) - min(box[:, 0])\n        _h = max(box[:, 1]) - min(box[:, 1])\n        if _h >= _w:\n            vert = True\n            angle = 0\n\n    img = np.zeros((height*scale, width*scale, 3), np.uint8)\n    img = Image.fromarray(img)\n\n    # infer font size\n    image4ratio = Image.new(\"RGB\", img.size, \"white\")\n    draw = ImageDraw.Draw(image4ratio)\n    _, _, _tw, _th = draw.textbbox(xy=(0, 0), text=text, font=font)\n    text_w = min(w, h) * (_tw / _th)\n    if text_w <= max(w, h):\n        # add space\n        if len(text) > 1 and not vert and add_space:\n            for i in range(1, 100):\n                text_space = insert_spaces(text, i)\n                _, _, _tw2, _th2 = draw.textbbox(xy=(0, 0), text=text_space, font=font)\n                if min(w, h) * (_tw2 / _th2) > max(w, h):\n                    break\n            text = insert_spaces(text, i-1)\n        font_size = min(w, h)*0.80\n    else:\n        shrink = 0.75 if vert else 0.85\n        font_size = min(w, h) / (text_w/max(w, h)) * shrink\n    new_font = font.font_variant(size=int(font_size))\n\n    left, top, right, bottom = new_font.getbbox(text)\n    text_width = right-left\n    text_height = bottom - top\n\n    layer = Image.new('RGBA', img.size, (0, 0, 0, 0))\n    draw = ImageDraw.Draw(layer)\n    if not vert:\n        draw.text((rect[0][0]-text_width//2, rect[0][1]-text_height//2-top), text, font=new_font, fill=(255, 255, 255, 255))\n    else:\n        x_s = min(box[:, 0]) + _w//2 - text_height//2\n        y_s = min(box[:, 1])\n        for c in text:\n            draw.text((x_s, y_s), c, font=new_font, fill=(255, 255, 255, 255))\n            _, _t, _, _b = new_font.getbbox(c)\n            y_s += _b\n\n    rotated_layer = layer.rotate(angle, expand=1, center=(rect[0][0], rect[0][1]))\n\n    x_offset = int((img.width - rotated_layer.width) / 2)\n    y_offset = int((img.height - rotated_layer.height) / 2)\n    img.paste(rotated_layer, (x_offset, y_offset), rotated_layer)\n    img = np.expand_dims(np.array(img.convert('1')), axis=2).astype(np.float64)\n    return img\n\n\ndef get_caption_pos(ori_caption, pos_idxs, prob=1.0, place_holder='*'):\n    idx2pos = {\n        0: \" top left\",\n        1: \" top\",\n        2: \" top right\",\n        3: \" left\",\n        4: random.choice([\" middle\", \" center\"]),\n        5: \" right\",\n        6: \" bottom left\",\n        7: \" bottom\",\n        8: \" bottom right\"\n    }\n    new_caption = ori_caption + random.choice(phrase_list)\n    pos = ''\n    for i in range(len(pos_idxs)):\n        if random.random() < prob and pos_idxs[i] > 0:\n            pos += place_holder + random.choice([' located', ' placed', ' positioned', '']) + random.choice([' at', ' in', ' on']) + idx2pos[pos_idxs[i]] + ', '\n        else:\n            pos += place_holder + ' , '\n    pos = pos[:-2] + '.'\n    new_caption += pos\n    return new_caption\n\n\ndef generate_random_rectangles(w, h, box_num):\n    rectangles = []\n    for i in range(box_num):\n        x = random.randint(0, w)\n        y = random.randint(0, h)\n        w = random.randint(16, 256)\n        h = random.randint(16, 96)\n        angle = random.randint(-45, 45)\n        p1 = (x, y)\n        p2 = (x + w, y)\n        p3 = (x + w, y + h)\n        p4 = (x, y + h)\n        center = ((x + x + w) / 2, (y + y + h) / 2)\n        p1 = rotate_point(p1, center, angle)\n        p2 = rotate_point(p2, center, angle)\n        p3 = rotate_point(p3, center, angle)\n        p4 = rotate_point(p4, center, angle)\n        rectangles.append((p1, p2, p3, p4))\n    return rectangles\n\n\ndef rotate_point(point, center, angle):\n    # rotation\n    angle = math.radians(angle)\n    x = point[0] - center[0]\n    y = point[1] - center[1]\n    x1 = x * math.cos(angle) - y * math.sin(angle)\n    y1 = x * math.sin(angle) + y * math.cos(angle)\n    x1 += center[0]\n    y1 += center[1]\n    return int(x1), int(y1)\n\n\nclass T3DataSet(Dataset):\n    def __init__(\n            self,\n            json_path,\n            max_lines=5,\n            max_chars=20,\n            place_holder='*',\n            font_path='./font/Arial_Unicode.ttf',\n            caption_pos_prob=1.0,\n            mask_pos_prob=1.0,\n            mask_img_prob=0.5,\n            for_show=False,\n            using_dlc=False,\n            glyph_scale=1,\n            percent=1.0,\n            debug=False,\n            wm_thresh=1.0,\n            ):\n        assert isinstance(json_path, (str, list))\n        if isinstance(json_path, str):\n            json_path = [json_path]\n        data_list = []\n        self.using_dlc = using_dlc\n        self.max_lines = max_lines\n        self.max_chars = max_chars\n        self.place_holder = place_holder\n        self.font = ImageFont.truetype(font_path, size=60)\n        self.caption_pos_porb = caption_pos_prob\n        self.mask_pos_prob = mask_pos_prob\n        self.mask_img_prob = mask_img_prob\n        self.for_show = for_show\n        self.glyph_scale = glyph_scale\n        self.wm_thresh = wm_thresh\n        for jp in json_path:\n            data_list += self.load_data(jp, percent)\n        self.data_list = data_list\n        print(f'All dataset loaded, imgs={len(self.data_list)}')\n        self.debug = debug\n        if self.debug:\n            self.tmp_items = [i for i in range(100)]\n\n    def load_data(self, json_path, percent):\n        tic = time.time()\n        content = load(json_path)\n        d = []\n        count = 0\n        wm_skip = 0\n        max_img = len(content['data_list']) * percent\n        for gt in content['data_list']:\n            if len(d) > max_img:\n                break\n            if 'wm_score' in gt and gt['wm_score'] > self.wm_thresh:  # wm_score > thresh will be skiped as an img with watermark\n                wm_skip += 1\n                continue\n            data_root = content['data_root']\n            if self.using_dlc:\n                data_root = data_root.replace('/data/vdb', '/mnt/data', 1)\n            img_path = os.path.join(data_root, gt['img_name'])\n            info = {}\n            info['img_path'] = img_path\n            info['caption'] = gt['caption'] if 'caption' in gt else ''\n            if self.place_holder in info['caption']:\n                count += 1\n                info['caption'] = info['caption'].replace(self.place_holder, \" \")\n            if 'annotations' in gt:\n                polygons = []\n                invalid_polygons = []\n                texts = []\n                languages = []\n                pos = []\n                for annotation in gt['annotations']:\n                    if len(annotation['polygon']) == 0:\n                        continue\n                    if 'valid' in annotation and annotation['valid'] is False:\n                        invalid_polygons.append(annotation['polygon'])\n                        continue\n                    polygons.append(annotation['polygon'])\n                    texts.append(annotation['text'])\n                    languages.append(annotation['language'])\n                    if 'pos' in annotation:\n                        pos.append(annotation['pos'])\n                info['polygons'] = [np.array(i) for i in polygons]\n                info['invalid_polygons'] = [np.array(i) for i in invalid_polygons]\n                info['texts'] = texts\n                info['language'] = languages\n                info['pos'] = pos\n            d.append(info)\n        print(f'{json_path} loaded, imgs={len(d)}, wm_skip={wm_skip}, time={(time.time()-tic):.2f}s')\n        if count > 0:\n            print(f\"Found {count} image's caption contain placeholder: {self.place_holder}, change to ' '...\")\n        return d\n\n    def __getitem__(self, item):\n        item_dict = {}\n        if self.debug:  # sample fixed items\n            item = self.tmp_items.pop()\n            print(f'item = {item}')\n        cur_item = self.data_list[item]\n        # img\n        target = np.array(Image.open(cur_item['img_path']).convert('RGB'))\n        if target.shape[0] != 512 or target.shape[1] != 512:\n            target = cv2.resize(target, (512, 512))\n        target = (target.astype(np.float32) / 127.5) - 1.0\n        item_dict['img'] = target\n        # caption\n        item_dict['caption'] = cur_item['caption']\n        item_dict['glyphs'] = []\n        item_dict['gly_line'] = []\n        item_dict['positions'] = []\n        item_dict['texts'] = []\n        item_dict['language'] = []\n        item_dict['inv_mask'] = []\n        texts = cur_item.get('texts', [])\n        if len(texts) > 0:\n            idxs = [i for i in range(len(texts))]\n            if len(texts) > self.max_lines:\n                sel_idxs = random.sample(idxs, self.max_lines)\n                unsel_idxs = [i for i in idxs if i not in sel_idxs]\n            else:\n                sel_idxs = idxs\n                unsel_idxs = []\n            if len(cur_item['pos']) > 0:\n                pos_idxs = [cur_item['pos'][i] for i in sel_idxs]\n            else:\n                pos_idxs = [-1 for i in sel_idxs]\n            item_dict['caption'] = get_caption_pos(item_dict['caption'], pos_idxs, self.caption_pos_porb, self.place_holder)\n            item_dict['polygons'] = [cur_item['polygons'][i] for i in sel_idxs]\n            item_dict['texts'] = [cur_item['texts'][i][:self.max_chars] for i in sel_idxs]\n            item_dict['language'] = [cur_item['language'][i] for i in sel_idxs]\n            # glyphs\n            for idx, text in enumerate(item_dict['texts']):\n                gly_line = draw_glyph(self.font, text)\n                glyphs = draw_glyph2(self.font, text, item_dict['polygons'][idx], scale=self.glyph_scale)\n                item_dict['glyphs'] += [glyphs]\n                item_dict['gly_line'] += [gly_line]\n            # mask_pos\n            for polygon in item_dict['polygons']:\n                item_dict['positions'] += [self.draw_pos(polygon, self.mask_pos_prob)]\n        # inv_mask\n        invalid_polygons = cur_item['invalid_polygons'] if 'invalid_polygons' in cur_item else []\n        if len(texts) > 0:\n            invalid_polygons += [cur_item['polygons'][i] for i in unsel_idxs]\n        item_dict['inv_mask'] = self.draw_inv_mask(invalid_polygons)\n        item_dict['hint'] = self.get_hint(item_dict['positions'])\n        if random.random() < self.mask_img_prob:\n            # randomly generate 0~3 masks\n            box_num = random.randint(0, 3)\n            boxes = generate_random_rectangles(512, 512, box_num)\n            boxes = np.array(boxes)\n            pos_list = item_dict['positions'].copy()\n            for i in range(box_num):\n                pos_list += [self.draw_pos(boxes[i], self.mask_pos_prob)]\n            mask = self.get_hint(pos_list)\n            masked_img = target*(1-mask)\n        else:\n            masked_img = np.zeros_like(target)\n        item_dict['masked_img'] = masked_img\n\n        if self.for_show:\n            item_dict['img_name'] = os.path.split(cur_item['img_path'])[-1]\n            return item_dict\n        if len(texts) > 0:\n            del item_dict['polygons']\n        # padding\n        n_lines = min(len(texts), self.max_lines)\n        item_dict['n_lines'] = n_lines\n        n_pad = self.max_lines - n_lines\n        if n_pad > 0:\n            item_dict['glyphs'] += [np.zeros((512*self.glyph_scale, 512*self.glyph_scale, 1))] * n_pad\n            item_dict['gly_line'] += [np.zeros((80, 512, 1))] * n_pad\n            item_dict['positions'] += [np.zeros((512, 512, 1))] * n_pad\n            item_dict['texts'] += [' '] * n_pad\n            item_dict['language'] += [' '] * n_pad\n\n        return item_dict\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def draw_inv_mask(self, polygons):\n        img = np.zeros((512, 512))\n        for p in polygons:\n            pts = p.reshape((-1, 1, 2))\n            cv2.fillPoly(img, [pts], color=255)\n        img = img[..., None]\n        return img/255.\n\n    def draw_pos(self, ploygon, prob=1.0):\n        img = np.zeros((512, 512))\n        rect = cv2.minAreaRect(ploygon)\n        w, h = rect[1]\n        small = False\n        if w < 20 or h < 20:\n            small = True\n        if random.random() < prob:\n            pts = ploygon.reshape((-1, 1, 2))\n            cv2.fillPoly(img, [pts], color=255)\n            # 10% dilate / 10% erode / 5% dilatex2  5% erodex2\n            random_value = random.random()\n            kernel = np.ones((3, 3), dtype=np.uint8)\n            if random_value < 0.7:\n                pass\n            elif random_value < 0.8:\n                img = cv2.dilate(img.astype(np.uint8), kernel, iterations=1)\n            elif random_value < 0.9 and not small:\n                img = cv2.erode(img.astype(np.uint8), kernel, iterations=1)\n            elif random_value < 0.95:\n                img = cv2.dilate(img.astype(np.uint8), kernel, iterations=2)\n            elif random_value < 1.0 and not small:\n                img = cv2.erode(img.astype(np.uint8), kernel, iterations=2)\n        img = img[..., None]\n        return img/255.\n\n    def get_hint(self, positions):\n        if len(positions) == 0:\n            return np.zeros((512, 512, 1))\n        return np.sum(positions, axis=0).clip(0, 1)\n\n\nif __name__ == '__main__':\n    '''\n    Run this script to show details of your dataset, such as ocr annotations, glyphs, prompts, etc.\n    '''\n    from tqdm import tqdm\n    from matplotlib import pyplot as plt\n    import shutil\n\n    show_imgs_dir = 'show_results'\n    show_count = 50\n    if os.path.exists(show_imgs_dir):\n        shutil.rmtree(show_imgs_dir)\n    os.makedirs(show_imgs_dir)\n    plt.rcParams['axes.unicode_minus'] = False\n    json_paths = [\n        '/path/of/your/dataset/data1.json',\n        '/path/of/your/dataset/data2.json',\n        # ...\n    ]\n\n    dataset = T3DataSet(json_paths, for_show=True, max_lines=20, glyph_scale=2, mask_img_prob=1.0, caption_pos_prob=0.0)\n    train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False, num_workers=0)\n    pbar = tqdm(total=show_count)\n    for i, data in enumerate(train_loader):\n        if i == show_count:\n            break\n        img = ((data['img'][0].numpy() + 1.0) / 2.0 * 255).astype(np.uint8)\n        masked_img = ((data['masked_img'][0].numpy() + 1.0) / 2.0 * 255)[..., ::-1].astype(np.uint8)\n        cv2.imwrite(os.path.join(show_imgs_dir, f'plots_{i}_masked.jpg'), masked_img)\n        if 'texts' in data and len(data['texts']) > 0:\n            texts = [x[0] for x in data['texts']]\n            img = show_bbox_on_image(Image.fromarray(img), data['polygons'], texts)\n        cv2.imwrite(os.path.join(show_imgs_dir, f'plots_{i}.jpg'),  np.array(img)[..., ::-1])\n        with open(os.path.join(show_imgs_dir, f'plots_{i}.txt'), 'w') as fin:\n            fin.writelines([data['caption'][0]])\n        all_glyphs = []\n        for k, glyphs in enumerate(data['glyphs']):\n            cv2.imwrite(os.path.join(show_imgs_dir, f'plots_{i}_glyph_{k}.jpg'), glyphs[0].numpy().astype(np.int32)*255)\n            all_glyphs += [glyphs[0].numpy().astype(np.int32)*255]\n        cv2.imwrite(os.path.join(show_imgs_dir, f'plots_{i}_allglyphs.jpg'), np.sum(all_glyphs, axis=0))\n        for k, gly_line in enumerate(data['gly_line']):\n            cv2.imwrite(os.path.join(show_imgs_dir, f'plots_{i}_gly_line_{k}.jpg'), gly_line[0].numpy().astype(np.int32)*255)\n        for k, position in enumerate(data['positions']):\n            if position is not None:\n                cv2.imwrite(os.path.join(show_imgs_dir, f'plots_{i}_pos_{k}.jpg'), position[0].numpy().astype(np.int32)*255)\n        cv2.imwrite(os.path.join(show_imgs_dir, f'plots_{i}_hint.jpg'), data['hint'][0].numpy().astype(np.int32)*255)\n        cv2.imwrite(os.path.join(show_imgs_dir, f'plots_{i}_inv_mask.jpg'), np.array(img)[..., ::-1]*(1-data['inv_mask'][0].numpy().astype(np.int32)))\n        pbar.update(1)\n    pbar.close()\n"
        },
        {
          "name": "tool_add_anytext.py",
          "type": "blob",
          "size": 1.974609375,
          "content": "'''\nAnyText: Multilingual Visual Text Generation And Editing\nPaper: https://arxiv.org/abs/2311.03054\nCode: https://github.com/tyxsspa/AnyText\nCopyright (c) Alibaba, Inc. and its affiliates.\n'''\nimport sys\nimport os\nimport torch\nfrom cldm.model import create_model\n\nadd_ocr = True  # merge OCR model\nocr_path = './ocr_weights/ppv3_rec.pth'\n\n\nif len(sys.argv) == 3:\n    input_path = sys.argv[1]\n    output_path = sys.argv[2]\nelse:\n    print('Args are wrong, using default input and output path!')\n    input_path = './models/v1-5-pruned.ckpt'  # sd1.5\n    output_path = './models/anytext_sd15_scratch.ckpt'\n\nassert os.path.exists(input_path), 'Input model does not exist.'\nassert os.path.exists(os.path.dirname(output_path)), 'Output path is not valid.'\n\n\ndef get_node_name(name, parent_name):\n    if len(name) <= len(parent_name):\n        return False, ''\n    p = name[:len(parent_name)]\n    if p != parent_name:\n        return False, ''\n    return True, name[len(parent_name):]\n\n\nmodel = create_model(config_path='./models_yaml/anytext_sd15.yaml')\n\npretrained_weights = torch.load(input_path)\nif 'state_dict' in pretrained_weights:\n    pretrained_weights = pretrained_weights['state_dict']\n\nscratch_dict = model.state_dict()\n\ntarget_dict = {}\nfor k in scratch_dict.keys():\n    is_control, name = get_node_name(k, 'control_')\n    if is_control:\n        copy_k = 'model.diffusion_' + name\n    else:\n        copy_k = k\n    if copy_k in pretrained_weights:\n        target_dict[k] = pretrained_weights[copy_k].clone()\n    else:\n        target_dict[k] = scratch_dict[k].clone()\n        print(f'These weights are newly added: {k}')\n\nif add_ocr:\n    ocr_weights = torch.load(ocr_path)\n    if 'state_dict' in ocr_weights:\n        ocr_weights = ocr_weights['state_dict']\n    for key in ocr_weights:\n        new_key = 'text_predictor.' + key\n        target_dict[new_key] = ocr_weights[key]\n    print('ocr weights are added!')\n\nmodel.load_state_dict(target_dict, strict=True)\ntorch.save(model.state_dict(), output_path)\nprint('Done.')\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.6328125,
          "content": "import os\n\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nfrom t3_dataset import T3DataSet\nfrom cldm.logger import ImageLogger\nfrom cldm.model import create_model, load_state_dict\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport shutil\n\nNUM_NODES = 1\n# Configs\nbatch_size = 6  # default 6\ngrad_accum = 1  # enable perceptual loss may cost a lot of VRAM, you can set a smaller batch_size and make sure grad_accum * batch_size = 6\nckpt_path = None  # if not None, load ckpt_path and continue training task, will not load \"resume_path\"\nresume_path = './models/anytext_sd15_scratch.ckpt'  # finetune from scratch\nmodel_config = './models_yaml/anytext_sd15.yaml'  # use anytext_sd15_perloss.yaml to enable perceptual loss\nlogger_freq = 1000\nlearning_rate = 2e-5  # default 2e-5\nmask_ratio = 0  # default 0.5, ratio of mask for inpainting(text editing task), set 0 to disable\nwm_thresh = 1.0  # set 0.5 to skip watermark imgs from training(ch:~25%, en:~8%, @Precision93.67%+Recall88.80%), 1.0 not skip\nroot_dir = './models'  # path for save checkpoints\ndataset_percent = 0.0566  # 1.0 use full datasets, 0.0566 use ~200k images for ablation study\nsave_steps = None  # step frequency of saving checkpoints\nsave_epochs = 1  # epoch frequency of saving checkpoints\nmax_epochs = 15  # default 60\nassert (save_steps is None) != (save_epochs is None)\n\n\nif __name__ == '__main__':\n    log_img = os.path.join(root_dir, 'image_log/train')\n    if os.path.exists(log_img):\n        try:\n            shutil.rmtree(log_img)\n        except OSError:\n            pass\n    model = create_model(model_config).cpu()\n    if ckpt_path is None:\n        model.load_state_dict(load_state_dict(resume_path, location='cpu'))\n    model.learning_rate = learning_rate\n    model.sd_locked = True\n    model.only_mid_control = False\n    model.unlockKV = False\n\n    checkpoint_callback = ModelCheckpoint(\n        every_n_train_steps=save_steps,\n        every_n_epochs=save_epochs,\n        save_top_k=3,\n        monitor=\"global_step\",\n        mode=\"max\",\n    )\n    json_paths = [\n        r'/data/vdb/yuxiang.tyx/AIGC/data/ocr_data/Art/data.json',\n        r'/data/vdb/yuxiang.tyx/AIGC/data/ocr_data/COCO_Text/data.json',\n        r'/data/vdb/yuxiang.tyx/AIGC/data/ocr_data/icdar2017rctw/data.json',\n        r'/data/vdb/yuxiang.tyx/AIGC/data/ocr_data/LSVT/data.json',\n        r'/data/vdb/yuxiang.tyx/AIGC/data/ocr_data/mlt2019/data.json',\n        r'/data/vdb/yuxiang.tyx/AIGC/data/ocr_data/MTWI2018/data.json',\n        r'/data/vdb/yuxiang.tyx/AIGC/data/ocr_data/ReCTS/data.json',\n        '/data/vdb/yuxiang.tyx/AIGC/data/laion_word/data_v1.1.json',\n        '/data/vdb/yuxiang.tyx/AIGC/data/wukong_word/wukong_1of5/data_v1.1.json',\n        '/data/vdb/yuxiang.tyx/AIGC/data/wukong_word/wukong_2of5/data_v1.1.json',\n        '/data/vdb/yuxiang.tyx/AIGC/data/wukong_word/wukong_3of5/data_v1.1.json',\n        '/data/vdb/yuxiang.tyx/AIGC/data/wukong_word/wukong_4of5/data_v1.1.json',\n        '/data/vdb/yuxiang.tyx/AIGC/data/wukong_word/wukong_5of5/data_v1.1.json',\n        ]\n    dataset = T3DataSet(json_paths, max_lines=5, max_chars=20, caption_pos_prob=0.0, mask_pos_prob=1.0, mask_img_prob=mask_ratio, glyph_scale=2, percent=dataset_percent, debug=False, using_dlc=False, wm_thresh=wm_thresh)\n    dataloader = DataLoader(dataset, num_workers=8, persistent_workers=True, batch_size=batch_size, shuffle=True)\n    logger = ImageLogger(batch_frequency=logger_freq)\n    trainer = pl.Trainer(gpus=-1, precision=32, max_epochs=max_epochs, num_nodes=NUM_NODES, accumulate_grad_batches=grad_accum, callbacks=[logger, checkpoint_callback], default_root_dir=root_dir, strategy='ddp')\n    trainer.fit(model, dataloader, ckpt_path=ckpt_path)\n"
        },
        {
          "name": "util.py",
          "type": "blob",
          "size": 1.3193359375,
          "content": "import datetime\nimport os\nimport cv2\n\n\ndef save_images(img_list, folder):\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    now = datetime.datetime.now()\n    date_str = now.strftime(\"%Y-%m-%d\")\n    folder_path = os.path.join(folder, date_str)\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n    time_str = now.strftime(\"%H_%M_%S\")\n    for idx, img in enumerate(img_list):\n        image_number = idx + 1\n        filename = f\"{time_str}_{image_number}.jpg\"\n        save_path = os.path.join(folder_path, filename)\n        cv2.imwrite(save_path, img[..., ::-1])\n\n\ndef check_channels(image):\n    channels = image.shape[2] if len(image.shape) == 3 else 1\n    if channels == 1:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    elif channels > 3:\n        image = image[:, :, :3]\n    return image\n\n\ndef resize_image(img, max_length=768):\n    height, width = img.shape[:2]\n    max_dimension = max(height, width)\n\n    if max_dimension > max_length:\n        scale_factor = max_length / max_dimension\n        new_width = int(round(width * scale_factor))\n        new_height = int(round(height * scale_factor))\n        new_size = (new_width, new_height)\n        img = cv2.resize(img, new_size)\n    height, width = img.shape[:2]\n    img = cv2.resize(img, (width-(width % 64), height-(height % 64)))\n    return img\n"
        }
      ]
    }
  ]
}