{
  "metadata": {
    "timestamp": 1736559953969,
    "page": 731,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "podgorskiy/ALAE",
      "stars": 3518,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.07421875,
          "content": "*.pyc\n*.ckpt\n*.mat\n\n__pycache__/\n.idea/\n*.pdf\n# *.png\n*.eps\n*.txt\nresults*/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.958984375,
          "content": "<h1 align=\"center\">\n  <br>\n  [CVPR2020] Adversarial Latent Autoencoders\n  <br>\n</h1>\n  <p align=\"center\">\n    <a href=\"https://podgorskiy.com/\">Stanislav Pidhorskyi</a> •\n    <a href=\"https://www.statler.wvu.edu/faculty-staff/faculty/donald-a-adjeroh\">Donald A. Adjeroh </a> •\n    <a href=\"http://vision.csee.wvu.edu/~doretto/\">Gianfranco Doretto</a>\n  </p>\n<h4 align=\"center\">Official repository of the paper</h4>\n<table>\n  <p align=\"center\">\n  <img src=\"https://podgorskiy.com/static/reconstructions_multiresolution_2.jpg\">  </p>\n<tbody>\n<tr>\n<td style=\"padding:0;\"><img src=\"https://user-images.githubusercontent.com/3229783/79670218-63080d80-818f-11ea-9e50-927b8af3e7b5.gif\"></td>\n<td style=\"padding:0;\"><img src=\"https://user-images.githubusercontent.com/3229783/79530431-4bb90b00-803d-11ea-9ce3-25dfc3df253a.gif\"></td>\n</tr>\n</tbody>\n</table>\n\n<p align=\"center\">\n  <img src=\"https://podgorskiy.com/static/stylemix.jpg\">\n</p>\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/pytorch-1.4.0-green.svg?style=plastic\" alt=\"pytorch version\">\n  <a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\"></a>\n</p>\n\n  <p align=\"center\">\n    <a href=\"https://drive.google.com/drive/folders/1iZodDA4q1IKRRgV2nJuAyyuCwQGtL4vp?usp=sharing\">Google Drive folder with models and qualitative results</a>\n  </p>\n\n\n# ALAE\n\n> **Adversarial Latent Autoencoders**<br>\n> Stanislav Pidhorskyi, Donald Adjeroh, Gianfranco Doretto<br>\n>\n> **Abstract:** *Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.*\n\n## Citation\n* Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. Adversarial Latent Autoencoders. In *Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020. [to appear] \n>\n\n    @InProceedings{pidhorskyi2020adversarial,\n     author   = {Pidhorskyi, Stanislav and Adjeroh, Donald A and Doretto, Gianfranco},\n     booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},\n     title    = {Adversarial Latent Autoencoders},\n     year     = {2020},\n     note     = {[to appear]},\n    }\n<h4 align=\"center\">preprint on arXiv: <a href=\"https://arxiv.org/abs/2004.04467\">2004.04467</a></h4>\n\n## To run the demo\n\nTo run the demo, you will need to have a CUDA capable GPU, PyTorch >= v1.3.1 and cuda/cuDNN drivers installed.\nInstall the required packages:\n\n    pip install -r requirements.txt\n  \nDownload pre-trained models:\n\n    python training_artifacts/download_all.py\n\nRun the demo:\n\n    python interactive_demo.py\n\nYou can specify **yaml** config to use. Configs are located here: https://github.com/podgorskiy/ALAE/tree/master/configs.\nBy default, it uses one for FFHQ dataset.\nYou can change the config using `-c` parameter. To run on `celeb-hq` in 256x256 resolution, run:\n\n    python interactive_demo.py -c celeba-hq256\n\nHowever, for configs other then FFHQ, you need to obtain new principal direction vectors for the attributes.\n\n## Repository organization\n\n#### Running scripts\n\nThe code in the repository is organized in such a way that all scripts must be run from the root of the repository.\nIf you use an IDE (e.g. PyCharm or Visual Studio Code), just set *Working Directory* to point to the root of the repository.\n\nIf you want to run from the command line, then you also need to set **PYTHONPATH** variable to point to the root of the repository.\n\nFor example, let's say we've cloned repository to *~/ALAE* directory, then do:\n\n    $ cd ~/ALAE\n    $ export PYTHONPATH=$PYTHONPATH:$(pwd)\n\n![pythonpath](https://podgorskiy.com/static/pythonpath.svg)\n\nNow you can run scripts as follows:\n\n    $ python style_mixing/stylemix.py\n\n#### Repository structure\n\n\n| Path | Description\n| :--- | :----------\n| ALAE | Repository root folder\n| &boxvr;&nbsp; configs | Folder with yaml config files.\n| &boxv;&nbsp; &boxvr;&nbsp; bedroom.yaml | Config file for LSUN bedroom dataset at 256x256 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; celeba.yaml | Config file for CelebA dataset at 128x128 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; celeba-hq256.yaml | Config file for CelebA-HQ dataset at 256x256 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; celeba_ablation_nostyle.yaml | Config file for CelebA 128x128 dataset for ablation study (no styles).\n| &boxv;&nbsp; &boxvr;&nbsp; celeba_ablation_separate.yaml | Config file for CelebA 128x128 dataset for ablation study (separate encoder and discriminator).\n| &boxv;&nbsp; &boxvr;&nbsp; celeba_ablation_z_reg.yaml | Config file for CelebA 128x128 dataset for ablation study (regress in Z space, not W).\n| &boxv;&nbsp; &boxvr;&nbsp; ffhq.yaml | Config file for FFHQ dataset at 1024x1024 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; mnist.yaml | Config file for MNIST dataset using Style architecture.\n| &boxv;&nbsp; &boxur;&nbsp; mnist_fc.yaml | Config file for MNIST dataset using only fully connected layers (Permutation Invariant MNIST).\n| &boxvr;&nbsp; dataset_preparation | Folder with scripts for dataset preparation.\n| &boxv;&nbsp; &boxvr;&nbsp; prepare_celeba_hq_tfrec.py | To prepare TFRecords for CelebA-HQ dataset at 256x256 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; prepare_celeba_tfrec.py | To prepare TFRecords for CelebA dataset at 128x128 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; prepare_mnist_tfrec.py | To prepare TFRecords for MNIST dataset.\n| &boxv;&nbsp; &boxvr;&nbsp; split_tfrecords_bedroom.py | To split official TFRecords from StyleGAN paper for LSUN bedroom dataset.\n| &boxv;&nbsp; &boxur;&nbsp; split_tfrecords_ffhq.py | To split official TFRecords from StyleGAN paper for FFHQ dataset.\n| &boxvr;&nbsp; dataset_samples | Folder with sample inputs for different datasets. Used for figures and for test inputs during training.\n| &boxvr;&nbsp; make_figures | Scripts for making various figures.\n| &boxvr;&nbsp; metrics | Scripts for computing metrics.\n| &boxvr;&nbsp; principal_directions | Scripts for computing principal direction vectors for various attributes. **For interactive demo**.\n| &boxvr;&nbsp; style_mixing | Sample inputs and script for producing style-mixing figures.\n| &boxvr;&nbsp; training_artifacts | Default place for saving checkpoints/sample outputs/plots.\n| &boxv;&nbsp; &boxur;&nbsp; download_all.py | Script for downloading all pretrained models.\n| &boxvr;&nbsp; interactive_demo.py | Runnable script for interactive demo.\n| &boxvr;&nbsp; train_alae.py | Runnable script for training.\n| &boxvr;&nbsp; train_alae_separate.py | Runnable script for training for ablation study (separate encoder and discriminator).\n| &boxvr;&nbsp; checkpointer.py | Module for saving/restoring model weights, optimizer state and loss history.\n| &boxvr;&nbsp; custom_adam.py | Customized adam optimizer for learning rate equalization and zero second beta.\n| &boxvr;&nbsp; dataloader.py | Module with dataset classes, loaders, iterators, etc.\n| &boxvr;&nbsp; defaults.py | Definition for config variables with default values.\n| &boxvr;&nbsp; launcher.py | Helper for running multi-GPU, multiprocess training. Sets up config and logging.\n| &boxvr;&nbsp; lod_driver.py | Helper class for managing growing/stabilizing network.\n| &boxvr;&nbsp; lreq.py | Custom `Linear`, `Conv2d` and `ConvTranspose2d` modules for learning rate equalization.\n| &boxvr;&nbsp; model.py | Module with high-level model definition.\n| &boxvr;&nbsp; model_separate.py | Same as above, but for ablation study.\n| &boxvr;&nbsp; net.py | Definition of all network blocks for multiple architectures.\n| &boxvr;&nbsp; registry.py | Registry of network blocks for selecting from config file.\n| &boxvr;&nbsp; scheduler.py | Custom schedulers with warm start and aggregating several optimizers.\n| &boxvr;&nbsp; tracker.py | Module for plotting losses.\n| &boxur;&nbsp; utils.py | Decorator for async call, decorator for caching, registry for network blocks.\n\n\n#### Configs\n\nIn this codebase [**yacs**](https://github.com/rbgirshick/yacs) is used to handle configurations. \n\nMost of the runnable scripts accept `-c` parameter that can specify config files to use.\nFor example, to make reconstruction figures, you can run:\n\n    python make_figures/make_recon_figure_paged.py\n    python make_figures/make_recon_figure_paged.py -c celeba\n    python make_figures/make_recon_figure_paged.py -c celeba-hq256\n    python make_figures/make_recon_figure_paged.py -c bedroom\n    \nThe Default config is `ffhq`.\n\n#### Datasets\n\nTraining is done using TFRecords. TFRecords are read using [DareBlopy](https://github.com/podgorskiy/DareBlopy), which allows using them with Pytorch.\n\nIn config files as well as in all preparation scripts, it is assumed that all datasets are in `/data/datasets/`. You can either change path in config files, either create a symlink to where you store datasets.\n\nThe official way of generating CelebA-HQ can be challenging. Please refer to this page: https://github.com/suvojit-0x55aa/celebA-HQ-dataset-download\nYou can get the pre-generated dataset from: https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P\n\n#### Pre-trained models\n\nTo download pre-trained models run:\n\n    python training_artifacts/download_all.py\n\n**Note**: There used to be problems with downloading models from Google Drive due to download limit. \nNow, the script is setup in a such way that if it fails to download data from Google Drive it will try to download it from S3.\n\nIf you experience problems, try deleting all *.pth files, updating *dlutils* package (`pip install dlutils --upgrade`) and then run `download_all.py` again.\nIf that does not solve the problem, please open an issue. Also, you can try downloading models manually from here: https://drive.google.com/drive/folders/1tsI1q1u8QRX5t7_lWCSjpniLGlNY-3VY?usp=sharing\n\n\nIn config files, `OUTPUT_DIR` points to where weights are saved to and read from. For example: `OUTPUT_DIR: training_artifacts/celeba-hq256`\n\nIn `OUTPUT_DIR` it saves a file `last_checkpoint` which contains path to the actual `.pth` pickle with model weight. If you want to test the model with a specific weight file, you can simply modify `last_checkpoint` file.\n\n\n## Generating figures\n\n#### Style-mixing\n\nTo generate style-mixing figures run:\n\n    python style_mixing/stylemix.py -c <config>\n    \nWhere instead of `<config>` put one of: `ffhq`, `celeba`, `celeba-hq256`, `bedroom`\n    \n\n#### Reconstructions\n\nTo generate reconstruction with multiple scale images:\n\n    python make_figures/make_recon_figure_multires.py -c <config>\n    \nTo generate reconstruction from all sample inputs on multiple pages:\n\n    python make_figures/make_recon_figure_paged.py -c <config>\n\nThere are also:\n\n    python make_figures/old/make_recon_figure_celeba.py\n    python make_figures/old/make_recon_figure_bed.py\n\nTo generate reconstruction from test set of FFHQ:\n\n    python make_figures/make_recon_figure_ffhq_real.py\n    \nTo generate interpolation figure:\n\n    python make_figures/make_recon_figure_interpolation.py -c <config>\n    \nTo generate traversals figure:\n\n(For datasets other then FFHQ, you will need to find principal directions first)\n\n    python make_figures/make_traversarls.py -c <config>\n    \n#### Generations\n\nTo make generation figure run:\n\n    make_generation_figure.py -c <config>\n\n## Training\n\nIn addition to installing required packages:\n\n    pip install -r requirements.txt\n\nYou will need to install [DareBlopy](https://github.com/podgorskiy/DareBlopy):\n\n    pip install dareblopy\n\nTo run training:\n\n    python train_alae.py -c <config>\n    \nIt will run multi-GPU training on all available GPUs. It uses `DistributedDataParallel` for parallelism. \nIf only one GPU available, it will run on single GPU, no special care is needed.\n\nThe recommended number of GPUs is 8. Reproducibility on a smaller number of GPUs may have issues. You might need to adjust the batch size in the config file depending on the memory size of the GPUs.\n\n## Running metrics\n\nIn addition to installing required packages and [DareBlopy](https://github.com/podgorskiy/DareBlopy), you need to install TensorFlow and dnnlib from StyleGAN.\n\nTensorflow must be of version `1.10`:\n\n    pip install tensorflow-gpu==1.10\n\nIt requires CUDA version 9.0.\n\nPerhaps, the best way is to use Anaconda to handle this, but I prefer installing CUDA 9.0 from pop-os repositories (works on Ubuntu):\n\n```\nsudo echo \"deb http://apt.pop-os.org/proprietary bionic main\" | sudo tee -a /etc/apt/sources.list.d/pop-proprietary.list\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key 204DD8AEC33A7AFF\nsudo apt update\n\nsudo apt install system76-cuda-9.0\nsudo apt install system76-cudnn-9.0\n```\n\nThen just set `LD_LIBRARY_PATH` variable:\n\n```\nexport LD_LIBRARY_PATH=/usr/lib/cuda-9.0/lib64\n```\n\nDnnlib is a package used in StyleGAN. You can install it with:\n\n    pip install https://github.com/podgorskiy/dnnlib/releases/download/0.0.1/dnnlib-0.0.1-py3-none-any.whl\n \nAll code for running metrics is heavily based on those from StyleGAN repository. It also uses the same pre-trained models:\n\n[https://github.com/NVlabs/stylegan#licenses](https://github.com/NVlabs/stylegan#licenses)\n\n> inception_v3_features.pkl and inception_v3_softmax.pkl are derived from the pre-trained Inception-v3 network by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. The network was originally shared under Apache 2.0 license on the TensorFlow Models repository.\n> \n> vgg16.pkl and vgg16_zhang_perceptual.pkl are derived from the pre-trained VGG-16 network by Karen Simonyan and Andrew Zisserman. The network was originally shared under Creative Commons BY 4.0 license on the Very Deep Convolutional Networks for Large-Scale Visual Recognition project page.\n> \n> vgg16_zhang_perceptual.pkl is further derived from the pre-trained LPIPS weights by Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The weights were originally shared under BSD 2-Clause \"Simplified\" License on the PerceptualSimilarity repository.\n\nFinally, to run metrics:\n\n    python metrics/fid.py -c <config>       # FID score on generations\n    python metrics/fid_rec.py -c <config>   # FID score on reconstructions\n    python metrics/ppl.py -c <config>       # PPL score on generations\n    python metrics/lpips.py -c <config>     # LPIPS score of reconstructions\n \n"
        },
        {
          "name": "align_faces.py",
          "type": "blob",
          "size": 5.4736328125,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport os\nimport numpy as np\nimport dlib\nfrom PIL import Image\nimport PIL\nimport scipy\nimport scipy.ndimage\n\n# lefteye_x lefteye_y righteye_x righteye_y nose_x nose_y leftmouth_x leftmouth_y rightmouth_x rightmouth_y\n# 69\t111\t108\t111\t88\t136\t72\t152\t105\t152\n# 44\t51\t83\t51\t63\t76\t47\t92\t80\t92\n\nuse_1024 = True\n\n\ndef align(img, parts, dst_dir='realign1024x1024', output_size=1024, transform_size=4096, item_idx=0, enable_padding=True):\n    # Parse landmarks.\n    lm = np.array(parts)\n    lm_chin          = lm[0: 17]  # left-right\n    lm_eyebrow_left = lm[17: 22]  # left-right\n    lm_eyebrow_right = lm[22: 27]  # left-right\n    lm_nose = lm[27: 31]  # top-down\n    lm_nostrils = lm[31: 36]  # top-down\n    lm_eye_left = lm[36: 42]  # left-clockwise\n    lm_eye_right = lm[42: 48]  # left-clockwise\n    lm_mouth_outer = lm[48: 60]  # left-clockwise\n    lm_mouth_inner = lm[60: 68]  # left-clockwise\n\n    # Calculate auxiliary vectors.\n    eye_left = np.mean(lm_eye_left, axis=0)\n    eye_right = np.mean(lm_eye_right, axis=0)\n    eye_avg = (eye_left + eye_right) * 0.5\n    eye_to_eye = eye_right - eye_left\n    mouth_left = lm_mouth_outer[0]\n    mouth_right = lm_mouth_outer[6]\n    mouth_avg = (mouth_left + mouth_right) * 0.5\n    eye_to_mouth = mouth_avg - eye_avg\n\n    # Choose oriented crop rectangle.\n    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n    x /= np.hypot(*x)\n\n    if use_1024:\n        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n    else:\n        x *= (np.hypot(*eye_to_eye) * 1.6410 + np.hypot(*eye_to_mouth) * 1.560) / 2.0\n\n    y = np.flipud(x) * [-1, 1]\n\n    if use_1024:\n        c = eye_avg + eye_to_mouth * 0.1\n        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n    else:\n        c = eye_avg + eye_to_mouth * 0.317\n        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n\n    qsize = np.hypot(*x) * 2\n\n    img = Image.fromarray(img)\n\n    # Shrink.\n    shrink = int(np.floor(qsize / output_size * 0.5))\n    if shrink > 1:\n        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n        quad /= shrink\n        qsize /= shrink\n\n    # Crop.\n    border = max(int(np.rint(qsize * 0.1)), 3)\n    crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n            int(np.ceil(max(quad[:, 1]))))\n    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]),\n            min(crop[3] + border, img.size[1]))\n    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n        img = img.crop(crop)\n        quad -= crop[0:2]\n\n    # Pad.\n    pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n           int(np.ceil(max(quad[:, 1]))))\n    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),\n           max(pad[3] - img.size[1] + border, 0))\n    if enable_padding and max(pad) > border - 4:\n        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n        h, w, _ = img.shape\n        y, x, _ = np.ogrid[:h, :w, :1]\n        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w - 1 - x) / pad[2]),\n                          1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h - 1 - y) / pad[3]))\n        blur = qsize * 0.02\n        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n        img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n        quad += pad[:2]\n\n    # Transform.\n    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n    if output_size < transform_size:\n        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n\n    # Save aligned image.\n    dst_subdir = dst_dir\n    os.makedirs(dst_subdir, exist_ok=True)\n    img.save(os.path.join(dst_subdir, '%05d.png' % item_idx))\n\n\npredictor_path = 'shape_predictor_68_face_landmarks.dat'\n\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(predictor_path)\n\nitem_idx = 0\n\nfor filename in os.listdir('celebs'):\n    img = np.asarray(Image.open('celebs/' + filename))\n    if img.shape[2] == 4:\n        img = img[:, :, :3]\n\n    dets = detector(img, 0)\n    print(\"Number of faces detected: {}\".format(len(dets)))\n\n    for i, d in enumerate(dets):\n        print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n        i, d.left(), d.top(), d.right(), d.bottom()))\n\n        shape = predictor(img, d)\n\n        parts = shape.parts()\n\n        parts = [[part.x, part.y] for part in parts]\n\n        if use_1024:\n            align(img, parts, dst_dir='dataset_samples/faces/realign1024x1024', output_size=1024, transform_size=4098, item_idx=item_idx)\n        else:\n            align(img, parts, dst_dir='dataset_samples/faces/realign128x128', output_size=128, transform_size=512, item_idx=item_idx)\n\n        item_idx += 1\n"
        },
        {
          "name": "checkpointer.py",
          "type": "blob",
          "size": 4.58203125,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nfrom torch import nn\nimport torch\nimport utils\n\n\ndef get_model_dict(x):\n    if x is None:\n        return None\n    if isinstance(x, nn.DataParallel):\n        return x.module.state_dict()\n    else:\n        return x.state_dict()\n\n\ndef load_model(x, state_dict):\n    if isinstance(x, nn.DataParallel):\n        x.module.load_state_dict(state_dict)\n    else:\n        x.load_state_dict(state_dict)\n\n\nclass Checkpointer(object):\n    def __init__(self, cfg, models, auxiliary=None, logger=None, save=True):\n        self.models = models\n        self.auxiliary = auxiliary\n        self.cfg = cfg\n        self.logger = logger\n        self._save = save\n\n    def save(self, _name, **kwargs):\n        if not self._save:\n            return\n        data = dict()\n        data[\"models\"] = dict()\n        data[\"auxiliary\"] = dict()\n        for name, model in self.models.items():\n            data[\"models\"][name] = get_model_dict(model)\n\n        if self.auxiliary is not None:\n            for name, item in self.auxiliary.items():\n                data[\"auxiliary\"][name] = item.state_dict()\n        data.update(kwargs)\n\n        @utils.async_func\n        def save_data():\n            save_file = os.path.join(self.cfg.OUTPUT_DIR, \"%s.pth\" % _name)\n            self.logger.info(\"Saving checkpoint to %s\" % save_file)\n            torch.save(data, save_file)\n            self.tag_last_checkpoint(save_file)\n\n        return save_data()\n\n    def load(self, ignore_last_checkpoint=False, file_name=None):\n        save_file = os.path.join(self.cfg.OUTPUT_DIR, \"last_checkpoint\")\n        try:\n            with open(save_file, \"r\") as last_checkpoint:\n                f = last_checkpoint.read().strip()\n        except IOError:\n            self.logger.info(\"No checkpoint found. Initializing model from scratch\")\n            if file_name is None:\n                return {}\n\n        if ignore_last_checkpoint:\n            self.logger.info(\"Forced to Initialize model from scratch\")\n            return {}\n        if file_name is not None:\n            f = file_name\n\n        self.logger.info(\"Loading checkpoint from {}\".format(f))\n        checkpoint = torch.load(f, map_location=torch.device(\"cpu\"))\n        for name, model in self.models.items():\n            if name in checkpoint[\"models\"]:\n                try:\n                    model_dict = checkpoint[\"models\"].pop(name)\n                    if model_dict is not None:\n                        self.models[name].load_state_dict(model_dict, strict=False)\n                    else:\n                        self.logger.warning(\"State dict for model \\\"%s\\\" is None \" % name)\n                except RuntimeError as e:\n                    self.logger.warning('%s\\nFailed to load: %s\\n%s' % ('!' * 160, name, '!' * 160))\n                    self.logger.warning('\\nFailed to load: %s' % str(e))\n            else:\n                self.logger.warning(\"No state dict for model: %s\" % name)\n        checkpoint.pop('models')\n        if \"auxiliary\" in checkpoint and self.auxiliary:\n            self.logger.info(\"Loading auxiliary from {}\".format(f))\n            for name, item in self.auxiliary.items():\n                try:\n                    if name in checkpoint[\"auxiliary\"]:\n                        self.auxiliary[name].load_state_dict(checkpoint[\"auxiliary\"].pop(name))\n                    if \"optimizers\" in checkpoint and name in checkpoint[\"optimizers\"]:\n                        self.auxiliary[name].load_state_dict(checkpoint[\"optimizers\"].pop(name))\n                    if name in checkpoint:\n                        self.auxiliary[name].load_state_dict(checkpoint.pop(name))\n                except (IndexError, ValueError):\n                    self.logger.warning('%s\\nFailed to load: %s\\n%s' % ('!' * 160, name, '!' * 160))\n            checkpoint.pop('auxiliary')\n\n        return checkpoint\n\n    def tag_last_checkpoint(self, last_filename):\n        save_file = os.path.join(self.cfg.OUTPUT_DIR, \"last_checkpoint\")\n        with open(save_file, \"w\") as f:\n            f.write(last_filename)\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "custom_adam.py",
          "type": "blob",
          "size": 3.8671875,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n# lr_equalization_coef was added for LREQ\n\n# Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)\n# Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\n# Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n# Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n# Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n# Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n# Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n# Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n# Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\n# https://github.com/pytorch/pytorch/blob/master/LICENSE\n\n\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass LREQAdam(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.0, 0.99), eps=1e-8,\n                 weight_decay=0):\n        beta_2 = betas[1]\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 == betas[0]:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= beta_2 < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(beta_2))\n        defaults = dict(lr=lr, beta_2=beta_2, eps=eps,\n                        weight_decay=weight_decay)\n        super(LREQAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(LREQAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    # state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                # exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                exp_avg_sq = state['exp_avg_sq']\n                beta_2 = group['beta_2']\n\n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    grad.add_(group['weight_decay'], p.data / p.coef)\n\n                # Decay the first and second moment running average coefficient\n                # exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta_2).addcmul_(1 - beta_2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                # bias_correction1 = 1 - beta1 ** state['step'] # 1\n                bias_correction2 = 1 - beta_2 ** state['step']\n                # step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n                step_size = group['lr'] * math.sqrt(bias_correction2)\n\n                # p.data.addcdiv_(-step_size, exp_avg, denom)\n                if hasattr(p, 'lr_equalization_coef'):\n                    step_size *= p.lr_equalization_coef\n\n                p.data.addcdiv_(-step_size, grad, denom)\n\n        return loss\n"
        },
        {
          "name": "dataloader.py",
          "type": "blob",
          "size": 11.126953125,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport dareblopy as db\nimport random\n\nimport numpy as np\nimport torch\nimport torch.tensor\nimport torch.utils\nimport torch.utils.data\nimport time\nimport math\n\ncpu = torch.device('cpu')\n\n\nclass TFRecordsDataset:\n    def __init__(self, cfg, logger, rank=0, world_size=1, buffer_size_mb=200, channels=3, seed=None, train=True, needs_labels=False):\n        self.cfg = cfg\n        self.logger = logger\n        self.rank = rank\n        self.last_data = \"\"\n        if train:\n            self.part_count = cfg.DATASET.PART_COUNT\n            self.part_size = cfg.DATASET.SIZE // self.part_count\n        else:\n            self.part_count = cfg.DATASET.PART_COUNT_TEST\n            self.part_size = cfg.DATASET.SIZE_TEST // self.part_count\n        self.workers = []\n        self.workers_active = 0\n        self.iterator = None\n        self.filenames = {}\n        self.batch_size = 512\n        self.features = {}\n        self.channels = channels\n        self.seed = seed\n        self.train = train\n        self.needs_labels = needs_labels\n\n        assert self.part_count % world_size == 0\n\n        self.part_count_local = self.part_count // world_size\n\n        if train:\n            path = cfg.DATASET.PATH\n        else:\n            path = cfg.DATASET.PATH_TEST\n\n        for r in range(2, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n            files = []\n            for i in range(self.part_count_local * rank, self.part_count_local * (rank + 1)):\n                file = path % (r, i)\n                files.append(file)\n            self.filenames[r] = files\n\n        self.buffer_size_b = 1024 ** 2 * buffer_size_mb\n\n        self.current_filenames = []\n\n    def reset(self, lod, batch_size):\n        assert lod in self.filenames.keys()\n        self.current_filenames = self.filenames[lod]\n        self.batch_size = batch_size\n\n        img_size = 2 ** lod\n\n        if self.needs_labels:\n            self.features = {\n                # 'shape': db.FixedLenFeature([3], db.int64),\n                'data': db.FixedLenFeature([self.channels, img_size, img_size], db.uint8),\n                'label': db.FixedLenFeature([], db.int64)\n            }\n        else:\n            self.features = {\n                # 'shape': db.FixedLenFeature([3], db.int64),\n                'data': db.FixedLenFeature([self.channels, img_size, img_size], db.uint8)\n            }\n\n        buffer_size = self.buffer_size_b // (self.channels * img_size * img_size)\n\n        if self.seed is None:\n            seed = np.uint64(time.time() * 1000)\n        else:\n            seed = self.seed\n            self.logger.info('!' * 80)\n            self.logger.info('! Seed is used for to shuffle data in TFRecordsDataset!')\n            self.logger.info('!' * 80)\n\n        self.iterator = db.ParsedTFRecordsDatasetIterator(self.current_filenames, self.features, self.batch_size, buffer_size, seed=seed)\n\n    def __iter__(self):\n        return self.iterator\n\n    def __len__(self):\n        return self.part_count_local * self.part_size\n\n\ndef make_dataloader(cfg, logger, dataset, GPU_batch_size, local_rank, numpy=False):\n    class BatchCollator(object):\n        def __init__(self, device=torch.device(\"cpu\")):\n            self.device = device\n            self.flip = cfg.DATASET.FLIP_IMAGES\n            self.numpy = numpy\n\n        def __call__(self, batch):\n            with torch.no_grad():\n                x, = batch\n                if self.flip:\n                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n                if self.numpy:\n                    return x\n                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n                return x\n\n    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n\n    return batches\n\n\ndef make_dataloader_y(cfg, logger, dataset, GPU_batch_size, local_rank):\n    class BatchCollator(object):\n        def __init__(self, device=torch.device(\"cpu\")):\n            self.device = device\n            self.flip = cfg.DATASET.FLIP_IMAGES\n\n        def __call__(self, batch):\n            with torch.no_grad():\n                x, y = batch\n                if self.flip:\n                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n                return x, y\n\n    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n\n    return batches\n\n\nclass TFRecordsDatasetImageNet:\n    def __init__(self, cfg, logger, rank=0, world_size=1, buffer_size_mb=200, channels=3, seed=None, train=True, needs_labels=False):\n        self.cfg = cfg\n        self.logger = logger\n        self.rank = rank\n        self.last_data = \"\"\n        self.part_count = cfg.DATASET.PART_COUNT\n        if train:\n            self.part_size = cfg.DATASET.SIZE // cfg.DATASET.PART_COUNT\n        else:\n            self.part_size = cfg.DATASET.SIZE_TEST // cfg.DATASET.PART_COUNT\n        self.workers = []\n        self.workers_active = 0\n        self.iterator = None\n        self.filenames = {}\n        self.batch_size = 512\n        self.features = {}\n        self.channels = channels\n        self.seed = seed\n        self.train = train\n        self.needs_labels = needs_labels\n\n        assert self.part_count % world_size == 0\n\n        self.part_count_local = cfg.DATASET.PART_COUNT // world_size\n\n        if train:\n            path = cfg.DATASET.PATH\n        else:\n            path = cfg.DATASET.PATH_TEST\n\n        for r in range(2, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n            files = []\n            for i in range(self.part_count_local * rank, self.part_count_local * (rank + 1)):\n                file = path % (r, i)\n                files.append(file)\n            self.filenames[r] = files\n\n        self.buffer_size_b = 1024 ** 2 * buffer_size_mb\n\n        self.current_filenames = []\n\n    def reset(self, lod, batch_size):\n        assert lod in self.filenames.keys()\n        self.current_filenames = self.filenames[lod]\n        self.batch_size = batch_size\n\n        if self.train:\n            img_size = 2 ** lod + 2 ** (lod - 3)\n        else:\n            img_size = 2 ** lod\n\n        if self.needs_labels:\n            self.features = {\n                'data': db.FixedLenFeature([3, img_size, img_size], db.uint8),\n                'label': db.FixedLenFeature([], db.int64)\n            }\n        else:\n            self.features = {\n                'data': db.FixedLenFeature([3, img_size, img_size], db.uint8)\n            }\n\n        buffer_size = self.buffer_size_b // (self.channels * img_size * img_size)\n\n        if self.seed is None:\n            seed = np.uint64(time.time() * 1000)\n        else:\n            seed = self.seed\n            self.logger.info('!' * 80)\n            self.logger.info('! Seed is used for to shuffle data in TFRecordsDataset!')\n            self.logger.info('!' * 80)\n\n        self.iterator = db.ParsedTFRecordsDatasetIterator(self.current_filenames, self.features, self.batch_size, buffer_size, seed=seed)\n\n    def __iter__(self):\n        return self.iterator\n\n    def __len__(self):\n        return self.part_count_local * self.part_size\n\n\ndef make_imagenet_dataloader(cfg, logger, dataset, GPU_batch_size, target_size, local_rank, do_random_crops=True):\n    class BatchCollator(object):\n        def __init__(self, device=torch.device(\"cpu\")):\n            self.device = device\n            self.flip = cfg.DATASET.FLIP_IMAGES\n            self.size = target_size\n            p = math.log2(target_size)\n            self.source_size = 2 ** p + 2 ** (p - 3)\n            self.do_random_crops = do_random_crops\n\n        def __call__(self, batch):\n            with torch.no_grad():\n                x, = batch\n\n                if self.do_random_crops:\n                    images = []\n                    for im in x:\n                        deltax = self.source_size - target_size\n                        deltay = self.source_size - target_size\n                        offx = np.random.randint(deltax + 1)\n                        offy = np.random.randint(deltay + 1)\n                        im = im[:, offy:offy + self.size, offx:offx + self.size]\n                        images.append(im)\n                    x = np.stack(images)\n\n                if self.flip:\n                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n\n                return x\n\n    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n\n    return batches\n\n\ndef make_imagenet_dataloader_y(cfg, logger, dataset, GPU_batch_size, target_size, local_rank, do_random_crops=True):\n    class BatchCollator(object):\n        def __init__(self, device=torch.device(\"cpu\")):\n            self.device = device\n            self.flip = cfg.DATASET.FLIP_IMAGES\n            self.size = target_size\n            p = math.log2(target_size)\n            self.source_size = 2 ** p + 2 ** (p - 3)\n            self.do_random_crops = do_random_crops\n\n        def __call__(self, batch):\n            with torch.no_grad():\n                x, y = batch\n\n                if self.do_random_crops:\n                    images = []\n                    for im in x:\n                        deltax = self.source_size - target_size\n                        deltay = self.source_size - target_size\n                        offx = np.random.randint(deltax + 1)\n                        offy = np.random.randint(deltay + 1)\n                        im = im[:, offy:offy+self.size, offx:offx+self.size]\n                        images.append(im)\n                    x = np.stack(images)\n\n                if self.flip:\n                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n                return x, y\n\n    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n\n    return batches\n"
        },
        {
          "name": "dataset_preparation",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset_samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "defaults.py",
          "type": "blob",
          "size": 2.423828125,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom yacs.config import CfgNode as CN\n\n\n_C = CN()\n\n_C.NAME = \"\"\n_C.PPL_CELEBA_ADJUSTMENT = False\n_C.OUTPUT_DIR = \"results\"\n\n_C.DATASET = CN()\n_C.DATASET.PATH = 'celeba/data_fold_%d_lod_%d.pkl'\n_C.DATASET.PATH_TEST = ''\n_C.DATASET.FFHQ_SOURCE = '/data/datasets/ffhq-dataset/tfrecords/ffhq/ffhq-r%02d.tfrecords'\n_C.DATASET.PART_COUNT = 1\n_C.DATASET.PART_COUNT_TEST = 1\n_C.DATASET.SIZE = 70000\n_C.DATASET.SIZE_TEST = 10000\n_C.DATASET.FLIP_IMAGES = True\n_C.DATASET.SAMPLES_PATH = 'dataset_samples/faces/realign128x128'\n\n_C.DATASET.STYLE_MIX_PATH = 'style_mixing/test_images/set_celeba/'\n\n_C.DATASET.MAX_RESOLUTION_LEVEL = 10\n\n_C.MODEL = CN()\n\n_C.MODEL.LAYER_COUNT = 6\n_C.MODEL.START_CHANNEL_COUNT = 64\n_C.MODEL.MAX_CHANNEL_COUNT = 512\n_C.MODEL.LATENT_SPACE_SIZE = 256\n_C.MODEL.DLATENT_AVG_BETA = 0.995\n_C.MODEL.TRUNCATIOM_PSI = 0.7\n_C.MODEL.TRUNCATIOM_CUTOFF = 8\n_C.MODEL.STYLE_MIXING_PROB = 0.9\n_C.MODEL.MAPPING_LAYERS = 5\n_C.MODEL.CHANNELS = 3\n_C.MODEL.GENERATOR = \"GeneratorDefault\"\n_C.MODEL.ENCODER = \"EncoderDefault\"\n_C.MODEL.MAPPING_D = \"MappingD\"\n_C.MODEL.MAPPING_F = \"MappingF\"\n_C.MODEL.Z_REGRESSION = False\n\n_C.TRAIN = CN()\n\n_C.TRAIN.EPOCHS_PER_LOD = 15\n\n_C.TRAIN.BASE_LEARNING_RATE = 0.0015\n_C.TRAIN.ADAM_BETA_0 = 0.0\n_C.TRAIN.ADAM_BETA_1 = 0.99\n_C.TRAIN.LEARNING_DECAY_RATE = 0.1\n_C.TRAIN.LEARNING_DECAY_STEPS = []\n_C.TRAIN.TRAIN_EPOCHS = 110\n\n_C.TRAIN.LOD_2_BATCH_8GPU = [512, 256, 128,   64,   32,    32]\n_C.TRAIN.LOD_2_BATCH_4GPU = [512, 256, 128,   64,   32,    16]\n_C.TRAIN.LOD_2_BATCH_2GPU = [256, 256, 128,   64,   32,    16]\n_C.TRAIN.LOD_2_BATCH_1GPU = [128, 128, 128,   64,   32,    16]\n\n\n_C.TRAIN.SNAPSHOT_FREQ = [300, 300, 300, 100, 50, 30, 20, 20, 10]\n\n_C.TRAIN.REPORT_FREQ = [100, 80, 60, 30, 20, 10, 10, 5, 5]\n\n_C.TRAIN.LEARNING_RATES = [0.002]\n\n\ndef get_cfg_defaults():\n    return _C.clone()\n"
        },
        {
          "name": "interactive_demo.py",
          "type": "blob",
          "size": 8.3125,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\n\nfrom PIL import Image\nimport bimpy\n\n\nlreq.use_implicit_lreq.set(True)\n\n\nindices = [0, 1, 2, 3, 4, 10, 11, 17, 19]\n\nlabels = [\"gender\",\n          \"smile\",\n          \"attractive\",\n          \"wavy-hair\",\n          \"young\",\n          \"big lips\",\n          \"big nose\",\n          \"chubby\",\n          \"glasses\",\n          ]\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_d\n    mapping_fl = model.mapping_f\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(\"Trainable parameters generator:\")\n    count_parameters(decoder)\n\n    logger.info(\"Trainable parameters discriminator:\")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[\"iteration\"] = 0\n\n    model_dict = {\n        'discriminator_s': encoder,\n        'generator_s': decoder,\n        'mapping_tl_s': mapping_tl,\n        'mapping_fl_s': mapping_fl,\n        'dlatent_avg': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    path = 'dataset_samples/faces/realign1024x1024'\n\n    paths = list(os.listdir(path))\n    paths.sort()\n    paths_backup = paths[:]\n    randomize = bimpy.Bool(True)\n    current_file = bimpy.String(\"\")\n\n    ctx = bimpy.Context()\n\n    attribute_values = [bimpy.Float(0) for i in indices]\n\n    W = [torch.tensor(np.load(\"principal_directions/direction_%d.npy\" % i), dtype=torch.float32) for i in indices]\n\n    rnd = np.random.RandomState(5)\n\n    def loadNext():\n        img = np.asarray(Image.open(path + '/' + paths[0]))\n        current_file.value = paths[0]\n        paths.pop(0)\n        if len(paths) == 0:\n            paths.extend(paths_backup)\n\n        if img.shape[2] == 4:\n            img = img[:, :, :3]\n        im = img.transpose((2, 0, 1))\n        x = torch.tensor(np.asarray(im, dtype=np.float32), device='cpu', requires_grad=True).cuda() / 127.5 - 1.\n        if x.shape[0] == 4:\n            x = x[:3]\n\n        needed_resolution = model.decoder.layer_to_resolution[-1]\n        while x.shape[2] > needed_resolution:\n            x = F.avg_pool2d(x, 2, 2)\n        if x.shape[2] != needed_resolution:\n            x = F.adaptive_avg_pool2d(x, (needed_resolution, needed_resolution))\n\n        img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n\n        latents_original = encode(x[None, ...].cuda())\n        latents = latents_original[0, 0].clone()\n        latents -= model.dlatent_avg.buff.data[0]\n\n        for v, w in zip(attribute_values, W):\n            v.value = (latents * w).sum()\n\n        for v, w in zip(attribute_values, W):\n            latents = latents - v.value * w\n\n        return latents, latents_original, img_src\n\n    def loadRandom():\n        latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n        lat = torch.tensor(latents).float().cuda()\n        dlat = mapping_fl(lat)\n        layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        dlat = torch.lerp(model.dlatent_avg.buff.data, dlat, coefs)\n        x = decode(dlat)[0]\n        img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n        latents_original = dlat\n        latents = latents_original[0, 0].clone()\n        latents -= model.dlatent_avg.buff.data[0]\n\n        for v, w in zip(attribute_values, W):\n            v.value = (latents * w).sum()\n\n        for v, w in zip(attribute_values, W):\n            latents = latents - v.value * w\n\n        return latents, latents_original, img_src\n\n    latents, latents_original, img_src = loadNext()\n\n    ctx.init(1800, 1600, \"Styles\")\n\n    def update_image(w, latents_original):\n        with torch.no_grad():\n            w = w + model.dlatent_avg.buff.data[0]\n            w = w[None, None, ...].repeat(1, model.mapping_f.num_layers, 1)\n\n            layer_idx = torch.arange(model.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n            cur_layers = (7 + 1) * 2\n            mixing_cutoff = cur_layers\n            styles = torch.where(layer_idx < mixing_cutoff, w, latents_original)\n\n            x_rec = decode(styles)\n            resultsample = ((x_rec * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255)\n            resultsample = resultsample.cpu()[0, :, :, :]\n            return resultsample.type(torch.uint8).transpose(0, 2).transpose(0, 1)\n\n    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n    im = update_image(latents, latents_original)\n    print(im.shape)\n    im = bimpy.Image(im)\n\n    display_original = True\n\n    seed = 0\n\n    while not ctx.should_close():\n        with ctx:\n            new_latents = latents + sum([v.value * w for v, w in zip(attribute_values, W)])\n\n            if display_original:\n                im = bimpy.Image(img_src)\n            else:\n                im = bimpy.Image(update_image(new_latents, latents_original))\n\n            bimpy.begin(\"Principal directions\")\n            bimpy.columns(2)\n            bimpy.set_column_width(0, im_size + 20)\n            bimpy.image(im)\n            bimpy.next_column()\n\n            for v, label in zip(attribute_values, labels):\n                bimpy.slider_float(label, v, -40.0, 40.0)\n\n            bimpy.checkbox(\"Randomize noise\", randomize)\n\n            if randomize.value:\n                seed += 1\n\n            torch.manual_seed(seed)\n\n            if bimpy.button('Next'):\n                latents, latents_original, img_src = loadNext()\n                display_original = True\n            if bimpy.button('Display Reconstruction'):\n                display_original = False\n            if bimpy.button('Generate random'):\n                latents, latents_original, img_src = loadRandom()\n                display_original = False\n\n            if bimpy.input_text(\"Current file\", current_file, 64) and os.path.exists(path + '/' + current_file.value):\n                paths.insert(0, current_file.value)\n                latents, latents_original, img_src = loadNext()\n\n            bimpy.end()\n\n\nif __name__ == \"__main__\":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description='ALAE-interactive', default_config='configs/ffhq.yaml',\n        world_size=gpu_count, write_log=False)\n"
        },
        {
          "name": "launcher.py",
          "type": "blob",
          "size": 4.17578125,
          "content": "# Copyright 2019 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport sys\nimport argparse\nimport logging\nimport torch\nimport torch.multiprocessing as mp\nfrom torch import distributed\nimport inspect\n\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    distributed.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n\ndef cleanup():\n    distributed.destroy_process_group()\n\n\ndef _run(rank, world_size, fn, defaults, write_log, no_cuda, args):\n    if world_size > 1:\n        setup(rank, world_size)\n    if not no_cuda:\n        torch.cuda.set_device(rank)\n\n    cfg = defaults\n    config_file = args.config_file\n    if len(os.path.splitext(config_file)[1]) == 0:\n        config_file += '.yaml'\n    if not os.path.exists(config_file) and os.path.exists(os.path.join('configs', config_file)):\n        config_file = os.path.join('configs', config_file)\n    cfg.merge_from_file(config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(\"logger\")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    if rank == 0:\n        ch = logging.StreamHandler(stream=sys.stdout)\n        ch.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\"%(asctime)s %(name)s %(levelname)s: %(message)s\")\n        ch.setFormatter(formatter)\n        logger.addHandler(ch)\n\n        if write_log:\n            filepath = os.path.join(output_dir, 'log.txt')\n            if isinstance(write_log, str):\n                filepath = write_log\n            fh = logging.FileHandler(filepath)\n            fh.setLevel(logging.DEBUG)\n            fh.setFormatter(formatter)\n            logger.addHandler(fh)\n\n    logger.info(args)\n\n    logger.info(\"World size: {}\".format(world_size))\n\n    logger.info(\"Loaded configuration file {}\".format(config_file))\n    with open(config_file, \"r\") as cf:\n        config_str = \"\\n\" + cf.read()\n        logger.info(config_str)\n    logger.info(\"Running with config:\\n{}\".format(cfg))\n\n    if not no_cuda:\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n        device = torch.cuda.current_device()\n        print(\"Running on \", torch.cuda.get_device_name(device))\n\n    args.distributed = world_size > 1\n    args_to_pass = dict(cfg=cfg, logger=logger, local_rank=rank, world_size=world_size, distributed=args.distributed)\n    signature = inspect.signature(fn)\n    matching_args = {}\n    for key in args_to_pass.keys():\n        if key in signature.parameters.keys():\n            matching_args[key] = args_to_pass[key]\n    fn(**matching_args)\n\n    if world_size > 1:\n        cleanup()\n\n\ndef run(fn, defaults, description='', default_config='configs/experiment.yaml', world_size=1, write_log=True, no_cuda=False):\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\n        \"-c\", \"--config-file\",\n        default=default_config,\n        metavar=\"FILE\",\n        help=\"path to config file\",\n        type=str,\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"Modify config options using the command-line\",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    import multiprocessing\n    cpu_count = multiprocessing.cpu_count()\n    os.environ[\"OMP_NUM_THREADS\"] = str(max(1, int(cpu_count / world_size)))\n    del multiprocessing\n\n    args = parser.parse_args()\n\n    if world_size > 1:\n        mp.spawn(_run,\n                 args=(world_size, fn, defaults, write_log, no_cuda, args),\n                 nprocs=world_size,\n                 join=True)\n    else:\n        _run(0, world_size, fn, defaults, write_log, no_cuda, args)\n"
        },
        {
          "name": "lod_driver.py",
          "type": "blob",
          "size": 4.4462890625,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch\nimport math\nimport time\nfrom collections import defaultdict\n\n\nclass LODDriver:\n    def __init__(self, cfg, logger, world_size, dataset_size):\n        if world_size == 8:\n            self.lod_2_batch = cfg.TRAIN.LOD_2_BATCH_8GPU\n        if world_size == 4:\n            self.lod_2_batch = cfg.TRAIN.LOD_2_BATCH_4GPU\n        if world_size == 2:\n            self.lod_2_batch = cfg.TRAIN.LOD_2_BATCH_2GPU\n        if world_size == 1:\n            self.lod_2_batch = cfg.TRAIN.LOD_2_BATCH_1GPU\n\n        self.world_size = world_size\n        self.minibatch_base = 16\n        self.cfg = cfg\n        self.dataset_size = dataset_size\n        self.current_epoch = 0\n        self.lod = -1\n        self.in_transition = False\n        self.logger = logger\n        self.iteration = 0\n        self.epoch_end_time = 0\n        self.epoch_start_time = 0\n        self.per_epoch_ptime = 0\n        self.reports = cfg.TRAIN.REPORT_FREQ\n        self.snapshots = cfg.TRAIN.SNAPSHOT_FREQ\n        self.tick_start_nimg_report = 0\n        self.tick_start_nimg_snapshot = 0\n\n    def get_lod_power2(self):\n        return self.lod + 2\n\n    def get_batch_size(self):\n        return self.lod_2_batch[min(self.lod, len(self.lod_2_batch) - 1)]\n\n    def get_dataset_size(self):\n        return self.dataset_size\n\n    def get_per_GPU_batch_size(self):\n        return self.get_batch_size() // self.world_size\n\n    def get_blend_factor(self):\n        if self.cfg.TRAIN.EPOCHS_PER_LOD == 0:\n            return 1\n        blend_factor = float((self.current_epoch % self.cfg.TRAIN.EPOCHS_PER_LOD) * self.dataset_size + self.iteration)\n        blend_factor /= float(self.cfg.TRAIN.EPOCHS_PER_LOD // 2 * self.dataset_size)\n        blend_factor = math.sin(blend_factor * math.pi - 0.5 * math.pi) * 0.5 + 0.5\n\n        if not self.in_transition:\n            blend_factor = 1\n\n        return blend_factor\n\n    def is_time_to_report(self):\n        if self.iteration >= self.tick_start_nimg_report + self.reports[min(self.lod, len(self.reports) - 1)] * 1000:\n            self.tick_start_nimg_report = self.iteration\n            return True\n        return False\n\n    def is_time_to_save(self):\n        if self.iteration >= self.tick_start_nimg_snapshot + self.snapshots[min(self.lod, len(self.snapshots) - 1)] * 1000:\n            self.tick_start_nimg_snapshot = self.iteration\n            return True\n        return False\n\n    def step(self):\n        self.iteration += self.get_batch_size()\n        self.epoch_end_time = time.time()\n        self.per_epoch_ptime = self.epoch_end_time - self.epoch_start_time\n\n    def set_epoch(self, epoch, optimizers):\n        self.current_epoch = epoch\n        self.iteration = 0\n        self.tick_start_nimg_report = 0\n        self.tick_start_nimg_snapshot = 0\n        self.epoch_start_time = time.time()\n\n        if self.cfg.TRAIN.EPOCHS_PER_LOD == 0:\n            self.lod = self.cfg.MODEL.LAYER_COUNT - 1\n            return\n\n        new_lod = min(self.cfg.MODEL.LAYER_COUNT - 1, epoch // self.cfg.TRAIN.EPOCHS_PER_LOD)\n        if new_lod != self.lod:\n            self.lod = new_lod\n            self.logger.info(\"#\" * 80)\n            self.logger.info(\"# Switching LOD to %d\" % self.lod)\n            self.logger.info(\"# Starting transition\")\n            self.logger.info(\"#\" * 80)\n            self.in_transition = True\n            for opt in optimizers:\n                opt.state = defaultdict(dict)\n\n        is_in_first_half_of_cycle = (epoch % self.cfg.TRAIN.EPOCHS_PER_LOD) < (self.cfg.TRAIN.EPOCHS_PER_LOD // 2)\n        is_growing = epoch // self.cfg.TRAIN.EPOCHS_PER_LOD == self.lod > 0\n        new_in_transition = is_in_first_half_of_cycle and is_growing\n\n        if new_in_transition != self.in_transition:\n            self.in_transition = new_in_transition\n            self.logger.info(\"#\" * 80)\n            self.logger.info(\"# Transition ended\")\n            self.logger.info(\"#\" * 80)\n"
        },
        {
          "name": "losses.py",
          "type": "blob",
          "size": 1.916015625,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch\nimport torch.nn.functional as F\n\n\n__all__ = ['kl', 'reconstruction', 'discriminator_logistic_simple_gp',\n           'discriminator_gradient_penalty', 'generator_logistic_non_saturating']\n\n\ndef kl(mu, log_var):\n    return -0.5 * torch.mean(torch.mean(1 + log_var - mu.pow(2) - log_var.exp(), 1))\n\n\ndef reconstruction(recon_x, x, lod=None):\n    return torch.mean((recon_x - x)**2)\n\n\ndef discriminator_logistic_simple_gp(d_result_fake, d_result_real, reals, r1_gamma=10.0):\n    loss = (F.softplus(d_result_fake) + F.softplus(-d_result_real))\n\n    if r1_gamma != 0.0:\n        real_loss = d_result_real.sum()\n        real_grads = torch.autograd.grad(real_loss, reals, create_graph=True, retain_graph=True)[0]\n        r1_penalty = torch.sum(real_grads.pow(2.0), dim=[1, 2, 3])\n        loss = loss + r1_penalty * (r1_gamma * 0.5)\n    return loss.mean()\n\n\ndef discriminator_gradient_penalty(d_result_real, reals, r1_gamma=10.0):\n    real_loss = d_result_real.sum()\n    real_grads = torch.autograd.grad(real_loss, reals, create_graph=True, retain_graph=True)[0]\n    r1_penalty = torch.sum(real_grads.pow(2.0), dim=[1, 2, 3])\n    loss = r1_penalty * (r1_gamma * 0.5)\n    return loss.mean()\n\n\ndef generator_logistic_non_saturating(d_result_fake):\n    return F.softplus(-d_result_fake).mean()\n"
        },
        {
          "name": "lreq.py",
          "type": "blob",
          "size": 8.7392578125,
          "content": "# Copyright 2019 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\nfrom torch.nn.parameter import Parameter\nimport numpy as np\n\n\nclass Bool:\n    def __init__(self):\n        self.value = False\n\n    def __bool__(self):\n        return self.value\n    __nonzero__ = __bool__\n\n    def set(self, value):\n        self.value = value\n\n\nuse_implicit_lreq = Bool()\nuse_implicit_lreq.set(True)\n\n\ndef is_sequence(arg):\n    return (not hasattr(arg, \"strip\") and\n            hasattr(arg, \"__getitem__\") or\n            hasattr(arg, \"__iter__\"))\n\n\ndef make_tuple(x, n):\n    if is_sequence(x):\n        return x\n    return tuple([x for _ in range(n)])\n\n\nclass Linear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, gain=np.sqrt(2.0), lrmul=1.0, implicit_lreq=use_implicit_lreq):\n        super(Linear, self).__init__()\n        self.in_features = in_features\n        self.weight = Parameter(torch.Tensor(out_features, in_features))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.std = 0\n        self.gain = gain\n        self.lrmul = lrmul\n        self.implicit_lreq = implicit_lreq\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.std = self.gain / np.sqrt(self.in_features) * self.lrmul\n        if not self.implicit_lreq:\n            init.normal_(self.weight, mean=0, std=1.0 / self.lrmul)\n        else:\n            init.normal_(self.weight, mean=0, std=self.std / self.lrmul)\n            setattr(self.weight, 'lr_equalization_coef', self.std)\n            if self.bias is not None:\n                setattr(self.bias, 'lr_equalization_coef', self.lrmul)\n\n        if self.bias is not None:\n            with torch.no_grad():\n                self.bias.zero_()\n\n    def forward(self, input):\n        if not self.implicit_lreq:\n            bias = self.bias\n            if bias is not None:\n                bias = bias * self.lrmul\n            return F.linear(input, self.weight * self.std, bias)\n        else:\n            return F.linear(input, self.weight, self.bias)\n\n\nclass Conv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n                 groups=1, bias=True, gain=np.sqrt(2.0), transpose=False, transform_kernel=False, lrmul=1.0,\n                 implicit_lreq=use_implicit_lreq):\n        super(Conv2d, self).__init__()\n        if in_channels % groups != 0:\n            raise ValueError('in_channels must be divisible by groups')\n        if out_channels % groups != 0:\n            raise ValueError('out_channels must be divisible by groups')\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = make_tuple(kernel_size, 2)\n        self.stride = make_tuple(stride, 2)\n        self.padding = make_tuple(padding, 2)\n        self.output_padding = make_tuple(output_padding, 2)\n        self.dilation = make_tuple(dilation, 2)\n        self.groups = groups\n        self.gain = gain\n        self.lrmul = lrmul\n        self.transpose = transpose\n        self.fan_in = np.prod(self.kernel_size) * in_channels // groups\n        self.transform_kernel = transform_kernel\n        if transpose:\n            self.weight = Parameter(torch.Tensor(in_channels, out_channels // groups, *self.kernel_size))\n        else:\n            self.weight = Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.std = 0\n        self.implicit_lreq = implicit_lreq\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.std = self.gain / np.sqrt(self.fan_in) * self.lrmul\n        if not self.implicit_lreq:\n            init.normal_(self.weight, mean=0, std=1.0 / self.lrmul)\n        else:\n            init.normal_(self.weight, mean=0, std=self.std / self.lrmul)\n            setattr(self.weight, 'lr_equalization_coef', self.std)\n            if self.bias is not None:\n                setattr(self.bias, 'lr_equalization_coef', self.lrmul)\n\n        if self.bias is not None:\n            with torch.no_grad():\n                self.bias.zero_()\n\n    def forward(self, x):\n        if self.transpose:\n            w = self.weight\n            if self.transform_kernel:\n                w = F.pad(w, (1, 1, 1, 1), mode='constant')\n                w = w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n            if not self.implicit_lreq:\n                bias = self.bias\n                if bias is not None:\n                    bias = bias * self.lrmul\n                return F.conv_transpose2d(x, w * self.std, bias, stride=self.stride,\n                                          padding=self.padding, output_padding=self.output_padding,\n                                          dilation=self.dilation, groups=self.groups)\n            else:\n                return F.conv_transpose2d(x, w, self.bias, stride=self.stride, padding=self.padding,\n                                          output_padding=self.output_padding, dilation=self.dilation,\n                                          groups=self.groups)\n        else:\n            w = self.weight\n            if self.transform_kernel:\n                w = F.pad(w, (1, 1, 1, 1), mode='constant')\n                w = (w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]) * 0.25\n            if not self.implicit_lreq:\n                bias = self.bias\n                if bias is not None:\n                    bias = bias * self.lrmul\n                return F.conv2d(x, w * self.std, bias, stride=self.stride, padding=self.padding,\n                                dilation=self.dilation, groups=self.groups)\n            else:\n                return F.conv2d(x, w, self.bias, stride=self.stride, padding=self.padding,\n                                dilation=self.dilation, groups=self.groups)\n\n\nclass ConvTranspose2d(Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n                 groups=1, bias=True, gain=np.sqrt(2.0), transform_kernel=False, lrmul=1.0, implicit_lreq=use_implicit_lreq):\n        super(ConvTranspose2d, self).__init__(in_channels=in_channels,\n                                              out_channels=out_channels,\n                                              kernel_size=kernel_size,\n                                              stride=stride,\n                                              padding=padding,\n                                              output_padding=output_padding,\n                                              dilation=dilation,\n                                              groups=groups,\n                                              bias=bias,\n                                              gain=gain,\n                                              transpose=True,\n                                              transform_kernel=transform_kernel,\n                                              lrmul=lrmul,\n                                              implicit_lreq=implicit_lreq)\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n                 bias=True, gain=np.sqrt(2.0), transpose=False):\n        super(SeparableConv2d, self).__init__()\n        self.spatial_conv = Conv2d(in_channels, in_channels, kernel_size, stride, padding, output_padding, dilation,\n                                   in_channels, False, 1, transpose)\n        self.channel_conv = Conv2d(in_channels, out_channels, 1, bias, 1, gain=gain)\n\n    def forward(self, x):\n        return self.channel_conv(self.spatial_conv(x))\n\n\nclass SeparableConvTranspose2d(Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n                 bias=True, gain=np.sqrt(2.0)):\n        super(SeparableConvTranspose2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding,\n                                              output_padding, dilation, bias, gain, True)\n\n"
        },
        {
          "name": "make_figures",
          "type": "tree",
          "content": null
        },
        {
          "name": "metrics",
          "type": "tree",
          "content": null
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 8.255859375,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport random\nimport losses\nfrom net import *\nimport numpy as np\n\n\nclass DLatent(nn.Module):\n    def __init__(self, dlatent_size, layer_count):\n        super(DLatent, self).__init__()\n        buffer = torch.zeros(layer_count, dlatent_size, dtype=torch.float32)\n        self.register_buffer('buff', buffer)\n\n\nclass Model(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, mapping_layers=5, dlatent_avg_beta=None,\n                 truncation_psi=None, truncation_cutoff=None, style_mixing_prob=None, channels=3, generator=\"\",\n                 encoder=\"\", z_regression=False):\n        super(Model, self).__init__()\n\n        self.layer_count = layer_count\n        self.z_regression = z_regression\n\n        self.mapping_d = MAPPINGS[\"MappingD\"](\n            latent_size=latent_size,\n            dlatent_size=latent_size,\n            mapping_fmaps=latent_size,\n            mapping_layers=3)\n\n        self.mapping_f = MAPPINGS[\"MappingF\"](\n            num_layers=2 * layer_count,\n            latent_size=latent_size,\n            dlatent_size=latent_size,\n            mapping_fmaps=latent_size,\n            mapping_layers=mapping_layers)\n\n        self.decoder = GENERATORS[generator](\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            latent_size=latent_size,\n            channels=channels)\n\n        self.encoder = ENCODERS[encoder](\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            latent_size=latent_size,\n            channels=channels)\n\n        self.dlatent_avg = DLatent(latent_size, self.mapping_f.num_layers)\n        self.latent_size = latent_size\n        self.dlatent_avg_beta = dlatent_avg_beta\n        self.truncation_psi = truncation_psi\n        self.style_mixing_prob = style_mixing_prob\n        self.truncation_cutoff = truncation_cutoff\n\n    def generate(self, lod, blend_factor, z=None, count=32, mixing=True, noise=True, return_styles=False, no_truncation=False):\n        if z is None:\n            z = torch.randn(count, self.latent_size)\n        styles = self.mapping_f(z)[:, 0]\n        s = styles.view(styles.shape[0], 1, styles.shape[1])\n\n        styles = s.repeat(1, self.mapping_f.num_layers, 1)\n\n        if self.dlatent_avg_beta is not None:\n            with torch.no_grad():\n                batch_avg = styles.mean(dim=0)\n                self.dlatent_avg.buff.data.lerp_(batch_avg.data, 1.0 - self.dlatent_avg_beta)\n\n        if mixing and self.style_mixing_prob is not None:\n            if random.random() < self.style_mixing_prob:\n                z2 = torch.randn(count, self.latent_size)\n                styles2 = self.mapping_f(z2)[:, 0]\n                styles2 = styles2.view(styles2.shape[0], 1, styles2.shape[1]).repeat(1, self.mapping_f.num_layers, 1)\n\n                layer_idx = torch.arange(self.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n                cur_layers = (lod + 1) * 2\n                mixing_cutoff = random.randint(1, cur_layers)\n                styles = torch.where(layer_idx < mixing_cutoff, styles, styles2)\n\n        if (self.truncation_psi is not None) and not no_truncation:\n            layer_idx = torch.arange(self.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n            ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n            coefs = torch.where(layer_idx < self.truncation_cutoff, self.truncation_psi * ones, ones)\n            styles = torch.lerp(self.dlatent_avg.buff.data, styles, coefs)\n\n        rec = self.decoder.forward(styles, lod, blend_factor, noise)\n        if return_styles:\n            return s, rec\n        else:\n            return rec\n\n    def encode(self, x, lod, blend_factor):\n        Z = self.encoder(x, lod, blend_factor)\n        discriminator_prediction = self.mapping_d(Z)\n        return Z[:, :1], discriminator_prediction\n\n    def forward(self, x, lod, blend_factor, d_train, ae):\n        if ae:\n            self.encoder.requires_grad_(True)\n\n            z = torch.randn(x.shape[0], self.latent_size)\n            s, rec = self.generate(lod, blend_factor, z=z, mixing=False, noise=True, return_styles=True)\n\n            Z, d_result_real = self.encode(rec, lod, blend_factor)\n\n            assert Z.shape == s.shape\n\n            if self.z_regression:\n                Lae = torch.mean(((Z[:, 0] - z)**2))\n            else:\n                Lae = torch.mean(((Z - s.detach())**2))\n\n            return Lae\n\n        elif d_train:\n            with torch.no_grad():\n                Xp = self.generate(lod, blend_factor, count=x.shape[0], noise=True)\n\n            self.encoder.requires_grad_(True)\n\n            _, d_result_real = self.encode(x, lod, blend_factor)\n\n            _, d_result_fake = self.encode(Xp, lod, blend_factor)\n\n            loss_d = losses.discriminator_logistic_simple_gp(d_result_fake, d_result_real, x)\n            return loss_d\n        else:\n            with torch.no_grad():\n                z = torch.randn(x.shape[0], self.latent_size)\n\n            self.encoder.requires_grad_(False)\n\n            rec = self.generate(lod, blend_factor, count=x.shape[0], z=z.detach(), noise=True)\n\n            _, d_result_fake = self.encode(rec, lod, blend_factor)\n\n            loss_g = losses.generator_logistic_non_saturating(d_result_fake)\n\n            return loss_g\n\n    def lerp(self, other, betta):\n        if hasattr(other, 'module'):\n            other = other.module\n        with torch.no_grad():\n            params = list(self.mapping_d.parameters()) + list(self.mapping_f.parameters()) + list(self.decoder.parameters()) + list(self.encoder.parameters()) + list(self.dlatent_avg.parameters())\n            other_param = list(other.mapping_d.parameters()) + list(other.mapping_f.parameters()) + list(other.decoder.parameters()) + list(other.encoder.parameters()) + list(other.dlatent_avg.parameters())\n            for p, p_other in zip(params, other_param):\n                p.data.lerp_(p_other.data, 1.0 - betta)\n\n\nclass GenModel(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, mapping_layers=5, dlatent_avg_beta=None,\n                 truncation_psi=None, truncation_cutoff=None, style_mixing_prob=None, channels=3, generator=\"\", encoder=\"\", z_regression=False):\n        super(GenModel, self).__init__()\n\n        self.layer_count = layer_count\n\n        self.mapping_f = MAPPINGS[\"MappingF\"](\n            num_layers=2 * layer_count,\n            latent_size=latent_size,\n            dlatent_size=latent_size,\n            mapping_fmaps=latent_size,\n            mapping_layers=mapping_layers)\n\n        self.decoder = GENERATORS[generator](\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            latent_size=latent_size,\n            channels=channels)\n\n        self.dlatent_avg = DLatent(latent_size, self.mapping_f.num_layers)\n        self.latent_size = latent_size\n        self.dlatent_avg_beta = dlatent_avg_beta\n        self.truncation_psi = truncation_psi\n        self.style_mixing_prob = style_mixing_prob\n        self.truncation_cutoff = truncation_cutoff\n\n    def generate(self, lod, blend_factor, z=None):\n        styles = self.mapping_f(z)[:, 0]\n        s = styles.view(styles.shape[0], 1, styles.shape[1])\n\n        styles = s.repeat(1, self.mapping_f.num_layers, 1)\n\n        layer_idx = torch.arange(self.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < self.truncation_cutoff, self.truncation_psi * ones, ones)\n        styles = torch.lerp(self.dlatent_avg.buff.data, styles, coefs)\n\n        rec = self.decoder.forward(styles, lod, blend_factor, True)\n        return rec\n\n    def forward(self, x):\n        return self.generate(self.layer_count-1, 1.0, z=x)\n"
        },
        {
          "name": "net.py",
          "type": "blob",
          "size": 33.4208984375,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#  http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\nfrom torch.nn.parameter import Parameter\nimport numpy as np\nimport lreq as ln\nimport math\nfrom registry import *\n\n\ndef pixel_norm(x, epsilon=1e-8):\n    return x * torch.rsqrt(torch.mean(x.pow(2.0), dim=1, keepdim=True) + epsilon)\n\n\ndef style_mod(x, style):\n    style = style.view(style.shape[0], 2, x.shape[1], 1, 1)\n    return torch.addcmul(style[:, 1], value=1.0, tensor1=x, tensor2=style[:, 0] + 1)\n\n\ndef upscale2d(x, factor=2):\n    s = x.shape\n    x = torch.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n    x = x.repeat(1, 1, 1, factor, 1, factor)\n    x = torch.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n    return x\n\n\ndef downscale2d(x, factor=2):\n    return F.avg_pool2d(x, factor, factor)\n\n\nclass Blur(nn.Module):\n    def __init__(self, channels):\n        super(Blur, self).__init__()\n        f = np.array([1, 2, 1], dtype=np.float32)\n        f = f[:, np.newaxis] * f[np.newaxis, :]\n        f /= np.sum(f)\n        kernel = torch.Tensor(f).view(1, 1, 3, 3).repeat(channels, 1, 1, 1)\n        self.register_buffer('weight', kernel)\n        self.groups = channels\n\n    def forward(self, x):\n        return F.conv2d(x, weight=self.weight, groups=self.groups, padding=1)\n\n\nclass EncodeBlock(nn.Module):\n    def __init__(self, inputs, outputs, latent_size, last=False, fused_scale=True):\n        super(EncodeBlock, self).__init__()\n        self.conv_1 = ln.Conv2d(inputs, inputs, 3, 1, 1, bias=False)\n        self.bias_1 = nn.Parameter(torch.Tensor(1, inputs, 1, 1))\n        self.instance_norm_1 = nn.InstanceNorm2d(inputs, affine=False)\n        self.blur = Blur(inputs)\n        self.last = last\n        self.fused_scale = fused_scale\n        if last:\n            self.dense = ln.Linear(inputs * 4 * 4, outputs)\n        else:\n            if fused_scale:\n                self.conv_2 = ln.Conv2d(inputs, outputs, 3, 2, 1, bias=False, transform_kernel=True)\n            else:\n                self.conv_2 = ln.Conv2d(inputs, outputs, 3, 1, 1, bias=False)\n\n        self.bias_2 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.instance_norm_2 = nn.InstanceNorm2d(outputs, affine=False)\n        self.style_1 = ln.Linear(2 * inputs, latent_size)\n        if last:\n            self.style_2 = ln.Linear(outputs, latent_size)\n        else:\n            self.style_2 = ln.Linear(2 * outputs, latent_size)\n\n        with torch.no_grad():\n            self.bias_1.zero_()\n            self.bias_2.zero_()\n\n    def forward(self, x):\n        x = self.conv_1(x) + self.bias_1\n        x = F.leaky_relu(x, 0.2)\n\n        m = torch.mean(x, dim=[2, 3], keepdim=True)\n        std = torch.sqrt(torch.mean((x - m) ** 2, dim=[2, 3], keepdim=True))\n        style_1 = torch.cat((m, std), dim=1)\n\n        x = self.instance_norm_1(x)\n\n        if self.last:\n            x = self.dense(x.view(x.shape[0], -1))\n\n            x = F.leaky_relu(x, 0.2)\n            w1 = self.style_1(style_1.view(style_1.shape[0], style_1.shape[1]))\n            w2 = self.style_2(x.view(x.shape[0], x.shape[1]))\n        else:\n            x = self.conv_2(self.blur(x))\n            if not self.fused_scale:\n                x = downscale2d(x)\n            x = x + self.bias_2\n\n            x = F.leaky_relu(x, 0.2)\n\n            m = torch.mean(x, dim=[2, 3], keepdim=True)\n            std = torch.sqrt(torch.mean((x - m) ** 2, dim=[2, 3], keepdim=True))\n            style_2 = torch.cat((m, std), dim=1)\n\n            x = self.instance_norm_2(x)\n\n            w1 = self.style_1(style_1.view(style_1.shape[0], style_1.shape[1]))\n            w2 = self.style_2(style_2.view(style_2.shape[0], style_2.shape[1]))\n\n        return x, w1, w2\n\n\nclass DiscriminatorBlock(nn.Module):\n    def __init__(self, inputs, outputs, last=False, fused_scale=True, dense=False):\n        super(DiscriminatorBlock, self).__init__()\n        self.conv_1 = ln.Conv2d(inputs + (1 if last else 0), inputs, 3, 1, 1, bias=False)\n        self.bias_1 = nn.Parameter(torch.Tensor(1, inputs, 1, 1))\n        self.blur = Blur(inputs)\n        self.last = last\n        self.dense_ = dense\n        self.fused_scale = fused_scale\n        if self.dense_:\n            self.dense = ln.Linear(inputs * 4 * 4, outputs)\n        else:\n            if fused_scale:\n                self.conv_2 = ln.Conv2d(inputs, outputs, 3, 2, 1, bias=False, transform_kernel=True)\n            else:\n                self.conv_2 = ln.Conv2d(inputs, outputs, 3, 1, 1, bias=False)\n\n        self.bias_2 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n\n        with torch.no_grad():\n            self.bias_1.zero_()\n            self.bias_2.zero_()\n\n    def forward(self, x):\n        if self.last:\n            x = minibatch_stddev_layer(x)\n\n        x = self.conv_1(x) + self.bias_1\n        x = F.leaky_relu(x, 0.2)\n\n        if self.dense_:\n            x = self.dense(x.view(x.shape[0], -1))\n        else:\n            x = self.conv_2(self.blur(x))\n            if not self.fused_scale:\n                x = downscale2d(x)\n            x = x + self.bias_2\n        x = F.leaky_relu(x, 0.2)\n\n        return x\n\n\nclass DecodeBlock(nn.Module):\n    def __init__(self, inputs, outputs, latent_size, has_first_conv=True, fused_scale=True, layer=0):\n        super(DecodeBlock, self).__init__()\n        self.has_first_conv = has_first_conv\n        self.inputs = inputs\n        self.has_first_conv = has_first_conv\n        self.fused_scale = fused_scale\n        if has_first_conv:\n            if fused_scale:\n                self.conv_1 = ln.ConvTranspose2d(inputs, outputs, 3, 2, 1, bias=False, transform_kernel=True)\n            else:\n                self.conv_1 = ln.Conv2d(inputs, outputs, 3, 1, 1, bias=False)\n\n        self.blur = Blur(outputs)\n        self.noise_weight_1 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.noise_weight_1.data.zero_()\n        self.bias_1 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.instance_norm_1 = nn.InstanceNorm2d(outputs, affine=False, eps=1e-8)\n        self.style_1 = ln.Linear(latent_size, 2 * outputs, gain=1)\n\n        self.conv_2 = ln.Conv2d(outputs, outputs, 3, 1, 1, bias=False)\n        self.noise_weight_2 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.noise_weight_2.data.zero_()\n        self.bias_2 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.instance_norm_2 = nn.InstanceNorm2d(outputs, affine=False, eps=1e-8)\n        self.style_2 = ln.Linear(latent_size, 2 * outputs, gain=1)\n\n        self.layer = layer\n\n        with torch.no_grad():\n            self.bias_1.zero_()\n            self.bias_2.zero_()\n\n    def forward(self, x, s1, s2, noise):\n        if self.has_first_conv:\n            if not self.fused_scale:\n                x = upscale2d(x)\n            x = self.conv_1(x)\n            x = self.blur(x)\n\n        if noise:\n            if noise == 'batch_constant':\n                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_1,\n                                  tensor2=torch.randn([1, 1, x.shape[2], x.shape[3]]))\n            else:\n                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_1,\n                                  tensor2=torch.randn([x.shape[0], 1, x.shape[2], x.shape[3]]))\n        else:\n            s = math.pow(self.layer + 1, 0.5)\n            x = x + s * torch.exp(-x * x / (2.0 * s * s)) / math.sqrt(2 * math.pi) * 0.8\n        x = x + self.bias_1\n\n        x = F.leaky_relu(x, 0.2)\n\n        x = self.instance_norm_1(x)\n\n        x = style_mod(x, self.style_1(s1))\n\n        x = self.conv_2(x)\n\n        if noise:\n            if noise == 'batch_constant':\n                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_2,\n                                  tensor2=torch.randn([1, 1, x.shape[2], x.shape[3]]))\n            else:\n                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_2,\n                                  tensor2=torch.randn([x.shape[0], 1, x.shape[2], x.shape[3]]))\n        else:\n            s = math.pow(self.layer + 1, 0.5)\n            x = x + s * torch.exp(-x * x / (2.0 * s * s)) / math.sqrt(2 * math.pi) * 0.8\n\n        x = x + self.bias_2\n\n        x = F.leaky_relu(x, 0.2)\n        x = self.instance_norm_2(x)\n\n        x = style_mod(x, self.style_2(s2))\n\n        return x\n\n\nclass FromRGB(nn.Module):\n    def __init__(self, channels, outputs):\n        super(FromRGB, self).__init__()\n        self.from_rgb = ln.Conv2d(channels, outputs, 1, 1, 0)\n\n    def forward(self, x):\n        x = self.from_rgb(x)\n        x = F.leaky_relu(x, 0.2)\n\n        return x\n\n\nclass ToRGB(nn.Module):\n    def __init__(self, inputs, channels):\n        super(ToRGB, self).__init__()\n        self.inputs = inputs\n        self.channels = channels\n        self.to_rgb = ln.Conv2d(inputs, channels, 1, 1, 0, gain=0.03)\n\n    def forward(self, x):\n        x = self.to_rgb(x)\n        return x\n\n\n# Default Encoder. E network\n@ENCODERS.register(\"EncoderDefault\")\nclass EncoderDefault(nn.Module):\n    def __init__(self, startf, maxf, layer_count, latent_size, channels=3):\n        super(EncoderDefault, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb: nn.ModuleList[FromRGB] = nn.ModuleList()\n        self.channels = channels\n        self.latent_size = latent_size\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[EncodeBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = EncodeBlock(inputs, outputs, latent_size, False, fused_scale=fused_scale)\n\n            resolution //= 2\n\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n    def encode(self, x, lod):\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        x, s1, s2 = self.encode_block[self.layer_count - lod - 1](x)\n        styles[:, 0] += s1 * blend + s2 * blend\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n    def get_statistics(self, lod):\n        rgb_std = self.from_rgb[self.layer_count - lod - 1].from_rgb.weight.std().item()\n        rgb_std_c = self.from_rgb[self.layer_count - lod - 1].from_rgb.std\n\n        layers = []\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            conv_1 = self.encode_block[i].conv_1.weight.std().item()\n            conv_1_c = self.encode_block[i].conv_1.std\n            conv_2 = self.encode_block[i].conv_2.weight.std().item()\n            conv_2_c = self.encode_block[i].conv_2.std\n            layers.append(((conv_1 / conv_1_c), (conv_2 / conv_2_c)))\n        return rgb_std / rgb_std_c, layers\n\n\n# For ablation only. Not used in default configuration\n@ENCODERS.register(\"EncoderWithFC\")\nclass EncoderWithFC(nn.Module):\n    def __init__(self, startf, maxf, layer_count, latent_size, channels=3):\n        super(EncoderWithFC, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb: nn.ModuleList[FromRGB] = nn.ModuleList()\n        self.channels = channels\n        self.latent_size = latent_size\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[EncodeBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = EncodeBlock(inputs, outputs, latent_size, i == self.layer_count - 1, fused_scale=fused_scale)\n\n            resolution //= 2\n\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n        self.fc2 = ln.Linear(inputs, 1, gain=1)\n\n    def encode(self, x, lod):\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles, self.fc2(x)\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        x, s1, s2 = self.encode_block[self.layer_count - lod - 1](x)\n        styles[:, 0] += s1 * blend + s2 * blend\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles, self.fc2(x)\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n    def get_statistics(self, lod):\n        rgb_std = self.from_rgb[self.layer_count - lod - 1].from_rgb.weight.std().item()\n        rgb_std_c = self.from_rgb[self.layer_count - lod - 1].from_rgb.std\n\n        layers = []\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            conv_1 = self.encode_block[i].conv_1.weight.std().item()\n            conv_1_c = self.encode_block[i].conv_1.std\n            conv_2 = self.encode_block[i].conv_2.weight.std().item()\n            conv_2_c = self.encode_block[i].conv_2.std\n            layers.append(((conv_1 / conv_1_c), (conv_2 / conv_2_c)))\n        return rgb_std / rgb_std_c, layers\n\n\n@ENCODERS.register(\"EncoderWithStatistics\")\nclass Encoder(nn.Module):\n    def __init__(self, startf, maxf, layer_count, latent_size, channels=3):\n        super(Encoder, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb: nn.ModuleList[FromRGB] = nn.ModuleList()\n        self.channels = channels\n        self.latent_size = latent_size\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[EncodeBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = EncodeBlock(inputs, outputs, latent_size, i == self.layer_count - 1, fused_scale=fused_scale)\n\n            resolution //= 2\n\n            #print(\"encode_block%d %s styles out: %d\" % ((i + 1), millify(count_parameters(block)), inputs))\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n    def encode(self, x, lod):\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        x, s1, s2 = self.encode_block[self.layer_count - lod - 1](x)\n        styles[:, 0] += s1 * blend + s2 * blend\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n    def get_statistics(self, lod):\n        rgb_std = self.from_rgb[self.layer_count - lod - 1].from_rgb.weight.std().item()\n        rgb_std_c = self.from_rgb[self.layer_count - lod - 1].from_rgb.std\n\n        layers = []\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            conv_1 = self.encode_block[i].conv_1.weight.std().item()\n            conv_1_c = self.encode_block[i].conv_1.std\n            conv_2 = self.encode_block[i].conv_2.weight.std().item()\n            conv_2_c = self.encode_block[i].conv_2.std\n            layers.append(((conv_1 / conv_1_c), (conv_2 / conv_2_c)))\n        return rgb_std / rgb_std_c, layers\n\n\n# For ablation only. Not used in default configuration\n@ENCODERS.register(\"EncoderNoStyle\")\nclass EncoderNoStyle(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=512, channels=3):\n        super(EncoderNoStyle, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb = nn.ModuleList()\n        self.channels = channels\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[DiscriminatorBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = DiscriminatorBlock(inputs, outputs, last=False, fused_scale=fused_scale, dense=i == self.layer_count - 1)\n\n            resolution //= 2\n\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n        self.fc2 = ln.Linear(inputs, latent_size, gain=1)\n\n    def encode(self, x, lod):\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x = self.encode_block[i](x)\n\n        return self.fc2(x).view(x.shape[0], 1, x.shape[1])\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.encode_block[self.layer_count - lod - 1](x)\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x = self.encode_block[i](x)\n\n        return self.fc2(x).view(x.shape[0], 1, x.shape[1])\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n\n# For ablation only. Not used in default configuration\n@DISCRIMINATORS.register(\"DiscriminatorDefault\")\nclass Discriminator(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, channels=3):\n        super(Discriminator, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb = nn.ModuleList()\n        self.channels = channels\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[DiscriminatorBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = DiscriminatorBlock(inputs, outputs, i == self.layer_count - 1, fused_scale=fused_scale)\n\n            resolution //= 2\n\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n        self.fc2 = ln.Linear(inputs, 1, gain=1)\n\n    def encode(self, x, lod):\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x = self.encode_block[i](x)\n\n        return self.fc2(x)\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.encode_block[self.layer_count - lod - 1](x)\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x = self.encode_block[i](x)\n\n        return self.fc2(x)\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n\n@GENERATORS.register(\"GeneratorDefault\")\nclass Generator(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, channels=3):\n        super(Generator, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n\n        self.channels = channels\n        self.latent_size = latent_size\n\n        mul = 2 ** (self.layer_count - 1)\n\n        inputs = min(self.maxf, startf * mul)\n        self.const = Parameter(torch.Tensor(1, inputs, 4, 4))\n        init.ones_(self.const)\n\n        self.layer_to_resolution = [0 for _ in range(layer_count)]\n        resolution = 2\n\n        self.style_sizes = []\n\n        to_rgb = nn.ModuleList()\n\n        self.decode_block: nn.ModuleList[DecodeBlock] = nn.ModuleList()\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            has_first_conv = i != 0\n            fused_scale = resolution * 2 >= 128\n\n            block = DecodeBlock(inputs, outputs, latent_size, has_first_conv, fused_scale=fused_scale, layer=i)\n\n            resolution *= 2\n            self.layer_to_resolution[i] = resolution\n\n            self.style_sizes += [2 * (inputs if has_first_conv else outputs), 2 * outputs]\n\n            to_rgb.append(ToRGB(outputs, channels))\n\n            self.decode_block.append(block)\n            inputs = outputs\n            mul //= 2\n\n        self.to_rgb = to_rgb\n\n    def decode(self, styles, lod, noise):\n        x = self.const\n\n        for i in range(lod + 1):\n            x = self.decode_block[i](x, styles[:, 2 * i + 0], styles[:, 2 * i + 1], noise)\n\n        x = self.to_rgb[lod](x)\n        return x\n\n    def decode2(self, styles, lod, blend, noise):\n        x = self.const\n\n        for i in range(lod):\n            x = self.decode_block[i](x, styles[:, 2 * i + 0], styles[:, 2 * i + 1], noise)\n\n        x_prev = self.to_rgb[lod - 1](x)\n\n        x = self.decode_block[lod](x, styles[:, 2 * lod + 0], styles[:, 2 * lod + 1], noise)\n        x = self.to_rgb[lod](x)\n\n        needed_resolution = self.layer_to_resolution[lod]\n\n        x_prev = F.interpolate(x_prev, size=needed_resolution)\n        x = torch.lerp(x_prev, x, blend)\n\n        return x\n\n    def forward(self, styles, lod, blend, noise):\n        if blend == 1:\n            return self.decode(styles, lod, noise)\n        else:\n            return self.decode2(styles, lod, blend, noise)\n\n    def get_statistics(self, lod):\n        rgb_std = self.to_rgb[lod].to_rgb.weight.std().item()\n        rgb_std_c = self.to_rgb[lod].to_rgb.std\n\n        layers = []\n        for i in range(lod + 1):\n            conv_1 = 1.0\n            conv_1_c = 1.0\n            if i != 0:\n                conv_1 = self.decode_block[i].conv_1.weight.std().item()\n                conv_1_c = self.decode_block[i].conv_1.std\n            conv_2 = self.decode_block[i].conv_2.weight.std().item()\n            conv_2_c = self.decode_block[i].conv_2.std\n            layers.append(((conv_1 / conv_1_c), (conv_2 / conv_2_c)))\n        return rgb_std / rgb_std_c, layers\n\n\ndef minibatch_stddev_layer(x, group_size=4):\n    group_size = min(group_size, x.shape[0])\n    size = x.shape[0]\n    if x.shape[0] % group_size != 0:\n        x = torch.cat([x, x[:(group_size - (x.shape[0] % group_size)) % group_size]])\n    y = x.view(group_size, -1, x.shape[1], x.shape[2], x.shape[3])\n    y = y - y.mean(dim=0, keepdim=True)\n    y = torch.sqrt((y ** 2).mean(dim=0) + 1e-8).mean(dim=[1, 2, 3], keepdim=True)\n    y = y.repeat(group_size, 1, x.shape[2], x.shape[3])\n    return torch.cat([x, y], dim=1)[:size]\n\n\nimage_size = 64\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 24\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n\n@GENERATORS.register(\"DCGANGenerator\")\nclass DCGANGenerator(nn.Module):\n    def __init__(self):\n        super(DCGANGenerator, self).__init__()\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, 512, 4, 1, 0),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(128, nc, 4, 2, 1),\n            #nn.BatchNorm2d(ngf),\n            #nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            #nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=True),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, x):\n        return self.main(x.view(x.shape[0], nz, 1, 1))\n\n\n@ENCODERS.register(\"DCGANEncoder\")\nclass DCGANEncoder(nn.Module):\n    def __init__(self):\n        super(DCGANEncoder, self).__init__()\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            #nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            #nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(nc, 64, 4, 2, 1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(128, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(256, 24, 4, 1, 0),\n            nn.LeakyReLU(0.01),\n        )\n\n    def forward(self, x):\n        x = self.main(x)\n        return x.view(x.shape[0], x.shape[1])\n\n\nclass MappingBlock(nn.Module):\n    def __init__(self, inputs, output, lrmul):\n        super(MappingBlock, self).__init__()\n        self.fc = ln.Linear(inputs, output, lrmul=lrmul)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.fc(x), 0.2)\n        return x\n\n\n# For ablation only. Not used in default configuration\n@MAPPINGS.register(\"MappingDefault\")\nclass Mapping(nn.Module):\n    def __init__(self, num_layers, mapping_layers=5, latent_size=256, dlatent_size=256, mapping_fmaps=256):\n        super(Mapping, self).__init__()\n        inputs = latent_size\n        self.mapping_layers = mapping_layers\n        self.num_layers = num_layers\n        for i in range(mapping_layers):\n            outputs = dlatent_size if i == mapping_layers - 1 else mapping_fmaps\n            block = MappingBlock(inputs, outputs, lrmul=0.01)\n            inputs = outputs\n            setattr(self, \"block_%d\" % (i + 1), block)\n\n    def forward(self, z):\n        x = pixel_norm(z)\n\n        for i in range(self.mapping_layers):\n            x = getattr(self, \"block_%d\" % (i + 1))(x)\n\n        return x.view(x.shape[0], 1, x.shape[1]).repeat(1, self.num_layers, 1)\n\n\n# Used in default configuration. The D network\n@MAPPINGS.register(\"MappingD\")\nclass MappingD(nn.Module):\n    def __init__(self, mapping_layers=5, latent_size=256, dlatent_size=256, mapping_fmaps=256):\n        super(MappingD, self).__init__()\n        inputs = latent_size\n        self.mapping_layers = mapping_layers\n        self.map_blocks: nn.ModuleList[MappingBlock] = nn.ModuleList()\n        for i in range(mapping_layers):\n            outputs = 2 * dlatent_size if i == mapping_layers - 1 else mapping_fmaps\n            block = ln.Linear(inputs, outputs, lrmul=0.1)\n            inputs = outputs\n            self.map_blocks.append(block)\n\n    def forward(self, x):\n        for i in range(self.mapping_layers):\n            x = self.map_blocks[i](x)\n        # We select just one output. For compatibility with older models.\n        # All other outputs are ignored\n        # It is the same as if the last layer had one output.\n        return x[:, 0, x.shape[2] // 2]\n\n\n@MAPPINGS.register(\"MappingDNoStyle\")\nclass MappingDNoStyle(nn.Module):\n    def __init__(self, mapping_layers=5, latent_size=256, dlatent_size=256, mapping_fmaps=256):\n        super(MappingDNoStyle, self).__init__()\n        inputs = latent_size\n        self.mapping_layers = mapping_layers\n        self.map_blocks: nn.ModuleList[MappingBlock] = nn.ModuleList()\n        for i in range(mapping_layers):\n            outputs = dlatent_size if i == mapping_layers - 1 else mapping_fmaps\n            block = ln.Linear(inputs, outputs, lrmul=0.1)\n            inputs = outputs\n            self.map_blocks.append(block)\n\n    def forward(self, x):\n        for i in range(self.mapping_layers):\n            x = self.map_blocks[i](x)\n        return x[:, 0]\n\n\n@MAPPINGS.register(\"MappingF\")\nclass MappingF(nn.Module):\n    def __init__(self, num_layers, mapping_layers=5, latent_size=256, dlatent_size=256, mapping_fmaps=256):\n        super(MappingF, self).__init__()\n        inputs = dlatent_size\n        self.mapping_layers = mapping_layers\n        self.num_layers = num_layers\n        self.map_blocks: nn.ModuleList[MappingBlock] = nn.ModuleList()\n        for i in range(mapping_layers):\n            outputs = latent_size if i == mapping_layers - 1 else mapping_fmaps\n            block = MappingBlock(inputs, outputs, lrmul=0.1)\n            inputs = outputs\n            self.map_blocks.append(block)\n\n    def forward(self, x):\n        x = pixel_norm(x)\n\n        for i in range(self.mapping_layers):\n            x = self.map_blocks[i](x)\n\n        return x.view(x.shape[0], 1, x.shape[1]).repeat(1, self.num_layers, 1)\n\n\n@ENCODERS.register(\"EncoderFC\")\nclass EncoderFC(nn.Module):\n    def __init__(self, startf, maxf, layer_count, latent_size, channels=3):\n        super(EncoderFC, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.channels = channels\n        self.latent_size = latent_size\n\n        self.fc_1 = ln.Linear(28 * 28, 1024)\n        self.fc_2 = ln.Linear(1024, 1024)\n        self.fc_3 = ln.Linear(1024, latent_size)\n\n    def encode(self, x, lod):\n        x = F.interpolate(x, 28)\n        x = x.view(x.shape[0], 28 * 28)\n\n        x = self.fc_1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc_2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc_3(x)\n        x = F.leaky_relu(x, 0.2)\n\n        return x\n\n    def forward(self, x, lod, blend):\n        return self.encode(x, lod)\n\n\n@GENERATORS.register(\"GeneratorFC\")\nclass GeneratorFC(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, channels=3):\n        super(GeneratorFC, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.channels = channels\n        self.latent_size = latent_size\n\n        self.fc_1 = ln.Linear(latent_size, 1024)\n        self.fc_2 = ln.Linear(1024, 1024)\n        self.fc_3 = ln.Linear(1024, 28 * 28)\n\n        self.layer_to_resolution = [28] * 10\n\n    def decode(self, x, lod, blend_factor, noise):\n        if len(x.shape) == 3:\n            x = x[:, 0]  # no styles\n        x.view(x.shape[0], self.latent_size)\n\n        x = self.fc_1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc_2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc_3(x)\n\n        x = x.view(x.shape[0], 1, 28, 28)\n        x = F.interpolate(x, 2 ** (2 + lod))\n        return x\n\n    def forward(self, x, lod, blend_factor, noise):\n        return self.decode(x, lod, blend_factor, noise)\n"
        },
        {
          "name": "principal_directions",
          "type": "tree",
          "content": null
        },
        {
          "name": "registry.py",
          "type": "blob",
          "size": 0.140625,
          "content": "from utils import Registry\n\nMODELS = Registry()\nENCODERS = Registry()\nGENERATORS = Registry()\nMAPPINGS = Registry()\nDISCRIMINATORS = Registry()\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.123046875,
          "content": "packaging\nimageio\nnumpy\nscipy\ntqdm\ndlutils\nbimpy >= 0.1.1\ndareblopy >= 0.0.5\ntorch >= 1.3\ntorchvision\nsklearn\nyacs\nmatplotlib\n"
        },
        {
          "name": "scheduler.py",
          "type": "blob",
          "size": 3.4921875,
          "content": "from bisect import bisect_right\nimport torch\nimport numpy as np\n\n\nclass WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self,\n        optimizer,\n        milestones,\n        gamma=0.1,\n        warmup_factor=1.0 / 1.0,\n        warmup_iters=1,\n        last_epoch=-1,\n        reference_batch_size=128,\n        lr=[]\n    ):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError(\n                \"Milestones should be a list of\" \" increasing integers. Got {}\",\n                milestones,\n            )\n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.batch_size = 1\n        self.lod = 0\n        self.reference_batch_size = reference_batch_size\n\n        self.optimizer = optimizer\n        self.base_lrs = []\n        for _ in self.optimizer.param_groups:\n            self.base_lrs.append(lr)\n\n        self.last_epoch = last_epoch\n\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if last_epoch == -1:\n            for group in optimizer.param_groups:\n                group.setdefault('initial_lr', group['lr'])\n            last_epoch = 0\n\n        self.last_epoch = last_epoch\n\n        self.optimizer._step_count = 0\n        self._step_count = 0\n        self.step(last_epoch)\n\n    def set_batch_size(self, batch_size, lod):\n        self.batch_size = batch_size\n        self.lod = lod\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def get_lr(self):\n        warmup_factor = 1\n        if self.last_epoch < self.warmup_iters:\n            alpha = float(self.last_epoch) / self.warmup_iters\n            warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n        return [\n            base_lr[self.lod]\n            * warmup_factor\n            * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n            # * float(self.batch_size)\n            # / float(self.reference_batch_size)\n            for base_lr in self.base_lrs\n        ]\n\n    def state_dict(self):\n        return {\n            \"last_epoch\": self.last_epoch\n        }\n\n    def load_state_dict(self, state_dict):\n        self.__dict__.update(dict(last_epoch=state_dict[\"last_epoch\"]))\n\n\nclass ComboMultiStepLR:\n    def __init__(\n        self,\n        optimizers, base_lr,\n        **kwargs\n    ):\n        self.schedulers = dict()\n        for name, opt in optimizers.items():\n            self.schedulers[name] = WarmupMultiStepLR(opt, lr=base_lr, **kwargs)\n        self.last_epoch = 0\n\n    def set_batch_size(self, batch_size, lod):\n        for x in self.schedulers.values():\n            x.set_batch_size(batch_size, lod)\n\n    def step(self, epoch=None):\n        for x in self.schedulers.values():\n            x.step(epoch)\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n\n    def state_dict(self):\n        return {key: value.state_dict() for key, value in self.schedulers.items()}\n\n    def load_state_dict(self, state_dict):\n        for k, x in self.schedulers.items():\n            x.load_state_dict(state_dict[k])\n\n        last_epochs = [x.last_epoch for k, x in self.schedulers.items()]\n        assert np.all(np.asarray(last_epochs) == last_epochs[0])\n        self.last_epoch = last_epochs[0]\n\n    def start_epoch(self):\n        return self.last_epoch\n"
        },
        {
          "name": "style_mixing",
          "type": "tree",
          "content": null
        },
        {
          "name": "tracker.py",
          "type": "blob",
          "size": 4.5302734375,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport csv\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use('Agg')\nimport numpy as np\nimport torch\nimport os\n\n\nclass RunningMean:\n    def __init__(self):\n        self._mean = 0.0\n        self.n = 0\n\n    def __iadd__(self, value):\n        self._mean = (float(value) + self._mean * self.n)/(self.n + 1)\n        self.n += 1\n        return self\n\n    def reset(self):\n        self._mean = 0.0\n        self.n = 0\n\n    def mean(self):\n        return self._mean\n\n\nclass RunningMeanTorch:\n    def __init__(self):\n        self.values = []\n\n    def __iadd__(self, value):\n        with torch.no_grad():\n            self.values.append(value.detach().cpu().unsqueeze(0))\n            return self\n\n    def reset(self):\n        self.values = []\n\n    def mean(self):\n        with torch.no_grad():\n            if len(self.values) == 0:\n                return 0.0\n            return float(torch.cat(self.values).mean().item())\n\n\nclass LossTracker:\n    def __init__(self, output_folder='.'):\n        self.tracks = OrderedDict()\n        self.epochs = []\n        self.means_over_epochs = OrderedDict()\n        self.output_folder = output_folder\n\n    def update(self, d):\n        for k, v in d.items():\n            if k not in self.tracks:\n                self.add(k, isinstance(v, torch.Tensor))\n            self.tracks[k] += v\n\n    def add(self, name, pytorch=True):\n        assert name not in self.tracks, \"Name is already used\"\n        if pytorch:\n            track = RunningMeanTorch()\n        else:\n            track = RunningMean()\n        self.tracks[name] = track\n        self.means_over_epochs[name] = []\n        return track\n\n    def register_means(self, epoch):\n        self.epochs.append(epoch)\n\n        for key in self.means_over_epochs.keys():\n            if key in self.tracks:\n                value = self.tracks[key]\n                self.means_over_epochs[key].append(value.mean())\n                value.reset()\n            else:\n                self.means_over_epochs[key].append(None)\n\n        with open(os.path.join(self.output_folder, 'log.csv'), mode='w') as csv_file:\n            fieldnames = ['epoch'] + list(self.tracks.keys())\n            writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n            writer.writerow(fieldnames)\n            for i in range(len(self.epochs)):\n                writer.writerow([self.epochs[i]] + [self.means_over_epochs[x][i] for x in self.tracks.keys()])\n\n    def __str__(self):\n        result = \"\"\n        for key, value in self.tracks.items():\n            result += \"%s: %.7f, \" % (key, value.mean())\n        return result[:-2]\n\n    def plot(self):\n        fig = plt.figure()\n        fig.set_size_inches(12, 8)\n        ax = fig.add_subplot(111)\n        for key in self.tracks.keys():\n            try:\n                plt.plot(self.epochs, self.means_over_epochs[key], label=key)\n            except ValueError:\n                continue\n\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('Loss')\n\n        ax.legend(loc=4)\n        ax.grid(True)\n        fig.tight_layout()\n\n        fig.savefig(os.path.join(self.output_folder, 'plot.png'))\n        fig.clf()\n        plt.close()\n\n    def state_dict(self):\n        return {\n            'tracks': self.tracks,\n            'epochs': self.epochs,\n            'means_over_epochs': self.means_over_epochs}\n\n    def load_state_dict(self, state_dict):\n        self.tracks = state_dict['tracks']\n        self.epochs = state_dict['epochs']\n        self.means_over_epochs = state_dict['means_over_epochs']\n\n        counts = list(map(len, self.means_over_epochs.values()))\n\n        if len(counts) == 0:\n            counts = [0]\n        m = min(counts)\n\n        if m < len(self.epochs):\n            self.epochs = self.epochs[:m]\n\n        for key in self.means_over_epochs.keys():\n            if len(self.means_over_epochs[key]) > m:\n                self.means_over_epochs[key] = self.means_over_epochs[key][:m]\n"
        },
        {
          "name": "train_alae.py",
          "type": "blob",
          "size": 13.87890625,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#  http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nfrom net import *\nimport os\nimport utils\nfrom checkpointer import Checkpointer\nfrom scheduler import ComboMultiStepLR\nfrom custom_adam import LREQAdam\nfrom dataloader import *\nfrom tqdm import tqdm\nfrom dlutils.pytorch import count_parameters\nimport dlutils.pytorch.count_parameters as count_param_override\nfrom tracker import LossTracker\nfrom model import Model\nfrom launcher import run\nfrom defaults import get_cfg_defaults\nimport lod_driver\nfrom PIL import Image\n\n\ndef save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cmodel, cfg, encoder_optimizer, decoder_optimizer):\n    os.makedirs('results', exist_ok=True)\n\n    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (\n        (lod2batch.current_epoch + 1), cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker),\n        lod2batch.get_blend_factor(),\n        encoder_optimizer.param_groups[0]['lr'], decoder_optimizer.param_groups[0]['lr'],\n        torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n\n    with torch.no_grad():\n        model.eval()\n        cmodel.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] > needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n        assert sample_in.shape[2] == needed_resolution\n\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n\n        Z, _ = model.encode(sample_in, lod2batch.lod, blend_factor)\n\n        if cfg.MODEL.Z_REGRESSION:\n            Z = model.mapping_f(Z[:, 0])\n        else:\n            Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        rec2 = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n\n        Z = model.mapping_f(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n\n        Z = cmodel.mapping_f(samplez)\n        cg_rec = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec, cg_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR,\n                             'sample_%d_%d.jpg' % (\n                                 lod2batch.current_epoch + 1,\n                                 lod2batch.iteration // 1000)\n                             )\n            print(\"Saved to %s\" % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n\n        save_pic(resultsample)\n\n\ndef train(cfg, logger, local_rank, world_size, distributed):\n    torch.cuda.set_device(local_rank)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA,\n        style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER,\n        z_regression=cfg.MODEL.Z_REGRESSION\n    )\n    model.cuda(local_rank)\n    model.train()\n\n    if local_rank == 0:\n        model_s = Model(\n            startf=cfg.MODEL.START_CHANNEL_COUNT,\n            layer_count=cfg.MODEL.LAYER_COUNT,\n            maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n            latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n            truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n            truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n            mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n            channels=cfg.MODEL.CHANNELS,\n            generator=cfg.MODEL.GENERATOR,\n            encoder=cfg.MODEL.ENCODER,\n            z_regression=cfg.MODEL.Z_REGRESSION)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[local_rank],\n            broadcast_buffers=False,\n            bucket_cap_mb=25,\n            find_unused_parameters=True)\n        model.device_ids = None\n\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        mapping_d = model.module.mapping_d\n        mapping_f = model.module.mapping_f\n\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        mapping_d = model.mapping_d\n        mapping_f = model.mapping_f\n        dlatent_avg = model.dlatent_avg\n\n    count_param_override.print = lambda a: logger.info(a)\n\n    logger.info(\"Trainable parameters generator:\")\n    count_parameters(decoder)\n\n    logger.info(\"Trainable parameters discriminator:\")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[\"iteration\"] = 0\n\n    decoder_optimizer = LREQAdam([\n        {'params': decoder.parameters()},\n        {'params': mapping_f.parameters()}\n    ], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n\n    encoder_optimizer = LREQAdam([\n        {'params': encoder.parameters()},\n        {'params': mapping_d.parameters()},\n    ], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n\n    scheduler = ComboMultiStepLR(optimizers=\n                                 {\n                                    'encoder_optimizer': encoder_optimizer,\n                                    'decoder_optimizer': decoder_optimizer\n                                 },\n                                 milestones=cfg.TRAIN.LEARNING_DECAY_STEPS,\n                                 gamma=cfg.TRAIN.LEARNING_DECAY_RATE,\n                                 reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n\n    model_dict = {\n        'discriminator': encoder,\n        'generator': decoder,\n        'mapping_tl': mapping_d,\n        'mapping_fl': mapping_f,\n        'dlatent_avg': dlatent_avg\n    }\n    if local_rank == 0:\n        model_dict['discriminator_s'] = model_s.encoder\n        model_dict['generator_s'] = model_s.decoder\n        model_dict['mapping_tl_s'] = model_s.mapping_d\n        model_dict['mapping_fl_s'] = model_s.mapping_f\n\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {\n                                    'encoder_optimizer': encoder_optimizer,\n                                    'decoder_optimizer': decoder_optimizer,\n                                    'scheduler': scheduler,\n                                    'tracker': tracker\n                                },\n                                logger=logger,\n                                save=local_rank == 0)\n\n    extra_checkpoint_data = checkpointer.load()\n    logger.info(\"Starting from epoch: %d\" % (scheduler.start_epoch()))\n\n    arguments.update(extra_checkpoint_data)\n\n    layer_to_resolution = decoder.layer_to_resolution\n\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n\n    if cfg.DATASET.SAMPLES_PATH != 'no_path':\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = (sample / 127.5 - 1.)\n\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n\n        logger.info(\"Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d\" % (\n                                                                lod2batch.get_batch_size(),\n                                                                lod2batch.get_per_GPU_batch_size(),\n                                                                lod2batch.lod,\n                                                                2 ** lod2batch.get_lod_power2(),\n                                                                2 ** lod2batch.get_lod_power2(),\n                                                                lod2batch.get_blend_factor(),\n                                                                len(dataset) * world_size))\n\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n\n        model.train()\n\n        need_permute = False\n        epoch_start_time = time.time()\n\n        i = 0\n        for x_orig in tqdm(batches):\n            i += 1\n            with torch.no_grad():\n                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                    continue\n                if need_permute:\n                    x_orig = x_orig.permute(0, 3, 1, 2)\n                x_orig = (x_orig / 127.5 - 1.)\n\n                blend_factor = lod2batch.get_blend_factor()\n\n                needed_resolution = layer_to_resolution[lod2batch.lod]\n                x = x_orig\n\n                if lod2batch.in_transition:\n                    needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                    x_prev = F.avg_pool2d(x_orig, 2, 2)\n                    x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                    x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n\n            x.requires_grad = True\n\n            encoder_optimizer.zero_grad()\n            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n            tracker.update(dict(loss_d=loss_d))\n            loss_d.backward()\n            encoder_optimizer.step()\n\n            decoder_optimizer.zero_grad()\n            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n            tracker.update(dict(loss_g=loss_g))\n            loss_g.backward()\n            decoder_optimizer.step()\n\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n            tracker.update(dict(lae=lae))\n            lae.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            if local_rank == 0:\n                betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                model_s.lerp(model, betta)\n\n            epoch_end_time = time.time()\n            per_epoch_ptime = epoch_end_time - epoch_start_time\n\n            lod_for_saving_model = lod2batch.lod\n            lod2batch.step()\n            if local_rank == 0:\n                if lod2batch.is_time_to_save():\n                    checkpointer.save(\"model_tmp_intermediate_lod%d\" % lod_for_saving_model)\n                if lod2batch.is_time_to_report():\n                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s,\n                                model.module if hasattr(model, \"module\") else model, cfg, encoder_optimizer,\n                                decoder_optimizer)\n\n        scheduler.step()\n\n        if local_rank == 0:\n            checkpointer.save(\"model_tmp_lod%d\" % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s,\n                        model.module if hasattr(model, \"module\") else model, cfg, encoder_optimizer, decoder_optimizer)\n\n    logger.info(\"Training finish!... save training results\")\n    if local_rank == 0:\n        checkpointer.save(\"model_final\").wait()\n\n\nif __name__ == \"__main__\":\n    gpu_count = torch.cuda.device_count()\n    run(train, get_cfg_defaults(), description='StyleGAN', default_config='configs/ffhq.yaml',\n        world_size=gpu_count)\n"
        },
        {
          "name": "training_artifacts",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 3.10546875,
          "content": "# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom torch import nn\nimport torch\nimport threading\nimport hashlib\nimport pickle\nimport os\n\n\nclass cache:\n    def __init__(self, function):\n        self.function = function\n        self.pickle_name = self.function.__name__\n\n    def __call__(self, *args, **kwargs):\n        m = hashlib.sha256()\n        m.update(pickle.dumps((self.function.__name__, args, frozenset(kwargs.items()))))\n        output_path = os.path.join('.cache', \"%s_%s\" % (m.hexdigest(), self.pickle_name))\n        try:\n            with open(output_path, 'rb') as f:\n                data = pickle.load(f)\n        except (FileNotFoundError, pickle.PickleError):\n            data = self.function(*args, **kwargs)\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, 'wb') as f:\n                pickle.dump(data, f)\n        return data\n\n\ndef save_model(x, name):\n    if isinstance(x, nn.DataParallel):\n        torch.save(x.module.state_dict(), name)\n    else:\n        torch.save(x.state_dict(), name)\n\n\nclass AsyncCall(object):\n    def __init__(self, fnc, callback=None):\n        self.Callable = fnc\n        self.Callback = callback\n        self.result = None\n\n    def __call__(self, *args, **kwargs):\n        self.Thread = threading.Thread(target=self.run, name=self.Callable.__name__, args=args, kwargs=kwargs)\n        self.Thread.start()\n        return self\n\n    def wait(self, timeout=None):\n        self.Thread.join(timeout)\n        if self.Thread.isAlive():\n            raise TimeoutError\n        else:\n            return self.result\n\n    def run(self, *args, **kwargs):\n        self.result = self.Callable(*args, **kwargs)\n        if self.Callback:\n            self.Callback(self.result)\n\n\nclass AsyncMethod(object):\n    def __init__(self, fnc, callback=None):\n        self.Callable = fnc\n        self.Callback = callback\n\n    def __call__(self, *args, **kwargs):\n        return AsyncCall(self.Callable, self.Callback)(*args, **kwargs)\n\n\ndef async_func(fnc=None, callback=None):\n    if fnc is None:\n        def add_async_callback(f):\n            return AsyncMethod(f, callback)\n        return add_async_callback\n    else:\n        return AsyncMethod(fnc, callback)\n\n\nclass Registry(dict):\n    def __init__(self, *args, **kwargs):\n        super(Registry, self).__init__(*args, **kwargs)\n\n    def register(self, module_name):\n        def register_fn(module):\n            assert module_name not in self\n            self[module_name] = module\n            return module\n        return register_fn\n"
        }
      ]
    }
  ]
}