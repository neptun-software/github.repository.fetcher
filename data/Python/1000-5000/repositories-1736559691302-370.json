{
  "metadata": {
    "timestamp": 1736559691302,
    "page": 370,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/deit",
      "stars": 4110,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0810546875,
          "content": "*.swp\n**/__pycache__/**\nimnet_resnet50_scratch/timm_temp/\n.dumbo.json\ncheckpoints/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.087890625,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2020 - present, Facebook, Inc\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.9130859375,
          "content": "# Data-Efficient architectures and training for Image classification\n\nThis repository contains PyTorch evaluation code, training code and pretrained models for the following papers:\n<details>\n<summary>\n  <a href=\"README_deit.md\">DeiT</a> Data-Efficient Image Transformers, ICML 2021 [<b>bib</b>]\n</summary>\n\n```\n@InProceedings{pmlr-v139-touvron21a,\n  title =     {Training data-efficient image transformers &amp; distillation through attention},\n  author =    {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},\n  booktitle = {International Conference on Machine Learning},\n  pages =     {10347--10357},\n  year =      {2021},\n  volume =    {139},\n  month =     {July}\n}\n```\n</details>\n<details>\n<summary>\n<a href=\"README_cait.md\">CaiT</a> (Going deeper with Image Transformers), ICCV 2021  [<b>bib</b>]\n</summary>\n\n```\n@InProceedings{Touvron_2021_ICCV,\n    author    = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J\\'egou, Herv\\'e},\n    title     = {Going Deeper With Image Transformers},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month     = {October},\n    year      = {2021},\n    pages     = {32-42}\n}\n```\n</details>\n<details>\n<summary>\n<a href=\"README_resmlp.md\">ResMLP</a> (ResMLP: Feedforward networks for image classification with data-efficient training), TPAMI 2022 [<b>bib</b>]\n</summary>\n\n```\n@article{touvron2021resmlp,\n  title={ResMLP: Feedforward networks for image classification with data-efficient training},\n  author={Hugo Touvron and Piotr Bojanowski and Mathilde Caron and Matthieu Cord and Alaaeldin El-Nouby and Edouard Grave and Gautier Izacard and Armand Joulin and Gabriel Synnaeve and Jakob Verbeek and Herv'e J'egou},\n  journal={arXiv preprint arXiv:2105.03404},\n  year={2021},\n}\n```\n</details>\n<details>\n<summary>\n<a href=\"README_patchconvnet.md\">PatchConvnet</a> (Augmenting Convolutional networks with attention-based aggregation) [<b>bib</b>]\n</summary>\n\n```\n@article{touvron2021patchconvnet,\n  title={Augmenting Convolutional networks with attention-based aggregation},\n  author={Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Piotr Bojanowski and Armand Joulin and Gabriel Synnaeve and Jakob Verbeek and Herve Jegou},\n  journal={arXiv preprint arXiv:2112.13692},\n  year={2021},\n}\n```\n</details>\n<details>\n<summary>\n<a href=\"README_3things.md\">3Things</a> (Three things everyone should know about Vision Transformers), ECCV 2022 [<b>bib</b>]\n</summary>\n\n```\n@article{Touvron2022ThreeTE,\n  title={Three things everyone should know about Vision Transformers},\n  author={Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Jakob Verbeek and Herve Jegou},\n  journal={arXiv preprint arXiv:2203.09795},\n  year={2022},\n}\n```\n</details>\n<details>\n<summary>\n<a href=\"README_revenge.md\">DeiT III</a> (DeiT III: Revenge of the ViT), ECCV 2022 [<b>bib</b>]\n</summary>\n\n```\n@article{Touvron2022DeiTIR,\n  title={DeiT III: Revenge of the ViT},\n  author={Hugo Touvron and Matthieu Cord and Herve Jegou},\n  journal={arXiv preprint arXiv:2204.07118},\n  year={2022},\n}\n```\n</details>\n<details>\n<summary>\n<a href=\"README_cosub.md\">Cosub</a> (Co-training 2L Submodels for Visual Recognition), CVPR 2023 [<b>bib</b>]\n</summary>\n\n```\n@article{Touvron2022Cotraining2S,\n  title={Co-training 2L Submodels for Visual Recognition},\n  author={Hugo Touvron and Matthieu Cord and Maxime Oquab and Piotr Bojanowski and Jakob Verbeek and Herv'e J'egou},\n  journal={arXiv preprint arXiv:2212.04884},\n  year={2022},\n}\n```\n</details>\nIf you find this repository useful, please consider giving a star ⭐ and cite the relevant papers. \n\n# License\nThis repository is released under the Apache 2.0 license as found in the [LICENSE](LICENSE) file.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n"
        },
        {
          "name": "README_3things.md",
          "type": "blob",
          "size": 2.185546875,
          "content": "# Three things everyone should know about Vision Transformers\n\nThis repository contains PyTorch evaluation code, training code and pretrained models for the following projects:\n* [DeiT](README_deit.md) (Data-Efficient Image Transformers), ICML 2021 \n* [CaiT](README_cait.md) (Going deeper with Image Transformers), ICCV 2021 (Oral)\n* [ResMLP](README_resmlp.md) (ResMLP: Feedforward networks for image classification with data-efficient training)\n* [PatchConvnet](README_patchconvnet.md) (Augmenting Convolutional networks with attention-based aggregation)\n* 3Things (Three things everyone should know about Vision Transformers)\n* [DeiT III](README_revenge.md) (DeiT III: Revenge of the ViT)\n\n\n\nFor details see [Three things everyone should know about Vision Transformers](https://arxiv.org/pdf/2203.09795) by Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek and Hervé Jégou. \n\nIf you use this code for a paper please cite:\n\n```\n@article{Touvron2022ThreeTE,\n  title={Three things everyone should know about Vision Transformers},\n  author={Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Jakob Verbeek and Herve Jegou},\n  journal={arXiv preprint arXiv:2203.09795},\n  year={2022},\n}\n```\n\n\n\n## Attention only fine-tuning\n\nWe propose to finetune only the attentions (flag ```--attn-only```) to adapt the models to higher resolutions or to do transfer learning.\n\n<img src=\".github/attn.png\" height=\"190\">\n\n\n## MLP patch projection\nWe propose to replace the linear patch projection by an MLP patch projection (see class [hMLP_stem](models_v2.py)). A key advantage is that this pre-processing stem is compatible with and improves mask-based self-supervised training like BeiT.  \n\n<img src=\".github/hmlp.png\" height=\"190\">\n\n\n\n## Parallel blocks\n\nWe propose to use block in parallele in order to have more flexible architectures (see class [Layer_scale_init_Block_paralx2](models_v2.py)):\n\n<img src=\".github/paral.png\" height=\"190\">\n\n\n# License\nThis repository is released under the Apache 2.0 license as found in the [LICENSE](LICENSE) file.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n\n"
        },
        {
          "name": "README_cait.md",
          "type": "blob",
          "size": 3.44140625,
          "content": "\n# CaiT: Going deeper with Image Transformers \n\nThis repository contains PyTorch evaluation code, training code and pretrained models for:\n* [DeiT](README_deit.md) (Data-Efficient Image Transformers), ICML 2021\n* CaiT (Going deeper with Image Transformers), ICCV 2021 (Oral)\n* [ResMLP](README_resmlp.md) (ResMLP: Feedforward networks for image classification with data-efficient training) \n* [PatchConvnet](README_patchconvnet.md) (Augmenting Convolutional networks with attention-based aggregation)\n* [3Things](README_3things.md) (Three things everyone should know about Vision Transformers)\n* [DeiT III](README_revenge.md) (DeiT III: Revenge of the ViT)\n\nCaiT obtain competitive tradeoffs in terms of flops / precision:\n\n<p align=\"center\">\n  <img width=\"600\" height=\"600\" src=\".github/cait.png\">\n</p>\n\nFor details see [Going deeper with Image Transformers](https://arxiv.org/abs/2103.17239) by Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve and Hervé Jégou\n\nIf you use this code for a paper please cite:\n\n```\n@InProceedings{Touvron_2021_ICCV,\n    author    = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J\\'egou, Herv\\'e},\n    title     = {Going Deeper With Image Transformers},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month     = {October},\n    year      = {2021},\n    pages     = {32-42}\n}\n```\n\n# Model Zoo\n\nWe provide baseline CaiT models pretrained on ImageNet1k 2012 only, using the distilled version of our method.\n\n| name | acc@1 | res | FLOPs| #params | url |\n| --- | --- | --- | --- | --- | --- | \n| S24 | 83.5 | 224 |9.4B| 47M| [model](https://dl.fbaipublicfiles.com/deit/S24_224.pth) |\n| XS24| 84.1 | 384 |  19.3B |27M | [model](https://dl.fbaipublicfiles.com/deit/XS24_384.pth) |\n| S24 | 85.1 | 384 |  32.2B |47M | [model](https://dl.fbaipublicfiles.com/deit/S24_384.pth) |\n| S36 | 85.4 | 384 | 48.0B| 68M| [model](https://dl.fbaipublicfiles.com/deit/S36_384.pth) |\n| M36 | 86.1 | 384 |  173.3B| 271M | [model](https://dl.fbaipublicfiles.com/deit/M36_384.pth) |\n| M48 | 86.5 | 448 |  329.6B| 356M | [model](https://dl.fbaipublicfiles.com/deit/M48_448.pth) |\n\n\nThe models are also available via torch hub.\nBefore using it, make sure you have the pytorch-image-models package [`timm==0.3.2`](https://github.com/rwightman/pytorch-image-models) by [Ross Wightman](https://github.com/rwightman) installed. \n\n# Evaluation transforms\n\nCaiT employs a slightly different pre-processing, in particular a crop-ratio of 1.0 at test time. To reproduce the results of our paper please use the following pre-processing:\n\n```\ndef get_test_transforms(input_size):\n    mean, std = [0.485, 0.456, 0.406],[0.229, 0.224, 0.225]    \n    transformations = {}\n    transformations= transforms.Compose(\n        [transforms.Resize(input_size, interpolation=3),\n         transforms.CenterCrop(input_size),\n         transforms.ToTensor(),\n         transforms.Normalize(mean, std)])\n    return transformations\n ```  \n\nRemark: for CaiT M48 it is best to evaluate with FP32 precision\n\n### Other: Unofficial Implementations\n\n - [TensorFlow](https://github.com/sayakpaul/cait-tf) by [Sayak Paul](https://github.com/sayakpaul)\n \n# License\nThis repository is released under the Apache 2.0 license as found in the [LICENSE](LICENSE) file.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n"
        },
        {
          "name": "README_cosub.md",
          "type": "blob",
          "size": 1.376953125,
          "content": "# Co-training 2L Submodels for Visual Recognition\n\nThis repository contains PyTorch evaluation code, training code and pretrained models for the following projects:\n* [DeiT](README_deit.md) (Data-Efficient Image Transformers), ICML 2021 \n* [CaiT](README_cait.md) (Going deeper with Image Transformers), ICCV 2021 (Oral)\n* [ResMLP](README_resmlp.md) (ResMLP: Feedforward networks for image classification with data-efficient training)\n* [PatchConvnet](README_patchconvnet.md) (Augmenting Convolutional networks with attention-based aggregation)\n* [3Things](README_3things.md) (Three things everyone should know about Vision Transformers)\n* [DeiT III](README_revenge.md) (DeiT III: Revenge of the ViT)\n* Cosub (Co-training 2L Submodels for Visual Recognition)\n\nThis new training recipes improve previous training strategy with different architectures:\n\n![Cosub](.github/cosub.png)\n\nFor details see [Co-training 2L Submodels for Visual Recognition](https://arxiv.org/pdf/2212.04884.pdf) by Hugo Touvron, Matthieu Cord, Maxime Oquab, Piotr Bojanowski, Jakob Verbeek and Hervé Jégou. \n\nIf you use this code for a paper please cite:\n\n```\n@article{Touvron2022Cotraining2S,\n  title={Co-training 2L Submodels for Visual Recognition},\n  author={Hugo Touvron and Matthieu Cord and Maxime Oquab and Piotr Bojanowski and Jakob Verbeek and Herv'e J'egou},\n  journal={arXiv preprint arXiv:2212.04884},\n  year={2022},\n}\n```\n"
        },
        {
          "name": "README_deit.md",
          "type": "blob",
          "size": 9.482421875,
          "content": "\n# DeiT: Data-efficient Image Transformers\n\nThis repository contains PyTorch evaluation code, training code and pretrained models for the following projects:\n* DeiT (Data-Efficient Image Transformers), ICML 2021 \n* [CaiT](README_cait.md) (Going deeper with Image Transformers), ICCV 2021 (Oral)\n* [ResMLP](README_resmlp.md) (ResMLP: Feedforward networks for image classification with data-efficient training)\n* [PatchConvnet](README_patchconvnet.md) (Augmenting Convolutional networks with attention-based aggregation)\n* [3Things](README_3things.md) (Three things everyone should know about Vision Transformers)\n* [DeiT III](README_revenge.md) (DeiT III: Revenge of the ViT)\n\nThey obtain competitive tradeoffs in terms of speed / precision:\n\n![DeiT](.github/deit.png)\n\nFor details see [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles and Hervé Jégou. \n\nIf you use this code for a paper please cite:\n\n```\n@InProceedings{pmlr-v139-touvron21a,\n  title =     {Training data-efficient image transformers &amp; distillation through attention},\n  author =    {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},\n  booktitle = {International Conference on Machine Learning},\n  pages =     {10347--10357},\n  year =      {2021},\n  volume =    {139},\n  month =     {July}\n}\n```\n\n# Model Zoo\n\nWe provide baseline DeiT models pretrained on ImageNet 2012.\n\n| name | acc@1 | acc@5 | #params | url |\n| --- | --- | --- | --- | --- |\n| DeiT-tiny | 72.2 | 91.1 | 5M | [model](https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth) |\n| DeiT-small | 79.9 | 95.0 | 22M| [model](https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth) |\n| DeiT-base | 81.8 | 95.6 | 86M | [model](https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth) |\n| DeiT-tiny distilled | 74.5 | 91.9 | 6M | [model](https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth) |\n| DeiT-small distilled | 81.2 | 95.4 | 22M| [model](https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth) |\n| DeiT-base distilled | 83.4 | 96.5 | 87M | [model](https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth) |\n| DeiT-base 384 | 82.9 | 96.2 | 87M | [model](https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth) |\n| DeiT-base distilled 384 (1000 epochs) | 85.2 | 97.2 | 88M | [model](https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth) |\n|CaiT-S24 distilled 384| 85.1 | 97.3 | 47M | [model](README_cait.md)|\n|CaiT-M48 distilled 448| 86.5 | 97.7 | 356M | [model](README_cait.md)|\n\nThe models are also available via torch hub.\nBefore using it, make sure you have the pytorch-image-models package [`timm==0.3.2`](https://github.com/rwightman/pytorch-image-models) by [Ross Wightman](https://github.com/rwightman) installed. Note that our work relies of the augmentations proposed in this library. In particular, the RandAugment and RandErasing augmentations that we invoke are the improved versions from the timm library, which already led the timm authors to report up to 79.35% top-1 accuracy with Imagenet training for their best model, i.e., an improvement of about +1.5% compared to prior art. \n\nTo load DeiT-base with pretrained weights on ImageNet simply do:\n\n```python\nimport torch\n# check you have the right version of timm\nimport timm\nassert timm.__version__ == \"0.3.2\"\n\n# now load it with torchhub\nmodel = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n```\n\nAdditionnally, we provide a [Colab notebook](https://colab.research.google.com/github/facebookresearch/deit/blob/colab/notebooks/deit_inference.ipynb) which goes over the steps needed to perform inference with DeiT.\n\n# Usage\n\nFirst, clone the repository locally:\n```\ngit clone https://github.com/facebookresearch/deit.git\n```\nThen, install PyTorch 1.7.0+ and torchvision 0.8.1+ and [pytorch-image-models 0.3.2](https://github.com/rwightman/pytorch-image-models):\n\n```\nconda install -c pytorch pytorch torchvision\npip install timm==0.3.2\n```\n\n## Data preparation\n\nDownload and extract ImageNet train and val images from http://image-net.org/.\nThe directory structure is the standard layout for the torchvision [`datasets.ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder), and the training and validation data is expected to be in the `train/` folder and `val` folder respectively:\n\n```\n/path/to/imagenet/\n  train/\n    class1/\n      img1.jpeg\n    class2/\n      img2.jpeg\n  val/\n    class1/\n      img3.jpeg\n    class2/\n      img4.jpeg\n```\n\n## Evaluation\nTo evaluate a pre-trained DeiT-base on ImageNet val with a single GPU run:\n```\npython main.py --eval --resume https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth --data-path /path/to/imagenet\n```\nThis should give\n```\n* Acc@1 81.846 Acc@5 95.594 loss 0.820\n```\n\nFor Deit-small, run:\n```\npython main.py --eval --resume https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth --model deit_small_patch16_224 --data-path /path/to/imagenet\n```\ngiving\n```\n* Acc@1 79.854 Acc@5 94.968 loss 0.881\n```\n\nNote that Deit-small is *not* the same model as in Timm. \n\nAnd for Deit-tiny:\n```\npython main.py --eval --resume https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth --model deit_tiny_patch16_224 --data-path /path/to/imagenet\n```\nwhich should give\n```\n* Acc@1 72.202 Acc@5 91.124 loss 1.219\n```\n\nHere you'll find the command-lines to reproduce the inference results for the distilled and finetuned models\n\n<details>\n\n<summary>\ndeit_base_distilled_patch16_224\n</summary>\n\n```\npython main.py --eval --model deit_base_distilled_patch16_224 --resume https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\n```\ngiving\n```\n* Acc@1 83.372 Acc@5 96.482 loss 0.685\n```\n\n</details>\n\n\n<details>\n\n<summary>\ndeit_small_distilled_patch16_224\n</summary>\n\n```\npython main.py --eval --model deit_small_distilled_patch16_224 --resume https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth\n```\ngiving\n```\n* Acc@1 81.164 Acc@5 95.376 loss 0.752\n```\n\n</details>\n\n<details>\n\n<summary>\ndeit_tiny_distilled_patch16_224\n</summary>\n\n```\npython main.py --eval --model deit_tiny_distilled_patch16_224 --resume https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth\n```\ngiving\n```\n* Acc@1 74.476 Acc@5 91.920 loss 1.021\n```\n\n</details>\n\n<details>\n\n<summary>\ndeit_base_patch16_384\n</summary>\n\n```\npython main.py --eval --model deit_base_patch16_384 --input-size 384 --resume https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth\n```\ngiving\n```\n* Acc@1 82.890 Acc@5 96.222 loss 0.764\n```\n\n</details>\n\n<details>\n\n<summary>\ndeit_base_distilled_patch16_384\n</summary>\n\n```\npython main.py --eval --model deit_base_distilled_patch16_384 --input-size 384 --resume https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n```\ngiving\n```\n* Acc@1 85.224 Acc@5 97.186 loss 0.636\n```\n\n</details>\n\n## Training\nTo train DeiT-small and Deit-tiny on ImageNet on a single node with 4 gpus for 300 epochs run:\n\nDeiT-small\n```\npython -m torch.distributed.launch --nproc_per_node=4 --use_env main.py --model deit_small_patch16_224 --batch-size 256 --data-path /path/to/imagenet --output_dir /path/to/save\n```\n\nDeiT-tiny\n```\npython -m torch.distributed.launch --nproc_per_node=4 --use_env main.py --model deit_tiny_patch16_224 --batch-size 256 --data-path /path/to/imagenet --output_dir /path/to/save\n```\n\n\n### Multinode training\n\nDistributed training is available via Slurm and [submitit](https://github.com/facebookincubator/submitit):\n\n```\npip install submitit\n```\n\nTo train DeiT-base model on ImageNet on 2 nodes with 8 gpus each for 300 epochs:\n\n```\npython run_with_submitit.py --model deit_base_patch16_224 --data-path /path/to/imagenet\n```\n\nTo train DeiT-base with hard distillation using a RegNetY-160 as teacher, on 2 nodes with 8 GPUs with 32GB each for 300 epochs (make sure that the model weights for the teacher have been downloaded before to the correct location, to avoid multiple workers writing to the same file):\n```\npython run_with_submitit.py --model deit_base_distilled_patch16_224 --distillation-type hard --teacher-model regnety_160 --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth --use_volta32\n```\n\nTo finetune a DeiT-base on 384 resolution images for 30 epochs, starting from a DeiT-base trained on 224 resolution images, do (make sure that the weights to the original model have been downloaded before, to avoid multiple workers writing to the same file):\n```\npython run_with_submitit.py --model deit_base_patch16_384 --batch-size 32 --finetune https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth --input-size 384 --use_volta32 --nodes 2 --lr 5e-6 --weight-decay 1e-8 --epochs 30 --min-lr 5e-6\n```\n\n### Other: Unofficial Implementations and Tutorial\n\n - [TensorFlow](https://github.com/sayakpaul/deit-tf) by [Sayak Paul](https://github.com/sayakpaul)\n - [Tutorial](https://github.com/sayakpaul/probing-vits/) by [Aritra Roy Gosthipaty](https://github.com/ariG23498) and [Sayak Paul](https://github.com/sayakpaul)\n\n\n# License\nThis repository is released under the Apache 2.0 license as found in the [LICENSE](LICENSE) file.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n"
        },
        {
          "name": "README_patchconvnet.md",
          "type": "blob",
          "size": 4.1767578125,
          "content": "# Augmenting Convolutional networks with attention-based aggregation\n\nThis repository contains PyTorch evaluation code, training code and pretrained models for the following projects:\n* [DeiT](README_deit.md) (Data-Efficient Image Transformers), ICML 2021 \n* [CaiT](README_cait.md) (Going deeper with Image Transformers), ICCV 2021 (Oral)\n* [ResMLP](README_resmlp.md) (ResMLP: Feedforward networks for image classification with data-efficient training)\n* PatchConvnet (Augmenting Convolutional networks with attention-based aggregation)\n* [3Things](README_3things.md) (Three things everyone should know about Vision Transformers)\n* [DeiT III](README_revenge.md) (DeiT III: Revenge of the ViT)\n\nPatchConvnet provides interpretable attention maps to convnets:\n\n<p align=\"center\">\n  <img width=\"900\"  src=\".github/patch_convnet.png\">\n</p>\n\nFor details see [Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692) by Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Matthieu Cord, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve and Hervé Jégou.\n\nIf you use this code for a paper please cite:\n\n```\n@article{touvron2021patchconvnet,\n  title={Augmenting Convolutional networks with attention-based aggregation},\n  author={Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Piotr Bojanowski and Armand Joulin and Gabriel Synnaeve and Jakob Verbeek and Herv'e J'egou},\n  journal={arXiv preprint arXiv:2112.13692},\n  year={2021},\n}\n```\n\n# Model Zoo\n\nWe provide PatchConvnet models pretrained on ImageNet-1k:\n\n| name | acc@1 | res | FLOPs (B)| #params (M)| Peak Mem. (MB) |throughput(im/s) | url |\n| --- | --- | --- | --- | --- | --- | --- | --- | \n| S60 | 82.1 | 224 |4.0| 25.2 | 1322| 1129| [model](https://dl.fbaipublicfiles.com/deit/s60_224_1k.pth) |\n| S120| 83.2 | 224 |  7.5 |47.7 |1450 |580| [model](https://dl.fbaipublicfiles.com/deit/s120_224_1k.pth) |\n| B60 | 83.5 | 224 |  15.8 |99.4 |2790 |541|[model](https://dl.fbaipublicfiles.com/deit/b60_224_1k.pth) |\n| B120 |84.1 | 224 |  29.9 |188.6 |3314 |280|[model](https://dl.fbaipublicfiles.com/deit/b120_224_1k.pth) |\n\nModel pretrained on ImageNet-21k with finetuning on ImageNet-1k:\n\n| name | acc@1 | res | FLOPs (B)| #params (M)| Peak Mem. (MB) |throughput(im/s) | url |\n| --- | --- | --- | --- | --- | --- |  --- | --- | \n| S60 |83.5 | 224 |  4.0 |25.2 | 1322 | 1129|[model](https://dl.fbaipublicfiles.com/deit/s60_224_21k.pth) |\n| S60 |84.9 | 384 |  11.8 |25.2 |3604| 388| [model](https://dl.fbaipublicfiles.com/deit/s60_384_21k.pth) |\n| S60 |85.4 | 512 |  20.9 |25.2 |6296 |216|[model](https://dl.fbaipublicfiles.com/deit/s60_512_21k.pth) |\n| B60 |85.4 | 224 |  15.8 |99.4 |2790 |541|[model](https://dl.fbaipublicfiles.com/deit/b60_224_21k.pth) |\n| B60 |86.5 | 384 |  46.5 |99.4 |7067|185|[model](https://dl.fbaipublicfiles.com/deit/b60_384_21k.pth) |\n| B120 |86.0 | 224 |  29.8 |188.6 |3314|280|[model](https://dl.fbaipublicfiles.com/deit/b120_224_21k.pth) |\n| B120 |86.9 | 384 |  87.7 |188.6|7587|96|[model](https://dl.fbaipublicfiles.com/deit/b120_384_21k.pth) |\n\nPatchConvnet models with multi-class tokens on ImageNet-1k:\n\n| name | acc@1 | res | FLOPs (B)| #params (M) | url |\n| --- | --- | --- | --- | --- | --- | \n| S60 (scratch)|81.1 | 224 |  5.3 |25.6 |[model](https://dl.fbaipublicfiles.com/deit/s60_multi_scratch_1k.pth) |\n| S60 (finetune)|82.0 | 224 |  5.3 |25.6 |[model](https://dl.fbaipublicfiles.com/deit/s60_multi_finetune_1k.pth) |\n\n\nThe models are also available via torch hub.\nBefore using it, make sure you have the latest pytorch-image-models package [`timm`](https://github.com/rwightman/pytorch-image-models) by [Ross Wightman](https://github.com/rwightman) installed. \n\n# Notebook for visualization\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bmdwOaLmMt7IVrnbumVaXPGDwHeBL36M?usp=sharing#scrollTo=ntcNsFB6kWAf) We provide a notebook to visualize the attention maps of our networks.\n\n# License\nThis repository is released under the Apache 2.0 license as found in the [LICENSE](LICENSE) file.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n"
        },
        {
          "name": "README_resmlp.md",
          "type": "blob",
          "size": 3.91015625,
          "content": "\n# ResMLP: Feedforward networks for image classification with data-efficient training\n\nThis repository contains PyTorch evaluation code, training code and pretrained models for the following projects:\n* [DeiT](README_deit.md) (Data-Efficient Image Transformers), ICML 2021 \n* [CaiT](README_cait.md) (Going deeper with Image Transformers), ICCV 2021 (Oral)\n* ResMLP (ResMLP: Feedforward networks for image classification with data-efficient training)\n* [PatchConvnet](README_patchconvnet.md) (Augmenting Convolutional networks with attention-based aggregation)\n* [3Things](README_3things.md) (Three things everyone should know about Vision Transformers)\n* [DeiT III](README_revenge.md) (DeiT III: Revenge of the ViT)\n\nResMLP obtain good performance given its simplicity:\n\n<p align=\"center\">\n  <img width=\"900\"  src=\".github/resmlp.png\">\n</p>\n\nFor details see [ResMLP: Feedforward networks for image classification with data-efficient training](https://arxiv.org/abs/2105.03404) by Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek and Hervé Jégou.\n\nIf you use this code for a paper please cite:\n\n```\n@article{touvron2021resmlp,\n  title={ResMLP: Feedforward networks for image classification with data-efficient training},\n  author={Hugo Touvron and Piotr Bojanowski and Mathilde Caron and Matthieu Cord and Alaaeldin El-Nouby and Edouard Grave and Gautier Izacard and Armand Joulin and Gabriel Synnaeve and Jakob Verbeek and Herv'e J'egou},\n  journal={arXiv preprint arXiv:2105.03404},\n  year={2021},\n}\n```\n\n# Model Zoo\n\nWe provide baseline ResMLP models pretrained on ImageNet1k 2012, using the distilled version of our method:\n\n| name | acc@1 | res | FLOPs| #params | url |\n| --- | --- | --- | --- | --- | --- | \n| ResMLP-S12 | 77.8 | 224 |3B| 15M| [model](https://dl.fbaipublicfiles.com/deit/resmlp_12_dist.pth) |\n| ResMLP-S24| 80.8 | 224 |  6B |30M | [model](https://dl.fbaipublicfiles.com/deit/resmlp_24_dist.pth) |\n| ResMLP-S36 | 81.1 | 224 |  23B |116M | [model](https://dl.fbaipublicfiles.com/deit/resmlp_36_dist.pth) |\n| ResMLP-B24 |83.6 | 224 |  100B |129M | [model](https://dl.fbaipublicfiles.com/deit/resmlpB_24_dist.pth) |\n\nModel pretrained on ImageNet-22k with finetuning on ImageNet1k 2012:\n\n| name | acc@1 | res | FLOPs| #params | url |\n| --- | --- | --- | --- | --- | --- | \n| ResMLP-B24 |84.4 | 224 |  100B |129M | [model](https://dl.fbaipublicfiles.com/deit/resmlpB_24_22k.pth) |\n\nModels pretrained with DINO without finetuning:\n\n| name | acc@1 (knn)| res | FLOPs| #params | url |\n| --- | --- | --- | --- | --- | --- | \n| ResMLP-S12 | 62.6 | 224 |3B| 15M| [model](https://dl.fbaipublicfiles.com/deit/resmlp_12_dino.pth) |\n| ResMLP-S24| 69.4 | 224 |  6B |30M | [model](https://dl.fbaipublicfiles.com/deit/resmlp_24_dino.pth) |\n\nThe models are also available via torch hub.\nBefore using it, make sure you have the pytorch-image-models package [`timm==0.3.2`](https://github.com/rwightman/pytorch-image-models) by [Ross Wightman](https://github.com/rwightman) installed. \n\n# Evaluation transforms\n\nResMLP employs a slightly different pre-processing, in particular a crop-ratio of 0.9 at test time. To reproduce the results of our paper please use the following pre-processing:\n\n```\ndef get_test_transforms(input_size):\n    mean, std = [0.485, 0.456, 0.406],[0.229, 0.224, 0.225]    \n    transformations = {}\n    Rs_size=int(input_size/0.9)\n    transformations= transforms.Compose(\n        [transforms.Resize(Rs_size, interpolation=3),\n         transforms.CenterCrop(input_size),\n         transforms.ToTensor(),\n         transforms.Normalize(mean, std)])\n    return transformations\n ```  \n\n# License\nThis repository is released under the Apache 2.0 license as found in the [LICENSE](LICENSE) file.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n"
        },
        {
          "name": "README_revenge.md",
          "type": "blob",
          "size": 29.484375,
          "content": "# DeiT III: Revenge of the ViT\n\nThis repository contains PyTorch evaluation code, training code and pretrained models for the following projects:\n* [DeiT](README_deit.md) (Data-Efficient Image Transformers), ICML 2021 \n* [CaiT](README_cait.md) (Going deeper with Image Transformers), ICCV 2021 (Oral)\n* [ResMLP](README_resmlp.md) (ResMLP: Feedforward networks for image classification with data-efficient training)\n* [PatchConvnet](README_patchconvnet.md) (Augmenting Convolutional networks with attention-based aggregation)\n* [3Things](README_3things.md) (Three things everyone should know about Vision Transformers)\n* DeiT III (DeiT III: Revenge of the ViT)\n\n\nThis new training recipes improve previous training strategy for ViT architectures:\n\n![DeiT III](.github/revenge.png)\n\nFor details see [DeiT III: Revenge of the ViT](https://arxiv.org/pdf/2204.07118.pdf) by Hugo Touvron, Matthieu Cord and Hervé Jégou. \n\nIf you use this code for a paper please cite:\n\n```\n@article{Touvron2022DeiTIR,\n  title={DeiT III: Revenge of the ViT},\n  author={Hugo Touvron and Matthieu Cord and Herve Jegou},\n  journal={arXiv preprint arXiv:2204.07118},\n  year={2022},\n}\n```\n\n# Model Zoo\n\nWe provide baseline ViT models pretrained on ImageNet-1k and ImageNet-21k.\n\n## ImageNet-1k pre-training\n\n| name | #params | GFLOPs | throughput (im/s) | Peak Mem (MB) | Resolution | acc@1 (INet-1k) | acc@1 (v2) | url |\n| ---  | --- | --- | --- | --- | --- | --- | --- | --- |\n| ViT-S | 22.0 | 4.6 | 1891 | 987 | 224x224 | 81.4 | 70.5 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_small_224_1k.pth) |\n| ViT-S | 22.0 | 15.5 | 424 | 4569 | 384x384 | 83.4 | 73.1 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_small_384_1k.pth) |\n| ViT-M | 38.8 | 8.0 | - | - | 224x224 | 83.0 | 72.3 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_medium_224_1k.pth) |\n| ViT-B | 86.6 | 17.5 | 831 | 2078 | 224x224 | 83.8 | 73.6 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_base_224_1k.pth) |\n| ViT-B | 86.9 | 55.5 | 190 | 8956 | 384x384 | 85.0 | 74.8 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_base_384_1k.pth) |\n| ViT-L | 304.4 | 61.6 | 277 | 3789 | 224x224 | 84.9 | 75.1 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_large_224_1k.pth) |\n| ViT-L | 304.8 | 191.2 | 67 | 12866 | 384x384 | 85.8 | 76.7 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_large_384_1k.pth) |\n| ViT-H | 632.1 | 167.4 | 112 | 6984 | 224x224 | 85.2 | 75.9 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_huge_224_1k.pth) |\n\n\n\n## ImageNet-21k pre-training\n\n| name | #params | GFLOPs | throughput (im/s) | Peak Mem (MB) | Resolution | acc@1 (INet-1k) | acc@1 (v2) | url |\n| ---  | --- | --- | --- | --- | --- | --- | --- | --- |\n| ViT-S | 22.0 | 4.6 | 1891 | 987 | 224x224 | 83.1 | 73.8 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_small_224_21k.pth) |\n| ViT-S | 22.0 | 15.5 | 424 | 4569 | 384x384 | 84.8 | 75.1 |[model](https://dl.fbaipublicfiles.com/deit/deit_3_small_384_21k.pth) |\n| ViT-M | 38.8 | 8.0 | - | - | 224x224 | 84.5 | 74.7 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_medium_224_21k.pth) |\n| ViT-B | 86.6 | 17.5 | 831 | 2078 | 224x224 | 85.7 | 76.5 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_base_224_21k.pth) |\n| ViT-B | 86.9 | 55.5 | 190 | 8956 | 384x384 | 86.7 | 77.9 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_base_384_21k.pth) |\n| ViT-L | 304.4 | 61.6 | 277 | 3789 | 224x224 | 87.0 | 78.6 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_large_224_21k.pth) |\n| ViT-L | 304.8 | 191.2 | 67 | 12866 | 384x384 | 87.7 | 79.1 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_large_384_21k.pth) |\n| ViT-H | 632.1 | 167.4 | 112 | 6984 | 224x224 | 87.2 | 79.2 | [model](https://dl.fbaipublicfiles.com/deit/deit_3_huge_224_21k_v1.pth) |\n\n# 3-Augment\n\nWe use a simple data-augmentation ([3-Augment](augment.py)) strategy.\n\n![3-Augment](.github/revenge_da.png)\n\n\n# Training command\n\n### Multinode training\n\nDistributed training is available via Slurm and [submitit](https://github.com/facebookincubator/submitit):\n\n```\npip install submitit\n```\n\nTraining on ImageNet-1k:\n\n```\npython run_with_submitit.py --model deit_small_patch16_LS --data-path /path/to/imagenet --batch 256 --lr 4e-3 --epochs 800 --weight-decay 0.05 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 1 --ngpus 8 --smoothing 0.0 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup .8 --drop-path 0.05 --cutmix 1.0 --unscale-lr --repeated-aug--bce-loss  --color-jitter 0.3 --ThreeAugment\n```\n<details>\n<summary>\n  logs\n</summary>\n\nepoch 0: 0.1\n\nepoch 5: 2.008\n\nepoch 10: 9.258\n\nepoch 15: 16.356\n\nepoch 20: 24.752\n\nepoch 25: 31.84\n\nepoch 30: 38.52\n\nepoch 35: 44.0\n\nepoch 40: 48.034\n\nepoch 45: 51.834\n\nepoch 50: 53.628\n\nepoch 55: 55.828\n\nepoch 60: 57.12\n\nepoch 65: 58.504\n\nepoch 70: 59.218\n\nepoch 75: 60.732\n\nepoch 80: 60.98\n\nepoch 85: 61.36\n\nepoch 90: 61.842\n\nepoch 95: 62.772\n\nepoch 100: 63.122\n\nepoch 105: 63.838\n\nepoch 110: 63.902\n\nepoch 115: 64.51\n\nepoch 120: 64.822\n\nepoch 125: 64.916\n\nepoch 130: 65.518\n\nepoch 135: 65.68\n\nepoch 140: 65.796\n\nepoch 145: 66.12\n\nepoch 150: 66.18\n\nepoch 155: 66.14\n\nepoch 160: 66.666\n\nepoch 165: 66.798\n\nepoch 170: 66.974\n\nepoch 175: 67.238\n\nepoch 180: 67.446\n\nepoch 185: 67.478\n\nepoch 190: 67.656\n\nepoch 195: 67.304\n\nepoch 200: 67.9\n\nepoch 205: 67.826\n\nepoch 210: 68.372\n\nepoch 215: 68.458\n\nepoch 220: 68.764\n\nepoch 225: 69.116\n\nepoch 230: 69.016\n\nepoch 235: 69.49\n\nepoch 240: 69.296\n\nepoch 245: 69.522\n\nepoch 250: 69.678\n\nepoch 255: 70.048\n\nepoch 260: 69.648\n\nepoch 265: 70.404\n\nepoch 270: 70.232\n\nepoch 275: 70.38\n\nepoch 280: 70.726\n\nepoch 285: 70.8\n\nepoch 290: 70.844\n\nepoch 295: 71.15\n\nepoch 300: 71.56\n\nepoch 305: 71.54\n\nepoch 310: 71.538\n\nepoch 315: 71.73\n\nepoch 320: 72.144\n\nepoch 325: 72.068\n\nepoch 330: 72.182\n\nepoch 335: 72.286\n\nepoch 340: 72.434\n\nepoch 345: 73.072\n\nepoch 350: 72.998\n\nepoch 355: 72.96\n\nepoch 360: 73.062\n\nepoch 365: 73.296\n\nepoch 370: 73.85\n\nepoch 375: 73.614\n\nepoch 380: 73.934\n\nepoch 385: 74.064\n\nepoch 390: 74.35\n\nepoch 395: 74.032\n\nepoch 400: 74.446\n\nepoch 405: 74.52\n\nepoch 410: 74.736\n\nepoch 415: 74.908\n\nepoch 420: 75.154\n\nepoch 425: 75.226\n\nepoch 430: 75.556\n\nepoch 435: 75.716\n\nepoch 440: 75.612\n\nepoch 445: 76.014\n\nepoch 450: 75.798\n\nepoch 455: 76.156\n\nepoch 460: 76.268\n\nepoch 465: 76.344\n\nepoch 470: 76.656\n\nepoch 475: 76.54\n\nepoch 480: 76.572\n\nepoch 485: 76.864\n\nepoch 490: 77.142\n\nepoch 495: 77.146\n\nepoch 500: 77.664\n\nepoch 505: 77.442\n\nepoch 510: 77.72\n\nepoch 515: 77.482\n\nepoch 520: 77.788\n\nepoch 525: 77.92\n\nepoch 530: 78.172\n\nepoch 535: 78.258\n\nepoch 540: 78.44\n\nepoch 545: 78.454\n\nepoch 550: 78.522\n\nepoch 555: 78.648\n\nepoch 560: 78.574\n\nepoch 565: 78.826\n\nepoch 570: 78.874\n\nepoch 575: 79.194\n\nepoch 580: 78.97\n\nepoch 585: 79.498\n\nepoch 590: 79.432\n\nepoch 595: 79.474\n\nepoch 600: 79.6\n\nepoch 605: 79.594\n\nepoch 610: 79.934\n\nepoch 615: 79.84\n\nepoch 620: 79.916\n\nepoch 625: 80.058\n\nepoch 630: 80.038\n\nepoch 635: 80.106\n\nepoch 640: 80.302\n\nepoch 645: 80.234\n\nepoch 650: 80.388\n\nepoch 655: 80.41\n\nepoch 660: 80.47\n\nepoch 665: 80.568\n\nepoch 670: 80.57\n\nepoch 675: 80.85\n\nepoch 680: 80.862\n\nepoch 685: 80.872\n\nepoch 690: 80.918\n\nepoch 695: 80.912\n\nepoch 700: 81.012\n\nepoch 705: 81.098\n\nepoch 710: 81.042\n\nepoch 715: 81.112\n\nepoch 720: 81.154\n\nepoch 725: 81.23\n\nepoch 730: 81.118\n\nepoch 735: 81.264\n\nepoch 740: 81.24\n\nepoch 745: 81.242\n\nepoch 750: 81.21\n\nepoch 755: 81.258\n\nepoch 760: 81.342\n\nepoch 765: 81.296\n\nepoch 770: 81.33\n\nepoch 775: 81.314\n\nepoch 780: 81.312\n\nepoch 785: 81.314\n\nepoch 790: 81.334\n\nepoch 795: 81.352\n\nepoch 800: 81.376\n</details>\n\n```\npython run_with_submitit.py --model deit_base_patch16_LS --data-path /path/to/imagenet --batch 256 --lr 3e-3 --epochs 800 --weight-decay 0.05 --sched cosine --input-size 192 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 1 --ngpus 8 --smoothing 0.0 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup .8 --drop-path 0.2 --cutmix 1.0 --unscale-lr --repeated-aug --bce-loss  --color-jitter 0.3 --ThreeAugment\n```\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.1\n\nepoch 5: 2.838\n\nepoch 10: 10.308\n\nepoch 15: 17.292\n\nepoch 20: 25.55\n\nepoch 25: 33.436\n\nepoch 30: 40.38\n\nepoch 35: 45.446\n\nepoch 40: 50.766\n\nepoch 45: 54.162\n\nepoch 50: 56.572\n\nepoch 55: 58.8\n\nepoch 60: 60.496\n\nepoch 65: 61.994\n\nepoch 70: 62.904\n\nepoch 75: 64.06\n\nepoch 80: 65.224\n\nepoch 85: 65.718\n\nepoch 90: 66.012\n\nepoch 95: 66.984\n\nepoch 100: 67.198\n\nepoch 105: 67.864\n\nepoch 110: 68.098\n\nepoch 115: 68.568\n\nepoch 120: 69.208\n\nepoch 125: 69.448\n\nepoch 130: 69.286\n\nepoch 135: 69.568\n\nepoch 140: 70.282\n\nepoch 145: 70.396\n\nepoch 150: 70.46\n\nepoch 155: 70.844\n\nepoch 160: 70.852\n\nepoch 165: 71.33\n\nepoch 170: 71.56\n\nepoch 175: 71.784\n\nepoch 180: 71.798\n\nepoch 185: 72.162\n\nepoch 190: 72.14\n\nepoch 195: 72.082\n\nepoch 200: 72.978\n\nepoch 205: 72.562\n\nepoch 210: 72.714\n\nepoch 215: 73.006\n\nepoch 220: 73.02\n\nepoch 225: 73.466\n\nepoch 230: 73.62\n\nepoch 235: 73.656\n\nepoch 240: 73.754\n\nepoch 245: 73.718\n\nepoch 250: 73.806\n\nepoch 255: 74.182\n\nepoch 260: 74.276\n\nepoch 265: 74.592\n\nepoch 270: 74.752\n\nepoch 275: 74.86\n\nepoch 280: 74.88\n\nepoch 285: 75.044\n\nepoch 290: 75.154\n\nepoch 295: 75.482\n\nepoch 300: 75.332\n\nepoch 305: 75.548\n\nepoch 310: 75.79\n\nepoch 315: 75.832\n\nepoch 320: 76.11\n\nepoch 325: 76.27\n\nepoch 330: 76.102\n\nepoch 335: 76.556\n\nepoch 340: 76.468\n\nepoch 345: 76.514\n\nepoch 350: 76.84\n\nepoch 355: 77.128\n\nepoch 360: 77.1\n\nepoch 365: 76.99\n\nepoch 370: 77.588\n\nepoch 375: 77.336\n\nepoch 380: 77.444\n\nepoch 385: 77.924\n\nepoch 390: 77.766\n\nepoch 395: 77.756\n\nepoch 400: 78.004\n\nepoch 405: 78.238\n\nepoch 410: 78.376\n\nepoch 415: 78.45\n\nepoch 420: 78.62\n\nepoch 425: 78.628\n\nepoch 430: 78.828\n\nepoch 435: 78.92\n\nepoch 440: 78.902\n\nepoch 445: 78.994\n\nepoch 450: 79.086\n\nepoch 455: 79.206\n\nepoch 460: 79.422\n\nepoch 465: 79.394\n\nepoch 470: 79.556\n\nepoch 475: 79.736\n\nepoch 480: 79.69\n\nepoch 485: 79.978\n\nepoch 490: 80.108\n\nepoch 495: 80.162\n\nepoch 500: 80.23\n\nepoch 505: 80.346\n\nepoch 510: 80.378\n\nepoch 515: 80.488\n\nepoch 520: 80.638\n\nepoch 525: 80.666\n\nepoch 530: 80.694\n\nepoch 535: 80.798\n\nepoch 540: 80.866\n\nepoch 545: 81.024\n\nepoch 550: 81.266\n\nepoch 555: 81.028\n\nepoch 560: 81.27\n\nepoch 565: 81.306\n\nepoch 570: 81.384\n\nepoch 575: 81.474\n\nepoch 580: 81.42\n\nepoch 585: 81.53\n\nepoch 590: 81.578\n\nepoch 595: 81.726\n\nepoch 600: 81.75\n\nepoch 605: 81.672\n\nepoch 610: 81.86\n\nepoch 615: 82.006\n\nepoch 620: 81.926\n\nepoch 625: 81.806\n\nepoch 630: 81.968\n\nepoch 635: 82.104\n\nepoch 640: 82.184\n\nepoch 645: 82.202\n\nepoch 650: 82.262\n\nepoch 655: 82.224\n\nepoch 660: 82.256\n\nepoch 665: 82.284\n\nepoch 670: 82.38\n\nepoch 675: 82.438\n\nepoch 680: 82.498\n\nepoch 685: 82.554\n\nepoch 690: 82.552\n\nepoch 695: 82.648\n\nepoch 700: 82.548\n\nepoch 705: 82.702\n\nepoch 710: 82.64\n\nepoch 715: 82.598\n\nepoch 720: 82.7\n\nepoch 725: 82.686\n\nepoch 730: 82.718\n\nepoch 735: 82.71\n\nepoch 740: 82.748\n\nepoch 745: 82.784\n\nepoch 750: 82.736\n\nepoch 755: 82.77\n\nepoch 760: 82.808\n\nepoch 765: 82.754\n\nepoch 770: 82.766\n\nepoch 775: 82.798\n\nepoch 780: 82.784\n\nepoch 785: 82.802\n\nepoch 790: 82.858\n\nepoch 795: 82.866\n\nepoch 800: 82.82\n  \n</details>\n  \n  \n```\npython run_with_submitit.py --model deit_large_patch16_LS --data-path /path/to/imagenet --batch 64 --lr 3e-3 --epochs 800 --weight-decay 0.05 --sched cosine --input-size 192 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.0 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup .8 --drop-path 0.45 --cutmix 1.0 --unscale-lr --repeated-aug --bce-loss  --color-jitter 0.3 --ThreeAugment\n```\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.092\n\nepoch 5: 2.961\n\nepoch 10: 9.847\n\nepoch 15: 16.421\n\nepoch 20: 24.51\n\nepoch 25: 31.918\n\nepoch 30: 39.016\n\nepoch 35: 44.352\n\nepoch 40: 49.568\n\nepoch 45: 53.111\n\nepoch 50: 55.806\n\nepoch 55: 58.831\n\nepoch 60: 60.379\n\nepoch 65: 62.038\n\nepoch 70: 63.162\n\nepoch 75: 64.655\n\nepoch 80: 65.499\n\nepoch 85: 66.191\n\nepoch 90: 66.557\n\nepoch 95: 67.462\n\nepoch 100: 68.096\n\nepoch 105: 68.636\n\nepoch 110: 68.924\n\nepoch 115: 69.534\n\nepoch 120: 70.024\n\nepoch 125: 70.527\n\nepoch 130: 70.783\n\nepoch 135: 71.007\n\nepoch 140: 71.211\n\nepoch 145: 71.559\n\nepoch 150: 71.819\n\nepoch 155: 72.241\n\nepoch 160: 72.095\n\nepoch 165: 72.553\n\nepoch 170: 72.945\n\nepoch 175: 72.831\n\nepoch 180: 73.313\n\nepoch 185: 73.476\n\nepoch 190: 73.534\n\nepoch 195: 73.305\n\nepoch 200: 73.85\n\nepoch 205: 74.24\n\nepoch 210: 74.34\n\nepoch 215: 74.492\n\nepoch 220: 74.67\n\nepoch 225: 74.854\n\nepoch 230: 74.94\n\nepoch 235: 74.848\n\nepoch 240: 75.082\n\nepoch 245: 75.572\n\nepoch 250: 75.698\n\nepoch 255: 75.568\n\nepoch 260: 75.948\n\nepoch 265: 75.79\n\nepoch 270: 76.116\n\nepoch 275: 76.312\n\nepoch 280: 76.322\n\nepoch 285: 76.751\n\nepoch 290: 76.891\n\nepoch 295: 76.999\n\nepoch 300: 77.251\n\nepoch 305: 77.139\n\nepoch 310: 77.207\n\nepoch 315: 77.501\n\nepoch 320: 77.467\n\nepoch 325: 77.871\n\nepoch 330: 77.689\n\nepoch 335: 77.895\n\nepoch 340: 78.013\n\nepoch 345: 78.291\n\nepoch 350: 78.295\n\nepoch 355: 78.555\n\nepoch 360: 78.877\n\nepoch 365: 78.799\n\nepoch 370: 79.163\n\nepoch 375: 79.127\n\nepoch 380: 79.319\n\nepoch 385: 79.283\n\nepoch 390: 79.239\n\nepoch 395: 79.575\n\nepoch 400: 79.617\n\nepoch 405: 79.75\n\nepoch 410: 79.95\n\nepoch 415: 80.028\n\nepoch 420: 80.21\n\nepoch 425: 80.234\n\nepoch 430: 80.42\n\nepoch 435: 80.516\n\nepoch 440: 80.802\n\nepoch 445: 80.718\n\nepoch 450: 80.878\n\nepoch 455: 80.926\n\nepoch 460: 81.118\n\nepoch 465: 81.326\n\nepoch 470: 81.298\n\nepoch 475: 81.546\n\nepoch 480: 81.626\n\nepoch 485: 81.662\n\nepoch 490: 81.624\n\nepoch 495: 81.83\n\nepoch 500: 81.854\n\nepoch 505: 81.904\n\nepoch 510: 82.192\n\nepoch 515: 82.0\n\nepoch 520: 82.112\n\nepoch 525: 82.286\n\nepoch 530: 82.318\n\nepoch 535: 82.402\n\nepoch 540: 82.514\n\nepoch 545: 82.58\n\nepoch 550: 82.626\n\nepoch 555: 82.75\n\nepoch 560: 82.762\n\nepoch 565: 82.835\n\nepoch 570: 83.019\n\nepoch 575: 82.977\n\nepoch 580: 83.033\n\nepoch 585: 83.191\n\nepoch 590: 83.145\n\nepoch 595: 83.303\n\nepoch 600: 83.313\n\nepoch 605: 83.285\n\nepoch 610: 83.443\n\nepoch 615: 83.515\n\nepoch 620: 83.517\n\nepoch 625: 83.425\n\nepoch 630: 83.487\n\nepoch 635: 83.641\n\nepoch 640: 83.635\n\nepoch 645: 83.581\n\nepoch 650: 83.667\n\nepoch 655: 83.537\n\nepoch 660: 83.669\n\nepoch 665: 83.603\n\nepoch 670: 83.775\n\nepoch 675: 83.769\n\nepoch 680: 83.703\n\nepoch 685: 83.741\n\nepoch 690: 83.737\n\nepoch 695: 83.839\n\nepoch 700: 83.873\n\nepoch 705: 83.963\n\nepoch 710: 83.929\n\nepoch 715: 83.877\n\nepoch 720: 83.919\n\nepoch 725: 83.927\n\nepoch 730: 83.943\n\nepoch 735: 83.991\n\nepoch 740: 84.037\n\nepoch 745: 83.969\n\nepoch 750: 83.941\n\nepoch 755: 84.009\n\nepoch 760: 84.009\n\nepoch 765: 84.039\n\nepoch 770: 84.047\n\nepoch 775: 84.007\n\nepoch 780: 83.995\n\nepoch 785: 84.031\n\nepoch 790: 84.031\n\nepoch 795: 84.049\n\nepoch 800: 84.024\n  \n</details>\n\n\n```\npython run_with_submitit.py --model deit_huge_patch14_LS --data-path /path/to/imagenet --batch 64 --lr 3e-3 --epochs 800 --weight-decay 0.05 --sched cosine --input-size 160 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.0 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup .8 --drop-path 0.6 --cutmix 1.0 --unscale-lr --repeated-aug --bce-loss  --color-jitter 0.3 --ThreeAugment\n```\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.062\n\nepoch 5: 2.553\n\nepoch 10: 8.143\n\nepoch 15: 13.69\n\nepoch 20: 19.48\n\nepoch 25: 25.138\n\nepoch 30: 30.98\n\nepoch 35: 35.323\n\nepoch 40: 40.369\n\nepoch 45: 44.196\n\nepoch 50: 47.299\n\nepoch 55: 50.34\n\nepoch 60: 52.623\n\nepoch 65: 54.659\n\nepoch 70: 56.062\n\nepoch 75: 57.616\n\nepoch 80: 58.977\n\nepoch 85: 59.841\n\nepoch 90: 60.899\n\nepoch 95: 61.578\n\nepoch 100: 62.444\n\nepoch 105: 63.266\n\nepoch 110: 63.598\n\nepoch 115: 64.008\n\nepoch 120: 64.867\n\nepoch 125: 65.447\n\nepoch 130: 65.621\n\nepoch 135: 66.247\n\nepoch 140: 66.597\n\nepoch 145: 67.422\n\nepoch 150: 67.266\n\nepoch 155: 67.77\n\nepoch 160: 67.966\n\nepoch 165: 68.492\n\nepoch 170: 68.84\n\nepoch 175: 68.996\n\nepoch 180: 69.322\n\nepoch 185: 69.654\n\nepoch 190: 69.816\n\nepoch 195: 69.628\n\nepoch 200: 70.276\n\nepoch 205: 70.601\n\nepoch 210: 70.879\n\nepoch 215: 70.991\n\nepoch 220: 71.069\n\nepoch 225: 71.605\n\nepoch 230: 71.439\n\nepoch 235: 71.983\n\nepoch 240: 72.033\n\nepoch 245: 72.319\n\nepoch 250: 72.567\n\nepoch 255: 72.537\n\nepoch 260: 73.025\n\nepoch 265: 73.249\n\nepoch 270: 73.205\n\nepoch 275: 73.281\n\nepoch 280: 73.556\n\nepoch 285: 73.848\n\nepoch 290: 73.974\n\nepoch 295: 74.332\n\nepoch 300: 74.044\n\nepoch 305: 74.54\n\nepoch 310: 74.576\n\nepoch 315: 74.756\n\nepoch 320: 75.01\n\nepoch 325: 75.146\n\nepoch 330: 75.42\n\nepoch 335: 75.566\n\nepoch 340: 75.764\n\nepoch 345: 75.932\n\nepoch 350: 76.132\n\nepoch 355: 76.212\n\nepoch 360: 76.426\n\nepoch 365: 76.43\n\nepoch 370: 76.953\n\nepoch 375: 76.755\n\nepoch 380: 77.125\n\nepoch 385: 77.437\n\nepoch 390: 77.227\n\nepoch 395: 77.505\n\nepoch 400: 77.715\n\nepoch 405: 77.833\n\nepoch 410: 77.801\n\nepoch 415: 78.339\n\nepoch 420: 78.231\n\nepoch 425: 78.491\n\nepoch 430: 78.541\n\nepoch 435: 78.651\n\nepoch 440: 78.877\n\nepoch 445: 79.009\n\nepoch 450: 79.079\n\nepoch 455: 79.409\n\nepoch 460: 79.365\n\nepoch 465: 79.513\n\nepoch 470: 79.782\n\nepoch 475: 79.8\n\nepoch 480: 79.938\n\nepoch 485: 79.938\n\nepoch 490: 80.01\n\nepoch 495: 80.148\n\nepoch 500: 80.348\n\nepoch 505: 80.51\n\nepoch 510: 80.79\n\nepoch 515: 80.722\n\nepoch 520: 80.72\n\nepoch 525: 80.872\n\nepoch 530: 81.276\n\nepoch 535: 80.992\n\nepoch 540: 81.354\n\nepoch 545: 81.264\n\nepoch 550: 81.312\n\nepoch 555: 81.474\n\nepoch 560: 81.46\n\nepoch 565: 81.652\n\nepoch 570: 81.634\n\nepoch 575: 81.75\n\nepoch 580: 81.868\n\nepoch 585: 81.972\n\nepoch 590: 81.924\n\nepoch 595: 82.09\n\nepoch 600: 82.236\n\nepoch 605: 82.174\n\nepoch 610: 82.36\n\nepoch 615: 82.526\n\nepoch 620: 82.47\n\nepoch 625: 82.534\n\nepoch 630: 82.678\n\nepoch 635: 82.55\n\nepoch 640: 82.716\n\nepoch 645: 82.726\n\nepoch 650: 82.825\n\nepoch 655: 82.903\n\nepoch 660: 82.913\n\nepoch 665: 82.965\n\nepoch 670: 83.003\n\nepoch 675: 82.995\n\nepoch 680: 82.989\n\nepoch 685: 83.013\n\nepoch 690: 83.047\n\nepoch 695: 83.137\n\nepoch 700: 83.241\n\nepoch 705: 83.147\n\nepoch 710: 83.207\n\nepoch 715: 83.187\n\nepoch 720: 83.307\n\nepoch 725: 83.319\n\nepoch 730: 83.371\n\nepoch 735: 83.309\n\nepoch 740: 83.259\n\nepoch 745: 83.335\n\nepoch 750: 83.389\n\nepoch 755: 83.373\n\nepoch 760: 83.357\n\nepoch 765: 83.425\n\nepoch 770: 83.367\n\nepoch 775: 83.433\n\nepoch 780: 83.387\n\nepoch 785: 83.387\n\nepoch 790: 83.449\n\nepoch 795: 83.449\n\nepoch 800: 83.484\n</details>\n\nfinetuning for ViT-B, L and H at resolution 224x224:\n\n```\npython run_with_submitit.py --model deit_base_patch16_LS --data-path /path/to/imagenet --batch 64 --lr 1e-5 --epochs 20 --weight-decay 0.1 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 1 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt adamw --warmup-lr 1e-6 --mixup .8 --drop-path 0.2 --cutmix 1.0 --unscale-lr  --aa rand-m9-mstd0.5-inc1 --no-repeated-aug --finetune model_path\n```\n\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 83.448\n\nepoch 5: 83.71\n\nepoch 10: 83.788\n\nepoch 15: 83.826\n\nepoch 20: 83.802\n\n</details>\n\n```\npython run_with_submitit.py --model deit_large_patch16_LS --data-path /path/to/imagenet --batch 64 --lr 1e-5 --epochs 20 --weight-decay 0.1 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 1 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt adamw --warmup-lr 1e-6 --mixup .8 --drop-path 0.45 --cutmix 1.0 --unscale-lr  --aa rand-m9-mstd0.5-inc1 --no-repeated-aug --finetune model_path\n```\n\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 84.458\n\nepoch 5: 84.716\n\nepoch 10: 84.826\n\nepoch 15: 84.796\n\nepoch 20: 84.862\n  \n</details>\n\n```\npython run_with_submitit.py --model deit_huge_patch14_LS --data-path /path/to/imagenet --batch 32 --lr 1e-5 --epochs 20 --weight-decay 0.1 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 2 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt adamw --warmup-lr 1e-6 --mixup .8 --drop-path 0.55 --cutmix 1.0 --unscale-lr  --aa rand-m9-mstd0.5-inc1 --no-repeated-aug --finetune model_path\n```\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 84.328\n\nepoch 5: 85.028\n\nepoch 10: 85.128\n\nepoch 15: 85.184\n\nepoch 20: 85.218\n  \n</details>\n\nfinetuning for ViT-S, B and L at resolution 384x384:\n\n```\npython run_with_submitit.py --model deit_small_patch16_LS --data-path /path/to/imagenet --batch 64 --lr 1e-5 --epochs 20 --weight-decay 0.1 --sched cosine --input-size 384 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 1 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt adamw --warmup-lr 1e-6 --mixup .8 --drop-path 0.0 --cutmix 1.0 --unscale-lr --no-repeated-aug --aa rand-m9-mstd0.5-inc1 --finetune model_path\n```\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 82.134\n\nepoch 5: 83.256\n\nepoch 10: 83.452\n\nepoch 15: 83.428\n\nepoch 20: 83.422\n\n</details>\n\n```\npython run_with_submitit.py --model deit_base_patch16_LS --data-path /path/to/imagenet --batch 32 --lr 1e-5 --epochs 20 --weight-decay 0.1 --sched cosine --input-size 384 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 2 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt adamw --warmup-lr 1e-6 --mixup .8 --drop-path 0.15 --cutmix 1.0 --unscale-lr --no-repeated-aug --aa rand-m9-mstd0.5-inc1 --finetune model_path\n```\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 84.296\n\nepoch 5: 84.968\n\nepoch 10: 85.002\n\nepoch 15: 84.986\n\nepoch 20: 85.076 \n  \n</details>\n\n```\npython run_with_submitit.py --model deit_large_patch16_LS --data-path /path/to/imagenet --batch 16 --lr 1e-5 --epochs 20 --weight-decay 0.1 --sched cosine --input-size 384 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt adamw --warmup-lr 1e-6 --mixup .8 --drop-path 0.4 --cutmix 1.0 --unscale-lr --no-repeated-aug --aa rand-m9-mstd0.5-inc1 --finetune model_path\n```\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 84.691\n\nepoch 5: 85.458\n\nepoch 10: 85.736\n\nepoch 15: 85.794\n\nepoch 20: 85.812\n  \n</details>\n\nTraining on ImageNet-21k:\n\nIt is possible to train with a batch size of 4096 with a learning rate of 0.0015 instead of 0.001.\n\n```\npython run_with_submitit.py --model deit_small_patch16_LS --data-path /path/to/imagenet --batch 128 --lr 0.001 --epochs 240 --weight-decay 0.02 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 2 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup 0 --drop-path 0.05 --cutmix 1.0 --unscale-lr --no-repeated-aug   --color-jitter 0.3 --ThreeAugment --src\n```\n\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.0\n\nepoch 5: 8.105\n\nepoch 10: 32.78\n\nepoch 15: 41.501\n\nepoch 20: 44.226\n\nepoch 25: 45.172\n\nepoch 30: 45.991\n\nepoch 35: 47.491\n\nepoch 40: 47.151\n\nepoch 45: 47.923\n\nepoch 50: 48.327\n\nepoch 55: 48.039\n\nepoch 60: 48.325\n\nepoch 65: 48.972\n\nepoch 70: 49.1\n\nepoch 75: 49.572\n\nepoch 80: 49.368\n\nepoch 85: 50.138\n\nepoch 90: 49.82\n\nepoch 95: 50.33\n\nepoch 100: 50.748\n\nepoch 105: 50.624\n\nepoch 110: 50.63\n\nepoch 115: 50.458\n\nepoch 120: 51.13\n\nepoch 125: 51.184\n\nepoch 130: 51.871\n\nepoch 135: 51.931\n\nepoch 140: 51.791\n\nepoch 145: 52.369\n\nepoch 150: 52.409\n\nepoch 155: 52.449\n\nepoch 160: 52.499\n\nepoch 165: 52.723\n\nepoch 170: 53.267\n\nepoch 175: 53.413\n\nepoch 180: 53.373\n\nepoch 185: 53.481\n\nepoch 190: 53.321\n\nepoch 195: 53.945\n\nepoch 200: 53.913\n\nepoch 205: 53.791\n\nepoch 210: 54.111\n\nepoch 215: 53.963\n\nepoch 220: 54.057\n\nepoch 225: 54.045\n\nepoch 230: 54.107\n\nepoch 235: 54.143\n\nepoch 240: 54.12\n  \n</details>\n\n```\npython run_with_submitit.py --model deit_base_patch16_LS --data-path /path/to/imagenet --batch 64 --lr 0.001 --epochs 240 --weight-decay 0.02 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup 0 --drop-path 0.1 --cutmix 1.0 --unscale-lr --no-repeated-aug   --color-jitter 0.3 --ThreeAugment --src\n```\n<details>\n<summary>\n  logs\n</summary>\n\nepoch 0: 0.0\n\nepoch 5: 14.083\n\nepoch 10: 40.471\n\nepoch 15: 46.165\n\nepoch 20: 48.88\n\nepoch 25: 49.576\n\nepoch 30: 50.562\n\nepoch 35: 51.402\n\nepoch 40: 51.681\n\nepoch 45: 51.799\n\nepoch 50: 52.783\n\nepoch 55: 52.457\n\nepoch 60: 52.675\n\nepoch 65: 53.413\n\nepoch 70: 53.029\n\nepoch 75: 53.679\n\nepoch 80: 53.265\n\nepoch 85: 53.295\n\nepoch 90: 53.811\n\nepoch 95: 54.467\n\nepoch 100: 54.223\n\nepoch 105: 55.15\n\nepoch 110: 55.112\n\nepoch 115: 54.605\n\nepoch 120: 54.826\n\nepoch 125: 55.006\n\nepoch 130: 55.244\n\nepoch 135: 55.628\n\nepoch 140: 55.344\n\nepoch 145: 55.78\n\nepoch 150: 55.896\n\nepoch 155: 55.756\n\nepoch 160: 56.002\n\nepoch 165: 56.072\n\nepoch 170: 56.472\n\nepoch 175: 56.278\n\nepoch 180: 56.442\n\nepoch 185: 56.47\n\nepoch 190: 56.298\n\nepoch 195: 56.49\n\nepoch 200: 56.64\n\nepoch 205: 56.396\n\nepoch 210: 56.678\n\nepoch 215: 56.652\n\nepoch 220: 56.444\n\nepoch 225: 56.446\n\nepoch 230: 56.512\n\nepoch 235: 56.586\n\nepoch 240: 56.462\n  \n</details>\n\n```\npython run_with_submitit.py --model deit_large_patch16_LS --data-path /path/to/imagenet --batch 64 --lr 0.001 --epochs 240 --weight-decay 0.02 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup 0 --drop-path 0.3 --cutmix 1.0 --unscale-lr --no-repeated-aug   --color-jitter 0.3 --ThreeAugment --src\n```\n\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.0\n\nepoch 5: 18.702\n\nepoch 10: 43.678\n\nepoch 15: 48.805\n\nepoch 20: 51.103\n\nepoch 25: 51.852\n\nepoch 30: 52.725\n\nepoch 35: 53.211\n\nepoch 40: 53.82\n\nepoch 45: 54.136\n\nepoch 50: 55.141\n\nepoch 55: 54.302\n\nepoch 60: 55.085\n\nepoch 65: 55.099\n\nepoch 70: 55.113\n\nepoch 75: 55.792\n\nepoch 80: 55.493\n\nepoch 85: 55.782\n\nepoch 90: 55.567\n\nepoch 95: 55.896\n\nepoch 100: 56.132\n\nepoch 105: 55.936\n\nepoch 110: 56.498\n\nepoch 115: 56.264\n\nepoch 120: 56.45\n\nepoch 125: 56.568\n\nepoch 130: 56.734\n\nepoch 135: 57.083\n\nepoch 140: 56.915\n\nepoch 145: 57.197\n\nepoch 150: 56.999\n\nepoch 155: 56.929\n\nepoch 160: 57.211\n\nepoch 165: 57.219\n\nepoch 170: 57.623\n\nepoch 175: 57.329\n\nepoch 180: 57.243\n\nepoch 185: 57.523\n\nepoch 190: 57.257\n\nepoch 195: 57.391\n\nepoch 200: 57.343\n\nepoch 205: 57.211\n\nepoch 210: 57.437\n\nepoch 215: 57.471\n\nepoch 220: 57.481\n\nepoch 225: 57.417\n\nepoch 230: 57.439\n\nepoch 235: 57.497\n\nepoch 240: 57.412\n  \n</details>\n\n\n```\npython run_with_submitit.py --model deit_huge_patch14_LS --data-path /path/to/imagenet --batch 64 --lr 0.001 --epochs 90 --weight-decay 0.02 --sched cosine --input-size 128 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup 0 --drop-path 0.5 --cutmix 1.0 --unscale-lr --no-repeated-aug   --color-jitter 0.3 --ThreeAugment --src\n```\n\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.0\n\nepoch 5: 31.292\n\nepoch 10: 45.668\n\nepoch 15: 48.563\n\nepoch 20: 50.37\n\nepoch 25: 51.143\n\nepoch 30: 52.518\n\nepoch 35: 53.205\n\nepoch 40: 53.459\n\nepoch 45: 54.482\n\nepoch 50: 55.956\n\nepoch 55: 55.872\n\nepoch 60: 56.658\n\nepoch 65: 57.173\n\nepoch 70: 57.525\n\nepoch 75: 57.882\n\nepoch 80: 57.761\n\nepoch 85: 58.006\n\nepoch 90: 57.952\n  \n</details>\n\nfinetuning on ImageNet-1k:\n\n```\npython run_with_submitit.py --model deit_small_patch16_LS --data-path /path/to/imagenet --batch 128 --lr 0.0003 --epochs 50 --weight-decay 0.02 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 2 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup 0 --drop-path 0.05 --cutmix 1.0 --unscale-lr --no-repeated-aug   --color-jitter 0.3 --ThreeAugment --src\n```\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.13\n\nepoch 5: 79.023\n\nepoch 10: 80.952\n\nepoch 15: 81.68\n\nepoch 20: 82.178\n\nepoch 25: 82.436\n\nepoch 30: 82.678\n\nepoch 35: 82.925\n\nepoch 40: 82.963\n\nepoch 45: 83.053\n\nepoch 50: 83.074\n</details>\n\n\n```\npython run_with_submitit.py --model deit_base_patch16_LS --data-path /path/to/imagenet --batch 64 --lr 0.0003 --epochs 50 --weight-decay 0.02 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup 0 --drop-path 0.15 --cutmix 1.0 --unscale-lr --no-repeated-aug   --color-jitter 0.3 --ThreeAugment --src\n```\n\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.11\n\nepoch 5: 83.445\n\nepoch 10: 84.677\n\nepoch 15: 85.211\n\nepoch 20: 85.265\n\nepoch 25: 85.333\n\nepoch 30: 85.437\n\nepoch 35: 85.675\n\nepoch 40: 85.671\n\nepoch 45: 85.707\n\nepoch 50: 85.698\n  \n</details>\n\n```\npython run_with_submitit.py --model deit_large_patch16_LS --data-path /path/to/imagenet --batch 64 --lr 0.0003 --epochs 50 --weight-decay 0.02 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup 0 --drop-path 0.4 --cutmix 1.0 --unscale-lr --no-repeated-aug   --color-jitter 0.3 --ThreeAugment --src\n```\n\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.106\n\nepoch 5: 85.427\n\nepoch 10: 86.362\n\nepoch 15: 86.624\n\nepoch 20: 86.696\n\nepoch 25: 86.826\n\nepoch 30: 86.83\n\nepoch 35: 86.954\n\nepoch 40: 86.944\n\nepoch 45: 86.934\n\nepoch 50: 86.98\n  \n</details>\n\n```\npython run_with_submitit.py --model deit_huge_patch14_LS --data-path /path/to/imagenet --batch 64 --lr 0.0003 --epochs 50 --weight-decay 0.02 --sched cosine --input-size 224 --eval-crop-ratio 1.0 --reprob 0.0 --nodes 4 --ngpus 8 --smoothing 0.1 --warmup-epochs 5 --drop 0.0 --nb-classes 1000 --seed 0 --opt fusedlamb --warmup-lr 1e-6 --mixup 0 --drop-path 0.45 --cutmix 1.0 --unscale-lr --no-repeated-aug   --color-jitter 0.3 --ThreeAugment --src\n```\n\n<details>\n<summary>\n  logs\n</summary>\n  \nepoch 0: 0.126\n\nepoch 5: 84.681\n\nepoch 10: 86.291\n\nepoch 15: 86.753\n\nepoch 20: 86.827\n\nepoch 25: 86.903\n\nepoch 30: 87.126\n\nepoch 35: 87.056\n\nepoch 40: 87.11\n\nepoch 45: 87.154\n\nepoch 50: 87.184\n</details>\n\n# License\nThis repository is released under the Apache 2.0 license as found in the [LICENSE](LICENSE) file.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n\n"
        },
        {
          "name": "augment.py",
          "type": "blob",
          "size": 3.369140625,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n\"\"\"\n3Augment implementation\nData-augmentation (DA) based on dino DA (https://github.com/facebookresearch/dino)\nand timm DA(https://github.com/rwightman/pytorch-image-models)\n\"\"\"\nimport torch\nfrom torchvision import transforms\n\nfrom timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\n\nimport numpy as np\nfrom torchvision import datasets, transforms\nimport random\n\n\n\nfrom PIL import ImageFilter, ImageOps\nimport torchvision.transforms.functional as TF\n\n\nclass GaussianBlur(object):\n    \"\"\"\n    Apply Gaussian Blur to the PIL image.\n    \"\"\"\n    def __init__(self, p=0.1, radius_min=0.1, radius_max=2.):\n        self.prob = p\n        self.radius_min = radius_min\n        self.radius_max = radius_max\n\n    def __call__(self, img):\n        do_it = random.random() <= self.prob\n        if not do_it:\n            return img\n\n        img = img.filter(\n            ImageFilter.GaussianBlur(\n                radius=random.uniform(self.radius_min, self.radius_max)\n            )\n        )\n        return img\n\nclass Solarization(object):\n    \"\"\"\n    Apply Solarization to the PIL image.\n    \"\"\"\n    def __init__(self, p=0.2):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            return ImageOps.solarize(img)\n        else:\n            return img\n\nclass gray_scale(object):\n    \"\"\"\n    Apply Solarization to the PIL image.\n    \"\"\"\n    def __init__(self, p=0.2):\n        self.p = p\n        self.transf = transforms.Grayscale(3)\n \n    def __call__(self, img):\n        if random.random() < self.p:\n            return self.transf(img)\n        else:\n            return img\n \n    \n    \nclass horizontal_flip(object):\n    \"\"\"\n    Apply Solarization to the PIL image.\n    \"\"\"\n    def __init__(self, p=0.2,activate_pred=False):\n        self.p = p\n        self.transf = transforms.RandomHorizontalFlip(p=1.0)\n \n    def __call__(self, img):\n        if random.random() < self.p:\n            return self.transf(img)\n        else:\n            return img\n        \n    \n    \ndef new_data_aug_generator(args = None):\n    img_size = args.input_size\n    remove_random_resized_crop = args.src\n    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n    primary_tfl = []\n    scale=(0.08, 1.0)\n    interpolation='bicubic'\n    if remove_random_resized_crop:\n        primary_tfl = [\n            transforms.Resize(img_size, interpolation=3),\n            transforms.RandomCrop(img_size, padding=4,padding_mode='reflect'),\n            transforms.RandomHorizontalFlip()\n        ]\n    else:\n        primary_tfl = [\n            RandomResizedCropAndInterpolation(\n                img_size, scale=scale, interpolation=interpolation),\n            transforms.RandomHorizontalFlip()\n        ]\n\n        \n    secondary_tfl = [transforms.RandomChoice([gray_scale(p=1.0),\n                                              Solarization(p=1.0),\n                                              GaussianBlur(p=1.0)])]\n   \n    if args.color_jitter is not None and not args.color_jitter==0:\n        secondary_tfl.append(transforms.ColorJitter(args.color_jitter, args.color_jitter, args.color_jitter))\n    final_tfl = [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=torch.tensor(mean),\n                std=torch.tensor(std))\n        ]\n    return transforms.Compose(primary_tfl+secondary_tfl+final_tfl)\n"
        },
        {
          "name": "cait_models.py",
          "type": "blob",
          "size": 17.4951171875,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\nimport torch\nimport torch.nn as nn\nfrom functools import partial\n\nfrom timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\nfrom timm.models.registry import register_model\nfrom timm.models.layers import trunc_normal_, DropPath\n\n\n__all__ = [\n    'cait_M48', 'cait_M36',\n    'cait_S36', 'cait_S24','cait_S24_224',\n    'cait_XS24','cait_XXS24','cait_XXS24_224',\n    'cait_XXS36','cait_XXS36_224'\n]\n\nclass Class_Attention(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to do CA \n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    \n    def forward(self, x ):\n        \n        B, N, C = x.shape\n        q = self.q(x[:,0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        q = q * self.scale\n        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        attn = (q @ k.transpose(-2, -1)) \n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x_cls = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n        x_cls = self.proj(x_cls)\n        x_cls = self.proj_drop(x_cls)\n        \n        return x_cls     \n        \nclass LayerScale_Block_CA(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add CA and LayerScale\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, Attention_block = Class_Attention,\n                 Mlp_block=Mlp,init_values=1e-4):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n\n    \n    def forward(self, x, x_cls):\n        \n        u = torch.cat((x_cls,x),dim=1)\n        \n        \n        x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))\n        \n        x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))\n        \n        return x_cls \n        \n        \nclass Attention_talking_head(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add Talking Heads Attention (https://arxiv.org/pdf/2003.02436v1.pdf)\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        \n        self.num_heads = num_heads\n        \n        head_dim = dim // num_heads\n        \n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        \n        self.proj = nn.Linear(dim, dim)\n        \n        self.proj_l = nn.Linear(num_heads, num_heads)\n        self.proj_w = nn.Linear(num_heads, num_heads)\n        \n        self.proj_drop = nn.Dropout(proj_drop)\n\n\n    \n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0] * self.scale , qkv[1], qkv[2] \n    \n        attn = (q @ k.transpose(-2, -1)) \n        \n        attn = self.proj_l(attn.permute(0,2,3,1)).permute(0,3,1,2)\n                \n        attn = attn.softmax(dim=-1)\n  \n        attn = self.proj_w(attn.permute(0,2,3,1)).permute(0,3,1,2)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n    \nclass LayerScale_Block(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add layerScale\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention_talking_head,\n                 Mlp_block=Mlp,init_values=1e-4):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n\n    def forward(self, x):        \n        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x \n    \n    \n    \n    \nclass cait_models(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to adapt to our cait models\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, global_pool=None,\n                 block_layers = LayerScale_Block,\n                 block_layers_token = LayerScale_Block_CA,\n                 Patch_layer=PatchEmbed,act_layer=nn.GELU,\n                 Attention_block = Attention_talking_head,Mlp_block=Mlp,\n                init_scale=1e-4,\n                Attention_block_token_only=Class_Attention,\n                Mlp_block_token_only= Mlp, \n                depth_token_only=2,\n                mlp_ratio_clstk = 4.0):\n        super().__init__()\n        \n\n            \n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  \n\n        self.patch_embed = Patch_layer(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        \n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [drop_path_rate for i in range(depth)] \n        self.blocks = nn.ModuleList([\n            block_layers(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                act_layer=act_layer,Attention_block=Attention_block,Mlp_block=Mlp_block,init_values=init_scale)\n            for i in range(depth)])\n        \n\n        self.blocks_token_only = nn.ModuleList([\n            block_layers_token(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio_clstk, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=0.0, attn_drop=0.0, drop_path=0.0, norm_layer=norm_layer,\n                act_layer=act_layer,Attention_block=Attention_block_token_only,\n                Mlp_block=Mlp_block_token_only,init_values=init_scale)\n            for i in range(depth_token_only)])\n            \n        self.norm = norm_layer(embed_dim)\n\n\n        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  \n        \n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for i , blk in enumerate(self.blocks):\n            x = blk(x)\n            \n        for i , blk in enumerate(self.blocks_token_only):\n            cls_tokens = blk(x,cls_tokens)\n\n        x = torch.cat((cls_tokens, x), dim=1)\n            \n                \n        x = self.norm(x)\n        return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        \n        x = self.head(x)\n\n        return x \n        \n@register_model\ndef cait_XXS24_224(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 224,patch_size=16, embed_dim=192, depth=24, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XXS24_224.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_XXS24(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=192, depth=24, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XXS24_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n@register_model\ndef cait_XXS36_224(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 224,patch_size=16, embed_dim=192, depth=36, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XXS36_224.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_XXS36(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=192, depth=36, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XXS36_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_XS24(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=288, depth=24, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XS24_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n\n\n\n@register_model\ndef cait_S24_224(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 224,patch_size=16, embed_dim=384, depth=24, num_heads=8, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/S24_224.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_S24(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=384, depth=24, num_heads=8, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/S24_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_S36(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=384, depth=36, num_heads=8, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-6,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/S36_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n\n    return model \n\n\n\n\n\n@register_model\ndef cait_M36(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384, patch_size=16, embed_dim=768, depth=36, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-6,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/M36_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n\n    return model \n\n\n@register_model\ndef cait_M48(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 448 , patch_size=16, embed_dim=768, depth=48, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-6,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/M48_448.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model         \n"
        },
        {
          "name": "datasets.py",
          "type": "blob",
          "size": 4.0263671875,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nimport os\nimport json\n\nfrom torchvision import datasets, transforms\nfrom torchvision.datasets.folder import ImageFolder, default_loader\n\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.data import create_transform\n\n\nclass INatDataset(ImageFolder):\n    def __init__(self, root, train=True, year=2018, transform=None, target_transform=None,\n                 category='name', loader=default_loader):\n        self.transform = transform\n        self.loader = loader\n        self.target_transform = target_transform\n        self.year = year\n        # assert category in ['kingdom','phylum','class','order','supercategory','family','genus','name']\n        path_json = os.path.join(root, f'{\"train\" if train else \"val\"}{year}.json')\n        with open(path_json) as json_file:\n            data = json.load(json_file)\n\n        with open(os.path.join(root, 'categories.json')) as json_file:\n            data_catg = json.load(json_file)\n\n        path_json_for_targeter = os.path.join(root, f\"train{year}.json\")\n\n        with open(path_json_for_targeter) as json_file:\n            data_for_targeter = json.load(json_file)\n\n        targeter = {}\n        indexer = 0\n        for elem in data_for_targeter['annotations']:\n            king = []\n            king.append(data_catg[int(elem['category_id'])][category])\n            if king[0] not in targeter.keys():\n                targeter[king[0]] = indexer\n                indexer += 1\n        self.nb_classes = len(targeter)\n\n        self.samples = []\n        for elem in data['images']:\n            cut = elem['file_name'].split('/')\n            target_current = int(cut[2])\n            path_current = os.path.join(root, cut[0], cut[2], cut[3])\n\n            categors = data_catg[target_current]\n            target_current_true = targeter[categors[category]]\n            self.samples.append((path_current, target_current_true))\n\n    # __getitem__ and __len__ inherited from ImageFolder\n\n\ndef build_dataset(is_train, args):\n    transform = build_transform(is_train, args)\n\n    if args.data_set == 'CIFAR':\n        dataset = datasets.CIFAR100(args.data_path, train=is_train, transform=transform)\n        nb_classes = 100\n    elif args.data_set == 'IMNET':\n        root = os.path.join(args.data_path, 'train' if is_train else 'val')\n        dataset = datasets.ImageFolder(root, transform=transform)\n        nb_classes = 1000\n    elif args.data_set == 'INAT':\n        dataset = INatDataset(args.data_path, train=is_train, year=2018,\n                              category=args.inat_category, transform=transform)\n        nb_classes = dataset.nb_classes\n    elif args.data_set == 'INAT19':\n        dataset = INatDataset(args.data_path, train=is_train, year=2019,\n                              category=args.inat_category, transform=transform)\n        nb_classes = dataset.nb_classes\n\n    return dataset, nb_classes\n\n\ndef build_transform(is_train, args):\n    resize_im = args.input_size > 32\n    if is_train:\n        # this should always dispatch to transforms_imagenet_train\n        transform = create_transform(\n            input_size=args.input_size,\n            is_training=True,\n            color_jitter=args.color_jitter,\n            auto_augment=args.aa,\n            interpolation=args.train_interpolation,\n            re_prob=args.reprob,\n            re_mode=args.remode,\n            re_count=args.recount,\n        )\n        if not resize_im:\n            # replace RandomResizedCropAndInterpolation with\n            # RandomCrop\n            transform.transforms[0] = transforms.RandomCrop(\n                args.input_size, padding=4)\n        return transform\n\n    t = []\n    if resize_im:\n        size = int(args.input_size / args.eval_crop_ratio)\n        t.append(\n            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n        )\n        t.append(transforms.CenterCrop(args.input_size))\n\n    t.append(transforms.ToTensor())\n    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n    return transforms.Compose(t)\n"
        },
        {
          "name": "engine.py",
          "type": "blob",
          "size": 4.1416015625,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\"\"\"\nTrain and eval functions used in main.py\n\"\"\"\nimport math\nimport sys\nfrom typing import Iterable, Optional\n\nimport torch\n\nfrom timm.data import Mixup\nfrom timm.utils import accuracy, ModelEma\n\nfrom losses import DistillationLoss\nimport utils\n\n\ndef train_one_epoch(model: torch.nn.Module, criterion: DistillationLoss,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, loss_scaler, max_norm: float = 0,\n                    model_ema: Optional[ModelEma] = None, mixup_fn: Optional[Mixup] = None,\n                    set_training_mode=True, args = None):\n    model.train(set_training_mode)\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = 10\n    \n    if args.cosub:\n        criterion = torch.nn.BCEWithLogitsLoss()\n        \n    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n        samples = samples.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets)\n            \n        if args.cosub:\n            samples = torch.cat((samples,samples),dim=0)\n            \n        if args.bce_loss:\n            targets = targets.gt(0.0).type(targets.dtype)\n         \n        with torch.cuda.amp.autocast():\n            outputs = model(samples)\n            if not args.cosub:\n                loss = criterion(samples, outputs, targets)\n            else:\n                outputs = torch.split(outputs, outputs.shape[0]//2, dim=0)\n                loss = 0.25 * criterion(outputs[0], targets) \n                loss = loss + 0.25 * criterion(outputs[1], targets) \n                loss = loss + 0.25 * criterion(outputs[0], outputs[1].detach().sigmoid())\n                loss = loss + 0.25 * criterion(outputs[1], outputs[0].detach().sigmoid()) \n\n        loss_value = loss.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            sys.exit(1)\n\n        optimizer.zero_grad()\n\n        # this attribute is added by timm on one optimizer (adahessian)\n        is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n        loss_scaler(loss, optimizer, clip_grad=max_norm,\n                    parameters=model.parameters(), create_graph=is_second_order)\n\n        torch.cuda.synchronize()\n        if model_ema is not None:\n            model_ema.update(model)\n\n        metric_logger.update(loss=loss_value)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n\n\n@torch.no_grad()\ndef evaluate(data_loader, model, device):\n    criterion = torch.nn.CrossEntropyLoss()\n\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Test:'\n\n    # switch to evaluation mode\n    model.eval()\n\n    for images, target in metric_logger.log_every(data_loader, 10, header):\n        images = images.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n\n        # compute output\n        with torch.cuda.amp.autocast():\n            output = model(images)\n            loss = criterion(output, target)\n\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        batch_size = images.shape[0]\n        metric_logger.update(loss=loss.item())\n        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n          .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))\n\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n"
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 0.2216796875,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nfrom models import *\nfrom cait_models import *\nfrom resmlp_models import *\n#from patchconvnet_models import *\n\ndependencies = [\"torch\", \"torchvision\", \"timm\"]\n"
        },
        {
          "name": "losses.py",
          "type": "blob",
          "size": 3.30859375,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\"\"\"\nImplements the knowledge distillation loss\n\"\"\"\nimport torch\nfrom torch.nn import functional as F\n\n\nclass DistillationLoss(torch.nn.Module):\n    \"\"\"\n    This module wraps a standard criterion and adds an extra knowledge distillation loss by\n    taking a teacher model prediction and using it as additional supervision.\n    \"\"\"\n    def __init__(self, base_criterion: torch.nn.Module, teacher_model: torch.nn.Module,\n                 distillation_type: str, alpha: float, tau: float):\n        super().__init__()\n        self.base_criterion = base_criterion\n        self.teacher_model = teacher_model\n        assert distillation_type in ['none', 'soft', 'hard']\n        self.distillation_type = distillation_type\n        self.alpha = alpha\n        self.tau = tau\n\n    def forward(self, inputs, outputs, labels):\n        \"\"\"\n        Args:\n            inputs: The original inputs that are feed to the teacher model\n            outputs: the outputs of the model to be trained. It is expected to be\n                either a Tensor, or a Tuple[Tensor, Tensor], with the original output\n                in the first position and the distillation predictions as the second output\n            labels: the labels for the base criterion\n        \"\"\"\n        outputs_kd = None\n        if not isinstance(outputs, torch.Tensor):\n            # assume that the model outputs a tuple of [outputs, outputs_kd]\n            outputs, outputs_kd = outputs\n        base_loss = self.base_criterion(outputs, labels)\n        if self.distillation_type == 'none':\n            return base_loss\n\n        if outputs_kd is None:\n            raise ValueError(\"When knowledge distillation is enabled, the model is \"\n                             \"expected to return a Tuple[Tensor, Tensor] with the output of the \"\n                             \"class_token and the dist_token\")\n        # don't backprop throught the teacher\n        with torch.no_grad():\n            teacher_outputs = self.teacher_model(inputs)\n\n        if self.distillation_type == 'soft':\n            T = self.tau\n            # taken from https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py#L100\n            # with slight modifications\n            distillation_loss = F.kl_div(\n                F.log_softmax(outputs_kd / T, dim=1),\n                #We provide the teacher's targets in log probability because we use log_target=True \n                #(as recommended in pytorch https://github.com/pytorch/pytorch/blob/9324181d0ac7b4f7949a574dbc3e8be30abe7041/torch/nn/functional.py#L2719)\n                #but it is possible to give just the probabilities and set log_target=False. In our experiments we tried both.\n                F.log_softmax(teacher_outputs / T, dim=1),\n                reduction='sum',\n                log_target=True\n            ) * (T * T) / outputs_kd.numel()\n            #We divide by outputs_kd.numel() to have the legacy PyTorch behavior. \n            #But we also experiments output_kd.size(0) \n            #see issue 61(https://github.com/facebookresearch/deit/issues/61) for more details\n        elif self.distillation_type == 'hard':\n            distillation_loss = F.cross_entropy(outputs_kd, teacher_outputs.argmax(dim=1))\n\n        loss = base_loss * (1 - self.alpha) + distillation_loss * self.alpha\n        return loss\n"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 22.564453125,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nimport argparse\nimport datetime\nimport numpy as np\nimport time\nimport torch\nimport torch.backends.cudnn as cudnn\nimport json\n\nfrom pathlib import Path\n\nfrom timm.data import Mixup\nfrom timm.models import create_model\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.scheduler import create_scheduler\nfrom timm.optim import create_optimizer\nfrom timm.utils import NativeScaler, get_state_dict, ModelEma\n\nfrom datasets import build_dataset\nfrom engine import train_one_epoch, evaluate\nfrom losses import DistillationLoss\nfrom samplers import RASampler\nfrom augment import new_data_aug_generator\n\nimport models\nimport models_v2\n\nimport utils\n\n\ndef get_args_parser():\n    parser = argparse.ArgumentParser('DeiT training and evaluation script', add_help=False)\n    parser.add_argument('--batch-size', default=64, type=int)\n    parser.add_argument('--epochs', default=300, type=int)\n    parser.add_argument('--bce-loss', action='store_true')\n    parser.add_argument('--unscale-lr', action='store_true')\n\n    # Model parameters\n    parser.add_argument('--model', default='deit_base_patch16_224', type=str, metavar='MODEL',\n                        help='Name of model to train')\n    parser.add_argument('--input-size', default=224, type=int, help='images input size')\n\n    parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n                        help='Dropout rate (default: 0.)')\n    parser.add_argument('--drop-path', type=float, default=0.1, metavar='PCT',\n                        help='Drop path rate (default: 0.1)')\n\n    parser.add_argument('--model-ema', action='store_true')\n    parser.add_argument('--no-model-ema', action='store_false', dest='model_ema')\n    parser.set_defaults(model_ema=True)\n    parser.add_argument('--model-ema-decay', type=float, default=0.99996, help='')\n    parser.add_argument('--model-ema-force-cpu', action='store_true', default=False, help='')\n\n    # Optimizer parameters\n    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n                        help='Optimizer (default: \"adamw\"')\n    parser.add_argument('--opt-eps', default=1e-8, type=float, metavar='EPSILON',\n                        help='Optimizer Epsilon (default: 1e-8)')\n    parser.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n                        help='Optimizer Betas (default: None, use opt default)')\n    parser.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n                        help='Clip gradient norm (default: None, no clipping)')\n    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n                        help='SGD momentum (default: 0.9)')\n    parser.add_argument('--weight-decay', type=float, default=0.05,\n                        help='weight decay (default: 0.05)')\n    # Learning rate schedule parameters\n    parser.add_argument('--sched', default='cosine', type=str, metavar='SCHEDULER',\n                        help='LR scheduler (default: \"cosine\"')\n    parser.add_argument('--lr', type=float, default=5e-4, metavar='LR',\n                        help='learning rate (default: 5e-4)')\n    parser.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n                        help='learning rate noise on/off epoch percentages')\n    parser.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n                        help='learning rate noise limit percent (default: 0.67)')\n    parser.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n                        help='learning rate noise std-dev (default: 1.0)')\n    parser.add_argument('--warmup-lr', type=float, default=1e-6, metavar='LR',\n                        help='warmup learning rate (default: 1e-6)')\n    parser.add_argument('--min-lr', type=float, default=1e-5, metavar='LR',\n                        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n\n    parser.add_argument('--decay-epochs', type=float, default=30, metavar='N',\n                        help='epoch interval to decay LR')\n    parser.add_argument('--warmup-epochs', type=int, default=5, metavar='N',\n                        help='epochs to warmup LR, if scheduler supports')\n    parser.add_argument('--cooldown-epochs', type=int, default=10, metavar='N',\n                        help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\n    parser.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n                        help='patience epochs for Plateau LR scheduler (default: 10')\n    parser.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n                        help='LR decay rate (default: 0.1)')\n\n    # Augmentation parameters\n    parser.add_argument('--color-jitter', type=float, default=0.3, metavar='PCT',\n                        help='Color jitter factor (default: 0.3)')\n    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \\\n                             \"(default: rand-m9-mstd0.5-inc1)'),\n    parser.add_argument('--smoothing', type=float, default=0.1, help='Label smoothing (default: 0.1)')\n    parser.add_argument('--train-interpolation', type=str, default='bicubic',\n                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n\n    parser.add_argument('--repeated-aug', action='store_true')\n    parser.add_argument('--no-repeated-aug', action='store_false', dest='repeated_aug')\n    parser.set_defaults(repeated_aug=True)\n    \n    parser.add_argument('--train-mode', action='store_true')\n    parser.add_argument('--no-train-mode', action='store_false', dest='train_mode')\n    parser.set_defaults(train_mode=True)\n    \n    parser.add_argument('--ThreeAugment', action='store_true') #3augment\n    \n    parser.add_argument('--src', action='store_true') #simple random crop\n    \n    # * Random Erase params\n    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n                        help='Random erase prob (default: 0.25)')\n    parser.add_argument('--remode', type=str, default='pixel',\n                        help='Random erase mode (default: \"pixel\")')\n    parser.add_argument('--recount', type=int, default=1,\n                        help='Random erase count (default: 1)')\n    parser.add_argument('--resplit', action='store_true', default=False,\n                        help='Do not random erase first (clean) augmentation split')\n\n    # * Mixup params\n    parser.add_argument('--mixup', type=float, default=0.8,\n                        help='mixup alpha, mixup enabled if > 0. (default: 0.8)')\n    parser.add_argument('--cutmix', type=float, default=1.0,\n                        help='cutmix alpha, cutmix enabled if > 0. (default: 1.0)')\n    parser.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n    parser.add_argument('--mixup-prob', type=float, default=1.0,\n                        help='Probability of performing mixup or cutmix when either/both is enabled')\n    parser.add_argument('--mixup-switch-prob', type=float, default=0.5,\n                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n    parser.add_argument('--mixup-mode', type=str, default='batch',\n                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n\n    # Distillation parameters\n    parser.add_argument('--teacher-model', default='regnety_160', type=str, metavar='MODEL',\n                        help='Name of teacher model to train (default: \"regnety_160\"')\n    parser.add_argument('--teacher-path', type=str, default='')\n    parser.add_argument('--distillation-type', default='none', choices=['none', 'soft', 'hard'], type=str, help=\"\")\n    parser.add_argument('--distillation-alpha', default=0.5, type=float, help=\"\")\n    parser.add_argument('--distillation-tau', default=1.0, type=float, help=\"\")\n    \n    # * Cosub params\n    parser.add_argument('--cosub', action='store_true') \n    \n    # * Finetuning params\n    parser.add_argument('--finetune', default='', help='finetune from checkpoint')\n    parser.add_argument('--attn-only', action='store_true') \n    \n    # Dataset parameters\n    parser.add_argument('--data-path', default='/datasets01/imagenet_full_size/061417/', type=str,\n                        help='dataset path')\n    parser.add_argument('--data-set', default='IMNET', choices=['CIFAR', 'IMNET', 'INAT', 'INAT19'],\n                        type=str, help='Image Net dataset path')\n    parser.add_argument('--inat-category', default='name',\n                        choices=['kingdom', 'phylum', 'class', 'order', 'supercategory', 'family', 'genus', 'name'],\n                        type=str, help='semantic granularity')\n\n    parser.add_argument('--output_dir', default='',\n                        help='path where to save, empty for no saving')\n    parser.add_argument('--device', default='cuda',\n                        help='device to use for training / testing')\n    parser.add_argument('--seed', default=0, type=int)\n    parser.add_argument('--resume', default='', help='resume from checkpoint')\n    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n                        help='start epoch')\n    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument('--eval-crop-ratio', default=0.875, type=float, help=\"Crop ratio for evaluation\")\n    parser.add_argument('--dist-eval', action='store_true', default=False, help='Enabling distributed evaluation')\n    parser.add_argument('--num_workers', default=10, type=int)\n    parser.add_argument('--pin-mem', action='store_true',\n                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n    parser.add_argument('--no-pin-mem', action='store_false', dest='pin_mem',\n                        help='')\n    parser.set_defaults(pin_mem=True)\n\n    # distributed training parameters\n    parser.add_argument('--distributed', action='store_true', default=False, help='Enabling distributed training')\n    parser.add_argument('--world_size', default=1, type=int,\n                        help='number of distributed processes')\n    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n    return parser\n\n\ndef main(args):\n    utils.init_distributed_mode(args)\n\n    print(args)\n\n    if args.distillation_type != 'none' and args.finetune and not args.eval:\n        raise NotImplementedError(\"Finetuning with distillation not yet supported\")\n\n    device = torch.device(args.device)\n\n    # fix the seed for reproducibility\n    seed = args.seed + utils.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    # random.seed(seed)\n\n    cudnn.benchmark = True\n\n    dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)\n    dataset_val, _ = build_dataset(is_train=False, args=args)\n\n    if args.distributed:\n        num_tasks = utils.get_world_size()\n        global_rank = utils.get_rank()\n        if args.repeated_aug:\n            sampler_train = RASampler(\n                dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n            )\n        else:\n            sampler_train = torch.utils.data.DistributedSampler(\n                dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n            )\n        if args.dist_eval:\n            if len(dataset_val) % num_tasks != 0:\n                print('Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n                      'This will slightly alter validation results as extra duplicate entries are added to achieve '\n                      'equal num of samples per-process.')\n            sampler_val = torch.utils.data.DistributedSampler(\n                dataset_val, num_replicas=num_tasks, rank=global_rank, shuffle=False)\n        else:\n            sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n    else:\n        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train, sampler=sampler_train,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        pin_memory=args.pin_mem,\n        drop_last=True,\n    )\n    if args.ThreeAugment:\n        data_loader_train.dataset.transform = new_data_aug_generator(args)\n\n    data_loader_val = torch.utils.data.DataLoader(\n        dataset_val, sampler=sampler_val,\n        batch_size=int(1.5 * args.batch_size),\n        num_workers=args.num_workers,\n        pin_memory=args.pin_mem,\n        drop_last=False\n    )\n\n    mixup_fn = None\n    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n    if mixup_active:\n        mixup_fn = Mixup(\n            mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,\n            prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,\n            label_smoothing=args.smoothing, num_classes=args.nb_classes)\n\n    print(f\"Creating model: {args.model}\")\n    model = create_model(\n        args.model,\n        pretrained=False,\n        num_classes=args.nb_classes,\n        drop_rate=args.drop,\n        drop_path_rate=args.drop_path,\n        drop_block_rate=None,\n        img_size=args.input_size\n    )\n\n                    \n    if args.finetune:\n        if args.finetune.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(\n                args.finetune, map_location='cpu', check_hash=True)\n        else:\n            checkpoint = torch.load(args.finetune, map_location='cpu')\n\n        checkpoint_model = checkpoint['model']\n        state_dict = model.state_dict()\n        for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n                print(f\"Removing key {k} from pretrained checkpoint\")\n                del checkpoint_model[k]\n\n        # interpolate position embedding\n        pos_embed_checkpoint = checkpoint_model['pos_embed']\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.patch_embed.num_patches\n        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches ** 0.5)\n        # class_token and dist_token are kept unchanged\n        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n        # only the position tokens are interpolated\n        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n        pos_tokens = torch.nn.functional.interpolate(\n            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n        checkpoint_model['pos_embed'] = new_pos_embed\n\n        model.load_state_dict(checkpoint_model, strict=False)\n        \n    if args.attn_only:\n        for name_p,p in model.named_parameters():\n            if '.attn.' in name_p:\n                p.requires_grad = True\n            else:\n                p.requires_grad = False\n        try:\n            model.head.weight.requires_grad = True\n            model.head.bias.requires_grad = True\n        except:\n            model.fc.weight.requires_grad = True\n            model.fc.bias.requires_grad = True\n        try:\n            model.pos_embed.requires_grad = True\n        except:\n            print('no position encoding')\n        try:\n            for p in model.patch_embed.parameters():\n                p.requires_grad = False\n        except:\n            print('no patch embed')\n            \n    model.to(device)\n\n    model_ema = None\n    if args.model_ema:\n        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n        model_ema = ModelEma(\n            model,\n            decay=args.model_ema_decay,\n            device='cpu' if args.model_ema_force_cpu else '',\n            resume='')\n\n    model_without_ddp = model\n    if args.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n        model_without_ddp = model.module\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print('number of params:', n_parameters)\n    if not args.unscale_lr:\n        linear_scaled_lr = args.lr * args.batch_size * utils.get_world_size() / 512.0\n        args.lr = linear_scaled_lr\n    optimizer = create_optimizer(args, model_without_ddp)\n    loss_scaler = NativeScaler()\n\n    lr_scheduler, _ = create_scheduler(args, optimizer)\n\n    criterion = LabelSmoothingCrossEntropy()\n\n    if mixup_active:\n        # smoothing is handled with mixup label transform\n        criterion = SoftTargetCrossEntropy()\n    elif args.smoothing:\n        criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n    else:\n        criterion = torch.nn.CrossEntropyLoss()\n        \n    if args.bce_loss:\n        criterion = torch.nn.BCEWithLogitsLoss()\n        \n    teacher_model = None\n    if args.distillation_type != 'none':\n        assert args.teacher_path, 'need to specify teacher-path when using distillation'\n        print(f\"Creating teacher model: {args.teacher_model}\")\n        teacher_model = create_model(\n            args.teacher_model,\n            pretrained=False,\n            num_classes=args.nb_classes,\n            global_pool='avg',\n        )\n        if args.teacher_path.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(\n                args.teacher_path, map_location='cpu', check_hash=True)\n        else:\n            checkpoint = torch.load(args.teacher_path, map_location='cpu')\n        teacher_model.load_state_dict(checkpoint['model'])\n        teacher_model.to(device)\n        teacher_model.eval()\n\n    # wrap the criterion in our custom DistillationLoss, which\n    # just dispatches to the original criterion if args.distillation_type is 'none'\n    criterion = DistillationLoss(\n        criterion, teacher_model, args.distillation_type, args.distillation_alpha, args.distillation_tau\n    )\n\n    output_dir = Path(args.output_dir)\n    if args.resume:\n        if args.resume.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(\n                args.resume, map_location='cpu', check_hash=True)\n        else:\n            checkpoint = torch.load(args.resume, map_location='cpu')\n        model_without_ddp.load_state_dict(checkpoint['model'])\n        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n            args.start_epoch = checkpoint['epoch'] + 1\n            if args.model_ema:\n                utils._load_checkpoint_for_ema(model_ema, checkpoint['model_ema'])\n            if 'scaler' in checkpoint:\n                loss_scaler.load_state_dict(checkpoint['scaler'])\n        lr_scheduler.step(args.start_epoch)\n    if args.eval:\n        test_stats = evaluate(data_loader_val, model, device)\n        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n        return\n\n    print(f\"Start training for {args.epochs} epochs\")\n    start_time = time.time()\n    max_accuracy = 0.0\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            data_loader_train.sampler.set_epoch(epoch)\n\n        train_stats = train_one_epoch(\n            model, criterion, data_loader_train,\n            optimizer, device, epoch, loss_scaler,\n            args.clip_grad, model_ema, mixup_fn,\n            set_training_mode=args.train_mode,  # keep in eval mode for deit finetuning / train mode for training and deit III finetuning\n            args = args,\n        )\n\n        lr_scheduler.step(epoch)\n        if args.output_dir:\n            checkpoint_paths = [output_dir / 'checkpoint.pth']\n            for checkpoint_path in checkpoint_paths:\n                utils.save_on_master({\n                    'model': model_without_ddp.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'lr_scheduler': lr_scheduler.state_dict(),\n                    'epoch': epoch,\n                    'model_ema': get_state_dict(model_ema),\n                    'scaler': loss_scaler.state_dict(),\n                    'args': args,\n                }, checkpoint_path)\n             \n\n        test_stats = evaluate(data_loader_val, model, device)\n        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n        \n        if max_accuracy < test_stats[\"acc1\"]:\n            max_accuracy = test_stats[\"acc1\"]\n            if args.output_dir:\n                checkpoint_paths = [output_dir / 'best_checkpoint.pth']\n                for checkpoint_path in checkpoint_paths:\n                    utils.save_on_master({\n                        'model': model_without_ddp.state_dict(),\n                        'optimizer': optimizer.state_dict(),\n                        'lr_scheduler': lr_scheduler.state_dict(),\n                        'epoch': epoch,\n                        'model_ema': get_state_dict(model_ema),\n                        'scaler': loss_scaler.state_dict(),\n                        'args': args,\n                    }, checkpoint_path)\n            \n        print(f'Max accuracy: {max_accuracy:.2f}%')\n\n        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n                     **{f'test_{k}': v for k, v in test_stats.items()},\n                     'epoch': epoch,\n                     'n_parameters': n_parameters}\n        \n        \n        \n        \n        if args.output_dir and utils.is_main_process():\n            with (output_dir / \"log.txt\").open(\"a\") as f:\n                f.write(json.dumps(log_stats) + \"\\n\")\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Training time {}'.format(total_time_str))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('DeiT training and evaluation script', parents=[get_args_parser()])\n    args = parser.parse_args()\n    if args.output_dir:\n        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n    main(args)\n"
        },
        {
          "name": "models.py",
          "type": "blob",
          "size": 6.76953125,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nimport torch\nimport torch.nn as nn\nfrom functools import partial\n\nfrom timm.models.vision_transformer import VisionTransformer, _cfg\nfrom timm.models.registry import register_model\nfrom timm.models.layers import trunc_normal_\n\n\n__all__ = [\n    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n    'deit_base_distilled_patch16_384',\n]\n\n\nclass DistilledVisionTransformer(VisionTransformer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        num_patches = self.patch_embed.num_patches\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.dist_token, std=.02)\n        trunc_normal_(self.pos_embed, std=.02)\n        self.head_dist.apply(self._init_weights)\n\n    def forward_features(self, x):\n        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n        # with slight modifications to add the dist_token\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        dist_token = self.dist_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.norm(x)\n        return x[:, 0], x[:, 1]\n\n    def forward(self, x):\n        x, x_dist = self.forward_features(x)\n        x = self.head(x)\n        x_dist = self.head_dist(x_dist)\n        if self.training:\n            return x, x_dist\n        else:\n            # during inference, return the average of both classifier predictions\n            return (x + x_dist) / 2\n\n\n@register_model\ndef deit_tiny_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n\n@register_model\ndef deit_small_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n\n@register_model\ndef deit_base_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n\n@register_model\ndef deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):\n    model = DistilledVisionTransformer(\n        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n\n@register_model\ndef deit_small_distilled_patch16_224(pretrained=False, **kwargs):\n    model = DistilledVisionTransformer(\n        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n\n@register_model\ndef deit_base_distilled_patch16_224(pretrained=False, **kwargs):\n    model = DistilledVisionTransformer(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n\n@register_model\ndef deit_base_patch16_384(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n\n\n@register_model\ndef deit_base_distilled_patch16_384(pretrained=False, **kwargs):\n    model = DistilledVisionTransformer(\n        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n"
        },
        {
          "name": "models_v2.py",
          "type": "blob",
          "size": 21.6611328125,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\nimport torch\nimport torch.nn as nn\nfrom functools import partial\n\nfrom timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\n\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\nclass Attention(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        q = q * self.scale\n\n        attn = (q @ k.transpose(-2, -1))\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n    \nclass Block(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention,Mlp_block=Mlp\n                 ,init_values=1e-4):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x \n    \nclass Layer_scale_init_Block(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention,Mlp_block=Mlp\n                 ,init_values=1e-4):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\nclass Layer_scale_init_Block_paralx2(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention,Mlp_block=Mlp\n                 ,init_values=1e-4):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.norm11 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.attn1 = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.norm21 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.mlp1 = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_1_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_2_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        \n    def forward(self, x):\n        x = x + self.drop_path(self.gamma_1*self.attn(self.norm1(x))) + self.drop_path(self.gamma_1_1 * self.attn1(self.norm11(x)))\n        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x))) + self.drop_path(self.gamma_2_1 * self.mlp1(self.norm21(x)))\n        return x\n        \nclass Block_paralx2(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention,Mlp_block=Mlp\n                 ,init_values=1e-4):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.norm11 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.attn1 = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.norm21 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.mlp1 = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x))) + self.drop_path(self.attn1(self.norm11(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x))) + self.drop_path(self.mlp1(self.norm21(x)))\n        return x\n        \n        \nclass hMLP_stem(nn.Module):\n    \"\"\" hMLP_stem: https://arxiv.org/pdf/2203.09795.pdf\n    taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    with slight modifications\n    \"\"\"\n    def __init__(self, img_size=224,  patch_size=16, in_chans=3, embed_dim=768,norm_layer=nn.SyncBatchNorm):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        self.proj = torch.nn.Sequential(*[nn.Conv2d(in_chans, embed_dim//4, kernel_size=4, stride=4),\n                                          norm_layer(embed_dim//4),\n                                          nn.GELU(),\n                                          nn.Conv2d(embed_dim//4, embed_dim//4, kernel_size=2, stride=2),\n                                          norm_layer(embed_dim//4),\n                                          nn.GELU(),\n                                          nn.Conv2d(embed_dim//4, embed_dim, kernel_size=2, stride=2),\n                                          norm_layer(embed_dim),\n                                         ])\n        \n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n    \nclass vit_models(nn.Module):\n    \"\"\" Vision Transformer with LayerScale (https://arxiv.org/abs/2103.17239) support\n    taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    with slight modifications\n    \"\"\"\n    def __init__(self, img_size=224,  patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, global_pool=None,\n                 block_layers = Block,\n                 Patch_layer=PatchEmbed,act_layer=nn.GELU,\n                 Attention_block = Attention, Mlp_block=Mlp,\n                dpr_constant=True,init_scale=1e-4,\n                mlp_ratio_clstk = 4.0,**kwargs):\n        super().__init__()\n        \n        self.dropout_rate = drop_rate\n\n            \n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim\n\n        self.patch_embed = Patch_layer(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n\n        dpr = [drop_path_rate for i in range(depth)]\n        self.blocks = nn.ModuleList([\n            block_layers(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=0.0, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                act_layer=act_layer,Attention_block=Attention_block,Mlp_block=Mlp_block,init_values=init_scale)\n            for i in range(depth)])\n        \n\n        \n            \n        self.norm = norm_layer(embed_dim)\n\n        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def get_classifier(self):\n        return self.head\n    \n    def get_num_layers(self):\n        return len(self.blocks)\n    \n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        \n        x = x + self.pos_embed\n        \n        x = torch.cat((cls_tokens, x), dim=1)\n            \n        for i , blk in enumerate(self.blocks):\n            x = blk(x)\n            \n        x = self.norm(x)\n        return x[:, 0]\n\n    def forward(self, x):\n\n        x = self.forward_features(x)\n        \n        if self.dropout_rate:\n            x = F.dropout(x, p=float(self.dropout_rate), training=self.training)\n        x = self.head(x)\n        \n        return x\n\n# DeiT III: Revenge of the ViT (https://arxiv.org/abs/2204.07118)\n\n@register_model\ndef deit_tiny_patch16_LS(pretrained=False, img_size=224, pretrained_21k = False,   **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Layer_scale_init_Block, **kwargs)\n    \n    return model\n    \n    \n@register_model\ndef deit_small_patch16_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Layer_scale_init_Block, **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        name = 'https://dl.fbaipublicfiles.com/deit/deit_3_small_'+str(img_size)+'_'\n        if pretrained_21k:\n            name+='21k.pth'\n        else:\n            name+='1k.pth'\n            \n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=name,\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n\n    return model\n\n@register_model\ndef deit_medium_patch16_LS(pretrained=False, img_size=224, pretrained_21k = False, **kwargs):\n    model = vit_models(\n        patch_size=16, embed_dim=512, depth=12, num_heads=8, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers = Layer_scale_init_Block, **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        name = 'https://dl.fbaipublicfiles.com/deit/deit_3_medium_'+str(img_size)+'_'\n        if pretrained_21k:\n            name+='21k.pth'\n        else:\n            name+='1k.pth'\n            \n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=name,\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model \n\n@register_model\ndef deit_base_patch16_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Layer_scale_init_Block, **kwargs)\n    if pretrained:\n        name = 'https://dl.fbaipublicfiles.com/deit/deit_3_base_'+str(img_size)+'_'\n        if pretrained_21k:\n            name+='21k.pth'\n        else:\n            name+='1k.pth'\n            \n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=name,\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n    \n@register_model\ndef deit_large_patch16_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Layer_scale_init_Block, **kwargs)\n    if pretrained:\n        name = 'https://dl.fbaipublicfiles.com/deit/deit_3_large_'+str(img_size)+'_'\n        if pretrained_21k:\n            name+='21k.pth'\n        else:\n            name+='1k.pth'\n            \n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=name,\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n    \n@register_model\ndef deit_huge_patch14_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=14, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers = Layer_scale_init_Block, **kwargs)\n    if pretrained:\n        name = 'https://dl.fbaipublicfiles.com/deit/deit_3_huge_'+str(img_size)+'_'\n        if pretrained_21k:\n            name+='21k_v1.pth'\n        else:\n            name+='1k_v1.pth'\n            \n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=name,\n            map_location=\"cpu\", check_hash=True\n        )\n        model.load_state_dict(checkpoint[\"model\"])\n    return model\n    \n@register_model\ndef deit_huge_patch14_52_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=14, embed_dim=1280, depth=52, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers = Layer_scale_init_Block, **kwargs)\n\n    return model\n    \n@register_model\ndef deit_huge_patch14_26x2_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=14, embed_dim=1280, depth=26, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers = Layer_scale_init_Block_paralx2, **kwargs)\n\n    return model\n    \n@register_model\ndef deit_Giant_48x2_patch14_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=14, embed_dim=1664, depth=48, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers = Block_paral_LS, **kwargs)\n\n    return model\n\n@register_model\ndef deit_giant_40x2_patch14_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers = Block_paral_LS, **kwargs)\n    return model\n\n@register_model\ndef deit_Giant_48_patch14_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=14, embed_dim=1664, depth=48, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers = Layer_scale_init_Block, **kwargs)\n    return model\n\n@register_model\ndef deit_giant_40_patch14_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers = Layer_scale_init_Block, **kwargs)\n    #model.default_cfg = _cfg()\n\n    return model\n\n# Models from Three things everyone should know about Vision Transformers (https://arxiv.org/pdf/2203.09795.pdf)\n\n@register_model\ndef deit_small_patch16_36_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=384, depth=36, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Layer_scale_init_Block, **kwargs)\n\n    return model\n    \n@register_model\ndef deit_small_patch16_36(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=384, depth=36, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\n    return model\n    \n@register_model\ndef deit_small_patch16_18x2_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=384, depth=18, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Layer_scale_init_Block_paralx2, **kwargs)\n\n    return model\n    \n@register_model\ndef deit_small_patch16_18x2(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=384, depth=18, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Block_paralx2, **kwargs)\n\n    return model\n    \n  \n@register_model\ndef deit_base_patch16_18x2_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=768, depth=18, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Layer_scale_init_Block_paralx2, **kwargs)\n\n    return model\n\n\n@register_model\ndef deit_base_patch16_18x2(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=768, depth=18, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Block_paralx2, **kwargs)\n\n    return model\n    \n\n@register_model\ndef deit_base_patch16_36x1_LS(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=768, depth=36, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),block_layers=Layer_scale_init_Block, **kwargs)\n\n    return model\n\n@register_model\ndef deit_base_patch16_36x1(pretrained=False, img_size=224, pretrained_21k = False,  **kwargs):\n    model = vit_models(\n        img_size = img_size, patch_size=16, embed_dim=768, depth=36, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\n    return model\n\n"
        },
        {
          "name": "patchconvnet_models.py",
          "type": "blob",
          "size": 16.5595703125,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\n\nfrom functools import partial\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.efficientnet_blocks import SqueezeExcite\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\n__all__ = ['S60', 'S120', 'B60', 'B120', 'L60', 'L120', 'S60_multi']\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: nn.Module = nn.GELU,\n        drop: float = 0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Learned_Aggregation_Layer(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 1,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim: int = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.id = nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, N, C = x.shape\n        q = self.q(x[:, 0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        q = q * self.scale\n        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        attn = q @ k.transpose(-2, -1)\n        attn = self.id(attn)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x_cls = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n        x_cls = self.proj(x_cls)\n        x_cls = self.proj_drop(x_cls)\n\n        return x_cls\n\n\nclass Learned_Aggregation_Layer_multi(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        num_classes: int = 1000,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim: int = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.num_classes = num_classes\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, N, C = x.shape\n        q = (\n            self.q(x[:, : self.num_classes])\n            .reshape(B, self.num_classes, self.num_heads, C // self.num_heads)\n            .permute(0, 2, 1, 3)\n        )\n        k = (\n            self.k(x[:, self.num_classes:])\n            .reshape(B, N - self.num_classes, self.num_heads, C // self.num_heads)\n            .permute(0, 2, 1, 3)\n        )\n\n        q = q * self.scale\n        v = (\n            self.v(x[:, self.num_classes:])\n            .reshape(B, N - self.num_classes, self.num_heads, C // self.num_heads)\n            .permute(0, 2, 1, 3)\n        )\n\n        attn = q @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x_cls = (attn @ v).transpose(1, 2).reshape(B, self.num_classes, C)\n        x_cls = self.proj(x_cls)\n        x_cls = self.proj_drop(x_cls)\n\n        return x_cls\n\n\nclass Layer_scale_init_Block_only_token(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: nn.Module = nn.GELU,\n        norm_layer=nn.LayerNorm,\n        Attention_block=Learned_Aggregation_Layer,\n        Mlp_block=Mlp,\n        init_values: float = 1e-4,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n\n    def forward(self, x: torch.Tensor, x_cls: torch.Tensor) -> torch.Tensor:\n        u = torch.cat((x_cls, x), dim=1)\n        x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))\n        x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))\n        return x_cls\n\n\nclass Conv_blocks_se(nn.Module):\n    def __init__(self, dim: int):\n        super().__init__()\n\n        self.qkv_pos = nn.Sequential(\n            nn.Conv2d(dim, dim, kernel_size=1),\n            nn.GELU(),\n            nn.Conv2d(dim, dim, groups=dim, kernel_size=3, padding=1, stride=1, bias=True),\n            nn.GELU(),\n            SqueezeExcite(dim, rd_ratio=0.25),\n            nn.Conv2d(dim, dim, kernel_size=1),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, N, C = x.shape\n        H = W = int(N ** 0.5)\n        x = x.transpose(-1, -2)\n        x = x.reshape(B, C, H, W)\n        x = self.qkv_pos(x)\n        x = x.reshape(B, C, N)\n        x = x.transpose(-1, -2)\n        return x\n\n\nclass Layer_scale_init_Block(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        drop_path: float = 0.0,\n        act_layer: nn.Module = nn.GELU,\n        norm_layer=nn.LayerNorm,\n        Attention_block=None,\n        init_values: float = 1e-4,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention_block(dim)\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n\n\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> nn.Sequential:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n    )\n\n\nclass ConvStem(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size: int = 224, patch_size: int = 16, in_chans: int = 3, embed_dim: int = 768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Sequential(\n            conv3x3(in_chans, embed_dim // 8, 2),\n            nn.GELU(),\n            conv3x3(embed_dim // 8, embed_dim // 4, 2),\n            nn.GELU(),\n            conv3x3(embed_dim // 4, embed_dim // 2, 2),\n            nn.GELU(),\n            conv3x3(embed_dim // 2, embed_dim, 2),\n        )\n\n    def forward(self, x: torch.Tensor, padding_size: Optional[int] = None) -> torch.Tensor:\n        B, C, H, W = x.shape\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass PatchConvnet(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 224,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        num_classes: int = 1000,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 1,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        hybrid_backbone: Optional = None,\n        norm_layer=nn.LayerNorm,\n        global_pool: Optional[str] = None,\n        block_layers=Layer_scale_init_Block,\n        block_layers_token=Layer_scale_init_Block_only_token,\n        Patch_layer=ConvStem,\n        act_layer: nn.Module = nn.GELU,\n        Attention_block=Conv_blocks_se,\n        dpr_constant: bool = True,\n        init_scale: float = 1e-4,\n        Attention_block_token_only=Learned_Aggregation_Layer,\n        Mlp_block_token_only=Mlp,\n        depth_token_only: int = 1,\n        mlp_ratio_clstk: float = 3.0,\n        multiclass: bool = False,\n    ):\n        super().__init__()\n\n        self.multiclass = multiclass\n        self.patch_size = patch_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = Patch_layer(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim\n        )\n\n        if not self.multiclass:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, int(embed_dim)))\n        else:\n            self.cls_token = nn.Parameter(torch.zeros(1, num_classes, int(embed_dim)))\n\n        if not dpr_constant:\n            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        else:\n            dpr = [drop_path_rate for i in range(depth)]\n\n        self.blocks = nn.ModuleList(\n            [\n                block_layers(\n                    dim=embed_dim,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    act_layer=act_layer,\n                    Attention_block=Attention_block,\n                    init_values=init_scale,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.blocks_token_only = nn.ModuleList(\n            [\n                block_layers_token(\n                    dim=int(embed_dim),\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio_clstk,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=0.0,\n                    norm_layer=norm_layer,\n                    act_layer=act_layer,\n                    Attention_block=Attention_block_token_only,\n                    Mlp_block=Mlp_block_token_only,\n                    init_values=init_scale,\n                )\n                for i in range(depth_token_only)\n            ]\n        )\n\n        self.norm = norm_layer(int(embed_dim))\n\n        self.total_len = depth_token_only + depth\n\n        self.feature_info = [dict(num_chs=int(embed_dim), reduction=0, module='head')]\n        if not self.multiclass:\n            self.head = nn.Linear(int(embed_dim), num_classes) if num_classes > 0 else nn.Identity()\n        else:\n            self.head = nn.ModuleList([nn.Linear(int(embed_dim), 1) for _ in range(num_classes)])\n\n        self.rescale: float = 0.02\n\n        trunc_normal_(self.cls_token, std=self.rescale)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=self.rescale)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'cls_token'}\n\n    def get_classifier(self):\n        return self.head\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def reset_classifier(self, num_classes: int, global_pool: str = ''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n\n        for i, blk in enumerate(self.blocks_token_only):\n            cls_tokens = blk(x, cls_tokens)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        x = self.norm(x)\n\n        if not self.multiclass:\n            return x[:, 0]\n        else:\n            return x[:, : self.num_classes].reshape(B, self.num_classes, -1)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B = x.shape[0]\n        x = self.forward_features(x)\n        if not self.multiclass:\n            x = self.head(x)\n            return x\n        else:\n            all_results = []\n            for i in range(self.num_classes):\n                all_results.append(self.head[i](x[:, i]))\n            return torch.cat(all_results, dim=1).reshape(B, self.num_classes)\n\n\n@register_model\ndef S60(pretrained: bool = False, **kwargs):\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=384,\n        depth=60,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        depth_token_only=1,\n        mlp_ratio_clstk=3.0,\n        **kwargs\n    )\n\n    return model\n\n\n@register_model\ndef S120(pretrained: bool = False, **kwargs):\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=384,\n        depth=120,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        init_scale=1e-6,\n        mlp_ratio_clstk=3.0,\n        **kwargs\n    )\n\n    return model\n\n\n@register_model\ndef B60(pretrained: bool = False, **kwargs):\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=768,\n        depth=60,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Attention_block=Conv_blocks_se,\n        init_scale=1e-6,\n        **kwargs\n    )\n\n    return model\n\n\n@register_model\ndef B120(pretrained: bool = False, **kwargs):\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=768,\n        depth=120,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        init_scale=1e-6,\n        **kwargs\n    )\n\n    return model\n\n\n@register_model\ndef L60(pretrained: bool = False, **kwargs):\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=1024,\n        depth=60,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        init_scale=1e-6,\n        mlp_ratio_clstk=3.0,\n        **kwargs\n    )\n\n    return model\n\n\n@register_model\ndef L120(pretrained: bool = False, **kwargs):\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=1024,\n        depth=120,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        init_scale=1e-6,\n        mlp_ratio_clstk=3.0,\n        **kwargs\n    )\n\n    return model\n\n\n@register_model\ndef S60_multi(pretrained: bool = False, **kwargs):\n    model = PatchConvnet(\n        patch_size=16,\n        embed_dim=384,\n        depth=60,\n        num_heads=1,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        Patch_layer=ConvStem,\n        Attention_block=Conv_blocks_se,\n        Attention_block_token_only=Learned_Aggregation_Layer_multi,\n        depth_token_only=1,\n        mlp_ratio_clstk=3.0,\n        multiclass=True,\n        **kwargs\n    )\n\n    return model\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0439453125,
          "content": "torch==1.13.1\ntorchvision==0.8.1\ntimm==0.3.2\n"
        },
        {
          "name": "resmlp_models.py",
          "type": "blob",
          "size": 6.390625,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nimport torch\nimport torch.nn as nn\nfrom functools import partial\n\nfrom timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\nfrom timm.models.registry import register_model\nfrom timm.models.layers import trunc_normal_,  DropPath\n\n\n__all__ = [\n    'resmlp_12', 'resmlp_24', 'resmlp_36', 'resmlpB_24'\n]\n\nclass Affine(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.ones(dim))\n        self.beta = nn.Parameter(torch.zeros(dim))\n\n    def forward(self, x):\n        return self.alpha * x + self.beta    \n    \nclass layers_scale_mlp_blocks(nn.Module):\n\n    def __init__(self, dim, drop=0., drop_path=0., act_layer=nn.GELU,init_values=1e-4,num_patches = 196):\n        super().__init__()\n        self.norm1 = Affine(dim)\n        self.attn = nn.Linear(num_patches, num_patches)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = Affine(dim)\n        self.mlp = Mlp(in_features=dim, hidden_features=int(4.0 * dim), act_layer=act_layer, drop=drop)\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x).transpose(1,2)).transpose(1,2))\n        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x \n\n\nclass resmlp_models(nn.Module):\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,drop_rate=0.,\n                 Patch_layer=PatchEmbed,act_layer=nn.GELU,\n                drop_path_rate=0.0,init_scale=1e-4):\n        super().__init__()\n\n\n\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  \n\n        self.patch_embed = Patch_layer(\n                img_size=img_size, patch_size=patch_size, in_chans=int(in_chans), embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        dpr = [drop_path_rate for i in range(depth)]\n\n        self.blocks = nn.ModuleList([\n            layers_scale_mlp_blocks(\n                dim=embed_dim,drop=drop_rate,drop_path=dpr[i],\n                act_layer=act_layer,init_values=init_scale,\n                num_patches=num_patches)\n            for i in range(depth)])\n\n\n        self.norm = Affine(embed_dim)\n\n\n\n        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n\n        x = self.patch_embed(x)\n\n        for i , blk in enumerate(self.blocks):\n            x  = blk(x)\n\n        x = self.norm(x)\n        x = x.mean(dim=1).reshape(B,1,-1)\n\n        return x[:, 0]\n\n    def forward(self, x):\n        x  = self.forward_features(x)\n        x = self.head(x)\n        return x \n\n@register_model\ndef resmlp_12(pretrained=False,dist=False, **kwargs):\n    model = resmlp_models(\n        patch_size=16, embed_dim=384, depth=12,\n        Patch_layer=PatchEmbed,\n        init_scale=0.1,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        if dist:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlp_12_dist.pth\"\n        else:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlp_12_no_dist.pth\"\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=url_path,\n            map_location=\"cpu\", check_hash=True\n        )\n            \n        model.load_state_dict(checkpoint)\n    return model\n  \n@register_model\ndef resmlp_24(pretrained=False,dist=False,dino=False, **kwargs):\n    model = resmlp_models(\n        patch_size=16, embed_dim=384, depth=24,\n        Patch_layer=PatchEmbed,\n        init_scale=1e-5,**kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        if dist:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlp_24_dist.pth\"\n        elif dino:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlp_24_dino.pth\"\n        else:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlp_24_no_dist.pth\"\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=url_path,\n            map_location=\"cpu\", check_hash=True\n        )\n            \n        model.load_state_dict(checkpoint)\n    return model\n  \n@register_model\ndef resmlp_36(pretrained=False,dist=False, **kwargs):\n    model = resmlp_models(\n        patch_size=16, embed_dim=384, depth=36,\n        Patch_layer=PatchEmbed,\n        init_scale=1e-6,**kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        if dist:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlp_36_dist.pth\"\n        else:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlp_36_no_dist.pth\"\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=url_path,\n            map_location=\"cpu\", check_hash=True\n        )\n            \n        model.load_state_dict(checkpoint)\n    return model\n\n@register_model\ndef resmlpB_24(pretrained=False,dist=False, in_22k = False, **kwargs):\n    model = resmlp_models(\n        patch_size=8, embed_dim=768, depth=24,\n        Patch_layer=PatchEmbed,\n        init_scale=1e-6,**kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        if dist:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlpB_24_dist.pth\"\n        elif in_22k:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlpB_24_22k.pth\"\n        else:\n          url_path = \"https://dl.fbaipublicfiles.com/deit/resmlpB_24_no_dist.pth\"\n            \n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=url_path,\n            map_location=\"cpu\", check_hash=True\n        )\n            \n        model.load_state_dict(checkpoint)\n    \n    return model\n"
        },
        {
          "name": "run_with_submitit.py",
          "type": "blob",
          "size": 3.9794921875,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\"\"\"\nA script to run multinode training with submitit.\n\"\"\"\nimport argparse\nimport os\nimport uuid\nfrom pathlib import Path\n\nimport main as classification\nimport submitit\n\n\ndef parse_args():\n    classification_parser = classification.get_args_parser()\n    parser = argparse.ArgumentParser(\"Submitit for DeiT\", parents=[classification_parser])\n    parser.add_argument(\"--ngpus\", default=8, type=int, help=\"Number of gpus to request on each node\")\n    parser.add_argument(\"--nodes\", default=2, type=int, help=\"Number of nodes to request\")\n    parser.add_argument(\"--timeout\", default=2800, type=int, help=\"Duration of the job\")\n    parser.add_argument(\"--job_dir\", default=\"\", type=str, help=\"Job dir. Leave empty for automatic.\")\n\n    parser.add_argument(\"--partition\", default=\"learnfair\", type=str, help=\"Partition where to submit\")\n    parser.add_argument(\"--use_volta32\", action='store_true', help=\"Big models? Use this\")\n    parser.add_argument('--comment', default=\"\", type=str,\n                        help='Comment to pass to scheduler, e.g. priority message')\n    return parser.parse_args()\n\n\ndef get_shared_folder() -> Path:\n    user = os.getenv(\"USER\")\n    if Path(\"/checkpoint/\").is_dir():\n        p = Path(f\"/checkpoint/{user}/experiments\")\n        p.mkdir(exist_ok=True)\n        return p\n    raise RuntimeError(\"No shared folder available\")\n\n\ndef get_init_file():\n    # Init file must not exist, but it's parent dir must exist.\n    os.makedirs(str(get_shared_folder()), exist_ok=True)\n    init_file = get_shared_folder() / f\"{uuid.uuid4().hex}_init\"\n    if init_file.exists():\n        os.remove(str(init_file))\n    return init_file\n\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n\n    def __call__(self):\n        import main as classification\n\n        self._setup_gpu_args()\n        classification.main(self.args)\n\n    def checkpoint(self):\n        import os\n        import submitit\n\n        self.args.dist_url = get_init_file().as_uri()\n        checkpoint_file = os.path.join(self.args.output_dir, \"checkpoint.pth\")\n        if os.path.exists(checkpoint_file):\n            self.args.resume = checkpoint_file\n        print(\"Requeuing \", self.args)\n        empty_trainer = type(self)(self.args)\n        return submitit.helpers.DelayedSubmission(empty_trainer)\n\n    def _setup_gpu_args(self):\n        import submitit\n        from pathlib import Path\n\n        job_env = submitit.JobEnvironment()\n        self.args.output_dir = Path(str(self.args.output_dir).replace(\"%j\", str(job_env.job_id)))\n        self.args.gpu = job_env.local_rank\n        self.args.rank = job_env.global_rank\n        self.args.world_size = job_env.num_tasks\n        print(f\"Process group: {job_env.num_tasks} tasks, rank: {job_env.global_rank}\")\n\n\ndef main():\n    args = parse_args()\n    if args.job_dir == \"\":\n        args.job_dir = get_shared_folder() / \"%j\"\n\n    # Note that the folder will depend on the job_id, to easily track experiments\n    executor = submitit.AutoExecutor(folder=args.job_dir, slurm_max_num_timeout=30)\n\n    num_gpus_per_node = args.ngpus\n    nodes = args.nodes\n    timeout_min = args.timeout\n\n    partition = args.partition\n    kwargs = {}\n    if args.use_volta32:\n        kwargs['slurm_constraint'] = 'volta32gb'\n    if args.comment:\n        kwargs['slurm_comment'] = args.comment\n\n    executor.update_parameters(\n        mem_gb=40 * num_gpus_per_node,\n        gpus_per_node=num_gpus_per_node,\n        tasks_per_node=num_gpus_per_node,  # one task per GPU\n        cpus_per_task=10,\n        nodes=nodes,\n        timeout_min=timeout_min,  # max is 60 * 72\n        # Below are cluster dependent parameters\n        slurm_partition=partition,\n        slurm_signal_delay_s=120,\n        **kwargs\n    )\n\n    executor.update_parameters(name=\"deit\")\n\n    args.dist_url = get_init_file().as_uri()\n    args.output_dir = args.job_dir\n\n    trainer = Trainer(args)\n    job = executor.submit(trainer)\n\n    print(\"Submitted job_id:\", job.job_id)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "samplers.py",
          "type": "blob",
          "size": 2.5234375,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nimport torch\nimport torch.distributed as dist\nimport math\n\n\nclass RASampler(torch.utils.data.Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset for distributed,\n    with repeated augmentation.\n    It ensures that different each augmented version of a sample will be visible to a\n    different process (GPU)\n    Heavily based on torch.utils.data.DistributedSampler\n    \"\"\"\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True, num_repeats: int = 3):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if num_repeats < 1:\n            raise ValueError(\"num_repeats should be greater than 0\")\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.num_repeats = num_repeats\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * self.num_repeats / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        # self.num_selected_samples = int(math.ceil(len(self.dataset) / self.num_replicas))\n        self.num_selected_samples = int(math.floor(len(self.dataset) // 256 * 256 / self.num_replicas))\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g)\n        else:\n            indices = torch.arange(start=0, end=len(self.dataset))\n\n        # add extra samples to make it evenly divisible\n        indices = torch.repeat_interleave(indices, repeats=self.num_repeats, dim=0).tolist()\n        padding_size: int = self.total_size - len(indices)\n        if padding_size > 0:\n            indices += indices[:padding_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices[:self.num_selected_samples])\n\n    def __len__(self):\n        return self.num_selected_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n"
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.0634765625,
          "content": "[flake8]\nmax-line-length = 120\nignore = F401,E402,F403,W503,W504\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 6.919921875,
          "content": "# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\"\"\"\nMisc functions, including distributed helpers.\n\nMostly copy-paste from torchvision references.\n\"\"\"\nimport io\nimport os\nimport time\nfrom collections import defaultdict, deque\nimport datetime\n\nimport torch\nimport torch.distributed as dist\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)\n\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                \"{}: {}\".format(name, str(meter))\n            )\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = ''\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.4f}')\n        data_time = SmoothedValue(fmt='{avg:.4f}')\n        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n        log_msg = [\n            header,\n            '[{0' + space_fmt + '}/{1}]',\n            'eta: {eta}',\n            '{meters}',\n            'time: {time}',\n            'data: {data}'\n        ]\n        if torch.cuda.is_available():\n            log_msg.append('max mem: {memory:.0f}')\n        log_msg = self.delimiter.join(log_msg)\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time),\n                        memory=torch.cuda.max_memory_allocated() / MB))\n                else:\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time)))\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print('{} Total time: {} ({:.4f} s / it)'.format(\n            header, total_time_str, total_time / len(iterable)))\n\n\ndef _load_checkpoint_for_ema(model_ema, checkpoint):\n    \"\"\"\n    Workaround for ModelEma._load_checkpoint to accept an already-loaded object\n    \"\"\"\n    mem_file = io.BytesIO()\n    torch.save({'state_dict_ema':checkpoint}, mem_file)\n    mem_file.seek(0)\n    model_ema._load_checkpoint(mem_file)\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\n\n\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    elif 'SLURM_PROCID' in os.environ:\n        args.rank = int(os.environ['SLURM_PROCID'])\n        args.gpu = args.rank % torch.cuda.device_count()\n    else:\n        print('Not using distributed mode')\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = 'nccl'\n    print('| distributed init (rank {}): {}'.format(\n        args.rank, args.dist_url), flush=True)\n    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                         world_size=args.world_size, rank=args.rank)\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)\n"
        }
      ]
    }
  ]
}