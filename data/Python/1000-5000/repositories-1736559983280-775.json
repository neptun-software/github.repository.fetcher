{
  "metadata": {
    "timestamp": 1736559983280,
    "page": 775,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "williamleif/GraphSAGE",
      "stars": 3455,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.02734375,
          "content": ".git\nDockerfile*\n.gitignore\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.142578125,
          "content": "# Custom\n*.idea\n*.png\n*.pdf\ntmp/\n*.txt\n*swp*\n*.sw?\ngcn_back\n.DS_STORE\n*.aux\n*.log\n*.out\n*.bbl\n*.synctex.gz\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n\n*.pickle\n*.pkl\n\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.1015625,
          "content": "FROM tensorflow/tensorflow:1.3.0\n\nRUN pip install networkx==1.11\nRUN rm /notebooks/*\n\nCOPY . /notebooks\n"
        },
        {
          "name": "Dockerfile.gpu",
          "type": "blob",
          "size": 0.10546875,
          "content": "FROM tensorflow/tensorflow:1.3.0-gpu\n\nRUN pip install networkx==1.11\nRUN rm /notebooks/*\n\nCOPY . /notebooks\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 2.216796875,
          "content": "The MIT License\n\nCopyright (c) 2017 William L. Hamilton, Rex Ying\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\nPortions of this code base were orginally forked from: https://github.com/tkipf/gcn, which is under the following License:\n\nCopyright (c) 2016 Thomas Kipf\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.720703125,
          "content": "## GraphSage: Representation Learning on Large Graphs\n\n#### Authors: [William L. Hamilton](http://stanford.edu/~wleif) (wleif@stanford.edu), [Rex Ying](http://joy-of-thinking.weebly.com/) (rexying@stanford.edu)\n#### [Project Website](http://snap.stanford.edu/graphsage/)\n\n#### [Alternative reference PyTorch implementation](https://github.com/williamleif/graphsage-simple/)\n\n### Overview\n\nThis directory contains code necessary to run the GraphSage algorithm.\nGraphSage can be viewed as a stochastic generalization of graph convolutions, and it is especially useful for massive, dynamic graphs that contain rich feature information.\nSee our [paper](https://arxiv.org/pdf/1706.02216.pdf) for details on the algorithm.\n\n*Note:* GraphSage now also has better support for training on smaller, static graphs and graphs that don't have node features.\nThe original algorithm and paper are focused on the task of inductive generalization (i.e., generating embeddings for nodes that were not present during training),\nbut many benchmarks/tasks use simple static graphs that do not necessarily have features.\nTo support this use case, GraphSage now includes optional \"identity features\" that can be used with or without other node attributes.\nIncluding identity features will increase the runtime, but also potentially increase performance (at the usual risk of overfitting).\nSee the section on \"Running the code\" below.\n\n*Note:* GraphSage is intended for use on large graphs (>100,000) nodes. The overhead of subsampling will start to outweigh its benefits on smaller graphs. \n\nThe example_data subdirectory contains a small example of the protein-protein interaction data,\nwhich includes 3 training graphs + one validation graph and one test graph.\nThe full Reddit and PPI datasets (described in the paper) are available on the [project website](http://snap.stanford.edu/graphsage/).\n\nIf you make use of this code or the GraphSage algorithm in your work, please cite the following paper:\n\n     @inproceedings{hamilton2017inductive,\n\t     author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},\n\t     title = {Inductive Representation Learning on Large Graphs},\n\t     booktitle = {NIPS},\n\t     year = {2017}\n\t   }\n\n### Requirements\n\nRecent versions of TensorFlow, numpy, scipy, sklearn, and networkx are required (but networkx must be <=1.11). You can install all the required packages using the following command:\n\n\t$ pip install -r requirements.txt\n\nTo guarantee that you have the right package versions, you can use [docker](https://docs.docker.com/) to easily set up a virtual environment. See the Docker subsection below for more info.\n\n#### Docker\n\nIf you do not have [docker](https://docs.docker.com/) installed, you will need to do so. (Just click on the preceding link, the installation is pretty painless).  \n\nYou can run GraphSage inside a [docker](https://docs.docker.com/) image. After cloning the project, build and run the image as following:\n\n\t$ docker build -t graphsage .\n\t$ docker run -it graphsage bash\n\nor start a Jupyter Notebook instead of bash:\n\n\t$ docker run -it -p 8888:8888 graphsage\n\nYou can also run the GPU image using [nvidia-docker](https://github.com/NVIDIA/nvidia-docker):\n\n\t$ docker build -t graphsage:gpu -f Dockerfile.gpu .\n\t$ nvidia-docker run -it graphsage:gpu bash\t\n\n### Running the code\n\nThe example_unsupervised.sh and example_supervised.sh files contain example usages of the code, which use the unsupervised and supervised variants of GraphSage, respectively.\n\nIf your benchmark/task does not require generalizing to unseen data, we recommend you try setting the \"--identity_dim\" flag to a value in the range [64,256].\nThis flag will make the model embed unique node ids as attributes, which will increase the runtime and number of parameters but also potentially increase the performance.\nNote that you should set this flag and *not* try to pass dense one-hot vectors as features (due to sparsity).\nThe \"dimension\" of identity features specifies how many parameters there are per node in the sparse identity-feature lookup table.\n\nNote that example_unsupervised.sh sets a very small max iteration number, which can be increased to improve performance.\nWe generally found that performance continued to improve even after the loss was very near convergence (i.e., even when the loss was decreasing at a very slow rate).\n\n*Note:* For the PPI data, and any other multi-ouput dataset that allows individual nodes to belong to multiple classes, it is necessary to set the `--sigmoid` flag during supervised training. By default the model assumes that the dataset is in the \"one-hot\" categorical setting.\n\n\n#### Input format\nAs input, at minimum the code requires that a --train_prefix option is specified which specifies the following data files:\n\n* <train_prefix>-G.json -- A networkx-specified json file describing the input graph. Nodes have 'val' and 'test' attributes specifying if they are a part of the validation and test sets, respectively.\n* <train_prefix>-id_map.json -- A json-stored dictionary mapping the graph node ids to consecutive integers.\n* <train_prefix>-class_map.json -- A json-stored dictionary mapping the graph node ids to classes.\n* <train_prefix>-feats.npy [optional] --- A numpy-stored array of node features; ordering given by id_map.json. Can be omitted and only identity features will be used.\n* <train_prefix>-walks.txt [optional] --- A text file specifying random walk co-occurrences (one pair per line) (*only for unsupervised version of graphsage)\n\nTo run the model on a new dataset, you need to make data files in the format described above.\nTo run random walks for the unsupervised model and to generate the <prefix>-walks.txt file)\nyou can use the `run_walks` function in `graphsage.utils`.\n\n#### Model variants\nThe user must also specify a --model, the variants of which are described in detail in the paper:\n* graphsage_mean -- GraphSage with mean-based aggregator\n* graphsage_seq -- GraphSage with LSTM-based aggregator\n* graphsage_maxpool -- GraphSage with max-pooling aggregator (as described in the NIPS 2017 paper)\n* graphsage_meanpool -- GraphSage with mean-pooling aggregator (a variant of the pooling aggregator, where the element-wie mean replaces the element-wise max).\n* gcn -- GraphSage with GCN-based aggregator\n* n2v -- an implementation of [DeepWalk](https://arxiv.org/abs/1403.6652) (called n2v for short in the code.)\n\n#### Logging directory\nFinally, a --base_log_dir should be specified (it defaults to the current directory).\nThe output of the model and log files will be stored in a subdirectory of the base_log_dir.\nThe path to the logged data will be of the form `<sup/unsup>-<data_prefix>/graphsage-<model_description>/`.\nThe supervised model will output F1 scores, while the unsupervised model will train embeddings and store them.\nThe unsupervised embeddings will be stored in a numpy formated file named val.npy with val.txt specifying the order of embeddings as a per-line list of node ids.\nNote that the full log outputs and stored embeddings can be 5-10Gb in size (on the full data when running with the unsupervised variant).\n\n#### Using the output of the unsupervised models\n\nThe unsupervised variants of GraphSage will output embeddings to the logging directory as described above.\nThese embeddings can then be used in downstream machine learning applications.\nThe `eval_scripts` directory contains examples of feeding the embeddings into simple logistic classifiers.\n\n#### Acknowledgements\n\nThe original version of this code base was originally forked from https://github.com/tkipf/gcn/, and we owe many thanks to Thomas Kipf for making his code available.\nWe also thank Yuanfang Li and Xin Li who contributed to a course project that was based on this work.\nPlease see the [paper](https://arxiv.org/pdf/1706.02216.pdf) for funding details and additional (non-code related) acknowledgements.\n"
        },
        {
          "name": "eval_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "example_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "example_supervised.sh",
          "type": "blob",
          "size": 0.1015625,
          "content": "python -m graphsage.supervised_train --train_prefix ./example_data/ppi --model graphsage_mean --sigmoid\n"
        },
        {
          "name": "example_unsupervised.sh",
          "type": "blob",
          "size": 0.134765625,
          "content": "python -m graphsage.unsupervised_train --train_prefix ./example_data/ppi --model graphsage_mean --max_total_steps 1000 --validate_iter 10\n"
        },
        {
          "name": "graphsage",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3857421875,
          "content": "absl-py==0.2.2\nastor==0.6.2\nbackports.weakref==1.0.post1\nbleach==1.5.0\ndecorator==4.3.0\nenum34==1.1.6\nfuncsigs==1.0.2\nfutures==3.2.0\ngast==0.2.0\ngrpcio==1.12.1\nhtml5lib==0.9999999\nMarkdown==2.6.11\nmock==2.0.0\nnetworkx==1.11\nnumpy==1.14.5\npbr==4.0.4\nprotobuf==3.6.0\nscikit-learn==0.19.1\nscipy==1.1.0\nsix==1.11.0\nsklearn==0.0\ntensorboard==1.8.0\ntensorflow==1.8.0\ntermcolor==1.1.0\nWerkzeug==0.14.1\n"
        }
      ]
    }
  ]
}