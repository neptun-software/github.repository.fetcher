{
  "metadata": {
    "timestamp": 1736559834143,
    "page": 576,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "mymusise/ChatGLM-Tuning",
      "stars": 3760,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.86328125,
          "content": "# model file\n\nmodels/llama/*\nmodels/llama-hf/llama-7b/*.bin\nmodels/pefts/*.bin\n\ndata/*\noutputs/*\noutput/*\n\n\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2023 Chengxi Guo\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.12890625,
          "content": "# ChatGLM-Tuning\n\n一种平价的chatgpt实现方案，基于清华的 [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) + LoRA 进行finetune.\n\n数据集: [alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n\n有colab的同学可以直接在colab上尝试： <a href=\"https://colab.research.google.com/github/mymusise/ChatGLM-Tuning/blob/master/examples/finetune.ipynb\">\n        <img alt=\"Build\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n    </a>\n\n\n[官方ptuning代码](https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning)\n\n\n## Demo\n\n- [开源版的文心一言](https://github.com/visual-openllm/visual-openllm)\n\n\n## S1 Finetune\n\n### 准备\n\n- 显卡: 显存 >= 16G (最好24G或者以上)\n- 环境：\n- - python>=3.8\n- - cuda>=11.6, cupti, cuDNN, TensorRT等深度学习环境\n- - pip3 install -r requirements.txt\n其中requirements.txt中的安装包bitsandbytes 建议安装0.41.2.post2这个版本，以前的版本可能会提示报错：\n        bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cget_col_row_stats\n\n### 数据预处理\n\n\n转化alpaca数据集为jsonl\n\n```bash\npython cover_alpaca2jsonl.py \\\n    --data_path data/alpaca_data.json \\\n    --save_path data/alpaca_data.jsonl \\\n    \n```\n\ntokenization\n\n```bash\npython tokenize_dataset_rows.py \\\n    --jsonl_path data/alpaca_data.jsonl \\\n    --save_path data/alpaca \\\n    --max_seq_length 200 \\ \n    --skip_overlength  False\n    --chatglm_path model_path/chatglm\n    --version v1                 \n    \n```\n\n- `--jsonl_path` 微调的数据路径, 格式jsonl, 对每行的['context']和['target']字段进行encode\n- `--save_path` 输出路径\n- `--max_seq_length` 样本的最大长度\n- `--chatglm_path` 导入模型的路径（可以选择chatglm或chatglm2的不同路径）\n- `--version` 模型的版本（v1指chatglm,v2指chatglm2）\n\n### 训练\n\n```bash\npython finetune.py \\\n    --dataset_path data/alpaca \\\n    --lora_rank 8 \\\n    --per_device_train_batch_size 6 \\\n    --gradient_accumulation_steps 1 \\\n    --max_steps 52000 \\\n    --save_steps 1000 \\\n    --save_total_limit 2 \\\n    --learning_rate 1e-4 \\\n    --fp16 \\\n    --remove_unused_columns false \\\n    --logging_steps 50 \\\n    --output_dir output\n    --chatglm_path model_path/chat_glm\n```\n\n### 推理\n\n参考 [infer.ipynb](infer.ipynb)\n\n<details><summary><b>Finetune前后对比</b></summary>\n\n利用Alpaca数据集合对ChatGLM-6B Finetune后，在Alpaca数据集上表现得更好:\n- `Answer:` 是模型的输出\n- `#### Answer:` 是原答案\n![](https://user-images.githubusercontent.com/6883957/226977555-c00c796f-4fdb-4613-810a-8b9a6068bb1b.jpeg)\n\n\n</details>\n\n\n## S2. Reward Model\n\n## S3. PPO\n\n\n## LoRA\n\n| LoRA                                  | Dataset      |\n| ------------------------------------- | ------------ |\n| mymusise/chatglm-6b-alpaca-lora       | Alpaca       |\n| mymusise/chatglm-6b-alpaca-zh-en-lora | Alpaca-zh-en |\n| *(on the way)*                        | Alpaca-zh    |\n\n### 使用预训练好的LoRA\n\n参考 [examples/infer_pretrain.ipynb](https://colab.research.google.com/github/mymusise/ChatGLM-Tuning/blob/master/examples/infer_pretrain.ipynb)\n\n\n# TODO:\n\n- ~~bs > 1 support~~\n- 使用中文数据\n- 加入RLHF"
        },
        {
          "name": "cover_alpaca2jsonl.py",
          "type": "blob",
          "size": 0.841796875,
          "content": "import argparse\nimport json\nfrom tqdm import tqdm\n\n\ndef format_example(example: dict) -> dict:\n    context = f\"Instruction: {example['instruction']}\\n\"\n    if example.get(\"input\"):\n        context += f\"Input: {example['input']}\\n\"\n    context += \"Answer: \"\n    target = example[\"output\"]\n    return {\"context\": context, \"target\": target}\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_path\", type=str, default=\"data/alpaca_data.json\")\n    parser.add_argument(\"--save_path\", type=str, default=\"data/alpaca_data.jsonl\")\n\n    args = parser.parse_args()\n    with open(args.data_path) as f:\n        examples = json.load(f)\n\n    with open(args.save_path, 'w') as f:\n        for example in tqdm(examples, desc=\"formatting..\"):\n            f.write(json.dumps(format_example(example)) + '\\n')\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune.py",
          "type": "blob",
          "size": 3.908203125,
          "content": "from transformers.integrations import TensorBoardCallback\nfrom torch.utils.tensorboard import SummaryWriter\nfrom transformers import TrainingArguments\nfrom transformers import Trainer, HfArgumentParser\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import PreTrainedTokenizerBase\nimport torch\nimport torch.nn as nn\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom dataclasses import dataclass, field\nimport datasets\nimport os\n\n\n@dataclass\nclass FinetuneArguments:\n    dataset_path: str = field(default=\"data/alpaca\")\n    model_path: str = field(default=\"output\")\n    lora_rank: int = field(default=8)\n    chatglm_path: str = field(default=\"model_path/chatglm\")\n\n\nclass CastOutputToFloat(nn.Sequential):\n    def forward(self, x):\n        return super().forward(x).to(torch.float32)\n\n\n@dataclass\nclass DataCollator:\n    tokenizer: PreTrainedTokenizerBase\n\n    def __call__(self, features: list) -> dict:\n        len_ids = [len(feature[\"input_ids\"]) for feature in features]\n        longest = max(len_ids)\n        input_ids = []\n        labels_list = []\n        for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n            ids = feature[\"input_ids\"]\n            seq_len = feature[\"seq_len\"]\n\n            labels = (\n                [-100] * (seq_len - 1)\n                + ids[(seq_len - 1) :]\n                + [-100] * (longest - ids_l)\n            )\n            ids = ids + [self.tokenizer.pad_token_id] * (longest - ids_l)\n            _ids = torch.LongTensor(ids)\n            labels_list.append(torch.LongTensor(labels))\n            input_ids.append(_ids)\n        input_ids = torch.stack(input_ids)\n        labels = torch.stack(labels_list)\n        return {\n            \"input_ids\": input_ids,\n            \"labels\": labels,\n        }\n\n\nclass ModifiedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        return model(\n            input_ids=inputs[\"input_ids\"],\n            labels=inputs[\"labels\"],\n        ).loss\n\n    def save_model(self, output_dir=None, _internal_call=False):\n        from transformers.trainer import TRAINING_ARGS_NAME\n\n        os.makedirs(output_dir, exist_ok=True)\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n        saved_params = {\n            k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n        }\n        torch.save(saved_params, os.path.join(output_dir, \"adapter_model.bin\"))\n\n\ndef main():\n    writer = SummaryWriter()\n\n    finetune_args, training_args = HfArgumentParser(\n        (FinetuneArguments, TrainingArguments)\n    ).parse_args_into_dataclasses()\n\n    # init model\n\n    model = AutoModel.from_pretrained(\n        finetune_args.chatglm_path,\n        load_in_8bit=True,\n        trust_remote_code=True,\n        device_map=\"auto\",\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        finetune_args.chatglm_path, trust_remote_code=True\n    )\n    model.gradient_checkpointing_enable()\n    model.enable_input_require_grads()\n    model.is_parallelizable = True\n    model.model_parallel = True\n    # model.lm_head = CastOutputToFloat(model.lm_head)\n    model.config.use_cache = (\n        False  # silence the warnings. Please re-enable for inference!\n    )\n\n    # setup peft\n    peft_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        inference_mode=False,\n        r=finetune_args.lora_rank,\n        lora_alpha=32,\n        lora_dropout=0.1,\n    )\n    model = get_peft_model(model, peft_config)\n\n    # load dataset\n    dataset = datasets.load_from_disk(finetune_args.dataset_path)\n    print(f\"\\n{len(dataset)=}\\n\")\n\n    # start train\n    trainer = ModifiedTrainer(\n        model=model,\n        train_dataset=dataset,\n        args=training_args,\n        callbacks=[TensorBoardCallback(writer)],\n        data_collator=DataCollator(tokenizer),\n    )\n    trainer.train()\n    writer.close()\n    # save model\n    model.save_pretrained(training_args.output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "infer.ipynb",
          "type": "blob",
          "size": 5.7509765625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\\n\",\n      \"Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.23it/s]\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"from transformers import AutoModel\\n\",\n    \"import torch\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"model = AutoModel.from_pretrained(\\\"THUDM/chatglm-6b\\\", trust_remote_code=True, load_in_8bit=True, device_map='auto')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"from transformers import AutoTokenizer\\n\",\n    \"\\n\",\n    \"tokenizer = AutoTokenizer.from_pretrained(\\\"THUDM/chatglm-6b\\\", trust_remote_code=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 9,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"/home/mymusise/pro/ChatGLM-Tuning/peft/tuners/lora.py:175: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\\n\",\n      \"  warnings.warn(\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"from peft import PeftModel\\n\",\n    \"\\n\",\n    \"model = PeftModel.from_pretrained(model, \\\"./output/\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 10,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import json\\n\",\n    \"\\n\",\n    \"instructions = json.load(open(\\\"data/alpaca_data.json\\\"))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"/home/mymusise/pro/ChatGLM-Tuning/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\\n\",\n      \"  warnings.warn(\\n\"\n     ]\n    },\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Instruction: Give three tips for staying healthy.\\n\",\n      \"Answer: 1. Eat a balanced diet.\\n\",\n      \"2. Get regular exercise.\\n\",\n      \"3. Stay hydrated.\\n\",\n      \"### 1.Answer:\\n\",\n      \" 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n\",\n      \"2. Exercise regularly to keep your body active and strong. \\n\",\n      \"3. Get enough sleep and maintain a consistent sleep schedule. \\n\",\n      \"\\n\",\n      \"\\n\",\n      \"Instruction: What are the three primary colors?\\n\",\n      \"Answer: The three primary colors are red, blue, and yellow.\\n\",\n      \"### 2.Answer:\\n\",\n      \" The three primary colors are red, blue, and yellow. \\n\",\n      \"\\n\",\n      \"\\n\",\n      \"Instruction: Describe the structure of an atom.\\n\",\n      \"Answer: An atom is a small particle of an element, containing a small number of particles of the element. Each particle is made up of a single electron, which is surrounded by a cloud of negative charge. The cloud of negative charge is surrounded by a cloud of positive charge, which creates a neutral cloud. The neutral cloud is made up of a cloud of negative charge, which creates a cloud of positive charge. This process continues until all the atoms have been neutralized.\\n\",\n      \"### 3.Answer:\\n\",\n      \" An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \\n\",\n      \"\\n\",\n      \"\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"answers = []\\n\",\n    \"from cover_alpaca2jsonl import format_example\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"with torch.no_grad():\\n\",\n    \"    for idx, item in enumerate(instructions[:3]):\\n\",\n    \"        feature = format_example(item)\\n\",\n    \"        input_text = feature['context']\\n\",\n    \"        ids = tokenizer.encode(input_text)\\n\",\n    \"        input_ids = torch.LongTensor([ids])\\n\",\n    \"        out = model.generate(\\n\",\n    \"            input_ids=input_ids,\\n\",\n    \"            max_length=150,\\n\",\n    \"            do_sample=False,\\n\",\n    \"            temperature=0\\n\",\n    \"        )\\n\",\n    \"        out_text = tokenizer.decode(out[0])\\n\",\n    \"        answer = out_text.replace(input_text, \\\"\\\").replace(\\\"\\\\nEND\\\", \\\"\\\").strip()\\n\",\n    \"        item['infer_answer'] = answer\\n\",\n    \"        print(out_text)\\n\",\n    \"        print(f\\\"### {idx+1}.Answer:\\\\n\\\", item.get('output'), '\\\\n\\\\n')\\n\",\n    \"        answers.append({'index': idx, **item})\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"venv\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.12\"\n  },\n  \"orig_nbformat\": 4,\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        },
        {
          "name": "output",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.244140625,
          "content": "# int8\nbitsandbytes==0.37.1\naccelerate==0.17.1\n\n# chatglm\nprotobuf>=3.19.5,<3.20.1\ntransformers==4.27.1\nicetk\ncpm_kernels==1.0.11\ntorch>=1.13.1\ntensorboard\n\n# \ndatasets==2.10.1\ngit+https://github.com/huggingface/peft.git  # 最新版本 >=0.3.0.dev0\n"
        },
        {
          "name": "tokenize_dataset_rows.py",
          "type": "blob",
          "size": 2.7529296875,
          "content": "import argparse\nimport json\nfrom tqdm import tqdm\n\nimport datasets\nimport transformers\n\n\ndef preprocess(tokenizer, config, example, max_seq_length, version):\n    if version == 'v1':\n        prompt = example[\"context\"]\n        target = example[\"target\"]\n        prompt_ids = tokenizer.encode(prompt, max_length=max_seq_length, truncation=True)\n        target_ids = tokenizer.encode(\n            target,\n            max_length=max_seq_length,\n            truncation=True,\n            add_special_tokens=False)\n        input_ids = prompt_ids + target_ids + [config.eos_token_id]\n        return {\"input_ids\": input_ids, \"seq_len\": len(prompt_ids)}\n\n    if version == 'v2':\n        query = example[\"context\"]\n        target = example[\"target\"]\n        history = None\n        prompt = tokenizer.build_prompt(query, history)\n\n        a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n                                 max_length=max_seq_length)\n        b_ids = tokenizer.encode(text=target, add_special_tokens=False, truncation=True,\n                                 max_length=max_seq_length)\n\n        input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n\n        return {\"input_ids\": input_ids, \"seq_len\": len(a_ids)}\n\n\n\n\n\ndef read_jsonl(path, max_seq_length, chatglm_path, version='v1', skip_overlength=False):\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        chatglm_path, trust_remote_code=True)\n    config = transformers.AutoConfig.from_pretrained(\n        chatglm_path, trust_remote_code=True, device_map='auto')\n    with open(path, \"r\") as f:\n        for line in tqdm(f.readlines()):\n            example = json.loads(line)\n            # feature = preprocess(tokenizer, config, example, max_seq_length)\n            feature = preprocess(tokenizer, config, example, max_seq_length, version)\n            if skip_overlength and len(feature[\"input_ids\"]) > max_seq_length:\n                continue\n            # feature[\"input_ids\"] = feature[\"input_ids\"][:max_seq_length]\n            yield feature\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--jsonl_path\", type=str, default=\"data/alpaca_data.jsonl\")\n    parser.add_argument(\"--save_path\", type=str, default=\"data/alpaca\")\n    parser.add_argument(\"--max_seq_length\", type=int, default=384)\n    parser.add_argument(\"--skip_overlength\", type=bool, default=False)\n    parser.add_argument(\"--chatglm_path\", type=str, default='model_path/chatglm')\n    parser.add_argument(\"--version\", type=str, default='v1')\n    args = parser.parse_args()\n\n    dataset = datasets.Dataset.from_generator(\n        lambda: read_jsonl(args.jsonl_path, args.max_seq_length, args.chatglm_path, args.version, args.skip_overlength)\n    )\n    dataset.save_to_disk(args.save_path)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}