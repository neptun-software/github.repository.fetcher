{
  "metadata": {
    "timestamp": 1736559724802,
    "page": 421,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "yosinski/deep-visualization-toolbox",
      "stars": 4029,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0556640625,
          "content": "settings_local.py\nsettings.py.templatec\n*.pyc\n*.DS_Store\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0556640625,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2015 Jason Yosinski\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.53515625,
          "content": "# Deep Visualization Toolbox\n\nThis is the code required to run the Deep Visualization Toolbox, as well as to generate the neuron-by-neuron visualizations using regularized optimization.\nThe toolbox and methods are described casually [here](http://yosinski.com/deepvis) and more formally in this paper:\n\n * Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. [Understanding neural networks through deep visualization](http://arxiv.org/abs/1506.06579). Presented at the Deep Learning Workshop, International Conference on Machine Learning (ICML), 2015.\n\nIf you find this paper or code useful, we encourage you to cite the paper. BibTeX:\n\n    @inproceedings{yosinski-2015-ICML-DL-understanding-neural-networks,\n    Author = {Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod Lipson},\n    Booktitle = {Deep Learning Workshop, International Conference on Machine Learning (ICML)},\n    Title = {Understanding Neural Networks Through Deep Visualization},\n    Year = {2015}}\n\n\n\n# Features\n\nThe main toolbox window looks like this, here showing a convolutional unit that responds to automobile wheels:\n\n![DeepVis Toolbox Screenshot bus](doc/example_caffenet-yos_bus_wheel_unit.jpg?raw=true)\n\nFor a quick tour of the toolbox features, including what each pane of the above interface is showing, watch this [4 min YouTube video](https://www.youtube.com/watch?v=AgkfIQ4IGaM). In addition to processing images files from disk, the toolbox can run off a webcam for live network visualization **(below left)**.\nThe toolbox comes bundled with the default [caffenet-yos](models/caffenet-yos) model weights and pre-computed per-unit visualizations shown in the paper. Weights, but not per-unit visualizations, for [bvlc-googlenet](models/bvlc-googlenet) **(below right)** and [squeezenet](models/squeezenet) can be downloaded by scripts in their respective directories.\n\n[![DeepVis Toolbox Screenshot webcam](doc/example_caffenet-yos_webcam.300.jpg)](doc/example_caffenet-yos_webcam.jpg?raw=true)\n[![DeepVis Toolbox Screenshot bvlc-googlenet](doc/example_bvlc-googlenet_bus.300.jpg)](doc/example_bvlc-googlenet_bus.jpg?raw=true)\n\nYou can visualize your own model as well. However, note that the toolbox provides two rather separate sets of features; the first is easy to use with your own model, and the second is more involved:\n\n1. **Forward/backward prop**: Images can be run forward through the network to visualize activations, and derivatives of any unit with respect to any other unit can be computed using backprop. In addition to traditional backprop, deconv from [Zeiler and Fergus (2014)](https://scholar.google.com/scholar?q=Zeiler+Visualizing+and+understanding+convolutional+networks) is supported as a way of flowing information backwards through the network. Doing forward and backward passes works for any model that can be run in Caffe (including yours!).\n\n2. **Per-unit visualizations**: Three types of per-unit visualizations can be computed for a network &mdash; max image, deconv of max image, activation maximization via regularized optimization &mdash; but these visualizations must be computed *outside* the toolbox and saved as jpg. The toolbox then loads these jpgs to display alongside units as they are selected. Visualizations must be pre-computed because they are far too expensive to run live. For example, going through the 1.3m image training set to find the images causing top-9 activations took 40 hours on our system (for all units). Per-unit visualization jpgs are provided for the caffenet-yos model, but not for the bvlc-googlenet or squeezenet models (and not for yours, but you can [compute them yourself](doc/computing_per_unit_visualizations.md)).\n\nSummary:\n\n| Model          | Forward/Backward prop | Per-unit visualizations |\n| -------------  | ---------------- | ----------------------- |\n| [caffenet-yos](models/caffenet-yos) | **easy**             | **included**            |\n| [bvlc-googlenet](bvlc-googlenet)    | **easy**             | not-included, [generate](doc/computing_per_unit_visualizations.md) if desired            |\n| [squeezenet](models/squeezenet)     | **easy**             | not-included, [generate](doc/computing_per_unit_visualizations.md) if desired            |\n| your network | **easy** (just point to your model in `settings_local.py`)  | not-included, [generate](doc/computing_per_unit_visualizations.md) if desired  |\n\n\n\n# Setting up and running the toolbox\n\n### Step 0: Compile master branch of caffe (optional but recommended)\n\nCheckout the master branch of [Caffe](http://caffe.berkeleyvision.org/) and compile it on your\nmachine. If you've never used Caffe before, it can take a bit of time to get all the required libraries in place. Fortunately, the [installation process is well documented](http://caffe.berkeleyvision.org/installation.html). When you're installing the OpenCV dependency, install the Python bindings as well (see Step 2 below).\n\nNote: When compiling Caffe, you can set `CPU_ONLY := 1` in your `Makefile.config` to skip all the Cuda/GPU stuff. The Deep Visualization Toolbox can run with Caffe in either CPU or GPU mode, and it's simpler to get Caffe to compile for the first time in `CPU_ONLY` mode. If Caffe is compiled with GPU options enabled, CPU vs. GPU may be switched at runtime via a setting in `settings_local.py`. Also, cuDNN may be enabled or disabled by recompiling Caffe with or without cuDNN.\n\n\n### Step 1: Compile the deconv-deep-vis-toolbox branch of caffe\n\nInstead of using the master branch of Caffe, to use the demo\nyou'll need the slightly modified [deconv-deep-vis-toolbox Caffe branch](https://github.com/yosinski/caffe/tree/deconv-deep-vis-toolbox) (supporting deconv and a few\nextra Python bindings). Getting the branch and switching to it is easy.\nStarting from your Caffe directory (that is, the directory where you've checked out Caffe, *not* the directory where you've checked out the DeepVis Toolbox), run:\n\n    $ git remote add yosinski https://github.com/yosinski/caffe.git\n    $ git fetch --all\n    $ git checkout --track -b deconv-deep-vis-toolbox yosinski/deconv-deep-vis-toolbox\n    $ < edit Makefile.config to suit your system if not already done in Step 0 >\n    $ make clean\n    $ make -j\n    $ make -j pycaffe\n\nAs noted above, feel free to compile in `CPU_ONLY` mode.\n\n\n\n### Step 2: Install prerequisites\n\nThe only prerequisites beyond those required for Caffe are `python-opencv`, `scipy`, and `scikit-image`, which may be installed as follows (other install options exist as well):\n\n#### Ubuntu:\n\n    $ sudo apt-get install python-opencv scipy python-skimage\n\n#### Mac using [homebrew](http://brew.sh/):\n\nInstall `python-opencv` using one of the following two lines, depending on whether you want to compile using Intel TBB to enable parallel operations:\n\n    $ brew install opencv\n    $ brew install --with-tbb opencv\n\nInstall `scipy` either with OpenBLAS...\n\n    $ brew install openblas\n    $ brew install --with-openblas scipy\n\n...or without it\n\n    $ brew install scipy\n\nAnd install `scikit-image` using pip:\n\n    $ pip install scikit-image\n\nYou may have already installed the `python-opencv` bindings as part of the Caffe setup process. If `import cv2` works from Python, then you're all set. Similarly for `import scipy` and `import skimage`.\n\n\n\n### Step 3: Download and configure Deep Visualization Toolbox code\n\nYou can put it wherever you like:\n\n    $ git clone https://github.com/yosinski/deep-visualization-toolbox\n    $ cd deep-visualization-toolbox\n\nThe settings in the latest version of the toolbox (February 2016) work a bit differently than in earlier versions (April 2015). If you have the latest version (recommended!), \nthe minimal steps are to create a `settings_local.py` file using the template for the default `caffenet-yos` model:\n\n    $ cp models/caffenet-yos/settings_local.template-caffenet-yos.py settings_local.py\n\nAnd then edit the `settings_local.py` file to make the `caffevis_caffe_root` variable point to the directory where you've compiled caffe in Step 1:\n\n    $ < edit settings_local.py >\n\n*Note on settings:* Settings are now split into two files: a versioned `settings.py` file that provides documentation and default values for all settings and an unversioned `settings_local.py` file. This latter file allows you to override any default setting to tailor the toolbox to your specific setup (Caffe path, CPU vs. GPU, webcam device, etc) and model (model weights, prototxt, sizes of the various panels shown in the toolbox, etc). This also makes it easy to distribute settings tweaks alongside models: for example, `models/bvlc-googlenet/settings_local.template-bvlc-googlenet.py` includes the appropriate window pane sizes and so on for the `bvlc-googlenet` model. To load a new model, just change the details in `settings_local.py`, perhaps by copying from the included template.\n\nFinally, download the default model weights and corresponding top-9 visualizations saved as jpg (downloads a 230MB model and 1.1GB of jpgs to show as visualization):\n\n    $ cd models/caffenet-yos/\n    $ ./fetch.sh\n    $ cd ../..\n\n\n\n### Step 4: Run it!\n\nSimple:\n\n    $ ./run_toolbox.py\n\nOnce the toolbox is running, push 'h' to show a help screen. You can also have a look at `bindings.py` to see what the various keys do. If the window is too large or too small for your screen, set the `global_scale` and `global_font_size` variables in `settings_local.py` to values smaller or larger than 1.0.\n\n\n\n# Troubleshooting\n\nIf you have any problems running the Deep Vis Toolbox, here are a few things to try:\n\n * Make sure you can compile the master branch of Caffe (Step 0 above)! If you can't, see the [detailed compilation instructions for Caffe](http://caffe.berkeleyvision.org/installation.html). If you encounter issues, the [caffe-users](https://groups.google.com/forum/#!forum/caffe-users) mailing list is a good place to look for solutions others have found.\n * Try using the `dev` branch of this toolbox instead of `master` (`git checkout dev`). Sometimes it's a little more up to date.\n * If you get an error (`AttributeError: 'Classifier' object has no attribute 'backward_from_layer'`) when switching to backprop or deconv modes, it's because your compiled branch of Caffe does not have the necessary Python bindings for backprop/deconv. Follow the directions in \"Step 1: Compile the deconv-deep-vis-toolbox branch of caffe\" above.\n * If the backprop pane in the lower left is just gray, it's probably because backprop and deconv are producing all zeros. By default, Caffe won't compute derivatives at the data layer, because they're not needed to update parameters. The fix is simple: just add `force_backward: true` to your network prototxt, [like this](https://github.com/yosinski/deep-visualization-toolbox/blob/master/models/caffenet-yos/caffenet-yos-deploy.prototxt#L7).\n * If the toolbox runs but the keys don't respond as expected, this may be because keys behave differently on different platforms. Run the `test_keys.py` script to test behavior on your system.\n * If none of that helps, feel free to [email me](http://yosinski.com/) or [submit an issue](https://github.com/yosinski/deep-visualization-toolbox/issues). I might have left out an important detail here or there :).\n\n\n\n# Other ways of running the toolbox\n\nIf running the toolbox on a local Mac or Linux machine isn't working for you, you might want to try one of these other options:\n\n * John Moeller has put together a [Docker container for the toolbox](https://github.com/fishcorn/dvtb-container). This should even work on Windows! (confirmation needed)\n\n * If you're desperate, it's also possible to [run the toolbox on Amazon EC2](doc/deep-vis-on-aws.md), but display will be much slower and images can be loaded only from file (not from webcam).\n\n\n\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "app_base.py",
          "type": "blob",
          "size": 1.365234375,
          "content": "#! /usr/bin/env python\n\nclass BaseApp(object):\n    '''Base App class.'''\n\n    def __init__(self, settings, key_bindings):\n        self.debug_level = 0\n\n    def handle_input(self, input_image, panes):\n        pass\n        \n    def handle_key(self, key, panes):\n        '''Handle key and return either key (to let someone downstream handle it) or None (if this app handled it)'''\n        pass\n\n    def redraw_needed(self, key, panes):\n        '''App should return whether or not its internal state has\n        been updated (perhaps in response to handle_key, handle_input,\n        or some internal processing finishing).\n        '''\n        return False\n\n    def draw(self, panes):\n        '''Tells the app to draw in the given panes. Returns True if panes were changed and require a redraw, False if nothing was changed.'''\n        return False\n\n    def draw_help(self, panes):\n        '''Tells the app to draw its help screen in the given pane. No return necessary.'''\n        pass\n\n    def start(self):\n        '''Notify app to start, possibly creating any necessary threads'''\n        pass\n\n    def get_heartbeats(self):\n        '''Returns a list of heartbeat functions, if any, that should be called regularly.'''\n        return []\n\n    def set_debug(self, level):\n        self.debug_level = level\n\n    def quit(self):\n        '''Notify app to quit, possibly joining any threads'''\n        pass\n"
        },
        {
          "name": "bindings.py",
          "type": "blob",
          "size": 4.4765625,
          "content": "# Define key bindings\n\nfrom keys import key_patterns\n\nclass Bindings(object):\n    def __init__(self, key_patterns):\n        self._tag_to_key_labels = {}\n        self._tag_to_help = {}\n        self._key_label_to_tag = {}\n        self._key_patterns = key_patterns\n        self._cache_keycode_to_tag = {}\n        \n    def get_tag(self, keycode):\n        '''Gets tag for keycode, returns None if no tag found.'''\n        if keycode is None:\n            return None\n        if not keycode in self._cache_keycode_to_tag:\n            label = self.get_key_label_from_keycode(keycode)\n            self._cache_keycode_to_tag[keycode] = self.get_tag_from_key_label(label)\n        return self._cache_keycode_to_tag[keycode]\n\n    def get_tag_from_key_label(self, label):\n        '''Get tag using key label, if no match, returns None.'''\n        \n        return self._key_label_to_tag.get(label, None)\n        \n    def get_key_label_from_keycode(self, keycode, extra_info = False):\n        '''Get tag using keycode, if no match, returns None.'''\n        \n        label = None\n        for mask in reversed(sorted(self._key_patterns.keys())):\n            masked_keycode = keycode & mask\n            if masked_keycode in self._key_patterns[mask]:\n                label = self._key_patterns[mask][masked_keycode]\n                break\n        \n        if extra_info:\n            return label, [keycode & mask for mask in reversed(sorted(self._key_patterns.keys()))]\n        else:\n            return label\n    \n    def add(self, tag, key, help_text):\n        self.add_multikey(tag, (key,), help_text)\n\n    def add_multikey(self, tag, key_labels, help_text):\n        for key_label in key_labels:\n            assert key_label not in self._key_label_to_tag, (\n                'Key \"%s\" cannot be bound to \"%s\" because it is already bound to \"%s\"' %\n                (key_label, tag, self._key_label_to_tag[key_label])\n                )\n            self._key_label_to_tag[key_label] = tag\n        self._tag_to_key_labels[tag] = key_labels\n        self._tag_to_help[tag] = help_text\n\n    def get_key_help(self, tag):\n        return (self._tag_to_key_labels[tag], self._tag_to_help[tag])\n\n_ = Bindings(key_patterns)\n\n# Core\n_.add('freeze_cam', 'f',\n       'Freeze or unfreeze camera capture')\n_.add('toggle_input_mode', 'c',\n       'Toggle between camera and static files')\n_.add_multikey('static_file_increment', ['e', 'pgdn'],\n       'Load next static file')\n_.add_multikey('static_file_decrement', ['w', 'pgup'],\n       'Load previous static file')\n_.add('help_mode', 'h',\n       'Toggle this help screen')\n_.add('stretch_mode', '0',\n       'Toggle between cropping and stretching static files to be square')\n_.add('debug_level', '5',\n       'Cycle debug level between 0 (quiet), 1 (some timing info) and 2 (all timing info)')\n_.add('quit', 'q',\n       'Quit')\n\n# Caffevis\n_.add_multikey('reset_state', ['esc'],\n       'Reset: turn off backprop, reset to layer 0, unit 0, default boost.')\n_.add_multikey('sel_left', ['left', 'j'],\n       '')\n_.add_multikey('sel_right', ['right', 'l'],\n       '')\n_.add_multikey('sel_down', ['down', 'k'],\n       '')\n_.add_multikey('sel_up', ['up', 'i'],\n       '')\n_.add('sel_left_fast', 'J',\n       '')\n_.add('sel_right_fast', 'L',\n       '')\n_.add('sel_down_fast', 'K',\n       '')\n_.add('sel_up_fast', 'I',\n       '')\n_.add_multikey('sel_layer_left', ['u', 'U'],\n       'Select previous layer without moving cursor')\n_.add_multikey('sel_layer_right', ['o', 'O'],\n       'Select next layer without moving cursor')\n\n_.add('zoom_mode', 'z',\n       'Cycle zooming through {currently selected unit, backprop results, none}')\n_.add('pattern_mode', 's',\n       'Toggle overlay of preferred input pattern (regularized optimized images)')\n\n_.add('ez_back_mode_loop', 'b',\n       'Cycle through a few common backprop/deconv modes')\n_.add('freeze_back_unit', 'd',\n       'Freeze the bprop/deconv origin to be the currently selected unit')\n_.add('show_back', 'a',\n       'Toggle between showing forward activations and back/deconv diffs')\n_.add('back_mode', 'n',\n       '(expert) Change back mode directly.')\n_.add('back_filt_mode', 'm',\n       '(expert) Change back output filter directly.')\n\n_.add('boost_gamma', 't',\n       'Boost contrast using gamma correction')\n_.add('boost_individual', 'T',\n       'Boost contrast by scaling each channel to use more of its individual range')\n_.add('toggle_label_predictions', '8',\n       'Turn on or off display of prob label values')\n_.add('toggle_unit_jpgs', '9',\n       'Turn on or off display of loaded jpg visualization')\n\nbindings = _\n"
        },
        {
          "name": "caffevis",
          "type": "tree",
          "content": null
        },
        {
          "name": "codependent_thread.py",
          "type": "blob",
          "size": 0.8427734375,
          "content": "import time\nfrom threading import Lock, Thread\n\n\nclass CodependentThread(Thread):\n    '''A Thread that must be occasionally poked to stay alive. '''\n\n    def __init__(self, heartbeat_timeout = 1.0):\n        Thread.__init__(self)\n        self.heartbeat_timeout = heartbeat_timeout\n        self.heartbeat_lock = Lock()\n        self.heartbeat()\n        \n    def heartbeat(self):\n        with self.heartbeat_lock:\n            self.last_beat = time.time()\n\n    def is_timed_out(self):\n        with self.heartbeat_lock:\n            now = time.time()\n            if now - self.last_beat > self.heartbeat_timeout:\n                print '%s instance %s timed out after %s seconds (%s - %s = %s)' % (self.__class__.__name__, self, self.heartbeat_timeout, now, self.last_beat, now - self.last_beat)\n                return True\n            else:\n                return False\n"
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "find_maxes",
          "type": "tree",
          "content": null
        },
        {
          "name": "image_misc.py",
          "type": "blob",
          "size": 17.55859375,
          "content": "#! /usr/bin/env python\n\nimport cv2\nimport numpy as np\nimport skimage\nimport skimage.io\nfrom copy import deepcopy\n\nfrom misc import WithTimer\n\n\ndef norm01(arr):\n    arr = arr.copy()\n    arr -= arr.min()\n    arr /= arr.max() + 1e-10\n    return arr\n\n\ndef norm01c(arr, center):\n    '''Maps the input range to [0,1] such that the center value maps to .5'''\n    arr = arr.copy()\n    arr -= center\n    arr /= max(2 * arr.max(), -2 * arr.min()) + 1e-10\n    arr += .5\n    assert arr.min() >= 0\n    assert arr.max() <= 1\n    return arr\n\n\ndef norm0255(arr):\n    '''Maps the input range to [0,255] as dtype uint8'''\n    arr = arr.copy()\n    arr -= arr.min()\n    arr *= 255.0 / (arr.max() + 1e-10)\n    arr = np.array(arr, 'uint8')\n    return arr\n\n\ndef cv2_read_cap_rgb(cap, saveto = None):\n    rval, frame = cap.read()\n    if saveto:\n        cv2.imwrite(saveto, frame)\n    if len(frame.shape) == 2:\n        # Upconvert single channel grayscale to color\n        frame = frame[:,:,np.newaxis]\n    if frame.shape[2] == 1:\n        frame = np.tile(frame, (1,1,3))\n    if frame.shape[2] > 3:\n        # Chop off transparency\n        frame = frame[:,:,:3]\n    frame = frame[:,:,::-1]   # Convert native OpenCV BGR -> RGB\n    return frame\n\n    \ndef cv2_read_file_rgb(filename):\n    '''Reads an image from file. Always returns (x,y,3)'''\n    im = cv2.imread(filename)\n    if len(im.shape) == 2:\n        # Upconvert single channel grayscale to color\n        im = im[:,:,np.newaxis]\n    if im.shape[2] == 1:\n        im = np.tile(im, (1,1,3))\n    if im.shape[2] > 3:\n        # Chop off transparency\n        im = im[:,:,:3]\n    im = im[:,:,::-1]   # Convert native OpenCV BGR -> RGB\n    return im\n\n    \ndef read_cam_frame(cap, saveto = None):\n    #frame = np.array(cv2_read_cap_rgb(cap, saveto = saveto), dtype='float32')\n    frame = cv2_read_cap_rgb(cap, saveto = saveto)\n    frame = frame[:,::-1,:]  # flip L-R for display\n    frame -= frame.min()\n    frame = frame * (255.0 / (frame.max() + 1e-6))\n    return frame\n\n\ndef crop_to_square(frame):\n    i_size,j_size = frame.shape[0],frame.shape[1]\n    if j_size > i_size:\n        # landscape\n        offset = (j_size - i_size) / 2\n        return frame[:,offset:offset+i_size,:]\n    else:\n        # portrait\n        offset = (i_size - j_size) / 2\n        return frame[offset:offset+j_size,:,:]\n\n\ndef cv2_imshow_rgb(window_name, img):\n    # Convert native OpenCV BGR -> RGB before displaying\n    cv2.imshow(window_name, img[:,:,::-1])\n    #cv2.imshow(window_name, img)\n\n\ndef caffe_load_image(filename, color=True, as_uint=False):\n    '''\n    Copied from Caffe to simplify potential import problems.\n    \n    Load an image converting from grayscale or alpha as needed.\n\n    Take\n    filename: string\n    color: flag for color format. True (default) loads as RGB while False\n        loads as intensity (if image is already grayscale).\n\n    Give\n    image: an image with type np.float32 in range [0, 1]\n        of size (H x W x 3) in RGB or\n        of size (H x W x 1) in grayscale.\n    '''\n    with WithTimer('imread', quiet = True):\n        if as_uint:\n            img = skimage.io.imread(filename)\n        else:\n            img = skimage.img_as_float(skimage.io.imread(filename)).astype(np.float32)\n    if img.ndim == 2:\n        img = img[:, :, np.newaxis]\n        if color:\n            img = np.tile(img, (1, 1, 3))\n    elif img.shape[2] == 4:\n        img = img[:, :, :3]\n    return img\n\n\ndef get_tiles_height_width(n_tiles, desired_width = None):\n    '''Get a height x width size that will fit n_tiles tiles.'''\n    if desired_width == None:\n        # square\n        width = int(np.ceil(np.sqrt(n_tiles)))\n        height = width\n    else:\n        assert isinstance(desired_width, int)\n        width = desired_width\n        height = int(np.ceil(float(n_tiles) / width))\n    return height,width\n        \n\ndef get_tiles_height_width_ratio(n_tiles, width_ratio = 1.0):\n    '''Get a height x width size that will fit n_tiles tiles.'''\n    width = int(np.ceil(np.sqrt(n_tiles * width_ratio)))\n    return get_tiles_height_width(n_tiles, desired_width = width)\n        \n\ndef tile_images_normalize(data, c01 = False, boost_indiv = 0.0,  boost_gamma = 1.0, single_tile = False, scale_range = 1.0, neg_pos_colors = None):\n    data = data.copy()\n    if single_tile:\n        # promote 2D image -> 3D batch (01 -> b01) or 3D image -> 4D batch (01c -> b01c OR c01 -> bc01)\n        data = data[np.newaxis]\n    if c01:\n        # Convert bc01 -> b01c\n        assert len(data.shape) == 4, 'expected bc01 data'\n        data = data.transpose(0, 2, 3, 1)\n\n    if neg_pos_colors:\n        neg_clr, pos_clr = neg_pos_colors\n        neg_clr = np.array(neg_clr).reshape((1,3))\n        pos_clr = np.array(pos_clr).reshape((1,3))\n        # Keep 0 at 0\n        data /= max(data.max(), -data.min()) + 1e-10     # Map data to [-1, 1]\n        \n        #data += .5 * scale_range  # now in [0, scale_range]\n        #assert data.min() >= 0\n        #assert data.max() <= scale_range\n        if len(data.shape) == 3:\n            data = data.reshape(data.shape + (1,))\n        assert data.shape[3] == 1, 'neg_pos_color only makes sense if color data is not provided (channels should be 1)'\n        data = np.dot((data > 0) * data, pos_clr) + np.dot((data < 0) * -data, neg_clr)\n\n    data -= data.min()\n    data *= scale_range / (data.max() + 1e-10)\n\n    # sqrt-scale (0->0, .1->.3, 1->1)\n    assert boost_indiv >= 0 and boost_indiv <= 1, 'boost_indiv out of range'\n    #print 'using boost_indiv:', boost_indiv\n    if boost_indiv > 0:\n        if len(data.shape) == 4:\n            mm = (data.max(-1).max(-1).max(-1) + 1e-10) ** -boost_indiv\n        else:\n            mm = (data.max(-1).max(-1) + 1e-10) ** -boost_indiv\n        data = (data.T * mm).T\n    if boost_gamma != 1.0:\n        data = data ** boost_gamma\n\n    # Promote single-channel data to 3 channel color\n    if len(data.shape) == 3:\n        # b01 -> b01c\n        data = np.tile(data[:,:,:,np.newaxis], 3)\n\n    return data\n\n\ndef tile_images_make_tiles(data, padsize=1, padval=0, hw=None, highlights = None):\n    if hw:\n        height,width = hw\n    else:\n        height,width = get_tiles_height_width(data.shape[0])\n    assert height*width >= data.shape[0], '%d rows x %d columns cannot fit %d tiles' % (height, width, data.shape[0])\n\n    # First iteration: one-way padding, no highlights\n    #padding = ((0, width*height - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n    #data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n\n    # Second iteration: padding with highlights\n    #padding = ((0, width*height - data.shape[0]), (padsize, padsize), (padsize, padsize)) + ((0, 0),) * (data.ndim - 3)\n    #print 'tile_images: data min,max =', data.min(), data.max()\n    #padder = SmartPadder()\n    ##data = np.pad(data, padding, mode=jy_pad_fn)\n    #data = np.pad(data, padding, mode=padder.pad_function)\n    #print 'padder.calls =', padder.calls\n    \n    # Third iteration: two-way padding with highlights\n    if highlights is not None:\n        assert len(highlights) == data.shape[0]\n    padding = ((0, width*height - data.shape[0]), (padsize, padsize), (padsize, padsize)) + ((0, 0),) * (data.ndim - 3)\n\n    # First pad with constant vals\n    try:\n        len(padval)\n    except:\n        padval = tuple((padval,))\n    assert len(padval) in (1,3), 'padval should be grayscale (len 1) or color (len 3)'\n    if len(padval) == 1:\n        data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n    else:\n        data = np.pad(data, padding, mode='constant', constant_values=(0, 0))\n        for cc in (0,1,2):\n            # Replace 0s with proper color in each channel\n            data[:padding[0][0],  :, :, cc] = padval[cc]\n            if padding[0][1] > 0:\n                data[-padding[0][1]:, :, :, cc] = padval[cc]\n            data[:, :padding[1][0],  :, cc] = padval[cc]\n            if padding[1][1] > 0:    \n                data[:, -padding[1][1]:, :, cc] = padval[cc]\n            data[:, :, :padding[2][0],  cc] = padval[cc]\n            if padding[2][1] > 0:\n                data[:, :, -padding[2][1]:, cc] = padval[cc]\n    if highlights is not None:\n        # Then highlight if necessary\n        for ii,highlight in enumerate(highlights):\n            if highlight is not None:\n                data[ii,:padding[1][0],:,:] = highlight\n                if padding[1][1] > 0:\n                    data[ii,-padding[1][1]:,:,:] = highlight\n                data[ii,:,:padding[2][0],:] = highlight\n                if padding[2][1] > 0:\n                    data[ii,:,-padding[2][1]:,:] = highlight\n\n    # tile the filters into an image\n    data = data.reshape((height, width) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n    data = data.reshape((height * data.shape[1], width * data.shape[3]) + data.shape[4:])\n    data = data[0:-padsize, 0:-padsize]  # remove excess padding\n    \n    return (height,width), data\n\n\ndef to_255(vals_01):\n    '''Convert vals in [0,1] to [0,255]'''\n    try:\n        ret = [v*255 for v in vals_01]\n        if type(vals_01) is tuple:\n            return tuple(ret)\n        else:\n            return ret\n    except TypeError:\n        # Not iterable (single int or float)\n        return vals_01*255\n\n\ndef ensure_uint255_and_resize_to_fit(img, out_max_shape,\n                                     shrink_interpolation = cv2.INTER_LINEAR,\n                                     grow_interpolation = cv2.INTER_NEAREST):\n    as_uint255 = ensure_uint255(img)\n    return resize_to_fit(as_uint255, out_max_shape,\n                         dtype_out = 'uint8',\n                         shrink_interpolation = shrink_interpolation,\n                         grow_interpolation = grow_interpolation)\n\n\ndef ensure_uint255(arr):\n    '''If data is float, multiply by 255 and convert to uint8. Else leave as uint8.'''\n    if arr.dtype == 'uint8':\n        return arr\n    elif arr.dtype in ('float32', 'float64'):\n        #print 'extra check...'\n        #assert arr.max() <= 1.1\n        return np.array(arr * 255, dtype = 'uint8')\n    else:\n        raise Exception('ensure_uint255 expects uint8 or float input but got %s with range [%g,%g,].' % (arr.dtype, arr.min(), arr.max()))\n\n\ndef ensure_float01(arr, dtype_preference = 'float32'):\n    '''If data is uint, convert to float and divide by 255. Else leave at float.'''\n    if arr.dtype == 'uint8':\n        #print 'extra check...'\n        #assert arr.max() <= 256\n        return np.array(arr, dtype = dtype_preference) / 255\n    elif arr.dtype in ('float32', 'float64'):\n        return arr\n    else:\n        raise Exception('ensure_float01 expects uint8 or float input but got %s with range [%g,%g,].' % (arr.dtype, arr.min(), arr.max()))\n\n\ndef resize_to_fit(img, out_max_shape,\n                  dtype_out = None,\n                  shrink_interpolation = cv2.INTER_LINEAR,\n                  grow_interpolation = cv2.INTER_NEAREST):\n    '''Resizes to fit within out_max_shape. If ratio is different,\n    returns an image that fits but is smaller along one of the two\n    dimensions.\n\n    If one of the out_max_shape dimensions is None, then use only the other dimension to perform resizing.\n\n    Timing info on MBP Retina with OpenBlas:\n     - conclusion: uint8 is always tied or faster. float64 is slower.\n\n    Scaling down:\n    In [79]: timeit.Timer('resize_to_fit(aa, (200,200))', setup='from caffevis.app import resize_to_fit; import numpy as np; aa = np.array(np.random.uniform(0,255,(1000,1000,3)), dtype=\"uint8\")').timeit(100)\n    Out[79]: 0.04950380325317383\n\n    In [77]: timeit.Timer('resize_to_fit(aa, (200,200))', setup='from caffevis.app import resize_to_fit; import numpy as np; aa = np.array(np.random.uniform(0,255,(1000,1000,3)), dtype=\"float32\")').timeit(100)\n    Out[77]: 0.049156904220581055\n\n    In [76]: timeit.Timer('resize_to_fit(aa, (200,200))', setup='from caffevis.app import resize_to_fit; import numpy as np; aa = np.array(np.random.uniform(0,255,(1000,1000,3)), dtype=\"float64\")').timeit(100)\n    Out[76]: 0.11808204650878906\n\n    Scaling up:\n    In [68]: timeit.Timer('resize_to_fit(aa, (2000,2000))', setup='from caffevis.app import resize_to_fit; import numpy as np; aa = np.array(np.random.uniform(0,255,(1000,1000,3)), dtype=\"uint8\")').timeit(100)\n    Out[68]: 0.4357950687408447\n\n    In [70]: timeit.Timer('resize_to_fit(aa, (2000,2000))', setup='from caffevis.app import resize_to_fit; import numpy as np; aa = np.array(np.random.uniform(0,255,(1000,1000,3)), dtype=\"float32\")').timeit(100)\n    Out[70]: 1.3411099910736084\n\n    In [73]: timeit.Timer('resize_to_fit(aa, (2000,2000))', setup='from caffevis.app import resize_to_fit; import numpy as np; aa = np.array(np.random.uniform(0,255,(1000,1000,3)), dtype=\"float64\")').timeit(100)\n    Out[73]: 2.6078310012817383\n    '''\n\n    if dtype_out is not None and img.dtype != dtype_out:\n        dtype_in_size = img.dtype.itemsize\n        dtype_out_size = np.dtype(dtype_out).itemsize\n        convert_early = (dtype_out_size < dtype_in_size)\n        convert_late = not convert_early\n    else:\n        convert_early = False\n        convert_late = False\n    if out_max_shape[0] is None:\n        scale = float(out_max_shape[1]) / img.shape[1]\n    elif out_max_shape[1] is None:\n        scale = float(out_max_shape[0]) / img.shape[0]\n    else:\n        scale = min(float(out_max_shape[0]) / img.shape[0],\n                    float(out_max_shape[1]) / img.shape[1])\n\n    if convert_early:\n        img = np.array(img, dtype=dtype_out)\n    out = cv2.resize(img,\n            (int(img.shape[1] * scale), int(img.shape[0] * scale)),   # in (c,r) order\n                     interpolation = grow_interpolation if scale > 1 else shrink_interpolation)\n    if convert_late:\n        out = np.array(out, dtype=dtype_out)\n    return out\n\n\nclass FormattedString(object):\n    def __init__(self, string, defaults, face=None, fsize=None, clr=None, thick=None, align=None, width=None):\n        self.string = string\n        self.face  = face  if face  else defaults['face']\n        self.fsize = fsize if fsize else defaults['fsize']\n        self.clr   = clr   if clr   else defaults['clr']\n        self.thick = thick if thick else defaults['thick']\n        self.width = width # if None: calculate width automatically\n        self.align = align if align else defaults.get('align', 'left')\n        \n\ndef cv2_typeset_text(data, lines, loc, between = ' ', string_spacing = 0, line_spacing = 0, wrap = False):\n    '''Typesets mutliple strings on multiple lines of text, where each string may have its own formatting.\n\n    Given:\n    data: as in cv2.putText\n    loc: as in cv2.putText\n    lines: list of lists of FormattedString objects, may be modified by this function!\n    between: what to insert between each string on each line, ala str.join\n    string_spacing: extra spacing to insert between strings on a line\n    line_spacing: extra spacing to insert between lines\n    wrap: if true, wraps words to next line\n\n    Returns:\n    locy: new y location = loc[1] + y-offset resulting from lines of text\n    '''\n\n    data_width = data.shape[1]\n\n    #lines_modified = False\n    #lines = lines_in    # will be deepcopied if modification is needed later\n\n    if isinstance(lines, FormattedString):\n        lines = [lines]\n    assert isinstance(lines, list), 'lines must be a list of lines or list of FormattedString objects or a single FormattedString object'\n    if len(lines) == 0:\n        return loc[1]\n    if not isinstance(lines[0], list):\n        # If a single line of text is given as a list of strings, convert to multiline format\n        lines = [lines]\n    \n    locy = loc[1]\n\n    line_num = 0\n    while line_num < len(lines):\n        line = lines[line_num]\n        maxy = 0\n        locx = loc[0]\n        for ii,fs in enumerate(line):\n            last_on_line = (ii == len(line) - 1)\n            if not last_on_line:\n                fs.string += between\n            boxsize, _ = cv2.getTextSize(fs.string, fs.face, fs.fsize, fs.thick)\n            if fs.width is not None:\n                if fs.align == 'right':\n                    locx += fs.width - boxsize[0]\n                elif fs.align == 'center':\n                    locx += (fs.width - boxsize[0])/2\n            #print 'right boundary is', locx + boxsize[0], '(%s)' % fs.string\n                    #                print 'HERE'\n            right_edge = locx + boxsize[0]\n            if wrap and ii > 0 and right_edge > data_width:\n                # Wrap rest of line to the next line\n                #if not lines_modified:\n                #    lines = deepcopy(lines_in)\n                #    lines_modified = True\n                new_this_line = line[:ii]\n                new_next_line = line[ii:]\n                lines[line_num] = new_this_line\n                lines.insert(line_num+1, new_next_line)\n                break\n                ###line_num += 1\n                ###continue    \n            cv2.putText(data, fs.string, (locx,locy), fs.face, fs.fsize, fs.clr, fs.thick)\n            maxy = max(maxy, boxsize[1])\n            if fs.width is not None:\n                if fs.align == 'right':\n                    locx += boxsize[0]\n                elif fs.align == 'left':\n                    locx += fs.width\n                elif fs.align == 'center':\n                    locx += fs.width - (fs.width - boxsize[0])/2\n            else:\n                locx += boxsize[0]\n            locx += string_spacing\n        line_num += 1\n        locy += maxy + line_spacing\n        \n    return locy\n\n\n\ndef saveimage(filename, im):\n    '''Saves an image with pixel values in [0,1]'''\n    #matplotlib.image.imsave(filename, im)\n    if len(im.shape) == 3:\n        # Reverse RGB to OpenCV BGR order for color images\n        cv2.imwrite(filename, 255*im[:,:,::-1])\n    else:\n        cv2.imwrite(filename, 255*im)\n\n\n\ndef saveimagesc(filename, im):\n    saveimage(filename, norm01(im))\n\n\n\ndef saveimagescc(filename, im, center):\n    saveimage(filename, norm01c(im, center))\n"
        },
        {
          "name": "input_fetcher.py",
          "type": "blob",
          "size": 10.470703125,
          "content": "import os\nimport cv2\nimport re\nimport time\nfrom threading import RLock\nimport numpy as np\n\nfrom codependent_thread import CodependentThread\nfrom image_misc import cv2_imshow_rgb, cv2_read_file_rgb, read_cam_frame, crop_to_square\nfrom misc import tsplit\n\n\nclass InputImageFetcher(CodependentThread):\n    '''Fetches images from a webcam or loads from a directory.'''\n    \n    def __init__(self, settings):\n        CodependentThread.__init__(self, settings.input_updater_heartbeat_required)\n        self.daemon = True\n        self.lock = RLock()\n        self.quit = False\n        self.latest_frame_idx = -1\n        self.latest_frame_data = None\n        self.latest_frame_is_from_cam = False\n\n        # True for loading from file, False for loading from camera\n        self.static_file_mode = True\n        self.settings = settings\n\n        # True for streching the image, False for cropping largest square\n        self.static_file_stretch_mode = self.settings.static_file_stretch_mode\n        \n        # Cam input\n        self.capture_device = settings.input_updater_capture_device\n        self.no_cam_present = (self.capture_device is None)     # Disable all cam functionality\n        self.bound_cap_device = None\n        self.sleep_after_read_frame = settings.input_updater_sleep_after_read_frame\n        self.latest_cam_frame = None\n        self.freeze_cam = False\n\n        # Static file input\n\n        # latest image filename selected, used to avoid reloading\n        self.latest_static_filename = None\n\n        # latest loaded image frame, holds the pixels and used to force reloading\n        self.latest_static_frame = None\n\n        # keeps current index of loaded file, doesn't seem important\n        self.static_file_idx = None\n\n        # contains the requested number of increaments for file index\n        self.static_file_idx_increment = 0\n        \n    def bind_camera(self):\n        # Due to OpenCV limitations, this should be called from the main thread\n        print 'InputImageFetcher: bind_camera starting'\n        if self.no_cam_present:\n            print 'InputImageFetcher: skipping camera bind (device: None)'\n        else:\n            self.bound_cap_device = cv2.VideoCapture(self.capture_device)\n            if self.bound_cap_device.isOpened():\n                print 'InputImageFetcher: capture device %s is open' % self.capture_device\n            else:\n                print '\\n\\nWARNING: InputImageFetcher: capture device %s failed to open! Camera will not be available!\\n\\n' % self.capture_device\n                self.bound_cap_device = None\n                self.no_cam_present = True\n        print 'InputImageFetcher: bind_camera finished'\n\n    def free_camera(self):\n        # Due to OpenCV limitations, this should be called from the main thread\n        if self.no_cam_present:\n            print 'InputImageFetcher: skipping camera free (device: None)'\n        else:\n            print 'InputImageFetcher: freeing camera'\n            del self.bound_cap_device  # free the camera\n            self.bound_cap_device = None\n            print 'InputImageFetcher: camera freed'\n\n    def set_mode_static(self):\n        with self.lock:\n            self.static_file_mode = True\n        \n    def set_mode_cam(self):\n        with self.lock:\n            if self.no_cam_present:\n                print 'WARNING: ignoring set_mode_cam, no cam present'\n            else:\n                self.static_file_mode = False\n                assert self.bound_cap_device != None, 'Call bind_camera first'\n        \n    def toggle_input_mode(self):\n        with self.lock:\n            if self.static_file_mode:\n                self.set_mode_cam()\n            else:\n                self.set_mode_static()\n        \n    def set_mode_stretch_on(self):\n        with self.lock:\n            if not self.static_file_stretch_mode:\n                self.static_file_stretch_mode = True\n                self.latest_static_frame = None   # Force reload\n                #self.latest_frame_is_from_cam = True  # Force reload\n        \n    def set_mode_stretch_off(self):\n        with self.lock:\n            if self.static_file_stretch_mode:\n                self.static_file_stretch_mode = False\n                self.latest_static_frame = None   # Force reload\n                #self.latest_frame_is_from_cam = True  # Force reload\n        \n    def toggle_stretch_mode(self):\n        with self.lock:\n            if self.static_file_stretch_mode:\n                self.set_mode_stretch_off()\n            else:\n                self.set_mode_stretch_on()\n        \n    def run(self):\n        while not self.quit and not self.is_timed_out():\n            #start_time = time.time()\n            if self.static_file_mode:\n                self.check_increment_and_load_image()\n            else:\n                if self.freeze_cam and self.latest_cam_frame is not None:\n                    # If static file mode was switched to cam mode but cam is still frozen, we need to push the cam frame again\n                    if not self.latest_frame_is_from_cam:\n                        self._increment_and_set_frame(self.latest_cam_frame, True)\n                else:\n                    frame_full = read_cam_frame(self.bound_cap_device)\n                    #print '====> just read frame', frame_full.shape\n                    frame = crop_to_square(frame_full)\n                    with self.lock:\n                        self.latest_cam_frame = frame\n                        self._increment_and_set_frame(self.latest_cam_frame, True)\n            \n            time.sleep(self.sleep_after_read_frame)\n            #print 'Reading one frame took', time.time() - start_time\n\n        print 'InputImageFetcher: exiting run method'\n        #print 'InputImageFetcher: read', self.read_frames, 'frames'\n\n    def get_frame(self):\n        '''Fetch the latest frame_idx and frame. The idx increments\n        any time the frame data changes. If the idx is < 0, the frame\n        is not valid.\n        '''\n        with self.lock:\n            return (self.latest_frame_idx, self.latest_frame_data)\n\n    def increment_static_file_idx(self, amount = 1):\n        with self.lock:\n            self.static_file_idx_increment += amount\n\n    def _increment_and_set_frame(self, frame, from_cam):\n        assert frame is not None\n        with self.lock:\n            self.latest_frame_idx += 1\n            self.latest_frame_data = frame\n            self.latest_frame_is_from_cam = from_cam\n\n    def get_files_from_directory(self):\n        # returns list of files in requested directory\n\n        available_files = []\n        match_flags = re.IGNORECASE if self.settings.static_files_ignore_case else 0\n        for filename in os.listdir(self.settings.static_files_dir):\n            if re.match(self.settings.static_files_regexp, filename, match_flags):\n                available_files.append(filename)\n\n        return available_files\n\n    def get_files_from_image_list(self):\n        # returns list of files in requested image list file\n\n        available_files = []\n\n        with open(self.settings.static_files_input_file, 'r') as image_list_file:\n            lines = image_list_file.readlines()\n            # take first token from each line\n            available_files = [tsplit(line, True,' ',',','\\t')[0] for line in lines if line.strip() != \"\"]\n\n        return available_files\n\n    def get_files_from_siamese_image_list(self):\n        # returns list of pair files in requested siamese image list file\n\n        available_files = []\n\n        with open(self.settings.static_files_input_file, 'r') as image_list_file:\n            lines = image_list_file.readlines()\n            # take first and second tokens from each line\n            available_files = [(tsplit(line, True, ' ', ',','\\t')[0], tsplit(line, True, ' ', ',','\\t')[1])\n                               for line in lines if line.strip() != \"\"]\n\n        return available_files\n\n    def check_increment_and_load_image(self):\n        with self.lock:\n            if (self.static_file_idx_increment == 0 and\n                self.static_file_idx is not None and\n                not self.latest_frame_is_from_cam and\n                self.latest_static_frame is not None):\n                # Skip if a static frame is already loaded and there is no increment\n                return\n\n            # available_files - local list of files\n            if self.settings.static_files_input_mode == \"directory\":\n                available_files = self.get_files_from_directory()\n            elif self.settings.static_files_input_mode == \"image_list\":\n                available_files = self.get_files_from_image_list()\n            elif self.settings.static_files_input_mode == \"siamese_image_list\":\n                available_files = self.get_files_from_siamese_image_list()\n            else:\n                raise Exception(('Error: setting static_files_input_mode has invalid option (%s)' %\n                                (self.settings.static_files_input_mode) ))\n\n            #print 'Found files:'\n            #for filename in available_files:\n            #    print '   %s' % filename\n            assert len(available_files) != 0, ('Error: No files found in %s matching %s (current working directory is %s)' %\n                                               (self.settings.static_files_dir, self.settings.static_files_regexp, os.getcwd()))\n            if self.static_file_idx is None:\n                self.static_file_idx = 0\n            self.static_file_idx = (self.static_file_idx + self.static_file_idx_increment) % len(available_files)\n            self.static_file_idx_increment = 0\n            if self.latest_static_filename != available_files[self.static_file_idx] or self.latest_static_frame is None:\n                self.latest_static_filename = available_files[self.static_file_idx]\n\n                if self.settings.static_files_input_mode == \"siamese_image_list\":\n                    # loading two images for siamese network\n                    im1 = cv2_read_file_rgb(os.path.join(self.settings.static_files_dir, self.latest_static_filename[0]))\n                    im2 = cv2_read_file_rgb(os.path.join(self.settings.static_files_dir, self.latest_static_filename[1]))\n                    if not self.static_file_stretch_mode:\n                        im1 = crop_to_square(im1)\n                        im2 = crop_to_square(im2)\n\n                    im = (im1,im2)\n\n                else:\n                    im = cv2_read_file_rgb(os.path.join(self.settings.static_files_dir, self.latest_static_filename))\n                    if not self.static_file_stretch_mode:\n                        im = crop_to_square(im)\n\n                self.latest_static_frame = im\n            self._increment_and_set_frame(self.latest_static_frame, False)\n"
        },
        {
          "name": "input_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "keys.py",
          "type": "blob",
          "size": 4.58984375,
          "content": "# Define keys\n\n#class KeyPatten(object):\n#    '''Define a pattern that will be matched against a keycode.\n#    \n#    A KeyPattern is used to determine which key was pressed in\n#    OpenCV. This process is complicated by the fact that different\n#    platforms define different key codes for each key. Further, on\n#    some platforms the value returned by OpenCV is different than that\n#    returned by Python ord(). See the following link for more\n#    information:\n#    https://stackoverflow.com/questions/14494101/using-other-keys-for-the-waitkey-function-of-opencv/20577067#20577067\n#    '''\n#    def __init__(self, code, mask = None):\n#        self.code = code\n#        self.mask = mask\n#        #self.mask = 0xffffffff    # 64 bits. All codes observed so far are < 2**64\n\n\n\n# Larger masks (requiring a more specific pattern) are matched first\nkey_data = []\nfor letter in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789':\n#for letter in 'abefghijklmnopqrstuvwxyzABEFGHIJKLMNOPQRSTUVWXYZ01456789':\n    key_data.append((letter, ord(letter), 0xff))   # Match only lowest byte\n\nkey_data.extend([\n    # Mac (note diff order vs Linux)\n    ('up',         0xf700, 0xffff),\n    ('down',       0xf701, 0xffff),\n    ('left',       0xf702, 0xffff),\n    ('right',      0xf703, 0xffff),\n    ('pgup',       0xf72c, 0xffff),\n    ('pgdn',       0xf72d, 0xffff),\n\n    # Ubuntu US/UK (note diff order vs Mac)\n    ('left',       0xff51, 0xffff),\n    ('up',         0xff52, 0xffff),\n    ('right',      0xff53, 0xffff),\n    ('down',       0xff54, 0xffff),\n\n    # Ubuntu only; modified keys to not produce separate events on\n    # Mac. These are included only so they be ignored without\n    # producing error messages.\n    ('leftshift',  0xffe1, 0xffff),\n    ('rightshift', 0xffe2, 0xffff),\n    ('leftctrl',   0xffe3, 0xffff),\n    ('rightctrl',  0xffe4, 0xffff),\n    ('esc',        27, 0xff),   # Mac\n    ('enter',      13, 0xff),   # Mac\n    ('enter',      10, 0xff),   # Ubuntu with UK keyboard\n    ])\n\nkey_patterns = dict()\n# Store key_patterns by mask in a dict of dicts\n# Eventually, e.g.:\n#   key_patterns[0xff][97] = 'a'\nfor key_datum in key_data:\n    #print key_datum\n    assert len(key_datum) in (2,3), 'Key information should be tuple of length 2 or 3 but it is %s' % repr(key_datum)\n    if len(key_datum) == 3:\n        label, key_code, mask = key_datum\n    else:\n        label, key_code = key_datum\n        mask = 0xffffffff    # 64 bits. All codes observed so far are < 2**64\n    if not mask in key_patterns:\n        key_patterns[mask] = dict()\n    if key_code in key_patterns[mask]:\n        old_label = key_patterns[mask][code]\n        if old_label != label:\n            print 'Warning: key_patterns[%s][%s] old value %s being overwritten with %s' % (mask, key_code, old_label, label)\n    if key_code != (key_code & mask):\n        print 'Warning: key_code %s for key label %s will never trigger using mask %s' % (key_code, label, mask)\n    key_patterns[mask][key_code] = label\n    #if not label in key_patterns[mask]:\n    #    key_patterns[mask][label] = set()\n    #key_patterns[mask][label].add(code)\n\n\n\n#class Key:\n#    up=(63232, 'up')\n#    right=(63235, 'right')\n#    down=(63233, 'down')\n#    left=(63234, 'left')\n#    esc=(27, 'esc')\n#    enter=(13, 'enter')\n#    a =(ord('a'),'a')\n#    b =(ord('b'),'b')\n#    c =(ord('c'),'c')\n#    d =(ord('d'),'d')\n#    e =(ord('e'),'e')\n#    f =(ord('f'),'f')\n#    g =(ord('g'),'g')\n#    h =(ord('h'),'h')\n#    i =(ord('i'),'i')\n#    j =(ord('j'),'j')\n#    k =(ord('k'),'k')\n#    l =(ord('l'),'l')\n#    m =(ord('m'),'m')\n#    n =(ord('n'),'n')\n#    o =(ord('o'),'o')\n#    p =(ord('p'),'p')\n#    q =(ord('q'),'q')\n#    r =(ord('r'),'r')\n#    s =(ord('s'),'s')\n#    t =(ord('t'),'t')\n#    u =(ord('u'),'u')\n#    v =(ord('v'),'v')\n#    w =(ord('w'),'w')\n#    x =(ord('x'),'x')\n#    y =(ord('y'),'y')\n#    z =(ord('z'),'z')\n#    A =(ord('A'),'A')\n#    B =(ord('B'),'B')\n#    C =(ord('C'),'C')\n#    D =(ord('D'),'D')\n#    E =(ord('E'),'E')\n#    F =(ord('F'),'F')\n#    G =(ord('G'),'G')\n#    H =(ord('H'),'H')\n#    I =(ord('I'),'I')\n#    J =(ord('J'),'J')\n#    K =(ord('K'),'K')\n#    L =(ord('L'),'L')\n#    M =(ord('M'),'M')\n#    N =(ord('N'),'N')\n#    O =(ord('O'),'O')\n#    P =(ord('P'),'P')\n#    Q =(ord('Q'),'Q')\n#    R =(ord('R'),'R')\n#    S =(ord('S'),'S')\n#    T =(ord('T'),'T')\n#    U =(ord('U'),'U')\n#    V =(ord('V'),'V')\n#    W =(ord('W'),'W')\n#    X =(ord('X'),'X')\n#    Y =(ord('Y'),'Y')\n#    Z =(ord('Z'),'Z')\n#    n1=(ord('1'),'1')\n#    n2=(ord('2'),'2')\n#    n3=(ord('3'),'3')\n#    n4=(ord('4'),'4')\n#    n5=(ord('5'),'5')\n#    n6=(ord('6'),'6')\n#    n7=(ord('7'),'7')\n#    n8=(ord('8'),'8')\n#    n9=(ord('9'),'9')\n#    n0=(ord('0'),'0')\n"
        },
        {
          "name": "live_vis.py",
          "type": "blob",
          "size": 14.943359375,
          "content": "#! /usr/bin/env python\n\nimport sys\nimport importlib\nfrom collections import OrderedDict\nimport numpy as np\nfrom threading import Lock, RLock, Thread\nimport time\nimport glob\n\ntry:\n    import cv2\nexcept ImportError:\n    print 'Error: Could not import cv2, please install it first.'\n    raise\n\nfrom misc import WithTimer\nfrom image_misc import cv2_imshow_rgb, FormattedString, cv2_typeset_text, to_255\nfrom bindings import bindings\nfrom input_fetcher import InputImageFetcher\n\npane_debug_clr = (255, 64, 64)\n\nclass ImproperlyConfigured(Exception):\n    pass\n\n\n\n\nclass Pane(object):\n    '''Hold info about one window pane (rectangular region within the main window)'''\n\n    def __init__(self, i_begin, j_begin, i_size, j_size):\n        self.i_begin = i_begin\n        self.j_begin = j_begin\n        self.i_size = i_size\n        self.j_size = j_size\n        self.i_end = i_begin + i_size\n        self.j_end = j_begin + j_size\n        self.data = None    # eventually contains a slice of the window buffer\n\n\n\nclass LiveVis(object):\n    '''Runs the demo'''\n\n    def __init__(self, settings):\n        self.settings = settings\n        self.bindings = bindings\n\n        self.app_classes = OrderedDict()\n        self.apps = OrderedDict()\n\n        for module_path, app_name in settings.installed_apps:\n            module = importlib.import_module(module_path)\n            print 'got module', module\n            app_class  = getattr(module, app_name)\n            print 'got app', app_class\n            self.app_classes[app_name] = app_class\n\n        for app_name, app_class in self.app_classes.iteritems():\n            app = app_class(settings, self.bindings)\n            self.apps[app_name] = app\n        self.help_mode = False\n        self.window_name = 'Deep Visualization Toolbox'\n        self.quit = False\n        self.debug_level = 0\n\n        self.debug_pane_defaults = {\n            'face': getattr(cv2, self.settings.help_face),\n            'fsize': self.settings.help_fsize,\n            'clr': pane_debug_clr,\n            'thick': self.settings.help_thick\n        }\n        self.help_pane_defaults = {\n            'face': getattr(cv2, self.settings.help_face),\n            'fsize': self.settings.help_fsize,\n            'clr': to_255(self.settings.help_clr),\n            'thick': self.settings.help_thick\n        }\n\n\n    def init_window(self):\n        cv2.namedWindow(self.window_name)\n        max_i, max_j = 0, 0\n        if len(self.settings.window_panes) == 0:\n            raise ImproperlyConfigured('settings.window_panes is empty.')\n        self.panes = OrderedDict()\n        for pane_name, pane_dimensions in self.settings.window_panes:\n            if len(pane_dimensions) != 4:\n                raise ImproperlyConfigured('pane dimensions should be a tuple of length 4, but it is \"%s\"' % repr(pane_dimensions))\n            i_begin, j_begin, i_size, j_size = pane_dimensions\n            max_i = max(max_i, i_begin + i_size)\n            max_j = max(max_j, j_begin + j_size)\n            if pane_name in self.panes:\n                raise Exception('Duplicate pane name in settings: %s' % pane_name)\n            self.panes[pane_name] = Pane(i_begin, j_begin, i_size, j_size)\n        self.buffer_height = max_i\n        self.buffer_width = max_j\n\n        self.window_buffer = np.tile(np.array(np.array(self.settings.window_background) * 255, 'uint8'),\n                                     (max_i,max_j,1))\n        #print 'BUFFER IS:', self.window_buffer.shape, self.window_buffer.min(), self.window_buffer.max()\n\n        for _,pane in self.panes.iteritems():\n            pane.data = self.window_buffer[pane.i_begin:pane.i_end, pane.j_begin:pane.j_end]\n\n        # Allocate help pane\n        for ll in self.settings.help_pane_loc:\n            assert ll >= 0 and ll <= 1, 'help_pane_loc values should be in [0,1]'\n        self.help_pane = Pane(int(self.settings.help_pane_loc[0]*max_i),\n                              int(self.settings.help_pane_loc[1]*max_j),\n                              int(self.settings.help_pane_loc[2]*max_i),\n                              int(self.settings.help_pane_loc[3]*max_j))\n        self.help_buffer = self.window_buffer.copy() # For rendering help mode\n        self.help_pane.data = self.help_buffer[self.help_pane.i_begin:self.help_pane.i_end, self.help_pane.j_begin:self.help_pane.j_end]\n\n    def run_loop(self):\n        self.quit = False\n        # Setup\n        self.init_window()\n        #cap = cv2.VideoCapture(self.settings.capture_device)\n        self.input_updater = InputImageFetcher(self.settings)\n        self.input_updater.bind_camera()\n        self.input_updater.start()\n\n        heartbeat_functions = [self.input_updater.heartbeat]\n        for app_name, app in self.apps.iteritems():\n            print 'Starting app:', app_name\n            app.start()\n            heartbeat_functions.extend(app.get_heartbeats())\n\n        ii = 0\n        since_keypress = 999\n        since_redraw = 999\n        since_imshow = 0\n        last_render = time.time() - 999\n        latest_frame_idx = None\n        latest_frame_data = None\n        frame_for_apps = None\n        redraw_needed = True    # Force redraw the first time\n        imshow_needed = True\n        while not self.quit:\n            # Call any heartbeats\n            for heartbeat in heartbeat_functions:\n                #print 'Heartbeat: calling', heartbeat\n                heartbeat()\n\n            # Handle key presses\n            keys = []\n            # Collect key presses (multiple if len(range)>1)\n            for cc in range(1):\n                with WithTimer('LiveVis:waitKey', quiet = self.debug_level < 2):\n                    key = cv2.waitKey(self.settings.main_loop_sleep_ms)\n                if key == -1:\n                    break\n                else:\n                    if (key != 255):\n                        keys.append(key)\n                    #print 'Got key:', key\n            now = time.time()\n            #print 'Since last:', now - last_render\n\n            skip_imshow = False\n            #if now - last_render > .05 and since_imshow < 1:\n            #    skip_imshow = True\n\n            if skip_imshow:\n                since_imshow += 1\n            else:\n                since_imshow = 0\n                last_render = now\n\n            #print '                                                         Number of keys:', len(keys)\n            for key in keys:\n                since_keypress = 0\n                #print 'Got Key:', key\n                key,do_redraw = self.handle_key_pre_apps(key)\n                redraw_needed |= do_redraw\n                imshow_needed |= do_redraw\n                for app_name, app in self.apps.iteritems():\n                    with WithTimer('%s:handle_key' % app_name, quiet = self.debug_level < 1):\n                        key = app.handle_key(key, self.panes)\n                key = self.handle_key_post_apps(key)\n                if self.quit:\n                    break\n            for app_name, app in self.apps.iteritems():\n                redraw_needed |= app.redraw_needed()\n\n            # Grab latest frame from input_updater thread\n            fr_idx,fr_data = self.input_updater.get_frame()\n            is_new_frame = (fr_idx != latest_frame_idx and fr_data is not None)\n            if is_new_frame:\n                latest_frame_idx = fr_idx\n                latest_frame_data = fr_data\n                frame_for_apps = fr_data\n\n            if is_new_frame:\n                with WithTimer('LiveVis.display_frame', quiet = self.debug_level < 1):\n                    self.display_frame(latest_frame_data)\n                imshow_needed = True\n\n            do_handle_input = (ii == 0 or\n                               since_keypress >= self.settings.keypress_pause_handle_iterations)\n            if frame_for_apps is not None and do_handle_input:\n                # Pass frame to apps for processing\n                for app_name, app in self.apps.iteritems():\n                    with WithTimer('%s:handle_input' % app_name, quiet = self.debug_level < 1):\n                        app.handle_input(latest_frame_data, self.panes)\n                frame_for_apps = None\n\n            # Tell each app to draw\n            do_redraw = (redraw_needed and\n                         (since_keypress >= self.settings.keypress_pause_redraw_iterations or\n                          since_redraw >= self.settings.redraw_at_least_every))\n            if redraw_needed and do_redraw:\n                for app_name, app in self.apps.iteritems():\n                    with WithTimer('%s:draw' % app_name, quiet = self.debug_level < 1):\n                        imshow_needed |= app.draw(self.panes)\n                redraw_needed = False\n                since_redraw = 0\n\n            # Render buffer\n            if imshow_needed:\n                # Only redraw pane debug if display will be updated\n                if hasattr(self.settings, 'debug_window_panes') and self.settings.debug_window_panes:\n                    for pane_name,pane in self.panes.iteritems():\n                        print pane_name, pane\n                        pane.data[:] = pane.data * .5\n                        line = [FormattedString('%s |' % pane_name, self.debug_pane_defaults),\n                                FormattedString('pos: %d,%d |' % (pane.i_begin, pane.j_begin), self.debug_pane_defaults),\n                                FormattedString('shape: %d,%d' % (pane.i_size, pane.j_size), self.debug_pane_defaults)]\n                        cv2_typeset_text(pane.data, line, (5,20), line_spacing = 5, wrap = True)\n                        pane.data[:1,:] = pane_debug_clr\n                        pane.data[-1:,:] = pane_debug_clr\n                        pane.data[:,:1] = pane_debug_clr\n                        pane.data[:,-1:] = pane_debug_clr\n\n                with WithTimer('LiveVis:imshow', quiet = self.debug_level < 1):\n                    if self.help_mode:\n                        # Copy main buffer to help buffer\n                        self.help_buffer[:] = self.window_buffer[:]\n                        self.draw_help()\n                        cv2_imshow_rgb(self.window_name, self.help_buffer)\n                    else:\n                        cv2_imshow_rgb(self.window_name, self.window_buffer)\n                    imshow_needed = False\n\n            ii += 1\n            since_keypress += 1\n            since_redraw += 1\n            if ii % 2 == 0 and self.settings.print_dots:\n                sys.stdout.write('.')\n            sys.stdout.flush()\n            # Extra sleep just for debugging. In production all main loop sleep should be in cv2.waitKey.\n            #time.sleep(2)\n\n        print '\\n\\nTrying to exit run_loop...'\n        self.input_updater.quit = True\n        self.input_updater.join(.01 + float(self.settings.input_updater_sleep_after_read_frame) * 5)\n        if self.input_updater.is_alive():\n            raise Exception('Could not join self.input_updater thread')\n        else:\n            self.input_updater.free_camera()\n\n        for app_name, app in self.apps.iteritems():\n            print 'Quitting app:', app_name\n            app.quit()\n\n        print 'Input thread joined and apps quit; exiting run_loop.'\n\n    def handle_key_pre_apps(self, key):\n        tag = self.bindings.get_tag(key)\n        if tag == 'freeze_cam':\n            self.input_updater.freeze_cam = not self.input_updater.freeze_cam\n        elif tag == 'toggle_input_mode':\n            self.input_updater.toggle_input_mode()\n        elif tag == 'static_file_increment':\n            if self.input_updater.static_file_mode:\n                self.input_updater.increment_static_file_idx(1)\n            else:\n                self.input_updater.static_file_mode = True\n        elif tag == 'static_file_decrement':\n            if self.input_updater.static_file_mode:\n                self.input_updater.increment_static_file_idx(-1)\n            else:\n                self.input_updater.static_file_mode = True\n        elif tag == 'help_mode':\n            self.help_mode = not self.help_mode\n        elif tag == 'stretch_mode':\n            self.input_updater.toggle_stretch_mode()\n            print 'Stretch mode is now', self.input_updater.static_file_stretch_mode\n        elif tag == 'debug_level':\n            self.debug_level = (self.debug_level + 1) % 3\n            for app_name, app in self.apps.iteritems():\n                app.set_debug(self.debug_level)\n        else:\n            return key, False\n        return None, True\n\n    def handle_key_post_apps(self, key):\n        tag = self.bindings.get_tag(key)\n        if tag == 'quit':\n            self.quit = True\n        elif key == None:\n            pass\n        else:\n            key_label, masked_vals = self.bindings.get_key_label_from_keycode(key, extra_info = True)\n            masked_vals_pp = ', '.join(['%d (%s)' % (mv, hex(mv)) for mv in masked_vals])\n            if key_label is None:\n                print 'Got key code %d (%s), did not match any known key (masked vals tried: %s)' % (key, hex(key), masked_vals_pp)\n            elif tag is None:\n                print 'Got key code %d (%s), matched key \"%s\", but key is not bound to any function' % (key, hex(key), key_label)\n            else:\n                print 'Got key code %d (%s), matched key \"%s\", bound to \"%s\", but nobody handled \"%s\"' % (\n                    key, hex(key), key_label, tag, tag)\n\n    def display_frame(self, frame):\n        if self.settings.static_files_input_mode == \"siamese_image_list\":\n            frame1 = frame[0]\n            frame2 = frame[1]\n            full_pane_shape = self.panes['input'].data.shape[:2][::-1]\n            half_pane_shape = (full_pane_shape[0] / 2, full_pane_shape[1])\n            frame_disp1 = cv2.resize(frame1[:], half_pane_shape)\n            frame_disp2 = cv2.resize(frame2[:], half_pane_shape)\n            frame_disp = np.concatenate((frame_disp1, frame_disp2), axis=1)\n\n        else:\n            frame_disp = cv2.resize(frame[:], self.panes['input'].data.shape[:2][::-1])\n\n        self.panes['input'].data[:] = frame_disp\n\n    def draw_help(self):\n        self.help_buffer[:] = self.help_buffer[:] * .7\n        self.help_pane.data[:] = self.help_pane.data[:] * .7\n\n        loc = self.settings.help_loc[::-1]   # Reverse to OpenCV c,r order\n        defaults = self.help_pane_defaults\n        lines = []\n        lines.append([FormattedString('~ ~ ~ Deep Visualization Toolbox ~ ~ ~', defaults, align='center', width=self.help_pane.j_size)])\n        lines.append([FormattedString('', defaults)])\n        lines.append([FormattedString('Base keys', defaults)])\n\n        for tag in ('help_mode', 'freeze_cam', 'toggle_input_mode', 'static_file_increment', 'static_file_decrement', 'stretch_mode', 'quit'):\n            key_strings, help_string = self.bindings.get_key_help(tag)\n            label = '%10s:' % (','.join(key_strings))\n            lines.append([FormattedString(label, defaults, width=120, align='right'),\n                          FormattedString(help_string, defaults)])\n\n        locy = cv2_typeset_text(self.help_pane.data, lines, loc,\n                                line_spacing = self.settings.help_line_spacing)\n\n        for app_name, app in self.apps.iteritems():\n            locy = app.draw_help(self.help_pane, locy)\n\n\n\nif __name__ == '__main__':\n    print 'You probably want to run ./run_toolbox.py instead.'\n"
        },
        {
          "name": "misc.py",
          "type": "blob",
          "size": 1.5185546875,
          "content": "#! /usr/bin/env python\n\nimport os\nimport time\nimport errno\nimport re\n\n\nclass WithTimer:\n    def __init__(self, title = '', quiet = False):\n        self.title = title\n        self.quiet = quiet\n        \n    def elapsed(self):\n        return time.time() - self.wall, time.clock() - self.proc\n\n    def enter(self):\n        '''Manually trigger enter'''\n        self.__enter__()\n    \n    def __enter__(self):\n        self.proc = time.clock()\n        self.wall = time.time()\n        return self\n        \n    def __exit__(self, *args):\n        if not self.quiet:\n            titlestr = (' ' + self.title) if self.title else ''\n            print 'Elapsed%s: wall: %.06f, sys: %.06f' % ((titlestr,) + self.elapsed())\n\n\n\ndef mkdir_p(path):\n    # From https://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n\n\ndef combine_dicts(dicts_tuple):\n    '''Combines multiple dictionaries into one by adding a prefix to keys'''\n    ret = {}\n    for prefix,dictionary in dicts_tuple:\n        for key in dictionary.keys():\n            ret['%s%s' % (prefix, key)] = dictionary[key]\n    return ret\n\n\ndef tsplit(string, no_empty_strings, *delimiters):\n    # split string using multiple delimiters\n\n    pattern = '|'.join(map(re.escape, delimiters))\n    strings = re.split(pattern, string)\n    if no_empty_strings:\n        strings = filter(None, strings)\n\n    return strings"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "numpy_cache.py",
          "type": "blob",
          "size": 1.685546875,
          "content": "import numpy as np\nfrom collections import OrderedDict\nfrom threading import RLock\n\nclass FIFOLimitedArrayCache(object):\n    '''Threadsafe cache that stores numpy arrays (or any other object that\n    defines obj.nbytes) and limits total memory. Items are ejected, if\n    necessary, in the same order in which they were added.\n    '''\n\n    def __init__(self, max_bytes = 1e7):\n        self._store = OrderedDict()\n        self._store_bytes = 0\n        self._max_bytes = max_bytes\n        self._lock = RLock()\n        \n    def get(self, key, default = None):\n        with self._lock:\n            if key in self._store:\n                return self._store[key]\n            else:\n                return default\n\n    def set(self, key, val):\n        with self._lock:\n            if key in self._store:\n                self._store_bytes -= self._store[key].nbytes\n            self._store[key] = val\n            self._store_bytes += self._store[key].nbytes\n            self._trim()\n\n    def _trim(self):\n        while len(self._store) > 0 and self._store_bytes > self._max_bytes:\n            key,val = self._store.popitem(last = False)\n            self._store_bytes -= val.nbytes\n        \n    def delete(self, key, raise_if_missing = False):\n        with self._lock:\n            if key in self._store:\n                self._store_bytes -= val.nbytes\n                del self._store[key]\n            elif raise_if_missing:\n                raise Exception('key %s not found in cache' % repr(key))\n\n    def get_size(self):\n        return self._store_bytes\n\n    def __str__(self):\n        with self._lock:\n            return 'FIFOLimitedArrayCache<%d items, bytes used/max %g/%g >' % (len(self._store), self._store_bytes, self._max_bytes)\n"
        },
        {
          "name": "optimize",
          "type": "tree",
          "content": null
        },
        {
          "name": "optimize_image.py",
          "type": "blob",
          "size": 12.927734375,
          "content": "#! /usr/bin/env python\n\nimport os\nimport sys\nimport argparse\nimport numpy as np\n\nimport settings\nfrom optimize.gradient_optimizer import GradientOptimizer, FindParams\nfrom caffevis.caffevis_helper import check_force_backward_true, read_label_file\n\nLR_POLICY_CHOICES = ('constant', 'progress', 'progress01')\n\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description='Script to find, with or without regularization, images that cause high or low activations of specific neurons in a network via numerical optimization. Settings are read from settings.py, overridden in settings_local.py, and may be further overridden on the command line.',\n                                     formatter_class=lambda prog: argparse.ArgumentDefaultsHelpFormatter(prog, width=100)\n    )\n\n    # Network and data options\n    parser.add_argument('--caffe-root', type = str, default = settings.caffevis_caffe_root,\n                        help = 'Path to caffe root directory.')\n    parser.add_argument('--deploy-proto', type = str, default = settings.caffevis_deploy_prototxt,\n                        help = 'Path to caffe network prototxt.')\n    parser.add_argument('--net-weights', type = str, default = settings.caffevis_network_weights,\n                        help = 'Path to caffe network weights.')\n    parser.add_argument('--mean', type = str, default = repr(settings.caffevis_data_mean),\n                        help = '''Mean. The mean may be None, a tuple of one mean value per channel, or a string specifying the path to a mean image to load. Because of the multiple datatypes supported, this argument must be specified as a string that evaluates to a valid Python object. For example: \"None\", \"(10,20,30)\", and \"'mean.npy'\" are all valid values. Note that to specify a string path to a mean file, it must be passed with quotes, which usually entails passing it with double quotes in the shell! Alternately, just provide the mean in settings_local.py.''')\n    parser.add_argument('--channel-swap-to-rgb', type = str, default = '(2,1,0)',\n                        help = 'Permutation to apply to channels to change to RGB space for plotting. Hint: (0,1,2) if your network is trained for RGB, (2,1,0) if it is trained for BGR.')\n    parser.add_argument('--data-size', type = str, default = '(227,227)',\n                        help = 'Size of network input.')\n\n    #### FindParams\n\n    # Where to start\n    parser.add_argument('--start-at', type = str, default = 'mean_plus_rand', choices = ('mean_plus_rand', 'randu', 'mean'),\n                        help = 'How to generate x0, the initial point used in optimization.')\n    parser.add_argument('--rand-seed', type = int, default = 0,\n                        help = 'Random seed used for generating the start-at image (use different seeds to generate different images).')\n\n    # What to optimize\n    parser.add_argument('--push-layer', type = str, default = 'fc8',\n                        help = 'Name of layer that contains the desired neuron whose value is optimized.')\n    parser.add_argument('--push-channel', type = int, default = '130',\n                        help = 'Channel number for desired neuron whose value is optimized (channel for conv, neuron index for FC).')\n    parser.add_argument('--push-spatial', type = str, default = 'None',\n                        help = 'Which spatial location to push for conv layers. For FC layers, set this to None. For conv layers, set it to a tuple, e.g. when using `--push-layer conv5` on AlexNet, --push-spatial (6,6) will maximize the center unit of the 13x13 spatial grid.')\n    parser.add_argument('--push-dir', type = float, default = 1,\n                        help = 'Which direction to push the activation of the selected neuron, that is, the value used to begin backprop. For example, use 1 to maximize the selected neuron activation and  -1 to minimize it.')\n\n    # Use regularization?\n    parser.add_argument('--decay', type = float, default = 0,\n                        help = 'Amount of L2 decay to use.')\n    parser.add_argument('--blur-radius', type = float, default = 0,\n                        help = 'Radius in pixels of blur to apply after each BLUR_EVERY steps. If 0, perform no blurring. Blur sizes between 0 and 0.3 work poorly.')\n    parser.add_argument('--blur-every', type = int, default = 0,\n                        help = 'Blur every BLUR_EVERY steps. If 0, perform no blurring.')\n    parser.add_argument('--small-val-percentile', type = float, default = 0,\n                        help = 'Induce sparsity by setting pixels with absolute value under SMALL_VAL_PERCENTILE percentile to 0. Not discussed in paper. 0 to disable.')\n    parser.add_argument('--small-norm-percentile', type = float, default = 0,\n                        help = 'Induce sparsity by setting pixels with norm under SMALL_NORM_PERCENTILE percentile to 0. \\\\theta_{n_pct} from the paper. 0 to disable.')\n    parser.add_argument('--px-benefit-percentile', type = float, default = 0,\n                        help = 'Induce sparsity by setting pixels with contribution under PX_BENEFIT_PERCENTILE percentile to 0. Mentioned briefly in paper but not used. 0 to disable.')\n    parser.add_argument('--px-abs-benefit-percentile', type = float, default = 0,\n                        help = 'Induce sparsity by setting pixels with contribution under PX_BENEFIT_PERCENTILE percentile to 0. \\\\theta_{c_pct} from the paper. 0 to disable.')\n\n    # How much to optimize\n    parser.add_argument('--lr-policy', type = str, default = 'constant', choices = LR_POLICY_CHOICES,\n                        help = 'Learning rate policy. See description in lr-params.')\n    parser.add_argument('--lr-params', type = str, default = '{\"lr\": 1}',\n                        help = 'Learning rate params, specified as a string that evalutes to a Python dict. Params that must be provided dependon which lr-policy is selected. The \"constant\" policy requires the \"lr\" key and uses the constant given learning rate. The \"progress\" policy requires the \"max_lr\" and \"desired_prog\" keys and scales the learning rate such that the objective function will change by an amount equal to DESIRED_PROG under a linear objective assumption, except the LR is limited to MAX_LR. The \"progress01\" policy requires the \"max_lr\", \"early_prog\", and \"late_prog_mult\" keys and is tuned for optimizing neurons with outputs in the [0,1] range, e.g. neurons on a softmax layer. Under this policy optimization slows down as the output approaches 1 (see code for details).')\n    parser.add_argument('--max-iter', type = int, default = 500,\n                        help = 'Number of iterations of the optimization loop.')\n\n    # Where to save results\n    parser.add_argument('--output-prefix', type = str, default = 'optimize_results/opt',\n                        help = 'Output path and filename prefix (default: optimize_results/opt)')\n    parser.add_argument('--output-template', type = str, default = '%(p.push_layer)s_%(p.push_channel)04d_%(p.rand_seed)d',\n                        help = 'Output filename template; see code for details (default: \"%%(p.push_layer)s_%%(p.push_channel)04d_%%(p.rand_seed)d\"). '\n                        'The default output-prefix and output-template produce filenames like \"optimize_results/opt_prob_0278_0_best_X.jpg\"')\n    parser.add_argument('--brave', action = 'store_true', help = 'Allow overwriting existing results files. Default: off, i.e. cowardly refuse to overwrite existing files.')\n    parser.add_argument('--skipbig', action = 'store_true', help = 'Skip outputting large *info_big.pkl files (contains pickled version of x0, last x, best x, first x that attained max on the specified layer.')\n\n    return parser\n\n\n\ndef parse_and_validate_lr_params(parser, lr_policy, lr_params):\n    assert lr_policy in LR_POLICY_CHOICES\n\n    try:\n        lr_params = eval(lr_params)\n    except (SyntaxError,NameError) as _:\n        err = 'Tried to parse the following lr_params value\\n%s\\nas a Python expression, but it failed. lr_params should evaluate to a valid Python dict.' % lr_params\n        parser.error(err)\n\n    if lr_policy == 'constant':\n        if not 'lr' in lr_params:\n            parser.error('Expected lr_params to be dict with at least \"lr\" key, but dict is %s' % repr(lr_params))\n    elif lr_policy == 'progress':\n        if not ('max_lr' in lr_params and 'desired_prog' in lr_params):\n            parser.error('Expected lr_params to be dict with at least \"max_lr\" and \"desired_prog\" keys, but dict is %s' % repr(lr_params))\n    elif lr_policy == 'progress01':\n        if not ('max_lr' in lr_params and 'early_prog' in lr_params and 'late_prog_mult' in lr_params):\n            parser.error('Expected lr_params to be dict with at least \"max_lr\", \"early_prog\", and \"late_prog_mult\" keys, but dict is %s' % repr(lr_params))\n\n    return lr_params\n\n\n\ndef parse_and_validate_push_spatial(parser, push_spatial):\n    '''Returns tuple of length 2.'''\n    try:\n        push_spatial = eval(push_spatial)\n    except (SyntaxError,NameError) as _:\n        err = 'Tried to parse the following push_spatial value\\n%s\\nas a Python expression, but it failed. push_spatial should be a valid Python expression.' % push_spatial\n        parser.error(err)\n\n    if push_spatial == None:\n        push_spatial = (0,0)    # Convert to tuple format\n    elif isinstance(push_spatial, tuple) and len(push_spatial) == 2:\n        pass\n    else:\n        err = 'push_spatial should be None or a valid tuple of indices of length 2, but it is: %s' % push_spatial\n        parser.error(err)\n\n    return push_spatial\n\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n    \n    # Finish parsing args\n    channel_swap_to_rgb = eval(args.channel_swap_to_rgb)\n    assert isinstance(channel_swap_to_rgb, tuple) and len(channel_swap_to_rgb) > 0, 'channel_swap_to_rgb should be a tuple'\n    data_size = eval(args.data_size)\n    assert isinstance(data_size, tuple) and len(data_size) == 2, 'data_size should be a length 2 tuple'\n    #channel_swap_inv = tuple([net_channel_swap.index(ii) for ii in range(len(net_channel_swap))])\n\n    lr_params = parse_and_validate_lr_params(parser, args.lr_policy, args.lr_params)\n    push_spatial = parse_and_validate_push_spatial(parser, args.push_spatial)\n    \n    # Load mean\n    data_mean = eval(args.mean)\n\n    if isinstance(data_mean, basestring):\n        # If the mean is given as a filename, load the file\n        try:\n            data_mean = np.load(data_mean)\n        except IOError:\n            print '\\n\\nCound not load mean file:', data_mean\n            print 'To fetch a default model and mean file, use:\\n'\n            print '  $ cd models/caffenet-yos/'\n            print '  $ cp ./fetch.sh\\n\\n'\n            print 'Or to use your own mean, change caffevis_data_mean in settings_local.py or override by running with `--mean MEAN_FILE` (see --help).\\n'\n            raise\n        # Crop center region (e.g. 227x227) if mean is larger (e.g. 256x256)\n        excess_h = data_mean.shape[1] - data_size[0]\n        excess_w = data_mean.shape[2] - data_size[1]\n        assert excess_h >= 0 and excess_w >= 0, 'mean should be at least as large as %s' % repr(data_size)\n        data_mean = data_mean[:, (excess_h/2):(excess_h/2+data_size[0]), (excess_w/2):(excess_w/2+data_size[1])]\n    elif data_mean is None:\n        pass\n    else:\n        # The mean has been given as a value or a tuple of values\n        data_mean = np.array(data_mean)\n        # Promote to shape C,1,1\n        while len(data_mean.shape) < 3:\n            data_mean = np.expand_dims(data_mean, -1)\n\n    print 'Using mean:', repr(data_mean)\n            \n    # Load network\n    sys.path.insert(0, os.path.join(args.caffe_root, 'python'))\n    import caffe\n    net = caffe.Classifier(\n        args.deploy_proto,\n        args.net_weights,\n        mean = data_mean,\n        raw_scale = 1.0,\n    )\n    check_force_backward_true(settings.caffevis_deploy_prototxt)\n\n    labels = None\n    if settings.caffevis_labels:\n        labels = read_label_file(settings.caffevis_labels)\n\n    optimizer = GradientOptimizer(net, data_mean, labels = labels,\n                                  label_layers = settings.caffevis_label_layers,\n                                  channel_swap_to_rgb = channel_swap_to_rgb)\n    \n    params = FindParams(\n        start_at = args.start_at,\n        rand_seed = args.rand_seed,\n        push_layer = args.push_layer,\n        push_channel = args.push_channel,\n        push_spatial = push_spatial,\n        push_dir = args.push_dir,\n        decay = args.decay,\n        blur_radius = args.blur_radius,\n        blur_every = args.blur_every,\n        small_val_percentile = args.small_val_percentile,\n        small_norm_percentile = args.small_norm_percentile,\n        px_benefit_percentile = args.px_benefit_percentile,\n        px_abs_benefit_percentile = args.px_abs_benefit_percentile,\n        lr_policy = args.lr_policy,\n        lr_params = lr_params,\n        max_iter = args.max_iter,\n    )\n\n    prefix_template = '%s_%s_' % (args.output_prefix, args.output_template)\n    im = optimizer.run_optimize(params, prefix_template = prefix_template,\n                                brave = args.brave, skipbig = args.skipbig)\n\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "run_toolbox.py",
          "type": "blob",
          "size": 0.98828125,
          "content": "#! /usr/bin/env python\n\nimport os\nfrom live_vis import LiveVis\nfrom bindings import bindings\ntry:\n    import settings\nexcept:\n    print '\\nError importing settings.py. Check the error message below for more information.'\n    print \"If you haven't already, you'll want to copy one of the settings_local.template-*.py files\"\n    print 'to settings_local.py and edit it to point to your caffe checkout. E.g. via:'\n    print\n    print '  $ cp models/caffenet-yos/settings_local.template-caffenet-yos.py settings_local.py'\n    print '  $ < edit settings_local.py >\\n'\n    raise\n\nif not os.path.exists(settings.caffevis_caffe_root):\n    raise Exception('ERROR: Set caffevis_caffe_root in settings.py first.')\n\n\n\ndef main():\n    lv = LiveVis(settings)\n\n    help_keys, _ = bindings.get_key_help('help_mode')\n    quit_keys, _ = bindings.get_key_help('quit')\n    print '\\n\\nRunning toolbox. Push %s for help or %s to quit.\\n\\n' % (help_keys[0], quit_keys[0])\n    lv.run_loop()\n\n\n    \nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "settings.py",
          "type": "blob",
          "size": 16.734375,
          "content": "# Settings for Deep Visualization Toolbox\n#\n# Note: Probably don't change anything in this file. To override\n# settings, define them in settings_local.py rather than changing them\n# here.\n\nimport os\nimport sys\n\n# Import local / overridden settings. Turn off creation of settings_local.pyc to avoid stale settings if settings_local.py is removed.\nsys.dont_write_bytecode = True\ntry:\n    from settings_local import *\nexcept ImportError:\n    if not os.path.exists('settings_local.py'):\n        raise Exception('Could not import settings_local. Did you create it from the template? See README and start with:\\n\\n  $ cp models/caffenet-yos/settings_local.template-caffenet-yos.py settings_local.py')\n    else:\n        raise\n# Resume usual pyc creation\nsys.dont_write_bytecode = False\n\n\n\n####################################\n#\n#  General settings\n#\n####################################\n\n# Which device to use for webcam input. On Mac the default device, 0,\n# works for builtin camera or external USB webcam, if plugged in. If\n# you have multiple cameras, you might need to update this value. To\n# disable webcam input, set to None.\ninput_updater_capture_device = locals().get('input_updater_capture_device', 0)\n\n# How long to sleep in the input reading thread after reading a frame from the camera\ninput_updater_sleep_after_read_frame = locals().get('input_updater_sleep_after_read_frame', 1.0/20)\n\n# Input updater thread die after this many seconds without a heartbeat. Useful during debugging to avoid other threads running after main thread has crashed.\ninput_updater_heartbeat_required = locals().get('input_updater_heartbeat_required', 15.0)\n\n# How long to sleep while waiting for key presses and redraws. Recommendation: 1 (min: 1)\nmain_loop_sleep_ms = locals().get('main_loop_sleep_ms', 1)\n\n# Whether or not to print a \".\" every second time through the main loop to visualize the loop rate\nprint_dots = locals().get('print_dots', False)\n\n\n\n####################################\n#\n#  Window pane layout and colors/fonts\n#\n####################################\n\n# Show border for each panel and annotate each with its name. Useful\n# for debugging window_panes arrangement.\ndebug_window_panes = locals().get('debug_window_panes', False)\n\n# The window panes available and their layout is determined by the\n# \"window_panes\" variable. By default all panes are enabled with a\n# standard size. This setting will often be overridden on a per-model\n# basis, e.g. if the model does not have pre-computed jpgvis\n# information, the caffevis_jpgvis pane can be omitted. For\n# convenience, if the only variable that needs to be overridden is the\n# height of the control panel (to accomodate varying length of layer\n# names), one can simply define control_pane_height. If more\nif 'default_window_panes' in locals():\n    raise Exception('Override window panes in settings_local.py by defining window_panes, not default_window_panes')\ndefault_window_panes = (\n    # (i, j, i_size, j_size)\n    ('input',            (  0,    0,  300,   300)),    # This pane is required to show the input picture\n    ('caffevis_aux',     (300,    0,  300,   300)),\n    ('caffevis_back',    (600,    0,  300,   300)),\n    ('caffevis_status',  (900,    0,   30,  1500)),\n    ('caffevis_control', (  0,  300,   30,   900)),\n    ('caffevis_layers',  ( 30,  300,  870,   900)),\n    ('caffevis_jpgvis',  (  0, 1200,  900,   300)),\n)\nwindow_panes = locals().get('window_panes', default_window_panes)\n\n# Define global_scale as a float to rescale window and all\n# panes. Handy for quickly changing resolution for a different screen.\nglobal_scale = locals().get('global_scale', 1.0)\n\n# Define global_font_size to scale all font sizes by this amount.\nglobal_font_size = locals().get('global_font_size', 1.0)\n\nif global_scale != 1.0:\n    scaled_window_panes = []\n    for wp in window_panes:\n        scaled_window_panes.append([wp[0], [int(val*global_scale) for val in wp[1]]])\n    window_panes = scaled_window_panes\n\n# All window configuation information is now contained in the\n# window_panes variable. Print if desired:\nif debug_window_panes:\n    print 'Final window panes and locations/sizes (i, j, i_size, j_size):'\n    for pane in window_panes:\n        print '  Pane: %s' % repr(pane)\n\nhelp_pane_loc = locals().get('help_pane_loc', (.07, .07, .86, .86))    # as a fraction of main window\nwindow_background = locals().get('window_background', (.2, .2, .2))\nstale_background = locals().get('stale_background',  (.3, .3, .2))\nstatic_files_dir = locals().get('static_files_dir', 'input_images')\nstatic_files_regexp = locals().get('static_files_regexp', '.*\\.(jpg|jpeg|png)$')\nstatic_files_ignore_case = locals().get('static_files_ignore_case', True)\n# True to stretch to square, False to crop to square. (Can change at\n# runtime via 'stretch_mode' key.)\nstatic_file_stretch_mode = locals().get('static_file_stretch_mode', False)\n\n# contains the input mode for reading static images, can be: 'directory', 'image_list', 'siamese_image_list'\nstatic_files_input_mode = locals().get('static_files_input_mode', 'directory')\n\n# contains the file name to read, relevant only when static_files_input_mode is 'image_list' or 'siamese_image_list'\nstatic_files_input_file = locals().get('static_files_input_file', 'images_file_list.txt')\n\n# int, 0+. How many times to go through the main loop after a keypress\n# before resuming handling frames (0 to handle every frame as it\n# arrives). Setting this to a value > 0 can enable more responsive\n# keyboard input even when other settings are tuned to maximize the\n# framerate. Default: 2\nkeypress_pause_handle_iterations = locals().get('keypress_pause_handle_iterations', 2)\n\n# int, 0+. How many times to go through the main loop after a keypress\n# before resuming redraws (0 to redraw every time it is\n# needed). Setting this to a value > 0 can enable more responsive\n# keyboard input even when other settings are tuned to maximize the\n# framerate. Default: 1\nkeypress_pause_redraw_iterations = locals().get('keypress_pause_redraw_iterations', 1)\n\n# int, 1+. Force a redraw even when keys are pressed if there have\n# been this many passes through the main loop without a redraw due to\n# the keypress_pause_redraw_iterations setting combined with many key\n# presses. Default: 3.\nredraw_at_least_every = locals().get('redraw_at_least_every', 3)\n\n# Tuple of tuples describing the file to import and class from it to\n# instantiate for each app to be run. Apps are run and given keys to\n# handle in the order specified.\ndefault_installed_apps = (\n    ('caffevis.app', 'CaffeVisApp'),\n)\ninstalled_apps = locals().get('installed_apps', default_installed_apps)\n\n# Font settings for the help pane. Text is rendered using OpenCV; see\n# http://docs.opencv.org/2.4/modules/core/doc/drawing_functions.html#puttext\n# for information on parameters.\nhelp_face = locals().get('help_face', 'FONT_HERSHEY_COMPLEX_SMALL')\nhelp_loc = locals().get('help_loc', (20,10))   # r,c order\nhelp_line_spacing = locals().get('help_line_spacing', 10)     # extra pixel spacing between lines\nhelp_clr   = locals().get('help_clr', (1,1,1))\nhelp_fsize = locals().get('help_fsize', 1.0 * global_font_size)\nhelp_thick = locals().get('help_thick', 1)\n\n\n\n####################################\n#\n#  Caffevis settings\n#\n####################################\n\n# Whether to use GPU mode (if True) or CPU mode (if False)\ncaffevis_mode_gpu = locals().get('caffevis_mode_gpu', True)\n\n# Data mean, if any, to be subtracted from input image file / webcam\n# image. Specify as string path to file or tuple of one value per\n# channel or None.\ncaffevis_data_mean = locals().get('caffevis_data_mean', None)\n\n# Path to file listing labels in order, one per line, used for the\n# below two features. None to disable.\ncaffevis_labels = locals().get('caffevis_labels', None)\n\n# Which layers have channels/neurons corresponding to the order given\n# in the caffevis_labels file? Annotate these units with label text\n# (when those neurons are selected). None to disable.\ncaffevis_label_layers = locals().get('caffevis_label_layers', None)\n\n# Which layer to use for displaying class output numbers in left pane\n# (when no neurons are selected). None to disable.\ncaffevis_prob_layer = locals().get('caffevis_prob_layer', None)\n\n# String or None. Which directory to load pre-computed per-unit\n# visualizations from, if any. None to disable.\ncaffevis_unit_jpg_dir = locals().get('caffevis_unit_jpg_dir', None)\n\n# List. For which layers should jpgs be loaded for\n# visualization? If a layer name (full name, not prettified) is given\n# here, we will try to load jpgs to visualize each unit. This is used\n# for pattern mode ('s' key by default) and for the right\n# caffevis_jpgvis pane ('9' key by default). Empty list to disable.\ncaffevis_jpgvis_layers = locals().get('caffevis_jpgvis_layers', [])\n\n# Dict specifying string:string mapping. Steal pattern mode and right\n# jpgvis pane visualizations for certain layers (e.g. pool1) from\n# other layers (e.g. conv1). We can do this because\n# optimization/max-act/deconv-of-max results are identical.\ncaffevis_jpgvis_remap = locals().get('caffevis_jpgvis_remap', {})\n\n# Function mapping old name -> new name to modify/prettify/shorten\n# layer names.\ncaffevis_layer_pretty_name_fn = locals().get('caffevis_layer_pretty_name_fn', lambda name: name)\n\n# The CaffeVisApp computes a layout of neurons for the caffevis_layers\n# pane given the aspect ratio in caffevis_layers_aspect_ratio (< 1 for\n# portrait, 1 for square, > 1 for landscape). Default: 1 (square).\ncaffevis_layers_aspect_ratio = locals().get('caffevis_layers_aspect_ratio', 1.0)\n\n# Replace magic '%DVT_ROOT%' string with the root DeepVis Toolbox\n# directory (the location of this settings file)\ndvt_root = os.path.dirname(os.path.abspath(__file__))\nif 'caffevis_deploy_prototxt' in locals():\n    caffevis_deploy_prototxt = caffevis_deploy_prototxt.replace('%DVT_ROOT%', dvt_root)\nif 'caffevis_network_weights' in locals():\n    caffevis_network_weights = caffevis_network_weights.replace('%DVT_ROOT%', dvt_root)\nif isinstance(caffevis_data_mean, basestring):\n    caffevis_data_mean = caffevis_data_mean.replace('%DVT_ROOT%', dvt_root)\nif isinstance(caffevis_labels, basestring):\n    caffevis_labels = caffevis_labels.replace('%DVT_ROOT%', dvt_root)\nif isinstance(caffevis_unit_jpg_dir, basestring):\n    caffevis_unit_jpg_dir = caffevis_unit_jpg_dir.replace('%DVT_ROOT%', dvt_root)\n\n# Pause Caffe forward/backward computation for this many seconds after a keypress. This is to keep the processor free for a brief period after a keypress, which allow the interface to feel much more responsive. After this period has passed, Caffe resumes computation, in CPU mode often occupying all cores. Default: .1\ncaffevis_pause_after_keys = locals().get('caffevis_pause_after_keys', .10)\ncaffevis_frame_wait_sleep = locals().get('caffevis_frame_wait_sleep', .01)\ncaffevis_jpg_load_sleep = locals().get('caffevis_jpg_load_sleep', .01)\n# CaffeProc thread dies after this many seconds without a\n# heartbeat. Useful during debugging to avoid other threads running\n# after main thread has crashed.\ncaffevis_heartbeat_required = locals().get('caffevis_heartbeat_required', 15.0)\n\n# How far to move when using fast left/right/up/down keys\ncaffevis_fast_move_dist = locals().get('caffevis_fast_move_dist', 3)\n# Size of jpg reading cache in bytes (default: 2GB)\n# Note: largest fc6/fc7 images are ~600MB. Cache smaller than this will be painfully slow when using patterns_mode for fc6 and fc7.\n# Cache use when all layers have been loaded is ~1.6GB\ncaffevis_jpg_cache_size  = locals().get('caffevis_jpg_cache_size', 2000*1024**2)\n\ncaffevis_grad_norm_blur_radius = locals().get('caffevis_grad_norm_blur_radius', 4.0)\n\n# Boost display of individual channels. For channel activations in the\n# range [0,1], boost_indiv rescales the activations of that channel\n# such that the new_max = old_max ** -boost_indiv. Thus no-op value =\n# 0.0, and a value of 1.0 means each channel is scaled to use the\n# entire [0,1] range.\ncaffevis_boost_indiv_choices = locals().get('caffevis_boost_indiv_choices', (0, .3, .5, .8, 1))\n# Default boost indiv given as index into caffevis_boost_indiv_choices\ncaffevis_boost_indiv_default_idx = locals().get('caffevis_boost_indiv_default_idx', 0)\n# Boost display of entire layer activation by the given gamma value\n# (for values in [0,1], display_val = old_val ** gamma. No-op value:\n# 1.0)\ncaffevis_boost_gamma_choices = locals().get('caffevis_boost_gamma_choices', (1, .7, .5, .3))\n# Default boost gamma given as index into caffevis_boost_gamma_choices\ncaffevis_boost_gamma_default_idx = locals().get('caffevis_boost_gamma_default_idx', 0)\n# Initially show label predictions or not (toggle with default key '8')\ncaffevis_init_show_label_predictions = locals().get('caffevis_init_show_label_predictions', True)\n# Initially show jpg vis or not (toggle with default key '9')\ncaffevis_init_show_unit_jpgs = locals().get('caffevis_init_show_unit_jpgs', True)\n\n# extra pixel spacing between lines. Default: 4 = not much space / tight layout\ncaffevis_control_line_spacing = locals().get('caffevis_control_line_spacing', 4)\n# Font settings for control pane (list of layers)\ncaffevis_control_face = locals().get('caffevis_control_face', 'FONT_HERSHEY_COMPLEX_SMALL')\ncaffevis_control_loc = locals().get('caffevis_control_loc', (15,5))   # r,c order\ncaffevis_control_clr = locals().get('caffevis_control_clr', (.8,.8,.8))\ncaffevis_control_clr_selected = locals().get('caffevis_control_clr_selected', (1, 1, 1))\ncaffevis_control_clr_cursor = locals().get('caffevis_control_clr_cursor', (.5,1,.5))\ncaffevis_control_clr_bp = locals().get('caffevis_control_clr_bp', (.8, .8, 1))\ncaffevis_control_fsize = locals().get('caffevis_control_fsize', 1.0 * global_font_size)\ncaffevis_control_thick = locals().get('caffevis_control_thick', 1)\ncaffevis_control_thick_selected = locals().get('caffevis_control_thick_selected', 2)\ncaffevis_control_thick_cursor = locals().get('caffevis_control_thick_cursor', 2)\ncaffevis_control_thick_bp = locals().get('caffevis_control_thick_bp', 2)\n\n# Color settings for layer activation pane\ncaffevis_layer_clr_cursor   = locals().get('caffevis_layer_clr_cursor', (.5,1,.5))\ncaffevis_layer_clr_back_background = locals().get('caffevis_layer_clr_back_background', (.2,.2,.5))\ncaffevis_layer_clr_back_sel = locals().get('caffevis_layer_clr_back_sel', (.2,.2,1))\n\n# Font settings for status pane (bottom line)\ncaffevis_status_face = locals().get('caffevis_status_face', 'FONT_HERSHEY_COMPLEX_SMALL')\ncaffevis_status_loc = locals().get('caffevis_status_loc', (15,10))   # r,c order\ncaffevis_status_line_spacing = locals().get('caffevis_status_line_spacing', 5)     # extra pixel spacing between lines\ncaffevis_status_clr = locals().get('caffevis_status_clr', (.8,.8,.8))\ncaffevis_status_fsize = locals().get('caffevis_status_fsize', 1.0 * global_font_size)\ncaffevis_status_thick = locals().get('caffevis_status_thick', 1)\ncaffevis_jpgvis_stack_vert = locals().get('caffevis_jpgvis_stack_vert', True)\n\n# Font settings for class prob output (top 5 classes listed on left)\ncaffevis_class_face = locals().get('caffevis_class_face', 'FONT_HERSHEY_COMPLEX_SMALL')\ncaffevis_class_loc = locals().get('caffevis_class_loc', (20,10))   # r,c order\ncaffevis_class_line_spacing = locals().get('caffevis_class_line_spacing', 10)     # extra pixel spacing between lines\ncaffevis_class_clr_0 = locals().get('caffevis_class_clr_0', (.5,.5,.5))\ncaffevis_class_clr_1 = locals().get('caffevis_class_clr_1', (.5,1,.5))\ncaffevis_class_fsize = locals().get('caffevis_class_fsize', 1.0 * global_font_size)\ncaffevis_class_thick = locals().get('caffevis_class_thick', 1)\n\n# Font settings for label overlay text (shown on layer pane only for caffevis_label_layers layers)\ncaffevis_label_face = locals().get('caffevis_label_face', 'FONT_HERSHEY_COMPLEX_SMALL')\ncaffevis_label_loc = locals().get('caffevis_label_loc', (30,20))   # r,c order\ncaffevis_label_clr = locals().get('caffevis_label_clr', (.8,.8,.8))\ncaffevis_label_fsize = locals().get('caffevis_label_fsize', 1.0 * global_font_size)\ncaffevis_label_thick = locals().get('caffevis_label_thick', 1)\n\n# caffe net parameter - channel swap\ncaffe_net_channel_swap = locals().get('caffe_net_channel_swap', (2,1,0))\n\n\n####################################\n#\n#  A few final sanity checks\n#\n####################################\n\n# Check that required setting have been defined\nbound_locals = locals()\ndef assert_in_settings(setting_name):\n    if not setting_name in bound_locals:\n        raise Exception('The \"%s\" setting is required; be sure to define it in settings_local.py' % setting_name)\n\nassert_in_settings('caffevis_caffe_root')\nassert_in_settings('caffevis_deploy_prototxt')\nassert_in_settings('caffevis_network_weights')\nassert_in_settings('caffevis_data_mean')\n\n# Check that caffe directory actually exists\nif not os.path.exists(caffevis_caffe_root):\n    raise Exception('The Caffe directory specified in settings_local.py, %s, does not exist. Set the caffevis_caffe_root variable in your settings_local.py to the path of your compiled Caffe checkout.' % caffevis_caffe_root)\n"
        },
        {
          "name": "test_keys.py",
          "type": "blob",
          "size": 2.3212890625,
          "content": "#! /usr/bin/env python\n\n# Different platforms give different codes for keys. This can cause\n# bindings to be messed up. To test the keymapping on your system, run\n# this script.\n#\n# On Mac OS X 10.8, this produces:\n# $ ./test_keys.py\n# Click on the picture and then carefully push the following keys:\n#     Press key     j:  got code 106 = j\n#     Press key     k:  got code 107 = k\n#     Press key     J:  got code 74 = J\n#     Press key     K:  got code 75 = K\n#     Press key     1:  got code 49 = 1\n#     Press key     2:  got code 50 = 2\n#     Press key  left:  got code 63234 = left\n#     Press key right:  got code 63235 = right\n#     Press key   esc:  got code 27 = esc\n\nimport sys\nimport cv2\nimport keys\nfrom bindings import bindings\nimg = cv2.imread('input_images/ILSVRC2012_val_00000610.jpg')    # load example image\n\n\n\ndef check_key(key_str):\n    print '  Press key %5s: ' % key_str,\n    sys.stdout.flush()\n    while True:\n        keycode = cv2.waitKey(0)\n        label, masked_vals = bindings.get_key_label_from_keycode(keycode, extra_info = True)\n        if label and ('shift' in label or 'ctrl' in label):\n            print '(ignoring modifier %s)' % label,\n            sys.stdout.flush()\n        else:\n            break\n    masked_vals_pp = ', '.join(['%d (%s)' % (mv, hex(mv)) for mv in masked_vals])\n    if label == key_str:\n        print '  %d (%s) matched %s' % (keycode, hex(keycode), label)\n    elif label is not None:\n        print '* %d (%s) failed, matched key %s (masked vals tried: %s)' % (keycode, hex(keycode), label, masked_vals_pp)\n    else:\n        print '* %d (%s) failed, no match found (masked vals tried: %s)' % (keycode, hex(keycode), masked_vals_pp)\n    #print 'Got:', label\n    #found = False\n    #for k,v in keys.Key.__dict__.iteritems():\n    #    if '__' in k: continue     # Skip __module__, etc.\n    #    num,st = v\n    #    if num == key:\n    #        print 'got code %d = %s' % (num,st)\n    #        found = True\n    #        break\n    #if not found:\n    #    print 'code not found:', key\n\n\n    \ndef main():\n    print 'Click on the picture and then carefully push the following keys:'\n    cv2.imshow('img',img)\n    check_key('j')\n    check_key('k')\n    check_key('J')\n    check_key('K')\n    check_key('1')\n    check_key('2')\n    check_key('left')\n    check_key('right')\n    check_key('esc')\n\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    }
  ]
}