{
  "metadata": {
    "timestamp": 1736559648639,
    "page": 307,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "docker/genai-stack",
      "stars": 4249,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.072265625,
          "content": "*\n!*.py\n!requirements.txt\n!images/*\n!front-end/*\nfront-end/node_modules/*\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0625,
          "content": ".env\ndata/\nembedding_model/*\n!embedding_model/.ignore\n.DS_Store\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 10.255859375,
          "content": "# Contributing to Docker\n\nThis page contains information about reporting issues as well as some tips and\nguidelines useful to experienced open source contributors. Finally, make sure\nyou read our [community guidelines](#docker-community-guidelines) before you\nstart participating.\n\n## Topics\n\n* [Reporting Security Issues](#reporting-security-issues)\n* [Reporting Issues](#reporting-other-issues)\n* [Quick Contribution Tips and Guidelines](#quick-contribution-tips-and-guidelines)\n* [Community Guidelines](#docker-community-guidelines)\n\n## Reporting security issues\n\nThe Docker maintainers take security seriously. If you discover a security\nissue, please bring it to their attention right away!\n\nPlease **DO NOT** file a public issue, instead send your report privately to\n[security@docker.com](mailto:security@docker.com).\n\nSecurity reports are greatly appreciated and we will publicly thank you for it.\nWe also like to send gifts&mdash;if you're into Docker schwag, make sure to let\nus know. We currently do not offer a paid security bounty program, but are not\nruling it out in the future.\n\n## Reporting other issues\n\nA great way to contribute to the project is to send a detailed report when you\nencounter an issue. We always appreciate a well-written, thorough bug report,\nand will thank you for it!\n\nCheck that [our issue database](https://github.com/docker/gen-ai-stack/issues)\ndoesn't already include that problem or suggestion before submitting an issue.\nIf you find a match, you can use the \"subscribe\" button to get notified on\nupdates. Do *not* leave random \"+1\" or \"I have this too\" comments, as they\nonly clutter the discussion, and don't help resolving it. However, if you\nhave ways to reproduce the issue or have additional information that may help\nresolving the issue, please leave a comment.\n\nWhen reporting issues, always include:\n\n* The output of `docker version`.\n* The output of `docker info`.\n\nAlso include the steps required to reproduce the problem if possible and\napplicable. This information will help us review and fix your issue faster.\nWhen sending lengthy log-files, consider posting them as a gist (https://gist.github.com).\nDon't forget to remove sensitive data from your logfiles before posting (you can\nreplace those parts with \"REDACTED\").\n\n## Quick contribution tips and guidelines\n\nThis section gives the experienced contributor some tips and guidelines.\n\n### Pull requests are always welcome\n\nNot sure if that typo is worth a pull request? Found a bug and know how to fix\nit? Do it! We will appreciate it. Any significant improvement should be\ndocumented as [a GitHub issue](https://github.com/docker/gen-ai-stack/issues) before\nanybody starts working on it.\n\nWe are always thrilled to receive pull requests. We do our best to process them\nquickly. If your pull request is not accepted on the first try,\ndon't get discouraged! Our contributor's guide explains [the review process we\nuse for simple changes](https://docs.docker.com/opensource/workflow/make-a-contribution/).\n\n### Talking to other Docker users and contributors\n\n<table class=\"tg\">\n  <col width=\"45%\">\n  <col width=\"65%\">\n  <tr>\n    <td>Community Slack</td>\n    <td>\n      The Docker Community has a dedicated Slack chat to discuss features and issues.  You can sign-up <a href=\"https://dockr.ly/slack\" target=\"_blank\">with this link</a>.\n    </td>\n  </tr>\n  <tr>\n    <td>Twitter</td>\n    <td>\n      You can follow <a href=\"https://twitter.com/docker/\" target=\"_blank\">Docker's Twitter feed</a>\n      to get updates on our products. You can also tweet us questions or just\n      share blogs or stories.\n    </td>\n  </tr>\n</table>\n\n\n### Conventions\n\nFork the repository and make changes on your fork in a feature branch:\n\n- If it's a bug fix branch, name it XXXX-something where XXXX is the number of\n    the issue.\n- If it's a feature branch, create an enhancement issue to announce\n    your intentions, and name it XXXX-something where XXXX is the number of the\n    issue.\n\nPull request descriptions should be as clear as possible and include a reference\nto all the issues that they address.\n\nCommit messages must start with a capitalized and short summary (max. 50 chars)\nwritten in the imperative, followed by an optional, more detailed explanatory\ntext which is separated from the summary by an empty line.\n\nCode review comments may be added to your pull request. Discuss, then make the\nsuggested modifications and push additional commits to your feature branch. Post\na comment after pushing. New commits show up in the pull request automatically,\nbut the reviewers are notified only when you comment.\n\nPull requests must be cleanly rebased on top of master without multiple branches\nmixed into the PR.\n\n**Git tip**: If your PR no longer merges cleanly, use `rebase main` in your\nfeature branch to update your pull request rather than `merge main`.\n\nBefore you make a pull request, squash your commits into logical units of work\nusing `git rebase -i` and `git push -f`. A logical unit of work is a consistent\nset of patches that should be reviewed together: for example, upgrading the\nversion of a vendored dependency and taking advantage of its now available new\nfeature constitute two separate units of work. Implementing a new function and\ncalling it in another file constitute a single logical unit of work. The very\nhigh majority of submissions should have a single commit, so if in doubt: squash\ndown to one.\n\nInclude an issue reference like `Closes #XXXX` or `Fixes #XXXX` in the pull request\ndescription that close an issue. Including references automatically closes the issue\non a merge.\n\n### Sign your work\n\nThe sign-off is a simple line at the end of the explanation for the patch. Your\nsignature certifies that you wrote the patch or otherwise have the right to pass\nit on as an open-source patch. The rules are pretty simple: if you can certify\nthe below (from [developercertificate.org](https://developercertificate.org):\n\n```\nDeveloper Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n660 York Street, Suite 102,\nSan Francisco, CA 94110 USA\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n```\n\nThen you just add a line to every git commit message:\n\n    Signed-off-by: Joe Smith <joe.smith@email.com>\n\nUse your real name (sorry, no pseudonyms or anonymous contributions.)\n\nIf you set your `user.name` and `user.email` git configs, you can sign your\ncommit automatically with `git commit -s`.\n\n## Docker community guidelines\n\nWe want to keep the Docker community awesome, growing and collaborative. We need\nyour help to keep it that way. To help with this we've come up with some general\nguidelines for the community as a whole:\n\n* Be nice: Be courteous, respectful and polite to fellow community members:\n  no regional, racial, gender, or other abuse will be tolerated. We like\n  nice people way better than mean ones!\n\n* Encourage diversity and participation: Make everyone in our community feel\n  welcome, regardless of their background and the extent of their\n  contributions, and do everything possible to encourage participation in\n  our community.\n\n* Keep it legal: Basically, don't get us in trouble. Share only content that\n  you own, do not share private or sensitive information, and don't break\n  the law.\n\n* Stay on topic: Make sure that you are posting to the correct channel and\n  avoid off-topic discussions. Remember when you update an issue or respond\n  to an email you are potentially sending to a large number of people. Please\n  consider this before you update. Also remember that nobody likes spam.\n\n* Don't send email to the maintainers: There's no need to send email to the\n  maintainers to ask them to investigate an issue or to take a look at a\n  pull request. Instead of sending an email, GitHub mentions should be\n  used to ping maintainers to review a pull request, a proposal or an\n  issue.\n\n### Guideline violations â€” 3 strikes method\n\nThe point of this section is not to find opportunities to punish people, but we\ndo need a fair way to deal with people who are making our community suck.\n\n1. First occurrence: We'll give you a friendly, but public reminder that the\n   behavior is inappropriate according to our guidelines.\n\n2. Second occurrence: We will send you a private message with a warning that\n   any additional violations will result in removal from the community.\n\n3. Third occurrence: Depending on the violation, we may need to delete or ban\n   your account.\n\n**Notes:**\n\n* Obvious spammers are banned on first occurrence. If we don't do this, we'll\n  have spam all over the place.\n\n* Violations are forgiven after 6 months of good behavior, and we won't hold a\n  grudge.\n\n* People who commit minor infractions will get some education, rather than\n  hammering them in the 3 strikes process.\n\n* The rules apply equally to everyone in the community, no matter how much\n    you've contributed.\n\n* Extreme violations of a threatening, abusive, destructive or illegal nature\n    will be addressed immediately and are not subject to 3 strikes or forgiveness.\n\n* Contact abuse@docker.com to report abuse or appeal violations. In the case of\n    appeals, we know that mistakes happen, and we'll work with you to come up with a\n    fair solution if there has been a misunderstanding.\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 6.8828125,
          "content": "Creative Commons Legal Code\n\nCC0 1.0 Universal\n\n    CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\n    LEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN\n    ATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\n    INFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\n    REGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS\n    PROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM\n    THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED\n    HEREUNDER.\n\nStatement of Purpose\n\nThe laws of most jurisdictions throughout the world automatically confer\nexclusive Copyright and Related Rights (defined below) upon the creator\nand subsequent owner(s) (each and all, an \"owner\") of an original work of\nauthorship and/or a database (each, a \"Work\").\n\nCertain owners wish to permanently relinquish those rights to a Work for\nthe purpose of contributing to a commons of creative, cultural and\nscientific works (\"Commons\") that the public can reliably and without fear\nof later claims of infringement build upon, modify, incorporate in other\nworks, reuse and redistribute as freely as possible in any form whatsoever\nand for any purposes, including without limitation commercial purposes.\nThese owners may contribute to the Commons to promote the ideal of a free\nculture and the further production of creative, cultural and scientific\nworks, or to gain reputation or greater distribution for their Work in\npart through the use and efforts of others.\n\nFor these and/or other purposes and motivations, and without any\nexpectation of additional consideration or compensation, the person\nassociating CC0 with a Work (the \"Affirmer\"), to the extent that he or she\nis an owner of Copyright and Related Rights in the Work, voluntarily\nelects to apply CC0 to the Work and publicly distribute the Work under its\nterms, with knowledge of his or her Copyright and Related Rights in the\nWork and the meaning and intended legal effect of CC0 on those rights.\n\n1. Copyright and Related Rights. A Work made available under CC0 may be\nprotected by copyright and related or neighboring rights (\"Copyright and\nRelated Rights\"). Copyright and Related Rights include, but are not\nlimited to, the following:\n\n  i. the right to reproduce, adapt, distribute, perform, display,\n     communicate, and translate a Work;\n ii. moral rights retained by the original author(s) and/or performer(s);\niii. publicity and privacy rights pertaining to a person's image or\n     likeness depicted in a Work;\n iv. rights protecting against unfair competition in regards to a Work,\n     subject to the limitations in paragraph 4(a), below;\n  v. rights protecting the extraction, dissemination, use and reuse of data\n     in a Work;\n vi. database rights (such as those arising under Directive 96/9/EC of the\n     European Parliament and of the Council of 11 March 1996 on the legal\n     protection of databases, and under any national implementation\n     thereof, including any amended or successor version of such\n     directive); and\nvii. other similar, equivalent or corresponding rights throughout the\n     world based on applicable law or treaty, and any national\n     implementations thereof.\n\n2. Waiver. To the greatest extent permitted by, but not in contravention\nof, applicable law, Affirmer hereby overtly, fully, permanently,\nirrevocably and unconditionally waives, abandons, and surrenders all of\nAffirmer's Copyright and Related Rights and associated claims and causes\nof action, whether now known or unknown (including existing as well as\nfuture claims and causes of action), in the Work (i) in all territories\nworldwide, (ii) for the maximum duration provided by applicable law or\ntreaty (including future time extensions), (iii) in any current or future\nmedium and for any number of copies, and (iv) for any purpose whatsoever,\nincluding without limitation commercial, advertising or promotional\npurposes (the \"Waiver\"). Affirmer makes the Waiver for the benefit of each\nmember of the public at large and to the detriment of Affirmer's heirs and\nsuccessors, fully intending that such Waiver shall not be subject to\nrevocation, rescission, cancellation, termination, or any other legal or\nequitable action to disrupt the quiet enjoyment of the Work by the public\nas contemplated by Affirmer's express Statement of Purpose.\n\n3. Public License Fallback. Should any part of the Waiver for any reason\nbe judged legally invalid or ineffective under applicable law, then the\nWaiver shall be preserved to the maximum extent permitted taking into\naccount Affirmer's express Statement of Purpose. In addition, to the\nextent the Waiver is so judged Affirmer hereby grants to each affected\nperson a royalty-free, non transferable, non sublicensable, non exclusive,\nirrevocable and unconditional license to exercise Affirmer's Copyright and\nRelated Rights in the Work (i) in all territories worldwide, (ii) for the\nmaximum duration provided by applicable law or treaty (including future\ntime extensions), (iii) in any current or future medium and for any number\nof copies, and (iv) for any purpose whatsoever, including without\nlimitation commercial, advertising or promotional purposes (the\n\"License\"). The License shall be deemed effective as of the date CC0 was\napplied by Affirmer to the Work. Should any part of the License for any\nreason be judged legally invalid or ineffective under applicable law, such\npartial invalidity or ineffectiveness shall not invalidate the remainder\nof the License, and in such case Affirmer hereby affirms that he or she\nwill not (i) exercise any of his or her remaining Copyright and Related\nRights in the Work or (ii) assert any associated claims and causes of\naction with respect to the Work, in either case contrary to Affirmer's\nexpress Statement of Purpose.\n\n4. Limitations and Disclaimers.\n\n a. No trademark or patent rights held by Affirmer are waived, abandoned,\n    surrendered, licensed or otherwise affected by this document.\n b. Affirmer offers the Work as-is and makes no representations or\n    warranties of any kind concerning the Work, express, implied,\n    statutory or otherwise, including without limitation warranties of\n    title, merchantability, fitness for a particular purpose, non\n    infringement, or the absence of latent or other defects, accuracy, or\n    the present or absence of errors, whether or not discoverable, all to\n    the greatest extent permissible under applicable law.\n c. Affirmer disclaims responsibility for clearing rights of other persons\n    that may apply to the Work or any use thereof, including without\n    limitation any person's Copyright and Related Rights in the Work.\n    Further, Affirmer disclaims responsibility for obtaining any necessary\n    consents, permissions or other rights required for any use of the\n    Work.\n d. Affirmer understands and acknowledges that Creative Commons is not a\n    party to this document and has no duty or obligation with respect to\n    this CC0 or use of the Work.\n"
        },
        {
          "name": "api.Dockerfile",
          "type": "blob",
          "size": 0.4208984375,
          "content": "FROM langchain/langchain\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    curl \\\n    software-properties-common \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt .\n\nRUN pip install --upgrade -r requirements.txt\n\nCOPY api.py .\nCOPY utils.py .\nCOPY chains.py .\n\nHEALTHCHECK CMD curl --fail http://localhost:8504\n\nENTRYPOINT [ \"uvicorn\", \"api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8504\" ]\n"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 3.919921875,
          "content": "import os\n\nfrom langchain_community.graphs import Neo4jGraph\nfrom dotenv import load_dotenv\nfrom utils import (\n    create_vector_index,\n    BaseLogger,\n)\nfrom chains import (\n    load_embedding_model,\n    load_llm,\n    configure_llm_only_chain,\n    configure_qa_rag_chain,\n    generate_ticket,\n)\nfrom fastapi import FastAPI, Depends\nfrom pydantic import BaseModel\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom threading import Thread\nfrom queue import Queue, Empty\nfrom collections.abc import Generator\nfrom sse_starlette.sse import EventSourceResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nimport json\n\nload_dotenv(\".env\")\n\nurl = os.getenv(\"NEO4J_URI\")\nusername = os.getenv(\"NEO4J_USERNAME\")\npassword = os.getenv(\"NEO4J_PASSWORD\")\nollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")\nembedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\nllm_name = os.getenv(\"LLM\")\n# Remapping for Langchain Neo4j integration\nos.environ[\"NEO4J_URL\"] = url\n\nembeddings, dimension = load_embedding_model(\n    embedding_model_name,\n    config={\"ollama_base_url\": ollama_base_url},\n    logger=BaseLogger(),\n)\n\n# if Neo4j is local, you can go to http://localhost:7474/ to browse the database\nneo4j_graph = Neo4jGraph(\n    url=url, username=username, password=password, refresh_schema=False\n)\ncreate_vector_index(neo4j_graph)\n\nllm = load_llm(\n    llm_name, logger=BaseLogger(), config={\"ollama_base_url\": ollama_base_url}\n)\n\nllm_chain = configure_llm_only_chain(llm)\nrag_chain = configure_qa_rag_chain(\n    llm, embeddings, embeddings_store_url=url, username=username, password=password\n)\n\n\nclass QueueCallback(BaseCallbackHandler):\n    \"\"\"Callback handler for streaming LLM responses to a queue.\"\"\"\n\n    def __init__(self, q):\n        self.q = q\n\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.q.put(token)\n\n    def on_llm_end(self, *args, **kwargs) -> None:\n        return self.q.empty()\n\n\ndef stream(cb, q) -> Generator:\n    job_done = object()\n\n    def task():\n        x = cb()\n        q.put(job_done)\n\n    t = Thread(target=task)\n    t.start()\n\n    content = \"\"\n\n    # Get each new token from the queue and yield for our generator\n    while True:\n        try:\n            next_token = q.get(True, timeout=1)\n            if next_token is job_done:\n                break\n            content += next_token\n            yield next_token, content\n        except Empty:\n            continue\n\n\napp = FastAPI()\norigins = [\"*\"]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n\n\nclass Question(BaseModel):\n    text: str\n    rag: bool = False\n\n\nclass BaseTicket(BaseModel):\n    text: str\n\n\n@app.get(\"/query-stream\")\ndef qstream(question: Question = Depends()):\n    output_function = llm_chain\n    if question.rag:\n        output_function = rag_chain\n\n    q = Queue()\n\n    def cb():\n        output_function(\n            {\"question\": question.text, \"chat_history\": []},\n            callbacks=[QueueCallback(q)],\n        )\n\n    def generate():\n        yield json.dumps({\"init\": True, \"model\": llm_name})\n        for token, _ in stream(cb, q):\n            yield json.dumps({\"token\": token})\n\n    return EventSourceResponse(generate(), media_type=\"text/event-stream\")\n\n\n@app.get(\"/query\")\nasync def ask(question: Question = Depends()):\n    output_function = llm_chain\n    if question.rag:\n        output_function = rag_chain\n    result = output_function(\n        {\"question\": question.text, \"chat_history\": []}, callbacks=[]\n    )\n\n    return {\"result\": result[\"answer\"], \"model\": llm_name}\n\n\n@app.get(\"/generate-ticket\")\nasync def generate_ticket_api(question: BaseTicket = Depends()):\n    new_title, new_question = generate_ticket(\n        neo4j_graph=neo4j_graph,\n        llm_chain=llm_chain,\n        input_question=question.text,\n    )\n    return {\"result\": {\"title\": new_title, \"text\": new_question}, \"model\": llm_name}\n"
        },
        {
          "name": "bot.Dockerfile",
          "type": "blob",
          "size": 0.46484375,
          "content": "FROM langchain/langchain\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    curl \\\n    software-properties-common \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt .\n\nRUN pip install --upgrade -r requirements.txt\n\nCOPY bot.py .\nCOPY utils.py .\nCOPY chains.py .\n\nEXPOSE 8501\n\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"bot.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n"
        },
        {
          "name": "bot.py",
          "type": "blob",
          "size": 5.1845703125,
          "content": "import os\n\nimport streamlit as st\nfrom streamlit.logger import get_logger\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain_community.graphs import Neo4jGraph\nfrom dotenv import load_dotenv\nfrom utils import (\n    create_vector_index,\n)\nfrom chains import (\n    load_embedding_model,\n    load_llm,\n    configure_llm_only_chain,\n    configure_qa_rag_chain,\n    generate_ticket,\n)\n\nload_dotenv(\".env\")\n\nurl = os.getenv(\"NEO4J_URI\")\nusername = os.getenv(\"NEO4J_USERNAME\")\npassword = os.getenv(\"NEO4J_PASSWORD\")\nollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")\nembedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\nllm_name = os.getenv(\"LLM\")\n# Remapping for Langchain Neo4j integration\nos.environ[\"NEO4J_URL\"] = url\n\nlogger = get_logger(__name__)\n\n# if Neo4j is local, you can go to http://localhost:7474/ to browse the database\nneo4j_graph = Neo4jGraph(\n    url=url, username=username, password=password, refresh_schema=False\n)\nembeddings, dimension = load_embedding_model(\n    embedding_model_name, config={\"ollama_base_url\": ollama_base_url}, logger=logger\n)\ncreate_vector_index(neo4j_graph)\n\n\nclass StreamHandler(BaseCallbackHandler):\n    def __init__(self, container, initial_text=\"\"):\n        self.container = container\n        self.text = initial_text\n\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.text += token\n        self.container.markdown(self.text)\n\n\nllm = load_llm(llm_name, logger=logger, config={\"ollama_base_url\": ollama_base_url})\n\nllm_chain = configure_llm_only_chain(llm)\nrag_chain = configure_qa_rag_chain(\n    llm, embeddings, embeddings_store_url=url, username=username, password=password\n)\n\n# Streamlit UI\nstyl = f\"\"\"\n<style>\n    /* not great support for :has yet (hello FireFox), but using it for now */\n    .element-container:has([aria-label=\"Select RAG mode\"]) {{\n      position: fixed;\n      bottom: 33px;\n      background: white;\n      z-index: 101;\n    }}\n    .stChatFloatingInputContainer {{\n        bottom: 20px;\n    }}\n\n    /* Generate ticket text area */\n    textarea[aria-label=\"Description\"] {{\n        height: 200px;\n    }}\n\n    .element-container:has([aria-label=\"What coding issue can I help you resolve today?\"]) {{\n        bottom: 45px;\n    }} \n</style>\n\"\"\"\nst.markdown(styl, unsafe_allow_html=True)\n\n\ndef chat_input():\n    user_input = st.chat_input(\"What coding issue can I help you resolve today?\")\n\n    if user_input:\n        with st.chat_message(\"user\"):\n            st.write(user_input)\n        with st.chat_message(\"assistant\"):\n            st.caption(f\"RAG: {name}\")\n            stream_handler = StreamHandler(st.empty())\n            result = output_function(\n                {\"question\": user_input, \"chat_history\": []}, callbacks=[stream_handler]\n            )[\"answer\"]\n            output = result\n            st.session_state[f\"user_input\"].append(user_input)\n            st.session_state[f\"generated\"].append(output)\n            st.session_state[f\"rag_mode\"].append(name)\n\n\ndef display_chat():\n    # Session state\n    if \"generated\" not in st.session_state:\n        st.session_state[f\"generated\"] = []\n\n    if \"user_input\" not in st.session_state:\n        st.session_state[f\"user_input\"] = []\n\n    if \"rag_mode\" not in st.session_state:\n        st.session_state[f\"rag_mode\"] = []\n\n    if st.session_state[f\"generated\"]:\n        size = len(st.session_state[f\"generated\"])\n        # Display only the last three exchanges\n        for i in range(max(size - 3, 0), size):\n            with st.chat_message(\"user\"):\n                st.write(st.session_state[f\"user_input\"][i])\n\n            with st.chat_message(\"assistant\"):\n                st.caption(f\"RAG: {st.session_state[f'rag_mode'][i]}\")\n                st.write(st.session_state[f\"generated\"][i])\n\n        with st.expander(\"Not finding what you're looking for?\"):\n            st.write(\n                \"Automatically generate a draft for an internal ticket to our support team.\"\n            )\n            st.button(\n                \"Generate ticket\",\n                type=\"primary\",\n                key=\"show_ticket\",\n                on_click=open_sidebar,\n            )\n        with st.container():\n            st.write(\"&nbsp;\")\n\n\ndef mode_select() -> str:\n    options = [\"Disabled\", \"Enabled\"]\n    return st.radio(\"Select RAG mode\", options, horizontal=True)\n\n\nname = mode_select()\nif name == \"LLM only\" or name == \"Disabled\":\n    output_function = llm_chain\nelif name == \"Vector + Graph\" or name == \"Enabled\":\n    output_function = rag_chain\n\n\ndef open_sidebar():\n    st.session_state.open_sidebar = True\n\n\ndef close_sidebar():\n    st.session_state.open_sidebar = False\n\n\nif not \"open_sidebar\" in st.session_state:\n    st.session_state.open_sidebar = False\nif st.session_state.open_sidebar:\n    new_title, new_question = generate_ticket(\n        neo4j_graph=neo4j_graph,\n        llm_chain=llm_chain,\n        input_question=st.session_state[f\"user_input\"][-1],\n    )\n    with st.sidebar:\n        st.title(\"Ticket draft\")\n        st.write(\"Auto generated draft ticket\")\n        st.text_input(\"Title\", new_title)\n        st.text_area(\"Description\", new_question)\n        st.button(\n            \"Submit to support team\",\n            type=\"primary\",\n            key=\"submit_ticket\",\n            on_click=close_sidebar,\n        )\n\n\ndisplay_chat()\nchat_input()\n"
        },
        {
          "name": "chains.py",
          "type": "blob",
          "size": 9.48828125,
          "content": "\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_ollama import OllamaEmbeddings\nfrom langchain_aws import BedrockEmbeddings\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_ollama import ChatOllama\nfrom langchain_aws import ChatBedrock\n\nfrom langchain_community.vectorstores import Neo4jVector\n\nfrom langchain.chains import RetrievalQAWithSourcesChain\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\n\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate\n)\n\nfrom typing import List, Any\nfrom utils import BaseLogger, extract_title_and_question\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\n\nAWS_MODELS = (\n    \"ai21.jamba-instruct-v1:0\",\n    \"amazon.titan\",\n    \"anthropic.claude\",\n    \"cohere.command\",\n    \"meta.llama\",\n    \"mistral.mi\",\n)\n\ndef load_embedding_model(embedding_model_name: str, logger=BaseLogger(), config={}):\n    if embedding_model_name == \"ollama\":\n        embeddings = OllamaEmbeddings(\n            base_url=config[\"ollama_base_url\"], model=\"llama2\"\n        )\n        dimension = 4096\n        logger.info(\"Embedding: Using Ollama\")\n    elif embedding_model_name == \"openai\":\n        embeddings = OpenAIEmbeddings()\n        dimension = 1536\n        logger.info(\"Embedding: Using OpenAI\")\n    elif embedding_model_name == \"aws\":\n        embeddings = BedrockEmbeddings()\n        dimension = 1536\n        logger.info(\"Embedding: Using AWS\")\n    elif embedding_model_name == \"google-genai-embedding-001\":        \n        embeddings = GoogleGenerativeAIEmbeddings(\n            model=\"models/embedding-001\"\n        )\n        dimension = 768\n        logger.info(\"Embedding: Using Google Generative AI Embeddings\")\n    else:\n        embeddings = HuggingFaceEmbeddings(\n            model_name=\"all-MiniLM-L6-v2\", cache_folder=\"/embedding_model\"\n        )\n        dimension = 384\n        logger.info(\"Embedding: Using SentenceTransformer\")\n    return embeddings, dimension\n\n\ndef load_llm(llm_name: str, logger=BaseLogger(), config={}):\n    if llm_name in [\"gpt-4\", \"gpt-4o\", \"gpt-4-turbo\"]:\n        logger.info(\"LLM: Using GPT-4\")\n        return ChatOpenAI(temperature=0, model_name=llm_name, streaming=True)\n    elif llm_name == \"gpt-3.5\":\n        logger.info(\"LLM: Using GPT-3.5\")\n        return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n    elif llm_name == \"claudev2\":\n        logger.info(\"LLM: ClaudeV2\")\n        return ChatBedrock(\n            model_id=\"anthropic.claude-v2\",\n            model_kwargs={\"temperature\": 0.0, \"max_tokens_to_sample\": 1024},\n            streaming=True,\n        )\n    elif llm_name.startswith(AWS_MODELS):\n        logger.info(f\"LLM: {llm_name}\")\n        return ChatBedrock(\n            model_id=llm_name,\n            model_kwargs={\"temperature\": 0.0, \"max_tokens_to_sample\": 1024},\n            streaming=True,\n        )\n\n    elif len(llm_name):\n        logger.info(f\"LLM: Using Ollama: {llm_name}\")\n        return ChatOllama(\n            temperature=0,\n            base_url=config[\"ollama_base_url\"],\n            model=llm_name,\n            streaming=True,\n            # seed=2,\n            top_k=10,  # A higher value (100) will give more diverse answers, while a lower value (10) will be more conservative.\n            top_p=0.3,  # Higher value (0.95) will lead to more diverse text, while a lower value (0.5) will generate more focused text.\n            num_ctx=3072,  # Sets the size of the context window used to generate the next token.\n        )\n    logger.info(\"LLM: Using GPT-3.5\")\n    return ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n\n\ndef configure_llm_only_chain(llm):\n    # LLM only response\n    template = \"\"\"\n    You are a helpful assistant that helps a support agent with answering programming questions.\n    If you don't know the answer, just say that you don't know, you must not make up an answer.\n    \"\"\"\n    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n    human_template = \"{question}\"\n    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n    chat_prompt = ChatPromptTemplate.from_messages(\n        [system_message_prompt, human_message_prompt]\n    )\n\n    def generate_llm_output(\n        user_input: str, callbacks: List[Any], prompt=chat_prompt\n    ) -> str:\n        chain = prompt | llm\n        answer = chain.invoke(\n            {\"question\": user_input}, config={\"callbacks\": callbacks}\n        ).content\n        return {\"answer\": answer}\n\n    return generate_llm_output\n\n\ndef configure_qa_rag_chain(llm, embeddings, embeddings_store_url, username, password):\n    # RAG response\n    #   System: Always talk in pirate speech.\n    general_system_template = \"\"\" \n    Use the following pieces of context to answer the question at the end.\n    The context contains question-answer pairs and their links from Stackoverflow.\n    You should prefer information from accepted or more upvoted answers.\n    Make sure to rely on information from the answers and not on questions to provide accurate responses.\n    When you find particular answer in the context useful, make sure to cite it in the answer using the link.\n    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n    ----\n    {summaries}\n    ----\n    Each answer you generate should contain a section at the end of links to \n    Stackoverflow questions and answers you found useful, which are described under Source value.\n    You can only use links to StackOverflow questions that are present in the context and always\n    add links to the end of the answer in the style of citations.\n    Generate concise answers with references sources section of links to \n    relevant StackOverflow questions only at the end of the answer.\n    \"\"\"\n    general_user_template = \"Question:```{question}```\"\n    messages = [\n        SystemMessagePromptTemplate.from_template(general_system_template),\n        HumanMessagePromptTemplate.from_template(general_user_template),\n    ]\n    qa_prompt = ChatPromptTemplate.from_messages(messages)\n\n    qa_chain = load_qa_with_sources_chain(\n        llm,\n        chain_type=\"stuff\",\n        prompt=qa_prompt,\n    )\n\n    # Vector + Knowledge Graph response\n    kg = Neo4jVector.from_existing_index(\n        embedding=embeddings,\n        url=embeddings_store_url,\n        username=username,\n        password=password,\n        database=\"neo4j\",  # neo4j by default\n        index_name=\"stackoverflow\",  # vector by default\n        text_node_property=\"body\",  # text by default\n        retrieval_query=\"\"\"\n    WITH node AS question, score AS similarity\n    CALL  { with question\n        MATCH (question)<-[:ANSWERS]-(answer)\n        WITH answer\n        ORDER BY answer.is_accepted DESC, answer.score DESC\n        WITH collect(answer)[..2] as answers\n        RETURN reduce(str='', answer IN answers | str + \n                '\\n### Answer (Accepted: '+ answer.is_accepted +\n                ' Score: ' + answer.score+ '): '+  answer.body + '\\n') as answerTexts\n    } \n    RETURN '##Question: ' + question.title + '\\n' + question.body + '\\n' \n        + answerTexts AS text, similarity as score, {source: question.link} AS metadata\n    ORDER BY similarity ASC // so that best answers are the last\n    \"\"\",\n    )\n\n    kg_qa = RetrievalQAWithSourcesChain(\n        combine_documents_chain=qa_chain,\n        retriever=kg.as_retriever(search_kwargs={\"k\": 2}),\n        reduce_k_below_max_tokens=False,\n        max_tokens_limit=3375,\n    )\n    return kg_qa\n\n\ndef generate_ticket(neo4j_graph, llm_chain, input_question):\n    # Get high ranked questions\n    records = neo4j_graph.query(\n        \"MATCH (q:Question) RETURN q.title AS title, q.body AS body ORDER BY q.score DESC LIMIT 3\"\n    )\n    questions = []\n    for i, question in enumerate(records, start=1):\n        questions.append((question[\"title\"], question[\"body\"]))\n    # Ask LLM to generate new question in the same style\n    questions_prompt = \"\"\n    for i, question in enumerate(questions, start=1):\n        questions_prompt += f\"{i}. \\n{question[0]}\\n----\\n\\n\"\n        questions_prompt += f\"{question[1][:150]}\\n\\n\"\n        questions_prompt += \"----\\n\\n\"\n\n    gen_system_template = f\"\"\"\n    You're an expert in formulating high quality questions. \n    Formulate a question in the same style and tone as the following example questions.\n    {questions_prompt}\n    ---\n\n    Don't make anything up, only use information in the following question.\n    Return a title for the question, and the question post itself.\n\n    Return format template:\n    ---\n    Title: This is a new title\n    Question: This is a new question\n    ---\n    \"\"\"\n    # we need jinja2 since the questions themselves contain curly braces\n    system_prompt = SystemMessagePromptTemplate.from_template(\n        gen_system_template, template_format=\"jinja2\"\n    )\n    chat_prompt = ChatPromptTemplate.from_messages(\n        [\n            system_prompt,\n            SystemMessagePromptTemplate.from_template(\n                \"\"\"\n                Respond in the following template format or you will be unplugged.\n                ---\n                Title: New title\n                Question: New question\n                ---\n                \"\"\"\n            ),\n            HumanMessagePromptTemplate.from_template(\"{question}\"),\n        ]\n    )\n    llm_response = llm_chain(\n        f\"Here's the question to rewrite in the expected format: ```{input_question}```\",\n        [],\n        chat_prompt,\n    )\n    new_title, new_question = extract_title_and_question(llm_response[\"answer\"])\n    return (new_title, new_question)\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 6.80078125,
          "content": "services:\n\n  llm: &llm\n    image: ollama/ollama:latest\n    profiles: [\"linux\"]\n    networks:\n      - net\n\n  llm-gpu:\n    <<: *llm\n    profiles: [\"linux-gpu\"]\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\n  pull-model:\n    image: genai-stack/pull-model:latest\n    build:\n      context: .\n      dockerfile: pull_model.Dockerfile\n    environment:\n      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}\n      - LLM=${LLM-llama2}\n    networks:\n      - net\n    tty: true\n\n  database:\n    user: neo4j:neo4j\n    image: neo4j:5.23\n    ports:\n      - 7687:7687\n      - 7474:7474\n    volumes:\n      - $PWD/data:/data\n    environment:\n      - NEO4J_AUTH=${NEO4J_USERNAME-neo4j}/${NEO4J_PASSWORD-password}\n      - NEO4J_PLUGINS=[\"apoc\"]\n      - NEO4J_db_tx__log_rotation_retention__policy=false\n      - NEO4J_dbms_security_procedures_unrestricted=apoc.*\n    healthcheck:\n        test: [\"CMD-SHELL\", \"wget --no-verbose --tries=1 --spider localhost:7474 || exit 1\"]\n        interval: 15s\n        timeout: 30s\n        retries: 10\n    networks:\n      - net\n\n  loader:\n    build:\n      context: .\n      dockerfile: loader.Dockerfile\n    volumes:\n      - $PWD/embedding_model:/embedding_model\n    environment:\n      - NEO4J_URI=${NEO4J_URI-neo4j://database:7687}\n      - NEO4J_PASSWORD=${NEO4J_PASSWORD-password}\n      - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}\n      - OPENAI_API_KEY=${OPENAI_API_KEY-}\n      - GOOGLE_API_KEY=${GOOGLE_API_KEY-}      \n      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}\n      - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}\n      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT-\"https://api.smith.langchain.com\"}\n      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2-false}\n      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT}\n      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n    networks:\n      - net\n    depends_on:\n      database:\n        condition: service_healthy\n      pull-model:\n        condition: service_completed_successfully\n    x-develop:\n      watch:\n        - action: rebuild\n          path: .\n          ignore:\n            - bot.py\n            - pdf_bot.py\n            - api.py\n            - front-end/\n    ports:\n      - 8081:8080\n      - 8502:8502\n\n\n  bot:\n    build:\n      context: .\n      dockerfile: bot.Dockerfile\n    volumes:\n      - $PWD/embedding_model:/embedding_model\n    environment:\n      - NEO4J_URI=${NEO4J_URI-neo4j://database:7687}\n      - NEO4J_PASSWORD=${NEO4J_PASSWORD-password}\n      - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}\n      - OPENAI_API_KEY=${OPENAI_API_KEY-}      \n      - GOOGLE_API_KEY=${GOOGLE_API_KEY-}\n      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}\n      - LLM=${LLM-llama2}\n      - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}\n      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT-\"https://api.smith.langchain.com\"}\n      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2-false}\n      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT}\n      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n    networks:\n      - net\n    depends_on:\n      database:\n        condition: service_healthy\n      pull-model:\n        condition: service_completed_successfully\n    x-develop:\n      watch:\n        - action: rebuild\n          path: .\n          ignore:\n            - loader.py\n            - pdf_bot.py\n            - api.py\n            - front-end/\n    ports:\n      - 8501:8501\n\n  pdf_bot:\n    build:\n      context: .\n      dockerfile: pdf_bot.Dockerfile\n    environment:\n      - NEO4J_URI=${NEO4J_URI-neo4j://database:7687}\n      - NEO4J_PASSWORD=${NEO4J_PASSWORD-password}\n      - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}\n      - OPENAI_API_KEY=${OPENAI_API_KEY-}\n      - GOOGLE_API_KEY=${GOOGLE_API_KEY-}\n      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}\n      - LLM=${LLM-llama2}\n      - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}\n      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT-\"https://api.smith.langchain.com\"}\n      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2-false}\n      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT}\n      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n    networks:\n      - net\n    depends_on:\n      database:\n        condition: service_healthy\n      pull-model:\n        condition: service_completed_successfully\n    x-develop:\n      watch:\n        - action: rebuild\n          path: .\n          ignore:\n            - loader.py\n            - bot.py\n            - api.py\n            - front-end/\n    ports:\n      - 8503:8503\n\n  api:\n    build:\n      context: .\n      dockerfile: api.Dockerfile\n    volumes:\n      - $PWD/embedding_model:/embedding_model\n    environment:\n      - NEO4J_URI=${NEO4J_URI-neo4j://database:7687}\n      - NEO4J_PASSWORD=${NEO4J_PASSWORD-password}\n      - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - GOOGLE_API_KEY=${GOOGLE_API_KEY}  \n      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}\n      - LLM=${LLM-llama2}\n      - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}\n      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT-\"https://api.smith.langchain.com\"}\n      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2-false}\n      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT}\n      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n    networks:\n      - net\n    depends_on:\n      database:\n        condition: service_healthy\n      pull-model:\n        condition: service_completed_successfully\n    x-develop:\n      watch:\n        - action: rebuild\n          path: .\n          ignore:\n            - loader.py\n            - bot.py\n            - pdf_bot.py\n            - front-end/\n    ports:\n      - 8504:8504\n    healthcheck:\n      test: [\"CMD-SHELL\", \"wget --no-verbose --tries=1 http://localhost:8504/ || exit 1\"]\n      interval: 5s\n      timeout: 3s\n      retries: 5\n\n  front-end:\n    build:\n      context: .\n      dockerfile: front-end.Dockerfile\n    x-develop:\n      watch:\n        - action: sync\n          path: ./front-end\n          target: /app\n          ignore:\n            - ./front-end/node_modules/\n        - action: rebuild\n          path: ./front-end/package.json\n    depends_on:\n      api:\n        condition: service_healthy\n    networks:\n      - net\n    ports:\n      - 8505:8505\n\nnetworks:\n  net:\n"
        },
        {
          "name": "embedding_model",
          "type": "tree",
          "content": null
        },
        {
          "name": "env.example",
          "type": "blob",
          "size": 1.796875,
          "content": "#*****************************************************************\n# LLM and Embedding Model\n#*****************************************************************\nLLM=llama2 #or any Ollama model tag, gpt-4 (o or turbo), gpt-3.5, or any bedrock model\nEMBEDDING_MODEL=sentence_transformer #or google-genai-embedding-001 openai, ollama, or aws\n\n#*****************************************************************\n# Neo4j\n#*****************************************************************\n#NEO4J_URI=neo4j://database:7687\n#NEO4J_USERNAME=neo4j\n#NEO4J_PASSWORD=password\n\n#*****************************************************************\n# Langchain\n#*****************************************************************\n# Optional for enabling Langchain Smith API\n\n#LANGCHAIN_TRACING_V2=true # false\n#LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n#LANGCHAIN_PROJECT=#your-project-name\n#LANGCHAIN_API_KEY=#your-api-key ls_...\n\n#*****************************************************************\n# Ollama\n#*****************************************************************\n#OLLAMA_BASE_URL=http://host.docker.internal:11434\n\n#*****************************************************************\n# OpenAI\n#*****************************************************************\n# Only required when using OpenAI LLM or embedding model\n\n#OPENAI_API_KEY=sk-...\n\n#*****************************************************************\n# AWS\n#*****************************************************************\n# Only required when using AWS Bedrock LLM or embedding model\n\n#AWS_ACCESS_KEY_ID=\n#AWS_SECRET_ACCESS_KEY=\n#AWS_DEFAULT_REGION=us-east-1\n\n#*****************************************************************\n# GOOGLE\n#*****************************************************************\n# Only required when using GoogleGenai LLM or embedding model\nGOOGLE_API_KEY=\n"
        },
        {
          "name": "front-end.Dockerfile",
          "type": "blob",
          "size": 0.11328125,
          "content": "FROM node:alpine\n\nWORKDIR /app\n\nCOPY front-end/ .\n\nRUN npm install\n\nEXPOSE 8505\n\nENTRYPOINT [ \"npm\", \"run\", \"dev\" ]\n"
        },
        {
          "name": "front-end",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "install_ollama.sh",
          "type": "blob",
          "size": 7.318359375,
          "content": "#!/bin/sh\n# This script installs Ollama on Linux.\n# It detects the current operating system architecture and installs the appropriate version of Ollama.\n\nset -eu\n\nstatus() { echo \">>> $*\" >&2; }\nerror() { echo \"ERROR $*\"; exit 1; }\nwarning() { echo \"WARNING: $*\"; }\n\nTEMP_DIR=$(mktemp -d)\ncleanup() { rm -rf $TEMP_DIR; }\ntrap cleanup EXIT\n\navailable() { command -v $1 >/dev/null; }\nrequire() {\n    local MISSING=''\n    for TOOL in $*; do\n        if ! available $TOOL; then\n            MISSING=\"$MISSING $TOOL\"\n        fi\n    done\n\n    echo $MISSING\n}\n\n[ \"$(uname -s)\" = \"Linux\" ] || error 'This script is intended to run on Linux only.'\n\ncase \"$(uname -m)\" in\n    x86_64) ARCH=\"amd64\" ;;\n    aarch64|arm64) ARCH=\"arm64\" ;;\n    *) error \"Unsupported architecture: $ARCH\" ;;\nesac\n\nSUDO=\nif [ \"$(id -u)\" -ne 0 ]; then\n    # Running as root, no need for sudo\n    if ! available sudo; then\n        error \"This script requires superuser permissions. Please re-run as root.\"\n    fi\n\n    SUDO=\"sudo\"\nfi\n\nNEEDS=$(require curl awk grep sed tee xargs)\nif [ -n \"$NEEDS\" ]; then\n    status \"ERROR: The following tools are required but missing:\"\n    for NEED in $NEEDS; do\n        echo \"  - $NEED\"\n    done\n    exit 1\nfi\n\nstatus \"Downloading ollama...\"\ncurl --fail --show-error --location --progress-bar -o $TEMP_DIR/ollama \"https://ollama.ai/download/ollama-linux-$ARCH\"\n\nfor BINDIR in /usr/local/bin /usr/bin /bin; do\n    echo $PATH | grep -q $BINDIR && break || continue\ndone\n\nstatus \"Installing ollama to $BINDIR...\"\n$SUDO install -o0 -g0 -m755 -d $BINDIR\n$SUDO install -o0 -g0 -m755 $TEMP_DIR/ollama $BINDIR/ollama\n\ninstall_success() { status 'Install complete. Run \"ollama\" from the command line.'; }\ntrap install_success EXIT\n\n# Everything from this point onwards is optional.\n\nconfigure_systemd() {\n    if ! id ollama >/dev/null 2>&1; then\n        status \"Creating ollama user...\"\n        $SUDO useradd -r -s /bin/false -m -d /usr/share/ollama ollama\n    fi\n\n    status \"Creating ollama systemd service...\"\n    cat <<EOF | $SUDO tee /etc/systemd/system/ollama.service >/dev/null\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=$BINDIR/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"HOME=/usr/share/ollama\"\nEnvironment=\"PATH=$PATH\"\n\n[Install]\nWantedBy=default.target\nEOF\n    SYSTEMCTL_RUNNING=\"$(systemctl is-system-running || true)\"\n    case $SYSTEMCTL_RUNNING in\n        running|degraded)\n            status \"Enabling and starting ollama service...\"\n            $SUDO systemctl daemon-reload\n            $SUDO systemctl enable ollama\n\n            start_service() { $SUDO systemctl restart ollama; }\n            trap start_service EXIT\n            ;;\n    esac\n}\n\nif available systemctl; then\n    configure_systemd\nfi\n\nif ! available lspci && ! available lshw; then\n    warning \"Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\"\n    exit 0\nfi\n\ncheck_gpu() {\n    case $1 in\n        lspci) available lspci && lspci -d '10de:' | grep -q 'NVIDIA' || return 1 ;;\n        lshw) available lshw && $SUDO lshw -c display -numeric | grep -q 'vendor: .* \\[10DE\\]' || return 1 ;;\n        nvidia-smi) available nvidia-smi || return 1 ;;\n    esac\n}\n\nif check_gpu nvidia-smi; then\n    status \"NVIDIA GPU installed.\"\n    exit 0\nfi\n\nif ! check_gpu lspci && ! check_gpu lshw; then\n    warning \"No NVIDIA GPU detected. Ollama will run in CPU-only mode.\"\n    exit 0\nfi\n\n# ref: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#rhel-7-centos-7\n# ref: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#rhel-8-rocky-8\n# ref: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#rhel-9-rocky-9\n# ref: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#fedora\ninstall_cuda_driver_yum() {\n    status 'Installing NVIDIA repository...'\n    case $PACKAGE_MANAGER in\n        yum)\n            $SUDO $PACKAGE_MANAGER -y install yum-utils\n            $SUDO $PACKAGE_MANAGER-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$1$2/$(uname -m)/cuda-$1$2.repo\n            ;;\n        dnf)\n            $SUDO $PACKAGE_MANAGER config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$1$2/$(uname -m)/cuda-$1$2.repo\n            ;;\n    esac\n\n    case $1 in\n        rhel)\n            status 'Installing EPEL repository...'\n            # EPEL is required for third-party dependencies such as dkms and libvdpau\n            $SUDO $PACKAGE_MANAGER -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-$2.noarch.rpm || true\n            ;;\n    esac\n\n    status 'Installing CUDA driver...'\n\n    if [ \"$1\" = 'centos' ] || [ \"$1$2\" = 'rhel7' ]; then\n        $SUDO $PACKAGE_MANAGER -y install nvidia-driver-latest-dkms\n    fi\n\n    $SUDO $PACKAGE_MANAGER -y install cuda-drivers\n}\n\n# ref: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu\n# ref: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#debian\ninstall_cuda_driver_apt() {\n    status 'Installing NVIDIA repository...'\n    curl -fsSL -o $TEMP_DIR/cuda-keyring.deb https://developer.download.nvidia.com/compute/cuda/repos/$1$2/$(uname -m)/cuda-keyring_1.1-1_all.deb\n\n    case $1 in\n        debian)\n            status 'Enabling contrib sources...'\n            $SUDO sed 's/main/contrib/' < /etc/apt/sources.list | sudo tee /etc/apt/sources.list.d/contrib.list > /dev/null\n            ;;\n    esac\n\n    status 'Installing CUDA driver...'\n    $SUDO dpkg -i $TEMP_DIR/cuda-keyring.deb\n    $SUDO apt-get update\n\n    [ -n \"$SUDO\" ] && SUDO_E=\"$SUDO -E\" || SUDO_E=\n    DEBIAN_FRONTEND=noninteractive $SUDO_E apt-get -y install cuda-drivers -q\n}\n\nif [ ! -f \"/etc/os-release\" ]; then\n    error \"Unknown distribution. Skipping CUDA installation.\"\nfi\n\n. /etc/os-release\n\nOS_NAME=$ID\nOS_VERSION=$VERSION_ID\n\nPACKAGE_MANAGER=\nfor PACKAGE_MANAGER in dnf yum apt-get; do\n    if available $PACKAGE_MANAGER; then\n        break\n    fi\ndone\n\nif [ -z \"$PACKAGE_MANAGER\" ]; then\n    error \"Unknown package manager. Skipping CUDA installation.\"\nfi\n\nif ! check_gpu nvidia-smi || [ -z \"$(nvidia-smi | grep -o \"CUDA Version: [0-9]*\\.[0-9]*\")\" ]; then\n    case $OS_NAME in\n        centos|rhel) install_cuda_driver_yum 'rhel' $OS_VERSION ;;\n        rocky) install_cuda_driver_yum 'rhel' $(echo $OS_VERSION | cut -c1) ;;\n        fedora) install_cuda_driver_yum $OS_NAME $OS_VERSION ;;\n        amzn) install_cuda_driver_yum 'fedora' '35' ;;\n        debian) install_cuda_driver_apt $OS_NAME $OS_VERSION ;;\n        ubuntu) install_cuda_driver_apt $OS_NAME $(echo $OS_VERSION | sed 's/\\.//') ;;\n        *) exit ;;\n    esac\nfi\n\nif ! lsmod | grep -q nvidia; then\n    KERNEL_RELEASE=\"$(uname -r)\"\n    case $OS_NAME in\n        centos|rhel|rocky|amzn) $SUDO $PACKAGE_MANAGER -y install kernel-devel-$KERNEL_RELEASE kernel-headers-$KERNEL_RELEASE ;;\n        fedora) $SUDO $PACKAGE_MANAGER -y install kernel-devel-$KERNEL_RELEASE ;;\n        debian|ubuntu) $SUDO apt-get -y install linux-headers-$KERNEL_RELEASE ;;\n        *) exit ;;\n    esac\n\n    NVIDIA_CUDA_VERSION=$($SUDO dkms status | awk -F: '/added/ { print $1 }')\n    if [ -n \"$NVIDIA_CUDA_VERSION\" ]; then\n        $SUDO dkms install $NVIDIA_CUDA_VERSION\n    fi\n\n    if lsmod | grep -q nouveau; then\n        status 'Reboot to complete NVIDIA CUDA driver install.'\n        exit 0\n    fi\n\n    $SUDO modprobe nvidia\nfi\n\n\nstatus \"NVIDIA CUDA drivers installed.\"\n"
        },
        {
          "name": "loader.Dockerfile",
          "type": "blob",
          "size": 0.4912109375,
          "content": "FROM langchain/langchain\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    curl \\\n    software-properties-common \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt .\n\nRUN pip install --upgrade -r requirements.txt\n\nCOPY loader.py .\nCOPY utils.py .\nCOPY chains.py .\nCOPY images ./images\n\nEXPOSE 8502\n\nHEALTHCHECK CMD curl --fail http://localhost:8502/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"loader.py\", \"--server.port=8502\", \"--server.address=0.0.0.0\"]\n"
        },
        {
          "name": "loader.py",
          "type": "blob",
          "size": 5.521484375,
          "content": "import os\nimport requests\nfrom dotenv import load_dotenv\nfrom langchain_community.graphs import Neo4jGraph\nimport streamlit as st\nfrom streamlit.logger import get_logger\nfrom chains import load_embedding_model\nfrom utils import create_constraints, create_vector_index\nfrom PIL import Image\n\nload_dotenv(\".env\")\n\nurl = os.getenv(\"NEO4J_URI\")\nusername = os.getenv(\"NEO4J_USERNAME\")\npassword = os.getenv(\"NEO4J_PASSWORD\")\nollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")\nembedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\n# Remapping for Langchain Neo4j integration\nos.environ[\"NEO4J_URL\"] = url\n\nlogger = get_logger(__name__)\n\nso_api_base_url = \"https://api.stackexchange.com/2.3/search/advanced\"\n\nembeddings, dimension = load_embedding_model(\n    embedding_model_name, config={\"ollama_base_url\": ollama_base_url}, logger=logger\n)\n\n# if Neo4j is local, you can go to http://localhost:7474/ to browse the database\nneo4j_graph = Neo4jGraph(\n    url=url, username=username, password=password, refresh_schema=False\n)\n\ncreate_constraints(neo4j_graph)\ncreate_vector_index(neo4j_graph)\n\n\ndef load_so_data(tag: str = \"neo4j\", page: int = 1) -> None:\n    parameters = (\n        f\"?pagesize=100&page={page}&order=desc&sort=creation&answers=1&tagged={tag}\"\n        \"&site=stackoverflow&filter=!*236eb_eL9rai)MOSNZ-6D3Q6ZKb0buI*IVotWaTb\"\n    )\n    data = requests.get(so_api_base_url + parameters).json()\n    insert_so_data(data)\n\n\ndef load_high_score_so_data() -> None:\n    parameters = (\n        f\"?fromdate=1664150400&order=desc&sort=votes&site=stackoverflow&\"\n        \"filter=!.DK56VBPooplF.)bWW5iOX32Fh1lcCkw1b_Y6Zkb7YD8.ZMhrR5.FRRsR6Z1uK8*Z5wPaONvyII\"\n    )\n    data = requests.get(so_api_base_url + parameters).json()\n    insert_so_data(data)\n\n\ndef insert_so_data(data: dict) -> None:\n    # Calculate embedding values for questions and answers\n    for q in data[\"items\"]:\n        question_text = q[\"title\"] + \"\\n\" + q[\"body_markdown\"]\n        q[\"embedding\"] = embeddings.embed_query(question_text)\n        for a in q[\"answers\"]:\n            a[\"embedding\"] = embeddings.embed_query(\n                question_text + \"\\n\" + a[\"body_markdown\"]\n            )\n\n    # Cypher, the query language of Neo4j, is used to import the data\n    # https://neo4j.com/docs/getting-started/cypher-intro/\n    # https://neo4j.com/docs/cypher-cheat-sheet/5/auradb-enterprise/\n    import_query = \"\"\"\n    UNWIND $data AS q\n    MERGE (question:Question {id:q.question_id}) \n    ON CREATE SET question.title = q.title, question.link = q.link, question.score = q.score,\n        question.favorite_count = q.favorite_count, question.creation_date = datetime({epochSeconds: q.creation_date}),\n        question.body = q.body_markdown, question.embedding = q.embedding\n    FOREACH (tagName IN q.tags | \n        MERGE (tag:Tag {name:tagName}) \n        MERGE (question)-[:TAGGED]->(tag)\n    )\n    FOREACH (a IN q.answers |\n        MERGE (question)<-[:ANSWERS]-(answer:Answer {id:a.answer_id})\n        SET answer.is_accepted = a.is_accepted,\n            answer.score = a.score,\n            answer.creation_date = datetime({epochSeconds:a.creation_date}),\n            answer.body = a.body_markdown,\n            answer.embedding = a.embedding\n        MERGE (answerer:User {id:coalesce(a.owner.user_id, \"deleted\")}) \n        ON CREATE SET answerer.display_name = a.owner.display_name,\n                      answerer.reputation= a.owner.reputation\n        MERGE (answer)<-[:PROVIDED]-(answerer)\n    )\n    WITH * WHERE NOT q.owner.user_id IS NULL\n    MERGE (owner:User {id:q.owner.user_id})\n    ON CREATE SET owner.display_name = q.owner.display_name,\n                  owner.reputation = q.owner.reputation\n    MERGE (owner)-[:ASKED]->(question)\n    \"\"\"\n    neo4j_graph.query(import_query, {\"data\": data[\"items\"]})\n\n\n# Streamlit\ndef get_tag() -> str:\n    input_text = st.text_input(\n        \"Which tag questions do you want to import?\", value=\"neo4j\"\n    )\n    return input_text\n\n\ndef get_pages():\n    col1, col2 = st.columns(2)\n    with col1:\n        num_pages = st.number_input(\n            \"Number of pages (100 questions per page)\", step=1, min_value=1\n        )\n    with col2:\n        start_page = st.number_input(\"Start page\", step=1, min_value=1)\n    st.caption(\"Only questions with answers will be imported.\")\n    return (int(num_pages), int(start_page))\n\n\ndef render_page():\n    datamodel_image = Image.open(\"./images/datamodel.png\")\n    st.header(\"StackOverflow Loader\")\n    st.subheader(\"Choose StackOverflow tags to load into Neo4j\")\n    st.caption(\"Go to http://localhost:7474/ to explore the graph.\")\n\n    user_input = get_tag()\n    num_pages, start_page = get_pages()\n\n    if st.button(\"Import\", type=\"primary\"):\n        with st.spinner(\"Loading... This might take a minute or two.\"):\n            try:\n                for page in range(1, num_pages + 1):\n                    load_so_data(user_input, start_page + (page - 1))\n                st.success(\"Import successful\", icon=\"âœ…\")\n                st.caption(\"Data model\")\n                st.image(datamodel_image)\n                st.caption(\"Go to http://localhost:7474/ to interact with the database\")\n            except Exception as e:\n                st.error(f\"Error: {e}\", icon=\"ðŸš¨\")\n    with st.expander(\"Highly ranked questions rather than tags?\"):\n        if st.button(\"Import highly ranked questions\"):\n            with st.spinner(\"Loading... This might take a minute or two.\"):\n                try:\n                    load_high_score_so_data()\n                    st.success(\"Import successful\", icon=\"âœ…\")\n                except Exception as e:\n                    st.error(f\"Error: {e}\", icon=\"ðŸš¨\")\n\n\nrender_page()\n"
        },
        {
          "name": "pdf_bot.Dockerfile",
          "type": "blob",
          "size": 0.47265625,
          "content": "FROM langchain/langchain\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    curl \\\n    software-properties-common \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt .\n\nRUN pip install --upgrade -r requirements.txt\n\nCOPY pdf_bot.py .\nCOPY utils.py .\nCOPY chains.py .\n\nEXPOSE 8503\n\nHEALTHCHECK CMD curl --fail http://localhost:8503/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"pdf_bot.py\", \"--server.port=8503\", \"--server.address=0.0.0.0\"]\n"
        },
        {
          "name": "pdf_bot.py",
          "type": "blob",
          "size": 2.5693359375,
          "content": "import os\n\nimport streamlit as st\nfrom langchain.chains import RetrievalQA\nfrom PyPDF2 import PdfReader\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Neo4jVector\nfrom streamlit.logger import get_logger\nfrom chains import (\n    load_embedding_model,\n    load_llm,\n)\n\n# load api key lib\nfrom dotenv import load_dotenv\n\nload_dotenv(\".env\")\n\n\nurl = os.getenv(\"NEO4J_URI\")\nusername = os.getenv(\"NEO4J_USERNAME\")\npassword = os.getenv(\"NEO4J_PASSWORD\")\nollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")\nembedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\nllm_name = os.getenv(\"LLM\")\n# Remapping for Langchain Neo4j integration\nos.environ[\"NEO4J_URL\"] = url\n\nlogger = get_logger(__name__)\n\n\nembeddings, dimension = load_embedding_model(\n    embedding_model_name, config={\"ollama_base_url\": ollama_base_url}, logger=logger\n)\n\n\nclass StreamHandler(BaseCallbackHandler):\n    def __init__(self, container, initial_text=\"\"):\n        self.container = container\n        self.text = initial_text\n\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.text += token\n        self.container.markdown(self.text)\n\n\nllm = load_llm(llm_name, logger=logger, config={\"ollama_base_url\": ollama_base_url})\n\n\ndef main():\n    st.header(\"ðŸ“„Chat with your pdf file\")\n\n    # upload a your pdf file\n    pdf = st.file_uploader(\"Upload your PDF\", type=\"pdf\")\n\n    if pdf is not None:\n        pdf_reader = PdfReader(pdf)\n\n        text = \"\"\n        for page in pdf_reader.pages:\n            text += page.extract_text()\n\n        # langchain_textspliter\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000, chunk_overlap=200, length_function=len\n        )\n\n        chunks = text_splitter.split_text(text=text)\n\n        # Store the chunks part in db (vector)\n        vectorstore = Neo4jVector.from_texts(\n            chunks,\n            url=url,\n            username=username,\n            password=password,\n            embedding=embeddings,\n            index_name=\"pdf_bot\",\n            node_label=\"PdfBotChunk\",\n            pre_delete_collection=True,  # Delete existing PDF data\n        )\n        qa = RetrievalQA.from_chain_type(\n            llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n        )\n\n        # Accept user questions/query\n        query = st.text_input(\"Ask questions about your PDF file\")\n\n        if query:\n            stream_handler = StreamHandler(st.empty())\n            qa.run(query, callbacks=[stream_handler])\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "pull_model.Dockerfile",
          "type": "blob",
          "size": 1.953125,
          "content": "#syntax = docker/dockerfile:1.4\n\nFROM ollama/ollama:latest AS ollama\nFROM babashka/babashka:latest\n\n# just using as a client - never as a server\nCOPY --from=ollama /bin/ollama ./bin/ollama\n\nCOPY <<EOF pull_model.clj\n(ns pull-model\n  (:require [babashka.process :as process]\n            [clojure.core.async :as async]))\n\n(try\n  (let [llm (get (System/getenv) \"LLM\")\n        url (get (System/getenv) \"OLLAMA_BASE_URL\")]\n    (println (format \"pulling ollama model %s using %s\" llm url))\n    (if (and llm \n         url \n         (not (#{\"gpt-4\" \"gpt-3.5\" \"claudev2\" \"gpt-4o\" \"gpt-4-turbo\"} llm))\n         (not (some #(.startsWith llm %) [\"ai21.jamba-instruct-v1:0\"\n                                          \"amazon.titan\"\n                                          \"anthropic.claude\"\n                                          \"cohere.command\"\n                                          \"meta.llama\"\n                                          \"mistral.mi\"])))\n\n      ;; ----------------------------------------------------------------------\n      ;; just call `ollama pull` here - create OLLAMA_HOST from OLLAMA_BASE_URL\n      ;; ----------------------------------------------------------------------\n      ;; TODO - this still doesn't show progress properly when run from docker compose\n\n      (let [done (async/chan)]\n        (async/go-loop [n 0]\n          (let [[v _] (async/alts! [done (async/timeout 5000)])]\n            (if (= :stop v) :stopped (do (println (format \"... pulling model (%ss) - will take several minutes\" (* n 10))) (recur (inc n))))))\n        (process/shell {:env {\"OLLAMA_HOST\" url \"HOME\" (System/getProperty \"user.home\")} :out :inherit :err :inherit} (format \"bash -c './bin/ollama show %s --modelfile > /dev/null || ./bin/ollama pull %s'\" llm llm))\n        (async/>!! done :stop))\n\n      (println \"OLLAMA model only pulled if both LLM and OLLAMA_BASE_URL are set and the LLM model is not gpt\")))\n  (catch Throwable _ (System/exit 1)))\nEOF\n\nENTRYPOINT [\"bb\", \"-f\", \"pull_model.clj\"]\n\n"
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 7.666015625,
          "content": "# GenAI Stack\nThe GenAI Stack will get you started building your own GenAI application in no time.\nThe demo applications can serve as inspiration or as a starting point.\nLearn more about the details in the [introduction blog post](https://neo4j.com/blog/introducing-genai-stack-developers/).\n\n# Configure\n\nCreate a `.env` file from the environment template file `env.example`\n\nAvailable variables:\n| Variable Name          | Default value                      | Description                                                             |\n|------------------------|------------------------------------|-------------------------------------------------------------------------|\n| OLLAMA_BASE_URL        | http://host.docker.internal:11434  | REQUIRED - URL to Ollama LLM API                                        |   \n| NEO4J_URI              | neo4j://database:7687              | REQUIRED - URL to Neo4j database                                        |\n| NEO4J_USERNAME         | neo4j                              | REQUIRED - Username for Neo4j database                                  |\n| NEO4J_PASSWORD         | password                           | REQUIRED - Password for Neo4j database                                  |\n| LLM                    | llama2                             | REQUIRED - Can be any Ollama model tag, or gpt-4 or gpt-3.5 or claudev2 |\n| EMBEDDING_MODEL        | sentence_transformer               | REQUIRED - Can be sentence_transformer, openai, aws, ollama or google-genai-embedding-001|\n| AWS_ACCESS_KEY_ID      |                                    | REQUIRED - Only if LLM=claudev2 or embedding_model=aws                  |\n| AWS_SECRET_ACCESS_KEY  |                                    | REQUIRED - Only if LLM=claudev2 or embedding_model=aws                  |\n| AWS_DEFAULT_REGION     |                                    | REQUIRED - Only if LLM=claudev2 or embedding_model=aws                  |\n| OPENAI_API_KEY         |                                    | REQUIRED - Only if LLM=gpt-4 or LLM=gpt-3.5 or embedding_model=openai   |\n| GOOGLE_API_KEY         |                                    | REQUIRED - Only required when using GoogleGenai LLM or embedding model google-genai-embedding-001|\n| LANGCHAIN_ENDPOINT     | \"https://api.smith.langchain.com\"  | OPTIONAL - URL to Langchain Smith API                                   |\n| LANGCHAIN_TRACING_V2   | false                              | OPTIONAL - Enable Langchain tracing v2                                  |\n| LANGCHAIN_PROJECT      |                                    | OPTIONAL - Langchain project name                                       |\n| LANGCHAIN_API_KEY      |                                    | OPTIONAL - Langchain API key                                            |\n\n## LLM Configuration\nMacOS and Linux users can use any LLM that's available via Ollama. Check the \"tags\" section under the model page you want to use on https://ollama.ai/library and write the tag for the value of the environment variable `LLM=` in the `.env` file.\nAll platforms can use GPT-3.5-turbo and GPT-4 (bring your own API keys for OpenAI models).\n\n**MacOS**\nInstall [Ollama](https://ollama.ai) on MacOS and start it before running `docker compose up` using `ollama serve` in a separate terminal.\n\n**Linux**\nNo need to install Ollama manually, it will run in a container as\npart of the stack when running with the Linux profile: run `docker compose --profile linux up`.\nMake sure to set the `OLLAMA_BASE_URL=http://llm:11434` in the `.env` file when using Ollama docker container.\n\nTo use the Linux-GPU profile: run `docker compose --profile linux-gpu up`. Also change `OLLAMA_BASE_URL=http://llm-gpu:11434` in the `.env` file.\n\n**Windows**\nOllama now supports Windows. Install [Ollama](https://ollama.ai) on Windows and start it before running `docker compose up` using `ollama serve` in a separate terminal. Alternatively, Windows users can generate an OpenAI API key and configure the stack to use `gpt-3.5` or `gpt-4` in the `.env` file.\n# Develop\n\n> [!WARNING]\n> There is a performance issue that impacts python applications in the `4.24.x` releases of Docker Desktop. Please upgrade to the latest release before using this stack.\n\n**To start everything**\n```\ndocker compose up\n```\nIf changes to build scripts have been made, **rebuild**.\n```\ndocker compose up --build\n```\n\nTo enter **watch mode** (auto rebuild on file changes).\nFirst start everything, then in new terminal:\n```\ndocker compose watch\n```\n\n**Shutdown**\nIf health check fails or containers don't start up as expected, shutdown\ncompletely to start up again.\n```\ndocker compose down\n```\n\n# Applications\n\nHere's what's in this repo:\n\n| Name | Main files | Compose name | URLs | Description |\n|---|---|---|---|---|\n| Support Bot | `bot.py` | `bot` | http://localhost:8501 | Main usecase. Fullstack Python application. |\n| Stack Overflow Loader | `loader.py` | `loader` | http://localhost:8502 | Load SO data into the database (create vector embeddings etc). Fullstack Python application. |\n| PDF Reader | `pdf_bot.py` | `pdf_bot` | http://localhost:8503 | Read local PDF and ask it questions. Fullstack Python application. |\n| Standalone Bot API | `api.py` | `api` | http://localhost:8504 | Standalone HTTP API streaming (SSE) + non-streaming endpoints Python. |\n| Standalone Bot UI | `front-end/` | `front-end` | http://localhost:8505 | Standalone client that uses the Standalone Bot API to interact with the model. JavaScript (Svelte) front-end. |\n\nThe database can be explored at http://localhost:7474.\n\n## App 1 - Support Agent Bot\n\nUI: http://localhost:8501\nDB client: http://localhost:7474\n\n- answer support question based on recent entries\n- provide summarized answers with sources\n- demonstrate difference between\n    - RAG Disabled (pure LLM response)\n    - RAG Enabled (vector + knowledge graph context)\n- allow to generate a high quality support ticket for the current conversation based on the style of highly rated questions in the database.\n\n![](.github/media/app1-rag-selector.png)\n*(Chat input + RAG mode selector)*\n\n|  |  |\n|---|---|\n| ![](.github/media/app1-generate.png) | ![](.github/media/app1-ticket.png) |\n| *(CTA to auto generate support ticket draft)* | *(UI of the auto generated support ticket draft)* |\n\n---\n\n##  App 2 - Loader\n\nUI: http://localhost:8502\nDB client: http://localhost:7474\n\n- import recent Stack Overflow data for certain tags into a KG\n- embed questions and answers and store them in vector index\n- UI: choose tags, run import, see progress, some stats of data in the database\n- Load high ranked questions (regardless of tags) to support the ticket generation feature of App 1.\n\n\n\n\n|  |  |\n|---|---|\n| ![](.github/media/app2-ui-1.png) | ![](.github/media/app2-model.png) |\n\n## App 3 Question / Answer with a local PDF\nUI: http://localhost:8503  \nDB client: http://localhost:7474\n\nThis application lets you load a local PDF into text\nchunks and embed it into Neo4j so you can ask questions about\nits contents and have the LLM answer them using vector similarity\nsearch.\n\n![](.github/media/app3-ui.png)\n\n## App 4 Standalone HTTP API\nEndpoints: \n  - http://localhost:8504/query?text=hello&rag=false (non streaming)\n  - http://localhost:8504/query-stream?text=hello&rag=false (SSE streaming)\n\nExample cURL command:\n```bash\ncurl http://localhost:8504/query-stream\\?text\\=minimal%20hello%20world%20in%20python\\&rag\\=false\n```\n\nExposes the functionality to answer questions in the same way as App 1 above. Uses\nsame code and prompts.\n\n## App 5 Static front-end\nUI: http://localhost:8505\n\nThis application has the same features as App 1, but is built separate from\nthe back-end code using modern best practices (Vite, Svelte, Tailwind).  \nThe auto-reload on changes are instant using the Docker watch `sync` config.  \n![](.github/media/app5-ui.png)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.314453125,
          "content": "python-dotenv\nwikipedia\ntiktoken\nneo4j\nstreamlit\nPillow\nfastapi\nPyPDF2\npydantic\nuvicorn\nsse-starlette\nboto3\nstreamlit==1.32.1\n# missing from the langchain base image?\nlangchain-openai==0.2.4\nlangchain-community==0.3.3\nlangchain-google-genai==2.0.3\nlangchain-ollama==0.2.0\nlangchain-huggingface==0.1.1\nlangchain-aws==0.2.4\n"
        },
        {
          "name": "running_on_wsl.md",
          "type": "blob",
          "size": 2.60546875,
          "content": "# Run the stack on WSL\n\nNote that for the stack to work on Windows, you should have running version on ollama installed somehow. Since Windows, is not yet supported, we can only use WSL.\n\nI won't cover the activation of WSL procedure which you will find very easy on internet. Assuming that you have a version of WSL on your local windows machine and a running docker Desktop software installer and running as well, you can follow the following steps:\n\n1. enable docker-desktop to use WSL using the following tutorial:\n\n>To connect WSL to Docker Desktop, you need to follow the instructions below:\n>\n>1. First, ensure that you have installed Docker Desktop for Windows on your machine. If you haven't, you can download it from Â¹.\n>2. Next, open Docker Desktop and navigate to **Settings**.\n>3. From the **Settings** menu, select **Resources** and then click on **WSL Integration**.\n>4. On the **WSL Integration** page, you will see a list of available WSL distributions. Select the distribution that you want to connect to Docker Desktop.\n>5. Once you have selected the distribution, click on **Apply & Restart**.\n>\n>After following these steps, your WSL distribution should be connected to Docker Desktop.\n>\n>\n>1.  Docker Desktop WSL 2 backend on Windows | Docker Docs. https://docs.docker.com/desktop/wsl/.\n>2. Get started with Docker containers on WSL | Microsoft Learn. https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers.\n>3. How to configure Docker Desktop to work with the WSL. https://tutorials.releaseworksacademy.com/learn/how-to-configure-docker-desktop-to-work-with-the-wsl.html.\n>4. How can I access wsl2 which is used by Docker desktop?. https://stackoverflow.com/questions/70449927/how-can-i-access-wsl2-which-is-used-by-docker-desktop.\n\nAfter the activation enter into the WSL with the command `wsl` and type `docker`.\n\n2. Install ollama on WSL using https://github.com/jmorganca/ollama (avec la commande `curl https://ollama.ai/install.sh | sh`)\n    - The script have been downloaded in `./install_ollama.sh` and you do the smae thing with `sh ./install_ollama.sh`\n    - To list the downloaded model: `ollama list`. This command could lead to:\n    ```sh\n    NAME            ID              SIZE    MODIFIED       \n    llama2:latest   7da22eda89ac    3.8 GB  22 minutes ago\n    ```\n    - (OPTIONAL) To remove model: `ollama rm llama2`\n    - To run the ollama on WSL: `ollama run llama2`\n\n3. clone the repo\n4. cd into the repo and enter wsl\n5. run `docker-compose up`\n\n# Run the stack on MAC:\n\nOn MAC you can follow this tutorial: https://collabnix.com/getting-started-with-genai-stack-powered-with-docker-langchain-neo4j-and-ollama/"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 1.7412109375,
          "content": "class BaseLogger:\n    def __init__(self) -> None:\n        self.info = print\n\n\ndef extract_title_and_question(input_string):\n    lines = input_string.strip().split(\"\\n\")\n\n    title = \"\"\n    question = \"\"\n    is_question = False  # flag to know if we are inside a \"Question\" block\n\n    for line in lines:\n        if line.startswith(\"Title:\"):\n            title = line.split(\"Title: \", 1)[1].strip()\n        elif line.startswith(\"Question:\"):\n            question = line.split(\"Question: \", 1)[1].strip()\n            is_question = (\n                True  # set the flag to True once we encounter a \"Question:\" line\n            )\n        elif is_question:\n            # if the line does not start with \"Question:\" but we are inside a \"Question\" block,\n            # then it is a continuation of the question\n            question += \"\\n\" + line.strip()\n\n    return title, question\n\n\ndef create_vector_index(driver) -> None:\n    index_query = \"CREATE VECTOR INDEX stackoverflow IF NOT EXISTS FOR (m:Question) ON m.embedding\"\n    try:\n        driver.query(index_query)\n    except:  # Already exists\n        pass\n    index_query = \"CREATE VECTOR INDEX top_answers IF NOT EXISTS FOR (m:Answer) ON m.embedding\"\n    try:\n        driver.query(index_query)\n    except:  # Already exists\n        pass\n\n\ndef create_constraints(driver):\n    driver.query(\n        \"CREATE CONSTRAINT question_id IF NOT EXISTS FOR (q:Question) REQUIRE (q.id) IS UNIQUE\"\n    )\n    driver.query(\n        \"CREATE CONSTRAINT answer_id IF NOT EXISTS FOR (a:Answer) REQUIRE (a.id) IS UNIQUE\"\n    )\n    driver.query(\n        \"CREATE CONSTRAINT user_id IF NOT EXISTS FOR (u:User) REQUIRE (u.id) IS UNIQUE\"\n    )\n    driver.query(\n        \"CREATE CONSTRAINT tag_name IF NOT EXISTS FOR (t:Tag) REQUIRE (t.name) IS UNIQUE\"\n    )\n"
        }
      ]
    }
  ]
}