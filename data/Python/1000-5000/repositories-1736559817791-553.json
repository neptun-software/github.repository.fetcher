{
  "metadata": {
    "timestamp": 1736559817791,
    "page": 553,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "AI4Finance-Foundation/ElegantRL",
      "stars": 3812,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2568359375,
          "content": "# Python-created cache files\n**/__pycache__\n\n# Saved actor/critic networks from training\n*.pth\n\n# Plots, recorders, replay buffers\n*plot_*.jpg\n*recorder.npy\n*replay*.npz\n\n# Runs created by Isaac Gym\nruns/\n\n# VS Code\n**/.vscode\n# JetBrains folder\n.idea/\n.env-erl/\n"
        },
        {
          "name": "Awesome_Deep_Reinforcement_Learning_List.md",
          "type": "blob",
          "size": 1.16015625,
          "content": "##\n\n## Distributed Frameworks\n\n[1] Massively Parallel Methods for Deep Reinforcement Learning (SGD, first distributed architecture, Gorilla DQN).\n\n[2] Asynchronous Methods for Deep Reinforcement Learning (SGD, A3C).\n\n[3] Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU (A3C on GPU).\n\n[4] Efficient Parallel Methods for Deep Reinforcement Learning (Batched A2C, GPU).\n\n[5] Evolution Strategies as a Scalable Alternative to Reinforcement Learning (ES).\n\n[6] Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for\nReinforcement Learning (ES).\n\n[7] RLlib: Abstractions for Distributed Reinforcement Learning (Library)\n\n[8] Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes (Batched A3C).\n\n[9] Distributed Prioritized Experience Replay (Ape-X, distributed replay buffer).\n\n[10] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (CPU+GPU).\n\n[11] Accelerated Methods for Deep Reinforcement Learning (Simulation Acceleration).\n\n[12] GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning (Simulation Acceleration).\n\n##  \n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.587890625,
          "content": "Copyright 2024 AI4Finance Foundation Inc.\r\nAll rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.697265625,
          "content": "<div align=\"center\">\r\n<img align=\"center\" width=\"30%\" alt=\"image\" src=\"https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139\">\r\n</div>\r\n\r\n## ElegantRL “小雅”: Massively Parallel Deep Reinforcement Learning\r\n\r\n[![Downloads](https://pepy.tech/badge/elegantrl)](https://pepy.tech/project/elegantrl)\r\n[![Downloads](https://pepy.tech/badge/elegantrl/week)](https://pepy.tech/project/elegantrl)\r\n[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\r\n[![PyPI](https://img.shields.io/pypi/v/elegantrl.svg)](https://pypi.org/project/elegantrl/)\r\n![License](https://img.shields.io/github/license/AI4Finance-Foundation/elegantrl.svg?color=brightgreen)\r\n![](https://img.shields.io/github/issues-raw/AI4Finance-Foundation/elegantrl?label=Issues)\r\n![](https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/elegantrl?label=Closed+Issues)\r\n![](https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/elegantrl?label=Open+PRs)\r\n![](https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/elegantrl?label=Closed+PRs)\r\n\r\n<br/>\r\n<a href=\"https://github.com/AI4Finance-Foundation/ElegantRL\" target=\"\\_blank\">\r\n\t<div align=\"center\">\r\n\t\t<img src=\"figs/icon.jpg\" width=\"40%\"/> \r\n\t</div>\r\n<!-- \t<div align=\"center\"><caption>Slack Invitation Link</caption></div> -->\r\n</a>\r\n<br/>\r\n\r\n“小雅”源于《诗经·小雅·鹤鸣》，旨在「他山之石，可以攻玉」。\r\n\r\n[![](https://dcbadge.vercel.app/api/server/trsr8SXpW5)](https://discord.gg/trsr8SXpW5)\r\n\r\n![Visitors](https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&repo=elegantrl&countColor=%23B17A)\r\n\r\n\r\n\r\nElegantRL ([website](https://elegantrl.readthedocs.io/en/latest/index.html)) is developed for users/developers with the following advantages:\r\n\r\n- **Cloud-native**: follows a cloud-native paradigm through micro-service architecture and containerization, and supports [ElegantRL-Podracer](https://elegantrl.readthedocs.io/en/latest/tutorial/elegantrl-podracer.html) and [FinRL-Podracer](https://elegantrl.readthedocs.io/en/latest/tutorial/finrl-podracer.html).\r\n\r\n- **Scalable**: fully exploits the parallelism of DRL algorithms, making it easily scale out to hundreds or thousands of computing nodes on a cloud platform, say, a [DGX SuperPOD platform](https://www.nvidia.com/en-us/data-center/dgx-superpod/) with thousands of GPUs.\r\n\r\n- **Elastic**: allows to elastically and automatically allocate computing resources on the cloud.\r\n\r\n- **Lightweight**: the core codes have <1,000 lines (check [Elegantrl_Helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/helloworld)).\r\n\r\n- **Efficient**: in many testing cases (e.g., single-GPU/multi-GPU/GPU-cloud), we find it more efficient than [Ray RLlib](https://github.com/ray-project/ray). \r\n\r\n- **Stable**: much much much more stable than [Stable Baselines 3](https://github.com/DLR-RM/stable-baselines3) by utilizing various methods such as the Hamiltonian term.\r\n  \r\n- **Practical**: used in multipe projects ([RLSolver](https://github.com/AI4Finance-Foundation/RLSolver), [FinRL](https://github.com/AI4Finance-Foundation/FinRL), [FinRL-Meta](https://github.com/AI4Finance-Foundation/FinRL-Meta), etc.)\r\n\r\n- **Massively parallel simulations** are used in multipe projects ([RLSolver](https://github.com/AI4Finance-Foundation/RLSolver), [FinRL](https://github.com/AI4Finance-Foundation/FinRL), etc.); therefore, the sampling speed is high since we can build many many GPU-based environments. \r\n  \r\nElegantRL implements the following model-free deep reinforcement learning (DRL) algorithms:\r\n\r\n- **DDPG, TD3, SAC, PPO, REDQ** for continuous actions in single-agent environment,\r\n- **DQN, Double DQN, D3QN** for discrete actions in single-agent environment,\r\n- **QMIX, VDN, MADDPG, MAPPO, MATD3** in multi-agent environment.\r\n\r\nFor more details of DRL algorithms, please refer to the educational webpage [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/).\r\n\r\nElegantRL supports the following simulators:\r\n\r\n- **Isaac Gym** for massively parallel simulations,\r\n- **OpenAI Gym, MuJoCo, PyBullet, FinRL** for benchmarking.\r\n\r\n\r\n## Contents\r\n\r\n- [News](#News)\r\n- [ElegantRL-Helloworld](#ElegantRL-Helloworld)\r\n- [File Structure](#File-Structure)\r\n- [Experimental Demos](#Experimental-Demos)\r\n- [Requirements](#Requirements)\r\n- [Citation](#Citation)\r\n\r\n\r\n## Tutorials\r\n\r\n- [Towardsdatascience] [A New Era of Massively Parallel Simulation: A Practical Tutorial Using ElegantRL](https://medium.com/towards-data-science/a-new-era-of-massively-parallel-simulation-a-practical-tutorial-using-elegantrl-5ebc483c3385), Nov. 2, 2022.\r\n- [MLearning.ai] [ElegantRL: Much More Stable Deep Reinforcement Learning Algorithms than Stable-Baseline3](https://medium.com/mlearning-ai/elegantrl-much-much-more-stable-than-stable-baseline3-f096533c26db), Mar. 3, 2022.\r\n- [Towardsdatascience] [ElegantRL-Podracer: A Scalable and Elastic Library for Cloud-Native Deep Reinforcement Learning](https://elegantrl.medium.com/elegantrl-podracer-scalable-and-elastic-library-for-cloud-native-deep-reinforcement-learning-bafda6f7fbe0), Dec. 11, 2021.\r\n- [Towardsdatascience] [ElegantRL: Mastering PPO Algorithms](https://medium.com/@elegantrl/elegantrl-mastering-the-ppo-algorithm-part-i-9f36bc47b791), May. 3, 2021.\r\n- [MLearning.ai] [ElegantRL Demo: Stock Trading Using DDPG (Part II)](https://medium.com/mlearning-ai/elegantrl-demo-stock-trading-using-ddpg-part-ii-d3d97e01999f), Apr. 19, 2021.\r\n- [MLearning.ai] [ElegantRL Demo: Stock Trading Using DDPG (Part I)](https://elegantrl.medium.com/elegantrl-demo-stock-trading-using-ddpg-part-i-e77d7dc9d208), Mar. 28, 2021.\r\n- [Towardsdatascience] [ElegantRL-Helloworld: A Lightweight and Stable Deep Reinforcement Learning Library](https://towardsdatascience.com/elegantrl-a-lightweight-and-stable-deep-reinforcement-learning-library-95cef5f3460b), Mar. 4, 2021.\r\n\r\n## ElegantRL-Helloworld\r\n\r\n<div align=\"center\">\r\n\t<img align=\"center\" src=figs/File_structure.png width=\"800\">\r\n</div>\r\n\r\nFor beginners, we maintain [ElegantRL-Helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/helloworld) as a tutorial. Its goal is to get hands-on experience with ELegantRL.\r\n\r\n- Run the [tutorial code and learn about RL algorithms in this order: DQN -> DDPG -> PPO](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld)\r\n- Write the [suggestion for Eleagant_HelloWorld in github issue](https://github.com/AI4Finance-Foundation/ElegantRL/issues/135).\r\n\r\nOne sentence summary: an agent (_agent.py_) with Actor-Critic networks (_net.py_) is trained (_run.py_) by interacting with an environment (_env.py_).\r\n\r\n## File Structure\r\n\r\n- **elegantrl** # main folder\r\n  - **agents**  # a collection of DRL algorithms\r\n  \t- AgentXXX.py  # a collection of one kind of DRL algorithms\r\n  \t- **net.py**  # a collection of network architectures\r\n  - **envs** # a collection of environments\r\n  \t- XxxEnv.py  # a training environment for RL\r\n  - train # a collection of training programs\r\n      \t- demo.py  # a collection of demos\r\n  \t- config.py  # configurations (hyper-parameter)\r\n  \t- **run.py**  # training loop\r\n  \t- worker.py  # the worker class (explores the env, saving the data to replay buffer)\r\n  \t- learner.py  # the learner class (update the networks, using the data in replay buffer)\r\n  \t- evaluator.py  # the evaluator class (evaluate the cumulative rewards of policy network)\r\n  \t- replay_buffer.py # the buffer class (save sequences of transitions for training)\r\n- **elegantrl_helloworld** # tutorial version\r\n  - config.py  # configurations (hyper-parameter)\r\n  - **agent.py**  # DRL algorithms\r\n  - **net.py**  # network architectures \r\n  - **run.py**  # training loop\r\n  - **env.py**  # environments for RL training\r\n\r\n- **examples** # a collection of example codes\r\n- **ready-to-run Google-Colab notebooks**\r\n  - quickstart_Pendulum_v1.ipynb\r\n  - tutorial_BipedalWalker_v3.ipynb\r\n  - tutorial_Creating_ChasingVecEnv.ipynb\r\n  - tutorial_LunarLanderContinuous_v2.ipynb\r\n- **unit_tests** # a collection of tests\r\n\r\n## Experimental Demos\r\n\r\n### More efficient than Ray RLlib\r\n\r\nExperiments on Ant (MuJoCo), Humainoid (MuJoCo), Ant (Isaac Gym), Humanoid (Isaac Gym) # from left to right\r\n\r\n<div align=\"center\">\r\n\t<img align=\"center\" src=figs/envs.png width=\"800\">\r\n\t<img align=\"center\" src=figs/performance1.png width=\"800\">\r\n\t<img align=\"center\" src=figs/performance2.png width=\"800\">\r\n</div>\r\n\r\nElegantRL fully supports Isaac Gym that runs massively parallel simulation (e.g., 4096 sub-envs) on one GPU.\r\n\r\n### More stable than Stable-baseline 3\r\n\r\nExperiment on Hopper-v2 # ElegantRL achieves much smaller variance (average over 8 runs).\r\n\r\nAlso, PPO+H in ElegantRL completed the training process of 5M samples about 6x faster than Stable-Baseline3.\r\n\r\n<div align=\"center\">\r\n\t<img align=\"center\" src=figs/SB3_vs_ElegantRL.png width=\"640\">\r\n</div>\r\n\r\n## Testing and Contributing\r\n\r\nOur tests are written with the built-in `unittest` Python module for easy access. In order to run a specific test file (for example, `test_training_agents.py`), use the following command from the root directory:\r\n\r\n    python -m unittest unit_tests/test_training_agents.py\r\n\r\nIn order to run all the tests sequentially, you can use the following command:\r\n\r\n    python -m unittest discover\r\n\r\nPlease note that some of the tests require [Isaac Gym](https://developer.nvidia.com/isaac-gym) to be installed on your system. If it is not, any tests related to Isaac Gym will fail.\r\n\r\nWe welcome any contributions to the codebase, but we ask that you please **do not** submit/push code that breaks the tests. Also, please shy away from modifying the tests just to get your proposed changes to pass them. As it stands, the tests on their own are quite minimal (instantiating environments, training agents for one step, etc.), so if they're breaking, it's almost certainly a problem with your code and not with the tests.\r\n\r\nWe're actively working on refactoring and trying to make the codebase cleaner and more performant as a whole. If you'd like to help us clean up some code, we'd strongly encourage you to also watch [Uncle Bob's clean coding lessons](https://www.youtube.com/playlist?list=PLmmYSbUCWJ4x1GO839azG_BBw8rkh-zOj) if you haven't already.\r\n\r\n## Requirements\r\n\r\n    Necessary:\r\n    | Python 3.6+     |\r\n    | PyTorch 1.6+    |\r\n\r\n    Not necessary:\r\n    | Numpy 1.18+     | For ReplayBuffer. Numpy will be installed along with PyTorch.\r\n    | gym 0.17.0      | For env. Gym provides tutorial env for DRL training. (env.render() bug in gym==0.18 pyglet==1.6. Change to gym==0.17.0, pyglet==1.5)\r\n    | pybullet 2.7+   | For env. We use PyBullet (free) as an alternative of MuJoCo (not free).\r\n    | box2d-py 2.3.8  | For gym. Use pip install Box2D (instead of box2d-py)\r\n    | matplotlib 3.2  | For plots.\r\n\r\n    pip3 install gym==0.17.0 pybullet Box2D matplotlib # or pip install -r requirements.txt\r\n    \r\n    To install StarCraftII env,\r\n    bash ./elegantrl/envs/installsc2.sh\r\n    pip install -r sc2_requirements.txt\r\n\r\n## Citation:\r\n\r\nTo cite this repository:\r\n\r\n```\r\n@misc{erl,\r\n  author = {Liu, Xiao-Yang and Li, Zechu and Zhu, Ming and Wang, Zhaoran and Zheng, Jiahao},\r\n  title = {{ElegantRL}: Massively Parallel Framework for Cloud-native Deep Reinforcement Learning},\r\n  year = {2021},\r\n  publisher = {GitHub},\r\n  journal = {GitHub repository},\r\n  howpublished = {\\url{https://github.com/AI4Finance-Foundation/ElegantRL}},\r\n}\r\n```\r\n\r\n```\r\n@article{liu2021elegantrl,\r\n  title={ElegantRL-Podracer: Scalable and elastic library for cloud-native deep reinforcement learning},\r\n  author={Liu, Xiao-Yang and Li, Zechu and Yang, Zhuoran and Zheng, Jiahao and Wang, Zhaoran and Walid, Anwar and Guo, Jian and Jordan, Michael I},\r\n  journal={NeurIPS, Workshop on Deep Reinforcement Learning},\r\n  year={2021}\r\n}\r\n```\r\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "elegantrl",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "figs",
          "type": "tree",
          "content": null
        },
        {
          "name": "helloworld",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.115234375,
          "content": "# ML framework\ntorch\n\n# data handling\nnumpy\n\n# plot/simulation\nmatplotlib\ngymnasium\n\n# profiling (no necessary)\nwandb\n"
        },
        {
          "name": "rlsolver",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.244140625,
          "content": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"ElegantRL\",\n    version=\"0.3.10\",\n    author=\"AI4Finance Foundation\",\n    author_email=\"contact@ai4finance.org\",\n    url=\"https://github.com/AI4Finance-Foundation/ElegantRL\",\n    license=\"Apache 2.0\",\n    packages=find_packages(),\n    install_requires=[\n        \"th\",\n        \"numpy\",\n        \"gymnasium\",\n        \"matplotlib\",\n    ],\n    description=\"Lightweight, Efficient and Stable DRL Implementation Using PyTorch\",\n    classifiers=[\n        # Trove classifiers\n        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n    ],\n    keywords=\"Deep Reinforcement Learning\",\n    python_requires=\">=3.6\",\n)\n"
        },
        {
          "name": "tutorial_BipedalWalker_v3.ipynb",
          "type": "blob",
          "size": 11.765625,
          "content": "{\n    \"cells\": [\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"colab_type\": \"text\",\n                \"id\": \"view-in-github\"\n            },\n            \"source\": [\n                \"<a href=\\\"https://colab.research.google.com/github/AI4Finance-Foundation/ElegantRL/blob/master/tutorial_BipedalWalker_v3.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"c1gUG3OCJ5GS\"\n            },\n            \"source\": [\n                \"# **BipedalWalker-v3 Example in ElegantRL**\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"\\n\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"FGXyBBvL0dR2\"\n            },\n            \"source\": [\n                \"# **Task Description**\\n\",\n                \"\\n\",\n                \"[BipedalWalker-v3](https://gym.openai.com/envs/BipedalWalker-v2/) is a robotic task in OpenAI Gym since it performs one of the most fundamental skills: moving. In this task, our goal is to get a 2D bipedal walker to walk through rough terrain. BipedalWalker is a difficult task in continuous action space, and there are only a few RL implementations can reach the target reward.\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"DbamGVHC3AeW\"\n            },\n            \"source\": [\n                \"# **Part 1: Install ElegantRL**\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": null,\n            \"metadata\": {\n                \"colab\": {\n                    \"base_uri\": \"https://localhost:8080/\"\n                },\n                \"id\": \"U35bhkUqOqbS\",\n                \"outputId\": \"79ace170-9a20-46cd-db96-957fd42a472f\"\n            },\n            \"outputs\": [],\n            \"source\": [\n                \"# install elegantrl library\\n\",\n                \"!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"UVdmpnK_3Zcn\"\n            },\n            \"source\": [\n                \"# **Part 2: Import Packages**\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"*   **elegantrl**\\n\",\n                \"*   **OpenAI Gym**: a toolkit for developing and comparing reinforcement learning algorithms.\\n\",\n                \"\\n\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 1,\n            \"metadata\": {\n                \"id\": \"AAPdjovQrTpE\"\n            },\n            \"outputs\": [],\n            \"source\": [\n                \"import gym\\n\",\n                \"from elegantrl.agents import AgentPPO\\n\",\n                \"from elegantrl.train.config import get_gym_env_args, Config\\n\",\n                \"from elegantrl.train.run import *\\n\",\n                \"\\n\",\n                \"gym.logger.set_level(40) # Block warning\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"z2Ik5cDoyPGU\"\n            },\n            \"source\": [\n                \"# **Part 3: Get environment information**\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 2,\n            \"metadata\": {\n                \"colab\": {\n                    \"base_uri\": \"https://localhost:8080/\"\n                },\n                \"id\": \"wwkZXiHtyV6f\",\n                \"outputId\": \"880d25f5-d1f0-4cd2-8f78-bb5409330101\"\n            },\n            \"outputs\": [\n                {\n                    \"data\": {\n                        \"text/plain\": [\n                            \"{'env_name': 'BipedalWalker-v3',\\n\",\n                            \" 'num_envs': 1,\\n\",\n                            \" 'max_step': 1600,\\n\",\n                            \" 'state_dim': 24,\\n\",\n                            \" 'action_dim': 4,\\n\",\n                            \" 'if_discrete': False}\"\n                        ]\n                    },\n                    \"execution_count\": 2,\n                    \"metadata\": {},\n                    \"output_type\": \"execute_result\"\n                }\n            ],\n            \"source\": [\n                \"get_gym_env_args(gym.make(\\\"BipedalWalker-v3\\\"), if_print=False)\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"3n8zcgcn14uq\"\n            },\n            \"source\": [\n                \"# **Part 4: Specify Agent and Environment**\\n\",\n                \"\\n\",\n                \"*   **agent**: chooses a agent (DRL algorithm) from a set of agents in the [directory](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl/agents).\\n\",\n                \"*   **env_func**: the function to create an environment, in this case, we use gym.make to create BipedalWalker-v3.\\n\",\n                \"*   **env_args**: the environment information.\\n\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 8,\n            \"metadata\": {\n                \"id\": \"E03f6cTeajK4\"\n            },\n            \"outputs\": [],\n            \"source\": [\n                \"env_func = gym.make\\n\",\n                \"env_args = {\\n\",\n                \"    \\\"env_num\\\": 1,\\n\",\n                \"    \\\"env_name\\\": \\\"BipedalWalker-v3\\\",\\n\",\n                \"    \\\"max_step\\\": 1600,\\n\",\n                \"    \\\"state_dim\\\": 24,\\n\",\n                \"    \\\"action_dim\\\": 4,\\n\",\n                \"    \\\"if_discrete\\\": False,\\n\",\n                \"    \\\"target_return\\\": 300,\\n\",\n                \"    \\\"id\\\": \\\"BipedalWalker-v3\\\",\\n\",\n                \"}\\n\",\n                \"# env = build_env(env_class=env_func, env_args=env_args)\\n\",\n                \"args = Config(AgentPPO, env_class=env_func, env_args=env_args)\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"rcFcUkwfzHLE\"\n            },\n            \"source\": [\n                \"# **Part 4: Specify hyper-parameters**\\n\",\n                \"A list of hyper-parameters is available [here](https://elegantrl.readthedocs.io/en/latest/api/config.html).\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 9,\n            \"metadata\": {\n                \"id\": \"9WCAcmIfzGyE\"\n            },\n            \"outputs\": [],\n            \"source\": [\n                \"args.target_step = args.max_step * 4\\n\",\n                \"args.gamma = 0.98\\n\",\n                \"args.eval_times = 2**2\\n\",\n                \"args.repeat_times = 8\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"z1j5kLHF2dhJ\"\n            },\n            \"source\": [\n                \"# **Part 5: Train and Evaluate the Agent**\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"\\n\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 10,\n            \"metadata\": {\n                \"colab\": {\n                    \"base_uri\": \"https://localhost:8080/\"\n                },\n                \"id\": \"KGOPSD6da23k\",\n                \"outputId\": \"2a8ed03b-b306-45f8-c530-adf72438c5bd\"\n            },\n            \"outputs\": [\n                {\n                    \"name\": \"stdout\",\n                    \"output_type\": \"stream\",\n                    \"text\": [\n                        \"| Arguments Remove cwd: ./BipedalWalker-v3_PPO_0\\n\",\n                        \"| Evaluator:\\n\",\n                        \"| `step`: Number of samples, or total training steps, or running times of `env.step()`.\\n\",\n                        \"| `time`: Time spent from the start of training to this moment.\\n\",\n                        \"| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n                        \"| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n                        \"| `avgS`: Average of steps in an episode.\\n\",\n                        \"| `objC`: Objective of Critic network. Or call it loss function of critic network.\\n\",\n                        \"| `objA`: Objective of Actor network. It is the average Q value of the critic network.\\n\",\n                        \"################################################################################\\n\",\n                        \"ID     Step    Time |    avgR   stdR   avgS  stdS |    expR   objC   objA   etc.\\n\"\n                    ]\n                },\n                {\n                    \"name\": \"stderr\",\n                    \"output_type\": \"stream\",\n                    \"text\": [\n                        \"/home/adhi/ElegantRL/.env-erl/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\\n\",\n                        \"  if not isinstance(terminated, (bool, np.bool8)):\\n\"\n                    ]\n                },\n                {\n                    \"name\": \"stdout\",\n                    \"output_type\": \"stream\",\n                    \"text\": [\n                        \"0  2.05e+03       4 | -105.90    6.8    160     5 |   -5.64   1.22   0.06  -0.00\\n\",\n                        \"0  2.25e+04      40 | -101.19    0.3    156    32 |   -5.63   0.91   0.06  -0.00\\n\",\n                        \"0  4.30e+04      77 | -105.62    0.2    142     5 |   -5.65   1.96   0.06  -0.00\\n\",\n                        \"0  6.35e+04     116 | -106.94    0.1     96     2 |   -5.63   0.06   0.07  -0.00\\n\",\n                        \"0  8.40e+04     155 |  -76.43    0.8   1600     0 |   -5.69   0.08   0.05  -0.00\\n\"\n                    ]\n                },\n                {\n                    \"name\": \"stdout\",\n                    \"output_type\": \"stream\",\n                    \"text\": [\n                        \"0  1.04e+05     199 |  -72.13    0.1   1600     0 |   -5.62   0.07   0.05  -0.01\\n\"\n                    ]\n                }\n            ],\n            \"source\": [\n                \"train_agent(args)\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"JPXOxLSqh5cP\"\n            },\n            \"source\": [\n                \"Understanding the above results::\\n\",\n                \"*   **Step**: the total training steps.\\n\",\n                \"*  **MaxR**: the maximum reward.\\n\",\n                \"*   **avgR**: the average of the rewards.\\n\",\n                \"*   **stdR**: the standard deviation of the rewards.\\n\",\n                \"*   **objA**: the objective function value of Actor Network (Policy Network).\\n\",\n                \"*   **objC**: the objective function value (Q-value)  of Critic Network (Value Network).\"\n            ]\n        }\n    ],\n    \"metadata\": {\n        \"accelerator\": \"GPU\",\n        \"colab\": {\n            \"collapsed_sections\": [],\n            \"include_colab_link\": true,\n            \"name\": \"tutorial_BipedalWalker-v3.ipynb\",\n            \"provenance\": []\n        },\n        \"kernelspec\": {\n            \"display_name\": \"Python 3.9.13 64-bit (microsoft store)\",\n            \"language\": \"python\",\n            \"name\": \"python3\"\n        },\n        \"language_info\": {\n            \"codemirror_mode\": {\n                \"name\": \"ipython\",\n                \"version\": 3\n            },\n            \"file_extension\": \".py\",\n            \"mimetype\": \"text/x-python\",\n            \"name\": \"python\",\n            \"nbconvert_exporter\": \"python\",\n            \"pygments_lexer\": \"ipython3\",\n            \"version\": \"3.10.6\"\n        },\n        \"vscode\": {\n            \"interpreter\": {\n                \"hash\": \"8fec15aaf15af2f7b25d7149644915fb0538c5beb7ab358bd639337cd8050469\"\n            }\n        }\n    },\n    \"nbformat\": 4,\n    \"nbformat_minor\": 0\n}\n"
        },
        {
          "name": "tutorial_Creating_ChasingVecEnv.ipynb",
          "type": "blob",
          "size": 18.44140625,
          "content": "{\n \"nbformat\": 4,\n \"nbformat_minor\": 0,\n \"metadata\": {\n  \"colab\": {\n   \"name\": \"tutorial_Creating_ChasingVecEnv.ipynb\",\n   \"provenance\": [],\n   \"authorship_tag\": \"ABX9TyO8laSVRpW52hqbf2o1VXta\",\n   \"include_colab_link\": true\n  },\n  \"kernelspec\": {\n   \"name\": \"python3\",\n   \"display_name\": \"Python 3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  },\n  \"accelerator\": \"GPU\"\n },\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"view-in-github\",\n    \"colab_type\": \"text\"\n   },\n   \"source\": [\n    \"<a href=\\\"https://colab.research.google.com/github/AI4Finance-Foundation/ElegantRL/blob/master/tutorial_Creating_ChasingVecEnv.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"colab\": {\n     \"base_uri\": \"https://localhost:8080/\"\n    },\n    \"id\": \"8g5P1KyKLer3\",\n    \"outputId\": \"3e5b00f0-17b4-4fa3-e4c5-6b7bc25e3c3b\"\n   },\n   \"outputs\": [\n    {\n     \"output_type\": \"stream\",\n     \"name\": \"stdout\",\n     \"text\": [\n      \"Env num: 4096\\n\",\n      \"Steps: tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\\n\",\n      \"Steps: tensor([2., 2., 2.,  ..., 2., 2., 2.], device='cuda:0')\\n\",\n      \"Steps: tensor([3., 3., 3.,  ..., 3., 3., 3.], device='cuda:0')\\n\",\n      \"Steps: tensor([4., 4., 4.,  ..., 4., 4., 4.], device='cuda:0')\\n\",\n      \"Steps: tensor([5., 5., 5.,  ..., 5., 5., 5.], device='cuda:0')\\n\",\n      \"Steps: tensor([6., 6., 6.,  ..., 6., 6., 6.], device='cuda:0')\\n\",\n      \"Steps: tensor([7., 7., 7.,  ..., 7., 7., 7.], device='cuda:0')\\n\",\n      \"Steps: tensor([8., 8., 8.,  ..., 8., 8., 8.], device='cuda:0')\\n\",\n      \"Steps: tensor([9., 9., 9.,  ..., 9., 9., 9.], device='cuda:0')\\n\",\n      \"Steps: tensor([10., 10., 10.,  ..., 10., 10., 10.], device='cuda:0')\\n\",\n      \"Steps: tensor([11., 11., 11.,  ..., 11., 11., 11.], device='cuda:0')\\n\",\n      \"Steps: tensor([12., 12., 12.,  ..., 12., 12., 12.], device='cuda:0')\\n\",\n      \"Steps: tensor([13., 13., 13.,  ..., 13., 13., 13.], device='cuda:0')\\n\",\n      \"Steps: tensor([14., 14., 14.,  ..., 14., 14., 14.], device='cuda:0')\\n\",\n      \"Steps: tensor([15., 15., 15.,  ..., 15., 15., 15.], device='cuda:0')\\n\",\n      \"Steps: tensor([16., 16., 16.,  ..., 16., 16., 16.], device='cuda:0')\\n\",\n      \"Steps: tensor([17., 17., 17.,  ..., 17., 17., 17.], device='cuda:0')\\n\",\n      \"Steps: tensor([18., 18., 18.,  ..., 18., 18., 18.], device='cuda:0')\\n\",\n      \"Steps: tensor([19., 19., 19.,  ..., 19., 19., 19.], device='cuda:0')\\n\",\n      \"Steps: tensor([20., 20., 20.,  ..., 20., 20., 20.], device='cuda:0')\\n\",\n      \"Steps: tensor([21., 21., 21.,  ..., 21., 21., 21.], device='cuda:0')\\n\",\n      \"Steps: tensor([22., 22., 22.,  ..., 22., 22., 22.], device='cuda:0')\\n\",\n      \"Steps: tensor([23., 23., 23.,  ..., 23., 23., 23.], device='cuda:0')\\n\",\n      \"Steps: tensor([24., 24., 24.,  ..., 24., 24., 24.], device='cuda:0')\\n\",\n      \"Steps: tensor([25., 25., 25.,  ..., 25., 25., 25.], device='cuda:0')\\n\",\n      \"Steps: tensor([26., 26., 26.,  ..., 26., 26., 26.], device='cuda:0')\\n\",\n      \"Steps: tensor([27., 27., 27.,  ..., 27., 27., 27.], device='cuda:0')\\n\",\n      \"Steps: tensor([28., 28., 28.,  ..., 28., 28., 28.], device='cuda:0')\\n\",\n      \"Steps: tensor([29., 29., 29.,  ..., 29., 29., 29.], device='cuda:0')\\n\",\n      \"Steps: tensor([30., 30., 30.,  ..., 30., 30., 30.], device='cuda:0')\\n\",\n      \"Steps: tensor([31., 31., 31.,  ..., 31., 31., 31.], device='cuda:0')\\n\",\n      \"Steps: tensor([32., 32., 32.,  ..., 32., 32., 32.], device='cuda:0')\\n\",\n      \"Steps: tensor([33., 33., 33.,  ..., 33., 33., 33.], device='cuda:0')\\n\",\n      \"Steps: tensor([34., 34., 34.,  ..., 34., 34., 34.], device='cuda:0')\\n\",\n      \"Steps: tensor([35., 35., 35.,  ..., 35., 35., 35.], device='cuda:0')\\n\",\n      \"Steps: tensor([36., 36., 36.,  ..., 36., 36., 36.], device='cuda:0')\\n\",\n      \"Steps: tensor([37., 37., 37.,  ..., 37., 37., 37.], device='cuda:0')\\n\",\n      \"Steps: tensor([38., 38., 38.,  ..., 38., 38., 38.], device='cuda:0')\\n\",\n      \"Steps: tensor([39., 39., 39.,  ..., 39., 39., 39.], device='cuda:0')\\n\",\n      \"Steps: tensor([40., 40., 40.,  ..., 40., 40., 40.], device='cuda:0')\\n\",\n      \"Steps: tensor([41., 41., 41.,  ..., 41., 41., 41.], device='cuda:0')\\n\",\n      \"Steps: tensor([42., 42., 42.,  ..., 42., 42., 42.], device='cuda:0')\\n\",\n      \"Steps: tensor([43., 43., 43.,  ..., 43., 43., 43.], device='cuda:0')\\n\",\n      \"Steps: tensor([44., 44., 44.,  ..., 44., 44., 44.], device='cuda:0')\\n\",\n      \"Steps: tensor([45., 45., 45.,  ..., 45., 45., 45.], device='cuda:0')\\n\",\n      \"Steps: tensor([46., 46., 46.,  ..., 46., 46., 46.], device='cuda:0')\\n\",\n      \"Steps: tensor([47., 47., 47.,  ..., 47., 47., 47.], device='cuda:0')\\n\",\n      \"Steps: tensor([48., 48., 48.,  ..., 48., 48., 48.], device='cuda:0')\\n\",\n      \"Steps: tensor([49., 49., 49.,  ..., 49., 49., 49.], device='cuda:0')\\n\",\n      \"Steps: tensor([50., 50., 50.,  ..., 50., 50., 50.], device='cuda:0')\\n\",\n      \"Steps: tensor([51., 51., 51.,  ..., 51., 51., 51.], device='cuda:0')\\n\",\n      \"Steps: tensor([52., 52., 52.,  ..., 52., 52., 52.], device='cuda:0')\\n\",\n      \"Steps: tensor([53., 53., 53.,  ..., 53., 53., 53.], device='cuda:0')\\n\",\n      \"Steps: tensor([54., 54., 54.,  ..., 54., 54., 54.], device='cuda:0')\\n\",\n      \"Steps: tensor([55., 55., 55.,  ..., 55., 55., 55.], device='cuda:0')\\n\",\n      \"Steps: tensor([56., 56., 56.,  ..., 56., 56., 56.], device='cuda:0')\\n\",\n      \"Steps: tensor([57., 57., 57.,  ..., 57., 57., 57.], device='cuda:0')\\n\",\n      \"Steps: tensor([58., 58., 58.,  ..., 58., 58., 58.], device='cuda:0')\\n\",\n      \"Steps: tensor([59., 59., 59.,  ..., 59., 59., 59.], device='cuda:0')\\n\",\n      \"Steps: tensor([60., 60., 60.,  ..., 60., 60., 60.], device='cuda:0')\\n\",\n      \"Steps: tensor([61., 61., 61.,  ..., 61., 61., 61.], device='cuda:0')\\n\",\n      \"Steps: tensor([62., 62., 62.,  ..., 62., 62., 62.], device='cuda:0')\\n\",\n      \"Steps: tensor([63., 63., 63.,  ..., 63., 63., 63.], device='cuda:0')\\n\",\n      \"Steps: tensor([64., 64., 64.,  ..., 64., 64., 64.], device='cuda:0')\\n\",\n      \"Steps: tensor([65., 65., 65.,  ..., 65., 65., 65.], device='cuda:0')\\n\",\n      \"Steps: tensor([66., 66., 66.,  ..., 66., 66., 66.], device='cuda:0')\\n\",\n      \"Steps: tensor([67., 67., 67.,  ..., 67., 67., 67.], device='cuda:0')\\n\",\n      \"Steps: tensor([68., 68., 68.,  ..., 68., 68., 68.], device='cuda:0')\\n\",\n      \"Steps: tensor([69., 69., 69.,  ..., 69., 69., 69.], device='cuda:0')\\n\",\n      \"Steps: tensor([70., 70., 70.,  ..., 70., 70., 70.], device='cuda:0')\\n\",\n      \"Steps: tensor([71., 71., 71.,  ..., 71., 71., 71.], device='cuda:0')\\n\",\n      \"Steps: tensor([72., 72., 72.,  ..., 72., 72., 72.], device='cuda:0')\\n\",\n      \"Steps: tensor([73., 73., 73.,  ..., 73., 73., 73.], device='cuda:0')\\n\",\n      \"Steps: tensor([74., 74., 74.,  ..., 74., 74., 74.], device='cuda:0')\\n\",\n      \"Steps: tensor([75., 75., 75.,  ..., 75., 75., 75.], device='cuda:0')\\n\",\n      \"Steps: tensor([76., 76., 76.,  ..., 76., 76., 76.], device='cuda:0')\\n\",\n      \"Steps: tensor([77., 77., 77.,  ..., 77., 77., 77.], device='cuda:0')\\n\",\n      \"Steps: tensor([78., 78., 78.,  ..., 78., 78., 78.], device='cuda:0')\\n\",\n      \"Steps: tensor([79., 79., 79.,  ..., 79., 79., 79.], device='cuda:0')\\n\",\n      \"Steps: tensor([80., 80., 80.,  ..., 80., 80., 80.], device='cuda:0')\\n\",\n      \"Steps: tensor([81., 81., 81.,  ..., 81., 81., 81.], device='cuda:0')\\n\",\n      \"Steps: tensor([82., 82., 82.,  ..., 82., 82., 82.], device='cuda:0')\\n\",\n      \"Steps: tensor([83., 83., 83.,  ..., 83., 83., 83.], device='cuda:0')\\n\",\n      \"Steps: tensor([84., 84., 84.,  ..., 84., 84., 84.], device='cuda:0')\\n\",\n      \"Steps: tensor([85., 85., 85.,  ..., 85., 85., 85.], device='cuda:0')\\n\",\n      \"Steps: tensor([86., 86., 86.,  ..., 86., 86., 86.], device='cuda:0')\\n\",\n      \"Steps: tensor([87., 87., 87.,  ..., 87., 87., 87.], device='cuda:0')\\n\",\n      \"Steps: tensor([88., 88., 88.,  ..., 88., 88., 88.], device='cuda:0')\\n\",\n      \"Steps: tensor([89., 89., 89.,  ..., 89., 89., 89.], device='cuda:0')\\n\",\n      \"Steps: tensor([90., 90., 90.,  ..., 90., 90., 90.], device='cuda:0')\\n\",\n      \"Steps: tensor([91., 91., 91.,  ..., 91., 91., 91.], device='cuda:0')\\n\",\n      \"Steps: tensor([92., 92., 92.,  ..., 92., 92., 92.], device='cuda:0')\\n\",\n      \"Steps: tensor([93., 93., 93.,  ..., 93., 93., 93.], device='cuda:0')\\n\",\n      \"Steps: tensor([94., 94., 94.,  ..., 94., 94., 94.], device='cuda:0')\\n\",\n      \"Steps: tensor([95., 95., 95.,  ..., 95., 95., 95.], device='cuda:0')\\n\",\n      \"Steps: tensor([96., 96., 96.,  ..., 96., 96., 96.], device='cuda:0')\\n\",\n      \"Steps: tensor([97., 97., 97.,  ..., 97., 97., 97.], device='cuda:0')\\n\",\n      \"Steps: tensor([98., 98., 98.,  ..., 98., 98., 98.], device='cuda:0')\\n\",\n      \"Steps: tensor([99., 99., 99.,  ..., 99., 99., 99.], device='cuda:0')\\n\",\n      \"Steps: tensor([100., 100., 100.,  ..., 100., 100., 100.], device='cuda:0')\\n\",\n      \"Steps: tensor([101., 101., 101.,  ..., 101., 101., 101.], device='cuda:0')\\n\",\n      \"Steps: tensor([102., 102., 102.,  ..., 102., 102., 102.], device='cuda:0')\\n\",\n      \"Steps: tensor([103., 103., 103.,  ..., 103., 103., 103.], device='cuda:0')\\n\",\n      \"Steps: tensor([104., 104., 104.,  ..., 104., 104., 104.], device='cuda:0')\\n\",\n      \"Steps: tensor([105., 105., 105.,  ..., 105., 105., 105.], device='cuda:0')\\n\",\n      \"Steps: tensor([106., 106., 106.,  ..., 106., 106., 106.], device='cuda:0')\\n\",\n      \"Steps: tensor([107., 107., 107.,  ..., 107., 107., 107.], device='cuda:0')\\n\",\n      \"Steps: tensor([108., 108., 108.,  ..., 108., 108., 108.], device='cuda:0')\\n\",\n      \"Steps: tensor([109., 109., 109.,  ..., 109., 109., 109.], device='cuda:0')\\n\",\n      \"Steps: tensor([110., 110., 110.,  ..., 110., 110., 110.], device='cuda:0')\\n\",\n      \"Steps: tensor([111., 111., 111.,  ..., 111., 111., 111.], device='cuda:0')\\n\",\n      \"Steps: tensor([112., 112., 112.,  ..., 112., 112., 112.], device='cuda:0')\\n\",\n      \"Steps: tensor([113., 113., 113.,  ..., 113., 113., 113.], device='cuda:0')\\n\",\n      \"Steps: tensor([114., 114., 114.,  ..., 114., 114., 114.], device='cuda:0')\\n\",\n      \"Steps: tensor([115., 115., 115.,  ..., 115., 115., 115.], device='cuda:0')\\n\",\n      \"Steps: tensor([116., 116., 116.,  ..., 116., 116., 116.], device='cuda:0')\\n\",\n      \"Steps: tensor([117., 117., 117.,  ..., 117., 117., 117.], device='cuda:0')\\n\",\n      \"Steps: tensor([118., 118., 118.,  ..., 118., 118., 118.], device='cuda:0')\\n\",\n      \"Steps: tensor([119., 119., 119.,  ..., 119., 119., 119.], device='cuda:0')\\n\",\n      \"Steps: tensor([120., 120., 120.,  ..., 120., 120., 120.], device='cuda:0')\\n\",\n      \"Steps: tensor([121., 121., 121.,  ..., 121., 121., 121.], device='cuda:0')\\n\",\n      \"Steps: tensor([122., 122., 122.,  ..., 122., 122., 122.], device='cuda:0')\\n\",\n      \"Steps: tensor([123., 123., 123.,  ..., 123., 123., 123.], device='cuda:0')\\n\",\n      \"Steps: tensor([124., 124., 124.,  ..., 124., 124., 124.], device='cuda:0')\\n\",\n      \"Steps: tensor([125., 125., 125.,  ..., 125., 125., 125.], device='cuda:0')\\n\",\n      \"Steps: tensor([126., 126., 126.,  ..., 126., 126., 126.], device='cuda:0')\\n\",\n      \"Steps: tensor([127., 127., 127.,  ..., 127., 127., 127.], device='cuda:0')\\n\",\n      \"Steps: tensor([128., 128., 128.,  ..., 128., 128., 128.], device='cuda:0')\\n\",\n      \"Steps: tensor([129., 129., 129.,  ..., 129., 129., 129.], device='cuda:0')\\n\",\n      \"Steps: tensor([130., 130., 130.,  ..., 130., 130., 130.], device='cuda:0')\\n\",\n      \"Steps: tensor([131., 131., 131.,  ..., 131., 131., 131.], device='cuda:0')\\n\",\n      \"Steps: tensor([132., 132., 132.,  ..., 132., 132., 132.], device='cuda:0')\\n\",\n      \"Steps: tensor([133., 133., 133.,  ..., 133., 133., 133.], device='cuda:0')\\n\",\n      \"Steps: tensor([134., 134., 134.,  ..., 134., 134., 134.], device='cuda:0')\\n\",\n      \"Steps: tensor([135., 135., 135.,  ..., 135., 135., 135.], device='cuda:0')\\n\",\n      \"Steps: tensor([136., 136., 136.,  ..., 136., 136., 136.], device='cuda:0')\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"import torch\\n\",\n    \"import numpy as np\\n\",\n    \"import numpy.random as rd\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"TargetReturnDict = {\\n\",\n    \"    2: 5.5,\\n\",\n    \"    3: 3.5,\\n\",\n    \"    4: 2.5,\\n\",\n    \"    8: -1.5,  # -1.37\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class ChasingVecEnv:\\n\",\n    \"    def __init__(self, dim=2, env_num=32, device_id=0):\\n\",\n    \"        self.dim = dim\\n\",\n    \"        self.init_distance = 8.0\\n\",\n    \"\\n\",\n    \"        # reset\\n\",\n    \"        self.p0s = None  # position\\n\",\n    \"        self.v0s = None  # velocity\\n\",\n    \"        self.p1s = None\\n\",\n    \"        self.v1s = None\\n\",\n    \"\\n\",\n    \"        self.distances = None  # a tensor of distance between point0 and point1\\n\",\n    \"        self.cur_steps = None  # a tensor of current step number\\n\",\n    \"        # env.step() is a function, so I can't name it `steps`\\n\",\n    \"\\n\",\n    \"        \\\"\\\"\\\"env info\\\"\\\"\\\"\\n\",\n    \"        self.env_name = \\\"ChasingVecEnv\\\"\\n\",\n    \"        self.state_dim = self.dim * 4\\n\",\n    \"        self.action_dim = self.dim\\n\",\n    \"        self.max_step = 2**10\\n\",\n    \"        self.if_discrete = False\\n\",\n    \"        self.target_return = TargetReturnDict[dim]\\n\",\n    \"\\n\",\n    \"        self.env_num = env_num\\n\",\n    \"        self.device = torch.device(\\\"cpu\\\" if device_id == -1 else f\\\"cuda:{device_id}\\\")\\n\",\n    \"\\n\",\n    \"    def reset(self):\\n\",\n    \"        self.p0s = torch.zeros(\\n\",\n    \"            (self.env_num, self.dim), dtype=torch.float32, device=self.device\\n\",\n    \"        )\\n\",\n    \"        self.v0s = torch.zeros(\\n\",\n    \"            (self.env_num, self.dim), dtype=torch.float32, device=self.device\\n\",\n    \"        )\\n\",\n    \"        self.p1s = torch.zeros(\\n\",\n    \"            (self.env_num, self.dim), dtype=torch.float32, device=self.device\\n\",\n    \"        )\\n\",\n    \"        self.v1s = torch.zeros(\\n\",\n    \"            (self.env_num, self.dim), dtype=torch.float32, device=self.device\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"        self.cur_steps = torch.zeros(\\n\",\n    \"            self.env_num, dtype=torch.float32, device=self.device\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"        for env_i in range(self.env_num):\\n\",\n    \"            self.reset_env_i(env_i)\\n\",\n    \"\\n\",\n    \"        self.distances = ((self.p0s - self.p1s) ** 2).sum(dim=1) ** 0.5\\n\",\n    \"\\n\",\n    \"        return self.get_state()\\n\",\n    \"\\n\",\n    \"    def reset_env_i(self, i):\\n\",\n    \"        self.p0s[i] = torch.normal(0, 1, size=(self.dim,))\\n\",\n    \"        self.v0s[i] = torch.zeros((self.dim,))\\n\",\n    \"        self.p1s[i] = torch.normal(-self.init_distance, 1, size=(self.dim,))\\n\",\n    \"        self.v1s[i] = torch.zeros((self.dim,))\\n\",\n    \"\\n\",\n    \"        self.cur_steps[i] = 0\\n\",\n    \"\\n\",\n    \"    def step(self, actions):\\n\",\n    \"        \\\"\\\"\\\"\\n\",\n    \"        :param actions: [tensor] actions.shape == (env_num, action_dim)\\n\",\n    \"        :return: next_states [tensor] next_states.shape == (env_num, state_dim)\\n\",\n    \"        :return: rewards [tensor] rewards == (env_num, )\\n\",\n    \"        :return: dones [tensor] dones == (env_num, ), done = 1. if done else 0.\\n\",\n    \"        :return: None [None or dict]\\n\",\n    \"        \\\"\\\"\\\"\\n\",\n    \"        # assert actions.get_device() == self.device.index\\n\",\n    \"        actions_l2 = (actions**2).sum(dim=1, keepdim=True) ** 0.5\\n\",\n    \"        actions_l2 = actions_l2.clamp_min(1.0)\\n\",\n    \"        actions = actions / actions_l2\\n\",\n    \"\\n\",\n    \"        self.v1s *= 0.75\\n\",\n    \"        self.v1s += actions\\n\",\n    \"        self.p1s += self.v1s * 0.01\\n\",\n    \"\\n\",\n    \"        self.v0s *= 0.50\\n\",\n    \"        self.v0s += torch.rand(\\n\",\n    \"            size=(self.env_num, self.dim), dtype=torch.float32, device=self.device\\n\",\n    \"        )\\n\",\n    \"        self.p0s += self.v0s * 0.01\\n\",\n    \"\\n\",\n    \"        \\\"\\\"\\\"reward\\\"\\\"\\\"\\n\",\n    \"        distances = ((self.p0s - self.p1s) ** 2).sum(dim=1) ** 0.5\\n\",\n    \"        rewards = self.distances - distances - actions_l2.squeeze(1) * 0.02\\n\",\n    \"        self.distances = distances\\n\",\n    \"\\n\",\n    \"        \\\"\\\"\\\"done\\\"\\\"\\\"\\n\",\n    \"        self.cur_steps += 1  # array\\n\",\n    \"        dones = (distances < self.dim) | (self.cur_steps == self.max_step)\\n\",\n    \"        for env_i in range(self.env_num):\\n\",\n    \"            if dones[env_i]:\\n\",\n    \"                self.reset_env_i(env_i)\\n\",\n    \"        dones = dones.type(torch.float32)\\n\",\n    \"\\n\",\n    \"        \\\"\\\"\\\"next_state\\\"\\\"\\\"\\n\",\n    \"        next_states = self.get_state()\\n\",\n    \"\\n\",\n    \"        # assert next_states.get_device() == self.device.index\\n\",\n    \"        # assert rewards.get_device() == self.device.index\\n\",\n    \"        # assert dones.get_device() == self.device.index\\n\",\n    \"        return next_states, rewards, dones, None\\n\",\n    \"\\n\",\n    \"    def get_state(self):\\n\",\n    \"        return torch.cat((self.p0s, self.v0s, self.p1s, self.v1s), dim=1)\\n\",\n    \"\\n\",\n    \"    @staticmethod\\n\",\n    \"    def get_action(states):\\n\",\n    \"        states_reshape = states.reshape((states.shape[0], 4, -1))\\n\",\n    \"        p0s = states_reshape[:, 0]\\n\",\n    \"        p1s = states_reshape[:, 2]\\n\",\n    \"        actions = p0s - p1s\\n\",\n    \"        return actions\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"def check_chasing_vec_env():\\n\",\n    \"    env = ChasingVecEnv(dim=2, env_num=4096, device_id=0)\\n\",\n    \"    print(\\\"Env num:\\\", env.env_num)\\n\",\n    \"\\n\",\n    \"    reward_sums = [\\n\",\n    \"        0.0,\\n\",\n    \"    ] * env.env_num  # episode returns\\n\",\n    \"    reward_sums_list = [\\n\",\n    \"        [],\\n\",\n    \"    ] * env.env_num\\n\",\n    \"\\n\",\n    \"    states = env.reset()\\n\",\n    \"    for _ in range(env.max_step * 4):\\n\",\n    \"        actions = env.get_action(states)\\n\",\n    \"        states, rewards, dones, _ = env.step(actions)\\n\",\n    \"        print(\\\"Steps:\\\", env.cur_steps)\\n\",\n    \"        for env_i in range(env.env_num):\\n\",\n    \"            reward_sums[env_i] += rewards[env_i].item()\\n\",\n    \"\\n\",\n    \"            if dones[env_i]:\\n\",\n    \"                print(\\n\",\n    \"                    f\\\"{env.distances[env_i].item():8.4f}    {actions[env_i].detach().cpu().numpy().round(2)}\\\"\\n\",\n    \"                )\\n\",\n    \"                reward_sums_list[env_i].append(reward_sums[env_i])\\n\",\n    \"                reward_sums[env_i] = 0.0\\n\",\n    \"\\n\",\n    \"    reward_sums_list = np.array(reward_sums_list)\\n\",\n    \"    print(\\\"shape:\\\", reward_sums_list.shape)\\n\",\n    \"    print(\\\"mean: \\\", np.mean(reward_sums_list, axis=1))\\n\",\n    \"    print(\\\"std:  \\\", np.std(reward_sums_list, axis=1))\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"if __name__ == \\\"__main__\\\":\\n\",\n    \"    check_chasing_vec_env()\"\n   ]\n  }\n ]\n}"
        },
        {
          "name": "tutorial_LunarLanderContinuous_v2.ipynb",
          "type": "blob",
          "size": 15.4443359375,
          "content": "{\n    \"cells\": [\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"colab_type\": \"text\",\n                \"id\": \"view-in-github\"\n            },\n            \"source\": [\n                \"<a href=\\\"https://colab.research.google.com/github/AI4Finance-Foundation/ElegantRL/blob/master/tutorial_LunarLanderContinuous_v2.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"c1gUG3OCJ5GS\"\n            },\n            \"source\": [\n                \"# **LunarLanderContinuous-v2 Example in ElegantRL**\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"\\n\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"FGXyBBvL0dR2\"\n            },\n            \"source\": [\n                \"# **Task Description**\\n\",\n                \"\\n\",\n                \"[LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) is a robotic control task. The goal is to get a Lander to rest on the landing pad. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points.\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"DbamGVHC3AeW\"\n            },\n            \"source\": [\n                \"# **Part 1: Install ElegantRL**\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 1,\n            \"metadata\": {\n                \"colab\": {\n                    \"base_uri\": \"https://localhost:8080/\"\n                },\n                \"id\": \"U35bhkUqOqbS\",\n                \"outputId\": \"0acfcd5b-ebfe-4dba-dd65-10f5e9a3a58e\"\n            },\n            \"outputs\": [\n                {\n                    \"name\": \"stdout\",\n                    \"output_type\": \"stream\",\n                    \"text\": [\n                        \"Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\\n\",\n                        \"  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to /tmp/pip-req-build-q0f_9pry\\n\",\n                        \"  Running command git clone -q https://github.com/AI4Finance-LLC/ElegantRL.git /tmp/pip-req-build-q0f_9pry\\n\",\n                        \"Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (0.17.3)\\n\",\n                        \"Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (3.2.2)\\n\",\n                        \"Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (1.19.5)\\n\",\n                        \"Collecting pybullet\\n\",\n                        \"  Downloading pybullet-3.2.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (90.8 MB)\\n\",\n                        \"\\u001b[K     |████████████████████████████████| 90.8 MB 291 bytes/s \\n\",\n                        \"\\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (1.10.0+cu111)\\n\",\n                        \"Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (4.1.2.30)\\n\",\n                        \"Collecting box2d-py\\n\",\n                        \"  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\\n\",\n                        \"\\u001b[K     |████████████████████████████████| 448 kB 70.5 MB/s \\n\",\n                        \"\\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.4.1)\\n\",\n                        \"Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.5.0)\\n\",\n                        \"Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.3.0)\\n\",\n                        \"Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->elegantrl==0.3.3) (0.16.0)\\n\",\n                        \"Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (0.11.0)\\n\",\n                        \"Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (3.0.6)\\n\",\n                        \"Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (1.3.2)\\n\",\n                        \"Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (2.8.2)\\n\",\n                        \"Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->elegantrl==0.3.3) (1.15.0)\\n\",\n                        \"Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->elegantrl==0.3.3) (3.10.0.2)\\n\",\n                        \"Building wheels for collected packages: elegantrl\\n\",\n                        \"  Building wheel for elegantrl (setup.py) ... \\u001b[?25l\\u001b[?25hdone\\n\",\n                        \"  Created wheel for elegantrl: filename=elegantrl-0.3.3-py3-none-any.whl size=183567 sha256=a2b2116b1f175b6cad721c1dfeba30620a45100aa1a7e65fe9c19df809fe68d3\\n\",\n                        \"  Stored in directory: /tmp/pip-ephem-wheel-cache-ltz2mxds/wheels/52/9a/b3/08c8a0b5be22a65da0132538c05e7e961b1253c90d6845e0c6\\n\",\n                        \"Successfully built elegantrl\\n\",\n                        \"Installing collected packages: pybullet, box2d-py, elegantrl\\n\",\n                        \"Successfully installed box2d-py-2.3.8 elegantrl-0.3.3 pybullet-3.2.1\\n\"\n                    ]\n                }\n            ],\n            \"source\": [\n                \"# install elegantrl library\\n\",\n                \"!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"UVdmpnK_3Zcn\"\n            },\n            \"source\": [\n                \"# **Part 2: Import Packages**\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"*   **elegantrl**\\n\",\n                \"*   **OpenAI Gym**: a toolkit for developing and comparing reinforcement learning algorithms.\\n\",\n                \"\\n\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 2,\n            \"metadata\": {\n                \"id\": \"AAPdjovQrTpE\"\n            },\n            \"outputs\": [],\n            \"source\": [\n                \"import gym\\n\",\n                \"from elegantrl.agents import AgentModSAC\\n\",\n                \"from elegantrl.train.config import get_gym_env_args, Config\\n\",\n                \"from elegantrl.train.run import *\\n\",\n                \"\\n\",\n                \"gym.logger.set_level(40)  # Block warning\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"z2Ik5cDoyPGU\"\n            },\n            \"source\": [\n                \"# **Part 3: Get environment information**\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 3,\n            \"metadata\": {\n                \"colab\": {\n                    \"base_uri\": \"https://localhost:8080/\"\n                },\n                \"id\": \"wwkZXiHtyV6f\",\n                \"outputId\": \"24720b42-25ca-4491-8fff-c6e7470dedf5\"\n            },\n            \"outputs\": [\n                {\n                    \"data\": {\n                        \"text/plain\": [\n                            \"{'env_name': 'LunarLanderContinuous-v2',\\n\",\n                            \" 'num_envs': 1,\\n\",\n                            \" 'max_step': 1000,\\n\",\n                            \" 'state_dim': 8,\\n\",\n                            \" 'action_dim': 2,\\n\",\n                            \" 'if_discrete': False}\"\n                        ]\n                    },\n                    \"execution_count\": 3,\n                    \"metadata\": {},\n                    \"output_type\": \"execute_result\"\n                }\n            ],\n            \"source\": [\n                \"get_gym_env_args(gym.make(\\\"LunarLanderContinuous-v2\\\"), if_print=False)\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"3n8zcgcn14uq\"\n            },\n            \"source\": [\n                \"# **Part 4: Specify Agent and Environment**\\n\",\n                \"\\n\",\n                \"*   **agent**: chooses a agent (DRL algorithm) from a set of agents in the [directory](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl/agents).\\n\",\n                \"*   **env_func**: the function to create an environment, in this case, we use gym.make to create BipedalWalker-v3.\\n\",\n                \"*   **env_args**: the environment information.\\n\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 5,\n            \"metadata\": {\n                \"id\": \"E03f6cTeajK4\"\n            },\n            \"outputs\": [],\n            \"source\": [\n                \"env_func = gym.make\\n\",\n                \"env_args = {\\n\",\n                \"    \\\"env_num\\\": 1,\\n\",\n                \"    \\\"env_name\\\": \\\"LunarLanderContinuous-v2\\\",\\n\",\n                \"    \\\"max_step\\\": 1000,\\n\",\n                \"    \\\"state_dim\\\": 8,\\n\",\n                \"    \\\"action_dim\\\": 2,\\n\",\n                \"    \\\"if_discrete\\\": False,\\n\",\n                \"    \\\"target_return\\\": 200,\\n\",\n                \"    \\\"id\\\": \\\"LunarLanderContinuous-v2\\\",\\n\",\n                \"}\\n\",\n                \"args = Config(AgentModSAC, env_class=env_func, env_args=env_args)\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"rcFcUkwfzHLE\"\n            },\n            \"source\": [\n                \"# **Part 4: Specify hyper-parameters**\\n\",\n                \"A list of hyper-parameters is available [here](https://elegantrl.readthedocs.io/en/latest/api/config.html).\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 6,\n            \"metadata\": {\n                \"id\": \"9WCAcmIfzGyE\"\n            },\n            \"outputs\": [],\n            \"source\": [\n                \"args.target_step = args.max_step\\n\",\n                \"args.gamma = 0.99\\n\",\n                \"args.eval_times = 2**5\\n\",\n                \"args.random_seed = 2022\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"z1j5kLHF2dhJ\"\n            },\n            \"source\": [\n                \"# **Part 5: Train and Evaluate the Agent**\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"\\n\",\n                \"\\n\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"execution_count\": 7,\n            \"metadata\": {\n                \"colab\": {\n                    \"base_uri\": \"https://localhost:8080/\"\n                },\n                \"id\": \"KGOPSD6da23k\",\n                \"outputId\": \"84157a77-bcfa-406d-c25e-d3dea0e8fc20\"\n            },\n            \"outputs\": [\n                {\n                    \"name\": \"stdout\",\n                    \"output_type\": \"stream\",\n                    \"text\": [\n                        \"| Arguments Remove cwd: ./LunarLanderContinuous-v2_ModSAC_2022\\n\"\n                    ]\n                },\n                {\n                    \"name\": \"stderr\",\n                    \"output_type\": \"stream\",\n                    \"text\": [\n                        \"/home/adhi/ElegantRL/.env-erl/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\\n\",\n                        \"  if not isinstance(terminated, (bool, np.bool8)):\\n\"\n                    ]\n                },\n                {\n                    \"name\": \"stdout\",\n                    \"output_type\": \"stream\",\n                    \"text\": [\n                        \"| Evaluator:\\n\",\n                        \"| `step`: Number of samples, or total training steps, or running times of `env.step()`.\\n\",\n                        \"| `time`: Time spent from the start of training to this moment.\\n\",\n                        \"| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n                        \"| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n                        \"| `avgS`: Average of steps in an episode.\\n\",\n                        \"| `objC`: Objective of Critic network. Or call it loss function of critic network.\\n\",\n                        \"| `objA`: Objective of Actor network. It is the average Q value of the critic network.\\n\",\n                        \"################################################################################\\n\",\n                        \"ID     Step    Time |    avgR   stdR   avgS  stdS |    expR   objC   objA   etc.\\n\",\n                        \"0  5.12e+02       8 | -209.81  126.8    125    38 |   -2.32   5.02   0.01   0.35\\n\",\n                        \"0  2.10e+04     213 |   42.28  134.3    251    90 |   -0.04   4.05  23.89   0.16\\n\",\n                        \"0  4.15e+04     423 |    2.52   81.0    307    83 |    0.45   3.24  39.48   0.06\\n\",\n                        \"0  6.20e+04     625 |  -12.21   28.4    319    68 |   -0.10   2.42  31.58   0.05\\n\",\n                        \"0  8.24e+04     820 |  -67.12   38.4    325    61 |   -0.14   2.40  24.88   0.03\\n\"\n                    ]\n                }\n            ],\n            \"source\": [\n                \"train_agent(args)\"\n            ]\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {\n                \"id\": \"JPXOxLSqh5cP\"\n            },\n            \"source\": [\n                \"Understanding the above results::\\n\",\n                \"*   **Step**: the total training steps.\\n\",\n                \"*  **MaxR**: the maximum reward.\\n\",\n                \"*   **avgR**: the average of the rewards.\\n\",\n                \"*   **stdR**: the standard deviation of the rewards.\\n\",\n                \"*   **objA**: the objective function value of Actor Network (Policy Network).\\n\",\n                \"*   **objC**: the objective function value (Q-value)  of Critic Network (Value Network).\"\n            ]\n        }\n    ],\n    \"metadata\": {\n        \"accelerator\": \"GPU\",\n        \"colab\": {\n            \"collapsed_sections\": [],\n            \"include_colab_link\": true,\n            \"name\": \"tutorial_LunarLanderContinuous-v2.ipynb\",\n            \"provenance\": []\n        },\n        \"kernelspec\": {\n            \"display_name\": \"Python 3\",\n            \"name\": \"python3\"\n        },\n        \"language_info\": {\n            \"codemirror_mode\": {\n                \"name\": \"ipython\",\n                \"version\": 3\n            },\n            \"file_extension\": \".py\",\n            \"mimetype\": \"text/x-python\",\n            \"name\": \"python\",\n            \"nbconvert_exporter\": \"python\",\n            \"pygments_lexer\": \"ipython3\",\n            \"version\": \"3.10.6\"\n        }\n    },\n    \"nbformat\": 4,\n    \"nbformat_minor\": 0\n}\n"
        },
        {
          "name": "tutorial_Pendulum_v1.ipynb",
          "type": "blob",
          "size": 10.1123046875,
          "content": "{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"c1gUG3OCJ5GS\"\n      },\n      \"source\": [\n        \"# **Pendulum-v1 Example in ElegantRL-HelloWorld**\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"DbamGVHC3AeW\"\n      },\n      \"source\": [\n        \"# **Part 1: Install ElegantRL**\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": 1,\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"U35bhkUqOqbS\",\n        \"outputId\": \"1536fb29-9e4c-4598-f26f-a907c591a8ba\"\n      },\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\\n\",\n            \"  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to /tmp/pip-req-build-1hyi8kfj\\n\",\n            \"  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-LLC/ElegantRL.git /tmp/pip-req-build-1hyi8kfj\\n\",\n            \"  Resolved https://github.com/AI4Finance-LLC/ElegantRL.git to commit 6f7096c9f825d529b90cf8cba4fc75929af979ed\\n\",\n            \"  Preparing metadata (setup.py) ... \\u001b[?25l\\u001b[?25hdone\\n\",\n            \"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ElegantRL==0.3.10) (2.4.1+cu121)\\n\",\n            \"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ElegantRL==0.3.10) (1.26.4)\\n\",\n            \"Collecting gymnasium (from ElegantRL==0.3.10)\\n\",\n            \"  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\\n\",\n            \"Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ElegantRL==0.3.10) (3.7.1)\\n\",\n            \"Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->ElegantRL==0.3.10) (2.2.1)\\n\",\n            \"Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->ElegantRL==0.3.10) (4.12.2)\\n\",\n            \"Collecting farama-notifications>=0.0.1 (from gymnasium->ElegantRL==0.3.10)\\n\",\n            \"  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\\n\",\n            \"Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.10) (1.3.0)\\n\",\n            \"Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.10) (0.12.1)\\n\",\n            \"Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.10) (4.54.1)\\n\",\n            \"Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.10) (1.4.7)\\n\",\n            \"Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.10) (24.1)\\n\",\n            \"Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.10) (10.4.0)\\n\",\n            \"Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.10) (3.1.4)\\n\",\n            \"Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.10) (2.8.2)\\n\",\n            \"Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.10) (3.16.1)\\n\",\n            \"Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.10) (1.13.3)\\n\",\n            \"Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.10) (3.3)\\n\",\n            \"Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.10) (3.1.4)\\n\",\n            \"Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.10) (2024.6.1)\\n\",\n            \"Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ElegantRL==0.3.10) (1.16.0)\\n\",\n            \"Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ElegantRL==0.3.10) (2.1.5)\\n\",\n            \"Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ElegantRL==0.3.10) (1.3.0)\\n\",\n            \"Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\\n\",\n            \"\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m958.1/958.1 kB\\u001b[0m \\u001b[31m10.8 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n\",\n            \"\\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\\n\",\n            \"Building wheels for collected packages: ElegantRL\\n\",\n            \"  Building wheel for ElegantRL (setup.py) ... \\u001b[?25l\\u001b[?25hdone\\n\",\n            \"  Created wheel for ElegantRL: filename=ElegantRL-0.3.10-py3-none-any.whl size=385443 sha256=d56ab75d104550140310837a32d2d7234eebf76afc3846174eb25693d7e48db6\\n\",\n            \"  Stored in directory: /tmp/pip-ephem-wheel-cache-gx2sgm52/wheels/85/a7/e7/37369f52de5c5e77ba80ec8d0dd898ef99fbb23707e77a3958\\n\",\n            \"Successfully built ElegantRL\\n\",\n            \"Installing collected packages: farama-notifications, gymnasium, ElegantRL\\n\",\n            \"Successfully installed ElegantRL-0.3.10 farama-notifications-0.0.4 gymnasium-1.0.0\\n\"\n          ]\n        }\n      ],\n      \"source\": [\n        \"# install elegantrl library\\n\",\n        \"!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"3n8zcgcn14uq\"\n      },\n      \"source\": [\n        \"# **Part 2: Specify Environment and Agent**\\n\",\n        \"\\n\",\n        \"*   **agent**: chooses a agent (DRL algorithm) from a set of agents in the [directory](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl/agents).\\n\",\n        \"*   **env**: creates an environment for your agent.\\n\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"import gymnasium as gym\"\n      ],\n      \"metadata\": {\n        \"id\": \"1eyRQkk9pkBf\"\n      },\n      \"execution_count\": 2,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": 3,\n      \"metadata\": {\n        \"id\": \"E03f6cTeajK4\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"from elegantrl.train.config import Config\\n\",\n        \"from elegantrl.agents.AgentSAC import AgentSAC\\n\",\n        \"\\n\",\n        \"env = gym.make\\n\",\n        \"\\n\",\n        \"env_args = {\\n\",\n        \"    \\\"id\\\": \\\"Pendulum-v1\\\",\\n\",\n        \"    \\\"env_name\\\": \\\"Pendulum-v1\\\",\\n\",\n        \"    \\\"num_envs\\\": 1,\\n\",\n        \"    \\\"max_step\\\": 1000,\\n\",\n        \"    \\\"state_dim\\\": 3,\\n\",\n        \"    \\\"action_dim\\\": 1,\\n\",\n        \"    \\\"if_discrete\\\": False,\\n\",\n        \"    \\\"reward_scale\\\": 2**-1,\\n\",\n        \"    \\\"gpu_id\\\": 0, # if you have GPU\\n\",\n        \"}\\n\",\n        \"args = Config(AgentSAC, env_class=env, env_args=env_args)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"rcFcUkwfzHLE\"\n      },\n      \"source\": [\n        \"# **Part 3: Specify hyper-parameters**\\n\",\n        \"A list of hyper-parameters is available [here](https://elegantrl.readthedocs.io/en/latest/api/config.html).\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": 4,\n      \"metadata\": {\n        \"id\": \"9WCAcmIfzGyE\"\n      },\n      \"outputs\": [],\n      \"source\": [\n        \"args.max_step = 1000\\n\",\n        \"args.reward_scale = 2**-1  # RewardRange: -1800 < -200 < -50 < 0\\n\",\n        \"args.gamma = 0.97\\n\",\n        \"args.target_step = args.max_step\\n\",\n        \"args.eval_times = 2**3\\n\",\n        \"args.gpu_id = 0\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"z1j5kLHF2dhJ\"\n      },\n      \"source\": [\n        \"# **Part 4: Train and Evaluate the Agent**\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"KGOPSD6da23k\",\n        \"outputId\": \"fd200e35-217c-41d6-b01c-e4a8a535840c\"\n      },\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"| train_agent_multiprocessing() with GPU_ID 0\\n\",\n            \"| Arguments Remove cwd: ./Pendulum-v1_SAC_0\\n\"\n          ]\n        }\n      ],\n      \"source\": [\n        \"from elegantrl.train.run import train_agent\\n\",\n        \"\\n\",\n        \"train_agent(args)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"JPXOxLSqh5cP\"\n      },\n      \"source\": [\n        \"Understanding the above results::\\n\",\n        \"*   **Step**: the total training steps.\\n\",\n        \"*  **MaxR**: the maximum reward.\\n\",\n        \"*   **avgR**: the average of the rewards.\\n\",\n        \"*   **stdR**: the standard deviation of the rewards.\\n\",\n        \"*   **objA**: the objective function value of Actor Network (Policy Network).\\n\",\n        \"*   **objC**: the objective function value (Q-value)  of Critic Network (Value Network).\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"accelerator\": \"GPU\",\n    \"colab\": {\n      \"name\": \"quickstart_Pendulum-v1.ipynb\",\n      \"provenance\": []\n    },\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"codemirror_mode\": {\n        \"name\": \"ipython\",\n        \"version\": 3\n      },\n      \"file_extension\": \".py\",\n      \"mimetype\": \"text/x-python\",\n      \"name\": \"python\",\n      \"nbconvert_exporter\": \"python\",\n      \"pygments_lexer\": \"ipython3\",\n      \"version\": \"3.10.6\"\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0\n}"
        },
        {
          "name": "tutorial_helloworld_DQN_DDPG_PPO.ipynb",
          "type": "blob",
          "size": 55.6689453125,
          "content": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"name\": \"tutorial_helloworld_DQN_DDPG_PPO.ipynb\",\n      \"provenance\": [],\n      \"collapsed_sections\": []\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    }\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"c1gUG3OCJ5GS\"\n      },\n      \"source\": [\n        \"# Demo: ElegantRL_HelloWorld_tutorial (DQN --> DDPG --> PPO)\\n\",\n        \"\\n\",\n        \"We suggest to following this order to quickly learn about RL:\\n\",\n        \"- DQN (Deep Q Network), a basic RL algorithms in discrete action space.\\n\",\n        \"- DDPG (Deep Deterministic Policy Gradient), a basic RL algorithm in continuous action space.\\n\",\n        \"- PPO (Proximal Policy Gradient), a widely used RL algorithms in continuous action space.\\n\",\n        \"\\n\",\n        \"If you have any suggestion about ElegantRL Helloworld, you can discuss them in [ElegantRL issues/135: Suggestions for elegant_helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/issues/135), and we will keep an eye on this issue.\\n\",\n        \"ElegantRL's code, especially the Helloworld, really needs a lot of feedback to be better.\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"DbamGVHC3AeW\"\n      },\n      \"source\": [\n        \"# **Part 1: Install ElegantRL**\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"U35bhkUqOqbS\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"42c4d1a1-3e31-40d4-de5a-511dad532915\"\n      },\n      \"source\": [\n        \"# install elegantrl library\\n\",\n        \"!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git\"\n      ],\n      \"execution_count\": 5,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\\n\",\n            \"  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to /tmp/pip-req-build-120vlmei\\n\",\n            \"  Running command git clone -q https://github.com/AI4Finance-LLC/ElegantRL.git /tmp/pip-req-build-120vlmei\\n\",\n            \"Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (0.17.3)\\n\",\n            \"Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (3.2.2)\\n\",\n            \"Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (1.21.6)\\n\",\n            \"Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (3.2.2)\\n\",\n            \"Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (1.10.0+cu111)\\n\",\n            \"Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (4.1.2.30)\\n\",\n            \"Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (2.3.8)\\n\",\n            \"Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.5.0)\\n\",\n            \"Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.4.1)\\n\",\n            \"Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.3.0)\\n\",\n            \"Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->elegantrl==0.3.3) (0.16.0)\\n\",\n            \"Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (2.8.2)\\n\",\n            \"Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (0.11.0)\\n\",\n            \"Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (3.0.8)\\n\",\n            \"Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (1.4.2)\\n\",\n            \"Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->elegantrl==0.3.3) (4.1.1)\\n\",\n            \"Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->elegantrl==0.3.3) (1.15.0)\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"\\n\",\n        \"## **Part 2: Import ElegantRL helloworld**\\n\",\n        \"\\n\",\n        \"We hope that the `ElegantRL Helloworld` would help people who want to learn about reinforcement learning to quickly run a few introductory examples.\\n\",\n        \"- **Less lines of code**. (code lines <1000)\\n\",\n        \"- **Less packages requirements**. (only `torch` and `gym` )\\n\",\n        \"- **keep a consistent style with the full version of ElegantRL**.\\n\",\n        \"\\n\",\n        \"![File_structure of ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/figs/File_structure.png)\\n\",\n        \"\\n\",\n        \"One sentence summary: an agent `agent.py` with Actor-Critic networks `net.py` is trained `run.py` by interacting with an environment `env.py`.\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"In this tutorial, we only need to download the directory from [elegantrl_helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld) using the following code.\\n\",\n        \"\\n\",\n        \"The files in `elegantrl_helloworld` including:\\n\",\n        \"`config.py`, `agent.py`, `net.py`, `env.py`, `run.py`\"\n      ],\n      \"metadata\": {\n        \"id\": \"zJPivVxHMrAt\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"!rm -r -f /content/elegantrl_helloworld  # remove if the directory exists\\n\",\n        \"!wget https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/elegantrl_helloworld -P /content/\"\n      ],\n      \"metadata\": {\n        \"id\": \"sw_gE-IpovQ4\",\n        \"outputId\": \"b291f901-7b68-41ea-c261-96b8d6fe4fdc\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        }\n      },\n      \"execution_count\": 6,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"--2022-04-21 03:32:48--  https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/elegantrl_helloworld\\n\",\n            \"Resolving github.com (github.com)... 140.82.121.4\\n\",\n            \"Connecting to github.com (github.com)|140.82.121.4|:443... connected.\\n\",\n            \"HTTP request sent, awaiting response... 301 Moved Permanently\\n\",\n            \"Location: https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld [following]\\n\",\n            \"--2022-04-21 03:32:49--  https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld\\n\",\n            \"Reusing existing connection to github.com:443.\\n\",\n            \"HTTP request sent, awaiting response... 200 OK\\n\",\n            \"Length: unspecified [text/html]\\n\",\n            \"Saving to: ‘/content/elegantrl_helloworld’\\n\",\n            \"\\n\",\n            \"elegantrl_helloworl     [ <=>                ] 125.96K  --.-KB/s    in 0.03s   \\n\",\n            \"\\n\",\n            \"2022-04-21 03:32:49 (4.68 MB/s) - ‘/content/elegantrl_helloworld’ saved [128986]\\n\",\n            \"\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"from elegantrl_helloworld.run import train_agent, evaluate_agent\\n\",\n        \"from elegantrl_helloworld.env import get_gym_env_args\\n\",\n        \"from elegantrl_helloworld.config import Arguments\"\n      ],\n      \"metadata\": {\n        \"id\": \"nweGpiR1M0yA\"\n      },\n      \"execution_count\": 7,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"UVdmpnK_3Zcn\"\n      },\n      \"source\": [\n        \"## **Part 3: Train DQN on discreted action space task.**\\n\",\n        \"\\n\",\n        \"Train DQN on [**Discreted action** space task `CartPole`](https://gym.openai.com/envs/CartPole-v1/)\\n\",\n        \"\\n\",\n        \"You can see [/helloworld/erl_config.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/helloworld/erl_config.py) to get more information about hyperparameter.\\n\",\n        \"\\n\",\n        \"```\\n\",\n        \"class Arguments:\\n\",\n        \"    def __init__(self, agent_class, env_func=None, env_args=None):\\n\",\n        \"        self.env_num = self.env_args['env_num']  # env_num = 1. In vector env, env_num > 1.\\n\",\n        \"        self.max_step = self.env_args['max_step']  # the max step of an episode\\n\",\n        \"        self.env_name = self.env_args['env_name']  # the env name. Be used to set 'cwd'.\\n\",\n        \"        self.state_dim = self.env_args['state_dim']  # vector dimension (feature number) of state\\n\",\n        \"        self.action_dim = self.env_args['action_dim']  # vector dimension (feature number) of action\\n\",\n        \"        self.if_discrete = self.env_args['if_discrete']  # discrete or continuous action space\\n\",\n        \"        ...\\n\",\n        \"```\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"from elegantrl_helloworld.agent import AgentDQN\\n\",\n        \"agent_class = AgentDQN\\n\",\n        \"env_name = \\\"CartPole-v0\\\"\\n\",\n        \"\\n\",\n        \"import gym\\n\",\n        \"gym.logger.set_level(40)  # Block warning\\n\",\n        \"env = gym.make(env_name)\\n\",\n        \"env_func = gym.make\\n\",\n        \"env_args = get_gym_env_args(env, if_print=True)\\n\",\n        \"\\n\",\n        \"args = Arguments(agent_class, env_func, env_args)\\n\",\n        \"\\n\",\n        \"'''reward shaping'''\\n\",\n        \"args.reward_scale = 2 ** 0  # an approximate target reward usually be closed to 256\\n\",\n        \"args.gamma = 0.97  # discount factor of future rewards\\n\",\n        \"\\n\",\n        \"'''network update'''\\n\",\n        \"args.target_step = args.max_step * 2  # collect target_step, then update network\\n\",\n        \"args.net_dim = 2 ** 7  # the middle layer dimension of Fully Connected Network\\n\",\n        \"args.num_layer = 3  # the layer number of MultiLayer Perceptron, `assert num_layer >= 2`\\n\",\n        \"args.batch_size = 2 ** 7  # num of transitions sampled from replay buffer.\\n\",\n        \"args.repeat_times = 2 ** 0  # repeatedly update network using ReplayBuffer to keep critic's loss small\\n\",\n        \"args.explore_rate = 0.25  # epsilon-greedy for exploration.\\n\",\n        \"\\n\",\n        \"'''evaluate'''\\n\",\n        \"args.eval_gap = 2 ** 5  # number of times that get episode return\\n\",\n        \"args.eval_times = 2 ** 3  # number of times that get episode return\\n\",\n        \"args.break_step = int(8e4)  # break training if 'total_step > break_step'\"\n      ],\n      \"metadata\": {\n        \"id\": \"AAPdjovQrTpE\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"df46a9ae-4d8c-4836-b471-f755282a5393\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"env_args = {'env_num': 1,\\n\",\n            \"            'env_name': 'CartPole-v0',\\n\",\n            \"            'max_step': 200,\\n\",\n            \"            'state_dim': 4,\\n\",\n            \"            'action_dim': 2,\\n\",\n            \"            'if_discrete': True}\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"Choose gpu id `0` using `args.learner_gpu = 0`. Set as `-1` or GPU is unavaliable, the training program will choose CPU automatically.\\n\",\n        \"\\n\",\n        \"- The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200) \\n\",\n        \"- The cumulative returns of task_name is ∈ (min score, (score of random action, target score), max score).\"\n      ],\n      \"metadata\": {\n        \"id\": \"Rq5LPOH2B0aw\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"args.learner_gpus = -1\\n\",\n        \"\\n\",\n        \"train_agent(args)\\n\",\n        \"evaluate_agent(args)\\n\",\n        \"print('| The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200)')\"\n      ],\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"n7SBwVAkA8lA\",\n        \"outputId\": \"a0385b35-5886-4a96-c55c-3d19606f9bb8\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"| Arguments Keep cwd: ./CartPole-v0_DQN_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `ExpR` denotes average rewards during exploration. The agent gets this rewards with noisy action.\\n\",\n            \"| `ObjC` denotes the objective of Critic network. Or call it loss function of critic network.\\n\",\n            \"| `ObjA` denotes the objective of Actor network. It is the average Q value of the critic network.\\n\",\n            \"\\n\",\n            \"| Steps 4.08e+02  ExpR     1.00  | ObjC     0.08  ObjA     0.89\\n\",\n            \"| Steps 1.79e+04  ExpR     1.00  | ObjC     0.25  ObjA    16.98\\n\",\n            \"| Steps 3.13e+04  ExpR     1.00  | ObjC     0.32  ObjA    26.03\\n\",\n            \"| Steps 4.11e+04  ExpR     1.00  | ObjC     0.93  ObjA    29.85\\n\",\n            \"| Steps 4.98e+04  ExpR     1.00  | ObjC     1.19  ObjA    31.70\\n\",\n            \"| Steps 5.72e+04  ExpR     1.00  | ObjC     0.06  ObjA    30.98\\n\",\n            \"| Steps 6.43e+04  ExpR     1.00  | ObjC     0.21  ObjA    31.57\\n\",\n            \"| Steps 7.06e+04  ExpR     1.00  | ObjC     0.81  ObjA    31.66\\n\",\n            \"| Steps 7.59e+04  ExpR     1.00  | ObjC     0.58  ObjA    32.71\\n\",\n            \"| Steps 8.03e+04  ExpR     1.00  | ObjC     0.49  ObjA    30.81\\n\",\n            \"| UsedTime: 299 | SavedDir: ./CartPole-v0_DQN_-1\\n\",\n            \"| Arguments Keep cwd: ./CartPole-v0_DQN_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `avgR` denotes average value of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `stdR` denotes standard dev of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `avgS` denotes the average number of steps in an episode.\\n\",\n            \"\\n\",\n            \"| Steps 4.02e+02  | avgR     9.250  stdR     0.829  | avgS      9\\n\",\n            \"| Steps 4.08e+02  | avgR     9.125  stdR     0.599  | avgS      9\\n\",\n            \"| Steps 1.79e+04  | avgR    95.375  stdR     8.230  | avgS     95\\n\",\n            \"| Steps 1.85e+04  | avgR    99.625  stdR     7.777  | avgS    100\\n\",\n            \"| Steps 3.05e+04  | avgR    84.250  stdR     5.673  | avgS     84\\n\",\n            \"| Steps 3.13e+04  | avgR   133.625  stdR     4.442  | avgS    134\\n\",\n            \"| Steps 4.04e+04  | avgR   147.875  stdR     5.862  | avgS    148\\n\",\n            \"| Steps 4.11e+04  | avgR   122.625  stdR     4.794  | avgS    123\\n\",\n            \"| Steps 4.89e+04  | avgR   200.000  stdR     0.000  | avgS    200\\n\",\n            \"| Steps 4.98e+04  | avgR   194.875  stdR     6.827  | avgS    195\\n\",\n            \"| Steps 5.66e+04  | avgR   177.750  stdR    21.376  | avgS    178\\n\",\n            \"| Steps 5.72e+04  | avgR   184.750  stdR    14.403  | avgS    185\\n\",\n            \"| Steps 6.38e+04  | avgR   200.000  stdR     0.000  | avgS    200\\n\",\n            \"| Steps 6.43e+04  | avgR   152.250  stdR     7.446  | avgS    152\\n\",\n            \"| Steps 7.04e+04  | avgR   140.000  stdR     3.317  | avgS    140\\n\",\n            \"| Steps 7.06e+04  | avgR   120.500  stdR     4.664  | avgS    120\\n\",\n            \"| Steps 7.59e+04  | avgR    90.625  stdR     3.199  | avgS     91\\n\",\n            \"| Steps 7.60e+04  | avgR   190.625  stdR     9.027  | avgS    191\\n\",\n            \"| Steps 8.03e+04  | avgR   190.375  stdR    16.763  | avgS    190\\n\",\n            \"| Save learning curve in ./CartPole-v0_DQN_-1/LearningCurve_CartPole-v0_AgentDQN.jpg\\n\",\n            \"| The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200)\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"Train DQN on [**Discreted action** space env `LunarLander`](https://gym.openai.com/envs/LunarLander-v2/)\\n\",\n        \"\\n\",\n        \"**You can pass and run codes below.**. Because DQN takes over 6000 seconds for training. It is too slow. (DuelingDoubleDQN taks less than 1000 second for training on LunarLander-v2 task.)\\n\",\n        \"\\n\",\n        \"And there are many other DQN variance algorithms which get higher cumulative returns and takes less time for training. See [examples/demo_DQN_Dueling_Double_DQN.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/examples/demo_DQN_Dueling_Double_DQN.py)\"\n      ],\n      \"metadata\": {\n        \"id\": \"qK21xTxnHGOp\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"from elegantrl_helloworld.agent import AgentDQN\\n\",\n        \"agent_class = AgentDQN\\n\",\n        \"env_name = \\\"LunarLander-v2\\\"\\n\",\n        \"\\n\",\n        \"import gym\\n\",\n        \"gym.logger.set_level(40)  # Block warning\\n\",\n        \"env = gym.make(env_name)\\n\",\n        \"env_func = gym.make\\n\",\n        \"env_args = get_gym_env_args(env, if_print=True)\\n\",\n        \"\\n\",\n        \"args = Arguments(agent_class, env_func, env_args)\\n\",\n        \"\\n\",\n        \"'''reward shaping'''\\n\",\n        \"args.reward_scale = 2 ** 0\\n\",\n        \"args.gamma = 0.99\\n\",\n        \"\\n\",\n        \"'''network update'''\\n\",\n        \"args.target_step = args.max_step\\n\",\n        \"args.net_dim = 2 ** 7\\n\",\n        \"args.num_layer = 3\\n\",\n        \"\\n\",\n        \"args.batch_size = 2 ** 6\\n\",\n        \"\\n\",\n        \"args.repeat_times = 2 ** 0\\n\",\n        \"args.explore_noise = 0.125\\n\",\n        \"\\n\",\n        \"'''evaluate'''\\n\",\n        \"args.eval_gap = 2 ** 7\\n\",\n        \"args.eval_times = 2 ** 4\\n\",\n        \"args.break_step = int(4e5)  # LunarLander needs a larger `break_step`\\n\",\n        \"\\n\",\n        \"args.learner_gpus = -1  # denotes use CPU\\n\",\n        \"train_agent(args)\\n\",\n        \"evaluate_agent(args)\\n\",\n        \"print('| The cumulative returns of LunarLander-v2 is ∈ (-1800, (-600, 200), 340)')\"\n      ],\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"yH91VA17Hcsn\",\n        \"outputId\": \"fc4e96cb-9000-4ead-d899-ff0722218929\"\n      },\n      \"execution_count\": 8,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"env_args = {'env_num': 1,\\n\",\n            \"            'env_name': 'LunarLander-v2',\\n\",\n            \"            'max_step': 1000,\\n\",\n            \"            'state_dim': 8,\\n\",\n            \"            'action_dim': 4,\\n\",\n            \"            'if_discrete': True}\\n\",\n            \"| Arguments Remove cwd: ./LunarLander-v2_DQN_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `ExpR` denotes average rewards during exploration. The agent gets this rewards with noisy action.\\n\",\n            \"| `ObjC` denotes the objective of Critic network. Or call it loss function of critic network.\\n\",\n            \"| `ObjA` denotes the objective of Actor network. It is the average Q value of the critic network.\\n\",\n            \"\\n\",\n            \"| Steps 1.12e+03  ExpR    -2.70  | ObjC     3.05  ObjA    -4.95\\n\",\n            \"| Steps 1.67e+04  ExpR    -0.34  | ObjC    11.37  ObjA   -49.77\\n\",\n            \"| Steps 3.32e+04  ExpR    -0.39  | ObjC    17.30  ObjA   -46.68\\n\",\n            \"| Steps 4.42e+04  ExpR    -0.11  | ObjC     1.10  ObjA   -27.90\\n\",\n            \"| Steps 5.71e+04  ExpR    -0.06  | ObjC     1.55  ObjA   -33.38\\n\",\n            \"| Steps 6.81e+04  ExpR     0.04  | ObjC     0.70  ObjA    32.92\\n\",\n            \"| Steps 7.83e+04  ExpR    -0.22  | ObjC     0.91  ObjA   -22.68\\n\",\n            \"| Steps 9.10e+04  ExpR    -0.31  | ObjC     0.83  ObjA    25.71\\n\",\n            \"| Steps 1.00e+05  ExpR    -0.11  | ObjC     6.26  ObjA    23.71\\n\",\n            \"| Steps 1.12e+05  ExpR    -0.22  | ObjC     0.62  ObjA    34.25\\n\",\n            \"| Steps 1.22e+05  ExpR    -0.34  | ObjC    14.76  ObjA    16.74\\n\",\n            \"| Steps 1.31e+05  ExpR     0.03  | ObjC     2.37  ObjA   -14.47\\n\",\n            \"| Steps 1.39e+05  ExpR     0.07  | ObjC     0.94  ObjA    27.17\\n\",\n            \"| Steps 1.50e+05  ExpR    -0.10  | ObjC     0.85  ObjA   -12.59\\n\",\n            \"| Steps 1.58e+05  ExpR    -0.62  | ObjC     1.40  ObjA    52.57\\n\",\n            \"| Steps 1.67e+05  ExpR     0.11  | ObjC     4.06  ObjA    26.94\\n\",\n            \"| Steps 1.76e+05  ExpR     0.08  | ObjC     0.94  ObjA    41.06\\n\",\n            \"| Steps 1.85e+05  ExpR     0.12  | ObjC     1.09  ObjA    27.66\\n\",\n            \"| Steps 1.94e+05  ExpR    -0.06  | ObjC     0.90  ObjA    26.59\\n\",\n            \"| Steps 2.01e+05  ExpR     0.19  | ObjC     0.73  ObjA    41.54\\n\",\n            \"| Steps 2.10e+05  ExpR    -0.01  | ObjC     0.84  ObjA    47.05\\n\",\n            \"| Steps 2.17e+05  ExpR    -0.06  | ObjC     0.46  ObjA    45.40\\n\",\n            \"| Steps 2.24e+05  ExpR     0.11  | ObjC     0.68  ObjA    49.06\\n\",\n            \"| Steps 2.32e+05  ExpR     0.18  | ObjC     0.55  ObjA    45.73\\n\",\n            \"| Steps 2.39e+05  ExpR     0.06  | ObjC    12.54  ObjA    38.81\\n\",\n            \"| Steps 2.46e+05  ExpR    -0.03  | ObjC     2.05  ObjA    37.78\\n\",\n            \"| Steps 2.52e+05  ExpR     0.07  | ObjC     0.81  ObjA    39.93\\n\",\n            \"| Steps 2.58e+05  ExpR     0.01  | ObjC     6.43  ObjA    25.01\\n\",\n            \"| Steps 2.65e+05  ExpR     0.13  | ObjC     3.54  ObjA    30.40\\n\",\n            \"| Steps 2.70e+05  ExpR    -0.02  | ObjC     1.04  ObjA    34.32\\n\",\n            \"| Steps 2.75e+05  ExpR    -0.09  | ObjC     1.18  ObjA    44.72\\n\",\n            \"| Steps 2.83e+05  ExpR    -0.11  | ObjC     0.38  ObjA    41.80\\n\",\n            \"| Steps 2.89e+05  ExpR    -0.08  | ObjC     3.91  ObjA    32.60\\n\",\n            \"| Steps 2.95e+05  ExpR    -0.01  | ObjC     0.93  ObjA    56.21\\n\",\n            \"| Steps 3.00e+05  ExpR     0.21  | ObjC     0.91  ObjA    51.45\\n\",\n            \"| Steps 3.06e+05  ExpR     0.04  | ObjC     2.16  ObjA    37.36\\n\",\n            \"| Steps 3.12e+05  ExpR     0.08  | ObjC     0.84  ObjA    28.29\\n\",\n            \"| Steps 3.18e+05  ExpR     0.13  | ObjC     0.57  ObjA    54.10\\n\",\n            \"| Steps 3.23e+05  ExpR     0.03  | ObjC     1.09  ObjA    10.06\\n\",\n            \"| Steps 3.30e+05  ExpR    -0.01  | ObjC     0.70  ObjA    51.98\\n\",\n            \"| Steps 3.36e+05  ExpR     0.05  | ObjC     2.70  ObjA    35.15\\n\",\n            \"| Steps 3.42e+05  ExpR    -0.02  | ObjC     1.05  ObjA    31.27\\n\",\n            \"| Steps 3.47e+05  ExpR     0.03  | ObjC    10.52  ObjA    33.38\\n\",\n            \"| Steps 3.52e+05  ExpR     0.14  | ObjC     0.75  ObjA    42.52\\n\",\n            \"| Steps 3.58e+05  ExpR     0.10  | ObjC     0.73  ObjA    50.54\\n\",\n            \"| Steps 3.63e+05  ExpR     0.13  | ObjC     0.74  ObjA    29.97\\n\",\n            \"| Steps 3.67e+05  ExpR    -0.06  | ObjC     1.20  ObjA    39.28\\n\",\n            \"| Steps 3.71e+05  ExpR     0.11  | ObjC     2.31  ObjA    10.67\\n\",\n            \"| Steps 3.76e+05  ExpR     0.16  | ObjC    17.53  ObjA     0.34\\n\",\n            \"| Steps 3.81e+05  ExpR     0.09  | ObjC     1.03  ObjA    33.35\\n\",\n            \"| Steps 3.86e+05  ExpR     0.14  | ObjC     2.96  ObjA    51.90\\n\",\n            \"| Steps 3.91e+05  ExpR    -0.02  | ObjC     0.71  ObjA    54.86\\n\",\n            \"| Steps 3.95e+05  ExpR    -0.41  | ObjC     1.60  ObjA    55.81\\n\",\n            \"| Steps 4.00e+05  ExpR    -0.22  | ObjC     0.64  ObjA    59.22\\n\",\n            \"| UsedTime: 7384 | SavedDir: ./LunarLander-v2_DQN_-1\\n\",\n            \"| Arguments Keep cwd: ./LunarLander-v2_DQN_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `avgR` denotes average value of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `stdR` denotes standard dev of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `avgS` denotes the average number of steps in an episode.\\n\",\n            \"\\n\",\n            \"| Steps 1.12e+03  | avgR  -530.343  stdR   183.737  | avgS    103\\n\",\n            \"| Steps 1.67e+04  | avgR  -135.901  stdR    46.043  | avgS    587\\n\",\n            \"| Steps 3.32e+04  | avgR  -129.204  stdR    23.668  | avgS    951\\n\",\n            \"| Steps 4.42e+04  | avgR  -117.652  stdR    18.386  | avgS    836\\n\",\n            \"| Steps 5.71e+04  | avgR   -91.914  stdR    52.586  | avgS    960\\n\",\n            \"| Steps 6.81e+04  | avgR   -61.360  stdR    22.485  | avgS   1000\\n\",\n            \"| Steps 7.83e+04  | avgR   -69.344  stdR    88.238  | avgS    875\\n\",\n            \"| Steps 9.10e+04  | avgR  -119.279  stdR    18.228  | avgS    356\\n\",\n            \"| Steps 1.00e+05  | avgR   -43.961  stdR    59.145  | avgS    982\\n\",\n            \"| Steps 1.12e+05  | avgR   -67.955  stdR    31.958  | avgS    215\\n\",\n            \"| Steps 1.22e+05  | avgR   -35.413  stdR    29.490  | avgS    167\\n\",\n            \"| Steps 1.31e+05  | avgR   -52.169  stdR    36.981  | avgS    954\\n\",\n            \"| Steps 1.39e+05  | avgR   -34.231  stdR    72.037  | avgS    909\\n\",\n            \"| Steps 1.50e+05  | avgR    54.501  stdR    97.816  | avgS    292\\n\",\n            \"| Steps 1.58e+05  | avgR  -122.727  stdR    71.232  | avgS    213\\n\",\n            \"| Steps 1.67e+05  | avgR    12.992  stdR   129.530  | avgS    286\\n\",\n            \"| Steps 1.76e+05  | avgR    31.913  stdR   105.382  | avgS    652\\n\",\n            \"| Steps 1.85e+05  | avgR   105.082  stdR   110.050  | avgS    548\\n\",\n            \"| Steps 1.94e+05  | avgR    14.563  stdR   135.964  | avgS    704\\n\",\n            \"| Steps 2.01e+05  | avgR    34.576  stdR   112.856  | avgS    342\\n\",\n            \"| Steps 2.10e+05  | avgR     5.256  stdR    50.242  | avgS    226\\n\",\n            \"| Steps 2.17e+05  | avgR    47.514  stdR   111.849  | avgS    263\\n\",\n            \"| Steps 2.24e+05  | avgR   -24.851  stdR    51.499  | avgS    258\\n\",\n            \"| Steps 2.32e+05  | avgR   200.027  stdR   104.010  | avgS    520\\n\",\n            \"| Steps 2.39e+05  | avgR     8.645  stdR    63.075  | avgS    977\\n\",\n            \"| Steps 2.46e+05  | avgR   -49.730  stdR    16.815  | avgS   1000\\n\",\n            \"| Steps 2.52e+05  | avgR   -46.238  stdR    15.962  | avgS   1000\\n\",\n            \"| Steps 2.58e+05  | avgR     5.002  stdR   102.488  | avgS    757\\n\",\n            \"| Steps 2.65e+05  | avgR   -50.510  stdR    23.158  | avgS   1000\\n\",\n            \"| Steps 2.70e+05  | avgR    33.518  stdR    82.924  | avgS    905\\n\",\n            \"| Steps 2.75e+05  | avgR    -2.839  stdR    73.202  | avgS    960\\n\",\n            \"| Steps 2.83e+05  | avgR   -22.532  stdR    23.164  | avgS    948\\n\",\n            \"| Steps 2.89e+05  | avgR   -13.891  stdR    65.856  | avgS    116\\n\",\n            \"| Steps 2.95e+05  | avgR    82.550  stdR   120.489  | avgS    241\\n\",\n            \"| Steps 3.00e+05  | avgR   169.858  stdR    92.044  | avgS    387\\n\",\n            \"| Steps 3.06e+05  | avgR    18.055  stdR   104.873  | avgS    652\\n\",\n            \"| Steps 3.12e+05  | avgR   131.547  stdR   116.128  | avgS    597\\n\",\n            \"| Steps 3.18e+05  | avgR    17.401  stdR   123.102  | avgS    485\\n\",\n            \"| Steps 3.23e+05  | avgR   200.386  stdR   105.895  | avgS    324\\n\",\n            \"| Steps 3.30e+05  | avgR   145.581  stdR   121.974  | avgS    597\\n\",\n            \"| Steps 3.36e+05  | avgR    11.743  stdR   116.963  | avgS    712\\n\",\n            \"| Steps 3.42e+05  | avgR     1.907  stdR    41.075  | avgS    984\\n\",\n            \"| Steps 3.47e+05  | avgR    -2.172  stdR    25.627  | avgS   1000\\n\",\n            \"| Steps 3.52e+05  | avgR    98.222  stdR   108.038  | avgS    808\\n\",\n            \"| Steps 3.58e+05  | avgR   200.371  stdR    60.832  | avgS    535\\n\",\n            \"| Steps 3.63e+05  | avgR   164.193  stdR    70.218  | avgS    720\\n\",\n            \"| Steps 3.67e+05  | avgR    80.374  stdR   108.111  | avgS    795\\n\",\n            \"| Steps 3.71e+05  | avgR   173.379  stdR   105.248  | avgS    501\\n\",\n            \"| Steps 3.76e+05  | avgR   174.892  stdR    75.100  | avgS    667\\n\",\n            \"| Steps 3.81e+05  | avgR   138.224  stdR    88.497  | avgS    488\\n\",\n            \"| Steps 3.86e+05  | avgR   179.048  stdR    83.636  | avgS    440\\n\",\n            \"| Steps 3.91e+05  | avgR   -20.371  stdR    93.536  | avgS    156\\n\",\n            \"| Steps 3.95e+05  | avgR   -11.912  stdR   105.748  | avgS    153\\n\",\n            \"| Steps 4.00e+05  | avgR     9.476  stdR    72.703  | avgS    103\\n\",\n            \"| Save learning curve in ./LunarLander-v2_DQN_-1/LearningCurve_LunarLander-v2_AgentDQN.jpg\\n\",\n            \"| The cumulative returns of LunarLander-v2 is ∈ (-1800, (-600, 200), 340)\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## **Part 4: Train DDPG on continuous action space task.**\\n\",\n        \"\\n\",\n        \"Train DDPG on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/)\\n\",\n        \"\\n\",\n        \"We show a cunstom env in helloworld/erl_env.py `class PendulumEnv`](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/helloworld/erl_env.py#L19-L23)\\n\",\n        \"\\n\",\n        \"OpenAI Pendulum env set its action space as (-2, +2). It is bad. We suggest that adjust action space to (-1, +1) when designing your own env.\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"z2Ik5cDoyPGU\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"from elegantrl_helloworld.config import Arguments\\n\",\n        \"from elegantrl_helloworld.run import train_agent, evaluate_agent\\n\",\n        \"from elegantrl_helloworld.env import get_gym_env_args\\n\",\n        \"from elegantrl_helloworld.agent import AgentDDPG\\n\",\n        \"agent_class = AgentDDPG\\n\",\n        \"\\n\",\n        \"from elegantrl_helloworld.env import PendulumEnv\\n\",\n        \"env = PendulumEnv('Pendulum-v0')  # PendulumEnv('Pendulum-v1')\\n\",\n        \"env_func = PendulumEnv\\n\",\n        \"env_args = get_gym_env_args(env, if_print=True)\\n\",\n        \"\\n\",\n        \"args = Arguments(agent_class, env_func, env_args)\\n\",\n        \"\\n\",\n        \"'''reward shaping'''\\n\",\n        \"args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\\n\",\n        \"args.gamma = 0.97\\n\",\n        \"\\n\",\n        \"'''network update'''\\n\",\n        \"args.target_step = args.max_step * 2\\n\",\n        \"args.net_dim = 2 ** 7\\n\",\n        \"args.batch_size = 2 ** 7\\n\",\n        \"args.repeat_times = 2 ** 0\\n\",\n        \"args.explore_noise = 0.1\\n\",\n        \"\\n\",\n        \"'''evaluate'''\\n\",\n        \"args.eval_gap = 2 ** 6\\n\",\n        \"args.eval_times = 2 ** 3\\n\",\n        \"args.break_step = int(1e5)\\n\",\n        \"\\n\",\n        \"args.learner_gpus = -1  # denotes use CPU\\n\",\n        \"train_agent(args)\\n\",\n        \"evaluate_agent(args)\\n\",\n        \"print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')\"\n      ],\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"wwkZXiHtyV6f\",\n        \"outputId\": \"0e7c8c26-9b4b-42e3-de2a-9da7f76bf670\"\n      },\n      \"execution_count\": 9,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"WARNING: env.action_space.high [2.]\\n\",\n            \"WARNING: env.action_space.low [-2.]\\n\",\n            \"env_args = {'env_num': 1,\\n\",\n            \"            'env_name': 'Pendulum-v0',\\n\",\n            \"            'max_step': 200,\\n\",\n            \"            'state_dim': 3,\\n\",\n            \"            'action_dim': 1,\\n\",\n            \"            'if_discrete': False}\\n\",\n            \"| Arguments Remove cwd: ./Pendulum-v0_DDPG_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `ExpR` denotes average rewards during exploration. The agent gets this rewards with noisy action.\\n\",\n            \"| `ObjC` denotes the objective of Critic network. Or call it loss function of critic network.\\n\",\n            \"| `ObjA` denotes the objective of Actor network. It is the average Q value of the critic network.\\n\",\n            \"\\n\",\n            \"| Steps 4.00e+02  ExpR    -3.71  | ObjC     1.13  ObjA    -0.83\\n\",\n            \"| Steps 1.12e+04  ExpR    -0.32  | ObjC     0.32  ObjA   -39.47\\n\",\n            \"| Steps 1.96e+04  ExpR    -0.65  | ObjC     0.57  ObjA   -40.92\\n\",\n            \"| Steps 2.68e+04  ExpR    -0.16  | ObjC     0.32  ObjA   -27.83\\n\",\n            \"| Steps 3.32e+04  ExpR    -0.31  | ObjC     0.14  ObjA   -25.19\\n\",\n            \"| Steps 3.88e+04  ExpR    -0.32  | ObjC     1.16  ObjA   -24.87\\n\",\n            \"| Steps 4.40e+04  ExpR    -0.31  | ObjC     0.09  ObjA   -27.35\\n\",\n            \"| Steps 4.88e+04  ExpR    -0.46  | ObjC     0.11  ObjA   -16.21\\n\",\n            \"| Steps 5.36e+04  ExpR    -0.16  | ObjC     0.24  ObjA   -22.40\\n\",\n            \"| Steps 5.80e+04  ExpR    -0.32  | ObjC     0.10  ObjA   -18.49\\n\",\n            \"| Steps 6.20e+04  ExpR    -0.57  | ObjC     0.66  ObjA   -22.49\\n\",\n            \"| Steps 6.60e+04  ExpR    -0.31  | ObjC     0.13  ObjA   -20.69\\n\",\n            \"| Steps 7.00e+04  ExpR    -0.31  | ObjC     0.11  ObjA   -20.38\\n\",\n            \"| Steps 7.36e+04  ExpR    -0.30  | ObjC     0.12  ObjA   -15.18\\n\",\n            \"| Steps 7.72e+04  ExpR    -0.45  | ObjC     0.12  ObjA   -16.68\\n\",\n            \"| Steps 8.08e+04  ExpR    -0.45  | ObjC     0.08  ObjA   -19.84\\n\",\n            \"| Steps 8.40e+04  ExpR    -0.47  | ObjC     0.10  ObjA   -17.64\\n\",\n            \"| Steps 8.72e+04  ExpR    -0.31  | ObjC     0.07  ObjA   -19.66\\n\",\n            \"| Steps 9.04e+04  ExpR    -0.45  | ObjC     0.12  ObjA   -16.49\\n\",\n            \"| Steps 9.36e+04  ExpR    -0.31  | ObjC     0.04  ObjA   -14.00\\n\",\n            \"| Steps 9.64e+04  ExpR    -0.45  | ObjC     0.08  ObjA   -19.62\\n\",\n            \"| Steps 9.92e+04  ExpR    -0.62  | ObjC     0.10  ObjA   -15.54\\n\",\n            \"| UsedTime: 1453 | SavedDir: ./Pendulum-v0_DDPG_-1\\n\",\n            \"| Arguments Keep cwd: ./Pendulum-v0_DDPG_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `avgR` denotes average value of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `stdR` denotes standard dev of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `avgS` denotes the average number of steps in an episode.\\n\",\n            \"\\n\",\n            \"| Steps 4.00e+02  | avgR -1348.050  stdR   271.455  | avgS    200\\n\",\n            \"| Steps 1.12e+04  | avgR  -217.716  stdR   128.023  | avgS    200\\n\",\n            \"| Steps 1.96e+04  | avgR  -183.775  stdR   152.724  | avgS    200\\n\",\n            \"| Steps 2.68e+04  | avgR  -153.652  stdR   114.133  | avgS    200\\n\",\n            \"| Steps 3.32e+04  | avgR  -135.305  stdR   105.206  | avgS    200\\n\",\n            \"| Steps 3.88e+04  | avgR  -115.037  stdR    39.912  | avgS    200\\n\",\n            \"| Steps 4.40e+04  | avgR  -168.948  stdR    58.590  | avgS    200\\n\",\n            \"| Steps 4.88e+04  | avgR  -139.247  stdR    72.530  | avgS    200\\n\",\n            \"| Steps 5.36e+04  | avgR  -163.751  stdR    56.435  | avgS    200\\n\",\n            \"| Steps 5.80e+04  | avgR  -138.569  stdR    39.777  | avgS    200\\n\",\n            \"| Steps 6.20e+04  | avgR  -109.240  stdR    39.215  | avgS    200\\n\",\n            \"| Steps 6.60e+04  | avgR  -122.601  stdR    82.795  | avgS    200\\n\",\n            \"| Steps 7.00e+04  | avgR  -108.108  stdR    73.511  | avgS    200\\n\",\n            \"| Steps 7.36e+04  | avgR  -166.443  stdR    56.351  | avgS    200\\n\",\n            \"| Steps 7.72e+04  | avgR  -109.545  stdR    38.913  | avgS    200\\n\",\n            \"| Steps 8.08e+04  | avgR  -184.884  stdR    60.408  | avgS    200\\n\",\n            \"| Steps 8.40e+04  | avgR  -166.229  stdR   113.710  | avgS    200\\n\",\n            \"| Steps 8.72e+04  | avgR  -194.961  stdR   100.822  | avgS    200\\n\",\n            \"| Steps 9.04e+04  | avgR  -191.578  stdR    56.208  | avgS    200\\n\",\n            \"| Steps 9.36e+04  | avgR  -136.921  stdR    69.107  | avgS    200\\n\",\n            \"| Steps 9.64e+04  | avgR  -175.967  stdR    77.816  | avgS    200\\n\",\n            \"| Steps 9.92e+04  | avgR  -150.688  stdR    51.610  | avgS    200\\n\",\n            \"| Save learning curve in ./Pendulum-v0_DDPG_-1/LearningCurve_Pendulum-v0_AgentDDPG.jpg\\n\",\n            \"| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"3n8zcgcn14uq\"\n      },\n      \"source\": [\n        \"# **Part 5: Train PPO on continuous action space task.**\\n\",\n        \"\\n\",\n        \"Train PPO on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/). \\n\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"E03f6cTeajK4\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"0e62173c-b6af-4b36-9073-875c2f72fd73\"\n      },\n      \"source\": [\n        \"from elegantrl_helloworld.config import Arguments\\n\",\n        \"from elegantrl_helloworld.run import train_agent, evaluate_agent\\n\",\n        \"from elegantrl_helloworld.env import get_gym_env_args\\n\",\n        \"from elegantrl_helloworld.agent import AgentPPO\\n\",\n        \"agent_class = AgentPPO\\n\",\n        \"\\n\",\n        \"from elegantrl_helloworld.env import PendulumEnv\\n\",\n        \"env = PendulumEnv()\\n\",\n        \"env_func = PendulumEnv\\n\",\n        \"env_args = get_gym_env_args(env, if_print=True)\\n\",\n        \"\\n\",\n        \"args = Arguments(agent_class, env_func, env_args)\\n\",\n        \"\\n\",\n        \"'''reward shaping'''\\n\",\n        \"args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\\n\",\n        \"args.gamma = 0.97\\n\",\n        \"\\n\",\n        \"'''network update'''\\n\",\n        \"args.target_step = args.max_step * 8\\n\",\n        \"args.net_dim = 2 ** 7\\n\",\n        \"args.num_layer = 2\\n\",\n        \"args.batch_size = 2 ** 8\\n\",\n        \"args.repeat_times = 2 ** 5\\n\",\n        \"\\n\",\n        \"'''evaluate'''\\n\",\n        \"args.eval_gap = 2 ** 6\\n\",\n        \"args.eval_times = 2 ** 3\\n\",\n        \"args.break_step = int(8e5)\\n\",\n        \"\\n\",\n        \"args.learner_gpus = -1\\n\",\n        \"train_agent(args)\\n\",\n        \"evaluate_agent(args)\\n\",\n        \"print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')\"\n      ],\n      \"execution_count\": 11,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"WARNING: env.action_space.high [2.]\\n\",\n            \"WARNING: env.action_space.low [-2.]\\n\",\n            \"env_args = {'env_num': 1,\\n\",\n            \"            'env_name': 'Pendulum-v0',\\n\",\n            \"            'max_step': 200,\\n\",\n            \"            'state_dim': 3,\\n\",\n            \"            'action_dim': 1,\\n\",\n            \"            'if_discrete': False}\\n\",\n            \"| Arguments Remove cwd: ./Pendulum-v0_PPO_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `ExpR` denotes average rewards during exploration. The agent gets this rewards with noisy action.\\n\",\n            \"| `ObjC` denotes the objective of Critic network. Or call it loss function of critic network.\\n\",\n            \"| `ObjA` denotes the objective of Actor network. It is the average Q value of the critic network.\\n\",\n            \"\\n\",\n            \"| Steps 1.60e+03  ExpR    -3.34  | ObjC    93.37  ObjA     0.02\\n\",\n            \"| Steps 9.76e+04  ExpR    -2.69  | ObjC    26.47  ObjA     0.13\\n\",\n            \"| Steps 1.95e+05  ExpR    -2.37  | ObjC    14.92  ObjA     0.12\\n\",\n            \"| Steps 2.94e+05  ExpR    -1.95  | ObjC    10.23  ObjA     0.03\\n\",\n            \"| Steps 3.94e+05  ExpR    -1.75  | ObjC     7.16  ObjA    -0.01\\n\",\n            \"| Steps 4.93e+05  ExpR    -0.87  | ObjC     6.48  ObjA     0.03\\n\",\n            \"| Steps 5.92e+05  ExpR    -0.67  | ObjC     5.73  ObjA    -0.04\\n\",\n            \"| Steps 6.93e+05  ExpR    -0.54  | ObjC     1.24  ObjA    -0.05\\n\",\n            \"| Steps 7.92e+05  ExpR    -0.57  | ObjC     1.65  ObjA    -0.21\\n\",\n            \"| UsedTime: 524 | SavedDir: ./Pendulum-v0_PPO_-1\\n\",\n            \"| Arguments Keep cwd: ./Pendulum-v0_PPO_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `avgR` denotes average value of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `stdR` denotes standard dev of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `avgS` denotes the average number of steps in an episode.\\n\",\n            \"\\n\",\n            \"| Steps 1.60e+03  | avgR -1397.987  stdR   200.431  | avgS    200\\n\",\n            \"| Steps 9.76e+04  | avgR -1037.976  stdR   111.783  | avgS    200\\n\",\n            \"| Steps 1.95e+05  | avgR  -875.206  stdR   103.358  | avgS    200\\n\",\n            \"| Steps 2.94e+05  | avgR  -691.654  stdR    86.157  | avgS    200\\n\",\n            \"| Steps 3.94e+05  | avgR  -564.713  stdR   102.412  | avgS    200\\n\",\n            \"| Steps 4.93e+05  | avgR  -318.184  stdR   141.380  | avgS    200\\n\",\n            \"| Steps 5.92e+05  | avgR  -177.265  stdR   108.117  | avgS    200\\n\",\n            \"| Steps 6.93e+05  | avgR  -216.057  stdR   209.142  | avgS    200\\n\",\n            \"| Steps 7.92e+05  | avgR  -230.227  stdR    94.815  | avgS    200\\n\",\n            \"| Save learning curve in ./Pendulum-v0_PPO_-1/LearningCurve_Pendulum-v0_AgentPPO.jpg\\n\",\n            \"| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"Train PPO on [**Continuous action** space env `LunarLanderContinuous`](https://gym.openai.com/envs/LunarLanderContinuous-v2/)\"\n      ],\n      \"metadata\": {\n        \"id\": \"rcFcUkwfzHLE\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"from elegantrl_helloworld.config import Arguments\\n\",\n        \"from elegantrl_helloworld.run import train_agent, evaluate_agent\\n\",\n        \"from elegantrl_helloworld.env import get_gym_env_args\\n\",\n        \"from elegantrl_helloworld.agent import AgentPPO\\n\",\n        \"agent_class = AgentPPO\\n\",\n        \"env_name = \\\"LunarLanderContinuous-v2\\\"\\n\",\n        \"\\n\",\n        \"import gym\\n\",\n        \"env = gym.make(env_name)\\n\",\n        \"env_func = gym.make\\n\",\n        \"env_args = get_gym_env_args(env, if_print=True)\\n\",\n        \"\\n\",\n        \"args = Arguments(agent_class, env_func, env_args)\\n\",\n        \"\\n\",\n        \"'''reward shaping'''\\n\",\n        \"args.gamma = 0.99\\n\",\n        \"args.reward_scale = 2 ** -1\\n\",\n        \"\\n\",\n        \"'''network update'''\\n\",\n        \"args.target_step = args.max_step * 8\\n\",\n        \"args.num_layer = 3\\n\",\n        \"args.batch_size = 2 ** 7\\n\",\n        \"args.repeat_times = 2 ** 4\\n\",\n        \"args.lambda_entropy = 0.04\\n\",\n        \"\\n\",\n        \"'''evaluate'''\\n\",\n        \"args.eval_gap = 2 ** 6\\n\",\n        \"args.eval_times = 2 ** 5\\n\",\n        \"args.break_step = int(4e5)\\n\",\n        \"\\n\",\n        \"args.learner_gpus = -1\\n\",\n        \"train_agent(args)\\n\",\n        \"evaluate_agent(args)\\n\",\n        \"print('| The cumulative returns of LunarLanderContinuous-v2 is ∈ (-1800, (-300, 200), 310+)')\"\n      ],\n      \"metadata\": {\n        \"id\": \"9WCAcmIfzGyE\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"03513c3c-e5b8-4f4a-b4f2-00ef753d7c95\"\n      },\n      \"execution_count\": 12,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"env_args = {'env_num': 1,\\n\",\n            \"            'env_name': 'LunarLanderContinuous-v2',\\n\",\n            \"            'max_step': 1000,\\n\",\n            \"            'state_dim': 8,\\n\",\n            \"            'action_dim': 2,\\n\",\n            \"            'if_discrete': False}\\n\",\n            \"| Arguments Remove cwd: ./LunarLanderContinuous-v2_PPO_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `ExpR` denotes average rewards during exploration. The agent gets this rewards with noisy action.\\n\",\n            \"| `ObjC` denotes the objective of Critic network. Or call it loss function of critic network.\\n\",\n            \"| `ObjA` denotes the objective of Actor network. It is the average Q value of the critic network.\\n\",\n            \"\\n\",\n            \"| Steps 8.10e+03  ExpR    -1.06  | ObjC    26.53  ObjA     0.02\\n\",\n            \"| Steps 5.63e+04  ExpR    -0.21  | ObjC     9.94  ObjA     0.01\\n\",\n            \"| Steps 1.05e+05  ExpR    -0.05  | ObjC    10.62  ObjA    -0.02\\n\",\n            \"| Steps 1.30e+05  ExpR    -0.01  | ObjC     8.52  ObjA     0.02\\n\",\n            \"| Steps 1.54e+05  ExpR     0.02  | ObjC     6.58  ObjA    -0.10\\n\",\n            \"| Steps 1.79e+05  ExpR     0.03  | ObjC     5.60  ObjA     0.11\\n\",\n            \"| Steps 2.04e+05  ExpR     0.02  | ObjC     6.21  ObjA     0.01\\n\",\n            \"| Steps 2.37e+05  ExpR     0.06  | ObjC     2.69  ObjA    -0.07\\n\",\n            \"| Steps 2.70e+05  ExpR     0.05  | ObjC     8.49  ObjA    -0.06\\n\",\n            \"| Steps 3.05e+05  ExpR     0.06  | ObjC     4.05  ObjA    -0.04\\n\",\n            \"| Steps 3.39e+05  ExpR     0.05  | ObjC     5.11  ObjA    -0.16\\n\",\n            \"| Steps 3.72e+05  ExpR     0.08  | ObjC     2.91  ObjA     0.01\\n\",\n            \"| Steps 4.05e+05  ExpR     0.07  | ObjC     4.80  ObjA    -0.10\\n\",\n            \"| UsedTime: 911 | SavedDir: ./LunarLanderContinuous-v2_PPO_-1\\n\",\n            \"| Arguments Keep cwd: ./LunarLanderContinuous-v2_PPO_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `avgR` denotes average value of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `stdR` denotes standard dev of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `avgS` denotes the average number of steps in an episode.\\n\",\n            \"\\n\",\n            \"| Steps 8.10e+03  | avgR  -177.288  stdR   124.057  | avgS    151\\n\",\n            \"| Steps 5.63e+04  | avgR  -170.839  stdR   151.327  | avgS    146\\n\",\n            \"| Steps 1.05e+05  | avgR  -209.592  stdR    46.286  | avgS    152\\n\",\n            \"| Steps 1.30e+05  | avgR  -165.752  stdR   117.630  | avgS    182\\n\",\n            \"| Steps 1.54e+05  | avgR  -138.190  stdR    93.117  | avgS    184\\n\",\n            \"| Steps 1.79e+05  | avgR   -54.291  stdR   169.554  | avgS    236\\n\",\n            \"| Steps 2.04e+05  | avgR  -100.748  stdR   113.405  | avgS    225\\n\",\n            \"| Steps 2.37e+05  | avgR    -7.361  stdR   176.139  | avgS    288\\n\",\n            \"| Steps 2.70e+05  | avgR    10.899  stdR   189.107  | avgS    367\\n\",\n            \"| Steps 3.05e+05  | avgR    57.944  stdR   188.546  | avgS    342\\n\",\n            \"| Steps 3.39e+05  | avgR   118.386  stdR   157.707  | avgS    350\\n\",\n            \"| Steps 3.72e+05  | avgR   102.001  stdR   231.197  | avgS    338\\n\",\n            \"| Steps 4.05e+05  | avgR   198.335  stdR    90.755  | avgS    278\\n\",\n            \"| Save learning curve in ./LunarLanderContinuous-v2_PPO_-1/LearningCurve_LunarLanderContinuous-v2_AgentPPO.jpg\\n\",\n            \"| The cumulative returns of LunarLanderContinuous-v2 is ∈ (-1800, (-300, 200), 310+)\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"z1j5kLHF2dhJ\"\n      },\n      \"source\": [\n        \"Train PPO on [**Continuous action** space env `BipedalWalker`](https://gym.openai.com/envs/BipedalWalker-v2/)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"KGOPSD6da23k\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"8b126d76-ea1d-40bb-f00c-05b59c1f9669\"\n      },\n      \"source\": [\n        \"from elegantrl_helloworld.config import Arguments\\n\",\n        \"from elegantrl_helloworld.run import train_agent, evaluate_agent\\n\",\n        \"from elegantrl_helloworld.env import get_gym_env_args\\n\",\n        \"from elegantrl_helloworld.agent import AgentPPO\\n\",\n        \"agent_class = AgentPPO\\n\",\n        \"env_name = \\\"BipedalWalker-v3\\\"\\n\",\n        \"\\n\",\n        \"import gym\\n\",\n        \"env = gym.make(env_name)\\n\",\n        \"env_func = gym.make\\n\",\n        \"env_args = get_gym_env_args(env, if_print=True)\\n\",\n        \"\\n\",\n        \"args = Arguments(agent_class, env_func, env_args)\\n\",\n        \"\\n\",\n        \"'''reward shaping'''\\n\",\n        \"args.reward_scale = 2 ** -1\\n\",\n        \"args.gamma = 0.98\\n\",\n        \"\\n\",\n        \"'''network update'''\\n\",\n        \"args.target_step = args.max_step\\n\",\n        \"args.net_dim = 2 ** 8\\n\",\n        \"args.num_layer = 3\\n\",\n        \"args.batch_size = 2 ** 8\\n\",\n        \"args.repeat_times = 2 ** 4\\n\",\n        \"\\n\",\n        \"'''evaluate'''\\n\",\n        \"args.eval_gap = 2 ** 6\\n\",\n        \"args.eval_times = 2 ** 4\\n\",\n        \"args.break_step = int(1e6)\\n\",\n        \"\\n\",\n        \"args.learner_gpus = -1\\n\",\n        \"train_agent(args)\\n\",\n        \"evaluate_agent(args)\\n\",\n        \"print('| The cumulative returns of BipedalWalker-v3 is ∈ (-150, (-100, 280), 320+)')\\n\"\n      ],\n      \"execution_count\": 13,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"env_args = {'env_num': 1,\\n\",\n            \"            'env_name': 'BipedalWalker-v3',\\n\",\n            \"            'max_step': 1600,\\n\",\n            \"            'state_dim': 24,\\n\",\n            \"            'action_dim': 4,\\n\",\n            \"            'if_discrete': False}\\n\",\n            \"| Arguments Remove cwd: ./BipedalWalker-v3_PPO_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `ExpR` denotes average rewards during exploration. The agent gets this rewards with noisy action.\\n\",\n            \"| `ObjC` denotes the objective of Critic network. Or call it loss function of critic network.\\n\",\n            \"| `ObjA` denotes the objective of Actor network. It is the average Q value of the critic network.\\n\",\n            \"\\n\",\n            \"| Steps 1.60e+03  ExpR    -0.02  | ObjC     0.10  ObjA     0.05\\n\",\n            \"| Steps 3.78e+04  ExpR    -0.05  | ObjC     1.02  ObjA     0.01\\n\",\n            \"| Steps 7.37e+04  ExpR    -0.05  | ObjC     0.80  ObjA     0.04\\n\",\n            \"| Steps 1.10e+05  ExpR    -0.02  | ObjC     0.03  ObjA    -0.01\\n\",\n            \"| Steps 1.45e+05  ExpR    -0.00  | ObjC     0.05  ObjA     0.01\\n\",\n            \"| Steps 1.82e+05  ExpR     0.01  | ObjC     0.05  ObjA     0.09\\n\",\n            \"| Steps 2.17e+05  ExpR     0.02  | ObjC     0.08  ObjA     0.05\\n\",\n            \"| Steps 2.52e+05  ExpR     0.03  | ObjC     0.06  ObjA     0.06\\n\",\n            \"| Steps 2.89e+05  ExpR     0.03  | ObjC     0.05  ObjA     0.08\\n\",\n            \"| Steps 3.24e+05  ExpR     0.03  | ObjC     0.08  ObjA     0.00\\n\",\n            \"| Steps 3.60e+05  ExpR     0.01  | ObjC     0.22  ObjA     0.02\\n\",\n            \"| Steps 3.96e+05  ExpR    -0.02  | ObjC     0.69  ObjA     0.06\\n\",\n            \"| Steps 4.32e+05  ExpR     0.04  | ObjC     0.05  ObjA     0.10\\n\",\n            \"| Steps 4.69e+05  ExpR    -0.02  | ObjC     1.39  ObjA     0.04\\n\",\n            \"| Steps 5.06e+05  ExpR    -0.04  | ObjC     1.72  ObjA     0.09\\n\",\n            \"| Steps 5.43e+05  ExpR     0.05  | ObjC     0.06  ObjA     0.10\\n\",\n            \"| Steps 5.79e+05  ExpR     0.05  | ObjC     0.09  ObjA    -0.02\\n\",\n            \"| Steps 6.15e+05  ExpR    -0.03  | ObjC     1.36  ObjA     0.10\\n\",\n            \"| Steps 6.51e+05  ExpR     0.05  | ObjC     0.08  ObjA     0.03\\n\",\n            \"| Steps 6.87e+05  ExpR     0.05  | ObjC     0.04  ObjA     0.07\\n\",\n            \"| Steps 7.24e+05  ExpR     0.05  | ObjC     0.04  ObjA    -0.01\\n\",\n            \"| Steps 7.62e+05  ExpR    -0.01  | ObjC     1.10  ObjA     0.04\\n\",\n            \"| Steps 7.99e+05  ExpR     0.03  | ObjC     0.65  ObjA     0.04\\n\",\n            \"| Steps 8.34e+05  ExpR     0.06  | ObjC     0.08  ObjA     0.08\\n\",\n            \"| Steps 8.70e+05  ExpR     0.07  | ObjC     0.16  ObjA    -0.02\\n\",\n            \"| Steps 9.06e+05  ExpR     0.07  | ObjC     0.08  ObjA     0.17\\n\",\n            \"| Steps 9.42e+05  ExpR     0.07  | ObjC     0.11  ObjA     0.11\\n\",\n            \"| Steps 9.78e+05  ExpR     0.07  | ObjC     0.17  ObjA     0.08\\n\",\n            \"| UsedTime: 1820 | SavedDir: ./BipedalWalker-v3_PPO_-1\\n\",\n            \"| Arguments Keep cwd: ./BipedalWalker-v3_PPO_-1\\n\",\n            \"\\n\",\n            \"| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\\n\",\n            \"| `avgR` denotes average value of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `stdR` denotes standard dev of cumulative rewards, which is the sum of rewards in an episode.\\n\",\n            \"| `avgS` denotes the average number of steps in an episode.\\n\",\n            \"\\n\",\n            \"| Steps 1.60e+03  | avgR   -93.814  stdR     1.263  | avgS     88\\n\",\n            \"| Steps 3.78e+04  | avgR   -19.896  stdR     0.677  | avgS   1600\\n\",\n            \"| Steps 7.37e+04  | avgR   -37.093  stdR     4.867  | avgS   1600\\n\",\n            \"| Steps 1.10e+05  | avgR   -51.127  stdR     3.032  | avgS   1600\\n\",\n            \"| Steps 1.45e+05  | avgR   -48.666  stdR    29.034  | avgS   1365\\n\",\n            \"| Steps 1.82e+05  | avgR   -14.884  stdR    67.086  | avgS   1199\\n\",\n            \"| Steps 2.17e+05  | avgR    -3.247  stdR   111.651  | avgS    827\\n\",\n            \"| Steps 2.52e+05  | avgR   -98.306  stdR     1.053  | avgS    102\\n\",\n            \"| Steps 2.89e+05  | avgR    10.513  stdR   123.090  | avgS    788\\n\",\n            \"| Steps 3.24e+05  | avgR   172.433  stdR     4.698  | avgS   1600\\n\",\n            \"| Steps 3.60e+05  | avgR    43.901  stdR   119.611  | avgS   1145\\n\",\n            \"| Steps 3.96e+05  | avgR  -100.273  stdR     1.573  | avgS     90\\n\",\n            \"| Steps 4.32e+05  | avgR  -102.632  stdR     0.809  | avgS     83\\n\",\n            \"| Steps 4.69e+05  | avgR   -19.897  stdR    84.923  | avgS    686\\n\",\n            \"| Steps 5.06e+05  | avgR    -0.357  stdR    90.928  | avgS    796\\n\",\n            \"| Steps 5.43e+05  | avgR   122.198  stdR   106.949  | avgS   1328\\n\",\n            \"| Steps 5.79e+05  | avgR   156.253  stdR    54.301  | avgS   1546\\n\",\n            \"| Steps 6.15e+05  | avgR   112.193  stdR   133.756  | avgS   1195\\n\",\n            \"| Steps 6.51e+05  | avgR   167.152  stdR    94.265  | avgS   1433\\n\",\n            \"| Steps 6.87e+05  | avgR   188.158  stdR    48.614  | avgS   1556\\n\",\n            \"| Steps 7.24e+05  | avgR   145.986  stdR   125.801  | avgS   1287\\n\",\n            \"| Steps 7.62e+05  | avgR   135.115  stdR    95.265  | avgS   1337\\n\",\n            \"| Steps 7.99e+05  | avgR   228.162  stdR     6.880  | avgS   1600\\n\",\n            \"| Steps 8.34e+05  | avgR   233.372  stdR     4.996  | avgS   1600\\n\",\n            \"| Steps 8.70e+05  | avgR   266.571  stdR     3.626  | avgS   1576\\n\",\n            \"| Steps 9.06e+05  | avgR   229.514  stdR    91.325  | avgS   1431\\n\",\n            \"| Steps 9.42e+05  | avgR    57.267  stdR   168.962  | avgS    748\\n\",\n            \"| Steps 9.78e+05  | avgR   206.148  stdR   119.650  | avgS   1338\\n\",\n            \"| Save learning curve in ./BipedalWalker-v3_PPO_-1/LearningCurve_BipedalWalker-v3_AgentPPO.jpg\\n\",\n            \"| The cumulative returns of BipedalWalker-v3 is ∈ (-150, (-100, 280), 320+)\\n\"\n          ]\n        }\n      ]\n    }\n  ]\n}"
        },
        {
          "name": "unit_tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}