{
  "metadata": {
    "timestamp": 1736560006617,
    "page": 811,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "meetps/pytorch-semseg",
      "stars": 3402,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1337890625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# Torch Models\n*.pkl\n*.pth\ncurrent_train.py\nvideo_test*.py\n*.swp\ndata\nckpt\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nlocal_test.py\n.DS_STORE\n.idea/\n.vscode/\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.05078125,
          "content": "MIT License\n\nCopyright (c) 2017 Meet Pragnesh Shah \n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.2236328125,
          "content": "# pytorch-semseg\n\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/meetshah1995/pytorch-semseg/blob/master/LICENSE)\n[![pypi](https://img.shields.io/pypi/v/pytorch_semseg.svg)](https://pypi.python.org/pypi/pytorch-semseg/0.1.2)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1185075.svg)](https://doi.org/10.5281/zenodo.1185075)\n\n\n\n## Semantic Segmentation Algorithms Implemented in PyTorch\n\nThis repository aims at mirroring popular semantic segmentation architectures in PyTorch. \n\n\n<p align=\"center\">\n<a href=\"https://www.youtube.com/watch?v=iXh9aCK3ubs\" target=\"_blank\"><img src=\"https://i.imgur.com/agvJOPF.gif\" width=\"364\"/></a>\n<img src=\"https://meetshah1995.github.io/images/blog/ss/ptsemseg.png\" width=\"49%\"/>\n</p>\n\n\n### Networks implemented\n\n* [PSPNet](https://arxiv.org/abs/1612.01105) - With support for loading pretrained models w/o caffe dependency\n* [ICNet](https://arxiv.org/pdf/1704.08545.pdf) - With optional batchnorm and pretrained models\n* [FRRN](https://arxiv.org/abs/1611.08323) - Model A and B\n* [FCN](https://arxiv.org/abs/1411.4038) - All 1 (FCN32s), 2 (FCN16s) and 3 (FCN8s) stream variants\n* [U-Net](https://arxiv.org/abs/1505.04597) - With optional deconvolution and batchnorm\n* [Link-Net](https://codeac29.github.io/projects/linknet/) - With multiple resnet backends\n* [Segnet](https://arxiv.org/abs/1511.00561) - With Unpooling using Maxpool indices\n\n\n#### Upcoming \n\n* [E-Net](https://arxiv.org/abs/1606.02147)\n* [RefineNet](https://arxiv.org/abs/1611.06612)\n\n### DataLoaders implemented\n\n* [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)\n* [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html)\n* [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/)\n* [MIT Scene Parsing Benchmark](http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip)\n* [Cityscapes](https://www.cityscapes-dataset.com/)\n* [NYUDv2](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)\n* [Sun-RGBD](http://rgbd.cs.princeton.edu/)\n\n\n### Requirements\n\n* pytorch >=0.4.0\n* torchvision ==0.2.0\n* scipy\n* tqdm\n* tensorboardX\n\n#### One-line installation\n    \n`pip install -r requirements.txt`\n\n### Data\n\n* Download data for desired dataset(s) from list of URLs [here](https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html#sec_datasets).\n* Extract the zip / tar and modify the path appropriately in your `config.yaml`\n\n\n### Usage\n\n**Setup config file**\n\n```yaml\n# Model Configuration\nmodel:\n    arch: <name> [options: 'fcn[8,16,32]s, unet, segnet, pspnet, icnet, icnetBN, linknet, frrn[A,B]'\n    <model_keyarg_1>:<value>\n\n# Data Configuration\ndata:\n    dataset: <name> [options: 'pascal, camvid, ade20k, mit_sceneparsing_benchmark, cityscapes, nyuv2, sunrgbd, vistas'] \n    train_split: <split_to_train_on>\n    val_split: <spit_to_validate_on>\n    img_rows: 512\n    img_cols: 1024\n    path: <path/to/data>\n    <dataset_keyarg1>:<value>\n\n# Training Configuration\ntraining:\n    n_workers: 64\n    train_iters: 35000\n    batch_size: 16\n    val_interval: 500\n    print_interval: 25\n    loss:\n        name: <loss_type> [options: 'cross_entropy, bootstrapped_cross_entropy, multi_scale_crossentropy']\n        <loss_keyarg1>:<value>\n\n    # Optmizer Configuration\n    optimizer:\n        name: <optimizer_name> [options: 'sgd, adam, adamax, asgd, adadelta, adagrad, rmsprop']\n        lr: 1.0e-3\n        <optimizer_keyarg1>:<value>\n\n        # Warmup LR Configuration\n        warmup_iters: <iters for lr warmup>\n        mode: <'constant' or 'linear' for warmup'>\n        gamma: <gamma for warm up>\n       \n    # Augmentations Configuration\n    augmentations:\n        gamma: x                                     #[gamma varied in 1 to 1+x]\n        hue: x                                       #[hue varied in -x to x]\n        brightness: x                                #[brightness varied in 1-x to 1+x]\n        saturation: x                                #[saturation varied in 1-x to 1+x]\n        contrast: x                                  #[contrast varied in 1-x to 1+x]\n        rcrop: [h, w]                                #[crop of size (h,w)]\n        translate: [dh, dw]                          #[reflective translation by (dh, dw)]\n        rotate: d                                    #[rotate -d to d degrees]\n        scale: [h,w]                                 #[scale to size (h,w)]\n        ccrop: [h,w]                                 #[center crop of (h,w)]\n        hflip: p                                     #[flip horizontally with chance p]\n        vflip: p                                     #[flip vertically with chance p]\n\n    # LR Schedule Configuration\n    lr_schedule:\n        name: <schedule_type> [options: 'constant_lr, poly_lr, multi_step, cosine_annealing, exp_lr']\n        <scheduler_keyarg1>:<value>\n\n    # Resume from checkpoint  \n    resume: <path_to_checkpoint>\n```\n\n**To train the model :**\n\n```\npython train.py [-h] [--config [CONFIG]] \n\n--config                Configuration file to use\n```\n\n**To validate the model :**\n\n```\nusage: validate.py [-h] [--config [CONFIG]] [--model_path [MODEL_PATH]]\n                       [--eval_flip] [--measure_time]\n\n  --config              Config file to be used\n  --model_path          Path to the saved model\n  --eval_flip           Enable evaluation with flipped image | True by default\n  --measure_time        Enable evaluation with time (fps) measurement | True\n                        by default\n```\n\n**To test the model w.r.t. a dataset on custom images(s):**\n\n```\npython test.py [-h] [--model_path [MODEL_PATH]] [--dataset [DATASET]]\n               [--dcrf [DCRF]] [--img_path [IMG_PATH]] [--out_path [OUT_PATH]]\n \n  --model_path          Path to the saved model\n  --dataset             Dataset to use ['pascal, camvid, ade20k etc']\n  --dcrf                Enable DenseCRF based post-processing\n  --img_path            Path of the input image\n  --out_path            Path of the output segmap\n```\n\n\n**If you find this code useful in your research, please consider citing:**\n\n```\n@article{mshahsemseg,\n    Author = {Meet P Shah},\n    Title = {Semantic Segmentation Architectures Implemented in PyTorch.},\n    Journal = {https://github.com/meetshah1995/pytorch-semseg},\n    Year = {2017}\n}\n```\n\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "ptsemseg",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.12109375,
          "content": "matplotlib==2.0.0\nnumpy==1.12.1\nscipy==0.19.0\ntorch==0.4.1\ntorchvision==0.2.0\ntqdm==4.11.2\npydensecrf\nprotobuf\ntensorboardX\n"
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 4.7919921875,
          "content": "import os\nimport torch\nimport argparse\nimport numpy as np\nimport scipy.misc as misc\n\n\nfrom ptsemseg.models import get_model\nfrom ptsemseg.loader import get_loader\nfrom ptsemseg.utils import convert_state_dict\n\ntry:\n    import pydensecrf.densecrf as dcrf\nexcept:\n    print(\n        \"Failed to import pydensecrf,\\\n           CRF post-processing will not work\"\n    )\n\n\ndef test(args):\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model_file_name = os.path.split(args.model_path)[1]\n    model_name = model_file_name[: model_file_name.find(\"_\")]\n\n    # Setup image\n    print(\"Read Input Image from : {}\".format(args.img_path))\n    img = misc.imread(args.img_path)\n\n    data_loader = get_loader(args.dataset)\n    loader = data_loader(root=None, is_transform=True, img_norm=args.img_norm, test_mode=True)\n    n_classes = loader.n_classes\n\n    resized_img = misc.imresize(img, (loader.img_size[0], loader.img_size[1]), interp=\"bicubic\")\n\n    orig_size = img.shape[:-1]\n    if model_name in [\"pspnet\", \"icnet\", \"icnetBN\"]:\n        # uint8 with RGB mode, resize width and height which are odd numbers\n        img = misc.imresize(img, (orig_size[0] // 2 * 2 + 1, orig_size[1] // 2 * 2 + 1))\n    else:\n        img = misc.imresize(img, (loader.img_size[0], loader.img_size[1]))\n\n    img = img[:, :, ::-1]\n    img = img.astype(np.float64)\n    img -= loader.mean\n    if args.img_norm:\n        img = img.astype(float) / 255.0\n\n    # NHWC -> NCHW\n    img = img.transpose(2, 0, 1)\n    img = np.expand_dims(img, 0)\n    img = torch.from_numpy(img).float()\n\n    # Setup Model\n    model_dict = {\"arch\": model_name}\n    model = get_model(model_dict, n_classes, version=args.dataset)\n    state = convert_state_dict(torch.load(args.model_path)[\"model_state\"])\n    model.load_state_dict(state)\n    model.eval()\n    model.to(device)\n\n    images = img.to(device)\n    outputs = model(images)\n\n    if args.dcrf:\n        unary = outputs.data.cpu().numpy()\n        unary = np.squeeze(unary, 0)\n        unary = -np.log(unary)\n        unary = unary.transpose(2, 1, 0)\n        w, h, c = unary.shape\n        unary = unary.transpose(2, 0, 1).reshape(loader.n_classes, -1)\n        unary = np.ascontiguousarray(unary)\n\n        resized_img = np.ascontiguousarray(resized_img)\n\n        d = dcrf.DenseCRF2D(w, h, loader.n_classes)\n        d.setUnaryEnergy(unary)\n        d.addPairwiseBilateral(sxy=5, srgb=3, rgbim=resized_img, compat=1)\n\n        q = d.inference(50)\n        mask = np.argmax(q, axis=0).reshape(w, h).transpose(1, 0)\n        decoded_crf = loader.decode_segmap(np.array(mask, dtype=np.uint8))\n        dcrf_path = args.out_path[:-4] + \"_drf.png\"\n        misc.imsave(dcrf_path, decoded_crf)\n        print(\"Dense CRF Processed Mask Saved at: {}\".format(dcrf_path))\n\n    pred = np.squeeze(outputs.data.max(1)[1].cpu().numpy(), axis=0)\n    if model_name in [\"pspnet\", \"icnet\", \"icnetBN\"]:\n        pred = pred.astype(np.float32)\n        # float32 with F mode, resize back to orig_size\n        pred = misc.imresize(pred, orig_size, \"nearest\", mode=\"F\")\n\n    decoded = loader.decode_segmap(pred)\n    print(\"Classes found: \", np.unique(pred))\n    misc.imsave(args.out_path, decoded)\n    print(\"Segmentation Mask Saved at: {}\".format(args.out_path))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Params\")\n    parser.add_argument(\n        \"--model_path\",\n        nargs=\"?\",\n        type=str,\n        default=\"fcn8s_pascal_1_26.pkl\",\n        help=\"Path to the saved model\",\n    )\n    parser.add_argument(\n        \"--dataset\",\n        nargs=\"?\",\n        type=str,\n        default=\"pascal\",\n        help=\"Dataset to use ['pascal, camvid, ade20k etc']\",\n    )\n\n    parser.add_argument(\n        \"--img_norm\",\n        dest=\"img_norm\",\n        action=\"store_true\",\n        help=\"Enable input image scales normalization [0, 1] \\\n                              | True by default\",\n    )\n    parser.add_argument(\n        \"--no-img_norm\",\n        dest=\"img_norm\",\n        action=\"store_false\",\n        help=\"Disable input image scales normalization [0, 1] |\\\n                              True by default\",\n    )\n    parser.set_defaults(img_norm=True)\n\n    parser.add_argument(\n        \"--dcrf\",\n        dest=\"dcrf\",\n        action=\"store_true\",\n        help=\"Enable DenseCRF based post-processing | \\\n                              False by default\",\n    )\n    parser.add_argument(\n        \"--no-dcrf\",\n        dest=\"dcrf\",\n        action=\"store_false\",\n        help=\"Disable DenseCRF based post-processing | \\\n                              False by default\",\n    )\n    parser.set_defaults(dcrf=False)\n\n    parser.add_argument(\n        \"--img_path\", nargs=\"?\", type=str, default=None, help=\"Path of the input image\"\n    )\n    parser.add_argument(\n        \"--out_path\", nargs=\"?\", type=str, default=None, help=\"Path of the output segmap\"\n    )\n    args = parser.parse_args()\n    test(args)\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 7.560546875,
          "content": "import os\nimport yaml\nimport time\nimport shutil\nimport torch\nimport random\nimport argparse\nimport numpy as np\n\nfrom torch.utils import data\nfrom tqdm import tqdm\n\nfrom ptsemseg.models import get_model\nfrom ptsemseg.loss import get_loss_function\nfrom ptsemseg.loader import get_loader\nfrom ptsemseg.utils import get_logger\nfrom ptsemseg.metrics import runningScore, averageMeter\nfrom ptsemseg.augmentations import get_composed_augmentations\nfrom ptsemseg.schedulers import get_scheduler\nfrom ptsemseg.optimizers import get_optimizer\n\nfrom tensorboardX import SummaryWriter\n\n\ndef train(cfg, writer, logger):\n\n    # Setup seeds\n    torch.manual_seed(cfg.get(\"seed\", 1337))\n    torch.cuda.manual_seed(cfg.get(\"seed\", 1337))\n    np.random.seed(cfg.get(\"seed\", 1337))\n    random.seed(cfg.get(\"seed\", 1337))\n\n    # Setup device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Setup Augmentations\n    augmentations = cfg[\"training\"].get(\"augmentations\", None)\n    data_aug = get_composed_augmentations(augmentations)\n\n    # Setup Dataloader\n    data_loader = get_loader(cfg[\"data\"][\"dataset\"])\n    data_path = cfg[\"data\"][\"path\"]\n\n    t_loader = data_loader(\n        data_path,\n        is_transform=True,\n        split=cfg[\"data\"][\"train_split\"],\n        img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n        augmentations=data_aug,\n    )\n\n    v_loader = data_loader(\n        data_path,\n        is_transform=True,\n        split=cfg[\"data\"][\"val_split\"],\n        img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n    )\n\n    n_classes = t_loader.n_classes\n    trainloader = data.DataLoader(\n        t_loader,\n        batch_size=cfg[\"training\"][\"batch_size\"],\n        num_workers=cfg[\"training\"][\"n_workers\"],\n        shuffle=True,\n    )\n\n    valloader = data.DataLoader(\n        v_loader, batch_size=cfg[\"training\"][\"batch_size\"], num_workers=cfg[\"training\"][\"n_workers\"]\n    )\n\n    # Setup Metrics\n    running_metrics_val = runningScore(n_classes)\n\n    # Setup Model\n    model = get_model(cfg[\"model\"], n_classes).to(device)\n\n    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n\n    # Setup optimizer, lr_scheduler and loss function\n    optimizer_cls = get_optimizer(cfg)\n    optimizer_params = {k: v for k, v in cfg[\"training\"][\"optimizer\"].items() if k != \"name\"}\n\n    optimizer = optimizer_cls(model.parameters(), **optimizer_params)\n    logger.info(\"Using optimizer {}\".format(optimizer))\n\n    scheduler = get_scheduler(optimizer, cfg[\"training\"][\"lr_schedule\"])\n\n    loss_fn = get_loss_function(cfg)\n    logger.info(\"Using loss {}\".format(loss_fn))\n\n    start_iter = 0\n    if cfg[\"training\"][\"resume\"] is not None:\n        if os.path.isfile(cfg[\"training\"][\"resume\"]):\n            logger.info(\n                \"Loading model and optimizer from checkpoint '{}'\".format(cfg[\"training\"][\"resume\"])\n            )\n            checkpoint = torch.load(cfg[\"training\"][\"resume\"])\n            model.load_state_dict(checkpoint[\"model_state\"])\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n            start_iter = checkpoint[\"epoch\"]\n            logger.info(\n                \"Loaded checkpoint '{}' (iter {})\".format(\n                    cfg[\"training\"][\"resume\"], checkpoint[\"epoch\"]\n                )\n            )\n        else:\n            logger.info(\"No checkpoint found at '{}'\".format(cfg[\"training\"][\"resume\"]))\n\n    val_loss_meter = averageMeter()\n    time_meter = averageMeter()\n\n    best_iou = -100.0\n    i = start_iter\n    flag = True\n\n    while i <= cfg[\"training\"][\"train_iters\"] and flag:\n        for (images, labels) in trainloader:\n            i += 1\n            start_ts = time.time()\n            scheduler.step()\n            model.train()\n            images = images.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n\n            loss = loss_fn(input=outputs, target=labels)\n\n            loss.backward()\n            optimizer.step()\n\n            time_meter.update(time.time() - start_ts)\n\n            if (i + 1) % cfg[\"training\"][\"print_interval\"] == 0:\n                fmt_str = \"Iter [{:d}/{:d}]  Loss: {:.4f}  Time/Image: {:.4f}\"\n                print_str = fmt_str.format(\n                    i + 1,\n                    cfg[\"training\"][\"train_iters\"],\n                    loss.item(),\n                    time_meter.avg / cfg[\"training\"][\"batch_size\"],\n                )\n\n                print(print_str)\n                logger.info(print_str)\n                writer.add_scalar(\"loss/train_loss\", loss.item(), i + 1)\n                time_meter.reset()\n\n            if (i + 1) % cfg[\"training\"][\"val_interval\"] == 0 or (i + 1) == cfg[\"training\"][\n                \"train_iters\"\n            ]:\n                model.eval()\n                with torch.no_grad():\n                    for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n                        images_val = images_val.to(device)\n                        labels_val = labels_val.to(device)\n\n                        outputs = model(images_val)\n                        val_loss = loss_fn(input=outputs, target=labels_val)\n\n                        pred = outputs.data.max(1)[1].cpu().numpy()\n                        gt = labels_val.data.cpu().numpy()\n\n                        running_metrics_val.update(gt, pred)\n                        val_loss_meter.update(val_loss.item())\n\n                writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n                logger.info(\"Iter %d Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n\n                score, class_iou = running_metrics_val.get_scores()\n                for k, v in score.items():\n                    print(k, v)\n                    logger.info(\"{}: {}\".format(k, v))\n                    writer.add_scalar(\"val_metrics/{}\".format(k), v, i + 1)\n\n                for k, v in class_iou.items():\n                    logger.info(\"{}: {}\".format(k, v))\n                    writer.add_scalar(\"val_metrics/cls_{}\".format(k), v, i + 1)\n\n                val_loss_meter.reset()\n                running_metrics_val.reset()\n\n                if score[\"Mean IoU : \\t\"] >= best_iou:\n                    best_iou = score[\"Mean IoU : \\t\"]\n                    state = {\n                        \"epoch\": i + 1,\n                        \"model_state\": model.state_dict(),\n                        \"optimizer_state\": optimizer.state_dict(),\n                        \"scheduler_state\": scheduler.state_dict(),\n                        \"best_iou\": best_iou,\n                    }\n                    save_path = os.path.join(\n                        writer.file_writer.get_logdir(),\n                        \"{}_{}_best_model.pkl\".format(cfg[\"model\"][\"arch\"], cfg[\"data\"][\"dataset\"]),\n                    )\n                    torch.save(state, save_path)\n\n            if (i + 1) == cfg[\"training\"][\"train_iters\"]:\n                flag = False\n                break\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"config\")\n    parser.add_argument(\n        \"--config\",\n        nargs=\"?\",\n        type=str,\n        default=\"configs/fcn8s_pascal.yml\",\n        help=\"Configuration file to use\",\n    )\n\n    args = parser.parse_args()\n\n    with open(args.config) as fp:\n        cfg = yaml.load(fp)\n\n    run_id = random.randint(1, 100000)\n    logdir = os.path.join(\"runs\", os.path.basename(args.config)[:-4], str(run_id))\n    writer = SummaryWriter(log_dir=logdir)\n\n    print(\"RUNDIR: {}\".format(logdir))\n    shutil.copy(args.config, logdir)\n\n    logger = get_logger(logdir)\n    logger.info(\"Let the games begin\")\n\n    train(cfg, writer, logger)\n"
        },
        {
          "name": "validate.py",
          "type": "blob",
          "size": 3.8828125,
          "content": "import yaml\nimport torch\nimport argparse\nimport timeit\nimport numpy as np\n\nfrom torch.utils import data\n\n\nfrom ptsemseg.models import get_model\nfrom ptsemseg.loader import get_loader\nfrom ptsemseg.metrics import runningScore\nfrom ptsemseg.utils import convert_state_dict\n\ntorch.backends.cudnn.benchmark = True\n\n\ndef validate(cfg, args):\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Setup Dataloader\n    data_loader = get_loader(cfg[\"data\"][\"dataset\"])\n    data_path = cfg[\"data\"][\"path\"]\n\n    loader = data_loader(\n        data_path,\n        split=cfg[\"data\"][\"val_split\"],\n        is_transform=True,\n        img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n    )\n\n    n_classes = loader.n_classes\n\n    valloader = data.DataLoader(loader, batch_size=cfg[\"training\"][\"batch_size\"], num_workers=8)\n    running_metrics = runningScore(n_classes)\n\n    # Setup Model\n\n    model = get_model(cfg[\"model\"], n_classes).to(device)\n    state = convert_state_dict(torch.load(args.model_path)[\"model_state\"])\n    model.load_state_dict(state)\n    model.eval()\n    model.to(device)\n\n    for i, (images, labels) in enumerate(valloader):\n        start_time = timeit.default_timer()\n\n        images = images.to(device)\n\n        if args.eval_flip:\n            outputs = model(images)\n\n            # Flip images in numpy (not support in tensor)\n            outputs = outputs.data.cpu().numpy()\n            flipped_images = np.copy(images.data.cpu().numpy()[:, :, :, ::-1])\n            flipped_images = torch.from_numpy(flipped_images).float().to(device)\n            outputs_flipped = model(flipped_images)\n            outputs_flipped = outputs_flipped.data.cpu().numpy()\n            outputs = (outputs + outputs_flipped[:, :, :, ::-1]) / 2.0\n\n            pred = np.argmax(outputs, axis=1)\n        else:\n            outputs = model(images)\n            pred = outputs.data.max(1)[1].cpu().numpy()\n\n        gt = labels.numpy()\n\n        if args.measure_time:\n            elapsed_time = timeit.default_timer() - start_time\n            print(\n                \"Inference time \\\n                  (iter {0:5d}): {1:3.5f} fps\".format(\n                    i + 1, pred.shape[0] / elapsed_time\n                )\n            )\n        running_metrics.update(gt, pred)\n\n    score, class_iou = running_metrics.get_scores()\n\n    for k, v in score.items():\n        print(k, v)\n\n    for i in range(n_classes):\n        print(i, class_iou[i])\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Hyperparams\")\n    parser.add_argument(\n        \"--config\",\n        nargs=\"?\",\n        type=str,\n        default=\"configs/fcn8s_pascal.yml\",\n        help=\"Config file to be used\",\n    )\n    parser.add_argument(\n        \"--model_path\",\n        nargs=\"?\",\n        type=str,\n        default=\"fcn8s_pascal_1_26.pkl\",\n        help=\"Path to the saved model\",\n    )\n    parser.add_argument(\n        \"--eval_flip\",\n        dest=\"eval_flip\",\n        action=\"store_true\",\n        help=\"Enable evaluation with flipped image |\\\n                              True by default\",\n    )\n    parser.add_argument(\n        \"--no-eval_flip\",\n        dest=\"eval_flip\",\n        action=\"store_false\",\n        help=\"Disable evaluation with flipped image |\\\n                              True by default\",\n    )\n    parser.set_defaults(eval_flip=True)\n\n    parser.add_argument(\n        \"--measure_time\",\n        dest=\"measure_time\",\n        action=\"store_true\",\n        help=\"Enable evaluation with time (fps) measurement |\\\n                              True by default\",\n    )\n    parser.add_argument(\n        \"--no-measure_time\",\n        dest=\"measure_time\",\n        action=\"store_false\",\n        help=\"Disable evaluation with time (fps) measurement |\\\n                              True by default\",\n    )\n    parser.set_defaults(measure_time=True)\n\n    args = parser.parse_args()\n\n    with open(args.config) as fp:\n        cfg = yaml.load(fp)\n\n    validate(cfg, args)\n"
        }
      ]
    }
  ]
}