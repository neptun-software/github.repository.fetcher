{
  "metadata": {
    "timestamp": 1736559919397,
    "page": 682,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Luodian/Otter",
      "stars": 3576,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.26171875,
          "content": "example_unified_data/negative_sample/all_captions.txt filter=lfs diff=lfs merge=lfs -text\nLAVIS/coco-caption/annotations/captions_val2014.json filter=lfs diff=lfs merge=lfs -text\nLAVIS/coco-caption/annotations/caption_flickr30k.json filter=lfs diff=lfs merge=lfs -text"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.951171875,
          "content": "# Byte-compiled / optimized / DLL files\n*.py[cod]\n__pycache__/\n*$py.class\n*.sage.py\n\n# Special and Backup files\n*.bak\n*.log\n*.tsv\n*.gz\n*.zip\n*.dat\n*.dir\n*.html\n*.mp4\n*.MP4\n*.png\n*.pt\n*.bin\n*.mo\n*.pot\n*.manifest\n*.spec\n*.egg\n*.so\ncheckpoint\nLICENSE\n\n# Editor and IDE configurations\n.vscode/\n.spyderproject\n.spyproject\n.ropeproject\n\n# Packaging and Distribution\n*.egg-info/\ndist/\nbuild/\ndevelop-eggs/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nsdist/\nwheels/\nparts/\nshare/python-wheels/\n.installed.cfg\nMANIFEST\n\n# Unit Test and Coverage\n.coverage*\n.cache\n*.cover\n*.py,cover\n.htmlcov/\n.tox/\n.nox/\n.hypothesis/\n.pytest_cache/\ncover/\nmypy_cache/\n.dmypy.json\ndmypy.json\n.pyre/\n.pytype/\ncython_debug/\n*/__pycache__/**\n*/.pytest_cache/**\n*/.mypy_cache/**\n*/.cache/**\n*/.coverage.*\n*/.c\n\n# Environments and Dependencies\n.env\n.venv\n.env.bak/\n.venv.bak/\nenv/\nvenv/\nENV/\npip-log.txt\npip-delete-this-directory.txt\nPipfile.lock\npoetry.lock\n__pypackages__/\n\n# Web Frameworks\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\ninstance/\n.webassets-cache\n.scrapy\n.site\ncelerybeat-schedule\ncelerybeat.pid\n\n# Documentation and Notebooks\ndocs/_build/\n.ipynb_checkpoints\nprofile_default/\nipython_config.py\n\n# Project specific\noutput/\ndebug*/\nwandb/\narchived/\namlt/\nscripts/\nnginx/\nlogs/\nofa_compress/\ntrain_*.sh\ngpt_playground/\ndata/\nazure/\n.deepspeed_env\ncheckpoints/\npipeline/serve/examples/\nmimic-it/syphus/annotations/**\ntools/\notter9B-mpt7b-0705/\ntokenizer_checklist.chk\ntokenizer.model\ndownload.sh\nUSE_POLICY.md\n\n# Miscellaneous\n*.pyc\nopen_flamingo.egg-info\nllama-7b-hf/*\ncache/\n*.code-workspace\npipeline/benchmarks/ckpts/*\npipeline/benchmarks/models/Ask_Anything\npipeline/benchmarks/models/FrozenBiLM\npipeline/benchmarks/models/LLaMA_Adapter\n\nshared_scripts/gcp_instance/**\nshared_scripts/shai_instance/**\npipeline/benchmarks/models/Video_ChatGPT\npipeline/benchmarks/models/LLaVA\npipeline/benchmarks/models/llava\npipeline/benchmarks/evaluation_result\npipeline/serve/user_logs\nconfig.yaml\n\nazure_storage/\ncheckpoints/\n*.ttf"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.080078125,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0126953125,
          "content": "MIT License\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.8076171875,
          "content": "<p align=\"center\" width=\"100%\">\n<img src=\"https://i.postimg.cc/mksBCbV9/brand-title.png\"  width=\"80%\" height=\"80%\">\n</p>\n\n---\n![](https://img.shields.io/badge/otter-v0.3-darkcyan)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40Us)](https://twitter.com/BoLi68567011)\n![](https://img.shields.io/github/stars/luodian/otter?style=social)\n[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FLuodian%2Fotter&count_bg=%23FFA500&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=visitors&edge_flat=false)](https://hits.seeyoufarm.com)\n[![litellm](https://img.shields.io/badge/%20%F0%9F%9A%85%20liteLLM-OpenAI%7CAzure%7CAnthropic%7CPalm%7CCohere-blue?color=green)](https://github.com/BerriAI/litellm)\n\n[Project Credits](https://github.com/Luodian/Otter/blob/main/docs/credits.md) | [Otter Paper](https://arxiv.org/abs/2305.03726) | [OtterHD Paper](https://arxiv.org/abs/2311.04219) | [MIMIC-IT Paper](https://arxiv.org/abs/2306.05425)\n\n**Checkpoints:**\n\n- [luodian/OTTER-Image-MPT7B](https://huggingface.co/luodian/OTTER-Image-MPT7B)\n- [luodian/OTTER-Video-LLaMA7B-DenseCaption](https://huggingface.co/luodian/OTTER-Video-LLaMA7B-DenseCaption)\n\nFor who in the mainland China: [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/YuanhanZhang/OTTER-Image-MPT7B) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/YuanhanZhang/OTTER-Video-LLaMA7B-DenseCaption)\n\n**Disclaimer:** The code may not be perfectly polished and refactored, but **all opensourced codes are tested and runnable** as we also use the code to support our research. If you have any questions, please feel free to open an issue. We are eagerly looking forward to suggestions and PRs to improve the code quality.\n\n## 🦾 Update\n\n**[2023-11]: Supporting GPT4V's Evaluation on 8 Benchmarks; Anouncing OtterHD-8B, improved from Fuyu-8B. Checkout [OtterHD](./docs/OtterHD.md) for details.**\n\n<div style=\"text-align:center\">\n<img src=\"https://i.postimg.cc/dtxQQzt6/demo0.png\"  width=\"100%\" height=\"100%\">\n</div>\n\n1. 🦦 Added [OtterHD](./docs/OtterHD.md), a multimodal fine-tuned from [Fuyu-8B](https://huggingface.co/adept/fuyu-8b) to facilitate fine-grained interpretations of high-resolution visual input *without a explicit vision encoder module*. All image patches are linear transformed and processed together with text tokens. This is a very innovative and elegant exploration. We are fascinated and paved in this way, we opensourced the finetune script for Fuyu-8B and improve training throughput by 4-5 times faster with [Flash-Attention-2](https://github.com/Dao-AILab/flash-attention). Try our finetune script at [OtterHD](./docs/OtterHD.md).\n2. 🔍 Added [MagnifierBench](./docs/OtterHD.md), an evaluation benchmark tailored to assess whether the model can identify the tiny objects' information (1% image size) and spatial relationships.\n3. Improved pipeline for [Pretrain](pipeline/train/pretraining.py) | [SFT](pipeline/train/instruction_following.py) | [RLHF]() with (part of) current leading LMMs.\n   1. **Models**: [Otter](https://arxiv.org/abs/2305.03726) | [OpenFlamingo](https://arxiv.org/abs/2308.01390) | [Idefics](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) | [Fuyu](https://huggingface.co/adept/fuyu-8b)\n   2. **Training Datasets Interface: (Pretrain)** MMC4 | LAION2B | CC3M | CC12M, **(SFT)** MIMIC-IT | M3IT | LLAVAR | LRV | SVIT...\n        - *We tested above datasets for both pretraining and instruction tuning with OpenFlamingo and Otter. We also tested the datasets with Idefics and Fuyu for instruction tuning. We will opensource the training scripts gradually.*\n   3. [**Benchmark Interface**](https://huggingface.co/Otter-AI): MagnifierBench/MMBench/MM-VET/MathVista/POPE/MME/SicenceQA/SeedBench. Run them can be in one-click, please see [Benchmark](./docs/benchmark_eval.md) for details.\n    ```yaml\n        datasets:\n        - name: magnifierbench\n            split: test\n            prompt: Answer with the option's letter from the given choices directly.\n            api_key: [Your API Key] # GPT4 or GPT3.5 to evaluate the answers and ground truth.\n            debug: true # put debug=true will save the model response in log file.\n        - name: mme\n            split: test\n            debug: true\n        - name: mmbench\n            split: test\n            debug: true\n\n        models:\n        - name: gpt4v\n            api_key: [Your API Key] # to call GPT4V model.\n    ```\n   4. **Code refactorization** for **organizing multiple groups of datasets with integrated yaml file**, see details at [managing datasets in MIMIC-IT format](docs/mimicit_format.md). For example, \n    ```yaml\n        IMAGE_TEXT: # Group name should be in [IMAGE_TEXT, TEXT_ONLY, IMAGE_TEXT_IN_CONTEXT]\n            LADD: # Dataset name can be assigned at any name you want\n                mimicit_path: azure_storage/json/LA/LADD_instructions.json # Path of the instruction json file\n                images_path: azure_storage/Parquets/LA.parquet # Path of the image parquet file\n                num_samples: -1 # Number of samples you want to use, -1 means use all samples, if not set, default is -1.\n            M3IT_CAPTIONING:\n                mimicit_path: azure_storage/json/M3IT/captioning/coco/coco_instructions.json\n                images_path: azure_storage/Parquets/coco.parquet\n                num_samples: 20000\n    ```\n   *This is a major change and would result previous code not runnable, please check the details.*\n\n**[2023-08]**\n\n1. Added Support for using Azure, Anthropic, Palm, Cohere models for Self-Instruct with Syphus pipeline, for information on usage modify [this line](https://github.com/Luodian/Otter/blob/16d73b399fac6352ebff7504b1acb1f228fbf3f4/mimic-it/syphus/file_utils.py#L53) with your selected model and set your API keys in the environment. For more information see [LiteLLM](https://github.com/BerriAI/litellm/)\n\n**[2023-07]: Anouncing MIMIC-IT dataset for multiple interleaved image-text/video instruction tuning.**\n\n1. 🤗 Checkout [MIMIC-IT](https://huggingface.co/datasets/pufanyi/MIMICIT) on Huggingface datasets.\n2. 🥚 Update [Eggs](./mimic-it/README.md/#eggs) section for downloading MIMIC-IT dataset.\n3. 🥃 Contact us **if you wish to develop Otter for your scenarios** (for satellite images or funny videos?). We aim to support and assist with Otter's diverse use cases. OpenFlamingo and Otter are strong models with the [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)'s excellently designed architecture that accepts multiple images/videos or other modality inputs. Let's build more interesting models together.\n\n**[2023-06]**\n\n1. 🧨 [Download MIMIC-IT Dataset](https://entuedu-my.sharepoint.com/:f:/g/personal/libo0013_e_ntu_edu_sg/Eo9bgNV5cjtEswfA-HfjNNABiKsjDzSWAl5QYAlRZPiuZA?e=M9isDT). For more details on navigating the dataset, please refer to [MIMIC-IT Dataset README](mimic-it/README.md).\n2. 🏎️ [Run Otter Locally](./pipeline/demo). You can run our model locally with at least 16G GPU mem for tasks like image/video tagging and captioning and identifying harmful content. We fix a bug related to video inference where `frame tensors` were mistakenly unsqueezed to a wrong `vision_x`.\n   > Make sure to adjust the `sys.path.append(\"../..\")` correctly to access `otter.modeling_otter` in order to launch the model.\n3. 🤗 Check our [paper](https://arxiv.org/abs/2306.05425) introducing MIMIC-IT in details. Meet MIMIC-IT, the first multimodal in-context instruction tuning dataset with 2.8M instructions! From general scene understanding to spotting subtle differences and enhancing egocentric view comprehension for AR headsets, our MIMIC-IT dataset has it all.\n\n## 🦦 Why In-Context Instruction Tuning?\n\nLarge Language Models (LLMs) have demonstrated exceptional universal aptitude as few/zero-shot learners for numerous tasks, owing to their pre-training on extensive text data. Among these LLMs, GPT-3 stands out as a prominent model with significant capabilities. Additionally, variants of GPT-3, namely InstructGPT and ChatGPT, have proven effective in interpreting natural language instructions to perform complex real-world tasks, thanks to instruction tuning.\n\nMotivated by the upstream interleaved format pretraining of the Flamingo model, we present 🦦 Otter, a multi-modal model based on OpenFlamingo (the open-sourced version of DeepMind's Flamingo). We train our Otter in an in-context instruction tuning way on our proposed **MI**-**M**odal **I**n-**C**ontext **I**nstruction **T**uning (**MIMIC-IT**) dataset. Otter showcases improved instruction-following and in-context learning ability in both images and videos.\n\n## 🗄 MIMIC-IT Dataset Details\n\n<p align=\"center\" width=\"100%\">\n<img src=\"https://i.postimg.cc/yYMm1G5X/mimicit-logo.png\"  width=\"80%\" height=\"80%\">\n</p>\n\nMIMIC-IT enables the application of egocentric visual assistant model that can serve that can answer your questions like **Hey, Do you think I left my keys on the table?**. Harness the power of MIMIC-IT to unlock the full potential of your AI-driven visual assistant and elevate your interactive vision-language tasks to new heights.\n\n<p align=\"center\" width=\"100%\">\n<img src=\"https://i.postimg.cc/RCGp0vQ1/syphus.png\"  width=\"80%\" height=\"80%\">\n</p>\n\nWe also introduce **Syphus**, an automated pipeline for generating high-quality instruction-response pairs in multiple languages. Building upon the framework proposed by LLaVA, we utilize ChatGPT to generate instruction-response pairs based on visual content. To ensure the quality of the generated instruction-response pairs, our pipeline incorporates system messages, visual annotations, and in-context examples as prompts for ChatGPT.\n\nFor more details, please check the [MIMIC-IT dataset](mimic-it/README.md).\n\n## 🤖 Otter Model Details\n\n<div style=\"text-align:center\">\n<img src=\"https://i.postimg.cc/CKgQ2PP7/otter-teaser.png\"  width=\"100%\" height=\"100%\">\n</div>\n\nOtter is designed to support multi-modal in-context instruction tuning based on the OpenFlamingo model, which involves conditioning the language model on the corresponding media, such as an image that corresponds to a caption or an instruction-response pair.\n\nWe train Otter on MIMIC-IT dataset with approximately 2.8 million in-context instruction-response pairs, which are structured into a cohesive template to facilitate various tasks. Otter supports videos inputs (frames are arranged as original Flamingo's implementation) and multiple images inputs as in-context examples, which is **the first multi-modal instruction tuned model**.\n\nThe following template encompasses images, user instructions, and model-generated responses, utilizing the `User` and `GPT` role labels to enable seamless user-assistant interactions.\n\n```python\nprompt = f\"<image>User: {instruction} GPT:<answer> {response}<endofchunk>\"\n```\n\nTraining the Otter model on the MIMIC-IT dataset allows it to acquire different capacities, as demonstrated by the LA and SD tasks. Trained on the LA task, the model exhibits exceptional scene comprehension, reasoning abilities, and multi-round conversation capabilities.\n\n```python\n# multi-round of conversation\nprompt = f\"<image>User: {first_instruction} GPT:<answer> {first_response}<endofchunk>User: {second_instruction} GPT:<answer>\"\n```\n\nRegarding the concept of organizing visual-language in-context examples, we demonstrate here the acquired ability of the Otter model to follow inter-contextual instructions after training on the LA-T2T task. The organized input data format is as follows:\n\n```python\n# Multiple in-context example with similar instructions\nprompt = f\"<image>User:{ict_first_instruction} GPT: <answer>{ict_first_response}<|endofchunk|><image>User:{ict_second_instruction} GPT: <answer>{ict_second_response}<|endofchunk|><image>User:{query_instruction} GPT: <answer>\"\n```\n\nFor more details, please refer to our [paper](https://arxiv.org/abs/2306.05425)'s appendix for other tasks.\n\n## 🗂️ Environments\n\n1. Compare cuda version returned by nvidia-smi and nvcc --version. They need to match. Or at least, the version get by nvcc --version should be <= the version get by nvidia-smi.\n2. Install the pytorch that matches your cuda version. (e.g. cuda 11.7 torch 2.0.0). We have successfully run this code on cuda 11.1 torch 1.10.1 and cuda 11.7 torch 2.0.0. You can refer to PyTorch's documentation, [Latest](https://pytorch.org/) or [Previous](https://pytorch.org/get-started/previous-versions/).\n3. You may install via `conda env create -f environment.yml`. Especially to make sure the `transformers>=4.28.0`, `accelerate>=0.18.0`.\n\nAfter configuring environment, you can use the 🦩 Flamingo model / 🦦 Otter model as a 🤗 Hugging Face model with only a few lines! One-click and then model configs/weights are downloaded automatically. Please refer to [Huggingface Otter/Flamingo](./docs/huggingface_compatible.md) for details.\n\n## ☄️ Training\n\nOtter is trained based on OpenFlamingo. You may need to use converted weights at [luodian/OTTER-9B-INIT](https://huggingface.co/luodian/OTTER-9B-INIT) or [luodian/OTTER-MPT7B-Init](https://huggingface.co/luodian/OTTER-MPT7B-Init). They are respectively converted from [OpenFlamingo-LLaMA7B-v1](https://huggingface.co/openflamingo/OpenFlamingo-9B) and [OpenFlamingo-MPT7B-v2](https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b), we added a `<answer>` token for Otter's downstream instruction tuning.\n\nYou may also use any trained Otter weights to start with your training on top of ours, see them at [Otter Weights](https://huggingface.co/luodian). You can refer to [MIMIC-IT](https://github.com/Luodian/Otter/tree/main/mimic-it) for preparing image/instruction/train json files.\n\n```bash\nexport PYTHONPATH=.\nRUN_NAME=\"Otter_MPT7B\"\nGPU=8\nWORKERS=$((${GPU}*2))\n\necho \"Using ${GPU} GPUs and ${WORKERS} workers\"\necho \"Running ${RUN_NAME}\"\n\naccelerate launch --config_file=./pipeline/accelerate_configs/accelerate_config_zero3.yaml \\\n    --num_processes=${GPU} \\\n    pipeline/train/instruction_following.py \\\n    --pretrained_model_name_or_path=luodian/OTTER-MPT7B-Init \\\n    --model_name=otter \\\n    --instruction_format=simple \\\n    --training_data_yaml=./shared_scripts/Demo_Data.yaml \\\n    --batch_size=8 \\\n    --num_epochs=3 \\\n    --report_to_wandb \\\n    --wandb_entity=ntu-slab \\\n    --external_save_dir=./checkpoints \\\n    --run_name=${RUN_NAME} \\\n    --wandb_project=Otter_MPTV \\\n    --workers=${WORKERS} \\\n    --lr_scheduler=cosine \\\n    --learning_rate=2e-5 \\\n    --warmup_steps_ratio=0.01 \\\n    --save_hf_model \\\n    --max_seq_len=1024 \\\n```\n\n## 📑 Citation\n\nIf you found this repository useful, please consider citing:\n\n```\n@article{li2023otter,\n  title={Otter: A Multi-Modal Model with In-Context Instruction Tuning},\n  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},\n  journal={arXiv preprint arXiv:2305.03726},\n  year={2023}\n}\n\n@article{li2023mimicit,\n    title={MIMIC-IT: Multi-Modal In-Context Instruction Tuning},\n    author={Bo Li and Yuanhan Zhang and Liangyu Chen and Jinghao Wang and Fanyi Pu and Jingkang Yang and Chunyuan Li and Ziwei Liu},\n    year={2023},\n    eprint={2306.05425},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n### 👨‍🏫 Acknowledgements\n\nWe thank [Jack Hessel](https://jmhessel.com/) for the advise and support, as well as the [OpenFlamingo](https://github.com/mlfoundations/open_flamingo) team for their great contribution to the open source community.\n\nHuge accolades to [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) and [OpenFlamingo](https://github.com/mlfoundations/open_flamingo) team for the work on this great architecture.\n\n### 📝 Related Projects\n\n- [LLaVA: Visual Instruction Tuning](https://github.com/haotian-liu/LLaVA)\n- [Instruction Tuning with GPT4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n"
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 0.287109375,
          "content": "import pytest\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        \"--yaml-path\",\n        action=\"store\",\n        default=\"default_yaml_path.yaml\",\n        help=\"Path to the YAML file\",\n    )\n\n\n@pytest.fixture\ndef yaml_path(request):\n    return request.config.getoption(\"--yaml-path\")\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.1279296875,
          "content": "name: otter\nchannels:\n  - defaults\ndependencies:\n  - python=3.9\n  - conda-forge::openjdk\n  - pip\n  - pip:\n    - -r requirements.txt"
        },
        {
          "name": "mimic-it",
          "type": "tree",
          "content": null
        },
        {
          "name": "pipeline",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.15234375,
          "content": "[tool.black]\nline-length = 240\n\n[build-system]\nrequires = [\"setuptools>=42\", \"wheel\", \"setuptools_scm[tomli]>=6.3\"]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.0595703125,
          "content": "[pytest]\nmarkers =\n    prerun: mark a test as a prerun check."
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.6083984375,
          "content": "accelerate>=0.19.0\nbraceexpand>=0.1.7\neinops>=0.6.1\neinops_exts>=0.0.4\nfastapi>=0.95.2\ngradio>=3.33.1\nhuggingface_hub>=0.13.3\nimportlib_metadata>=6.6.0\ninflection>=0.5.1\nmarkdown2>=2.4.8\nmore_itertools>=9.1.0\nnltk>=3.8.1\nnumpy>=1.23.5\nopen_clip_torch>=2.16.0\nopenai>=1.1.1\nopencv_python_headless>=4.5.5.64\nPillow>=9.5.0\npycocoevalcap>=1\npycocotools>=2.0.6\nRequests>=2.31.0\nscipy>=1.10.1\ntimm>=0.9.2\ntqdm>=4.65.0\ntransformers==4.35.1\nuvicorn>=0.22.0\nwebdataset>=0.2.48\nnatsort>=8.4.0\npeft>=0.4.0\nijson>=3.2.3\nyajl>=0.3.5\ndeepspeed>=0.10.0\nwandb>=0.15.8\ntrl>=0.5.0\ncffi>=1.15.1\npyyaml>=6.0.1\npytest>=7.4.2\nprettytable>=3.9.0\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.099609375,
          "content": "import json\nfrom setuptools import setup, find_packages\n\n\nwith open(\"requirements.txt\") as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"otter-ai\",\n    version=\"0.0.0-alpha-7\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=requirements,\n    # package_data={\n    #     \"otter\": [\"resources/**/*\"],\n    # },\n    # include_package_data=True,\n    author=\"Otter Team\",\n    author_email=\"drluodian@gmail.com\",\n    description=\"Otter: A Multi-Modal Model with In-Context Instruction Tuning\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/Luodian/Otter\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n    ],\n    # entry_points={\n    #     \"console_scripts\": [\n    #         # \"syphus = syphus.cli.syphus_cli:main\",\n    #     ],\n    # },\n)\n"
        },
        {
          "name": "shared_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "unit_tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "xformers_model",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}