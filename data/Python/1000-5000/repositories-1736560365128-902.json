{
  "metadata": {
    "timestamp": 1736560365128,
    "page": 902,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ayooshkathuria/pytorch-yolo-v3",
      "stars": 3305,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1298828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.3583984375,
          "content": "# A PyTorch implementation of a YOLO v3 Object Detector\n\n[UPDATE] : This repo serves as a driver code for my research. I just graduated college, and am very busy looking for research internship / fellowship roles before eventually applying for a masters. I won't have the time to look into issues for the time being. Thank you.\n\n\nThis repository contains code for a object detector based on [YOLOv3: An Incremental Improvement](https://pjreddie.com/media/files/papers/YOLOv3.pdf), implementedin PyTorch. The code is based on the official code of [YOLO v3](https://github.com/pjreddie/darknet), as well as a PyTorch \nport of the original code, by [marvis](https://github.com/marvis/pytorch-yolo2). One of the goals of this code is to improve\nupon the original port by removing redundant parts of the code (The official code is basically a fully blown deep learning \nlibrary, and includes stuff like sequence models, which are not used in YOLO). I've also tried to keep the code minimal, and \ndocument it as well as I can. \n\n### Tutorial for building this detector from scratch\nIf you want to understand how to implement this detector by yourself from scratch, then you can go through this very detailed 5-part tutorial series I wrote on Paperspace. Perfect for someone who wants to move from beginner to intermediate pytorch skills. \n\n[Implement YOLO v3 from scratch](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)\n\nAs of now, the code only contains the detection module, but you should expect the training module soon. :) \n\n## Requirements\n1. Python 3.5\n2. OpenCV\n3. PyTorch 0.4\n\nUsing PyTorch 0.3 will break the detector.\n\n\n\n## Detection Example\n\n![Detection Example](https://i.imgur.com/m2jwneng.png)\n## Running the detector\n\n### On single or multiple images\n\nClone, and `cd` into the repo directory. The first thing you need to do is to get the weights file\nThis time around, for v3, authors has supplied a weightsfile only for COCO [here](https://pjreddie.com/media/files/yolov3.weights), and place \n\nthe weights file into your repo directory. Or, you could just type (if you're on Linux)\n\n```\nwget https://pjreddie.com/media/files/yolov3.weights \npython detect.py --images imgs --det det \n```\n\n\n`--images` flag defines the directory to load images from, or a single image file (it will figure it out), and `--det` is the directory\nto save images to. Other setting such as batch size (using `--bs` flag) , object threshold confidence can be tweaked with flags that can be looked up with. \n\n```\npython detect.py -h\n```\n\n### Speed Accuracy Tradeoff\nYou can change the resolutions of the input image by the `--reso` flag. The default value is 416. Whatever value you chose, rememeber **it should be a multiple of 32 and greater than 32**. Weird things will happen if you don't. You've been warned. \n\n```\npython detect.py --images imgs --det det --reso 320\n```\n\n### On Video\nFor this, you should run the file, video_demo.py with --video flag specifying the video file. The video file should be in .avi format\nsince openCV only accepts OpenCV as the input format. \n\n```\npython video_demo.py --video video.avi\n```\n\nTweakable settings can be seen with -h flag. \n\n### Speeding up Video Inference\n\nTo speed video inference, you can try using the video_demo_half.py file instead which does all the inference with 16-bit half \nprecision floats instead of 32-bit float. I haven't seen big improvements, but I attribute that to having an older card \n(Tesla K80, Kepler arch). If you have one of cards with fast float16 support, try it out, and if possible, benchmark it. \n\n### On a Camera\nSame as video module, but you don't have to specify the video file since feed will be taken from your camera. To be precise, \nfeed will be taken from what the OpenCV, recognises as camera 0. The default image resolution is 160 here, though you can change it with `reso` flag.\n\n```\npython cam_demo.py\n```\nYou can easily tweak the code to use different weightsfiles, available at [yolo website](https://pjreddie.com/darknet/yolo/)\n\nNOTE: The scales features has been disabled for better refactoring.\n### Detection across different scales\nYOLO v3 makes detections across different scales, each of which deputise in detecting objects of different sizes depending upon whether they capture coarse features, fine grained features or something between. You can experiment with these scales by the `--scales` flag. \n\n```\npython detect.py --scales 1,3\n```\n\n\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "bbox.py",
          "type": "blob",
          "size": 3.2802734375,
          "content": "from __future__ import division\n\nimport torch \nimport random\n\nimport numpy as np\nimport cv2\n\ndef confidence_filter(result, confidence):\n    conf_mask = (result[:,:,4] > confidence).float().unsqueeze(2)\n    result = result*conf_mask    \n    \n    return result\n\ndef confidence_filter_cls(result, confidence):\n    max_scores = torch.max(result[:,:,5:25], 2)[0]\n    res = torch.cat((result, max_scores),2)\n    print(res.shape)\n    \n    \n    cond_1 = (res[:,:,4] > confidence).float()\n    cond_2 = (res[:,:,25] > 0.995).float()\n    \n    conf = cond_1 + cond_2\n    conf = torch.clamp(conf, 0.0, 1.0)\n    conf = conf.unsqueeze(2)\n    result = result*conf   \n    return result\n\n\n\ndef get_abs_coord(box):\n    box[2], box[3] = abs(box[2]), abs(box[3])\n    x1 = (box[0] - box[2]/2) - 1 \n    y1 = (box[1] - box[3]/2) - 1 \n    x2 = (box[0] + box[2]/2) - 1 \n    y2 = (box[1] + box[3]/2) - 1\n    return x1, y1, x2, y2\n    \n\n\ndef sanity_fix(box):\n    if (box[0] > box[2]):\n        box[0], box[2] = box[2], box[0]\n    \n    if (box[1] >  box[3]):\n        box[1], box[3] = box[3], box[1]\n        \n    return box\n\ndef bbox_iou(box1, box2):\n    \"\"\"\n    Returns the IoU of two bounding boxes \n    \n    \n    \"\"\"\n    #Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n    \n    #get the corrdinates of the intersection rectangle\n    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n    \n    #Intersection area\n    if torch.cuda.is_available():\n            inter_area = torch.max(inter_rect_x2 - inter_rect_x1 + 1,torch.zeros(inter_rect_x2.shape).cuda())*torch.max(inter_rect_y2 - inter_rect_y1 + 1, torch.zeros(inter_rect_x2.shape).cuda())\n    else:\n            inter_area = torch.max(inter_rect_x2 - inter_rect_x1 + 1,torch.zeros(inter_rect_x2.shape))*torch.max(inter_rect_y2 - inter_rect_y1 + 1, torch.zeros(inter_rect_x2.shape))\n    \n    #Union Area\n    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n    \n    iou = inter_area / (b1_area + b2_area - inter_area)\n    \n    return iou\n\n\ndef pred_corner_coord(prediction):\n    #Get indices of non-zero confidence bboxes\n    ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    \n    box = prediction[ind_nz[0], ind_nz[1]]\n    \n    \n    box_a = box.new(box.shape)\n    box_a[:,0] = (box[:,0] - box[:,2]/2)\n    box_a[:,1] = (box[:,1] - box[:,3]/2)\n    box_a[:,2] = (box[:,0] + box[:,2]/2) \n    box_a[:,3] = (box[:,1] + box[:,3]/2)\n    box[:,:4] = box_a[:,:4]\n    \n    prediction[ind_nz[0], ind_nz[1]] = box\n    \n    return prediction\n\n\n\n\ndef write(x, batches, results, colors, classes):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    img = results[int(x[0])]\n    cls = int(x[-1])\n    label = \"{0}\".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n"
        },
        {
          "name": "cam_demo.py",
          "type": "blob",
          "size": 4.5615234375,
          "content": "from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image\nimport pandas as pd\nimport random \nimport argparse\nimport pickle as pkl\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(\"imgs/messi.jpg\")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    \"\"\"\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    \"\"\"\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = cv2.resize(orig_im, (inp_dim, inp_dim))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = \"{0}\".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    \"\"\"\n    Parse arguements to the detect module\n    \n    \"\"\"\n    \n    \n    parser = argparse.ArgumentParser(description='YOLO v3 Cam Demo')\n    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.25)\n    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n    parser.add_argument(\"--reso\", dest = 'reso', help = \n                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n                        default = \"160\", type = str)\n    return parser.parse_args()\n\n\n\nif __name__ == '__main__':\n    cfgfile = \"cfg/yolov3.cfg\"\n    weightsfile = \"yolov3.weights\"\n    num_classes = 80\n\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n    CUDA = torch.cuda.is_available()\n    \n\n    \n    \n    num_classes = 80\n    bbox_attrs = 5 + num_classes\n    \n    model = Darknet(cfgfile)\n    model.load_weights(weightsfile)\n    \n    model.net_info[\"height\"] = args.reso\n    inp_dim = int(model.net_info[\"height\"])\n    \n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    if CUDA:\n        model.cuda()\n            \n    model.eval()\n    \n    videofile = 'video.avi'\n    \n    cap = cv2.VideoCapture(0)\n    \n    assert cap.isOpened(), 'Cannot capture source'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n#            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                im_dim = im_dim.cuda()\n                img = img.cuda()\n            \n            \n            output = model(Variable(img), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n            if type(output) == int:\n                frames += 1\n                print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n                cv2.imshow(\"frame\", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord('q'):\n                    break\n                continue\n            \n\n        \n            output[:,1:5] = torch.clamp(output[:,1:5], 0.0, float(inp_dim))/inp_dim\n            \n#            im_dim = im_dim.repeat(output.size(0), 1)\n            output[:,[1,3]] *= frame.shape[1]\n            output[:,[2,4]] *= frame.shape[0]\n\n            \n            classes = load_classes('data/coco.names')\n            colors = pkl.load(open(\"pallete\", \"rb\"))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(\"frame\", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord('q'):\n                break\n            frames += 1\n            print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n"
        },
        {
          "name": "cfg",
          "type": "tree",
          "content": null
        },
        {
          "name": "darknet.py",
          "type": "blob",
          "size": 16.8857421875,
          "content": "from __future__ import division\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\nfrom util import count_parameters as count\nfrom util import convert2cpu as cpu\nfrom util import predict_transform\n\nclass test_net(nn.Module):\n    def __init__(self, num_layers, input_size):\n        super(test_net, self).__init__()\n        self.num_layers= num_layers\n        self.linear_1 = nn.Linear(input_size, 5)\n        self.middle = nn.ModuleList([nn.Linear(5,5) for x in range(num_layers)])\n        self.output = nn.Linear(5,2)\n    \n    def forward(self, x):\n        x = x.view(-1)\n        fwd = nn.Sequential(self.linear_1, *self.middle, self.output)\n        return fwd(x)\n        \ndef get_test_input():\n    img = cv2.imread(\"dog-cycle-car.png\")\n    img = cv2.resize(img, (416,416)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    return img_\n\n\ndef parse_cfg(cfgfile):\n    \"\"\"\n    Takes a configuration file\n    \n    Returns a list of blocks. Each blocks describes a block in the neural\n    network to be built. Block is represented as a dictionary in the list\n    \n    \"\"\"\n    file = open(cfgfile, 'r')\n    lines = file.read().split('\\n')     #store the lines in a list\n    lines = [x for x in lines if len(x) > 0] #get read of the empty lines \n    lines = [x for x in lines if x[0] != '#']  \n    lines = [x.rstrip().lstrip() for x in lines]\n\n    \n    block = {}\n    blocks = []\n    \n    for line in lines:\n        if line[0] == \"[\":               #This marks the start of a new block\n            if len(block) != 0:\n                blocks.append(block)\n                block = {}\n            block[\"type\"] = line[1:-1].rstrip()\n        else:\n            key,value = line.split(\"=\")\n            block[key.rstrip()] = value.lstrip()\n    blocks.append(block)\n\n    return blocks\n#    print('\\n\\n'.join([repr(x) for x in blocks]))\n\nimport pickle as pkl\n\nclass MaxPoolStride1(nn.Module):\n    def __init__(self, kernel_size):\n        super(MaxPoolStride1, self).__init__()\n        self.kernel_size = kernel_size\n        self.pad = kernel_size - 1\n    \n    def forward(self, x):\n        padded_x = F.pad(x, (0,self.pad,0,self.pad), mode=\"replicate\")\n        pooled_x = nn.MaxPool2d(self.kernel_size, self.pad)(padded_x)\n        return pooled_x\n    \n\nclass EmptyLayer(nn.Module):\n    def __init__(self):\n        super(EmptyLayer, self).__init__()\n        \n\nclass DetectionLayer(nn.Module):\n    def __init__(self, anchors):\n        super(DetectionLayer, self).__init__()\n        self.anchors = anchors\n    \n    def forward(self, x, inp_dim, num_classes, confidence):\n        x = x.data\n        global CUDA\n        prediction = x\n        prediction = predict_transform(prediction, inp_dim, self.anchors, num_classes, confidence, CUDA)\n        return prediction\n        \n\n        \n\n\nclass Upsample(nn.Module):\n    def __init__(self, stride=2):\n        super(Upsample, self).__init__()\n        self.stride = stride\n        \n    def forward(self, x):\n        stride = self.stride\n        assert(x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, stride, W, stride).contiguous().view(B, C, H*stride, W*stride)\n        return x\n#       \n        \nclass ReOrgLayer(nn.Module):\n    def __init__(self, stride = 2):\n        super(ReOrgLayer, self).__init__()\n        self.stride= stride\n        \n    def forward(self,x):\n        assert(x.data.dim() == 4)\n        B,C,H,W = x.data.shape\n        hs = self.stride\n        ws = self.stride\n        assert(H % hs == 0),  \"The stride \" + str(self.stride) + \" is not a proper divisor of height \" + str(H)\n        assert(W % ws == 0),  \"The stride \" + str(self.stride) + \" is not a proper divisor of height \" + str(W)\n        x = x.view(B,C, H // hs, hs, W // ws, ws).transpose(-2,-3).contiguous()\n        x = x.view(B,C, H // hs * W // ws, hs, ws)\n        x = x.view(B,C, H // hs * W // ws, hs*ws).transpose(-1,-2).contiguous()\n        x = x.view(B, C, ws*hs, H // ws, W // ws).transpose(1,2).contiguous()\n        x = x.view(B, C*ws*hs, H // ws, W // ws)\n        return x\n\n\ndef create_modules(blocks):\n    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n    \n    module_list = nn.ModuleList()\n    \n    index = 0    #indexing blocks helps with implementing route  layers (skip connections)\n\n    \n    prev_filters = 3\n    \n    output_filters = []\n    \n    for x in blocks:\n        module = nn.Sequential()\n        \n        if (x[\"type\"] == \"net\"):\n            continue\n        \n        #If it's a convolutional layer\n        if (x[\"type\"] == \"convolutional\"):\n            #Get the info about the layer\n            activation = x[\"activation\"]\n            try:\n                batch_normalize = int(x[\"batch_normalize\"])\n                bias = False\n            except:\n                batch_normalize = 0\n                bias = True\n                \n            filters= int(x[\"filters\"])\n            padding = int(x[\"pad\"])\n            kernel_size = int(x[\"size\"])\n            stride = int(x[\"stride\"])\n            \n            if padding:\n                pad = (kernel_size - 1) // 2\n            else:\n                pad = 0\n                \n            #Add the convolutional layer\n            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n            module.add_module(\"conv_{0}\".format(index), conv)\n            \n            #Add the Batch Norm Layer\n            if batch_normalize:\n                bn = nn.BatchNorm2d(filters)\n                module.add_module(\"batch_norm_{0}\".format(index), bn)\n            \n            #Check the activation. \n            #It is either Linear or a Leaky ReLU for YOLO\n            if activation == \"leaky\":\n                activn = nn.LeakyReLU(0.1, inplace = True)\n                module.add_module(\"leaky_{0}\".format(index), activn)\n            \n            \n            \n        #If it's an upsampling layer\n        #We use Bilinear2dUpsampling\n        \n        elif (x[\"type\"] == \"upsample\"):\n            stride = int(x[\"stride\"])\n#            upsample = Upsample(stride)\n            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n            module.add_module(\"upsample_{}\".format(index), upsample)\n        \n        #If it is a route layer\n        elif (x[\"type\"] == \"route\"):\n            x[\"layers\"] = x[\"layers\"].split(',')\n            \n            #Start  of a route\n            start = int(x[\"layers\"][0])\n            \n            #end, if there exists one.\n            try:\n                end = int(x[\"layers\"][1])\n            except:\n                end = 0\n                \n            \n            \n            #Positive anotation\n            if start > 0: \n                start = start - index\n            \n            if end > 0:\n                end = end - index\n\n            \n            route = EmptyLayer()\n            module.add_module(\"route_{0}\".format(index), route)\n            \n            \n            \n            if end < 0:\n                filters = output_filters[index + start] + output_filters[index + end]\n            else:\n                filters= output_filters[index + start]\n                        \n            \n        \n        #shortcut corresponds to skip connection\n        elif x[\"type\"] == \"shortcut\":\n            from_ = int(x[\"from\"])\n            shortcut = EmptyLayer()\n            module.add_module(\"shortcut_{}\".format(index), shortcut)\n            \n            \n        elif x[\"type\"] == \"maxpool\":\n            stride = int(x[\"stride\"])\n            size = int(x[\"size\"])\n            if stride != 1:\n                maxpool = nn.MaxPool2d(size, stride)\n            else:\n                maxpool = MaxPoolStride1(size)\n            \n            module.add_module(\"maxpool_{}\".format(index), maxpool)\n        \n        #Yolo is the detection layer\n        elif x[\"type\"] == \"yolo\":\n            mask = x[\"mask\"].split(\",\")\n            mask = [int(x) for x in mask]\n            \n            \n            anchors = x[\"anchors\"].split(\",\")\n            anchors = [int(a) for a in anchors]\n            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n            anchors = [anchors[i] for i in mask]\n            \n            detection = DetectionLayer(anchors)\n            module.add_module(\"Detection_{}\".format(index), detection)\n        \n            \n            \n        else:\n            print(\"Something I dunno\")\n            assert False\n\n\n        module_list.append(module)\n        prev_filters = filters\n        output_filters.append(filters)\n        index += 1\n        \n    \n    return (net_info, module_list)\n\n\n\nclass Darknet(nn.Module):\n    def __init__(self, cfgfile):\n        super(Darknet, self).__init__()\n        self.blocks = parse_cfg(cfgfile)\n        self.net_info, self.module_list = create_modules(self.blocks)\n        self.header = torch.IntTensor([0,0,0,0])\n        self.seen = 0\n\n        \n        \n    def get_blocks(self):\n        return self.blocks\n    \n    def get_module_list(self):\n        return self.module_list\n\n                \n    def forward(self, x, CUDA):\n        detections = []\n        modules = self.blocks[1:]\n        outputs = {}   #We cache the outputs for the route layer\n        \n        \n        write = 0\n        for i in range(len(modules)):        \n            \n            module_type = (modules[i][\"type\"])\n            if module_type == \"convolutional\" or module_type == \"upsample\" or module_type == \"maxpool\":\n                \n                x = self.module_list[i](x)\n                outputs[i] = x\n\n                \n            elif module_type == \"route\":\n                layers = modules[i][\"layers\"]\n                layers = [int(a) for a in layers]\n                \n                if (layers[0]) > 0:\n                    layers[0] = layers[0] - i\n\n                if len(layers) == 1:\n                    x = outputs[i + (layers[0])]\n\n                else:\n                    if (layers[1]) > 0:\n                        layers[1] = layers[1] - i\n                        \n                    map1 = outputs[i + layers[0]]\n                    map2 = outputs[i + layers[1]]\n                    \n                    \n                    x = torch.cat((map1, map2), 1)\n                outputs[i] = x\n            \n            elif  module_type == \"shortcut\":\n                from_ = int(modules[i][\"from\"])\n                x = outputs[i-1] + outputs[i+from_]\n                outputs[i] = x\n                \n            \n            \n            elif module_type == 'yolo':        \n                \n                anchors = self.module_list[i][0].anchors\n                #Get the input dimensions\n                inp_dim = int (self.net_info[\"height\"])\n                \n                #Get the number of classes\n                num_classes = int (modules[i][\"classes\"])\n                \n                #Output the result\n                x = x.data\n                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n                \n                if type(x) == int:\n                    continue\n\n                \n                if not write:\n                    detections = x\n                    write = 1\n                \n                else:\n                    detections = torch.cat((detections, x), 1)\n                \n                outputs[i] = outputs[i-1]\n                \n        \n        \n        try:\n            return detections\n        except:\n            return 0\n\n            \n    def load_weights(self, weightfile):\n        \n        #Open the weights file\n        fp = open(weightfile, \"rb\")\n\n        #The first 4 values are header information \n        # 1. Major version number\n        # 2. Minor Version Number\n        # 3. Subversion number \n        # 4. IMages seen \n        header = np.fromfile(fp, dtype = np.int32, count = 5)\n        self.header = torch.from_numpy(header)\n        self.seen = self.header[3]\n        \n        #The rest of the values are the weights\n        # Let's load them up\n        weights = np.fromfile(fp, dtype = np.float32)\n        \n        ptr = 0\n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i + 1][\"type\"]\n            \n            if module_type == \"convolutional\":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n                except:\n                    batch_normalize = 0\n                \n                conv = model[0]\n                \n                if (batch_normalize):\n                    bn = model[1]\n                    \n                    #Get the number of weights of Batch Norm Layer\n                    num_bn_biases = bn.bias.numel()\n                    \n                    #Load the weights\n                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    \n                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    #Cast the loaded weights into dims of model weights. \n                    bn_biases = bn_biases.view_as(bn.bias.data)\n                    bn_weights = bn_weights.view_as(bn.weight.data)\n                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n                    bn_running_var = bn_running_var.view_as(bn.running_var)\n\n                    #Copy the data to model\n                    bn.bias.data.copy_(bn_biases)\n                    bn.weight.data.copy_(bn_weights)\n                    bn.running_mean.copy_(bn_running_mean)\n                    bn.running_var.copy_(bn_running_var)\n                \n                else:\n                    #Number of biases\n                    num_biases = conv.bias.numel()\n                \n                    #Load the weights\n                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n                    ptr = ptr + num_biases\n                    \n                    #reshape the loaded weights according to the dims of the model weights\n                    conv_biases = conv_biases.view_as(conv.bias.data)\n                    \n                    #Finally copy the data\n                    conv.bias.data.copy_(conv_biases)\n                    \n                    \n                #Let us load the weights for the Convolutional layers\n                num_weights = conv.weight.numel()\n                \n                #Do the same as above for weights\n                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n                ptr = ptr + num_weights\n\n                conv_weights = conv_weights.view_as(conv.weight.data)\n                conv.weight.data.copy_(conv_weights)\n                \n    def save_weights(self, savedfile, cutoff = 0):\n            \n        if cutoff <= 0:\n            cutoff = len(self.blocks) - 1\n        \n        fp = open(savedfile, 'wb')\n        \n        # Attach the header at the top of the file\n        self.header[3] = self.seen\n        header = self.header\n\n        header = header.numpy()\n        header.tofile(fp)\n        \n        # Now, let us save the weights \n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i+1][\"type\"]\n            \n            if (module_type) == \"convolutional\":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n                except:\n                    batch_normalize = 0\n                    \n                conv = model[0]\n\n                if (batch_normalize):\n                    bn = model[1]\n                \n                    #If the parameters are on GPU, convert them back to CPU\n                    #We don't convert the parameter to GPU\n                    #Instead. we copy the parameter and then convert it to CPU\n                    #This is done as weight are need to be saved during training\n                    cpu(bn.bias.data).numpy().tofile(fp)\n                    cpu(bn.weight.data).numpy().tofile(fp)\n                    cpu(bn.running_mean).numpy().tofile(fp)\n                    cpu(bn.running_var).numpy().tofile(fp)\n                \n            \n                else:\n                    cpu(conv.bias.data).numpy().tofile(fp)\n                \n                \n                #Let us save the weights for the Convolutional layers\n                cpu(conv.weight.data).numpy().tofile(fp)\n               \n\n\n\n\n#\n#dn = Darknet('cfg/yolov3.cfg')\n#dn.load_weights(\"yolov3.weights\")\n#inp = get_test_input()\n#a, interms = dn(inp)\n#dn.eval()\n#a_i, interms_i = dn(inp)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "det_messi.jpg",
          "type": "blob",
          "size": 232.6904296875,
          "content": null
        },
        {
          "name": "detect.py",
          "type": "blob",
          "size": 10.1083984375,
          "content": "from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nimport argparse\nimport os \nimport os.path as osp\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport itertools\n\nclass test_net(nn.Module):\n    def __init__(self, num_layers, input_size):\n        super(test_net, self).__init__()\n        self.num_layers= num_layers\n        self.linear_1 = nn.Linear(input_size, 5)\n        self.middle = nn.ModuleList([nn.Linear(5,5) for x in range(num_layers)])\n        self.output = nn.Linear(5,2)\n    \n    def forward(self, x):\n        x = x.view(-1)\n        fwd = nn.Sequential(self.linear_1, *self.middle, self.output)\n        return fwd(x)\n        \ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(\"dog-cycle-car.png\")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    num_classes\n    return img_\n\n\n\ndef arg_parse():\n    \"\"\"\n    Parse arguements to the detect module\n    \n    \"\"\"\n    \n    \n    parser = argparse.ArgumentParser(description='YOLO v3 Detection Module')\n   \n    parser.add_argument(\"--images\", dest = 'images', help = \n                        \"Image / Directory containing images to perform detection upon\",\n                        default = \"imgs\", type = str)\n    parser.add_argument(\"--det\", dest = 'det', help = \n                        \"Image / Directory to store detections to\",\n                        default = \"det\", type = str)\n    parser.add_argument(\"--bs\", dest = \"bs\", help = \"Batch size\", default = 1)\n    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n                        \"Config file\",\n                        default = \"cfg/yolov3.cfg\", type = str)\n    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n                        \"weightsfile\",\n                        default = \"yolov3.weights\", type = str)\n    parser.add_argument(\"--reso\", dest = 'reso', help = \n                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n                        default = \"416\", type = str)\n    parser.add_argument(\"--scales\", dest = \"scales\", help = \"Scales to use for detection\",\n                        default = \"1,2,3\", type = str)\n    \n    return parser.parse_args()\n\nif __name__ ==  '__main__':\n    args = arg_parse()\n    \n    scales = args.scales\n    \n    \n#        scales = [int(x) for x in scales.split(',')]\n#        \n#        \n#        \n#        args.reso = int(args.reso)\n#        \n#        num_boxes = [args.reso//32, args.reso//16, args.reso//8]    \n#        scale_indices = [3*(x**2) for x in num_boxes]\n#        scale_indices = list(itertools.accumulate(scale_indices, lambda x,y : x+y))\n#    \n#        \n#        li = []\n#        i = 0\n#        for scale in scale_indices:        \n#            li.extend(list(range(i, scale))) \n#            i = scale\n#        \n#        scale_indices = li\n\n    images = args.images\n    batch_size = int(args.bs)\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n\n    CUDA = torch.cuda.is_available()\n\n    num_classes = 80\n    classes = load_classes('data/coco.names') \n\n    #Set up the neural network\n    print(\"Loading network.....\")\n    model = Darknet(args.cfgfile)\n    model.load_weights(args.weightsfile)\n    print(\"Network successfully loaded\")\n    \n    model.net_info[\"height\"] = args.reso\n    inp_dim = int(model.net_info[\"height\"])\n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    #If there's a GPU availible, put the model on GPU\n    if CUDA:\n        model.cuda()\n    \n    \n    #Set the model in evaluation mode\n    model.eval()\n    \n    read_dir = time.time()\n    #Detection phase\n    try:\n        imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images) if os.path.splitext(img)[1] == '.png' or os.path.splitext(img)[1] =='.jpeg' or os.path.splitext(img)[1] =='.jpg']\n    except NotADirectoryError:\n        imlist = []\n        imlist.append(osp.join(osp.realpath('.'), images))\n    except FileNotFoundError:\n        print (\"No file or directory with the name {}\".format(images))\n        exit()\n        \n    if not os.path.exists(args.det):\n        os.makedirs(args.det)\n        \n    load_batch = time.time()\n    \n    batches = list(map(prep_image, imlist, [inp_dim for x in range(len(imlist))]))\n    im_batches = [x[0] for x in batches]\n    orig_ims = [x[1] for x in batches]\n    im_dim_list = [x[2] for x in batches]\n    im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n    \n    \n    \n    if CUDA:\n        im_dim_list = im_dim_list.cuda()\n    \n    leftover = 0\n    \n    if (len(im_dim_list) % batch_size):\n        leftover = 1\n        \n        \n    if batch_size != 1:\n        num_batches = len(imlist) // batch_size + leftover            \n        im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n                            len(im_batches))]))  for i in range(num_batches)]        \n\n\n    i = 0\n    \n\n    write = False\n    model(get_test_input(inp_dim, CUDA), CUDA)\n    \n    start_det_loop = time.time()\n    \n    objs = {}\n    \n    \n    \n    for batch in im_batches:\n        #load the image \n        start = time.time()\n        if CUDA:\n            batch = batch.cuda()\n        \n\n        #Apply offsets to the result predictions\n        #Tranform the predictions as described in the YOLO paper\n        #flatten the prediction vector \n        # B x (bbox cord x no. of anchors) x grid_w x grid_h --> B x bbox x (all the boxes) \n        # Put every proposed box as a row.\n        with torch.no_grad():\n            prediction = model(Variable(batch), CUDA)\n        \n#        prediction = prediction[:,scale_indices]\n\n        \n        #get the boxes with object confidence > threshold\n        #Convert the cordinates to absolute coordinates\n        #perform NMS on these boxes, and save the results \n        #I could have done NMS and saving seperately to have a better abstraction\n        #But both these operations require looping, hence \n        #clubbing these ops in one loop instead of two. \n        #loops are slower than vectorised operations. \n        \n        prediction = write_results(prediction, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n        \n        \n        if type(prediction) == int:\n            i += 1\n            continue\n\n        end = time.time()\n        \n                    \n#        print(end - start)\n\n            \n\n        prediction[:,0] += i*batch_size\n        \n    \n            \n          \n        if not write:\n            output = prediction\n            write = 1\n        else:\n            output = torch.cat((output,prediction))\n            \n        \n        \n\n        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n            im_id = i*batch_size + im_num\n            objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n            print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n            print(\"----------------------------------------------------------\")\n        i += 1\n\n        \n        if CUDA:\n            torch.cuda.synchronize()\n    \n    try:\n        output\n    except NameError:\n        print(\"No detections were made\")\n        exit()\n        \n    im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n    \n    scaling_factor = torch.min(inp_dim/im_dim_list,1)[0].view(-1,1)\n    \n    \n    output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n    output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n    \n    \n    \n    output[:,1:5] /= scaling_factor\n    \n    for i in range(output.shape[0]):\n        output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n        output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n        \n        \n    output_recast = time.time()\n    \n    \n    class_load = time.time()\n\n    colors = pkl.load(open(\"pallete\", \"rb\"))\n    \n    \n    draw = time.time()\n\n\n    def write(x, batches, results):\n        c1 = tuple(x[1:3].int())\n        c2 = tuple(x[3:5].int())\n        img = results[int(x[0])]\n        cls = int(x[-1])\n        label = \"{0}\".format(classes[cls])\n        color = random.choice(colors)\n        cv2.rectangle(img, c1, c2,color, 1)\n        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n        c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n        cv2.rectangle(img, c1, c2,color, -1)\n        cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1)\n        return img\n    \n            \n    list(map(lambda x: write(x, im_batches, orig_ims), output))\n      \n    det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(args.det,x.split(\"/\")[-1]))\n    \n    list(map(cv2.imwrite, det_names, orig_ims))\n    \n    end = time.time()\n    \n    print()\n    print(\"SUMMARY\")\n    print(\"----------------------------------------------------------\")\n    print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n    print()\n    print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n    print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n    print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n    print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n    print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n    print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n    print(\"----------------------------------------------------------\")\n\n    \n    torch.cuda.empty_cache()\n    \n    \n        \n        \n    \n    \n"
        },
        {
          "name": "dog-cycle-car.png",
          "type": "blob",
          "size": 339.3017578125,
          "content": null
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pallete",
          "type": "blob",
          "size": 0.88671875,
          "content": null
        },
        {
          "name": "preprocess.py",
          "type": "blob",
          "size": 1.8642578125,
          "content": "from __future__ import division\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\nfrom util import count_parameters as count\nfrom util import convert2cpu as cpu\nfrom PIL import Image, ImageDraw\n\n\ndef letterbox_image(img, inp_dim):\n    '''resize image with unchanged aspect ratio using padding'''\n    img_w, img_h = img.shape[1], img.shape[0]\n    w, h = inp_dim\n    new_w = int(img_w * min(w/img_w, h/img_h))\n    new_h = int(img_h * min(w/img_w, h/img_h))\n    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n    \n    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n\n    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n    \n    return canvas\n\n\n        \ndef prep_image(img, inp_dim):\n    \"\"\"\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    \"\"\"\n\n    orig_im = cv2.imread(img)\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef prep_image_pil(img, network_dim):\n    orig_im = Image.open(img)\n    img = orig_im.convert('RGB')\n    dim = img.size\n    img = img.resize(network_dim)\n    img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n    img = img.view(*network_dim, 3).transpose(0,1).transpose(0,2).contiguous()\n    img = img.view(1, 3,*network_dim)\n    img = img.float().div(255.0)\n    return (img, orig_im, dim)\n\ndef inp_to_image(inp):\n    inp = inp.cpu().squeeze()\n    inp = inp*255\n    try:\n        inp = inp.data.numpy()\n    except RuntimeError:\n        inp = inp.numpy()\n    inp = inp.transpose(1,2,0)\n\n    inp = inp[:,:,::-1]\n    return inp\n\n\n"
        },
        {
          "name": "util.py",
          "type": "blob",
          "size": 12.6572265625,
          "content": "\nfrom __future__ import division\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\nfrom bbox import bbox_iou\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef count_learnable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef convert2cpu(matrix):\n    if matrix.is_cuda:\n        return torch.FloatTensor(matrix.size()).copy_(matrix)\n    else:\n        return matrix\n\ndef predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True):\n    batch_size = prediction.size(0)\n    stride =  inp_dim // prediction.size(2)\n    grid_size = inp_dim // stride\n    bbox_attrs = 5 + num_classes\n    num_anchors = len(anchors)\n    \n    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n\n\n\n    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n    prediction = prediction.transpose(1,2).contiguous()\n    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n\n\n    #Sigmoid the  centre_X, centre_Y. and object confidencce\n    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n    \n\n    \n    #Add the center offsets\n    grid_len = np.arange(grid_size)\n    a,b = np.meshgrid(grid_len, grid_len)\n    \n    x_offset = torch.FloatTensor(a).view(-1,1)\n    y_offset = torch.FloatTensor(b).view(-1,1)\n    \n    if CUDA:\n        x_offset = x_offset.cuda()\n        y_offset = y_offset.cuda()\n    \n    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n    \n    prediction[:,:,:2] += x_y_offset\n      \n    #log space transform height and the width\n    anchors = torch.FloatTensor(anchors)\n    \n    if CUDA:\n        anchors = anchors.cuda()\n    \n    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n\n    #Softmax the class scores\n    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n\n    prediction[:,:,:4] *= stride\n   \n    \n    return prediction\n\ndef load_classes(namesfile):\n    fp = open(namesfile, \"r\")\n    names = fp.read().split(\"\\n\")[:-1]\n    return names\n\ndef get_im_dim(im):\n    im = cv2.imread(im)\n    w,h = im.shape[1], im.shape[0]\n    return w,h\n\ndef unique(tensor):\n    tensor_np = tensor.cpu().numpy()\n    unique_np = np.unique(tensor_np)\n    unique_tensor = torch.from_numpy(unique_np)\n    \n    tensor_res = tensor.new(unique_tensor.shape)\n    tensor_res.copy_(unique_tensor)\n    return tensor_res\n\ndef write_results(prediction, confidence, num_classes, nms = True, nms_conf = 0.4):\n    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n    prediction = prediction*conf_mask\n    \n\n    try:\n        ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    except:\n        return 0\n    \n    \n    box_a = prediction.new(prediction.shape)\n    box_a[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n    box_a[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n    box_a[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n    box_a[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n    prediction[:,:,:4] = box_a[:,:,:4]\n    \n\n    \n    batch_size = prediction.size(0)\n    \n    output = prediction.new(1, prediction.size(2) + 1)\n    write = False\n\n\n    for ind in range(batch_size):\n        #select the image from the batch\n        image_pred = prediction[ind]\n        \n\n        \n        #Get the class having maximum score, and the index of that class\n        #Get rid of num_classes softmax scores \n        #Add the class index and the class score of class having maximum score\n        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n        max_conf = max_conf.float().unsqueeze(1)\n        max_conf_score = max_conf_score.float().unsqueeze(1)\n        seq = (image_pred[:,:5], max_conf, max_conf_score)\n        image_pred = torch.cat(seq, 1)\n        \n\n        \n        #Get rid of the zero entries\n        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n\n        \n        image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n        \n        #Get the various classes detected in the image\n        try:\n            img_classes = unique(image_pred_[:,-1])\n        except:\n             continue\n        #WE will do NMS classwise\n        for cls in img_classes:\n            #get the detections with one particular class\n            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n            \n\n            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n\n\t\t\n        \n             #sort the detections such that the entry with the maximum objectness\n             #confidence is at the top\n            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n            image_pred_class = image_pred_class[conf_sort_index]\n            idx = image_pred_class.size(0)\n            \n            #if nms has to be done\n            if nms:\n                #For each detection\n                for i in range(idx):\n                    #Get the IOUs of all boxes that come after the one we are looking at \n                    #in the loop\n                    try:\n                        ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n                    except ValueError:\n                        break\n        \n                    except IndexError:\n                        break\n                    \n                    #Zero out all the detections that have IoU > treshhold\n                    iou_mask = (ious < nms_conf).float().unsqueeze(1)\n                    image_pred_class[i+1:] *= iou_mask       \n                    \n                    #Remove the non-zero entries\n                    non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n                    image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n                    \n                    \n\n            #Concatenate the batch_id of the image to the detection\n            #this helps us identify which image does the detection correspond to \n            #We use a linear straucture to hold ALL the detections from the batch\n            #the batch_dim is flattened\n            #batch is identified by extra batch column\n            \n            \n            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n            seq = batch_ind, image_pred_class\n            if not write:\n                output = torch.cat(seq,1)\n                write = True\n            else:\n                out = torch.cat(seq,1)\n                output = torch.cat((output,out))\n    \n    return output\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Mar 24 00:12:16 2018\n\n@author: ayooshmac\n\"\"\"\n\ndef predict_transform_half(prediction, inp_dim, anchors, num_classes, CUDA = True):\n    batch_size = prediction.size(0)\n    stride =  inp_dim // prediction.size(2)\n\n    bbox_attrs = 5 + num_classes\n    num_anchors = len(anchors)\n    grid_size = inp_dim // stride\n\n    \n    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n    prediction = prediction.transpose(1,2).contiguous()\n    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n    \n    \n    #Sigmoid the  centre_X, centre_Y. and object confidencce\n    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n\n    \n    #Add the center offsets\n    grid_len = np.arange(grid_size)\n    a,b = np.meshgrid(grid_len, grid_len)\n    \n    x_offset = torch.FloatTensor(a).view(-1,1)\n    y_offset = torch.FloatTensor(b).view(-1,1)\n    \n    if CUDA:\n        x_offset = x_offset.cuda().half()\n        y_offset = y_offset.cuda().half()\n    \n    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n    \n    prediction[:,:,:2] += x_y_offset\n      \n    #log space transform height and the width\n    anchors = torch.HalfTensor(anchors)\n    \n    if CUDA:\n        anchors = anchors.cuda()\n    \n    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n\n    #Softmax the class scores\n    prediction[:,:,5: 5 + num_classes] = nn.Softmax(-1)(Variable(prediction[:,:, 5 : 5 + num_classes])).data\n\n    prediction[:,:,:4] *= stride\n    \n    \n    return prediction\n\n\ndef write_results_half(prediction, confidence, num_classes, nms = True, nms_conf = 0.4):\n    conf_mask = (prediction[:,:,4] > confidence).half().unsqueeze(2)\n    prediction = prediction*conf_mask\n    \n    try:\n        ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    except:\n        return 0\n    \n    \n    \n    box_a = prediction.new(prediction.shape)\n    box_a[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n    box_a[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n    box_a[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n    box_a[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n    prediction[:,:,:4] = box_a[:,:,:4]\n    \n    \n    \n    batch_size = prediction.size(0)\n    \n    output = prediction.new(1, prediction.size(2) + 1)\n    write = False\n    \n    for ind in range(batch_size):\n        #select the image from the batch\n        image_pred = prediction[ind]\n\n        \n        #Get the class having maximum score, and the index of that class\n        #Get rid of num_classes softmax scores \n        #Add the class index and the class score of class having maximum score\n        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n        max_conf = max_conf.half().unsqueeze(1)\n        max_conf_score = max_conf_score.half().unsqueeze(1)\n        seq = (image_pred[:,:5], max_conf, max_conf_score)\n        image_pred = torch.cat(seq, 1)\n        \n        \n        #Get rid of the zero entries\n        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n        try:\n            image_pred_ = image_pred[non_zero_ind.squeeze(),:]\n        except:\n            continue\n        \n        #Get the various classes detected in the image\n        img_classes = unique(image_pred_[:,-1].long()).half()\n        \n        \n        \n                \n        #WE will do NMS classwise\n        for cls in img_classes:\n            #get the detections with one particular class\n            cls_mask = image_pred_*(image_pred_[:,-1] == cls).half().unsqueeze(1)\n            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n            \n\n            image_pred_class = image_pred_[class_mask_ind]\n\n        \n             #sort the detections such that the entry with the maximum objectness\n             #confidence is at the top\n            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n            image_pred_class = image_pred_class[conf_sort_index]\n            idx = image_pred_class.size(0)\n            \n            #if nms has to be done\n            if nms:\n                #For each detection\n                for i in range(idx):\n                    #Get the IOUs of all boxes that come after the one we are looking at \n                    #in the loop\n                    try:\n                        ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n                    except ValueError:\n                        break\n        \n                    except IndexError:\n                        break\n                    \n                    #Zero out all the detections that have IoU > treshhold\n                    iou_mask = (ious < nms_conf).half().unsqueeze(1)\n                    image_pred_class[i+1:] *= iou_mask       \n                    \n                    #Remove the non-zero entries\n                    non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n                    image_pred_class = image_pred_class[non_zero_ind]\n                    \n                    \n            \n            #Concatenate the batch_id of the image to the detection\n            #this helps us identify which image does the detection correspond to \n            #We use a linear straucture to hold ALL the detections from the batch\n            #the batch_dim is flattened\n            #batch is identified by extra batch column\n            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n            seq = batch_ind, image_pred_class\n            \n            if not write:\n                output = torch.cat(seq,1)\n                write = True\n            else:\n                out = torch.cat(seq,1)\n                output = torch.cat((output,out))\n    \n    return output\n"
        },
        {
          "name": "video_demo.py",
          "type": "blob",
          "size": 5.66015625,
          "content": "from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image, letterbox_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport argparse\n\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(\"dog-cycle-car.png\")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    \"\"\"\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    \"\"\"\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = \"{0}\".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    \"\"\"\n    Parse arguements to the detect module\n    \n    \"\"\"\n    \n    \n    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n   \n    parser.add_argument(\"--video\", dest = 'video', help = \n                        \"Video to run detection upon\",\n                        default = \"video.avi\", type = str)\n    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n                        \"Config file\",\n                        default = \"cfg/yolov3.cfg\", type = str)\n    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n                        \"weightsfile\",\n                        default = \"yolov3.weights\", type = str)\n    parser.add_argument(\"--reso\", dest = 'reso', help = \n                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n                        default = \"416\", type = str)\n    return parser.parse_args()\n\n\nif __name__ == '__main__':\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n\n    CUDA = torch.cuda.is_available()\n\n    num_classes = 80\n\n    CUDA = torch.cuda.is_available()\n    \n    bbox_attrs = 5 + num_classes\n    \n    print(\"Loading network.....\")\n    model = Darknet(args.cfgfile)\n    model.load_weights(args.weightsfile)\n    print(\"Network successfully loaded\")\n\n    model.net_info[\"height\"] = args.reso\n    inp_dim = int(model.net_info[\"height\"])\n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    if CUDA:\n        model.cuda()\n        \n    model(get_test_input(inp_dim, CUDA), CUDA)\n\n    model.eval()\n    \n    videofile = args.video\n    \n    cap = cv2.VideoCapture(videofile)\n    \n    assert cap.isOpened(), 'Cannot capture source'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n\n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                im_dim = im_dim.cuda()\n                img = img.cuda()\n            \n            with torch.no_grad():   \n                output = model(Variable(img), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n            if type(output) == int:\n                frames += 1\n                print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n                cv2.imshow(\"frame\", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord('q'):\n                    break\n                continue\n            \n            \n\n            \n            im_dim = im_dim.repeat(output.size(0), 1)\n            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n            \n            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n            \n            output[:,1:5] /= scaling_factor\n    \n            for i in range(output.shape[0]):\n                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n            \n            classes = load_classes('data/coco.names')\n            colors = pkl.load(open(\"pallete\", \"rb\"))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(\"frame\", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord('q'):\n                break\n            frames += 1\n            print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n"
        },
        {
          "name": "video_demo_half.py",
          "type": "blob",
          "size": 5.787109375,
          "content": "from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image, letterbox_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport argparse\n\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(\"dog-cycle-car.png\")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    \"\"\"\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    \"\"\"\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = \"{0}\".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    \"\"\"\n    Parse arguements to the detect module\n    \n    \"\"\"\n    \n    \n    parser = argparse.ArgumentParser(description='YOLO v2 Video Detection Module')\n   \n    parser.add_argument(\"--video\", dest = 'video', help = \n                        \"Video to run detection upon\",\n                        default = \"video.avi\", type = str)\n    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n                        \"Config file\",\n                        default = \"cfg/yolov3.cfg\", type = str)\n    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n                        \"weightsfile\",\n                        default = \"yolov3.weights\", type = str)\n    parser.add_argument(\"--reso\", dest = 'reso', help = \n                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n                        default = \"416\", type = str)\n    return parser.parse_args()\n\n\nif __name__ == '__main__':\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n\n    CUDA = torch.cuda.is_available()\n\n        \n\n    CUDA = torch.cuda.is_available()\n    num_classes = 80 \n    bbox_attrs = 5 + num_classes\n    \n    print(\"Loading network.....\")\n    model = Darknet(args.cfgfile)\n    model.load_weights(args.weightsfile)\n    print(\"Network successfully loaded\")\n    \n    model.net_info[\"height\"] = args.reso\n    inp_dim = int(model.net_info[\"height\"])\n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    \n    if CUDA:\n        model.cuda().half()\n        \n    model(get_test_input(inp_dim, CUDA), CUDA)\n\n    model.eval()\n    \n    videofile = 'video.avi'\n    \n    cap = cv2.VideoCapture(videofile)\n    \n    assert cap.isOpened(), 'Cannot capture source'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n\n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                img = img.cuda().half()\n                im_dim = im_dim.half().cuda()\n                write_results = write_results_half\n                predict_transform = predict_transform_half\n            \n            \n            output = model(Variable(img, volatile = True), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n           \n            if type(output) == int:\n                frames += 1\n                print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n                cv2.imshow(\"frame\", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord('q'):\n                    break\n                continue\n\n        \n            im_dim = im_dim.repeat(output.size(0), 1)\n            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n            \n            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n            \n            output[:,1:5] /= scaling_factor\n    \n            for i in range(output.shape[0]):\n                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n            \n            \n            classes = load_classes('data/coco.names')\n            colors = pkl.load(open(\"pallete\", \"rb\"))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(\"frame\", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord('q'):\n                break\n            frames += 1\n            print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n"
        }
      ]
    }
  ]
}