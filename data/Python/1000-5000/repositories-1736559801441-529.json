{
  "metadata": {
    "timestamp": 1736559801441,
    "page": 529,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "yanx27/Pointnet_Pointnet2_pytorch",
      "stars": 3850,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.064453125,
          "content": "# Auto detect text files and perform LF normalization\n* text=auto\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.275390625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0361328125,
          "content": "MIT License\n\nCopyright (c) 2019 benny\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.6162109375,
          "content": "# Pytorch Implementation of PointNet and PointNet++ \n\nThis repo is implementation for [PointNet](http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf) and [PointNet++](http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space.pdf) in pytorch.\n\n## Update\n**2021/03/27:** \n\n(1) Release pre-trained models for semantic segmentation, where PointNet++ can achieve **53.5\\%** mIoU.\n\n(2) Release pre-trained models for classification and part segmentation in `log/`.\n\n**2021/03/20:** Update codes for classification, including:\n\n(1) Add codes for training **ModelNet10** dataset. Using setting of ``--num_category 10``. \n\n(2) Add codes for running on CPU only. Using setting of ``--use_cpu``. \n\n(3) Add codes for offline data preprocessing to accelerate training. Using setting of ``--process_data``. \n\n(4) Add codes for training with uniform sampling. Using setting of ``--use_uniform_sample``. \n\n**2019/11/26:**\n\n(1) Fixed some errors in previous codes and added data augmentation tricks. Now classification by only 1024 points can achieve **92.8\\%**! \n\n(2) Added testing codes, including classification and segmentation, and semantic segmentation with visualization. \n\n(3) Organized all models into `./models` files for easy using.\n\n## Install\nThe latest codes are tested on Ubuntu 16.04, CUDA10.1, PyTorch 1.6 and Python 3.7:\n```shell\nconda install pytorch==1.6.0 cudatoolkit=10.1 -c pytorch\n```\n\n## Classification (ModelNet10/40)\n### Data Preparation\nDownload alignment **ModelNet** [here](https://shapenet.cs.stanford.edu/media/modelnet40_normal_resampled.zip) and save in `data/modelnet40_normal_resampled/`.\n\n### Run\nYou can run different modes with following codes. \n* If you want to use offline processing of data, you can use `--process_data` in the first run. You can download pre-processd data [here](https://drive.google.com/drive/folders/1_fBYbDO3XSdRt3DSbEBe41r5l9YpIGWF?usp=sharing) and save it in `data/modelnet40_normal_resampled/`.\n* If you want to train on ModelNet10, you can use `--num_category 10`.\n```shell\n# ModelNet40\n## Select different models in ./models \n\n## e.g., pointnet2_ssg without normal features\npython train_classification.py --model pointnet2_cls_ssg --log_dir pointnet2_cls_ssg\npython test_classification.py --log_dir pointnet2_cls_ssg\n\n## e.g., pointnet2_ssg with normal features\npython train_classification.py --model pointnet2_cls_ssg --use_normals --log_dir pointnet2_cls_ssg_normal\npython test_classification.py --use_normals --log_dir pointnet2_cls_ssg_normal\n\n## e.g., pointnet2_ssg with uniform sampling\npython train_classification.py --model pointnet2_cls_ssg --use_uniform_sample --log_dir pointnet2_cls_ssg_fps\npython test_classification.py --use_uniform_sample --log_dir pointnet2_cls_ssg_fps\n\n# ModelNet10\n## Similar setting like ModelNet40, just using --num_category 10\n\n## e.g., pointnet2_ssg without normal features\npython train_classification.py --model pointnet2_cls_ssg --log_dir pointnet2_cls_ssg --num_category 10\npython test_classification.py --log_dir pointnet2_cls_ssg --num_category 10\n```\n\n### Performance\n| Model | Accuracy |\n|--|--|\n| PointNet (Official) |  89.2|\n| PointNet2 (Official) | 91.9 |\n| PointNet (Pytorch without normal) |  90.6|\n| PointNet (Pytorch with normal) |  91.4|\n| PointNet2_SSG (Pytorch without normal) |  92.2|\n| PointNet2_SSG (Pytorch with normal) |  92.4|\n| PointNet2_MSG (Pytorch with normal) |  **92.8**|\n\n## Part Segmentation (ShapeNet)\n### Data Preparation\nDownload alignment **ShapeNet** [here](https://shapenet.cs.stanford.edu/media/shapenetcore_partanno_segmentation_benchmark_v0_normal.zip)  and save in `data/shapenetcore_partanno_segmentation_benchmark_v0_normal/`.\n### Run\n```\n## Check model in ./models \n## e.g., pointnet2_msg\npython train_partseg.py --model pointnet2_part_seg_msg --normal --log_dir pointnet2_part_seg_msg\npython test_partseg.py --normal --log_dir pointnet2_part_seg_msg\n```\n### Performance\n| Model | Inctance avg IoU| Class avg IoU \n|--|--|--|\n|PointNet (Official)\t|83.7|80.4\t\n|PointNet2 (Official)|85.1\t|81.9\t\n|PointNet (Pytorch)|\t84.3\t|81.1|\t\n|PointNet2_SSG (Pytorch)|\t84.9|\t81.8\t\n|PointNet2_MSG (Pytorch)|\t**85.4**|\t**82.5**\t\n\n\n## Semantic Segmentation (S3DIS)\n### Data Preparation\nDownload 3D indoor parsing dataset (**S3DIS**) [here](http://buildingparser.stanford.edu/dataset.html)  and save in `data/s3dis/Stanford3dDataset_v1.2_Aligned_Version/`.\n```\ncd data_utils\npython collect_indoor3d_data.py\n```\nProcessed data will save in `data/stanford_indoor3d/`.\n### Run\n```\n## Check model in ./models \n## e.g., pointnet2_ssg\npython train_semseg.py --model pointnet2_sem_seg --test_area 5 --log_dir pointnet2_sem_seg\npython test_semseg.py --log_dir pointnet2_sem_seg --test_area 5 --visual\n```\nVisualization results will save in `log/sem_seg/pointnet2_sem_seg/visual/` and you can visualize these .obj file by [MeshLab](http://www.meshlab.net/).\n\n### Performance\n|Model  | Overall Acc |Class avg IoU | Checkpoint \n|--|--|--|--|\n| PointNet (Pytorch) | 78.9 | 43.7| [40.7MB](log/sem_seg/pointnet_sem_seg) |\n| PointNet2_ssg (Pytorch) | **83.0** | **53.5**| [11.2MB](log/sem_seg/pointnet2_sem_seg) |\n\n## Visualization\n### Using show3d_balls.py\n```\n## build C++ code for visualization\ncd visualizer\nbash build.sh \n## run one example \npython show3d_balls.py\n```\n![](/visualizer/pic.png)\n### Using MeshLab\n![](/visualizer/pic2.png)\n\n\n## Reference By\n[halimacc/pointnet3](https://github.com/halimacc/pointnet3)<br>\n[fxia22/pointnet.pytorch](https://github.com/fxia22/pointnet.pytorch)<br>\n[charlesq34/PointNet](https://github.com/charlesq34/pointnet) <br>\n[charlesq34/PointNet++](https://github.com/charlesq34/pointnet2)\n\n\n## Citation\nIf you find this repo useful in your research, please consider citing it and our other works:\n```\n@article{Pytorch_Pointnet_Pointnet2,\n      Author = {Xu Yan},\n      Title = {Pointnet/Pointnet++ Pytorch},\n      Journal = {https://github.com/yanx27/Pointnet_Pointnet2_pytorch},\n      Year = {2019}\n}\n```\n```\n@InProceedings{yan2020pointasnl,\n  title={PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks with Adaptive Sampling},\n  author={Yan, Xu and Zheng, Chaoda and Li, Zhen and Wang, Sheng and Cui, Shuguang},\n  journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2020}\n}\n```\n```\n@InProceedings{yan2021sparse,\n  title={Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion},\n  author={Yan, Xu and Gao, Jiantao and Li, Jie and Zhang, Ruimao, and Li, Zhen and Huang, Rui and Cui, Shuguang},\n  journal={AAAI Conference on Artificial Intelligence ({AAAI})},\n  year={2021}\n}\n```\n```\n@InProceedings{yan20222dpass,\n      title={2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds}, \n      author={Xu Yan and Jiantao Gao and Chaoda Zheng and Chao Zheng and Ruimao Zhang and Shuguang Cui and Zhen Li},\n      year={2022},\n      journal={ECCV}\n}\n```\n## Selected Projects using This Codebase\n* [PointConv: Deep Convolutional Networks on 3D Point Clouds, CVPR'19](https://github.com/Young98CN/pointconv_pytorch)\n* [On Isometry Robustness of Deep 3D Point Cloud Models under Adversarial Attacks, CVPR'20](https://github.com/skywalker6174/3d-isometry-robust)\n* [Label-Efficient Learning on Point Clouds using Approximate Convex Decompositions, ECCV'20](https://github.com/matheusgadelha/PointCloudLearningACD)\n* [PCT: Point Cloud Transformer](https://github.com/MenghaoGuo/PCT)\n* [PSNet: Fast Data Structuring for Hierarchical Deep Learning on Point Cloud](https://github.com/lly007/PointStructuringNet)\n* [Stratified Transformer for 3D Point Cloud Segmentation, CVPR'22](https://github.com/dvlab-research/stratified-transformer)\n"
        },
        {
          "name": "data_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "log",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "provider.py",
          "type": "blob",
          "size": 9.724609375,
          "content": "import numpy as np\n\ndef normalize_data(batch_data):\n    \"\"\" Normalize the batch data, use coordinates of the block centered at origin,\n        Input:\n            BxNxC array\n        Output:\n            BxNxC array\n    \"\"\"\n    B, N, C = batch_data.shape\n    normal_data = np.zeros((B, N, C))\n    for b in range(B):\n        pc = batch_data[b]\n        centroid = np.mean(pc, axis=0)\n        pc = pc - centroid\n        m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n        pc = pc / m\n        normal_data[b] = pc\n    return normal_data\n\n\ndef shuffle_data(data, labels):\n    \"\"\" Shuffle data and labels.\n        Input:\n          data: B,N,... numpy array\n          label: B,... numpy array\n        Return:\n          shuffled data, label and shuffle indices\n    \"\"\"\n    idx = np.arange(len(labels))\n    np.random.shuffle(idx)\n    return data[idx, ...], labels[idx], idx\n\ndef shuffle_points(batch_data):\n    \"\"\" Shuffle orders of points in each point cloud -- changes FPS behavior.\n        Use the same shuffling idx for the entire batch.\n        Input:\n            BxNxC array\n        Output:\n            BxNxC array\n    \"\"\"\n    idx = np.arange(batch_data.shape[1])\n    np.random.shuffle(idx)\n    return batch_data[:,idx,:]\n\ndef rotate_point_cloud(batch_data):\n    \"\"\" Randomly rotate the point clouds to augument the dataset\n        rotation is per shape based along up direction\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    \"\"\"\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, 0, sinval],\n                                    [0, 1, 0],\n                                    [-sinval, 0, cosval]])\n        shape_pc = batch_data[k, ...]\n        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n    return rotated_data\n\ndef rotate_point_cloud_z(batch_data):\n    \"\"\" Randomly rotate the point clouds to augument the dataset\n        rotation is per shape based along up direction\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    \"\"\"\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, sinval, 0],\n                                    [-sinval, cosval, 0],\n                                    [0, 0, 1]])\n        shape_pc = batch_data[k, ...]\n        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n    return rotated_data\n\ndef rotate_point_cloud_with_normal(batch_xyz_normal):\n    ''' Randomly rotate XYZ, normal point cloud.\n        Input:\n            batch_xyz_normal: B,N,6, first three channels are XYZ, last 3 all normal\n        Output:\n            B,N,6, rotated XYZ, normal point cloud\n    '''\n    for k in range(batch_xyz_normal.shape[0]):\n        rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, 0, sinval],\n                                    [0, 1, 0],\n                                    [-sinval, 0, cosval]])\n        shape_pc = batch_xyz_normal[k,:,0:3]\n        shape_normal = batch_xyz_normal[k,:,3:6]\n        batch_xyz_normal[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n        batch_xyz_normal[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), rotation_matrix)\n    return batch_xyz_normal\n\ndef rotate_perturbation_point_cloud_with_normal(batch_data, angle_sigma=0.06, angle_clip=0.18):\n    \"\"\" Randomly perturb the point clouds by small rotations\n        Input:\n          BxNx6 array, original batch of point clouds and point normals\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    \"\"\"\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)\n        Rx = np.array([[1,0,0],\n                       [0,np.cos(angles[0]),-np.sin(angles[0])],\n                       [0,np.sin(angles[0]),np.cos(angles[0])]])\n        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],\n                       [0,1,0],\n                       [-np.sin(angles[1]),0,np.cos(angles[1])]])\n        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],\n                       [np.sin(angles[2]),np.cos(angles[2]),0],\n                       [0,0,1]])\n        R = np.dot(Rz, np.dot(Ry,Rx))\n        shape_pc = batch_data[k,:,0:3]\n        shape_normal = batch_data[k,:,3:6]\n        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), R)\n        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), R)\n    return rotated_data\n\n\ndef rotate_point_cloud_by_angle(batch_data, rotation_angle):\n    \"\"\" Rotate the point cloud along up direction with certain angle.\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    \"\"\"\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        #rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, 0, sinval],\n                                    [0, 1, 0],\n                                    [-sinval, 0, cosval]])\n        shape_pc = batch_data[k,:,0:3]\n        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n    return rotated_data\n\ndef rotate_point_cloud_by_angle_with_normal(batch_data, rotation_angle):\n    \"\"\" Rotate the point cloud along up direction with certain angle.\n        Input:\n          BxNx6 array, original batch of point clouds with normal\n          scalar, angle of rotation\n        Return:\n          BxNx6 array, rotated batch of point clouds iwth normal\n    \"\"\"\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        #rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, 0, sinval],\n                                    [0, 1, 0],\n                                    [-sinval, 0, cosval]])\n        shape_pc = batch_data[k,:,0:3]\n        shape_normal = batch_data[k,:,3:6]\n        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1,3)), rotation_matrix)\n    return rotated_data\n\n\n\ndef rotate_perturbation_point_cloud(batch_data, angle_sigma=0.06, angle_clip=0.18):\n    \"\"\" Randomly perturb the point clouds by small rotations\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    \"\"\"\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)\n        Rx = np.array([[1,0,0],\n                       [0,np.cos(angles[0]),-np.sin(angles[0])],\n                       [0,np.sin(angles[0]),np.cos(angles[0])]])\n        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],\n                       [0,1,0],\n                       [-np.sin(angles[1]),0,np.cos(angles[1])]])\n        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],\n                       [np.sin(angles[2]),np.cos(angles[2]),0],\n                       [0,0,1]])\n        R = np.dot(Rz, np.dot(Ry,Rx))\n        shape_pc = batch_data[k, ...]\n        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), R)\n    return rotated_data\n\n\ndef jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n    \"\"\" Randomly jitter points. jittering is per point.\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, jittered batch of point clouds\n    \"\"\"\n    B, N, C = batch_data.shape\n    assert(clip > 0)\n    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)\n    jittered_data += batch_data\n    return jittered_data\n\ndef shift_point_cloud(batch_data, shift_range=0.1):\n    \"\"\" Randomly shift point cloud. Shift is per point cloud.\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, shifted batch of point clouds\n    \"\"\"\n    B, N, C = batch_data.shape\n    shifts = np.random.uniform(-shift_range, shift_range, (B,3))\n    for batch_index in range(B):\n        batch_data[batch_index,:,:] += shifts[batch_index,:]\n    return batch_data\n\n\ndef random_scale_point_cloud(batch_data, scale_low=0.8, scale_high=1.25):\n    \"\"\" Randomly scale the point cloud. Scale is per point cloud.\n        Input:\n            BxNx3 array, original batch of point clouds\n        Return:\n            BxNx3 array, scaled batch of point clouds\n    \"\"\"\n    B, N, C = batch_data.shape\n    scales = np.random.uniform(scale_low, scale_high, B)\n    for batch_index in range(B):\n        batch_data[batch_index,:,:] *= scales[batch_index]\n    return batch_data\n\ndef random_point_dropout(batch_pc, max_dropout_ratio=0.875):\n    ''' batch_pc: BxNx3 '''\n    for b in range(batch_pc.shape[0]):\n        dropout_ratio =  np.random.random()*max_dropout_ratio # 0~0.875\n        drop_idx = np.where(np.random.random((batch_pc.shape[1]))<=dropout_ratio)[0]\n        if len(drop_idx)>0:\n            batch_pc[b,drop_idx,:] = batch_pc[b,0,:] # set to the first point\n    return batch_pc\n\n\n\n"
        },
        {
          "name": "test_classification.py",
          "type": "blob",
          "size": 4.2236328125,
          "content": "\"\"\"\nAuthor: Benny\nDate: Nov 2019\n\"\"\"\nfrom data_utils.ModelNetDataLoader import ModelNetDataLoader\nimport argparse\nimport numpy as np\nimport os\nimport torch\nimport logging\nfrom tqdm import tqdm\nimport sys\nimport importlib\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, 'models'))\n\n\ndef parse_args():\n    '''PARAMETERS'''\n    parser = argparse.ArgumentParser('Testing')\n    parser.add_argument('--use_cpu', action='store_true', default=False, help='use cpu mode')\n    parser.add_argument('--gpu', type=str, default='0', help='specify gpu device')\n    parser.add_argument('--batch_size', type=int, default=24, help='batch size in training')\n    parser.add_argument('--num_category', default=40, type=int, choices=[10, 40],  help='training on ModelNet10/40')\n    parser.add_argument('--num_point', type=int, default=1024, help='Point Number')\n    parser.add_argument('--log_dir', type=str, required=True, help='Experiment root')\n    parser.add_argument('--use_normals', action='store_true', default=False, help='use normals')\n    parser.add_argument('--use_uniform_sample', action='store_true', default=False, help='use uniform sampiling')\n    parser.add_argument('--num_votes', type=int, default=3, help='Aggregate classification scores with voting')\n    return parser.parse_args()\n\n\ndef test(model, loader, num_class=40, vote_num=1):\n    mean_correct = []\n    classifier = model.eval()\n    class_acc = np.zeros((num_class, 3))\n\n    for j, (points, target) in tqdm(enumerate(loader), total=len(loader)):\n        if not args.use_cpu:\n            points, target = points.cuda(), target.cuda()\n\n        points = points.transpose(2, 1)\n        vote_pool = torch.zeros(target.size()[0], num_class).cuda()\n\n        for _ in range(vote_num):\n            pred, _ = classifier(points)\n            vote_pool += pred\n        pred = vote_pool / vote_num\n        pred_choice = pred.data.max(1)[1]\n\n        for cat in np.unique(target.cpu()):\n            classacc = pred_choice[target == cat].eq(target[target == cat].long().data).cpu().sum()\n            class_acc[cat, 0] += classacc.item() / float(points[target == cat].size()[0])\n            class_acc[cat, 1] += 1\n        correct = pred_choice.eq(target.long().data).cpu().sum()\n        mean_correct.append(correct.item() / float(points.size()[0]))\n\n    class_acc[:, 2] = class_acc[:, 0] / class_acc[:, 1]\n    class_acc = np.mean(class_acc[:, 2])\n    instance_acc = np.mean(mean_correct)\n    return instance_acc, class_acc\n\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    '''HYPER PARAMETER'''\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n\n    '''CREATE DIR'''\n    experiment_dir = 'log/classification/' + args.log_dir\n\n    '''LOG'''\n    args = parse_args()\n    logger = logging.getLogger(\"Model\")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler = logging.FileHandler('%s/eval.txt' % experiment_dir)\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string('PARAMETER ...')\n    log_string(args)\n\n    '''DATA LOADING'''\n    log_string('Load dataset ...')\n    data_path = 'data/modelnet40_normal_resampled/'\n\n    test_dataset = ModelNetDataLoader(root=data_path, args=args, split='test', process_data=False)\n    testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=10)\n\n    '''MODEL LOADING'''\n    num_class = args.num_category\n    model_name = os.listdir(experiment_dir + '/logs')[0].split('.')[0]\n    model = importlib.import_module(model_name)\n\n    classifier = model.get_model(num_class, normal_channel=args.use_normals)\n    if not args.use_cpu:\n        classifier = classifier.cuda()\n\n    checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n    classifier.load_state_dict(checkpoint['model_state_dict'])\n\n    with torch.no_grad():\n        instance_acc, class_acc = test(classifier.eval(), testDataLoader, vote_num=args.num_votes, num_class=num_class)\n        log_string('Test Instance Accuracy: %f, Class Accuracy: %f' % (instance_acc, class_acc))\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    main(args)\n"
        },
        {
          "name": "test_partseg.py",
          "type": "blob",
          "size": 7.029296875,
          "content": "\"\"\"\nAuthor: Benny\nDate: Nov 2019\n\"\"\"\nimport argparse\nimport os\nfrom data_utils.ShapeNetDataLoader import PartNormalDataset\nimport torch\nimport logging\nimport sys\nimport importlib\nfrom tqdm import tqdm\nimport numpy as np\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, 'models'))\n\nseg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43],\n               'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46], 'Mug': [36, 37],\n               'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27], 'Table': [47, 48, 49],\n               'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40], 'Chair': [12, 13, 14, 15], 'Knife': [22, 23]}\n\nseg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}\nfor cat in seg_classes.keys():\n    for label in seg_classes[cat]:\n        seg_label_to_cat[label] = cat\n\n\ndef to_categorical(y, num_classes):\n    \"\"\" 1-hot encodes a tensor \"\"\"\n    new_y = torch.eye(num_classes)[y.cpu().data.numpy(),]\n    if (y.is_cuda):\n        return new_y.cuda()\n    return new_y\n\n\ndef parse_args():\n    '''PARAMETERS'''\n    parser = argparse.ArgumentParser('PointNet')\n    parser.add_argument('--batch_size', type=int, default=24, help='batch size in testing')\n    parser.add_argument('--gpu', type=str, default='0', help='specify gpu device')\n    parser.add_argument('--num_point', type=int, default=2048, help='point Number')\n    parser.add_argument('--log_dir', type=str, required=True, help='experiment root')\n    parser.add_argument('--normal', action='store_true', default=False, help='use normals')\n    parser.add_argument('--num_votes', type=int, default=3, help='aggregate segmentation scores with voting')\n    return parser.parse_args()\n\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    '''HYPER PARAMETER'''\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n    experiment_dir = 'log/part_seg/' + args.log_dir\n\n    '''LOG'''\n    args = parse_args()\n    logger = logging.getLogger(\"Model\")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler = logging.FileHandler('%s/eval.txt' % experiment_dir)\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string('PARAMETER ...')\n    log_string(args)\n\n    root = 'data/shapenetcore_partanno_segmentation_benchmark_v0_normal/'\n\n    TEST_DATASET = PartNormalDataset(root=root, npoints=args.num_point, split='test', normal_channel=args.normal)\n    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size, shuffle=False, num_workers=4)\n    log_string(\"The number of test data is: %d\" % len(TEST_DATASET))\n    num_classes = 16\n    num_part = 50\n\n    '''MODEL LOADING'''\n    model_name = os.listdir(experiment_dir + '/logs')[0].split('.')[0]\n    MODEL = importlib.import_module(model_name)\n    classifier = MODEL.get_model(num_part, normal_channel=args.normal).cuda()\n    checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n    classifier.load_state_dict(checkpoint['model_state_dict'])\n\n    with torch.no_grad():\n        test_metrics = {}\n        total_correct = 0\n        total_seen = 0\n        total_seen_class = [0 for _ in range(num_part)]\n        total_correct_class = [0 for _ in range(num_part)]\n        shape_ious = {cat: [] for cat in seg_classes.keys()}\n        seg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}\n\n        for cat in seg_classes.keys():\n            for label in seg_classes[cat]:\n                seg_label_to_cat[label] = cat\n\n        classifier = classifier.eval()\n        for batch_id, (points, label, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader),\n                                                      smoothing=0.9):\n            batchsize, num_point, _ = points.size()\n            cur_batch_size, NUM_POINT, _ = points.size()\n            points, label, target = points.float().cuda(), label.long().cuda(), target.long().cuda()\n            points = points.transpose(2, 1)\n            vote_pool = torch.zeros(target.size()[0], target.size()[1], num_part).cuda()\n\n            for _ in range(args.num_votes):\n                seg_pred, _ = classifier(points, to_categorical(label, num_classes))\n                vote_pool += seg_pred\n\n            seg_pred = vote_pool / args.num_votes\n            cur_pred_val = seg_pred.cpu().data.numpy()\n            cur_pred_val_logits = cur_pred_val\n            cur_pred_val = np.zeros((cur_batch_size, NUM_POINT)).astype(np.int32)\n            target = target.cpu().data.numpy()\n\n            for i in range(cur_batch_size):\n                cat = seg_label_to_cat[target[i, 0]]\n                logits = cur_pred_val_logits[i, :, :]\n                cur_pred_val[i, :] = np.argmax(logits[:, seg_classes[cat]], 1) + seg_classes[cat][0]\n\n            correct = np.sum(cur_pred_val == target)\n            total_correct += correct\n            total_seen += (cur_batch_size * NUM_POINT)\n\n            for l in range(num_part):\n                total_seen_class[l] += np.sum(target == l)\n                total_correct_class[l] += (np.sum((cur_pred_val == l) & (target == l)))\n\n            for i in range(cur_batch_size):\n                segp = cur_pred_val[i, :]\n                segl = target[i, :]\n                cat = seg_label_to_cat[segl[0]]\n                part_ious = [0.0 for _ in range(len(seg_classes[cat]))]\n                for l in seg_classes[cat]:\n                    if (np.sum(segl == l) == 0) and (\n                            np.sum(segp == l) == 0):  # part is not present, no prediction as well\n                        part_ious[l - seg_classes[cat][0]] = 1.0\n                    else:\n                        part_ious[l - seg_classes[cat][0]] = np.sum((segl == l) & (segp == l)) / float(\n                            np.sum((segl == l) | (segp == l)))\n                shape_ious[cat].append(np.mean(part_ious))\n\n        all_shape_ious = []\n        for cat in shape_ious.keys():\n            for iou in shape_ious[cat]:\n                all_shape_ious.append(iou)\n            shape_ious[cat] = np.mean(shape_ious[cat])\n        mean_shape_ious = np.mean(list(shape_ious.values()))\n        test_metrics['accuracy'] = total_correct / float(total_seen)\n        test_metrics['class_avg_accuracy'] = np.mean(\n            np.array(total_correct_class) / np.array(total_seen_class, dtype=np.float))\n        for cat in sorted(shape_ious.keys()):\n            log_string('eval mIoU of %s %f' % (cat + ' ' * (14 - len(cat)), shape_ious[cat]))\n        test_metrics['class_avg_iou'] = mean_shape_ious\n        test_metrics['inctance_avg_iou'] = np.mean(all_shape_ious)\n\n    log_string('Accuracy is: %.5f' % test_metrics['accuracy'])\n    log_string('Class avg accuracy is: %.5f' % test_metrics['class_avg_accuracy'])\n    log_string('Class avg mIOU is: %.5f' % test_metrics['class_avg_iou'])\n    log_string('Inctance avg mIOU is: %.5f' % test_metrics['inctance_avg_iou'])\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    main(args)\n"
        },
        {
          "name": "test_semseg.py",
          "type": "blob",
          "size": 9.185546875,
          "content": "\"\"\"\nAuthor: Benny\nDate: Nov 2019\n\"\"\"\nimport argparse\nimport os\nfrom data_utils.S3DISDataLoader import ScannetDatasetWholeScene\nfrom data_utils.indoor3d_util import g_label2color\nimport torch\nimport logging\nfrom pathlib import Path\nimport sys\nimport importlib\nfrom tqdm import tqdm\nimport provider\nimport numpy as np\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, 'models'))\n\nclasses = ['ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door', 'table', 'chair', 'sofa', 'bookcase',\n           'board', 'clutter']\nclass2label = {cls: i for i, cls in enumerate(classes)}\nseg_classes = class2label\nseg_label_to_cat = {}\nfor i, cat in enumerate(seg_classes.keys()):\n    seg_label_to_cat[i] = cat\n\n\ndef parse_args():\n    '''PARAMETERS'''\n    parser = argparse.ArgumentParser('Model')\n    parser.add_argument('--batch_size', type=int, default=32, help='batch size in testing [default: 32]')\n    parser.add_argument('--gpu', type=str, default='0', help='specify gpu device')\n    parser.add_argument('--num_point', type=int, default=4096, help='point number [default: 4096]')\n    parser.add_argument('--log_dir', type=str, required=True, help='experiment root')\n    parser.add_argument('--visual', action='store_true', default=False, help='visualize result [default: False]')\n    parser.add_argument('--test_area', type=int, default=5, help='area for testing, option: 1-6 [default: 5]')\n    parser.add_argument('--num_votes', type=int, default=3, help='aggregate segmentation scores with voting [default: 5]')\n    return parser.parse_args()\n\n\ndef add_vote(vote_label_pool, point_idx, pred_label, weight):\n    B = pred_label.shape[0]\n    N = pred_label.shape[1]\n    for b in range(B):\n        for n in range(N):\n            if weight[b, n] != 0 and not np.isinf(weight[b, n]):\n                vote_label_pool[int(point_idx[b, n]), int(pred_label[b, n])] += 1\n    return vote_label_pool\n\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    '''HYPER PARAMETER'''\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n    experiment_dir = 'log/sem_seg/' + args.log_dir\n    visual_dir = experiment_dir + '/visual/'\n    visual_dir = Path(visual_dir)\n    visual_dir.mkdir(exist_ok=True)\n\n    '''LOG'''\n    args = parse_args()\n    logger = logging.getLogger(\"Model\")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler = logging.FileHandler('%s/eval.txt' % experiment_dir)\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string('PARAMETER ...')\n    log_string(args)\n\n    NUM_CLASSES = 13\n    BATCH_SIZE = args.batch_size\n    NUM_POINT = args.num_point\n\n    root = 'data/s3dis/stanford_indoor3d/'\n\n    TEST_DATASET_WHOLE_SCENE = ScannetDatasetWholeScene(root, split='test', test_area=args.test_area, block_points=NUM_POINT)\n    log_string(\"The number of test data is: %d\" % len(TEST_DATASET_WHOLE_SCENE))\n\n    '''MODEL LOADING'''\n    model_name = os.listdir(experiment_dir + '/logs')[0].split('.')[0]\n    MODEL = importlib.import_module(model_name)\n    classifier = MODEL.get_model(NUM_CLASSES).cuda()\n    checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n    classifier.load_state_dict(checkpoint['model_state_dict'])\n    classifier = classifier.eval()\n\n    with torch.no_grad():\n        scene_id = TEST_DATASET_WHOLE_SCENE.file_list\n        scene_id = [x[:-4] for x in scene_id]\n        num_batches = len(TEST_DATASET_WHOLE_SCENE)\n\n        total_seen_class = [0 for _ in range(NUM_CLASSES)]\n        total_correct_class = [0 for _ in range(NUM_CLASSES)]\n        total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n\n        log_string('---- EVALUATION WHOLE SCENE----')\n\n        for batch_idx in range(num_batches):\n            print(\"Inference [%d/%d] %s ...\" % (batch_idx + 1, num_batches, scene_id[batch_idx]))\n            total_seen_class_tmp = [0 for _ in range(NUM_CLASSES)]\n            total_correct_class_tmp = [0 for _ in range(NUM_CLASSES)]\n            total_iou_deno_class_tmp = [0 for _ in range(NUM_CLASSES)]\n            if args.visual:\n                fout = open(os.path.join(visual_dir, scene_id[batch_idx] + '_pred.obj'), 'w')\n                fout_gt = open(os.path.join(visual_dir, scene_id[batch_idx] + '_gt.obj'), 'w')\n\n            whole_scene_data = TEST_DATASET_WHOLE_SCENE.scene_points_list[batch_idx]\n            whole_scene_label = TEST_DATASET_WHOLE_SCENE.semantic_labels_list[batch_idx]\n            vote_label_pool = np.zeros((whole_scene_label.shape[0], NUM_CLASSES))\n            for _ in tqdm(range(args.num_votes), total=args.num_votes):\n                scene_data, scene_label, scene_smpw, scene_point_index = TEST_DATASET_WHOLE_SCENE[batch_idx]\n                num_blocks = scene_data.shape[0]\n                s_batch_num = (num_blocks + BATCH_SIZE - 1) // BATCH_SIZE\n                batch_data = np.zeros((BATCH_SIZE, NUM_POINT, 9))\n\n                batch_label = np.zeros((BATCH_SIZE, NUM_POINT))\n                batch_point_index = np.zeros((BATCH_SIZE, NUM_POINT))\n                batch_smpw = np.zeros((BATCH_SIZE, NUM_POINT))\n\n                for sbatch in range(s_batch_num):\n                    start_idx = sbatch * BATCH_SIZE\n                    end_idx = min((sbatch + 1) * BATCH_SIZE, num_blocks)\n                    real_batch_size = end_idx - start_idx\n                    batch_data[0:real_batch_size, ...] = scene_data[start_idx:end_idx, ...]\n                    batch_label[0:real_batch_size, ...] = scene_label[start_idx:end_idx, ...]\n                    batch_point_index[0:real_batch_size, ...] = scene_point_index[start_idx:end_idx, ...]\n                    batch_smpw[0:real_batch_size, ...] = scene_smpw[start_idx:end_idx, ...]\n                    batch_data[:, :, 3:6] /= 1.0\n\n                    torch_data = torch.Tensor(batch_data)\n                    torch_data = torch_data.float().cuda()\n                    torch_data = torch_data.transpose(2, 1)\n                    seg_pred, _ = classifier(torch_data)\n                    batch_pred_label = seg_pred.contiguous().cpu().data.max(2)[1].numpy()\n\n                    vote_label_pool = add_vote(vote_label_pool, batch_point_index[0:real_batch_size, ...],\n                                               batch_pred_label[0:real_batch_size, ...],\n                                               batch_smpw[0:real_batch_size, ...])\n\n            pred_label = np.argmax(vote_label_pool, 1)\n\n            for l in range(NUM_CLASSES):\n                total_seen_class_tmp[l] += np.sum((whole_scene_label == l))\n                total_correct_class_tmp[l] += np.sum((pred_label == l) & (whole_scene_label == l))\n                total_iou_deno_class_tmp[l] += np.sum(((pred_label == l) | (whole_scene_label == l)))\n                total_seen_class[l] += total_seen_class_tmp[l]\n                total_correct_class[l] += total_correct_class_tmp[l]\n                total_iou_deno_class[l] += total_iou_deno_class_tmp[l]\n\n            iou_map = np.array(total_correct_class_tmp) / (np.array(total_iou_deno_class_tmp, dtype=np.float) + 1e-6)\n            print(iou_map)\n            arr = np.array(total_seen_class_tmp)\n            tmp_iou = np.mean(iou_map[arr != 0])\n            log_string('Mean IoU of %s: %.4f' % (scene_id[batch_idx], tmp_iou))\n            print('----------------------------')\n\n            filename = os.path.join(visual_dir, scene_id[batch_idx] + '.txt')\n            with open(filename, 'w') as pl_save:\n                for i in pred_label:\n                    pl_save.write(str(int(i)) + '\\n')\n                pl_save.close()\n            for i in range(whole_scene_label.shape[0]):\n                color = g_label2color[pred_label[i]]\n                color_gt = g_label2color[whole_scene_label[i]]\n                if args.visual:\n                    fout.write('v %f %f %f %d %d %d\\n' % (\n                        whole_scene_data[i, 0], whole_scene_data[i, 1], whole_scene_data[i, 2], color[0], color[1],\n                        color[2]))\n                    fout_gt.write(\n                        'v %f %f %f %d %d %d\\n' % (\n                            whole_scene_data[i, 0], whole_scene_data[i, 1], whole_scene_data[i, 2], color_gt[0],\n                            color_gt[1], color_gt[2]))\n            if args.visual:\n                fout.close()\n                fout_gt.close()\n\n        IoU = np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=np.float) + 1e-6)\n        iou_per_class_str = '------- IoU --------\\n'\n        for l in range(NUM_CLASSES):\n            iou_per_class_str += 'class %s, IoU: %.3f \\n' % (\n                seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])),\n                total_correct_class[l] / float(total_iou_deno_class[l]))\n        log_string(iou_per_class_str)\n        log_string('eval point avg class IoU: %f' % np.mean(IoU))\n        log_string('eval whole scene point avg class acc: %f' % (\n            np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=np.float) + 1e-6))))\n        log_string('eval whole scene point accuracy: %f' % (\n                np.sum(total_correct_class) / float(np.sum(total_seen_class) + 1e-6)))\n\n        print(\"Done!\")\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    main(args)\n"
        },
        {
          "name": "train_classification.py",
          "type": "blob",
          "size": 8.669921875,
          "content": "\"\"\"\nAuthor: Benny\nDate: Nov 2019\n\"\"\"\n\nimport os\nimport sys\nimport torch\nimport numpy as np\n\nimport datetime\nimport logging\nimport provider\nimport importlib\nimport shutil\nimport argparse\n\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom data_utils.ModelNetDataLoader import ModelNetDataLoader\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, 'models'))\n\ndef parse_args():\n    '''PARAMETERS'''\n    parser = argparse.ArgumentParser('training')\n    parser.add_argument('--use_cpu', action='store_true', default=False, help='use cpu mode')\n    parser.add_argument('--gpu', type=str, default='0', help='specify gpu device')\n    parser.add_argument('--batch_size', type=int, default=24, help='batch size in training')\n    parser.add_argument('--model', default='pointnet_cls', help='model name [default: pointnet_cls]')\n    parser.add_argument('--num_category', default=40, type=int, choices=[10, 40],  help='training on ModelNet10/40')\n    parser.add_argument('--epoch', default=200, type=int, help='number of epoch in training')\n    parser.add_argument('--learning_rate', default=0.001, type=float, help='learning rate in training')\n    parser.add_argument('--num_point', type=int, default=1024, help='Point Number')\n    parser.add_argument('--optimizer', type=str, default='Adam', help='optimizer for training')\n    parser.add_argument('--log_dir', type=str, default=None, help='experiment root')\n    parser.add_argument('--decay_rate', type=float, default=1e-4, help='decay rate')\n    parser.add_argument('--use_normals', action='store_true', default=False, help='use normals')\n    parser.add_argument('--process_data', action='store_true', default=False, help='save data offline')\n    parser.add_argument('--use_uniform_sample', action='store_true', default=False, help='use uniform sampiling')\n    return parser.parse_args()\n\n\ndef inplace_relu(m):\n    classname = m.__class__.__name__\n    if classname.find('ReLU') != -1:\n        m.inplace=True\n\n\ndef test(model, loader, num_class=40):\n    mean_correct = []\n    class_acc = np.zeros((num_class, 3))\n    classifier = model.eval()\n\n    for j, (points, target) in tqdm(enumerate(loader), total=len(loader)):\n\n        if not args.use_cpu:\n            points, target = points.cuda(), target.cuda()\n\n        points = points.transpose(2, 1)\n        pred, _ = classifier(points)\n        pred_choice = pred.data.max(1)[1]\n\n        for cat in np.unique(target.cpu()):\n            classacc = pred_choice[target == cat].eq(target[target == cat].long().data).cpu().sum()\n            class_acc[cat, 0] += classacc.item() / float(points[target == cat].size()[0])\n            class_acc[cat, 1] += 1\n\n        correct = pred_choice.eq(target.long().data).cpu().sum()\n        mean_correct.append(correct.item() / float(points.size()[0]))\n\n    class_acc[:, 2] = class_acc[:, 0] / class_acc[:, 1]\n    class_acc = np.mean(class_acc[:, 2])\n    instance_acc = np.mean(mean_correct)\n\n    return instance_acc, class_acc\n\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    '''HYPER PARAMETER'''\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n\n    '''CREATE DIR'''\n    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n    exp_dir = Path('./log/')\n    exp_dir.mkdir(exist_ok=True)\n    exp_dir = exp_dir.joinpath('classification')\n    exp_dir.mkdir(exist_ok=True)\n    if args.log_dir is None:\n        exp_dir = exp_dir.joinpath(timestr)\n    else:\n        exp_dir = exp_dir.joinpath(args.log_dir)\n    exp_dir.mkdir(exist_ok=True)\n    checkpoints_dir = exp_dir.joinpath('checkpoints/')\n    checkpoints_dir.mkdir(exist_ok=True)\n    log_dir = exp_dir.joinpath('logs/')\n    log_dir.mkdir(exist_ok=True)\n\n    '''LOG'''\n    args = parse_args()\n    logger = logging.getLogger(\"Model\")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string('PARAMETER ...')\n    log_string(args)\n\n    '''DATA LOADING'''\n    log_string('Load dataset ...')\n    data_path = 'data/modelnet40_normal_resampled/'\n\n    train_dataset = ModelNetDataLoader(root=data_path, args=args, split='train', process_data=args.process_data)\n    test_dataset = ModelNetDataLoader(root=data_path, args=args, split='test', process_data=args.process_data)\n    trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)\n    testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=10)\n\n    '''MODEL LOADING'''\n    num_class = args.num_category\n    model = importlib.import_module(args.model)\n    shutil.copy('./models/%s.py' % args.model, str(exp_dir))\n    shutil.copy('models/pointnet2_utils.py', str(exp_dir))\n    shutil.copy('./train_classification.py', str(exp_dir))\n\n    classifier = model.get_model(num_class, normal_channel=args.use_normals)\n    criterion = model.get_loss()\n    classifier.apply(inplace_relu)\n\n    if not args.use_cpu:\n        classifier = classifier.cuda()\n        criterion = criterion.cuda()\n\n    try:\n        checkpoint = torch.load(str(exp_dir) + '/checkpoints/best_model.pth')\n        start_epoch = checkpoint['epoch']\n        classifier.load_state_dict(checkpoint['model_state_dict'])\n        log_string('Use pretrain model')\n    except:\n        log_string('No existing model, starting training from scratch...')\n        start_epoch = 0\n\n    if args.optimizer == 'Adam':\n        optimizer = torch.optim.Adam(\n            classifier.parameters(),\n            lr=args.learning_rate,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=args.decay_rate\n        )\n    else:\n        optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n    global_epoch = 0\n    global_step = 0\n    best_instance_acc = 0.0\n    best_class_acc = 0.0\n\n    '''TRANING'''\n    logger.info('Start training...')\n    for epoch in range(start_epoch, args.epoch):\n        log_string('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n        mean_correct = []\n        classifier = classifier.train()\n\n        scheduler.step()\n        for batch_id, (points, target) in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader), smoothing=0.9):\n            optimizer.zero_grad()\n\n            points = points.data.numpy()\n            points = provider.random_point_dropout(points)\n            points[:, :, 0:3] = provider.random_scale_point_cloud(points[:, :, 0:3])\n            points[:, :, 0:3] = provider.shift_point_cloud(points[:, :, 0:3])\n            points = torch.Tensor(points)\n            points = points.transpose(2, 1)\n\n            if not args.use_cpu:\n                points, target = points.cuda(), target.cuda()\n\n            pred, trans_feat = classifier(points)\n            loss = criterion(pred, target.long(), trans_feat)\n            pred_choice = pred.data.max(1)[1]\n\n            correct = pred_choice.eq(target.long().data).cpu().sum()\n            mean_correct.append(correct.item() / float(points.size()[0]))\n            loss.backward()\n            optimizer.step()\n            global_step += 1\n\n        train_instance_acc = np.mean(mean_correct)\n        log_string('Train Instance Accuracy: %f' % train_instance_acc)\n\n        with torch.no_grad():\n            instance_acc, class_acc = test(classifier.eval(), testDataLoader, num_class=num_class)\n\n            if (instance_acc >= best_instance_acc):\n                best_instance_acc = instance_acc\n                best_epoch = epoch + 1\n\n            if (class_acc >= best_class_acc):\n                best_class_acc = class_acc\n            log_string('Test Instance Accuracy: %f, Class Accuracy: %f' % (instance_acc, class_acc))\n            log_string('Best Instance Accuracy: %f, Class Accuracy: %f' % (best_instance_acc, best_class_acc))\n\n            if (instance_acc >= best_instance_acc):\n                logger.info('Save model...')\n                savepath = str(checkpoints_dir) + '/best_model.pth'\n                log_string('Saving at %s' % savepath)\n                state = {\n                    'epoch': best_epoch,\n                    'instance_acc': instance_acc,\n                    'class_acc': class_acc,\n                    'model_state_dict': classifier.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                }\n                torch.save(state, savepath)\n            global_epoch += 1\n\n    logger.info('End of training...')\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    main(args)\n"
        },
        {
          "name": "train_partseg.py",
          "type": "blob",
          "size": 13.033203125,
          "content": "\"\"\"\nAuthor: Benny\nDate: Nov 2019\n\"\"\"\nimport argparse\nimport os\nimport torch\nimport datetime\nimport logging\nimport sys\nimport importlib\nimport shutil\nimport provider\nimport numpy as np\n\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom data_utils.ShapeNetDataLoader import PartNormalDataset\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, 'models'))\n\nseg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43],\n               'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46], 'Mug': [36, 37],\n               'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27], 'Table': [47, 48, 49],\n               'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40], 'Chair': [12, 13, 14, 15], 'Knife': [22, 23]}\nseg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}\nfor cat in seg_classes.keys():\n    for label in seg_classes[cat]:\n        seg_label_to_cat[label] = cat\n\n\ndef inplace_relu(m):\n    classname = m.__class__.__name__\n    if classname.find('ReLU') != -1:\n        m.inplace=True\n\ndef to_categorical(y, num_classes):\n    \"\"\" 1-hot encodes a tensor \"\"\"\n    new_y = torch.eye(num_classes)[y.cpu().data.numpy(),]\n    if (y.is_cuda):\n        return new_y.cuda()\n    return new_y\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Model')\n    parser.add_argument('--model', type=str, default='pointnet_part_seg', help='model name')\n    parser.add_argument('--batch_size', type=int, default=16, help='batch Size during training')\n    parser.add_argument('--epoch', default=251, type=int, help='epoch to run')\n    parser.add_argument('--learning_rate', default=0.001, type=float, help='initial learning rate')\n    parser.add_argument('--gpu', type=str, default='0', help='specify GPU devices')\n    parser.add_argument('--optimizer', type=str, default='Adam', help='Adam or SGD')\n    parser.add_argument('--log_dir', type=str, default=None, help='log path')\n    parser.add_argument('--decay_rate', type=float, default=1e-4, help='weight decay')\n    parser.add_argument('--npoint', type=int, default=2048, help='point Number')\n    parser.add_argument('--normal', action='store_true', default=False, help='use normals')\n    parser.add_argument('--step_size', type=int, default=20, help='decay step for lr decay')\n    parser.add_argument('--lr_decay', type=float, default=0.5, help='decay rate for lr decay')\n\n    return parser.parse_args()\n\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    '''HYPER PARAMETER'''\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n\n    '''CREATE DIR'''\n    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n    exp_dir = Path('./log/')\n    exp_dir.mkdir(exist_ok=True)\n    exp_dir = exp_dir.joinpath('part_seg')\n    exp_dir.mkdir(exist_ok=True)\n    if args.log_dir is None:\n        exp_dir = exp_dir.joinpath(timestr)\n    else:\n        exp_dir = exp_dir.joinpath(args.log_dir)\n    exp_dir.mkdir(exist_ok=True)\n    checkpoints_dir = exp_dir.joinpath('checkpoints/')\n    checkpoints_dir.mkdir(exist_ok=True)\n    log_dir = exp_dir.joinpath('logs/')\n    log_dir.mkdir(exist_ok=True)\n\n    '''LOG'''\n    args = parse_args()\n    logger = logging.getLogger(\"Model\")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string('PARAMETER ...')\n    log_string(args)\n\n    root = 'data/shapenetcore_partanno_segmentation_benchmark_v0_normal/'\n\n    TRAIN_DATASET = PartNormalDataset(root=root, npoints=args.npoint, split='trainval', normal_channel=args.normal)\n    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)\n    TEST_DATASET = PartNormalDataset(root=root, npoints=args.npoint, split='test', normal_channel=args.normal)\n    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size, shuffle=False, num_workers=10)\n    log_string(\"The number of training data is: %d\" % len(TRAIN_DATASET))\n    log_string(\"The number of test data is: %d\" % len(TEST_DATASET))\n\n    num_classes = 16\n    num_part = 50\n\n    '''MODEL LOADING'''\n    MODEL = importlib.import_module(args.model)\n    shutil.copy('models/%s.py' % args.model, str(exp_dir))\n    shutil.copy('models/pointnet2_utils.py', str(exp_dir))\n\n    classifier = MODEL.get_model(num_part, normal_channel=args.normal).cuda()\n    criterion = MODEL.get_loss().cuda()\n    classifier.apply(inplace_relu)\n\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find('Conv2d') != -1:\n            torch.nn.init.xavier_normal_(m.weight.data)\n            torch.nn.init.constant_(m.bias.data, 0.0)\n        elif classname.find('Linear') != -1:\n            torch.nn.init.xavier_normal_(m.weight.data)\n            torch.nn.init.constant_(m.bias.data, 0.0)\n\n    try:\n        checkpoint = torch.load(str(exp_dir) + '/checkpoints/best_model.pth')\n        start_epoch = checkpoint['epoch']\n        classifier.load_state_dict(checkpoint['model_state_dict'])\n        log_string('Use pretrain model')\n    except:\n        log_string('No existing model, starting training from scratch...')\n        start_epoch = 0\n        classifier = classifier.apply(weights_init)\n\n    if args.optimizer == 'Adam':\n        optimizer = torch.optim.Adam(\n            classifier.parameters(),\n            lr=args.learning_rate,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=args.decay_rate\n        )\n    else:\n        optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n\n    def bn_momentum_adjust(m, momentum):\n        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n            m.momentum = momentum\n\n    LEARNING_RATE_CLIP = 1e-5\n    MOMENTUM_ORIGINAL = 0.1\n    MOMENTUM_DECCAY = 0.5\n    MOMENTUM_DECCAY_STEP = args.step_size\n\n    best_acc = 0\n    global_epoch = 0\n    best_class_avg_iou = 0\n    best_inctance_avg_iou = 0\n\n    for epoch in range(start_epoch, args.epoch):\n        mean_correct = []\n\n        log_string('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n        '''Adjust learning rate and BN momentum'''\n        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n        log_string('Learning rate:%f' % lr)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n        if momentum < 0.01:\n            momentum = 0.01\n        print('BN momentum updated to: %f' % momentum)\n        classifier = classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n        classifier = classifier.train()\n\n        '''learning one epoch'''\n        for i, (points, label, target) in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n            optimizer.zero_grad()\n\n            points = points.data.numpy()\n            points[:, :, 0:3] = provider.random_scale_point_cloud(points[:, :, 0:3])\n            points[:, :, 0:3] = provider.shift_point_cloud(points[:, :, 0:3])\n            points = torch.Tensor(points)\n            points, label, target = points.float().cuda(), label.long().cuda(), target.long().cuda()\n            points = points.transpose(2, 1)\n\n            seg_pred, trans_feat = classifier(points, to_categorical(label, num_classes))\n            seg_pred = seg_pred.contiguous().view(-1, num_part)\n            target = target.view(-1, 1)[:, 0]\n            pred_choice = seg_pred.data.max(1)[1]\n\n            correct = pred_choice.eq(target.data).cpu().sum()\n            mean_correct.append(correct.item() / (args.batch_size * args.npoint))\n            loss = criterion(seg_pred, target, trans_feat)\n            loss.backward()\n            optimizer.step()\n\n        train_instance_acc = np.mean(mean_correct)\n        log_string('Train accuracy is: %.5f' % train_instance_acc)\n\n        with torch.no_grad():\n            test_metrics = {}\n            total_correct = 0\n            total_seen = 0\n            total_seen_class = [0 for _ in range(num_part)]\n            total_correct_class = [0 for _ in range(num_part)]\n            shape_ious = {cat: [] for cat in seg_classes.keys()}\n            seg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}\n\n            for cat in seg_classes.keys():\n                for label in seg_classes[cat]:\n                    seg_label_to_cat[label] = cat\n\n            classifier = classifier.eval()\n\n            for batch_id, (points, label, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n                cur_batch_size, NUM_POINT, _ = points.size()\n                points, label, target = points.float().cuda(), label.long().cuda(), target.long().cuda()\n                points = points.transpose(2, 1)\n                seg_pred, _ = classifier(points, to_categorical(label, num_classes))\n                cur_pred_val = seg_pred.cpu().data.numpy()\n                cur_pred_val_logits = cur_pred_val\n                cur_pred_val = np.zeros((cur_batch_size, NUM_POINT)).astype(np.int32)\n                target = target.cpu().data.numpy()\n\n                for i in range(cur_batch_size):\n                    cat = seg_label_to_cat[target[i, 0]]\n                    logits = cur_pred_val_logits[i, :, :]\n                    cur_pred_val[i, :] = np.argmax(logits[:, seg_classes[cat]], 1) + seg_classes[cat][0]\n\n                correct = np.sum(cur_pred_val == target)\n                total_correct += correct\n                total_seen += (cur_batch_size * NUM_POINT)\n\n                for l in range(num_part):\n                    total_seen_class[l] += np.sum(target == l)\n                    total_correct_class[l] += (np.sum((cur_pred_val == l) & (target == l)))\n\n                for i in range(cur_batch_size):\n                    segp = cur_pred_val[i, :]\n                    segl = target[i, :]\n                    cat = seg_label_to_cat[segl[0]]\n                    part_ious = [0.0 for _ in range(len(seg_classes[cat]))]\n                    for l in seg_classes[cat]:\n                        if (np.sum(segl == l) == 0) and (\n                                np.sum(segp == l) == 0):  # part is not present, no prediction as well\n                            part_ious[l - seg_classes[cat][0]] = 1.0\n                        else:\n                            part_ious[l - seg_classes[cat][0]] = np.sum((segl == l) & (segp == l)) / float(\n                                np.sum((segl == l) | (segp == l)))\n                    shape_ious[cat].append(np.mean(part_ious))\n\n            all_shape_ious = []\n            for cat in shape_ious.keys():\n                for iou in shape_ious[cat]:\n                    all_shape_ious.append(iou)\n                shape_ious[cat] = np.mean(shape_ious[cat])\n            mean_shape_ious = np.mean(list(shape_ious.values()))\n            test_metrics['accuracy'] = total_correct / float(total_seen)\n            test_metrics['class_avg_accuracy'] = np.mean(\n                np.array(total_correct_class) / np.array(total_seen_class, dtype=np.float))\n            for cat in sorted(shape_ious.keys()):\n                log_string('eval mIoU of %s %f' % (cat + ' ' * (14 - len(cat)), shape_ious[cat]))\n            test_metrics['class_avg_iou'] = mean_shape_ious\n            test_metrics['inctance_avg_iou'] = np.mean(all_shape_ious)\n\n        log_string('Epoch %d test Accuracy: %f  Class avg mIOU: %f   Inctance avg mIOU: %f' % (\n            epoch + 1, test_metrics['accuracy'], test_metrics['class_avg_iou'], test_metrics['inctance_avg_iou']))\n        if (test_metrics['inctance_avg_iou'] >= best_inctance_avg_iou):\n            logger.info('Save model...')\n            savepath = str(checkpoints_dir) + '/best_model.pth'\n            log_string('Saving at %s' % savepath)\n            state = {\n                'epoch': epoch,\n                'train_acc': train_instance_acc,\n                'test_acc': test_metrics['accuracy'],\n                'class_avg_iou': test_metrics['class_avg_iou'],\n                'inctance_avg_iou': test_metrics['inctance_avg_iou'],\n                'model_state_dict': classifier.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            }\n            torch.save(state, savepath)\n            log_string('Saving model....')\n\n        if test_metrics['accuracy'] > best_acc:\n            best_acc = test_metrics['accuracy']\n        if test_metrics['class_avg_iou'] > best_class_avg_iou:\n            best_class_avg_iou = test_metrics['class_avg_iou']\n        if test_metrics['inctance_avg_iou'] > best_inctance_avg_iou:\n            best_inctance_avg_iou = test_metrics['inctance_avg_iou']\n        log_string('Best accuracy is: %.5f' % best_acc)\n        log_string('Best class avg mIOU is: %.5f' % best_class_avg_iou)\n        log_string('Best inctance avg mIOU is: %.5f' % best_inctance_avg_iou)\n        global_epoch += 1\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    main(args)\n"
        },
        {
          "name": "train_semseg.py",
          "type": "blob",
          "size": 12.55859375,
          "content": "\"\"\"\nAuthor: Benny\nDate: Nov 2019\n\"\"\"\nimport argparse\nimport os\nfrom data_utils.S3DISDataLoader import S3DISDataset\nimport torch\nimport datetime\nimport logging\nfrom pathlib import Path\nimport sys\nimport importlib\nimport shutil\nfrom tqdm import tqdm\nimport provider\nimport numpy as np\nimport time\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, 'models'))\n\nclasses = ['ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door', 'table', 'chair', 'sofa', 'bookcase',\n           'board', 'clutter']\nclass2label = {cls: i for i, cls in enumerate(classes)}\nseg_classes = class2label\nseg_label_to_cat = {}\nfor i, cat in enumerate(seg_classes.keys()):\n    seg_label_to_cat[i] = cat\n\ndef inplace_relu(m):\n    classname = m.__class__.__name__\n    if classname.find('ReLU') != -1:\n        m.inplace=True\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Model')\n    parser.add_argument('--model', type=str, default='pointnet_sem_seg', help='model name [default: pointnet_sem_seg]')\n    parser.add_argument('--batch_size', type=int, default=16, help='Batch Size during training [default: 16]')\n    parser.add_argument('--epoch', default=32, type=int, help='Epoch to run [default: 32]')\n    parser.add_argument('--learning_rate', default=0.001, type=float, help='Initial learning rate [default: 0.001]')\n    parser.add_argument('--gpu', type=str, default='0', help='GPU to use [default: GPU 0]')\n    parser.add_argument('--optimizer', type=str, default='Adam', help='Adam or SGD [default: Adam]')\n    parser.add_argument('--log_dir', type=str, default=None, help='Log path [default: None]')\n    parser.add_argument('--decay_rate', type=float, default=1e-4, help='weight decay [default: 1e-4]')\n    parser.add_argument('--npoint', type=int, default=4096, help='Point Number [default: 4096]')\n    parser.add_argument('--step_size', type=int, default=10, help='Decay step for lr decay [default: every 10 epochs]')\n    parser.add_argument('--lr_decay', type=float, default=0.7, help='Decay rate for lr decay [default: 0.7]')\n    parser.add_argument('--test_area', type=int, default=5, help='Which area to use for test, option: 1-6 [default: 5]')\n\n    return parser.parse_args()\n\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    '''HYPER PARAMETER'''\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n\n    '''CREATE DIR'''\n    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n    experiment_dir = Path('./log/')\n    experiment_dir.mkdir(exist_ok=True)\n    experiment_dir = experiment_dir.joinpath('sem_seg')\n    experiment_dir.mkdir(exist_ok=True)\n    if args.log_dir is None:\n        experiment_dir = experiment_dir.joinpath(timestr)\n    else:\n        experiment_dir = experiment_dir.joinpath(args.log_dir)\n    experiment_dir.mkdir(exist_ok=True)\n    checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n    checkpoints_dir.mkdir(exist_ok=True)\n    log_dir = experiment_dir.joinpath('logs/')\n    log_dir.mkdir(exist_ok=True)\n\n    '''LOG'''\n    args = parse_args()\n    logger = logging.getLogger(\"Model\")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string('PARAMETER ...')\n    log_string(args)\n\n    root = 'data/stanford_indoor3d/'\n    NUM_CLASSES = 13\n    NUM_POINT = args.npoint\n    BATCH_SIZE = args.batch_size\n\n    print(\"start loading training data ...\")\n    TRAIN_DATASET = S3DISDataset(split='train', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n    print(\"start loading test data ...\")\n    TEST_DATASET = S3DISDataset(split='test', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n\n    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n                                                  pin_memory=True, drop_last=True,\n                                                  worker_init_fn=lambda x: np.random.seed(x + int(time.time())))\n    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=10,\n                                                 pin_memory=True, drop_last=True)\n    weights = torch.Tensor(TRAIN_DATASET.labelweights).cuda()\n\n    log_string(\"The number of training data is: %d\" % len(TRAIN_DATASET))\n    log_string(\"The number of test data is: %d\" % len(TEST_DATASET))\n\n    '''MODEL LOADING'''\n    MODEL = importlib.import_module(args.model)\n    shutil.copy('models/%s.py' % args.model, str(experiment_dir))\n    shutil.copy('models/pointnet2_utils.py', str(experiment_dir))\n\n    classifier = MODEL.get_model(NUM_CLASSES).cuda()\n    criterion = MODEL.get_loss().cuda()\n    classifier.apply(inplace_relu)\n\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find('Conv2d') != -1:\n            torch.nn.init.xavier_normal_(m.weight.data)\n            torch.nn.init.constant_(m.bias.data, 0.0)\n        elif classname.find('Linear') != -1:\n            torch.nn.init.xavier_normal_(m.weight.data)\n            torch.nn.init.constant_(m.bias.data, 0.0)\n\n    try:\n        checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n        start_epoch = checkpoint['epoch']\n        classifier.load_state_dict(checkpoint['model_state_dict'])\n        log_string('Use pretrain model')\n    except:\n        log_string('No existing model, starting training from scratch...')\n        start_epoch = 0\n        classifier = classifier.apply(weights_init)\n\n    if args.optimizer == 'Adam':\n        optimizer = torch.optim.Adam(\n            classifier.parameters(),\n            lr=args.learning_rate,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=args.decay_rate\n        )\n    else:\n        optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n\n    def bn_momentum_adjust(m, momentum):\n        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n            m.momentum = momentum\n\n    LEARNING_RATE_CLIP = 1e-5\n    MOMENTUM_ORIGINAL = 0.1\n    MOMENTUM_DECCAY = 0.5\n    MOMENTUM_DECCAY_STEP = args.step_size\n\n    global_epoch = 0\n    best_iou = 0\n\n    for epoch in range(start_epoch, args.epoch):\n        '''Train on chopped scenes'''\n        log_string('**** Epoch %d (%d/%s) ****' % (global_epoch + 1, epoch + 1, args.epoch))\n        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n        log_string('Learning rate:%f' % lr)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n        if momentum < 0.01:\n            momentum = 0.01\n        print('BN momentum updated to: %f' % momentum)\n        classifier = classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n        num_batches = len(trainDataLoader)\n        total_correct = 0\n        total_seen = 0\n        loss_sum = 0\n        classifier = classifier.train()\n\n        for i, (points, target) in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n            optimizer.zero_grad()\n\n            points = points.data.numpy()\n            points[:, :, :3] = provider.rotate_point_cloud_z(points[:, :, :3])\n            points = torch.Tensor(points)\n            points, target = points.float().cuda(), target.long().cuda()\n            points = points.transpose(2, 1)\n\n            seg_pred, trans_feat = classifier(points)\n            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n\n            batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n            target = target.view(-1, 1)[:, 0]\n            loss = criterion(seg_pred, target, trans_feat, weights)\n            loss.backward()\n            optimizer.step()\n\n            pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n            correct = np.sum(pred_choice == batch_label)\n            total_correct += correct\n            total_seen += (BATCH_SIZE * NUM_POINT)\n            loss_sum += loss\n        log_string('Training mean loss: %f' % (loss_sum / num_batches))\n        log_string('Training accuracy: %f' % (total_correct / float(total_seen)))\n\n        if epoch % 5 == 0:\n            logger.info('Save model...')\n            savepath = str(checkpoints_dir) + '/model.pth'\n            log_string('Saving at %s' % savepath)\n            state = {\n                'epoch': epoch,\n                'model_state_dict': classifier.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            }\n            torch.save(state, savepath)\n            log_string('Saving model....')\n\n        '''Evaluate on chopped scenes'''\n        with torch.no_grad():\n            num_batches = len(testDataLoader)\n            total_correct = 0\n            total_seen = 0\n            loss_sum = 0\n            labelweights = np.zeros(NUM_CLASSES)\n            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n            total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n            classifier = classifier.eval()\n\n            log_string('---- EPOCH %03d EVALUATION ----' % (global_epoch + 1))\n            for i, (points, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n                points = points.data.numpy()\n                points = torch.Tensor(points)\n                points, target = points.float().cuda(), target.long().cuda()\n                points = points.transpose(2, 1)\n\n                seg_pred, trans_feat = classifier(points)\n                pred_val = seg_pred.contiguous().cpu().data.numpy()\n                seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n\n                batch_label = target.cpu().data.numpy()\n                target = target.view(-1, 1)[:, 0]\n                loss = criterion(seg_pred, target, trans_feat, weights)\n                loss_sum += loss\n                pred_val = np.argmax(pred_val, 2)\n                correct = np.sum((pred_val == batch_label))\n                total_correct += correct\n                total_seen += (BATCH_SIZE * NUM_POINT)\n                tmp, _ = np.histogram(batch_label, range(NUM_CLASSES + 1))\n                labelweights += tmp\n\n                for l in range(NUM_CLASSES):\n                    total_seen_class[l] += np.sum((batch_label == l))\n                    total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l))\n                    total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)))\n\n            labelweights = labelweights.astype(np.float32) / np.sum(labelweights.astype(np.float32))\n            mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=np.float) + 1e-6))\n            log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n            log_string('eval point avg class IoU: %f' % (mIoU))\n            log_string('eval point accuracy: %f' % (total_correct / float(total_seen)))\n            log_string('eval point avg class acc: %f' % (\n                np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=np.float) + 1e-6))))\n\n            iou_per_class_str = '------- IoU --------\\n'\n            for l in range(NUM_CLASSES):\n                iou_per_class_str += 'class %s weight: %.3f, IoU: %.3f \\n' % (\n                    seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])), labelweights[l - 1],\n                    total_correct_class[l] / float(total_iou_deno_class[l]))\n\n            log_string(iou_per_class_str)\n            log_string('Eval mean loss: %f' % (loss_sum / num_batches))\n            log_string('Eval accuracy: %f' % (total_correct / float(total_seen)))\n\n            if mIoU >= best_iou:\n                best_iou = mIoU\n                logger.info('Save model...')\n                savepath = str(checkpoints_dir) + '/best_model.pth'\n                log_string('Saving at %s' % savepath)\n                state = {\n                    'epoch': epoch,\n                    'class_avg_iou': mIoU,\n                    'model_state_dict': classifier.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                }\n                torch.save(state, savepath)\n                log_string('Saving model....')\n            log_string('Best mIoU: %f' % best_iou)\n        global_epoch += 1\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    main(args)\n"
        },
        {
          "name": "visualizer",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}