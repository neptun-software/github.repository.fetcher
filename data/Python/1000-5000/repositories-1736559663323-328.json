{
  "metadata": {
    "timestamp": 1736559663323,
    "page": 328,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pipecat-ai/pipecat",
      "stars": 4194,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.33984375,
          "content": "# flyctl launch added from .gitignore\n**/.vscode\n**/env\n**/__pycache__\n**/*~\n**/venv\n#*#\n\n# Distribution / packaging\n**/.Python\n**/build\n**/develop-eggs\n**/dist\n**/downloads\n**/eggs\n**/.eggs\n**/lib\n**/lib64\n**/parts\n**/sdist\n**/var\n**/wheels\n**/share/python-wheels\n**/*.egg-info\n**/.installed.cfg\n**/*.egg\n**/MANIFEST\n**/.DS_Store\n**/.env\nfly.toml\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3720703125,
          "content": ".vscode\nenv/\n__pycache__/\n*~\nvenv\n.venv\n/.idea\n#*#\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n.DS_Store\n.env\nfly.toml\n\n# Example files\npipecat/examples/twilio-chatbot/templates/streams.xml\n\n# Documentation\ndocs/api/_build/\ndocs/api/api"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.56640625,
          "content": "version: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: '3.12'\n  apt_packages:\n    - portaudio19-dev\n    - python3-dev\n    - libasound2-dev\n  jobs:\n    pre_build:\n      - python -m pip install --upgrade pip\n      - pip install wheel setuptools\n    post_build:\n      - echo \"Build completed\"\n\nsphinx:\n  configuration: docs/api/conf.py\n  fail_on_warning: false\n\npython:\n  install:\n    - requirements: docs/api/requirements.txt\n    - method: pip\n      path: .\n\nsearch:\n  ranking:\n    api/*: 5\n    getting-started/*: 4\n    guides/*: 3\n\nsubmodules:\n  include: all\n  recursive: true\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 53.84765625,
          "content": "# Changelog\n\nAll notable changes to **Pipecat** will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n\n- Added a new foundational example `07e-interruptible-playht-http.py` for easy\n  testing of `PlayHTHttpTTSService`.\n\n- Added support for Google TTS Journey voices in `GoogleTTSService`.\n\n- Added `29-livekit-audio-chat.py`, as a new foundational examples for\n  `LiveKitTransportLayer`.\n\n- Added `enable_prejoin_ui`, `max_participants` and `start_video_off` params\n  to `DailyRoomProperties`.\n\n- Added `session_timeout` to `FastAPIWebsocketTransport` and `WebsocketServerTransport`\n  for configuring session timeouts (in seconds). Triggers `on_session_timeout` for custom timeout handling.\n  See [examples/websocket-server/bot.py](https://github.com/pipecat-ai/pipecat/blob/main/examples/websocket-server/bot.py).\n\n- Added the new modalities option and helper function to set Gemini output modalities.\n\n- Added `examples/foundational/26d-gemini-multimodal-live-text.py` which is using Gemini as TEXT modality and using another TTS provider for TTS process.\n\n### Changed\n\n- Changed the default model for `PlayHTHttpTTSService` to `Play3.0-mini-http`.\n\n- api_key, aws_access_key_id and region are no longer required parameters for the PollyTTSService (AWSTTSService)\n\n- Added `session_timeout` example in `examples/websocket-server/bot.py` to handle session timeout event.\n\n- Changed `InputParams` in `src/pipecat/services/gemini_multimodal_live/gemini.py` to support different modalities.\n\n### Fixed\n\n- Fixed an import issue for `PlayHTHttpTTSService`.\n\n- Fixed an issue where languages couldn't be used with the `PlayHTHttpTTSService`.\n\n- Fixed an issue where `OpenAIRealtimeBetaLLMService` audio chunks were hitting\n  an error when truncating audio content.\n\n- Fixed an issue where setting the voice and model for `RimeHttpTTSService` wasn't working.\n\n## [0.0.52] - 2024-12-24\n\n### Added\n\n- Constructor arguments for GoogleLLMService to directly set tools and tool_config.\n\n- Smart turn detection example (`22d-natural-conversation-gemini-audio.py`) that\n  leverages Gemini 2.0 capabilities ().\n  (see https://x.com/kwindla/status/1870974144831275410)\n\n- Added `DailyTransport.send_dtmf()` to send dial-out DTMF tones.\n\n- Added `DailyTransport.sip_call_transfer()` to forward SIP and PSTN calls to\n  another address or number. For example, transfer a SIP call to a different\n  SIP address or transfer a PSTN phone number to a different PSTN phone number.\n\n- Added `DailyTransport.sip_refer()` to transfer incoming SIP/PSTN calls from\n  outside Daily to another SIP/PSTN address.\n\n- Added an `auto_mode` input parameter to `ElevenLabsTTSService`. `auto_mode`\n  is set to `True` by default. Enabling this setting disables the chunk\n  schedule and all buffers, which reduces latency.\n\n- Added `KoalaFilter` which implement on device noise reduction using Koala\n  Noise Suppression.\n  (see https://picovoice.ai/platform/koala/)\n\n- Added `CerebrasLLMService` for Cerebras integration with an OpenAI-compatible\n  interface. Added foundational example `14k-function-calling-cerebras.py`.\n\n- Pipecat now supports Python 3.13. We had a dependency on the `audioop` package\n  which was deprecated and now removed on Python 3.13. We are now using\n  `audioop-lts` (https://github.com/AbstractUmbra/audioop) to provide the same\n  functionality.\n\n- Added timestamped conversation transcript support:\n\n  - New `TranscriptProcessor` factory provides access to user and assistant\n    transcript processors.\n  - `UserTranscriptProcessor` processes user speech with timestamps from\n    transcription.\n  - `AssistantTranscriptProcessor` processes assistant responses with LLM\n    context timestamps.\n  - Messages emitted with ISO 8601 timestamps indicating when they were spoken.\n  - Supports all LLM formats (OpenAI, Anthropic, Google) via standard message\n    format.\n  - New examples: `28a-transcription-processor-openai.py`,\n    `28b-transcription-processor-anthropic.py`, and\n    `28c-transcription-processor-gemini.py`.\n\n- Add support for more languages to ElevenLabs (Arabic, Croatian, Filipino,\n  Tamil) and PlayHT (Afrikans, Albanian, Amharic, Arabic, Bengali, Croatian,\n  Galician, Hebrew, Mandarin, Serbian, Tagalog, Urdu, Xhosa).\n\n### Changed\n\n- `PlayHTTTSService` uses the new v4 websocket API, which also fixes an issue\n  where text inputted to the TTS didn't return audio.\n\n- The default model for `ElevenLabsTTSService` is now `eleven_flash_v2_5`.\n\n- `OpenAIRealtimeBetaLLMService` now takes a `model` parameter in the\n  constructor.\n\n- Updated the default model for the `OpenAIRealtimeBetaLLMService`.\n\n- Room expiration (`exp`) in `DailyRoomProperties` is now optional (`None`) by\n  default instead of automatically setting a 5-minute expiration time. You must\n  explicitly set expiration time if desired.\n\n### Deprecated\n\n- `AWSTTSService` is now deprecated, use `PollyTTSService` instead.\n\n### Fixed\n\n- Fixed token counting in `GoogleLLMService`. Tokens were summed incorrectly\n  (double-counted in many cases).\n\n- Fixed an issue that could cause the bot to stop talking if there was a user\n  interruption before getting any audio from the TTS service.\n\n- Fixed an issue that would cause `ParallelPipeline` to handle `EndFrame`\n  incorrectly causing the main pipeline to not terminate or terminate too early.\n\n- Fixed an audio stuttering issue in `FastPitchTTSService`.\n\n- Fixed a `BaseOutputTransport` issue that was causing non-audio frames being\n  processed before the previous audio frames were played. This will allow, for\n  example, sending a frame `A` after a `TTSSpeakFrame` and the frame `A` will\n  only be pushed downstream after the audio generated from `TTSSpeakFrame` has\n  been spoken.\n\n- Fixed a `DeepgramSTTService` issue that was causing language to be passed as\n  an object instead of a string resulting in the connection to fail.\n\n## [0.0.51] - 2024-12-16\n\n### Fixed\n\n- Fixed an issue in websocket-based TTS services that was causing infinite\n  reconnections (Cartesia, ElevenLabs, PlayHT and LMNT).\n\n## [0.0.50] - 2024-12-11\n\n### Added\n\n- Added `GeminiMultimodalLiveLLMService`. This is an integration for Google's\n  Gemini Multimodal Live API, supporting:\n\n  - Real-time audio and video input processing\n  - Streaming text responses with TTS\n  - Audio transcription for both user and bot speech\n  - Function calling\n  - System instructions and context management\n  - Dynamic parameter updates (temperature, top_p, etc.)\n\n- Added `AudioTranscriber` utility class for handling audio transcription with\n  Gemini models.\n\n- Added new context classes for Gemini:\n\n  - `GeminiMultimodalLiveContext`\n  - `GeminiMultimodalLiveUserContextAggregator`\n  - `GeminiMultimodalLiveAssistantContextAggregator`\n  - `GeminiMultimodalLiveContextAggregatorPair`\n\n- Added new foundational examples for `GeminiMultimodalLiveLLMService`:\n\n  - `26-gemini-multimodal-live.py`\n  - `26a-gemini-multimodal-live-transcription.py`\n  - `26b-gemini-multimodal-live-video.py`\n  - `26c-gemini-multimodal-live-video.py`\n\n- Added `SimliVideoService`. This is an integration for Simli AI avatars.\n  (see https://www.simli.com)\n\n- Added NVIDIA Riva's `FastPitchTTSService` and `ParakeetSTTService`.\n  (see https://www.nvidia.com/en-us/ai-data-science/products/riva/)\n\n- Added `IdentityFilter`. This is the simplest frame filter that lets through\n  all incoming frames.\n\n- New `STTMuteStrategy` called `FUNCTION_CALL` which mutes the STT service\n  during LLM function calls.\n\n- `DeepgramSTTService` now exposes two event handlers `on_speech_started` and\n  `on_utterance_end` that could be used to implement interruptions. See new\n  example `examples/foundational/07c-interruptible-deepgram-vad.py`.\n\n- Added `GroqLLMService`, `GrokLLMService`, and `NimLLMService` for Groq, Grok,\n  and NVIDIA NIM API integration, with an OpenAI-compatible interface.\n\n- New examples demonstrating function calling with Groq, Grok, Azure OpenAI,\n  Fireworks, and NVIDIA NIM: `14f-function-calling-groq.py`,\n  `14g-function-calling-grok.py`, `14h-function-calling-azure.py`,\n  `14i-function-calling-fireworks.py`, and `14j-function-calling-nvidia.py`.\n\n- In order to obtain the audio stored by the `AudioBufferProcessor` you can now\n  also register an `on_audio_data` event handler. The `on_audio_data` handler\n  will be called every time `buffer_size` (a new constructor argument) is\n  reached. If `buffer_size` is 0 (default) you need to manually get the audio as\n  before using `AudioBufferProcessor.merge_audio_buffers()`.\n\n```\n@audiobuffer.event_handler(\"on_audio_data\")\nasync def on_audio_data(processor, audio, sample_rate, num_channels):\n    await save_audio(audio, sample_rate, num_channels)\n```\n\n- Added a new RTVI message called `disconnect-bot`, which when handled pushes\n  an `EndFrame` to trigger the pipeline to stop.\n\n### Changed\n\n- `STTMuteFilter` now supports multiple simultaneous muting strategies.\n\n- `XTTSService` language now defaults to `Language.EN`.\n\n- `SoundfileMixer` doesn't resample input files anymore to avoid startup\n  delays. The sample rate of the provided sound files now need to match the\n  sample rate of the output transport.\n\n- Input frames (audio, image and transport messages) are now system frames. This\n  means they are processed immediately by all processors instead of being queued\n  internally.\n\n- Expanded the transcriptions.language module to support a superset of\n  languages.\n\n- Updated STT and TTS services with language options that match the supported\n  languages for each service.\n\n- Updated the `AzureLLMService` to use the `OpenAILLMService`. Updated the\n  `api_version` to `2024-09-01-preview`.\n\n- Updated the `FireworksLLMService` to use the `OpenAILLMService`. Updated the\n  default model to `accounts/fireworks/models/firefunction-v2`.\n\n- Updated the `simple-chatbot` example to include a Javascript and React client\n  example, using RTVI JS and React.\n\n### Removed\n\n- Removed `AppFrame`. This was used as a special user custom frame, but there's\n  actually no use case for that.\n\n### Fixed\n\n- Fixed a `ParallelPipeline` issue that would cause system frames to be queued.\n\n- Fixed `FastAPIWebsocketTransport` so it can work with binary data (e.g. using\n  the protobuf serializer).\n\n- Fixed an issue in `CartesiaTTSService` that could cause previous audio to be\n  received after an interruption.\n\n- Fixed Cartesia, ElevenLabs, LMNT and PlayHT TTS websocket\n  reconnection. Before, if an error occurred no reconnection was happening.\n\n- Fixed a `BaseOutputTransport` issue that was causing audio to be discarded\n  after an `EndFrame` was received.\n\n- Fixed an issue in `WebsocketServerTransport` and `FastAPIWebsocketTransport`\n  that would cause a busy loop when using audio mixer.\n\n- Fixed a `DailyTransport` and `LiveKitTransport` issue where connections were\n  being closed in the input transport prematurely. This was causing frames\n  queued inside the pipeline being discarded.\n\n- Fixed an issue in `DailyTransport` that would cause some internal callbacks to\n  not be executed.\n\n- Fixed an issue where other frames were being processed while a `CancelFrame`\n  was being pushed down the pipeline.\n\n- `AudioBufferProcessor` now handles interruptions properly.\n\n- Fixed a `WebsocketServerTransport` issue that would prevent interruptions with\n  `TwilioSerializer` from working.\n\n- `DailyTransport.capture_participant_video` now allows capturing user's screen\n  share by simply passing `video_source=\"screenVideo\"`.\n\n- Fixed Google Gemini message handling to properly convert appended messages to\n  Gemini's required format.\n\n- Fixed an issue with `FireworksLLMService` where chat completions were failing\n  by removing the `stream_options` from the chat completion options.\n\n## [0.0.49] - 2024-11-17\n\n### Added\n\n- Added RTVI `on_bot_started` event which is useful in a single turn\n  interaction.\n\n- Added `DailyTransport` events `dialin-connected`, `dialin-stopped`,\n  `dialin-error` and `dialin-warning`. Needs daily-python >= 0.13.0.\n\n- Added `RimeHttpTTSService` and the `07q-interruptible-rime.py` foundational\n  example.\n\n- Added `STTMuteFilter`, a general-purpose processor that combines STT\n  muting and interruption control. When active, it prevents both transcription\n  and interruptions during bot speech. The processor supports multiple\n  strategies: `FIRST_SPEECH` (mute only during bot's first\n  speech), `ALWAYS` (mute during all bot speech), or `CUSTOM` (using provided\n  callback).\n\n- Added `STTMuteFrame`, a control frame that enables/disables speech\n  transcription in STT services.\n\n## [0.0.48] - 2024-11-10 \"Antonio release\"\n\n### Added\n\n- There's now an input queue in each frame processor. When you call\n  `FrameProcessor.push_frame()` this will internally call\n  `FrameProcessor.queue_frame()` on the next processor (upstream or downstream)\n  and the frame will be internally queued (except system frames). Then, the\n  queued frames will get processed. With this input queue it is also possible\n  for FrameProcessors to block processing more frames by calling\n  `FrameProcessor.pause_processing_frames()`. The way to resume processing\n  frames is by calling `FrameProcessor.resume_processing_frames()`.\n\n- Added audio filter `NoisereduceFilter`.\n\n- Introduce input transport audio filters (`BaseAudioFilter`). Audio filters can\n  be used to remove background noises before audio is sent to VAD.\n\n- Introduce output transport audio mixers (`BaseAudioMixer`). Output transport\n  audio mixers can be used, for example, to add background sounds or any other\n  audio mixing functionality before the output audio is actually written to the\n  transport.\n\n- Added `GatedOpenAILLMContextAggregator`. This aggregator keeps the last\n  received OpenAI LLM context frame and it doesn't let it through until the\n  notifier is notified.\n\n- Added `WakeNotifierFilter`. This processor expects a list of frame types and\n  will execute a given callback predicate when a frame of any of those type is\n  being processed. If the callback returns true the notifier will be notified.\n\n- Added `NullFilter`. A null filter doesn't push any frames upstream or\n  downstream. This is usually used to disable one of the pipelines in\n  `ParallelPipeline`.\n\n- Added `EventNotifier`. This can be used as a very simple synchronization\n  feature between processors.\n\n- Added `TavusVideoService`. This is an integration for Tavus digital twins.\n  (see https://www.tavus.io/)\n\n- Added `DailyTransport.update_subscriptions()`. This allows you to have fine\n  grained control of what media subscriptions you want for each participant in a\n  room.\n\n- Added audio filter `KrispFilter`.\n\n### Changed\n\n- The following `DailyTransport` functions are now `async` which means they need\n  to be awaited: `start_dialout`, `stop_dialout`, `start_recording`,\n  `stop_recording`, `capture_participant_transcription` and\n  `capture_participant_video`.\n\n- Changed default output sample rate to 24000. This changes all TTS service to\n  output to 24000 and also the default output transport sample rate. This\n  improves audio quality at the cost of some extra bandwidth.\n\n- `AzureTTSService` now uses Azure websockets instead of HTTP requests.\n\n- The previous `AzureTTSService` HTTP implementation is now\n  `AzureHttpTTSService`.\n\n### Fixed\n\n- Websocket transports (FastAPI and Websocket) now synchronize with time before\n  sending data. This allows for interruptions to just work out of the box.\n\n- Improved bot speaking detection for all TTS services by using actual bot\n  audio.\n\n- Fixed an issue that was generating constant bot started/stopped speaking\n  frames for HTTP TTS services.\n\n- Fixed an issue that was causing stuttering with AWS TTS service.\n\n- Fixed an issue with PlayHTTTSService, where the TTFB metrics were reporting\n  very small time values.\n\n- Fixed an issue where AzureTTSService wasn't initializing the specified\n  language.\n\n### Other\n\n- Add `23-bot-background-sound.py` foundational example.\n\n- Added a new foundational example `22-natural-conversation.py`. This example\n  shows how to achieve a more natural conversation detecting when the user ends\n  statement.\n\n## [0.0.47] - 2024-10-22\n\n### Added\n\n- Added `AssemblyAISTTService` and corresponding foundational examples\n  `07o-interruptible-assemblyai.py` and `13d-assemblyai-transcription.py`.\n\n- Added a foundational example for Gladia transcription:\n  `13c-gladia-transcription.py`\n\n### Changed\n\n- Updated `GladiaSTTService` to use the V2 API.\n\n- Changed `DailyTransport` transcription model to `nova-2-general`.\n\n### Fixed\n\n- Fixed an issue that would cause an import error when importing\n  `SileroVADAnalyzer` from the old package `pipecat.vad.silero`.\n\n- Fixed `enable_usage_metrics` to control LLM/TTS usage metrics separately\n  from `enable_metrics`.\n\n## [0.0.46] - 2024-10-19\n\n### Added\n\n- Added `audio_passthrough` parameter to `STTService`. If enabled it allows\n  audio frames to be pushed downstream in case other processors need them.\n\n- Added input parameter options for `PlayHTTTSService` and\n  `PlayHTHttpTTSService`.\n\n### Changed\n\n- Changed `DeepgramSTTService` model to `nova-2-general`.\n\n- Moved `SileroVAD` audio processor to `processors.audio.vad`.\n\n- Module `utils.audio` is now `audio.utils`. A new `resample_audio` function has\n  been added.\n\n- `PlayHTTTSService` now uses PlayHT websockets instead of HTTP requests.\n\n- The previous `PlayHTTTSService` HTTP implementation is now\n  `PlayHTHttpTTSService`.\n\n- `PlayHTTTSService` and `PlayHTHttpTTSService` now use a `voice_engine` of\n  `PlayHT3.0-mini`, which allows for multi-lingual support.\n\n- Renamed `OpenAILLMServiceRealtimeBeta` to `OpenAIRealtimeBetaLLMService` to\n  match other services.\n\n### Deprecated\n\n- `LLMUserResponseAggregator` and `LLMAssistantResponseAggregator` are\n  mostly deprecated, use `OpenAILLMContext` instead.\n\n- The `vad` package is now deprecated and `audio.vad` should be used\n  instead. The `avd` package will get removed in a future release.\n\n### Fixed\n\n- Fixed an issue that would cause an error if no VAD analyzer was passed to\n  `LiveKitTransport` params.\n\n- Fixed `SileroVAD` processor to support interruptions properly.\n\n### Other\n\n- Added `examples/foundational/07-interruptible-vad.py`. This is the same as\n  `07-interruptible.py` but using the `SileroVAD` processor instead of passing\n  the `VADAnalyzer` in the transport.\n\n## [0.0.45] - 2024-10-16\n\n### Changed\n\n- Metrics messages have moved out from the transport's base output into RTVI.\n\n## [0.0.44] - 2024-10-15\n\n### Added\n\n- Added support for OpenAI Realtime API with the new\n  `OpenAILLMServiceRealtimeBeta` processor.\n  (see https://platform.openai.com/docs/guides/realtime/overview)\n\n- Added `RTVIBotTranscriptionProcessor` which will send the RTVI\n  `bot-transcription` protocol message. These are TTS text aggregated (into\n  sentences) messages.\n\n- Added new input params to the `MarkdownTextFilter` utility. You can set\n  `filter_code` to filter code from text and `filter_tables` to filter tables\n  from text.\n\n- Added `CanonicalMetricsService`. This processor uses the new\n  `AudioBufferProcessor` to capture conversation audio and later send it to\n  Canonical AI.\n  (see https://canonical.chat/)\n\n- Added `AudioBufferProcessor`. This processor can be used to buffer mixed user and\n  bot audio. This can later be saved into an audio file or processed by some\n  audio analyzer.\n\n- Added `on_first_participant_joined` event to `LiveKitTransport`.\n\n### Changed\n\n- LLM text responses are now logged properly as unicode characters.\n\n- `UserStartedSpeakingFrame`, `UserStoppedSpeakingFrame`,\n  `BotStartedSpeakingFrame`, `BotStoppedSpeakingFrame`, `BotSpeakingFrame` and\n  `UserImageRequestFrame` are now based from `SystemFrame`\n\n### Fixed\n\n- Merge `RTVIBotLLMProcessor`/`RTVIBotLLMTextProcessor` and\n  `RTVIBotTTSProcessor`/`RTVIBotTTSTextProcessor` to avoid out of order issues.\n\n- Fixed an issue in RTVI protocol that could cause a `bot-llm-stopped` or\n  `bot-tts-stopped` message to be sent before a `bot-llm-text` or `bot-tts-text`\n  message.\n\n- Fixed `DeepgramSTTService` constructor settings not being merged with default\n  ones.\n\n- Fixed an issue in Daily transport that would cause tasks to be hanging if\n  urgent transport messages were being sent from a transport event handler.\n\n- Fixed an issue in `BaseOutputTransport` that would cause `EndFrame` to be\n  pushed downed too early and call `FrameProcessor.cleanup()` before letting the\n  transport stop properly.\n\n## [0.0.43] - 2024-10-10\n\n### Added\n\n- Added a new util called `MarkdownTextFilter` which is a subclass of a new\n  base class called `BaseTextFilter`. This is a configurable utility which\n  is intended to filter text received by TTS services.\n\n- Added new `RTVIUserLLMTextProcessor`. This processor will send an RTVI\n  `user-llm-text` message with the user content's that was sent to the LLM.\n\n### Changed\n\n- `TransportMessageFrame` doesn't have an `urgent` field anymore, instead\n  there's now a `TransportMessageUrgentFrame` which is a `SystemFrame` and\n  therefore skip all internal queuing.\n\n- For TTS services, convert inputted languages to match each service's language\n  format\n\n### Fixed\n\n- Fixed an issue where changing a language with the Deepgram STT service\n  wouldn't apply the change. This was fixed by disconnecting and reconnecting\n  when the language changes.\n\n## [0.0.42] - 2024-10-02\n\n### Added\n\n- `SentryMetrics` has been added to report frame processor metrics to\n  Sentry. This is now possible because `FrameProcessorMetrics` can now be passed\n  to `FrameProcessor`.\n\n- Added Google TTS service and corresponding foundational example\n  `07n-interruptible-google.py`\n\n- Added AWS Polly TTS support and `07m-interruptible-aws.py` as an example.\n\n- Added InputParams to Azure TTS service.\n\n- Added `LivekitTransport` (audio-only for now).\n\n- RTVI 0.2.0 is now supported.\n\n- All `FrameProcessors` can now register event handlers.\n\n```\ntts = SomeTTSService(...)\n\n@tts.event_handler(\"on_connected\"):\nasync def on_connected(processor):\n  ...\n```\n\n- Added `AsyncGeneratorProcessor`. This processor can be used together with a\n  `FrameSerializer` as an async generator. It provides a `generator()` function\n  that returns an `AsyncGenerator` and that yields serialized frames.\n\n- Added `EndTaskFrame` and `CancelTaskFrame`. These are new frames that are\n  meant to be pushed upstream to tell the pipeline task to stop nicely or\n  immediately respectively.\n\n- Added configurable LLM parameters (e.g., temperature, top_p, max_tokens, seed)\n  for OpenAI, Anthropic, and Together AI services along with corresponding\n  setter functions.\n\n- Added `sample_rate` as a constructor parameter for TTS services.\n\n- Pipecat has a pipeline-based architecture. The pipeline consists of frame\n  processors linked to each other. The elements traveling across the pipeline\n  are called frames.\n\n  To have a deterministic behavior the frames traveling through the pipeline\n  should always be ordered, except system frames which are out-of-band\n  frames. To achieve that, each frame processor should only output frames from a\n  single task.\n\n  In this version all the frame processors have their own task to push\n  frames. That is, when `push_frame()` is called the given frame will be put\n  into an internal queue (with the exception of system frames) and a frame\n  processor task will push it out.\n\n- Added pipeline clocks. A pipeline clock is used by the output transport to\n  know when a frame needs to be presented. For that, all frames now have an\n  optional `pts` field (prensentation timestamp). There's currently just one\n  clock implementation `SystemClock` and the `pts` field is currently only used\n  for `TextFrame`s (audio and image frames will be next).\n\n- A clock can now be specified to `PipelineTask` (defaults to\n  `SystemClock`). This clock will be passed to each frame processor via the\n  `StartFrame`.\n\n- Added `CartesiaHttpTTSService`.\n\n- `DailyTransport` now supports setting the audio bitrate to improve audio\n  quality through the `DailyParams.audio_out_bitrate` parameter. The new\n  default is 96kbps.\n\n- `DailyTransport` now uses the number of audio output channels (1 or 2) to set\n  mono or stereo audio when needed.\n\n- Interruptions support has been added to `TwilioFrameSerializer` when using\n  `FastAPIWebsocketTransport`.\n\n- Added new `LmntTTSService` text-to-speech service.\n  (see https://www.lmnt.com/)\n\n- Added `TTSModelUpdateFrame`, `TTSLanguageUpdateFrame`, `STTModelUpdateFrame`,\n  and `STTLanguageUpdateFrame` frames to allow you to switch models, language\n  and voices in TTS and STT services.\n\n- Added new `transcriptions.Language` enum.\n\n### Changed\n\n- Context frames are now pushed downstream from assistant context aggregators.\n\n- Removed Silero VAD torch dependency.\n\n- Updated individual update settings frame classes into a single\n  `ServiceUpdateSettingsFrame` class.\n\n- We now distinguish between input and output audio and image frames. We\n  introduce `InputAudioRawFrame`, `OutputAudioRawFrame`, `InputImageRawFrame`\n  and `OutputImageRawFrame` (and other subclasses of those). The input frames\n  usually come from an input transport and are meant to be processed inside the\n  pipeline to generate new frames. However, the input frames will not be sent\n  through an output transport. The output frames can also be processed by any\n  frame processor in the pipeline and they are allowed to be sent by the output\n  transport.\n\n- `ParallelTask` has been renamed to `SyncParallelPipeline`. A\n  `SyncParallelPipeline` is a frame processor that contains a list of different\n  pipelines to be executed concurrently. The difference between a\n  `SyncParallelPipeline` and a `ParallelPipeline` is that, given an input frame,\n  the `SyncParallelPipeline` will wait for all the internal pipelines to\n  complete. This is achieved by making sure the last processor in each of the\n  pipelines is synchronous (e.g. an HTTP-based service that waits for the\n  response).\n\n- `StartFrame` is back a system frame to make sure it's processed immediately by\n  all processors. `EndFrame` stays a control frame since it needs to be ordered\n  allowing the frames in the pipeline to be processed.\n\n- Updated `MoondreamService` revision to `2024-08-26`.\n\n- `CartesiaTTSService` and `ElevenLabsTTSService` now add presentation\n  timestamps to their text output. This allows the output transport to push the\n  text frames downstream at almost the same time the words are spoken. We say\n  \"almost\" because currently the audio frames don't have presentation timestamp\n  but they should be played at roughly the same time.\n\n- `DailyTransport.on_joined` event now returns the full session data instead of\n  just the participant.\n\n- `CartesiaTTSService` is now a subclass of `TTSService`.\n\n- `DeepgramSTTService` is now a subclass of `STTService`.\n\n- `WhisperSTTService` is now a subclass of `SegmentedSTTService`. A\n  `SegmentedSTTService` is a `STTService` where the provided audio is given in a\n  big chunk (i.e. from when the user starts speaking until the user stops\n  speaking) instead of a continous stream.\n\n### Fixed\n\n- Fixed OpenAI multiple function calls.\n\n- Fixed a Cartesia TTS issue that would cause audio to be truncated in some\n  cases.\n\n- Fixed a `BaseOutputTransport` issue that would stop audio and video rendering\n  tasks (after receiving and `EndFrame`) before the internal queue was emptied,\n  causing the pipeline to finish prematurely.\n\n- `StartFrame` should be the first frame every processor receives to avoid\n  situations where things are not initialized (because initialization happens on\n  `StartFrame`) and other frames come in resulting in undesired behavior.\n\n### Performance\n\n- `obj_id()` and `obj_count()` now use `itertools.count` avoiding the need of\n  `threading.Lock`.\n\n### Other\n\n- Pipecat now uses Ruff as its formatter (https://github.com/astral-sh/ruff).\n\n## [0.0.41] - 2024-08-22\n\n### Added\n\n- Added `LivekitFrameSerializer` audio frame serializer.\n\n### Fixed\n\n- Fix `FastAPIWebsocketOutputTransport` variable name clash with subclass.\n\n- Fix an `AnthropicLLMService` issue with empty arguments in function calling.\n\n### Other\n\n- Fixed `studypal` example errors.\n\n## [0.0.40] - 2024-08-20\n\n### Added\n\n- VAD parameters can now be dynamicallt updated using the\n  `VADParamsUpdateFrame`.\n\n- `ErrorFrame` has now a `fatal` field to indicate the bot should exit if a\n  fatal error is pushed upstream (false by default). A new `FatalErrorFrame`\n  that sets this flag to true has been added.\n\n- `AnthropicLLMService` now supports function calling and initial support for\n  prompt caching.\n  (see https://www.anthropic.com/news/prompt-caching)\n\n- `ElevenLabsTTSService` can now specify ElevenLabs input parameters such as\n  `output_format`.\n\n- `TwilioFrameSerializer` can now specify Twilio's and Pipecat's desired sample\n  rates to use.\n\n- Added new `on_participant_updated` event to `DailyTransport`.\n\n- Added `DailyRESTHelper.delete_room_by_name()` and\n  `DailyRESTHelper.delete_room_by_url()`.\n\n- Added LLM and TTS usage metrics. Those are enabled when\n  `PipelineParams.enable_usage_metrics` is True.\n\n- `AudioRawFrame`s are now pushed downstream from the base output\n  transport. This allows capturing the exact words the bot says by adding an STT\n  service at the end of the pipeline.\n\n- Added new `GStreamerPipelineSource`. This processor can generate image or\n  audio frames from a GStreamer pipeline (e.g. reading an MP4 file, and RTP\n  stream or anything supported by GStreamer).\n\n- Added `TransportParams.audio_out_is_live`. This flag is False by default and\n  it is useful to indicate we should not synchronize audio with sporadic images.\n\n- Added new `BotStartedSpeakingFrame` and `BotStoppedSpeakingFrame` control\n  frames. These frames are pushed upstream and they should wrap\n  `BotSpeakingFrame`.\n\n- Transports now allow you to register event handlers without decorators.\n\n### Changed\n\n- Support RTVI message protocol 0.1. This includes new messages, support for\n  messages responses, support for actions, configuration, webhooks and a bunch\n  of new cool stuff.\n  (see https://docs.rtvi.ai/)\n\n- `SileroVAD` dependency is now imported via pip's `silero-vad` package.\n\n- `ElevenLabsTTSService` now uses `eleven_turbo_v2_5` model by default.\n\n- `BotSpeakingFrame` is now a control frame.\n\n- `StartFrame` is now a control frame similar to `EndFrame`.\n\n- `DeepgramTTSService` now is more customizable. You can adjust the encoding and\n  sample rate.\n\n### Fixed\n\n- `TTSStartFrame` and `TTSStopFrame` are now sent when TTS really starts and\n  stops. This allows for knowing when the bot starts and stops speaking even\n  with asynchronous services (like Cartesia).\n\n- Fixed `AzureSTTService` transcription frame timestamps.\n\n- Fixed an issue with `DailyRESTHelper.create_room()` expirations which would\n  cause this function to stop working after the initial expiration elapsed.\n\n- Improved `EndFrame` and `CancelFrame` handling. `EndFrame` should end things\n  gracefully while a `CancelFrame` should cancel all running tasks as soon as\n  possible.\n\n- Fixed an issue in `AIService` that would cause a yielded `None` value to be\n  processed.\n\n- RTVI's `bot-ready` message is now sent when the RTVI pipeline is ready and\n  a first participant joins.\n\n- Fixed a `BaseInputTransport` issue that was causing incoming system frames to\n  be queued instead of being pushed immediately.\n\n- Fixed a `BaseInputTransport` issue that was causing start/stop interruptions\n  incoming frames to not cancel tasks and be processed properly.\n\n### Other\n\n- Added `studypal` example (from to the Cartesia folks!).\n\n- Most examples now use Cartesia.\n\n- Added examples `foundational/19a-tools-anthropic.py`,\n  `foundational/19b-tools-video-anthropic.py` and\n  `foundational/19a-tools-togetherai.py`.\n\n- Added examples `foundational/18-gstreamer-filesrc.py` and\n  `foundational/18a-gstreamer-videotestsrc.py` that show how to use\n  `GStreamerPipelineSource`\n\n- Remove `requests` library usage.\n\n- Cleanup examples and use `DailyRESTHelper`.\n\n## [0.0.39] - 2024-07-23\n\n### Fixed\n\n- Fixed a regression introduced in 0.0.38 that would cause Daily transcription\n  to stop the Pipeline.\n\n## [0.0.38] - 2024-07-23\n\n### Added\n\n- Added `force_reload`, `skip_validation` and `trust_repo` to `SileroVAD` and\n  `SileroVADAnalyzer`. This allows caching and various GitHub repo validations.\n\n- Added `send_initial_empty_metrics` flag to `PipelineParams` to request for\n  initial empty metrics (zero values). True by default.\n\n### Fixed\n\n- Fixed initial metrics format. It was using the wrong keys name/time instead of\n  processor/value.\n\n- STT services should be using ISO 8601 time format for transcription frames.\n\n- Fixed an issue that would cause Daily transport to show a stop transcription\n  error when actually none occurred.\n\n## [0.0.37] - 2024-07-22\n\n### Added\n\n- Added `RTVIProcessor` which implements the RTVI-AI standard.\n  See https://github.com/rtvi-ai\n\n- Added `BotInterruptionFrame` which allows interrupting the bot while talking.\n\n- Added `LLMMessagesAppendFrame` which allows appending messages to the current\n  LLM context.\n\n- Added `LLMMessagesUpdateFrame` which allows changing the LLM context for the\n  one provided in this new frame.\n\n- Added `LLMModelUpdateFrame` which allows updating the LLM model.\n\n- Added `TTSSpeakFrame` which causes the bot say some text. This text will not\n  be part of the LLM context.\n\n- Added `TTSVoiceUpdateFrame` which allows updating the TTS voice.\n\n### Removed\n\n- We remove the `LLMResponseStartFrame` and `LLMResponseEndFrame` frames. These\n  were added in the past to properly handle interruptions for the\n  `LLMAssistantContextAggregator`. But the `LLMContextAggregator` is now based\n  on `LLMResponseAggregator` which handles interruptions properly by just\n  processing the `StartInterruptionFrame`, so there's no need for these extra\n  frames any more.\n\n### Fixed\n\n- Fixed an issue with `StatelessTextTransformer` where it was pushing a string\n  instead of a `TextFrame`.\n\n- `TTSService` end of sentence detection has been improved. It now works with\n  acronyms, numbers, hours and others.\n\n- Fixed an issue in `TTSService` that would not properly flush the current\n  aggregated sentence if an `LLMFullResponseEndFrame` was found.\n\n### Performance\n\n- `CartesiaTTSService` now uses websockets which improves speed. It also\n  leverages the new Cartesia contexts which maintains generated audio prosody\n  when multiple inputs are sent, therefore improving audio quality a lot.\n\n## [0.0.36] - 2024-07-02\n\n### Added\n\n- Added `GladiaSTTService`.\n  See https://docs.gladia.io/chapters/speech-to-text-api/pages/live-speech-recognition\n\n- Added `XTTSService`. This is a local Text-To-Speech service.\n  See https://github.com/coqui-ai/TTS\n\n- Added `UserIdleProcessor`. This processor can be used to wait for any\n  interaction with the user. If the user doesn't say anything within a given\n  timeout a provided callback is called.\n\n- Added `IdleFrameProcessor`. This processor can be used to wait for frames\n  within a given timeout. If no frame is received within the timeout a provided\n  callback is called.\n\n- Added new frame `BotSpeakingFrame`. This frame will be continuously pushed\n  upstream while the bot is talking.\n\n- It is now possible to specify a Silero VAD version when using `SileroVADAnalyzer`\n  or `SileroVAD`.\n\n- Added `AysncFrameProcessor` and `AsyncAIService`. Some services like\n  `DeepgramSTTService` need to process things asynchronously. For example, audio\n  is sent to Deepgram but transcriptions are not returned immediately. In these\n  cases we still require all frames (except system frames) to be pushed\n  downstream from a single task. That's what `AsyncFrameProcessor` is for. It\n  creates a task and all frames should be pushed from that task. So, whenever a\n  new Deepgram transcription is ready that transcription will also be pushed\n  from this internal task.\n\n- The `MetricsFrame` now includes processing metrics if metrics are enabled. The\n  processing metrics indicate the time a processor needs to generate all its\n  output. Note that not all processors generate these kind of metrics.\n\n### Changed\n\n- `WhisperSTTService` model can now also be a string.\n\n- Added missing \\* keyword separators in services.\n\n### Fixed\n\n- `WebsocketServerTransport` doesn't try to send frames anymore if serializers\n  returns `None`.\n\n- Fixed an issue where exceptions that occurred inside frame processors were\n  being swallowed and not displayed.\n\n- Fixed an issue in `FastAPIWebsocketTransport` where it would still try to send\n  data to the websocket after being closed.\n\n### Other\n\n- Added Fly.io deployment example in `examples/deployment/flyio-example`.\n\n- Added new `17-detect-user-idle.py` example that shows how to use the new\n  `UserIdleProcessor`.\n\n## [0.0.35] - 2024-06-28\n\n### Changed\n\n- `FastAPIWebsocketParams` now require a serializer.\n\n- `TwilioFrameSerializer` now requires a `streamSid`.\n\n### Fixed\n\n- Silero VAD number of frames needs to be 512 for 16000 sample rate or 256 for\n  8000 sample rate.\n\n## [0.0.34] - 2024-06-25\n\n### Fixed\n\n- Fixed an issue with asynchronous STT services (Deepgram and Azure) that could\n  interruptions to ignore transcriptions.\n\n- Fixed an issue introduced in 0.0.33 that would cause the LLM to generate\n  shorter output.\n\n## [0.0.33] - 2024-06-25\n\n### Changed\n\n- Upgraded to Cartesia's new Python library 1.0.0. `CartesiaTTSService` now\n  expects a voice ID instead of a voice name (you can get the voice ID from\n  Cartesia's playground). You can also specify the audio `sample_rate` and\n  `encoding` instead of the previous `output_format`.\n\n### Fixed\n\n- Fixed an issue with asynchronous STT services (Deepgram and Azure) that could\n  cause static audio issues and interruptions to not work properly when dealing\n  with multiple LLMs sentences.\n\n- Fixed an issue that could mix new LLM responses with previous ones when\n  handling interruptions.\n\n- Fixed a Daily transport blocking situation that occurred while reading audio\n  frames after a participant left the room. Needs daily-python >= 0.10.1.\n\n## [0.0.32] - 2024-06-22\n\n### Added\n\n- Allow specifying a `DeepgramSTTService` url which allows using on-prem\n  Deepgram.\n\n- Added new `FastAPIWebsocketTransport`. This is a new websocket transport that\n  can be integrated with FastAPI websockets.\n\n- Added new `TwilioFrameSerializer`. This is a new serializer that knows how to\n  serialize and deserialize audio frames from Twilio.\n\n- Added Daily transport event: `on_dialout_answered`. See\n  https://reference-python.daily.co/api_reference.html#daily.EventHandler\n\n- Added new `AzureSTTService`. This allows you to use Azure Speech-To-Text.\n\n### Performance\n\n- Convert `BaseOutputTransport` and `BaseOutputTransport` to fully use asyncio\n  and remove the use of threads.\n\n### Other\n\n- Added `twilio-chatbot`. This is an example that shows how to integrate Twilio\n  phone numbers with a Pipecat bot.\n\n- Updated `07f-interruptible-azure.py` to use `AzureLLMService`,\n  `AzureSTTService` and `AzureTTSService`.\n\n## [0.0.31] - 2024-06-13\n\n### Performance\n\n- Break long audio frames into 20ms chunks instead of 10ms.\n\n## [0.0.30] - 2024-06-13\n\n### Added\n\n- Added `report_only_initial_ttfb` to `PipelineParams`. This will make it so\n  only the initial TTFB metrics after the user stops talking are reported.\n\n- Added `OpenPipeLLMService`. This service will let you run OpenAI through\n  OpenPipe's SDK.\n\n- Allow specifying frame processors' name through a new `name` constructor\n  argument.\n\n- Added `DeepgramSTTService`. This service has an ongoing websocket\n  connection. To handle this, it subclasses `AIService` instead of\n  `STTService`. The output of this service will be pushed from the same task,\n  except system frames like `StartFrame`, `CancelFrame` or\n  `StartInterruptionFrame`.\n\n### Changed\n\n- `FrameSerializer.deserialize()` can now return `None` in case it is not\n  possible to desearialize the given data.\n\n- `daily_rest.DailyRoomProperties` now allows extra unknown parameters.\n\n### Fixed\n\n- Fixed an issue where `DailyRoomProperties.exp` always had the same old\n  timestamp unless set by the user.\n\n- Fixed a couple of issues with `WebsocketServerTransport`. It needed to use\n  `push_audio_frame()` and also VAD was not working properly.\n\n- Fixed an issue that would cause LLM aggregator to fail with small\n  `VADParams.stop_secs` values.\n\n- Fixed an issue where `BaseOutputTransport` would send longer audio frames\n  preventing interruptions.\n\n### Other\n\n- Added new `07h-interruptible-openpipe.py` example. This example shows how to\n  use OpenPipe to run OpenAI LLMs and get the logs stored in OpenPipe.\n\n- Added new `dialin-chatbot` example. This examples shows how to call the bot\n  using a phone number.\n\n## [0.0.29] - 2024-06-07\n\n### Added\n\n- Added a new `FunctionFilter`. This filter will let you filter frames based on\n  a given function, except system messages which should never be filtered.\n\n- Added `FrameProcessor.can_generate_metrics()` method to indicate if a\n  processor can generate metrics. In the future this might get an extra argument\n  to ask for a specific type of metric.\n\n- Added `BasePipeline`. All pipeline classes should be based on this class. All\n  subclasses should implement a `processors_with_metrics()` method that returns\n  a list of all `FrameProcessor`s in the pipeline that can generate metrics.\n\n- Added `enable_metrics` to `PipelineParams`.\n\n- Added `MetricsFrame`. The `MetricsFrame` will report different metrics in the\n  system. Right now, it can report TTFB (Time To First Byte) values for\n  different services, that is the time spent between the arrival of a `Frame` to\n  the processor/service until the first `DataFrame` is pushed downstream. If\n  metrics are enabled an intial `MetricsFrame` with all the services in the\n  pipeline will be sent.\n\n- Added TTFB metrics and debug logging for TTS services.\n\n### Changed\n\n- Moved `ParallelTask` to `pipecat.pipeline.parallel_task`.\n\n### Fixed\n\n- Fixed PlayHT TTS service to work properly async.\n\n## [0.0.28] - 2024-06-05\n\n### Fixed\n\n- Fixed an issue with `SileroVADAnalyzer` that would cause memory to keep\n  growing indefinitely.\n\n## [0.0.27] - 2024-06-05\n\n### Added\n\n- Added `DailyTransport.participants()` and `DailyTransport.participant_counts()`.\n\n## [0.0.26] - 2024-06-05\n\n### Added\n\n- Added `OpenAITTSService`.\n\n- Allow passing `output_format` and `model_id` to `CartesiaTTSService` to change\n  audio sample format and the model to use.\n\n- Added `DailyRESTHelper` which helps you create Daily rooms and tokens in an\n  easy way.\n\n- `PipelineTask` now has a `has_finished()` method to indicate if the task has\n  completed. If a task is never ran `has_finished()` will return False.\n\n- `PipelineRunner` now supports SIGTERM. If received, the runner will be\n  canceled.\n\n### Fixed\n\n- Fixed an issue where `BaseInputTransport` and `BaseOutputTransport` where\n  stopping push tasks before pushing `EndFrame` frames could cause the bots to\n  get stuck.\n\n- Fixed an error closing local audio transports.\n\n- Fixed an issue with Deepgram TTS that was introduced in the previous release.\n\n- Fixed `AnthropicLLMService` interruptions. If an interruption occurred, a\n  `user` message could be appended after the previous `user` message. Anthropic\n  does not allow that because it requires alternate `user` and `assistant`\n  messages.\n\n### Performance\n\n- The `BaseInputTransport` does not pull audio frames from sub-classes any\n  more. Instead, sub-classes now push audio frames into a queue in the base\n  class. Also, `DailyInputTransport` now pushes audio frames every 20ms instead\n  of 10ms.\n\n- Remove redundant camera input thread from `DailyInputTransport`. This should\n  improve performance a little bit when processing participant videos.\n\n- Load Cartesia voice on startup.\n\n## [0.0.25] - 2024-05-31\n\n### Added\n\n- Added WebsocketServerTransport. This will create a websocket server and will\n  read messages coming from a client. The messages are serialized/deserialized\n  with protobufs. See `examples/websocket-server` for a detailed example.\n\n- Added function calling (LLMService.register_function()). This will allow the\n  LLM to call functions you have registered when needed. For example, if you\n  register a function to get the weather in Los Angeles and ask the LLM about\n  the weather in Los Angeles, the LLM will call your function.\n  See https://platform.openai.com/docs/guides/function-calling\n\n- Added new `LangchainProcessor`.\n\n- Added Cartesia TTS support (https://cartesia.ai/)\n\n### Fixed\n\n- Fixed SileroVAD frame processor.\n\n- Fixed an issue where `camera_out_enabled` would cause the highg CPU usage if\n  no image was provided.\n\n### Performance\n\n- Removed unnecessary audio input tasks.\n\n## [0.0.24] - 2024-05-29\n\n### Added\n\n- Exposed `on_dialin_ready` for Daily transport SIP endpoint handling. This\n  notifies when the Daily room SIP endpoints are ready. This allows integrating\n  with third-party services like Twilio.\n\n- Exposed Daily transport `on_app_message` event.\n\n- Added Daily transport `on_call_state_updated` event.\n\n- Added Daily transport `start_recording()`, `stop_recording` and\n  `stop_dialout`.\n\n### Changed\n\n- Added `PipelineParams`. This replaces the `allow_interruptions` argument in\n  `PipelineTask` and will allow future parameters in the future.\n\n- Fixed Deepgram Aura TTS base_url and added ErrorFrame reporting.\n\n- GoogleLLMService `api_key` argument is now mandatory.\n\n### Fixed\n\n- Daily tranport `dialin-ready` doesn't not block anymore and it now handles\n  timeouts.\n\n- Fixed AzureLLMService.\n\n## [0.0.23] - 2024-05-23\n\n### Fixed\n\n- Fixed an issue handling Daily transport `dialin-ready` event.\n\n## [0.0.22] - 2024-05-23\n\n### Added\n\n- Added Daily transport `start_dialout()` to be able to make phone or SIP calls.\n  See https://reference-python.daily.co/api_reference.html#daily.CallClient.start_dialout\n\n- Added Daily transport support for dial-in use cases.\n\n- Added Daily transport events: `on_dialout_connected`, `on_dialout_stopped`,\n  `on_dialout_error` and `on_dialout_warning`. See\n  https://reference-python.daily.co/api_reference.html#daily.EventHandler\n\n## [0.0.21] - 2024-05-22\n\n### Added\n\n- Added vision support to Anthropic service.\n\n- Added `WakeCheckFilter` which allows you to pass information downstream only\n  if you say a certain phrase/word.\n\n### Changed\n\n- `Filter` has been renamed to `FrameFilter` and it's now under\n  `processors/filters`.\n\n### Fixed\n\n- Fixed Anthropic service to use new frame types.\n\n- Fixed an issue in `LLMUserResponseAggregator` and `UserResponseAggregator`\n  that would cause frames after a brief pause to not be pushed to the LLM.\n\n- Clear the audio output buffer if we are interrupted.\n\n- Re-add exponential smoothing after volume calculation. This makes sure the\n  volume value being used doesn't fluctuate so much.\n\n## [0.0.20] - 2024-05-22\n\n### Added\n\n- In order to improve interruptions we now compute a loudness level using\n  [pyloudnorm](https://github.com/csteinmetz1/pyloudnorm). The audio coming\n  WebRTC transports (e.g. Daily) have an Automatic Gain Control (AGC) algorithm\n  applied to the signal, however we don't do that on our local PyAudio\n  signals. This means that currently incoming audio from PyAudio is kind of\n  broken. We will fix it in future releases.\n\n### Fixed\n\n- Fixed an issue where `StartInterruptionFrame` would cause\n  `LLMUserResponseAggregator` to push the accumulated text causing the LLM\n  respond in the wrong task. The `StartInterruptionFrame` should not trigger any\n  new LLM response because that would be spoken in a different task.\n\n- Fixed an issue where tasks and threads could be paused because the executor\n  didn't have more tasks available. This was causing issues when cancelling and\n  recreating tasks during interruptions.\n\n## [0.0.19] - 2024-05-20\n\n### Changed\n\n- `LLMUserResponseAggregator` and `LLMAssistantResponseAggregator` internal\n  messages are now exposed through the `messages` property.\n\n### Fixed\n\n- Fixed an issue where `LLMAssistantResponseAggregator` was not accumulating the\n  full response but short sentences instead. If there's an interruption we only\n  accumulate what the bot has spoken until now in a long response as well.\n\n## [0.0.18] - 2024-05-20\n\n### Fixed\n\n- Fixed an issue in `DailyOuputTransport` where transport messages were not\n  being sent.\n\n## [0.0.17] - 2024-05-19\n\n### Added\n\n- Added `google.generativeai` model support, including vision. This new `google`\n  service defaults to using `gemini-1.5-flash-latest`. Example in\n  `examples/foundational/12a-describe-video-gemini-flash.py`.\n\n- Added vision support to `openai` service. Example in\n  `examples/foundational/12a-describe-video-gemini-flash.py`.\n\n- Added initial interruptions support. The assistant contexts (or aggregators)\n  should now be placed after the output transport. This way, only the completed\n  spoken context is added to the assistant context.\n\n- Added `VADParams` so you can control voice confidence level and others.\n\n- `VADAnalyzer` now uses an exponential smoothed volume to improve speech\n  detection. This is useful when voice confidence is high (because there's\n  someone talking near you) but volume is low.\n\n### Fixed\n\n- Fixed an issue where TTSService was not pushing TextFrames downstream.\n\n- Fixed issues with Ctrl-C program termination.\n\n- Fixed an issue that was causing `StopTaskFrame` to actually not exit the\n  `PipelineTask`.\n\n## [0.0.16] - 2024-05-16\n\n### Fixed\n\n- `DailyTransport`: don't publish camera and audio tracks if not enabled.\n\n- Fixed an issue in `BaseInputTransport` that was causing frames pushed\n  downstream not pushed in the right order.\n\n## [0.0.15] - 2024-05-15\n\n### Fixed\n\n- Quick hot fix for receiving `DailyTransportMessage`.\n\n## [0.0.14] - 2024-05-15\n\n### Added\n\n- Added `DailyTransport` event `on_participant_left`.\n\n- Added support for receiving `DailyTransportMessage`.\n\n### Fixed\n\n- Images are now resized to the size of the output camera. This was causing\n  images not being displayed.\n\n- Fixed an issue in `DailyTransport` that would not allow the input processor to\n  shutdown if no participant ever joined the room.\n\n- Fixed base transports start and stop. In some situation processors would halt\n  or not shutdown properly.\n\n## [0.0.13] - 2024-05-14\n\n### Changed\n\n- `MoondreamService` argument `model_id` is now `model`.\n\n- `VADAnalyzer` arguments have been renamed for more clarity.\n\n### Fixed\n\n- Fixed an issue with `DailyInputTransport` and `DailyOutputTransport` that\n  could cause some threads to not start properly.\n\n- Fixed `STTService`. Add `max_silence_secs` and `max_buffer_secs` to handle\n  better what's being passed to the STT service. Also add exponential smoothing\n  to the RMS.\n\n- Fixed `WhisperSTTService`. Add `no_speech_prob` to avoid garbage output text.\n\n## [0.0.12] - 2024-05-14\n\n### Added\n\n- Added `DailyTranscriptionSettings` to be able to specify transcription\n  settings much easier (e.g. language).\n\n### Other\n\n- Updated `simple-chatbot` with Spanish.\n\n- Add missing dependencies in some of the examples.\n\n## [0.0.11] - 2024-05-13\n\n### Added\n\n- Allow stopping pipeline tasks with new `StopTaskFrame`.\n\n### Changed\n\n- TTS, STT and image generation service now use `AsyncGenerator`.\n\n### Fixed\n\n- `DailyTransport`: allow registering for participant transcriptions even if\n  input transport is not initialized yet.\n\n### Other\n\n- Updated `storytelling-chatbot`.\n\n## [0.0.10] - 2024-05-13\n\n### Added\n\n- Added Intel GPU support to `MoondreamService`.\n\n- Added support for sending transport messages (e.g. to communicate with an app\n  at the other end of the transport).\n\n- Added `FrameProcessor.push_error()` to easily send an `ErrorFrame` upstream.\n\n### Fixed\n\n- Fixed Azure services (TTS and image generation).\n\n### Other\n\n- Updated `simple-chatbot`, `moondream-chatbot` and `translation-chatbot`\n  examples.\n\n## [0.0.9] - 2024-05-12\n\n### Changed\n\nMany things have changed in this version. Many of the main ideas such as frames,\nprocessors, services and transports are still there but some things have changed\na bit.\n\n- `Frame`s describe the basic units for processing. For example, text, image or\n  audio frames. Or control frames to indicate a user has started or stopped\n  speaking.\n\n- `FrameProcessor`s process frames (e.g. they convert a `TextFrame` to an\n  `ImageRawFrame`) and push new frames downstream or upstream to their linked\n  peers.\n\n- `FrameProcessor`s can be linked together. The easiest wait is to use the\n  `Pipeline` which is a container for processors. Linking processors allow\n  frames to travel upstream or downstream easily.\n\n- `Transport`s are a way to send or receive frames. There can be local\n  transports (e.g. local audio or native apps), network transports\n  (e.g. websocket) or service transports (e.g. https://daily.co).\n\n- `Pipeline`s are just a processor container for other processors.\n\n- A `PipelineTask` know how to run a pipeline.\n\n- A `PipelineRunner` can run one or more tasks and it is also used, for example,\n  to capture Ctrl-C from the user.\n\n## [0.0.8] - 2024-04-11\n\n### Added\n\n- Added `FireworksLLMService`.\n\n- Added `InterimTranscriptionFrame` and enable interim results in\n  `DailyTransport` transcriptions.\n\n### Changed\n\n- `FalImageGenService` now uses new `fal_client` package.\n\n### Fixed\n\n- `FalImageGenService`: use `asyncio.to_thread` to not block main loop when\n  generating images.\n\n- Allow `TranscriptionFrame` after an end frame (transcriptions can be delayed\n  and received after `UserStoppedSpeakingFrame`).\n\n## [0.0.7] - 2024-04-10\n\n### Added\n\n- Add `use_cpu` argument to `MoondreamService`.\n\n## [0.0.6] - 2024-04-10\n\n### Added\n\n- Added `FalImageGenService.InputParams`.\n\n- Added `URLImageFrame` and `UserImageFrame`.\n\n- Added `UserImageRequestFrame` and allow requesting an image from a participant.\n\n- Added base `VisionService` and `MoondreamService`\n\n### Changed\n\n- Don't pass `image_size` to `ImageGenService`, images should have their own size.\n\n- `ImageFrame` now receives a tuple`(width,height)` to specify the size.\n\n- `on_first_other_participant_joined` now gets a participant argument.\n\n### Fixed\n\n- Check if camera, speaker and microphone are enabled before writing to them.\n\n### Performance\n\n- `DailyTransport` only subscribe to desired participant video track.\n\n## [0.0.5] - 2024-04-06\n\n### Changed\n\n- Use `camera_bitrate` and `camera_framerate`.\n\n- Increase `camera_framerate` to 30 by default.\n\n### Fixed\n\n- Fixed `LocalTransport.read_audio_frames`.\n\n## [0.0.4] - 2024-04-04\n\n### Added\n\n- Added project optional dependencies `[silero,openai,...]`.\n\n### Changed\n\n- Moved thransports to its own directory.\n\n- Use `OPENAI_API_KEY` instead of `OPENAI_CHATGPT_API_KEY`.\n\n### Fixed\n\n- Don't write to microphone/speaker if not enabled.\n\n### Other\n\n- Added live translation example.\n\n- Fix foundational examples.\n\n## [0.0.3] - 2024-03-13\n\n### Other\n\n- Added `storybot` and `chatbot` examples.\n\n## [0.0.2] - 2024-03-12\n\nInitial public release.\n"
        },
        {
          "name": "CHANGELOG.md.template",
          "type": "blob",
          "size": 0.83203125,
          "content": "# Changelog\n\nAll notable changes to the **&lt;project name&gt;** SDK will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\nPlease make sure to add your changes to the appropriate categories:\n\n## [Unreleased]\n\n### Added\n\n<!-- for new functionality -->\n\n- n/a\n\n### Changed\n\n<!-- for changed functionality -->\n\n- n/a\n\n### Deprecated\n\n<!-- for soon-to-be removed functionality -->\n\n- n/a\n\n### Removed\n\n<!-- for removed functionality -->\n\n- n/a\n\n### Fixed\n\n<!-- for fixed bugs -->\n\n- n/a\n\n### Performance\n\n<!-- for performance-relevant changes -->\n\n- n/a\n\n### Security\n\n<!-- for security-relevant changes -->\n\n- n/a\n\n### Other\n\n<!-- for everything else -->\n\n- n/a\n\n## [0.1.0] - YYYY-MM-DD\n\nInitial release.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 6.55078125,
          "content": "## Contributing to Pipecat\n\nWe welcome contributions of all kinds! Your help is appreciated. Follow these steps to get involved:\n\n1. **Fork this repository**: Start by forking the Pipecat Documentation repository to your GitHub account.\n\n2. **Clone the repository**: Clone your forked repository to your local machine.\n   ```bash\n   git clone https://github.com/your-username/pipecat\n   ```\n3. **Create a branch**: For your contribution, create a new branch.\n   ```bash\n   git checkout -b your-branch-name\n   ```\n4. **Make your changes**: Edit or add files as necessary.\n5. **Test your changes**: Ensure that your changes look correct and follow the style set in the codebase.\n6. **Commit your changes**: Once you're satisfied with your changes, commit them with a meaningful message.\n\n```bash\ngit commit -m \"Description of your changes\"\n```\n\n7. **Push your changes**: Push your branch to your forked repository.\n\n```bash\ngit push origin your-branch-name\n```\n\n9. **Submit a Pull Request (PR)**: Open a PR from your forked repository to the main branch of this repo. \n> Important: Describe the changes you've made clearly!\n\nOur maintainers will review your PR, and once everything is good, your contributions will be merged!\n\n\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official email address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at pipecat-ai@daily.co.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series of\nactions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n\n[homepage]: https://www.contributor-covenant.org\n[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html\n[Mozilla CoC]: https://github.com/mozilla/diversity\n[FAQ]: https://www.contributor-covenant.org/faq\n[translations]: https://www.contributor-covenant.org/translations"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.173828125,
          "content": "# setup\nFROM python:3.11.5\n\nWORKDIR /app\nCOPY requirements.txt /app\nCOPY *.py /app\nCOPY pyproject.toml /app\n\nCOPY src/ /app/src/\nCOPY examples/ /app/examples/\n\nWORKDIR /app\nRUN ls --recursive /app/\nRUN pip3 install --upgrade -r requirements.txt\nRUN python -m build .\nRUN pip3 install .\nRUN pip3 install gunicorn\n# If running on Ubuntu, Azure TTS requires some extra config\n# https://learn.microsoft.com/en-us/azure/ai-services/speech-service/quickstarts/setup-platform?pivots=programming-language-python&tabs=linux%2Cubuntu%2Cdotnetcli%2Cdotnet%2Cjre%2Cmaven%2Cnodejs%2Cmac%2Cpypi\n\nRUN wget -O - https://www.openssl.org/source/openssl-1.1.1w.tar.gz | tar zxf -\nWORKDIR openssl-1.1.1w\nRUN ./config --prefix=/usr/local\nRUN make -j $(nproc)\nRUN make install_sw install_ssldirs\nRUN ldconfig -v\nENV SSL_CERT_DIR=/etc/ssl/certs\n\n#ENV LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\nRUN apt clean\nRUN apt-get update\nRUN apt-get -y install build-essential libssl-dev ca-certificates libasound2 wget\n\nENV PYTHONUNBUFFERED=1\n\nWORKDIR /app\n\nEXPOSE 8000\n# run\nCMD [\"gunicorn\", \"--workers=2\", \"--log-level\", \"debug\", \"--chdir\", \"examples/server\", \"--capture-output\", \"daily-bot-manager:app\", \"--bind=0.0.0.0:8000\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.263671875,
          "content": "BSD 2-Clause License\n\nCopyright (c) 2025, Daily\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.8466796875,
          "content": "<h1><div align=\"center\">\n <img alt=\"pipecat\" width=\"300px\" height=\"auto\" src=\"https://raw.githubusercontent.com/pipecat-ai/pipecat/main/pipecat.png\">\n</div></h1>\n\n[![PyPI](https://img.shields.io/pypi/v/pipecat-ai)](https://pypi.org/project/pipecat-ai) [![Docs](https://img.shields.io/badge/Documentation-blue)](https://docs.pipecat.ai) [![Discord](https://img.shields.io/discord/1239284677165056021)](https://discord.gg/pipecat) <a href=\"https://app.commanddash.io/agent/github_pipecat-ai_pipecat\"><img src=\"https://img.shields.io/badge/AI-Code%20Agent-EB9FDA\"></a>\n\nPipecat is an open source Python framework for building voice and multimodal conversational agents. It handles the complex orchestration of AI services, network transport, audio processing, and multimodal interactions, letting you focus on creating engaging experiences.\n\n## What you can build\n\n- **Voice Assistants**: [Natural, real-time conversations with AI](https://demo.dailybots.ai/)\n- **Interactive Agents**: Personal coaches and meeting assistants\n- **Multimodal Apps**: Combine voice, video, images, and text\n- **Creative Tools**: [Story-telling experiences](https://storytelling-chatbot.fly.dev/) and social companions\n- **Business Solutions**: [Customer intake flows](https://www.youtube.com/watch?v=lDevgsp9vn0) and support bots\n- **Complex conversational flows**: [Refer to Pipecat Flows](https://github.com/pipecat-ai/pipecat-flows) to learn more\n\n## See it in action\n\n<p float=\"left\">\n    <a href=\"https://github.com/pipecat-ai/pipecat/tree/main/examples/simple-chatbot\"><img src=\"https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/simple-chatbot/image.png\" width=\"280\" /></a>&nbsp;\n    <a href=\"https://github.com/pipecat-ai/pipecat/tree/main/examples/storytelling-chatbot\"><img src=\"https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/storytelling-chatbot/image.png\" width=\"280\" /></a>\n    <br/>\n    <a href=\"https://github.com/pipecat-ai/pipecat/tree/main/examples/translation-chatbot\"><img src=\"https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/translation-chatbot/image.png\" width=\"280\" /></a>&nbsp;\n    <a href=\"https://github.com/pipecat-ai/pipecat/tree/main/examples/moondream-chatbot\"><img src=\"https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/moondream-chatbot/image.png\" width=\"280\" /></a>\n</p>\n\n## Key features\n\n- **Voice-first Design**: Built-in speech recognition, TTS, and conversation handling\n- **Flexible Integration**: Works with popular AI services (OpenAI, ElevenLabs, etc.)\n- **Pipeline Architecture**: Build complex apps from simple, reusable components\n- **Real-time Processing**: Frame-based pipeline architecture for fluid interactions\n- **Production Ready**: Enterprise-grade WebRTC and Websocket support\n\n💡 Looking to build structured conversations? Check out [Pipecat Flows](https://github.com/pipecat-ai/pipecat-flows) for managing complex conversational states and transitions.\n\n## Getting started\n\nYou can get started with Pipecat running on your local machine, then move your agent processes to the cloud when you’re ready. You can also add a 📞 telephone number, 🖼️ image output, 📺 video input, use different LLMs, and more.\n\n```shell\n# Install the module\npip install pipecat-ai\n\n# Set up your environment\ncp dot-env.template .env\n```\n\nTo keep things lightweight, only the core framework is included by default. If you need support for third-party AI services, you can add the necessary dependencies with:\n\n```shell\npip install \"pipecat-ai[option,...]\"\n```\n\nAvailable options include:\n\n| Category            | Services                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Install Command Example                 |\n| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- |\n| Speech-to-Text      | [AssemblyAI](https://docs.pipecat.ai/server/services/stt/assemblyai), [Azure](https://docs.pipecat.ai/server/services/stt/azure), [Deepgram](https://docs.pipecat.ai/server/services/stt/deepgram), [Gladia](https://docs.pipecat.ai/server/services/stt/gladia), [Whisper](https://docs.pipecat.ai/server/services/stt/whisper)                                                                                                                                                                                                                                                                                                                                                                                                                               | `pip install \"pipecat-ai[deepgram]\"`    |\n| LLMs                | [Anthropic](https://docs.pipecat.ai/server/services/llm/anthropic), [Azure](https://docs.pipecat.ai/server/services/llm/azure), [Cerebras](https://docs.pipecat.ai/server/services/llm/cerebras), [Fireworks AI](https://docs.pipecat.ai/server/services/llm/fireworks), [Gemini](https://docs.pipecat.ai/server/services/llm/gemini), [Grok](https://docs.pipecat.ai/server/services/llm/grok), [Groq](https://docs.pipecat.ai/server/services/llm/groq), [NVIDIA NIM](https://docs.pipecat.ai/server/services/llm/nim), [Ollama](https://docs.pipecat.ai/server/services/llm/ollama), [OpenAI](https://docs.pipecat.ai/server/services/llm/openai), [Together AI](https://docs.pipecat.ai/server/services/llm/together)                                      | `pip install \"pipecat-ai[openai]\"`      |\n| Text-to-Speech      | [AWS](https://docs.pipecat.ai/server/services/tts/aws), [Azure](https://docs.pipecat.ai/server/services/tts/azure), [Cartesia](https://docs.pipecat.ai/server/services/tts/cartesia), [Deepgram](https://docs.pipecat.ai/server/services/tts/deepgram), [ElevenLabs](https://docs.pipecat.ai/server/services/tts/elevenlabs), [Fish](https://docs.pipecat.ai/server/services/tts/fish), [Google](https://docs.pipecat.ai/server/services/tts/google), [LMNT](https://docs.pipecat.ai/server/services/tts/lmnt), [OpenAI](https://docs.pipecat.ai/server/services/tts/openai), [PlayHT](https://docs.pipecat.ai/server/services/tts/playht), [Rime](https://docs.pipecat.ai/server/services/tts/rime), [XTTS](https://docs.pipecat.ai/server/services/tts/xtts) | `pip install \"pipecat-ai[cartesia]\"`    |\n| Speech-to-Speech    | [Gemini Multimodal Live](https://docs.pipecat.ai/server/services/s2s/gemini), [OpenAI Realtime](https://docs.pipecat.ai/server/services/s2s/openai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `pip install \"pipecat-ai[openai]\"`      |\n| Transport           | [Daily (WebRTC)](https://docs.pipecat.ai/server/services/transport/daily), WebSocket, Local                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `pip install \"pipecat-ai[daily]\"`       |\n| Video               | [Tavus](https://docs.pipecat.ai/server/services/video/tavus), [Simli](https://docs.pipecat.ai/server/services/video/simli)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `pip install \"pipecat-ai[tavus,simli]\"` |\n| Vision & Image      | [Moondream](https://docs.pipecat.ai/server/services/vision/moondream), [fal](https://docs.pipecat.ai/server/services/image-generation/fal)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `pip install \"pipecat-ai[moondream]\"`   |\n| Audio Processing    | [Silero VAD](https://docs.pipecat.ai/server/utilities/audio/silero-vad-analyzer), [Krisp](https://docs.pipecat.ai/server/utilities/audio/krisp-filter), [Koala](https://docs.pipecat.ai/server/utilities/audio/koala-filter), [Noisereduce](https://docs.pipecat.ai/server/utilities/audio/noisereduce-filter)                                                                                                                                                                                                                                                                                                                                                                                                                                                | `pip install \"pipecat-ai[silero]\"`      |\n| Analytics & Metrics | [Canonical AI](https://docs.pipecat.ai/server/services/analytics/canonical), [Sentry](https://docs.pipecat.ai/server/services/analytics/sentry)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `pip install \"pipecat-ai[canonical]\"`   |\n\n📚 [View full services documentation →](https://docs.pipecat.ai/server/services/supported-services)\n\n## Code examples\n\n- [Foundational](https://github.com/pipecat-ai/pipecat/tree/main/examples/foundational) — small snippets that build on each other, introducing one or two concepts at a time\n- [Example apps](https://github.com/pipecat-ai/pipecat/tree/main/examples/) — complete applications that you can use as starting points for development\n\n## A simple voice agent running locally\n\nHere is a very basic Pipecat bot that greets a user when they join a real-time session. We'll use [Daily](https://daily.co) for real-time media transport, and [Cartesia](https://cartesia.ai/) for text-to-speech.\n\n```python\nimport asyncio\n\nfrom pipecat.frames.frames import EndFrame, TextFrame\nfrom pipecat.pipeline.pipeline import Pipeline\nfrom pipecat.pipeline.task import PipelineTask\nfrom pipecat.pipeline.runner import PipelineRunner\nfrom pipecat.services.cartesia import CartesiaTTSService\nfrom pipecat.transports.services.daily import DailyParams, DailyTransport\n\nasync def main():\n  # Use Daily as a real-time media transport (WebRTC)\n  transport = DailyTransport(\n    room_url=...,\n    token=\"\", # leave empty. Note: token is _not_ your api key\n    bot_name=\"Bot Name\",\n    params=DailyParams(audio_out_enabled=True))\n\n  # Use Cartesia for Text-to-Speech\n  tts = CartesiaTTSService(\n    api_key=...,\n    voice_id=...\n  )\n\n  # Simple pipeline that will process text to speech and output the result\n  pipeline = Pipeline([tts, transport.output()])\n\n  # Create Pipecat processor that can run one or more pipelines tasks\n  runner = PipelineRunner()\n\n  # Assign the task callable to run the pipeline\n  task = PipelineTask(pipeline)\n\n  # Register an event handler to play audio when a\n  # participant joins the transport WebRTC session\n  @transport.event_handler(\"on_first_participant_joined\")\n  async def on_first_participant_joined(transport, participant):\n    participant_name = participant.get(\"info\", {}).get(\"userName\", \"\")\n    # Queue a TextFrame that will get spoken by the TTS service (Cartesia)\n    await task.queue_frame(TextFrame(f\"Hello there, {participant_name}!\"))\n\n  # Register an event handler to exit the application when the user leaves.\n  @transport.event_handler(\"on_participant_left\")\n  async def on_participant_left(transport, participant, reason):\n    await task.queue_frame(EndFrame())\n\n  # Run the pipeline task\n  await runner.run(task)\n\nif __name__ == \"__main__\":\n  asyncio.run(main())\n```\n\nRun it with:\n\n```shell\npython app.py\n```\n\nDaily provides a prebuilt WebRTC user interface. While the app is running, you can visit at `https://<yourdomain>.daily.co/<room_url>` and listen to the bot say hello!\n\n## WebRTC for production use\n\nWebSockets are fine for server-to-server communication or for initial development. But for production use, you’ll need client-server audio to use a protocol designed for real-time media transport. (For an explanation of the difference between WebSockets and WebRTC, see [this post.](https://www.daily.co/blog/how-to-talk-to-an-llm-with-your-voice/#webrtc))\n\nOne way to get up and running quickly with WebRTC is to sign up for a Daily developer account. Daily gives you SDKs and global infrastructure for audio (and video) routing. Every account gets 10,000 audio/video/transcription minutes free each month.\n\nSign up [here](https://dashboard.daily.co/u/signup) and [create a room](https://docs.daily.co/reference/rest-api/rooms) in the developer Dashboard.\n\n## Hacking on the framework itself\n\n_Note that you may need to set up a virtual environment before following the instructions below. For instance, you might need to run the following from the root of the repo:_\n\n```shell\npython3 -m venv venv\nsource venv/bin/activate\n```\n\nFrom the root of this repo, run the following:\n\n```shell\npip install -r dev-requirements.txt\npython -m build\n```\n\nThis builds the package. To use the package locally (e.g. to run sample files), run\n\n```shell\npip install --editable \".[option,...]\"\n```\n\nIf you want to use this package from another directory, you can run:\n\n```shell\npip install \"path_to_this_repo[option,...]\"\n```\n\n### Running tests\n\nFrom the root directory, run:\n\n```shell\npytest --doctest-modules --ignore-glob=\"*to_be_updated*\" --ignore-glob=*pipeline_source* src tests\n```\n\n## Setting up your editor\n\nThis project uses strict [PEP 8](https://peps.python.org/pep-0008/) formatting via [Ruff](https://github.com/astral-sh/ruff).\n\n### Emacs\n\nYou can use [use-package](https://github.com/jwiegley/use-package) to install [emacs-lazy-ruff](https://github.com/christophermadsen/emacs-lazy-ruff) package and configure `ruff` arguments:\n\n```elisp\n(use-package lazy-ruff\n  :ensure t\n  :hook ((python-mode . lazy-ruff-mode))\n  :config\n  (setq lazy-ruff-format-command \"ruff format\")\n  (setq lazy-ruff-check-command \"ruff check --select I\"))\n```\n\n`ruff` was installed in the `venv` environment described before, so you should be able to use [pyvenv-auto](https://github.com/ryotaro612/pyvenv-auto) to automatically load that environment inside Emacs.\n\n```elisp\n(use-package pyvenv-auto\n  :ensure t\n  :defer t\n  :hook ((python-mode . pyvenv-auto-run)))\n```\n\n### Visual Studio Code\n\nInstall the\n[Ruff](https://marketplace.visualstudio.com/items?itemName=charliermarsh.ruff) extension. Then edit the user settings (_Ctrl-Shift-P_ `Open User Settings (JSON)`) and set it as the default Python formatter, and enable formatting on save:\n\n```json\n\"[python]\": {\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n    \"editor.formatOnSave\": true\n}\n```\n\n### PyCharm\n\n`ruff` was installed in the `venv` environment described before, now to enable autoformatting on save, go to `File` -> `Settings` -> `Tools` -> `File Watchers` and add a new watcher with the following settings:\n\n1. **Name**: `Ruff formatter`\n2. **File type**: `Python`\n3. **Working directory**: `$ContentRoot$`\n4. **Arguments**: `format $FilePath$`\n5. **Program**: `$PyInterpreterDirectory$/ruff`\n\n## Contributing\n\nWe welcome contributions from the community! Whether you're fixing bugs, improving documentation, or adding new features, here's how you can help:\n\n- **Found a bug?** Open an [issue](https://github.com/pipecat-ai/pipecat/issues)\n- **Have a feature idea?** Start a [discussion](https://discord.gg/pipecat)\n- **Want to contribute code?** Check our [CONTRIBUTING.md](CONTRIBUTING.md) guide\n- **Documentation improvements?** [Docs](https://github.com/pipecat-ai/docs) PRs are always welcome\n\nBefore submitting a pull request, please check existing issues and PRs to avoid duplicates.\n\nWe aim to review all contributions promptly and provide constructive feedback to help get your changes merged.\n\n## Getting help\n\n➡️ [Join our Discord](https://discord.gg/pipecat)\n\n➡️ [Read the docs](https://docs.pipecat.ai)\n\n➡️ [Reach us on X](https://x.com/pipecat_ai)\n"
        },
        {
          "name": "dev-requirements.txt",
          "type": "blob",
          "size": 0.15234375,
          "content": "build~=1.2.2\ngrpcio-tools~=1.68.1\npip-tools~=7.4.1\npyright~=1.1.390\npytest~=8.3.4\nruff~=0.8.3\nsetuptools~=75.6.0\nsetuptools_scm~=8.1.0\npython-dotenv~=1.0.1\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dot-env.template",
          "type": "blob",
          "size": 0.8125,
          "content": "# Anthropic\nANTHROPIC_API_KEY=...\n\n# AWS\nAWS_SECRET_ACCESS_KEY=...\nAWS_ACCESS_KEY_ID=...\nAWS_REGION=...\n\n# Azure\nAZURE_SPEECH_REGION=...\nAZURE_SPEECH_API_KEY=...\n\nAZURE_CHATGPT_API_KEY=...\nAZURE_CHATGPT_ENDPOINT=https://...\nAZURE_CHATGPT_MODEL=...\n\nAZURE_DALLE_API_KEY=...\nAZURE_DALLE_ENDPOINT=https://...\nAZURE_DALLE_MODEL=...\n\n# Daily\nDAILY_API_KEY=...\nDAILY_SAMPLE_ROOM_URL=https://...\n\n# ElevenLabs\nELEVENLABS_API_KEY=...\nELEVENLABS_VOICE_ID=...\n\n# Fal\nFAL_KEY=...\n\n# Fireworks\nFIREWORKS_API_KEY=...\n\n# Gladia\nGLADIA_API_KEY=...\n\n# LMNT\nLMNT_API_KEY=...\nLMNT_VOICE_ID=...\n\n# PlayHT\nPLAY_HT_USER_ID=...\nPLAY_HT_API_KEY=...\n\n# OpenAI\nOPENAI_API_KEY=...\n\n# OpenPipe\nOPENPIPE_API_KEY=...\n\n# Tavus\nTAVUS_API_KEY=...\nTAVUS_REPLICA_ID=...\nTAVUS_PERSONA_ID=...\n\n# Simli\nSIMLI_API_KEY=...\nSIMLI_FACE_ID=...\n\n# Krisp\nKRISP_MODEL_PATH=...\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pipecat.png",
          "type": "blob",
          "size": 12.5498046875,
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 3.05859375,
          "content": "[build-system]\nrequires = [\"setuptools>=64\", \"setuptools_scm>=8\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"pipecat-ai\"\ndynamic = [\"version\"]\ndescription = \"An open source framework for voice (and multimodal) assistants\"\nlicense = { text = \"BSD 2-Clause License\" }\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nkeywords = [\"webrtc\", \"audio\", \"video\", \"ai\"]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Topic :: Communications :: Conferencing\",\n    \"Topic :: Multimedia :: Sound/Audio\",\n    \"Topic :: Multimedia :: Video\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\"\n]\ndependencies = [\n    \"aiohttp~=3.11.10\",\n    \"audioop-lts~=0.2.1; python_version>='3.13'\",\n    \"loguru~=0.7.3\",\n    \"Markdown~=3.7\",\n    \"numpy~=2.1.3\",\n    \"numba~=0.61.0rc1\",\n    \"Pillow~=11.0.0\",\n    \"protobuf~=5.29.1\",\n    \"pydantic~=2.10.3\",\n    \"pyloudnorm~=0.1.1\",\n    \"resampy~=0.4.3\",\n    \"tenacity~=9.0.0\"\n]\n\n[project.urls]\nSource = \"https://github.com/pipecat-ai/pipecat\"\nWebsite = \"https://pipecat.ai\"\n\n[project.optional-dependencies]\nanthropic = [ \"anthropic~=0.40.0\" ]\nassemblyai = [ \"assemblyai~=0.34.0\" ]\naws = [ \"boto3~=1.35.27\" ]\nazure = [ \"azure-cognitiveservices-speech~=1.41.1\", \"openai~=1.59.0\" ]\ncanonical = [ \"aiofiles~=24.1.0\" ]\ncartesia = [ \"cartesia~=1.0.13\", \"websockets~=13.1\" ]\ncerebras = [ \"openai~=1.59.0\" ]\ndaily = [ \"daily-python~=0.14.2\" ]\ndeepgram = [ \"deepgram-sdk~=3.7.7\" ]\nelevenlabs = [ \"websockets~=13.1\" ]\nfal = [ \"fal-client~=0.4.1\" ]\nfish = [ \"ormsgpack~=1.7.0\", \"websockets~=13.1\" ]\ngladia = [ \"websockets~=13.1\" ]\ngoogle = [ \"google-generativeai~=0.8.3\", \"google-cloud-texttospeech~=2.21.1\" ]\ngrok = [ \"openai~=1.59.0\" ]\ngroq = [ \"openai~=1.59.0\" ]\ngstreamer = [ \"pygobject~=3.48.2\" ]\nfireworks = [ \"openai~=1.59.0\" ]\nkrisp = [ \"pipecat-ai-krisp~=0.3.0\" ]\nkoala = [ \"pvkoala~=2.0.2\" ]\nlangchain = [ \"langchain~=0.3.12\", \"langchain-community~=0.3.12\", \"langchain-openai~=0.2.12\" ]\nlivekit = [ \"livekit~=0.17.5\", \"livekit-api~=0.7.1\" ]\nlmnt = [ \"lmnt~=1.1.4\" ]\nlocal = [ \"pyaudio~=0.2.14\" ]\nmoondream = [ \"einops~=0.8.0\", \"timm~=1.0.8\", \"transformers~=4.44.0\" ]\nnim = [ \"openai~=1.59.0\" ]\nnoisereduce = [ \"noisereduce~=3.0.3\" ]\nopenai = [ \"openai~=1.59.0\", \"websockets~=13.1\", \"python-deepcompare~=1.0.1\" ]\nopenpipe = [ \"openpipe~=4.40.0\" ]\nplayht = [ \"pyht~=0.1.9\", \"websockets~=13.1\" ]\nriva = [ \"nvidia-riva-client~=2.17.0\" ]\nsilero = [ \"onnxruntime~=1.20.1\" ]\nsimli = [ \"simli-ai~=0.1.7\"]\nsoundfile = [ \"soundfile~=0.12.1\" ]\ntogether = [ \"openai~=1.59.0\" ]\nwebsocket = [ \"websockets~=13.1\", \"fastapi~=0.115.0\" ]\nwhisper = [ \"faster-whisper~=1.1.0\" ]\n\n[tool.setuptools.packages.find]\n# All the following settings are optional:\nwhere = [\"src\"]\n\n[tool.pytest.ini_options]\npythonpath = [\"src\"]\n\n[tool.setuptools_scm]\nlocal_scheme = \"no-local-version\"\nfallback_version = \"0.0.0-dev\"\n\n[tool.ruff]\nexclude = [\"*_pb2.py\"]\nline-length = 100\n\n[tool.ruff.lint]\nselect = [\n    \"D\", # Docstring rules\n    \"I\", # Import rules\n]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "test-requirements.txt",
          "type": "blob",
          "size": 0.5234375,
          "content": "aiohttp~=3.10.3\nanthropic~=0.30.0\nazure-cognitiveservices-speech~=1.40.0\nboto3~=1.35.27\ndaily-python~=0.11.0\ndeepgram-sdk~=3.5.0\nfal-client~=0.4.1\nfastapi~=0.115.0\nfaster-whisper~=1.0.3\ngoogle-cloud-texttospeech~=2.21.1\ngoogle-generativeai~=0.8.3\nlangchain~=0.2.14\nlivekit~=0.13.1\nlmnt~=1.1.4\nloguru~=0.7.2\nnumpy~=1.26.4\nopenai~=1.37.2\nopenpipe~=4.24.0\nPillow~=10.4.0\npyaudio~=0.2.14\npydantic~=2.8.2\npyloudnorm~=0.1.1\npyht~=0.1.4\npython-dotenv~=1.0.1\nresampy~=0.4.3\nsilero-vad~=5.1\ntogether~=1.2.7\ntransformers~=4.44.0\nwebsockets~=13.1\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}