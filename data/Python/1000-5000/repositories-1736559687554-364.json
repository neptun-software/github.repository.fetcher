{
  "metadata": {
    "timestamp": 1736559687554,
    "page": 364,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "649453932/Bert-Chinese-Text-Classification-Pytorch",
      "stars": 4120,
      "defaultBranch": "master",
      "files": [
        {
          "name": "ERNIE_pretrain",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2019 huwenxing\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.9794921875,
          "content": "# Bert-Chinese-Text-Classification-Pytorch\n[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE)\n\n中文文本分类，Bert，ERNIE，基于pytorch，开箱即用。\n\n## 介绍\n模型介绍、数据流动过程：~~还没写完，写好之后再贴博客地址。~~ 工作忙，懒得写了，类似文章有很多。\n\n机器：一块2080Ti ， 训练时间：30分钟。  \n\n## 环境\npython 3.7  \npytorch 1.1  \ntqdm  \nsklearn  \ntensorboardX  \n~~pytorch_pretrained_bert~~(预训练代码也上传了, 不需要这个库了)  \n\n\n## 中文数据集\n我从[THUCNews](http://thuctc.thunlp.org/)中抽取了20万条新闻标题，已上传至github，文本长度在20到30之间。一共10个类别，每类2万条。数据以字为单位输入模型。\n\n类别：财经、房产、股票、教育、科技、社会、时政、体育、游戏、娱乐。\n\n数据集划分：\n\n数据集|数据量\n--|--\n训练集|18万\n验证集|1万\n测试集|1万\n\n\n### 更换自己的数据集\n - 按照我数据集的格式来格式化你的中文数据集。  \n\n\n## 效果\n\n模型|acc|备注\n--|--|--\nbert|94.83%|单纯的bert\nERNIE|94.61%|说好的中文碾压bert呢  \nbert_CNN|94.44%|bert + CNN  \nbert_RNN|94.57%|bert + RNN  \nbert_RCNN|94.51%|bert + RCNN  \nbert_DPCNN|94.47%|bert + DPCNN  \n\n原始的bert效果就很好了，把bert当作embedding层送入其它模型，效果反而降了，之后会尝试长文本的效果对比。\n\nCNN、RNN、DPCNN、RCNN、RNN+Attention、FastText等模型效果，请见我另外一个[仓库](https://github.com/649453932/Chinese-Text-Classification-Pytorch)。  \n\n## 预训练语言模型\nbert模型放在 bert_pretain目录下，ERNIE模型放在ERNIE_pretrain目录下，每个目录下都是三个文件：\n - pytorch_model.bin  \n - bert_config.json  \n - vocab.txt  \n\n预训练模型下载地址：  \nbert_Chinese: 模型 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz  \n              词表 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt  \n来自[这里](https://github.com/huggingface/pytorch-transformers)   \n备用：模型的网盘地址：https://pan.baidu.com/s/1qSAD5gwClq7xlgzl_4W3Pw\n\nERNIE_Chinese: http://image.nghuyong.top/ERNIE.zip  \n来自[这里](https://github.com/nghuyong/ERNIE-Pytorch)  \n备用：网盘地址：https://pan.baidu.com/s/1lEPdDN1-YQJmKEd_g9rLgw  \n\n解压后，按照上面说的放在对应目录下，文件名称确认无误即可。  \n\n## 使用说明\n下载好预训练模型就可以跑了。\n```\n# 训练并测试：\n# bert\npython run.py --model bert\n\n# bert + 其它\npython run.py --model bert_CNN\n\n# ERNIE\npython run.py --model ERNIE\n```\n\n### 参数\n模型都在models目录下，超参定义和模型定义在同一文件中。  \n\n## 未完待续\n - 封装预测功能\n\n\n## 对应论文\n[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  \n[2] ERNIE: Enhanced Representation through Knowledge Integration  \n"
        },
        {
          "name": "THUCNews",
          "type": "tree",
          "content": null
        },
        {
          "name": "bert_pretrain",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "pytorch_pretrained",
          "type": "tree",
          "content": null
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 1.181640625,
          "content": "# coding: UTF-8\nimport time\nimport torch\nimport numpy as np\nfrom train_eval import train, init_network\nfrom importlib import import_module\nimport argparse\nfrom utils import build_dataset, build_iterator, get_time_dif\n\nparser = argparse.ArgumentParser(description='Chinese Text Classification')\nparser.add_argument('--model', type=str, required=True, help='choose a model: Bert, ERNIE')\nargs = parser.parse_args()\n\n\nif __name__ == '__main__':\n    dataset = 'THUCNews'  # 数据集\n\n    model_name = args.model  # bert\n    x = import_module('models.' + model_name)\n    config = x.Config(dataset)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed_all(1)\n    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n\n    start_time = time.time()\n    print(\"Loading data...\")\n    train_data, dev_data, test_data = build_dataset(config)\n    train_iter = build_iterator(train_data, config)\n    dev_iter = build_iterator(dev_data, config)\n    test_iter = build_iterator(test_data, config)\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n\n    # train\n    model = x.Model(config).to(config.device)\n    train(config, model, train_iter, dev_iter, test_iter)\n"
        },
        {
          "name": "train_eval.py",
          "type": "blob",
          "size": 4.9208984375,
          "content": "# coding: UTF-8\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn import metrics\nimport time\nfrom utils import get_time_dif\nfrom pytorch_pretrained.optimization import BertAdam\n\n\n# 权重初始化，默认xavier\ndef init_network(model, method='xavier', exclude='embedding', seed=123):\n    for name, w in model.named_parameters():\n        if exclude not in name:\n            if len(w.size()) < 2:\n                continue\n            if 'weight' in name:\n                if method == 'xavier':\n                    nn.init.xavier_normal_(w)\n                elif method == 'kaiming':\n                    nn.init.kaiming_normal_(w)\n                else:\n                    nn.init.normal_(w)\n            elif 'bias' in name:\n                nn.init.constant_(w, 0)\n            else:\n                pass\n\n\ndef train(config, model, train_iter, dev_iter, test_iter):\n    start_time = time.time()\n    model.train()\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n    # optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    optimizer = BertAdam(optimizer_grouped_parameters,\n                         lr=config.learning_rate,\n                         warmup=0.05,\n                         t_total=len(train_iter) * config.num_epochs)\n    total_batch = 0  # 记录进行到多少batch\n    dev_best_loss = float('inf')\n    last_improve = 0  # 记录上次验证集loss下降的batch数\n    flag = False  # 记录是否很久没有效果提升\n    model.train()\n    for epoch in range(config.num_epochs):\n        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n        for i, (trains, labels) in enumerate(train_iter):\n            outputs = model(trains)\n            model.zero_grad()\n            loss = F.cross_entropy(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            if total_batch % 100 == 0:\n                # 每多少轮输出在训练集和验证集上的效果\n                true = labels.data.cpu()\n                predic = torch.max(outputs.data, 1)[1].cpu()\n                train_acc = metrics.accuracy_score(true, predic)\n                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n                if dev_loss < dev_best_loss:\n                    dev_best_loss = dev_loss\n                    torch.save(model.state_dict(), config.save_path)\n                    improve = '*'\n                    last_improve = total_batch\n                else:\n                    improve = ''\n                time_dif = get_time_dif(start_time)\n                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n                model.train()\n            total_batch += 1\n            if total_batch - last_improve > config.require_improvement:\n                # 验证集loss超过1000batch没下降，结束训练\n                print(\"No optimization for a long time, auto-stopping...\")\n                flag = True\n                break\n        if flag:\n            break\n    test(config, model, test_iter)\n\n\ndef test(config, model, test_iter):\n    # test\n    model.load_state_dict(torch.load(config.save_path))\n    model.eval()\n    start_time = time.time()\n    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n    print(msg.format(test_loss, test_acc))\n    print(\"Precision, Recall and F1-Score...\")\n    print(test_report)\n    print(\"Confusion Matrix...\")\n    print(test_confusion)\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n\n\ndef evaluate(config, model, data_iter, test=False):\n    model.eval()\n    loss_total = 0\n    predict_all = np.array([], dtype=int)\n    labels_all = np.array([], dtype=int)\n    with torch.no_grad():\n        for texts, labels in data_iter:\n            outputs = model(texts)\n            loss = F.cross_entropy(outputs, labels)\n            loss_total += loss\n            labels = labels.data.cpu().numpy()\n            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n            labels_all = np.append(labels_all, labels)\n            predict_all = np.append(predict_all, predic)\n\n    acc = metrics.accuracy_score(labels_all, predict_all)\n    if test:\n        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n        confusion = metrics.confusion_matrix(labels_all, predict_all)\n        return acc, loss_total / len(data_iter), report, confusion\n    return acc, loss_total / len(data_iter)\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 3.232421875,
          "content": "# coding: UTF-8\nimport torch\nfrom tqdm import tqdm\nimport time\nfrom datetime import timedelta\n\nPAD, CLS = '[PAD]', '[CLS]'  # padding符号, bert中综合信息符号\n\n\ndef build_dataset(config):\n\n    def load_dataset(path, pad_size=32):\n        contents = []\n        with open(path, 'r', encoding='UTF-8') as f:\n            for line in tqdm(f):\n                lin = line.strip()\n                if not lin:\n                    continue\n                content, label = lin.split('\\t')\n                token = config.tokenizer.tokenize(content)\n                token = [CLS] + token\n                seq_len = len(token)\n                mask = []\n                token_ids = config.tokenizer.convert_tokens_to_ids(token)\n\n                if pad_size:\n                    if len(token) < pad_size:\n                        mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n                        token_ids += ([0] * (pad_size - len(token)))\n                    else:\n                        mask = [1] * pad_size\n                        token_ids = token_ids[:pad_size]\n                        seq_len = pad_size\n                contents.append((token_ids, int(label), seq_len, mask))\n        return contents\n    train = load_dataset(config.train_path, config.pad_size)\n    dev = load_dataset(config.dev_path, config.pad_size)\n    test = load_dataset(config.test_path, config.pad_size)\n    return train, dev, test\n\n\nclass DatasetIterater(object):\n    def __init__(self, batches, batch_size, device):\n        self.batch_size = batch_size\n        self.batches = batches\n        self.n_batches = len(batches) // batch_size\n        self.residue = False  # 记录batch数量是否为整数\n        if len(batches) % self.n_batches != 0:\n            self.residue = True\n        self.index = 0\n        self.device = device\n\n    def _to_tensor(self, datas):\n        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n\n        # pad前的长度(超过pad_size的设为pad_size)\n        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n        return (x, seq_len, mask), y\n\n    def __next__(self):\n        if self.residue and self.index == self.n_batches:\n            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n            self.index += 1\n            batches = self._to_tensor(batches)\n            return batches\n\n        elif self.index >= self.n_batches:\n            self.index = 0\n            raise StopIteration\n        else:\n            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n            self.index += 1\n            batches = self._to_tensor(batches)\n            return batches\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        if self.residue:\n            return self.n_batches + 1\n        else:\n            return self.n_batches\n\n\ndef build_iterator(dataset, config):\n    iter = DatasetIterater(dataset, config.batch_size, config.device)\n    return iter\n\n\ndef get_time_dif(start_time):\n    \"\"\"获取已使用时间\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n"
        }
      ]
    }
  ]
}