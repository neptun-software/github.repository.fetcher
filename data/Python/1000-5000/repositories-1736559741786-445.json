{
  "metadata": {
    "timestamp": 1736559741786,
    "page": 445,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "minivision-ai/photo2cartoon",
      "stars": 3981,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.876953125,
          "content": "#From https://github.com/github/gitignore/blob/master/Python.gitignore\n\n# Byte-compiled / optimized / DLL files\n*/__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\ndevelop-eggs/\ndist/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don’t work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n.project\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n.py~\n.gitignore~\n\n.vscode/\n.idea/\n*.py~\n*.ipynb\n\n# weights\n*.pb\n*.pth\n*.pt\n\nresults/\nphoto2cartoon/\nexperiment/\nupload/\nmodels_src/\nphoto2cartoon_resources.zip\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.044921875,
          "content": "MIT License\n\nCopyright (c) 2020 minivision-ai\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.65234375,
          "content": "<div align='center'>\r\n  <img src='./images/title.png'>\r\n</div>\r\n\r\n# 人像卡通化 (Photo to Cartoon)\r\n\r\n**中文版** | [English Version](README_EN.md)\r\n\r\n该项目为[小视科技](https://www.minivision.cn/)卡通肖像探索项目。您可使用微信扫描下方二维码或搜索“AI卡通秀”小程序体验卡通化效果。\r\n\r\n<div>\r\n  <img src='./images/QRcode.jpg' height='150px' width='150px'>\r\n</div>\r\n\r\n也可以前往我们的ai开放平台进行在线体验：[https://ai.minivision.cn/#/coreability/cartoon](https://ai.minivision.cn/#/coreability/cartoon)\r\n\r\n技术交流QQ群：937627932\r\n\r\n**Updates**\r\n- `2021.12.2`: [在Replicate平台体验](https://beta.replicate.ai/hao-qiang/photo2cartoon)\r\n- `2020.12.2`: 开源基于paddlepaddle的项目[photo2cartoon-paddle](https://github.com/minivision-ai/photo2cartoon-paddle)。\r\n- `2020.12.1`: 增加onnx测试模型, 详情请见 [test_onnx.py](./test_onnx.py)。\r\n\r\n## 简介\r\n人像卡通风格渲染的目标是，在保持原图像ID信息和纹理细节的同时，将真实照片转换为卡通风格的非真实感图像。我们的思路是，从大量照片/卡通数据中习得照片到卡通画的映射。一般而言，基于成对数据的pix2pix方法能达到较好的图像转换效果，但本任务的输入输出轮廓并非一一对应，例如卡通风格的眼睛更大、下巴更瘦；且成对的数据绘制难度大、成本较高，因此我们采用unpaired image translation方法来实现。\r\n\r\nUnpaired image translation流派最经典方法是CycleGAN，但原始CycleGAN的生成结果往往存在较为明显的伪影且不稳定。近期的论文U-GAT-IT提出了一种归一化方法——AdaLIN，能够自动调节Instance Norm和Layer Norm的比重，再结合attention机制能够实现精美的人像日漫风格转换。\r\n\r\n与夸张的日漫风不同，我们的卡通风格更偏写实，要求既有卡通画的简洁Q萌，又有明确的身份信息。为此我们增加了Face ID Loss，使用预训练的人脸识别模型提取照片和卡通画的ID特征，通过余弦距离来约束生成的卡通画。\r\n\r\n此外，我们提出了一种Soft-AdaLIN（Soft Adaptive Layer-Instance Normalization）归一化方法，在反规范化时将编码器的均值方差（照片特征）与解码器的均值方差（卡通特征）相融合。\r\n\r\n模型结构方面，在U-GAT-IT的基础上，我们在编码器之前和解码器之后各增加了2个hourglass模块，渐进地提升模型特征抽象和重建能力。\r\n\r\n由于实验数据较为匮乏，为了降低训练难度，我们将数据处理成固定的模式。首先检测图像中的人脸及关键点，根据人脸关键点旋转校正图像，并按统一标准裁剪，再将裁剪后的头像输入人像分割模型去除背景。\r\n\r\n<div align='center'>\r\n  <img src='./images/results.png'>\r\n</div>\r\n\r\n## Start\r\n\r\n### 安装依赖库\r\n项目所需的主要依赖库如下：\r\n- python 3.6\r\n- pytorch 1.4\r\n- tensorflow-gpu 1.14\r\n- face-alignment\r\n- dlib\r\n- onnxruntime\r\n\r\n### Clone：\r\n```\r\ngit clone https://github.com/minivision-ai/photo2cartoon.git\r\ncd ./photo2cartoon\r\n```\r\n\r\n### 下载资源\r\n[谷歌网盘](https://drive.google.com/open?id=1lsQS8hOCquMFKJFhK_z-n03ixWGkjT2P) | [百度网盘](https://pan.baidu.com/s/1MsT3-He3UGipKhUi4OcCJw) 提取码:y2ch\r\n\r\n1. 人像卡通化预训练模型：photo2cartoon_weights.pt(20200504更新)，存放在`models`路径下。\r\n2. 头像分割模型：seg_model_384.pb，存放在`utils`路径下。\r\n3. 人脸识别预训练模型：model_mobilefacenet.pth，存放在`models`路径下。（From: [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch)）\r\n4. 卡通画开源数据：`cartoon_data`，包含`trainB`和`testB`。\r\n5. 人像卡通化onnx模型：photo2cartoon_weights.onnx [谷歌网盘](https://drive.google.com/file/d/1PhwKDUhiq8p-UqrfHCqj257QnqBWD523/view?usp=sharing)，存放在`models`路径下。\r\n\r\n### 测试\r\n将一张测试照片（亚洲年轻女性）转换为卡通风格：\r\n```\r\npython test.py --photo_path ./images/photo_test.jpg --save_path ./images/cartoon_result.png\r\n```\r\n\r\n### 测试onnx模型\r\n```\r\npython test_onnx.py --photo_path ./images/photo_test.jpg --save_path ./images/cartoon_result.png\r\n```\r\n\r\n### 训练\r\n**1.数据准备**\r\n\r\n训练数据包括真实照片和卡通画像，为降低训练复杂度，我们对两类数据进行了如下预处理：\r\n- 检测人脸及关键点。\r\n- 根据关键点旋转校正人脸。\r\n- 将关键点边界框按固定的比例扩张并裁剪出人脸区域。\r\n- 使用人像分割模型将背景置白。\r\n\r\n<div align='center'>\r\n  <img src='./images/data_process.jpg'>\r\n</div>\r\n\r\n我们开源了204张处理后的卡通画数据，您还需准备约1000张人像照片（为匹配卡通数据，尽量使用亚洲年轻女性照片，人脸大小最好超过200x200像素），使用以下命令进行预处理：\r\n\r\n```\r\npython data_process.py --data_path YourPhotoFolderPath --save_path YourSaveFolderPath\r\n```\r\n\r\n将处理后的数据按照以下层级存放，`trainA`、`testA`中存放照片头像数据，`trainB`、`testB`中存放卡通头像数据。\r\n\r\n```\r\n├── dataset\r\n    └── photo2cartoon\r\n        ├── trainA\r\n            ├── xxx.jpg\r\n            ├── yyy.png\r\n            └── ...\r\n        ├── trainB\r\n            ├── zzz.jpg\r\n            ├── www.png\r\n            └── ...\r\n        ├── testA\r\n            ├── aaa.jpg \r\n            ├── bbb.png\r\n            └── ...\r\n        └── testB\r\n            ├── ccc.jpg \r\n            ├── ddd.png\r\n            └── ...\r\n```\r\n\r\n**2.训练**\r\n\r\n重新训练:\r\n```\r\npython train.py --dataset photo2cartoon\r\n```\r\n\r\n加载预训练参数:\r\n```\r\npython train.py --dataset photo2cartoon --pretrained_weights models/photo2cartoon_weights.pt\r\n```\r\n\r\n多GPU训练(仍建议使用batch_size=1，单卡训练):\r\n```\r\npython train.py --dataset photo2cartoon --batch_size 4 --gpu_ids 0 1 2 3\r\n```\r\n\r\n## Q&A\r\n#### Q：为什么开源的卡通化模型与小程序中的效果有差异？\r\n\r\nA：开源模型的训练数据收集自互联网，为了得到更加精美的效果，我们在训练小程序中卡通化模型时，采用了定制的卡通画数据（200多张），且增大了输入分辨率。此外，小程序中的人脸特征提取器采用自研的识别模型，效果优于本项目使用的开源识别模型。\r\n\r\n#### Q：如何选取效果最好的模型？\r\n\r\nA：首先训练模型200k iterations，然后使用FID指标挑选出最优模型，最终挑选出的模型为迭代90k iterations时的模型。\r\n\r\n#### Q：关于人脸特征提取模型。\r\n\r\nA：实验中我们发现，使用自研的识别模型计算Face ID Loss训练效果远好于使用开源识别模型，若训练效果出现鲁棒性问题，可尝试将Face ID Loss权重置零。\r\n\r\n#### Q：人像分割模型是否能用与分割半身像？\r\nA：不能。该模型是针对本项目训练的专用模型，需先裁剪出人脸区域再输入。\r\n\r\n## Tips\r\n我们开源的模型是基于亚洲年轻女性训练的，对于其他人群覆盖不足，您可根据使用场景自行收集相应人群的数据进行训练。我们的[开放平台](https://ai.minivision.cn/#/coreability/cartoon)提供了能够覆盖各类人群的卡通化服务，您可前往体验。如有定制卡通风格需求请联系商务:18852075216。\r\n\r\n## 参考\r\nU-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation [[Paper](https://arxiv.org/abs/1907.10830)][[Code](https://github.com/znxlwm/UGATIT-pytorch)]\r\n\r\n[InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch)\r\n\r\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 6.125,
          "content": "<div align='center'>\n  <img src='./images/title.png'>\n</div>\n\n# Photo to Cartoon\n\n[中文版](README.md) | English Version\n\n[Minivision](https://www.minivision.cn/)'s photo-to-cartoon translation project is opened source in this repo, you can try our WeChat mini program \"AI Cartoon Show\" via scanning the QR code below.\n\n<div>\n  <img src='./images/QRcode.jpg' height='150px' width='150px'>\n</div>\n\nYou can also try on this page: [https://ai.minivision.cn/#/coreability/cartoon](https://ai.minivision.cn/#/coreability/cartoon)\n\n**Updates**\n- `2021.12.2`: [Run this model on Replicate](https://beta.replicate.ai/hao-qiang/photo2cartoon).\n- `2020.12.2`: [photo2cartoon-paddle](https://github.com/minivision-ai/photo2cartoon-paddle) is released.\n- `2020.12.1`: Add onnx test model, see [test_onnx.py](./test_onnx.py) for details.\n\n## Introduce\n\nThe aim of portrait cartoon stylization is to transform real photos into cartoon images with portrait's ID information and texture details. We use Generative Adversarial Network method to realize the mapping of picture to cartoon. Considering the difficulty in obtaining paired data and the non-corresponding shape of input and output, we adopt unpaired image translation fashion.\n\nThe results of CycleGAN, a classic unpaired image translation method, often have obvious artifacts and are unstable. Recently, Kim et al. propose a novel normalization function (AdaLIN) and an attention module in paper \"U-GAT-IT\" and achieve exquisite selfie2anime results.\n\nDifferent from the exaggerated anime style, our cartoon style is more realistic and contains unequivocal ID information. To this end, we add a Face ID Loss (cosine distance of ID features between input image and cartoon image) to reach identity invariance. \n\nWe propose a Soft Adaptive Layer-Instance Normalization (Soft-AdaLIN) method which fuses the statistics of encoding features and decoding features in de-standardization. \n\nBased on U-GAT-IT, two hourglass modules are introduced before encoder and after decoder to improve the performance in a progressively way.\n\nWe also pre-process the data to a fixed pattern to help reduce the difficulty of optimization. For details, see below.\n\n<div align='center'>\n  <img src='./images/results.png'>\n</div>\n\n## Start\n\n### Requirements\n- python 3.6\n- pytorch 1.4\n- tensorflow-gpu 1.14\n- face-alignment\n- dlib\n- onnxruntime\n\n### Clone\n\n```\ngit clone https://github.com/minivision-ai/photo2cartoon.git\ncd ./photo2cartoon\n```\n\n### Download\n\n[Google Drive](https://drive.google.com/open?id=1lsQS8hOCquMFKJFhK_z-n03ixWGkjT2P) | [Baidu Cloud](https://pan.baidu.com/s/1MsT3-He3UGipKhUi4OcCJw) acess code: y2ch\n\n1. Put the pre-trained photo2cartoon model **photo2cartoon_weights.pt** into `models` folder (update on may 4, 2020).\n2. Place the head segmentation model **seg_model_384.pb** in `utils` folder. \n3. Put the pre-trained face recognition model **model_mobilefacenet.pth** into `models` folder (From [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch)).\n4. Open-source cartoon dataset **`cartoon_data/`** contains `trainB` and `testB`.\n5. Put the photo2cartoon onnx model **photo2cartoon_weights.onnx** [Google Drive](https://drive.google.com/file/d/1PhwKDUhiq8p-UqrfHCqj257QnqBWD523/view?usp=sharing) into `models` folder.\n\n### Test\n\nPlease use a young Asian woman photo.\n```\npython test.py --photo_path ./images/photo_test.jpg --save_path ./images/cartoon_result.png\n```\n\n### Test onnx model\n```\npython test_onnx.py --photo_path ./images/photo_test.jpg --save_path ./images/cartoon_result.png\n```\n\n### Train\n**1.Data**\n\nTraining data contains portrait photos (domain A) and cartoon images (domain B). The following process can help reduce the difficulty of optimization.\n- Detect face and its landmarks.\n- Face alignment according to landmarks.\n- expand the bbox of landmarks and crop face.\n- remove the background by semantic segment.\n\n<div align='center'>\n  <img src='./images/data_process.jpg'>\n</div>\n\nWe provide 204 cartoon images, besides, you need to prepare about 1,000 young Asian women photos and pre-process them by following command.\n\n```\npython data_process.py --data_path YourPhotoFolderPath --save_path YourSaveFolderPath\n```\n\nThe `dataset` directory should look like this:\n```\n├── dataset\n    └── photo2cartoon\n        ├── trainA\n            ├── xxx.jpg\n            ├── yyy.png\n            └── ...\n        ├── trainB\n            ├── zzz.jpg\n            ├── www.png\n            └── ...\n        ├── testA\n            ├── aaa.jpg \n            ├── bbb.png\n            └── ...\n        └── testB\n            ├── ccc.jpg \n            ├── ddd.png\n            └── ...\n```\n\n**2.Train**\n\nTrain from scratch:\n```\npython train.py --dataset photo2cartoon\n```\n\nLoad pre-trained weights:\n```\npython train.py --dataset photo2cartoon --pretrained_weights models/photo2cartoon_weights.pt\n```\n\nTrain with Multi-GPU:\n```\npython train.py --dataset photo2cartoon --batch_size 4 --gpu_ids 0 1 2 3\n```\n\n## Q&A\n#### Q：Why is the result of this project different from mini program?\n\nA: For better performance, we customized the cartoon data (about 200 images) when training model for mini program. We also improved input size for high definition. Besides, we adopted our internal recognition model to calculate Face ID Loss which is much better than the open-sourced one used in this repo.\n\n#### Q: How to select best model?\n\nA: We trained model about 200k iterations, then selected best model according to FID metric.\n\n#### Q: About face recognition model.\n\nA: We found that the experimental result calculated Face ID Loss by our internal recognition model is much better than the open-sourced one. You can try to remove Face ID Loss if the result is unstable.\n\n#### Q：Can I use the segmentation model to predict half-length portrait?\nA：No. The model is trained for croped face specifically.\n\n## Reference\n\nU-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation [[Paper](https://arxiv.org/abs/1907.10830)][[Code](https://github.com/znxlwm/UGATIT-pytorch)]\n\n[InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch)\n"
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.392578125,
          "content": "predict: \"predict.py:Predictor\"\nbuild:\n  python_version: \"3.8\"\n  system_packages:\n    - \"libgl1-mesa-glx\"\n    - \"libglib2.0-0\"\n  python_packages:\n    - \"cmake==3.21.1\"\n    - \"torch==1.8.0\"\n    - \"torchvision==0.9.0\"\n    - \"numpy==1.19.2\"\n    - \"ipython==7.21.0\"\n    - \"opencv-python==4.3.0.38\"\n    - \"face-alignment==1.3.4\"\n    - \"tensorflow-gpu==2.5.0\"\n  pre_install:\n    - pip install dlib\n\n\n\n\n\n\n\n\n\n\n"
        },
        {
          "name": "data_process.py",
          "type": "blob",
          "size": 0.951171875,
          "content": "import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport argparse\n\nfrom utils import Preprocess\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data_path', type=str, help='photo folder path')\nparser.add_argument('--save_path', type=str, help='save folder path')\n\nargs = parser.parse_args()\nos.makedirs(args.save_path, exist_ok=True)\n\npre = Preprocess()\n\nfor idx, img_name in enumerate(tqdm(os.listdir(args.data_path))):\n    img = cv2.cvtColor(cv2.imread(os.path.join(args.data_path, img_name)), cv2.COLOR_BGR2RGB)\n    \n    # face alignment and segmentation\n    face_rgba = pre.process(img)\n    if face_rgba is not None:\n        # change background to white\n        face = face_rgba[:,:,:3].copy()\n        mask = face_rgba[:,:,3].copy()[:,:,np.newaxis]/255.\n        face_white_bg = (face*mask + (1-mask)*255).astype(np.uint8)\n\n        cv2.imwrite(os.path.join(args.save_path, str(idx).zfill(4)+'.png'), cv2.cvtColor(face_white_bg, cv2.COLOR_RGB2BGR))\n"
        },
        {
          "name": "dataset.py",
          "type": "blob",
          "size": 3.5029296875,
          "content": "import torch.utils.data as data\r\n\r\nfrom PIL import Image\r\n\r\nimport os\r\nimport os.path\r\n\r\n\r\ndef has_file_allowed_extension(filename, extensions):\r\n    \"\"\"Checks if a file is an allowed extension.\r\n\r\n    Args:\r\n        filename (string): path to a file\r\n\r\n    Returns:\r\n        bool: True if the filename ends with a known image extension\r\n    \"\"\"\r\n    filename_lower = filename.lower()\r\n    return any(filename_lower.endswith(ext) for ext in extensions)\r\n\r\n\r\ndef find_classes(dir):\r\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\r\n    classes.sort()\r\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\r\n    return classes, class_to_idx\r\n\r\n\r\ndef make_dataset(dir, extensions):\r\n    images = []\r\n    for root, _, fnames in sorted(os.walk(dir)):\r\n        for fname in sorted(fnames):\r\n            if has_file_allowed_extension(fname, extensions):\r\n                path = os.path.join(root, fname)\r\n                item = (path, 0)\r\n                images.append(item)\r\n\r\n    return images\r\n\r\n\r\nclass DatasetFolder(data.Dataset):\r\n    def __init__(self, root, loader, extensions, transform=None, target_transform=None):\r\n        # classes, class_to_idx = find_classes(root)\r\n        samples = make_dataset(root, extensions)\r\n        if len(samples) == 0:\r\n            raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\r\n                               \"Supported extensions are: \" + \",\".join(extensions)))\r\n\r\n        self.root = root\r\n        self.loader = loader\r\n        self.extensions = extensions\r\n        self.samples = samples\r\n\r\n        self.transform = transform\r\n        self.target_transform = target_transform\r\n\r\n    def __getitem__(self, index):\r\n        \"\"\"\r\n        Args:\r\n            index (int): Index\r\n\r\n        Returns:\r\n            tuple: (sample, target) where target is class_index of the target class.\r\n        \"\"\"\r\n        path, target = self.samples[index]\r\n        sample = self.loader(path)\r\n        if self.transform is not None:\r\n            sample = self.transform(sample)\r\n        if self.target_transform is not None:\r\n            target = self.target_transform(target)\r\n\r\n        return sample, target\r\n\r\n    def __len__(self):\r\n        return len(self.samples)\r\n\r\n    def __repr__(self):\r\n        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\r\n        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\r\n        fmt_str += '    Root Location: {}\\n'.format(self.root)\r\n        tmp = '    Transforms (if any): '\r\n        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\r\n        tmp = '    Target Transforms (if any): '\r\n        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\r\n        return fmt_str\r\n\r\n\r\nIMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\r\n\r\n\r\ndef pil_loader(path):\r\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\r\n    with open(path, 'rb') as f:\r\n        img = Image.open(f)\r\n        return img.convert('RGB')\r\n\r\n\r\ndef default_loader(path):\r\n    return pil_loader(path)\r\n\r\n\r\nclass ImageFolder(DatasetFolder):\r\n    def __init__(self, root, transform=None, target_transform=None,\r\n                 loader=default_loader):\r\n        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\r\n                                          transform=transform,\r\n                                          target_transform=target_transform)\r\n        self.imgs = self.samples\r\n"
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "predict.py",
          "type": "blob",
          "size": 1.8154296875,
          "content": "import cog\nimport cv2\nimport tempfile\nimport torch\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom utils import Preprocess\nfrom models import ResnetGenerator\n\n\nclass Predictor(cog.Predictor):\n    def setup(self):\n        pass\n\n    @cog.input(\"photo\", type=Path, help=\"portrait photo (size < 1M)\")\n    def predict(self, photo):\n        img = cv2.cvtColor(cv2.imread(str(photo)), cv2.COLOR_BGR2RGB)\n        out_path = gen_cartoon(img)\n        return out_path\n\n\ndef gen_cartoon(img):\n    pre = Preprocess()\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    net = ResnetGenerator(ngf=32, img_size=256, light=True).to(device)\n\n    assert os.path.exists(\n        './models/photo2cartoon_weights.pt'), \"[Step1: load weights] Can not find 'photo2cartoon_weights.pt' in folder 'models!!!'\"\n    params = torch.load('./models/photo2cartoon_weights.pt', map_location=device)\n    net.load_state_dict(params['genA2B'])\n\n    # face alignment and segmentation\n    face_rgba = pre.process(img)\n    if face_rgba is None:\n        return None\n\n    face_rgba = cv2.resize(face_rgba, (256, 256), interpolation=cv2.INTER_AREA)\n    face = face_rgba[:, :, :3].copy()\n    mask = face_rgba[:, :, 3][:, :, np.newaxis].copy() / 255.\n    face = (face * mask + (1 - mask) * 255) / 127.5 - 1\n\n    face = np.transpose(face[np.newaxis, :, :, :], (0, 3, 1, 2)).astype(np.float32)\n    face = torch.from_numpy(face).to(device)\n\n    # inference\n    with torch.no_grad():\n        cartoon = net(face)[0][0]\n\n    # post-process\n    cartoon = np.transpose(cartoon.cpu().numpy(), (1, 2, 0))\n    cartoon = (cartoon + 1) * 127.5\n    cartoon = (cartoon * mask + 255 * (1 - mask)).astype(np.uint8)\n    cartoon = cv2.cvtColor(cartoon, cv2.COLOR_RGB2BGR)\n    out_path = Path(tempfile.mkdtemp()) / \"out.png\"\n    cv2.imwrite(str(out_path), cartoon)\n    return out_path\n"
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 2.3466796875,
          "content": "import os\nimport cv2\nimport torch\nimport numpy as np\nfrom models import ResnetGenerator\nimport argparse\nfrom utils import Preprocess\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--photo_path', type=str, help='input photo path')\nparser.add_argument('--save_path', type=str, help='cartoon save path')\nargs = parser.parse_args()\n\nos.makedirs(os.path.dirname(args.save_path), exist_ok=True)\n\nclass Photo2Cartoon:\n    def __init__(self):\n        self.pre = Preprocess()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.net = ResnetGenerator(ngf=32, img_size=256, light=True).to(self.device)\n        \n        assert os.path.exists('./models/photo2cartoon_weights.pt'), \"[Step1: load weights] Can not find 'photo2cartoon_weights.pt' in folder 'models!!!'\"\n        params = torch.load('./models/photo2cartoon_weights.pt', map_location=self.device)\n        self.net.load_state_dict(params['genA2B'])\n        print('[Step1: load weights] success!')\n\n    def inference(self, img):\n        # face alignment and segmentation\n        face_rgba = self.pre.process(img)\n        if face_rgba is None:\n            print('[Step2: face detect] can not detect face!!!')\n            return None\n        \n        print('[Step2: face detect] success!')\n        face_rgba = cv2.resize(face_rgba, (256, 256), interpolation=cv2.INTER_AREA)\n        face = face_rgba[:, :, :3].copy()\n        mask = face_rgba[:, :, 3][:, :, np.newaxis].copy() / 255.\n        face = (face*mask + (1-mask)*255) / 127.5 - 1\n\n        face = np.transpose(face[np.newaxis, :, :, :], (0, 3, 1, 2)).astype(np.float32)\n        face = torch.from_numpy(face).to(self.device)\n\n        # inference\n        with torch.no_grad():\n            cartoon = self.net(face)[0][0]\n\n        # post-process\n        cartoon = np.transpose(cartoon.cpu().numpy(), (1, 2, 0))\n        cartoon = (cartoon + 1) * 127.5\n        cartoon = (cartoon * mask + 255 * (1 - mask)).astype(np.uint8)\n        cartoon = cv2.cvtColor(cartoon, cv2.COLOR_RGB2BGR)\n        print('[Step3: photo to cartoon] success!')\n        return cartoon\n\n\nif __name__ == '__main__':\n    img = cv2.cvtColor(cv2.imread(args.photo_path), cv2.COLOR_BGR2RGB)\n    c2p = Photo2Cartoon()\n    cartoon = c2p.inference(img)\n    if cartoon is not None:\n        cv2.imwrite(args.save_path, cartoon)\n        print('Cartoon portrait has been saved successfully!')\n"
        },
        {
          "name": "test_onnx.py",
          "type": "blob",
          "size": 2.046875,
          "content": "import os\nimport cv2\nimport numpy as np\nimport onnxruntime\nimport argparse\nfrom utils import Preprocess\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--photo_path', type=str, help='input photo path')\nparser.add_argument('--save_path', type=str, help='cartoon save path')\nargs = parser.parse_args()\n\nos.makedirs(os.path.dirname(args.save_path), exist_ok=True)\n\nclass Photo2Cartoon:\n    def __init__(self):\n        self.pre = Preprocess()\n        \n        assert os.path.exists('./models/photo2cartoon_weights.onnx'), \"[Step1: load weights] Can not find 'photo2cartoon_weights.onnx' in folder 'models!!!'\"\n        self.session = onnxruntime.InferenceSession('./models/photo2cartoon_weights.onnx')\n        print('[Step1: load weights] success!')\n\n    def inference(self, img):\n        # face alignment and segmentation\n        face_rgba = self.pre.process(img)\n        if face_rgba is None:\n            print('[Step2: face detect] can not detect face!!!')\n            return None\n        \n        print('[Step2: face detect] success!')\n        face_rgba = cv2.resize(face_rgba, (256, 256), interpolation=cv2.INTER_AREA)\n        face = face_rgba[:, :, :3].copy()\n        mask = face_rgba[:, :, 3][:, :, np.newaxis].copy() / 255.\n        face = (face*mask + (1-mask)*255) / 127.5 - 1\n\n        face = np.transpose(face[np.newaxis, :, :, :], (0, 3, 1, 2)).astype(np.float32)\n\n        # inference\n        cartoon = self.session.run(['output'], input_feed={'input':face})\n\n        # post-process\n        cartoon = np.transpose(cartoon[0][0], (1, 2, 0))\n        cartoon = (cartoon + 1) * 127.5\n        cartoon = (cartoon * mask + 255 * (1 - mask)).astype(np.uint8)\n        cartoon = cv2.cvtColor(cartoon, cv2.COLOR_RGB2BGR)\n        print('[Step3: photo to cartoon] success!')\n        return cartoon\n\n\nif __name__ == '__main__':\n    img = cv2.cvtColor(cv2.imread(args.photo_path), cv2.COLOR_BGR2RGB)\n    c2p = Photo2Cartoon()\n    cartoon = c2p.inference(img)\n    if cartoon is not None:\n        cv2.imwrite(args.save_path, cartoon)\n        print('Cartoon portrait has been saved successfully!')\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.5810546875,
          "content": "from models import UgatitSadalinHourglass\r\nimport argparse\r\nimport shutil\r\nfrom utils import *\r\n\r\n\r\ndef parse_args():\r\n    \"\"\"parsing and configuration\"\"\"\r\n    desc = \"photo2cartoon\"\r\n    parser = argparse.ArgumentParser(description=desc)\r\n    parser.add_argument('--phase', type=str, default='train', help='[train / test]')\r\n    parser.add_argument('--light', type=str2bool, default=True, help='[U-GAT-IT full version / U-GAT-IT light version]')\r\n    parser.add_argument('--dataset', type=str, default='photo2cartoon', help='dataset name')\r\n\r\n    parser.add_argument('--iteration', type=int, default=1000000, help='The number of training iterations')\r\n    parser.add_argument('--batch_size', type=int, default=1, help='The size of batch size')\r\n    parser.add_argument('--print_freq', type=int, default=1000, help='The number of image print freq')\r\n    parser.add_argument('--save_freq', type=int, default=1000, help='The number of model save freq')\r\n    parser.add_argument('--decay_flag', type=str2bool, default=True, help='The decay_flag')\r\n\r\n    parser.add_argument('--lr', type=float, default=0.0001, help='The learning rate')\r\n    parser.add_argument('--adv_weight', type=int, default=1, help='Weight for GAN')\r\n    parser.add_argument('--cycle_weight', type=int, default=50, help='Weight for Cycle')\r\n    parser.add_argument('--identity_weight', type=int, default=10, help='Weight for Identity')\r\n    parser.add_argument('--cam_weight', type=int, default=1000, help='Weight for CAM')\r\n    parser.add_argument('--faceid_weight', type=int, default=1, help='Weight for Face ID')\r\n\r\n    parser.add_argument('--ch', type=int, default=32, help='base channel number per layer')\r\n    parser.add_argument('--n_dis', type=int, default=6, help='The number of discriminator layer')\r\n\r\n    parser.add_argument('--img_size', type=int, default=256, help='The size of image')\r\n    parser.add_argument('--img_ch', type=int, default=3, help='The size of image channel')\r\n\r\n    # parser.add_argument('--device', type=str, default='cuda:0', help='Set gpu mode: [cpu, cuda]')\r\n    parser.add_argument('--gpu_ids', type=int, default=[0], nargs='+', help='Set [0, 1, 2, 3] for multi-gpu training')\r\n    parser.add_argument('--benchmark_flag', type=str2bool, default=False)\r\n    parser.add_argument('--resume', type=str2bool, default=False)\r\n    parser.add_argument('--rho_clipper', type=float, default=1.0)\r\n    parser.add_argument('--w_clipper', type=float, default=1.0)\r\n    parser.add_argument('--pretrained_weights', type=str, default='', help='pretrained weight path')\r\n\r\n    args = parser.parse_args()\r\n    args.result_dir = './experiment/{}-size{}-ch{}-{}-lr{}-adv{}-cyc{}-id{}-identity{}-cam{}'.format(\r\n        os.path.basename(__file__)[:-3],\r\n        args.img_size,\r\n        args.ch,\r\n        args.light,\r\n        args.lr,\r\n        args.adv_weight,\r\n        args.cycle_weight,\r\n        args.faceid_weight,\r\n        args.identity_weight,\r\n        args.cam_weight)\r\n\r\n    return check_args(args)\r\n\r\n\r\ndef check_args(args):\r\n    check_folder(os.path.join(args.result_dir, args.dataset, 'model'))\r\n    check_folder(os.path.join(args.result_dir, args.dataset, 'img'))\r\n    check_folder(os.path.join(args.result_dir, args.dataset, 'test'))\r\n    shutil.copy(__file__, args.result_dir)\r\n    return args\r\n\r\n\r\ndef main():\r\n    args = parse_args()\r\n    if args is None:\r\n        exit()\r\n\r\n    gan = UgatitSadalinHourglass(args)\r\n    gan.build_model()\r\n\r\n    if args.phase == 'train':\r\n        gan.train()\r\n        print(\" [*] Training finished!\")\r\n\r\n    if args.phase == 'test':\r\n        gan.test()\r\n        print(\" [*] Test finished!\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}