{
  "metadata": {
    "timestamp": 1736559450941,
    "page": 3,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OpenGVLab/DragGAN",
      "stars": 4992,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.021484375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\ncheckpoints/\ntmp/"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.1630859375,
          "content": "FROM python:3.7\n\nWORKDIR /app\nCOPY . .\nEXPOSE 7860\n\nRUN pip install --no-cache-dir -r requirements.txt\n\nENTRYPOINT [ \"python\", \"-m\", \"draggan.web\", \"--ip\", \"0.0.0.0\"]\n"
        },
        {
          "name": "INSTALL.md",
          "type": "blob",
          "size": 3.69140625,
          "content": "# Installation\n\n- [System Requirements](#system-requirements)\n- [Install with PyPI](#install-with-pypi)\n- [Install Manually](#install-manually)\n- [Install with Docker](#install-with-docker)\n\n## System requirements\n\n- This implementation support running on CPU, Nvidia GPU, and Apple's m1/m2 chips. \n- When using with GPU, 8 GB memory is required for 1024 models. 6 GB is recommended for 512 models.\n\n\n## Install with PyPI\n\n📑 [Step by Step Tutorial](https://zeqiang-lai.github.io/blog/en/posts/drag_gan/) | [中文部署教程](https://zeqiang-lai.github.io/blog/posts/ai/drag_gan/)\n\nWe recommend to use Conda to install requirements.\n\n```bash\nconda create -n draggan python=3.7\nconda activate draggan\n```\n\nInstall PyTorch following the [official instructions](https://pytorch.org/get-started/locally/)\n```bash\nconda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia \n```\n\nInstall DragGAN\n```bash\npip install draggan\n# If you meet ERROR: Could not find a version that satisfies the requirement draggan (from versions: none), use\npip install draggan -i https://pypi.org/simple/\n```\n\nLaunch the Gradio demo\n\n```bash\n# if you have a Nvidia GPU\npython -m draggan.web\n# if you use m1/m2 mac\npython -m draggan.web --device mps\n# otherwise\npython -m draggan.web --device cpu\n```\n\n## Install Manually\n\nEnsure you have a GPU and CUDA installed. We use Python 3.7 for testing, other versions (>= 3.7) of Python should work too, but not tested. We recommend to use [Conda](https://conda.io/projects/conda/en/stable/user-guide/install/download.html) to prepare all the requirements.\n\nFor Windows users, you might encounter some issues caused by StyleGAN custom ops, youd could find some solutions from the [issues pannel](https://github.com/Zeqiang-Lai/DragGAN/issues). We are also working on a more friendly package without setup.\n\n```bash\ngit clone https://github.com/Zeqiang-Lai/DragGAN.git\ncd DragGAN\nconda create -n draggan python=3.7\nconda activate draggan\npip install -r requirements.txt\n```\n\nLaunch the Gradio demo\n\n```bash\n# if you have a Nvidia GPU\npython gradio_app.py\n# if you use m1/m2 mac\npython gradio_app.py --device mps\n# otherwise\npython gradio_app.py --device cpu\n```\n\n> If you have any issue for downloading the checkpoint, you could manually download it from [here](https://huggingface.co/aaronb/StyleGAN2/tree/main) and put it into the folder `checkpoints`.\n\n## Install with Docker\n\nFollow these steps to run DragGAN using Docker:\n\n### Prerequisites\n\n1. Install Docker on your system from the [official Docker website](https://www.docker.com/).\n2. Ensure that your system has [NVIDIA Docker support](https://github.com/NVIDIA/nvidia-docker) if you are using GPUs.\n\n### Run using docker Hub image\n\n```bash\n  # For GPU\n  docker run -t -p 7860:7860 --gpus all baydarov/draggan\n```\n\n```bash\n  # For CPU only (not recommended)\n  docker run -t -p 7860:7860 baydarov/draggan --device cpu\n```\n\n### Step-by-step Guide with building image locally\n\n1. Clone the DragGAN repository and build the Docker image:\n\n```bash\n   git clone https://github.com/Zeqiang-Lai/DragGAN.git # clone repo\n   cd DragGAN                                           # change into the repo directory\n   docker build -t draggan .                            # build image\n```\n\n2. Run the DragGAN Docker container:\n\n```bash\n  # For GPU\n  docker run -t -p 7860:7860 --gpus all draggan\n```\n\n```bash\n  # For CPU (not recommended)\n  docker run -t -p 7860:7860 draggan --device cpu\n```\n\n3. The DragGAN Web UI will be accessible once you see the following output in your console:\n\n```\n  ...\n  Running on local URL: http://0.0.0.0:7860\n  ...\n```\n\nVisit [http://localhost:7860](http://localhost:7860/) to access the Web UI.\n\nThat's it! You're now running DragGAN in a Docker container.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5,
          "content": "# DragGAN\n[![PyPI](https://img.shields.io/pypi/v/draggan)](https://pypi.org/project/draggan/) \n[![support](https://img.shields.io/badge/Support-macOS%20%7C%20Windows%20%7C%20Linux-blue)](#running-locally)\n\n:boom:  [`Colab Demo`](https://colab.research.google.com/github/Zeqiang-Lai/DragGAN/blob/master/colab.ipynb)  [`Awesome-DragGAN`](https://github.com/OpenGVLab/Awesome-DragGAN) [`InternGPT Demo`](https://github.com/OpenGVLab/InternGPT)  [`Local Deployment`](#running-locally)  \n\n> **Note for Colab, remember to select a GPU via `Runtime/Change runtime type` (`代码执行程序/更改运行时类型`).**\n> \n> If you want to upload custom image, please install 1.1.0 via `pip install draggan==1.1.0`.\n\n\nUnofficial implementation of [Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)\n\n<p float=\"left\">\n  <img src=\"assets/mouse.gif\" width=\"200\" />\n  <img src=\"assets/nose.gif\" width=\"200\" /> \n  <img src=\"assets/cat.gif\" width=\"200\" />\n  <img src=\"assets/horse.gif\" width=\"200\" />\n</p>\n\n## How it Work ?\n\n\nHere is a simple tutorial video showing how to use our implementation.\n\nhttps://github.com/Zeqiang-Lai/DragGAN/assets/26198430/f1516101-5667-4f73-9330-57fc45754283\n\nCheck out the original [paper](https://vcai.mpi-inf.mpg.de/projects/DragGAN/) for the backend algorithm and math.\n\n![demo](assets/paper.png)\n\n## News\n\n:star2: **What's New**\n\n- [2023/6/25] Relase version 1.1.1, it includes a major bug fix and speed improvement.\n- [2023/6/25] [Official Code](https://github.com/XingangPan/DragGAN) is released, check it out.\n- [2023/5/29] A new version is in beta, install via `pip install draggan==1.1.0b2`, includes speed improvement and more models.\n- [2023/5/25] DragGAN is on PyPI, simple install via `pip install draggan`. Also addressed the common CUDA problems https://github.com/Zeqiang-Lai/DragGAN/issues/38  https://github.com/Zeqiang-Lai/DragGAN/issues/12\n- [2023/5/25] We now support StyleGAN2-ada with much higher quality and more types of images. Try it by selecting models started with \"ada\".\n- [2023/5/24] An out-of-box online demo is integrated in [InternGPT](https://github.com/OpenGVLab/InternGPT) - a super cool pointing-language-driven visual interactive system. Enjoy for free.:lollipop:\n- [2023/5/24] Custom Image with GAN inversion is supported, but it is possible that your custom images are distorted  due to the limitation of GAN inversion. Besides, it is also possible the manipulations fail due to the limitation of our implementation.\n\n:star2: **Changelog**\n\n- [x] Add a docker image, thanks [@egbaydarov](https://github.com/egbaydarov).\n- [ ] PTI GAN inversion https://github.com/Zeqiang-Lai/DragGAN/issues/71#issuecomment-1573461314\n- [x] Tweak performance, See [v2](https://github.com/Zeqiang-Lai/DragGAN/tree/v2).\n- [x] Improving installation experience, DragGAN is now on [PyPI](https://pypi.org/project/draggan).\n- [x] Automatically determining the number of iterations, See [v2](https://github.com/Zeqiang-Lai/DragGAN/tree/v2).\n- [ ] Allow to save video without point annotations, custom image size.\n- [x] Support StyleGAN2-ada.\n- [x] Integrate into [InternGPT](https://github.com/OpenGVLab/InternGPT)\n- [x] Custom Image with GAN inversion.\n- [x] Download generated image and generation trajectory.\n- [x] Controlling generation process with GUI.\n- [x] Automatically download stylegan2 checkpoint.\n- [x] Support movable region, multiple handle points.\n- [x] Gradio and Colab Demo.\n\n> This project is now a sub-project of [InternGPT](https://github.com/OpenGVLab/InternGPT) for interactive image editing. Future updates of more cool tools beyond DragGAN would be added in [InternGPT](https://github.com/OpenGVLab/InternGPT). \n\n## Running Locally\n\nPlease refer to [INSTALL.md](INSTALL.md).\n\n\n## Citation\n\n```bibtex\n@inproceedings{pan2023draggan,\n    title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold}, \n    author={Pan, Xingang and Tewari, Ayush, and Leimk{\\\"u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},\n    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},\n    year={2023}\n}\n```\n\n\n## Acknowledgement\n\n[Official DragGAN](https://github.com/XingangPan/DragGAN) &ensp; [DragGAN-Streamlit](https://github.com/skimai/DragGAN) &ensp; [StyleGAN2](https://github.com/NVlabs/stylegan2)  &ensp; [StyleGAN2-pytorch](https://github.com/rosinality/stylegan2-pytorch)  &ensp; [StyleGAN2-Ada](https://github.com/NVlabs/stylegan2-ada-pytorch)   &ensp;  [StyleGAN-Human](https://github.com/stylegan-human/StyleGAN-Human) &ensp;  [Self-Distilled-StyleGAN](https://github.com/self-distilled-stylegan/self-distilled-internet-photos)\n\n Welcome to discuss with us and continuously improve the user experience of DragGAN.\nReach us with this WeChat QR Code. \n\n\n<p align=\"left\"><img width=\"300\" alt=\"image\" src=\"https://github.com/OpenGVLab/DragGAN/assets/26198430/885cb87a-4acc-490d-8a45-96f3ab870611\"><img width=\"300\" alt=\"image\" src=\"https://github.com/OpenGVLab/DragGAN/assets/26198430/e3f0807f-956a-474e-8fd2-1f7c22d73997\"></p> \n\n\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "colab.ipynb",
          "type": "blob",
          "size": 1.849609375,
          "content": "{\n \"cells\": [\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# DragGAN Colab Demo\\n\",\n    \"\\n\",\n    \"Wild implementation of [Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)\\n\",\n    \"\\n\",\n    \"**Note for Colab, remember to select a GPU via `Runtime/Change runtime type` (`代码执行程序/更改运行时类型`).**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#@title Installation\\n\",\n    \"!git clone https://github.com/Zeqiang-Lai/DragGAN.git\\n\",\n    \"\\n\",\n    \"import sys\\n\",\n    \"sys.path.append(\\\".\\\")\\n\",\n    \"sys.path.append('./DragGAN')\\n\",\n    \"\\n\",\n    \"!pip install -r DragGAN/requirements.txt\\n\",\n    \"\\n\",\n    \"from gradio_app import main\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"**If you have problem in the following demo, such as the incorrected image, or facing errors. Please try to run the following block again.**\\n\",\n    \"\\n\",\n    \"If the errors still exist, you could fire an issue on [Github](https://github.com/Zeqiang-Lai/DragGAN).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"demo = main()\\n\",\n    \"demo.queue(concurrency_count=1, max_size=20).launch()\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"torch1.10\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.12\"\n  },\n  \"orig_nbformat\": 4\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        },
        {
          "name": "draggan",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradio_app.py",
          "type": "blob",
          "size": 11.3623046875,
          "content": "import os\nimport gradio as gr\nimport torch\nimport numpy as np\nimport imageio\nfrom PIL import Image\nimport uuid\n\nfrom draggan import utils\nfrom draggan.draggan import drag_gan\nfrom draggan import draggan as draggan\n\ndevice = 'cuda'\n\n\nSIZE_TO_CLICK_SIZE = {\n    1024: 8,\n    512: 5,\n    256: 2\n}\n\nCKPT_SIZE = {\n    'stylegan2/stylegan2-ffhq-config-f.pkl': 1024,\n    'stylegan2/stylegan2-cat-config-f.pkl': 256,\n    'stylegan2/stylegan2-church-config-f.pkl': 256,\n    'stylegan2/stylegan2-horse-config-f.pkl': 256,\n    'ada/ffhq.pkl': 1024,\n    'ada/afhqcat.pkl': 512,\n    'ada/afhqdog.pkl': 512,\n    'ada/afhqwild.pkl': 512,\n    'ada/brecahad.pkl': 512,\n    'ada/metfaces.pkl': 512,\n    'human/stylegan_human_v2_512.pkl': 512,\n    'human/stylegan_human_v2_1024.pkl': 1024,\n    'self_distill/bicycles_256_pytorch.pkl': 256,\n    'self_distill/dogs_1024_pytorch.pkl': 1024,\n    'self_distill/elephants_512_pytorch.pkl': 512,\n    'self_distill/giraffes_512_pytorch.pkl': 512,\n    'self_distill/horses_256_pytorch.pkl': 256,\n    'self_distill/lions_512_pytorch.pkl': 512,\n    'self_distill/parrots_512_pytorch.pkl': 512,\n}\n\nDEFAULT_CKPT = 'ada/afhqcat.pkl'\n\n\ndef to_image(tensor):\n    tensor = tensor.squeeze(0).permute(1, 2, 0)\n    arr = tensor.detach().cpu().numpy()\n    arr = (arr - arr.min()) / (arr.max() - arr.min())\n    arr = arr * 255\n    return arr.astype('uint8')\n\n\ndef add_points_to_image(image, points, size=5):\n    image = utils.draw_handle_target_points(image, points['handle'], points['target'], size)\n    return image\n\n\ndef on_click(image, target_point, points, size, evt: gr.SelectData):\n    if target_point:\n        points['target'].append([evt.index[1], evt.index[0]])\n        image = add_points_to_image(image, points, size=SIZE_TO_CLICK_SIZE[size])\n        return image, not target_point\n    points['handle'].append([evt.index[1], evt.index[0]])\n    image = add_points_to_image(image, points, size=SIZE_TO_CLICK_SIZE[size])\n    return image, not target_point\n\n\ndef on_drag(model, points, max_iters, state, size, mask, lr_box):\n    if len(points['handle']) == 0:\n        raise gr.Error('You must select at least one handle point and target point.')\n    if len(points['handle']) != len(points['target']):\n        raise gr.Error('You have uncompleted handle points, try to selct a target point or undo the handle point.')\n    max_iters = int(max_iters)\n    W = state['W']\n\n    handle_points = [torch.tensor(p, device=device).float() for p in points['handle']]\n    target_points = [torch.tensor(p, device=device).float() for p in points['target']]\n\n    if mask.get('mask') is not None:\n        mask = Image.fromarray(mask['mask']).convert('L')\n        mask = np.array(mask) == 255\n\n        mask = torch.from_numpy(mask).float().to(device)\n        mask = mask.unsqueeze(0).unsqueeze(0)\n    else:\n        mask = None\n\n    step = 0\n    for image, W, handle_points in drag_gan(W, model['G'],\n                                            handle_points, target_points, mask,\n                                            max_iters=max_iters, lr=lr_box):\n        points['handle'] = [p.cpu().numpy().astype('int') for p in handle_points]\n        image = add_points_to_image(image, points, size=SIZE_TO_CLICK_SIZE[size])\n\n        state['history'].append(image)\n        step += 1\n        yield image, state, step\n\n\ndef on_reset(points, image, state):\n    return {'target': [], 'handle': []}, state['img'], False\n\n\ndef on_undo(points, image, state, size):\n    image = state['img']\n\n    if len(points['target']) < len(points['handle']):\n        points['handle'] = points['handle'][:-1]\n    else:\n        points['handle'] = points['handle'][:-1]\n        points['target'] = points['target'][:-1]\n\n    image = add_points_to_image(image, points, size=SIZE_TO_CLICK_SIZE[size])\n    return points, image, False\n\n\ndef on_change_model(selected, model):\n    size = CKPT_SIZE[selected]\n\n    G = draggan.load_model(utils.get_path(selected), device=device)\n    model = {'G': G}\n    W = draggan.generate_W(\n        G,\n        seed=int(1),\n        device=device,\n        truncation_psi=0.8,\n        truncation_cutoff=8,\n    )\n    img, _ = draggan.generate_image(W, G, device=device)\n\n    state = {\n        'W': W,\n        'img': img,\n        'history': []\n    }\n\n    return model, state, img, img, size\n\n\ndef on_new_image(model, seed):\n    G = model['G']\n    W = draggan.generate_W(\n        G,\n        seed=int(seed),\n        device=device,\n        truncation_psi=0.8,\n        truncation_cutoff=8,\n    )\n    img, _ = draggan.generate_image(W, G, device=device)\n\n    state = {\n        'W': W,\n        'img': img,\n        'history': []\n    }\n\n    points = {'target': [], 'handle': []}\n    target_point = False\n    return img, img, state, points, target_point\n\n\ndef on_max_iter_change(max_iters):\n    return gr.update(maximum=max_iters)\n\n\ndef on_save_files(image, state):\n    os.makedirs('draggan_tmp', exist_ok=True)\n    image_name = f'draggan_tmp/image_{uuid.uuid4()}.png'\n    video_name = f'draggan_tmp/video_{uuid.uuid4()}.mp4'\n    imageio.imsave(image_name, image)\n    imageio.mimsave(video_name, state['history'])\n    return [image_name, video_name]\n\n\ndef on_show_save():\n    return gr.update(visible=True)\n\n\ndef on_image_change(model, image_size, image):\n    image = Image.fromarray(image)\n    result = inverse_image(\n        model.g_ema,\n        image,\n        image_size=image_size\n    )\n    result['history'] = []\n    image = to_image(result['sample'])\n    points = {'target': [], 'handle': []}\n    target_point = False\n    return image, image, result, points, target_point\n\n\ndef on_mask_change(mask):\n    return mask['image']\n\n\ndef on_select_mask_tab(state):\n    img = to_image(state['sample'])\n    return img\n\n\ndef main():\n    torch.cuda.manual_seed(25)\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\n            \"\"\"\n            # DragGAN\n            \n            Unofficial implementation of [Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)\n            \n            [Our Implementation](https://github.com/Zeqiang-Lai/DragGAN) | [Official Implementation](https://github.com/XingangPan/DragGAN) (Not released yet)\n\n            ## Tutorial\n            \n            1. (Optional) Draw a mask indicate the movable region.\n            2. Setup a least one pair of handle point and target point.\n            3. Click \"Drag it\". \n            \n            ## Hints\n            \n            - Handle points (Blue): the point you want to drag.\n            - Target points (Red): the destination you want to drag towards to.\n            \n            ## Primary Support of Custom Image.\n            \n            - We now support dragging user uploaded image by GAN inversion.\n            - **Please upload your image at `Setup Handle Points` pannel.** Upload it from `Draw a Mask` would cause errors for now.\n            - Due to the limitation of GAN inversion, \n                - You might wait roughly 1 minute to see the GAN version of the uploaded image.\n                - The shown image might be slightly difference from the uploaded one.\n                - It could also fail to invert the uploaded image and generate very poor results.\n                - Idealy, you should choose the closest model of the uploaded image. For example, choose `stylegan2-ffhq-config-f.pkl` for human face. `stylegan2-cat-config-f.pkl` for cat.\n                \n            > Please fire an issue if you have encounted any problem. Also don't forgot to give a star to the [Official Repo](https://github.com/XingangPan/DragGAN), [our project](https://github.com/Zeqiang-Lai/DragGAN) could not exist without it.\n            \"\"\",\n        )\n        G = draggan.load_model(utils.get_path(DEFAULT_CKPT), device=device)\n        model = gr.State({'G': G})\n        W = draggan.generate_W(\n            G,\n            seed=int(1),\n            device=device,\n            truncation_psi=0.8,\n            truncation_cutoff=8,\n        )\n        img, F0 = draggan.generate_image(W, G, device=device)\n\n        state = gr.State({\n            'W': W,\n            'img': img,\n            'history': []\n        })\n        points = gr.State({'target': [], 'handle': []})\n        size = gr.State(CKPT_SIZE[DEFAULT_CKPT])\n        target_point = gr.State(False)\n\n        with gr.Row():\n            with gr.Column(scale=0.3):\n                with gr.Accordion(\"Model\"):\n                    model_dropdown = gr.Dropdown(choices=list(CKPT_SIZE.keys()), value=DEFAULT_CKPT,\n                                                 label='StyleGAN2 model')\n                    seed = gr.Number(value=1, label='Seed', precision=0)\n                    new_btn = gr.Button('New Image')\n                with gr.Accordion('Drag'):\n                    with gr.Row():\n                        lr_box = gr.Number(value=2e-3, label='Learning Rate')\n                        max_iters = gr.Slider(1, 500, 20, step=1, label='Max Iterations')\n\n                    with gr.Row():\n                        with gr.Column(min_width=100):\n                            reset_btn = gr.Button('Reset All')\n                        with gr.Column(min_width=100):\n                            undo_btn = gr.Button('Undo Last')\n                    with gr.Row():\n                        btn = gr.Button('Drag it', variant='primary')\n\n                with gr.Accordion('Save', visible=False) as save_panel:\n                    files = gr.Files(value=[])\n\n                progress = gr.Slider(value=0, maximum=20, label='Progress', interactive=False)\n\n            with gr.Column():\n                with gr.Tabs():\n                    with gr.Tab('Setup Handle Points', id='input'):\n                        image = gr.Image(img).style(height=512, width=512)\n                    with gr.Tab('Draw a Mask', id='mask') as masktab:\n                        mask = gr.ImageMask(img, label='Mask').style(height=512, width=512)\n\n        image.select(on_click, [image, target_point, points, size], [image, target_point])\n        image.upload(on_image_change, [model, size, image], [image, mask, state, points, target_point])\n        mask.upload(on_mask_change, [mask], [image])\n        btn.click(on_drag, inputs=[model, points, max_iters, state, size, mask, lr_box], outputs=[image, state, progress]).then(\n            on_show_save, outputs=save_panel).then(\n            on_save_files, inputs=[image, state], outputs=[files]\n        )\n        reset_btn.click(on_reset, inputs=[points, image, state], outputs=[points, image, target_point])\n        undo_btn.click(on_undo, inputs=[points, image, state, size], outputs=[points, image, target_point])\n        model_dropdown.change(on_change_model, inputs=[model_dropdown, model], outputs=[model, state, image, mask, size])\n        new_btn.click(on_new_image, inputs=[model, seed], outputs=[image, mask, state, points, target_point])\n        max_iters.change(on_max_iter_change, inputs=max_iters, outputs=progress)\n        masktab.select(lambda: gr.update(value=None), outputs=[mask]).then(on_select_mask_tab, inputs=[state], outputs=[mask])\n    return demo\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--device', default='cuda')\n    parser.add_argument('--share', action='store_true')\n    parser.add_argument('-p', '--port', type=int, default=None)\n    parser.add_argument('--ip', default=None)\n    args = parser.parse_args()\n    device = args.device\n    demo = main()\n    print('Successfully loaded, starting gradio demo')\n    demo.queue(concurrency_count=1, max_size=20).launch(share=args.share, server_name=args.ip, server_port=args.port)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.095703125,
          "content": "gradio==3.34.0\ntqdm\ntorch\ntorchvision\nnumpy\nninja\nfire\nimageio\nimageio-ffmpeg\nscikit-image\nIPython"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.0732421875,
          "content": "from setuptools import setup, find_packages\n\nsetup(\n    name='draggan',\n    packages=find_packages(),\n    version='1.1.6',\n    package_data={\n        'draggan': ['deprecated/stylegan2/op/fused_bias_act.cpp', \n                    'deprecated/stylegan2/op/upfirdn2d.cpp',\n                    'deprecated/stylegan2/op/fused_bias_act_kernel.cu',\n                    'deprecated/stylegan2/op/upfirdn2d_kernel.cu',\n                    'stylegan2/torch_utils/ops/bias_act.cpp', \n                    'stylegan2/torch_utils/ops/upfirdn2d.cpp',\n                    'stylegan2/torch_utils/ops/bias_act.cu',\n                    'stylegan2/torch_utils/ops/upfirdn2d.cu',\n                    'stylegan2/torch_utils/ops/bias_act.h', \n                    'stylegan2/torch_utils/ops/upfirdn2d.h', \n                    ], \n    },\n    include_package_data=True,\n    install_requires=[\n        'gradio==3.34.0',\n        'tqdm',\n        'torch>=1.8',\n        'torchvision',\n        'numpy',\n        'ninja',\n        'fire',\n        'imageio',\n        'imageio-ffmpeg',\n        'scikit-image',\n        'IPython',\n    ]\n)\n"
        }
      ]
    }
  ]
}