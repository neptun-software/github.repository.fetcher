{
  "metadata": {
    "timestamp": 1736559537568,
    "page": 134,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVIDIA-AI-IOT/torch2trt",
      "stars": 4644,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2001953125,
          "content": ".ninja_deps\n.ninja_log\nbuild.ninja\ntags\n*.o\n*.pb.o\ntorch2trt.egg-info\nbuild/\ndist/\n__pycache__/\n*.so\n*.pb.h\n*.pb.cc\n*_pb2.py\n*.pyc\n*.ipynb_checkpoints\n*.pth\ndocs/converters.md\nsite\nToJetsonGrp\n.vscode\ndata"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 3.025390625,
          "content": "# Changes\n\n## [master](https://github.com/NVIDIA-AI-IOT/torch2trt/tree/master)\n\n- Added inference and conversion support for TensorRT 10\n- Removed redundant converters, and merged converters for ND convolutions, pooling, etc.\n- Migrated test cases to use PyTest\n- Added unique axis names when using ONNX to support mis-matched dynamic axes (needed for whisper)\n\n## [v0.5.0](https://github.com/NVIDIA-AI-IOT/torch2trt/tree/v0.5.0) - 05/3/2024\n\n- Added tensor shape tracking to support dynamic shapes for flatten, squeeze, unsqueeze, view, reshape, interpolate, and getitem methods\n- Added EasyOCR example\n- Added the ``DatasetRecorder`` context manager, allowing to easily capture of module inputs in large pipeline for calibration and shape inference\n- Added support for legacy max_batch_size using optimization profiles\n- Added support for nested tuple, dict and list module inputs and outputs via. the ``Flattener`` class\n- Added ability to accept dataset as ``inputs`` argument, and infer optimization profiles from the data\n- Added Dataset, TensorBatchDataset, ListDataset, and FolderDatset classes\n- Added support for dynamic shapes\n  - Known limitation: Currently some converters (ie: View) may have unexpected behavior if their arguments are defined with dynamic Tensor shapes.\n\n## [0.4.0](https://github.com/NVIDIA-AI-IOT/torch2trt/tree/v0.4.0) - 07/22/2022\n\n- Added converter for ``torch.nn.functional.group_norm`` using native TensorRT layers\n- Added converter for ``torch.nn.ReflectionPad2d`` using plugin layer\n- Added torch2trt_plugins library\n- Added support for Deep Learning Accelerator (DLA)\n- Added support for explicit batch\n- Added support for TensorRT 8\n\n## [0.3.0](https://github.com/NVIDIA-AI-IOT/torch2trt/tree/v0.3.0) - 07/15/2021\n\n- Added converter for ``torch.nn.functional.adaptive_avg_pool3d``\n- Added converter for ``torch.nn.functional.adaptive_max_pool3d``\n- Added converter for ``torch.maxpool3d`` and ``torch.nn.functional.max_pool3d``\n- Added Quantization Aware Training (QAT) workflow to contrib\n- Added converter for ``torch.roll``\n- Added converter for ``torch.nn.functional.layer_norm``\n- Added converter for ``torch.nn.functional.gelu``\n- Added converter for ``torch.nn.functional.linear``\n- Added converter for ``torch.nn.functional.silu``\n\n## [0.2.0](https://github.com/NVIDIA-AI-IOT/torch2trt/tree/v0.2.0) - 03/02/2021\n\n- Added converter for ``torch.Tensor.flatten``\n- Added converter for ``torch.nn.functional.conv2d`` and ``torch.nn.functional.conv3d``\n- Added converter for ``torch.Tensor.expand``\n- Added support for custom converters for methods defined outside of ``torch`` module\n- Added names for TensorRT layers\n- Added GroupNorm plugin which internally uses PyTorch aten::group_norm\n- Replaced Tensor.ndim references with len(tensor.shape) to support older pytorch versions\n- Added reduced precision documentation page\n- Added converters for ``floordiv``, ``mod``, ``ne``, and ``torch.tensor`` operations\n- Extended ``relu`` converter to support ``Tensor.relu`` operation\n- Extended ``sigmoid`` converter to support ``Tensor.sigmoid`` operation\n"
        },
        {
          "name": "CLA.pdf",
          "type": "blob",
          "size": 34.318359375,
          "content": null
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 1.3154296875,
          "content": "cmake_minimum_required(VERSION 3.0.0)\nproject(torch2trt_plugins VERSION 0.1.0)\n\n# VARIABLES\nset(CUDA_ARCHITECTURES 53 62 72 87)\n\n# BUILD PLUGINS LIBRARY\nfind_package(CUDA REQUIRED)\n\nenable_language(CUDA)\n\ninclude_directories(\"${CUDA_INCLUDE_DIRS}\")\n\nadd_library(torch2trt_plugins SHARED\n    plugins/src/example_plugin.cu\n    plugins/src/reflection_pad_2d_plugin.cu\n)\nset_property(TARGET torch2trt_plugins PROPERTY CUDA_ARCHITECTURES ${CUDA_ARCHITECTURES})\n\ntarget_link_libraries(\n    torch2trt_plugins\n    nvinfer\n    ${CUDA_LIBRARIES}\n)\n\ninstall (TARGETS torch2trt_plugins\n         LIBRARY DESTINATION lib)\n\n# BUILD TESTS\nfind_package(Catch2 QUIET)\n\nif(Catch2_FOUND)\n    include(CTest)\n    include(CPack)\n    include(Catch)\n    enable_testing()\n\n    add_executable(torch2trt_plugins_test\n        plugins/src/tests.cpp\n        plugins/src/example_plugin_test.cpp\n        plugins/src/reflection_pad_2d_plugin_test.cpp\n    )\n\n    set_property(TARGET torch2trt_plugins_test PROPERTY CUDA_ARCHITECTURES ${CUDA_ARCHITECTURES})\n\n    target_link_libraries(torch2trt_plugins_test \n        PRIVATE \n        Catch2::Catch2WithMain \n        torch2trt_plugins\n        nvinfer\n        ${CUDA_LIBRARIES}\n    )\n\n    set(CPACK_PROJECT_NAME ${PROJECT_NAME})\n    set(CPACK_PROJECT_VERSION ${PROJECT_VERSION})\n    catch_discover_tests(torch2trt_plugins_test)\nendif()"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.6796875,
          "content": "# Contributing\n\n## Forms of contribution\n\n### Submit an Issue\n\ntorch2trt is use case driven.  We originally created it to solve\nuse cases related to NVIDIA Jetson, but the layer support has grown\nlargely since it's release and we've found that it has \nhelped many other developers as well.  \n\nThe growth of torch2trt has been largely driven by issues submitted on [GitHub](https://github.com/NVIDIA-AI-IOT/torch2trt/issues).\nWe learn a lot from the reported issues. Submitting an issue it is one of the best ways to begin contributing to torch2trt.\n\nThe reported issues typically are one of the following,\n\n* A bug or unexpected result\n* A model with unsupported layers\n\nIf you report an issue, we typically find the following information helpful\n\n* PyTorch version\n* TensorRT version\n* Platform (ie: Jetson Nano)\n* The PyTorch Module you're attempting to convert\n* The steps taken to convert the PyTorch module\n\nIf you're not sure how to provide any of these pieces of information, don't worry.  Just open the issue\nand we're happy to discuss and help work out the details.\n\n### Ask a Question\n\nAnother great way to contribute is to ask a question on [GitHub](https://github.com/NVIDIA-AI-IOT/torch2trt/issues).\nThere are often other developers who share your question, and they may find the discussion helpful.  This also\nhelps us gauge feature interest and identify gaps in documentation.\n\n### Submit a Pull Request\n\ntorch2trt is use case driven and has limited maintainence, for this reason we value community contributions greatly.\nAnother great way to contribute is by submitting a pull request.  Pull requests which are most likely to be accepted are\n\n* A new converter\n* A test case\n* A bug fix\n\nIf you add a new converter, it is best to include a few test\ncases that cross validate the converter against the original PyTorch.  We provide a utility function to do this,\nas described in the [Custom Converter](usage/custom_converter.md) usage guide.\n\nIdeally pull requests solve one thing at a time.  This makes it easy\nto evaluate the impact that the changes have on the project step-by-step.  The more confident we are that\nthe changes will not adversely impact the experience of other developers, the more likely we are to accept them.\n\n## Running module test cases\n\nBefore any change is accepted, we run the test cases on at least one platform.  This performs a large number\nof cross validation checks against PyTorch.  To do this\n\n```bash\npython3 -m torch2trt.test --name=converters --tolerance=1e-2\n```\n\nThis will not hard-fail, but will highlight any build errors or max error checks.  It is helpful if you include\nthe status of this command in any pull-request, as well as system information like\n\n* PyTorch version\n* TensorRT version\n* Platform (ie: Jetson Nano)\n\n## Testing documentation\n\nIf you have a change that modifies the documentation, it is relatively straightforward to test.  We\nuse ``mkdocs-material`` for documentation, which parses markdown files in the ``docs`` folder.\n\nTo view the docs, simply call\n\n```\n./scripts/test_docs.sh\n```\n\nAnd then navigate to ``https://<ip_address>:8000``.\n\nPlease note, this will not include dynamically generated documentation pages like the converters page.\nThese contain cross reference links to the GitHub source code. If you want to test these\nyou can call \n    \n```bash\n./scripts/build_docs.sh <github url> <tag>\n```\n    \nPointing to the public reflection\nof your local repository.  For example, if we're working off the upstream master branch, we\nwould call \n   \n```bash\n./scripts/build_docs.sh https://github.com/NVIDIA-AI-IOT/torch2trt master\n```\n    \nIf your changes are pushed to your fork, you would do \n   \n```bash\n./scripts/build_docs.sh https://github.com/<user>/torch2trt my_branch\n```\n    \n"
        },
        {
          "name": "CONTRIBUTORS.md",
          "type": "blob",
          "size": 1.4814453125,
          "content": "# Contributors\n\nBelow is a list of developers who have contributed to torch2trt.  This is also used to track contributors\nwho have agreed to torch2trt's Contributor License Agreement.\n\n- [John Welsh](https://github.com/jaybdub) (CLA)\n- John Welsh\n\n## Becoming a Contributor\n\nIf you've made a notable contribution to torch2trt and wish to be listed as a contributor, simply do the following.\n\n1. Modify ``CONTRIBUTORS.md`` and add your name with a hyperlink to your GitHub account to the end of the contributors list.\n\n    ```md\n    - [<Full name or GitHub account>](https://github.com/<GitHub account>)\n    ```\n\n2. Stage the changes in a standalone commit\n\n    ```md\n    git add CONTRIBUTORS.md\n    ```\n\n3. Make a signed commit with the following message text\n\n    ```md\n    git commit -m \"Added <Full name or GitHub account> to CONTRIBUTORS.md\"\n    ```\n\n## Signing Contributor License Agreement (CLA)\n\nIn some instances, you may be requested to sign torch2trt's Contributor License Agreement (CLA). To do so,\n\n1. If you're not already listed as a contributor in CONTRIBUTORS.md, make a commit as described above to add yourself to CONTRIBUTORS.md\n\n2. Add the text ``(CLA)`` after your name in ``CONTRIBUTORS.md``\n3. Stage the changes in a standalone commit\n\n   ```md\n   git add CONTRIBUTORS.md\n   ```\n4. Make a signed commit with the following text\n\n   ```md\n   git commit -S -m \"I have read and agree to the Contributor License Agreement as written in the file CLA.md of this project.  Signed, <Full Name>\"\n   ```\n\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.0595703125,
          "content": "Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.47265625,
          "content": "# torch2trt\n\n> What models are you using, or hoping to use, with TensorRT?  Feel free to join the discussion [here](https://github.com/NVIDIA-AI-IOT/torch2trt/discussions/531).\n \n<a href=\"https://nvidia-ai-iot.github.io/torch2trt\"><img src=\"https://img.shields.io/badge/-Documentation-brightgreen\"/></a>\n\ntorch2trt is a PyTorch to TensorRT converter which utilizes the \nTensorRT Python API.  The converter is\n\n* Easy to use - Convert modules with a single function call ``torch2trt``\n\n* Easy to extend - Write your own layer converter in Python and register it with ``@tensorrt_converter``\n\nIf you find an issue, please [let us know](../..//issues)!\n\n> Please note, this converter has limited coverage of TensorRT / PyTorch.  We created it primarily\n> to easily optimize the models used in the [JetBot](https://github.com/NVIDIA-AI-IOT/jetbot) project.  If you find the converter helpful with other models, please [let us know](../..//issues).\n\n## Usage\n\nBelow are some usage examples, for more check out the [notebooks](notebooks).\n\n### Convert\n\n```python\nimport torch\nfrom torch2trt import torch2trt\nfrom torchvision.models.alexnet import alexnet\n\n# create some regular pytorch model...\nmodel = alexnet(pretrained=True).eval().cuda()\n\n# create example data\nx = torch.ones((1, 3, 224, 224)).cuda()\n\n# convert to TensorRT feeding sample data as input\nmodel_trt = torch2trt(model, [x])\n```\n\n### Execute\n\nWe can execute the returned ``TRTModule`` just like the original PyTorch model\n\n```python\ny = model(x)\ny_trt = model_trt(x)\n\n# check the output against PyTorch\nprint(torch.max(torch.abs(y - y_trt)))\n```\n\n### Save and load\n\nWe can save the model as a ``state_dict``.\n\n```python\ntorch.save(model_trt.state_dict(), 'alexnet_trt.pth')\n```\n\nWe can load the saved model into a ``TRTModule``\n\n```python\nfrom torch2trt import TRTModule\n\nmodel_trt = TRTModule()\n\nmodel_trt.load_state_dict(torch.load('alexnet_trt.pth'))\n```\n\n## Models\n\nWe tested the converter against these models using the [test.sh](test.sh) script.  You can generate the results by calling\n\n```bash\n./test.sh TEST_OUTPUT.md\n```\n\n> The results below show the throughput in FPS.  You can find the raw output, which includes latency, in the [benchmarks folder](benchmarks).\n\n| Model | Nano (PyTorch) | Nano (TensorRT) | Xavier (PyTorch) | Xavier (TensorRT) |\n|-------|:--------------:|:---------------:|:----------------:|:-----------------:|\n| alexnet | 46.4 | 69.9 | 250 | 580 |\n| squeezenet1_0 | 44 | 137 | 130 | 890 |\n| squeezenet1_1 | 76.6 | 248 | 132 | 1390 |\n| resnet18 | 29.4 | 90.2 | 140 | 712 |\n| resnet34 | 15.5 | 50.7 | 79.2 | 393 |\n| resnet50 | 12.4 | 34.2 | 55.5 | 312 |\n| resnet101 | 7.18 | 19.9 | 28.5 | 170 |\n| resnet152 | 4.96 | 14.1 | 18.9 | 121 |\n| densenet121 | 11.5 | 41.9 | 23.0 | 168 |\n| densenet169 | 8.25 | 33.2 | 16.3 | 118 |\n| densenet201 | 6.84 | 25.4 | 13.3 | 90.9 |\n| densenet161 | 4.71 | 15.6 | 17.2 | 82.4 |\n| vgg11 | 8.9 | 18.3 | 85.2 | 201 |\n| vgg13 | 6.53 | 14.7 | 71.9 | 166 |\n| vgg16 | 5.09 | 11.9 | 61.7 | 139 |\n| vgg19 |  |  | 54.1 | 121 |\n| vgg11_bn | 8.74 | 18.4 | 81.8 | 201 |\n| vgg13_bn | 6.31 | 14.8 | 68.0 | 166 |\n| vgg16_bn | 4.96 | 12.0 | 58.5 | 140 |\n| vgg19_bn |  |  | 51.4 | 121 |\n\n\n## Setup\n\n> Note: torch2trt depends on the TensorRT Python API.  On Jetson, this is included with the latest JetPack.  For desktop, please follow the [TensorRT Installation Guide](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html).  You may also try installing torch2trt inside one of the NGC PyTorch docker containers for [Desktop](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch) or [Jetson](https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch).\n\n### Step 1 - Install the torch2trt Python library\n\nTo install the torch2trt Python library, call the following\n\n```bash\ngit clone https://github.com/NVIDIA-AI-IOT/torch2trt\ncd torch2trt\npython setup.py install\n```\n\n### Step 2 (optional) - Install the torch2trt plugins library\n\nTo install the torch2trt plugins library, call the following\n\n```bash\ncmake -B build . && cmake --build build --target install && ldconfig\n```\n\nThis includes support for some layers which may not be supported natively by TensorRT.  Once this library is found in the system, the associated layer converters in torch2trt are implicitly enabled.\n\n> Note: torch2trt now maintains plugins as an independent library compiled with CMake.  This makes compiled TensorRT engines more portable.  If needed, the deprecated plugins (which depend on PyTorch) may still be installed by calling ``python setup.py install --plugins``.\n\n### Step 3 (optional) - Install experimental community contributed features\n\nTo install torch2trt with experimental community contributed features under ``torch2trt.contrib``, like Quantization Aware Training (QAT)(`requires TensorRT>=7.0`), call the following,      \n\n```bash\ngit clone https://github.com/NVIDIA-AI-IOT/torch2trt\ncd torch2trt/scripts    \nbash build_contrib.sh   \n```\n  \nThis enables you to run the QAT example located [here](examples/contrib/quantization_aware_training).   \n    \n\n## How does it work?\n\nThis converter works by attaching conversion functions (like ``convert_ReLU``) to the original \nPyTorch functional calls (like ``torch.nn.ReLU.forward``).  The sample input data is passed\nthrough the network, just as before, except now whenever a registered function (``torch.nn.ReLU.forward``)\nis encountered, the corresponding converter (``convert_ReLU``) is also called afterwards.  The converter\nis passed the arguments and return statement of the original PyTorch function, as well as the TensorRT\nnetwork that is being constructed.  The input tensors to the original PyTorch function are modified to\nhave an attribute ``_trt``, which is the TensorRT counterpart to the PyTorch tensor.  The conversion function\nuses this ``_trt`` to add layers to the TensorRT network, and then sets the ``_trt`` attribute for\nrelevant output tensors.  Once the model is fully executed, the final tensors returns are marked as outputs\nof the TensorRT network, and the optimized TensorRT engine is built.\n\n## How to add (or override) a converter\n\nHere we show how to add a converter for the ``ReLU`` module using the TensorRT\npython API.\n\n```python\nimport tensorrt as trt\nfrom torch2trt import tensorrt_converter\n\n@tensorrt_converter('torch.nn.ReLU.forward')\ndef convert_ReLU(ctx):\n    input = ctx.method_args[1]\n    output = ctx.method_return\n    layer = ctx.network.add_activation(input=input._trt, type=trt.ActivationType.RELU)  \n    output._trt = layer.get_output(0)\n```\n\nThe converter takes one argument, a ``ConversionContext``, which will contain\nthe following\n\n* ``ctx.network`` - The TensorRT network that is being constructed.\n\n* ``ctx.method_args`` - Positional arguments that were passed to the specified PyTorch function.  The ``_trt`` attribute is set for relevant input tensors.\n* ``ctx.method_kwargs`` - Keyword arguments that were passed to the specified PyTorch function.\n* ``ctx.method_return`` - The value returned by the specified PyTorch function.  The converter must set the ``_trt`` attribute where relevant.\n\nPlease see [this folder](torch2trt/converters) for more examples.\n\n## See also\n\n- [JetBot](http://github.com/NVIDIA-AI-IOT/jetbot) - An educational AI robot based on NVIDIA Jetson Nano\n\n- [JetRacer](http://github.com/NVIDIA-AI-IOT/jetracer) - An educational AI racecar using NVIDIA Jetson Nano\n- [JetCam](http://github.com/NVIDIA-AI-IOT/jetcam) - An easy to use Python camera interface for NVIDIA Jetson\n- [JetCard](http://github.com/NVIDIA-AI-IOT/jetcard) - An SD card image for web programming AI projects with NVIDIA Jetson Nano\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.py",
          "type": "blob",
          "size": 1.8955078125,
          "content": "import imp\nimport subprocess\nimport os\nfrom string import Template\n\nPLUGINS = [\n    'interpolate',\n    'group_norm',\n]\n\nBASE_FOLDER = 'torch2trt/converters'\n\nNINJA_TEMPLATE = Template((\n    \"rule link\\n\"\n    \"  command = g++ -shared -o $$out $$in -L$torch_dir/lib -L$cuda_dir/lib64 -L$trt_lib_dir -lc10 -lc10_cuda -ltorch -lcudart -lprotobuf -lprotobuf-lite -pthread -lpthread -lnvinfer\\n\"\n    \"rule protoc\\n\"\n    \"  command = protoc $$in --cpp_out=. --python_out=.\\n\"\n    \"rule cxx\\n\"\n    \"  command = g++ -c -fPIC $$in -I$cuda_dir/include -I$torch_dir/include -I$torch_dir/include/torch/csrc/api/include -I. -std=c++11 -I$trt_inc_dir\\n\"\n))\n\nPLUGIN_TEMPLATE = Template((\n    \"build $plugin_dir/$plugin.pb.h $plugin_dir/$plugin.pb.cc $plugin_dir/${plugin}_pb2.py: protoc $plugin_dir/$plugin.proto\\n\"\n    \"build $plugin.pb.o: cxx $plugin_dir/$plugin.pb.cc\\n\"\n    \"build $plugin.o: cxx $plugin_dir/$plugin.cpp\\n\"\n))\n\n\ndef build(cuda_dir=\"/usr/local/cuda\",\n          torch_dir=imp.find_module('torch')[1],\n          trt_inc_dir=\"/usr/include/aarch64-linux-gnu\",\n          trt_lib_dir=\"/usr/lib/aarch64-linux-gnu\"):\n\n    global PLUGINS, BASE_FOLDER, NINJA_TEMPLATE, PLUGIN_TEMPLATE\n\n    NINJA_STR = NINJA_TEMPLATE.substitute({\n        'torch_dir': torch_dir,\n        'cuda_dir': cuda_dir,\n        'trt_inc_dir': trt_inc_dir,\n        'trt_lib_dir': trt_lib_dir,\n    })\n\n\n    plugin_o_files = []\n    for plugin in PLUGINS:\n        NINJA_STR += \\\n            PLUGIN_TEMPLATE.substitute({\n                'plugin': plugin,\n                'plugin_dir': os.path.join(BASE_FOLDER, plugin),\n            })\n        plugin_o_files += [plugin + '.pb.o', plugin + '.o']\n\n    NINJA_STR += Template((\n        \"build torch2trt/libtorch2trt.so: link $o_files\\n\"\n    )).substitute({'o_files': ' '.join(plugin_o_files)})\n\n    with open('build.ninja', 'w') as f:\n        f.write(NINJA_STR)\n\n    subprocess.call(['ninja'])\n\n\nif __name__ == '__main__':\n    build()\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 1.1416015625,
          "content": "site_name: torch2trt\ntheme:\n    name: \"material\"\n    palette:\n        primary: green\n        secondary: light green\n\nrepo_url: https://github.com/NVIDIA-AI-IOT/torch2trt\n\nplugins:\n  - search\n  \nuse_directory_urls: False\n\nedit_uri: blob/master/docs\nmarkdown_extensions:\n  - pymdownx.tabbed\n  - pymdownx.keys\n  - pymdownx.snippets\n  - pymdownx.inlinehilite\n  - pymdownx.highlight:\n      use_pygments: true\n  - admonition\n  - pymdownx.details\n  - pymdownx.superfences\n  - attr_list\n  \n# use_directory_urls - False to fix broken raw html image links\n# https://github.com/mkdocs/mkdocs/issues/991\n\n\nnav:\n\n  - Home: index.md\n  - Getting Started: getting_started.md\n  - Usage:\n      - Basic Usage: usage/basic_usage.md\n      - Reduced Precision: usage/reduced_precision.md\n      - Custom Converter: usage/custom_converter.md\n  - Converters: converters.md\n  - Benchmarks: \n      - Jetson Nano: benchmarks/jetson_nano.md\n      - Jetson Xavier: benchmarks/jetson_xavier.md\n  - Contributing: CONTRIBUTING.md\n  - See Also: see_also.md\n\nextra_css:\n    - css/version-select.css\nextra_javascript:\n    - js/version-select.js\n    \ngoogle_analytics:\n    - UA-135919510-3\n    - auto\n    \n"
        },
        {
          "name": "plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.46875,
          "content": "import sys\nimport tensorrt\nimport torch\nfrom setuptools import setup, find_packages\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension, CppExtension\nfrom packaging import version\n\n\ndef trt_inc_dir():\n    return \"/usr/include/aarch64-linux-gnu\"\n\ndef trt_lib_dir():\n    return \"/usr/lib/aarch64-linux-gnu\"\n\next_modules = []\nexclude_dir = [\"torch2trt/contrib\",\"torch2trt/contrib.*\"]\n\ncompile_args_cxx = []\nif version.parse(torch.__version__) < version.parse('1.5'):\n    compile_args_cxx.append('-DUSE_DEPRECATED_INTLIST')\nif version.parse(tensorrt.__version__) < version.parse('8'):\n    compile_args_cxx.append('-DPRE_TRT8')\n\nplugins_ext_module = CUDAExtension(\n        name='plugins',\n        sources=[\n            'torch2trt/plugins/plugins.cpp'\n        ],\n        include_dirs=[\n            trt_inc_dir()\n        ],\n        library_dirs=[\n            trt_lib_dir()\n        ],\n        libraries=[\n            'nvinfer'\n        ],\n        extra_compile_args={\n            'cxx': compile_args_cxx,\n            'nvcc': []\n        }\n    )\n\nif '--plugins' in sys.argv:\n    ext_modules.append(plugins_ext_module)\n    sys.argv.remove('--plugins')\n\nif '--contrib' in sys.argv:\n    exclude_dir=[]\n    sys.argv.remove('--contrib')\n\nsetup(\n    name='torch2trt',\n    version='0.5.0',\n    description='An easy to use PyTorch to TensorRT converter',\n    packages=find_packages(exclude=exclude_dir),\n    ext_package='torch2trt',\n    ext_modules=ext_modules,\n    cmdclass={'build_ext': BuildExtension}\n)\n"
        },
        {
          "name": "test.sh",
          "type": "blob",
          "size": 2.69140625,
          "content": "#!/bin/bash\n\nOUTPUT_FILE=$1\n\ntouch $OUTPUT_FILE\n\necho \"| Name | Data Type | Input Shapes | torch2trt kwargs | Max Error | Throughput (PyTorch) | Throughput (TensorRT) | Latency (PyTorch) | Latency (TensorRT) |\" >> $OUTPUT_FILE\necho \"|------|-----------|--------------|------------------|-----------|----------------------|-----------------------|-------------------|--------------------|\" >> $OUTPUT_FILE\n\npython3 -m torch2trt.test -o $OUTPUT_FILE --name alexnet --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name squeezenet1_0 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name squeezenet1_1 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name resnet18 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name resnet34 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name resnet50 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name resnet101 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name resnet152 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name densenet121 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name densenet169 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name densenet201 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name densenet161 --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name vgg11$ --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name vgg13$ --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name vgg16$ --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name vgg19$ --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name vgg11_bn --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name vgg13_bn --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name vgg16_bn --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name vgg19_bn --include=torch2trt.tests.torchvision.classification\npython3 -m torch2trt.test -o $OUTPUT_FILE --name mobilenet_v2 --include=torch2trt.tests.torchvision.classification\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "torch2trt",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}