{
  "metadata": {
    "timestamp": 1736559538634,
    "page": 136,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "SCIR-HI/Huatuo-Llama-Med-Chinese",
      "stars": 4642,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1513671875,
          "content": "out/\n7B/\n13B/\n__pycache__/\ncheckpoint**\nminimal-llama**\nlora-llama-med\nupload.py\nlora-**\n*.out\n*result\n*ckpt\nwandb\ntodo.txt\n.vscode/\n*tmp*\n.DS_Store\n.idea\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.12890625,
          "content": "                                Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.765625,
          "content": "[**ä¸­æ–‡**](./README.md) | [**English**](./README_EN.md)\n\n<p align=\"center\" width=\"100%\">\n<a href=\"https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/\" target=\"_blank\"><img src=\"assets/logo/logo_new.png\" alt=\"SCIR-HI-HuaTuo\" style=\"width: 60%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n  \n\n# æœ¬è‰[åŸåï¼šåé©¼(HuaTuo)]: åŸºäºä¸­æ–‡åŒ»å­¦çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒ\n\n### BenTsao (original name: HuaTuo): Instruction-tuning Large Language Models With Chinese Medical Knowledge\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/blob/main/LICENSE) [![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n\n  \næœ¬é¡¹ç›®å¼€æºäº†ç»è¿‡ä¸­æ–‡åŒ»å­¦æŒ‡ä»¤ç²¾è°ƒ/æŒ‡ä»¤å¾®è°ƒ(Instruction-tuning) çš„å¤§è¯­è¨€æ¨¡å‹é›†ï¼ŒåŒ…æ‹¬LLaMAã€Alpaca-Chineseã€Bloomã€æ´»å­—æ¨¡å‹ç­‰ã€‚\n\n\næˆ‘ä»¬åŸºäºåŒ»å­¦çŸ¥è¯†å›¾è°±ä»¥åŠåŒ»å­¦æ–‡çŒ®ï¼Œç»“åˆChatGPT APIæ„å»ºäº†ä¸­æ–‡åŒ»å­¦æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œå¹¶ä»¥æ­¤å¯¹å„ç§åŸºæ¨¡å‹è¿›è¡Œäº†æŒ‡ä»¤å¾®è°ƒï¼Œæé«˜äº†åŸºæ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„é—®ç­”æ•ˆæœã€‚\n\n\n## News\n**[2023/09/24]å‘å¸ƒ[ã€Šé¢å‘æ™ºæ…§åŒ»ç–—çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæŠ€æœ¯ã€‹](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/doc/Tuning_Methods_for_LLMs_towards_Health_Intelligence.pdf)**\n\n**[2023/09/12]åœ¨arxivå‘å¸ƒ[ã€Šæ¢ç´¢å¤§æ¨¡å‹ä»åŒ»å­¦æ–‡çŒ®ä¸­äº¤äº’å¼çŸ¥è¯†çš„è·å–ã€‹](https://arxiv.org/pdf/2309.04198.pdf)**\n\n**[2023/09/08]åœ¨arxivå‘å¸ƒ[ã€ŠåŸºäºçŸ¥è¯†å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹å¯é ä¸­æ–‡åŒ»å­¦å›å¤ç”Ÿæˆæ–¹æ³•ã€‹](https://arxiv.org/pdf/2309.04175.pdf)**  \n\n**[2023/08/07] ğŸ”¥ğŸ”¥å¢åŠ äº†åŸºäº[æ´»å­—](https://github.com/HIT-SCIR/huozi)è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹å‘å¸ƒï¼Œæ¨¡å‹æ•ˆæœæ˜¾è‘—æå‡ã€‚ğŸ”¥ğŸ”¥**\n\n[2023/08/05] æœ¬è‰æ¨¡å‹åœ¨CCL 2023 Demo Trackè¿›è¡ŒPosterå±•ç¤ºã€‚\n\n [2023/08/03] SCIRå®éªŒå®¤å¼€æº[æ´»å­—](https://github.com/HIT-SCIR/huozi)é€šç”¨é—®ç­”æ¨¡å‹ï¼Œæ¬¢è¿å¤§å®¶å…³æ³¨ğŸ‰ğŸ‰\n\n[2023/07/19] å¢åŠ äº†åŸºäº[Bloom](https://huggingface.co/bigscience/bloom-7b1)è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹å‘å¸ƒã€‚\n\n[2023/05/12] æ¨¡å‹ç”±\"åé©¼\"æ›´åä¸º\"æœ¬è‰\"ã€‚\n\n[2023/04/28] å¢åŠ äº†åŸºäº[ä¸­æ–‡Alpacaå¤§æ¨¡å‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca)è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹å‘å¸ƒã€‚\n\n[2023/04/24] å¢åŠ äº†åŸºäºLLaMAå’ŒåŒ»å­¦æ–‡çŒ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹å‘å¸ƒã€‚\n\n[2023/03/31] å¢åŠ äº†åŸºäºLLaMAå’ŒåŒ»å­¦çŸ¥è¯†åº“è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹å‘å¸ƒã€‚\n\n\n## A Quick Start\n\né¦–å…ˆå®‰è£…ä¾èµ–åŒ…ï¼Œpythonç¯å¢ƒå»ºè®®3.9+\n\n```\npip install -r requirements.txt\n\n```\n\né’ˆå¯¹æ‰€æœ‰åŸºæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŠç²¾åº¦åŸºæ¨¡å‹LoRAå¾®è°ƒçš„æ–¹å¼è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒè®­ç»ƒï¼Œä»¥åœ¨è®¡ç®—èµ„æºä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚\n\n### åŸºæ¨¡å‹\n - [æ´»å­—1.0](https://github.com/HIT-SCIR/huozi)ï¼Œå“ˆå°”æ»¨å·¥ä¸šå¤§å­¦åŸºäºBloom-7BäºŒæ¬¡å¼€å‘çš„ä¸­æ–‡é€šç”¨é—®ç­”æ¨¡å‹\n - [Bloom-7B](https://huggingface.co/bigscience/bloomz-7b1)\n - [Alpaca-Chinese-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca)ï¼ŒåŸºäºLLaMAäºŒæ¬¡å¼€å‘çš„ä¸­æ–‡é—®ç­”æ¨¡å‹\n - [LLaMA-7B](https://huggingface.co/decapoda-research/llama-7b-hf)\n \n### LoRAæ¨¡å‹æƒé‡ä¸‹è½½\n\nLoRAæƒé‡å¯ä»¥é€šè¿‡ç™¾åº¦ç½‘ç›˜æˆ–Hugging Faceä¸‹è½½ï¼š\n\n1. ğŸ”¥å¯¹æ´»å­—è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„LoRAæƒé‡æ–‡ä»¶\n  - åŸºäºåŒ»å­¦çŸ¥è¯†åº“ä»¥åŠåŒ»å­¦é—®ç­”æ•°æ®é›† [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1BPnDNb1wQZTWy_Be6MfcnA?pwd=m21s)\n2. å¯¹Bloomè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„LoRAæƒé‡æ–‡ä»¶\n - åŸºäºåŒ»å­¦çŸ¥è¯†åº“ä»¥åŠåŒ»å­¦é—®ç­”æ•°æ®é›† [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1jPcuEOhesFGYpzJ7U52Fag?pwd=scir)å’Œ[Hugging Face](https://huggingface.co/lovepon/lora-bloom-med-bloom)\n3. å¯¹Alpacaè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„LoRAæƒé‡æ–‡ä»¶\n - åŸºäºåŒ»å­¦çŸ¥è¯†åº“ [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/16oxcjzXnXjDpL8SKihgNxw?pwd=scir)å’Œ[Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med)\n - åŸºäºåŒ»å­¦çŸ¥è¯†åº“å’ŒåŒ»å­¦æ–‡çŒ® [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1HDdK84ASHmzOFlkmypBIJw?pwd=scir)å’Œ[Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med-alldata)\n4. å¯¹LLaMAè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„LoRAæƒé‡æ–‡ä»¶\n - åŸºäºåŒ»å­¦çŸ¥è¯†åº“ [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1jih-pEr6jzEa6n2u6sUMOg?pwd=jjpf)å’Œ[Hugging Face](https://huggingface.co/thinksoso/lora-llama-med)\n - åŸºäºåŒ»å­¦æ–‡çŒ® [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1jADypClR2bLyXItuFfSjPA?pwd=odsk)å’Œ[Hugging Face](https://huggingface.co/lovepon/lora-llama-literature)\n\n\n\nä¸‹è½½LoRAæƒé‡å¹¶è§£å‹ï¼Œè§£å‹åçš„æ ¼å¼å¦‚ä¸‹ï¼š\n\n\n```\n**lora-folder-name**/\nÂ  - adapter_config.json Â  # LoRAæƒé‡é…ç½®æ–‡ä»¶\nÂ  - adapter_model.bin Â  # LoRAæƒé‡æ–‡ä»¶\n```\n\nåŸºäºç›¸åŒçš„æ•°æ®ï¼Œæˆ‘ä»¬è¿˜è®­ç»ƒäº†åŒ»ç–—ç‰ˆæœ¬çš„ChatGLMæ¨¡å‹: [ChatGLM-6B-Med](https://github.com/SCIR-HI/Med-ChatGLM)\n\n\n### Infer\n\næˆ‘ä»¬åœ¨`./data/infer.json`ä¸­æä¾›äº†ä¸€äº›æµ‹è¯•ç”¨ä¾‹ï¼Œå¯ä»¥æ›¿æ¢æˆå…¶å®ƒçš„æ•°æ®é›†ï¼Œè¯·æ³¨æ„ä¿æŒæ ¼å¼ä¸€è‡´\n  \n\nè¿è¡Œinferè„šæœ¬\n\n\n```\n#åŸºäºåŒ»å­¦çŸ¥è¯†åº“\nbash ./scripts/infer.sh\n\n#åŸºäºåŒ»å­¦æ–‡çŒ®\n#å•è½®\nbash ./scripts/infer-literature-single.sh\n\n#å¤šè½®\nbash ./scripts/infer-literature-multi.sh\n```\n\ninfer.shè„šæœ¬ä»£ç å¦‚ä¸‹ï¼Œè¯·å°†ä¸‹åˆ—ä»£ç ä¸­åŸºæ¨¡å‹base_modelã€loraæƒé‡lora_weightsä»¥åŠæµ‹è¯•æ•°æ®é›†è·¯å¾„instruct_dirè¿›è¡Œæ›¿æ¢åè¿è¡Œ\n\n    python infer.py \\\n\t\t    --base_model 'BASE_MODEL_PATH' \\\n\t\t    --lora_weights 'LORA_WEIGHTS_PATH' \\\n\t\t    --use_lora True \\\n\t\t    --instruct_dir 'INFER_DATA_PATH' \\\n\t\t    --prompt_template 'TEMPLATE_PATH'\n \n\n**_æç¤ºæ¨¡æ¿çš„é€‰æ‹©ä¸æ¨¡å‹ç›¸å…³ï¼Œè¯¦æƒ…å¦‚ä¸‹ï¼š_**\n\n| æ´»å­—&Bloom                      | LLaMA&Alpaca                                                                          |                                       \n|:------------------------------|:--------------------------------------------------------------------------------------|\n| `templates/bloom_deploy.json` | åŸºäºåŒ»å­¦çŸ¥è¯†åº“`templates/med_template.json` <br>  åŸºäºåŒ»å­¦æ–‡çŒ®`templates/literature_template.json` |\n\n\n\nä¹Ÿå¯å‚è€ƒ`./scripts/test.sh`\n\n  \n\n## æ–¹æ³•\n\nåŸºæ¨¡å‹åœ¨åŒ»å­¦é—®ç­”åœºæ™¯ä¸‹æ•ˆæœæœ‰é™ï¼ŒæŒ‡ä»¤å¾®è°ƒæ˜¯ä¸€ç§é«˜æ•ˆçš„ä½¿åŸºæ¨¡å‹æ‹¥æœ‰å›ç­”äººç±»é—®é¢˜èƒ½åŠ›çš„æ–¹æ³•ã€‚\n\n\n\n### æ•°æ®é›†æ„å»º\n#### åŒ»å­¦çŸ¥è¯†åº“\næˆ‘ä»¬é‡‡ç”¨äº†å…¬å¼€å’Œè‡ªå»ºçš„ä¸­æ–‡åŒ»å­¦çŸ¥è¯†åº“ï¼Œä¸»è¦å‚è€ƒäº†[cMeKG](https://github.com/king-yyf/CMeKG_tools)ã€‚\n\nåŒ»å­¦çŸ¥è¯†åº“å›´ç»•ç–¾ç—…ã€è¯ç‰©ã€æ£€æŸ¥æŒ‡æ ‡ç­‰æ„å»ºï¼Œå­—æ®µåŒ…æ‹¬å¹¶å‘ç—‡ï¼Œé«˜å±å› ç´ ï¼Œç»„ç»‡å­¦æ£€æŸ¥ï¼Œä¸´åºŠç—‡çŠ¶ï¼Œè¯ç‰©æ²»ç–—ï¼Œè¾…åŠ©æ²»ç–—ç­‰ã€‚çŸ¥è¯†åº“ç¤ºä¾‹å¦‚ä¸‹:\n\n\n```\n\n{\"ä¸­å¿ƒè¯\": \"åå¤´ç—›\", \"ç›¸å…³ç–¾ç—…\": [\"å¦Šå¨ åˆå¹¶åå¤´ç—›\", \"æ¶å¯’å‘çƒ­\"], \"ç›¸å…³ç—‡çŠ¶\": [\"çš®è‚¤å˜ç¡¬\", \"å¤´éƒ¨åŠçœ¼åéƒ¨ç–¼ç—›å¹¶èƒ½å¬åˆ°è¿ç»­ä¸æ–­çš„éš†éš†å£°\", \"æ™¨èµ·å¤´ç—›åŠ é‡\"], \"æ‰€å±ç§‘å®¤\": [\"ä¸­è¥¿åŒ»ç»“åˆç§‘\", \"å†…ç§‘\"], \"å‘ç—…éƒ¨ä½\": [\"å¤´éƒ¨\"]}\n\n```\n\næˆ‘ä»¬åˆ©ç”¨GPT3.5æ¥å£å›´ç»•åŒ»å­¦çŸ¥è¯†åº“æ„å»ºé—®ç­”æ•°æ®ï¼Œè®¾ç½®äº†å¤šç§Promptå½¢å¼æ¥å……åˆ†åˆ©ç”¨çŸ¥è¯†ã€‚\n\næŒ‡ä»¤å¾®è°ƒçš„è®­ç»ƒé›†æ•°æ®ç¤ºä¾‹å¦‚ä¸‹ï¼š\n\n```\n\n\"é—®é¢˜ï¼šä¸€ä½å¹´è½»ç”·æ€§é•¿æœŸä½¿ç”¨å¯å¡å› ï¼Œçªç„¶å‡ºç°èƒ¸ç—›ã€å‘•åã€å‡ºæ±—ç­‰ç—‡çŠ¶ï¼Œç»æ£€æŸ¥å‘ç°å¿ƒç”µå›¾åæ˜ å¿ƒè‚Œæ€¥æ€§æŸä¼¤ï¼Œè¯·é—®å¯èƒ½æ‚£çš„æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿæ²»ç–—æ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿ\"\n\nå›ç­”: å¯èƒ½æ‚£çš„æ˜¯å¿ƒè‚Œæ¢—å¡ï¼Œéœ€è¦è¿›è¡Œç»´æ‹‰å¸•ç±³ã€ä¾æ™®åˆ©é…®ã€ç¡é…¸ç”˜æ²¹ã€ÃŸé˜»æ»å‰‚ã€å—å•¡ç­‰è¯ç‰©æ²»ç–—ï¼Œå¹¶è¿›è¡Œæº¶æ “æ²»ç–—ã€ä½åˆ†å­é‡è‚ç´ ã€é’™é€šé“é˜»æ»å‰‚ç­‰è¾…åŠ©æ²»ç–—ã€‚æ­¤å¤–éœ€è¦åŠæ—¶åœç”¨å¯å¡å› ç­‰è¯ç‰©ï¼Œä»¥é˜²æ­¢ç—…æƒ…åŠ é‡ã€‚\"\n\n```\n\n  \n\næˆ‘ä»¬æä¾›äº†æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ï¼Œå…±è®¡å…«åƒä½™æ¡ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶è®­ç»ƒé›†çš„æ„å»ºèå…¥äº†çŸ¥è¯†ï¼Œä½†æ˜¯ä»å­˜åœ¨é”™è¯¯å’Œä¸å®Œå–„çš„åœ°æ–¹ï¼Œåç»­æˆ‘ä»¬ä¼šåˆ©ç”¨æ›´å¥½çš„ç­–ç•¥è¿­ä»£æ›´æ–°æ•°æ®é›†ã€‚\n\næŒ‡ä»¤å¾®è°ƒæ•°æ®é›†è´¨é‡ä»æœ‰é™ï¼Œåç»­å°†è¿›è¡Œä¸æ–­è¿­ä»£ï¼ŒåŒæ—¶åŒ»å­¦çŸ¥è¯†åº“å’Œæ•°æ®é›†æ„å»ºä»£ç è¿˜åœ¨æ•´ç†ä¸­ï¼Œæ•´ç†å®Œæˆå°†ä¼šå‘å¸ƒã€‚\n\n#### åŒ»å­¦æ–‡çŒ®\n\næ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†2023å¹´å…³äºè‚ç™Œç–¾ç—…çš„ä¸­æ–‡åŒ»å­¦æ–‡çŒ®ï¼Œåˆ©ç”¨GPT3.5æ¥å£å›´ç»•åŒ»å­¦æ–‡çŒ®çš„ã€ç»“è®ºã€‘æ„å»ºå¤šè½®é—®ç­”æ•°æ®ã€‚åœ¨Â·`./data_literature/liver_cancer.json`ä¸­æˆ‘ä»¬æä¾›äº†å…¶ä¸­çš„1kæ¡è®­ç»ƒæ ·ä¾‹ã€‚ç›®å‰ï¼Œè®­ç»ƒæ ·æœ¬çš„è´¨é‡ä»ç„¶æœ‰é™ï¼Œåœ¨åç»­æˆ‘ä»¬ä¼šè¿›ä¸€æ­¥è¿­ä»£æ•°æ®ï¼Œä¼šä»¥`å…¬å¼€æ•°æ®é›†`çš„å½¢å¼å¯¹å¤–è¿›è¡Œå‘å¸ƒã€‚è®­ç»ƒæ ·æœ¬çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š\n\n<p align=\"center\" width=\"100%\">\n\n<a href=\"https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/\" target=\"_blank\"><img src=\"assets/case.png\" alt=\"SCIR-HI-HuaTuo-literature\" style=\"width: 100%; min-width: 300px; display: block; margin: auto;\"></a>\n\n</p>\n\nç›®å‰ï¼Œæˆ‘ä»¬åªå¼€æ”¾é’ˆå¯¹\"è‚ç™Œ\"å•ä¸ªç–¾ç—…è®­ç»ƒçš„æ¨¡å‹å‚æ•°ã€‚åœ¨æœªæ¥ï¼Œæˆ‘ä»¬è®¡åˆ’å‘å¸ƒèå…¥æ–‡çŒ®ç»“è®ºçš„åŒ»å­¦å¯¹è¯æ•°æ®é›†ï¼Œå¹¶ä¸”ä¼šé’ˆå¯¹â€œè‚èƒ†èƒ°â€ç›¸å…³16ç§ç–¾ç—…è®­ç»ƒæ¨¡å‹ã€‚\n\nç›¸å…³ç»†èŠ‚å¯å‚è€ƒæˆ‘ä»¬çš„æ–‡ç« ï¼š[ã€Šæ¢ç´¢å¤§æ¨¡å‹ä»åŒ»å­¦æ–‡çŒ®ä¸­äº¤äº’å¼çŸ¥è¯†çš„è·å–ã€‹](https://arxiv.org/pdf/2309.04198.pdf)\n  \n\n### Finetune\n\nå¦‚æœæƒ³ç”¨è‡ªå·±çš„æ•°æ®é›†å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œè¯·æŒ‰ç…§`./data/llama_data.json`ä¸­çš„æ ¼å¼æ„å»ºè‡ªå·±çš„æ•°æ®é›†\n\nè¿è¡Œfinetuneè„šæœ¬\n\n```\n\nbash ./scripts/finetune.sh\n\n```\n\n  \n  \n  \n\n## è®­ç»ƒç»†èŠ‚\n\n### è®¡ç®—èµ„æºéœ€æ±‚å‚è€ƒ\n\nåŸºäºLLaMAæ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸€å¼ A100-SXM-80GBæ˜¾å¡ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œè®­ç»ƒæ€»è½®æ¬¡10è½®ï¼Œè€—æ—¶çº¦2h17mã€‚batch_size=128çš„æƒ…å†µä¸‹æ˜¾å­˜å ç”¨åœ¨40Gå·¦å³ã€‚é¢„è®¡3090/4090æ˜¾å¡(24GBæ˜¾å­˜)ä»¥ä¸Šæ˜¾å¡å¯ä»¥è¾ƒå¥½æ”¯æŒï¼Œæ ¹æ®æ˜¾å­˜å¤§å°æ¥è°ƒæ•´batch_sizeã€‚\n\n\n### å®éªŒè¿‡ç¨‹\n\nè¶…å‚è®¾ç½®å’Œå®éªŒè¿‡ç¨‹å¯ä»¥å‚è€ƒwandbçš„é“¾æ¥:\n\nhttps://wandb.ai/thinksoso/llama_med/runs/a5wgcnzt/overview?workspace=user-thinksoso\n\n  \n\n## æ¨¡å‹æ•ˆæœå¯¹æ¯”\n\næ³¨ï¼šæµ‹è¯•äº2023å¹´3æœˆ\n  \n| æµ‹è¯•è¾“å…¥ | Llamaè¾“å‡º Â | Alpacaè¾“å‡º | BenTsaoæœ¬è‰è¾“å‡º                                             |\n|:------------- |:---------------| :-------------|:--------------------------------------------------------|\n|å°å¼ æœ€è¿‘æ„Ÿè§‰èº«ä½“ä¸é€‚ï¼Œå‡ºç°å¿ƒæ‚¸ã€æ°”ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ£€å‘ç°å¿ƒè„æ‰©å¤§ã€æåŠ¨å‡å¼±ã€‚|é—®é¢˜ï¼šå°å¼ æœ€è¿‘æ„Ÿè§‰èº«ä½“ä¸é€‚ï¼Œå‡ºç°å¿ƒæ‚¸ã€æ°”ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ£€å‘ç°å¿ƒè„æ‰©å¤§ã€æåŠ¨å‡å¼±ã€‚| æœ€è¿‘ï¼Œå°å¼ ä¸€ç›´æ„Ÿè§‰èº«ä½“ä¸èˆ’æœï¼Œæœ‰å¿ƒçƒ¦æ„ä¹±ã€å‘¼å¸æ€¥ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ ¼æ£€æŸ¥æ˜¾ç¤ºå¿ƒè„å¢å¤§ï¼Œè‚Œè‚‰æ— åŠ›ã€‚| **å°å¼ å¯èƒ½æ‚£æœ‰å¿ƒè‚Œç‚ï¼Œå»ºè®®å°½å¿«åˆ°åŒ»é™¢è¿›è¡Œæ£€æŸ¥å’Œæ²»ç–—ã€‚æ²»ç–—æ–¹æ¡ˆåŒ…æ‹¬è¯ç‰©æ²»ç–—å’Œæ‰‹æœ¯æ²»ç–—ã€‚**           |\n|éº»é£ç—…å’Œå„¿ç«¥å“®å–˜çš„ç—…å› æ˜¯å¦ä¸€è‡´ï¼Ÿ|å„¿ç«¥å“®å–˜å’Œéº»é£ç—…çš„ç—…å› æ˜¯ä¸€è‡´ï¼Œä½†æ˜¯å„¿ç«¥å“®å–˜å’Œéº»é£ç—…çš„ç—…å› ä¸ä¸€è‡´|æ˜¯çš„ï¼Œå“®å–˜å’Œå„¿ç«¥å–˜æ¯æœ‰ç›¸ä¼¼çš„åŸå› ã€‚| **ä¸ä¸€è‡´ï¼Œéº»é£ç—…çš„ç—…å› æ˜¯éº»é£æ†èŒæ„ŸæŸ“ï¼Œè€Œå„¿ç«¥å“®å–˜çš„ç—…å› æ˜¯å–‚å…»ä¸è‰¯ã€å¸çƒŸç­‰å› ç´ ã€‚**              |\n|ä¸€ä½æ‚£è€…å‡ºç°äº†æŒç»­æ€§çš„èƒ€ç—›å’Œé»„ç–¸ï¼Œç»å½±åƒå­¦æ£€æŸ¥åç¡®è¯Šä¸ºè‚èƒ†ç®¡ç»“çŸ³ç—…ï¼Œåº”è¯¥é‡‡å–å“ªäº›æ²»ç–—æ–¹æ¡ˆï¼Ÿ|è‚èƒ†ç®¡ç»“çŸ³ç—…æ‚£è€…åº”è¯¥é‡‡å–å“ªäº›æ²»ç–—æ–¹æ¡ˆï¼Ÿ|æ‚£è€…åº”æ¥å—æ‰‹æœ¯å’ŒåŒ–ç–—ç›¸ç»“åˆçš„æ²»ç–—ã€‚| **é’ˆå¯¹è‚èƒ†ç®¡ç»“çŸ³ç—…ï¼Œæ²»ç–—æ–¹æ¡ˆåŒ…æ‹¬æ‰‹æœ¯æ²»ç–—ã€è¯ç‰©æ²»ç–—å’Œè¾…åŠ©æ²»ç–—ç­‰ã€‚å…·ä½“æ²»ç–—æ–¹æ¡ˆéœ€è¦æ ¹æ®æ‚£è€…çš„å…·ä½“æƒ…å†µè€Œå®šã€‚** |\n\n\n\n   \n## å¸¸è§é—®é¢˜\n1. Q: ä¸ºä»€ä¹ˆæ›´åä¸º\"æœ¬è‰\"ï¼Ÿ\n   \n   A: ä¸ºSCIRå®éªŒå®¤å¤§è¯­è¨€æ¨¡å‹å‘½åä¸€è‡´æ€§ï¼Œä¸­æ–‡åŒ»å­¦å¤§æ¨¡å‹åç§°è°ƒæ•´ä¸º\"æœ¬è‰\"ã€‚\n\n2. Q: ä¸ºä»€ä¹ˆæ˜¯\"åé©¼\"è€Œä¸æ˜¯\"åä½—\"ï¼Ÿ \n   \n   A: å«â€é©¼â€œæ˜¯å› ä¸ºæˆ‘ä»¬çš„åŸºæ¨¡å‹LLaMAæ˜¯ç¾æ´²é©¼ï¼ŒAlpacaæ˜¯ç¾Šé©¼ï¼Œå—ä»–ä»¬åå­—çš„å¯å‘ä»¥åŠåä½—çš„è°éŸ³æ¢—ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„æ¨¡å‹èµ·åä¸ºåé©¼ã€‚\n\n3. Q: æœ‰ä½¿ç”¨ä¸­åŒ»ç†è®ºæˆ–è€…ä¸­åŒ»æ•°æ®å—ï¼Ÿ\n    \n   A: ç›®å‰è¿˜æ²¡æœ‰\n   \n4. Q: æ¨¡å‹è¿è¡Œçš„ç»“æœä¸åŒã€æ•ˆæœæœ‰é™\n   \n   A: ç”±äºç”Ÿæˆæ¨¡å‹ç”Ÿæˆå¤šæ ·æ€§çš„è€ƒé‡ï¼Œå¤šæ¬¡è¿è¡Œçš„ç»“æœå¯èƒ½ä¼šæœ‰å·®å¼‚ã€‚å½“å‰å¼€æºçš„æ¨¡å‹ç”±äºLLaMAåŠAlpacaä¸­æ–‡è¯­æ–™æœ‰é™ï¼Œä¸”çŸ¥è¯†ç»“åˆçš„æ–¹å¼è¾ƒä¸ºç²—ç³™ï¼Œè¯·å¤§å®¶å°è¯•bloom-basedå’Œæ´»å­—-basedçš„æ¨¡å‹ã€‚\n   \n5. Q: æ¨¡å‹æ— æ³•è¿è¡Œ/æ¨ç†å†…å®¹å®Œå…¨æ— æ³•æ¥å—\n   \n   A: è¯·ç¡®å®šå·²å®‰è£…requirementsä¸­çš„ä¾èµ–ã€é…ç½®å¥½cudaç¯å¢ƒå¹¶æ·»åŠ ç¯å¢ƒå˜é‡ã€æ­£ç¡®è¾“å…¥ä¸‹è½½å¥½çš„æ¨¡å‹ä»¥åŠloraçš„å­˜å‚¨ä½ç½®ï¼›æ¨ç†å†…å®¹å¦‚å­˜åœ¨é‡å¤ç”Ÿæˆæˆ–éƒ¨åˆ†é”™è¯¯å†…å®¹å±äºllama-basedæ¨¡å‹çš„å¶å‘ç°è±¡ï¼Œä¸llamaæ¨¡å‹çš„ä¸­æ–‡èƒ½åŠ›ã€è®­ç»ƒæ•°æ®è§„æ¨¡ä»¥åŠè¶…å‚è®¾ç½®å‡æœ‰ä¸€å®šçš„å…³ç³»ï¼Œè¯·å°è¯•åŸºäºæ´»å­—çš„æ–°æ¨¡å‹ã€‚å¦‚å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œè¯·å°†è¿è¡Œçš„æ–‡ä»¶åã€æ¨¡å‹åã€loraç­‰é…ç½®ä¿¡æ¯è¯¦ç»†æè¿°åœ¨issueä¸­ï¼Œè°¢è°¢å¤§å®¶ã€‚\n   \n6.\tQ: å‘å¸ƒçš„è‹¥å¹²æ¨¡å‹å“ªä¸ªæœ€å¥½ï¼Ÿ\n\n    A: æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼ŒåŸºäºæ´»å­—æ¨¡å‹çš„æ•ˆæœç›¸å¯¹æ›´å¥½ä¸€äº›ã€‚ \n \n\n\n\n## é¡¹ç›®å‚ä¸è€…\n\næœ¬é¡¹ç›®ç”±å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ç¤¾ä¼šè®¡ç®—ä¸ä¿¡æ¯æ£€ç´¢ç ”ç©¶ä¸­å¿ƒå¥åº·æ™ºèƒ½ç»„[ç‹æ˜Šæ·³](https://haochun.wang) ã€[æœæ™ç¿](https://github.com/DYR1)ã€[åˆ˜é©°](https://github.com/thinksoso)ã€[ç™½ç¿](https://github.com/RuiBai1999)ã€[å¸­å¥´ç“¦](https://github.com/rootnx)ã€[é™ˆé›¨æ™—](https://github.com/Imsovegetable)ã€[å¼ºæ³½æ–‡](https://github.com/1278882181)ã€[é™ˆå¥å®‡](https://github.com/JianyuChen01)ã€[æå­å¥](https://github.com/FlowolfzzZ)å®Œæˆï¼ŒæŒ‡å¯¼æ•™å¸ˆä¸º[èµµæ£®æ ‹](http://homepage.hit.edu.cn/stanzhao?lang=zh)å‰¯æ•™æˆï¼Œç§¦å…µæ•™æˆä»¥åŠåˆ˜æŒºæ•™æˆã€‚\n\n  \n\n## è‡´è°¢\n\n  \n\næœ¬é¡¹ç›®å‚è€ƒäº†ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼Œåœ¨æ­¤å¯¹ç›¸å…³é¡¹ç›®å’Œç ”ç©¶å¼€å‘äººå‘˜è¡¨ç¤ºæ„Ÿè°¢ã€‚\n\n- æ´»å­—: https://github.com/HIT-SCIR/huozi\n- Facebook LLaMA: https://github.com/facebookresearch/llama\n- Stanford Alpaca: https://github.com/tatsu-lab/stanford_alpaca\n- alpaca-lora by @tloen: https://github.com/tloen/alpaca-lora\n- CMeKG https://github.com/king-yyf/CMeKG_tools\n- æ–‡å¿ƒä¸€è¨€ https://yiyan.baidu.com/welcome æœ¬é¡¹ç›®çš„logoç”±æ–‡å¿ƒä¸€è¨€è‡ªåŠ¨ç”Ÿæˆ\n\n  \n\n## å…è´£å£°æ˜\n\næœ¬é¡¹ç›®ç›¸å…³èµ„æºä»…ä¾›å­¦æœ¯ç ”ç©¶ä¹‹ç”¨ï¼Œä¸¥ç¦ç”¨äºå•†ä¸šç”¨é€”ã€‚ä½¿ç”¨æ¶‰åŠç¬¬ä¸‰æ–¹ä»£ç çš„éƒ¨åˆ†æ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªç›¸åº”çš„å¼€æºåè®®ã€‚æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å—æ¨¡å‹è®¡ç®—ã€éšæœºæ€§å’Œé‡åŒ–ç²¾åº¦æŸå¤±ç­‰å› ç´ å½±å“ï¼Œæœ¬é¡¹ç›®æ— æ³•å¯¹å…¶å‡†ç¡®æ€§ä½œå‡ºä¿è¯ã€‚æœ¬é¡¹ç›®æ•°æ®é›†ç»å¤§éƒ¨åˆ†ç”±æ¨¡å‹ç”Ÿæˆï¼Œå³ä½¿ç¬¦åˆæŸäº›åŒ»å­¦äº‹å®ï¼Œä¹Ÿä¸èƒ½è¢«ç”¨ä½œå®é™…åŒ»å­¦è¯Šæ–­çš„ä¾æ®ã€‚å¯¹äºæ¨¡å‹è¾“å‡ºçš„ä»»ä½•å†…å®¹ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œäº¦ä¸å¯¹å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœè€Œå¯èƒ½äº§ç”Ÿçš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚\n\n\n## Citation\n\nå¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„æ•°æ®æˆ–è€…ä»£ç ï¼Œæˆ–æ˜¯æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·å£°æ˜å¼•ç”¨\n\n\né¦–ç‰ˆæŠ€æœ¯æŠ¥å‘Š: [Huatuo: Tuning llama model with chinese medical knowledge](https://arxiv.org/pdf/2304.06975)\n\n```\n@misc{wang2023huatuo,\nÂ  Â  Â  title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge},\nÂ  Â  Â  author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},\nÂ  Â  Â  year={2023},\nÂ  Â  Â  eprint={2304.06975},\nÂ  Â  Â  archivePrefix={arXiv},\nÂ  Â  Â  primaryClass={cs.CL}\n}\n```\n\nçŸ¥è¯†å¾®è°ƒï¼š[Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese\n](https://arxiv.org/pdf/2309.04175.pdf)\n\n```\n@misc{wang2023knowledgetuning,\n      title={Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese}, \n      author={Haochun Wang and Sendong Zhao and Zewen Qiang and Zijian Li and Nuwa Xi and Yanrui Du and MuZhen Cai and Haoqiang Guo and Yuhan Chen and Haoming Xu and Bing Qin and Ting Liu},\n      year={2023},\n      eprint={2309.04175},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\nåŒ»å­¦æ–‡çŒ®çŸ¥è¯†è·å–ï¼š[The CALLA Dataset: Probing LLMsâ€™ Interactive Knowledge Acquisition from Chinese Medical Literature](https://arxiv.org/pdf/2309.04198.pdf)\n\n```\n@misc{du2023calla,\n      title={The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature}, \n      author={Yanrui Du and Sendong Zhao and Muzhen Cai and Jianyu Chen and Haochun Wang and Yuhan Chen and Haoqiang Guo and Bing Qin},\n      year={2023},\n      eprint={2309.04198},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 15.0625,
          "content": "[**ä¸­æ–‡**](./README.md) | [**English**](./README_EN.md)\n<p align=\"center\" width=\"100%\">\n<a href=\"https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/\" target=\"_blank\"><img src=\"assets/logo/logo_new.png\" alt=\"SCIR-HI-HuaTuo\" style=\"width: 60%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n# BenTsao (original name: HuaTuo): Instruction-tuning Large Language Models With Chinese Medical Knowledge\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/blob/main/LICENSE) [![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n\nThis repo open-sources a series of instruction-tuned large language models with Chinese medical instruction datasets, including LLaMAã€Alpaca-Chineseã€Bloomã€Huozi. \n\nWe constructed a Chinese medical instruct-tuning dataset using medical knowledge graphs, medical literatures and the GPT3.5 API, and performed instruction-tuning on various base models with this dataset, improving its question-answering performance in the medical field.\n\n\n## News\n**[2023/08/07] ğŸ”¥ğŸ”¥ Released a model instruction-tuned based on [Huozi](https://github.com/HIT-SCIR/huozi), resulting in a significant improvement in model performance. ğŸ”¥ğŸ”¥**\n\n[2023/08/05] The \"Bentsao\" model was presented as a poster at the CCL 2023 Demo Track.\n\n[2023/08/03] SCIR Lab open-sourced the [Huozi](https://github.com/HIT-SCIR/huozi) general question-answering model. Everyone is welcome to check it out! ğŸ‰ğŸ‰\n\n[2023/07/19] Released a model instruction-tuned based on [Bloom](https://huggingface.co/bigscience/bloom-7b1).\n\n[2023/05/12] The model was renamed from \"Huatuo\" to \"Bentsao\".\n\n[2023/04/28] Released a model instruction-tuned based on the [Chinese-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca).\n\n[2023/04/24] Released a model instruction-tuned based on LLaMA with medical literature.\n\n[2023/03/31] Released a model instruction-tuned based on LLaMA with a medical knowledge base.\n\n\n\n\n\n\n\n## A Quick Start\nFirstly, install the required packages. It is recommended to use Python 3.9 or above\n\n```\npip install -r requirements.txt\n```\n\nFor all base models, we adopted the semi-precision base model LoRA fine-tuning method for instruction fine-tuning training, in order to strike a balance between computational resources and model performance.\n\n### Base models\n - [Huozi1.0](https://github.com/HIT-SCIR/huozi), Bloom-7B-based Chinese QA model\n - [Bloom-7B](https://huggingface.co/bigscience/bloomz-7b1)\n - [Alpaca-Chinese-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca), LLaMA-based Chinese QA model\n - [LLaMA-7B](https://huggingface.co/decapoda-research/llama-7b-hf)\n\n\n### LoRA weight download\nLORA weights can be downloaded through Baidu Netdisk or Huggingface.\n\n1. ğŸ”¥LoRA for Huozi 1.0\n  - with the medical knowledge base and medical QA dataset [BaiduDisk](https://pan.baidu.com/s/1BPnDNb1wQZTWy_Be6MfcnA?pwd=m21s)\n2. LoRA for Bloom\n - with the medical knowledge base and medical QA dataset [BaiduDisk](https://pan.baidu.com/s/1jPcuEOhesFGYpzJ7U52Fag?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-bloom-med-bloom)\n3. LoRA for Chinese Alpaca\n - with the medical knowledge base [BaiduDisk](https://pan.baidu.com/s/16oxcjzXnXjDpL8SKihgNxw?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med)\n - with the medical knowledge base and medical literature [BaiduDisk](https://pan.baidu.com/s/1HDdK84ASHmzOFlkmypBIJw?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med-alldata)\n4. LoRA for LLaMA\n - with the medical knowledge base [BaiduDisk](https://pan.baidu.com/s/1jih-pEr6jzEa6n2u6sUMOg?pwd=jjpf) and [Hugging Face](https://huggingface.co/thinksoso/lora-llama-med)\n - with medical literature [BaiduDisk](https://pan.baidu.com/s/1jADypClR2bLyXItuFfSjPA?pwd=odsk) and [Hugging Face](https://huggingface.co/lovepon/lora-llama-literature)\n\n\nDownload the LORA weight file and extract it. The format of the extracted file should be as follows:\n\n```\n**lora-folder-name**/\nÂ  - adapter_config.json Â  # LoRA configuration\nÂ  - adapter_model.bin Â  # LoRA weight\n```\n\nWe also trained a medical version of ChatGLM: [ChatGLM-6B-Med](https://github.com/SCIR-HI/Med-ChatGLM) based on the same data.\n\n\n### Infer\nWe provided some test cases in `./data/infer.json`, which can be replaced with other datasets. Please make sure to keep the format consistent.\n\nRun the infer script\n\n```\n#Based on medical knowledge base\nbash ./scripts/infer.sh\n\n#Based on medical literature\n#single-epoch\nbash ./scripts/infer-literature-single.sh\n\n#multi-epoch\nbash ./scripts/infer-literature-multi.sh\n```\n\nThe code for the infer.sh script is as follows. Please replace the base model base_model, the LoRA weights lora_weights, and the test dataset path instruct_dir before running\n\n\tpython infer.py \\\n\t\t\t    --base_model 'BASE_MODEL_PATH' \\\n\t\t\t    --lora_weights 'LORA_WEIGHTS_PATH' \\\n\t\t\t    --use_lora True \\\n\t\t\t    --instruct_dir 'INFER_DATA_PATH' \\\n\t\t\t    --prompt_template 'TEMPLATE_PATH'\n\t\t    \n\nThe prompt template is relevant to the model as follows: \n\n| Huozi&Bloom                      | LLaMA&Alpaca                                                                          |                                       \n|:------------------------------|:--------------------------------------------------------------------------------------|\n| `templates/bloom_deploy.json` | with the medical knowledge base`templates/med_template.json` <br>  with the medical literature`templates/literature_template.json` |\n\n\n\nother reference in `./scripts/test.sh`\n\n\t\n## Methodology\nThe base model has limited effectiveness in medical question-answering scenarios. Instruction-tuning is an efficient method to give the base model the ability to answer human questions\n\t    \n### Dataset construction\n#### Medical knowledge base\nWe used both publicly available and self-built Chinese medical knowledge bases, with a primary reference to [cMeKG](https://github.com/king-yyf/CMeKG_tools). The medical knowledge base is built around diseases, drugs, and diagnostic indicators, with fields including complications, risk factors, histological examinations, clinical symptoms, drug treatments, and adjuvant therapies. An example of the knowledge base is shown below:\n\n\n```\n{\"ä¸­å¿ƒè¯\": \"åå¤´ç—›\", \"ç›¸å…³ç–¾ç—…\": [\"å¦Šå¨ åˆå¹¶åå¤´ç—›\", \"æ¶å¯’å‘çƒ­\"], \"ç›¸å…³ç—‡çŠ¶\": [\"çš®è‚¤å˜ç¡¬\", \"å¤´éƒ¨åŠçœ¼åéƒ¨ç–¼ç—›å¹¶èƒ½å¬åˆ°è¿ç»­ä¸æ–­çš„éš†éš†å£°\", \"æ™¨èµ·å¤´ç—›åŠ é‡\"], \"æ‰€å±ç§‘å®¤\": [\"ä¸­è¥¿åŒ»ç»“åˆç§‘\", \"å†…ç§‘\"], \"å‘ç—…éƒ¨ä½\": [\"å¤´éƒ¨\"]}\n```\nWe used the GPT3.5 API to build a Question-Answer dataset around a medical knowledge base, with multiple prompts to fully leverage the knowledge.\n\nAn example of the training dataset for instruct-tuning is shown below:\n\n\n```\n\"é—®é¢˜ï¼šä¸€ä½å¹´è½»ç”·æ€§é•¿æœŸä½¿ç”¨å¯å¡å› ï¼Œçªç„¶å‡ºç°èƒ¸ç—›ã€å‘•åã€å‡ºæ±—ç­‰ç—‡çŠ¶ï¼Œç»æ£€æŸ¥å‘ç°å¿ƒç”µå›¾åæ˜ å¿ƒè‚Œæ€¥æ€§æŸä¼¤ï¼Œè¯·é—®å¯èƒ½æ‚£çš„æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿæ²»ç–—æ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿ\"\nå›ç­”: å¯èƒ½æ‚£çš„æ˜¯å¿ƒè‚Œæ¢—å¡ï¼Œéœ€è¦è¿›è¡Œç»´æ‹‰å¸•ç±³ã€ä¾æ™®åˆ©é…®ã€ç¡é…¸ç”˜æ²¹ã€ÃŸé˜»æ»å‰‚ã€å—å•¡ç­‰è¯ç‰©æ²»ç–—ï¼Œå¹¶è¿›è¡Œæº¶æ “æ²»ç–—ã€ä½åˆ†å­é‡è‚ç´ ã€é’™é€šé“é˜»æ»å‰‚ç­‰è¾…åŠ©æ²»ç–—ã€‚æ­¤å¤–éœ€è¦åŠæ—¶åœç”¨å¯å¡å› ç­‰è¯ç‰©ï¼Œä»¥é˜²æ­¢ç—…æƒ…åŠ é‡ã€‚\"\n```\n\nWe provided a training dataset for the model, consisting of more than eight thousand entries. It should be noted that although knowledge has been incorporated into the construction of the training set, there are still errors and imperfections. We will use better strategies to iteratively update the dataset in the future.\n\nThe quality of the dataset for instruct-tuning is still limited. We will continue to iterate and improve it. Meanwhile, the medical knowledge base and dataset construction code are still being organized and will be released once completed.\n\n#### Medical literature\nIn addition, we collected Chinese medical literature on liver cancer in 2023, and used the GPT3.5 interface to collect multiple rounds of question-and-answer data around the medical literature. We provide 1k training examples in `./data_literature/liver_cancer.json`. At present, the quality of training samples is still limited. In the future, we will further iterate the data and release it in the form of `public dataset`. An example of a training sample is as follows:\n\n<p align=\"center\" width=\"100%\">\n\n<a href=\"https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/\" target=\"_blank\"><img src=\"assets/case.png\" alt=\"SCIR-HI-HuaTuo-literature\" style=\"width: 100%; min-width: 300px; display: block; margin: auto;\"></a>\n\n</p>\n\nAt present, we only open model parameters trained for the single disease \"liver cancer\". In the future, we plan to release a medical dialogue dataset incorporating medical literature conclusions, and plan to train models for 16 diseases related to \"liver, gallbladder and pancreas\".\n\n### Finetune\nTo fine-tune LLaMA with your own dataset, please construct your dataset following the format of `./data/llama_data.json` and run the finetune script.\n\n```\nbash ./scripts/finetune.sh\n```\n\n\n\n## Training details\n### GPU resource\nWe trained the model on an A100-SXM-80GB for a total of 10 epochs, which took approximately 2 hours and 17 minutes. With a batch size of 128, the GPU memory usage was around 40GB. We expect that GPU with 24GB or more of memory, such as the 3090/4090, will be able to support the model well, and the batch size can be adjusted based on the size of the GPU memory.\n\n\n### Experiments\nParameter settings and details training process can be found in the wandb log:\nhttps://wandb.ai/thinksoso/llama_med/runs/a5wgcnzt/overview?workspace=user-thinksoso\n\n## Comparison of models\n\nNote: test in March 2023.\n\n| Inputå…¥ | Llama output  | Alpaca output | HuaTuo output |\n|:------------- |:---------------| :-------------|:-------------|\n|å°å¼ æœ€è¿‘æ„Ÿè§‰èº«ä½“ä¸é€‚ï¼Œå‡ºç°å¿ƒæ‚¸ã€æ°”ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ£€å‘ç°å¿ƒè„æ‰©å¤§ã€æåŠ¨å‡å¼±ã€‚|é—®é¢˜ï¼šå°å¼ æœ€è¿‘æ„Ÿè§‰èº«ä½“ä¸é€‚ï¼Œå‡ºç°å¿ƒæ‚¸ã€æ°”ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ£€å‘ç°å¿ƒè„æ‰©å¤§ã€æåŠ¨å‡å¼±ã€‚| æœ€è¿‘ï¼Œå°å¼ ä¸€ç›´æ„Ÿè§‰èº«ä½“ä¸èˆ’æœï¼Œæœ‰å¿ƒçƒ¦æ„ä¹±ã€å‘¼å¸æ€¥ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ ¼æ£€æŸ¥æ˜¾ç¤ºå¿ƒè„å¢å¤§ï¼Œè‚Œè‚‰æ— åŠ›ã€‚|**å°å¼ å¯èƒ½æ‚£æœ‰å¿ƒè‚Œç‚ï¼Œå»ºè®®å°½å¿«åˆ°åŒ»é™¢è¿›è¡Œæ£€æŸ¥å’Œæ²»ç–—ã€‚æ²»ç–—æ–¹æ¡ˆåŒ…æ‹¬è¯ç‰©æ²»ç–—å’Œæ‰‹æœ¯æ²»ç–—ã€‚**|\n|éº»é£ç—…å’Œå„¿ç«¥å“®å–˜çš„ç—…å› æ˜¯å¦ä¸€è‡´ï¼Ÿ|å„¿ç«¥å“®å–˜å’Œéº»é£ç—…çš„ç—…å› æ˜¯ä¸€è‡´ï¼Œä½†æ˜¯å„¿ç«¥å“®å–˜å’Œéº»é£ç—…çš„ç—…å› ä¸ä¸€è‡´|æ˜¯çš„ï¼Œå“®å–˜å’Œå„¿ç«¥å–˜æ¯æœ‰ç›¸ä¼¼çš„åŸå› ã€‚| **ä¸ä¸€è‡´ï¼Œéº»é£ç—…çš„ç—…å› æ˜¯éº»é£æ†èŒæ„ŸæŸ“ï¼Œè€Œå„¿ç«¥å“®å–˜çš„ç—…å› æ˜¯å–‚å…»ä¸è‰¯ã€å¸çƒŸç­‰å› ç´ ã€‚**|\n|ä¸€ä½æ‚£è€…å‡ºç°äº†æŒç»­æ€§çš„èƒ€ç—›å’Œé»„ç–¸ï¼Œç»å½±åƒå­¦æ£€æŸ¥åç¡®è¯Šä¸ºè‚èƒ†ç®¡ç»“çŸ³ç—…ï¼Œåº”è¯¥é‡‡å–å“ªäº›æ²»ç–—æ–¹æ¡ˆï¼Ÿ|è‚èƒ†ç®¡ç»“çŸ³ç—…æ‚£è€…åº”è¯¥é‡‡å–å“ªäº›æ²»ç–—æ–¹æ¡ˆï¼Ÿ|æ‚£è€…åº”æ¥å—æ‰‹æœ¯å’ŒåŒ–ç–—ç›¸ç»“åˆçš„æ²»ç–—ã€‚| **é’ˆå¯¹è‚èƒ†ç®¡ç»“çŸ³ç—…ï¼Œæ²»ç–—æ–¹æ¡ˆåŒ…æ‹¬æ‰‹æœ¯æ²»ç–—ã€è¯ç‰©æ²»ç–—å’Œè¾…åŠ©æ²»ç–—ç­‰ã€‚å…·ä½“æ²»ç–—æ–¹æ¡ˆéœ€è¦æ ¹æ®æ‚£è€…çš„å…·ä½“æƒ…å†µè€Œå®šã€‚**|\n\n##FAQs\n\n1. \tQ: Why was it renamed to \"Bentsao\"?\n\t\n\tA: For naming consistency with the SCIR large language model series, the name of the Chinese medical model has been renamed to \"Bentsao\".\n\n2. \tQ: Why \"Huatuo(åé©¼)\" instead of \"Huatuo(åä½—)\"?\n\t\n\tA: We chose the name \"Huatuo(åé©¼)\" because our base model, LLaMA(ç¾æ´²é©¼), and Alpaca(ç¾Šé©¼). Inspired by their names and a pun on the name \"tuoé©¼\" & \"tuoä½—\", we named our model \"Huatuo(åé©¼)\".\n\n3. \tQ: Does it use Chinese traditional medicine theories or data?\n\t\n\tA: Not at the moment.\n\n4.\tQ: The results from the model vary and are limited in effectiveness.\n\n\tA: Due to the diversity considerations of the generative model, results may vary with multiple runs. The current open-source model, owing to the limited Chinese corpus in LLaMA and Alpaca, and a rather coarse way of knowledge integration, may yield inconsistent results. Please try the bloom-based and character-based models.\n5. Q: The model cannot run or the inferred content is completely unacceptable.\n\t\n\tA: Please ensure that you've installed the dependencies from the requirements, set up the CUDA environment and added the environment variables, and correctly input the downloaded model and Lora's storage location. Any repetition or partial mistakes in the inferred content are occasional issues with the llama-based model and relate to LLaMA's ability in Chinese, training data scale, and hyperparameter settings. Please try the new character-based model. If there are severe issues, please describe in detail the filename, model name, Lora configuration, etc., in an issue. Thank you all.\nQ: Among the released models, which one is the best?\nA: Based on our experience, the character-based model seems to perform relatively better.\n\n\n\n\n\n\n## Contributors\n\nThis project was founded by the Health Intelligence Group of the Research Center for Social Computing and Information Retrieval at Harbin Institute of Technology, including [Haochun Wang](https://github.com/s65b40), [Yanrui Du](https://github.com/DYR1), [Chi Liu](https://github.com/thinksoso), [Rui Bai](https://github.com/RuiBai1999), [Nuwa Xi](https://github.com/rootnx), [Yuhan Chen](https://github.com/Imsovegetable), [Zewen Qiang](https://github.com/1278882181), [Jianyu Chen](https://github.com/JianyuChen01), [Zijian Li](https://github.com/FlowolfzzZ) supervised by Associate Professor [Sendong Zhao](http://homepage.hit.edu.cn/stanzhao?lang=zh), Professor Bing Qin, and Professor Ting Liu.\n\n\n## Acknowledgements\n\nThis project has referred the following open-source projects. We would like to express our gratitude to the developers and researchers involved in those projects.\n\n- Facebook LLaMA: https://github.com/facebookresearch/llama\n- Stanford Alpaca: https://github.com/tatsu-lab/stanford_alpaca\n- alpaca-lora by @tloen: https://github.com/tloen/alpaca-lora\n- CMeKG https://github.com/king-yyf/CMeKG_tools\n- æ–‡å¿ƒä¸€è¨€ https://yiyan.baidu.com/welcome The logo of this project is automatically generated by Wenxin Yiyan.\n\n## Disclaimer\nThe resources related to this project are for academic research purposes only and strictly prohibited for commercial use. When using portions of third-party code, please strictly comply with the corresponding open source licenses. The content generated by the model is influenced by factors such as model computation, randomness, and quantization accuracy loss, and this project cannot guarantee its accuracy. The vast majority of the dataset used in this project is generated by the model, and even if it conforms to certain medical facts, it cannot be used as the basis for actual medical diagnosis. This project does not assume any legal liability for the content output by the model, nor is it responsible for any losses that may be incurred as a result of using the related resources and output results.\n\n\n## Citation\nIf you use the data or code from this project, please declare the reference:\n\n```\n@misc{wang2023huatuo,\n      title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge}, \n      author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},\n      year={2023},\n      eprint={2304.06975},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "data-literature",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "export_hf_checkpoint.py",
          "type": "blob",
          "size": 1.4736328125,
          "content": "import os\n\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import LlamaForCausalLM, LlamaTokenizer  # noqa: F402\n\nBASE_MODEL = os.environ.get(\"BASE_MODEL\", None)\nassert (\n    BASE_MODEL\n), \"Please specify a value for BASE_MODEL environment variable, e.g. `export BASE_MODEL=decapoda-research/llama-7b-hf`\"  # noqa: E501\n\ntokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n\nbase_model = LlamaForCausalLM.from_pretrained(\n    BASE_MODEL,\n    load_in_8bit=False,\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cpu\"},\n)\n\nfirst_weight = base_model.model.layers[0].self_attn.q_proj.weight\nfirst_weight_old = first_weight.clone()\n\nlora_model = PeftModel.from_pretrained(\n    base_model,\n    \"tloen/alpaca-lora-7b\",\n    device_map={\"\": \"cpu\"},\n    torch_dtype=torch.float16,\n)\n\nlora_weight = lora_model.base_model.model.model.layers[\n    0\n].self_attn.q_proj.weight\n\nassert torch.allclose(first_weight_old, first_weight)\n\n# merge weights\nfor layer in lora_model.base_model.model.model.layers:\n    layer.self_attn.q_proj.merge_weights = True\n    layer.self_attn.v_proj.merge_weights = True\n\nlora_model.train(False)\n\n# did we do anything?\nassert not torch.allclose(first_weight_old, first_weight)\n\nlora_model_sd = lora_model.state_dict()\ndeloreanized_sd = {\n    k.replace(\"base_model.model.\", \"\"): v\n    for k, v in lora_model_sd.items()\n    if \"lora\" not in k\n}\n\nLlamaForCausalLM.save_pretrained(\n    base_model, \"./hf_ckpt\", state_dict=deloreanized_sd, max_shard_size=\"400MB\"\n)\n"
        },
        {
          "name": "export_state_dict_checkpoint.py",
          "type": "blob",
          "size": 3.5263671875,
          "content": "import json\nimport os\n\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import LlamaForCausalLM, LlamaTokenizer  # noqa: E402\n\nBASE_MODEL = os.environ.get(\"BASE_MODEL\", None)\nassert (\n    BASE_MODEL\n), \"Please specify a value for BASE_MODEL environment variable, e.g. `export BASE_MODEL=decapoda-research/llama-7b-hf`\"  # noqa: E501\n\ntokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n\nbase_model = LlamaForCausalLM.from_pretrained(\n    BASE_MODEL,\n    load_in_8bit=False,\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cpu\"},\n)\n\nlora_model = PeftModel.from_pretrained(\n    base_model,\n    \"tloen/alpaca-lora-7b\",\n    device_map={\"\": \"cpu\"},\n    torch_dtype=torch.float16,\n)\n\n# merge weights\nfor layer in lora_model.base_model.model.model.layers:\n    layer.self_attn.q_proj.merge_weights = True\n    layer.self_attn.v_proj.merge_weights = True\n\nlora_model.train(False)\n\nlora_model_sd = lora_model.state_dict()\n\nparams = {\n    \"dim\": 4096,\n    \"multiple_of\": 256,\n    \"n_heads\": 32,\n    \"n_layers\": 32,\n    \"norm_eps\": 1e-06,\n    \"vocab_size\": -1,\n}\nn_layers = params[\"n_layers\"]\nn_heads = params[\"n_heads\"]\ndim = params[\"dim\"]\ndims_per_head = dim // n_heads\nbase = 10000.0\ninv_freq = 1.0 / (\n    base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head)\n)\n\n\ndef permute(w):\n    return (\n        w.view(n_heads, dim // n_heads // 2, 2, dim)\n        .transpose(1, 2)\n        .reshape(dim, dim)\n    )\n\n\ndef unpermute(w):\n    return (\n        w.view(n_heads, 2, dim // n_heads // 2, dim)\n        .transpose(1, 2)\n        .reshape(dim, dim)\n    )\n\n\ndef translate_state_dict_key(k):  # noqa: C901\n    k = k.replace(\"base_model.model.\", \"\")\n    if k == \"model.embed_tokens.weight\":\n        return \"tok_embeddings.weight\"\n    elif k == \"model.norm.weight\":\n        return \"norm.weight\"\n    elif k == \"lm_head.weight\":\n        return \"output.weight\"\n    elif k.startswith(\"model.layers.\"):\n        layer = k.split(\".\")[2]\n        if k.endswith(\".self_attn.q_proj.weight\"):\n            return f\"layers.{layer}.attention.wq.weight\"\n        elif k.endswith(\".self_attn.k_proj.weight\"):\n            return f\"layers.{layer}.attention.wk.weight\"\n        elif k.endswith(\".self_attn.v_proj.weight\"):\n            return f\"layers.{layer}.attention.wv.weight\"\n        elif k.endswith(\".self_attn.o_proj.weight\"):\n            return f\"layers.{layer}.attention.wo.weight\"\n        elif k.endswith(\".mlp.gate_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w1.weight\"\n        elif k.endswith(\".mlp.down_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w2.weight\"\n        elif k.endswith(\".mlp.up_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w3.weight\"\n        elif k.endswith(\".input_layernorm.weight\"):\n            return f\"layers.{layer}.attention_norm.weight\"\n        elif k.endswith(\".post_attention_layernorm.weight\"):\n            return f\"layers.{layer}.ffn_norm.weight\"\n        elif k.endswith(\"rotary_emb.inv_freq\") or \"lora\" in k:\n            return None\n        else:\n            print(layer, k)\n            raise NotImplementedError\n    else:\n        print(k)\n        raise NotImplementedError\n\n\nnew_state_dict = {}\nfor k, v in lora_model_sd.items():\n    new_k = translate_state_dict_key(k)\n    if new_k is not None:\n        if \"wq\" in new_k or \"wk\" in new_k:\n            new_state_dict[new_k] = unpermute(v)\n        else:\n            new_state_dict[new_k] = v\n\nos.makedirs(\"./ckpt\", exist_ok=True)\n\ntorch.save(new_state_dict, \"./ckpt/consolidated.00.pth\")\n\nwith open(\"./ckpt/params.json\", \"w\") as f:\n    json.dump(params, f)\n"
        },
        {
          "name": "finetune.py",
          "type": "blob",
          "size": 10.056640625,
          "content": "import os\nimport sys\nfrom typing import List\n\nimport fire\nimport wandb\nimport torch\nimport transformers\nfrom datasets import load_dataset\n\n\"\"\"\nUnused imports:\nimport torch.nn as nn\nimport bitsandbytes as bnb\n\"\"\"\n\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import Prompter\n\nfrom transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n\n\ndef train(\n    # model/data params\n    base_model: str = \"\",  # the only required argument\n    data_path: str = \"yahma/alpaca-cleaned\",\n    output_dir: str = \"./lora-alpaca\",\n    # training hyperparams\n    batch_size: int = 128,\n    micro_batch_size: int = 8,\n    num_epochs: int = 10,\n    learning_rate: float = 3e-4,\n    cutoff_len: int = 256,\n    val_set_size: int = 500,\n    # lora hyperparams\n    lora_r: int = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n    lora_target_modules: List[str] = [\n        \"q_proj\",\n        \"v_proj\",\n    ],\n    # llm hyperparams\n    train_on_inputs: bool = False,  # if False, masks out inputs in loss\n    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n    # wandb params\n    wandb_project: str = \"llama_med\",\n    wandb_run_name: str = \"\",\n    wandb_watch: str = \"\",  # options: false | gradients | all\n    wandb_log_model: str = \"\",  # options: false | true\n    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n):\n    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n        print(\n            f\"Training Alpaca-LoRA model with params:\\n\"\n            f\"base_model: {base_model}\\n\"\n            f\"data_path: {data_path}\\n\"\n            f\"output_dir: {output_dir}\\n\"\n            f\"batch_size: {batch_size}\\n\"\n            f\"micro_batch_size: {micro_batch_size}\\n\"\n            f\"num_epochs: {num_epochs}\\n\"\n            f\"learning_rate: {learning_rate}\\n\"\n            f\"cutoff_len: {cutoff_len}\\n\"\n            f\"val_set_size: {val_set_size}\\n\"\n            f\"lora_r: {lora_r}\\n\"\n            f\"lora_alpha: {lora_alpha}\\n\"\n            f\"lora_dropout: {lora_dropout}\\n\"\n            f\"lora_target_modules: {lora_target_modules}\\n\"\n            f\"train_on_inputs: {train_on_inputs}\\n\"\n            f\"group_by_length: {group_by_length}\\n\"\n            f\"wandb_project: {wandb_project}\\n\"\n            f\"wandb_run_name: {wandb_run_name}\\n\"\n            f\"wandb_watch: {wandb_watch}\\n\"\n            f\"wandb_log_model: {wandb_log_model}\\n\"\n            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n            f\"prompt template: {prompt_template_name}\\n\"\n        )\n    assert (\n        base_model\n    ), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n    gradient_accumulation_steps = batch_size // micro_batch_size\n\n    prompter = Prompter(prompt_template_name)\n\n    device_map = \"auto\"\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if ddp:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n\n    # Check if parameter passed or if set within environ\n    use_wandb = len(wandb_project) > 0 or (\n        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n    )\n    # Only overwrite environ if wandb param passed\n    if len(wandb_project) > 0:\n        os.environ[\"WANDB_PROJECT\"] = wandb_project\n    if len(wandb_watch) > 0:\n        os.environ[\"WANDB_WATCH\"] = wandb_watch\n    if len(wandb_log_model) > 0:\n        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=True,\n        torch_dtype=torch.float16,\n        device_map=device_map,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different from the eos token\n    )\n    tokenizer.padding_side = \"left\"  # Allow batched inference\n\n    def tokenize(prompt, add_eos_token=True):\n        # there's probably a way to do this with the tokenizer settings\n        # but again, gotta move fast\n        result = tokenizer(\n            prompt,\n            truncation=True,\n            max_length=cutoff_len,\n            padding=False,\n            return_tensors=None,\n        )\n        if (\n            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < cutoff_len\n            and add_eos_token\n        ):\n            result[\"input_ids\"].append(tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n\n        return result\n\n    def generate_and_tokenize_prompt(data_point):\n        full_prompt = prompter.generate_prompt(\n            data_point[\"instruction\"],\n            data_point[\"input\"],\n            data_point[\"output\"],\n        )\n        tokenized_full_prompt = tokenize(full_prompt)\n        if not train_on_inputs:\n            user_prompt = prompter.generate_prompt(\n                data_point[\"instruction\"], data_point[\"input\"]\n            )\n            tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n\n            tokenized_full_prompt[\"labels\"] = [\n                -100\n            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n                user_prompt_len:\n            ]  # could be sped up, probably\n        return tokenized_full_prompt\n\n    model = prepare_model_for_int8_training(model)\n\n    config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        target_modules=lora_target_modules,\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model = get_peft_model(model, config)\n\n    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n        data = load_dataset(\"json\", data_files=data_path)\n    else:\n        data = load_dataset(data_path)\n\n    if resume_from_checkpoint:\n        # Check the available weights and load them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint, \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n            checkpoint_name = os.path.join(\n                resume_from_checkpoint, \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config above has to fit\n            resume_from_checkpoint = (\n                False  # So the trainer won't try loading its state\n            )\n        # The two files above have a different name depending on how they were saved, but are actually the same.\n        if os.path.exists(checkpoint_name):\n            print(f\"Restarting from {checkpoint_name}\")\n            adapters_weights = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model, adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name} not found\")\n\n    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n\n    if val_set_size > 0:\n        train_val = data[\"train\"].train_test_split(\n            test_size=val_set_size, shuffle=True, seed=2023\n        )\n        train_data = (\n            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n        )\n        val_data = (\n            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n        )\n    else:\n        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n        val_data = None\n\n    if not ddp and torch.cuda.device_count() > 1:\n        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n        model.is_parallelizable = True\n        model.model_parallel = True\n\n    class SavePeftModelCallback(TrainerCallback):\n        def on_save(\n            self,\n            args: TrainingArguments,\n            state: TrainerState,\n            control: TrainerControl,\n            **kwargs,\n        ):\n            checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n            kwargs[\"model\"].save_pretrained(checkpoint_folder)\n            pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n            if os.path.exists(pytorch_model_path):\n                os.remove(pytorch_model_path)\n            return control\n\n    trainer = transformers.Trainer(\n        model=model,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n        args=transformers.TrainingArguments(\n            per_device_train_batch_size=micro_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            warmup_ratio=0.1,\n            num_train_epochs=num_epochs,\n            learning_rate=learning_rate,\n            fp16=True,\n            logging_steps=8,\n            optim=\"adamw_torch\",\n            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n            save_strategy=\"steps\",\n            eval_steps=32 if val_set_size > 0 else None,\n            save_steps=32,\n            output_dir=output_dir,\n            save_total_limit=5,\n            load_best_model_at_end=True if val_set_size > 0 else False,\n            ddp_find_unused_parameters=False if ddp else None,\n            group_by_length=group_by_length,\n            report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n        ),\n        callbacks=[SavePeftModelCallback],\n    )\n    model.config.use_cache = False\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n    model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's a warning about missing keys above, please disregard :)\"\n    )\n\n\nif __name__ == \"__main__\":\n    fire.Fire(train)\n"
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 5.361328125,
          "content": "import sys\n\nimport fire\nimport gradio as gr\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import Prompter\n\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\ntry:\n    if torch.backends.mps.is_available():\n        device = \"mps\"\nexcept:  # noqa: E722\n    pass\n\n\ndef main(\n    load_8bit: bool = False,\n    base_model: str = \"\",\n    lora_weights: str = \"tloen/alpaca-lora-7b\",\n    prompt_template: str = \"med_template\",  # The prompt template to use, will default to alpaca.\n    server_name: str = \"0.0.0.0\",  # Allows to listen on all interfaces by providing '0.0.0.0'\n    share_gradio: bool = True,\n):\n    assert (\n        base_model\n    ), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n\n    prompter = Prompter(prompt_template)\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n    if device == \"cuda\":\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model,\n            load_in_8bit=load_8bit,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            torch_dtype=torch.float16,\n        )\n    elif device == \"mps\":\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model,\n            device_map={\"\": device},\n            torch_dtype=torch.float16,\n        )\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            device_map={\"\": device},\n            torch_dtype=torch.float16,\n        )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n        )\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            device_map={\"\": device},\n        )\n\n    # unwind broken decapoda-research config\n    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n    model.config.bos_token_id = 1\n    model.config.eos_token_id = 2\n\n    if not load_8bit:\n        model.half()  # seems to fix bugs for some users.\n\n    model.eval()\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    def evaluate(\n        instruction,\n        input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n        max_new_tokens=128,\n        **kwargs,\n    ):\n        prompt = prompter.generate_prompt(instruction, input)\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(device)\n        generation_config = GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            num_beams=num_beams,\n            **kwargs,\n        )\n        with torch.no_grad():\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=max_new_tokens,\n            )\n        s = generation_output.sequences[0]\n        output = tokenizer.decode(s)\n        return prompter.get_response(output)\n\n    gr.Interface(\n        fn=evaluate,\n        inputs=[\n            gr.components.Textbox(\n                lines=2,\n                label=\"Instruction\",\n                placeholder=\"Tell me about alpacas.\",\n            ),\n            gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n            gr.components.Slider(\n                minimum=0, maximum=1, value=0.1, label=\"Temperature\"\n            ),\n            gr.components.Slider(\n                minimum=0, maximum=1, value=0.75, label=\"Top p\"\n            ),\n            gr.components.Slider(\n                minimum=0, maximum=100, step=1, value=40, label=\"Top k\"\n            ),\n            gr.components.Slider(\n                minimum=1, maximum=4, step=1, value=4, label=\"Beams\"\n            ),\n            gr.components.Slider(\n                minimum=1, maximum=2000, step=1, value=128, label=\"Max tokens\"\n            ),\n        ],\n        outputs=[\n            gr.inputs.Textbox(\n                lines=5,\n                label=\"Output\",\n            )\n        ],\n        title=\"BenTsao\",\n        description=\"\",  # noqa: E501\n    ).launch(server_name=server_name, share=share_gradio)\n    # Old testing code follows.\n\n    \"\"\"\n    # testing code for readme\n    for instruction in [\n        \"Tell me about alpacas.\",\n        \"Tell me about the president of Mexico in 2019.\",\n        \"Tell me about the king of France in 2019.\",\n        \"List all Canadian provinces in alphabetical order.\",\n        \"Write a Python program that prints the first 10 Fibonacci numbers.\",\n        \"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n        \"Tell me five words that rhyme with 'shock'.\",\n        \"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n        \"Count up from 1 to 500.\",\n    ]:\n        print(\"Instruction:\", instruction)\n        print(\"Response:\", evaluate(instruction))\n        print()\n    \"\"\"\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"
        },
        {
          "name": "infer.py",
          "type": "blob",
          "size": 3.8271484375,
          "content": "import sys\nimport json\nimport fire\nimport gradio as gr\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import Prompter\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n\ndef load_instruction(instruct_dir):\n    input_data = []\n    with open(instruct_dir, \"r\") as f:\n        lines = f.readlines()\n        for line in lines:\n            line = line.strip()\n            d = json.loads(line)\n            input_data.append(d)\n    return input_data\n\n\ndef main(\n    load_8bit: bool = False,\n    base_model: str = \"\",\n    # the infer data, if not exists, infer the default instructions in code\n    instruct_dir: str = \"\",\n    use_lora: bool = True,\n    lora_weights: str = \"tloen/alpaca-lora-7b\",\n    # The prompt template to use, will default to med_template.\n    prompt_template: str = \"med_template\",\n):\n    prompter = Prompter(prompt_template)\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=load_8bit,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    if use_lora:\n        print(f\"using lora {lora_weights}\")\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            torch_dtype=torch.float16,\n        )\n    # unwind broken decapoda-research config\n    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n    model.config.bos_token_id = 1\n    model.config.eos_token_id = 2\n    if not load_8bit:\n        model.half()  # seems to fix bugs for some users.\n\n    model.eval()\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    def evaluate(\n        instruction,\n        input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n        max_new_tokens=256,\n        **kwargs,\n    ):\n        prompt = prompter.generate_prompt(instruction, input)\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(device)\n        generation_config = GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            num_beams=num_beams,\n            **kwargs,\n        )\n        with torch.no_grad():\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=max_new_tokens,\n            )\n        s = generation_output.sequences[0]\n        output = tokenizer.decode(s)\n        return prompter.get_response(output)\n\n    def infer_from_json(instruct_dir):\n        input_data = load_instruction(instruct_dir)\n        for d in input_data:\n            instruction = d[\"instruction\"]\n            output = d[\"output\"]\n            print(\"###infering###\")\n            model_output = evaluate(instruction)\n            print(\"###instruction###\")\n            print(instruction)\n            print(\"###golden output###\")\n            print(output)\n            print(\"###model output###\")\n            print(model_output)\n\n    if instruct_dir != \"\":\n        infer_from_json(instruct_dir)\n    else:\n        for instruction in [\n            \"æˆ‘æ„Ÿå†’äº†ï¼Œæ€ä¹ˆæ²»ç–—\",\n            \"ä¸€ä¸ªæ‚£æœ‰è‚è¡°ç«­ç»¼åˆå¾çš„ç—…äººï¼Œé™¤äº†å¸¸è§çš„ä¸´åºŠè¡¨ç°å¤–ï¼Œè¿˜æœ‰å“ªäº›ç‰¹æ®Šçš„ä½“å¾ï¼Ÿ\",\n            \"æ€¥æ€§é˜‘å°¾ç‚å’Œç¼ºè¡€æ€§å¿ƒè„ç—…çš„å¤šå‘ç¾¤ä½“æœ‰ä½•ä¸åŒï¼Ÿ\",\n            \"å°ææœ€è¿‘å‡ºç°äº†å¿ƒåŠ¨è¿‡é€Ÿçš„ç—‡çŠ¶ï¼Œä¼´æœ‰è½»åº¦èƒ¸ç—›ã€‚ä½“æ£€å‘ç°P-Ré—´æœŸå»¶é•¿ï¼Œä¼´æœ‰Tæ³¢ä½å¹³å’ŒSTæ®µå¼‚å¸¸\",\n        ]:\n            print(\"Instruction:\", instruction)\n            print(\"Response:\", evaluate(instruction))\n            print()\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"
        },
        {
          "name": "infer_literature.py",
          "type": "blob",
          "size": 4.126953125,
          "content": "import sys\nimport json\n\nimport fire\nimport gradio as gr\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import Prompter\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n\ndef load_instruction(instruct_dir):\n    input_data = []\n    with open(instruct_dir, \"r\") as f:\n        lines = f.readlines()\n        for line in lines:\n            line = line.strip()\n            d = json.loads(line)\n            input_data.append(d)\n    return input_data\n\n\ndef main(\n    load_8bit: bool = False,\n    base_model: str = \"\",\n    # the infer data, if not exists, infer the default instructions in code\n    single_or_multi: str = \"\",\n    use_lora: bool = True,\n    lora_weights: str = \"tloen/alpaca-lora-7b\",\n    # The prompt template to use, will default to med_template.\n    prompt_template: str = \"med_template\",\n):\n    prompter = Prompter(prompt_template)\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=load_8bit,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    if use_lora:\n        print(f\"using lora {lora_weights}\")\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            torch_dtype=torch.float16,\n        )\n    # unwind broken decapoda-research config\n    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n    model.config.bos_token_id = 1\n    model.config.eos_token_id = 2\n    if not load_8bit:\n        model.half()  # seems to fix bugs for some users.\n\n    model.eval()\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    def evaluate(\n        instruction,\n        input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n        max_new_tokens=256,\n        **kwargs,\n    ):\n        prompt = prompter.generate_prompt(instruction, input)\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(device)\n        generation_config = GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            num_beams=num_beams,\n            **kwargs,\n        )\n        with torch.no_grad():\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=max_new_tokens,\n            )\n        s = generation_output.sequences[0]\n        output = tokenizer.decode(s)\n        return prompter.get_response(output)\n\n    if single_or_multi == \"multi\":\n        response=\"\"\n        instruction=\"\"\n        for _ in range(0,5):  \n            inp=input(\"è¯·è¾“å…¥:\")\n            inp=\"<user>: \" + inp\n            instruction=instruction+inp\n            response=evaluate(instruction)\n            response=response.replace('\\n','')\n            print(\"Response:\", response)\n            instruction= instruction + \" <bot>: \" + response\n\n\n    elif single_or_multi == \"single\":\n        for instruction in [\n            \"è‚ç™Œæ˜¯ä»€ä¹ˆï¼Ÿæœ‰å“ªäº›ç—‡çŠ¶å’Œè¿¹è±¡ï¼Ÿ\",\n            \"è‚ç™Œæ˜¯å¦‚ä½•è¯Šæ–­çš„ï¼Ÿæœ‰å“ªäº›æ£€æŸ¥å’Œæµ‹è¯•å¯ä»¥å¸®åŠ©è¯Šæ–­ï¼Ÿ\",\n            \"Sorafenibæ˜¯ä¸€ç§å£æœçš„å¤šé¶ç‚¹é…ªæ°¨é…¸æ¿€é…¶æŠ‘åˆ¶å‰‚ï¼Œå®ƒçš„ä½œç”¨æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ\",\n            \"Regorafenibæ˜¯ä¸€ç§å£æœçš„å¤šé¶ç‚¹é…ªæ°¨é…¸æ¿€é…¶æŠ‘åˆ¶å‰‚ï¼Œå®ƒçš„ä½œç”¨æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå’ŒSorafenibæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\",\n            \"è‚ç™Œè¯ç‰©æ²»ç–—çš„å‰¯ä½œç”¨æœ‰å“ªäº›ï¼Ÿå¦‚ä½•ç¼“è§£è¿™äº›å‰¯ä½œç”¨ï¼Ÿ\",\n            \"è‚ç™Œè¯ç‰©æ²»ç–—çš„è´¹ç”¨é«˜æ˜‚ï¼Œå¦‚ä½•é™ä½æ²»ç–—çš„ç»æµè´Ÿæ‹…ï¼Ÿ\",\n            \"æˆ‘æƒ³äº†è§£ä¸€ä¸‹Î²-è°·ç”¾é†‡æ˜¯å¦å¯ä½œä¸ºè‚ç™Œçš„æ²»ç–—è¯ç‰©\",\n            \"èƒ½ä»‹ç»ä¸€ä¸‹æœ€è¿‘Hsaï¼¿circï¼¿0008583åœ¨è‚ç»†èƒç™Œæ²»ç–—ä¸­çš„æ½œåœ¨åº”ç”¨çš„ç ”ç©¶ä¹ˆ?\"\n        ]:  \n            print(\"instruction:\",instruction)\n            instruction=\"<user>: \"+instruction\n            print(\"Response:\", evaluate(instruction))\n          \n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2060546875,
          "content": "accelerate==0.20.1\nappdirs==1.4.4\nbitsandbytes==0.37.2\nblack==23.3.0\nblack[jupyter]==23.3.0\ndatasets==2.11.0\nfire==0.5.0\npeft==0.3.0\ntransformers==4.30.1\ngradio==3.33.1\nsentencepiece==0.1.97\nscipy==1.10.1\nwandb\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}