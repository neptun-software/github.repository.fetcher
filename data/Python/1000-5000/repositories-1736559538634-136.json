{
  "metadata": {
    "timestamp": 1736559538634,
    "page": 136,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "SCIR-HI/Huatuo-Llama-Med-Chinese",
      "stars": 4642,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1513671875,
          "content": "out/\n7B/\n13B/\n__pycache__/\ncheckpoint**\nminimal-llama**\nlora-llama-med\nupload.py\nlora-**\n*.out\n*result\n*ckpt\nwandb\ntodo.txt\n.vscode/\n*tmp*\n.DS_Store\n.idea\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.12890625,
          "content": "                                Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.765625,
          "content": "[**中文**](./README.md) | [**English**](./README_EN.md)\n\n<p align=\"center\" width=\"100%\">\n<a href=\"https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/\" target=\"_blank\"><img src=\"assets/logo/logo_new.png\" alt=\"SCIR-HI-HuaTuo\" style=\"width: 60%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n  \n\n# 本草[原名：华驼(HuaTuo)]: 基于中文医学知识的大语言模型指令微调\n\n### BenTsao (original name: HuaTuo): Instruction-tuning Large Language Models With Chinese Medical Knowledge\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/blob/main/LICENSE) [![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n\n  \n本项目开源了经过中文医学指令精调/指令微调(Instruction-tuning) 的大语言模型集，包括LLaMA、Alpaca-Chinese、Bloom、活字模型等。\n\n\n我们基于医学知识图谱以及医学文献，结合ChatGPT API构建了中文医学指令微调数据集，并以此对各种基模型进行了指令微调，提高了基模型在医疗领域的问答效果。\n\n\n## News\n**[2023/09/24]发布[《面向智慧医疗的大语言模型微调技术》](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/tree/main/doc/Tuning_Methods_for_LLMs_towards_Health_Intelligence.pdf)**\n\n**[2023/09/12]在arxiv发布[《探索大模型从医学文献中交互式知识的获取》](https://arxiv.org/pdf/2309.04198.pdf)**\n\n**[2023/09/08]在arxiv发布[《基于知识微调的大语言模型可靠中文医学回复生成方法》](https://arxiv.org/pdf/2309.04175.pdf)**  \n\n**[2023/08/07] 🔥🔥增加了基于[活字](https://github.com/HIT-SCIR/huozi)进行指令微调的模型发布，模型效果显著提升。🔥🔥**\n\n[2023/08/05] 本草模型在CCL 2023 Demo Track进行Poster展示。\n\n [2023/08/03] SCIR实验室开源[活字](https://github.com/HIT-SCIR/huozi)通用问答模型，欢迎大家关注🎉🎉\n\n[2023/07/19] 增加了基于[Bloom](https://huggingface.co/bigscience/bloom-7b1)进行指令微调的模型发布。\n\n[2023/05/12] 模型由\"华驼\"更名为\"本草\"。\n\n[2023/04/28] 增加了基于[中文Alpaca大模型](https://github.com/ymcui/Chinese-LLaMA-Alpaca)进行指令微调的模型发布。\n\n[2023/04/24] 增加了基于LLaMA和医学文献进行指令微调的模型发布。\n\n[2023/03/31] 增加了基于LLaMA和医学知识库进行指令微调的模型发布。\n\n\n## A Quick Start\n\n首先安装依赖包，python环境建议3.9+\n\n```\npip install -r requirements.txt\n\n```\n\n针对所有基模型，我们采用了半精度基模型LoRA微调的方式进行指令微调训练，以在计算资源与模型性能之间进行权衡。\n\n### 基模型\n - [活字1.0](https://github.com/HIT-SCIR/huozi)，哈尔滨工业大学基于Bloom-7B二次开发的中文通用问答模型\n - [Bloom-7B](https://huggingface.co/bigscience/bloomz-7b1)\n - [Alpaca-Chinese-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca)，基于LLaMA二次开发的中文问答模型\n - [LLaMA-7B](https://huggingface.co/decapoda-research/llama-7b-hf)\n \n### LoRA模型权重下载\n\nLoRA权重可以通过百度网盘或Hugging Face下载：\n\n1. 🔥对活字进行指令微调的LoRA权重文件\n  - 基于医学知识库以及医学问答数据集 [百度网盘](https://pan.baidu.com/s/1BPnDNb1wQZTWy_Be6MfcnA?pwd=m21s)\n2. 对Bloom进行指令微调的LoRA权重文件\n - 基于医学知识库以及医学问答数据集 [百度网盘](https://pan.baidu.com/s/1jPcuEOhesFGYpzJ7U52Fag?pwd=scir)和[Hugging Face](https://huggingface.co/lovepon/lora-bloom-med-bloom)\n3. 对Alpaca进行指令微调的LoRA权重文件\n - 基于医学知识库 [百度网盘](https://pan.baidu.com/s/16oxcjzXnXjDpL8SKihgNxw?pwd=scir)和[Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med)\n - 基于医学知识库和医学文献 [百度网盘](https://pan.baidu.com/s/1HDdK84ASHmzOFlkmypBIJw?pwd=scir)和[Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med-alldata)\n4. 对LLaMA进行指令微调的LoRA权重文件\n - 基于医学知识库 [百度网盘](https://pan.baidu.com/s/1jih-pEr6jzEa6n2u6sUMOg?pwd=jjpf)和[Hugging Face](https://huggingface.co/thinksoso/lora-llama-med)\n - 基于医学文献 [百度网盘](https://pan.baidu.com/s/1jADypClR2bLyXItuFfSjPA?pwd=odsk)和[Hugging Face](https://huggingface.co/lovepon/lora-llama-literature)\n\n\n\n下载LoRA权重并解压，解压后的格式如下：\n\n\n```\n**lora-folder-name**/\n  - adapter_config.json   # LoRA权重配置文件\n  - adapter_model.bin   # LoRA权重文件\n```\n\n基于相同的数据，我们还训练了医疗版本的ChatGLM模型: [ChatGLM-6B-Med](https://github.com/SCIR-HI/Med-ChatGLM)\n\n\n### Infer\n\n我们在`./data/infer.json`中提供了一些测试用例，可以替换成其它的数据集，请注意保持格式一致\n  \n\n运行infer脚本\n\n\n```\n#基于医学知识库\nbash ./scripts/infer.sh\n\n#基于医学文献\n#单轮\nbash ./scripts/infer-literature-single.sh\n\n#多轮\nbash ./scripts/infer-literature-multi.sh\n```\n\ninfer.sh脚本代码如下，请将下列代码中基模型base_model、lora权重lora_weights以及测试数据集路径instruct_dir进行替换后运行\n\n    python infer.py \\\n\t\t    --base_model 'BASE_MODEL_PATH' \\\n\t\t    --lora_weights 'LORA_WEIGHTS_PATH' \\\n\t\t    --use_lora True \\\n\t\t    --instruct_dir 'INFER_DATA_PATH' \\\n\t\t    --prompt_template 'TEMPLATE_PATH'\n \n\n**_提示模板的选择与模型相关，详情如下：_**\n\n| 活字&Bloom                      | LLaMA&Alpaca                                                                          |                                       \n|:------------------------------|:--------------------------------------------------------------------------------------|\n| `templates/bloom_deploy.json` | 基于医学知识库`templates/med_template.json` <br>  基于医学文献`templates/literature_template.json` |\n\n\n\n也可参考`./scripts/test.sh`\n\n  \n\n## 方法\n\n基模型在医学问答场景下效果有限，指令微调是一种高效的使基模型拥有回答人类问题能力的方法。\n\n\n\n### 数据集构建\n#### 医学知识库\n我们采用了公开和自建的中文医学知识库，主要参考了[cMeKG](https://github.com/king-yyf/CMeKG_tools)。\n\n医学知识库围绕疾病、药物、检查指标等构建，字段包括并发症，高危因素，组织学检查，临床症状，药物治疗，辅助治疗等。知识库示例如下:\n\n\n```\n\n{\"中心词\": \"偏头痛\", \"相关疾病\": [\"妊娠合并偏头痛\", \"恶寒发热\"], \"相关症状\": [\"皮肤变硬\", \"头部及眼后部疼痛并能听到连续不断的隆隆声\", \"晨起头痛加重\"], \"所属科室\": [\"中西医结合科\", \"内科\"], \"发病部位\": [\"头部\"]}\n\n```\n\n我们利用GPT3.5接口围绕医学知识库构建问答数据，设置了多种Prompt形式来充分利用知识。\n\n指令微调的训练集数据示例如下：\n\n```\n\n\"问题：一位年轻男性长期使用可卡因，突然出现胸痛、呕吐、出汗等症状，经检查发现心电图反映心肌急性损伤，请问可能患的是什么疾病？治疗方式是什么？\"\n\n回答: 可能患的是心肌梗塞，需要进行维拉帕米、依普利酮、硝酸甘油、ß阻滞剂、吗啡等药物治疗，并进行溶栓治疗、低分子量肝素、钙通道阻滞剂等辅助治疗。此外需要及时停用可卡因等药物，以防止病情加重。\"\n\n```\n\n  \n\n我们提供了模型的训练数据集，共计八千余条，需要注意的是，虽然训练集的构建融入了知识，但是仍存在错误和不完善的地方，后续我们会利用更好的策略迭代更新数据集。\n\n指令微调数据集质量仍有限，后续将进行不断迭代，同时医学知识库和数据集构建代码还在整理中，整理完成将会发布。\n\n#### 医学文献\n\n此外，我们收集了2023年关于肝癌疾病的中文医学文献，利用GPT3.5接口围绕医学文献的【结论】构建多轮问答数据。在·`./data_literature/liver_cancer.json`中我们提供了其中的1k条训练样例。目前，训练样本的质量仍然有限，在后续我们会进一步迭代数据，会以`公开数据集`的形式对外进行发布。训练样本的示例如下：\n\n<p align=\"center\" width=\"100%\">\n\n<a href=\"https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/\" target=\"_blank\"><img src=\"assets/case.png\" alt=\"SCIR-HI-HuaTuo-literature\" style=\"width: 100%; min-width: 300px; display: block; margin: auto;\"></a>\n\n</p>\n\n目前，我们只开放针对\"肝癌\"单个疾病训练的模型参数。在未来，我们计划发布融入文献结论的医学对话数据集，并且会针对“肝胆胰”相关16种疾病训练模型。\n\n相关细节可参考我们的文章：[《探索大模型从医学文献中交互式知识的获取》](https://arxiv.org/pdf/2309.04198.pdf)\n  \n\n### Finetune\n\n如果想用自己的数据集微调大语言模型，请按照`./data/llama_data.json`中的格式构建自己的数据集\n\n运行finetune脚本\n\n```\n\nbash ./scripts/finetune.sh\n\n```\n\n  \n  \n  \n\n## 训练细节\n\n### 计算资源需求参考\n\n基于LLaMA模型的指令微调过程中，我们在一张A100-SXM-80GB显卡上进行了训练，训练总轮次10轮，耗时约2h17m。batch_size=128的情况下显存占用在40G左右。预计3090/4090显卡(24GB显存)以上显卡可以较好支持，根据显存大小来调整batch_size。\n\n\n### 实验过程\n\n超参设置和实验过程可以参考wandb的链接:\n\nhttps://wandb.ai/thinksoso/llama_med/runs/a5wgcnzt/overview?workspace=user-thinksoso\n\n  \n\n## 模型效果对比\n\n注：测试于2023年3月\n  \n| 测试输入 | Llama输出  | Alpaca输出 | BenTsao本草输出                                             |\n|:------------- |:---------------| :-------------|:--------------------------------------------------------|\n|小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。|问题：小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。| 最近，小张一直感觉身体不舒服，有心烦意乱、呼吸急促等症状。体格检查显示心脏增大，肌肉无力。| **小张可能患有心肌炎，建议尽快到医院进行检查和治疗。治疗方案包括药物治疗和手术治疗。**           |\n|麻风病和儿童哮喘的病因是否一致？|儿童哮喘和麻风病的病因是一致，但是儿童哮喘和麻风病的病因不一致|是的，哮喘和儿童喘息有相似的原因。| **不一致，麻风病的病因是麻风杆菌感染，而儿童哮喘的病因是喂养不良、吸烟等因素。**              |\n|一位患者出现了持续性的胀痛和黄疸，经影像学检查后确诊为肝胆管结石病，应该采取哪些治疗方案？|肝胆管结石病患者应该采取哪些治疗方案？|患者应接受手术和化疗相结合的治疗。| **针对肝胆管结石病，治疗方案包括手术治疗、药物治疗和辅助治疗等。具体治疗方案需要根据患者的具体情况而定。** |\n\n\n\n   \n## 常见问题\n1. Q: 为什么更名为\"本草\"？\n   \n   A: 为SCIR实验室大语言模型命名一致性，中文医学大模型名称调整为\"本草\"。\n\n2. Q: 为什么是\"华驼\"而不是\"华佗\"？ \n   \n   A: 叫”驼“是因为我们的基模型LLaMA是美洲驼，Alpaca是羊驼，受他们名字的启发以及华佗的谐音梗，我们将我们的模型起名为华驼。\n\n3. Q: 有使用中医理论或者中医数据吗？\n    \n   A: 目前还没有\n   \n4. Q: 模型运行的结果不同、效果有限\n   \n   A: 由于生成模型生成多样性的考量，多次运行的结果可能会有差异。当前开源的模型由于LLaMA及Alpaca中文语料有限，且知识结合的方式较为粗糙，请大家尝试bloom-based和活字-based的模型。\n   \n5. Q: 模型无法运行/推理内容完全无法接受\n   \n   A: 请确定已安装requirements中的依赖、配置好cuda环境并添加环境变量、正确输入下载好的模型以及lora的存储位置；推理内容如存在重复生成或部分错误内容属于llama-based模型的偶发现象，与llama模型的中文能力、训练数据规模以及超参设置均有一定的关系，请尝试基于活字的新模型。如存在严重问题，请将运行的文件名、模型名、lora等配置信息详细描述在issue中，谢谢大家。\n   \n6.\tQ: 发布的若干模型哪个最好？\n\n    A: 根据我们的经验，基于活字模型的效果相对更好一些。 \n \n\n\n\n## 项目参与者\n\n本项目由哈尔滨工业大学社会计算与信息检索研究中心健康智能组[王昊淳](https://haochun.wang) 、[杜晏睿](https://github.com/DYR1)、[刘驰](https://github.com/thinksoso)、[白睿](https://github.com/RuiBai1999)、[席奴瓦](https://github.com/rootnx)、[陈雨晗](https://github.com/Imsovegetable)、[强泽文](https://github.com/1278882181)、[陈健宇](https://github.com/JianyuChen01)、[李子健](https://github.com/FlowolfzzZ)完成，指导教师为[赵森栋](http://homepage.hit.edu.cn/stanzhao?lang=zh)副教授，秦兵教授以及刘挺教授。\n\n  \n\n## 致谢\n\n  \n\n本项目参考了以下开源项目，在此对相关项目和研究开发人员表示感谢。\n\n- 活字: https://github.com/HIT-SCIR/huozi\n- Facebook LLaMA: https://github.com/facebookresearch/llama\n- Stanford Alpaca: https://github.com/tatsu-lab/stanford_alpaca\n- alpaca-lora by @tloen: https://github.com/tloen/alpaca-lora\n- CMeKG https://github.com/king-yyf/CMeKG_tools\n- 文心一言 https://yiyan.baidu.com/welcome 本项目的logo由文心一言自动生成\n\n  \n\n## 免责声明\n\n本项目相关资源仅供学术研究之用，严禁用于商业用途。使用涉及第三方代码的部分时，请严格遵循相应的开源协议。模型生成的内容受模型计算、随机性和量化精度损失等因素影响，本项目无法对其准确性作出保证。本项目数据集绝大部分由模型生成，即使符合某些医学事实，也不能被用作实际医学诊断的依据。对于模型输出的任何内容，本项目不承担任何法律责任，亦不对因使用相关资源和输出结果而可能产生的任何损失承担责任。\n\n\n## Citation\n\n如果您使用了本项目的数据或者代码，或是我们的工作对您有所帮助，请声明引用\n\n\n首版技术报告: [Huatuo: Tuning llama model with chinese medical knowledge](https://arxiv.org/pdf/2304.06975)\n\n```\n@misc{wang2023huatuo,\n      title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge},\n      author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},\n      year={2023},\n      eprint={2304.06975},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n知识微调：[Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese\n](https://arxiv.org/pdf/2309.04175.pdf)\n\n```\n@misc{wang2023knowledgetuning,\n      title={Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese}, \n      author={Haochun Wang and Sendong Zhao and Zewen Qiang and Zijian Li and Nuwa Xi and Yanrui Du and MuZhen Cai and Haoqiang Guo and Yuhan Chen and Haoming Xu and Bing Qin and Ting Liu},\n      year={2023},\n      eprint={2309.04175},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n医学文献知识获取：[The CALLA Dataset: Probing LLMs’ Interactive Knowledge Acquisition from Chinese Medical Literature](https://arxiv.org/pdf/2309.04198.pdf)\n\n```\n@misc{du2023calla,\n      title={The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature}, \n      author={Yanrui Du and Sendong Zhao and Muzhen Cai and Jianyu Chen and Haochun Wang and Yuhan Chen and Haoqiang Guo and Bing Qin},\n      year={2023},\n      eprint={2309.04198},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 15.0625,
          "content": "[**中文**](./README.md) | [**English**](./README_EN.md)\n<p align=\"center\" width=\"100%\">\n<a href=\"https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/\" target=\"_blank\"><img src=\"assets/logo/logo_new.png\" alt=\"SCIR-HI-HuaTuo\" style=\"width: 60%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n# BenTsao (original name: HuaTuo): Instruction-tuning Large Language Models With Chinese Medical Knowledge\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/blob/main/LICENSE) [![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n\nThis repo open-sources a series of instruction-tuned large language models with Chinese medical instruction datasets, including LLaMA、Alpaca-Chinese、Bloom、Huozi. \n\nWe constructed a Chinese medical instruct-tuning dataset using medical knowledge graphs, medical literatures and the GPT3.5 API, and performed instruction-tuning on various base models with this dataset, improving its question-answering performance in the medical field.\n\n\n## News\n**[2023/08/07] 🔥🔥 Released a model instruction-tuned based on [Huozi](https://github.com/HIT-SCIR/huozi), resulting in a significant improvement in model performance. 🔥🔥**\n\n[2023/08/05] The \"Bentsao\" model was presented as a poster at the CCL 2023 Demo Track.\n\n[2023/08/03] SCIR Lab open-sourced the [Huozi](https://github.com/HIT-SCIR/huozi) general question-answering model. Everyone is welcome to check it out! 🎉🎉\n\n[2023/07/19] Released a model instruction-tuned based on [Bloom](https://huggingface.co/bigscience/bloom-7b1).\n\n[2023/05/12] The model was renamed from \"Huatuo\" to \"Bentsao\".\n\n[2023/04/28] Released a model instruction-tuned based on the [Chinese-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca).\n\n[2023/04/24] Released a model instruction-tuned based on LLaMA with medical literature.\n\n[2023/03/31] Released a model instruction-tuned based on LLaMA with a medical knowledge base.\n\n\n\n\n\n\n\n## A Quick Start\nFirstly, install the required packages. It is recommended to use Python 3.9 or above\n\n```\npip install -r requirements.txt\n```\n\nFor all base models, we adopted the semi-precision base model LoRA fine-tuning method for instruction fine-tuning training, in order to strike a balance between computational resources and model performance.\n\n### Base models\n - [Huozi1.0](https://github.com/HIT-SCIR/huozi), Bloom-7B-based Chinese QA model\n - [Bloom-7B](https://huggingface.co/bigscience/bloomz-7b1)\n - [Alpaca-Chinese-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca), LLaMA-based Chinese QA model\n - [LLaMA-7B](https://huggingface.co/decapoda-research/llama-7b-hf)\n\n\n### LoRA weight download\nLORA weights can be downloaded through Baidu Netdisk or Huggingface.\n\n1. 🔥LoRA for Huozi 1.0\n  - with the medical knowledge base and medical QA dataset [BaiduDisk](https://pan.baidu.com/s/1BPnDNb1wQZTWy_Be6MfcnA?pwd=m21s)\n2. LoRA for Bloom\n - with the medical knowledge base and medical QA dataset [BaiduDisk](https://pan.baidu.com/s/1jPcuEOhesFGYpzJ7U52Fag?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-bloom-med-bloom)\n3. LoRA for Chinese Alpaca\n - with the medical knowledge base [BaiduDisk](https://pan.baidu.com/s/16oxcjzXnXjDpL8SKihgNxw?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med)\n - with the medical knowledge base and medical literature [BaiduDisk](https://pan.baidu.com/s/1HDdK84ASHmzOFlkmypBIJw?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med-alldata)\n4. LoRA for LLaMA\n - with the medical knowledge base [BaiduDisk](https://pan.baidu.com/s/1jih-pEr6jzEa6n2u6sUMOg?pwd=jjpf) and [Hugging Face](https://huggingface.co/thinksoso/lora-llama-med)\n - with medical literature [BaiduDisk](https://pan.baidu.com/s/1jADypClR2bLyXItuFfSjPA?pwd=odsk) and [Hugging Face](https://huggingface.co/lovepon/lora-llama-literature)\n\n\nDownload the LORA weight file and extract it. The format of the extracted file should be as follows:\n\n```\n**lora-folder-name**/\n  - adapter_config.json   # LoRA configuration\n  - adapter_model.bin   # LoRA weight\n```\n\nWe also trained a medical version of ChatGLM: [ChatGLM-6B-Med](https://github.com/SCIR-HI/Med-ChatGLM) based on the same data.\n\n\n### Infer\nWe provided some test cases in `./data/infer.json`, which can be replaced with other datasets. Please make sure to keep the format consistent.\n\nRun the infer script\n\n```\n#Based on medical knowledge base\nbash ./scripts/infer.sh\n\n#Based on medical literature\n#single-epoch\nbash ./scripts/infer-literature-single.sh\n\n#multi-epoch\nbash ./scripts/infer-literature-multi.sh\n```\n\nThe code for the infer.sh script is as follows. Please replace the base model base_model, the LoRA weights lora_weights, and the test dataset path instruct_dir before running\n\n\tpython infer.py \\\n\t\t\t    --base_model 'BASE_MODEL_PATH' \\\n\t\t\t    --lora_weights 'LORA_WEIGHTS_PATH' \\\n\t\t\t    --use_lora True \\\n\t\t\t    --instruct_dir 'INFER_DATA_PATH' \\\n\t\t\t    --prompt_template 'TEMPLATE_PATH'\n\t\t    \n\nThe prompt template is relevant to the model as follows: \n\n| Huozi&Bloom                      | LLaMA&Alpaca                                                                          |                                       \n|:------------------------------|:--------------------------------------------------------------------------------------|\n| `templates/bloom_deploy.json` | with the medical knowledge base`templates/med_template.json` <br>  with the medical literature`templates/literature_template.json` |\n\n\n\nother reference in `./scripts/test.sh`\n\n\t\n## Methodology\nThe base model has limited effectiveness in medical question-answering scenarios. Instruction-tuning is an efficient method to give the base model the ability to answer human questions\n\t    \n### Dataset construction\n#### Medical knowledge base\nWe used both publicly available and self-built Chinese medical knowledge bases, with a primary reference to [cMeKG](https://github.com/king-yyf/CMeKG_tools). The medical knowledge base is built around diseases, drugs, and diagnostic indicators, with fields including complications, risk factors, histological examinations, clinical symptoms, drug treatments, and adjuvant therapies. An example of the knowledge base is shown below:\n\n\n```\n{\"中心词\": \"偏头痛\", \"相关疾病\": [\"妊娠合并偏头痛\", \"恶寒发热\"], \"相关症状\": [\"皮肤变硬\", \"头部及眼后部疼痛并能听到连续不断的隆隆声\", \"晨起头痛加重\"], \"所属科室\": [\"中西医结合科\", \"内科\"], \"发病部位\": [\"头部\"]}\n```\nWe used the GPT3.5 API to build a Question-Answer dataset around a medical knowledge base, with multiple prompts to fully leverage the knowledge.\n\nAn example of the training dataset for instruct-tuning is shown below:\n\n\n```\n\"问题：一位年轻男性长期使用可卡因，突然出现胸痛、呕吐、出汗等症状，经检查发现心电图反映心肌急性损伤，请问可能患的是什么疾病？治疗方式是什么？\"\n回答: 可能患的是心肌梗塞，需要进行维拉帕米、依普利酮、硝酸甘油、ß阻滞剂、吗啡等药物治疗，并进行溶栓治疗、低分子量肝素、钙通道阻滞剂等辅助治疗。此外需要及时停用可卡因等药物，以防止病情加重。\"\n```\n\nWe provided a training dataset for the model, consisting of more than eight thousand entries. It should be noted that although knowledge has been incorporated into the construction of the training set, there are still errors and imperfections. We will use better strategies to iteratively update the dataset in the future.\n\nThe quality of the dataset for instruct-tuning is still limited. We will continue to iterate and improve it. Meanwhile, the medical knowledge base and dataset construction code are still being organized and will be released once completed.\n\n#### Medical literature\nIn addition, we collected Chinese medical literature on liver cancer in 2023, and used the GPT3.5 interface to collect multiple rounds of question-and-answer data around the medical literature. We provide 1k training examples in `./data_literature/liver_cancer.json`. At present, the quality of training samples is still limited. In the future, we will further iterate the data and release it in the form of `public dataset`. An example of a training sample is as follows:\n\n<p align=\"center\" width=\"100%\">\n\n<a href=\"https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/\" target=\"_blank\"><img src=\"assets/case.png\" alt=\"SCIR-HI-HuaTuo-literature\" style=\"width: 100%; min-width: 300px; display: block; margin: auto;\"></a>\n\n</p>\n\nAt present, we only open model parameters trained for the single disease \"liver cancer\". In the future, we plan to release a medical dialogue dataset incorporating medical literature conclusions, and plan to train models for 16 diseases related to \"liver, gallbladder and pancreas\".\n\n### Finetune\nTo fine-tune LLaMA with your own dataset, please construct your dataset following the format of `./data/llama_data.json` and run the finetune script.\n\n```\nbash ./scripts/finetune.sh\n```\n\n\n\n## Training details\n### GPU resource\nWe trained the model on an A100-SXM-80GB for a total of 10 epochs, which took approximately 2 hours and 17 minutes. With a batch size of 128, the GPU memory usage was around 40GB. We expect that GPU with 24GB or more of memory, such as the 3090/4090, will be able to support the model well, and the batch size can be adjusted based on the size of the GPU memory.\n\n\n### Experiments\nParameter settings and details training process can be found in the wandb log:\nhttps://wandb.ai/thinksoso/llama_med/runs/a5wgcnzt/overview?workspace=user-thinksoso\n\n## Comparison of models\n\nNote: test in March 2023.\n\n| Input入 | Llama output  | Alpaca output | HuaTuo output |\n|:------------- |:---------------| :-------------|:-------------|\n|小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。|问题：小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。| 最近，小张一直感觉身体不舒服，有心烦意乱、呼吸急促等症状。体格检查显示心脏增大，肌肉无力。|**小张可能患有心肌炎，建议尽快到医院进行检查和治疗。治疗方案包括药物治疗和手术治疗。**|\n|麻风病和儿童哮喘的病因是否一致？|儿童哮喘和麻风病的病因是一致，但是儿童哮喘和麻风病的病因不一致|是的，哮喘和儿童喘息有相似的原因。| **不一致，麻风病的病因是麻风杆菌感染，而儿童哮喘的病因是喂养不良、吸烟等因素。**|\n|一位患者出现了持续性的胀痛和黄疸，经影像学检查后确诊为肝胆管结石病，应该采取哪些治疗方案？|肝胆管结石病患者应该采取哪些治疗方案？|患者应接受手术和化疗相结合的治疗。| **针对肝胆管结石病，治疗方案包括手术治疗、药物治疗和辅助治疗等。具体治疗方案需要根据患者的具体情况而定。**|\n\n##FAQs\n\n1. \tQ: Why was it renamed to \"Bentsao\"?\n\t\n\tA: For naming consistency with the SCIR large language model series, the name of the Chinese medical model has been renamed to \"Bentsao\".\n\n2. \tQ: Why \"Huatuo(华驼)\" instead of \"Huatuo(华佗)\"?\n\t\n\tA: We chose the name \"Huatuo(华驼)\" because our base model, LLaMA(美洲驼), and Alpaca(羊驼). Inspired by their names and a pun on the name \"tuo驼\" & \"tuo佗\", we named our model \"Huatuo(华驼)\".\n\n3. \tQ: Does it use Chinese traditional medicine theories or data?\n\t\n\tA: Not at the moment.\n\n4.\tQ: The results from the model vary and are limited in effectiveness.\n\n\tA: Due to the diversity considerations of the generative model, results may vary with multiple runs. The current open-source model, owing to the limited Chinese corpus in LLaMA and Alpaca, and a rather coarse way of knowledge integration, may yield inconsistent results. Please try the bloom-based and character-based models.\n5. Q: The model cannot run or the inferred content is completely unacceptable.\n\t\n\tA: Please ensure that you've installed the dependencies from the requirements, set up the CUDA environment and added the environment variables, and correctly input the downloaded model and Lora's storage location. Any repetition or partial mistakes in the inferred content are occasional issues with the llama-based model and relate to LLaMA's ability in Chinese, training data scale, and hyperparameter settings. Please try the new character-based model. If there are severe issues, please describe in detail the filename, model name, Lora configuration, etc., in an issue. Thank you all.\nQ: Among the released models, which one is the best?\nA: Based on our experience, the character-based model seems to perform relatively better.\n\n\n\n\n\n\n## Contributors\n\nThis project was founded by the Health Intelligence Group of the Research Center for Social Computing and Information Retrieval at Harbin Institute of Technology, including [Haochun Wang](https://github.com/s65b40), [Yanrui Du](https://github.com/DYR1), [Chi Liu](https://github.com/thinksoso), [Rui Bai](https://github.com/RuiBai1999), [Nuwa Xi](https://github.com/rootnx), [Yuhan Chen](https://github.com/Imsovegetable), [Zewen Qiang](https://github.com/1278882181), [Jianyu Chen](https://github.com/JianyuChen01), [Zijian Li](https://github.com/FlowolfzzZ) supervised by Associate Professor [Sendong Zhao](http://homepage.hit.edu.cn/stanzhao?lang=zh), Professor Bing Qin, and Professor Ting Liu.\n\n\n## Acknowledgements\n\nThis project has referred the following open-source projects. We would like to express our gratitude to the developers and researchers involved in those projects.\n\n- Facebook LLaMA: https://github.com/facebookresearch/llama\n- Stanford Alpaca: https://github.com/tatsu-lab/stanford_alpaca\n- alpaca-lora by @tloen: https://github.com/tloen/alpaca-lora\n- CMeKG https://github.com/king-yyf/CMeKG_tools\n- 文心一言 https://yiyan.baidu.com/welcome The logo of this project is automatically generated by Wenxin Yiyan.\n\n## Disclaimer\nThe resources related to this project are for academic research purposes only and strictly prohibited for commercial use. When using portions of third-party code, please strictly comply with the corresponding open source licenses. The content generated by the model is influenced by factors such as model computation, randomness, and quantization accuracy loss, and this project cannot guarantee its accuracy. The vast majority of the dataset used in this project is generated by the model, and even if it conforms to certain medical facts, it cannot be used as the basis for actual medical diagnosis. This project does not assume any legal liability for the content output by the model, nor is it responsible for any losses that may be incurred as a result of using the related resources and output results.\n\n\n## Citation\nIf you use the data or code from this project, please declare the reference:\n\n```\n@misc{wang2023huatuo,\n      title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge}, \n      author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},\n      year={2023},\n      eprint={2304.06975},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "data-literature",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "export_hf_checkpoint.py",
          "type": "blob",
          "size": 1.4736328125,
          "content": "import os\n\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import LlamaForCausalLM, LlamaTokenizer  # noqa: F402\n\nBASE_MODEL = os.environ.get(\"BASE_MODEL\", None)\nassert (\n    BASE_MODEL\n), \"Please specify a value for BASE_MODEL environment variable, e.g. `export BASE_MODEL=decapoda-research/llama-7b-hf`\"  # noqa: E501\n\ntokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n\nbase_model = LlamaForCausalLM.from_pretrained(\n    BASE_MODEL,\n    load_in_8bit=False,\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cpu\"},\n)\n\nfirst_weight = base_model.model.layers[0].self_attn.q_proj.weight\nfirst_weight_old = first_weight.clone()\n\nlora_model = PeftModel.from_pretrained(\n    base_model,\n    \"tloen/alpaca-lora-7b\",\n    device_map={\"\": \"cpu\"},\n    torch_dtype=torch.float16,\n)\n\nlora_weight = lora_model.base_model.model.model.layers[\n    0\n].self_attn.q_proj.weight\n\nassert torch.allclose(first_weight_old, first_weight)\n\n# merge weights\nfor layer in lora_model.base_model.model.model.layers:\n    layer.self_attn.q_proj.merge_weights = True\n    layer.self_attn.v_proj.merge_weights = True\n\nlora_model.train(False)\n\n# did we do anything?\nassert not torch.allclose(first_weight_old, first_weight)\n\nlora_model_sd = lora_model.state_dict()\ndeloreanized_sd = {\n    k.replace(\"base_model.model.\", \"\"): v\n    for k, v in lora_model_sd.items()\n    if \"lora\" not in k\n}\n\nLlamaForCausalLM.save_pretrained(\n    base_model, \"./hf_ckpt\", state_dict=deloreanized_sd, max_shard_size=\"400MB\"\n)\n"
        },
        {
          "name": "export_state_dict_checkpoint.py",
          "type": "blob",
          "size": 3.5263671875,
          "content": "import json\nimport os\n\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import LlamaForCausalLM, LlamaTokenizer  # noqa: E402\n\nBASE_MODEL = os.environ.get(\"BASE_MODEL\", None)\nassert (\n    BASE_MODEL\n), \"Please specify a value for BASE_MODEL environment variable, e.g. `export BASE_MODEL=decapoda-research/llama-7b-hf`\"  # noqa: E501\n\ntokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n\nbase_model = LlamaForCausalLM.from_pretrained(\n    BASE_MODEL,\n    load_in_8bit=False,\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cpu\"},\n)\n\nlora_model = PeftModel.from_pretrained(\n    base_model,\n    \"tloen/alpaca-lora-7b\",\n    device_map={\"\": \"cpu\"},\n    torch_dtype=torch.float16,\n)\n\n# merge weights\nfor layer in lora_model.base_model.model.model.layers:\n    layer.self_attn.q_proj.merge_weights = True\n    layer.self_attn.v_proj.merge_weights = True\n\nlora_model.train(False)\n\nlora_model_sd = lora_model.state_dict()\n\nparams = {\n    \"dim\": 4096,\n    \"multiple_of\": 256,\n    \"n_heads\": 32,\n    \"n_layers\": 32,\n    \"norm_eps\": 1e-06,\n    \"vocab_size\": -1,\n}\nn_layers = params[\"n_layers\"]\nn_heads = params[\"n_heads\"]\ndim = params[\"dim\"]\ndims_per_head = dim // n_heads\nbase = 10000.0\ninv_freq = 1.0 / (\n    base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head)\n)\n\n\ndef permute(w):\n    return (\n        w.view(n_heads, dim // n_heads // 2, 2, dim)\n        .transpose(1, 2)\n        .reshape(dim, dim)\n    )\n\n\ndef unpermute(w):\n    return (\n        w.view(n_heads, 2, dim // n_heads // 2, dim)\n        .transpose(1, 2)\n        .reshape(dim, dim)\n    )\n\n\ndef translate_state_dict_key(k):  # noqa: C901\n    k = k.replace(\"base_model.model.\", \"\")\n    if k == \"model.embed_tokens.weight\":\n        return \"tok_embeddings.weight\"\n    elif k == \"model.norm.weight\":\n        return \"norm.weight\"\n    elif k == \"lm_head.weight\":\n        return \"output.weight\"\n    elif k.startswith(\"model.layers.\"):\n        layer = k.split(\".\")[2]\n        if k.endswith(\".self_attn.q_proj.weight\"):\n            return f\"layers.{layer}.attention.wq.weight\"\n        elif k.endswith(\".self_attn.k_proj.weight\"):\n            return f\"layers.{layer}.attention.wk.weight\"\n        elif k.endswith(\".self_attn.v_proj.weight\"):\n            return f\"layers.{layer}.attention.wv.weight\"\n        elif k.endswith(\".self_attn.o_proj.weight\"):\n            return f\"layers.{layer}.attention.wo.weight\"\n        elif k.endswith(\".mlp.gate_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w1.weight\"\n        elif k.endswith(\".mlp.down_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w2.weight\"\n        elif k.endswith(\".mlp.up_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w3.weight\"\n        elif k.endswith(\".input_layernorm.weight\"):\n            return f\"layers.{layer}.attention_norm.weight\"\n        elif k.endswith(\".post_attention_layernorm.weight\"):\n            return f\"layers.{layer}.ffn_norm.weight\"\n        elif k.endswith(\"rotary_emb.inv_freq\") or \"lora\" in k:\n            return None\n        else:\n            print(layer, k)\n            raise NotImplementedError\n    else:\n        print(k)\n        raise NotImplementedError\n\n\nnew_state_dict = {}\nfor k, v in lora_model_sd.items():\n    new_k = translate_state_dict_key(k)\n    if new_k is not None:\n        if \"wq\" in new_k or \"wk\" in new_k:\n            new_state_dict[new_k] = unpermute(v)\n        else:\n            new_state_dict[new_k] = v\n\nos.makedirs(\"./ckpt\", exist_ok=True)\n\ntorch.save(new_state_dict, \"./ckpt/consolidated.00.pth\")\n\nwith open(\"./ckpt/params.json\", \"w\") as f:\n    json.dump(params, f)\n"
        },
        {
          "name": "finetune.py",
          "type": "blob",
          "size": 10.056640625,
          "content": "import os\nimport sys\nfrom typing import List\n\nimport fire\nimport wandb\nimport torch\nimport transformers\nfrom datasets import load_dataset\n\n\"\"\"\nUnused imports:\nimport torch.nn as nn\nimport bitsandbytes as bnb\n\"\"\"\n\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import Prompter\n\nfrom transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n\n\ndef train(\n    # model/data params\n    base_model: str = \"\",  # the only required argument\n    data_path: str = \"yahma/alpaca-cleaned\",\n    output_dir: str = \"./lora-alpaca\",\n    # training hyperparams\n    batch_size: int = 128,\n    micro_batch_size: int = 8,\n    num_epochs: int = 10,\n    learning_rate: float = 3e-4,\n    cutoff_len: int = 256,\n    val_set_size: int = 500,\n    # lora hyperparams\n    lora_r: int = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n    lora_target_modules: List[str] = [\n        \"q_proj\",\n        \"v_proj\",\n    ],\n    # llm hyperparams\n    train_on_inputs: bool = False,  # if False, masks out inputs in loss\n    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n    # wandb params\n    wandb_project: str = \"llama_med\",\n    wandb_run_name: str = \"\",\n    wandb_watch: str = \"\",  # options: false | gradients | all\n    wandb_log_model: str = \"\",  # options: false | true\n    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n):\n    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n        print(\n            f\"Training Alpaca-LoRA model with params:\\n\"\n            f\"base_model: {base_model}\\n\"\n            f\"data_path: {data_path}\\n\"\n            f\"output_dir: {output_dir}\\n\"\n            f\"batch_size: {batch_size}\\n\"\n            f\"micro_batch_size: {micro_batch_size}\\n\"\n            f\"num_epochs: {num_epochs}\\n\"\n            f\"learning_rate: {learning_rate}\\n\"\n            f\"cutoff_len: {cutoff_len}\\n\"\n            f\"val_set_size: {val_set_size}\\n\"\n            f\"lora_r: {lora_r}\\n\"\n            f\"lora_alpha: {lora_alpha}\\n\"\n            f\"lora_dropout: {lora_dropout}\\n\"\n            f\"lora_target_modules: {lora_target_modules}\\n\"\n            f\"train_on_inputs: {train_on_inputs}\\n\"\n            f\"group_by_length: {group_by_length}\\n\"\n            f\"wandb_project: {wandb_project}\\n\"\n            f\"wandb_run_name: {wandb_run_name}\\n\"\n            f\"wandb_watch: {wandb_watch}\\n\"\n            f\"wandb_log_model: {wandb_log_model}\\n\"\n            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n            f\"prompt template: {prompt_template_name}\\n\"\n        )\n    assert (\n        base_model\n    ), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n    gradient_accumulation_steps = batch_size // micro_batch_size\n\n    prompter = Prompter(prompt_template_name)\n\n    device_map = \"auto\"\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if ddp:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n\n    # Check if parameter passed or if set within environ\n    use_wandb = len(wandb_project) > 0 or (\n        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n    )\n    # Only overwrite environ if wandb param passed\n    if len(wandb_project) > 0:\n        os.environ[\"WANDB_PROJECT\"] = wandb_project\n    if len(wandb_watch) > 0:\n        os.environ[\"WANDB_WATCH\"] = wandb_watch\n    if len(wandb_log_model) > 0:\n        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=True,\n        torch_dtype=torch.float16,\n        device_map=device_map,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different from the eos token\n    )\n    tokenizer.padding_side = \"left\"  # Allow batched inference\n\n    def tokenize(prompt, add_eos_token=True):\n        # there's probably a way to do this with the tokenizer settings\n        # but again, gotta move fast\n        result = tokenizer(\n            prompt,\n            truncation=True,\n            max_length=cutoff_len,\n            padding=False,\n            return_tensors=None,\n        )\n        if (\n            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < cutoff_len\n            and add_eos_token\n        ):\n            result[\"input_ids\"].append(tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n\n        return result\n\n    def generate_and_tokenize_prompt(data_point):\n        full_prompt = prompter.generate_prompt(\n            data_point[\"instruction\"],\n            data_point[\"input\"],\n            data_point[\"output\"],\n        )\n        tokenized_full_prompt = tokenize(full_prompt)\n        if not train_on_inputs:\n            user_prompt = prompter.generate_prompt(\n                data_point[\"instruction\"], data_point[\"input\"]\n            )\n            tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n\n            tokenized_full_prompt[\"labels\"] = [\n                -100\n            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n                user_prompt_len:\n            ]  # could be sped up, probably\n        return tokenized_full_prompt\n\n    model = prepare_model_for_int8_training(model)\n\n    config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        target_modules=lora_target_modules,\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model = get_peft_model(model, config)\n\n    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n        data = load_dataset(\"json\", data_files=data_path)\n    else:\n        data = load_dataset(data_path)\n\n    if resume_from_checkpoint:\n        # Check the available weights and load them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint, \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n            checkpoint_name = os.path.join(\n                resume_from_checkpoint, \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config above has to fit\n            resume_from_checkpoint = (\n                False  # So the trainer won't try loading its state\n            )\n        # The two files above have a different name depending on how they were saved, but are actually the same.\n        if os.path.exists(checkpoint_name):\n            print(f\"Restarting from {checkpoint_name}\")\n            adapters_weights = torch.load(checkpoint_name)\n            set_peft_model_state_dict(model, adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name} not found\")\n\n    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n\n    if val_set_size > 0:\n        train_val = data[\"train\"].train_test_split(\n            test_size=val_set_size, shuffle=True, seed=2023\n        )\n        train_data = (\n            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n        )\n        val_data = (\n            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n        )\n    else:\n        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n        val_data = None\n\n    if not ddp and torch.cuda.device_count() > 1:\n        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n        model.is_parallelizable = True\n        model.model_parallel = True\n\n    class SavePeftModelCallback(TrainerCallback):\n        def on_save(\n            self,\n            args: TrainingArguments,\n            state: TrainerState,\n            control: TrainerControl,\n            **kwargs,\n        ):\n            checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n            kwargs[\"model\"].save_pretrained(checkpoint_folder)\n            pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n            if os.path.exists(pytorch_model_path):\n                os.remove(pytorch_model_path)\n            return control\n\n    trainer = transformers.Trainer(\n        model=model,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n        args=transformers.TrainingArguments(\n            per_device_train_batch_size=micro_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            warmup_ratio=0.1,\n            num_train_epochs=num_epochs,\n            learning_rate=learning_rate,\n            fp16=True,\n            logging_steps=8,\n            optim=\"adamw_torch\",\n            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n            save_strategy=\"steps\",\n            eval_steps=32 if val_set_size > 0 else None,\n            save_steps=32,\n            output_dir=output_dir,\n            save_total_limit=5,\n            load_best_model_at_end=True if val_set_size > 0 else False,\n            ddp_find_unused_parameters=False if ddp else None,\n            group_by_length=group_by_length,\n            report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n        ),\n        callbacks=[SavePeftModelCallback],\n    )\n    model.config.use_cache = False\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n    model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's a warning about missing keys above, please disregard :)\"\n    )\n\n\nif __name__ == \"__main__\":\n    fire.Fire(train)\n"
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 5.361328125,
          "content": "import sys\n\nimport fire\nimport gradio as gr\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import Prompter\n\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\ntry:\n    if torch.backends.mps.is_available():\n        device = \"mps\"\nexcept:  # noqa: E722\n    pass\n\n\ndef main(\n    load_8bit: bool = False,\n    base_model: str = \"\",\n    lora_weights: str = \"tloen/alpaca-lora-7b\",\n    prompt_template: str = \"med_template\",  # The prompt template to use, will default to alpaca.\n    server_name: str = \"0.0.0.0\",  # Allows to listen on all interfaces by providing '0.0.0.0'\n    share_gradio: bool = True,\n):\n    assert (\n        base_model\n    ), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n\n    prompter = Prompter(prompt_template)\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n    if device == \"cuda\":\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model,\n            load_in_8bit=load_8bit,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            torch_dtype=torch.float16,\n        )\n    elif device == \"mps\":\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model,\n            device_map={\"\": device},\n            torch_dtype=torch.float16,\n        )\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            device_map={\"\": device},\n            torch_dtype=torch.float16,\n        )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n        )\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            device_map={\"\": device},\n        )\n\n    # unwind broken decapoda-research config\n    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n    model.config.bos_token_id = 1\n    model.config.eos_token_id = 2\n\n    if not load_8bit:\n        model.half()  # seems to fix bugs for some users.\n\n    model.eval()\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    def evaluate(\n        instruction,\n        input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n        max_new_tokens=128,\n        **kwargs,\n    ):\n        prompt = prompter.generate_prompt(instruction, input)\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(device)\n        generation_config = GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            num_beams=num_beams,\n            **kwargs,\n        )\n        with torch.no_grad():\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=max_new_tokens,\n            )\n        s = generation_output.sequences[0]\n        output = tokenizer.decode(s)\n        return prompter.get_response(output)\n\n    gr.Interface(\n        fn=evaluate,\n        inputs=[\n            gr.components.Textbox(\n                lines=2,\n                label=\"Instruction\",\n                placeholder=\"Tell me about alpacas.\",\n            ),\n            gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n            gr.components.Slider(\n                minimum=0, maximum=1, value=0.1, label=\"Temperature\"\n            ),\n            gr.components.Slider(\n                minimum=0, maximum=1, value=0.75, label=\"Top p\"\n            ),\n            gr.components.Slider(\n                minimum=0, maximum=100, step=1, value=40, label=\"Top k\"\n            ),\n            gr.components.Slider(\n                minimum=1, maximum=4, step=1, value=4, label=\"Beams\"\n            ),\n            gr.components.Slider(\n                minimum=1, maximum=2000, step=1, value=128, label=\"Max tokens\"\n            ),\n        ],\n        outputs=[\n            gr.inputs.Textbox(\n                lines=5,\n                label=\"Output\",\n            )\n        ],\n        title=\"BenTsao\",\n        description=\"\",  # noqa: E501\n    ).launch(server_name=server_name, share=share_gradio)\n    # Old testing code follows.\n\n    \"\"\"\n    # testing code for readme\n    for instruction in [\n        \"Tell me about alpacas.\",\n        \"Tell me about the president of Mexico in 2019.\",\n        \"Tell me about the king of France in 2019.\",\n        \"List all Canadian provinces in alphabetical order.\",\n        \"Write a Python program that prints the first 10 Fibonacci numbers.\",\n        \"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n        \"Tell me five words that rhyme with 'shock'.\",\n        \"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n        \"Count up from 1 to 500.\",\n    ]:\n        print(\"Instruction:\", instruction)\n        print(\"Response:\", evaluate(instruction))\n        print()\n    \"\"\"\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"
        },
        {
          "name": "infer.py",
          "type": "blob",
          "size": 3.8271484375,
          "content": "import sys\nimport json\nimport fire\nimport gradio as gr\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import Prompter\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n\ndef load_instruction(instruct_dir):\n    input_data = []\n    with open(instruct_dir, \"r\") as f:\n        lines = f.readlines()\n        for line in lines:\n            line = line.strip()\n            d = json.loads(line)\n            input_data.append(d)\n    return input_data\n\n\ndef main(\n    load_8bit: bool = False,\n    base_model: str = \"\",\n    # the infer data, if not exists, infer the default instructions in code\n    instruct_dir: str = \"\",\n    use_lora: bool = True,\n    lora_weights: str = \"tloen/alpaca-lora-7b\",\n    # The prompt template to use, will default to med_template.\n    prompt_template: str = \"med_template\",\n):\n    prompter = Prompter(prompt_template)\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=load_8bit,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    if use_lora:\n        print(f\"using lora {lora_weights}\")\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            torch_dtype=torch.float16,\n        )\n    # unwind broken decapoda-research config\n    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n    model.config.bos_token_id = 1\n    model.config.eos_token_id = 2\n    if not load_8bit:\n        model.half()  # seems to fix bugs for some users.\n\n    model.eval()\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    def evaluate(\n        instruction,\n        input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n        max_new_tokens=256,\n        **kwargs,\n    ):\n        prompt = prompter.generate_prompt(instruction, input)\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(device)\n        generation_config = GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            num_beams=num_beams,\n            **kwargs,\n        )\n        with torch.no_grad():\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=max_new_tokens,\n            )\n        s = generation_output.sequences[0]\n        output = tokenizer.decode(s)\n        return prompter.get_response(output)\n\n    def infer_from_json(instruct_dir):\n        input_data = load_instruction(instruct_dir)\n        for d in input_data:\n            instruction = d[\"instruction\"]\n            output = d[\"output\"]\n            print(\"###infering###\")\n            model_output = evaluate(instruction)\n            print(\"###instruction###\")\n            print(instruction)\n            print(\"###golden output###\")\n            print(output)\n            print(\"###model output###\")\n            print(model_output)\n\n    if instruct_dir != \"\":\n        infer_from_json(instruct_dir)\n    else:\n        for instruction in [\n            \"我感冒了，怎么治疗\",\n            \"一个患有肝衰竭综合征的病人，除了常见的临床表现外，还有哪些特殊的体征？\",\n            \"急性阑尾炎和缺血性心脏病的多发群体有何不同？\",\n            \"小李最近出现了心动过速的症状，伴有轻度胸痛。体检发现P-R间期延长，伴有T波低平和ST段异常\",\n        ]:\n            print(\"Instruction:\", instruction)\n            print(\"Response:\", evaluate(instruction))\n            print()\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"
        },
        {
          "name": "infer_literature.py",
          "type": "blob",
          "size": 4.126953125,
          "content": "import sys\nimport json\n\nimport fire\nimport gradio as gr\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n\nfrom utils.prompter import Prompter\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n\ndef load_instruction(instruct_dir):\n    input_data = []\n    with open(instruct_dir, \"r\") as f:\n        lines = f.readlines()\n        for line in lines:\n            line = line.strip()\n            d = json.loads(line)\n            input_data.append(d)\n    return input_data\n\n\ndef main(\n    load_8bit: bool = False,\n    base_model: str = \"\",\n    # the infer data, if not exists, infer the default instructions in code\n    single_or_multi: str = \"\",\n    use_lora: bool = True,\n    lora_weights: str = \"tloen/alpaca-lora-7b\",\n    # The prompt template to use, will default to med_template.\n    prompt_template: str = \"med_template\",\n):\n    prompter = Prompter(prompt_template)\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=load_8bit,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    if use_lora:\n        print(f\"using lora {lora_weights}\")\n        model = PeftModel.from_pretrained(\n            model,\n            lora_weights,\n            torch_dtype=torch.float16,\n        )\n    # unwind broken decapoda-research config\n    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n    model.config.bos_token_id = 1\n    model.config.eos_token_id = 2\n    if not load_8bit:\n        model.half()  # seems to fix bugs for some users.\n\n    model.eval()\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    def evaluate(\n        instruction,\n        input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n        max_new_tokens=256,\n        **kwargs,\n    ):\n        prompt = prompter.generate_prompt(instruction, input)\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(device)\n        generation_config = GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            num_beams=num_beams,\n            **kwargs,\n        )\n        with torch.no_grad():\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=max_new_tokens,\n            )\n        s = generation_output.sequences[0]\n        output = tokenizer.decode(s)\n        return prompter.get_response(output)\n\n    if single_or_multi == \"multi\":\n        response=\"\"\n        instruction=\"\"\n        for _ in range(0,5):  \n            inp=input(\"请输入:\")\n            inp=\"<user>: \" + inp\n            instruction=instruction+inp\n            response=evaluate(instruction)\n            response=response.replace('\\n','')\n            print(\"Response:\", response)\n            instruction= instruction + \" <bot>: \" + response\n\n\n    elif single_or_multi == \"single\":\n        for instruction in [\n            \"肝癌是什么？有哪些症状和迹象？\",\n            \"肝癌是如何诊断的？有哪些检查和测试可以帮助诊断？\",\n            \"Sorafenib是一种口服的多靶点酪氨酸激酶抑制剂，它的作用机制是什么？\",\n            \"Regorafenib是一种口服的多靶点酪氨酸激酶抑制剂，它的作用机制是什么？它和Sorafenib有什么不同？\",\n            \"肝癌药物治疗的副作用有哪些？如何缓解这些副作用？\",\n            \"肝癌药物治疗的费用高昂，如何降低治疗的经济负担？\",\n            \"我想了解一下β-谷甾醇是否可作为肝癌的治疗药物\",\n            \"能介绍一下最近Hsa＿circ＿0008583在肝细胞癌治疗中的潜在应用的研究么?\"\n        ]:  \n            print(\"instruction:\",instruction)\n            instruction=\"<user>: \"+instruction\n            print(\"Response:\", evaluate(instruction))\n          \n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2060546875,
          "content": "accelerate==0.20.1\nappdirs==1.4.4\nbitsandbytes==0.37.2\nblack==23.3.0\nblack[jupyter]==23.3.0\ndatasets==2.11.0\nfire==0.5.0\npeft==0.3.0\ntransformers==4.30.1\ngradio==3.33.1\nsentencepiece==0.1.97\nscipy==1.10.1\nwandb\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}