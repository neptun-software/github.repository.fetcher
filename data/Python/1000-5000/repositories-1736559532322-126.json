{
  "metadata": {
    "timestamp": 1736559532322,
    "page": 126,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pytorch/torchtune",
      "stars": 4668,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.9677734375,
          "content": "[flake8]\n# Suggested config from pytorch that we can adapt\nselect = B,C,E,F,N,P,T4,W,B9,TOR0,TOR1,TOR2\nmax-line-length = 120\n# C408 ignored because we like the dict keyword argument syntax\n# E501 is not flexible enough, we're using B950 instead\n# N812 ignored because import torch.nn.functional as F is PyTorch convention\n# N817 ignored because importing using acronyms is convention (DistributedDataParallel as DDP)\n# E731 allow usage of assigning lambda expressions\nignore =\n    E203,E305,E402,E501,E721,E741,F405,F821,F841,F999,W503,W504,C408,E302,W291,E303,N812,N817,E731\n    # shebang has extra meaning in fbcode lints, so I think it's not worth trying\n    # to line this up with executable bit\n    EXE001,\n    # these ignores are from flake8-bugbear; please fix!\n    B007,B008,\noptional-ascii-coding = True\nexclude =\n    ./.git,\n    ./docs\n    ./build\n    ./scripts,\n    ./venv,\n    *.pyi\n    .pre-commit-config.yaml\n    *.md\n    .flake8\n    tests/torchtune/models/llama2/scripts/*.py\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.6875,
          "content": "# Derived from basic .gitignore template for python projects:\n#   https://github.com/github/gitignore/blob/main/Python.gitignore\n# Please maintain the alphabetic order of the section titles\n# To debug why a file is being ignored, use the command:\n#    git check-ignore -v $my_ignored_file\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Cython debug symbols\ncython_debug/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# Django stuff\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Flask stuff\ninstance/\n.webassets-cache\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# IPython\nprofile_default/\nipython_config.py\n\n# Jupyter Notebook\n*.ipynb_checkpoints\n\n# mkdocs documentation\n/site\n\n# Model saving / checkpointing\n*.pt\n*.pth\n*.ckpt\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n# Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n# poetry.lock\n\n# PEP 582: https://peps.python.org/pep-0582/\n#   This PEP proposes to add to Python a mechanism to automatically recognize a __pypackages__\n#   directory and prefer importing packages installed in this location over user or global site-packages.\n#   This will avoid the steps to create, activate or deactivate virtual environments. Python will use\n#   the __pypackages__ from the base directory of the script when present.\n__pypackages__/\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Rope project settings\n.ropeproject\n\n# SageMath parsed files\n*.sage.py\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/build\n# sphinx-gallery\ndocs/source/generated_examples/\ndocs/source/gen_modules/\ndocs/source/generated/\ndocs/source/sg_execution_times.rst\n# pytorch-sphinx-theme gets installed here\ndocs/src\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# System / program generated files\n*.err\n*.log\n*.swp\n.DS_Store\n\n# Translations\n*.mo\n*.pot\n\n# TorchX\n*.torchxconfig\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# VSCode\n.vscode/\n\n# wandb\nwandb/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.26953125,
          "content": "exclude: 'build'\n\ndefault_language_version:\n    python: python3\n\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: 6306a48f7dae5861702d573c9c247e4e9498e867\n    hooks:\n    -   id: trailing-whitespace\n    -   id: check-ast\n    -   id: check-merge-conflict\n    -   id: no-commit-to-branch\n        args: ['--branch=main']\n    -   id: check-added-large-files\n        args: ['--maxkb=1000']\n    -   id: end-of-file-fixer\n        exclude: '^(.*\\.svg)$'\n\n-   repo: https://github.com/Lucas-C/pre-commit-hooks\n    rev: v1.5.4\n    hooks:\n    -   id: insert-license\n        files: \\.py$|\\.sh$\n        args:\n        - --license-filepath\n        - docs/license_header.txt\n\n-   repo: https://github.com/pycqa/flake8\n    rev: 34cbf8ef3950f43d09b85e2e45c15ae5717dc37b\n    hooks:\n    -   id: flake8\n        additional_dependencies:\n          - flake8-bugbear == 22.4.25\n          - pep8-naming == 0.12.1\n          - torchfix\n        args: ['--config=.flake8']\n\n-   repo: https://github.com/omnilib/ufmt\n    rev: v2.3.0\n    hooks:\n    -   id: ufmt\n        additional_dependencies:\n          - black == 22.12.0\n          - usort == 1.0.5\n\n- repo: https://github.com/jsh9/pydoclint\n  rev: 94efc5f989adbea30f3534b476b2931a02c1af90\n  hooks:\n    - id: pydoclint\n      args: [--config=pyproject.toml]\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.2998046875,
          "content": "cff-version: 1.2.0\ntitle: \"torchtune: PyTorch's finetuning library\"\nmessage: \"If you use this software, please cite it as below.\"\ntype: software\nauthors:\n  - given-names: \"torchtune maintainers and contributors\"\nurl: \"https//github.com/pytorch/torchtune\"\nlicense: \"BSD-3-Clause\"\ndate-released: \"2024-04-14\"\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.4541015625,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@meta.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 11.490234375,
          "content": "# Contributing to torchtune\nWe want to make contributing to this project as easy and transparent as possible.\n\n&nbsp;\n\n## Dev install\nYou should first [fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo) the torchtune repository\nand then clone your forked repository. Make sure to keep your fork in sync with the torchtune repository over time.\n\n```git clone https://github.com/<YOUR_GITHUB_USER>/torchtune.git```\n\nThen navigate into the newly cloned repo and install dependencies needed for development.\n\n**Step 1:** [Install PyTorch](https://pytorch.org/get-started/locally/). torchtune is tested with the latest stable PyTorch release as well as the preview nightly version.\n\n\n**Step 2:** Install all the additional dependencies and dev dependencies in the local repo:\n\n```\ncd torchtune\npip install -e \".[dev]\"\n```\n\n&nbsp;\n\n## Contributing workflow\nWe actively welcome your pull requests.\n\n1. Create your new branch from `main` in your forked repo, with a name describing the work you're completing e.g. `add-feature-x`.\n2. If you've added code that should be tested, add tests. Ensure all tests pass. See the [testing section](#testing) for more information.\n3. If you've changed APIs, [update the documentation](#updating-documentation).\n4. Make sure your [code lints](#coding-style).\n5. If you haven't already, complete the [Contributor License Agreement (\"CLA\")](#contributor-license-agreement-cla)\n\n&nbsp;\n\n## Testing\ntorchtune contains three different types of tests: unit tests, recipe tests, and regression tests. These tests are distinguished by their complexity and the resources they require to run. Recipe tests and regression tests are explicitly marked via pytest.mark decorators and both require S3 access to download the requisite assets.\n\n- **Unit tests**\n  - These should be minimal tests runnable without remote access. (No large models, no downloading weights). Unit tests should be under [tests/torchtune](https://github.com/pytorch/torchtune/tree/main/tests/torchtune).\n  - All unit tests can be run via ```pytest tests```.\n- **Recipe tests**\n  - These are relatively small-scale integration tests for running our recipes. These include\n  both single-device recipes and distributed recipes. In the latter case, tests should be marked with the `@gpu_test` decorator to indicate how many GPUs they need to run.\n  - Recipe tests require remote access as (small) model weights will be downloaded from S3 to run them.\n  - Recipe tests are found under [tests/recipes](https://github.com/pytorch/torchtune/tree/main/tests/recipes) and should be marked with the `@pytest.mark.integration_test` decorator.\n  - To run only recipe tests, you can run `pytest tests -m integration_test`.\n- **Regression tests**\n  - These are the most heavyweight tests in the repo. They involve building a full model (i.e. 7B size or larger), then running some finetune and/or evaluation via a combination of tune CLI commands. Whereas an individual recipe test runtime is generally still O(seconds), integration tests should be O(minutes) or greater. Like recipe tests, regression tests also require S3 access.\n  - Regression tests are found under [tests/regression_tests](https://github.com/pytorch/torchtune/tree/main/tests/regression_tests) and should be marked with the `@pytest.mark.slow_integration_test` decorator.\n  - To run only regression tests, you can use the command `pytest tests -m slow_integration_test`.\n\nWhenever running tests in torchtune, favor using the command line flags as much as possible (e.g. run `pytest tests -m integration_test` over `pytest tests/recipes`). This is because (a) the default behavior is to run unit tests only (so you will miss recipe tests without the flag), and (b) using the flags ensures pytest will automatically download any remote assets needed for your test run.\n\nNote that the above flags can be combined with other pytest flags, so e.g. `pytest tests -m integration_test -k 'test_loss'` will run only recipe tests matching the substring `test_loss`.\n\n&nbsp;\n\n## Updating documentation\nEach API and class should be clearly documented. Well-documented code is easier to review and understand/extend. All documentation is contained in the [docs directory](docs/source):\n\n* All files following the pattern `api_ref_*` document top-level APIs.\n* All files under the [deep dives directory](docs/source/deep_dives) contain \"deep-dive\" tutorials\n* All files under the [tutorials directory](docs/source/tutorials) contain regular tutorials\n\nDocumentation is written in [RST](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html) format.\n\n### Adding a new class/method to the API References\nOnce you've added an API that is meant to be exposed publically, you should add it to the appropriate rst file. For example, any new API within the [configs/](torchtune/configs)\ndirectory should be added to `api_ref_configs.rst`, [data/](torchtune/data) should be added to `api_ref_data.rst`, [datasets](torchtune/datasets) should be added to\n`api_ref_datasets.rst`, and so on. To add, it's as simple as adding the name of the exposed API somewhere in the appropriate RST file.\n\nAll code written within the docstring of the class or method will be correctly rendered there.\n\n> Note: Our RST theme expects code to be specified using double backticks instead of single. Eg: ``hidden_dim``. Single backticks will be rendered as italics instead of as \"code\".\n\n### Adding documentation for a recipe\n\nIf you've contributed a new recipe, or you're interesting in adding documentation for an existing recipe, you can add a new page in [the recipes directory](docs/source/recipes). Please refer to existing recipe docpages to understand the format of these documentation pages. Broadly speaking:\n\n- Recipe documentation pages are like beefed up API references for recipes.\n- They should have a low noise/information ratio, i.e. information in the recipe documentation page should mostly be relevant for using that recipe.\n- Relevant information could include:\n  - A cookbook/manual-style description of all the ways in which the recipe can be modified. For instance, does it support different loss functions? If so, describe those loss functions and help a user understand when they might want to use them.\n  - Example commands for using and customizing the recipe, particularly w.r.t the specific knobs and levers unique to the recipe.\n  - Pre-requisites for the recipe including models and datasets.\n  - Reference outputs for a recipe to help a user understand what successful training looks like e.g. loss curves, eval results, generations, etc.\n  - References to the appropriate [memory optimization](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html) features which can be used in the recipe. If you've contributed new memory optimization features which could be used across other recipes, consider adding them to the overview!\n\n\nFinally, make sure you update the [recipe overview page](docs/source/recipes/recipes_overview.rst), and the [index sidebar](docs/source/index.rst).\n\n### Building docs\n\nAll documentation is built for each PR and contains a preview on the PR. However, this takes awhile (~8 minutes) and you should first build docs from your local machine.\n\nFrom the [docs/](docs) directory:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run make command:\n\n```\nmake html\n# Now open build/html/index.html\n```\n\nTo avoid building the examples (which execute python code and can take time) you\ncan use `make html-noplot`. To build a subset of specific examples instead of\nall of them, you can use a regex like `EXAMPLES_PATTERN=\"plot_the_best_example*\"\nmake html`.\n\nIf the doc build starts failing for a weird reason, try `make clean`.\n\n#### Serving docs locally (if building from a GPU env)\n\nIf you're developing locally, you can just open the generated `index.html` file in your browser.\n\nIf instead you're using a remote machine, you can use a combination of a simple python HTTP server and port forwarding to serve the docs locally. This allows you to iterate on the documentation much more quickly than relying on PR previews.\n\nTo do so, after following the above doc build steps, run the following from the `docs/build/html` folder:\n\n```\npython -m http.server 8000 # or any free port\n```\n\nThis will open up a simple HTTP server serving the files in the build directory. If this is done on a remote machine, you can set up port forwarding from your local machine to access the server, for example:\n\n```\nssh -L 9000:localhost:8000 $REMOTE_DEV_HOST\n```\n\nNow, you can navigate to `localhost:9000` on your local machine to view the rendered documentation.\n\n&nbsp;\n\n## Coding Style\n`torchtune` uses pre-commit hooks to ensure style consistency and prevent common mistakes. Enable it by:\n\n```\npre-commit install\n```\n\nAfter this pre-commit hooks will be run before every commit.\n\nYou can also run this manually on every file using:\n\n```\npre-commit run --all-files\n```\n\n&nbsp;\n\n## Best Practices\n\nThis section captures some best practices for contributing code to torchtune. Following these will make PR reviews easier.\n\n- **Modular Blocks instead of Monolithic Classes**. Stuffing all of the logic into a single class limits readability and makes it hard to reuse logic. Think about breaking the implementation into self-contained blocks which can be used independently from a given model. For example, attention mechanisms, embedding classes, transformer layers etc.\n- **Say no to Implementation Inheritance**. You really don‚Äôt need it AND it makes the code much harder to understand or refactor since the logic is spread across many files/classes. Where needed, consider using Protocols.\n- **Clean Interfaces**. There‚Äôs nothing more challenging than reading through functions/constructors with ~100 parameters. Think carefully about what needs to be exposed to the user and don‚Äôt hesitate to hard-code parameters until there is a need to make them configurable.\n- **Intrusive Configs**. Config objects should not intrude into the class implementation. Configs should interact with these classes through cleanly defined builder functions which convert the config into flat parameters needed to instantiate an object.\n- **Limit Generalization**. Attempting to generalize code before this is needed unnecessarily complicates implementations - you are anticipating use cases you don‚Äôt know a lot about. When you actually need to generalize a component, think about whether it‚Äôs worth it to complicate a given interface to stuff in more functionality. Don‚Äôt be afraid of code duplication if it makes things easier to read.\n- **Value Checks and Asserts**. Don‚Äôt check values in higher level modules - defer the checks to the modules where the values are actually used. This helps reduce the number of raise statements in code which generally hurts readability, but are critical for correctness.\n\n&nbsp;\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is clear and has sufficient instructions to be able to reproduce the issue.\n\nMeta has a [bounty program](https://www.facebook.com/whitehat/) for the safe disclosure of security bugs. In those cases, please go through the process outlined on that page and do not file a public issue.\n\n&nbsp;\n\n## License\nBy contributing to torchtune, you agree that your contributions will be licensed under the LICENSE file in the root directory of this source tree.\n\n&nbsp;\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need to do this once to work on any of Meta's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n&nbsp;\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.439453125,
          "content": "BSD 3-Clause License\n\nCopyright 2024 Meta\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice,this list\nof conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this\nlist of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may\nbe used to endorse or promote products derived from this software without specific\nprior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\nOF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT\nSHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\nINCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED\nTO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\nBUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\nANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0556640625,
          "content": "prune tests  # Remove all testing files from final dist/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.4384765625,
          "content": "\n\n\n# torchtune\n\n[![Unit Test](https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml/badge.svg?branch=main)](https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml)\n![Recipe Integration Test](https://github.com/pytorch/torchtune/actions/workflows/recipe_test.yaml/badge.svg)\n[![](https://dcbadge.vercel.app/api/server/4Xsdn8Rr9Q?style=flat)](https://discord.gg/4Xsdn8Rr9Q)\n\n[**Introduction**](#introduction) | [**Installation**](#installation) | [**Get Started**](#get-started) |  [**Documentation**](https://pytorch.org/torchtune/main/index.html) | [**Community**](#community) | [**License**](#license) | [**Citing torchtune**](#citing-torchtune)\n\n### üì£ Recent updates üì£\n* *December 2024*: torchtune now supports **Llama 3.3 70B**! Try it out by following our installation instructions [here](#Installation), then run any of the configs [here](recipes/configs/llama3_3).\n* *November 2024*: torchtune has released [v0.4.0](https://github.com/pytorch/torchtune/releases/tag/v0.4.0) which includes stable support for exciting features like activation offloading and multimodal QLoRA\n* *November 2024*: torchtune has added [Gemma2](recipes/configs/gemma2) to its models!\n* *October 2024*: torchtune added support for Qwen2.5 models - find the recipes [here](recipes/configs/qwen2_5/)\n* *September 2024*: torchtune has support for **Llama 3.2 11B Vision**, **Llama 3.2 3B**, and **Llama 3.2 1B** models! Try them out by following our installation instructions [here](#Installation), then run any of the text configs [here](recipes/configs/llama3_2) or vision configs [here](recipes/configs/llama3_2_vision).\n\n\n&nbsp;\n\n## Introduction\n\ntorchtune is a PyTorch library for easily authoring, finetuning and experimenting with LLMs.\n\ntorchtune provides:\n\n- PyTorch implementations of popular LLMs from Llama, Gemma, Mistral, Phi, and Qwen model families\n- Hackable training recipes for full finetuning, LoRA, QLoRA, DPO, PPO, QAT, knowledge distillation, and more\n- Out-of-the-box memory efficiency, performance improvements, and scaling with the latest PyTorch APIs\n- YAML configs for easily configuring training, evaluation, quantization or inference recipes\n- Built-in support for many popular dataset formats and prompt templates\n\n\n&nbsp;\n\n### Models\n\ntorchtune currently supports the following models.\n\n| Model                                         | Sizes     |\n|-----------------------------------------------|-----------|\n| [Llama3.3](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3)    | 70B [[models](torchtune/models/llama3_3/_model_builders.py), [configs](recipes/configs/llama3_3/)]        |\n| [Llama3.2-Vision](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2#-llama-3.2-vision-models-(11b/90b)-)    | 11B, 90B [[models](torchtune/models/llama3_2_vision/_model_builders.py), [configs](recipes/configs/llama3_2_vision/)]        |\n| [Llama3.2](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2)    | 1B, 3B [[models](torchtune/models/llama3_2/_model_builders.py), [configs](recipes/configs/llama3_2/)]        |\n| [Llama3.1](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1)    | 8B, 70B, 405B [[models](torchtune/models/llama3_1/_model_builders.py), [configs](recipes/configs/llama3_1/)]        |\n| [Llama3](https://llama.meta.com/llama3)    | 8B, 70B [[models](torchtune/models/llama3/_model_builders.py), [configs](recipes/configs/llama3/)]        |\n| [Llama2](https://llama.meta.com/llama2/)   | 7B, 13B, 70B [[models](torchtune/models/llama2/_model_builders.py), [configs](recipes/configs/llama2/)]        |\n| [Code-Llama2](https://ai.meta.com/blog/code-llama-large-language-model-coding/)   | 7B, 13B, 70B [[models](torchtune/models/code_llama2/_model_builders.py), [configs](recipes/configs/code_llama2/)] |\n| [Mistral](https://huggingface.co/mistralai)   | 7B [[models](torchtune/models/mistral/_model_builders.py), [configs](recipes/configs/mistral/)] |\n| [Gemma](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b)   | 2B, 7B [[models](torchtune/models/gemma/_model_builders.py), [configs](recipes/configs/gemma/)] |\n| [Gemma2](https://huggingface.co/docs/transformers/main/en/model_doc/gemma2)   | 2B, 9B, 27B [[models](torchtune/models/gemma2/_model_builders.py), [configs](recipes/configs/gemma2/)] |\n| [Microsoft Phi3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) | Mini [[models](torchtune/models/phi3/), [configs](recipes/configs/phi3/)]\n| [Qwen2](https://qwenlm.github.io/blog/qwen2/) | 0.5B, 1.5B, 7B [[models](torchtune/models/qwen2/), [configs](recipes/configs/qwen2/)]\n| [Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/) | 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B [[models](torchtune/models/qwen2_5/), [configs](recipes/configs/qwen2_5/)]\n\nWe're always adding new models, but feel free to [file an issue](https://github.com/pytorch/torchtune/issues/new) if there's a new one you would like to see in torchtune.\n\n&nbsp;\n\n### Finetuning recipes\n\ntorchtune provides the following finetuning recipes for training on one or more devices.\n\n\n| Finetuning Method                          | Devices | Recipe  | Example Config(s) |\n|:-:|:-:|:-:|:-:|\n| Full Finetuning  | 1-8 | [full_finetune_single_device](recipes/full_finetune_single_device.py) <br> [full_finetune_distributed](recipes/full_finetune_distributed.py)| [Llama3.1 8B single-device](recipes/configs/llama3_1/8B_full_single_device.yaml) <br> [Llama 3.1 70B distributed](recipes/configs/llama3_1/70B_full.yaml)\n| LoRA Finetuning | 1-8  | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py) | [Qwen2 0.5B single-device](recipes/configs/qwen2/0.5B_lora_single_device.yaml) <br> [Gemma 7B distributed](recipes/configs/gemma/7B_lora.yaml)\n| QLoRA Finetuning | 1-8 | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py)| [Phi3 Mini single-device](recipes/configs/phi3/mini_qlora_single_device.yaml) <br> [Llama 3.1 405B distributed](recipes/configs/llama3_1/405B_qlora.yaml)\n| DoRA/QDoRA Finetuning | 1-8 | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py)| [Llama3 8B QDoRA single-device](recipes/configs/llama3/8B_qdora_single_device.yaml) <br> [Llama3 8B DoRA distributed](recipes/configs/llama3/8B_dora.yaml)\n| Quantization-Aware Training | 2-8 | [qat_distributed](recipes/qat_distributed.py)| [Llama3 8B QAT](recipes/configs/llama3/8B_qat_full.yaml)\n| Quantization-Aware Training and LoRA Finetuning | 2-8 | [qat_lora_finetune_distributed](recipes/qat_lora_finetune_distributed.py)| [Llama3 8B QAT](recipes/configs/llama3/8B_qat_lora.yaml)\n| Direct Preference Optimization |1-8 | [lora_dpo_single_device](recipes/lora_dpo_single_device.py) <br> [lora_dpo_distributed](recipes/lora_dpo_distributed.py) | [Llama2 7B single-device](recipes/configs/llama2/7B_lora_dpo_single_device.yaml) <br> [Llama2 7B distributed](recipes/configs/llama2/7B_lora_dpo.yaml)\n| Proximal Policy Optimization | 1 |  [ppo_full_finetune_single_device](recipes/ppo_full_finetune_single_device.py) | [Mistral 7B](recipes/configs/mistral/7B_full_ppo_low_memory.yaml)\n| Knowledge Distillation | 1 | [knowledge_distillation_single_device](recipes/knowledge_distillation_single_device.py) | [Qwen2 1.5B -> 0.5B](recipes/configs/qwen2/knowledge_distillation_single_device.yaml)\n\n\nThe above configs are just examples to get you started. If you see a model above not listed here, we likely still support it. If you're unsure whether something is supported, please open an issue on the repo.\n\n&nbsp;\n\n### Memory and training speed\n\nBelow is an example of the memory requirements and training speed for different Llama 3.1 models.\n\n> [!NOTE]\n> For ease of comparison, all the below numbers are provided for batch size 2 (without gradient accumulation), a dataset packed to sequence length 2048, and torch compile enabled.\n\nIf you are interested in running on different hardware or with different models, check out our documentation on memory optimizations [here](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html) to find the right setup for you.\n\n| Model | Finetuning Method | Runnable On | Peak Memory per GPU | Tokens/sec * |\n|:-:|:-:|:-:|:-:|:-:|\n| Llama 3.1 8B | Full finetune | 1x 4090 | 18.9 GiB | 1650 |\n| Llama 3.1 8B | Full finetune | 1x A6000 | 37.4 GiB |  2579|\n| Llama 3.1 8B | LoRA | 1x 4090 |  16.2 GiB | 3083 |\n| Llama 3.1 8B | LoRA | 1x A6000 | 30.3 GiB  | 4699 |\n| Llama 3.1 8B | QLoRA | 1x 4090 | 7.4 GiB | 2413  |\n| Llama 3.1 70B | Full finetune | 8x A100  | 13.9 GiB ** | 1568  |\n| Llama 3.1 70B | LoRA | 8x A100 | 27.6 GiB  | 3497  |\n| Llama 3.1 405B | QLoRA | 8x A100 | 44.8 GB  | 653  |\n\n*= Measured over one full training epoch\n\n**= Uses CPU offload with fused optimizer\n\n&nbsp;\n\n### Optimization flags\n\ntorchtune exposes a number of levers for memory efficiency and performance. The table below demonstrates the effects of applying some of these techniques sequentially to the Llama 3.2 3B model. Each technique is added on top of the previous one, except for LoRA and QLoRA, which do not use `optimizer_in_bwd` or `AdamW8bit` optimizer.\n\n**Baseline:**\n- **Model:** Llama 3.2 3B\n- **Batch size:** 2\n- **Max seq len:** 4096\n- **Precision:** bf16\n- **Hardware:** A100\n- **Recipe:** full_finetune_single_device\n\n| Technique | Peak Memory Active (GiB) | % Change Memory vs Previous | Tokens Per Second | % Change Tokens/sec vs Previous|\n|:--|:-:|:-:|:-:|:-:|\n| Baseline | 25.5 | - | 2091 | - |\n| [+ Packed Dataset](https://pytorch.org/torchtune/main/basics/packing.html) | 60.0 | +135.16% | 7075 | +238.40% |\n| [+ Compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) | 51.0 | -14.93% | 8998 | +27.18% |\n| [+ Chunked Cross Entropy](https://pytorch.org/torchtune/main/generated/torchtune.modules.loss.CEWithChunkedOutputLoss.html) | 42.9 | -15.83% | 9174 | +1.96% |\n| [+ Activation Checkpointing](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#activation-checkpointing) | 24.9 | -41.93% | 7210 | -21.41% |\n| [+ Fuse optimizer step into backward](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#fusing-optimizer-step-into-backward-pass) | 23.1 | -7.29% | 7309 | +1.38% |\n| [+ Activation Offloading](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#activation-offloading) | 21.8 | -5.48% | 7301 | -0.11% |\n| [+ 8-bit AdamW](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#lower-precision-optimizers) | 17.6 | -19.63% | 6960 | -4.67% |\n| [LoRA](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#glossary-lora) | 8.5 | -51.61% | 8210 | +17.96% |\n| [QLoRA](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#quantized-low-rank-adaptation-qlora) | 4.6 | -45.71% | 8035 | -2.13% |\n\nThe final row in the table vs baseline + Packed Dataset uses **81.9%** less memory with a **284.3%** increase in tokens per second. It can be run via the command:\n```\ntune run lora_finetune_single_device --config llama3_2/3B_qlora_single_device \\\ndataset.packed=True \\\ncompile=True \\\nloss=torchtune.modules.loss.CEWithChunkedOutputLoss \\\nenable_activation_checkpointing=True \\\noptimizer_in_bwd=False \\\nenable_activation_offloading=True \\\noptimizer=torch.optim.AdamW \\\ntokenizer.max_seq_len=4096 \\\ngradient_accumulation_steps=1 \\\nepochs=1 \\\nbatch_size=2\n```\n\n&nbsp;\n\n## Installation\n\ntorchtune is tested with the latest stable PyTorch release as well as the preview nightly version. torchtune leverages\ntorchvision for finetuning multimodal LLMs and torchao for the latest in quantization techniques; you should install these as well.\n\n&nbsp;\n\n### Install stable release\n\n```bash\n# Install stable PyTorch, torchvision, torchao stable releases\npip install torch torchvision torchao\npip install torchtune\n```\n\n&nbsp;\n\n### Install nightly release\n\n```bash\n# Install PyTorch, torchvision, torchao nightlies\npip install --pre --upgrade torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu126 # full options are cpu/cu118/cu121/cu124/cu126\npip install --pre --upgrade torchtune --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n```\n\nYou can also check out our [install documentation](https://pytorch.org/torchtune/main/install.html) for more information, including installing torchtune from source.\n\n&nbsp;\n\nTo confirm that the package is installed correctly, you can run the following command:\n\n```bash\ntune --help\n```\n\nAnd should see the following output:\n\n```bash\nusage: tune [-h] {ls,cp,download,run,validate} ...\n\nWelcome to the torchtune CLI!\n\noptions:\n  -h, --help            show this help message and exit\n\n...\n```\n\n&nbsp;\n\n## Get Started\n\nTo get started with torchtune, see our [First Finetune Tutorial](https://pytorch.org/torchtune/main/tutorials/first_finetune_tutorial.html). Our [End-to-End Workflow Tutorial](https://pytorch.org/torchtune/main/tutorials/e2e_flow.html) will show you how to evaluate, quantize and run inference with a Llama model. The rest of this section will provide a quick overview of these steps with Llama3.1.\n\n\n### Downloading a model\n\nFollow the instructions on the official [`meta-llama`](https://huggingface.co/meta-llama) repository to ensure you have access to the official Llama model weights. Once you have confirmed access, you can run the following command to download the weights to your local machine. This will also download the tokenizer model and a responsible use guide.\n\nTo download Llama3.1, you can run:\n\n```bash\ntune download meta-llama/Meta-Llama-3.1-8B-Instruct \\\n--output-dir /tmp/Meta-Llama-3.1-8B-Instruct \\\n--ignore-patterns \"original/consolidated.00.pth\" \\\n--hf-token <HF_TOKEN> \\\n```\n\n> [!Tip]\n> Set your environment variable `HF_TOKEN` or pass in `--hf-token` to the command in order to validate your access. You can find your token at https://huggingface.co/settings/tokens\n\n&nbsp;\n\n### Running finetuning recipes\n\nYou can finetune Llama3.1 8B with LoRA on a single GPU using the following command:\n\n```bash\ntune run lora_finetune_single_device --config llama3_1/8B_lora_single_device\n```\n\nFor distributed training, tune CLI integrates with [torchrun](https://pytorch.org/docs/stable/elastic/run.html).\nTo run a full finetune of Llama3.1 8B on two GPUs:\n\n```bash\ntune run --nproc_per_node 2 full_finetune_distributed --config llama3_1/8B_full\n```\n\n> [!Tip]\n> Make sure to place any torchrun commands **before** the recipe specification. Any CLI args after this will override the config and not impact distributed training.\n\n&nbsp;\n\n### Modify Configs\n\nThere are two ways in which you can modify configs:\n\n**Config Overrides**\n\nYou can directly overwrite config fields from the command line:\n\n```bash\ntune run lora_finetune_single_device \\\n--config llama2/7B_lora_single_device \\\nbatch_size=8 \\\nenable_activation_checkpointing=True \\\nmax_steps_per_epoch=128\n```\n\n**Update a Local Copy**\n\nYou can also copy the config to your local directory and modify the contents directly:\n\n```bash\ntune cp llama3_1/8B_full ./my_custom_config.yaml\nCopied to ./my_custom_config.yaml\n```\n\nThen, you can run your custom recipe by directing the `tune run` command to your local files:\n\n```bash\ntune run full_finetune_distributed --config ./my_custom_config.yaml\n```\n\n&nbsp;\n\nCheck out `tune --help` for all possible CLI commands and options. For more information on using and updating configs, take a look at our [config deep-dive](https://pytorch.org/torchtune/main/deep_dives/configs.html).\n\n&nbsp;\n\n### Custom Datasets\n\ntorchtune supports finetuning on a variety of different datasets, including [instruct-style](https://pytorch.org/torchtune/main/basics/instruct_datasets.html), [chat-style](https://pytorch.org/torchtune/main/basics/chat_datasets.html), [preference datasets](https://pytorch.org/torchtune/main/basics/preference_datasets.html), and more. If you want to learn more about how to apply these components to finetune on your own custom dataset, please check out the provided links along with our [API docs](https://pytorch.org/torchtune/main/api_ref_datasets.html).\n\n&nbsp;\n\n## Community\n\ntorchtune focuses on integrating with popular tools and libraries from the ecosystem. These are just a few examples, with more under development:\n\n- [Hugging Face Hub](https://huggingface.co/docs/hub/en/index) for [accessing model weights](torchtune/_cli/download.py)\n- [EleutherAI's LM Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness) for [evaluating](recipes/eleuther_eval.py) trained models\n- [Hugging Face Datasets](https://huggingface.co/docs/datasets/en/index) for [access](torchtune/datasets/_instruct.py) to training and evaluation datasets\n- [PyTorch FSDP2](https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md) for distributed training\n- [torchao](https://github.com/pytorch-labs/ao) for lower precision dtypes and [post-training quantization](recipes/quantize.py) techniques\n- [Weights & Biases](https://wandb.ai/site) for [logging](https://pytorch.org/torchtune/main/deep_dives/wandb_logging.html) metrics and checkpoints, and tracking training progress\n- [Comet](https://www.comet.com/site/) as another option for [logging](https://pytorch.org/torchtune/main/deep_dives/comet_logging.html)\n- [ExecuTorch](https://pytorch.org/executorch-overview) for [on-device inference](https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning) using finetuned models\n- [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index) for low memory optimizers for our [single-device recipes](recipes/configs/llama2/7B_full_low_memory.yaml)\n- [PEFT](https://github.com/huggingface/peft) for continued finetuning or inference with torchtune models in the Hugging Face ecosystem\n\n&nbsp;\n\n### Community Contributions\n\nWe really value our community and the contributions made by our wonderful users. We'll use this section to call out some of these contributions. If you'd like to help out as well, please see the [CONTRIBUTING](CONTRIBUTING.md) guide.\n\n- [@SalmanMohammadi](https://github.com/salmanmohammadi) for adding a comprehensive end-to-end recipe for [Reinforcement Learning from Human Feedback (RLHF)](recipes/ppo_full_finetune_single_device.py) finetuning with PPO to torchtune\n- [@fyabc](https://github.com/fyabc) for adding Qwen2 models, tokenizer, and recipe integration to torchtune\n- [@solitude-alive](https://github.com/solitude-alive) for adding the [Gemma 2B model](torchtune/models/gemma/) to torchtune, including recipe changes, numeric validations of the models and recipe correctness\n- [@yechenzhi](https://github.com/yechenzhi) for adding [Direct Preference Optimization (DPO)](recipes/lora_dpo_single_device.py) to torchtune, including the recipe and config along with correctness checks\n- [@Optimox](https://github.com/Optimox) for adding all the [Gemma2 variants](torchtune/models/gemma2) to torchtune!\n\n\n&nbsp;\n\n## Acknowledgements\n\nThe Llama2 code in this repository is inspired by the original [Llama2 code](https://github.com/meta-llama/llama/blob/main/llama/model.py).\n\nWe want to give a huge shout-out to EleutherAI, Hugging Face and Weights & Biases for being wonderful collaborators and for working with us on some of these integrations within torchtune.\n\nWe also want to acknowledge some awesome libraries and tools from the ecosystem:\n- [gpt-fast](https://github.com/pytorch-labs/gpt-fast) for performant LLM inference techniques which we've adopted out-of-the-box\n- [llama recipes](https://github.com/meta-llama/llama-recipes) for spring-boarding the llama2 community\n- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) for bringing several memory and performance based techniques to the PyTorch ecosystem\n- [@winglian](https://github.com/winglian/) and [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) for early feedback and brainstorming on torchtune's design and feature set.\n- [lit-gpt](https://github.com/Lightning-AI/litgpt) for pushing the LLM finetuning community forward.\n- [HF TRL](https://github.com/huggingface/trl) for making reward modeling more accessible to the PyTorch community.\n\n&nbsp;\n\n\n## License\n\ntorchtune is released under the [BSD 3 license](./LICENSE). However you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models.\n\n\n## Citing torchtune\n\nIf you find the torchtune library useful, please cite it in your work as below.\n\n```bibtex\n@software{torchtune,\n  title = {torchtune: PyTorch's finetuning library},\n  author = {torchtune maintainers and contributors},\n  url = {https//github.com/pytorch/torchtune},\n  license = {BSD-3-Clause},\n  month = apr,\n  year = {2024}\n}\n```\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.3515625,
          "content": "# ---- All project specifications ---- #\n[project]\nname = \"torchtune\"\ndescription = \"A native-PyTorch library for LLM fine-tuning\"\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\nlicense = {file = \"LICENSE\"}\nauthors = [\n    { name = \"PyTorch Team\", email = \"packages@pytorch.org\" },\n]\nkeywords = [\"pytorch\", \"finetuning\", \"llm\"]\ndependencies = [\n\n    # Hugging Face integrations\n    \"datasets\",\n    \"huggingface_hub[hf_transfer]\",\n    \"safetensors\",\n\n    # Kaggle Integrations\n    \"kagglehub\",\n\n    # Tokenization\n    \"sentencepiece\",\n    \"tiktoken\",\n    \"blobfile>=2\",\n\n    # Miscellaneous\n    \"numpy\",\n    \"tqdm\",\n    \"omegaconf\",\n    \"psutil\",\n\n    # Multimodal\n    \"Pillow>=9.4.0\",\n\n]\ndynamic = [\"version\"]\n\n[project.urls]\nGitHub = \"https://github.com/pytorch/torchtune\"\nDocumentation = \"https://pytorch.org/torchtune/main/index.html\"\nIssues = \"https://github.com/pytorch/torchtune/issues\"\n\n[project.scripts]\ntune = \"torchtune._cli.tune:main\"\n\n[project.optional-dependencies]\ndev = [\n    \"bitsandbytes>=0.43.0\",\n    \"comet_ml>=3.44.2\",\n    \"pre-commit\",\n    \"pytest==7.4.0\",\n    \"pytest-cov\",\n    \"pytest-mock\",\n    \"pytest-integration\",\n    \"tensorboard\",\n    # Pin urllib3 to avoid transient error from https://github.com/psf/requests/issues/6443\n    \"urllib3<2.0.0\",\n    \"wandb\",\n    \"expecttest\",\n]\n\n[tool.setuptools.dynamic]\nversion = {attr = \"torchtune.__version__\"}\n\n\n# ---- Explicit project build information ---- #\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools.packages.find]\nwhere = [\"\"]\ninclude = [\"torchtune*\", \"recipes*\"]\n\n[tool.setuptools.package-data]\nrecipes = [\"configs/*.yaml\", \"configs/*/*.yaml\", \"configs/*/*/*.yaml\"]\n\n\n# ---- Tooling specifications ---- #\n[tool.usort]\nfirst_party_detection = false\n\n[tool.black]\ntarget-version = [\"py38\"]\n\n[tool.pydoclint]\nstyle = 'google'\ncheck-return-types = 'False'\nexclude = 'tests/torchtune/models/(\\w+)/scripts/'\n\n[tool.pytest.ini_options]\naddopts = [\"--showlocals\", \"--import-mode=prepend\", \"--without-integration\", \"--without-slow-integration\"]\n# --showlocals will show local variables in tracebacks\n# --import-mode=prepend will add the root (the parent dir of torchtune/, tests/, recipes/)\n# to `sys.path` when invoking pytest, allowing us to treat `tests` as a package within the tests.\n# --without-integration and --without-slow-integration: default to running unit tests only\n"
        },
        {
          "name": "recipes",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "torchtune",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.005859375,
          "content": "0.6.0\n"
        }
      ]
    }
  ]
}