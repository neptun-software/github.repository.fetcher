{
  "metadata": {
    "timestamp": 1736559544855,
    "page": 146,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "s3tools/s3cmd",
      "stars": 4624,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".ci.s3cfg",
          "type": "blob",
          "size": 1.9521484375,
          "content": "[default]\naccess_key = Q3AM3UQ867SPQQA43P2F\naccess_token = \nadd_encoding_exts = \nadd_headers = \nbucket_location = us-east-1\nca_certs_file = \ncache_file = \ncheck_ssl_certificate = True\ncheck_ssl_hostname = True\ncloudfront_host = cloudfront.amazonaws.com\ndefault_mime_type = binary/octet-stream\ndelay_updates = False\ndelete_after = False\ndelete_after_fetch = False\ndelete_removed = False\ndry_run = False\nenable_multipart = True\nencoding = UTF-8\nencrypt = False\nexpiry_date = \nexpiry_days = \nexpiry_prefix = \nfollow_symlinks = False\nforce = False\nget_continue = False\ngpg_command = /usr/bin/gpg\ngpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s\ngpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s\ngpg_passphrase = \nguess_mime_type = True\nhost_base = localhost:9000\nhost_bucket = localhost:9000\nhuman_readable_sizes = False\ninvalidate_default_index_on_cf = False\ninvalidate_default_index_root_on_cf = True\ninvalidate_on_cf = False\nkms_key = \nlimit = -1\nlimitrate = 0\nlist_md5 = False\nlist_allow_unordered = False\nlog_target_prefix = \nlong_listing = False\nmax_delete = -1\nmime_type = \nmultipart_chunk_size_mb = 15\nmultipart_max_chunks = 10000\npreserve_attrs = True\nprogress_meter = True\nproxy_host = \nproxy_port = 0\nput_continue = False\nrecursive = False\nrecv_chunk = 65536\nreduced_redundancy = False\nrequester_pays = False\nrestore_days = 1\nsecret_key = zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\nsend_chunk = 65536\nserver_side_encryption = False\nsignature_v2 = False\nsimpledb_host = sdb.amazonaws.com\nskip_existing = False\nsocket_timeout = 300\nstats = False\nstop_on_error = False\nstorage_class = \nurlencoding_mode = normal\nuse_http_expect = False\nuse_https = False\nuse_mime_magic = True\nverbosity = WARNING\nwebsite_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/\nwebsite_error = \nwebsite_index = index.html\n"
        },
        {
          "name": ".codespellrc",
          "type": "blob",
          "size": 0.1591796875,
          "content": "[codespell]\nskip = .git,*.pdf,*.svg\n# OptionAll,parms -- variable names used\n# bu -- used in bu.ck.et, decided to just skip\nignore-words-list = optionall,parms,bu\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.037109375,
          "content": "testsuite\ntestsuite-out\n/dist\nbuild/*\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0966796875,
          "content": "*.pyc\n*.swp\ntestsuite\ntestsuite-out\n/MANIFEST\n/dist\nbuild/*\ns3cmd.egg-info\ns3cmd.spec\n.idea\n.s3cfg\n"
        },
        {
          "name": ".svnignore",
          "type": "blob",
          "size": 0.1669921875,
          "content": "## Run 'svn propset svn:ignore -F .svnignore .' after you change this list\n*.pyc\n.*.swp\ntestsuite\ntestsuite-out\nMANIFEST\ndist\nbuild\ns3cmd.egg-info\ns3cmd.spec\n.idea\n.s3cfg\n"
        },
        {
          "name": "INSTALL.md",
          "type": "blob",
          "size": 3.1689453125,
          "content": "Installation of s3cmd package\n=============================\n\nCopyright:\n    TGRMN Software and contributors\n\nS3tools / S3cmd project homepage:\n    http://s3tools.org\n\n!!!\n!!! Please consult README file for setup, usage and examples!\n!!!\n\nPackage formats\n---------------\nS3cmd is distributed in two formats:\n\n1) Prebuilt RPM file - should work on most RPM-based\n   distributions\n\n2) Source .tar.gz package\n\nInstallation of Brew package\n---------------------------\n```\nbrew install s3cmd\n```\n\nInstallation of RPM package\n---------------------------\nAs user \"root\" run:\n```\nrpm -ivh s3cmd-X.Y.Z.noarch.rpm\n```\nwhere X.Y.Z is the most recent s3cmd release version.\n\nYou may be informed about missing dependencies\non Python or some libraries. Please consult your \ndistribution documentation on ways to solve the problem.\n\nInstallation from PyPA (Python Package Authority)\n---------------------\nS3cmd can be installed from the PyPA using PIP (the recommended tool for PyPA).\n\n1) Confirm you have PIP installed. PIP home page is here: https://pypi.python.org/pypi/pip. Example install on a RHEL yum based machine\n```\nsudo yum install python-pip\n```\n2) Install with pip\n```\nsudo pip install s3cmd\n```\n\nInstallation from zip file \n--------------------------\nThere are three options to run s3cmd from source tarball:\n\n1) The S3cmd program, as distributed in s3cmd-X.Y.Z.tar.gz\n   on SourceForge or in master.zip on GitHub, can be run directly \n   from where you unzipped the package.\n\n2) Or you may want to move \"s3cmd\" file and \"S3\" subdirectory\n   to some other path. Make sure that \"S3\" subdirectory ends up\n   in the same place where you move the \"s3cmd\" file. \n\n   For instance if you decide to move s3cmd to you $HOME/bin\n   you will have $HOME/bin/s3cmd file and $HOME/bin/S3 directory \n   with a number of support files.\n\n3) The cleanest and most recommended approach is to unzip the \n   package and then just run:\n   \n   `python setup.py install`\n\n   You will however need Python \"distutils\" module for this to \n   work. It is often part of the core python package (e.g. in \n   OpenSuse Python 2.5 package) or it can be installed using your\n   package manager, e.g. in Debian use \n   \n   `apt-get install python-setuptools`\n\n   Again, consult your distribution documentation on how to \n   find out the actual package name and how to install it then.\n\n   Note that on Linux, if you are not \"root\" already, you may \n   need to run:\n   \n   `sudo python setup.py install`\n\n   instead.\n\n\nNote to distributions package maintainers\n----------------------------------------\nDefine shell environment variable S3CMD_PACKAGING=yes if you\ndon't want setup.py to install manpages and doc files. You'll\nhave to install them manually in your .spec or similar package\nbuild scripts.\n\nOn the other hand if you want setup.py to install manpages \nand docs, but to other than default path, define env \nvariables $S3CMD_INSTPATH_MAN and $S3CMD_INSTPATH_DOC. Check \nout setup.py for details and default values.\n\n\nWhere to get help\n-----------------\nIf in doubt, or if something doesn't work as expected, \nget back to us via mailing list:\n```\ns3tools-general@lists.sourceforge.net\n```\n\nor visit the S3cmd / S3tools homepage at: [http://s3tools.org](http://s3tools.org)\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 14.86328125,
          "content": "                    GNU GENERAL PUBLIC LICENSE\n                       Version 2, June 1991\n\n Copyright (C) 1989, 1991 Free Software Foundation, Inc.,\n 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The licenses for most software are designed to take away your\nfreedom to share and change it.  By contrast, the GNU General Public\nLicense is intended to guarantee your freedom to share and change free\nsoftware--to make sure the software is free for all its users.  This\nGeneral Public License applies to most of the Free Software\nFoundation's software and to any other program whose authors commit to\nusing it.  (Some other Free Software Foundation software is covered by\nthe GNU Lesser General Public License instead.)  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthis service if you wish), that you receive source code or can get it\nif you want it, that you can change the software or use pieces of it\nin new free programs; and that you know you can do these things.\n\n  To protect your rights, we need to make restrictions that forbid\nanyone to deny you these rights or to ask you to surrender the rights.\nThese restrictions translate to certain responsibilities for you if you\ndistribute copies of the software, or if you modify it.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must give the recipients all the rights that\nyou have.  You must make sure that they, too, receive or can get the\nsource code.  And you must show them these terms so they know their\nrights.\n\n  We protect your rights with two steps: (1) copyright the software, and\n(2) offer you this license which gives you legal permission to copy,\ndistribute and/or modify the software.\n\n  Also, for each author's protection and ours, we want to make certain\nthat everyone understands that there is no warranty for this free\nsoftware.  If the software is modified by someone else and passed on, we\nwant its recipients to know that what they have is not the original, so\nthat any problems introduced by others will not reflect on the original\nauthors' reputations.\n\n  Finally, any free program is threatened constantly by software\npatents.  We wish to avoid the danger that redistributors of a free\nprogram will individually obtain patent licenses, in effect making the\nprogram proprietary.  To prevent this, we have made it clear that any\npatent must be licensed for everyone's free use or not licensed at all.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                    GNU GENERAL PUBLIC LICENSE\n   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n\n  0. This License applies to any program or other work which contains\na notice placed by the copyright holder saying it may be distributed\nunder the terms of this General Public License.  The \"Program\", below,\nrefers to any such program or work, and a \"work based on the Program\"\nmeans either the Program or any derivative work under copyright law:\nthat is to say, a work containing the Program or a portion of it,\neither verbatim or with modifications and/or translated into another\nlanguage.  (Hereinafter, translation is included without limitation in\nthe term \"modification\".)  Each licensee is addressed as \"you\".\n\nActivities other than copying, distribution and modification are not\ncovered by this License; they are outside its scope.  The act of\nrunning the Program is not restricted, and the output from the Program\nis covered only if its contents constitute a work based on the\nProgram (independent of having been made by running the Program).\nWhether that is true depends on what the Program does.\n\n  1. You may copy and distribute verbatim copies of the Program's\nsource code as you receive it, in any medium, provided that you\nconspicuously and appropriately publish on each copy an appropriate\ncopyright notice and disclaimer of warranty; keep intact all the\nnotices that refer to this License and to the absence of any warranty;\nand give any other recipients of the Program a copy of this License\nalong with the Program.\n\nYou may charge a fee for the physical act of transferring a copy, and\nyou may at your option offer warranty protection in exchange for a fee.\n\n  2. You may modify your copy or copies of the Program or any portion\nof it, thus forming a work based on the Program, and copy and\ndistribute such modifications or work under the terms of Section 1\nabove, provided that you also meet all of these conditions:\n\n    a) You must cause the modified files to carry prominent notices\n    stating that you changed the files and the date of any change.\n\n    b) You must cause any work that you distribute or publish, that in\n    whole or in part contains or is derived from the Program or any\n    part thereof, to be licensed as a whole at no charge to all third\n    parties under the terms of this License.\n\n    c) If the modified program normally reads commands interactively\n    when run, you must cause it, when started running for such\n    interactive use in the most ordinary way, to print or display an\n    announcement including an appropriate copyright notice and a\n    notice that there is no warranty (or else, saying that you provide\n    a warranty) and that users may redistribute the program under\n    these conditions, and telling the user how to view a copy of this\n    License.  (Exception: if the Program itself is interactive but\n    does not normally print such an announcement, your work based on\n    the Program is not required to print an announcement.)\n\nThese requirements apply to the modified work as a whole.  If\nidentifiable sections of that work are not derived from the Program,\nand can be reasonably considered independent and separate works in\nthemselves, then this License, and its terms, do not apply to those\nsections when you distribute them as separate works.  But when you\ndistribute the same sections as part of a whole which is a work based\non the Program, the distribution of the whole must be on the terms of\nthis License, whose permissions for other licensees extend to the\nentire whole, and thus to each and every part regardless of who wrote it.\n\nThus, it is not the intent of this section to claim rights or contest\nyour rights to work written entirely by you; rather, the intent is to\nexercise the right to control the distribution of derivative or\ncollective works based on the Program.\n\nIn addition, mere aggregation of another work not based on the Program\nwith the Program (or with a work based on the Program) on a volume of\na storage or distribution medium does not bring the other work under\nthe scope of this License.\n\n  3. You may copy and distribute the Program (or a work based on it,\nunder Section 2) in object code or executable form under the terms of\nSections 1 and 2 above provided that you also do one of the following:\n\n    a) Accompany it with the complete corresponding machine-readable\n    source code, which must be distributed under the terms of Sections\n    1 and 2 above on a medium customarily used for software interchange; or,\n\n    b) Accompany it with a written offer, valid for at least three\n    years, to give any third party, for a charge no more than your\n    cost of physically performing source distribution, a complete\n    machine-readable copy of the corresponding source code, to be\n    distributed under the terms of Sections 1 and 2 above on a medium\n    customarily used for software interchange; or,\n\n    c) Accompany it with the information you received as to the offer\n    to distribute corresponding source code.  (This alternative is\n    allowed only for noncommercial distribution and only if you\n    received the program in object code or executable form with such\n    an offer, in accord with Subsection b above.)\n\nThe source code for a work means the preferred form of the work for\nmaking modifications to it.  For an executable work, complete source\ncode means all the source code for all modules it contains, plus any\nassociated interface definition files, plus the scripts used to\ncontrol compilation and installation of the executable.  However, as a\nspecial exception, the source code distributed need not include\nanything that is normally distributed (in either source or binary\nform) with the major components (compiler, kernel, and so on) of the\noperating system on which the executable runs, unless that component\nitself accompanies the executable.\n\nIf distribution of executable or object code is made by offering\naccess to copy from a designated place, then offering equivalent\naccess to copy the source code from the same place counts as\ndistribution of the source code, even though third parties are not\ncompelled to copy the source along with the object code.\n\n  4. You may not copy, modify, sublicense, or distribute the Program\nexcept as expressly provided under this License.  Any attempt\notherwise to copy, modify, sublicense or distribute the Program is\nvoid, and will automatically terminate your rights under this License.\nHowever, parties who have received copies, or rights, from you under\nthis License will not have their licenses terminated so long as such\nparties remain in full compliance.\n\n  5. You are not required to accept this License, since you have not\nsigned it.  However, nothing else grants you permission to modify or\ndistribute the Program or its derivative works.  These actions are\nprohibited by law if you do not accept this License.  Therefore, by\nmodifying or distributing the Program (or any work based on the\nProgram), you indicate your acceptance of this License to do so, and\nall its terms and conditions for copying, distributing or modifying\nthe Program or works based on it.\n\n  6. Each time you redistribute the Program (or any work based on the\nProgram), the recipient automatically receives a license from the\noriginal licensor to copy, distribute or modify the Program subject to\nthese terms and conditions.  You may not impose any further\nrestrictions on the recipients' exercise of the rights granted herein.\nYou are not responsible for enforcing compliance by third parties to\nthis License.\n\n  7. If, as a consequence of a court judgment or allegation of patent\ninfringement or for any other reason (not limited to patent issues),\nconditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot\ndistribute so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you\nmay not distribute the Program at all.  For example, if a patent\nlicense would not permit royalty-free redistribution of the Program by\nall those who receive copies directly or indirectly through you, then\nthe only way you could satisfy both it and this License would be to\nrefrain entirely from distribution of the Program.\n\nIf any portion of this section is held invalid or unenforceable under\nany particular circumstance, the balance of the section is intended to\napply and the section as a whole is intended to apply in other\ncircumstances.\n\nIt is not the purpose of this section to induce you to infringe any\npatents or other property right claims or to contest validity of any\nsuch claims; this section has the sole purpose of protecting the\nintegrity of the free software distribution system, which is\nimplemented by public license practices.  Many people have made\ngenerous contributions to the wide range of software distributed\nthrough that system in reliance on consistent application of that\nsystem; it is up to the author/donor to decide if he or she is willing\nto distribute software through any other system and a licensee cannot\nimpose that choice.\n\nThis section is intended to make thoroughly clear what is believed to\nbe a consequence of the rest of this License.\n\n  8. If the distribution and/or use of the Program is restricted in\ncertain countries either by patents or by copyrighted interfaces, the\noriginal copyright holder who places the Program under this License\nmay add an explicit geographical distribution limitation excluding\nthose countries, so that distribution is permitted only in or among\ncountries not thus excluded.  In such case, this License incorporates\nthe limitation as if written in the body of this License.\n\n  9. The Free Software Foundation may publish revised and/or new versions\nof the General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\nEach version is given a distinguishing version number.  If the Program\nspecifies a version number of this License which applies to it and \"any\nlater version\", you have the option of following the terms and conditions\neither of that version or of any later version published by the Free\nSoftware Foundation.  If the Program does not specify a version number of\nthis License, you may choose any version ever published by the Free Software\nFoundation.\n\n  10. If you wish to incorporate parts of the Program into other free\nprograms whose distribution conditions are different, write to the author\nto ask for permission.  For software which is copyrighted by the Free\nSoftware Foundation, write to the Free Software Foundation; we sometimes\nmake exceptions for this.  Our decision will be guided by the two goals\nof preserving the free status of all derivatives of our free software and\nof promoting the sharing and reuse of software generally.\n\n                            NO WARRANTY\n\n  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY\nFOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN\nOTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES\nPROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED\nOR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS\nTO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE\nPROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,\nREPAIR OR CORRECTION.\n\n  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR\nREDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,\nINCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING\nOUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED\nTO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY\nYOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER\nPROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n                     END OF TERMS AND CONDITIONS\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.056640625,
          "content": "include INSTALL.md README.md LICENSE NEWS\ninclude s3cmd.1\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.6474609375,
          "content": "SHELL  := /bin/bash\nVERSION := $(shell /usr/bin/env python2 -c 'from S3 import PkgInfo;print PkgInfo.version')\nSPEC   := s3cmd.spec\nCOMMIT := $(shell git rev-parse HEAD)\nSHORTCOMMIT := $(shell git rev-parse --short=8 HEAD)\nTARBALL = s3cmd-$(VERSION)-$(SHORTCOMMIT).tar.gz\n\nrelease:\n\tpython2 setup.py register sdist upload --sign\n\nclean:\n\t-rm -rf s3cmd-*.tar.gz *.rpm *~ $(SPEC)\n\t-find . -name \\*.pyc -exec rm \\{\\} \\;\n\t-find . -name \\*.pyo -exec rm \\{\\} \\;\n\n$(SPEC): $(SPEC).in\n\tsed -e 's/##VERSION##/$(VERSION)/' \\\n            -e 's/##COMMIT##/$(COMMIT)/' \\\n            -e 's/##SHORTCOMMIT##/$(SHORTCOMMIT)/' \\\n            $(SPEC).in > $(SPEC)\n\n# fixme: python setup.py sdist also generates a PKG-INFO file which we don't have using straight git archive\ngit-tarball:\n\tgit archive --format tar --prefix s3cmd-$(COMMIT)/ HEAD S3/ s3cmd NEWS README.md LICENSE INSTALL.md setup.cfg s3cmd.1 setup.py| gzip -c > $(TARBALL)\n\n# Use older digest algorithms for local rpmbuilds, as EPEL5 and\n# earlier releases need this.  When building using mock for a\n# particular target, it will use the proper (newer) digests if that\n# target supports it.\ngit-rpm: clean git-tarball $(SPEC)\n\ttmp_dir=`mktemp -d` ; \\\n\tmkdir -p $${tmp_dir}/{BUILD,RPMS,SRPMS,SPECS,SOURCES} ; \\\n\tcp $(TARBALL) $${tmp_dir}/SOURCES ; \\\n\tcp $(SPEC) $${tmp_dir}/SPECS ; \\\n\tcd $${tmp_dir} > /dev/null 2>&1; \\\n\trpmbuild -ba --define \"_topdir $${tmp_dir}\" \\\n\t  --define \"_source_filedigest_algorithm 0\" \\\n\t  --define \"_binary_filedigest_algorithm 0\" \\\n\t  --define \"dist %{nil}\" \\\n          SPECS/$(SPEC) ; \\\n\tcd - > /dev/null 2>&1; \\\n\tcp $${tmp_dir}/RPMS/noarch/* $${tmp_dir}/SRPMS/* . ; \\\n\trm -rf $${tmp_dir} ; \\\n\trpmlint *.rpm *.spec\n"
        },
        {
          "name": "NEWS",
          "type": "blob",
          "size": 25.7578125,
          "content": "s3cmd-2.4.0      - 2023-12-12\n===============\n* Added \"setversioning\" command for versioning configuration (Kuan-Chun Wang)\n* Added \"settagging\", \"gettagging\", and \"deltagging\" commands for bucket/object tagging (Kuan-Chun Wang)\n* Added \"setobjectretention\" and \"setobjectlegalhold\" commands (Etienne Adam/Withings SAS)\n* Added \"setownership\" and \"setblockpublicaccess\" commands\n* Added \"cfinval\" command to request Cloudfront to invalidate paths (#1256)\n* Added \"--keep-dirs\" option to have the folder structure preserved on remote side\n* Added --skip-destination-validation option for \"setnotification\" command (Kuan-Chun Wang)\n* Added \"--max-retries\" flag and \"max_retries\" config option (#914)\n* Added FIPS support (Michael Roth)\n* Added \"object ownership\" and block public access\" values to \"info\" command output for buckets\n* Added to \"ls\" command a \"DIROBJ\" tag for directory objects in S3 remote\n* Added server profiles to run-tests.py to skip tests depending on the server type\n* Fixed \"TypeError: sequence item 1: expected str instance, bytes found\" error with Python 3.12 (#1343)\n* Fixed a missing return for \"object_batch_delete\" of S3.py  (James Hewitt)\n* Fixed \"object is not callable\" error because of md5 FIPS test (#1005)\n* Fixed \"compute_content_md5 is not defined\" error for \"setversioning\" (#1312) (Gavin John)\n* Fixed list objects to use NextMarker when only prefixes are returned (Albin Parou)\n* Fixed upload to not retry when an S3 compatible server is full\n* Fixed recursive delete of objects named with whitespace (#976)\n* Fixed the mime type when uploading directories to be \"application/x-directory\"\n* Fixed \"string indices must be integers\" error for sync when in dry-run mode (#1313) (Shohei Tanaka)\n* Fixed SignatureDoesNotMatch error when modifying an object on Cloudflare R2 (#1332) (Philip C Nilsson)\n* Fixed Cloudfront invalidation issue for paths with wildcard or special characters\n* Fixed Cloudfront crash because of error reporting for retries\n* Fixed Cloudfront \"unable to parse URL\" error (#1292)\n* Improved the handling of \"empty\" files on the remote side to sync with local folders\n* Improved \"abortmp\" command by requiring an object to avoid bad accidents when using Ceph (Joshua Haas)\n* Improved file download by retrying when encountering SlowDown or TooManyRequests errors (Robin Geiger)\n* Improved error messages in case of connection error or host unreachable\n* Improved error messages to be more explicit for upload errors after retries\n* Improved remote2local attributes setting code\n* Improved remote2local with more explicit error messages when setting attributes (#1288)\n* Improved remote2local output messages by using the \"mkdir\" prefix instead of \"make dir\"\n* Improved the SortedDict class\n* Improved run-test.py by using \"--include\" when calling Curl instead of \"-include\" (Matthew James Kraai)\n* Improved GitHub CI by enabling pip cache in actions/setup-python (Anton Yakutovich)\n* Improved GitHub CI by adding a \"codespell\" check on push and PRs (Yaroslav Halchenko)\n* Updated GitHub CI tests to use more recent versions of Minio and Python\n* Upgraded GitHub actions (Anton Yakutovich)\n* Cleanup and update of copyright headers, docs, comments and setup.py\n* Cleanup to fix \"invalid escape sequence\" syntax warnings\n* Many other bug fixes and cleanups\n\ns3cmd-2.3.0      - 2022-10-03\n===============\n* Added \"getnotification\", \"setnotification\", and \"delnotification\" commands for notification policies (hrchu)\n* Added support for AWS_STS_REGIONAL_ENDPOINTS (#1218, #1228) (Johan Lanzrein)\n* Added ConnectionRefused [111] exit code to handle connection errors (Salar Nosrati-Ershad)\n* Added support for IMDSv2. Should work automatically on ec2 (Anthony Foiani)\n* Added --list-allow-unordered to list objects unordered. Only supported by Ceph based s3-compatible services (#1269) (Salar Nosrati-Ershad)\n* Fixed --exclude dir behavior for python >= 3.6 (Daniil Tararukhin)\n* Fixed Cloudfront invalidate retry issue (Yuan-Hsiang Lee)\n* Fixed 0 byte cache files crashing s3cmd (#1234) (Carlos Laviola)\n* Fixed --continue behavior for the \"get\" command (#1009) (Anton Ustyugov)\n* Fixed unicode issue with fixbucket (#1259)\n* Fixed CannotSendRequest and ConnectionRefusedError errors at startup (#1261)\n* Fixed error reporting for object info when the object does not exist\n* Fixed \"setup.py test\" to do nothing to avoid failure that could be problematic for distribution packaging (#996)\n* Improved expire command to use Rule/Filter/Prefix for LifecycleConfiguration (#1247)\n* Improved PASS/CHECK/INCLUDE/EXCLUDE debug log messages\n* Improved setup.py with python 3.9 and 3.10 support info(Ori Avtalion)\n* Many other bug fixes\n\n\ns3cmd-2.2.0      - 2021-09-27\n===============\n* Added support for metadata modification of files bigger than 5 GiB\n* Added support for remote copy of files bigger than 5 GiB using MultiPart copy (Damian Martinez, Florent Viard)\n* Added progress info output for multipart copy and current-total info in output for cp, mv and modify\n* Added support for all special/foreign character names in object names to cp/mv/modify\n* Added support for SSL authentication (Aleksandr Chazov)\n* Added the http error 429 to the list of retryable errors (#1096)\n* Added support for listing and resuming of multipart uploads of more than 1000 parts (#346)\n* Added time based expiration for idle pool connections in order to avoid random broken pipe errors (#1114)\n* Added support for STS webidentity authentication (ie AssumeRole and AssumeRoleWithWebIdentity) (Samskeyti, Florent Viard)\n* Added support for custom headers to the mb command (#1197) (Sébastien Vajda)\n* Improved MultiPart copy to preserve acl and metadata of objects\n* Improved the server errors catching and reporting for cp/mv/modify commands\n* Improved resiliency against servers sending garbage responses (#1088, #1090, #1093)\n* Improved remote copy to have consistent copy of metadata in all cases: multipart or not, aws or not\n* Improved security by revoking public-write acl when private acl is set (#1151) (ruanzitao)\n* Improved speed when running on an EC2 instance (#1117) (Patrick Allain)\n* Reduced connection_max_age to 5s to avoid broken pipes as AWS closes https conns after around 6s (#1114)\n* Ensure that KeyboardInterrupt are always properly raised (#1089)\n* Changed sized of multipart copy chunks to 1 GiB\n* Fixed ValueError when using more than one \":\" inside add_header in config file (#1087)\n* Fixed extra label issue when stdin used as source of a MultiPart upload\n* Fixed remote copy to allow changing the mime-type (ie content-type) of the new object\n* Fixed remote_copy to ensure that meta-s3cmd-attrs will be set based on the real source and not on the copy source\n* Fixed deprecation warnings due to invalid escape sequences (Karthikeyan Singaravelan)\n* Fixed getbucketinfo that was broken when the bucket lifecycle uses the filter element (Liu Lan)\n* Fixed RestoreRequest XML namespace URL (#1203) (Akete)\n* Fixed PARTIAL exit code that was not properly set when needed for object_get (#1190)\n* Fixed a possible infinite loop when a file is truncated during hashsum or upload (#1125) (Matthew Krokosz, Florent Viard)\n* Fixed report_exception wrong error when LANG env var was not set (#1113)\n* Fixed wrong wiki url in error messages (Alec Barrett)\n* Py3: Fixed an AttributeError when using the \"files-from\" option\n* Py3: Fixed compatibility issues due to the removal of getchildren() from ElementTree in python 3.9 (#1146, #1157, #1162, # 1182, #1210) (Ondřej Budai)\n* Py3: Fixed compatibility issues due to the removal of encodestring() in python 3.9 (#1161, #1174) (Kentaro Kaneki)\n* Fixed a crash when the AWS_ACCESS_KEY env var is set but not AWS_SECRET_KEY (#1201)\n* Cleanup of check_md5 (Riccardo Magliocchetti)\n* Removed legacy code for dreamhost that should be necessary anymore\n* Migrated CI tests to use github actions (Arnaud Jaffre)\n* Improved README with a link to INSTALL.md (Sia Karamalegos)\n* Improved help content (Dmitrii Korostelev, Roland Van Laar)\n* Improvements for setup and build configurations\n* Many other bug fixes\n\n\ns3cmd-2.1.0      - 2020-04-07\n===============\n* Changed size reporting using k instead of K as it a multiple of 1024 (#956)\n* Added \"public_url_use_https\" config to generate public url using https (#551, #666) (Jukka Nousiainen)\n* Added option to make connection pooling configurable and improvements (Arto Jantunen)\n* Added support for path-style bucket access to signurl (Zac Medico)\n* Added docker configuration and help to run test cases with multiple Python versions (Doug Crozier)\n* Relaxed limitation on special chars for --add-header key names (#1054)\n* Fixed all regions that were automatically converted to lower case (Harshavardhana)\n* Fixed size and alignment of DU and LS output reporting (#956)\n* Fixes for SignatureDoesNotMatch error when host port 80 or 443 is specified, due to stupid servers (#1059)\n* Fixed the useless retries of requests that fail because of ssl cert checks\n* Fixed a possible crash when a file disappears during cache generation (#377)\n* Fixed unicode issues with IAM (#987)\n* Fixed unicode errors with bucked Policy/CORS requests (#847) (Alex Offshore)\n* Fixed unicode issues when loading aws_credential_file (#989)\n* Fixed an issue with the tenant feature of CephRGW. Url encode bucket_name for path-style requests (#1080)\n* Fixed signature v2 always used when bucket_name had special chars (#1081)\n* Allow to use signature v4 only, even for commands without buckets specified (#1082)\n* Fixed small open file descriptor leaks.\n* Py3: Fixed hash-bang in headers to not force using python2 when setup/s3cmd/run-test scripts are executed directly.\n* Py3: Fixed unicode issues with Cloudfront (#1006)\n* Py3: Fixed http.client.RemoteDisconnected errors (#1014) (Ryan Huddleston)\n* Py3: Fixed 'dictionary changed size during iteration' error when using a cache-file (#945) (Doug Crozier)\n* Py3: Fixed the display of file sizes (Vlad Presnyak)\n* Py3: Python 3.8 compatibility fixes (Konstantin Shalygin)\n* Py2: Fixed unicode errors sometimes crashing remote2remote sync (#847)\n* Added s3cmd.egg-info to .gitignore (Philip Dubé)\n* Improved run-test script to not use hard-coded bucket names(#1066) (Doug Crozier)\n* Renamed INSTALL to INSTALL.md and improvements (Nitro, Prabhakar Gupta)\n* Improved the restore command help (Hrchu)\n* Updated the storage-class command help with the recent aws s3 classes (#1020)\n* Fixed typo in the --continue-put help message (Pengyu Chen)\n* Fixed typo (#1062) (Tim Gates)\n* Improvements for setup and build configurations\n* Many other bug fixes\n\n\ns3cmd-2.0.2      - 2018-07-15\n===============\n* Fixed unexpected timeouts encountered during requests or transfers due to AWS strange connection short timeouts (#941)\n* Fixed a throttle issue slowing down too much transfers in some cases (#913)\n* Added support for $AWS_PROFILE (#966) (Taras Postument)\n* Added clarification comment for the socket_timeout configuration value OS limit\n* Avoid distutils usage at runtime (Matthias Klose)\n* Python 2 compatibility: Fixed import error of which with fallback code (Gianfranco Costamagna)\n* Fixed Python 3 bytes string encoding when getting IAM credentials (Alexander Allakhverdiyev)\n* Fixed handling of config tri-state bool values (like acl_public) (Brian C. Lane)\n* Fixed V2 signature when restore command is used (Jan Kasiak)\n* Fixed setting full_control on objects with public read access (Matthew Vernon)\n* Fixed a bug when only one path is supplied with Cloudfront. (Mikael Svensson)\n* Fixed signature errors with 'modify' requests (Radek Simko)\n* Fixed #936 - Fix setacl command exception (Robert Moucha)\n* Fixed error reporting if deleting a source object failed after a move (#929)\n* Many other bug fixes (#525, #933, #940, #947, #957, #958, #960, #967)\n\nImportant info: AWS S3 doesn't allow anymore uppercases and underscores in bucket names since march 1, 2018\n\n\ns3cmd-2.0.1      - 2017-10-21\n===============\n* Support for Python 3 is now stable\n* Fixed signature issues due to upper cases in hostname (#920)\n* Improved support for Minio Azure gateway (Julien Maitrehenry, Harshavardhana)\n* Added signurl_use_https option to use https prefix for signurl (Julien Recurt)\n* Fixed a lot of remaining issues and regressions for Python 3 (#922, #921, #908)\n* Fixed --configure option with Python 3\n* Fixed non string cmdline parameters being ignored\n* Windows support fixes (#922)\n* Don't force anymore to have a / on last parameter for the \"modify\" command (#886)\n* Removed the python3 support warning\n* Detect and report error 403 in getpolicy for info command (#894)\n* Added a specific error message when getting policy by non owner (#885)\n* Many other bug fixes (#905, #892, #890, #888, #889, #887)\n\n\ns3cmd-2.0.0      - 2017-06-26\n===============\n* Added support for Python 3 (Shaform, Florent Viard)\n* Added getlifecycle command (Daniel Gryniewicz)\n* Added --cf-inval for invalidating multiple CF distributions (Joe Mifsud)\n* Added --limit to \"ls\" and \"la\" commands to return the specified number of objects (Masashi Ozawa)\n* Added --token-refresh and --no-token-refresh and get the access token from the environment (Marco Jakob)\n* Added --restore-priority and --restore-days for S3 Glacier (Robert Palmer)\n* Fixed requester pays header with HEAD requests  (Christian Rodriguez)\n* Don't allow mv/cp of multiple files to single file (Guy Gur-Ari)\n* Generalize wildcard certificate forgiveness (Mark Titorenko)\n* Multiple fixes for SSL connections and proxies\n* Added support for HTTP 100-CONTINUE\n* Fixes for s3-like servers\n* Big cleanup and many unicode fixes\n* Many other bug fixes\n\n\ns3cmd-1.6.1      - 2016-01-20\n===============\n* Added --host and --host-bucket\n* Added --stats\n* Fix for newer python 2.7.x SSL library updates\n* Many other bug fixes\n\n\ns3cmd-1.6.0      - 2015-09-18\n===============\n* Support signed URL content disposition type\n* Added 'ls -l' long listing including storage class\n* Added --limit-rate=RATE\n* Added --server-side-encryption-kms-id=KEY_ID\n* Added --storage-class=CLASS\n* Added --requester-pays, [payer] command\n* Added --[no-]check-hostname\n* Added --stop-on-error, removed --ignore-failed-copy\n* Added [setcors], [delcors] commands\n* Added support for cn-north-1 region hostname checks\n* Output strings may have changed.  Scripts calling s3cmd expecting\n  specific text may need to be updated.\n* HTTPS is now the default\n* Many unicode fixes\n* Many other bug fixes\n\ns3cmd-1.5.2      - 2015-02-08\n===============\n* Handle unvalidated SSL certificate.  Necessary on Ubuntu 14.04 for\n  SSL to function at all.\n* packaging fixes (require python-magic, drop ez_setup)\n\ns3cmd-1.5.1.2      - 2015-02-04\n===============\n* fix PyPi install\n\ns3cmd-1.5.1        - 2015-02-04\n===============\n\n* Sort s3cmd ls output by bucket name (Andrew Gaul)\n* Support relative expiry times in signurl. (Chris Lamb)\n* Fixed issue with mixed path separators with s3cmd get --recursive on\n  Windows. (Luke Winslow)\n* fix S3 wildcard certificate checking\n* Handle headers with spaces in their values properly (#460)\n* Fix lack of SSL certificate checking libraries on older python\n* set content-type header for stdin from command line or Config()\n* fix uploads from stdin (#464)\n* Fix directory exclusions (#467)\n* fix signurl\n* Don't retry in response to HTTP 405 error (#422)\n* Don't crash when a proxy returns an invalid XML error document\n\ns3cmd-1.5.0        - 2015-01-12\n===============\n* add support for newer regions such as Frankfurt that\n  require newer authorization signature v4 support\n  (Vasileios Mitrousis, Michal Ludvig, Matt Domsch)\n* drop support for python 2.4 due to signature v4 code.\n  python 2.6 is now the minimum, and python 3 is still not supported.\n* handle redirects to the \"right\" region for a bucket.\n* add --ca-cert=FILE for self-signed certs (Matt Domsch)\n* allow proxied SSL connections with python >= 2.7 (Damian Gerow)\n* add --remove-headers for [modify] command (Matt Domsch)\n* add -s/--ssl and --no-ssl options (Viktor Szakáts)\n* add --signature-v2 for backwards compatibility with S3 clones.\n* bugfixes by 17 contributors\n\ns3cmd 1.5.0-rc1      -   2014-06-29\n===============\n* add environment variable S3CMD_CONFIG (Devon Jones),\n  access key and secre keys (Vasileios Mitrousis)\n* added modify command (Francois Gaudin)\n* better debug messages (Matt Domsch)\n* faster batch deletes (Matt Domsch)\n* Added support for restoring files from Glacier storage (Robert Palmer)\n* Add and remove full lifecycle policies (Sam Rudge)\n* Add support for object expiration (hrchu)\n* bugfixes by 26 contributors\n\n\ns3cmd 1.5.0-beta1    -   2013-12-02\n=================\n* Brougt to you by Matt Domsch and contributors, thanks guys! :)\n* Multipart upload improvements (Eugene Brevdo, UENISHI Kota)\n* Allow --acl-grant on AWS groups (Dale Lovelace)\n* Added Server-Side Encryption support (Kevin Daub)\n* Improved MIME types detections and content encoding (radomir,\n  Eric Drechsel, George Melika)\n* Various smaller changes and bugfixes from many contributors\n\ns3cmd 1.5.0-alpha3   -   2013-03-11\n==================\n* Persistent HTTP/HTTPS connections for massive speedup (Michal Ludvig)\n* New switch --quiet for suppressing all output (Siddarth Prakash)\n* Honour \"umask\" on file downloads (Jason Dalton)\n* Various bugfixes from many contributors\n\ns3cmd 1.5.0-alpha2   -   2013-03-04\n==================\n* IAM roles support (David Kohen, Eric Dowd)\n* Manage bucket policies (Kota Uenishi)\n* Various bugfixes from many contributors\n\ns3cmd 1.5.0-alpha1   -   2013-02-19\n==================\n* Server-side copy for hardlinks/softlinks to improve performance\n  (Matt Domsch)\n* New [signurl] command (Craig Ringer)\n* Improved symlink-loop detection (Michal Ludvig)\n* Add --delete-after option for sync (Matt Domsch)\n* Handle empty return bodies when processing S3 errors.\n  (Kelly McLaughlin)\n* Upload from STDIN (Eric Connell)\n* Updated bucket locations (Stefhen Hovland)\n* Support custom HTTP headers (Brendan O'Connor, Karl Matthias)\n* Improved MIME support (Karsten Sperling, Christopher Noyes)\n* Added support for --acl-grant/--acl-revoke to 'sync' command\n  (Michael Tyson)\n* CloudFront: Support default index and default root invalidation\n  (Josep del Rio)\n* Command line options for access/secret keys (Matt Sweeney)\n* Support [setpolicy] for setting bucket policies (Joe Fiorini)\n* Respect the $TZ environment variable (James Brown)\n* Reduce memory consumption for [s3cmd du] (Charlie Schluting)\n* Rate limit progress updates (Steven Noonan)\n* Download from S3 to a temp file first (Sumit Kumar)\n* Reuse a single connection when doing a bucket list (Kelly McLaughlin)\n* Delete empty files if object_get() failed (Oren Held)\n\ns3cmd 1.1.0   -   (never released)\n===========\n* MultiPart upload enabled for both [put] and [sync]. Default chunk\n  size is 15MB.\n* CloudFront invalidation via [sync --cf-invalidate] and [cfinvalinfo].\n* Increased socket_timeout from 10 secs to 5 mins.\n* Added \"Static WebSite\" support [ws-create / ws-delete / ws-info]\n  (contributed by Jens Braeuer)\n* Force MIME type with --mime-type=abc/xyz, also --guess-mime-type\n  is now on by default, -M is no longer shorthand for --guess-mime-type\n* Allow parameters in MIME types, for example:\n  --mime-type=\"text/plain; charset=utf-8\"\n* MIME type can be guessed by python-magic which is a lot better than\n  relying on the extension. Contributed by Karsten Sperling.\n* Support for environment variables as config values. For instance\n  in ~/.s3cmd put \"access_key=$S3_ACCESS_KEY\". Contributed by Ori Bar.\n* Support for --configure checking access to a specific bucket instead\n  of listing all buckets.\n  Listing buckets requires the S3 ListAllMyBuckets permission which\n  is typically not available to delegated IAM accounts. With this change,\n  s3cmd --configure accepts an (optional) bucket uri as a parameter\n  and if it's provided, the access check will just verify access to this\n  bucket individually. Contributed by Mike Repass.\n* Allow STDOUT as a destination even for downloading multiple files.\n  They will be output one after another without any delimiters!\n  Contributed by Rob Wills.\n\ns3cmd 1.0.0   -   2011-01-18\n===========\n* [sync] now supports --no-check-md5\n* Network connections now have 10s timeout\n* [sync] now supports bucket-to-bucket synchronisation\n* Added [accesslog] command.\n* Added access logging for CloudFront distributions\n  using [cfmodify --log]\n* Added --acl-grant and --acl-revoke [Timothee Groleau]\n* Allow s3:// URI as well as cf:// URI as a distribution\n  name for most CloudFront related commands.\n* Support for Reduced Redundancy Storage (--reduced-redundancy)\n* Follow symlinks in [put] and [sync] with --follow-symlinks\n* Support for CloudFront DefaultRootObject [Luke Andrew]\n\ns3cmd 0.9.9.91 -  2009-10-08\n==============\n* Fixed invalid reference to a variable in failed upload handling.\n\ns3cmd 0.9.9.90 -  2009-10-06\n==============\n* New command 'sign' for signing e.g. POST upload policies.\n* Fixed handling of filenames that differ only in\n  capitalisation (eg blah.txt vs Blah.TXT).\n* Added --verbatim mode, preventing most filenames\n  pre-processing. Good for fixing unreadable buckets.\n* Added --recursive support for [cp] and [mv], including\n  multiple-source arguments, --include/--exclude, --dry-run, etc.\n* Added --exclude/--include and --dry-run for [del], [setacl].\n* Neutralise characters that are invalid in XML to avoid ExpatErrors.\n  http://boodebr.org/main/python/all-about-python-and-unicode\n* New command [fixbucket] for for fixing invalid object names\n  in a given Bucket. For instance names with &#x08; in them\n  (not sure how people manage to upload them but they do).\n\ns3cmd 0.9.9   -   2009-02-17\n===========\nNew commands:\n* Commands for copying and moving objects, within or\n  between buckets: [cp] and [mv] (Andrew Ryan)\n* CloudFront support through [cfcreate], [cfdelete],\n  [cfmodify] and [cfinfo] commands. (sponsored by Joseph Denne)\n* New command [setacl] for setting ACL on existing objects,\n  use together with --acl-public/--acl-private (sponsored by\n  Joseph Denne)\n\nOther major features:\n* Improved source dirname handling for [put], [get] and [sync].\n* Recursive and wildcard support for [put], [get] and [del].\n* Support for non-recursive [ls].\n* Enabled --dry-run for [put], [get] and [sync].\n* Allowed removal of non-empty buckets with [rb --force].\n* Implemented progress meter (--progress / --no-progress)\n* Added --include / --rinclude / --(r)include-from\n  options to override --exclude exclusions.\n* Added --add-header option for [put], [sync], [cp] and [mv].\n  Good for setting e.g. Expires or Cache-control headers.\n* Added --list-md5 option for [ls].\n* Continue [get] partially downloaded files with --continue\n* New option --skip-existing for [get] and [sync].\n\nMinor features and bugfixes:\n* Fixed GPG (--encrypt) compatibility with Python 2.6.\n* Always send Content-Length header to satisfy some http proxies.\n* Fixed installation on Windows and Mac OS X.\n* Don't print nasty backtrace on KeyboardInterrupt.\n* Should work fine on non-UTF8 systems, provided all\n  the files are in current system encoding.\n* System encoding can be overridden using --encoding.\n* Improved resistance to communication errors (Connection\n  reset by peer, etc.)\n\ns3cmd 0.9.8.4 -   2008-11-07\n=============\n* Stabilisation / bugfix release:\n* Restored access to upper-case named buckets.\n* Improved handling of filenames with Unicode characters.\n* Avoid ZeroDivisionError on ultrafast links (for instance\n  on Amazon EC2)\n* Re-issue failed requests (e.g. connection errors, internal\n  server errors, etc).\n* Sync skips over files that can't be open instead of\n  terminating the sync completely.\n* Doesn't run out of open files quota on sync with lots of\n  files.\n\ns3cmd 0.9.8.3 -   2008-07-29\n=============\n* Bugfix release. Avoid running out-of-memory in MD5'ing\n  large files.\n\ns3cmd 0.9.8.2 -   2008-06-27\n=============\n* Bugfix release. Re-upload file if Amazon doesn't send ETag\n  back.\n\ns3cmd 0.9.8.1 -   2008-06-27\n=============\n* Bugfix release. Fixed 'mb' and 'rb' commands again.\n\ns3cmd 0.9.8   -   2008-06-23\n===========\n* Added --exclude / --rexclude options for sync command.\n* Doesn't require $HOME env variable to be set anymore.\n* Better checking of bucket names to Amazon S3 rules.\n\ns3cmd 0.9.7   -   2008-06-05\n===========\n* Implemented 'sync' from S3 back to local folder, including\n  file attribute restoration.\n* Failed uploads are retried on lower speed to improve error\n  resilience.\n* Compare MD5 of the uploaded file, compare with checksum\n  reported by S3 and re-upload on mismatch.\n\ns3cmd 0.9.6   -   2008-02-28\n===========\n* Support for setting / guessing MIME-type of uploaded file\n* Correctly follow redirects when accessing buckets created\n  in Europe.\n* Introduced 'info' command both for buckets and objects\n* Correctly display public URL on uploads\n* Updated TODO list for everyone to see where we're heading\n* Various small fixes. See ChangeLog for details.\n\ns3cmd 0.9.5   -   2007-11-13\n===========\n* Support for buckets created in Europe\n* Initial 'sync' support, for now local to s3 direction only\n* Much better handling of multiple args to put, get and del\n* Tries to use ElementTree from any available module\n* Support for buckets with over 1000 objects.\n\ns3cmd 0.9.4   -   2007-08-13\n===========\n* Support for transparent GPG encryption of uploaded files\n* HTTP proxy support\n* HTTPS protocol support\n* Support for non-ASCII characters in uploaded filenames\n\ns3cmd 0.9.3   -   2007-05-26\n===========\n* New command \"du\" for displaying size of your data in S3.\n  (Basil Shubin)\n\ns3cmd 0.9.2   -   2007-04-09\n===========\n* Lots of new documentation\n* Allow \"get\" to stdout (use \"-\" in place of destination file\n  to get the file contents on stdout)\n* Better compatibility with Python 2.4\n* Output public HTTP URL for objects stored with Public ACL\n* Various bugfixes and improvements\n\ns3cmd 0.9.1   -   2007-02-06\n===========\n* All commands now use S3-URIs\n* Removed hard dependency on Python 2.5\n* Experimental support for Python 2.4\n  (requires external ElementTree module)\n\n\ns3cmd 0.9.0   -   2007-01-18\n===========\n* First public release brings support for all basic Amazon S3\n  operations: Creation and Removal of buckets, Upload (put),\n  Download (get) and Removal (del) of files/objects.\n\n"
        },
        {
          "name": "ObsoleteChangeLog",
          "type": "blob",
          "size": 42.03125,
          "content": "2011-06-06  Michal Ludvig  <mludvig@logix.net.nz>\n\n===== Migrated to GIT =====\n\nNo longer keeping ChangeLog up to date, use git log instead!\n\n* git://github.com/s3tools/s3cmd.git\n\n2011-04-11  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/S3Uri.py: Fixed cf:// uri parsing.\n\t* S3/CloudFront.py: Don't fail if there are no cfinval\n\t  requests.\n\n2011-04-11  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/PkgInfo.py: Updated to 1.1.0-beta1\n\t* NEWS: Updated.\n\t* s3cmd.1: Regenerated.\n\n2011-04-11  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/Config.py: Increase socket_timeout from 10 secs to 5 mins.\n\n2011-04-10  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/CloudFront.py, S3/S3Uri.py: Support for checking \n\t  status of CF Invalidation Requests [cfinvalinfo].\n\t* s3cmd, S3/CloudFront.py, S3/Config.py: Support for CloudFront\n\t  invalidation using [sync --cf-invalidate] command.\n\t* S3/Utils.py: getDictFromTree() now recurses into\n\t  sub-trees.\n\n2011-03-30  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/CloudFront.py: Fix warning with Python 2.7\n\t* S3/CloudFront.py: Cmd._get_dist_name_for_bucket() moved to\n\t  CloudFront class.\n\n2011-01-13  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/FileLists.py: Move file/object listing functions\n\t  to S3/FileLists.py\n\n2011-01-09  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* Released version 1.0.0\n\t  ----------------------\n\n\t* S3/PkgInfo.py: Updated to 1.0.0\n\t* NEWS: Updated.\n\n2011-01-02  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd: Improved r457 (Don't crash when file disappears\n\t  before checking MD5).\n\t* s3cmd, s3cmd.1, format-manpage.pl: Improved --help text\n\t  and manpage.\n\t* s3cmd: Removed explicit processing of --follow-symlinks\n\t  (is caught by the default / main loop).\n\n2010-12-24  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd: Set 10s socket timeout for read()/write().\n\t* s3cmd: Added --(no-)check-md5 for [sync].\n\t* run-tests.py, testsuite.tar.gz: Added testsuite for\n\t  the above.\n\t* NEWS: Document the above.\n\t* s3cmd: Don't crash when file disappears before\n\t  checking MD5.\n\n2010-12-09  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* Released version 1.0.0-rc2\n\t  --------------------------\n\n\t* S3/PkgInfo.py: Updated to 1.0.0-rc2\n\t* NEWS, TODO, s3cmd.1: Updated.\n\n2010-11-13  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd: Added support for remote-to-remote sync.\n\t  (Based on patch from Sundar Raman - thanks!)\n\t* run-tests.py: Testsuite for the above.\n\n2010-11-12  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd: Fixed typo in \"s3cmd du\" error path.\n\n2010-11-12  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* format-manpage.pl: new manpage auto-formatter\n\t* s3cmd.1: Updated using the above helper script\n\t* setup.py: Warn if manpage is too old.\n\n2010-10-27  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* run-tests.py, testsuite.tar.gz: Keep the testsuite in\n\t  SVN as a tarball. There's too many \"strange\" things \n\t  in the directory for it to be kept in SVN.\n\n2010-10-27  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* TODO: Updated.\n\t* upload-to-sf.sh: Updated for new SF.net system\n\n2010-10-26  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* Released version 1.0.0-rc1\n\t  --------------------------\n\n\t* S3/PkgInfo.py: Updated to 1.0.0-rc1\n\t* NEWS, TODO: Updated.\n\n2010-10-26  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/CloudFront.py, S3/Config.py: Added support\n\t  for CloudFront DefaultRootObject. Thanks to Luke Andrew.\n\n2010-10-25  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd: Improved 'fixbucket' command. Thanks to Srinivasa\n\t  Moorthy.\n\t* s3cmd: Read config file even if User Profile directory on \n\t  Windows contains non-ascii symbols. Thx Slava Vishnyakov\n\n2010-10-25  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd: Don't fail when a local node is a directory\n\t  and we expected a file. (as if for example /etc/passwd \n\t  was a dir)\n\n2010-10-25  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/S3.py: Ignore inaccessible (and missing) files\n\t  on upload.\n\t* run-tests.py: Extended [sync] test to verify correct\n\t  handling of inaccessible files.\n\t* testsuite/permission-tests: New testsuite files.\n\n2010-10-24  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/S3.py: \"Stringify\" all headers. Httplib should do\n\t  it but some Python 2.7 users reported problems that should\n\t  now be fixed.\n\t* run-tests.py: Fixed test #6\n\n2010-07-25  Aaron Maxwell  <amax@resymbol.net>\n\n\t* S3/Config.py, testsuite/etc/, run-tests.py, s3cmd.1, s3cmd:\n\t  Option to follow local symlinks for sync and \n\t  put (--follow-symlinks option), including tests and documentation\n\t* run-tests.py: --bucket-prefix option, to allow different \n\t  developers to run tests in their own sandbox\n\n2010-07-08  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* run-tests.py, testsuite/crappy-file-name.tar.gz:\n\t  Updated testsuite, work around a problem with [s3cmd cp]\n\t  when the source file contains '?' or '\\x7f' \n\t  (where the inability to copy '?' is especially annoying).\n\n2010-07-08  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/Utils.py, S3/S3Uri.py: Fixed names after moving \n\t  functions between modules.\n\n2010-06-29  Timothee Groleau <kde@timotheegroleau.com>\n\n\t* S3/ACL.py: Fix isAnonRead method on Grantees\n\t* ChangeLog: Update name of contributor for Timothee Groleau\n\n2010-06-13  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/CloudFront.py: Both [accesslog] and [cfmodify] \n\t  access logging can now be disabled with --no-access-logging\n\n2010-06-13  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/CloudFront.py: Allow s3:// URI as well as cf:// URI \n\t  for most CloudFront-related commands.\n\n2010-06-12  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/CloudFront.py, S3/Config.py: Support access \n\t  logging for CloudFront distributions.\n\t* S3/S3.py, S3/Utils.py: Moved some functions to Utils.py\n\t  to make them available to CloudFront.py\n\t* NEWS: Document the above.\n\n2010-05-27  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/S3.py: Fix bucket listing for buckets with\n\t  over 1000 prefixes. (contributed by Timothee Groleau)\n\t* S3/S3.py: Fixed code formatting.\n\n2010-05-21  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/S3.py: Added support for bucket locations\n\t  outside US/EU (i.e. us-west-1 and ap-southeast-1 as of now).\n\n2010-05-21  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/S3.py, S3/Config.py: Added --reduced-redundancy\n\t  switch for Reduced Redundancy Storage.\n\n2010-05-20  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/ACL.py, S3/Config.py: Support for --acl-grant\n\t  and --acl-revoke (contributed by Timothee Groleau)\n\t* s3cmd: Couple of fixes on top of the above commit.\n\t* s3cmd: Pre-parse ACL parameters in OptionS3ACL()\n\n2010-05-20  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/Exceptions.py, S3/S3.py: Some HTTP_400 exceptions \n\t  are retriable.\n\n2010-03-19  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd, S3/ACL.py: Print all ACLs for a Grantee\n\t(one Grantee can have multiple different Grant entries)\n\n2010-03-19  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd: Enable bucket-level ACL setting\n\t* s3cmd, S3/AccessLog.py, ...: Added [accesslog] command.\n\t* s3cmd: Fix imports from S3.Utils\n\n2009-12-10  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* s3cmd: Path separator conversion on Windows hosts.\n\n2009-10-08  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* Released version 0.9.9.91\n\t  -------------------------\n\n\t* S3/PkgInfo.py: Updated to 0.9.9.91\n\t* NEWS: News for 0.9.9.91\n\n2009-10-08  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/S3.py: fixed reference to _max_retries.\n\n2009-10-06  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* Released version 0.9.9.90\n\t  -------------------------\n\n\t* S3/PkgInfo.py: Updated to 0.9.9.90\n\t* NEWS: News for 0.9.9.90\n\n2009-10-06  Michal Ludvig  <mludvig@logix.net.nz>\n\n\t* S3/S3.py: Introduce throttling on upload only after\n\t  second failure. I.e. first retry at full speed.\n\t* TODO: Updated with new ideas.\n\n2009-06-02  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: New [fixbucket] command for fixing invalid object\n\t  names in a given Bucket. For instance names with &#x08; in\n\t  them (not sure how people manage to upload them but they do).\n\t* S3/S3.py, S3/Utils.py, S3/Config.py: Support methods for \n\t  the above, plus advise user to run 'fixbucket' when XML parsing \n\t  fails.\n\t* NEWS: Updated.\n\t\n2009-05-29  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Utils.py: New function replace_nonprintables()\n\t* s3cmd: Filter local filenames through the above function\n\t  to avoid problems with uploaded filenames containing invalid \n\t  XML entities, eg &#08; \n\t* S3/S3.py: Warn if a non-printables char is passed to\n\t  urlencode_string() - they should have been replaced earlier \n\t  in the processing.\n\t* run-tests.py, TODO, NEWS: Updated.\n\t* testsuite/crappy-file-name.tar.gz: Tarball with a crappy-named\n\t  file. Untar for the testsuite.\n\n2009-05-29  Michal Ludvig  <michal@logix.cz>\n\n\t* testsuite/blahBlah/*: Added files needed for run-tests.py\n\n2009-05-28  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Utils.py (dateS3toPython): Be more relaxed about\n\t  timestamps format.\n\n2009-05-28  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, run-test.py, TODO, NEWS: Added --dry-run\n\t  and --exclude/--include for [setacl].\n\t* s3cmd, run-test.py, TODO, NEWS: Added --dry-run\n\t  and --exclude/--include for [del].\n\n2009-05-28  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Support for recursive [cp] and [mv], including\n\t  multiple-source arguments, --include/--exclude,\n\t  --dry-run, etc.\n\t* run-tests.py: Tests for the above.\n\t* S3/S3.py: Preserve metadata (eg ACL or MIME type) \n\t  during [cp] and [mv].\n\t* NEWS, TODO: Updated.\n\n2009-05-28  Michal Ludvig  <michal@logix.cz>\n\n\t* run-tests.py: Added --verbose mode.\n\n2009-05-27  Michal Ludvig  <michal@logix.cz>\n\n\t* NEWS: Added info about --verbatim.\n\t* TODO: Added more tasks.\n\n2009-05-27  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/SortedDict.py: Add case-sensitive mode.\n\t* s3cmd, S3/S3.py, S3/Config.py: Use SortedDict() in \n\t  case-sensitive mode to avoid dropping filenames\n\t  differing only in capitalisation\n\t* run-tests.py: Testsuite for the above.\n\t* NEWS: Updated.\n\n2009-03-20  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Re-sign requests before retrial to avoid \n\t  RequestTimeTooSkewed errors on failed long-running\n\t  uploads.\n\t  BTW 'request' now has its own class S3Request.\n\n2009-03-04  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/Config.py, S3/S3.py: Support for --verbatim.\n\n2009-02-25  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Fixed \"put file.ext s3://bkt\" (ie just the bucket name).\n\t* s3cmd: Fixed reporting of ImportError of S3 modules.\n\t* s3cmd: Fixed Error: global name 'real_filename' is not defined\n\n2009-02-24  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: New command [sign]\n\t* S3/Utils.py: New function sign_string()\n\t* S3/S3.py, S3/CloudFront.py: Use sign_string().\n\t* NEWS: Updated.\n\n2009-02-17  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.9\n\t  ----------------------\n\n\t* S3/PkgInfo.py: Updated to 0.9.9\n\t* NEWS: Compile a big news list for 0.9.9\n\n2009-02-17  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd.1: Document all the new options and commands.\n\t* s3cmd, S3/Config.py: Updated some help texts. Removed\n\t  option --debug-syncmatch along the way (because --dry-run\n\t  with --debug is good enough).\n\t* TODO: Updated.\n\n2009-02-16  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Check Python version >= 2.4 as soon as possible.\n\n2009-02-14  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/Config.py, S3/S3.py: Added --add-header option.\n\t* NEWS: Documented --add-header.\n\t* run-tests.py: Fixed for new messages.\n\n2009-02-14  Michal Ludvig  <michal@logix.cz>\n\n\t* README: Updated for 0.9.9\n\t* s3cmd, S3/PkgInfo.py, s3cmd.1: Replaced project \n\t  URLs with http://s3tools.org\n\t* NEWS: Improved message.\n\n2009-02-12  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Added --list-md5 for 'ls' command.\n\t* S3/Config.py: New setting list_md5\n\n2009-02-12  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Set Content-Length header for requests with 'body'.\n\t* s3cmd: And send it for requests with no body as well...\n\n2009-02-02  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.9-rc3\n\t  --------------------------\n\n\t* S3/PkgInfo.py, NEWS: Updated for 0.9.9-rc3\n\n2009-02-01  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Exceptions.py: Correct S3Exception.__str__() to\n\t  avoid crash in S3Error() subclass. Reported by '~t2~'.\n\t* NEWS: Updated.\n\n2009-01-30  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.9-rc2\n\t  --------------------------\n\n\t* S3/PkgInfo.py, NEWS, TODO: Updated for 0.9.9-rc2\n\n2009-01-30  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Under some circumstance s3cmd crashed\n\t  when put/get/sync had 0 files to transmit. Fixed now.\n\n2009-01-28  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Output 'delete:' in --dry-run only when\n\t  used together with --delete-removed. Otherwise\n\t  the user will think that without --dry-run it\n\t  would really delete the files.\n\n2009-01-27  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.9-rc1\n\t  --------------------------\n\n\t* S3/PkgInfo.py, NEWS, TODO: Updated for 0.9.9-rc1\n\n2009-01-26  Michal Ludvig  <michal@logix.cz>\n\n\t* Merged CloudFront support from branches/s3cmd-airlock\n\t  See the ChangeLog in that branch for details.\n\n2009-01-25  W. Tell  <w_tell -at- sourceforge>\n\n\t* s3cmd: Implemented --include and friends.\n\n2009-01-25  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Enabled --dry-run and --exclude for 'put' and 'get'.\n\t* S3/Exceptions.py: Remove DeprecationWarning about \n\t  BaseException.message in Python 2.6\n\t* s3cmd: Rewritten gpg_command() to use subprocess.Popen()\n\t  instead of os.popen4() deprecated in 2.6\n\t* TODO: Note about failing GPG.\n\n2009-01-22  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Config.py: guess_mime_type = True (will affect new \n\t  installations only).\n\n2009-01-22  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.9-pre5\n\t  ---------------------------\n\n\t* S3/PkgInfo.py, NEWS, TODO: Updated for 0.9.9-pre5\n\n2009-01-22  Michal Ludvig  <michal@logix.cz>\n\n\t* run-tests.py: Updated paths for the new sync\n\t  semantics.\n\t* s3cmd, S3/S3.py: Small fixes to make testsuite happy.\n\n2009-01-21  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Migrated 'sync' local->remote to the new\n\t  scheme with fetch_{local,remote}_list().\n\t  Enabled --dry-run for 'sync'.\n\n2009-01-20  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Migrated 'sync' remote->local to the new\n\t  scheme with fetch_{local,remote}_list().\n\t  Changed fetch_remote_list() to return dict() compatible\n\t  with fetch_local_list().\n\t  Re-implemented --exclude / --include processing.\n\t* S3/Utils.py: functions for parsing RFC822 dates (for HTTP\n\t  header responses).\n\t* S3/Config.py: placeholders for --include.\n\n2009-01-15  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3Uri.py, NEWS: Support for recursive 'put'.\n\n2009-01-13  Michal Ludvig  <michal@logix.cz>\n\n\t* TODO: Updated.\n\t* s3cmd: renamed (fetch_)remote_keys to remote_list and\n\t  a few other renames for consistency.\n\n2009-01-08  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Some errors during file upload were incorrectly \n\t  interpreted as MD5 mismatch. (bug #2384990)\n\t* S3/ACL.py: Move attributes from class to instance.\n\t* run-tests.py: Tests for ACL.\n\t* s3cmd: Minor messages changes.\n\n2009-01-07  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: New command 'setacl'.\n\t* S3/S3.py: Implemented set_acl().\n\t* S3/ACL.py: Fill in <Owner/> tag in ACL XML.\n\t* NEWS: Info about 'setacl'.\n\n2009-01-07  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Factored remote_keys generation from cmd_object_get()\n\t  to fetch_remote_keys().\n\t* s3cmd: Display Public URL in 'info' for AnonRead objects.\n\t* S3/ACL.py: Generate XML from a current list of Grantees\n\n2009-01-07  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/ACL.py: Keep ACL internally as a list of of 'Grantee' objects.\n\t* S3/Utils.py: Fix crash in stripNameSpace() when the XML has no NS.\n\n2009-01-07  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/ACL.py: New object for handling ACL issues.\n\t* S3/S3.py: Moved most of S3.get_acl() to ACL class.\n\t* S3/Utils.py: Reworked XML helpers - remove XMLNS before \n\t  parsing the input XML to avoid having all Tags prefixed\n\t  with {XMLNS} by ElementTree.\n\n2009-01-03  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Don't fail when neither $HOME nor %USERPROFILE% is set.\n\t  (fixes #2483388)\n\n2009-01-01  W. Tell  <w_tell -at- sourceforge>\n\n\t* S3/S3.py, S3/Utils.py: Use 'hashlib' instead of md5 and sha \n\t  modules to avoid Python 2.6 warnings.\n\n2008-12-31  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.9-pre4\n\t  ---------------------------\n\n2008-12-31  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Reworked internal handling of unicode vs encoded filenames.\n\t  Should replace unknown characters with '?' instead of baling out.\n\n2008-12-31  Michal Ludvig  <michal@logix.cz>\n\n\t* run-tests.py: Display system encoding in use.\n\t* s3cmd: Print a nice error message when --exclude-from\n\t  file is not readable.\n\t* S3/PkgInfo.py: Bumped up version to 0.9.9-pre4\n\t* S3/Exceptions.py: Added missing imports.\n\t* NEWS: Updated.\n\t* testsuite: reorganised UTF-8 files, added GBK encoding files,\n\t  moved encoding-specific files to 'tar.gz' archives, removed \n\t  unicode dir.\n\t* run-tests.py: Adapted to the above change.\n\t* run-tests.sh: removed.\n\t* testsuite/exclude.encodings: Added.\n\t* run-tests.py: Don't assume utf-8, use preferred encoding \n\t  instead.\n\t* s3cmd, S3/Utils.py, S3/Exceptions.py, S3/Progress.py,\n\t  S3/Config.py, S3/S3.py: Added --encoding switch and \n\t  Config.encoding variable. Don't assume utf-8 for filesystem\n\t  and terminal output anymore.\n\t* s3cmd: Avoid ZeroDivisionError on fast links.\n\t* s3cmd: Unicodised all info() output.\n\n2008-12-30  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Replace unknown Unicode characters with '?'\n\t  to avoid UnicodeEncodeError's. Also make all output strings\n\t  unicode.\n\t* run-tests.py: Exit on failed test. Fixed order of tests.\n\n2008-12-29  Michal Ludvig  <michal@logix.cz>\n\n\t* TODO, NEWS: Updated\n\t* s3cmd: Improved wildcard get.\n\t* run-tests.py: Improved testsuite, added parameters support\n\t  to run only specified tests, cleaned up win/posix integration.\n\t* S3/Exception.py: Python 2.4 doesn't automatically set \n\t  Exception.message.\n\n2008-12-29  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, run-tests.py: Make it work on Windows.\n\n2008-12-26  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.cfg: Remove explicit install prefix. That should fix\n\t  Mac OS X and Windows \"setup.py install\" runs.\n\n2008-12-22  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3.py, S3/Progress.py: Display \"[X of Y]\"\n\t  in --progress mode.\n\t* s3cmd, S3/Config.py: Implemented recursive [get].\n\t  Added --skip-existing option for [get] and [sync]. \n\n2008-12-17  Michal Ludvig  <michal@logix.cz>\n\n\t* TODO: Updated\n\n2008-12-14  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Progress.py: Restructured import Utils to avoid import\n\t  conflicts.\n\n2008-12-12  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Better Exception output. Print sys.path on ImportError,\n\t  don't print backtrace on KeyboardInterrupt\n\n2008-12-11  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Support for multiple sources in 'get' command.\n\n2008-12-10  Michal Ludvig  <michal@logix.cz>\n\n\t* TODO: Updated list.\n\t* s3cmd: Don't display download/upload completed message\n\t  in --progress mode.\n\t* S3/S3.py: Pass src/dst names down to Progress class.\n\t* S3/Progress.py: added new class ProgressCR - apparently \n\t  ProgressANSI doesn't work on MacOS-X (and perhaps elsewhere).\n\t* S3/Config.py: Default progress meter is now ProgressCR\n\t* s3cmd: Updated email address for reporting bugs.\n\n2008-12-02  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3.py, NEWS: Support for (non-)recursive 'ls'\n\n2008-12-01  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.9-pre3\n\t  ---------------------------\n\n\t* S3/PkgInfo.py: Bumped up version to 0.9.9-pre3\n\n2008-12-01  Michal Ludvig  <michal@logix.cz>\n\n\t* run-tests.py: Added a lot of new tests.\n\t* testsuite/etc/logo.png: New file.\n\n2008-11-30  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: object_get() -- make start_position argument optional.\n\n2008-11-29  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Delete local files with \"sync --delete-removed\"\n\n2008-11-25  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/Progress.py: Fixed Unicode output in Progress meter.\n\t* s3cmd: Fixed 'del --recursive' without prefix (i.e. all objects).\n\t* TODO: Updated list.\n\t* upload-to-sf.sh: Helper script.\n\t* S3/PkgInfo.py: Bumped up version to 0.9.9-pre2+svn\n\n2008-11-24  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.9-pre2\n\t  ------------------------\n\n\t* S3/PkgInfo.py: Bumped up version to 0.9.9-pre2\n\t* NEWS: Added 0.9.9-pre2\n\n2008-11-24  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, s3cmd.1, S3/S3.py: Display or don't display progress meter\n\t  default depends on whether we're on TTY (console) or not.\n\n2008-11-24  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Fixed 'get' conflict.\n\t* s3cmd.1, TODO: Document 'mv' command.\n\n2008-11-24  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py, s3cmd, S3/Config.py, s3cmd.1: Added --continue for\n\t  'get' command, improved 'get' failure resiliency.\n\t* S3/Progress.py: Support for progress meter not starting in 0.\n\t* S3/S3.py: improved retrying in send_request() and send_file()\n\n2008-11-24  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3.py, NEWS: \"s3cmd mv\" for moving objects\n\n2008-11-24  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Utils.py: Common XML parser.\n\t* s3cmd, S3/Exceptions.py: Print info message on Error.\n\n2008-11-21  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Support for 'cp' command.\n\t* S3/S3.py: Added S3.object.copy() method.\n\t* s3cmd.1: Document 'cp' command.\n\t* NEWS: Let everyone know ;-)\n\tThanks Andrew Ryan for a patch proposal!\n\thttps://sourceforge.net/forum/forum.php?thread_id=2346987&forum_id=618865\n\n2008-11-17  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Progress.py: Two progress meter implementations.\n\t* S3/Config.py, s3cmd: New --progress / --no-progress parameters\n\t  and Config() members.\n\t* S3/S3.py: Call Progress() in send_file()/recv_file()\n\t* NEWS: Let everyone know ;-)\n\n2008-11-16  Michal Ludvig  <michal@logix.cz>\n\n\t* NEWS: Fetch 0.9.8.4 release news from 0.9.8.x branch.\n\n2008-11-16  Michal Ludvig  <michal@logix.cz>\n\n\tMerge from 0.9.8.x branch, rel 251:\n\t* S3/S3.py: Adjusting previous commit (orig 249) - it's not a good idea \n\t  to retry ALL failures. Especially not those code=4xx where AmazonS3 \n\t  servers are not happy with our requests.\n\tMerge from 0.9.8.x branch, rel 249:\n\t* S3/S3.py, S3/Exception.py: Re-issue failed requests in S3.send_request()\n\tMerge from 0.9.8.x branch, rel 248:\n\t* s3cmd: Don't leak open filehandles in sync. Thx Patrick Linskey for report.\n\tMerge from 0.9.8.x branch, rel 247:\n\t* s3cmd: Re-raise the right exception.\n\tMerge from 0.9.8.x branch, rel 246:\n\t* s3cmd, S3/S3.py, S3/Exceptions.py: Don't abort 'sync' or 'put' on files\n\t  that can't be open (e.g. Permission denied). Print a warning and skip over\n\t  instead.\n\tMerge from 0.9.8.x branch, rel 245:\n\t* S3/S3.py: Escape parameters in strings. Fixes sync to and \n\t  ls of directories with spaces. (Thx Lubomir Rintel from Fedora Project)\n\tMerge from 0.9.8.x branch, rel 244:\n\t* s3cmd: Unicode brainfuck again. This time force all output\n\t  in UTF-8, will see how many complaints we'll get...\n\n2008-09-16  Michal Ludvig  <michal@logix.cz>\n\n\t* NEWS: s3cmd 0.9.8.4 released from branches/0.9.8.x SVN branch.\n\n2008-09-16  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Don't run into ZeroDivisionError when speed counter\n\t  returns 0s elapsed on upload/download file.\n\n2008-09-15  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3.py, S3/Utils.py, S3/S3Uri.py, S3/Exceptions.py:\n\t  Yet another Unicode round. Unicodised all command line arguments \n\t  before processing.\n\n2008-09-15  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: \"s3cmd mb\" can create upper-case buckets again\n\t  in US. Non-US (e.g. EU) bucket names must conform to strict\n\t  DNS-rules.\n\t* S3/S3Uri.py: Display public URLs correctly for non-DNS buckets.\n\n2008-09-10  Michal Ludvig  <michal@logix.cz>\n\n\t* testsuite, run-tests.py: Added testsuite with first few tests.\n\n2008-09-10  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3Uri.py, S3/S3.py: All internal representations of\n\t  S3Uri()s are Unicode (i.e. not UTF-8 but type()==unicode). It \n\t  still doesn't work on non-UTF8 systems though.\n\n2008-09-04  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Rework UTF-8 output to keep sys.stdout untouched (or it'd\n\t  break 's3cmd get' to stdout for binary files).\n\n2008-09-03  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3.py, S3/Config.py: Removed --use-old-connect-method\n\t  again. Autodetect the need for old connect method instead.\n\n2008-09-03  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3.py: Make --verbose mode more useful and default \n\t  mode less verbose.\n\n2008-09-03  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/Config.py: [rb] Allow removal of non-empty buckets\n\t  with --force.\n\t  [mb, rb] Allow multiple arguments, i.e. create or remove\n\t  multiple buckets at once.\n\t  [del] Perform recursive removal with --recursive (or -r).\n\n2008-09-01  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Refuse 'sync' together with '--encrypt'.\n\t* S3/S3.py: removed object_{get,put,delete}_uri() functions\n\t  and made object_{get,put,delete}() accept URI instead of \n\t  bucket/object parameters.\n\n2008-09-01  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Bumped up version to 0.9.9-pre1\n\n2008-09-01  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3.py, S3/Config.py: Allow access to upper-case\n\t  named buckets again with --use-old-connect-method \n\t  (uses http://s3.amazonaws.com/bucket/object instead of\n\t  http://bucket.s3.amazonaws.com/object)\n\n2008-08-19  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Always output UTF-8, even on output redirects.\n\n2008-08-01  Michal Ludvig  <michal@logix.cz>\n\n\t* TODO: Add some items\n\n2008-07-29  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.8.3\n\t  ------------------------\n\n2008-07-29  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Bumped up version to 0.9.8.3\n\t* NEWS: Added 0.9.8.3\n\n2008-07-29  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Utils.py (hash_file_md5): Hash files in 32kB chunks\n\t  instead of reading it all up to a memory first to avoid\n\t  OOM on large files.\n\n2008-07-07  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd.1: couple of syntax fixes from Mikhail Gusarov\n\n2008-07-03  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.8.2\n\t  ------------------------\n\n2008-07-03  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Bumped up version to 0.9.8.2\n\t* NEWS: Added 0.9.8.2\n\t* s3cmd: Print version info on 'unexpected error' output.\n\n2008-06-30  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Re-upload when Amazon doesn't send ETag\n\t  in PUT response. It happens from time to time for\n\t  unknown reasons. Thanks \"Burtc\" for report and\n\t  \"hermzz\" for fix.\n\n2008-06-27  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.8.1\n\t  ------------------------\n\n2008-06-27  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Bumped up version to 0.9.8.1\n\t* NEWS: Added 0.9.8.1\n\t* s3cmd: make 'cfg' global\n\t* run-tests.sh: Sort-of testsuite\n\n2008-06-23  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.8\n\t  ----------------------\n\n2008-06-23  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Bumped up version to 0.9.8\n\t* NEWS: Added 0.9.8\n\t* TODO: Removed completed tasks\n\n2008-06-23  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Last-minute compatibility fixes for Python 2.4\n\t* s3cmd, s3cmd.1: --debug-exclude is an alias for --debug-syncmatch\n\t* s3cmd: Don't require $HOME env variable to be set.\n\t  Fixes #2000133\n\t* s3cmd: Wrapped all execution in a try/except block\n\t  to catch all exceptions and ask for a report.\n\n2008-06-18  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Version 0.9.8-rc3\n\n2008-06-18  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Bucket name can't contain upper-case letters (S3/DNS limitation).\n\n2008-06-12  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Version 0.9.8-rc2\n\n2008-06-12  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, s3cmd.1: Added GLOB (shell-style wildcard) exclude, renamed\n\t  orig regexp-style --exclude to --rexclude\n\n2008-06-11  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Version 0.9.8-rc1\n\n2008-06-11  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Remove python 2.5 specific code (try/except/finally \n\t  block) and make s3cmd compatible with python 2.4 again.\n\t* s3cmd, S3/Config.py, s3cmd.1: Added --exclude-from and --debug-syncmatch\n\t  switches for sync.\n\n2008-06-10  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Added --exclude switch for sync.\n\t* s3cmd.1, NEWS: Document --exclude\n\n2008-06-05  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.7\n\t  ----------------------\n\n2008-06-05  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Bumped up version to 0.9.7\n\t* NEWS: Added 0.9.7\n\t* TODO: Removed completed tasks\n\t* s3cmd, s3cmd.1: Updated help texts, \n\t  removed --dry-run option as it's not implemented.\n\t\n2008-06-05  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Config.py: Store more file attributes in sync to S3.\n\t* s3cmd: Make sync remote2local more error-resilient.\n\n2008-06-04  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Implemented cmd_sync_remote2local() for restoring\n\t  backup from S3 to a local filesystem\n\t* S3/S3.py: S3.object_get_uri() now requires writable stream \n\t  and not a path name.\n\t* S3/Utils.py: Added mkdir_with_parents()\n\n2008-06-04  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Refactored cmd_sync() in preparation \n\t  for remote->local sync.\n\n2008-04-30  Michal Ludvig  <michal@logix.cz>\n\n\t* s3db, S3/SimpleDB.py: Implemented almost full SimpleDB API.\n\n2008-04-29  Michal Ludvig  <michal@logix.cz>\n\n\t* s3db, S3/SimpleDB.py: Initial support for Amazon SimpleDB. \n\t  For now implements ListDomains() call and most of the \n\t  infrastructure required for request creation.\n\n2008-04-29  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Exceptions.py: Exceptions moved out of S3.S3\n\t* S3/SortedDict.py: rewritten from scratch to preserve\n\t  case of keys while still sorting in case-ignore mode.\n\n2008-04-28  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: send_file() now computes MD5 sum of the file\n\t  being uploaded, compares with ETag returned by Amazon\n\t  and retries upload if they don't match.\n\n2008-03-05  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/S3.py, S3/Utils.py: Throttle upload speed and retry \n\t  when upload failed.\n\t  Report download/upload speed and time elapsed.\n\n2008-02-28  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.6\n\t  ----------------------\n\n2008-02-28  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: bumped up version to 0.9.6\n\t* NEWS: What's new in 0.9.6\n\n2008-02-27  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, s3cmd.1: Updated help and man page.\n\t* S3/S3.py, S3/Utils.py, s3cmd: Support for 's3cmd info' command.\n\t* s3cmd: Fix crash when 'sync'ing files with unresolvable owner uid/gid.\n\t* S3/S3.py, S3/Utils.py: open files in binary mode (otherwise windows\n\t  users have problems).\n\t* S3/S3.py: modify 'x-amz-date' format (problems reported on MacOS X). \n\t  Thanks Jon Larkowski for fix.\n\n2008-02-27  Michal Ludvig  <michal@logix.cz>\n\n\t* TODO: Updated wishlist.\n\n2008-02-11  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Properly follow RedirectPermanent responses for EU buckets\n\t* S3/S3.py: Create public buckets with -P (#1837328)\n\t* S3/S3.py, s3cmd: Correctly display public URL on uploads.\n\t* S3/S3.py, S3/Config.py: Support for MIME types. Both \n\tdefault and guessing. Fixes bug #1872192 (Thanks Martin Herr)\n\n2007-11-13  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.5\n\t  ----------------------\n\n2007-11-13  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Support for buckets stored in Europe, access now \n\t  goes via <bucket>.s3.amazonaws.com where possible.\n\n2007-11-12  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Support for storing file attributes (like ownership, \n\t  mode, etc) in sync operation.\n\t* s3cmd, S3/S3.py: New command 'ib' to get information about \n\t  bucket (only 'LocationConstraint' supported for now).\n\n2007-10-01  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Fix typo in argument name (patch\n\t  from Kim-Minh KAPLAN, SF #1804808)\n\n2007-09-25  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Exit with error code on error (patch\n\t  from Kim-Minh KAPLAN, SF #1800583)\n\n2007-09-25  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Don't fail if bucket listing doesn't have\n\t  <IsTruncated> node.\n\t* s3cmd: Create ~/.s3cfg with 0600 permissions.\n\n2007-09-13  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Improved 'sync'\n\t* S3/S3.py: Support for buckets with over 1000 objects.\n\n2007-09-03  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Small tweaks to --configure workflow.\n\n2007-09-02  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Initial support for 'sync' operation. For\n\t  now only local->s3 direction. In this version doesn't\n\t  work well with non-ASCII filenames and doesn't support\n\t  encryption.\n\n2007-08-24  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/Util.py: More ElementTree imports cleanup\n\n2007-08-19  Michal Ludvig  <michal@logix.cz>\n\n\t* NEWS: Added news for 0.9.5\n\n2007-08-19  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Better handling of multiple arguments for put, get and del\n\n2007-08-14  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.py, S3/Utils.py: Try import xml.etree.ElementTree\n\t  or elementtree.ElementTree module.\n\n2007-08-14  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd.1: Add info about --encrypt parameter.\n\n2007-08-14  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/PkgInfo.py: Bump up version to 0.9.5-pre\n\n2007-08-13  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.4\n\t  ----------------------\n\n2007-08-13  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py: Added function urlencode_string() that encodes\n\t  non-ascii characters in object name before sending it to S3.\n\n2007-08-13  Michal Ludvig  <michal@logix.cz>\n\n\t* README: Updated Amazon S3 pricing overview\n\n2007-08-13  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd, S3/Config.py, S3/S3.py: HTTPS support\n\n2007-07-20  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.py: Check correct Python version and ElementTree availability.\n\n2007-07-05  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: --configure support for Proxy\n\t* S3/S3.py: HTTP proxy support from\n\t  John D. Rowell <jdrowell@exerciseyourbrain.com>\n\n2007-06-19  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.py: Check for S3CMD_PACKAGING and don't install\n\t  manpages and docs if defined.\n\t* INSTALL: Document the above change.\n\t* MANIFEST.in: Include uncompressed manpage\n\n2007-06-17  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Added encryption key support to --configure\n\t* S3/PkgInfo.py: Bump up version to 0.9.4-pre\n\t* setup.py: Cleaned up some rpm-specific stuff that \n\t  caused problems to Debian packager Mikhail Gusarov\n\t* setup.cfg: Removed [bdist_rpm] section\n\t* MANIFEST.in: Include S3/*.py\n\n2007-06-16  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd.1: Syntax fixes from Mikhail Gusarov <dottedmag@dottedmag.net>\n\n2007-05-27  Michal Ludvig  <michal@logix.cz>\n\n\t* Support for on-the-fly GPG encryption.\n\n2007-05-26  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd.1: Add info about \"s3cmd du\" command.\n\n2007-05-26  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.3\n\t  ----------------------\n\n2007-05-26  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Patch from Basil Shubin <basil.shubin@gmail.com>\n\t  adding support for \"s3cmd du\" command.\n\t* s3cmd: Modified output format of \"s3cmd du\" to conform\n\t  with unix \"du\".\n\t* setup.cfg: Require Python 2.5 in RPM. Otherwise it needs\n\t  to require additional python modules (e.g. ElementTree)\n\t  which may have different names in different distros. It's \n\t  indeed still possible to manually install s3cmd with \n\t  Python 2.4 and appropriate modules.\n\n2007-04-09  Michal Ludvig  <michal@logix.cz>\n\n\t* Released version 0.9.2\n\t  ----------------------\n\n2007-04-09  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd.1: Added manpage\n\t* Updated infrastructure files to create \"better\"\n\t  distribution archives.\n\n2007-03-26  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.py, S3/PkgInfo.py: Move package info out of setup.py\n\t* s3cmd: new parameter --version\n\t* s3cmd, S3/S3Uri.py: Output public HTTP URL for objects\n\t  stored with Public ACL.\n\t  \n2007-02-28  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Verify supplied accesskey and secretkey\n\t  in interactive configuration path.\n\t* S3/Config.py: Hide access key and secret key\n\t  from debug output.\n\t* S3/S3.py: Modify S3Error exception to work\n\t  in python 2.4 (=> don't expect Exception is\n\t  a new-style class).\n\t* s3cmd: Updated for the above change.\n\n2007-02-19  Michal Ludvig  <michal@logix.cz>\n\n\t* NEWS, INSTALL, README, setup.py: Added\n\t  more documentation.\n\n2007-02-19  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py, s3cmd: New feature - allow \"get\" to stdout\n\n2007-02-19  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3fs.py: Removed (development moved to branch s3fs-devel).\n\n2007-02-08  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3fs.py: \n\t  - Implemented mknod()\n\t  - Can create directory structure\n\t  - Rewritten to use SQLite3. Currently can create\n\t    the filesystem, and a root inode.\n\n2007-02-07  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd (from /s3py:74): Renamed SVN top-level project\n\t  s3py to s3cmd\n\n2007-02-07  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.cfg: Only require Python 2.4, not 2.5\n\t* S3/Config.py: Removed show_uri - no longer needed,\n\t  it's now default\n\n2007-02-07  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.py\n\t  - Version 0.9.1\n\n2007-02-07  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd: Change all \"exit()\" calls to \"sys.exit()\"\n\t  and allow for python 2.4\n\t* S3/S3.py: Removed dependency on hashlib -> allow for python 2.4\n\n2007-01-27  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py, S3/S3Uri.py: Case insensitive regex in S3Uri.py\n\n2007-01-26  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3fs.py: Added support for storing/loading inodes.\n\t  No data yet however.\n\n2007-01-26  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3fs.py: Initial version of S3fs module. \n\t  Can create filesystem via \"S3fs.mkfs()\"\n\n2007-01-26  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/BidirMap.py, S3/Config.py, S3/S3.py, S3/S3Uri.py,\n\t  S3/SortedDict.py, S3/Utils.py, s3cmd: Added headers with\n\t  copyright to all files\n\t* S3/S3.py, S3/S3Uri.py: Removed S3.compose_uri(), introduced\n\t  S3UriS3.compose_uri() instead.\n\n2007-01-26  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py, S3/S3Uri.py, s3cmd: \n\t  - Converted all users of parse_uri to S3Uri class API\n\t  - Removed \"cp\" command again. Will have to use 'put'\n\t    and 'get' for now.\n\n2007-01-25  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3Uri.py: New module S3/S3Uri.py\n\t* S3/S3.py, s3cmd: Converted \"put\" operation to use\n\t  the new S3Uri class.\n\n2007-01-24  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py\n\t* s3cmd\n\t  - Added 'cp' command\n\t  - Renamed parse_s3_uri to parse_uri (this will go away anyway)\n\n2007-01-19  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.cfg\n\t* setup.py\n\t  - Include README into tarballs\n\n2007-01-19  Michal Ludvig  <michal@logix.cz>\n\n\t* README\n\t  - Added comprehensive README file\n\n2007-01-19  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.cfg\n\t* setup.py\n\t  - Added configuration for setup.py sdist\n\n2007-01-19  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Config.py\n\t* s3cmd\n\t  - Added interactive configurator (--configure)\n\t  - Added config dumper (--dump-config)\n\t  - Improved --help output\n\n2007-01-19  Michal Ludvig  <michal@logix.cz>\n\n\t* setup.cfg\n\t* setup.py\n\t  Added info for building RPM packages.\n\n2007-01-18  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Config.py\n\t* S3/S3.py\n\t* s3cmd\n\t  Moved class Config from S3/S3.py to S3/Config.py\n\n2007-01-18  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/Config.py (from /s3py/trunk/S3/ConfigParser.py:47)\n\t* S3/ConfigParser.py\n\t* S3/S3.py\n\t  Renamed S3/ConfigParser.py to S3/Config.py\n\n2007-01-18  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd\n\t  Added info about homepage\n\n2007-01-17  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py\n\t* s3cmd\n\t  - Use prefix for listings if specified.\n\t  - List all commands in --help\n\n2007-01-16  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py\n\t* s3cmd\n\t  Major rework of Config class:\n\t  - Renamed from AwsConfig to Config\n\t  - Converted to Singleton (see Config.__new__() and an article on\n\t    Wikipedia)\n\t  - No more explicit listing of options - use introspection to get them\n\t    (class variables that of type str, int or bool that don't start with\n\t    underscore)\n\t  - Check values read from config file and verify their type.\n\t  \n\t  Added OptionMimeType and -m/-M options. Not yet implemented\n\t  functionality in the rest of S3/S3.py\n\n2007-01-15  Michal Ludvig  <michal@logix.cz>\n\n\t* S3/S3.py\n\t* s3cmd\n\t  - Merged list-buckets and bucket-list-objects operations into\n\t    a single 'ls' command.\n\t  - New parameter -P for uploading publicly readable objects\n\n2007-01-14  Michal Ludvig  <michal@logix.cz>\n\n\t* s3.py\n\t* setup.py\n\t  Renamed s3.py to s3cmd (take 2)\n\n2007-01-14  Michal Ludvig  <michal@logix.cz>\n\n\t* s3cmd (from /s3py/trunk/s3.py:45)\n\t  Renamed s3.py to s3cmd\n\n2007-01-14  Michal Ludvig  <michal@logix.cz>\n\n\t* S3\n\t* S3/S3.py\n\t* s3.py\n\t* setup.py\n\t  All classes from s3.py go to S3/S3.py\n\t  Added setup.py\n\n2007-01-14  Michal Ludvig  <michal@logix.cz>\n\n\t* s3.py\n\t  Minor fix S3.utils -> S3.Utils\n\n2007-01-14  Michal Ludvig  <michal@logix.cz>\n\n\t* .svnignore\n\t* BidirMap.py\n\t* ConfigParser.py\n\t* S3\n\t* S3/BidirMap.py (from /s3py/trunk/BidirMap.py:35)\n\t* S3/ConfigParser.py (from /s3py/trunk/ConfigParser.py:38)\n\t* S3/SortedDict.py (from /s3py/trunk/SortedDict.py:35)\n\t* S3/Utils.py (from /s3py/trunk/utils.py:39)\n\t* S3/__init__.py\n\t* SortedDict.py\n\t* s3.py\n\t* utils.py\n\t  Moved modules to their own package\n\n2007-01-12  Michal Ludvig  <michal@logix.cz>\n\n\t* s3.py\n\t  Added \"del\" command\n\t  Converted all (?) commands to accept s3-uri\n\t  Added -u/--show-uri parameter\n\n2007-01-11  Michal Ludvig  <michal@logix.cz>\n\n\t* s3.py\n\t  Verify MD5 on received files\n\t  Improved upload of multiple files\n\t  Initial S3-URI support (more tbd)\n\n2007-01-11  Michal Ludvig  <michal@logix.cz>\n\n\t* s3.py\n\t  Minor fixes:\n\t  - store names of parsed files in AwsConfig\n\t  - Print total size with upload/download\n\n2007-01-11  Michal Ludvig  <michal@logix.cz>\n\n\t* s3.py\n\t* utils.py\n\t  Added support for sending and receiving files.\n\n2007-01-11  Michal Ludvig  <michal@logix.cz>\n\n\t* ConfigParser.py\n\t* s3.py\n\t  List all Objects in all Buckets command\n\t  Yet another logging improvement\n\t  Version check for Python 2.5 or higher\n\n2007-01-11  Michal Ludvig  <michal@logix.cz>\n\n\t* ConfigParser.py\n\t* s3.py\n\t* utils.py\n\t  Added ConfigParser\n\t  Improved setting logging levels\n\t  It can now quite reliably list buckets and objects\n\n2007-01-11  Michal Ludvig  <michal@logix.cz>\n\n\t* .svnignore\n\t  Added ignore list\n\n2007-01-11  Michal Ludvig  <michal@logix.cz>\n\n\t* .svnignore\n\t* BidirMap.py\n\t* SortedDict.py\n\t* s3.py\n\t* utils.py\n\t  Initial import\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.0771484375,
          "content": "## S3cmd tool for Amazon Simple Storage Service (S3)\n\n[![Build Status](https://github.com/s3tools/s3cmd/actions/workflows/test.yml/badge.svg)](https://github.com/s3tools/s3cmd/actions/workflows/test.yml)\n\n* Authors: Michal Ludvig (michal@logix.cz), Florent Viard (florent@sodria.com)\n* [Project homepage](https://s3tools.org)\n* (c) [TGRMN Software](http://www.tgrmn.com), [Sodria SAS](http://www.sodria.com) and contributors\n\n\nS3tools / S3cmd mailing lists:\n\n* Announcements of new releases: s3tools-announce@lists.sourceforge.net\n* General questions and discussion: s3tools-general@lists.sourceforge.net\n* Bug reports: s3tools-bugs@lists.sourceforge.net\n\nS3cmd requires Python 2.6 or newer.\nPython 3+ is also supported starting with S3cmd version 2.\n\nSee [installation instructions](https://github.com/s3tools/s3cmd/blob/master/INSTALL.md).\n\n\n### What is S3cmd\n\nS3cmd (`s3cmd`) is a free command line tool and client for uploading, retrieving and managing data in Amazon S3 and other cloud storage service providers that use the S3 protocol, such as Google Cloud Storage or DreamHost DreamObjects. It is best suited for power users who are familiar with command line programs. It is also ideal for batch scripts and automated backup to S3, triggered from cron, etc.\n\nS3cmd is written in Python. It's an open source project available under GNU Public License v2 (GPLv2) and is free for both commercial and private use. You will only have to pay Amazon for using their storage.\n\nLots of features and options have been added to S3cmd, since its very first release in 2008.... we recently counted more than 60 command line options, including multipart uploads, encryption, incremental backup, s3 sync, ACL and Metadata management, S3 bucket size, bucket policies, and more!\n\n### What is Amazon S3\n\nAmazon S3 provides a managed internet-accessible storage service where anyone can store any amount of data and retrieve it later again.\n\nS3 is a paid service operated by Amazon. Before storing anything into S3 you must sign up for an \"AWS\" account (where AWS = Amazon Web Services) to obtain a pair of identifiers: Access Key and Secret Key. You will need to\ngive these keys to S3cmd. Think of them as if they were a username and password for your S3 account.\n\n### Amazon S3 pricing explained\n\nAt the time of this writing the costs of using S3 are (in USD):\n\n$0.023 per GB per month of storage space used\n\nplus\n\n$0.00 per GB - all data uploaded\n\nplus\n\n$0.000 per GB - first 1GB / month data downloaded\n$0.090 per GB - up to 10 TB / month data downloaded\n$0.085 per GB - next 40 TB / month data downloaded\n$0.070 per GB - next 100 TB / month data downloaded\n$0.050 per GB - data downloaded / month over 150 TB\n\nplus\n\n$0.005 per 1,000 PUT or COPY or LIST requests\n$0.004 per 10,000 GET and all other requests\n\nIf for instance on 1st of January you upload 2GB of photos in JPEG from your holiday in New Zealand, at the end of January you will be charged $0.05 for using 2GB of storage space for a month, $0.0 for uploading 2GB of data, and a few cents for requests. That comes to slightly over $0.06 for a complete backup of your precious holiday pictures.\n\nIn February you don't touch it. Your data are still on S3 servers so you pay $0.06 for those two gigabytes, but not a single cent will be charged for any transfer. That comes to $0.05 as an ongoing cost of your backup. Not too bad.\n\nIn March you allow anonymous read access to some of your pictures and your friends download, say, 1500MB of them. As the files are owned by you, you are responsible for the costs incurred. That means at the end of March you'll be charged $0.05 for storage plus $0.045 for the download traffic generated by your friends.\n\nThere is no minimum monthly contract or a setup fee. What you use is what you pay for. At the beginning my bill used to be like US$0.03 or even nil.\n\nThat's the pricing model of Amazon S3 in a nutshell. Check the [Amazon S3 homepage](https://aws.amazon.com/s3/pricing/) for more details.\n\nNeedless to say that all these money are charged by Amazon itself, there is obviously no payment for using S3cmd :-)\n\n### Amazon S3 basics\n\nFiles stored in S3 are called \"objects\" and their names are officially called \"keys\". Since this is sometimes confusing for the users we often refer to the objects as \"files\" or \"remote files\". Each object belongs to exactly one \"bucket\".\n\nTo describe objects in S3 storage we invented a URI-like schema in the following form:\n\n```\ns3://BUCKET\n```\nor\n\n```\ns3://BUCKET/OBJECT\n```\n\n### Buckets\n\nBuckets are sort of like directories or folders with some restrictions:\n\n1. each user can only have 100 buckets at the most,\n2. bucket names must be unique amongst all users of S3,\n3. buckets can not be nested into a deeper hierarchy and\n4. a name of a bucket can only consist of basic alphanumeric\n   characters plus dot (.) and dash (-). No spaces, no accented\n   or UTF-8 letters, etc.\n\nIt is a good idea to use DNS-compatible bucket names. That for instance means you should not use upper case characters. While DNS compliance is not strictly required some features described below are not available for DNS-incompatible named buckets. One more step further is using a fully qualified domain name (FQDN) for a bucket - that has even more benefits.\n\n* For example \"s3://--My-Bucket--\" is not DNS compatible.\n* On the other hand \"s3://my-bucket\" is DNS compatible but\n  is not FQDN.\n* Finally \"s3://my-bucket.s3tools.org\" is DNS compatible\n  and FQDN provided you own the s3tools.org domain and can\n  create the domain record for \"my-bucket.s3tools.org\".\n\nLook for \"Virtual Hosts\" later in this text for more details regarding FQDN named buckets.\n\n### Objects (files stored in Amazon S3)\n\nUnlike for buckets there are almost no restrictions on object names. These can be any UTF-8 strings of up to 1024 bytes long. Interestingly enough the object name can contain forward slash character (/) thus a `my/funny/picture.jpg` is a valid object name. Note that there are not directories nor buckets called `my` and `funny` - it is really a single object name called `my/funny/picture.jpg` and S3 does not care at all that it _looks_ like a directory structure.\n\nThe full URI of such an image could be, for example:\n\n```\ns3://my-bucket/my/funny/picture.jpg\n```\n\n### Public vs Private files\n\nThe files stored in S3 can be either Private or Public. The Private ones are readable only by the user who uploaded them while the Public ones can be read by anyone. Additionally the Public files can be accessed using HTTP protocol, not only using `s3cmd` or a similar tool.\n\nThe ACL (Access Control List) of a file can be set at the time of upload using `--acl-public` or `--acl-private` options with `s3cmd put` or `s3cmd sync` commands (see below).\n\nAlternatively the ACL can be altered for existing remote files with `s3cmd setacl --acl-public` (or `--acl-private`) command.\n\n### Simple s3cmd HowTo\n\n1) Register for Amazon AWS / S3\n\nGo to https://aws.amazon.com/s3, click the \"Sign up for web service\" button in the right column and work through the registration. You will have to supply your Credit Card details in order to allow Amazon charge you for S3 usage. At the end you should have your Access and Secret Keys.\n\nIf you set up a separate IAM user, that user's access key must have at least the following permissions to do anything:\n-  s3:ListAllMyBuckets\n-  s3:GetBucketLocation\n-  s3:ListBucket\n\nOther example policies can be found at https://docs.aws.amazon.com/AmazonS3/latest/dev/example-policies-s3.html\n\n2) Run `s3cmd --configure`\n\nYou will be asked for the two keys - copy and paste them from your confirmation email or from your Amazon account page. Be careful when copying them! They are case sensitive and must be entered accurately or you'll keep getting errors about invalid signatures or similar.\n\nRemember to add s3:ListAllMyBuckets permissions to the keys or you will get an AccessDenied error while testing access.\n\n3) Run `s3cmd ls` to list all your buckets.\n\nAs you just started using S3 there are no buckets owned by you as of now. So the output will be empty.\n\n4) Make a bucket with `s3cmd mb s3://my-new-bucket-name`\n\nAs mentioned above the bucket names must be unique amongst _all_ users of S3. That means the simple names like \"test\" or \"asdf\" are already taken and you must make up something more original. To demonstrate as many features as possible let's create a FQDN-named bucket `s3://public.s3tools.org`:\n\n```\n$ s3cmd mb s3://public.s3tools.org\n\nBucket 's3://public.s3tools.org' created\n```\n\n5) List your buckets again with `s3cmd ls`\n\nNow you should see your freshly created bucket:\n\n```\n$ s3cmd ls\n\n2009-01-28 12:34  s3://public.s3tools.org\n```\n\n6) List the contents of the bucket:\n\n```\n$ s3cmd ls s3://public.s3tools.org\n$\n```\n\nIt's empty, indeed.\n\n7) Upload a single file into the bucket:\n\n```\n$ s3cmd put some-file.xml s3://public.s3tools.org/somefile.xml\n\nsome-file.xml -> s3://public.s3tools.org/somefile.xml  [1 of 1]\n 123456 of 123456   100% in    2s    51.75 kB/s  done\n```\n\nUpload a two-directory tree into the bucket's virtual 'directory':\n\n```\n$ s3cmd put --recursive dir1 dir2 s3://public.s3tools.org/somewhere/\n\nFile 'dir1/file1-1.txt' stored as 's3://public.s3tools.org/somewhere/dir1/file1-1.txt' [1 of 5]\nFile 'dir1/file1-2.txt' stored as 's3://public.s3tools.org/somewhere/dir1/file1-2.txt' [2 of 5]\nFile 'dir1/file1-3.log' stored as 's3://public.s3tools.org/somewhere/dir1/file1-3.log' [3 of 5]\nFile 'dir2/file2-1.bin' stored as 's3://public.s3tools.org/somewhere/dir2/file2-1.bin' [4 of 5]\nFile 'dir2/file2-2.txt' stored as 's3://public.s3tools.org/somewhere/dir2/file2-2.txt' [5 of 5]\n```\n\nAs you can see we didn't have to create the `/somewhere` 'directory'. In fact it's only a filename prefix, not a real directory and it doesn't have to be created in any way beforehand.\n\nInstead of using `put` with the `--recursive` option, you could also use the `sync` command:\n\n```\n$ s3cmd sync dir1 dir2 s3://public.s3tools.org/somewhere/\n```\n\n8) Now list the bucket's contents again:\n\n```\n$ s3cmd ls s3://public.s3tools.org\n\n                       DIR   s3://public.s3tools.org/somewhere/\n2009-02-10 05:10    123456   s3://public.s3tools.org/somefile.xml\n```\n\nUse --recursive (or -r) to list all the remote files:\n\n```\n$ s3cmd ls --recursive s3://public.s3tools.org\n\n2009-02-10 05:10    123456   s3://public.s3tools.org/somefile.xml\n2009-02-10 05:13        18   s3://public.s3tools.org/somewhere/dir1/file1-1.txt\n2009-02-10 05:13         8   s3://public.s3tools.org/somewhere/dir1/file1-2.txt\n2009-02-10 05:13        16   s3://public.s3tools.org/somewhere/dir1/file1-3.log\n2009-02-10 05:13        11   s3://public.s3tools.org/somewhere/dir2/file2-1.bin\n2009-02-10 05:13         8   s3://public.s3tools.org/somewhere/dir2/file2-2.txt\n```\n\n9) Retrieve one of the files back and verify that it hasn't been\n   corrupted:\n\n```\n$ s3cmd get s3://public.s3tools.org/somefile.xml some-file-2.xml\n\ns3://public.s3tools.org/somefile.xml -> some-file-2.xml  [1 of 1]\n 123456 of 123456   100% in    3s    35.75 kB/s  done\n```\n\n```\n$ md5sum some-file.xml some-file-2.xml\n\n39bcb6992e461b269b95b3bda303addf  some-file.xml\n39bcb6992e461b269b95b3bda303addf  some-file-2.xml\n```\n\nChecksums of the original file matches the one of the retrieved ones. Looks like it worked :-)\n\nTo retrieve a whole 'directory tree' from S3 use recursive get:\n\n```\n$ s3cmd get --recursive s3://public.s3tools.org/somewhere\n\nFile s3://public.s3tools.org/somewhere/dir1/file1-1.txt saved as './somewhere/dir1/file1-1.txt'\nFile s3://public.s3tools.org/somewhere/dir1/file1-2.txt saved as './somewhere/dir1/file1-2.txt'\nFile s3://public.s3tools.org/somewhere/dir1/file1-3.log saved as './somewhere/dir1/file1-3.log'\nFile s3://public.s3tools.org/somewhere/dir2/file2-1.bin saved as './somewhere/dir2/file2-1.bin'\nFile s3://public.s3tools.org/somewhere/dir2/file2-2.txt saved as './somewhere/dir2/file2-2.txt'\n```\n\nSince the destination directory wasn't specified, `s3cmd` saved the directory structure in a current working directory ('.').\n\nThere is an important difference between:\n\n```\nget s3://public.s3tools.org/somewhere\n```\n\nand\n\n```\nget s3://public.s3tools.org/somewhere/\n```\n\n(note the trailing slash)\n\n`s3cmd` always uses the last path part, ie the word after the last slash, for naming files.\n\nIn the case of `s3://.../somewhere` the last path part is 'somewhere' and therefore the recursive get names the local files as somewhere/dir1, somewhere/dir2, etc.\n\nOn the other hand in `s3://.../somewhere/` the last path\npart is empty and s3cmd will only create 'dir1' and 'dir2'\nwithout the 'somewhere/' prefix:\n\n```\n$ s3cmd get --recursive s3://public.s3tools.org/somewhere/ ~/\n\nFile s3://public.s3tools.org/somewhere/dir1/file1-1.txt saved as '~/dir1/file1-1.txt'\nFile s3://public.s3tools.org/somewhere/dir1/file1-2.txt saved as '~/dir1/file1-2.txt'\nFile s3://public.s3tools.org/somewhere/dir1/file1-3.log saved as '~/dir1/file1-3.log'\nFile s3://public.s3tools.org/somewhere/dir2/file2-1.bin saved as '~/dir2/file2-1.bin'\n```\n\nSee? It's `~/dir1` and not `~/somewhere/dir1` as it was in the previous example.\n\n10) Clean up - delete the remote files and remove the bucket:\n\nRemove everything under s3://public.s3tools.org/somewhere/\n\n```\n$ s3cmd del --recursive s3://public.s3tools.org/somewhere/\n\nFile s3://public.s3tools.org/somewhere/dir1/file1-1.txt deleted\nFile s3://public.s3tools.org/somewhere/dir1/file1-2.txt deleted\n...\n```\n\nNow try to remove the bucket:\n\n```\n$ s3cmd rb s3://public.s3tools.org\n\nERROR: S3 error: 409 (BucketNotEmpty): The bucket you tried to delete is not empty\n```\n\nOuch, we forgot about `s3://public.s3tools.org/somefile.xml`. We can force the bucket removal anyway:\n\n```\n$ s3cmd rb --force s3://public.s3tools.org/\n\nWARNING: Bucket is not empty. Removing all the objects from it first. This may take some time...\nFile s3://public.s3tools.org/somefile.xml deleted\nBucket 's3://public.s3tools.org/' removed\n```\n\n### Hints\n\nThe basic usage is as simple as described in the previous section.\n\nYou can increase the level of verbosity with `-v` option and if you're really keen to know what the program does under its bonnet run it with `-d` to see all 'debugging' output.\n\nAfter configuring it with `--configure` all available options are spitted into your `~/.s3cfg` file. It's a text file ready to be modified in your favourite text editor.\n\nThe Transfer commands (put, get, cp, mv, and sync) continue transferring even if an object fails. If a failure occurs the failure is output to stderr and the exit status will be EX_PARTIAL (2). If the option `--stop-on-error` is specified, or the config option stop_on_error is true, the transfers stop and an appropriate error code is returned.\n\nFor more information refer to the [S3cmd / S3tools homepage](https://s3tools.org).\n\n### License\n\nCopyright (C) 2007-2023 TGRMN Software (https://www.tgrmn.com), Sodria SAS (https://www.sodria.com/) and contributors\n\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\n"
        },
        {
          "name": "RELEASE_INSTRUCTIONS",
          "type": "blob",
          "size": 3.2451171875,
          "content": "Instructions for s3cmd maintainers for doing a tagged release and publishing on sourceforge.net.\nIn the below, 2.1.0 is the example version being released.  Salt to taste.\n\nDependency that could be needed for the release:\n    pip install --user twine\n\n1.  Make a fresh clone of the repo:\n    git clone ssh+git://git@github.com/s3tools/s3cmd s3cmd-release\n\n2.  Run ./run-tests.py to verify it all works OK.\n\n3.  Update version to 2.1.0 in S3/PkgInfo.py\n\n4.  Update manpage with ./s3cmd --help | ./format-manpage.pl > s3cmd.1\n\n5.  Update NEWS with info about new features. Best to extract from git with:\n    git log --abbrev-commit --no-merges v2.0.2..\n    (list all tags with: \"git tag\")\n\n6.  Verify the above changes:\n    git diff --check && git diff\n    git status\n    (The only changed files should be NEWS, s3cmd.1, S3/PkgInfo.py)\n\n7.  Remove testsuite (intentionally inaccessible files break the next\n    step):\n    chmod -R +rwx testsuite/permission-tests/permission-denied-dir && rm -rf testsuite\n\n8.  If everything worked fine commit the above changes:\n    git commit -a -m \"Update version to 2.1.0\"\n\n9.  Tag it:\n    git tag --sign -a v2.1.0 -m \"Tag v2.1.0\"\n\n10. Push back to github:\n    git push --tags\n\n11. Build the \"Source Distribution\" and the universal \"Wheel\" package:\n    python setup.py sdist bdist_wheel --universal\n    -> Creates dist/s3cmd-2.1.0.tar.gz , dist/s3cmd-2.1.0.zip and dist/s3cmd-2.1.0-py2.py3-none-any.whl\n\n12. Generate the GPG signatures for the previously generated artefacts\n    gpg2 --default-key XXXX --detach-sign -a dist/s3cmd-2.1.0.tar.gz\n    gpg2 --default-key XXXX --detach-sign -a dist/s3cmd-2.1.0.zip\n    gpg2 --default-key XXXX --detach-sign -a dist/s3cmd-2.1.0-py2.py3-none-any.whl\n\n13. Publish to PyPi, so 'pip install s3cmd' downloads the new version.\n    twine upload dist/s3cmd-2.1.0.tar.gz* dist/s3cmd-2.1.0-py2.py3-none-any.whl*\n\nNote: we only publish the .tar.gz and the .whl (+.asc signatures) to Pypi, and\nnot the .zip as Pypi only accepts a single \"sdist\" source file for a given version.\n\n\nGitHub releases\n\n1. Login to github.com/s3tools/s3cmd\n\n2.  You will see your new tag in the Tags tab.  Click \"Draft a new\n    release\".\n\n3.  In the 'Tag version' drop-down, select your new tag.\n\n4.  In the 'Release title' field, name it v2.1.0.\n\n5.  In the 'Describe this release' text box, add in this release's\n    notes from the NEWS file.\n\n6.  Upload all 4 files from dist/.\n\n7.  Click \"Publish release\"\n\n\n\nSourceForge releases\n\n1.  Login to sf.net\n\n2.  Go to https://sourceforge.net/p/s3tools/admin/\n\n3.  Files -> s3cmd -> Add Folder -> Enter \"2.1.0\" -> Create\n\n4.  Go into 2.1.0 -> Add File -> upload dist/s3cmd-2.1.0.tar.gz\n\n5.  Once uploaded click the little \"i\" icon on the right and click\n    \"Select all\" under \"Default Download For:\" to update the default\n    download button to this new version.\n\n6.  Give it a few minutes and verify on the Summary page that the\n    download button has been updated to s3cmd-2.1.0.tar.gz\n\nNow it's time to send out an announcement email to\ns3tools-announce@lists.sourceforge.net and\ns3tools-general@lists.sourceforge.net (check out the s3cmd-announce\narchive for an inspiration :)\n\nAnd the last step is to ask the respective distribution maintainers\n(Fedora, Debian, Ubuntu, OpenSuse, ...?) to update the package in\ntheir builds.\n"
        },
        {
          "name": "S3",
          "type": "tree",
          "content": null
        },
        {
          "name": "format-manpage.pl",
          "type": "blob",
          "size": 7.009765625,
          "content": "#!/usr/bin/perl\n\n# Format s3cmd.1 manpage\n# Usage:\n#   s3cmd --help | format-manpage.pl > s3cmd.1\n\nuse strict;\n\nmy $commands = \"\";\nmy $cfcommands = \"\";\nmy $wscommands = \"\";\nmy $options = \"\";\n\nwhile (<>) {\n\tif (/^Commands:/) {\n\t\twhile (<>) {\n\t\t\tlast if (/^\\s*$/);\n\t\t\tmy ($desc, $cmd, $cmdline);\n\t\t\t($desc = $_) =~ s/^\\s*(.*?)\\s*$/$1/;\n\t\t\t($cmdline = <>) =~ s/^\\s*s3cmd (.*?) (.*?)\\s*$/s3cmd \\\\fB$1\\\\fR \\\\fI$2\\\\fR/;\n\t\t\t$cmd = $1;\n\t\t\t$cmdline =~ s/-/\\\\-/g;\n\t\t\tif ($cmd =~ /^cf/) {\n\t\t\t\t$cfcommands .= \".TP\\n$cmdline\\n$desc\\n\";\n\t\t\t} elsif ($cmd =~ /^ws/) {\n\t\t\t\t$wscommands .= \".TP\\n$cmdline\\n$desc\\n\";\n\t\t\t} else {\n\t\t\t\t$commands .= \".TP\\n$cmdline\\n$desc\\n\";\n\t\t\t}\n\t\t}\n\t}\n\tif (/^Options:/) {\n\t\tmy ($opt, $desc);\n\t\twhile (<>) {\n\t\t\tlast if (/^\\s*$/);\n\t\t\t$_ =~ s/(.*?)\\s*$/$1/;\n\t\t\t$desc = \"\";\n\t\t\t$opt = \"\";\n\t\t\tif (/^  (-.*)/) {\n\t\t\t\t$opt = $1;\n\t\t\t\tif ($opt =~ /  /) {\n\t\t\t\t\t($opt, $desc) = split(/\\s\\s+/, $opt, 2);\n\t\t\t\t}\n\t\t\t\t$opt =~ s/(-[^ ,=\\.]+)/\\\\fB$1\\\\fR/g;\n\t\t\t\t# escape all single dashes\n\t\t\t\t$opt =~ s/-/\\\\-/g;\n\t\t\t\t$options .= \".TP\\n$opt\\n\";\n\t\t\t} else {\n\t\t\t\t$_ =~ s/\\s*(.*?)\\s*$/$1/;\n\t\t\t\t$_ =~ s/(--[^ ,=\\.]+)/\\\\fB$1\\\\fR/g;\n\t\t\t\t# escape word 'Cache-Control'\n\t\t\t\t$_ =~ s/'(\\S+-\\S+)'/\\\\&'$1'/g;\n\t\t\t\t# escape all single dashes\n\t\t\t\t$_ =~ s/-/\\\\-/g;\n\t\t\t\t$desc .= $_;\n\t\t\t}\n\t\t\tif ($desc) {\n\t\t\t\t$options .= \"$desc\\n\";\n\t\t\t}\n\t\t}\n\t}\n}\nprint \"\n.\\\\\\\" !!! IMPORTANT: This file is generated from s3cmd \\\\-\\\\-help output using format-manpage.pl\n.\\\\\\\" !!!            Do your changes either in s3cmd file or in 'format\\\\-manpage.pl' otherwise\n.\\\\\\\" !!!            they will be overwritten!\n\n.TH s3cmd 1\n.SH NAME\ns3cmd \\\\- tool for managing Amazon S3 storage space and Amazon CloudFront content delivery network\n.SH SYNOPSIS\n.B s3cmd\n[\\\\fIOPTIONS\\\\fR] \\\\fICOMMAND\\\\fR [\\\\fIPARAMETERS\\\\fR]\n.SH DESCRIPTION\n.PP\n.B s3cmd\nis a command line client for copying files to/from\nAmazon S3 (Simple Storage Service) and performing other\nrelated tasks, for instance creating and removing buckets,\nlisting objects, etc.\n\n.SH COMMANDS\n.PP\n.B s3cmd\ncan do several \\\\fIactions\\\\fR specified by the following \\\\fIcommands\\\\fR.\n$commands\n\n.PP\nCommands for static WebSites configuration\n$wscommands\n\n.PP\nCommands for CloudFront management\n$cfcommands\n\n.SH OPTIONS\n.PP\nSome of the below specified options can have their default\nvalues set in\n.B s3cmd\nconfig file (by default \\$HOME/.s3cmd). As it's a simple text file\nfeel free to open it with your favorite text editor and do any\nchanges you like.\n$options\n\n.SH EXAMPLES\nOne of the most powerful commands of \\\\fIs3cmd\\\\fR is \\\\fBs3cmd sync\\\\fR used for\nsynchronising complete directory trees to or from remote S3 storage. To some extent\n\\\\fBs3cmd put\\\\fR and \\\\fBs3cmd get\\\\fR share a similar behaviour with \\\\fBsync\\\\fR.\n.PP\nBasic usage common in backup scenarios is as simple as:\n.nf\n\ts3cmd sync /local/path/ s3://test\\\\-bucket/backup/\n.fi\n.PP\nThis command will find all files under /local/path directory and copy them\nto corresponding paths under s3://test\\\\-bucket/backup on the remote side.\nFor example:\n.nf\n\t/local/path/\\\\fBfile1.ext\\\\fR         \\\\->  s3://bucket/backup/\\\\fBfile1.ext\\\\fR\n\t/local/path/\\\\fBdir123/file2.bin\\\\fR  \\\\->  s3://bucket/backup/\\\\fBdir123/file2.bin\\\\fR\n.fi\n.PP\nHowever if the local path doesn't end with a slash the last directory's name\nis used on the remote side as well. Compare these with the previous example:\n.nf\n\ts3cmd sync /local/path s3://test\\\\-bucket/backup/\n.fi\nwill sync:\n.nf\n\t/local/\\\\fBpath/file1.ext\\\\fR         \\\\->  s3://bucket/backup/\\\\fBpath/file1.ext\\\\fR\n\t/local/\\\\fBpath/dir123/file2.bin\\\\fR  \\\\->  s3://bucket/backup/\\\\fBpath/dir123/file2.bin\\\\fR\n.fi\n.PP\nTo retrieve the files back from S3 use inverted syntax:\n.nf\n\ts3cmd sync s3://test\\\\-bucket/backup/ ~/restore/\n.fi\nthat will download files:\n.nf\n\ts3://bucket/backup/\\\\fBfile1.ext\\\\fR         \\\\->  ~/restore/\\\\fBfile1.ext\\\\fR\n\ts3://bucket/backup/\\\\fBdir123/file2.bin\\\\fR  \\\\->  ~/restore/\\\\fBdir123/file2.bin\\\\fR\n.fi\n.PP\nWithout the trailing slash on source the behaviour is similar to\nwhat has been demonstrated with upload:\n.nf\n\ts3cmd sync s3://test\\\\-bucket/backup ~/restore/\n.fi\nwill download the files as:\n.nf\n\ts3://bucket/\\\\fBbackup/file1.ext\\\\fR         \\\\->  ~/restore/\\\\fBbackup/file1.ext\\\\fR\n\ts3://bucket/\\\\fBbackup/dir123/file2.bin\\\\fR  \\\\->  ~/restore/\\\\fBbackup/dir123/file2.bin\\\\fR\n.fi\n.PP\nAll source file names, the bold ones above, are matched against \\\\fBexclude\\\\fR\nrules and those that match are then re\\\\-checked against \\\\fBinclude\\\\fR rules to see\nwhether they should be excluded or kept in the source list.\n.PP\nFor the purpose of \\\\fB\\\\-\\\\-exclude\\\\fR and \\\\fB\\\\-\\\\-include\\\\fR matching only the\nbold file names above are used. For instance only \\\\fBpath/file1.ext\\\\fR is tested\nagainst the patterns, not \\\\fI/local/\\\\fBpath/file1.ext\\\\fR\n.PP\nBoth \\\\fB\\\\-\\\\-exclude\\\\fR and \\\\fB\\\\-\\\\-include\\\\fR work with shell\\\\-style wildcards (a.k.a. GLOB).\nFor a greater flexibility s3cmd provides Regular\\\\-expression versions of the two exclude options\nnamed \\\\fB\\\\-\\\\-rexclude\\\\fR and \\\\fB\\\\-\\\\-rinclude\\\\fR.\nThe options with ...\\\\fB\\\\-from\\\\fR suffix (eg \\\\-\\\\-rinclude\\\\-from) expect a filename as\nan argument. Each line of such a file is treated as one pattern.\n.PP\nThere is only one set of patterns built from all \\\\fB\\\\-\\\\-(r)exclude(\\\\-from)\\\\fR options\nand similarly for include variant. Any file excluded with eg \\\\-\\\\-exclude can\nbe put back with a pattern found in \\\\-\\\\-rinclude\\\\-from list.\n.PP\nRun s3cmd with \\\\fB\\\\-\\\\-dry\\\\-run\\\\fR to verify that your rules work as expected.\nUse together with \\\\fB\\\\-\\\\-debug\\\\fR get detailed information\nabout matching file names against exclude and include rules.\n.PP\nFor example to exclude all files with \\\".jpg\\\" extension except those beginning with a number use:\n.PP\n\t\\\\-\\\\-exclude '*.jpg' \\\\-\\\\-rinclude '[0\\\\-9].*\\\\.jpg'\n.PP\nTo exclude all files except \\\"*.jpg\\\" extension, use:\n.PP\n\t\\\\-\\\\-exclude '*' \\\\-\\\\-include '*.jpg'\n.PP\nTo exclude local directory 'somedir', be sure to use a trailing forward slash, as such:\n.PP\n\t\\\\-\\\\-exclude 'somedir/'\n.PP\n\n.SH SEE ALSO\nFor the most up to date list of options run:\n.B s3cmd \\\\-\\\\-help\n.br\nFor more info about usage, examples and other related info visit project homepage at:\n.B https://s3tools.org\n.SH AUTHOR\nWritten by Michal Ludvig, Florent Viard and contributors\n.SH CONTACT, SUPPORT\nPreferred way to get support is our mailing list:\n.br\n.I s3tools\\\\-general\\@lists.sourceforge.net\n.br\nor visit the project homepage:\n.br\n.B https://s3tools.org\n.SH REPORTING BUGS\nReport bugs to\n.I s3tools\\\\-bugs\\@lists.sourceforge.net\n.SH COPYRIGHT\nCopyright \\\\(co 2007\\\\-2023 TGRMN Software (https://www.tgrmn.com), Sodria SAS (https://www.sodria.com) and contributors\n.br\n.SH LICENSE\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n.br\n\";\n"
        },
        {
          "name": "magic",
          "type": "blob",
          "size": 1.666015625,
          "content": "# Additional magic for common web file types\n\n0\tstring/b\t{\\ \"\tJSON data\n!:mime application/json\n0\tstring/b\t{\\ }\tJSON data\n!:mime application/json\n0\tstring/b\t[\tJSON data\n!:mime application/json\n\n0\tsearch/4000\tfunction\n>&0\tsearch/32/b\t)\\ {\tJavaScript program\n!:mime application/javascript\n\n0\tsearch/4000\t@media\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000\t@import\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000\t@namespace\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ background\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ border\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ bottom\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ color\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ cursor\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ direction\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ display\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ float\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ font\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ height\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ left\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ line-\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ margin\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ padding\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ position\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ right\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ text-\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ top\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ width\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ visibility\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ -moz-\tCSS stylesheet\n!:mime text/css\n0\tsearch/4000/b\t{\\ -webkit-\tCSS stylesheet\n!:mime text/css\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0283203125,
          "content": "python-dateutil\npython-magic\n"
        },
        {
          "name": "run-tests.docker.README.md",
          "type": "blob",
          "size": 3.05078125,
          "content": "# Running test cases with Docker\n## (The  markdown formatting in this file is best viewed on Github)\n\n### tl;dr\n\n* Place a valid .s3cfg file in the root project directory.\n* `docker build -t s3cmd-tests --build-arg pyVersion=3.6 -f run-tests.dockerfile .`  \nNote the trailing period and substitute your desired Python version as needed.\n* `docker run --rm s3cmd-tests`\n\n### More Details\n\nThe included run-tests.dockerfile allows contributors to easily test their changes with Python versions that aren't installed locally.\n\nDocker must, of course, be installed on your system if it is not already. See https://docs.docker.com/install/ for instructions.\n\nTo begin, build the Dockerfile into an image for the Python version you wish to test against.  The build must be repeated whenever your source changes, but the Python image itself will be cached.  To build:\n\n* Place a valid .s3cfg file in the root project directory.  While .s3cfg has been added to the .gitignore to avoid sending your credentials to public repositories, you should still make sure you remove it when your testing is complete.\n\n* Run `docker build -t s3cmd-tests -f run-tests.dockerfile .` (the trailing period is required)\n\n  This will:\n\n  * Download the latest Python Docker image\n  * Add a testuser group and account\n  * Copy the .s3cfg into the user's home directory (/home/testuser)\n  * Copy the entire project folder into /home/testuser/src/s3cmd\n  * Install s3cmd dependencies (as root)\n\nThe main purpose of this Dockerfile is to allow you to run with multiple Python versions.  To see the Docker Python images available, visit [Docker Hub](https://hub.docker.com/_/python).  Most of the Linux variants should be usable, but the \"alpine\" variants will result in the smallest downloads and images.  For example:\n\n`docker build -t s3cmd-tests --build-arg pyVersion=3.8.1-alpine3.11 -f run-tests.dockerfile .`\n\nAfter successfully building the image, you can run it with `docker run --rm s3cmd-tests`.  This will execute the run-tests.py script in the Docker container with your .s3cfg credentials.\n\nNormal `run-tests.py` options may appended.  For example:\n\n`docker run --rm s3cmd-tests --bucket-prefix mytests`\n\nAdditional notes:\n\n* If you would like to enter a shell in the container, use `docker run -i -t --rm --entrypoint sh s3cmd-tests`.\n  * `bash` may be specified if you are using a Python image that supports it (not Alpine).\n* If it has been a few days since your last usage, you should check for updates to the upstream Python docker image using `docker pull python` or `docker pull python:3.7` (substituting your desired version)\n* Rebuilding does not over-write a previous image, but instead creates a new image and \"untags\" the previous one.  Use `docker images` to show all the images on your system, and `docker image prune` to cleanup unused, untagged images.  Please use this command carefully if you have other Docker images on your system.\n* When testing is completed, remove unused Python images with `docker rmi python:3.7`, substituting the tag/version you wish to remove. `docker images` will list the images on your system.\n"
        },
        {
          "name": "run-tests.dockerfile",
          "type": "blob",
          "size": 0.5166015625,
          "content": "ARG pyVersion=latest\nFROM python:${pyVersion}\nARG pyVersion\nRUN addgroup testuser \\\n  && adduser \\\n     --home /home/testuser \\\n     --ingroup testuser \\\n     --disabled-password \\\n     --gecos \"\" \\\n     testuser\n\nUSER testuser\nRUN mkdir /home/testuser/src\nWORKDIR /home/testuser/src\nCOPY --chown=testuser ./ s3cmd\nCOPY --chown=testuser .s3cfg /home/testuser/\nUSER root\nWORKDIR /home/testuser/src/s3cmd\nRUN pip install .\nUSER testuser\n\nENTRYPOINT [\"python\",\"run-tests.py\"]\n\nRUN echo Built with Python version $(python --version)\n"
        },
        {
          "name": "run-tests.py",
          "type": "blob",
          "size": 38.9013671875,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n## --------------------------------------------------------------------\n## Amazon S3cmd - testsuite\n##\n## Authors   : Michal Ludvig <michal@logix.cz> (https://www.logix.cz/michal)\n##             Florent Viard <florent@sodria.com> (https://www.sodria.com)\n## Copyright : TGRMN Software, Sodria SAS and contributors\n## License   : GPL Version 2\n## Website   : https://s3tools.org\n## --------------------------------------------------------------------\n\nfrom __future__ import absolute_import, print_function\n\nimport sys\nimport os\nimport re\nimport time\nfrom subprocess import Popen, PIPE, STDOUT\nimport locale\nimport getpass\nimport S3.Exceptions\nimport S3.Config\nfrom S3.ExitCodes import *\n\ntry:\n    unicode\nexcept NameError:\n    # python 3 support\n    # In python 3, unicode -> str, and str -> bytes\n    unicode = str\n\nALLOWED_SERVER_PROFILES = ['aws', 'minio']\n\ncount_pass = 0\ncount_fail = 0\ncount_skip = 0\n\ntest_counter = 0\nrun_tests = []\nexclude_tests = []\n\nverbose = False\n\nencoding = locale.getpreferredencoding()\nif not encoding:\n    print(\"Guessing current system encoding failed. Consider setting $LANG variable.\")\n    sys.exit(1)\nelse:\n    print(\"System encoding: \" + encoding)\n\ntry:\n    unicode\nexcept NameError:\n    # python 3 support\n    # In python 3, unicode -> str, and str -> bytes\n    unicode = str\n\ndef unicodise(string, encoding = \"utf-8\", errors = \"replace\"):\n    \"\"\"\n    Convert 'string' to Unicode or raise an exception.\n    Config can't use toolbox from Utils that is itself using Config\n    \"\"\"\n    if type(string) == unicode:\n        return string\n\n    try:\n        return unicode(string, encoding, errors)\n    except UnicodeDecodeError:\n        raise UnicodeDecodeError(\"Conversion to unicode failed: %r\" % string)\n\n# https://stackoverflow.com/questions/377017/test-if-executable-exists-in-python/377028#377028\ndef which(program):\n    def is_exe(fpath):\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\n    fpath, fname = os.path.split(program)\n    if fpath:\n        if is_exe(program):\n            return program\n    else:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            path = path.strip('\"')\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                return exe_file\n\n    return None\n\nif which('curl') is not None:\n    have_curl = True\nelse:\n    have_curl = False\n\nconfig_file = None\nif os.getenv(\"HOME\"):\n    config_file = os.path.join(unicodise(os.getenv(\"HOME\"), encoding),\n                               \".s3cfg\")\nelif os.name == \"nt\" and os.getenv(\"USERPROFILE\"):\n    config_file = os.path.join(\n        unicodise(os.getenv(\"USERPROFILE\"), encoding),\n        os.getenv(\"APPDATA\") and unicodise(os.getenv(\"APPDATA\"), encoding)\n        or 'Application Data',\n        \"s3cmd.ini\")\n\n\n## Unpack testsuite/ directory\nif not os.path.isdir('testsuite') and os.path.isfile('testsuite.tar.gz'):\n    os.system(\"tar -xz -f testsuite.tar.gz\")\nif not os.path.isdir('testsuite'):\n    print(\"Something went wrong while unpacking testsuite.tar.gz\")\n    sys.exit(1)\n\nos.system(\"tar -xf testsuite/checksum.tar -C testsuite\")\nif not os.path.isfile('testsuite/checksum/cksum33.txt'):\n    print(\"Something went wrong while unpacking testsuite/checksum.tar\")\n    sys.exit(1)\n\n## Fix up permissions for permission-denied tests\nos.chmod(\"testsuite/permission-tests/permission-denied-dir\", 0o444)\nos.chmod(\"testsuite/permission-tests/permission-denied.txt\", 0o000)\n\n## Patterns for Unicode tests\npatterns = {}\npatterns['UTF-8'] = u\"ŪņЇЌœđЗ/☺ unicode € rocks ™\"\npatterns['GBK'] = u\"12月31日/1-特色條目\"\n\nhave_encoding = os.path.isdir('testsuite/encodings/' + encoding)\nif not have_encoding and os.path.isfile('testsuite/encodings/%s.tar.gz' % encoding):\n    os.system(\"tar xvz -C testsuite/encodings -f testsuite/encodings/%s.tar.gz\" % encoding)\n    have_encoding = os.path.isdir('testsuite/encodings/' + encoding)\n\nif have_encoding:\n    #enc_base_remote = \"%s/xyz/%s/\" % (pbucket(1), encoding)\n    enc_pattern = patterns[encoding]\nelse:\n    print(encoding + \" specific files not found.\")\n\ndef unicodise(string):\n    if type(string) == unicode:\n        return string\n\n    return unicode(string, \"UTF-8\", \"replace\")\n\ndef deunicodise(string):\n    if type(string) != unicode:\n        return string\n\n    return string.encode(\"UTF-8\", \"replace\")\n\nif not os.path.isdir('testsuite/crappy-file-name'):\n    os.system(\"tar xvz -C testsuite -f testsuite/crappy-file-name.tar.gz\")\n    # TODO: also unpack if the tarball is newer than the directory timestamp\n    #       for instance when a new version was pulled from SVN.\n\ndef test(label, cmd_args = [], retcode = 0, must_find = [], must_not_find = [],\n         must_find_re = [], must_not_find_re = [], stdin = None,\n         skip_if_profile = None, skip_if_not_profile = None):\n    def command_output():\n        print(\"----\")\n        print(\" \".join([\" \" in arg and \"'%s'\" % arg or arg for arg in cmd_args]))\n        print(\"----\")\n        print(stdout)\n        print(\"----\")\n\n    def failure(message = \"\"):\n        global count_fail\n        if message:\n            message = u\"  (%r)\" % message\n        print(u\"\\x1b[31;1mFAIL%s\\x1b[0m\" % (message))\n        count_fail += 1\n        command_output()\n        #return 1\n        sys.exit(1)\n    def success(message = \"\"):\n        global count_pass\n        if message:\n            message = \"  (%r)\" % message\n        print(\"\\x1b[32;1mOK\\x1b[0m%s\" % (message))\n        count_pass += 1\n        if verbose:\n            command_output()\n        return 0\n    def skip(message = \"\"):\n        global count_skip\n        if message:\n            message = \"  (%r)\" % message\n        print(\"\\x1b[33;1mSKIP\\x1b[0m%s\" % (message))\n        count_skip += 1\n        return 0\n    def compile_list(_list, regexps = False):\n        if regexps == False:\n            _list = [re.escape(item) for item in _list]\n\n        return [re.compile(item, re.MULTILINE) for item in _list]\n\n    global test_counter\n    test_counter += 1\n    print((\"%3d  %s \" % (test_counter, label)).ljust(30, \".\"), end=' ')\n    sys.stdout.flush()\n\n    if run_tests.count(test_counter) == 0 or exclude_tests.count(test_counter) > 0:\n        return skip()\n\n    if not cmd_args:\n        return skip()\n\n    if skip_if_profile and server_profile in skip_if_profile:\n        return skip()\n\n    if skip_if_not_profile and server_profile not in skip_if_not_profile:\n        return skip()\n\n    p = Popen(cmd_args, stdin = stdin, stdout = PIPE, stderr = STDOUT, universal_newlines = True, close_fds = True)\n    stdout, stderr = p.communicate()\n    if type(retcode) not in [list, tuple]:\n        retcode = [retcode]\n    if p.returncode not in retcode:\n        return failure(\"retcode: %d, expected one of: %s\" % (p.returncode, retcode))\n\n    if type(must_find) not in [ list, tuple ]: must_find = [must_find]\n    if type(must_find_re) not in [ list, tuple ]: must_find_re = [must_find_re]\n    if type(must_not_find) not in [ list, tuple ]: must_not_find = [must_not_find]\n    if type(must_not_find_re) not in [ list, tuple ]: must_not_find_re = [must_not_find_re]\n\n    find_list = []\n    find_list.extend(compile_list(must_find))\n    find_list.extend(compile_list(must_find_re, regexps = True))\n    find_list_patterns = []\n    find_list_patterns.extend(must_find)\n    find_list_patterns.extend(must_find_re)\n\n    not_find_list = []\n    not_find_list.extend(compile_list(must_not_find))\n    not_find_list.extend(compile_list(must_not_find_re, regexps = True))\n    not_find_list_patterns = []\n    not_find_list_patterns.extend(must_not_find)\n    not_find_list_patterns.extend(must_not_find_re)\n\n    for index in range(len(find_list)):\n        stdout = unicodise(stdout)\n        match = find_list[index].search(stdout)\n        if not match:\n            return failure(\"pattern not found: %s\" % find_list_patterns[index])\n    for index in range(len(not_find_list)):\n        match = not_find_list[index].search(stdout)\n        if match:\n            return failure(\"pattern found: %s (match: %s)\" % (not_find_list_patterns[index], match.group(0)))\n\n    return success()\n\ndef test_s3cmd(label, cmd_args = [], **kwargs):\n    if not cmd_args[0].endswith(\"s3cmd\"):\n        cmd_args.insert(0, \"python\")\n        cmd_args.insert(1, \"s3cmd\")\n        if config_file:\n            cmd_args.insert(2, \"-c\")\n            cmd_args.insert(3, config_file)\n\n    return test(label, cmd_args, **kwargs)\n\ndef test_mkdir(label, dir_name):\n    if os.name in (\"posix\", \"nt\"):\n        cmd = ['mkdir', '-p']\n    else:\n        print(\"Unknown platform: %s\" % os.name)\n        sys.exit(1)\n    cmd.append(dir_name)\n    return test(label, cmd)\n\ndef test_rmdir(label, dir_name):\n    if os.path.isdir(dir_name):\n        if os.name == \"posix\":\n            cmd = ['rm', '-rf']\n        elif os.name == \"nt\":\n            cmd = ['rmdir', '/s/q']\n        else:\n            print(\"Unknown platform: %s\" % os.name)\n            sys.exit(1)\n        cmd.append(dir_name)\n        return test(label, cmd)\n    else:\n        return test(label, [])\n\ndef test_flushdir(label, dir_name):\n    test_rmdir(label + \"(rm)\", dir_name)\n    return test_mkdir(label + \"(mk)\", dir_name)\n\ndef test_copy(label, src_file, dst_file):\n    if os.name == \"posix\":\n        cmd = ['cp', '-f']\n    elif os.name == \"nt\":\n        cmd = ['copy']\n    else:\n        print(\"Unknown platform: %s\" % os.name)\n        sys.exit(1)\n    cmd.append(src_file)\n    cmd.append(dst_file)\n    return test(label, cmd)\n\ndef test_curl_HEAD(label, src_file, **kwargs):\n    cmd = ['curl', '--silent', '--head', '--include', '--location']\n    cmd.append(src_file)\n    return test(label, cmd, **kwargs)\n\nbucket_prefix = u\"%s-\" % getpass.getuser().lower()\nserver_profile = None\n\nargv = sys.argv[1:]\nwhile argv:\n    arg = argv.pop(0)\n    if arg.startswith('--bucket-prefix='):\n        print(\"Usage: '--bucket-prefix PREFIX', not '--bucket-prefix=PREFIX'\")\n        sys.exit(0)\n    if arg in (\"-h\", \"--help\"):\n        print(\"%s A B K..O -N\" % sys.argv[0])\n        print(\"Run tests number A, B and K through to O, except for N\")\n        sys.exit(0)\n\n    if arg in (\"-c\", \"--config\"):\n        config_file = argv.pop(0)\n        continue\n    if arg in (\"-l\", \"--list\"):\n        exclude_tests = range(0, 999)\n        break\n    if arg in (\"-v\", \"--verbose\"):\n        verbose = True\n        continue\n    if arg in (\"-p\", \"--bucket-prefix\"):\n        try:\n            bucket_prefix = argv.pop(0)\n        except IndexError:\n            print(\"Bucket prefix option must explicitly supply a bucket name prefix\")\n            sys.exit(0)\n        continue\n    if arg in (\"-s\", \"--server-profile\"):\n        try:\n            server_profile = argv.pop(0)\n            server_profile = server_profile.lower()\n        except IndexError:\n            print(\"Server profile option must explicitly supply a server profile name\")\n            sys.exit(0)\n        if server_profile not in ALLOWED_SERVER_PROFILES:\n            print(\"Server profile value must be one of %r\" % ALLOWED_SERVER_PROFILES)\n            sys.exit(0)\n        continue\n    if \"..\" in arg:\n        range_idx = arg.find(\"..\")\n        range_start = arg[:range_idx] or 0\n        range_end = arg[range_idx+2:] or 999\n        run_tests.extend(range(int(range_start), int(range_end) + 1))\n    elif arg.startswith(\"-\"):\n        exclude_tests.append(int(arg[1:]))\n    else:\n        run_tests.append(int(arg))\n\nprint(\"Using bucket prefix: '%s'\" % bucket_prefix)\n\ncfg = S3.Config.Config(config_file)\n\n# Autodetect server profile if not set:\nif server_profile is None:\n    if 's3.amazonaws.com' in cfg.host_base:\n        server_profile = 'aws'\nprint(\"Using server profile: '%s'\" % server_profile)\n\nif not run_tests:\n    run_tests = range(0, 999)\n\n# helper functions for generating bucket names\ndef bucket(tail):\n        '''Test bucket name'''\n        label = 'autotest'\n        if str(tail) == '3':\n                label = 'autotest'\n        return '%ss3cmd-%s-%s' % (bucket_prefix, label, tail)\n\ndef pbucket(tail):\n        '''Like bucket(), but prepends \"s3://\" for you'''\n        return 's3://' + bucket(tail)\n\n## ====== Remove test buckets\ntest_s3cmd(\"Remove test buckets\", ['rb', '-r', '--force', pbucket(1), pbucket(2), pbucket(3)])\n\n## ====== verify they were removed\ntest_s3cmd(\"Verify no test buckets\", ['ls'],\n           must_not_find = [pbucket(1), pbucket(2), pbucket(3)])\n\n\n## ====== Create one bucket (EU)\ntest_s3cmd(\"Create one bucket (EU)\", ['mb', '--bucket-location=EU', pbucket(1)],\n    must_find = \"Bucket '%s/' created\" % pbucket(1))\n\n\n## ====== Create multiple buckets\ntest_s3cmd(\"Create multiple buckets\", ['mb', pbucket(2), pbucket(3)],\n    must_find = [ \"Bucket '%s/' created\" % pbucket(2), \"Bucket '%s/' created\" % pbucket(3)])\n\n\n## ====== Invalid bucket name\ntest_s3cmd(\"Invalid bucket name\", [\"mb\", \"--bucket-location=EU\", pbucket('EU')],\n    retcode = EX_USAGE,\n    must_find = \"ERROR: Parameter problem: Bucket name '%s' contains disallowed character\" % bucket('EU'),\n    must_not_find_re = \"Bucket.*created\")\n\n\n## ====== Enable ACLs and public access to buckets\nfor idx, bpath in enumerate((pbucket(1), pbucket(2), pbucket(3))):\n    test_s3cmd(\"Enable ACLs for bucket %d\" % idx, ['setownership', bpath, 'ObjectWriter'],\n               must_find = \"%s/: Bucket Object Ownership updated\" % bpath,\n               skip_if_profile = ['minio'])\n\n    test_s3cmd(\"Disable Block Public Access for bucket %d\" % idx, ['setblockpublicaccess', bpath, ''],\n               must_find = \"%s/: Block Public Access updated\" % bpath,\n               skip_if_profile = ['minio'])\n\n\n## ====== Buckets list\ntest_s3cmd(\"Buckets list\", [\"ls\"],\n    must_find = [ pbucket(1), pbucket(2), pbucket(3) ], must_not_find_re = pbucket('EU'))\n\n## ====== Directory for cache\ntest_flushdir(\"Create cache dir\", \"testsuite/cachetest\")\n\n## ====== Sync to S3\ntest_s3cmd(\"Sync to S3\", ['sync', 'testsuite/', pbucket(1) + '/xyz/', '--exclude', 'demo/*', '--exclude', '*.png', '--no-encrypt', '--exclude-from', 'testsuite/exclude.encodings', '--exclude', 'testsuite/cachetest/.s3cmdcache', '--cache-file', 'testsuite/cachetest/.s3cmdcache'],\n           must_find = [\"ERROR: Upload of 'testsuite/permission-tests/permission-denied.txt' is not possible (Reason: Permission denied)\",\n                        \"WARNING: 32 non-printable characters replaced in: crappy-file-name/non-printables\",\n           ],\n           must_not_find_re = [\"demo/\", r\"^(?!WARNING: Skipping).*\\.png$\", \"permission-denied-dir\"],\n           retcode = EX_PARTIAL)\n\n## ====== Create new file and sync with caching enabled\ntest_mkdir(\"Create cache dir\", \"testsuite/cachetest/content\")\nif os.path.exists(\"testsuite/cachetest\"):\n    with open(\"testsuite/cachetest/content/testfile\", \"w\"):\n        pass\n\ntest_s3cmd(\"Sync to S3 with caching\", ['sync', 'testsuite/', pbucket(1) + '/xyz/', '--exclude', 'demo/*', '--exclude', '*.png', '--no-encrypt', '--exclude-from', 'testsuite/exclude.encodings', '--exclude', 'cachetest/.s3cmdcache', '--cache-file', 'testsuite/cachetest/.s3cmdcache' ],\n          must_find = \"upload: 'testsuite/cachetest/content/testfile' -> '%s/xyz/cachetest/content/testfile'\" % pbucket(1),\n          must_not_find = \"upload 'testsuite/cachetest/.s3cmdcache'\",\n          retcode = EX_PARTIAL)\n\n## ====== Remove content and retry cached sync with --delete-removed\ntest_rmdir(\"Remove local file\", \"testsuite/cachetest/content\")\n\ntest_s3cmd(\"Sync to S3 and delete removed with caching\", ['sync', 'testsuite/', pbucket(1) + '/xyz/', '--exclude', 'demo/*', '--exclude', '*.png', '--no-encrypt', '--exclude-from', 'testsuite/exclude.encodings', '--exclude', 'testsuite/cachetest/.s3cmdcache', '--cache-file', 'testsuite/cachetest/.s3cmdcache', '--delete-removed'],\n          must_find = \"delete: '%s/xyz/cachetest/content/testfile'\" % pbucket(1),\n          must_not_find = \"dictionary changed size during iteration\",\n          retcode = EX_PARTIAL)\n\n## ====== Remove cache directory and file\ntest_rmdir(\"Remove cache dir\", \"testsuite/cachetest\")\n\n\n## ====== Test empty directories\ntest_mkdir(\"Create empty dir\", \"testsuite/blahBlah/dirtest/emptydir\")\n\ntest_s3cmd(\"Sync to S3 empty dir without keep dir\", ['sync', 'testsuite/blahBlah', pbucket(1) + '/withoutdirs/', '--exclude', 'demo/*', '--exclude', '*.png', '--no-encrypt', '--exclude-from', 'testsuite/exclude.encodings'],\n          #must_find = \"upload: 'testsuite/cachetest/content/testfile' -> '%s/xyz/cachetest/content/testfile'\" % pbucket(1),\n          must_not_find = \"upload: 'testsuite/blahBlah/dirtest/emptydir'\")\n\ntest_s3cmd(\"Sync to S3 empty dir with keep dir\", ['sync', 'testsuite/blahBlah', pbucket(1) + '/withdirs/', '--exclude', 'demo/*', '--exclude', '*.png', '--no-encrypt', '--exclude-from', 'testsuite/exclude.encodings', '--keep-dirs'],\n          #must_find = \"upload: 'testsuite/cachetest/content/testfile' -> '%s/xyz/cachetest/content/testfile'\" % pbucket(1),\n          must_find = \"upload: 'testsuite/blahBlah/dirtest/emptydir'\")\n\n## ====== Remove cache directory and file\ntest_rmdir(\"Remove cache dir\", \"testsuite/blahBlah/dirtest\")\n\n\nif have_encoding:\n    ## ====== Sync UTF-8 / GBK / ... to S3\n    test_s3cmd(u\"Sync %s to S3\" % encoding, ['sync', 'testsuite/encodings/' + encoding, '%s/xyz/encodings/' % pbucket(1), '--exclude', 'demo/*', '--no-encrypt' ],\n        must_find = [ u\"'testsuite/encodings/%(encoding)s/%(pattern)s' -> '%(pbucket)s/xyz/encodings/%(encoding)s/%(pattern)s'\" % { 'encoding' : encoding, 'pattern' : enc_pattern , 'pbucket' : pbucket(1)} ])\n\n\n## ====== List bucket content\ntest_s3cmd(\"List bucket content\", ['ls', '%s/xyz/' % pbucket(1) ],\n    must_find_re = [ u\"DIR +%s/xyz/binary/$\" % pbucket(1) , u\"DIR +%s/xyz/etc/$\" % pbucket(1) ],\n    must_not_find = [ u\"random-crap.md5\", u\"/demo\" ])\n\n\n## ====== List bucket recursive\nmust_find = [ u\"%s/xyz/binary/random-crap.md5\" % pbucket(1) ]\nif have_encoding:\n    must_find.append(u\"%(pbucket)s/xyz/encodings/%(encoding)s/%(pattern)s\" % { 'encoding' : encoding, 'pattern' : enc_pattern, 'pbucket' : pbucket(1) })\n\ntest_s3cmd(\"List bucket recursive\", ['ls', '--recursive', pbucket(1)],\n    must_find = must_find,\n    must_not_find = [ \"logo.png\" ])\n\n## ====== FIXME\ntest_s3cmd(\"Recursive put\", ['put', '--recursive', 'testsuite/etc', '%s/xyz/' % pbucket(1) ])\n\n\n## ====== Clean up local destination dir\ntest_flushdir(\"Clean testsuite-out/\", \"testsuite-out\")\n\n## ====== Put from stdin\nf = open('testsuite/single-file/single-file.txt', 'r')\ntest_s3cmd(\"Put from stdin\", ['put', '-', '%s/single-file/single-file.txt' % pbucket(1)],\n           must_find = [\"'<stdin>' -> '%s/single-file/single-file.txt'\" % pbucket(1)],\n           stdin = f)\nf.close()\n\n## ====== Multipart put\nos.system('mkdir -p testsuite-out')\nos.system('dd if=/dev/urandom of=testsuite-out/urandom.bin bs=1M count=16 > /dev/null 2>&1')\ntest_s3cmd(\"Put multipart\", ['put', '--multipart-chunk-size-mb=5', 'testsuite-out/urandom.bin', '%s/urandom.bin' % pbucket(1)],\n           must_not_find = ['abortmp'])\n\n## ====== Multipart put from stdin\nf = open('testsuite-out/urandom.bin', 'r')\ntest_s3cmd(\"Multipart large put from stdin\", ['put', '--multipart-chunk-size-mb=5', '-', '%s/urandom2.bin' % pbucket(1)],\n           must_find = ['%s/urandom2.bin' % pbucket(1)],\n           must_not_find = ['abortmp'],\n           stdin = f)\nf.close()\n\n## ====== Clean up local destination dir\ntest_flushdir(\"Clean testsuite-out/\", \"testsuite-out\")\n\n## ====== Moving things without trailing '/'\nos.system('dd if=/dev/urandom of=testsuite-out/urandom1.bin bs=1k count=1 > /dev/null 2>&1')\nos.system('dd if=/dev/urandom of=testsuite-out/urandom2.bin bs=1k count=1 > /dev/null 2>&1')\ntest_s3cmd(\"Put multiple files\", ['put', 'testsuite-out/urandom1.bin', 'testsuite-out/urandom2.bin', '%s/' % pbucket(1)],\n           must_find = [\"%s/urandom1.bin\" % pbucket(1), \"%s/urandom2.bin\" % pbucket(1)])\n\ntest_s3cmd(\"Move without '/'\", ['mv', '%s/urandom1.bin' % pbucket(1), '%s/urandom2.bin' % pbucket(1), '%s/dir' % pbucket(1)],\n           retcode = 64,\n           must_find = ['Destination must be a directory'])\n\ntest_s3cmd(\"Move recursive w/a '/'\",\n           ['-r', 'mv', '%s/dir1' % pbucket(1), '%s/dir2' % pbucket(1)],\n           retcode = 64,\n           must_find = ['Destination must be a directory'])\n\n## ====== Moving multiple files into directory with trailing '/'\nmust_find = [\"'%s/urandom1.bin' -> '%s/dir/urandom1.bin'\" % (pbucket(1),pbucket(1)), \"'%s/urandom2.bin' -> '%s/dir/urandom2.bin'\" % (pbucket(1),pbucket(1))]\nmust_not_find = [\"'%s/urandom1.bin' -> '%s/dir'\" % (pbucket(1),pbucket(1)), \"'%s/urandom2.bin' -> '%s/dir'\" % (pbucket(1),pbucket(1))]\ntest_s3cmd(\"Move multiple files\",\n           ['mv', '%s/urandom1.bin' % pbucket(1), '%s/urandom2.bin' % pbucket(1), '%s/dir/' % pbucket(1)],\n           must_find = must_find,\n           must_not_find = must_not_find)\n\n## ====== Clean up local destination dir\ntest_flushdir(\"Clean testsuite-out/\", \"testsuite-out\")\n\n## ====== Sync from S3\nmust_find = [ \"'%s/xyz/binary/random-crap.md5' -> 'testsuite-out/xyz/binary/random-crap.md5'\" % pbucket(1) ]\nif have_encoding:\n    must_find.append(u\"'%(pbucket)s/xyz/encodings/%(encoding)s/%(pattern)s' -> 'testsuite-out/xyz/encodings/%(encoding)s/%(pattern)s' \" % { 'encoding' : encoding, 'pattern' : enc_pattern, 'pbucket' : pbucket(1) })\ntest_s3cmd(\"Sync from S3\", ['sync', '%s/xyz' % pbucket(1), 'testsuite-out'],\n    must_find = must_find)\n\n\n## ====== Create 'emptydirtests' test directories\ntest_rmdir(\"Create 'emptytests/withoutdirs'\", \"testsuite-out/emptytests/withoutdirs/\")\ntest_rmdir(\"Create 'emptytests/withdirs/'\", \"testsuite-out/emptytests/withdirs/\")\n\ntest_s3cmd(\"Sync from S3 no empty dir\", ['sync', '%s/withoutdirs/' % pbucket(1), 'testsuite-out/emptytests/withoutdirs/'],\n    must_not_find = [\"mkdir: '%s/withoutdirs/blahBlah/dirtest/emptydir/'\" % pbucket(1)])\n\ntest_s3cmd(\"Sync from S3 with empty dir\", ['sync', '%s/withdirs/' % pbucket(1), 'testsuite-out/emptytests/withdirs/'],\n    must_find = [\"mkdir: '%s/withdirs/blahBlah/dirtest/emptydir/'\" % pbucket(1)])\n\n## ====== Remove 'emptydirtests' directory\ntest_rmdir(\"Remove 'emptytests/'\", \"testsuite-out/emptytests/\")\n\n\n## ====== Remove 'demo' directory\ntest_rmdir(\"Remove 'dir-test/'\", \"testsuite-out/xyz/dir-test/\")\n\n\n## ====== Create dir with name of a file\ntest_mkdir(\"Create file-dir dir\", \"testsuite-out/xyz/dir-test/file-dir\")\n\n\n## ====== Skip dst dirs\ntest_s3cmd(\"Skip over dir\", ['sync', '%s/xyz' % pbucket(1), 'testsuite-out'],\n           must_find = \"ERROR: Download of 'xyz/dir-test/file-dir' failed (Reason: testsuite-out/xyz/dir-test/file-dir is a directory)\",\n           retcode = EX_PARTIAL)\n\n\n## ====== Clean up local destination dir\ntest_flushdir(\"Clean testsuite-out/\", \"testsuite-out\")\n\n\n## ====== Put public, guess MIME\ntest_s3cmd(\"Put public, guess MIME\", ['put', '--guess-mime-type', '--acl-public', 'testsuite/etc/logo.png', '%s/xyz/etc/logo.png' % pbucket(1)],\n    must_find = [ \"-> '%s/xyz/etc/logo.png'\" % pbucket(1) ])\n\n\n## ====== Retrieve from URL\nif have_curl:\n    test_curl_HEAD(\"Retrieve from URL\", 'http://%s.%s/xyz/etc/logo.png' % (bucket(1), cfg.host_base),\n                   must_find_re = ['Content-Length: 22059'],\n                   skip_if_profile = ['minio'])\n\n## ====== Change ACL to Private\ntest_s3cmd(\"Change ACL to Private\", ['setacl', '--acl-private', '%s/xyz/etc/l*.png' % pbucket(1)],\n           must_find = [ \"logo.png: ACL set to Private\" ],\n           skip_if_profile = ['minio'])\n\n\n## ====== Verify Private ACL\nif have_curl:\n    test_curl_HEAD(\"Verify Private ACL\", 'http://%s.%s/xyz/etc/logo.png' % (bucket(1), cfg.host_base),\n                   must_find_re = [ '403 Forbidden' ],\n                   skip_if_profile = ['minio'])\n\n\n## ====== Change ACL to Public\ntest_s3cmd(\"Change ACL to Public\", ['setacl', '--acl-public', '--recursive', '%s/xyz/etc/' % pbucket(1) , '-v'],\n           must_find = [ \"logo.png: ACL set to Public\" ],\n           skip_if_profile = ['minio'])\n\n\n## ====== Verify Public ACL\nif have_curl:\n    test_curl_HEAD(\"Verify Public ACL\", 'http://%s.%s/xyz/etc/logo.png' % (bucket(1), cfg.host_base),\n                   must_find_re = [ '200 OK', 'Content-Length: 22059'],\n                   skip_if_profile = ['minio'])\n\n\n## ====== Sync more to S3\ntest_s3cmd(\"Sync more to S3\", ['sync', 'testsuite/', 's3://%s/xyz/' % bucket(1), '--no-encrypt' ],\n           must_find = [ \"'testsuite/demo/some-file.xml' -> '%s/xyz/demo/some-file.xml' \" % pbucket(1) ],\n           must_not_find = [ \"'testsuite/etc/linked.png' -> '%s/xyz/etc/linked.png'\" % pbucket(1) ],\n           retcode = EX_PARTIAL)\n\n\n## ====== Don't check MD5 sum on Sync\ntest_copy(\"Change file cksum1.txt\", \"testsuite/checksum/cksum2.txt\", \"testsuite/checksum/cksum1.txt\")\ntest_copy(\"Change file cksum33.txt\", \"testsuite/checksum/cksum2.txt\", \"testsuite/checksum/cksum33.txt\")\ntest_s3cmd(\"Don't check MD5\", ['sync', 'testsuite/', 's3://%s/xyz/' % bucket(1), '--no-encrypt', '--no-check-md5'],\n           must_find = [ \"cksum33.txt\" ],\n           must_not_find = [ \"cksum1.txt\" ],\n           retcode = EX_PARTIAL)\n\n\n## ====== Check MD5 sum on Sync\ntest_s3cmd(\"Check MD5\", ['sync', 'testsuite/', 's3://%s/xyz/' % bucket(1), '--no-encrypt', '--check-md5'],\n           must_find = [ \"cksum1.txt\" ],\n           retcode = EX_PARTIAL)\n\n\n## ====== Rename within S3\ntest_s3cmd(\"Rename within S3\", ['mv', '%s/xyz/etc/logo.png' % pbucket(1), '%s/xyz/etc2/Logo.PNG' % pbucket(1)],\n    must_find = [ \"move: '%s/xyz/etc/logo.png' -> '%s/xyz/etc2/Logo.PNG'\" % (pbucket(1), pbucket(1))])\n\n\n## ====== Rename (NoSuchKey)\ntest_s3cmd(\"Rename (NoSuchKey)\", ['mv', '%s/xyz/etc/logo.png' % pbucket(1), '%s/xyz/etc2/Logo.PNG' % pbucket(1)],\n    retcode = EX_NOTFOUND,\n    must_find_re = [ 'Key not found' ],\n    must_not_find = [ \"move: '%s/xyz/etc/logo.png' -> '%s/xyz/etc2/Logo.PNG'\" % (pbucket(1), pbucket(1)) ])\n\n## ====== Sync more from S3 (invalid src)\ntest_s3cmd(\"Sync more from S3 (invalid src)\", ['sync', '--delete-removed', '%s/xyz/DOESNOTEXIST' % pbucket(1), 'testsuite-out'],\n    must_not_find = [ \"delete: 'testsuite-out/logo.png'\" ])\n\n## ====== Sync more from S3\ntest_s3cmd(\"Sync more from S3\", ['sync', '--delete-removed', '%s/xyz' % pbucket(1), 'testsuite-out'],\n    must_find = [ \"'%s/xyz/etc2/Logo.PNG' -> 'testsuite-out/xyz/etc2/Logo.PNG'\" % pbucket(1),\n                  \"'%s/xyz/demo/some-file.xml' -> 'testsuite-out/xyz/demo/some-file.xml'\" % pbucket(1) ],\n    must_not_find_re = [ \"not-deleted.*etc/logo.png\", \"delete: 'testsuite-out/logo.png'\" ])\n\n\n## ====== Make dst dir for get\ntest_rmdir(\"Remove dst dir for get\", \"testsuite-out\")\n\n\n## ====== Get multiple files\ntest_s3cmd(\"Get multiple files\", ['get', '%s/xyz/etc2/Logo.PNG' % pbucket(1), '%s/xyz/etc/AtomicClockRadio.ttf' % pbucket(1), 'testsuite-out'],\n    retcode = EX_USAGE,\n    must_find = [ 'Destination must be a directory or stdout when downloading multiple sources.' ])\n\n## ====== put/get non-ASCII filenames\ntest_s3cmd(\"Put unicode filenames\", ['put', u'testsuite/encodings/UTF-8/ŪņЇЌœđЗ/Žůžo',  u'%s/xyz/encodings/UTF-8/ŪņЇЌœđЗ/Žůžo' % pbucket(1)],\n           retcode = 0,\n           must_find = [ '->' ])\n\n\n## ====== Make dst dir for get\ntest_mkdir(\"Make dst dir for get\", \"testsuite-out\")\n\n\n## ====== put/get non-ASCII filenames\ntest_s3cmd(\"Get unicode filenames\", ['get', u'%s/xyz/encodings/UTF-8/ŪņЇЌœđЗ/Žůžo' % pbucket(1), 'testsuite-out'],\n           retcode = 0,\n           must_find = [ '->' ])\n\n\n## ====== Get multiple files\ntest_s3cmd(\"Get multiple files\", ['get', '%s/xyz/etc2/Logo.PNG' % pbucket(1), '%s/xyz/etc/AtomicClockRadio.ttf' % pbucket(1), 'testsuite-out'],\n    must_find = [ u\"-> 'testsuite-out/Logo.PNG'\",\n                  u\"-> 'testsuite-out/AtomicClockRadio.ttf'\" ])\n\n## ====== Upload files differing in capitalisation\ntest_s3cmd(\"blah.txt / Blah.txt\", ['put', '-r', 'testsuite/blahBlah', pbucket(1)],\n    must_find = [ '%s/blahBlah/Blah.txt' % pbucket(1), '%s/blahBlah/blah.txt' % pbucket(1)])\n\n## ====== Copy between buckets\ntest_s3cmd(\"Copy between buckets\", ['cp', '%s/xyz/etc2/Logo.PNG' % pbucket(1), '%s/xyz/etc2/logo.png' % pbucket(3)],\n    must_find = [ \"remote copy: '%s/xyz/etc2/Logo.PNG' -> '%s/xyz/etc2/logo.png'\" % (pbucket(1), pbucket(3)) ])\n\n## ====== Recursive copy\ntest_s3cmd(\"Recursive copy, set ACL\", ['cp', '-r', '--acl-public', '%s/xyz/' % pbucket(1), '%s/copy/' % pbucket(2), '--exclude', 'demo/dir?/*.txt', '--exclude', 'non-printables*'],\n    must_find = [ \"remote copy: '%s/xyz/etc2/Logo.PNG' -> '%s/copy/etc2/Logo.PNG'\" % (pbucket(1), pbucket(2)),\n                  \"remote copy: '%s/xyz/blahBlah/Blah.txt' -> '%s/copy/blahBlah/Blah.txt'\" % (pbucket(1), pbucket(2)),\n                  \"remote copy: '%s/xyz/blahBlah/blah.txt' -> '%s/copy/blahBlah/blah.txt'\" % (pbucket(1), pbucket(2)) ],\n    must_not_find = [ \"demo/dir1/file1-1.txt\" ])\n\n## ====== Verify ACL and MIME type\ntest_s3cmd(\"Verify ACL and MIME type\", ['info', '%s/copy/etc2/Logo.PNG' % pbucket(2) ],\n    must_find_re = [ \"MIME type:.*image/png\",\n                     r\"ACL:.*\\*anon\\*: READ\",\n                     \"URL:.*https?://%s.%s/copy/etc2/Logo.PNG\" % (bucket(2), cfg.host_base) ],\n           skip_if_profile = ['minio'])\n\n# Minio does not support ACL checks\ntest_s3cmd(\"Verify MIME type\", ['info', '%s/copy/etc2/Logo.PNG' % pbucket(2) ],\n           must_find_re = [\"MIME type:.*image/png\"],\n           skip_if_not_profile = ['minio'])\n\n## ====== modify MIME type\ntest_s3cmd(\"Modify MIME type\", ['modify', '--mime-type=binary/octet-stream', '%s/copy/etc2/Logo.PNG' % pbucket(2) ])\n\ntest_s3cmd(\"Verify ACL and MIME type\", ['info', '%s/copy/etc2/Logo.PNG' % pbucket(2) ],\n    must_find_re = [ \"MIME type:.*binary/octet-stream\",\n                     r\"ACL:.*\\*anon\\*: READ\",\n                     \"URL:.*https?://%s.%s/copy/etc2/Logo.PNG\" % (bucket(2), cfg.host_base) ],\n           skip_if_profile = ['minio'])\n\n# Minio does not support ACL checks\ntest_s3cmd(\"Verify MIME type\", ['info', '%s/copy/etc2/Logo.PNG' % pbucket(2) ],\n           must_find_re = [\"MIME type:.*binary/octet-stream\"],\n           skip_if_not_profile = ['minio'])\n\n## ====== reset MIME type\n\ntest_s3cmd(\"Modify MIME type back\", ['modify', '--mime-type=image/png', '%s/copy/etc2/Logo.PNG' % pbucket(2) ])\n\ntest_s3cmd(\"Verify ACL and MIME type\", ['info', '%s/copy/etc2/Logo.PNG' % pbucket(2) ],\n    must_find_re = [ \"MIME type:.*image/png\",\n                     r\"ACL:.*\\*anon\\*: READ\",\n                     \"URL:.*https?://%s.%s/copy/etc2/Logo.PNG\" % (bucket(2), cfg.host_base) ],\n           skip_if_profile = ['minio'])\n\n# Minio does not support ACL checks\ntest_s3cmd(\"Verify MIME type\", ['info', '%s/copy/etc2/Logo.PNG' % pbucket(2) ],\n           must_find_re = [\"MIME type:.*image/png\"],\n           skip_if_not_profile = ['minio'])\n\ntest_s3cmd(\"Add cache-control header\", ['modify', '--add-header=cache-control: max-age=3600, public', '%s/copy/etc2/Logo.PNG' % pbucket(2) ],\n    must_find_re = [ \"modify: .*\" ])\n\nif have_curl:\n    test_curl_HEAD(\"HEAD check Cache-Control present\", 'http://%s.%s/copy/etc2/Logo.PNG' % (bucket(2), cfg.host_base),\n                   must_find_re = [ \"Cache-Control: max-age=3600\" ],\n                   skip_if_profile = ['minio'])\n\ntest_s3cmd(\"Remove cache-control header\", ['modify', '--remove-header=cache-control', '%s/copy/etc2/Logo.PNG' % pbucket(2) ],\n           must_find_re = [ \"modify: .*\" ])\n\nif have_curl:\n    test_curl_HEAD(\"HEAD check Cache-Control not present\", 'http://%s.%s/copy/etc2/Logo.PNG' % (bucket(2), cfg.host_base),\n                   must_not_find_re = [ \"Cache-Control: max-age=3600\" ],\n                   skip_if_profile = ['minio'])\n\n## ====== sign\ntest_s3cmd(\"sign string\", ['sign', 's3cmd'], must_find_re = [\"Signature:\"])\ntest_s3cmd(\"signurl time\", ['signurl', '%s/copy/etc2/Logo.PNG' % pbucket(2), str(int(time.time()) + 60)], must_find_re = [\"http://\"])\ntest_s3cmd(\"signurl time offset\", ['signurl', '%s/copy/etc2/Logo.PNG' % pbucket(2), '+60'], must_find_re = [\"https?://\"])\ntest_s3cmd(\"signurl content disposition and type\", ['signurl', '%s/copy/etc2/Logo.PNG' % pbucket(2), '+60', '--content-disposition=inline; filename=video.mp4', '--content-type=video/mp4'], must_find_re = [ 'response-content-disposition', 'response-content-type' ] )\n\n## ====== Rename within S3\ntest_s3cmd(\"Rename within S3\", ['mv', '%s/copy/etc2/Logo.PNG' % pbucket(2), '%s/copy/etc/logo.png' % pbucket(2)],\n    must_find = [ \"move: '%s/copy/etc2/Logo.PNG' -> '%s/copy/etc/logo.png'\" % (pbucket(2), pbucket(2))])\n\n## ====== Sync between buckets\ntest_s3cmd(\"Sync remote2remote\", ['sync', '%s/xyz/' % pbucket(1), '%s/copy/' % pbucket(2), '--delete-removed', '--exclude', 'non-printables*'],\n    must_find = [ \"remote copy: '%s/xyz/demo/dir1/file1-1.txt' -> '%s/copy/demo/dir1/file1-1.txt'\" % (pbucket(1), pbucket(2)),\n                  \"remote copy: 'etc/logo.png' -> 'etc2/Logo.PNG'\",\n                  \"delete: '%s/copy/etc/logo.png'\" % pbucket(2) ],\n    must_not_find = [ \"blah.txt\" ])\n\n## ====== Exclude directory\ntest_s3cmd(\"Exclude directory\", ['put', '-r', 'testsuite/demo/', pbucket(1) + '/xyz/demo/', '--exclude', 'dir1/', '-d'],\n           must_find = [\"'testsuite/demo/dir2/file2-1.bin' -> '%s/xyz/demo/dir2/file2-1.bin'\" % pbucket(1),\n                        \"DEBUG: EXCLUDE: 'testsuite/demo/dir1/'\"],  # whole directory is excluded\n           must_not_find = [\"'testsuite/demo/dir1/file1-1.txt' -> '%s/xyz/demo/dir1/file1-1.txt'\" % pbucket(1),\n                            \"DEBUG: EXCLUDE: 'dir1/file1-1.txt'\"  # file is not synced, but also single file is not excluded\n                           ])\n\n## ====== Don't Put symbolic link\ntest_s3cmd(\"Don't put symbolic links\", ['put', 'testsuite/etc/linked1.png', 's3://%s/xyz/' % bucket(1),],\n           retcode = EX_USAGE,\n           must_find = [\"WARNING: Skipping over symbolic link: testsuite/etc/linked1.png\"],\n           must_not_find_re = [\"^(?!WARNING: Skipping).*linked1.png\"])\n\n## ====== Put symbolic link\ntest_s3cmd(\"Put symbolic links\", ['put', 'testsuite/etc/linked1.png', 's3://%s/xyz/' % bucket(1),'--follow-symlinks' ],\n           must_find = [ \"'testsuite/etc/linked1.png' -> '%s/xyz/linked1.png'\" % pbucket(1)])\n\n## ====== Sync symbolic links\ntest_s3cmd(\"Sync symbolic links\", ['sync', 'testsuite/', 's3://%s/xyz/' % bucket(1), '--no-encrypt', '--follow-symlinks' ],\n    must_find = [\"remote copy: 'etc2/Logo.PNG' -> 'etc/linked.png'\"],\n           # Don't want to recursively copy linked directories!\n           must_not_find_re = [\"etc/more/linked-dir/more/give-me-more.txt\",\n                               \"etc/brokenlink.png\"],\n           retcode = EX_PARTIAL)\n\n## ====== Multi source move\ntest_s3cmd(\"Multi-source move\", ['mv', '-r', '%s/copy/blahBlah/Blah.txt' % pbucket(2), '%s/copy/etc/' % pbucket(2), '%s/moved/' % pbucket(2)],\n    must_find = [ \"move: '%s/copy/blahBlah/Blah.txt' -> '%s/moved/Blah.txt'\" % (pbucket(2), pbucket(2)),\n                  \"move: '%s/copy/etc/AtomicClockRadio.ttf' -> '%s/moved/AtomicClockRadio.ttf'\" % (pbucket(2), pbucket(2)),\n                  \"move: '%s/copy/etc/TypeRa.ttf' -> '%s/moved/TypeRa.ttf'\" % (pbucket(2), pbucket(2)) ],\n    must_not_find = [ \"blah.txt\" ])\n\n## ====== Verify move\ntest_s3cmd(\"Verify move\", ['ls', '-r', pbucket(2)],\n    must_find = [ \"%s/moved/Blah.txt\" % pbucket(2),\n                  \"%s/moved/AtomicClockRadio.ttf\" % pbucket(2),\n                  \"%s/moved/TypeRa.ttf\" % pbucket(2),\n                  \"%s/copy/blahBlah/blah.txt\" % pbucket(2) ],\n    must_not_find = [ \"%s/copy/blahBlah/Blah.txt\" % pbucket(2),\n                      \"%s/copy/etc/AtomicClockRadio.ttf\" % pbucket(2),\n                      \"%s/copy/etc/TypeRa.ttf\" % pbucket(2) ])\n\n## ====== List all\ntest_s3cmd(\"List all\", ['la'],\n           must_find = [ \"%s/urandom.bin\" % pbucket(1)])\n\n## ====== Simple delete\ntest_s3cmd(\"Simple delete\", ['del', '%s/xyz/etc2/Logo.PNG' % pbucket(1)],\n    must_find = [ \"delete: '%s/xyz/etc2/Logo.PNG'\" % pbucket(1) ])\n\n## ====== Simple delete with rm\ntest_s3cmd(\"Simple delete with rm\", ['rm', '%s/xyz/test_rm/TypeRa.ttf' % pbucket(1)],\n    must_find = [ \"delete: '%s/xyz/test_rm/TypeRa.ttf'\" % pbucket(1) ])\n\n## ====== Create expiration rule with days and prefix\ntest_s3cmd(\"Create expiration rule with days and prefix\", ['expire', pbucket(1), '--expiry-days=365', '--expiry-prefix=log/'],\n    must_find = [ \"Bucket '%s/': expiration configuration is set.\" % pbucket(1)])\n\n## ====== Create expiration rule with date and prefix\ntest_s3cmd(\"Create expiration rule with date and prefix\", ['expire', pbucket(1), '--expiry-date=2030-12-31T00:00:00.000Z', '--expiry-prefix=log/'],\n    must_find = [ \"Bucket '%s/': expiration configuration is set.\" % pbucket(1)])\n\n## ====== Create expiration rule with days only\ntest_s3cmd(\"Create expiration rule with days only\", ['expire', pbucket(1), '--expiry-days=365'],\n    must_find = [ \"Bucket '%s/': expiration configuration is set.\" % pbucket(1)])\n\n## ====== Create expiration rule with date only\ntest_s3cmd(\"Create expiration rule with date only\", ['expire', pbucket(1), '--expiry-date=2030-12-31T00:00:00.000Z'],\n    must_find = [ \"Bucket '%s/': expiration configuration is set.\" % pbucket(1)])\n\n## ====== Get current expiration setting\ntest_s3cmd(\"Get current expiration setting\", ['info', pbucket(1)],\n    must_find_re = [ \"Expiration Rule: all objects in this bucket will expire in '2030-12-31T00:00:00(?:.000)?Z'\"])\n\n## ====== Delete expiration rule\ntest_s3cmd(\"Delete expiration rule\", ['expire', pbucket(1)],\n    must_find = [ \"Bucket '%s/': expiration configuration is deleted.\" % pbucket(1)])\n\n## ====== set Requester Pays flag\ntest_s3cmd(\"Set requester pays\", ['payer', '--requester-pays', pbucket(2)],\n           skip_if_profile=['minio'])\n\n## ====== get Requester Pays flag\ntest_s3cmd(\"Get requester pays flag\", ['info', pbucket(2)],\n    must_find = [ \"Payer:     Requester\"],\n           skip_if_profile=['minio'])\n\n## ====== ls using Requester Pays flag\ntest_s3cmd(\"ls using requester pays flag\", ['ls', '--requester-pays', pbucket(2)],\n           skip_if_profile=['minio'])\n\n## ====== clear Requester Pays flag\ntest_s3cmd(\"Clear requester pays\", ['payer', pbucket(2)],\n           skip_if_profile=['minio'])\n\n## ====== get Requester Pays flag\ntest_s3cmd(\"Get requester pays flag\", ['info', pbucket(2)],\n    must_find = [ \"Payer:     BucketOwner\"],\n           skip_if_profile=['minio'])\n\n## ====== Recursive delete maximum exceed\ntest_s3cmd(\"Recursive delete maximum exceeded\", ['del', '--recursive', '--max-delete=1', '--exclude', 'Atomic*', '%s/xyz/etc' % pbucket(1)],\n    must_not_find = [ \"delete: '%s/xyz/etc/TypeRa.ttf'\" % pbucket(1) ])\n\n## ====== Recursive delete\ntest_s3cmd(\"Recursive delete\", ['del', '--recursive', '--exclude', 'Atomic*', '%s/xyz/etc' % pbucket(1)],\n    must_find = [ \"delete: '%s/xyz/etc/TypeRa.ttf'\" % pbucket(1) ],\n    must_find_re = [ \"delete: '.*/etc/logo.png'\" ],\n    must_not_find = [ \"AtomicClockRadio.ttf\" ])\n\n## ====== Recursive delete with rm\ntest_s3cmd(\"Recursive delete with rm\", ['rm', '--recursive', '--exclude', 'Atomic*', '%s/xyz/test_rm' % pbucket(1)],\n    must_find = [ \"delete: '%s/xyz/test_rm/more/give-me-more.txt'\" % pbucket(1) ],\n    must_find_re = [ \"delete: '.*/test_rm/logo.png'\" ],\n    must_not_find = [ \"AtomicClockRadio.ttf\" ])\n\n## ====== Recursive delete all\ntest_s3cmd(\"Recursive delete all\", ['del', '--recursive', '--force', pbucket(1)],\n    must_find_re = [ \"delete: '.*binary/random-crap'\" ])\n\n## ====== Remove empty bucket\ntest_s3cmd(\"Remove empty bucket\", ['rb', pbucket(1)],\n    must_find = [ \"Bucket '%s/' removed\" % pbucket(1) ])\n\n## ====== Remove remaining buckets\ntest_s3cmd(\"Remove remaining buckets\", ['rb', '--recursive', pbucket(2), pbucket(3)],\n    must_find = [ \"Bucket '%s/' removed\" % pbucket(2),\n              \"Bucket '%s/' removed\" % pbucket(3) ])\n\n# vim:et:ts=4:sts=4:ai\n"
        },
        {
          "name": "s3cmd",
          "type": "blob",
          "size": 164.041015625,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n## --------------------------------------------------------------------\n## s3cmd - S3 client\n##\n## Authors   : Michal Ludvig <michal@logix.cz> (https://www.logix.cz/michal)\n##             Florent Viard <florent@sodria.com> (https://www.sodria.com)\n## Copyright : TGRMN Software, Sodria SAS and contributors\n## License   : GPL Version 2\n## Website   : https://s3tools.org\n## --------------------------------------------------------------------\n## This program is free software; you can redistribute it and/or modify\n## it under the terms of the GNU General Public License as published by\n## the Free Software Foundation; either version 2 of the License, or\n## (at your option) any later version.\n## This program is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n## GNU General Public License for more details.\n## --------------------------------------------------------------------\n\nfrom __future__ import absolute_import, print_function, division\n\nimport sys\n\nif sys.version_info < (2, 6):\n    sys.stderr.write(u\"ERROR: Python 2.6 or higher required, sorry.\\n\")\n    # 72 == EX_OSFILE\n    sys.exit(72)\n\nPY3 = (sys.version_info >= (3, 0))\n\nimport codecs\nimport errno\nimport glob\nimport io\nimport locale\nimport logging\nimport os\nimport re\nimport shutil\nimport socket\nimport subprocess\nimport tempfile\nimport datetime\nimport time\nimport traceback\n\nfrom copy import copy\nfrom optparse import OptionParser, Option, OptionValueError, IndentedHelpFormatter\nfrom logging import debug, info, warning, error\n\n\ntry:\n    import htmlentitydefs\nexcept Exception:\n    # python 3 support\n    import html.entities as htmlentitydefs\n\ntry:\n    unicode\nexcept NameError:\n    # python 3 support\n    # In python 3, unicode -> str, and str -> bytes\n    unicode = str\n\ntry:\n    unichr\nexcept NameError:\n    # python 3 support\n    # In python 3, unichr was removed as chr can now do the job\n    unichr = chr\n\ntry:\n    from shutil import which\nexcept ImportError:\n    # python2 fallback code\n    from distutils.spawn import find_executable as which\n\nif not PY3:\n    # ConnectionRefusedError does not exist in python2\n    class ConnectionError(OSError):\n        pass\n    class ConnectionRefusedError(ConnectionError):\n        pass\n\n\ndef output(message):\n    sys.stdout.write(message + \"\\n\")\n    sys.stdout.flush()\n\ndef check_args_type(args, type, verbose_type):\n    \"\"\"NOTE: This function looks like to not be used.\"\"\"\n    for arg in args:\n        if S3Uri(arg).type != type:\n            raise ParameterError(\"Expecting %s instead of '%s'\" % (verbose_type, arg))\n\ndef cmd_du(args):\n    s3 = S3(Config())\n    if len(args) > 0:\n        uri = S3Uri(args[0])\n        if uri.type == \"s3\" and uri.has_bucket():\n            subcmd_bucket_usage(s3, uri)\n            return EX_OK\n    subcmd_bucket_usage_all(s3)\n    return EX_OK\n\ndef subcmd_bucket_usage_all(s3):\n    \"\"\"\n    Returns: sum of bucket sizes as integer\n    Raises: S3Error\n    \"\"\"\n    cfg = Config()\n    response = s3.list_all_buckets()\n\n    buckets_size = 0\n    for bucket in response[\"list\"]:\n        size = subcmd_bucket_usage(s3, S3Uri(\"s3://\" + bucket[\"Name\"]))\n        if size != None:\n            buckets_size += size\n    total_size, size_coeff = formatSize(buckets_size, cfg.human_readable_sizes)\n    total_size_str = str(total_size) + size_coeff\n    output(u\"\".rjust(12, \"-\"))\n    output(u\"%s Total\" % (total_size_str.ljust(12)))\n    return buckets_size\n\ndef subcmd_bucket_usage(s3, uri):\n    \"\"\"\n    Returns: bucket size as integer\n    Raises: S3Error\n    \"\"\"\n    bucket_size = 0\n    object_count = 0\n    extra_info = u''\n\n    bucket = uri.bucket()\n    prefix = uri.object()\n    try:\n        for _, _, objects in s3.bucket_list_streaming(bucket, prefix=prefix, recursive=True):\n            for obj in objects:\n                bucket_size += int(obj[\"Size\"])\n                object_count += 1\n\n    except S3Error as e:\n        if e.info[\"Code\"] in S3.codes:\n            error(S3.codes[e.info[\"Code\"]] % bucket)\n        raise\n\n    except KeyboardInterrupt as e:\n        extra_info = u' [interrupted]'\n\n    total_size_str = u\"%d%s\" % formatSize(bucket_size,\n                                          Config().human_readable_sizes)\n    if Config().human_readable_sizes:\n        total_size_str = total_size_str.rjust(5)\n    else:\n        total_size_str = total_size_str.rjust(12)\n    output(u\"%s %7s objects %s%s\" % (total_size_str, object_count, uri,\n                                     extra_info))\n    return bucket_size\n\ndef cmd_ls(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    if len(args) > 0:\n        uri = S3Uri(args[0])\n        if uri.type == \"s3\" and uri.has_bucket():\n            subcmd_bucket_list(s3, uri, cfg.limit)\n            return EX_OK\n\n    # If not a s3 type uri or no bucket was provided, list all the buckets\n    subcmd_all_buckets_list(s3)\n    return EX_OK\n\ndef subcmd_all_buckets_list(s3):\n\n    response = s3.list_all_buckets()\n\n    for bucket in sorted(response[\"list\"], key=lambda b:b[\"Name\"]):\n        output(u\"%s  s3://%s\" % (formatDateTime(bucket[\"CreationDate\"]),\n                                 bucket[\"Name\"]))\n\ndef cmd_all_buckets_list_all_content(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    response = s3.list_all_buckets()\n\n    for bucket in response[\"list\"]:\n        subcmd_bucket_list(s3, S3Uri(\"s3://\" + bucket[\"Name\"]), cfg.limit)\n        output(u\"\")\n    return EX_OK\n\ndef subcmd_bucket_list(s3, uri, limit):\n    cfg = Config()\n\n    bucket = uri.bucket()\n    prefix = uri.object()\n\n    debug(u\"Bucket 's3://%s':\" % bucket)\n    if prefix.endswith('*'):\n        prefix = prefix[:-1]\n    try:\n        response = s3.bucket_list(bucket, prefix = prefix, limit = limit)\n    except S3Error as e:\n        if e.info[\"Code\"] in S3.codes:\n            error(S3.codes[e.info[\"Code\"]] % bucket)\n        raise\n\n    # md5 are 32 char long, but for multipart there could be a suffix\n    if Config().human_readable_sizes:\n        # %(size)5s%(coeff)1s\n        format_size = u\"%5d%1s\"\n        dir_str = u\"DIR\".rjust(6)\n        dirobj_str = u\"DIROBJ\".rjust(6)\n    else:\n        format_size = u\"%12d%s\"\n        dir_str = u\"DIR\".rjust(12)\n        dirobj_str = u\"DIROBJ\".rjust(12)\n    if cfg.long_listing:\n        format_string = u\"%(timestamp)16s %(size)s  %(md5)-35s  %(storageclass)-11s  %(uri)s\"\n    elif cfg.list_md5:\n        format_string = u\"%(timestamp)16s %(size)s  %(md5)-35s  %(uri)s\"\n    else:\n        format_string = u\"%(timestamp)16s %(size)s  %(uri)s\"\n\n    for prefix in response['common_prefixes']:\n        output(format_string % {\n            \"timestamp\": \"\",\n            \"size\": dir_str,\n            \"md5\": \"\",\n            \"storageclass\": \"\",\n            \"uri\": uri.compose_uri(bucket, prefix[\"Prefix\"])})\n\n    for object in response[\"list\"]:\n        md5 = object.get('ETag', '').strip('\"\\'')\n        storageclass = object.get('StorageClass','')\n        object_key = object['Key']\n\n        if cfg.list_md5:\n            if '-' in md5: # need to get md5 from the object\n                object_uri = uri.compose_uri(bucket, object_key)\n                info_response = s3.object_info(S3Uri(object_uri))\n                try:\n                    md5 = info_response['s3cmd-attrs']['md5']\n                except KeyError:\n                    pass\n\n        if object_key[-1] == '/':\n            size_str = dirobj_str\n        else:\n            size_and_coeff = formatSize(object[\"Size\"], Config().human_readable_sizes)\n            size_str = format_size % size_and_coeff\n\n        output(format_string % {\n            \"timestamp\": formatDateTime(object[\"LastModified\"]),\n            \"size\" : size_str,\n            \"md5\" : md5,\n            \"storageclass\" : storageclass,\n            \"uri\": uri.compose_uri(bucket, object_key),\n            })\n\n    if response[\"truncated\"]:\n        warning(u\"The list is truncated because the settings limit was reached.\")\n\ndef cmd_bucket_create(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    for arg in args:\n        uri = S3Uri(arg)\n        if not uri.type == \"s3\" or not uri.has_bucket() or uri.has_object():\n            raise ParameterError(\"Expecting S3 URI with just the bucket name set instead of '%s'\" % arg)\n        try:\n            response = s3.bucket_create(uri.bucket(), cfg.bucket_location, cfg.extra_headers)\n            output(u\"Bucket '%s' created\" % uri.uri())\n        except S3Error as e:\n            if e.info[\"Code\"] in S3.codes:\n                error(S3.codes[e.info[\"Code\"]] % uri.bucket())\n            raise\n    return EX_OK\n\ndef cmd_website_info(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    for arg in args:\n        uri = S3Uri(arg)\n        if not uri.type == \"s3\" or not uri.has_bucket() or uri.has_object():\n            raise ParameterError(\"Expecting S3 URI with just the bucket name set instead of '%s'\" % arg)\n        try:\n            response = s3.website_info(uri, cfg.bucket_location)\n            if response:\n                output(u\"Bucket %s: Website configuration\" % uri.uri())\n                output(u\"Website endpoint: %s\" % response['website_endpoint'])\n                output(u\"Index document:   %s\" % response['index_document'])\n                output(u\"Error document:   %s\" % response['error_document'])\n            else:\n                output(u\"Bucket %s: No website configuration found.\" % (uri.uri()))\n        except S3Error as e:\n            if e.info[\"Code\"] in S3.codes:\n                error(S3.codes[e.info[\"Code\"]] % uri.bucket())\n            raise\n    return EX_OK\n\ndef cmd_website_create(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    for arg in args:\n        uri = S3Uri(arg)\n        if not uri.type == \"s3\" or not uri.has_bucket() or uri.has_object():\n            raise ParameterError(\"Expecting S3 URI with just the bucket name set instead of '%s'\" % arg)\n        try:\n            response = s3.website_create(uri, cfg.bucket_location)\n            output(u\"Bucket '%s': website configuration created.\" % (uri.uri()))\n        except S3Error as e:\n            if e.info[\"Code\"] in S3.codes:\n                error(S3.codes[e.info[\"Code\"]] % uri.bucket())\n            raise\n    return EX_OK\n\ndef cmd_website_delete(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    for arg in args:\n        uri = S3Uri(arg)\n        if not uri.type == \"s3\" or not uri.has_bucket() or uri.has_object():\n            raise ParameterError(\"Expecting S3 URI with just the bucket name set instead of '%s'\" % arg)\n        try:\n            response = s3.website_delete(uri, cfg.bucket_location)\n            output(u\"Bucket '%s': website configuration deleted.\" % (uri.uri()))\n        except S3Error as e:\n            if e.info[\"Code\"] in S3.codes:\n                error(S3.codes[e.info[\"Code\"]] % uri.bucket())\n            raise\n    return EX_OK\n\ndef cmd_expiration_set(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    for arg in args:\n        uri = S3Uri(arg)\n        if not uri.type == \"s3\" or not uri.has_bucket() or uri.has_object():\n            raise ParameterError(\"Expecting S3 URI with just the bucket name set instead of '%s'\" % arg)\n        try:\n            response = s3.expiration_set(uri, cfg.bucket_location)\n            if response[\"status\"] == 200:\n                output(u\"Bucket '%s': expiration configuration is set.\" % (uri.uri()))\n            elif response[\"status\"] == 204:\n                output(u\"Bucket '%s': expiration configuration is deleted.\" % (uri.uri()))\n        except S3Error as e:\n            if e.info[\"Code\"] in S3.codes:\n                error(S3.codes[e.info[\"Code\"]] % uri.bucket())\n            raise\n    return EX_OK\n\ndef cmd_bucket_delete(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    def _bucket_delete_one(uri, retry=True):\n        try:\n            response = s3.bucket_delete(uri.bucket())\n            output(u\"Bucket '%s' removed\" % uri.uri())\n        except S3Error as e:\n            if e.info['Code'] == 'NoSuchBucket':\n                if cfg.force:\n                    return EX_OK\n                else:\n                    raise\n            if e.info['Code'] == 'BucketNotEmpty' and retry and (cfg.force or cfg.recursive):\n                warning(u\"Bucket is not empty. Removing all the objects from it first. This may take some time...\")\n                rc = subcmd_batch_del(uri_str = uri.uri())\n                if rc == EX_OK:\n                    return _bucket_delete_one(uri, False)\n                else:\n                    output(u\"Bucket was not removed\")\n            elif e.info[\"Code\"] in S3.codes:\n                error(S3.codes[e.info[\"Code\"]] % uri.bucket())\n            raise\n        return EX_OK\n\n    for arg in args:\n        uri = S3Uri(arg)\n        if not uri.type == \"s3\" or not uri.has_bucket() or uri.has_object():\n            raise ParameterError(\"Expecting S3 URI with just the bucket name set instead of '%s'\" % arg)\n        rc = _bucket_delete_one(uri)\n        if rc != EX_OK:\n            return rc\n    return EX_OK\n\ndef cmd_object_put(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    if len(args) == 0:\n        raise ParameterError(\"Nothing to upload. Expecting a local file or directory and a S3 URI destination.\")\n\n    ## Normalize URI to convert s3://bkt to s3://bkt/ (trailing slash)\n    destination_base_uri = S3Uri(args.pop())\n    if destination_base_uri.type != 's3':\n        raise ParameterError(\"Destination must be S3Uri. Got: %s\" % destination_base_uri)\n    destination_base = destination_base_uri.uri()\n\n    if len(args) == 0:\n        raise ParameterError(\"Nothing to upload. Expecting a local file or directory.\")\n\n    local_list, single_file_local, exclude_list, total_size_local = fetch_local_list(\n        args, is_src=True, with_dirs=cfg.keep_dirs)\n\n    local_count = len(local_list)\n\n    info(u\"Summary: %d local files to upload\" % local_count)\n\n    if local_count == 0:\n        raise ParameterError(\"Nothing to upload.\")\n\n    if local_count > 0:\n        if not single_file_local and '-' in local_list.keys():\n            raise ParameterError(\"Cannot specify multiple local files if uploading from '-' (ie stdin)\")\n        elif single_file_local and local_list.keys()[0] == \"-\" and destination_base.endswith(\"/\"):\n            raise ParameterError(\"Destination S3 URI must not end with '/' when uploading from stdin.\")\n        elif not destination_base.endswith(\"/\"):\n            if not single_file_local:\n                raise ParameterError(\"Destination S3 URI must end with '/' (ie must refer to a directory on the remote side).\")\n            local_list[local_list.keys()[0]]['remote_uri'] = destination_base\n        else:\n            for key in local_list:\n                local_list[key]['remote_uri'] = destination_base + key\n\n    if cfg.dry_run:\n        for key in exclude_list:\n            output(u\"exclude: %s\" % key)\n        for key in local_list:\n            if key != \"-\":\n                nicekey = local_list[key]['full_name']\n            else:\n                nicekey = \"<stdin>\"\n            output(u\"upload: '%s' -> '%s'\" % (nicekey, local_list[key]['remote_uri']))\n\n        warning(u\"Exiting now because of --dry-run\")\n        return EX_OK\n\n    seq = 0\n    ret = EX_OK\n    for key in local_list:\n        seq += 1\n\n        uri_final = S3Uri(local_list[key]['remote_uri'])\n        try:\n            src_md5 = local_list.get_md5(key)\n        except IOError:\n            src_md5 = None\n\n        extra_headers = copy(cfg.extra_headers)\n        full_name_orig = local_list[key]['full_name']\n        full_name = full_name_orig\n        seq_label = \"[%d of %d]\" % (seq, local_count)\n        if Config().encrypt:\n            gpg_exitcode, full_name, extra_headers[\"x-amz-meta-s3tools-gpgenc\"] = gpg_encrypt(full_name_orig)\n        attr_header = _build_attr_header(local_list[key], key, src_md5)\n        debug(u\"attr_header: %s\" % attr_header)\n        extra_headers.update(attr_header)\n        try:\n            response = s3.object_put(full_name, uri_final, extra_headers, extra_label = seq_label)\n        except S3UploadError as exc:\n            error(u\"Upload of '%s' failed too many times (Last reason: %s)\" % (full_name_orig, exc))\n            if cfg.stop_on_error:\n                ret = EX_DATAERR\n                error(u\"Exiting now because of --stop-on-error\")\n                break\n            ret = EX_PARTIAL\n            continue\n        except InvalidFileError as exc:\n            error(u\"Upload of '%s' is not possible (Reason: %s)\" % (full_name_orig, exc))\n            ret = EX_PARTIAL\n            if cfg.stop_on_error:\n                ret = EX_OSFILE\n                error(u\"Exiting now because of --stop-on-error\")\n                break\n            continue\n        if response is not None:\n            speed_fmt = formatSize(response[\"speed\"], human_readable = True, floating_point = True)\n            if not Config().progress_meter:\n                if full_name_orig != \"-\":\n                    nicekey = full_name_orig\n                else:\n                    nicekey = \"<stdin>\"\n                output(u\"upload: '%s' -> '%s' (%d bytes in %0.1f seconds, %0.2f %sB/s) %s\" %\n                       (nicekey, uri_final, response[\"size\"], response[\"elapsed\"],\n                        speed_fmt[0], speed_fmt[1], seq_label))\n        if Config().acl_public:\n            output(u\"Public URL of the object is: %s\" %\n                   (uri_final.public_url()))\n        if Config().encrypt and full_name != full_name_orig:\n            debug(u\"Removing temporary encrypted file: %s\" % full_name)\n            os.remove(deunicodise(full_name))\n    return ret\n\ndef cmd_object_get(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    ## Check arguments:\n    ## if not --recursive:\n    ##   - first N arguments must be S3Uri\n    ##   - if the last one is S3 make current dir the destination_base\n    ##   - if the last one is a directory:\n    ##       - take all 'basenames' of the remote objects and\n    ##         make the destination name be 'destination_base'+'basename'\n    ##   - if the last one is a file or not existing:\n    ##       - if the number of sources (N, above) == 1 treat it\n    ##         as a filename and save the object there.\n    ##       - if there's more sources -> Error\n    ## if --recursive:\n    ##   - first N arguments must be S3Uri\n    ##       - for each Uri get a list of remote objects with that Uri as a prefix\n    ##       - apply exclude/include rules\n    ##       - each list item will have MD5sum, Timestamp and pointer to S3Uri\n    ##         used as a prefix.\n    ##   - the last arg may be '-' (stdout)\n    ##   - the last arg may be a local directory - destination_base\n    ##   - if the last one is S3 make current dir the destination_base\n    ##   - if the last one doesn't exist check remote list:\n    ##       - if there is only one item and its_prefix==its_name\n    ##         download that item to the name given in last arg.\n    ##       - if there are more remote items use the last arg as a destination_base\n    ##         and try to create the directory (incl. all parents).\n    ##\n    ## In both cases we end up with a list mapping remote object names (keys) to local file names.\n\n    ## Each item will be a dict with the following attributes\n    # {'remote_uri', 'local_filename'}\n    download_list = []\n\n    if len(args) == 0:\n        raise ParameterError(\"Nothing to download. Expecting S3 URI.\")\n\n    if S3Uri(args[-1]).type == 'file':\n        destination_base = args.pop()\n    else:\n        destination_base = \".\"\n\n    if len(args) == 0:\n        raise ParameterError(\"Nothing to download. Expecting S3 URI.\")\n\n    try:\n        remote_list, exclude_list, remote_total_size = fetch_remote_list(\n            args, require_attribs = True)\n    except S3Error as exc:\n        if exc.code == 'NoSuchKey':\n            raise ParameterError(\"Source object '%s' does not exist.\" % exc.resource)\n        raise\n\n    remote_count = len(remote_list)\n\n    info(u\"Summary: %d remote files to download\" % remote_count)\n\n    if remote_count > 0:\n        if destination_base == \"-\":\n            ## stdout is ok for multiple remote files!\n            for key in remote_list:\n                remote_list[key]['local_filename'] = \"-\"\n        elif not os.path.isdir(deunicodise(destination_base)):\n            ## We were either given a file name (existing or not)\n            if remote_count > 1:\n                raise ParameterError(\"Destination must be a directory or stdout when downloading multiple sources.\")\n            remote_list[remote_list.keys()[0]]['local_filename'] = destination_base\n        else:\n            if destination_base[-1] != os.path.sep:\n                destination_base += os.path.sep\n            for key in remote_list:\n                local_filename = destination_base + key\n                if os.path.sep != \"/\":\n                    local_filename = os.path.sep.join(local_filename.split(\"/\"))\n                remote_obj = remote_list[key]\n                remote_obj['local_filename'] = local_filename\n\n    if cfg.dry_run:\n        for key in exclude_list:\n            output(u\"exclude: %s\" % key)\n        for key in remote_list:\n            output(u\"download: '%s' -> '%s'\" % (remote_list[key]['object_uri_str'], remote_list[key]['local_filename']))\n\n        warning(u\"Exiting now because of --dry-run\")\n        return EX_OK\n\n    seq = 0\n    ret = EX_OK\n    for key in remote_list:\n        seq += 1\n        item = remote_list[key]\n        uri = S3Uri(item['object_uri_str'])\n        ## Encode / Decode destination with \"replace\" to make sure it's compatible with current encoding\n        destination = unicodise_safe(item['local_filename'])\n        destination_bytes = deunicodise(destination)\n        last_modified_ts = item['timestamp']\n\n        seq_label = \"[%d of %d]\" % (seq, remote_count)\n\n        is_dir_obj = item['is_dir']\n\n        response = None\n        start_position = 0\n\n        if destination == \"-\":\n            ## stdout\n            dst_stream = io.open(sys.__stdout__.fileno(), mode='wb', closefd=False)\n            dst_stream.stream_name = u'<stdout>'\n            file_exists = True\n        elif is_dir_obj:\n            ## Folder\n            try:\n                file_exists = os.path.exists(destination_bytes)\n                if not file_exists:\n                    info(u\"Creating directory: %s\" % destination)\n                    os.makedirs(destination_bytes)\n            except IOError as e:\n                # If dir was created at the same time by a race condition, it is ok.\n                if e.errno != errno.EEXIST:\n                    error(u\"Creation of directory '%s' failed (Reason: %s)\"\n                        % (destination, e.strerror))\n                    if cfg.stop_on_error:\n                        error(u\"Exiting now because of --stop-on-error\")\n                        raise\n                    ret = EX_PARTIAL\n                    continue\n            if file_exists and not cfg.force:\n                # Directory already exists and we don't want to update metadata\n                continue\n            dst_stream = None\n        else:\n            ## File\n            try:\n                file_exists = os.path.exists(destination_bytes)\n                try:\n                    dst_stream = io.open(destination_bytes, mode='ab')\n                    dst_stream.stream_name = destination\n                except IOError as e:\n                    if e.errno != errno.ENOENT:\n                        raise\n                    dst_dir_bytes = os.path.dirname(destination)\n                    info(u\"Creating directory: %s\" % unicodise(dst_dir_bytes))\n                    os.makedirs(dst_dir_bytes)\n                    dst_stream = io.open(destination_bytes, mode='ab')\n                    dst_stream.stream_name = destination\n\n                if file_exists:\n                    force = False\n                    skip = False\n                    if cfg.get_continue:\n                        start_position = dst_stream.tell()\n                        item_size = item['size']\n                        if start_position == item_size:\n                            skip = True\n                        elif start_position > item_size:\n                            info(u\"Download forced for '%s' as source is \"\n                                 \"smaller than local file\" % destination)\n                            force = True\n                    elif cfg.force:\n                        force = True\n                    elif cfg.skip_existing:\n                        skip = True\n                    else:\n                        dst_stream.close()\n                        raise ParameterError(\n                            u\"File '%s' already exists. Use either of --force /\"\n                            \" --continue / --skip-existing or give it a new\"\n                            \" name.\" % destination\n                        )\n\n                    if skip:\n                        dst_stream.close()\n                        info(u\"Skipping over existing file: '%s'\" % destination)\n                        continue\n\n                    if force:\n                        start_position = 0\n                        dst_stream.seek(0)\n                        dst_stream.truncate()\n\n            except IOError as e:\n                error(u\"Creation of file '%s' failed (Reason: %s)\"\n                      % (destination, e.strerror))\n                if cfg.stop_on_error:\n                    error(u\"Exiting now because of --stop-on-error\")\n                    raise\n                ret = EX_PARTIAL\n                continue\n\n        try:\n            # Retrieve the file content\n            if dst_stream:\n                try:\n                    response = s3.object_get(uri, dst_stream, destination,\n                                             start_position=start_position,\n                                             extra_label=seq_label)\n                finally:\n                    dst_stream.close()\n        except S3DownloadError as e:\n            error(u\"Download of '%s' failed (Reason: %s)\" % (destination, e))\n            # Delete, only if file didn't exist before!\n            if not file_exists:\n                debug(u\"object_get failed for '%s', deleting...\" % (destination,))\n                os.unlink(destination_bytes)\n            if cfg.stop_on_error:\n                error(u\"Exiting now because of --stop-on-error\")\n                raise\n            ret = EX_PARTIAL\n            continue\n        except S3Error as e:\n            error(u\"Download of '%s' failed (Reason: %s)\" % (destination, e))\n            if not file_exists: # Delete, only if file didn't exist before!\n                debug(u\"object_get failed for '%s', deleting...\" % (destination,))\n                os.unlink(destination_bytes)\n            raise\n\n        \"\"\"\n        # TODO Enable once we add restoring s3cmd-attrs in get command\n        if is_dir_obj and cfg.preserve_attrs:\n            # Retrieve directory info to restore s3cmd-attrs metadata\n            try:\n                response = s3.object_info(uri)\n            except S3Error as exc:\n                error(u\"Retrieving directory metadata for '%s' failed (Reason: %s)\"\n                      % (destination, exc))\n                if cfg.stop_on_error:\n                    error(u\"Exiting now because of --stop-on-error\")\n                    raise\n                ret = EX_PARTIAL\n                continue\n        \"\"\"\n\n        if response:\n            if \"x-amz-meta-s3tools-gpgenc\" in response[\"headers\"]:\n                gpg_decrypt(destination, response[\"headers\"][\"x-amz-meta-s3tools-gpgenc\"])\n                response[\"size\"] = os.stat(destination_bytes)[6]\n            if \"last-modified\" in response[\"headers\"]:\n                last_modified_ts = time.mktime(time.strptime(response[\"headers\"][\"last-modified\"], \"%a, %d %b %Y %H:%M:%S GMT\"))\n\n        if last_modified_ts and destination != \"-\":\n            os.utime(destination_bytes, (last_modified_ts, last_modified_ts))\n            debug(\"set mtime to %s\" % last_modified_ts)\n        if not Config().progress_meter and destination != \"-\" and not is_dir_obj:\n            speed_fmt = formatSize(response[\"speed\"], human_readable = True, floating_point = True)\n            output(u\"download: '%s' -> '%s' (%d bytes in %0.1f seconds, %0.2f %sB/s)\" %\n                (uri, destination, response[\"size\"], response[\"elapsed\"], speed_fmt[0], speed_fmt[1]))\n        if Config().delete_after_fetch:\n            s3.object_delete(uri)\n            output(u\"File '%s' removed after fetch\" % (uri))\n    return ret\n\ndef cmd_object_del(args):\n    cfg = Config()\n    recursive = cfg.recursive\n    for uri_str in args:\n        uri = S3Uri(uri_str)\n        if uri.type != \"s3\":\n            raise ParameterError(\"Expecting S3 URI instead of '%s'\" % uri_str)\n        if not uri.has_object():\n            if recursive and not cfg.force:\n                raise ParameterError(\"Please use --force to delete ALL contents of %s\" % uri_str)\n            elif not recursive:\n                raise ParameterError(\"File name required, not only the bucket name. Alternatively use --recursive\")\n\n        if not recursive:\n            rc = subcmd_object_del_uri(uri_str)\n        elif cfg.exclude or cfg.include or cfg.max_delete > 0:\n            # subcmd_batch_del_iterative does not support file exclusion and can't\n            # accurately know how many total files will be deleted, so revert to batch delete.\n            rc = subcmd_batch_del(uri_str = uri_str)\n        else:\n            rc = subcmd_batch_del_iterative(uri_str = uri_str)\n        if not rc:\n            return rc\n    return EX_OK\n\ndef subcmd_batch_del_iterative(uri_str = None, bucket = None):\n    \"\"\" Streaming version of batch deletion (doesn't realize whole list in memory before deleting).\n\n    Differences from subcmd_batch_del:\n      - Does not obey --exclude directives or obey cfg.max_delete (use subcmd_batch_del in those cases)\n    \"\"\"\n    if bucket and uri_str:\n        raise ValueError(\"Pass only one of uri_str or bucket\")\n    if bucket: # bucket specified\n        uri_str = \"s3://%s\" % bucket\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(uri_str)\n    bucket = uri.bucket()\n\n    deleted_bytes = deleted_count = 0\n\n    for _, _, to_delete in s3.bucket_list_streaming(bucket, prefix=uri.object(), recursive=True):\n        if not to_delete:\n            continue\n        if not cfg.dry_run:\n            response = s3.object_batch_delete_uri_strs([uri.compose_uri(bucket, item['Key']) for item in to_delete])\n        deleted_bytes += sum(int(item[\"Size\"]) for item in to_delete)\n        deleted_count += len(to_delete)\n        output(u'\\n'.join(u\"delete: '%s'\" % uri.compose_uri(bucket, p['Key']) for p in to_delete))\n\n    if deleted_count:\n        # display summary data of deleted files\n        if cfg.stats:\n            stats_info = StatsInfo()\n            stats_info.files_deleted = deleted_count\n            stats_info.size_deleted = deleted_bytes\n            output(stats_info.format_output())\n        else:\n            total_size, size_coeff = formatSize(deleted_bytes, Config().human_readable_sizes)\n            total_size_str = str(total_size) + size_coeff\n            info(u\"Deleted %s objects (%s) from %s\" % (deleted_count, total_size_str, uri))\n    else:\n        warning(u\"Remote list is empty.\")\n\n    return EX_OK\n\ndef subcmd_batch_del(uri_str = None, bucket = None, remote_list = None):\n    \"\"\"\n    Returns: EX_OK\n    Raises: ValueError\n    \"\"\"\n    cfg = Config()\n    s3 = S3(cfg)\n    def _batch_del(remote_list):\n        to_delete = remote_list[:1000]\n        remote_list = remote_list[1000:]\n        while len(to_delete):\n            debug(u\"Batch delete %d, remaining %d\" % (len(to_delete), len(remote_list)))\n            if not cfg.dry_run:\n                response = s3.object_batch_delete(to_delete)\n            output(u'\\n'.join((u\"delete: '%s'\" % to_delete[p]['object_uri_str']) for p in to_delete))\n            to_delete = remote_list[:1000]\n            remote_list = remote_list[1000:]\n\n    if remote_list is not None and len(remote_list) == 0:\n        return False\n\n    if len([item for item in [uri_str, bucket, remote_list] if item]) != 1:\n        raise ValueError(\"One and only one of 'uri_str', 'bucket', 'remote_list' can be specified.\")\n\n    if bucket: # bucket specified\n        uri_str = \"s3://%s\" % bucket\n    if remote_list is None: # uri_str specified\n        remote_list, exclude_list, remote_total_size = fetch_remote_list(uri_str, require_attribs = False)\n\n    if len(remote_list) == 0:\n        warning(u\"Remote list is empty.\")\n        return EX_OK\n\n    if cfg.max_delete > 0 and len(remote_list) > cfg.max_delete:\n        warning(u\"delete: maximum requested number of deletes would be exceeded, none performed.\")\n        return EX_OK\n\n    _batch_del(remote_list)\n\n    if cfg.dry_run:\n        warning(u\"Exiting now because of --dry-run\")\n    return EX_OK\n\ndef subcmd_object_del_uri(uri_str, recursive = None):\n    \"\"\"\n    Returns: True if XXX, False if XXX\n    Raises: ValueError\n    \"\"\"\n    cfg = Config()\n    s3 = S3(cfg)\n\n    if recursive is None:\n        recursive = cfg.recursive\n\n    remote_list, exclude_list, remote_total_size = fetch_remote_list(uri_str, require_attribs = False, recursive = recursive)\n\n    remote_count = len(remote_list)\n\n    info(u\"Summary: %d remote files to delete\" % remote_count)\n    if cfg.max_delete > 0 and remote_count > cfg.max_delete:\n        warning(u\"delete: maximum requested number of deletes would be exceeded, none performed.\")\n        return False\n\n    if cfg.dry_run:\n        for key in exclude_list:\n            output(u\"exclude: %s\" % key)\n        for key in remote_list:\n            output(u\"delete: %s\" % remote_list[key]['object_uri_str'])\n\n        warning(u\"Exiting now because of --dry-run\")\n        return True\n\n    for key in remote_list:\n        item = remote_list[key]\n        response = s3.object_delete(S3Uri(item['object_uri_str']))\n        output(u\"delete: '%s'\" % item['object_uri_str'])\n    return True\n\ndef cmd_object_restore(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    if cfg.restore_days < 1:\n        raise ParameterError(\"You must restore a file for 1 or more days\")\n\n    # accept case-insensitive argument but fix it to match S3 API\n    if cfg.restore_priority.title() not in ['Standard', 'Expedited', 'Bulk']:\n        raise ParameterError(\"Valid restoration priorities: bulk, standard, expedited\")\n    else:\n        cfg.restore_priority = cfg.restore_priority.title()\n\n    remote_list, exclude_list, remote_total_size = fetch_remote_list(args, require_attribs = False, recursive = cfg.recursive)\n\n    remote_count = len(remote_list)\n\n    info(u\"Summary: Restoring %d remote files for %d days at %s priority\" % (remote_count, cfg.restore_days, cfg.restore_priority))\n\n    if cfg.dry_run:\n        for key in exclude_list:\n            output(u\"exclude: %s\" % key)\n        for key in remote_list:\n            output(u\"restore: '%s'\" % remote_list[key]['object_uri_str'])\n\n        warning(u\"Exiting now because of --dry-run\")\n        return EX_OK\n\n    ret = EX_OK\n\n    for key in remote_list:\n        item = remote_list[key]\n\n        uri = S3Uri(item['object_uri_str'])\n        if not item['object_uri_str'].endswith(\"/\"):\n            try:\n                response = s3.object_restore(S3Uri(item['object_uri_str']))\n                output(u\"restore: '%s'\" % item['object_uri_str'])\n            except S3Error as e:\n                if e.code in (\"RestoreAlreadyInProgress\", \"InvalidObjectState\"):\n                    warning(\"%s: %s\" % (e.message, item['object_uri_str']))\n                elif cfg.stop_on_error:\n                    raise e\n                else:\n                    error(\"restore failed for: '%s' (%s)\", item['object_uri_str'], e)\n                    ret = EX_PARTIAL\n\n        else:\n            debug(u\"Skipping directory since only files may be restored\")\n    return ret\n\n\ndef subcmd_cp_mv(args, process_fce, action_str, message):\n    cfg = Config()\n    if action_str == 'modify':\n        if len(args) < 1:\n            raise ParameterError(\"Expecting one or more S3 URIs for \"\n                                 + action_str)\n        destination_base = None\n    else:\n        if len(args) < 2:\n            raise ParameterError(\"Expecting two or more S3 URIs for \"\n                                 + action_str)\n        dst_base_uri = S3Uri(args.pop())\n        if dst_base_uri.type != \"s3\":\n            raise ParameterError(\"Destination must be S3 URI. To download a \"\n                                 \"file use 'get' or 'sync'.\")\n        destination_base = dst_base_uri.uri()\n\n    scoreboard = ExitScoreboard()\n\n    remote_list, exclude_list, remote_total_size = \\\n        fetch_remote_list(args, require_attribs=False)\n\n    remote_count = len(remote_list)\n\n    info(u\"Summary: %d remote files to %s\" % (remote_count, action_str))\n\n    if destination_base:\n        # Trying to mv dir1/ to dir2 will not pass a test in S3.FileLists,\n        # so we don't need to test for it here.\n        if not destination_base.endswith('/') \\\n           and (len(remote_list) > 1 or cfg.recursive):\n            raise ParameterError(\"Destination must be a directory and end with\"\n                                 \" '/' when acting on a folder content or on \"\n                                 \"multiple sources.\")\n\n        if cfg.recursive:\n            for key in remote_list:\n                remote_list[key]['dest_name'] = destination_base + key\n        else:\n            for key in remote_list:\n                if destination_base.endswith(\"/\"):\n                    remote_list[key]['dest_name'] = destination_base + key\n                else:\n                    remote_list[key]['dest_name'] = destination_base\n    else:\n        for key in remote_list:\n            remote_list[key]['dest_name'] = remote_list[key]['object_uri_str']\n\n    if cfg.dry_run:\n        for key in exclude_list:\n            output(u\"exclude: %s\" % key)\n        for key in remote_list:\n            output(u\"%s: '%s' -> '%s'\" % (action_str,\n                                          remote_list[key]['object_uri_str'],\n                                          remote_list[key]['dest_name']))\n\n        warning(u\"Exiting now because of --dry-run\")\n        return EX_OK\n\n    seq = 0\n    for key in remote_list:\n        seq += 1\n        seq_label = \"[%d of %d]\" % (seq, remote_count)\n\n        item = remote_list[key]\n        src_uri = S3Uri(item['object_uri_str'])\n        dst_uri = S3Uri(item['dest_name'])\n        src_size = item.get('size')\n\n        extra_headers = copy(cfg.extra_headers)\n        try:\n            response = process_fce(src_uri, dst_uri, extra_headers,\n                                   src_size=src_size,\n                                   extra_label=seq_label)\n            output(message % {\"src\": src_uri, \"dst\": dst_uri,\n                              \"extra\": seq_label})\n            if Config().acl_public:\n                info(u\"Public URL is: %s\" % dst_uri.public_url())\n            scoreboard.success()\n        except (S3Error, S3UploadError) as exc:\n            if isinstance(exc, S3Error) and exc.code == \"NoSuchKey\":\n                scoreboard.notfound()\n                warning(u\"Key not found %s\" % item['object_uri_str'])\n            else:\n                scoreboard.failed()\n                error(u\"Copy failed for: '%s' (%s)\", item['object_uri_str'],\n                      exc)\n\n            if cfg.stop_on_error:\n                break\n    return scoreboard.rc()\n\ndef cmd_cp(args):\n    s3 = S3(Config())\n    return subcmd_cp_mv(args, s3.object_copy, \"copy\",\n                        u\"remote copy: '%(src)s' -> '%(dst)s'  %(extra)s\")\n\ndef cmd_modify(args):\n    s3 = S3(Config())\n    return subcmd_cp_mv(args, s3.object_modify, \"modify\",\n                        u\"modify: '%(src)s'  %(extra)s\")\n\ndef cmd_mv(args):\n    s3 = S3(Config())\n    return subcmd_cp_mv(args, s3.object_move, \"move\",\n                        u\"move: '%(src)s' -> '%(dst)s'  %(extra)s\")\n\ndef cmd_info(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    while (len(args)):\n        uri_arg = args.pop(0)\n        uri = S3Uri(uri_arg)\n        if uri.type != \"s3\" or not uri.has_bucket():\n            raise ParameterError(\"Expecting S3 URI instead of '%s'\" % uri_arg)\n\n        try:\n            if uri.has_object():\n                info = s3.object_info(uri)\n                output(u\"%s (object):\" % uri.uri())\n                output(u\"   File size: %s\" % info['headers']['content-length'])\n                output(u\"   Last mod:  %s\" % info['headers']['last-modified'])\n                output(u\"   MIME type: %s\" % info['headers'].get('content-type', 'none'))\n                output(u\"   Storage:   %s\" % info['headers'].get('x-amz-storage-class', 'STANDARD'))\n                md5 = info['headers'].get('etag', '').strip('\"\\'')\n                try:\n                    md5 = info['s3cmd-attrs']['md5']\n                except KeyError:\n                    pass\n                output(u\"   MD5 sum:   %s\" % md5)\n                if 'x-amz-server-side-encryption' in info['headers']:\n                    output(u\"   SSE:       %s\" % info['headers']['x-amz-server-side-encryption'])\n                else:\n                    output(u\"   SSE:       none\")\n\n            else:\n                info = s3.bucket_info(uri)\n                output(u\"%s (bucket):\" % uri.uri())\n                output(u\"   Location:  %s\" % (info['bucket-location']\n                                              or 'none'))\n                output(u\"   Payer:     %s\" % (info['requester-pays']\n                                              or 'none'))\n                output(u\"   Ownership: %s\" % (info['ownership']\n                                              or 'none'))\n                output(u\"   Versioning:%s\" % (info['versioning']\n                                              or 'none'))\n\n                expiration = s3.expiration_info(uri, cfg.bucket_location)\n                if expiration and expiration['prefix'] is not None:\n                    expiration_desc = \"Expiration Rule: \"\n                    if expiration['prefix'] == \"\":\n                        expiration_desc += \"all objects in this bucket \"\n                    elif expiration['prefix'] is not None:\n                        expiration_desc += \"objects with key prefix '\" + expiration['prefix'] + \"' \"\n                    expiration_desc += \"will expire in '\"\n                    if expiration['days']:\n                        expiration_desc += expiration['days'] + \"' day(s) after creation\"\n                    elif expiration['date']:\n                        expiration_desc += expiration['date'] + \"' \"\n                    output(u\"   %s\" % expiration_desc)\n                else:\n                    output(u\"   Expiration rule: none\")\n\n                public_access_block = ','.join([\n                    key for key, val in info['public-access-block'].items()\n                    if val\n                ])\n                output(u\"   Block Public Access: %s\" % (public_access_block\n                                                        or 'none'))\n\n            try:\n                policy = s3.get_policy(uri)\n                output(u\"   Policy:    %s\" % policy)\n            except S3Error as exc:\n                # Ignore the exception and don't fail the info\n                # if the server doesn't support setting ACLs\n                if exc.status == 403:\n                    output(u\"   Policy:    Not available: GetPolicy permission is needed to read the policy\")\n                elif exc.status == 405:\n                    output(u\"   Policy:    Not available: Only the bucket owner can read the policy\")\n                elif exc.status not in [404, 501]:\n                    raise exc\n                else:\n                    output(u\"   Policy:    none\")\n\n            try:\n                cors = s3.get_cors(uri)\n                output(u\"   CORS:      %s\" % cors)\n            except S3Error as exc:\n                # Ignore the exception and don't fail the info\n                # if the server doesn't support setting ACLs\n                if exc.status not in [404, 501]:\n                    raise exc\n                output(u\"   CORS:      none\")\n\n            try:\n                acl = s3.get_acl(uri)\n                acl_grant_list = acl.getGrantList()\n                for grant in acl_grant_list:\n                    output(u\"   ACL:       %s: %s\" % (grant['grantee'], grant['permission']))\n                if acl.isAnonRead():\n                    output(u\"   URL:       %s\" % uri.public_url())\n            except S3Error as exc:\n                # Ignore the exception and don't fail the info\n                # if the server doesn't support setting ACLs\n                if exc.status not in [404, 501]:\n                    raise exc\n                else:\n                    output(u\"   ACL:       none\")\n\n            if uri.has_object():\n                # Temporary hack for performance + python3 compatibility\n                if PY3:\n                    info_headers_iter = info['headers'].items()\n                else:\n                    info_headers_iter = info['headers'].iteritems()\n                for header, value in info_headers_iter:\n                    if header.startswith('x-amz-meta-'):\n                        output(u\"   %s: %s\" % (header, value))\n\n        except S3Error as e:\n            if e.info[\"Code\"] in S3.codes:\n                error(S3.codes[e.info[\"Code\"]] % uri.bucket())\n            raise\n    return EX_OK\n\ndef filedicts_to_keys(*args):\n    keys = set()\n    for a in args:\n        keys.update(a.keys())\n    keys = list(keys)\n    keys.sort()\n    return keys\n\ndef cmd_sync_remote2remote(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    # Normalise s3://uri (e.g. assert trailing slash)\n    destination_base = S3Uri(args[-1]).uri()\n\n    destbase_with_source_list = set()\n    for source_arg in args[:-1]:\n        if source_arg.endswith('/'):\n            destbase_with_source_list.add(destination_base)\n        else:\n            destbase_with_source_list.add(s3path.join(\n                destination_base, s3path.basename(source_arg)\n            ))\n\n    stats_info = StatsInfo()\n\n    src_list, src_exclude_list, remote_total_size = fetch_remote_list(args[:-1], recursive = True, require_attribs = True)\n    dst_list, dst_exclude_list, _ = fetch_remote_list(destbase_with_source_list, recursive = True, require_attribs = True)\n\n    src_count = len(src_list)\n    orig_src_count = src_count\n    dst_count = len(dst_list)\n    deleted_count = 0\n\n    info(u\"Found %d source files, %d destination files\" % (src_count, dst_count))\n\n    src_list, dst_list, update_list, copy_pairs = compare_filelists(src_list, dst_list, src_remote = True, dst_remote = True)\n\n    src_count = len(src_list)\n    update_count = len(update_list)\n    dst_count = len(dst_list)\n\n    print(u\"Summary: %d source files to copy, %d files at destination to delete\" % (src_count + update_count, dst_count))\n\n    ### Populate 'target_uri' only if we've got something to sync from src to dst\n    for key in src_list:\n        src_list[key]['target_uri'] = destination_base + key\n    for key in update_list:\n        update_list[key]['target_uri'] = destination_base + key\n\n    if cfg.dry_run:\n        keys = filedicts_to_keys(src_exclude_list, dst_exclude_list)\n        for key in keys:\n            output(u\"exclude: %s\" % key)\n        if cfg.delete_removed:\n            for key in dst_list:\n                output(u\"delete: '%s'\" % dst_list[key]['object_uri_str'])\n        for key in src_list:\n            output(u\"remote copy: '%s' -> '%s'\" % (src_list[key]['object_uri_str'], src_list[key]['target_uri']))\n        for key in update_list:\n            output(u\"remote copy: '%s' -> '%s'\" % (update_list[key]['object_uri_str'], update_list[key]['target_uri']))\n        warning(u\"Exiting now because of --dry-run\")\n        return EX_OK\n\n    # if there are copy pairs, we can't do delete_before, on the chance\n    # we need one of the to-be-deleted files as a copy source.\n    if len(copy_pairs) > 0:\n        cfg.delete_after = True\n\n    if cfg.delete_removed and orig_src_count == 0 and len(dst_list) and not cfg.force:\n        warning(u\"delete: cowardly refusing to delete because no source files were found.  Use --force to override.\")\n        cfg.delete_removed = False\n\n    # Delete items in destination that are not in source\n    if cfg.delete_removed and not cfg.delete_after:\n        subcmd_batch_del(remote_list = dst_list)\n        deleted_count = len(dst_list)\n\n    def _upload(src_list, seq, src_count):\n        file_list = src_list.keys()\n        file_list.sort()\n        ret = EX_OK\n        total_nb_files = 0\n        total_size = 0\n        for file in file_list:\n            seq += 1\n            item = src_list[file]\n            src_uri = S3Uri(item['object_uri_str'])\n            dst_uri = S3Uri(item['target_uri'])\n            src_size = item.get('size')\n            seq_label = \"[%d of %d]\" % (seq, src_count)\n            extra_headers = copy(cfg.extra_headers)\n            try:\n                response = s3.object_copy(src_uri, dst_uri, extra_headers,\n                                          src_size=src_size,\n                                          extra_label=seq_label)\n                output(u\"remote copy: '%s' -> '%s'  %s\" %\n                       (src_uri, dst_uri, seq_label))\n                total_nb_files += 1\n                total_size += item.get(u'size', 0)\n            except (S3Error, S3UploadError) as exc:\n                ret = EX_PARTIAL\n                error(u\"File '%s' could not be copied: %s\", src_uri, exc)\n                if cfg.stop_on_error:\n                    raise\n        return ret, seq, total_nb_files, total_size\n\n    # Perform the synchronization of files\n    timestamp_start = time.time()\n    seq = 0\n    ret, seq, nb_files, size = _upload(src_list, seq, src_count + update_count)\n    total_files_copied = nb_files\n    total_size_copied = size\n\n    status, seq, nb_files, size = _upload(update_list, seq, src_count + update_count)\n    if ret == EX_OK:\n        ret = status\n    total_files_copied += nb_files\n    total_size_copied += size\n\n    n_copied, bytes_saved, failed_copy_files = remote_copy(\n        s3, copy_pairs, destination_base, None, False)\n    total_files_copied += n_copied\n    total_size_copied += bytes_saved\n\n    #process files not copied\n    debug(\"Process files that were not remotely copied\")\n    failed_copy_count = len(failed_copy_files)\n    for key in failed_copy_files:\n        failed_copy_files[key]['target_uri'] = destination_base + key\n    status, seq, nb_files, size = _upload(failed_copy_files, seq, src_count + update_count + failed_copy_count)\n    if ret == EX_OK:\n        ret = status\n    total_files_copied += nb_files\n    total_size_copied += size\n\n    # Delete items in destination that are not in source\n    if cfg.delete_removed and cfg.delete_after:\n        subcmd_batch_del(remote_list = dst_list)\n        deleted_count = len(dst_list)\n\n    stats_info.files = orig_src_count\n    stats_info.size = remote_total_size\n    stats_info.files_copied = total_files_copied\n    stats_info.size_copied = total_size_copied\n    stats_info.files_deleted = deleted_count\n\n    total_elapsed = max(1.0, time.time() - timestamp_start)\n    outstr = \"Done. Copied %d files in %0.1f seconds, %0.2f files/s.\" % (total_files_copied, total_elapsed, seq / total_elapsed)\n    if cfg.stats:\n        outstr += stats_info.format_output()\n        output(outstr)\n    elif seq > 0:\n        output(outstr)\n    else:\n        info(outstr)\n\n    return ret\n\ndef cmd_sync_remote2local(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    def _do_deletes(local_list):\n        total_size = 0\n        if cfg.max_delete > 0 and len(local_list) > cfg.max_delete:\n            warning(u\"delete: maximum requested number of deletes would be exceeded, none performed.\")\n            return total_size\n\n        # Reverse used to delete children before parent folders\n        for key in reversed(local_list):\n            item = local_list[key]\n            full_path = item['full_name']\n            if item.get('is_dir', True):\n                os.rmdir(deunicodise(full_path))\n            else:\n                os.unlink(deunicodise(full_path))\n            output(u\"delete: '%s'\" % full_path)\n            total_size += item.get(u'size', 0)\n        return len(local_list), total_size\n\n    destination_base = args[-1]\n    source_args = args[:-1]\n    fetch_source_args = args[:-1]\n\n    if not destination_base.endswith(os.path.sep):\n        if fetch_source_args[0].endswith(u'/') or len(fetch_source_args) > 1:\n            raise ParameterError(\"Destination must be a directory and end with '/' when downloading multiple sources.\")\n\n    stats_info = StatsInfo()\n\n    remote_list, src_exclude_list, remote_total_size = fetch_remote_list(fetch_source_args, recursive = True, require_attribs = True)\n\n\n    # - The source path is either like \"/myPath/my_src_folder\" and\n    # the user want to download this single folder and Optionally only delete\n    # things that have been removed inside this folder. For this case, we only\n    # have to look inside destination_base/my_src_folder and not at the root of\n    # destination_base.\n    # - Or like \"/myPath/my_src_folder/\" and the user want to have the sync\n    # with the content of this folder\n    destbase_with_source_list = set()\n    for source_arg in fetch_source_args:\n        if source_arg.endswith('/'):\n            if destination_base.endswith(os.path.sep):\n                destbase_with_source_list.add(destination_base)\n            else:\n                destbase_with_source_list.add(destination_base + os.path.sep)\n        else:\n            destbase_with_source_list.add(os.path.join(destination_base,\n                                                      os.path.basename(source_arg)))\n    # with_dirs is True, as we always want to compare source with the actual full local content\n    local_list, single_file_local, dst_exclude_list, local_total_size = fetch_local_list(\n        destbase_with_source_list, is_src=False, recursive=True, with_dirs=True\n    )\n\n    local_count = len(local_list)\n    remote_count = len(remote_list)\n    orig_remote_count = remote_count\n\n    info(u\"Found %d remote file objects, %d local files and directories\" % (remote_count, local_count))\n\n    remote_list, local_list, update_list, copy_pairs = compare_filelists(remote_list, local_list, src_remote = True, dst_remote = False)\n\n    dir_cache = {}\n\n    def _set_local_filename(remote_list, destination_base, source_args, dir_cache):\n        if len(remote_list) == 0:\n            return\n\n        if destination_base.endswith(os.path.sep):\n            if not os.path.exists(deunicodise(destination_base)):\n                if not cfg.dry_run:\n                    os.makedirs(deunicodise(destination_base))\n            if not os.path.isdir(deunicodise(destination_base)):\n                raise ParameterError(\"Destination is not an existing directory\")\n        elif len(remote_list) == 1 and \\\n             source_args[0] == remote_list[remote_list.keys()[0]].get(u'object_uri_str', ''):\n            if os.path.isdir(deunicodise(destination_base)):\n                raise ParameterError(\"Destination already exists and is a directory\")\n            remote_list[remote_list.keys()[0]]['local_filename'] = destination_base\n            return\n\n        if destination_base[-1] != os.path.sep:\n            destination_base += os.path.sep\n\n        for key in remote_list:\n            local_filename = destination_base + key\n            if os.path.sep != \"/\":\n                local_filename = os.path.sep.join(local_filename.split(\"/\"))\n\n            item = remote_list[key]\n            item['local_filename'] = local_filename\n\n            # Create parent folders if needed\n            # Extract key dirname\n            key_dir_path = key.rsplit('/', 1)[0]\n            dst_dir = None\n            if key_dir_path not in dir_cache:\n                if cfg.dry_run:\n                    mkdir_ret = True\n                else:\n                    dst_dir = unicodise(os.path.dirname(deunicodise(local_filename)))\n                    mkdir_ret = Utils.mkdir_with_parents(dst_dir)\n                # Also add to cache, all the parent dirs\n                path = key_dir_path\n                while path and path not in dir_cache:\n                    dir_cache[path] = mkdir_ret\n                    last_slash_idx = path.rfind('/')\n                    if last_slash_idx in [-1, 0]:\n                        break\n                    path = path[:last_slash_idx]\n            if dir_cache[key_dir_path] == False:\n                if not dst_dir:\n                    dst_dir = unicodise(os.path.dirname(deunicodise(local_filename)))\n                if cfg.stop_on_error:\n                    error(u\"Exiting now because of --stop-on-error\")\n                    raise OSError(\"Download of '%s' failed (Reason: %s destination directory is not writable)\" % (key, dst_dir))\n                error(u\"Download of '%s' failed (Reason: %s destination directory is not writable)\" % (key, dst_dir))\n                item['mark_failed'] = True\n                ret = EX_PARTIAL\n                continue\n\n    _set_local_filename(remote_list, destination_base, source_args, dir_cache)\n    _set_local_filename(update_list, destination_base, source_args, dir_cache)\n\n    local_count = len(local_list)\n    remote_count = len(remote_list)\n    update_count = len(update_list)\n    copy_pairs_count = len(copy_pairs)\n\n    info(u\"Summary: %d remote files to download, %d local files to delete, %d local files to hardlink\" % (remote_count + update_count, local_count, copy_pairs_count))\n\n    if cfg.dry_run:\n        keys = filedicts_to_keys(src_exclude_list, dst_exclude_list)\n        for key in keys:\n            output(u\"exclude: %s\" % key)\n        if cfg.delete_removed:\n            for key in local_list:\n                output(u\"delete: '%s'\" % local_list[key]['full_name'])\n        for key in remote_list:\n            output(u\"download: '%s' -> '%s'\" % (remote_list[key]['object_uri_str'], remote_list[key]['local_filename']))\n        for key in update_list:\n            output(u\"download: '%s' -> '%s'\" % (update_list[key]['object_uri_str'], update_list[key]['local_filename']))\n\n        warning(u\"Exiting now because of --dry-run\")\n        return EX_OK\n\n    # if there are copy pairs, we can't do delete_before, on the chance\n    # we need one of the to-be-deleted files as a copy source.\n    if len(copy_pairs) > 0:\n        cfg.delete_after = True\n\n    if cfg.delete_removed and orig_remote_count == 0 and len(local_list) and not cfg.force:\n        warning(u\"delete: cowardly refusing to delete because no source files were found.  Use --force to override.\")\n        cfg.delete_removed = False\n\n    if cfg.delete_removed and not cfg.delete_after:\n        deleted_count, deleted_size = _do_deletes(local_list)\n    else:\n        deleted_count, deleted_size = (0, 0)\n\n    def _download(remote_list, seq, total, total_size):\n        original_umask = os.umask(0)\n        os.umask(original_umask)\n        file_list = remote_list.keys()\n        file_list.sort()\n        ret = EX_OK\n        for file in file_list:\n            seq += 1\n            item = remote_list[file]\n            uri = S3Uri(item['object_uri_str'])\n            dst_file = item['local_filename']\n            last_modified_ts = item['timestamp']\n            is_dir = item['is_dir']\n            seq_label = \"[%d of %d]\" % (seq, total)\n\n            if item.get('mark_failed', False):\n                # Item is skipped because there was previously an issue with\n                # its destination directory.\n                continue\n\n            response = None\n            dst_files_b = deunicodise(dst_file)\n            try:\n                chkptfname_b = ''\n                # ignore empty directory at S3:\n                if not is_dir:\n                    debug(u\"dst_file=%s\" % dst_file)\n                    # create temporary files (of type .s3cmd.XXXX.tmp) in the same directory\n                    # for downloading and then rename once downloaded\n                    # unicode provided to mkstemp argument\n                    chkptfd, chkptfname_b = tempfile.mkstemp(\n                        u\".tmp\", u\".s3cmd.\", os.path.dirname(dst_file)\n                    )\n                    with io.open(chkptfd, mode='wb') as dst_stream:\n                        dst_stream.stream_name = unicodise(chkptfname_b)\n                        debug(u\"created chkptfname=%s\" % dst_stream.stream_name)\n                        response = s3.object_get(uri, dst_stream, dst_file, extra_label = seq_label)\n\n                    # download completed, rename the file to destination\n                    if os.name == \"nt\":\n                        # Windows is really a bad OS. Rename can't overwrite an existing file\n                        try:\n                            os.unlink(dst_files_b)\n                        except OSError:\n                            pass\n                    os.rename(chkptfname_b, dst_files_b)\n                    debug(u\"renamed chkptfname=%s to dst_file=%s\"\n                          % (dst_stream.stream_name, dst_file))\n            except OSError as exc:\n                allow_partial = True\n\n                if exc.errno == errno.EISDIR:\n                    error(u\"Download of '%s' failed (Reason: %s is a directory)\"\n                          % (file, dst_file))\n                elif os.name != \"nt\" and exc.errno == errno.ETXTBSY:\n                    error(u\"Download of '%s' failed (Reason: %s is currently open for execute, cannot be overwritten)\"\n                          % (file, dst_file))\n                elif exc.errno == errno.EPERM or exc.errno == errno.EACCES:\n                    error(u\"Download of '%s' failed (Reason: %s permission denied)\"\n                          % (file, dst_file))\n                elif exc.errno == errno.EBUSY:\n                    error(u\"Download of '%s' failed (Reason: %s is busy)\" % (file, dst_file))\n                elif exc.errno == errno.EFBIG:\n                    error(u\"Download of '%s' failed (Reason: %s is too big)\" % (file, dst_file))\n                elif exc.errno == errno.ENAMETOOLONG:\n                    error(u\"Download of '%s' failed (Reason: File Name is too long)\" % file)\n\n                elif (exc.errno == errno.ENOSPC or (os.name != \"nt\" and exc.errno == errno.EDQUOT)):\n                    error(u\"Download of '%s' failed (Reason: No space left)\" % file)\n                    allow_partial = False\n                else:\n                    error(u\"Download of '%s' failed (Reason: Unknown OsError %d)\" % (file, exc.errno or 0))\n                    allow_partial = False\n\n                try:\n                    # Try to remove the temp file if it exists\n                    if chkptfname_b:\n                        os.unlink(chkptfname_b)\n                except Exception:\n                    pass\n\n                if allow_partial and not cfg.stop_on_error:\n                    ret = EX_PARTIAL\n                    continue\n\n                ret = EX_OSFILE\n                if allow_partial:\n                    error(u\"Exiting now because of --stop-on-error\")\n                else:\n                    error(u\"Exiting now because of fatal error\")\n                raise\n            except S3DownloadError as exc:\n                error(u\"Download of '%s' failed too many times (Last Reason: %s). \"\n                      \"This is usually a transient error, please try again \"\n                      \"later.\" % (file, exc))\n                try:\n                    os.unlink(chkptfname_b)\n                except Exception as sub_exc:\n                    warning(u\"Error deleting temporary file %s (Reason: %s)\",\n                            (dst_stream.stream_name, sub_exc))\n                if cfg.stop_on_error:\n                    ret = EX_DATAERR\n                    error(u\"Exiting now because of --stop-on-error\")\n                    raise\n                ret = EX_PARTIAL\n                continue\n            except S3Error as exc:\n                warning(u\"Remote file '%s'. S3Error: %s\" % (exc.resource, exc))\n                try:\n                    os.unlink(chkptfname_b)\n                except Exception as sub_exc:\n                    warning(u\"Error deleting temporary file %s (Reason: %s)\",\n                            (dst_stream.stream_name, sub_exc))\n                if cfg.stop_on_error:\n                    raise\n                ret = EX_PARTIAL\n                continue\n\n            try:\n                # set permissions on destination file\n                if not is_dir: # a normal file\n                    mode = 0o777 - original_umask\n                else:\n                    # an empty directory, make them readable/executable\n                    mode = 0o775\n                debug(u\"mode=%s\" % oct(mode))\n                os.chmod(dst_files_b, mode)\n            except:\n                raise\n\n            # We can't get metadata for directories from an object_get, so we have to\n            # request them explicitly\n            if is_dir and cfg.preserve_attrs:\n                try:\n                    response = s3.object_info(uri)\n                except S3Error as exc:\n                    error(u\"Retrieving directory metadata for '%s' failed (Reason: %s)\"\n                        % (dst_file, exc))\n                    if cfg.stop_on_error:\n                        error(u\"Exiting now because of --stop-on-error\")\n                        raise\n                    ret = EX_PARTIAL\n                    continue\n\n            try:\n                if response and 's3cmd-attrs' in response and cfg.preserve_attrs:\n                    attrs = response['s3cmd-attrs']\n                    attr_mode = attrs.get('mode')\n                    attr_mtime = attrs.get('mtime')\n                    attr_atime = attrs.get('atime')\n                    attr_uid = attrs.get('uid')\n                    attr_gid = attrs.get('gid')\n                    if attr_mode is not None:\n                        os.chmod(dst_files_b, int(attr_mode))\n                    if attr_mtime is not None or attr_atime is not None:\n                        default_time = int(time.time())\n                        mtime = attr_mtime is not None and int(attr_mtime) or default_time\n                        atime = attr_atime is not None and int(attr_atime) or default_time\n                        os.utime(dst_files_b, (atime, mtime))\n                    if attr_uid is not None and attr_gid is not None:\n                        uid = int(attr_uid)\n                        gid = int(attr_gid)\n                        try:\n                            os.lchown(dst_files_b, uid, gid)\n                        except Exception as exc:\n                            exc.failed_step = 'lchown'\n                            raise\n                else:\n                    if response and 'last-modified' in response['headers']:\n                        last_modified_ts = time.mktime(time.strptime(\n                            response[\"headers\"][\"last-modified\"],\n                            \"%a, %d %b %Y %H:%M:%S GMT\"\n                        ))\n                    if last_modified_ts:\n                        os.utime(dst_files_b, (last_modified_ts, last_modified_ts))\n                        debug(\"set mtime to %s\" % last_modified_ts)\n            except OSError as e:\n                ret = EX_PARTIAL\n                if e.errno == errno.EEXIST:\n                    warning(u\"'%s' exists - not overwriting\" % dst_file)\n                    continue\n                if e.errno in (errno.EPERM, errno.EACCES):\n                    if getattr(e, 'failed_step') == 'lchown':\n                        warning(u\"Can't set owner/group: '%s' (%s)\" % (dst_file, e.strerror))\n                    else:\n                        warning(u\"Attrs not writable: '%s' (%s)\" % (dst_file, e.strerror))\n                    if cfg.stop_on_error:\n                        raise e\n                    continue\n                raise e\n            except KeyboardInterrupt:\n                warning(u\"Exiting after keyboard interrupt\")\n                return\n            except Exception as e:\n                ret = EX_PARTIAL\n                error(u\"%s: %s\" % (file, e))\n                if cfg.stop_on_error:\n                    raise OSError(e)\n                continue\n\n            if is_dir:\n                output(u\"mkdir: '%s' -> '%s' %s\" % (uri, dst_file, seq_label))\n            else:\n                speed_fmt = formatSize(response[\"speed\"], human_readable = True, floating_point = True)\n                if not Config().progress_meter:\n                    output(u\"download: '%s' -> '%s' (%d bytes in %0.1f seconds, %0.2f %sB/s) %s\" %\n                        (uri, dst_file, response[\"size\"], response[\"elapsed\"], speed_fmt[0], speed_fmt[1],\n                        seq_label))\n                total_size += response[\"size\"]\n            if Config().delete_after_fetch:\n                s3.object_delete(uri)\n                output(u\"File '%s' removed after syncing\" % (uri))\n        return ret, seq, total_size\n\n    size_transferred = 0\n    total_elapsed = 0.0\n    timestamp_start = time.time()\n    seq = 0\n    ret, seq, size_transferred = _download(remote_list, seq, remote_count + update_count, size_transferred)\n    remote_list = None\n\n    status, seq, size_transferred = _download(update_list, seq, remote_count + update_count, size_transferred)\n    if ret == EX_OK:\n        ret = status\n    update_list = None\n\n    _set_local_filename(copy_pairs, destination_base, source_args, dir_cache)\n    n_copies, size_copies, failed_copy_list = local_copy(copy_pairs, destination_base)\n    copy_pairs = None\n    dir_cache = None\n\n    # Download files that failed during local_copy\n    status, seq, size_transferred = _download(failed_copy_list, seq, len(failed_copy_list) + remote_count + update_count, size_transferred)\n    if ret == EX_OK:\n        ret = status\n\n    if cfg.delete_removed and cfg.delete_after:\n        deleted_count, deleted_size = _do_deletes(local_list)\n\n    total_elapsed = max(1.0, time.time() - timestamp_start)\n    speed_fmt = formatSize(size_transferred / total_elapsed, human_readable = True, floating_point = True)\n\n    stats_info.files = orig_remote_count\n    stats_info.size = remote_total_size\n    stats_info.files_transferred = len(failed_copy_list) + remote_count + update_count\n    stats_info.size_transferred = size_transferred\n    stats_info.files_copied = n_copies\n    stats_info.size_copied = size_copies\n    stats_info.files_deleted = deleted_count\n    stats_info.size_deleted = deleted_size\n\n    # Only print out the result if any work has been done or\n    # if the user asked for verbose output\n    outstr = \"Done. Downloaded %d bytes in %0.1f seconds, %0.2f %sB/s.\" % (size_transferred, total_elapsed, speed_fmt[0], speed_fmt[1])\n    if cfg.stats:\n        outstr += stats_info.format_output()\n        output(outstr)\n    elif size_transferred > 0:\n        output(outstr)\n    else:\n        info(outstr)\n\n    return ret\n\ndef local_copy(copy_pairs, destination_base):\n    # Do NOT hardlink local files by default, that'd be silly\n    # For instance all empty files would become hardlinked together!\n    saved_bytes = 0\n    failed_copy_list = FileDict()\n\n    if destination_base[-1] != os.path.sep:\n        destination_base += os.path.sep\n\n    for relative_file, src_obj in copy_pairs.items():\n        src_file = destination_base + src_obj['copy_src']\n        if os.path.sep != \"/\":\n            src_file = os.path.sep.join(src_file.split(\"/\"))\n\n        dst_file = src_obj['local_filename']\n        try:\n            debug(u\"Copying %s to %s\" % (src_file, dst_file))\n            shutil.copy2(deunicodise(src_file), deunicodise(dst_file))\n            saved_bytes += src_obj.get(u'size', 0)\n        except (IOError, OSError) as e:\n            warning(u'Unable to copy or hardlink files %s -> %s (Reason: %s)'\n                    % (src_file, dst_file, e))\n            failed_copy_list[relative_file] = src_obj\n    return len(copy_pairs), saved_bytes, failed_copy_list\n\ndef remote_copy(s3, copy_pairs, destination_base, uploaded_objects_list=None,\n                metadata_update=False):\n    cfg = Config()\n    saved_bytes = 0\n    failed_copy_list = FileDict()\n    seq = 0\n    src_count = len(copy_pairs)\n    for relative_file, src_obj in copy_pairs.items():\n        copy_src_file = src_obj['copy_src']\n        src_md5 = src_obj['md5']\n\n        seq += 1\n        debug(u\"Remote Copying from %s to %s\" % (copy_src_file, relative_file))\n        src_uri = S3Uri(destination_base + copy_src_file)\n        dst_uri = S3Uri(destination_base + relative_file)\n        src_obj_size = src_obj.get(u'size', 0)\n        seq_label = \"[%d of %d]\" % (seq, src_count)\n        extra_headers = copy(cfg.extra_headers)\n        if metadata_update:\n            # source is a real local file with its own personal metadata\n            attr_header = _build_attr_header(src_obj, relative_file, src_md5)\n            debug(u\"attr_header: %s\" % attr_header)\n            extra_headers.update(attr_header)\n            extra_headers['content-type'] = \\\n                s3.content_type(filename=src_obj['full_name'])\n        try:\n            s3.object_copy(src_uri, dst_uri, extra_headers,\n                           src_size=src_obj_size,\n                           extra_label=seq_label)\n            output(u\"remote copy: '%s' -> '%s'  %s\"\n                   % (copy_src_file, relative_file, seq_label))\n            saved_bytes += src_obj_size\n            if uploaded_objects_list is not None:\n                uploaded_objects_list.append(relative_file)\n        except Exception:\n            warning(u\"Unable to remote copy files '%s' -> '%s'\" % (src_uri, dst_uri))\n            failed_copy_list[relative_file] = src_obj\n    return (len(copy_pairs), saved_bytes, failed_copy_list)\n\ndef _build_attr_header(src_obj, src_relative_name, md5=None):\n    cfg = Config()\n    attrs = {}\n    if cfg.preserve_attrs:\n        for attr in cfg.preserve_attrs_list:\n            val = None\n            if attr == 'uname':\n                try:\n                    val = Utils.urlencode_string(Utils.getpwuid_username(src_obj['uid']), unicode_output=True)\n                except (KeyError, TypeError):\n                    attr = \"uid\"\n                    val = src_obj.get('uid')\n                    if val:\n                        warning(u\"%s: Owner username not known. Storing UID=%d instead.\" % (src_relative_name, val))\n            elif attr == 'gname':\n                try:\n                    val = Utils.urlencode_string(Utils.getgrgid_grpname(src_obj.get('gid')), unicode_output=True)\n                except (KeyError, TypeError):\n                    attr = \"gid\"\n                    val = src_obj.get('gid')\n                    if val:\n                        warning(u\"%s: Owner groupname not known. Storing GID=%d instead.\" % (src_relative_name, val))\n            elif attr != \"md5\":\n                try:\n                    val = getattr(src_obj['sr'], 'st_' + attr)\n                except Exception:\n                    val = None\n            if val is not None:\n                attrs[attr] = val\n\n    if 'md5' in cfg.preserve_attrs_list and md5:\n        attrs['md5'] = md5\n\n    if attrs:\n        attr_str_list = []\n        for k in sorted(attrs.keys()):\n            attr_str_list.append(u\"%s:%s\" % (k, attrs[k]))\n        attr_header = {'x-amz-meta-s3cmd-attrs': u'/'.join(attr_str_list)}\n    else:\n        attr_header = {}\n\n    return attr_header\n\ndef cmd_sync_local2remote(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    def _single_process(source_args):\n        for dest in destinations:\n            ## Normalize URI to convert s3://bkt to s3://bkt/ (trailing slash)\n            destination_base_uri = S3Uri(dest)\n            if destination_base_uri.type != 's3':\n                raise ParameterError(\"Destination must be S3Uri. Got: %s\" % destination_base_uri)\n            destination_base = destination_base_uri.uri()\n        return _child(destination_base, source_args)\n\n    def _parent(source_args):\n        # Now that we've done all the disk I/O to look at the local file system and\n        # calculate the md5 for each file, fork for each destination to upload to them separately\n        # and in parallel\n        child_pids = []\n        ret = EX_OK\n\n        for dest in destinations:\n            ## Normalize URI to convert s3://bkt to s3://bkt/ (trailing slash)\n            destination_base_uri = S3Uri(dest)\n            if destination_base_uri.type != 's3':\n                raise ParameterError(\"Destination must be S3Uri. Got: %s\" % destination_base_uri)\n            destination_base = destination_base_uri.uri()\n            child_pid = os.fork()\n            if child_pid == 0:\n                os._exit(_child(destination_base, source_args))\n            else:\n                child_pids.append(child_pid)\n\n        while len(child_pids):\n            (pid, status) = os.wait()\n            child_pids.remove(pid)\n            if ret == EX_OK:\n                ret = os.WEXITSTATUS(status)\n\n        return ret\n\n    def _child(destination_base, source_args):\n        def _set_remote_uri(local_list, destination_base, single_file_local):\n            if len(local_list) > 0:\n                ## Populate 'remote_uri' only if we've got something to upload\n                if not destination_base.endswith(\"/\"):\n                    if not single_file_local:\n                        raise ParameterError(\"Destination S3 URI must end with '/' (ie must refer to a directory on the remote side).\")\n                    local_list[local_list.keys()[0]]['remote_uri'] = destination_base\n                else:\n                    for key in local_list:\n                        local_list[key]['remote_uri'] = destination_base + key\n\n        def _upload(local_list, seq, total, total_size):\n            file_list = local_list.keys()\n            file_list.sort()\n            ret = EX_OK\n            for file in file_list:\n                seq += 1\n                item = local_list[file]\n                src = item['full_name']\n                try:\n                    src_md5 = local_list.get_md5(file)\n                except IOError:\n                    src_md5 = None\n                uri = S3Uri(item['remote_uri'])\n                seq_label = \"[%d of %d]\" % (seq, total)\n                extra_headers = copy(cfg.extra_headers)\n                try:\n                    attr_header = _build_attr_header(local_list[file],\n                                                     file, src_md5)\n                    debug(u\"attr_header: %s\" % attr_header)\n                    extra_headers.update(attr_header)\n                    response = s3.object_put(src, uri, extra_headers, extra_label = seq_label)\n                except S3UploadError as exc:\n                    error(u\"Upload of '%s' failed too many times (Last reason: %s)\" % (item['full_name'], exc))\n                    if cfg.stop_on_error:\n                        ret = EX_DATAERR\n                        error(u\"Exiting now because of --stop-on-error\")\n                        raise\n                    ret = EX_PARTIAL\n                    continue\n                except InvalidFileError as exc:\n                    error(u\"Upload of '%s' is not possible (Reason: %s)\" % (item['full_name'], exc))\n                    if cfg.stop_on_error:\n                        ret = EX_OSFILE\n                        error(u\"Exiting now because of --stop-on-error\")\n                        raise\n                    ret = EX_PARTIAL\n                    continue\n                speed_fmt = formatSize(response[\"speed\"], human_readable = True, floating_point = True)\n                if not cfg.progress_meter:\n                    output(u\"upload: '%s' -> '%s' (%d bytes in %0.1f seconds, %0.2f %sB/s) %s\" %\n                        (item['full_name'], uri, response[\"size\"], response[\"elapsed\"],\n                        speed_fmt[0], speed_fmt[1], seq_label))\n                total_size += response[\"size\"]\n                uploaded_objects_list.append(uri.object())\n            return ret, seq, total_size\n\n\n        stats_info = StatsInfo()\n\n        local_list, single_file_local, src_exclude_list, local_total_size = fetch_local_list(\n            args[:-1], is_src=True, recursive=True, with_dirs=cfg.keep_dirs\n        )\n\n        # - The source path is either like \"/myPath/my_src_folder\" and\n        # the user want to upload this single folder and optionally only delete\n        # things that have been removed inside this folder. For this case,\n        # we only have to look inside destination_base/my_src_folder and not at\n        # the root of destination_base.\n        # - Or like \"/myPath/my_src_folder/\" and the user want to have the sync\n        # with the content of this folder\n        # Special case, \".\" for current folder.\n        destbase_with_source_list = set()\n        for source_arg in source_args:\n            if not source_arg.endswith('/') and os.path.basename(source_arg) != '.' \\\n               and not single_file_local:\n                destbase_with_source_list.add(s3path.join(\n                    destination_base, os.path.basename(source_arg)\n                ))\n            else:\n                destbase_with_source_list.add(destination_base)\n\n        remote_list, dst_exclude_list, remote_total_size = fetch_remote_list(destbase_with_source_list, recursive = True, require_attribs = True)\n\n        local_count = len(local_list)\n        orig_local_count = local_count\n        remote_count = len(remote_list)\n\n        info(u\"Found %d local files, %d remote files\" % (local_count, remote_count))\n\n        if single_file_local and len(local_list) == 1 and len(remote_list) == 1:\n            ## Make remote_key same as local_key for comparison if we're dealing with only one file\n            remote_list_entry = remote_list[remote_list.keys()[0]]\n            # Flush remote_list, by the way\n            remote_list = FileDict()\n            remote_list[local_list.keys()[0]] =  remote_list_entry\n\n        local_list, remote_list, update_list, copy_pairs = compare_filelists(local_list, remote_list, src_remote = False, dst_remote = True)\n\n        local_count = len(local_list)\n        update_count = len(update_list)\n        copy_count = len(copy_pairs)\n        remote_count = len(remote_list)\n        upload_count = local_count + update_count\n\n        info(u\"Summary: %d local files to upload, %d files to remote copy, %d remote files to delete\" % (upload_count, copy_count, remote_count))\n\n        _set_remote_uri(local_list, destination_base, single_file_local)\n        _set_remote_uri(update_list, destination_base, single_file_local)\n\n        if cfg.dry_run:\n            keys = filedicts_to_keys(src_exclude_list, dst_exclude_list)\n            for key in keys:\n                output(u\"exclude: %s\" % key)\n            for key in local_list:\n                output(u\"upload: '%s' -> '%s'\" % (local_list[key]['full_name'], local_list[key]['remote_uri']))\n            for key in update_list:\n                output(u\"upload: '%s' -> '%s'\" % (update_list[key]['full_name'], update_list[key]['remote_uri']))\n            for relative_file, item in copy_pairs.items():\n                output(u\"remote copy: '%s' -> '%s'\" % (item['copy_src'], relative_file))\n            if cfg.delete_removed:\n                for key in remote_list:\n                    output(u\"delete: '%s'\" % remote_list[key]['object_uri_str'])\n\n            warning(u\"Exiting now because of --dry-run\")\n            return EX_OK\n\n        # if there are copy pairs, we can't do delete_before, on the chance\n        # we need one of the to-be-deleted files as a copy source.\n        if len(copy_pairs) > 0:\n            cfg.delete_after = True\n\n        if cfg.delete_removed and orig_local_count == 0 and len(remote_list) and not cfg.force:\n            warning(u\"delete: cowardly refusing to delete because no source files were found.  Use --force to override.\")\n            cfg.delete_removed = False\n\n        if cfg.delete_removed and not cfg.delete_after and remote_list:\n            subcmd_batch_del(remote_list = remote_list)\n\n        size_transferred = 0\n        total_elapsed = 0.0\n        timestamp_start = time.time()\n        ret, n, size_transferred = _upload(local_list, 0, upload_count, size_transferred)\n        status, n, size_transferred = _upload(update_list, n, upload_count, size_transferred)\n        if ret == EX_OK:\n            ret = status\n        # uploaded_objects_list reference is passed so it can be filled with\n        # destination object of successful copies so that they can be\n        # invalidated by cf\n        n_copies, saved_bytes, failed_copy_files  = remote_copy(\n            s3, copy_pairs, destination_base, uploaded_objects_list, True)\n\n        #upload file that could not be copied\n        debug(\"Process files that were not remotely copied\")\n        failed_copy_count = len(failed_copy_files)\n        _set_remote_uri(failed_copy_files, destination_base, single_file_local)\n        status, n, size_transferred = _upload(failed_copy_files, n, upload_count + failed_copy_count, size_transferred)\n        if ret == EX_OK:\n            ret = status\n\n        if cfg.delete_removed and cfg.delete_after and remote_list:\n            subcmd_batch_del(remote_list = remote_list)\n        total_elapsed = max(1.0, time.time() - timestamp_start)\n        total_speed = total_elapsed and size_transferred / total_elapsed or 0.0\n        speed_fmt = formatSize(total_speed, human_readable = True, floating_point = True)\n\n\n        stats_info.files = orig_local_count\n        stats_info.size = local_total_size\n        stats_info.files_transferred = upload_count + failed_copy_count\n        stats_info.size_transferred = size_transferred\n        stats_info.files_copied = n_copies\n        stats_info.size_copied = saved_bytes\n        stats_info.files_deleted = remote_count\n\n\n        # Only print out the result if any work has been done or\n        # if the user asked for verbose output\n        outstr = \"Done. Uploaded %d bytes in %0.1f seconds, %0.2f %sB/s.\" % (size_transferred, total_elapsed, speed_fmt[0], speed_fmt[1])\n        if cfg.stats:\n            outstr += stats_info.format_output()\n            output(outstr)\n        elif size_transferred + saved_bytes > 0:\n            output(outstr)\n        else:\n            info(outstr)\n\n        return ret\n\n    def _invalidate_on_cf(destination_base_uri):\n        cf = CloudFront(cfg)\n        default_index_file = None\n        if cfg.invalidate_default_index_on_cf or cfg.invalidate_default_index_root_on_cf:\n            info_response = s3.website_info(destination_base_uri, cfg.bucket_location)\n            if info_response:\n              default_index_file = info_response['index_document']\n              if len(default_index_file) < 1:\n                  default_index_file = None\n\n        results = cf.InvalidateObjects(destination_base_uri, uploaded_objects_list, default_index_file, cfg.invalidate_default_index_on_cf, cfg.invalidate_default_index_root_on_cf)\n        for result in results:\n            if result['status'] == 201:\n                output(u\"Created invalidation request for %d paths\" % len(uploaded_objects_list))\n                output(u\"Check progress with: s3cmd cfinvalinfo cf://%s/%s\" % (result['dist_id'], result['request_id']))\n\n    # main execution\n    uploaded_objects_list = []\n\n    if cfg.encrypt:\n        error(u\"S3cmd 'sync' doesn't yet support GPG encryption, sorry.\")\n        error(u\"Either use unconditional 's3cmd put --recursive'\")\n        error(u\"or disable encryption with --no-encrypt parameter.\")\n        sys.exit(EX_USAGE)\n\n    for arg in args[:-1]:\n        if not os.path.exists(deunicodise(arg)):\n            raise ParameterError(\"Invalid source: '%s' is not an existing file or directory\" % arg)\n\n    destinations = [args[-1]]\n    if cfg.additional_destinations:\n        destinations = destinations + cfg.additional_destinations\n\n    if 'fork' not in os.__all__ or len(destinations) < 2:\n        ret = _single_process(args[:-1])\n        destination_base_uri = S3Uri(destinations[-1])\n        if cfg.invalidate_on_cf:\n            if len(uploaded_objects_list) == 0:\n                info(\"Nothing to invalidate in CloudFront\")\n            else:\n                _invalidate_on_cf(destination_base_uri)\n    else:\n        ret = _parent(args[:-1])\n        if cfg.invalidate_on_cf:\n            error(u\"You cannot use both --cf-invalidate and --add-destination.\")\n            return(EX_USAGE)\n\n    return ret\n\ndef cmd_sync(args):\n    cfg = Config()\n    if (len(args) < 2):\n        syntax_msg = ''\n        commands_list = get_commands_list()\n        for cmd in commands_list:\n            if cmd.get('cmd') == 'sync':\n                syntax_msg = cmd.get('param', '')\n                break\n        raise ParameterError(\"Too few parameters! Expected: %s\" % syntax_msg)\n    if cfg.delay_updates:\n        warning(u\"`delay-updates` is obsolete.\")\n\n    for arg in args:\n        if arg == u'-':\n            raise ParameterError(\"Stdin or stdout ('-') can't be used for a source or a destination with the sync command.\")\n\n    if S3Uri(args[0]).type == \"file\" and S3Uri(args[-1]).type == \"s3\":\n        return cmd_sync_local2remote(args)\n    if S3Uri(args[0]).type == \"s3\" and S3Uri(args[-1]).type == \"file\":\n        return cmd_sync_remote2local(args)\n    if S3Uri(args[0]).type == \"s3\" and S3Uri(args[-1]).type == \"s3\":\n        return cmd_sync_remote2remote(args)\n    raise ParameterError(\"Invalid source/destination: '%s'\" % \"' '\".join(args))\n\ndef cmd_setacl(args):\n    cfg = Config()\n    s3 = S3(cfg)\n\n    set_to_acl = cfg.acl_public and \"Public\" or \"Private\"\n\n    if not cfg.recursive:\n        old_args = args\n        args = []\n        for arg in old_args:\n            uri = S3Uri(arg)\n            if not uri.has_object():\n                if cfg.acl_public != None:\n                    info(\"Setting bucket-level ACL for %s to %s\" % (uri.uri(), set_to_acl))\n                else:\n                    info(\"Setting bucket-level ACL for %s\" % (uri.uri()))\n                if not cfg.dry_run:\n                    update_acl(s3, uri)\n            else:\n                args.append(arg)\n\n    remote_list, exclude_list, _ = fetch_remote_list(args)\n\n    remote_count = len(remote_list)\n\n    info(u\"Summary: %d remote files to update\" % remote_count)\n\n    if cfg.dry_run:\n        for key in exclude_list:\n            output(u\"exclude: %s\" % key)\n        for key in remote_list:\n            output(u\"setacl: '%s'\" % remote_list[key]['object_uri_str'])\n\n        warning(u\"Exiting now because of --dry-run\")\n        return EX_OK\n\n    seq = 0\n    for key in remote_list:\n        seq += 1\n        seq_label = \"[%d of %d]\" % (seq, remote_count)\n        uri = S3Uri(remote_list[key]['object_uri_str'])\n        update_acl(s3, uri, seq_label)\n    return EX_OK\n\ndef cmd_setobjectlegalhold(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    legal_hold_status = args[0]\n    uri = S3Uri(args[1])\n\n    if legal_hold_status not in [\"ON\", \"OFF\"]:\n        raise ParameterError(\"Incorrect status\")\n\n    if cfg.dry_run:\n        return EX_OK\n\n    response = s3.set_object_legal_hold(uri, legal_hold_status)\n\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 204:\n        output(u\"%s: Legal Hold updated\" % uri)\n    return EX_OK\n\ndef cmd_setobjectretention(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    mode = args[0]\n    retain_until_date = args[1]\n    uri = S3Uri(args[2])\n\n    if mode not in [\"COMPLIANCE\", \"GOVERNANCE\"]:\n        raise ParameterError(\"Incorrect mode\")\n\n    try:\n        datetime.datetime.strptime(retain_until_date, '%Y-%m-%dT%H:%M:%SZ')\n    except ValueError:\n        raise ParameterError(\"Incorrect data format, should be YYYY-MM-DDTHH:MM:SSZ\")\n\n    if cfg.dry_run:\n        return EX_OK\n\n    response = s3.set_object_retention(uri, mode, retain_until_date)\n\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 204:\n        output(u\"%s: Retention updated\" % uri)\n\ndef cmd_setversioning(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    bucket_uri = S3Uri(args[0])\n    if bucket_uri.object():\n        raise ParameterError(\"Only bucket name is required for [setversioning] command\")\n    status = args[1]\n    if status not in [\"enable\", \"disable\"]:\n        raise ParameterError(\"Must be 'enable' or 'disable'. Got: %s\" % status)\n\n    enabled = True if status == \"enable\" else False\n    response = s3.set_versioning(bucket_uri, enabled)\n\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 200:\n        output(u\"%s: Versioning status updated\" % bucket_uri)\n    return EX_OK\n\ndef cmd_setownership(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    bucket_uri = S3Uri(args[0])\n    if bucket_uri.object():\n        raise ParameterError(\"Only bucket name is required for [setownership] command\")\n\n    valid_values = {x.lower():x for x in [\n        'BucketOwnerPreferred', 'BucketOwnerEnforced', 'ObjectWriter'\n    ]}\n    value = valid_values.get(args[1].lower())\n    if not value:\n        choices = \" or \".join(['%s' % x for x in valid_values.keys()])\n        raise ParameterError(\"Must be %s. Got: %s\" % (choices, args[1]))\n\n    response = s3.set_bucket_ownership(bucket_uri, value)\n\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 200:\n        output(u\"%s: Bucket Object Ownership updated\" % bucket_uri)\n    return EX_OK\n\ndef cmd_setblockpublicaccess(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    bucket_uri = S3Uri(args[0])\n    if bucket_uri.object():\n        raise ParameterError(\"Only bucket name is required for [setblockpublicaccess] command\")\n\n    valid_values = {x.lower():x for x in [\n        'BlockPublicAcls', 'IgnorePublicAcls', 'BlockPublicPolicy', 'RestrictPublicBuckets'\n    ]}\n    flags = {}\n    raw_flags = args[1].split(',')\n    for raw_value in raw_flags:\n        if not raw_value:\n            continue\n        value = valid_values.get(raw_value.lower())\n        if not value:\n            choices = \" or \".join(['%s' % x for x in valid_values.keys()])\n            raise ParameterError(\"Must be %s. Got: %s\" % (choices, raw_value))\n        flags[value] = True\n\n    response = s3.set_bucket_public_access_block(bucket_uri, flags)\n\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 200:\n        output(u\"%s: Block Public Access updated\" % bucket_uri)\n    return EX_OK\n\ndef cmd_setpolicy(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[1])\n    policy_file = args[0]\n\n    with open(deunicodise(policy_file), 'r') as fp:\n        policy = fp.read()\n\n    if cfg.dry_run:\n        return EX_OK\n\n    response = s3.set_policy(uri, policy)\n\n    #if retsponse['status'] == 200:\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 204:\n        output(u\"%s: Policy updated\" % uri)\n    return EX_OK\n\ndef cmd_delpolicy(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[0])\n    if cfg.dry_run: return EX_OK\n\n    response = s3.delete_policy(uri)\n\n    #if retsponse['status'] == 200:\n    debug(u\"response - %s\" % response['status'])\n    output(u\"%s: Policy deleted\" % uri)\n    return EX_OK\n\ndef cmd_setcors(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[1])\n    cors_file = args[0]\n\n    with open(deunicodise(cors_file), 'r') as fp:\n        cors = fp.read()\n\n    if cfg.dry_run:\n        return EX_OK\n\n    response = s3.set_cors(uri, cors)\n\n    #if retsponse['status'] == 200:\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 204:\n        output(u\"%s: CORS updated\" % uri)\n    return EX_OK\n\ndef cmd_delcors(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[0])\n    if cfg.dry_run: return EX_OK\n\n    response = s3.delete_cors(uri)\n\n    #if retsponse['status'] == 200:\n    debug(u\"response - %s\" % response['status'])\n    output(u\"%s: CORS deleted\" % uri)\n    return EX_OK\n\ndef cmd_set_payer(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[0])\n\n    if cfg.dry_run: return EX_OK\n\n    response = s3.set_payer(uri)\n    if response['status'] == 200:\n        output(u\"%s: Payer updated\" % uri)\n        return EX_OK\n    else:\n        output(u\"%s: Payer NOT updated\" % uri)\n        return EX_CONFLICT\n\ndef cmd_setlifecycle(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[1])\n    lifecycle_policy_file = args[0]\n\n    with open(deunicodise(lifecycle_policy_file), 'r') as fp:\n        lifecycle_policy = fp.read()\n\n    if cfg.dry_run:\n        return EX_OK\n\n    response = s3.set_lifecycle_policy(uri, lifecycle_policy)\n\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 200:\n        output(u\"%s: Lifecycle Policy updated\" % uri)\n    return EX_OK\n\ndef cmd_getlifecycle(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[0])\n\n    response = s3.get_lifecycle_policy(uri)\n\n    output(u\"%s\" % getPrettyFromXml(response['data']))\n    return EX_OK\n\ndef cmd_dellifecycle(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[0])\n    if cfg.dry_run: return EX_OK\n\n    response = s3.delete_lifecycle_policy(uri)\n\n    debug(u\"response - %s\" % response['status'])\n    output(u\"%s: Lifecycle Policy deleted\" % uri)\n    return EX_OK\n\ndef cmd_setnotification(args):\n    s3 = S3(Config())\n    uri = S3Uri(args[1])\n    notification_policy_file = args[0]\n\n    with open(deunicodise(notification_policy_file), 'r') as fp:\n        notification_policy = fp.read()\n\n    response = s3.set_notification_policy(uri, notification_policy)\n\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] == 200:\n        output(u\"%s: Notification Policy updated\" % uri)\n    return EX_OK\n\ndef cmd_getnotification(args):\n    s3 = S3(Config())\n    uri = S3Uri(args[0])\n\n    response = s3.get_notification_policy(uri)\n\n    output(getPrettyFromXml(response['data']))\n    return EX_OK\n\ndef cmd_delnotification(args):\n    s3 = S3(Config())\n    uri = S3Uri(args[0])\n\n    response = s3.delete_notification_policy(uri)\n\n    debug(u\"response - %s\" % response['status'])\n    output(u\"%s: Notification Policy deleted\" % uri)\n    return EX_OK\n\ndef cmd_settagging(args):\n    s3 = S3(Config())\n    uri = S3Uri(args[0])\n    tag_set_string = args[1]\n\n    tagsets = []\n    for tagset in tag_set_string.split(\"&\"):\n        keyval = tagset.split(\"=\", 1)\n        key = keyval[0]\n        if not key:\n            raise ParameterError(\"Tag key should not be empty\")\n        value = len(keyval) > 1 and keyval[1] or \"\"\n        tagsets.append((key, value))\n\n    debug(tagsets)\n    response = s3.set_tagging(uri, tagsets)\n\n    debug(u\"response - %s\" % response['status'])\n    if response['status'] in [200, 204]:\n        output(u\"%s: Tagging updated\" % uri)\n    return EX_OK\n\ndef cmd_gettagging(args):\n    s3 = S3(Config())\n    uri = S3Uri(args[0])\n\n    tagsets = s3.get_tagging(uri)\n    if uri.has_object():\n        output(u\"%s (object):\" % uri)\n    else:\n        output(u\"%s (bucket):\" % uri)\n    debug(tagsets)\n    for tag in tagsets:\n        try:\n            output(u\"\\t%s:\\t%s\" % (\n                tag['Key'],\n                tag['Value']))\n        except KeyError:\n            pass\n    return EX_OK\n\ndef cmd_deltagging(args):\n    s3 = S3(Config())\n    uri = S3Uri(args[0])\n\n    response = s3.delete_tagging(uri)\n\n    debug(u\"response - %s\" % response['status'])\n    output(u\"%s: Tagging deleted\" % uri)\n    return EX_OK\n\ndef cmd_multipart(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[0])\n\n    #id = ''\n    #if(len(args) > 1): id = args[1]\n\n    upload_list = s3.get_multipart(uri)\n    output(u\"%s\" % uri)\n    debug(upload_list)\n    output(u\"Initiated\\tPath\\tId\")\n    for mpupload in upload_list:\n        try:\n            output(u\"%s\\t%s\\t%s\" % (\n                mpupload['Initiated'],\n                \"s3://\" + uri.bucket() + \"/\" + mpupload['Key'],\n                mpupload['UploadId']))\n        except KeyError:\n            pass\n    return EX_OK\n\ndef cmd_abort_multipart(args):\n    '''{\"cmd\":\"abortmp\",   \"label\":\"abort a multipart upload\", \"param\":\"s3://BUCKET/OBJECT Id\", \"func\":cmd_abort_multipart, \"argc\":2},'''\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[0])\n    if not uri.object():\n        raise ParameterError(u\"Expecting S3 URI with a filename: %s\" % uri.uri())\n    id = args[1]\n    response = s3.abort_multipart(uri, id)\n    debug(u\"response - %s\" % response['status'])\n    output(u\"%s\" % uri)\n    return EX_OK\n\ndef cmd_list_multipart(args):\n    '''{\"cmd\":\"abortmp\",   \"label\":\"list a multipart upload\", \"param\":\"s3://BUCKET Id\", \"func\":cmd_list_multipart, \"argc\":2},'''\n    cfg = Config()\n    s3 = S3(cfg)\n    uri = S3Uri(args[0])\n    id = args[1]\n\n    part_list = s3.list_multipart(uri, id)\n    output(u\"LastModified\\t\\t\\tPartNumber\\tETag\\tSize\")\n    for mpupload in part_list:\n        try:\n            output(u\"%s\\t%s\\t%s\\t%s\" % (mpupload['LastModified'],\n                                        mpupload['PartNumber'],\n                                        mpupload['ETag'],\n                                        mpupload['Size']))\n        except KeyError:\n            pass\n    return EX_OK\n\ndef cmd_accesslog(args):\n    cfg = Config()\n    s3 = S3(cfg)\n    bucket_uri = S3Uri(args.pop())\n    if bucket_uri.object():\n        raise ParameterError(\"Only bucket name is required for [accesslog] command\")\n    if cfg.log_target_prefix == False:\n        accesslog, response = s3.set_accesslog(bucket_uri, enable = False)\n    elif cfg.log_target_prefix:\n        log_target_prefix_uri = S3Uri(cfg.log_target_prefix)\n        if log_target_prefix_uri.type != \"s3\":\n            raise ParameterError(\"--log-target-prefix must be a S3 URI\")\n        accesslog, response = s3.set_accesslog(bucket_uri, enable = True, log_target_prefix_uri = log_target_prefix_uri, acl_public = cfg.acl_public)\n    else:   # cfg.log_target_prefix == None\n        accesslog = s3.get_accesslog(bucket_uri)\n\n    output(u\"Access logging for: %s\" % bucket_uri.uri())\n    output(u\"   Logging Enabled: %s\" % accesslog.isLoggingEnabled())\n    if accesslog.isLoggingEnabled():\n        output(u\"     Target prefix: %s\" % accesslog.targetPrefix().uri())\n        #output(u\"   Public Access:   %s\" % accesslog.isAclPublic())\n    return EX_OK\n\ndef cmd_sign(args):\n    string_to_sign = args.pop()\n    debug(u\"string-to-sign: %r\" % string_to_sign)\n    signature = sign_string_v2(encode_to_s3(string_to_sign))\n    output(u\"Signature: %s\" % decode_from_s3(signature))\n    return EX_OK\n\ndef cmd_signurl(args):\n    expiry = args.pop()\n    url_to_sign = S3Uri(args.pop())\n    if url_to_sign.type != 's3':\n        raise ParameterError(\"Must be S3Uri. Got: %s\" % url_to_sign)\n    debug(\"url to sign: %r\" % url_to_sign)\n    signed_url = sign_url_v2(url_to_sign, expiry)\n    output(signed_url)\n    return EX_OK\n\ndef cmd_fixbucket(args):\n    def _unescape(text):\n        ##\n        # Removes HTML or XML character references and entities from a text string.\n        #\n        # @param text The HTML (or XML) source text.\n        # @return The plain text, as a Unicode string, if necessary.\n        #\n        # From: http://effbot.org/zone/re-sub.htm#unescape-html\n        def _unescape_fixup(m):\n            text = m.group(0)\n            if not 'apos' in htmlentitydefs.name2codepoint:\n                htmlentitydefs.name2codepoint['apos'] = ord(\"'\")\n            if text[:2] == \"&#\":\n                # character reference\n                try:\n                    if text[:3] == \"&#x\":\n                        return unichr(int(text[3:-1], 16))\n                    else:\n                        return unichr(int(text[2:-1]))\n                except ValueError:\n                    pass\n            else:\n                # named entity\n                try:\n                    text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])\n                except KeyError:\n                    pass\n            return text # leave as is\n            text = text.encode('ascii', 'xmlcharrefreplace')\n        return re.sub(r\"&#?\\w+;\", _unescape_fixup, text)\n\n    cfg = Config()\n    cfg.urlencoding_mode = \"fixbucket\"\n    s3 = S3(cfg)\n\n    count = 0\n    for arg in args:\n        culprit = S3Uri(arg)\n        if culprit.type != \"s3\":\n            raise ParameterError(\"Expecting S3Uri instead of: %s\" % arg)\n        response = s3.bucket_list_noparse(culprit.bucket(), culprit.object(), recursive = True)\n        r_xent = re.compile(r\"&#x[\\da-fA-F]+;\")\n        data = decode_from_s3(response['data'])\n        keys = re.findall(\"<Key>(.*?)</Key>\", data, re.MULTILINE | re.UNICODE)\n        debug(\"Keys: %r\" % keys)\n        for key in keys:\n            if r_xent.search(key):\n                info(\"Fixing: %s\" % key)\n                debug(\"Step 1: Transforming %s\" % key)\n                key_bin = _unescape(key)\n                debug(\"Step 2:       ... to %s\" % key_bin)\n                key_new = replace_nonprintables(key_bin)\n                debug(\"Step 3:  ... then to %s\" % key_new)\n                src = S3Uri(\"s3://%s/%s\" % (culprit.bucket(), key_bin))\n                dst = S3Uri(\"s3://%s/%s\" % (culprit.bucket(), key_new))\n                if cfg.dry_run:\n                    output(u\"[--dry-run] File %r would be renamed to %s\" % (key_bin, key_new))\n                    continue\n                try:\n                    resp_move = s3.object_move(src, dst)\n                    if resp_move['status'] == 200:\n                        output(u\"File '%r' renamed to '%s'\" % (key_bin, key_new))\n                        count += 1\n                    else:\n                        error(u\"Something went wrong for: %r\" % key)\n                        error(u\"Please report the problem to s3tools-bugs@lists.sourceforge.net\")\n                except S3Error:\n                    error(u\"Something went wrong for: %r\" % key)\n                    error(u\"Please report the problem to s3tools-bugs@lists.sourceforge.net\")\n\n    if count > 0:\n        warning(u\"Fixed %d files' names. Their ACL were reset to Private.\" % count)\n        warning(u\"Use 's3cmd setacl --acl-public s3://...' to make\")\n        warning(u\"them publicly readable if required.\")\n    return EX_OK\n\ndef resolve_list(lst, args):\n    retval = []\n    for item in lst:\n        retval.append(item % args)\n    return retval\n\ndef gpg_command(command, passphrase = \"\"):\n    debug(u\"GPG command: \" + \" \".join(command))\n    command = [deunicodise(cmd_entry) for cmd_entry in command]\n    p = subprocess.Popen(command, stdin = subprocess.PIPE, stdout = subprocess.PIPE, stderr = subprocess.STDOUT,\n                         close_fds = True)\n    p_stdout, p_stderr = p.communicate(deunicodise(passphrase + \"\\n\"))\n    debug(u\"GPG output:\")\n    for line in unicodise(p_stdout).split(\"\\n\"):\n        debug(u\"GPG: \" + line)\n    p_exitcode = p.wait()\n    return p_exitcode\n\ndef gpg_encrypt(filename):\n    cfg = Config()\n    tmp_filename = Utils.mktmpfile()\n    args = {\n        \"gpg_command\" : cfg.gpg_command,\n        \"passphrase_fd\" : \"0\",\n        \"input_file\" : filename,\n        \"output_file\" : tmp_filename,\n    }\n    info(u\"Encrypting file %s to %s...\" % (filename, tmp_filename))\n    command = resolve_list(cfg.gpg_encrypt.split(\" \"), args)\n    code = gpg_command(command, cfg.gpg_passphrase)\n    return (code, tmp_filename, \"gpg\")\n\ndef gpg_decrypt(filename, gpgenc_header = \"\", in_place = True):\n    cfg = Config()\n    tmp_filename = Utils.mktmpfile(filename)\n    args = {\n        \"gpg_command\" : cfg.gpg_command,\n        \"passphrase_fd\" : \"0\",\n        \"input_file\" : filename,\n        \"output_file\" : tmp_filename,\n    }\n    info(u\"Decrypting file %s to %s...\" % (filename, tmp_filename))\n    command = resolve_list(cfg.gpg_decrypt.split(\" \"), args)\n    code = gpg_command(command, cfg.gpg_passphrase)\n    if code == 0 and in_place:\n        debug(u\"Renaming %s to %s\" % (tmp_filename, filename))\n        os.unlink(deunicodise(filename))\n        os.rename(deunicodise(tmp_filename), deunicodise(filename))\n        tmp_filename = filename\n    return (code, tmp_filename)\n\ndef run_configure(config_file, args):\n    cfg = Config()\n    options = [\n        (\"access_key\", \"Access Key\", \"Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.\"),\n        (\"secret_key\", \"Secret Key\"),\n        (\"bucket_location\", \"Default Region\"),\n        (\"host_base\", \"S3 Endpoint\", \"Use \\\"s3.amazonaws.com\\\" for S3 Endpoint and not modify it to the target Amazon S3.\"),\n        (\"host_bucket\", \"DNS-style bucket+hostname:port template for accessing a bucket\", \"Use \\\"%(bucket)s.s3.amazonaws.com\\\" to the target Amazon S3. \\\"%(bucket)s\\\" and \\\"%(location)s\\\" vars can be used\\nif the target S3 system supports dns based buckets.\"),\n        (\"gpg_passphrase\", \"Encryption password\", \"Encryption password is used to protect your files from reading\\nby unauthorized persons while in transfer to S3\"),\n        (\"gpg_command\", \"Path to GPG program\"),\n        (\"use_https\", \"Use HTTPS protocol\", \"When using secure HTTPS protocol all communication with Amazon S3\\nservers is protected from 3rd party eavesdropping. This method is\\nslower than plain HTTP, and can only be proxied with Python 2.7 or newer\"),\n        (\"proxy_host\", \"HTTP Proxy server name\", \"On some networks all internet access must go through a HTTP proxy.\\nTry setting it here if you can't connect to S3 directly\"),\n        (\"proxy_port\", \"HTTP Proxy server port\"),\n        ]\n    ## Option-specfic defaults\n    if getattr(cfg, \"gpg_command\") == \"\":\n        setattr(cfg, \"gpg_command\", which(\"gpg\"))\n\n    if getattr(cfg, \"proxy_host\") == \"\" and os.getenv(\"http_proxy\"):\n        autodetected_encoding = locale.getpreferredencoding() or \"UTF-8\"\n        re_match=re.match(r\"(http://)?([^:]+):(\\d+)\",\n                          unicodise_s(os.getenv(\"http_proxy\"), autodetected_encoding))\n        if re_match:\n            setattr(cfg, \"proxy_host\", re_match.groups()[1])\n            setattr(cfg, \"proxy_port\", re_match.groups()[2])\n\n    try:\n        # Support for python3\n        # raw_input only exists in py2 and was renamed to input in py3\n        global input\n        input = raw_input\n    except NameError:\n        pass\n\n    try:\n        while True:\n            output(u\"\\nEnter new values or accept defaults in brackets with Enter.\")\n            output(u\"Refer to user manual for detailed description of all options.\")\n            for option in options:\n                prompt = option[1]\n                ## Option-specific handling\n                if option[0] == 'proxy_host' and getattr(cfg, 'use_https') == True and sys.hexversion < 0x02070000:\n                    setattr(cfg, option[0], \"\")\n                    continue\n                if option[0] == 'proxy_port' and getattr(cfg, 'proxy_host') == \"\":\n                    setattr(cfg, option[0], 0)\n                    continue\n\n                try:\n                    val = getattr(cfg, option[0])\n                    if type(val) is bool:\n                        val = val and \"Yes\" or \"No\"\n                    if val not in (None, \"\"):\n                        prompt += \" [%s]\" % val\n                except AttributeError:\n                    pass\n\n                if len(option) >= 3:\n                    output(u\"\\n%s\" % option[2])\n\n                val = unicodise_s(input(prompt + \": \"))\n                if val != \"\":\n                    if type(getattr(cfg, option[0])) is bool:\n                        # Turn 'Yes' into True, everything else into False\n                        val = val.lower().startswith('y')\n                    setattr(cfg, option[0], val)\n            output(u\"\\nNew settings:\")\n            for option in options:\n                output(u\"  %s: %s\" % (option[1], getattr(cfg, option[0])))\n            val = input(\"\\nTest access with supplied credentials? [Y/n] \")\n            if val.lower().startswith(\"y\") or val == \"\":\n                try:\n                    # Default, we try to list 'all' buckets which requires\n                    # ListAllMyBuckets permission\n                    if len(args) == 0:\n                        output(u\"Please wait, attempting to list all buckets...\")\n                        S3(Config()).bucket_list(\"\", \"\")\n                    else:\n                        # If user specified a bucket name directly, we check it and only it.\n                        # Thus, access check can succeed even if user only has access to\n                        # to a single bucket and not ListAllMyBuckets permission.\n                        output(u\"Please wait, attempting to list bucket: \" + args[0])\n                        uri = S3Uri(args[0])\n                        if uri.type == \"s3\" and uri.has_bucket():\n                            S3(Config()).bucket_list(uri.bucket(), \"\")\n                        else:\n                            raise Exception(u\"Invalid bucket uri: \" + args[0])\n\n                    output(u\"Success. Your access key and secret key worked fine :-)\")\n\n                    output(u\"\\nNow verifying that encryption works...\")\n                    if not getattr(cfg, \"gpg_command\") or not getattr(cfg, \"gpg_passphrase\"):\n                        output(u\"Not configured. Never mind.\")\n                    else:\n                        if not getattr(cfg, \"gpg_command\"):\n                            raise Exception(\"Path to GPG program not set\")\n                        if not os.path.isfile(deunicodise(getattr(cfg, \"gpg_command\"))):\n                            raise Exception(\"GPG program not found\")\n                        filename = Utils.mktmpfile()\n                        with open(deunicodise(filename), \"w\") as fp:\n                            fp.write(os.sys.copyright)\n                        ret_enc = gpg_encrypt(filename)\n                        ret_dec = gpg_decrypt(ret_enc[1], ret_enc[2], False)\n                        hash = [\n                            hash_file_md5(filename),\n                            hash_file_md5(ret_enc[1]),\n                            hash_file_md5(ret_dec[1]),\n                        ]\n                        os.unlink(deunicodise(filename))\n                        os.unlink(deunicodise(ret_enc[1]))\n                        os.unlink(deunicodise(ret_dec[1]))\n                        if hash[0] == hash[2] and hash[0] != hash[1]:\n                            output(u\"Success. Encryption and decryption worked fine :-)\")\n                        else:\n                            raise Exception(\"Encryption verification error.\")\n\n                except S3Error as e:\n                    error(u\"Test failed: %s\" % (e))\n                    if e.code == \"AccessDenied\":\n                        error(u\"Are you sure your keys have s3:ListAllMyBuckets permissions?\")\n                    val = input(\"\\nRetry configuration? [Y/n] \")\n                    if val.lower().startswith(\"y\") or val == \"\":\n                        continue\n                except Exception as e:\n                    error(u\"Test failed: %s\" % (e))\n                    val = input(\"\\nRetry configuration? [Y/n] \")\n                    if val.lower().startswith(\"y\") or val == \"\":\n                        continue\n\n\n            val = input(\"\\nSave settings? [y/N] \")\n            if val.lower().startswith(\"y\"):\n                break\n            val = input(\"Retry configuration? [Y/n] \")\n            if val.lower().startswith(\"n\"):\n                raise EOFError()\n\n        ## Overwrite existing config file, make it user-readable only\n        old_mask = os.umask(0o077)\n        try:\n            os.remove(deunicodise(config_file))\n        except OSError as e:\n            if e.errno != errno.ENOENT:\n                raise\n        try:\n            with io.open(deunicodise(config_file), \"w\", encoding=cfg.encoding) as fp:\n                cfg.dump_config(fp)\n        finally:\n            os.umask(old_mask)\n        output(u\"Configuration saved to '%s'\" % config_file)\n\n    except (EOFError, KeyboardInterrupt):\n        output(u\"\\nConfiguration aborted. Changes were NOT saved.\")\n        return\n\n    except IOError as e:\n        error(u\"Writing config file failed: %s: %s\" % (config_file, e.strerror))\n        sys.exit(EX_IOERR)\n\ndef process_patterns_from_file(fname, patterns_list):\n    try:\n        with open(deunicodise(fname), \"rt\") as fn:\n            for pattern in fn:\n                pattern = unicodise(pattern).strip()\n                if re.match(\"^#\", pattern) or re.match(r\"^\\s*$\", pattern):\n                    continue\n                debug(u\"%s: adding rule: %s\" % (fname, pattern))\n                patterns_list.append(pattern)\n    except IOError as e:\n        error(e)\n        sys.exit(EX_IOERR)\n\n    return patterns_list\n\ndef process_patterns(patterns_list, patterns_from, is_glob, option_txt = \"\"):\n    r\"\"\"\n    process_patterns(patterns, patterns_from, is_glob, option_txt = \"\")\n    Process --exclude / --include GLOB and REGEXP patterns.\n    'option_txt' is 'exclude' / 'include' / 'rexclude' / 'rinclude'\n    Returns: patterns_compiled, patterns_text\n    Note: process_patterns_from_file will ignore lines starting with # as these\n    are comments. To target escape the initial #, to use it in a file name, one\n    can use: \"[#]\" (for exclude) or \"\\#\" (for rexclude).\n    \"\"\"\n\n    patterns_compiled = []\n    patterns_textual = {}\n\n    if patterns_list is None:\n        patterns_list = []\n\n    if patterns_from:\n        ## Append patterns from glob_from\n        for fname in patterns_from:\n            debug(u\"processing --%s-from %s\" % (option_txt, fname))\n            patterns_list = process_patterns_from_file(fname, patterns_list)\n\n    for pattern in patterns_list:\n        debug(u\"processing %s rule: %s\" % (option_txt, patterns_list))\n        if is_glob:\n            pattern = glob.fnmatch.translate(pattern)\n        r = re.compile(pattern)\n        patterns_compiled.append(r)\n        patterns_textual[r] = pattern\n\n    return patterns_compiled, patterns_textual\n\ndef get_commands_list():\n    return [\n    {\"cmd\":\"mb\", \"label\":\"Make bucket\", \"param\":\"s3://BUCKET\", \"func\":cmd_bucket_create, \"argc\":1},\n    {\"cmd\":\"rb\", \"label\":\"Remove bucket\", \"param\":\"s3://BUCKET\", \"func\":cmd_bucket_delete, \"argc\":1},\n    {\"cmd\":\"ls\", \"label\":\"List objects or buckets\", \"param\":\"[s3://BUCKET[/PREFIX]]\", \"func\":cmd_ls, \"argc\":0},\n    {\"cmd\":\"la\", \"label\":\"List all object in all buckets\", \"param\":\"\", \"func\":cmd_all_buckets_list_all_content, \"argc\":0},\n    {\"cmd\":\"put\", \"label\":\"Put file into bucket\", \"param\":\"FILE [FILE...] s3://BUCKET[/PREFIX]\", \"func\":cmd_object_put, \"argc\":2},\n    {\"cmd\":\"get\", \"label\":\"Get file from bucket\", \"param\":\"s3://BUCKET/OBJECT LOCAL_FILE\", \"func\":cmd_object_get, \"argc\":1},\n    {\"cmd\":\"del\", \"label\":\"Delete file from bucket\", \"param\":\"s3://BUCKET/OBJECT\", \"func\":cmd_object_del, \"argc\":1},\n    {\"cmd\":\"rm\", \"label\":\"Delete file from bucket (alias for del)\", \"param\":\"s3://BUCKET/OBJECT\", \"func\":cmd_object_del, \"argc\":1},\n    #{\"cmd\":\"mkdir\", \"label\":\"Make a virtual S3 directory\", \"param\":\"s3://BUCKET/path/to/dir\", \"func\":cmd_mkdir, \"argc\":1},\n    {\"cmd\":\"restore\", \"label\":\"Restore file from Glacier storage\", \"param\":\"s3://BUCKET/OBJECT\", \"func\":cmd_object_restore, \"argc\":1},\n    {\"cmd\":\"sync\", \"label\":\"Synchronize a directory tree to S3 (checks files freshness using size and md5 checksum, unless overridden by options, see below)\", \"param\":\"LOCAL_DIR s3://BUCKET[/PREFIX] or s3://BUCKET[/PREFIX] LOCAL_DIR or s3://BUCKET[/PREFIX] s3://BUCKET[/PREFIX]\", \"func\":cmd_sync, \"argc\":2},\n    {\"cmd\":\"du\", \"label\":\"Disk usage by buckets\", \"param\":\"[s3://BUCKET[/PREFIX]]\", \"func\":cmd_du, \"argc\":0},\n    {\"cmd\":\"info\", \"label\":\"Get various information about Buckets or Files\", \"param\":\"s3://BUCKET[/OBJECT]\", \"func\":cmd_info, \"argc\":1},\n    {\"cmd\":\"cp\", \"label\":\"Copy object\", \"param\":\"s3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]\", \"func\":cmd_cp, \"argc\":2},\n    {\"cmd\":\"modify\", \"label\":\"Modify object metadata\", \"param\":\"s3://BUCKET1/OBJECT\", \"func\":cmd_modify, \"argc\":1},\n    {\"cmd\":\"mv\", \"label\":\"Move object\", \"param\":\"s3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]\", \"func\":cmd_mv, \"argc\":2},\n    {\"cmd\":\"setacl\", \"label\":\"Modify Access control list for Bucket or Files\", \"param\":\"s3://BUCKET[/OBJECT]\", \"func\":cmd_setacl, \"argc\":1},\n    {\"cmd\":\"setversioning\", \"label\":\"Modify Bucket Versioning\", \"param\":\"s3://BUCKET enable|disable\", \"func\":cmd_setversioning, \"argc\":2},\n    {\"cmd\":\"setownership\", \"label\":\"Modify Bucket Object Ownership\", \"param\":\"s3://BUCKET BucketOwnerPreferred|BucketOwnerEnforced|ObjectWriter\", \"func\":cmd_setownership, \"argc\":2},\n    {\"cmd\":\"setblockpublicaccess\", \"label\":\"Modify Block Public Access rules\", \"param\":\"s3://BUCKET BlockPublicAcls,IgnorePublicAcls,BlockPublicPolicy,RestrictPublicBuckets\", \"func\":cmd_setblockpublicaccess, \"argc\":2},\n\n    {\"cmd\":\"setobjectlegalhold\", \"label\":\"Modify Object Legal Hold\", \"param\":\"STATUS s3://BUCKET/OBJECT\", \"func\":cmd_setobjectlegalhold, \"argc\":2},\n    {\"cmd\":\"setobjectretention\", \"label\":\"Modify Object Retention\", \"param\":\"MODE RETAIN_UNTIL_DATE s3://BUCKET/OBJECT\", \"func\":cmd_setobjectretention, \"argc\":3},\n\n    {\"cmd\":\"setpolicy\", \"label\":\"Modify Bucket Policy\", \"param\":\"FILE s3://BUCKET\", \"func\":cmd_setpolicy, \"argc\":2},\n    {\"cmd\":\"delpolicy\", \"label\":\"Delete Bucket Policy\", \"param\":\"s3://BUCKET\", \"func\":cmd_delpolicy, \"argc\":1},\n    {\"cmd\":\"setcors\", \"label\":\"Modify Bucket CORS\", \"param\":\"FILE s3://BUCKET\", \"func\":cmd_setcors, \"argc\":2},\n    {\"cmd\":\"delcors\", \"label\":\"Delete Bucket CORS\", \"param\":\"s3://BUCKET\", \"func\":cmd_delcors, \"argc\":1},\n\n    {\"cmd\":\"payer\",     \"label\":\"Modify Bucket Requester Pays policy\", \"param\":\"s3://BUCKET\", \"func\":cmd_set_payer, \"argc\":1},\n    {\"cmd\":\"multipart\", \"label\":\"Show multipart uploads\", \"param\":\"s3://BUCKET [Id]\", \"func\":cmd_multipart, \"argc\":1},\n    {\"cmd\":\"abortmp\",   \"label\":\"Abort a multipart upload\", \"param\":\"s3://BUCKET/OBJECT Id\", \"func\":cmd_abort_multipart, \"argc\":2},\n\n    {\"cmd\":\"listmp\",    \"label\":\"List parts of a multipart upload\", \"param\":\"s3://BUCKET/OBJECT Id\", \"func\":cmd_list_multipart, \"argc\":2},\n\n    {\"cmd\":\"accesslog\", \"label\":\"Enable/disable bucket access logging\", \"param\":\"s3://BUCKET\", \"func\":cmd_accesslog, \"argc\":1},\n    {\"cmd\":\"sign\", \"label\":\"Sign arbitrary string using the secret key\", \"param\":\"STRING-TO-SIGN\", \"func\":cmd_sign, \"argc\":1},\n    {\"cmd\":\"signurl\", \"label\":\"Sign an S3 URL to provide limited public access with expiry\", \"param\":\"s3://BUCKET/OBJECT <expiry_epoch|+expiry_offset>\", \"func\":cmd_signurl, \"argc\":2},\n    {\"cmd\":\"fixbucket\", \"label\":\"Fix invalid file names in a bucket\", \"param\":\"s3://BUCKET[/PREFIX]\", \"func\":cmd_fixbucket, \"argc\":1},\n\n    ## Tagging commands\n    {\"cmd\":\"settagging\", \"label\":\"Modify tagging for Bucket or Files\", \"param\":\"s3://BUCKET[/OBJECT] \\\"KEY=VALUE[&KEY=VALUE ...]\\\"\", \"func\":cmd_settagging, \"argc\":2},\n    {\"cmd\":\"gettagging\", \"label\":\"Get tagging for Bucket or Files\",    \"param\":\"s3://BUCKET[/OBJECT]\", \"func\":cmd_gettagging, \"argc\":1},\n    {\"cmd\":\"deltagging\", \"label\":\"Delete tagging for Bucket or Files\", \"param\":\"s3://BUCKET[/OBJECT]\", \"func\":cmd_deltagging, \"argc\":1},\n\n    ## Website commands\n    {\"cmd\":\"ws-create\", \"label\":\"Create Website from bucket\", \"param\":\"s3://BUCKET\", \"func\":cmd_website_create, \"argc\":1},\n    {\"cmd\":\"ws-delete\", \"label\":\"Delete Website\", \"param\":\"s3://BUCKET\", \"func\":cmd_website_delete, \"argc\":1},\n    {\"cmd\":\"ws-info\", \"label\":\"Info about Website\", \"param\":\"s3://BUCKET\", \"func\":cmd_website_info, \"argc\":1},\n\n    ## Lifecycle commands\n    {\"cmd\":\"expire\", \"label\":\"Set or delete expiration rule for the bucket\", \"param\":\"s3://BUCKET\", \"func\":cmd_expiration_set, \"argc\":1},\n    {\"cmd\":\"setlifecycle\", \"label\":\"Upload a lifecycle policy for the bucket\", \"param\":\"FILE s3://BUCKET\", \"func\":cmd_setlifecycle, \"argc\":2},\n    {\"cmd\":\"getlifecycle\", \"label\":\"Get a lifecycle policy for the bucket\",    \"param\":\"s3://BUCKET\", \"func\":cmd_getlifecycle, \"argc\":1},\n    {\"cmd\":\"dellifecycle\", \"label\":\"Remove a lifecycle policy for the bucket\", \"param\":\"s3://BUCKET\", \"func\":cmd_dellifecycle, \"argc\":1},\n\n    ## Notification commands\n    {\"cmd\":\"setnotification\", \"label\":\"Upload a notification policy for the bucket\", \"param\":\"FILE s3://BUCKET\", \"func\":cmd_setnotification, \"argc\":2},\n    {\"cmd\":\"getnotification\", \"label\":\"Get a notification policy for the bucket\",    \"param\":\"s3://BUCKET\", \"func\":cmd_getnotification, \"argc\":1},\n    {\"cmd\":\"delnotification\", \"label\":\"Remove a notification policy for the bucket\", \"param\":\"s3://BUCKET\", \"func\":cmd_delnotification, \"argc\":1},\n\n    ## CloudFront commands\n    {\"cmd\":\"cflist\", \"label\":\"List CloudFront distribution points\", \"param\":\"\", \"func\":CfCmd.info, \"argc\":0},\n    {\"cmd\":\"cfinfo\", \"label\":\"Display CloudFront distribution point parameters\", \"param\":\"[cf://DIST_ID]\", \"func\":CfCmd.info, \"argc\":0},\n    {\"cmd\":\"cfcreate\", \"label\":\"Create CloudFront distribution point\", \"param\":\"s3://BUCKET\", \"func\":CfCmd.create, \"argc\":1},\n    {\"cmd\":\"cfdelete\", \"label\":\"Delete CloudFront distribution point\", \"param\":\"cf://DIST_ID\", \"func\":CfCmd.delete, \"argc\":1},\n    {\"cmd\":\"cfmodify\", \"label\":\"Change CloudFront distribution point parameters\", \"param\":\"cf://DIST_ID\", \"func\":CfCmd.modify, \"argc\":1},\n    {\"cmd\":\"cfinval\", \"label\":\"Invalidate CloudFront objects\", \"param\":\"s3://BUCKET/OBJECT [s3://BUCKET/OBJECT ...]\", \"func\":CfCmd.invalidate, \"argc\":1},\n    {\"cmd\":\"cfinvalinfo\", \"label\":\"Display CloudFront invalidation request(s) status\", \"param\":\"cf://DIST_ID[/INVAL_ID]\", \"func\":CfCmd.invalinfo, \"argc\":1},\n    ]\n\ndef format_commands(progname, commands_list):\n    help = \"Commands:\\n\"\n    for cmd in commands_list:\n        help += \"  %s\\n      %s %s %s\\n\" % (cmd[\"label\"], progname, cmd[\"cmd\"], cmd[\"param\"])\n    return help\n\n\ndef update_acl(s3, uri, seq_label=\"\"):\n    cfg = Config()\n    something_changed = False\n    acl = s3.get_acl(uri)\n    debug(u\"acl: %s - %r\" % (uri, acl.grantees))\n    if cfg.acl_public == True:\n        if acl.isAnonRead():\n            info(u\"%s: already Public, skipping %s\" % (uri, seq_label))\n        else:\n            acl.grantAnonRead()\n            something_changed = True\n    elif cfg.acl_public == False:  # we explicitly check for False, because it could be None\n        if not acl.isAnonRead() and not acl.isAnonWrite():\n            info(u\"%s: already Private, skipping %s\" % (uri, seq_label))\n        else:\n            acl.revokeAnonRead()\n            acl.revokeAnonWrite()\n            something_changed = True\n\n    # update acl with arguments\n    # grant first and revoke later, because revoke has priority\n    if cfg.acl_grants:\n        something_changed = True\n        for grant in cfg.acl_grants:\n            acl.grant(**grant)\n\n    if cfg.acl_revokes:\n        something_changed = True\n        for revoke in cfg.acl_revokes:\n            acl.revoke(**revoke)\n\n    if not something_changed:\n        return\n\n    retsponse = s3.set_acl(uri, acl)\n    if retsponse['status'] == 200:\n        if cfg.acl_public in (True, False):\n            set_to_acl = cfg.acl_public and \"Public\" or \"Private\"\n            output(u\"%s: ACL set to %s  %s\" % (uri, set_to_acl, seq_label))\n        else:\n            output(u\"%s: ACL updated\" % uri)\n\nclass OptionMimeType(Option):\n    def check_mimetype(self, opt, value):\n        if re.compile(r\"^[a-z0-9]+/[a-z0-9+\\.-]+(;.*)?$\", re.IGNORECASE).match(value):\n            return value\n        raise OptionValueError(\"option %s: invalid MIME-Type format: %r\" % (opt, value))\n\nclass OptionS3ACL(Option):\n    def check_s3acl(self, opt, value):\n        permissions = ('read', 'write', 'read_acp', 'write_acp', 'full_control', 'all')\n        try:\n            permission, grantee = re.compile(r\"^(\\w+):(.+)$\", re.IGNORECASE).match(value).groups()\n            if not permission or not grantee:\n                raise OptionValueError(\"option %s: invalid S3 ACL format: %r\" % (opt, value))\n            if permission in permissions:\n                return { 'name' : grantee, 'permission' : permission.upper() }\n            else:\n                raise OptionValueError(\"option %s: invalid S3 ACL permission: %s (valid values: %s)\" %\n                    (opt, permission, \", \".join(permissions)))\n        except OptionValueError:\n            raise\n        except Exception:\n            raise OptionValueError(\"option %s: invalid S3 ACL format: %r\" % (opt, value))\n\nclass OptionAll(OptionMimeType, OptionS3ACL):\n    TYPE_CHECKER = copy(Option.TYPE_CHECKER)\n    TYPE_CHECKER[\"mimetype\"] = OptionMimeType.check_mimetype\n    TYPE_CHECKER[\"s3acl\"] = OptionS3ACL.check_s3acl\n    TYPES = Option.TYPES + (\"mimetype\", \"s3acl\")\n\nclass MyHelpFormatter(IndentedHelpFormatter):\n    def format_epilog(self, epilog):\n        if epilog:\n            return \"\\n\" + epilog + \"\\n\"\n        else:\n            return \"\"\n\ndef main():\n    cfg = Config()\n    commands_list = get_commands_list()\n    commands = {}\n\n    ## Populate \"commands\" from \"commands_list\"\n    for cmd in commands_list:\n        if 'cmd' in cmd:\n            commands[cmd['cmd']] = cmd\n\n    optparser = OptionParser(option_class=OptionAll, formatter=MyHelpFormatter())\n    #optparser.disable_interspersed_args()\n\n    autodetected_encoding = locale.getpreferredencoding() or \"UTF-8\"\n\n    config_file = None\n    if os.getenv(\"S3CMD_CONFIG\"):\n        config_file = unicodise_s(os.getenv(\"S3CMD_CONFIG\"),\n                                  autodetected_encoding)\n    elif os.name == \"nt\" and os.getenv(\"USERPROFILE\"):\n        config_file = os.path.join(\n            unicodise_s(os.getenv(\"USERPROFILE\"), autodetected_encoding),\n            os.getenv(\"APPDATA\")\n               and unicodise_s(os.getenv(\"APPDATA\"), autodetected_encoding)\n               or 'Application Data',\n            \"s3cmd.ini\")\n    else:\n        from os.path import expanduser\n        config_file = os.path.join(expanduser(\"~\"), \".s3cfg\")\n\n    optparser.set_defaults(config = config_file)\n\n    optparser.add_option(      \"--configure\", dest=\"run_configure\", action=\"store_true\", help=\"Invoke interactive (re)configuration tool. Optionally use as '--configure s3://some-bucket' to test access to a specific bucket instead of attempting to list them all.\")\n    optparser.add_option(\"-c\", \"--config\", dest=\"config\", metavar=\"FILE\", help=\"Config file name. Defaults to $HOME/.s3cfg\")\n    optparser.add_option(      \"--dump-config\", dest=\"dump_config\", action=\"store_true\", help=\"Dump current configuration after parsing config files and command line options and exit.\")\n    optparser.add_option(      \"--access_key\", dest=\"access_key\", help=\"AWS Access Key\")\n    optparser.add_option(      \"--secret_key\", dest=\"secret_key\", help=\"AWS Secret Key\")\n    optparser.add_option(      \"--access_token\", dest=\"access_token\", help=\"AWS Access Token\")\n\n    optparser.add_option(\"-n\", \"--dry-run\", dest=\"dry_run\", action=\"store_true\", help=\"Only show what should be uploaded or downloaded but don't actually do it. May still perform S3 requests to get bucket listings and other information though (only for file transfer commands)\")\n\n    optparser.add_option(\"-s\", \"--ssl\", dest=\"use_https\", action=\"store_true\", help=\"Use HTTPS connection when communicating with S3. (default)\")\n    optparser.add_option(      \"--no-ssl\", dest=\"use_https\", action=\"store_false\", help=\"Don't use HTTPS.\")\n    optparser.add_option(\"-e\", \"--encrypt\", dest=\"encrypt\", action=\"store_true\", help=\"Encrypt files before uploading to S3.\")\n    optparser.add_option(      \"--no-encrypt\", dest=\"encrypt\", action=\"store_false\", help=\"Don't encrypt files.\")\n    optparser.add_option(\"-f\", \"--force\", dest=\"force\", action=\"store_true\", help=\"Force overwrite and other dangerous operations.\")\n    optparser.add_option(      \"--continue\", dest=\"get_continue\", action=\"store_true\", help=\"Continue getting a partially downloaded file (only for [get] command).\")\n    optparser.add_option(      \"--continue-put\", dest=\"put_continue\", action=\"store_true\", help=\"Continue uploading partially uploaded files or multipart upload parts.  Restarts parts/files that don't have matching size and md5.  Skips files/parts that do.  Note: md5sum checks are not always sufficient to check (part) file equality.  Enable this at your own risk.\")\n    optparser.add_option(      \"--upload-id\", dest=\"upload_id\", help=\"UploadId for Multipart Upload, in case you want continue an existing upload (equivalent to --continue-put) and there are multiple partial uploads.  Use s3cmd multipart [URI] to see what UploadIds are associated with the given URI.\")\n    optparser.add_option(      \"--skip-existing\", dest=\"skip_existing\", action=\"store_true\", help=\"Skip over files that exist at the destination (only for [get] and [sync] commands).\")\n    optparser.add_option(\"-r\", \"--recursive\", dest=\"recursive\", action=\"store_true\", help=\"Recursive upload, download or removal.\")\n    optparser.add_option(      \"--check-md5\", dest=\"check_md5\", action=\"store_true\", help=\"Check MD5 sums when comparing files for [sync]. (default)\")\n    optparser.add_option(      \"--no-check-md5\", dest=\"check_md5\", action=\"store_false\", help=\"Do not check MD5 sums when comparing files for [sync]. Only size will be compared. May significantly speed up transfer but may also miss some changed files.\")\n    optparser.add_option(\"-P\", \"--acl-public\", dest=\"acl_public\", action=\"store_true\", help=\"Store objects with ACL allowing read for anyone.\")\n    optparser.add_option(      \"--acl-private\", dest=\"acl_public\", action=\"store_false\", help=\"Store objects with default ACL allowing access for you only.\")\n    optparser.add_option(      \"--acl-grant\", dest=\"acl_grants\", type=\"s3acl\", action=\"append\", metavar=\"PERMISSION:EMAIL or USER_CANONICAL_ID\", help=\"Grant stated permission to a given amazon user. Permission is one of: read, write, read_acp, write_acp, full_control, all\")\n    optparser.add_option(      \"--acl-revoke\", dest=\"acl_revokes\", type=\"s3acl\", action=\"append\", metavar=\"PERMISSION:USER_CANONICAL_ID\", help=\"Revoke stated permission for a given amazon user. Permission is one of: read, write, read_acp, write_acp, full_control, all\")\n\n    optparser.add_option(\"-D\", \"--restore-days\", dest=\"restore_days\", action=\"store\", help=\"Number of days to keep restored file available (only for 'restore' command). Default is 1 day.\", metavar=\"NUM\")\n    optparser.add_option(      \"--restore-priority\", dest=\"restore_priority\", action=\"store\", choices=['standard', 'expedited', 'bulk'], help=\"Priority for restoring files from S3 Glacier (only for 'restore' command). Choices available: bulk, standard, expedited\")\n\n    optparser.add_option(      \"--delete-removed\", dest=\"delete_removed\", action=\"store_true\", help=\"Delete destination objects with no corresponding source file [sync]\")\n    optparser.add_option(      \"--no-delete-removed\", dest=\"delete_removed\", action=\"store_false\", help=\"Don't delete destination objects [sync]\")\n    optparser.add_option(      \"--delete-after\", dest=\"delete_after\", action=\"store_true\", help=\"Perform deletes AFTER new uploads when delete-removed is enabled [sync]\")\n    optparser.add_option(      \"--delay-updates\", dest=\"delay_updates\", action=\"store_true\", help=\"*OBSOLETE* Put all updated files into place at end [sync]\")  # OBSOLETE\n    optparser.add_option(      \"--max-delete\", dest=\"max_delete\", action=\"store\", help=\"Do not delete more than NUM files. [del] and [sync]\", metavar=\"NUM\")\n    optparser.add_option(      \"--limit\", dest=\"limit\", action=\"store\", help=\"Limit number of objects returned in the response body (only for [ls] and [la] commands)\", metavar=\"NUM\")\n    optparser.add_option(      \"--add-destination\", dest=\"additional_destinations\", action=\"append\", help=\"Additional destination for parallel uploads, in addition to last arg.  May be repeated.\")\n    optparser.add_option(      \"--delete-after-fetch\", dest=\"delete_after_fetch\", action=\"store_true\", help=\"Delete remote objects after fetching to local file (only for [get] and [sync] commands).\")\n    optparser.add_option(\"-p\", \"--preserve\", dest=\"preserve_attrs\", action=\"store_true\", help=\"Preserve filesystem attributes (mode, ownership, timestamps). Default for [sync] command.\")\n    optparser.add_option(      \"--no-preserve\", dest=\"preserve_attrs\", action=\"store_false\", help=\"Don't store FS attributes\")\n    optparser.add_option(      \"--keep-dirs\", dest=\"keep_dirs\", action=\"store_true\", help=\"Preserve all local directories as remote objects including empty directories. Experimental feature.\")\n    optparser.add_option(      \"--exclude\", dest=\"exclude\", action=\"append\", metavar=\"GLOB\", help=\"Filenames and paths matching GLOB will be excluded from sync\")\n    optparser.add_option(      \"--exclude-from\", dest=\"exclude_from\", action=\"append\", metavar=\"FILE\", help=\"Read --exclude GLOBs from FILE\")\n    optparser.add_option(      \"--rexclude\", dest=\"rexclude\", action=\"append\", metavar=\"REGEXP\", help=\"Filenames and paths matching REGEXP (regular expression) will be excluded from sync\")\n    optparser.add_option(      \"--rexclude-from\", dest=\"rexclude_from\", action=\"append\", metavar=\"FILE\", help=\"Read --rexclude REGEXPs from FILE\")\n    optparser.add_option(      \"--include\", dest=\"include\", action=\"append\", metavar=\"GLOB\", help=\"Filenames and paths matching GLOB will be included even if previously excluded by one of --(r)exclude(-from) patterns\")\n    optparser.add_option(      \"--include-from\", dest=\"include_from\", action=\"append\", metavar=\"FILE\", help=\"Read --include GLOBs from FILE\")\n    optparser.add_option(      \"--rinclude\", dest=\"rinclude\", action=\"append\", metavar=\"REGEXP\", help=\"Same as --include but uses REGEXP (regular expression) instead of GLOB\")\n    optparser.add_option(      \"--rinclude-from\", dest=\"rinclude_from\", action=\"append\", metavar=\"FILE\", help=\"Read --rinclude REGEXPs from FILE\")\n\n    optparser.add_option(      \"--files-from\", dest=\"files_from\", action=\"append\", metavar=\"FILE\", help=\"Read list of source-file names from FILE. Use - to read from stdin.\")\n    optparser.add_option(      \"--region\", \"--bucket-location\", metavar=\"REGION\", dest=\"bucket_location\", help=\"Region to create bucket in. As of now the regions are: us-east-1, us-west-1, us-west-2, eu-west-1, eu-central-1, ap-northeast-1, ap-southeast-1, ap-southeast-2, sa-east-1\")\n    optparser.add_option(      \"--host\", metavar=\"HOSTNAME\", dest=\"host_base\", help=\"HOSTNAME:PORT for S3 endpoint (default: %s, alternatives such as s3-eu-west-1.amazonaws.com). You should also set --host-bucket.\" % (cfg.host_base))\n    optparser.add_option(      \"--host-bucket\", dest=\"host_bucket\", help=\"DNS-style bucket+hostname:port template for accessing a bucket (default: %s)\" % (cfg.host_bucket))\n    optparser.add_option(      \"--reduced-redundancy\", \"--rr\", dest=\"reduced_redundancy\", action=\"store_true\", help=\"Store object with 'Reduced redundancy'. Lower per-GB price. [put, cp, mv]\")\n    optparser.add_option(      \"--no-reduced-redundancy\", \"--no-rr\", dest=\"reduced_redundancy\", action=\"store_false\", help=\"Store object without 'Reduced redundancy'. Higher per-GB price. [put, cp, mv]\")\n    optparser.add_option(      \"--storage-class\", dest=\"storage_class\", action=\"store\", metavar=\"CLASS\", help=\"Store object with specified CLASS (STANDARD, STANDARD_IA, ONEZONE_IA, INTELLIGENT_TIERING, GLACIER or DEEP_ARCHIVE). [put, cp, mv]\")\n    optparser.add_option(      \"--access-logging-target-prefix\", dest=\"log_target_prefix\", help=\"Target prefix for access logs (S3 URI) (for [cfmodify] and [accesslog] commands)\")\n    optparser.add_option(      \"--no-access-logging\", dest=\"log_target_prefix\", action=\"store_false\", help=\"Disable access logging (for [cfmodify] and [accesslog] commands)\")\n\n    optparser.add_option(      \"--default-mime-type\", dest=\"default_mime_type\", type=\"mimetype\", action=\"store\", help=\"Default MIME-type for stored objects. Application default is binary/octet-stream.\")\n    optparser.add_option(\"-M\", \"--guess-mime-type\", dest=\"guess_mime_type\", action=\"store_true\", help=\"Guess MIME-type of files by their extension or mime magic. Fall back to default MIME-Type as specified by --default-mime-type option\")\n    optparser.add_option(      \"--no-guess-mime-type\", dest=\"guess_mime_type\", action=\"store_false\", help=\"Don't guess MIME-type and use the default type instead.\")\n    optparser.add_option(      \"--no-mime-magic\", dest=\"use_mime_magic\", action=\"store_false\", help=\"Don't use mime magic when guessing MIME-type.\")\n    optparser.add_option(\"-m\", \"--mime-type\", dest=\"mime_type\", type=\"mimetype\", metavar=\"MIME/TYPE\", help=\"Force MIME-type. Override both --default-mime-type and --guess-mime-type.\")\n\n    optparser.add_option(      \"--add-header\", dest=\"add_header\", action=\"append\", metavar=\"NAME:VALUE\", help=\"Add a given HTTP header to the upload request. Can be used multiple times. For instance set 'Expires' or 'Cache-Control' headers (or both) using this option.\")\n    optparser.add_option(      \"--remove-header\", dest=\"remove_headers\", action=\"append\", metavar=\"NAME\", help=\"Remove a given HTTP header.  Can be used multiple times.  For instance, remove 'Expires' or 'Cache-Control' headers (or both) using this option. [modify]\")\n\n    optparser.add_option(      \"--server-side-encryption\", dest=\"server_side_encryption\", action=\"store_true\", help=\"Specifies that server-side encryption will be used when putting objects. [put, sync, cp, modify]\")\n    optparser.add_option(      \"--server-side-encryption-kms-id\", dest=\"kms_key\", action=\"store\", help=\"Specifies the key id used for server-side encryption with AWS KMS-Managed Keys (SSE-KMS) when putting objects. [put, sync, cp, modify]\")\n\n    optparser.add_option(      \"--encoding\", dest=\"encoding\", metavar=\"ENCODING\", help=\"Override autodetected terminal and filesystem encoding (character set). Autodetected: %s\" % autodetected_encoding)\n    optparser.add_option(      \"--add-encoding-exts\", dest=\"add_encoding_exts\", metavar=\"EXTENSIONs\", help=\"Add encoding to these comma delimited extensions i.e. (css,js,html) when uploading to S3 )\")\n    optparser.add_option(      \"--verbatim\", dest=\"urlencoding_mode\", action=\"store_const\", const=\"verbatim\", help=\"Use the S3 name as given on the command line. No pre-processing, encoding, etc. Use with caution!\")\n\n    optparser.add_option(      \"--disable-multipart\", dest=\"enable_multipart\", action=\"store_false\", help=\"Disable multipart upload on files bigger than --multipart-chunk-size-mb\")\n    optparser.add_option(      \"--multipart-chunk-size-mb\", dest=\"multipart_chunk_size_mb\", type=\"int\", action=\"store\", metavar=\"SIZE\", help=\"Size of each chunk of a multipart upload. Files bigger than SIZE are automatically uploaded as multithreaded-multipart, smaller files are uploaded using the traditional method. SIZE is in Mega-Bytes, default chunk size is 15MB, minimum allowed chunk size is 5MB, maximum is 5GB.\")\n\n    optparser.add_option(      \"--list-md5\", dest=\"list_md5\", action=\"store_true\", help=\"Include MD5 sums in bucket listings (only for 'ls' command).\")\n\n    optparser.add_option(      \"--list-allow-unordered\", dest=\"list_allow_unordered\", action=\"store_true\", help=\"Not an AWS standard. Allow the listing results to be returned in unsorted order. This may be faster when listing very large buckets.\")\n\n    optparser.add_option(\"-H\", \"--human-readable-sizes\", dest=\"human_readable_sizes\", action=\"store_true\", help=\"Print sizes in human readable form (eg 1kB instead of 1234).\")\n\n    optparser.add_option(      \"--ws-index\", dest=\"website_index\", action=\"store\", help=\"Name of index-document (only for [ws-create] command)\")\n    optparser.add_option(      \"--ws-error\", dest=\"website_error\", action=\"store\", help=\"Name of error-document (only for [ws-create] command)\")\n\n    optparser.add_option(      \"--expiry-date\", dest=\"expiry_date\", action=\"store\", help=\"Indicates when the expiration rule takes effect. (only for [expire] command)\")\n    optparser.add_option(      \"--expiry-days\", dest=\"expiry_days\", action=\"store\", help=\"Indicates the number of days after object creation the expiration rule takes effect. (only for [expire] command)\")\n    optparser.add_option(      \"--expiry-prefix\", dest=\"expiry_prefix\", action=\"store\", help=\"Identifying one or more objects with the prefix to which the expiration rule applies. (only for [expire] command)\")\n\n    optparser.add_option(      \"--skip-destination-validation\", dest=\"skip_destination_validation\", action=\"store_true\", help=\"Skips validation of Amazon SQS, Amazon SNS, and AWS Lambda destinations when applying notification configuration. (only for [setnotification] command)\")\n\n    optparser.add_option(      \"--progress\", dest=\"progress_meter\", action=\"store_true\", help=\"Display progress meter (default on TTY).\")\n    optparser.add_option(      \"--no-progress\", dest=\"progress_meter\", action=\"store_false\", help=\"Don't display progress meter (default on non-TTY).\")\n    optparser.add_option(      \"--stats\", dest=\"stats\", action=\"store_true\", help=\"Give some file-transfer stats.\")\n    optparser.add_option(      \"--enable\", dest=\"enable\", action=\"store_true\", help=\"Enable given CloudFront distribution (only for [cfmodify] command)\")\n    optparser.add_option(      \"--disable\", dest=\"enable\", action=\"store_false\", help=\"Disable given CloudFront distribution (only for [cfmodify] command)\")\n    optparser.add_option(      \"--cf-invalidate\", dest=\"invalidate_on_cf\", action=\"store_true\", help=\"Invalidate the uploaded filed in CloudFront. Also see [cfinval] command.\")\n    # joseprio: adding options to invalidate the default index and the default\n    # index root\n    optparser.add_option(      \"--cf-invalidate-default-index\", dest=\"invalidate_default_index_on_cf\", action=\"store_true\", help=\"When using Custom Origin and S3 static website, invalidate the default index file.\")\n    optparser.add_option(      \"--cf-no-invalidate-default-index-root\", dest=\"invalidate_default_index_root_on_cf\", action=\"store_false\", help=\"When using Custom Origin and S3 static website, don't invalidate the path to the default index file.\")\n    optparser.add_option(      \"--cf-add-cname\", dest=\"cf_cnames_add\", action=\"append\", metavar=\"CNAME\", help=\"Add given CNAME to a CloudFront distribution (only for [cfcreate] and [cfmodify] commands)\")\n    optparser.add_option(      \"--cf-remove-cname\", dest=\"cf_cnames_remove\", action=\"append\", metavar=\"CNAME\", help=\"Remove given CNAME from a CloudFront distribution (only for [cfmodify] command)\")\n    optparser.add_option(      \"--cf-comment\", dest=\"cf_comment\", action=\"store\", metavar=\"COMMENT\", help=\"Set COMMENT for a given CloudFront distribution (only for [cfcreate] and [cfmodify] commands)\")\n    optparser.add_option(      \"--cf-default-root-object\", dest=\"cf_default_root_object\", action=\"store\", metavar=\"DEFAULT_ROOT_OBJECT\", help=\"Set the default root object to return when no object is specified in the URL. Use a relative path, i.e. default/index.html instead of /default/index.html or s3://bucket/default/index.html (only for [cfcreate] and [cfmodify] commands)\")\n    optparser.add_option(\"-v\", \"--verbose\", dest=\"verbosity\", action=\"store_const\", const=logging.INFO, help=\"Enable verbose output.\")\n    optparser.add_option(\"-d\", \"--debug\", dest=\"verbosity\", action=\"store_const\", const=logging.DEBUG, help=\"Enable debug output.\")\n    optparser.add_option(      \"--version\", dest=\"show_version\", action=\"store_true\", help=\"Show s3cmd version (%s) and exit.\" % (PkgInfo.version))\n    optparser.add_option(\"-F\", \"--follow-symlinks\", dest=\"follow_symlinks\", action=\"store_true\", default=False, help=\"Follow symbolic links as if they are regular files\")\n    optparser.add_option(      \"--cache-file\", dest=\"cache_file\", action=\"store\", default=\"\",  metavar=\"FILE\", help=\"Cache FILE containing local source MD5 values\")\n    optparser.add_option(\"-q\", \"--quiet\", dest=\"quiet\", action=\"store_true\", default=False, help=\"Silence output on stdout\")\n    optparser.add_option(      \"--ca-certs\", dest=\"ca_certs_file\", action=\"store\", default=None, help=\"Path to SSL CA certificate FILE (instead of system default)\")\n    optparser.add_option(      \"--ssl-cert\", dest=\"ssl_client_cert_file\", action=\"store\", default=None, help=\"Path to client own SSL certificate CRT_FILE\")\n    optparser.add_option(      \"--ssl-key\", dest=\"ssl_client_key_file\", action=\"store\", default=None, help=\"Path to client own SSL certificate private key KEY_FILE\")\n    optparser.add_option(      \"--check-certificate\", dest=\"check_ssl_certificate\", action=\"store_true\", help=\"Check SSL certificate validity\")\n    optparser.add_option(      \"--no-check-certificate\", dest=\"check_ssl_certificate\", action=\"store_false\", help=\"Do not check SSL certificate validity\")\n    optparser.add_option(      \"--check-hostname\", dest=\"check_ssl_hostname\", action=\"store_true\", help=\"Check SSL certificate hostname validity\")\n    optparser.add_option(      \"--no-check-hostname\", dest=\"check_ssl_hostname\", action=\"store_false\", help=\"Do not check SSL certificate hostname validity\")\n    optparser.add_option(      \"--signature-v2\", dest=\"signature_v2\", action=\"store_true\", help=\"Use AWS Signature version 2 instead of newer signature methods. Helpful for S3-like systems that don't have AWS Signature v4 yet.\")\n    optparser.add_option(      \"--limit-rate\", dest=\"limitrate\", action=\"store\", type=\"string\", help=\"Limit the upload or download speed to amount bytes per second.  Amount may be expressed in bytes, kilobytes with the k suffix, or megabytes with the m suffix\")\n    optparser.add_option(      \"--no-connection-pooling\", dest=\"connection_pooling\", action=\"store_false\", help=\"Disable connection reuse\")\n    optparser.add_option(      \"--requester-pays\", dest=\"requester_pays\", action=\"store_true\", help=\"Set the REQUESTER PAYS flag for operations\")\n    optparser.add_option(\"-l\", \"--long-listing\", dest=\"long_listing\", action=\"store_true\", help=\"Produce long listing [ls]\")\n    optparser.add_option(      \"--stop-on-error\", dest=\"stop_on_error\", action=\"store_true\", help=\"stop if error in transfer\")\n    optparser.add_option(      \"--max-retries\", dest=\"max_retries\", action=\"store\", help=\"Maximum number of times to retry a failed request before giving up. Default is 5\", metavar=\"NUM\")\n    optparser.add_option(      \"--content-disposition\", dest=\"content_disposition\", action=\"store\", help=\"Provide a Content-Disposition for signed URLs, e.g., \\\"inline; filename=myvideo.mp4\\\"\")\n    optparser.add_option(      \"--content-type\", dest=\"content_type\", action=\"store\", help=\"Provide a Content-Type for signed URLs, e.g., \\\"video/mp4\\\"\")\n\n    optparser.set_usage(optparser.usage + \" COMMAND [parameters]\")\n    optparser.set_description('S3cmd is a tool for managing objects in '+\n        'Amazon S3 storage. It allows for making and removing '+\n        '\"buckets\" and uploading, downloading and removing '+\n        '\"objects\" from these buckets.')\n    optparser.epilog = format_commands(optparser.get_prog_name(), commands_list)\n    optparser.epilog += (\"\\nFor more information, updates and news, visit the s3cmd website:\\n%s\\n\" % PkgInfo.url)\n\n    (options, args) = optparser.parse_args()\n\n    ## Some mucking with logging levels to enable\n    ## debugging/verbose output for config file parser on request\n    logging.basicConfig(level=options.verbosity or Config().verbosity,\n                        format='%(levelname)s: %(message)s',\n                        stream = sys.stderr)\n\n    if options.show_version:\n        output(u\"s3cmd version %s\" % PkgInfo.version)\n        sys.exit(EX_OK)\n    debug(u\"s3cmd version %s\" % PkgInfo.version)\n\n    if options.quiet:\n        try:\n            f = open(\"/dev/null\", \"w\")\n            sys.stdout = f\n        except IOError:\n            warning(u\"Unable to open /dev/null: --quiet disabled.\")\n\n    ## Now finally parse the config file\n    if not options.config:\n        error(u\"Can't find a config file. Please use --config option.\")\n        sys.exit(EX_CONFIG)\n\n    try:\n        cfg = Config(options.config, options.access_key, options.secret_key, options.access_token)\n    except ValueError as exc:\n        raise ParameterError(unicode(exc))\n    except IOError as e:\n        if options.run_configure:\n            cfg = Config()\n        else:\n            error(u\"%s: %s\"  % (options.config, e.strerror))\n            error(u\"Configuration file not available.\")\n            error(u\"Consider using --configure parameter to create one.\")\n            sys.exit(EX_CONFIG)\n\n    # allow commandline verbosity config to override config file\n    if options.verbosity is not None:\n        cfg.verbosity = options.verbosity\n    logging.root.setLevel(cfg.verbosity)\n    ## Unsupported features on Win32 platform\n    if os.name == \"nt\":\n        if cfg.preserve_attrs:\n            error(u\"Option --preserve is not yet supported on MS Windows platform. Assuming --no-preserve.\")\n            cfg.preserve_attrs = False\n        if cfg.progress_meter:\n            error(u\"Option --progress is not yet supported on MS Windows platform. Assuming --no-progress.\")\n            cfg.progress_meter = False\n\n    ## Pre-process --add-header's and put them to Config.extra_headers SortedDict()\n    if options.add_header:\n        for hdr in options.add_header:\n            try:\n                key, val = unicodise_s(hdr).split(\":\", 1)\n            except ValueError:\n                raise ParameterError(\"Invalid header format: %s\" % unicodise_s(hdr))\n            # key char restrictions of the http headers name specification\n            key_inval = re.sub(r\"[a-zA-Z0-9\\-.!#$%&*+^_|]\", \"\", key)\n            if key_inval:\n                key_inval = key_inval.replace(\" \", \"<space>\")\n                key_inval = key_inval.replace(\"\\t\", \"<tab>\")\n                raise ParameterError(\"Invalid character(s) in header name '%s'\"\n                                     \": \\\"%s\\\"\" % (key, key_inval))\n            debug(u\"Updating Config.Config extra_headers[%s] -> %s\" %\n                  (key.strip().lower(), val.strip()))\n            cfg.extra_headers[key.strip().lower()] = val.strip()\n\n    # Process --remove-header\n    if options.remove_headers:\n        cfg.remove_headers = options.remove_headers\n\n    ## --acl-grant/--acl-revoke arguments are pre-parsed by OptionS3ACL()\n    if options.acl_grants:\n        for grant in options.acl_grants:\n            cfg.acl_grants.append(grant)\n\n    if options.acl_revokes:\n        for grant in options.acl_revokes:\n            cfg.acl_revokes.append(grant)\n\n    ## Process --(no-)check-md5\n    if options.check_md5 == False:\n        if \"md5\" in cfg.sync_checks:\n            cfg.sync_checks.remove(\"md5\")\n        if \"md5\" in cfg.preserve_attrs_list:\n            cfg.preserve_attrs_list.remove(\"md5\")\n    elif options.check_md5 == True:\n        if \"md5\" not in cfg.sync_checks:\n            cfg.sync_checks.append(\"md5\")\n        if \"md5\" not in cfg.preserve_attrs_list:\n            cfg.preserve_attrs_list.append(\"md5\")\n\n    ## Update Config with other parameters\n    for option in cfg.option_list():\n        try:\n            value = getattr(options, option)\n            if value != None:\n                if type(value) == type(b''):\n                    value = unicodise_s(value)\n                debug(u\"Updating Config.Config %s -> %s\" % (option, value))\n                cfg.update_option(option, value)\n        except AttributeError:\n            ## Some Config() options are not settable from command line\n            pass\n\n    ## Special handling for tri-state options (True, False, None)\n    cfg.update_option(\"enable\", options.enable)\n    if options.acl_public is not None:\n        cfg.update_option(\"acl_public\", options.acl_public)\n\n    ## Check multipart chunk constraints\n    if cfg.multipart_chunk_size_mb < MultiPartUpload.MIN_CHUNK_SIZE_MB:\n        raise ParameterError(\"Chunk size %d MB is too small, must be >= %d MB. Please adjust --multipart-chunk-size-mb\" % (cfg.multipart_chunk_size_mb, MultiPartUpload.MIN_CHUNK_SIZE_MB))\n    if cfg.multipart_chunk_size_mb > MultiPartUpload.MAX_CHUNK_SIZE_MB:\n        raise ParameterError(\"Chunk size %d MB is too large, must be <= %d MB. Please adjust --multipart-chunk-size-mb\" % (cfg.multipart_chunk_size_mb, MultiPartUpload.MAX_CHUNK_SIZE_MB))\n\n    ## If an UploadId was provided, set put_continue True\n    if options.upload_id:\n        cfg.upload_id = options.upload_id\n        cfg.put_continue = True\n\n    if cfg.upload_id and not cfg.multipart_chunk_size_mb:\n        raise ParameterError(\"Must have --multipart-chunk-size-mb if using --put-continue or --upload-id\")\n\n    ## CloudFront's cf_enable and Config's enable share the same --enable switch\n    options.cf_enable = options.enable\n\n    ## CloudFront's cf_logging and Config's log_target_prefix share the same --log-target-prefix switch\n    options.cf_logging = options.log_target_prefix\n\n    ## Update CloudFront options if some were set\n    for option in CfCmd.options.option_list():\n        try:\n            value = getattr(options, option)\n            if value != None:\n                if type(value) == type(b''):\n                    value = unicodise_s(value)\n            if value != None:\n                debug(u\"Updating CloudFront.Cmd %s -> %s\" % (option, value))\n                CfCmd.options.update_option(option, value)\n        except AttributeError:\n            ## Some CloudFront.Cmd.Options() options are not settable from command line\n            pass\n\n    if options.additional_destinations:\n        cfg.additional_destinations = options.additional_destinations\n    if options.files_from:\n        cfg.files_from = options.files_from\n\n    ## Set output and filesystem encoding for printing out filenames.\n    try:\n        # Support for python3\n        # That don't need codecs if output is the\n        # encoding of the system, but just in case, still use it.\n        # For that, we need to use directly the binary buffer\n        # of stdout/stderr\n        sys.stdout = codecs.getwriter(cfg.encoding)(sys.stdout.buffer, \"replace\")\n        sys.stderr = codecs.getwriter(cfg.encoding)(sys.stderr.buffer, \"replace\")\n        # getwriter with create an \"IObuffer\" that have not the encoding attribute\n        # better to add it to not break some functions like \"input\".\n        sys.stdout.encoding = cfg.encoding\n        sys.stderr.encoding = cfg.encoding\n    except AttributeError:\n        sys.stdout = codecs.getwriter(cfg.encoding)(sys.stdout, \"replace\")\n        sys.stderr = codecs.getwriter(cfg.encoding)(sys.stderr, \"replace\")\n\n    ## Process --exclude and --exclude-from\n    patterns_list, patterns_textual = process_patterns(options.exclude, options.exclude_from, is_glob = True, option_txt = \"exclude\")\n    cfg.exclude.extend(patterns_list)\n    cfg.debug_exclude.update(patterns_textual)\n\n    ## Process --rexclude and --rexclude-from\n    patterns_list, patterns_textual = process_patterns(options.rexclude, options.rexclude_from, is_glob = False, option_txt = \"rexclude\")\n    cfg.exclude.extend(patterns_list)\n    cfg.debug_exclude.update(patterns_textual)\n\n    ## Process --include and --include-from\n    patterns_list, patterns_textual = process_patterns(options.include, options.include_from, is_glob = True, option_txt = \"include\")\n    cfg.include.extend(patterns_list)\n    cfg.debug_include.update(patterns_textual)\n\n    ## Process --rinclude and --rinclude-from\n    patterns_list, patterns_textual = process_patterns(options.rinclude, options.rinclude_from, is_glob = False, option_txt = \"rinclude\")\n    cfg.include.extend(patterns_list)\n    cfg.debug_include.update(patterns_textual)\n\n    ## Set socket read()/write() timeout\n    socket.setdefaulttimeout(cfg.socket_timeout)\n\n    if cfg.encrypt and cfg.gpg_passphrase == \"\":\n        error(u\"Encryption requested but no passphrase set in config file.\")\n        error(u\"Please re-run 's3cmd --configure' and supply it.\")\n        sys.exit(EX_CONFIG)\n\n    if options.dump_config:\n        cfg.dump_config(sys.stdout)\n        sys.exit(EX_OK)\n\n    if options.run_configure:\n        # 'args' may contain the test-bucket URI\n        run_configure(options.config, args)\n        sys.exit(EX_OK)\n\n    ## set config if stop_on_error is set\n    if options.stop_on_error:\n        cfg.stop_on_error = options.stop_on_error\n\n    if options.content_disposition:\n        cfg.content_disposition = options.content_disposition\n\n    if options.content_type:\n        cfg.content_type = options.content_type\n\n    if len(args) < 1:\n        optparser.print_help()\n        sys.exit(EX_USAGE)\n\n    ## Unicodise all remaining arguments:\n    args = [unicodise(arg) for arg in args]\n\n    command = args.pop(0)\n    try:\n        debug(u\"Command: %s\" % commands[command][\"cmd\"])\n        ## We must do this lookup in extra step to\n        ## avoid catching all KeyError exceptions\n        ## from inner functions.\n        cmd_func = commands[command][\"func\"]\n    except KeyError as e:\n        error(u\"Invalid command: %s\", command)\n        sys.exit(EX_USAGE)\n\n    if len(args) < commands[command][\"argc\"]:\n        error(u\"Not enough parameters for command '%s'\" % command)\n        sys.exit(EX_USAGE)\n\n    rc = cmd_func(args)\n    if rc is None: # if we missed any cmd_*() returns\n        rc = EX_GENERAL\n    return rc\n\ndef report_exception(e, msg=u''):\n        alert_header = u\"\"\"\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    An unexpected error has occurred.\n  Please try reproducing the error using\n  the latest s3cmd code from the git master\n  branch found at:\n    https://github.com/s3tools/s3cmd\n  and have a look at the known issues list:\n    https://github.com/s3tools/s3cmd/wiki/Common-known-issues-and-their-solutions-(FAQ)\n  If the error persists, please report the\n  %s (removing any private\n  info as necessary) to:\n   s3tools-bugs@lists.sourceforge.net%s\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\"\"\"\n        sys.stderr.write(alert_header % (u\"following lines\", u\"\\n\\n\" + msg))\n        tb = traceback.format_exc()\n        try:\n            s = u' '.join([unicodise(a) for a in sys.argv])\n        except NameError:\n            # Error happened before Utils module was yet imported to provide\n            # unicodise\n            try:\n                s = u' '.join([(a) for a in sys.argv])\n            except UnicodeDecodeError:\n                s = u'[encoding safe] ' + u' '.join([('%r'%a) for a in sys.argv])\n        sys.stderr.write(u\"Invoked as: %s\\n\" % s)\n\n        e_class = str(e.__class__)\n        e_class = e_class[e_class.rfind(\".\")+1 : -2]\n        try:\n            sys.stderr.write(u\"Problem: %s: %s\\n\" % (e_class, e))\n        except UnicodeDecodeError:\n            sys.stderr.write(u\"Problem: [encoding safe] %r: %r\\n\"\n                             % (e_class, e))\n        try:\n            sys.stderr.write(u\"S3cmd:   %s\\n\" % PkgInfo.version)\n        except NameError:\n            sys.stderr.write(u\"S3cmd:   unknown version.\"\n                             \"Module import problem?\\n\")\n        sys.stderr.write(u\"python:   %s\\n\" % sys.version)\n        try:\n            sys.stderr.write(u\"environment LANG=%s\\n\"\n                             % unicodise_s(os.getenv(\"LANG\", \"NOTSET\"),\n                                           'ascii'))\n        except NameError:\n            # Error happened before Utils module was yet imported to provide\n            # unicodise\n            sys.stderr.write(u\"environment LANG=%s\\n\"\n                             % os.getenv(\"LANG\", \"NOTSET\"))\n        sys.stderr.write(u\"\\n\")\n        if type(tb) == unicode:\n            sys.stderr.write(tb)\n        else:\n            sys.stderr.write(unicode(tb, errors=\"replace\"))\n\n        if type(e) == ImportError:\n            sys.stderr.write(\"\\n\")\n            sys.stderr.write(\"Your sys.path contains these entries:\\n\")\n            for path in sys.path:\n                sys.stderr.write(u\"\\t%s\\n\" % path)\n            sys.stderr.write(\"Now the question is where have the s3cmd modules\"\n                             \" been installed?\\n\")\n\n        sys.stderr.write(alert_header % (u\"above lines\", u\"\"))\n\nif __name__ == '__main__':\n    try:\n        ## Our modules\n        ## Keep them in try/except block to\n        ## detect any syntax errors in there\n        from S3.ExitCodes import *\n        from S3.Exceptions import *\n        from S3 import PkgInfo\n        from S3.S3 import S3\n        from S3.Config import Config\n        from S3.SortedDict import SortedDict\n        from S3.FileDict import FileDict\n        from S3.S3Uri import S3Uri\n        from S3 import Utils\n        from S3.BaseUtils import (formatDateTime, getPrettyFromXml,\n                                  encode_to_s3, decode_from_s3, s3path)\n        from S3.Crypto import hash_file_md5, sign_string_v2, sign_url_v2\n        from S3.Utils import (formatSize, unicodise_safe, unicodise_s,\n                              unicodise, deunicodise, replace_nonprintables)\n        from S3.Progress import Progress, StatsInfo\n        from S3.CloudFront import Cmd as CfCmd\n        from S3.CloudFront import CloudFront\n        from S3.FileLists import *\n        from S3.MultiPart import MultiPartUpload\n    except Exception as e:\n        report_exception(e, \"Error loading some components of s3cmd (Import Error)\")\n        # 1 = EX_GENERAL but be safe in that situation\n        sys.exit(1)\n\n    try:\n        rc = main()\n        sys.exit(rc)\n\n    except ImportError as e:\n        report_exception(e)\n        sys.exit(EX_GENERAL)\n\n    except (ParameterError, InvalidFileError) as e:\n        error(u\"Parameter problem: %s\" % e)\n        sys.exit(EX_USAGE)\n\n    except (S3DownloadError, S3UploadError, S3RequestError) as e:\n        error(u\"S3 Temporary Error: %s.  Please try again later.\" % e)\n        sys.exit(EX_TEMPFAIL)\n\n    except S3Error as e:\n        error(u\"S3 error: %s\" % e)\n        sys.exit(e.get_error_code())\n\n    except (S3Exception, S3ResponseError, CloudFrontError) as e:\n        report_exception(e)\n        sys.exit(EX_SOFTWARE)\n\n    except SystemExit as e:\n        sys.exit(e.code)\n\n    except KeyboardInterrupt:\n        sys.stderr.write(\"See ya!\\n\")\n        sys.exit(EX_BREAK)\n\n    except (S3SSLError, S3SSLCertificateError) as e:\n        # SSLError is a subtype of IOError\n        error(\"SSL certificate verification failure: %s\" % e)\n        sys.exit(EX_ACCESSDENIED)\n\n    except ConnectionRefusedError as e:\n        error(\"Could not connect to server: %s\" % e)\n        sys.exit(EX_CONNECTIONREFUSED)\n        # typically encountered error is:\n        # ERROR: [Errno 111] Connection refused\n\n    except socket.gaierror as e:\n        # gaierror is a subset of IOError\n        # typically encountered error is:\n        # gaierror: [Errno -2] Name or service not known\n        error(e)\n        error(\"Connection Error: Error resolving a server hostname.\\n\"\n              \"Please check the servers address specified in 'host_base', 'host_bucket', 'cloudfront_host', 'website_endpoint'\")\n        sys.exit(EX_IOERR)\n\n    except IOError as e:\n        if e.errno in (errno.ECONNREFUSED, errno.EHOSTUNREACH):\n            # Python2 does not have ConnectionRefusedError\n            error(\"Could not connect to server: %s\" % e)\n            sys.exit(EX_CONNECTIONREFUSED)\n\n        if e.errno == errno.EPIPE:\n            # Fail silently on SIGPIPE. This likely means we wrote to a closed\n            # pipe and user does not care for any more output.\n            sys.exit(EX_IOERR)\n\n        report_exception(e)\n        sys.exit(EX_IOERR)\n\n    except OSError as e:\n        error(e)\n        sys.exit(EX_OSERR)\n\n    except MemoryError:\n        msg = \"\"\"\nMemoryError!  You have exceeded the amount of memory available for this process.\nThis usually occurs when syncing >750,000 files on a 32-bit python instance.\nThe solutions to this are:\n1) sync several smaller subtrees; or\n2) use a 64-bit python on a 64-bit OS with >8GB RAM\n        \"\"\"\n        sys.stderr.write(msg)\n        sys.exit(EX_OSERR)\n\n    except UnicodeEncodeError as e:\n        lang = unicodise_s(os.getenv(\"LANG\", \"NOTSET\"), 'ascii')\n        msg = \"\"\"\nYou have encountered a UnicodeEncodeError.  Your environment\nvariable LANG=%s may not specify a Unicode encoding (e.g. UTF-8).\nPlease set LANG=en_US.UTF-8 or similar in your environment before\ninvoking s3cmd.\n        \"\"\" % lang\n        report_exception(e, msg)\n        sys.exit(EX_GENERAL)\n\n    except Exception as e:\n        report_exception(e)\n        sys.exit(EX_GENERAL)\n\n# vim:et:ts=4:sts=4:ai\n"
        },
        {
          "name": "s3cmd.1",
          "type": "blob",
          "size": 22.8330078125,
          "content": "\n.\\\" !!! IMPORTANT: This file is generated from s3cmd \\-\\-help output using format-manpage.pl\n.\\\" !!!            Do your changes either in s3cmd file or in 'format\\-manpage.pl' otherwise\n.\\\" !!!            they will be overwritten!\n\n.TH s3cmd 1\n.SH NAME\ns3cmd \\- tool for managing Amazon S3 storage space and Amazon CloudFront content delivery network\n.SH SYNOPSIS\n.B s3cmd\n[\\fIOPTIONS\\fR] \\fICOMMAND\\fR [\\fIPARAMETERS\\fR]\n.SH DESCRIPTION\n.PP\n.B s3cmd\nis a command line client for copying files to/from\nAmazon S3 (Simple Storage Service) and performing other\nrelated tasks, for instance creating and removing buckets,\nlisting objects, etc.\n\n.SH COMMANDS\n.PP\n.B s3cmd\ncan do several \\fIactions\\fR specified by the following \\fIcommands\\fR.\n.TP\ns3cmd \\fBmb\\fR \\fIs3://BUCKET\\fR\nMake bucket\n.TP\ns3cmd \\fBrb\\fR \\fIs3://BUCKET\\fR\nRemove bucket\n.TP\ns3cmd \\fBls\\fR \\fI[s3://BUCKET[/PREFIX]]\\fR\nList objects or buckets\n.TP\ns3cmd \\fBla\\fR \\fI\\fR\nList all object in all buckets\n.TP\ns3cmd \\fBput\\fR \\fIFILE [FILE...] s3://BUCKET[/PREFIX]\\fR\nPut file into bucket\n.TP\ns3cmd \\fBget\\fR \\fIs3://BUCKET/OBJECT LOCAL_FILE\\fR\nGet file from bucket\n.TP\ns3cmd \\fBdel\\fR \\fIs3://BUCKET/OBJECT\\fR\nDelete file from bucket\n.TP\ns3cmd \\fBrm\\fR \\fIs3://BUCKET/OBJECT\\fR\nDelete file from bucket (alias for del)\n.TP\ns3cmd \\fBrestore\\fR \\fIs3://BUCKET/OBJECT\\fR\nRestore file from Glacier storage\n.TP\ns3cmd \\fBsync\\fR \\fILOCAL_DIR s3://BUCKET[/PREFIX] or s3://BUCKET[/PREFIX] LOCAL_DIR or s3://BUCKET[/PREFIX] s3://BUCKET[/PREFIX]\\fR\nSynchronize a directory tree to S3 (checks files freshness using size and md5 checksum, unless overridden by options, see below)\n.TP\ns3cmd \\fBdu\\fR \\fI[s3://BUCKET[/PREFIX]]\\fR\nDisk usage by buckets\n.TP\ns3cmd \\fBinfo\\fR \\fIs3://BUCKET[/OBJECT]\\fR\nGet various information about Buckets or Files\n.TP\ns3cmd \\fBcp\\fR \\fIs3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]\\fR\nCopy object\n.TP\ns3cmd \\fBmodify\\fR \\fIs3://BUCKET1/OBJECT\\fR\nModify object metadata\n.TP\ns3cmd \\fBmv\\fR \\fIs3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]\\fR\nMove object\n.TP\ns3cmd \\fBsetacl\\fR \\fIs3://BUCKET[/OBJECT]\\fR\nModify Access control list for Bucket or Files\n.TP\ns3cmd \\fBsetversioning\\fR \\fIs3://BUCKET enable|disable\\fR\nModify Bucket Versioning\n.TP\ns3cmd \\fBsetownership\\fR \\fIs3://BUCKET BucketOwnerPreferred|BucketOwnerEnforced|ObjectWriter\\fR\nModify Bucket Object Ownership\n.TP\ns3cmd \\fBsetblockpublicaccess\\fR \\fIs3://BUCKET BlockPublicAcls,IgnorePublicAcls,BlockPublicPolicy,RestrictPublicBuckets\\fR\nModify Block Public Access rules\n.TP\ns3cmd \\fBsetobjectlegalhold\\fR \\fISTATUS s3://BUCKET/OBJECT\\fR\nModify Object Legal Hold\n.TP\ns3cmd \\fBsetobjectretention\\fR \\fIMODE RETAIN_UNTIL_DATE s3://BUCKET/OBJECT\\fR\nModify Object Retention\n.TP\ns3cmd \\fBsetpolicy\\fR \\fIFILE s3://BUCKET\\fR\nModify Bucket Policy\n.TP\ns3cmd \\fBdelpolicy\\fR \\fIs3://BUCKET\\fR\nDelete Bucket Policy\n.TP\ns3cmd \\fBsetcors\\fR \\fIFILE s3://BUCKET\\fR\nModify Bucket CORS\n.TP\ns3cmd \\fBdelcors\\fR \\fIs3://BUCKET\\fR\nDelete Bucket CORS\n.TP\ns3cmd \\fBpayer\\fR \\fIs3://BUCKET\\fR\nModify Bucket Requester Pays policy\n.TP\ns3cmd \\fBmultipart\\fR \\fIs3://BUCKET [Id]\\fR\nShow multipart uploads\n.TP\ns3cmd \\fBabortmp\\fR \\fIs3://BUCKET/OBJECT Id\\fR\nAbort a multipart upload\n.TP\ns3cmd \\fBlistmp\\fR \\fIs3://BUCKET/OBJECT Id\\fR\nList parts of a multipart upload\n.TP\ns3cmd \\fBaccesslog\\fR \\fIs3://BUCKET\\fR\nEnable/disable bucket access logging\n.TP\ns3cmd \\fBsign\\fR \\fISTRING\\-TO\\-SIGN\\fR\nSign arbitrary string using the secret key\n.TP\ns3cmd \\fBsignurl\\fR \\fIs3://BUCKET/OBJECT <expiry_epoch|+expiry_offset>\\fR\nSign an S3 URL to provide limited public access with expiry\n.TP\ns3cmd \\fBfixbucket\\fR \\fIs3://BUCKET[/PREFIX]\\fR\nFix invalid file names in a bucket\n.TP\ns3cmd \\fBsettagging\\fR \\fIs3://BUCKET[/OBJECT] \"KEY=VALUE[&KEY=VALUE ...]\"\\fR\nModify tagging for Bucket or Files\n.TP\ns3cmd \\fBgettagging\\fR \\fIs3://BUCKET[/OBJECT]\\fR\nGet tagging for Bucket or Files\n.TP\ns3cmd \\fBdeltagging\\fR \\fIs3://BUCKET[/OBJECT]\\fR\nDelete tagging for Bucket or Files\n.TP\ns3cmd \\fBexpire\\fR \\fIs3://BUCKET\\fR\nSet or delete expiration rule for the bucket\n.TP\ns3cmd \\fBsetlifecycle\\fR \\fIFILE s3://BUCKET\\fR\nUpload a lifecycle policy for the bucket\n.TP\ns3cmd \\fBgetlifecycle\\fR \\fIs3://BUCKET\\fR\nGet a lifecycle policy for the bucket\n.TP\ns3cmd \\fBdellifecycle\\fR \\fIs3://BUCKET\\fR\nRemove a lifecycle policy for the bucket\n.TP\ns3cmd \\fBsetnotification\\fR \\fIFILE s3://BUCKET\\fR\nUpload a notification policy for the bucket\n.TP\ns3cmd \\fBgetnotification\\fR \\fIs3://BUCKET\\fR\nGet a notification policy for the bucket\n.TP\ns3cmd \\fBdelnotification\\fR \\fIs3://BUCKET\\fR\nRemove a notification policy for the bucket\n\n\n.PP\nCommands for static WebSites configuration\n.TP\ns3cmd \\fBws\\-create\\fR \\fIs3://BUCKET\\fR\nCreate Website from bucket\n.TP\ns3cmd \\fBws\\-delete\\fR \\fIs3://BUCKET\\fR\nDelete Website\n.TP\ns3cmd \\fBws\\-info\\fR \\fIs3://BUCKET\\fR\nInfo about Website\n\n\n.PP\nCommands for CloudFront management\n.TP\ns3cmd \\fBcflist\\fR \\fI\\fR\nList CloudFront distribution points\n.TP\ns3cmd \\fBcfinfo\\fR \\fI[cf://DIST_ID]\\fR\nDisplay CloudFront distribution point parameters\n.TP\ns3cmd \\fBcfcreate\\fR \\fIs3://BUCKET\\fR\nCreate CloudFront distribution point\n.TP\ns3cmd \\fBcfdelete\\fR \\fIcf://DIST_ID\\fR\nDelete CloudFront distribution point\n.TP\ns3cmd \\fBcfmodify\\fR \\fIcf://DIST_ID\\fR\nChange CloudFront distribution point parameters\n.TP\ns3cmd \\fBcfinval\\fR \\fIs3://BUCKET/OBJECT [s3://BUCKET/OBJECT ...]\\fR\nInvalidate CloudFront objects\n.TP\ns3cmd \\fBcfinvalinfo\\fR \\fIcf://DIST_ID[/INVAL_ID]\\fR\nDisplay CloudFront invalidation request(s) status\n\n\n.SH OPTIONS\n.PP\nSome of the below specified options can have their default\nvalues set in\n.B s3cmd\nconfig file (by default $HOME/.s3cmd). As it's a simple text file\nfeel free to open it with your favorite text editor and do any\nchanges you like.\n.TP\n\\fB\\-h\\fR, \\fB\\-\\-help\\fR\nshow this help message and exit\n.TP\n\\fB\\-\\-configure\\fR\nInvoke interactive (re)configuration tool. Optionally\nuse as '\\fB\\-\\-configure\\fR s3://some\\-bucket' to test access\nto a specific bucket instead of attempting to list\nthem all.\n.TP\n\\fB\\-c\\fR FILE, \\fB\\-\\-config\\fR=FILE\nConfig file name. Defaults to $HOME/.s3cfg\n.TP\n\\fB\\-\\-dump\\-config\\fR\nDump current configuration after parsing config files\nand command line options and exit.\n.TP\n\\fB\\-\\-access_key\\fR=ACCESS_KEY\nAWS Access Key\n.TP\n\\fB\\-\\-secret_key\\fR=SECRET_KEY\nAWS Secret Key\n.TP\n\\fB\\-\\-access_token\\fR=ACCESS_TOKEN\nAWS Access Token\n.TP\n\\fB\\-n\\fR, \\fB\\-\\-dry\\-run\\fR\nOnly show what should be uploaded or downloaded but\ndon't actually do it. May still perform S3 requests to\nget bucket listings and other information though (only\nfor file transfer commands)\n.TP\n\\fB\\-s\\fR, \\fB\\-\\-ssl\\fR\nUse HTTPS connection when communicating with S3.\n(default)\n.TP\n\\fB\\-\\-no\\-ssl\\fR\nDon't use HTTPS.\n.TP\n\\fB\\-e\\fR, \\fB\\-\\-encrypt\\fR\nEncrypt files before uploading to S3.\n.TP\n\\fB\\-\\-no\\-encrypt\\fR\nDon't encrypt files.\n.TP\n\\fB\\-f\\fR, \\fB\\-\\-force\\fR\nForce overwrite and other dangerous operations.\n.TP\n\\fB\\-\\-continue\\fR\nContinue getting a partially downloaded file (only for\n[get] command).\n.TP\n\\fB\\-\\-continue\\-put\\fR\nContinue uploading partially uploaded files or\nmultipart upload parts.  Restarts parts/files that\ndon't have matching size and md5.  Skips files/parts\nthat do.  Note: md5sum checks are not always\nsufficient to check (part) file equality.  Enable this\nat your own risk.\n.TP\n\\fB\\-\\-upload\\-id\\fR=UPLOAD_ID\nUploadId for Multipart Upload, in case you want\ncontinue an existing upload (equivalent to \\fB\\-\\-continue\\-\\fR\nput) and there are multiple partial uploads.  Use\ns3cmd multipart [URI] to see what UploadIds are\nassociated with the given URI.\n.TP\n\\fB\\-\\-skip\\-existing\\fR\nSkip over files that exist at the destination (only\nfor [get] and [sync] commands).\n.TP\n\\fB\\-r\\fR, \\fB\\-\\-recursive\\fR\nRecursive upload, download or removal.\n.TP\n\\fB\\-\\-check\\-md5\\fR\nCheck MD5 sums when comparing files for [sync].\n(default)\n.TP\n\\fB\\-\\-no\\-check\\-md5\\fR\nDo not check MD5 sums when comparing files for [sync].\nOnly size will be compared. May significantly speed up\ntransfer but may also miss some changed files.\n.TP\n\\fB\\-P\\fR, \\fB\\-\\-acl\\-public\\fR\nStore objects with ACL allowing read for anyone.\n.TP\n\\fB\\-\\-acl\\-private\\fR\nStore objects with default ACL allowing access for you\nonly.\n.TP\n\\fB\\-\\-acl\\-grant\\fR=PERMISSION:EMAIL or USER_CANONICAL_ID\nGrant stated permission to a given amazon user.\nPermission is one of: read, write, read_acp,\nwrite_acp, full_control, all\n.TP\n\\fB\\-\\-acl\\-revoke\\fR=PERMISSION:USER_CANONICAL_ID\nRevoke stated permission for a given amazon user.\nPermission is one of: read, write, read_acp,\nwrite_acp, full_control, all\n.TP\n\\fB\\-D\\fR NUM, \\fB\\-\\-restore\\-days\\fR=NUM\nNumber of days to keep restored file available (only\nfor 'restore' command). Default is 1 day.\n.TP\n\\fB\\-\\-restore\\-priority\\fR=RESTORE_PRIORITY\nPriority for restoring files from S3 Glacier (only for\n'restore' command). Choices available: bulk, standard,\nexpedited\n.TP\n\\fB\\-\\-delete\\-removed\\fR\nDelete destination objects with no corresponding\nsource file [sync]\n.TP\n\\fB\\-\\-no\\-delete\\-removed\\fR\nDon't delete destination objects [sync]\n.TP\n\\fB\\-\\-delete\\-after\\fR\nPerform deletes AFTER new uploads when delete-removed\nis enabled [sync]\n.TP\n\\fB\\-\\-delay\\-updates\\fR\n*OBSOLETE* Put all updated files into place at end\n[sync]\n.TP\n\\fB\\-\\-max\\-delete\\fR=NUM\nDo not delete more than NUM files. [del] and [sync]\n.TP\n\\fB\\-\\-limit\\fR=NUM\nLimit number of objects returned in the response body\n(only for [ls] and [la] commands)\n.TP\n\\fB\\-\\-add\\-destination\\fR=ADDITIONAL_DESTINATIONS\nAdditional destination for parallel uploads, in\naddition to last arg.  May be repeated.\n.TP\n\\fB\\-\\-delete\\-after\\-fetch\\fR\nDelete remote objects after fetching to local file\n(only for [get] and [sync] commands).\n.TP\n\\fB\\-p\\fR, \\fB\\-\\-preserve\\fR\nPreserve filesystem attributes (mode, ownership,\ntimestamps). Default for [sync] command.\n.TP\n\\fB\\-\\-no\\-preserve\\fR\nDon't store FS attributes\n.TP\n\\fB\\-\\-keep\\-dirs\\fR\nPreserve all local directories as remote objects\nincluding empty directories. Experimental feature.\n.TP\n\\fB\\-\\-exclude\\fR=GLOB\nFilenames and paths matching GLOB will be excluded\nfrom sync\n.TP\n\\fB\\-\\-exclude\\-from\\fR=FILE\nRead --exclude GLOBs from FILE\n.TP\n\\fB\\-\\-rexclude\\fR=REGEXP\nFilenames and paths matching REGEXP (regular\nexpression) will be excluded from sync\n.TP\n\\fB\\-\\-rexclude\\-from\\fR=FILE\nRead --rexclude REGEXPs from FILE\n.TP\n\\fB\\-\\-include\\fR=GLOB\nFilenames and paths matching GLOB will be included\neven if previously excluded by one of\n\\fB\\-\\-(r)exclude(\\-from)\\fR patterns\n.TP\n\\fB\\-\\-include\\-from\\fR=FILE\nRead --include GLOBs from FILE\n.TP\n\\fB\\-\\-rinclude\\fR=REGEXP\nSame as --include but uses REGEXP (regular expression)\ninstead of GLOB\n.TP\n\\fB\\-\\-rinclude\\-from\\fR=FILE\nRead --rinclude REGEXPs from FILE\n.TP\n\\fB\\-\\-files\\-from\\fR=FILE\nRead list of source-file names from FILE. Use - to\nread from stdin.\n.TP\n\\fB\\-\\-region\\fR=REGION, \\fB\\-\\-bucket\\-location\\fR=REGION\nRegion to create bucket in. As of now the regions are:\nus\\-east\\-1, us\\-west\\-1, us\\-west\\-2, eu\\-west\\-1, eu\\-\ncentral\\-1, ap\\-northeast\\-1, ap\\-southeast\\-1, ap\\-\nsoutheast\\-2, sa\\-east\\-1\n.TP\n\\fB\\-\\-host\\fR=HOSTNAME\nHOSTNAME:PORT for S3 endpoint (default:\ns3.amazonaws.com, alternatives such as s3\\-eu\\-\nwest\\-1.amazonaws.com). You should also set \\fB\\-\\-host\\-\\fR\nbucket.\n.TP\n\\fB\\-\\-host\\-bucket\\fR=HOST_BUCKET\nDNS\\-style bucket+hostname:port template for accessing\na bucket (default: %(bucket)s.s3.amazonaws.com)\n.TP\n\\fB\\-\\-reduced\\-redundancy\\fR, \\fB\\-\\-rr\\fR\nStore object with 'Reduced redundancy'. Lower per\\-GB\nprice. [put, cp, mv]\n.TP\n\\fB\\-\\-no\\-reduced\\-redundancy\\fR, \\fB\\-\\-no\\-rr\\fR\nStore object without 'Reduced redundancy'. Higher per\\-\nGB price. [put, cp, mv]\n.TP\n\\fB\\-\\-storage\\-class\\fR=CLASS\nStore object with specified CLASS (STANDARD,\nSTANDARD_IA, ONEZONE_IA, INTELLIGENT_TIERING, GLACIER\nor DEEP_ARCHIVE). [put, cp, mv]\n.TP\n\\fB\\-\\-access\\-logging\\-target\\-prefix\\fR=LOG_TARGET_PREFIX\nTarget prefix for access logs (S3 URI) (for [cfmodify]\nand [accesslog] commands)\n.TP\n\\fB\\-\\-no\\-access\\-logging\\fR\nDisable access logging (for [cfmodify] and [accesslog]\ncommands)\n.TP\n\\fB\\-\\-default\\-mime\\-type\\fR=DEFAULT_MIME_TYPE\nDefault MIME\\-type for stored objects. Application\ndefault is binary/octet\\-stream.\n.TP\n\\fB\\-M\\fR, \\fB\\-\\-guess\\-mime\\-type\\fR\nGuess MIME\\-type of files by their extension or mime\nmagic. Fall back to default MIME\\-Type as specified by\n\\fB\\-\\-default\\-mime\\-type\\fR option\n.TP\n\\fB\\-\\-no\\-guess\\-mime\\-type\\fR\nDon't guess MIME-type and use the default type\ninstead.\n.TP\n\\fB\\-\\-no\\-mime\\-magic\\fR\nDon't use mime magic when guessing MIME-type.\n.TP\n\\fB\\-m\\fR MIME/TYPE, \\fB\\-\\-mime\\-type\\fR=MIME/TYPE\nForce MIME\\-type. Override both \\fB\\-\\-default\\-mime\\-type\\fR and\n\\fB\\-\\-guess\\-mime\\-type\\fR.\n.TP\n\\fB\\-\\-add\\-header\\fR=NAME:VALUE\nAdd a given HTTP header to the upload request. Can be\nused multiple times. For instance set 'Expires' or\n\\&'Cache\\-Control' headers (or both) using this option.\n.TP\n\\fB\\-\\-remove\\-header\\fR=NAME\nRemove a given HTTP header.  Can be used multiple\ntimes.  For instance, remove 'Expires' or 'Cache\\-\nControl' headers (or both) using this option. [modify]\n.TP\n\\fB\\-\\-server\\-side\\-encryption\\fR\nSpecifies that server\\-side encryption will be used\nwhen putting objects. [put, sync, cp, modify]\n.TP\n\\fB\\-\\-server\\-side\\-encryption\\-kms\\-id\\fR=KMS_KEY\nSpecifies the key id used for server\\-side encryption\nwith AWS KMS\\-Managed Keys (SSE\\-KMS) when putting\nobjects. [put, sync, cp, modify]\n.TP\n\\fB\\-\\-encoding\\fR=ENCODING\nOverride autodetected terminal and filesystem encoding\n(character set). Autodetected: UTF\\-8\n.TP\n\\fB\\-\\-add\\-encoding\\-exts\\fR=EXTENSIONs\nAdd encoding to these comma delimited extensions i.e.\n(css,js,html) when uploading to S3 )\n.TP\n\\fB\\-\\-verbatim\\fR\nUse the S3 name as given on the command line. No pre-\nprocessing, encoding, etc. Use with caution!\n.TP\n\\fB\\-\\-disable\\-multipart\\fR\nDisable multipart upload on files bigger than\n\\fB\\-\\-multipart\\-chunk\\-size\\-mb\\fR\n.TP\n\\fB\\-\\-multipart\\-chunk\\-size\\-mb\\fR=SIZE\nSize of each chunk of a multipart upload. Files bigger\nthan SIZE are automatically uploaded as multithreaded\\-\nmultipart, smaller files are uploaded using the\ntraditional method. SIZE is in Mega\\-Bytes, default\nchunk size is 15MB, minimum allowed chunk size is 5MB,\nmaximum is 5GB.\n.TP\n\\fB\\-\\-list\\-md5\\fR\nInclude MD5 sums in bucket listings (only for 'ls'\ncommand).\n.TP\n\\fB\\-\\-list\\-allow\\-unordered\\fR\nNot an AWS standard. Allow the listing results to be\nreturned in unsorted order. This may be faster when\nlisting very large buckets.\n.TP\n\\fB\\-H\\fR, \\fB\\-\\-human\\-readable\\-sizes\\fR\nPrint sizes in human readable form (eg 1kB instead of\n1234).\n.TP\n\\fB\\-\\-ws\\-index\\fR=WEBSITE_INDEX\nName of index\\-document (only for [ws\\-create] command)\n.TP\n\\fB\\-\\-ws\\-error\\fR=WEBSITE_ERROR\nName of error\\-document (only for [ws\\-create] command)\n.TP\n\\fB\\-\\-expiry\\-date\\fR=EXPIRY_DATE\nIndicates when the expiration rule takes effect. (only\nfor [expire] command)\n.TP\n\\fB\\-\\-expiry\\-days\\fR=EXPIRY_DAYS\nIndicates the number of days after object creation the\nexpiration rule takes effect. (only for [expire]\ncommand)\n.TP\n\\fB\\-\\-expiry\\-prefix\\fR=EXPIRY_PREFIX\nIdentifying one or more objects with the prefix to\nwhich the expiration rule applies. (only for [expire]\ncommand)\n.TP\n\\fB\\-\\-skip\\-destination\\-validation\\fR\nSkips validation of Amazon SQS, Amazon SNS, and AWS\nLambda destinations when applying notification\nconfiguration. (only for [setnotification] command)\n.TP\n\\fB\\-\\-progress\\fR\nDisplay progress meter (default on TTY).\n.TP\n\\fB\\-\\-no\\-progress\\fR\nDon't display progress meter (default on non-TTY).\n.TP\n\\fB\\-\\-stats\\fR\nGive some file-transfer stats.\n.TP\n\\fB\\-\\-enable\\fR\nEnable given CloudFront distribution (only for\n[cfmodify] command)\n.TP\n\\fB\\-\\-disable\\fR\nDisable given CloudFront distribution (only for\n[cfmodify] command)\n.TP\n\\fB\\-\\-cf\\-invalidate\\fR\nInvalidate the uploaded filed in CloudFront. Also see\n[cfinval] command.\n.TP\n\\fB\\-\\-cf\\-invalidate\\-default\\-index\\fR\nWhen using Custom Origin and S3 static website,\ninvalidate the default index file.\n.TP\n\\fB\\-\\-cf\\-no\\-invalidate\\-default\\-index\\-root\\fR\nWhen using Custom Origin and S3 static website, don't\ninvalidate the path to the default index file.\n.TP\n\\fB\\-\\-cf\\-add\\-cname\\fR=CNAME\nAdd given CNAME to a CloudFront distribution (only for\n[cfcreate] and [cfmodify] commands)\n.TP\n\\fB\\-\\-cf\\-remove\\-cname\\fR=CNAME\nRemove given CNAME from a CloudFront distribution\n(only for [cfmodify] command)\n.TP\n\\fB\\-\\-cf\\-comment\\fR=COMMENT\nSet COMMENT for a given CloudFront distribution (only\nfor [cfcreate] and [cfmodify] commands)\n.TP\n\\fB\\-\\-cf\\-default\\-root\\-object\\fR=DEFAULT_ROOT_OBJECT\nSet the default root object to return when no object\nis specified in the URL. Use a relative path, i.e.\ndefault/index.html instead of /default/index.html or\ns3://bucket/default/index.html (only for [cfcreate]\nand [cfmodify] commands)\n.TP\n\\fB\\-v\\fR, \\fB\\-\\-verbose\\fR\nEnable verbose output.\n.TP\n\\fB\\-d\\fR, \\fB\\-\\-debug\\fR\nEnable debug output.\n.TP\n\\fB\\-\\-version\\fR\nShow s3cmd version (2.4.0) and exit.\n.TP\n\\fB\\-F\\fR, \\fB\\-\\-follow\\-symlinks\\fR\nFollow symbolic links as if they are regular files\n.TP\n\\fB\\-\\-cache\\-file\\fR=FILE\nCache FILE containing local source MD5 values\n.TP\n\\fB\\-q\\fR, \\fB\\-\\-quiet\\fR\nSilence output on stdout\n.TP\n\\fB\\-\\-ca\\-certs\\fR=CA_CERTS_FILE\nPath to SSL CA certificate FILE (instead of system\ndefault)\n.TP\n\\fB\\-\\-ssl\\-cert\\fR=SSL_CLIENT_CERT_FILE\nPath to client own SSL certificate CRT_FILE\n.TP\n\\fB\\-\\-ssl\\-key\\fR=SSL_CLIENT_KEY_FILE\nPath to client own SSL certificate private key\nKEY_FILE\n.TP\n\\fB\\-\\-check\\-certificate\\fR\nCheck SSL certificate validity\n.TP\n\\fB\\-\\-no\\-check\\-certificate\\fR\nDo not check SSL certificate validity\n.TP\n\\fB\\-\\-check\\-hostname\\fR\nCheck SSL certificate hostname validity\n.TP\n\\fB\\-\\-no\\-check\\-hostname\\fR\nDo not check SSL certificate hostname validity\n.TP\n\\fB\\-\\-signature\\-v2\\fR\nUse AWS Signature version 2 instead of newer signature\nmethods. Helpful for S3\\-like systems that don't have\nAWS Signature v4 yet.\n.TP\n\\fB\\-\\-limit\\-rate\\fR=LIMITRATE\nLimit the upload or download speed to amount bytes per\nsecond.  Amount may be expressed in bytes, kilobytes\nwith the k suffix, or megabytes with the m suffix\n.TP\n\\fB\\-\\-no\\-connection\\-pooling\\fR\nDisable connection reuse\n.TP\n\\fB\\-\\-requester\\-pays\\fR\nSet the REQUESTER PAYS flag for operations\n.TP\n\\fB\\-l\\fR, \\fB\\-\\-long\\-listing\\fR\nProduce long listing [ls]\n.TP\n\\fB\\-\\-stop\\-on\\-error\\fR\nstop if error in transfer\n.TP\n\\fB\\-\\-max\\-retries\\fR=NUM\nMaximum number of times to retry a failed request\nbefore giving up. Default is 5\n.TP\n\\fB\\-\\-content\\-disposition\\fR=CONTENT_DISPOSITION\nProvide a Content\\-Disposition for signed URLs, e.g.,\n\"inline; filename=myvideo.mp4\"\n.TP\n\\fB\\-\\-content\\-type\\fR=CONTENT_TYPE\nProvide a Content\\-Type for signed URLs, e.g.,\n\"video/mp4\"\n\n\n.SH EXAMPLES\nOne of the most powerful commands of \\fIs3cmd\\fR is \\fBs3cmd sync\\fR used for\nsynchronising complete directory trees to or from remote S3 storage. To some extent\n\\fBs3cmd put\\fR and \\fBs3cmd get\\fR share a similar behaviour with \\fBsync\\fR.\n.PP\nBasic usage common in backup scenarios is as simple as:\n.nf\n\ts3cmd sync /local/path/ s3://test\\-bucket/backup/\n.fi\n.PP\nThis command will find all files under /local/path directory and copy them\nto corresponding paths under s3://test\\-bucket/backup on the remote side.\nFor example:\n.nf\n\t/local/path/\\fBfile1.ext\\fR         \\->  s3://bucket/backup/\\fBfile1.ext\\fR\n\t/local/path/\\fBdir123/file2.bin\\fR  \\->  s3://bucket/backup/\\fBdir123/file2.bin\\fR\n.fi\n.PP\nHowever if the local path doesn't end with a slash the last directory's name\nis used on the remote side as well. Compare these with the previous example:\n.nf\n\ts3cmd sync /local/path s3://test\\-bucket/backup/\n.fi\nwill sync:\n.nf\n\t/local/\\fBpath/file1.ext\\fR         \\->  s3://bucket/backup/\\fBpath/file1.ext\\fR\n\t/local/\\fBpath/dir123/file2.bin\\fR  \\->  s3://bucket/backup/\\fBpath/dir123/file2.bin\\fR\n.fi\n.PP\nTo retrieve the files back from S3 use inverted syntax:\n.nf\n\ts3cmd sync s3://test\\-bucket/backup/ ~/restore/\n.fi\nthat will download files:\n.nf\n\ts3://bucket/backup/\\fBfile1.ext\\fR         \\->  ~/restore/\\fBfile1.ext\\fR\n\ts3://bucket/backup/\\fBdir123/file2.bin\\fR  \\->  ~/restore/\\fBdir123/file2.bin\\fR\n.fi\n.PP\nWithout the trailing slash on source the behaviour is similar to\nwhat has been demonstrated with upload:\n.nf\n\ts3cmd sync s3://test\\-bucket/backup ~/restore/\n.fi\nwill download the files as:\n.nf\n\ts3://bucket/\\fBbackup/file1.ext\\fR         \\->  ~/restore/\\fBbackup/file1.ext\\fR\n\ts3://bucket/\\fBbackup/dir123/file2.bin\\fR  \\->  ~/restore/\\fBbackup/dir123/file2.bin\\fR\n.fi\n.PP\nAll source file names, the bold ones above, are matched against \\fBexclude\\fR\nrules and those that match are then re\\-checked against \\fBinclude\\fR rules to see\nwhether they should be excluded or kept in the source list.\n.PP\nFor the purpose of \\fB\\-\\-exclude\\fR and \\fB\\-\\-include\\fR matching only the\nbold file names above are used. For instance only \\fBpath/file1.ext\\fR is tested\nagainst the patterns, not \\fI/local/\\fBpath/file1.ext\\fR\n.PP\nBoth \\fB\\-\\-exclude\\fR and \\fB\\-\\-include\\fR work with shell\\-style wildcards (a.k.a. GLOB).\nFor a greater flexibility s3cmd provides Regular\\-expression versions of the two exclude options\nnamed \\fB\\-\\-rexclude\\fR and \\fB\\-\\-rinclude\\fR.\nThe options with ...\\fB\\-from\\fR suffix (eg \\-\\-rinclude\\-from) expect a filename as\nan argument. Each line of such a file is treated as one pattern.\n.PP\nThere is only one set of patterns built from all \\fB\\-\\-(r)exclude(\\-from)\\fR options\nand similarly for include variant. Any file excluded with eg \\-\\-exclude can\nbe put back with a pattern found in \\-\\-rinclude\\-from list.\n.PP\nRun s3cmd with \\fB\\-\\-dry\\-run\\fR to verify that your rules work as expected.\nUse together with \\fB\\-\\-debug\\fR get detailed information\nabout matching file names against exclude and include rules.\n.PP\nFor example to exclude all files with \".jpg\" extension except those beginning with a number use:\n.PP\n\t\\-\\-exclude '*.jpg' \\-\\-rinclude '[0\\-9].*\\.jpg'\n.PP\nTo exclude all files except \"*.jpg\" extension, use:\n.PP\n\t\\-\\-exclude '*' \\-\\-include '*.jpg'\n.PP\nTo exclude local directory 'somedir', be sure to use a trailing forward slash, as such:\n.PP\n\t\\-\\-exclude 'somedir/'\n.PP\n\n.SH SEE ALSO\nFor the most up to date list of options run:\n.B s3cmd \\-\\-help\n.br\nFor more info about usage, examples and other related info visit project homepage at:\n.B https://s3tools.org\n.SH AUTHOR\nWritten by Michal Ludvig, Florent Viard and contributors\n.SH CONTACT, SUPPORT\nPreferred way to get support is our mailing list:\n.br\n.I s3tools\\-general@lists.sourceforge.net\n.br\nor visit the project homepage:\n.br\n.B https://s3tools.org\n.SH REPORTING BUGS\nReport bugs to\n.I s3tools\\-bugs@lists.sourceforge.net\n.SH COPYRIGHT\nCopyright \\(co 2007\\-2023 TGRMN Software (https://www.tgrmn.com), Sodria SAS (https://www.sodria.com) and contributors\n.br\n.SH LICENSE\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n.br\n"
        },
        {
          "name": "s3cmd.spec.in",
          "type": "blob",
          "size": 5.916015625,
          "content": "%{!?python_sitelib: %define python_sitelib %(%{__python} -c \"from distutils.sysconfig import get_python_lib; print get_python_lib()\")}\n\n%global commit ##COMMIT##\n%global shortcommit ##SHORTCOMMIT##\n\nName:           s3cmd\nVersion:        ##VERSION##\nRelease:        1%{dist}\nSummary:        Tool for accessing Amazon Simple Storage Service\n\nGroup:          Applications/Internet\nLicense:        GPLv2\nURL:            http://s3tools.com\n# git clone https://github.com/s3tools/s3cmd\n# python setup.py sdist\nSource0:        https://github.com/s3tools/s3cmd/archive/%{commit}/%{name}-%{version}-%{shortcommit}.tar.gz\nBuildRoot:      %{_tmppath}/%{name}-%{version}-%{release}-root-%(%{__id_u} -n)\nBuildArch:      noarch\n\n%if %{!?fedora:16}%{?fedora} < 16 || %{!?rhel:7}%{?rhel} < 7\nBuildRequires:  python-devel\n%else\nBuildRequires:  python2-devel\n%endif\n%if %{!?fedora:8}%{?fedora} < 8 || %{!?rhel:6}%{?rhel} < 6\n# This is in standard library since 2.5\nBuildRequires:  python-elementtree\nRequires:       python-elementtree\n%endif\nBuildRequires:  python-dateutil\nBuildRequires:  python-setuptools\nRequires:       python-dateutil\nRequires:       python-magic\n\n%description\nS3cmd lets you copy files from/to Amazon S3\n(Simple Storage Service) using a simple to use\ncommand line client.\n\n\n%prep\n%setup -q -n s3cmd-%{commit}\n\n%build\n\n\n%install\nrm -rf $RPM_BUILD_ROOT\nS3CMD_PACKAGING=Yes python setup.py install --prefix=%{_prefix} --root=$RPM_BUILD_ROOT\ninstall -d $RPM_BUILD_ROOT%{_mandir}/man1\ninstall -m 644 s3cmd.1 $RPM_BUILD_ROOT%{_mandir}/man1\n\n\n%clean\nrm -rf $RPM_BUILD_ROOT\n\n\n%files\n%defattr(-,root,root,-)\n%{_bindir}/s3cmd\n%{_mandir}/man1/s3cmd.1*\n%{python_sitelib}/S3\n%if 0%{?fedora} >= 9 || 0%{?rhel} >= 6\n%{python_sitelib}/s3cmd*.egg-info\n%endif\n%doc NEWS README.md LICENSE\n\n\n%changelog\n* Thu Feb  5 2015 Matt Domsch <mdomsch@fedoraproject.org> - 1.5.1.2-5\n- add Requires: python-magic\n\n* Wed Feb  4 2015 Matt Domsch <mdomsch@fedoraproject.org> - 1.5.1.2-4\n- upstream 1.5.1.2, mostly bug fixes\n- add dependency on python-setuptools\n\n Mon Jan 12 2015 Matt Domsch <mdomsch@fedoraproject.org> - 1.5.0-1\n- upstream 1.5.0 final\n\n* Tue Jul  1 2014 Matt Domsch <mdomsch@fedoraproject.org> - 1.5.0-0.6.rc1\n- upstream 1.5.0-rc1\n\n* Sun Mar 23 2014 Matt Domsch <mdomsch@fedoraproject.org> - 1.5.0-0.4.git\n- upstream 1.5.0-beta1 plus even newer upstream fixes\n\n* Sun Feb 02 2014 Matt Domsch <mdomsch@fedoraproject.org> - 1.5.0-0.3.git\n- upstream 1.5.0-beta1 plus newer upstream fixes\n\n* Wed May 29 2013 Matt Domsch <mdomsch@fedoraproject.org> - 1.5.0-0.2.gita122d97\n- more upstream bugfixes\n- drop pyxattr dep, that codepath got dropped in this release\n\n* Mon May 20 2013 Matt Domsch <mdomsch@fedoraproject.org> - 1.5.0-0.1.gitb1ae0fbe\n- upstream 1.5.0-alpha3 plus fixes\n- add dep on pyxattr for the --xattr option\n\n* Tue Jun 19 2012 Matt Domsch <mdomsch@fedoraproject.org> - 1.1.0-0.4.git11e5755e\n- add local MD5 cache\n\n* Mon Jun 18 2012 Matt Domsch <mdomsch@fedoraproject.org> - 1.1.0-0.3.git7de0789d\n- parallelize local->remote syncs\n\n* Mon Jun 18 2012 Matt Domsch <mdomsch@fedoraproject.org> - 1.1.0-0.2.gitf881b162\n- add hardlink / duplicate file detection support\n\n* Fri Mar  9 2012 Matt Domsch <mdomsch@fedoraproject.org> - 1.1.0-0.1.git2dfe4a65\n- build from git for mdomsch patches to s3cmd sync\n\n* Thu Feb 23 2012 Dennis Gilmore <dennis@ausil.us> - 1.0.1-1\n- update to 1.0.1 release\n\n* Sat Jan 14 2012 Fedora Release Engineering <rel-eng@lists.fedoraproject.org> - 1.0.0-4\n- Rebuilt for https://fedoraproject.org/wiki/Fedora_17_Mass_Rebuild\n\n* Thu May 05 2011 Lubomir Rintel (GoodData) <lubo.rintel@gooddata.com> - 1.0.0-3\n- No hashlib hackery\n\n* Wed Feb 09 2011 Fedora Release Engineering <rel-eng@lists.fedoraproject.org> - 1.0.0-2\n- Rebuilt for https://fedoraproject.org/wiki/Fedora_15_Mass_Rebuild\n\n* Tue Jan 11 2011 Lubomir Rintel (GoodData) <lubo.rintel@gooddata.com> - 1.0.0-1\n- New upstream release\n\n* Mon Nov 29 2010 Lubomir Rintel (GoodData) <lubo.rintel@gooddata.com> - 0.9.9.91-3\n- Patch for broken f14 httplib\n\n* Thu Jul 22 2010 David Malcolm <dmalcolm@redhat.com> - 0.9.9.91-2.1\n- Rebuilt for https://fedoraproject.org/wiki/Features/Python_2.7/MassRebuild\n\n* Wed Apr 28 2010 Lubomir Rintel (GoodData) <lubo.rintel@gooddata.com> - 0.9.9.91-1.1\n- Do not use sha1 from hashlib\n\n* Sun Feb 21 2010 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.9.91-1\n- New upstream release\n\n* Sun Jul 26 2009 Fedora Release Engineering <rel-eng@lists.fedoraproject.org> - 0.9.9-2\n- Rebuilt for https://fedoraproject.org/wiki/Fedora_12_Mass_Rebuild\n\n* Tue Feb 24 2009 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.9-1\n- New upstream release\n\n* Sat Nov 29 2008 Ignacio Vazquez-Abrams <ivazqueznet+rpm@gmail.com> - 0.9.8.4-2\n- Rebuild for Python 2.6\n\n* Tue Nov 11 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.4-1\n- New upstream release, URI encoding patch upstreamed\n\n* Fri Sep 26 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.3-4\n- Try 3/65536\n\n* Fri Sep 26 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.3-3\n- Whoops, forgot to actually apply the patch.\n\n* Fri Sep 26 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.3-2\n- Fix listing of directories with special characters in names\n\n* Thu Jul 31 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.3-1\n- New upstream release: Avoid running out-of-memory in MD5'ing large files.\n\n* Fri Jul 25 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.2-1.1\n- Fix a typo\n\n* Tue Jul 15 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.2-1\n- New upstream\n\n* Fri Jul 04 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.1-3\n- Be satisfied with ET provided by 2.5 python\n\n* Fri Jul 04 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.1-2\n- Added missing python-devel BR, thanks to Marek Mahut\n- Packaged the Python egg file\n\n* Wed Jul 02 2008 Lubomir Rintel (Good Data) <lubo.rintel@gooddata.com> - 0.9.8.1-1\n- Initial packaging attempt\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0546875,
          "content": "[sdist]\nformats = gztar,zip\n[bdist_wheel]\nuniversal = 1\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.099609375,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nimport sys\nimport os\n\ntry:\n    import xml.etree.ElementTree\n    print(\"Using xml.etree.ElementTree for XML processing\")\nexcept ImportError as e:\n    sys.stderr.write(str(e) + \"\\n\")\n    try:\n        import elementtree.ElementTree\n        print(\"Using elementtree.ElementTree for XML processing\")\n    except ImportError as e:\n        sys.stderr.write(str(e) + \"\\n\")\n        sys.stderr.write(\"Please install ElementTree module from\\n\")\n        sys.stderr.write(\"http://effbot.org/zone/element-index.htm\\n\")\n        sys.exit(1)\n\nfrom setuptools import setup\n\nimport S3.PkgInfo\n\nif float(\"%d.%d\" % sys.version_info[:2]) < 2.6:\n    sys.stderr.write(\"Your Python version %d.%d.%d is not supported.\\n\" % sys.version_info[:3])\n    sys.stderr.write(\"S3cmd requires Python 2.6 or newer.\\n\")\n    sys.exit(1)\n\n## Remove 'MANIFEST' file to force\n## distutils to recreate it.\n## Only in \"sdist\" stage. Otherwise\n## it makes life difficult to packagers.\nif len(sys.argv) > 1 and sys.argv[1] == \"sdist\":\n    try:\n        os.unlink(\"MANIFEST\")\n    except OSError as e:\n        pass\n\n## Re-create the manpage\n## (Beware! Perl script on the loose!!)\nif len(sys.argv) > 1 and sys.argv[1] == \"sdist\":\n    if os.stat_result(os.stat(\"s3cmd.1\")).st_mtime \\\n       < os.stat_result(os.stat(\"s3cmd\")).st_mtime:\n        sys.stderr.write(\"Re-create man page first!\\n\")\n        sys.stderr.write(\"Run: ./s3cmd --help | ./format-manpage.pl > s3cmd.1\\n\")\n        sys.exit(1)\n\n## Don't install manpages and docs when $S3CMD_PACKAGING is set\n## This was a requirement of Debian package maintainer.\nif not os.getenv(\"S3CMD_PACKAGING\"):\n    man_path = os.getenv(\"S3CMD_INSTPATH_MAN\") or \"share/man\"\n    doc_path = os.getenv(\"S3CMD_INSTPATH_DOC\") or \"share/doc/packages\"\n    data_files = [\n        (doc_path+\"/s3cmd\", [\"README.md\", \"INSTALL.md\", \"LICENSE\", \"NEWS\"]),\n        (man_path+\"/man1\", [\"s3cmd.1\"]),\n    ]\nelse:\n    data_files = None\n\n## Main distutils info\nsetup(\n    ## Content description\n    name=S3.PkgInfo.package,\n    version=S3.PkgInfo.version,\n    packages=['S3'],\n    scripts=['s3cmd'],\n    data_files=data_files,\n    test_suite='S3.PkgInfo',\n\n    ## Packaging details\n    author=\"Michal Ludvig\",\n    author_email=\"michal@logix.cz\",\n    maintainer=\"github.com/fviard, github.com/matteobar\",\n    maintainer_email=\"s3tools-bugs@lists.sourceforge.net\",\n    url=S3.PkgInfo.url,\n    license=S3.PkgInfo.license,\n    description=S3.PkgInfo.short_description,\n    long_description=\"\"\"\n%s\n\nAuthors:\n--------\n    Florent Viard <florent@sodria.com>\n\n    Michal Ludvig  <michal@logix.cz>\n\n    Matt Domsch (github.com/mdomsch)\n\"\"\" % (S3.PkgInfo.long_description),\n\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Console',\n        'Environment :: MacOS X',\n        'Environment :: Win32 (MS Windows)',\n        'Intended Audience :: End Users/Desktop',\n        'Intended Audience :: System Administrators',\n        'License :: OSI Approved :: GNU General Public License v2 or later (GPLv2+)',\n        'Natural Language :: English',\n        'Operating System :: MacOS :: MacOS X',\n        'Operating System :: Microsoft :: Windows',\n        'Operating System :: OS Independent',\n        'Operating System :: POSIX',\n        'Operating System :: Unix',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n        'Topic :: System :: Archiving',\n        'Topic :: Utilities',\n    ],\n\n    install_requires=[\"python-dateutil\", \"python-magic\"]\n)\n\n# vim:et:ts=4:sts=4:ai\n"
        },
        {
          "name": "testsuite.tar.gz",
          "type": "blob",
          "size": 280.0048828125,
          "content": null
        },
        {
          "name": "upload-to-sf.sh",
          "type": "blob",
          "size": 0.341796875,
          "content": "#!/bin/sh\n\nVERSION=$(./s3cmd --version | awk '{print $NF}')\necho -e \"Uploading \\033[32ms3cmd \\033[31m${VERSION}\\033[0m ...\"\n#rsync -avP dist/s3cmd-${VERSION}.* ludvigm@frs.sourceforge.net:uploads/\nln -f NEWS README.txt\nrsync -avP dist/s3cmd-${VERSION}.* README.txt ludvigm,s3tools@frs.sourceforge.net:/home/frs/project/s/s3/s3tools/s3cmd/${VERSION}/\n"
        }
      ]
    }
  ]
}