{
  "metadata": {
    "timestamp": 1736559487016,
    "page": 58,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ant-research/CoDeF",
      "stars": 4846,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4775390625,
          "content": "# Ignore compiled files.\n__pycache__/\n*.py[cod]\ntmp_build/\n*.pyc\n\n# Ignore files created by IDEs.\n/.vscode/\n/.idea/\n.ipynb_*/\n*.ipynb\n.DS_Store\n*.sw[pon]\n\n# Ignore data files.\ndata/\n*.npy\n*.tar\n*.zip\n*.mdb\n*.ckpt\n\n# Ignore network files.\nckpts/\n*.pth\n*.pt\n*.pkl\n*.h5\n*.dat\n*.ckpt\n\n# Ignore log files.\nresults/\nresources/\nevents/\nprofile/\nlogs/\n# *.json\n*.log\nevents.*\n\n# Files that should not be ignored.\n!/requirements/*\n\n# Others shuld be ignored.\nall_sequences/\nckpts/\nlogs/\n# configs/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.12109375,
          "content": "------------------------------ LICENSE for CoDeF ------------------------------\n\nCopyright (c) 2023 Ant Group.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.919921875,
          "content": "# CoDeF: Content Deformation Fields for Temporally Consistent Video Processing\n\n<img src='docs/teaser.gif'></img>\n\n[Hao Ouyang](https://ken-ouyang.github.io/)\\*, [Qiuyu Wang](https://github.com/qiuyu96/)\\*, [Yuxi Xiao](https://henry123-boy.github.io/)\\*, [Qingyan Bai](https://scholar.google.com/citations?user=xUMjxi4AAAAJ&hl=en), [Juntao Zhang](https://github.com/JordanZh), [Kecheng Zheng](https://scholar.google.com/citations?user=hMDQifQAAAAJ), [Xiaowei Zhou](https://xzhou.me/),\n[Qifeng Chen](https://cqf.io/)&#8224;, [Yujun Shen](https://shenyujun.github.io/)&#8224; (*equal contribution, &#8224;corresponding author)\n\n**CVPR 2024 Highlight**\n\n#### [Project Page](https://qiuyu96.github.io/CoDeF/) | [Paper](https://arxiv.org/abs/2308.07926) | [High-Res Translation Demo](https://ezioby.github.io/CoDeF_Demo/) | [Colab](https://colab.research.google.com/github/camenduru/CoDeF-colab/blob/main/CoDeF_colab.ipynb)\n\n<!-- Abstract: *This work presents the content deformation field **CoDeF** as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis. Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline. We also introduce some decent regularizations into the optimization process, urging the canonical content field to inherit semantics (e.g., the object shape) from the video. With such a design, **CoDeF** naturally supports lifting image algorithms to videos, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field. We experimentally show that **CoDeF** is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in translated videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog.* -->\n\n## Requirements\n\nThe codebase is tested on\n\n* Ubuntu 20.04\n* Python 3.10\n* [PyTorch](https://pytorch.org/) 2.0.0\n* [PyTorch Lightning](https://www.pytorchlightning.ai/index.html) 2.0.2\n* 1 NVIDIA GPU (RTX A6000) with CUDA version 11.7. (Other GPUs are also suitable, and 10GB GPU memory is sufficient to run our code.)\n\nTo use video visualizer, please install `ffmpeg` via\n\n```shell\nsudo apt-get install ffmpeg\n```\n\nFor additional Python libraries, please install with\n\n```shell\npip install -r requirements.txt\n```\n\nOur code also depends on [tiny-cuda-nn](https://github.com/NVlabs/tiny-cuda-nn).\nSee [this repository](https://github.com/NVlabs/tiny-cuda-nn#pytorch-extension)\nfor Pytorch extension install instructions.\n\n## Data\n\n### Provided data\n\nWe have provided some videos [here](https://drive.google.com/file/d/1cKZF6ILeokCjsSAGBmummcQh0uRGaC_F/view?usp=sharing) for quick test. Please download and unzip the data and put them in the root directory. More videos can be downloaded [here](https://drive.google.com/file/d/10Msz37MpjZQFPXlDWCZqrcQjhxpQSvCI/view?usp=sharing).\n\n### Customize your own data\n\nWe segement video sequences using [SAM-Track](https://github.com/z-x-yang/Segment-and-Track-Anything). Once you obtain the mask files, place them in the folder `all_sequences/{YOUR_SEQUENCE_NAME}/{YOUR_SEQUENCE_NAME}_masks`. Next, execute the following command:\n\n```shell\ncd data_preprocessing\npython preproc_mask.py\n```\n\nWe extract optical flows of video sequences using [RAFT](https://github.com/princeton-vl/RAFT). To get started, please follow the instructions provided [here](https://github.com/princeton-vl/RAFT#demos) to download their pretrained model. Once downloaded, place the model in the `data_preprocessing/RAFT/models` folder. After that, you can execute the following command:\n\n```shell\ncd data_preprocessing/RAFT\n./run_raft.sh\n```\n\nRemember to update the sequence name and root directory in both `data_preprocessing/preproc_mask.py` and `data_preprocessing/RAFT/run_raft.sh` accordingly.\n\nAfter obtaining the files, please organize your own data as follows:\n\n```\nCoDeF\n│\n└─── all_sequences\n    │\n    └─── NAME1\n           └─ NAME1\n           └─ NAME1_masks_0 (optional)\n           └─ NAME1_masks_1 (optional)\n           └─ NAME1_flow (optional)\n           └─ NAME1_flow_confidence (optional)\n    │\n    └─── NAME2\n           └─ NAME2\n           └─ NAME2_masks_0 (optional)\n           └─ NAME2_masks_1 (optional)\n           └─ NAME2_flow (optional)\n           └─ NAME2_flow_confidence (optional)\n    │\n    └─── ...\n```\n\n## Pretrained checkpoints\n\nYou can download checkpoints pre-trained on the provided videos via\n\n| Sequence Name | Config |                           Download                           | OpenXLab | \n| :-------- | :----: | :----------------------------------------------------------: | :---------:| \n| beauty_0 | configs/beauty_0/base.yaml |  [Google drive link](https://drive.google.com/file/d/11SWfnfDct8bE16802PyqYJqsU4x6ACn8/view?usp=sharing) |[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/HaoOuyang/CoDeF)|\n| beauty_1 | configs/beauty_1/base.yaml |  [Google drive link](https://drive.google.com/file/d/1bSK0ChbPdURWGLdtc9CPLkN4Tfnng51k/view?usp=sharing) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/HaoOuyang/CoDeF) |\n| white_smoke      | configs/white_smoke/base.yaml |  [Google drive link](https://drive.google.com/file/d/1QOBCDGV2hHwxq4eL1E_45z5zhZ-wTJR7/view?usp=sharing) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/HaoOuyang/CoDeF) |\n| lemon_hit      | configs/lemon_hit/base.yaml |  [Google drive link](https://drive.google.com/file/d/140ctcLbv7JTIiy53MuCYtI4_zpIvRXzq/view?usp=sharing) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/HaoOuyang/CoDeF)|\n| scene_0      | configs/scene_0/base.yaml |  [Google drive link](https://drive.google.com/file/d/1abOdREarfw1DGscahOJd2gZf1Xn_zN-F/view?usp=sharing) |[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/HaoOuyang/CoDeF)|\n\nAnd organize files as follows\n\n```\nCoDeF\n│\n└─── ckpts/all_sequences\n    │\n    └─── NAME1\n        │\n        └─── EXP_NAME (base)\n            │\n            └─── NAME1.ckpt\n    │\n    └─── NAME2\n        │\n        └─── EXP_NAME (base)\n            │\n            └─── NAME2.ckpt\n    |\n    └─── ...\n```\n\n## Train a new model\n\n```shell\n./scripts/train_multi.sh\n```\n\nwhere\n* `GPU`: Decide which GPU to train on;\n* `NAME`: Name of the video sequence;\n* `EXP_NAME`: Name of the experiment;\n* `ROOT_DIRECTORY`: Directory of the input video sequence;\n* `MODEL_SAVE_PATH`: Path to save the checkpoints;\n* `LOG_SAVE_PATH`: Path to save the logs;\n* `MASK_DIRECTORY`: Directory of the preprocessed masks (optional);\n* `FLOW_DIRECTORY`: Directory of the preprocessed optical flows (optional);\n\nPlease check configuration files in ``configs/``, and you can always add your own model config.\n\n## Test reconstruction <a id=\"anchor\"></a>\n\n```shell\n./scripts/test_multi.sh\n```\nAfter running the script, the reconstructed videos can be found in `results/all_sequences/{NAME}/{EXP_NAME}`, along with the canonical image.\n\n## Test video translation\n\nAfter obtaining the canonical image through [this step](#anchor), use your preferred text prompts to transfer it using [ControlNet](https://github.com/lllyasviel/ControlNet).\nOnce you have the transferred canonical image, place it in `all_sequences/${NAME}/${EXP_NAME}_control` (i.e. `CANONICAL_DIR` in `scripts/test_canonical.sh`).\n\nThen run\n\n```shell\n./scripts/test_canonical.sh\n```\n\nThe transferred results can be seen in `results/all_sequences/{NAME}/{EXP_NAME}_transformed`.\n\n*Note*: The `canonical_wh` option in the configuration file should be set with caution, usually a little larger than `img_wh`, as it determines the field of view of the canonical image.\n\n### BibTeX\n\n```bibtex\n@article{ouyang2023codef,\n      title={CoDeF: Content Deformation Fields for Temporally Consistent Video Processing},\n      author={Hao Ouyang and Qiuyu Wang and Yuxi Xiao and Qingyan Bai and Juntao Zhang and Kecheng Zheng and Xiaowei Zhou and Qifeng Chen and Yujun Shen},\n      journal={arXiv preprint arXiv:2308.07926},\n      year={2023}\n}\n```\n\n### Acknowledgements\nWe thank [camenduru](https://github.com/camenduru) for providing the [colab demo](https://github.com/camenduru/CoDeF-colab).\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data_preprocessing",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "losses.py",
          "type": "blob",
          "size": 1.9404296875,
          "content": "from torch import nn\nimport torch\nimport torchvision\nfrom einops import rearrange, reduce, repeat\n\n\nclass MSELoss(nn.Module):\n    def __init__(self, coef=1):\n        super().__init__()\n        self.coef = coef\n        self.loss = nn.MSELoss(reduction='mean')\n\n    def forward(self, inputs, targets):\n        loss = self.loss(inputs, targets)\n        return self.coef * loss\n\n\ndef rgb_to_gray(image):\n    gray_image = (0.299 * image[:, 0, :, :] + 0.587 * image[:, 1, :, :] +\n                  0.114 * image[:, 2, :, :])\n    gray_image = gray_image.unsqueeze(1)\n\n    return gray_image\n\n\ndef compute_gradient_loss(pred, gt, mask):\n    assert pred.shape == gt.shape, \"a and b must have the same shape\"\n\n    pred = rgb_to_gray(pred)\n    gt = rgb_to_gray(gt)\n\n    sobel_kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=pred.dtype, device=pred.device)\n    sobel_kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=pred.dtype, device=pred.device)\n\n    gradient_a_x = torch.nn.functional.conv2d(pred.repeat(1,3,1,1), sobel_kernel_x.unsqueeze(0).unsqueeze(0).repeat(1,3,1,1), padding=1)/3\n    gradient_a_y = torch.nn.functional.conv2d(pred.repeat(1,3,1,1), sobel_kernel_y.unsqueeze(0).unsqueeze(0).repeat(1,3,1,1), padding=1)/3\n    # gradient_a_magnitude = torch.sqrt(gradient_a_x ** 2 + gradient_a_y ** 2)\n\n    gradient_b_x = torch.nn.functional.conv2d(gt.repeat(1,3,1,1), sobel_kernel_x.unsqueeze(0).unsqueeze(0).repeat(1,3,1,1), padding=1)/3\n    gradient_b_y = torch.nn.functional.conv2d(gt.repeat(1,3,1,1), sobel_kernel_y.unsqueeze(0).unsqueeze(0).repeat(1,3,1,1), padding=1)/3\n    # gradient_b_magnitude = torch.sqrt(gradient_b_x ** 2 + gradient_b_y ** 2)\n\n    pred_grad = torch.cat([gradient_a_x, gradient_a_y], dim=1)\n    gt_grad = torch.cat([gradient_b_x, gradient_b_y], dim=1)\n\n    gradient_difference = torch.abs(pred_grad - gt_grad).mean(dim=1,keepdim=True)[mask].sum()/(mask.sum()+1e-8)\n\n    return gradient_difference\n\n\nloss_dict = {'mse': MSELoss}\n"
        },
        {
          "name": "metrics.py",
          "type": "blob",
          "size": 1.166015625,
          "content": "import torch\nfrom kornia.losses import ssim as dssim\nfrom skimage.metrics import structural_similarity\nfrom einops import rearrange\nimport numpy as np\n\ndef mse(image_pred, image_gt, valid_mask=None, reduction='mean'):\n    value = (image_pred-image_gt)**2\n    if valid_mask is not None:\n        value = value[valid_mask]\n    if reduction == 'mean':\n        return torch.mean(value)\n    return value\n\ndef psnr(image_pred, image_gt, valid_mask=None, reduction='mean'):\n    return -10*torch.log10(mse(image_pred, image_gt, valid_mask, reduction))\n\ndef ssim(image_pred, image_gt, reduction='mean'):\n    return structural_similarity(image_pred.cpu().numpy(), image_gt, win_size=11, multichannel=True, gaussian_weights=True)\n\ndef lpips(image_pred, image_gt, lpips_model):\n    gt_lpips = image_gt * 2.0 - 1.0\n    gt_lpips = rearrange(gt_lpips, '(b h) w c -> b c h w', b=1)\n    gt_lpips = torch.from_numpy(gt_lpips)\n    predict_image_lpips = image_pred.clone().detach().cpu() * 2.0 - 1.0\n    predict_image_lpips = rearrange(predict_image_lpips, '(b h) w c -> b c h w', b=1)\n    lpips_result = lpips_model.forward(predict_image_lpips, gt_lpips).cpu().detach().numpy()\n    return np.squeeze(lpips_result)\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "opt.py",
          "type": "blob",
          "size": 8.6689453125,
          "content": "import argparse\nimport yaml\n\n\ndef get_opts():\n    parser = argparse.ArgumentParser()\n\n    # General Setttings\n    parser.add_argument('--root_dir', type=str, default='Batman_masked_frames',\n                        help='root directory of dataset')\n    parser.add_argument('--canonical_dir', type=str, default=None,\n                        help='directory of canonical dataset')\n\n    # support multiple mask as input (each mask has different deformation fields)\n    parser.add_argument('--mask_dir', nargs=\"+\", type=str, default=None,\n                        help='mask of the dataset')\n    parser.add_argument('--flow_dir', type=str,\n                        default=None,\n                        help='masks of dataset')\n    parser.add_argument('--dataset_name', type=str, default='video',\n                        choices=['video'],\n                        help='which dataset to train/val')\n    parser.add_argument('--img_wh', nargs=\"+\", type=int, default=[842, 512],\n                        help='resolution (img_w, img_h) of the full image')\n    parser.add_argument('--canonical_wh', nargs=\"+\", type=int, default=None,\n                        help='default same as the img_wh, can be set to a larger range to include more content')\n    parser.add_argument('--ref_idx', type=int, default=None,\n                        help='manually select a frame as reference (for rigid movement)')\n\n    # Deformation Setting\n    parser.add_argument('--encode_w', default=False, action=\"store_true\",\n                        help='whether to apply warping')\n\n    # Training Setttings\n\n    parser.add_argument('--batch_size', type=int, default=1,\n                        help='batch size')\n    parser.add_argument('--num_steps', type=int, default=10000,\n                        help='number of training epochs')\n    parser.add_argument('--valid_iters', type=int, default=30,\n                        help='valid iters for each epoch')\n    parser.add_argument('--valid_batches', type=int, default=0,\n                        help='valid batches for each valid process')\n    parser.add_argument('--save_model_iters', type=int, default=5000,\n                        help='iterations to save the models')\n    parser.add_argument('--gpus', nargs=\"+\", type=int, default=[0],\n                        help='gpu devices')\n\n    # Test Setttings\n    parser.add_argument('--test', default=False, action=\"store_true\",\n                        help='whether to disable identity')\n\n    # Model Save and Load\n    parser.add_argument('--ckpt_path', type=str, default=None,\n                        help='pretrained checkpoint to load (including optimizers, etc)')\n    parser.add_argument('--prefixes_to_ignore', nargs='+', type=str, default=['loss'],\n                        help='the prefixes to ignore in the checkpoint state dict')\n    parser.add_argument('--weight_path', type=str, default=None,\n                        help='pretrained model weight to load (do not load optimizers, etc)')\n    parser.add_argument('--model_save_path', type=str, default='ckpts',\n                        help='save checkpoint to')\n    parser.add_argument('--log_save_path', type=str, default='logs',\n                        help='save log to')\n    parser.add_argument('--exp_name', type=str, default='exp',\n                        help='experiment name')\n\n    # Optimize Settings\n    parser.add_argument('--optimizer', type=str, default='adam',\n                        help='optimizer type',\n                        choices=['sgd', 'adam', 'radam', 'ranger'])\n    parser.add_argument('--lr', type=float, default=5e-4,\n                        help='learning rate')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='learning rate momentum')\n    parser.add_argument('--weight_decay', type=float, default=0,\n                        help='weight decay')\n    parser.add_argument('--lr_scheduler', type=str, default='steplr',\n                        help='scheduler type',\n                        choices=['steplr', 'cosine', 'poly', 'exponential'])\n\n    #### params for steplr ####\n    parser.add_argument('--decay_step', nargs='+', type=int,\n                        default=[2500, 5000, 7500],\n                        help='scheduler decay step')\n    parser.add_argument('--decay_gamma', type=float, default=0.5,\n                        help='learning rate decay amount')\n\n    #### params for warmup, only applied when optimizer == 'sgd' or 'adam'\n    parser.add_argument('--warmup_multiplier', type=float, default=1.0,\n                        help='lr is multiplied by this factor after --warmup_epochs')\n    parser.add_argument('--warmup_epochs', type=int, default=0,\n                        help='Gradually warm-up(increasing) learning rate in optimizer')\n\n    ##### annealed positional encoding ######\n    parser.add_argument('--annealed', default=False, action=\"store_true\",\n                        help='whether to apply annealed positional encoding (Only in the warping field)')\n    parser.add_argument('--annealed_begin_step', type=int, default=0,\n                        help='annealed step to begin for positional encoding')\n    parser.add_argument('--annealed_step', type=int, default=5000,\n                        help='maximum annealed step for positional encoding')\n\n    ##### Additional losses ######\n    parser.add_argument('--flow_loss', type=float, default=None,\n                        help='optical flow loss weight')\n    parser.add_argument('--bg_loss', type=float, default=None,\n                        help='regularize the rest part of each object ')\n    parser.add_argument('--grad_loss', type=float, default=0.1,\n                        help='image gradient loss weight')\n    parser.add_argument('--flow_step', type=int, default=-1,\n                        help='Step to begin to perform flow loss.')\n    parser.add_argument('--ref_step', type=int, default=-1,\n                        help='Step to stop reference frame loss.')\n    parser.add_argument('--self_bg', type=bool_parser, default=False,\n                        help='Whether to use self background as bg loss.')\n\n    ##### Special cases: for black-dominated images\n    parser.add_argument('--sigmoid_offset', type=float, default=0,\n                        help='whether to process balck-dominated images.')\n\n    # Other miscellaneous settings.\n    parser.add_argument('--save_deform', type=bool_parser, default=False,\n                        help='Whether to save deformation field or not.')\n    parser.add_argument('--save_video', type=bool_parser, default=True,\n                        help='Whether to save video or not.')\n    parser.add_argument('--fps', type=int, default=30,\n                        help='FPS of the saved video.')\n\n    # Network settings for PE.\n    parser.add_argument('--deform_D', type=int, default=6,\n                        help='The depth of deformation field MLP.')\n    parser.add_argument('--deform_W', type=int, default=128,\n                        help='The width of deformation field MLP.')\n    parser.add_argument('--vid_D', type=int, default=8,\n                        help='The depth of implicit video MLP.')\n    parser.add_argument('--vid_W', type=int, default=256,\n                        help='The width of implicit video MLP.')\n    parser.add_argument('--N_vocab_w', type=int, default=200,\n                        help='number of vocabulary for warp code in the dataset for nn.Embedding')\n    parser.add_argument('--N_w', type=int, default=8,\n                        help='embeddings size for warping')\n    parser.add_argument('--N_xyz_w', nargs=\"+\", type=int, default=[8, 8],\n                        help='positional encoding frequency of deformation field')\n\n    # Network settings for Hash, please see details in configs/hash.json\n    parser.add_argument('--vid_hash', type=bool_parser, default=False,\n                        help='Whether to use hash encoding in implicit video system.')\n    parser.add_argument('--deform_hash', type=bool_parser, default=False,\n                        help='Whether to use hash encoding in deformation field.')\n\n    # Config files\n    parser.add_argument('--config', type=str, default=None,\n                        help='path to the YAML config file.')\n\n    args = parser.parse_args()\n\n    if args.config is not None:\n        with open(args.config, 'r') as f:\n            config = yaml.safe_load(f)\n        args_dict = vars(args)\n        args_dict.update(config)\n        args_new = argparse.Namespace(**args_dict)\n        return args_new\n\n    return args\n\n\ndef bool_parser(arg):\n    \"\"\"Parses an argument to boolean.\"\"\"\n    if isinstance(arg, bool):\n        return arg\n    if arg is None:\n        return False\n    if arg.lower() in ['1', 'true', 't', 'yes', 'y']:\n        return True\n    if arg.lower() in ['0', 'false', 'f', 'no', 'n']:\n        return False\n    raise ValueError(f'`{arg}` cannot be converted to boolean!')\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.244140625,
          "content": "pytorch_lightning==2.0.2\neasydict==1.10\neinops==0.6.1\nipdb==0.13.13\nnumpy==1.24.3\nopencv-python-headless==4.5.5.62\nPyYAML==6.0\nscikit-image==0.21.0\nscipy==1.10.1\nsk-video==1.1.10\ntensorboard==2.13.0\ntqdm==4.65.0\ntorch_optimizer==0.3.0\nkornia==0.5.10\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 22.2578125,
          "content": "import os\nimport json\nimport cv2\nimport numpy as np\n\nfrom einops import rearrange\nfrom einops import repeat\nfrom pathlib import Path\nfrom easydict import EasyDict as edict\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom datasets import dataset_dict\nfrom losses import loss_dict\nfrom losses import compute_gradient_loss\n\nfrom models.implicit_model import TranslationField\nfrom models.implicit_model import ImplicitVideo\nfrom models.implicit_model import ImplicitVideo_Hash\nfrom models.implicit_model import Embedding\nfrom models.implicit_model import AnnealedEmbedding\nfrom models.implicit_model import AnnealedHash\nfrom models.implicit_model import Deform_Hash3d_Warp\n\nfrom utils import get_optimizer\nfrom utils import get_scheduler\nfrom utils import get_learning_rate\nfrom utils import load_ckpt\nfrom utils import VideoVisualizer\n\nfrom opt import get_opts\nfrom metrics import psnr\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\n\nclass ImplicitVideoSystem(LightningModule):\n    def __init__(self, hparams):\n        super(ImplicitVideoSystem, self).__init__()\n        self.save_hyperparameters(hparams)\n        self.color_loss = loss_dict['mse'](coef=1)\n        if hparams.save_video:\n            self.video_visualizer = VideoVisualizer(fps=hparams.fps)\n            self.raw_video_visualizer = VideoVisualizer(fps=hparams.fps)\n            self.dual_video_visualizer = VideoVisualizer(fps=hparams.fps)\n\n        self.models_to_train=[]\n        self.embedding_xyz = Embedding(2, 8)\n        self.embeddings = {'xyz': self.embedding_xyz}\n        self.models = {}\n\n        # Construct normalized meshgrid.\n        h = self.hparams.img_wh[1]\n        w = self.hparams.img_wh[0]\n        self.h = h\n        self.w = w\n\n        if self.hparams.mask_dir:\n            self.num_models = len(self.hparams.mask_dir)\n        else:\n            self.num_models = 1\n\n        # Decide the number of deformable mlp.\n        if hparams.encode_w:\n            # Multiple deformation MLP.\n            # Progressive Training for the Deformation (Annealed PE).\n            # No trainable parameters.\n            self.embeddings['xyz_w'] = []\n            assert (isinstance(self.hparams.N_xyz_w, list))\n            in_channels_xyz = []\n            for i in range(self.num_models):\n                N_xyz_w = self.hparams.N_xyz_w[i]\n                in_channels_xyz += [2 + 2 * N_xyz_w * 2]\n                if hparams.annealed:\n                    if hparams.deform_hash:\n                        self.embedding_hash = AnnealedHash(\n                        in_channels=2,\n                        annealed_step=hparams.annealed_step,\n                        annealed_begin_step=hparams.annealed_begin_step)\n                        self.embeddings['aneal_hash'] = self.embedding_hash\n                    else:\n                        self.embedding_xyz_w = AnnealedEmbedding(\n                            in_channels=2,\n                            N_freqs=N_xyz_w,\n                            annealed_step=hparams.annealed_step,\n                            annealed_begin_step=hparams.annealed_begin_step)\n                        self.embeddings['xyz_w'] += [self.embedding_xyz_w]\n                else:\n                    self.embedding_xyz_w = Embedding(2, N_xyz_w)\n                    self.embeddings['xyz_w'] += [self.embedding_xyz_w]\n\n            for i in range(self.num_models):\n                embedding_w = torch.nn.Embedding(hparams.N_vocab_w, hparams.N_w)\n                torch.nn.init.uniform_(embedding_w.weight, -0.05, 0.05)\n                load_ckpt(embedding_w, hparams.weight_path, model_name=f'w_{i}')\n                self.embeddings[f'w_{i}'] = embedding_w\n                self.models_to_train += [self.embeddings[f'w_{i}']]\n\n                # Add warping field mlp.\n                if hparams.deform_hash:\n                    with open('configs/hash.json') as f:\n                        config = json.load(f)\n                    warping_field = Deform_Hash3d_Warp(config=config)\n                else:\n                    warping_field = TranslationField(\n                        D=self.hparams.deform_D,\n                        W=self.hparams.deform_W,\n                        in_channels_xyz=in_channels_xyz[i])\n\n                load_ckpt(warping_field,\n                          hparams.weight_path,\n                          model_name=f'warping_field_{i}')\n                self.models[f'warping_field_{i}'] = warping_field\n\n        # Set up the canonical model.\n        if hparams.canonical_dir is None:\n            for i in range(self.num_models):\n                if hparams.vid_hash:\n                    with open('configs/hash.json') as f:\n                        config = json.load(f)\n                    implicit_video = ImplicitVideo_Hash(config=config)\n                else:\n                    implicit_video = ImplicitVideo(\n                        D=hparams.vid_D,\n                        W=hparams.vid_W,\n                        sigmoid_offset=hparams.sigmoid_offset)\n                load_ckpt(implicit_video, hparams.weight_path,\n                          f'implicit_video_{i}')\n                self.models[f'implicit_video_{i}'] = implicit_video\n\n        for key in self.embeddings:\n            setattr(self, key, self.embeddings[key])\n        for key in self.models:\n            setattr(self, key, self.models[key])\n\n        self.models_to_train += [self.models]\n\n    def deform_pts(self, ts_w, grid, encode_w, step=0, i=0):\n        if hparams.deform_hash:\n            ts_w_norm = ts_w / self.seq_len\n            ts_w_norm = ts_w_norm.repeat(grid.shape[0], 1)\n            input_xyt = torch.cat([grid, ts_w_norm], dim=-1)\n            if 'aneal_hash' in self.embeddings.keys():\n                deform = self.models[f'warping_field_{i}'](\n                    input_xyt,\n                    step=step,\n                    aneal_func=self.embeddings['aneal_hash'])\n            else:\n                deform = self.models[f'warping_field_{i}'](input_xyt)\n            if encode_w:\n                deformed_grid = deform + grid\n            else:\n                deformed_grid = grid\n        else:\n            if encode_w:\n                e_w = self.embeddings[f'w_{i}'](repeat(ts_w, 'b n ->  (b l) n ',\n                                                    l=grid.shape[0])[:, 0])\n                # Whether to use annealed positional encoding.\n                if self.hparams.annealed:\n                    pe_w = self.embeddings['xyz_w'][i](grid, step)\n                else:\n                    pe_w = self.embeddings['xyz_w'][i](grid)\n\n                # Warping field type.\n                deform = self.models[f'warping_field_{i}'](torch.cat(\n                    [e_w, pe_w], 1))\n                deformed_grid = deform + grid\n            else:\n                deformed_grid = grid\n\n        return deformed_grid\n\n    def forward(self,\n                ts_w,\n                grid,\n                encode_w,\n                step=0,\n                flows=None):\n        # grid -> positional encoding\n        # ts_w -> embedding\n        grid = rearrange(grid, 'b n c -> (b n) c')\n        results_list = []\n        flow_loss_list = []\n        deform_list = []\n        for i in range(self.num_models):\n            deformed_grid = self.deform_pts(ts_w, grid, encode_w, step, i)  # [batch * num_pixels, 2]\n            deform_list.append(deformed_grid)\n            # Compute optical flow loss.\n            flow_loss = 0\n            if self.hparams.flow_loss > 0 and not self.hparams.test:\n                if flows.max() > -1e2 and step > self.hparams.flow_step:\n                    grid_new = grid + flows.squeeze(0)\n                    deformed_grid_new = self.deform_pts(\n                        ts_w + 1, grid_new, encode_w, step, i)\n                    flow_loss = (deformed_grid_new, deformed_grid)\n            flow_loss_list.append(flow_loss)\n            if self.hparams.vid_hash:\n                pe_deformed_grid = (deformed_grid + 0.3) / 1.6\n            else:\n                pe_deformed_grid = self.embeddings['xyz'](deformed_grid)\n            if not self.training and self.hparams.canonical_dir is not None:\n                w, h = self.img_wh\n                canonical_img = self.canonical_img.squeeze(0)\n                h_c, w_c = canonical_img.shape[1:3]\n                grid_new = deformed_grid.clone()\n                grid_new[..., 1] = (2 * deformed_grid[..., 0] - 1) * h / h_c\n                grid_new[..., 0] = (2 * deformed_grid[..., 1] - 1) * w / w_c\n                if len(canonical_img.shape) == 3:\n                    canonical_img = canonical_img.unsqueeze(0)\n                results = torch.nn.functional.grid_sample(\n                    canonical_img[i:i + 1].permute(0, 3, 1, 2),\n                    grid_new.unsqueeze(1).unsqueeze(0),\n                    mode='bilinear',\n                    padding_mode='border')\n                results = results.squeeze().permute(1,0)\n            else:\n                results = self.models[f'implicit_video_{i}'](pe_deformed_grid)\n\n            results_list.append(results)\n\n        ret = edict(rgbs=results_list,\n                    flow_loss=flow_loss_list,\n                    deform=deform_list)\n\n        return ret\n\n    def setup(self, stage):\n        if not self.hparams.test:\n            dataset = dataset_dict[self.hparams.dataset_name]\n            kwargs = {\n                'root_dir': self.hparams.root_dir,\n                'img_wh': tuple(self.hparams.img_wh),\n                'mask_dir': self.hparams.mask_dir,\n                'flow_dir': self.hparams.flow_dir,\n                'canonical_wh': self.hparams.canonical_wh,\n                'ref_idx': self.hparams.ref_idx,\n                'canonical_dir': self.hparams.canonical_dir\n            }\n            self.train_dataset = dataset(split='train', **kwargs)\n            self.val_dataset = dataset(split='val', **kwargs)\n\n    def configure_optimizers(self):\n        self.optimizer = get_optimizer(self.hparams, self.models_to_train)\n        scheduler = get_scheduler(self.hparams, self.optimizer)\n        lr_dict = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [self.optimizer], [lr_dict]\n\n    def train_dataloader(self):\n        sampler = DistributedSampler(self.train_dataset, shuffle=True)\n        return DataLoader(self.train_dataset,\n                          num_workers=4,\n                          batch_size=self.hparams.batch_size,\n                          sampler=sampler,\n                          pin_memory=True)\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            shuffle=False,\n            num_workers=4,\n            batch_size=1,  # validate one image (H*W rays) at a time.\n            pin_memory=True)\n\n    def test_dataloader(self):\n        dataset = dataset_dict[self.hparams.dataset_name]\n        kwargs = {\n            'root_dir': self.hparams.root_dir,\n            'img_wh': tuple(self.hparams.img_wh),\n            'mask_dir': self.hparams.mask_dir,\n            'canonical_wh': self.hparams.canonical_wh,\n            'canonical_dir': self.hparams.canonical_dir,\n            'test': self.hparams.test\n        }\n        self.train_dataset = dataset(split='train', **kwargs)\n        return DataLoader(\n            self.train_dataset,\n            shuffle=False,\n            num_workers=4,\n            batch_size=1,  # validate one image (H*W rays) at a time.\n            pin_memory=True)\n\n    def training_step(self, batch, batch_idx):\n        # Fetch training data.\n        rgbs = batch['rgbs']\n        ts_w = batch['ts_w']\n        grid = batch['grid']\n        mk = batch['masks']\n        flows = batch['flows']\n        grid_c = batch['grid_c']\n        ref_batch = batch['reference']\n        self.seq_len = batch['seq_len']\n\n        loss = 0\n        rgbs_flattend = rearrange(rgbs, 'b h w c -> (b h w) c')\n\n        # Forward the model.\n        ret = self.forward(ts_w,\n                            grid,\n                            self.hparams.encode_w,\n                            self.global_step,\n                            flows=flows)\n\n        # Mannually set a reference frame.\n        if self.hparams.ref_step < 0: self.hparams.step = 1e10\n        if (self.hparams.ref_idx is not None\n                and self.global_step < self.hparams.ref_step):\n            rgbs_c_flattend = rearrange(ref_batch[0],\n                                        'b h w c -> (b h w) c')\n            ret_c = self(ts_w, grid, False, self.global_step, flows=flows)\n\n        # Loss computation.\n        for i in range(self.num_models):\n            results = ret.rgbs[i]\n            mk_t = rearrange(mk[i], 'b h w c -> (b h w) c')\n            mk_t = mk_t.sum(dim=-1) > 0.05\n\n            if (self.hparams.ref_idx is not None\n                and self.global_step < self.hparams.ref_step):\n                mk_c_t = rearrange(ref_batch[1][i], 'b h w c -> (b h w) c')\n                mk_c_t = mk_c_t.sum(dim=-1) > 0.05\n\n            # Background regularization.\n            if self.hparams.bg_loss:\n                mk1 = torch.logical_not(mk_t)\n                if self.hparams.self_bg:\n                    grid_flattened = rgbs_flattend\n                else:\n                    grid_flattened = rearrange(grid, 'b n c -> (b n) c')\n                    grid_flattened = torch.cat(\n                        [grid_flattened, grid_flattened[:, :1]], -1)\n\n            if self.hparams.bg_loss and self.hparams.mask_dir:\n                loss = loss + self.hparams.bg_loss * self.color_loss(\n                    results[mk1], grid_flattened[mk1])\n\n            # MSE color loss.\n            loss = loss + self.color_loss(results[mk_t],\n                                            rgbs_flattend[mk_t])\n\n            # Image gradient loss.\n            img_pred = rearrange(results,\n                                 '(b h w) c -> b h w c',\n                                 b=1,\n                                 h=self.h,\n                                 w=self.w)\n            rgbs_gt = rearrange(rgbs_flattend,\n                                '(b h w) c -> b h w c',\n                                b=1,\n                                h=self.h,\n                                w=self.w)\n            mk_t_re = rearrange(mk_t,\n                                '(b h w c) -> b h w c',\n                                b=1,\n                                h=self.h,\n                                w=self.w)\n            grad_loss = compute_gradient_loss(rgbs_gt.permute(0, 3, 1, 2),\n                                              img_pred.permute(0, 3, 1, 2),\n                                              mask=mk_t_re.permute(0, 3, 1, 2))\n            loss = loss + grad_loss * self.hparams.grad_loss\n\n            # Optical flow loss.\n            if ret.flow_loss[0] != 0:\n                mk_flow_t = torch.logical_and(mk_t, flows[0].sum(dim=-1)< 3)\n                loss = loss + torch.nn.functional.l1_loss(\n                    ret.flow_loss[i][0][mk_flow_t], ret.flow_loss[i][1]\n                    [mk_flow_t]) * self.hparams.flow_loss\n\n            # Reference loss.\n            if (self.hparams.ref_idx is not None\n                    and self.global_step < self.hparams.ref_step):\n                results_c = ret_c.rgbs[i]\n                loss += self.color_loss(results_c[mk_c_t],\n                                        rgbs_c_flattend[mk_c_t])\n\n            # PSNR metric.\n            with torch.no_grad():\n                if i == 0:\n                    psnr_ = psnr(results[mk_t], rgbs_flattend[mk_t])\n\n        self.log('lr', get_learning_rate(self.optimizer), prog_bar=True)\n        self.log('train/loss', loss, prog_bar=True)\n        self.log('train/psnr', psnr_, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        rgbs = batch['rgbs']\n        ts_w = batch['ts_w']\n        grid = batch['grid']\n        mk = batch['masks']\n        grid_c = grid  # batch['grid_c']\n        self.seq_len = batch['seq_len']\n        ret = self(ts_w, grid, self.hparams.encode_w, self.global_step)\n        ret_c = self(ts_w, grid_c, False, self.global_step)\n\n        log = {}\n        W, H = self.hparams.img_wh\n\n        rgbs_flattend = rearrange(rgbs, 'b h w c -> (b h w) c')\n        img_gt = rgbs_flattend.view(H, W, 3).permute(2, 0, 1).cpu()  # (3, H, W)\n        stack_list = [img_gt]\n        for i in range(self.num_models):\n            results = ret.rgbs[i]\n            results_c = ret_c.rgbs[i]\n            mk_t = rearrange(mk[i], 'b h w c -> (b h w) c')\n            if batch_idx == 0:\n                results[mk_t.sum(dim=-1) <= 0.05] = 0\n                img = results.view(H, W, 3).permute(2, 0, 1).cpu()  # (3, H, W)\n                img_c = results_c.view(H, W, 3).permute(2, 0, 1).cpu()  # (3, H, W)\n                stack_list.append(img)\n                stack_list.append(img_c)\n\n        stack = torch.stack(stack_list) # (3, 3, H, W)\n        self.logger.experiment.add_images('val/GT_Reconstructed', stack,\n                                          self.global_step)\n\n        return log\n\n    def test_step(self, batch, batch_idx):\n        ts_w = batch['ts_w']\n        grid = batch['grid']\n        mk = batch['masks']\n        grid_c = batch['grid_c']\n        W, H = self.hparams.img_wh\n        self.seq_len = batch['seq_len']\n        if self.hparams.canonical_dir is not None:\n            self.canonical_img = batch['canonical_img']\n            self.img_wh = batch['img_wh']\n\n        save_dir = os.path.join('results',\n                                self.hparams.root_dir.split('/')[0],\n                                self.hparams.root_dir.split('/')[1],\n                                self.hparams.exp_name)\n        sample_name = self.hparams.root_dir.split('/')[1]\n        if self.hparams.canonical_dir is not None:\n            test_dir = f'{save_dir}_transformed'\n            video_name = f'{sample_name}_{self.hparams.exp_name}_transformed'\n        else:\n            test_dir = f'{save_dir}'\n            video_name = f'{sample_name}_{self.hparams.exp_name}'\n        Path(test_dir).mkdir(parents=True, exist_ok=True)\n\n        if batch_idx > 0 and self.hparams.save_video:\n            self.video_visualizer.set_path(os.path.join(\n                test_dir, f'{video_name}.mp4'))\n            self.raw_video_visualizer.set_path(os.path.join(\n                test_dir, f'{video_name}_raw.mp4'))\n            self.dual_video_visualizer.set_path(os.path.join(\n                test_dir, f'{video_name}_dual.mp4'))\n\n        if batch_idx == 0 and self.hparams.canonical_dir is None:\n            # Save the canonical image.\n            ret = self(ts_w, grid_c, False, self.global_step)\n\n        ret_n = self(ts_w, grid, self.hparams.encode_w, self.global_step)\n\n        img = np.zeros((H * W, 3), dtype=np.float32)\n        for i in range(self.num_models):\n            if batch_idx == 0 and self.hparams.canonical_dir is None:\n                results_c = ret.rgbs[i]\n                if self.hparams.canonical_wh:\n                    img_c = results_c.view(self.hparams.canonical_wh[1],\n                                           self.hparams.canonical_wh[0],\n                                           3).float().cpu().numpy()\n                else:\n                    img_c = results_c.view(H, W, 3).float().cpu().numpy()\n\n                img_c = cv2.cvtColor(img_c, cv2.COLOR_BGR2RGB)\n                cv2.imwrite(f'{test_dir}/canonical_{i}.png', img_c * 255)\n\n            mk_n = rearrange(mk[i], 'b h w c -> (b h w) c')\n            mk_n = mk_n.sum(dim=-1) > 0.05\n            mk_n = mk_n.cpu().numpy()\n            results = ret_n.rgbs[i]\n            results = results.cpu().numpy()  # (3, H, W)\n            img[mk_n] = results[mk_n]\n\n        img = rearrange(img, '(h w) c -> h w c', h=H, w=W)\n        img = img * 255\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        cv2.imwrite(f'{test_dir}/{batch_idx:05d}.png', img)\n\n        if batch_idx > 0 and self.hparams.save_video:\n            img = img[..., ::-1]\n            self.video_visualizer.add(img)\n            rgbs = batch['rgbs'].view(H, W, 3).cpu().numpy() * 255\n            rgbs = rgbs.astype(np.uint8)\n            self.raw_video_visualizer.add(rgbs)\n            dual_img = np.concatenate((rgbs, img), axis=1)\n            self.dual_video_visualizer.add(dual_img)\n\n        if self.hparams.save_deform:\n            save_deform_dir = f'{test_dir}_deform'\n            Path(save_deform_dir).mkdir(parents=True, exist_ok=True)\n            deformation_field = ret_n.deform[0]\n            deformation_field = rearrange(deformation_field,\n                                          '(h w) c -> h w c', h=H, w=W)\n            grid_ = rearrange(grid[0], '(h w) c -> h w c', h=H, w=W)\n            deformation_delta = deformation_field - grid_\n            np.save(f'{save_deform_dir}/{batch_idx:05d}.npy',\n                    deformation_delta.cpu().numpy())\n\n    def on_test_epoch_end(self):\n        if self.hparams.save_video:\n            self.video_visualizer.save()\n            self.raw_video_visualizer.save()\n            self.dual_video_visualizer.save()\n\n    def get_progress_bar_dict(self):\n        items = super().get_progress_bar_dict()\n        items.pop(\"v_num\", None)\n        return items\n\ndef main(hparams):\n    system = ImplicitVideoSystem(hparams)\n\n    if not hparams.test:\n        os.makedirs(f'{hparams.model_save_path}/{hparams.exp_name}',\n                    exist_ok=True)\n\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=f'{hparams.model_save_path}/{hparams.exp_name}',\n        filename='{step:d}',\n        mode='max',\n        save_top_k=-1,\n        every_n_train_steps=hparams.save_model_iters,\n        save_last=True)\n\n    logger = TensorBoardLogger(save_dir=hparams.log_save_path,\n                               name=hparams.exp_name)\n\n    trainer = Trainer(max_steps=hparams.num_steps,\n                      precision=16 if hparams.vid_hash == True else 32,\n                      callbacks=[checkpoint_callback],\n                      logger=logger,\n                      accelerator='gpu',\n                      devices=hparams.gpus,\n                      num_sanity_val_steps=1,\n                      benchmark=True,\n                      profiler=\"simple\" if len(hparams.gpus) == 1 else None,\n                      val_check_interval=hparams.valid_iters,\n                      limit_val_batches=hparams.valid_batches,\n                      strategy=\"ddp_find_unused_parameters_true\")\n\n    if hparams.test:\n        trainer.test(system, dataloaders=system.test_dataloader())\n    else:\n        trainer.fit(system, ckpt_path=hparams.ckpt_path)\n\n\nif __name__ == '__main__':\n    hparams = get_opts()\n    main(hparams)\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}