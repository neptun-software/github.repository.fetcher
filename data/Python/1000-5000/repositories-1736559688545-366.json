{
  "metadata": {
    "timestamp": 1736559688545,
    "page": 366,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "baichuan-inc/Baichuan2",
      "stars": 4116,
      "defaultBranch": "main",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "OpenAI_api.py",
          "type": "blob",
          "size": 2.240234375,
          "content": "from flask import Flask, request, jsonify\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\n\napp = Flask(__name__)\n\n# Load the pre-trained model and tokenizer\nmodel_name = \"baichuan-inc/Baichuan2-13B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n#è¿™é‡Œçš„torch_dtype=torch.float16å¯æ”¹ä¸ºtorch_dtype=torch.bfloat16ï¼Œä»…æ”¯æŒ30ç³»åŠä»¥ä¸Šæ˜¾å¡\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\n\n@app.route('/v1/chat/completions', methods=['POST'])#URL: http://127.0.0.1:8000/v1/chat/completions\ndef chat_completion():\n    try:\n        # Parse incoming JSON data\n        data = request.get_json()\n        messages = data.get('messages', [])\n        is_streaming = data.get('stream', False)\n\n        # Check if streaming is enabled\n        if is_streaming:#è¿™é‡Œæš‚æ—¶åªæ”¯æŒéæµå¼è¾“å‡º\n            return jsonify({\"error\": \"Streaming is not supported.\"}), 400\n\n        # Generate response using the model\n        response_text = generate_response(messages)\n\n        # Calculate token counts\n        prompt_tokens = sum(len(tokenizer.encode(msg['content'])) for msg in messages)\n        # æš‚æ—¶ä¸æ”¯æŒè°ƒèŠ‚å…¶ä»–å‚æ•°\n        completion_tokens = len(response_text)\n        total_tokens = prompt_tokens + completion_tokens\n\n        # Build the response\n        response_data = {\n            \"object\": \"chat.completion\",\n            \"model\": model_name,\n            \"choices\": [{\"message\": {\"role\": \"assistant\", \"content\": response_text}, \"index\": 0, \"finish_reason\": \"stop\"}],\n            \"usage\": {\n                \"prompt_tokens\": prompt_tokens,\n                \"completion_tokens\": completion_tokens,\n                \"total_tokens\": total_tokens\n            }\n        }\n\n        return jsonify(response_data)\n\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\ndef generate_response(messages):\n    # Generate response using the model\n    response = model.chat(tokenizer, messages)\n    return response\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8000)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 37.875,
          "content": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n\n<div align=\"center\">\n<h1>\n  Baichuan 2\n</h1>\n</div>\n\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/baichuan-inc/\" target=\"_blank\">Hugging Face</a> â€¢ ğŸ¤– <a href=\"https://modelscope.cn/organization/baichuan-inc\" target=\"_blank\">ModelScope</a> â€¢ ğŸ’¬ <a href=\"https://github.com/baichuan-inc/Baichuan-7B/blob/main/media/wechat.jpeg?raw=true\" target=\"_blank\">WeChat</a>â€¢ ğŸ§© <a href=\"https://modelers.cn/spaces/Baichuan/Baichuan2-7B-Chat\" target=\"_blank\">Modelers</a>\n</p>\n\n<div align=\"center\">\n\n<div align=\"center\">\n<img src=\"https://github.com/baichuan-inc/Baichuan2/blob/main/media/xy.jpeg?raw=true\" width=20% />\n</div>\n\nğŸš€ ç™¾å°åº”APPå·²æ­£å¼ä¸Šçº¿ï¼æ‡‚æœç´¢ã€ä¼šæé—®çš„AIåŠ©æ‰‹ï¼Œå„å¤§åº”ç”¨å•†åº—æœç´¢ç™¾å°åº”ä¸‹è½½ï¼Œæ¬¢è¿ä¸‹è½½ä½“éªŒ ğŸ‰\n\n\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/Baichuan2/blob/main/LICENSE)\n\n\n<h4 align=\"center\">\n    <p>\n        <b>ä¸­æ–‡</b> |\n        <a href=\"https://github.com/baichuan-inc/Baichuan2/blob/main/README_EN.md\">English</a>\n    <p>\n</h4>\n\n</div>\n\n# ç›®å½•\n\n- [ğŸ“– æ¨¡å‹ä»‹ç»](#æ¨¡å‹ä»‹ç»)\n- [ğŸ“Š Benchmark ç»“æœ ğŸ¥‡ğŸ¥‡ğŸ”¥ğŸ”¥](#Benchmark-ç»“æœ)\n- [âš™ï¸ æ¨ç†å’Œéƒ¨ç½²](#æ¨ç†å’Œéƒ¨ç½²)\n- [ğŸ› ï¸ æ¨¡å‹å¾®è°ƒ](#æ¨¡å‹å¾®è°ƒ)\n- [ğŸ’¾ ä¸­é—´ Checkpoints ğŸ”¥ğŸ”¥](#ä¸­é—´-Checkpoints)\n- [ğŸ‘¥ ç¤¾åŒºä¸ç”Ÿæ€](#ç¤¾åŒºä¸ç”Ÿæ€)\n- [ğŸ“œ å£°æ˜ã€åè®®ã€å¼•ç”¨](#å£°æ˜åè®®å¼•ç”¨)\n\n# æ›´æ–°\n[2023.12.29] ğŸ‰ğŸ‰ğŸ‰ æˆ‘ä»¬å‘å¸ƒäº† **[Baichuan2-13B-Chat v2](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/tree/v2.0)** ç‰ˆæœ¬ã€‚å…¶ä¸­ï¼š\n- å¤§å¹…æå‡äº†æ¨¡å‹çš„ç»¼åˆèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯æ•°å­¦å’Œé€»è¾‘æ¨ç†ã€å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚\n\n# æ¨¡å‹ä»‹ç»\n\n- Baichuan 2 æ˜¯ç™¾å·æ™ºèƒ½æ¨å‡ºçš„**æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹**ï¼Œé‡‡ç”¨ **2.6 ä¸‡äº¿**  Tokens çš„é«˜è´¨é‡è¯­æ–™è®­ç»ƒã€‚\n- Baichuan 2 åœ¨å¤šä¸ªæƒå¨çš„ä¸­æ–‡ã€è‹±æ–‡å’Œå¤šè¯­è¨€çš„é€šç”¨ã€é¢†åŸŸ benchmark ä¸Šå–å¾—åŒå°ºå¯¸**æœ€ä½³**çš„æ•ˆæœã€‚\n- æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰ **7B**ã€**13B** çš„ **Base** å’Œ **Chat** ç‰ˆæœ¬ï¼Œå¹¶æä¾›äº† Chat ç‰ˆæœ¬çš„ **4bits é‡åŒ–**ã€‚\n- æ‰€æœ‰ç‰ˆæœ¬å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ã€‚åŒæ—¶ï¼Œå¼€å‘è€…é€šè¿‡é‚®ä»¶ç”³è¯·å¹¶è·å¾—å®˜æ–¹å•†ç”¨è®¸å¯åï¼Œå³å¯**å…è´¹å•†ç”¨**ï¼Œè¯·å‚è€ƒ[åè®®](#åè®®)ç« èŠ‚ã€‚\n- æ¬¢è¿é˜…è¯»æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Š [Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305) è·å–æ›´å¤šä¿¡æ¯ã€‚\n\næœ¬æ¬¡å‘å¸ƒç‰ˆæœ¬å’Œä¸‹è½½é“¾æ¥è§ä¸‹è¡¨ï¼š\n|         | åŸºåº§æ¨¡å‹  | å¯¹é½æ¨¡å‹ | å¯¹é½æ¨¡å‹ 4bits é‡åŒ– |\n|:-------:|:-------:|:-------:|:-----------------:|\n| 7B      | ğŸ¤— [Baichuan2-7B-Base](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base) | ğŸ¤— [Baichuan2-7B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat) | ğŸ¤— [Baichuan2-7B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits) |\n| 13B     | ğŸ¤— [Baichuan2-13B-Base](https://huggingface.co/baichuan-inc/Baichuan2-13B-Base) | ğŸ¤— [Baichuan2-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat) | ğŸ¤— [Baichuan2-13B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat-4bits) |\n\n# Benchmark ç»“æœ\n\næˆ‘ä»¬åœ¨[é€šç”¨](#é€šç”¨é¢†åŸŸ)ã€[æ³•å¾‹](#æ³•å¾‹åŒ»ç–—)ã€[åŒ»ç–—](#æ³•å¾‹åŒ»ç–—)ã€[æ•°å­¦](#æ•°å­¦ä»£ç )ã€[ä»£ç ](#æ•°å­¦ä»£ç )å’Œ[å¤šè¯­è¨€ç¿»è¯‘](#å¤šè¯­è¨€ç¿»è¯‘)å…­ä¸ªé¢†åŸŸçš„ä¸­è‹±æ–‡å’Œå¤šè¯­è¨€æƒå¨æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ã€‚\n\n## é€šç”¨é¢†åŸŸ\n\nåœ¨é€šç”¨é¢†åŸŸæˆ‘ä»¬åœ¨ä»¥ä¸‹æ•°æ®é›†ä¸Šè¿›è¡Œäº† 5-shot æµ‹è¯•ã€‚\n- [C-Eval](https://cevalbenchmark.com/index.html#home) æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº† 52 ä¸ªå­¦ç§‘å’Œå››ä¸ªéš¾åº¦çš„çº§åˆ«ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ•°æ®é›†çš„ dev é›†ä½œä¸º few-shot çš„æ¥æºï¼Œåœ¨ test é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬é‡‡ç”¨äº† [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B/tree/main) çš„è¯„æµ‹æ–¹æ¡ˆã€‚\n- [MMLU](https://arxiv.org/abs/2009.03300) æ˜¯åŒ…å« 57 ä¸ªä»»åŠ¡çš„è‹±æ–‡è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº†åˆç­‰æ•°å­¦ã€ç¾å›½å†å²ã€è®¡ç®—æœºç§‘å­¦ã€æ³•å¾‹ç­‰ï¼Œéš¾åº¦è¦†ç›–é«˜ä¸­æ°´å¹³åˆ°ä¸“å®¶æ°´å¹³ï¼Œæ˜¯ç›®å‰ä¸»æµçš„ LLM è¯„æµ‹æ•°æ®é›†ã€‚æˆ‘ä»¬é‡‡ç”¨äº†[å¼€æº](https://github.com/hendrycks/test)çš„è¯„æµ‹æ–¹æ¡ˆã€‚\n- [CMMLU](https://github.com/haonan-li/CMMLU) æ˜¯ä¸€ä¸ªåŒ…å« 67 ä¸ªä¸»é¢˜çš„ç»¼åˆæ€§æ€§ä¸­æ–‡è¯„ä¼°åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨äº†å…¶[å®˜æ–¹](https://github.com/haonan-li/CMMLU)çš„è¯„æµ‹æ–¹æ¡ˆã€‚\n- [Gaokao](https://github.com/OpenLMLab/GAOKAO-Bench) æ˜¯ä¸€ä¸ªä»¥ä¸­å›½é«˜è€ƒé¢˜ä½œä¸ºè¯„æµ‹å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æ•°æ®é›†ï¼Œç”¨ä»¥è¯„ä¼°æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ æˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œå¹¶è¿›è¡Œäº†éšæœºåˆ’åˆ†ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸ C-Eval ç±»ä¼¼çš„è¯„æµ‹æ–¹æ¡ˆã€‚\n- [AGIEval](https://github.com/microsoft/AGIEval) æ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„è®¤çŸ¥å’Œè§£å†³é—®é¢˜ç›¸å…³çš„ä»»åŠ¡ä¸­çš„ä¸€èˆ¬èƒ½åŠ›ã€‚ æˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å››é€‰ä¸€å•é¡¹é€‰æ‹©é¢˜ï¼Œå¹¶è¿›è¡Œäº†éšæœºåˆ’åˆ†ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸ C-Eval ç±»ä¼¼çš„è¯„æµ‹æ–¹æ¡ˆã€‚\n- [BBH](https://huggingface.co/datasets/lukaemon/bbh) æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡ Big-Bench çš„å­é›†ã€‚Big-Bench ç›®å‰åŒ…æ‹¬ 204 é¡¹ä»»åŠ¡ã€‚ä»»åŠ¡ä¸»é¢˜æ¶‰åŠè¯­è¨€å­¦ã€å„¿ç«¥å‘å±•ã€æ•°å­¦ã€å¸¸è¯†æ¨ç†ã€ç”Ÿç‰©å­¦ã€ç‰©ç†å­¦ã€ç¤¾ä¼šåè§ã€è½¯ä»¶å¼€å‘ç­‰æ–¹é¢ã€‚BBH æ˜¯ä» 204 é¡¹ Big-Bench è¯„æµ‹åŸºå‡†ä»»åŠ¡ä¸­å¤§æ¨¡å‹è¡¨ç°ä¸å¥½çš„ä»»åŠ¡å•ç‹¬æ‹¿å‡ºæ¥å½¢æˆçš„è¯„æµ‹åŸºå‡†ã€‚\n\n### 7B æ¨¡å‹ç»“æœ\n\n|                       | **C-Eval** | **MMLU** | **CMMLU** | **Gaokao** | **AGIEval** | **BBH** |\n|:---------------------:|:----------:|:--------:|:---------:|:----------:|:-----------:|:-------:|\n|                       |  5-shot    |  5-shot  |  5-shot   | 5-shot     | 5-shot      | 3-shot  |\n| **GPT-4**             | 68.40      | 83.93    | 70.33     | 66.15      | 63.27       | 75.12   |\n| **GPT-3.5 Turbo**     | 51.10      | 68.54    | 54.06     | 47.07      | 46.13       | 61.59   |\n| **LLaMA-7B**          | 27.10      | 35.10    | 26.75     | 27.81      | 28.17       | 32.38   |\n| **LLaMA2-7B**         | 28.90      | 45.73    | 31.38     | 25.97      | 26.53       | 39.16   |\n| **MPT-7B**            | 27.15      | 27.93    | 26.00     | 26.54      | 24.83       | 35.20   |\n| **Falcon-7B**         | 24.23      | 26.03    | 25.66     | 24.24      | 24.10       | 28.77   |\n| **ChatGLM2-6B**       | 50.20      | 45.90    | 49.00     | 49.44      | 45.28       | 31.65   |\n| **Baichuan-7B**       | 42.80      | 42.30    | 44.02     | 36.34      | 34.44       | 32.48   |\n| **Baichuan2-7B-Base** | 54.00      | 54.16    | 57.07     | 47.47      | 42.73       | 41.56   |\n\n### 13B æ¨¡å‹ç»“æœ\n\n|                             | **C-Eval** | **MMLU** | **CMMLU** | **Gaokao** | **AGIEval** | **BBH** |\n|:---------------------------:|:----------:|:--------:|:---------:|:----------:|:-----------:|:-------:|\n|                             |  5-shot    |  5-shot  |  5-shot   | 5-shot     | 5-shot      | 3-shot  |\n| **GPT-4**                   | 68.40      | 83.93    | 70.33     | 66.15      | 63.27       | 75.12   |\n| **GPT-3.5 Turbo**           | 51.10      | 68.54    | 54.06     | 47.07      | 46.13       | 61.59   |\n| **LLaMA-13B**               | 28.50      | 46.30    | 31.15     | 28.23      | 28.22       | 37.89   |\n| **LLaMA2-13B**              | 35.80      | 55.09    | 37.99     | 30.83      | 32.29       | 46.98   |\n| **Vicuna-13B**              | 32.80      | 52.00    | 36.28     | 30.11      | 31.55       | 43.04   |\n| **Chinese-Alpaca-Plus-13B** | 38.80      | 43.90    | 33.43     | 34.78      | 35.46       | 28.94   |\n| **XVERSE-13B**              | 53.70      | 55.21    | 58.44     | 44.69      | 42.54       | 38.06   |\n| **Baichuan-13B-Base**       | 52.40      | 51.60    | 55.30     | 49.69      | 43.20       | 43.01   |\n| **Baichuan2-13B-Base**      | 58.10      | 59.17    | 61.97     | 54.33      | 48.17       | 48.78   |\n\n## æ³•å¾‹ã€åŒ»ç–—\n\næ³•å¾‹é¢†åŸŸæˆ‘ä»¬ä½¿ç”¨äº† [JEC-QA](https://jecqa.thunlp.org/) æ•°æ®é›†ã€‚JEC-QA æ•°æ®é›†æ¥æºäºä¸­å›½å›½å®¶å¸æ³•è€ƒè¯•ã€‚æˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å•é€‰é¢˜ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸ C-Eval ç±»ä¼¼çš„è¯„æµ‹æ–¹æ¡ˆã€‚\n\nåŒ»ç–—é¢†åŸŸåˆ™ä½¿ç”¨é€šç”¨é¢†åŸŸæ•°æ®é›†ï¼ˆC-Evalã€MMLUã€CMMLUï¼‰ä¸­çš„åŒ»å­¦ç›¸å…³å­¦ç§‘ã€[MedQA](https://arxiv.org/abs/2009.13081) å’Œ [MedMCQA](https://medmcqa.github.io/)ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸ C-Eval ç±»ä¼¼çš„è¯„æµ‹æ–¹æ¡ˆã€‚\n- ä¸ºäº†æµ‹è¯•æ–¹ä¾¿ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† C-Eval çš„ val é›†è¿›è¡Œæµ‹è¯•ã€‚\n- MedQA æ•°æ®é›†æ¥æºäºç¾å›½ã€ä¸­å›½çš„åŒ»å­¦è€ƒè¯•ã€‚æˆ‘ä»¬æµ‹è¯•äº† [MedQAæ•°æ®é›†](https://huggingface.co/datasets/bigbio/med_qa) ä¸­çš„ USMLE å’Œ MCMLE ä¸¤ä¸ªå­é›†ï¼Œå¹¶é‡‡ç”¨äº†äº”ä¸ªå€™é€‰çš„ç‰ˆæœ¬ã€‚\n- MedMCQA æ•°æ®é›†æ¥æºäºå°åº¦åŒ»å­¦é™¢çš„å…¥å­¦è€ƒè¯•ã€‚æˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å•é€‰é¢˜ã€‚ç”±äº test é›†æ²¡æœ‰ç­”æ¡ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ dev é›†è¿›è¡Œæµ‹è¯•ã€‚\n- é€šç”¨é¢†åŸŸæ•°æ®é›†åŒ…å«çš„åŒ»å­¦ç›¸å…³å­¦ç§‘å¦‚ä¸‹ï¼š\n    - C-Eval: clinical_medicine, basic_medicine\n    - MMLU: clinical_knowledge, anatomy, college_medicine, college_biology, nutrition, virology, medical_genetics, professional_medicine\n    - CMMLU: anatomy, clinical_knowledge, college_medicine, genetics, nutrition, traditional_chinese_medicine, virology \n\næˆ‘ä»¬å¯¹ä»¥ä¸Šæ•°æ®é›†è¿›è¡Œäº† 5-shot æµ‹è¯•ã€‚\n\n### 7B æ¨¡å‹ç»“æœ\n\n|                       | **JEC-QA** | **CEval-MMLU-CMMLU** | **MedQA-USMLE** | **MedQA-MCMLE** | **MedMCQA** |\n|:---------------------:|:----------:|:--------------------:|:---------------:|:---------------:|:-----------:|\n|                       | 5-shot     | 5-shot               | 5-shot          | 5-shot          | 5-shot      |\n| **GPT-4**             | 59.32      | 77.16                | 80.28           | 74.58           | 72.51       |\n| **GPT-3.5 Turbo**     | 42.31      | 61.17                | 53.81           | 52.92           | 56.25       |\n| **LLaMA-7B**          | 27.45      | 33.34                | 24.12           | 21.72           | 27.45       |\n| **LLaMA2-7B**         | 29.20      | 36.75                | 27.49           | 24.78           | 37.93       |\n| **MPT-7B**            | 27.45      | 26.67                | 16.97           | 19.79           | 31.96       |\n| **Falcon-7B**         | 23.66      | 25.33                | 21.29           | 18.07           | 33.88       |\n| **ChatGLM2-6B**       | 40.76      | 44.54                | 26.24           | 45.53           | 30.22       |\n| **Baichuan-7B**       | 34.64      | 42.37                | 27.42           | 39.46           | 31.39       |\n| **Baichuan2-7B-Base** | 44.46      | 56.39                | 32.68           | 54.93           | 41.73       |\n\n### 13B æ¨¡å‹ç»“æœ\n\n|                             | **JEC-QA** | **CEval-MMLU-CMMLU** | **MedQA-USMLE** | **MedQA-MCMLE** | **MedMCQA** |\n|:---------------------------:|:----------:|:--------------------:|:---------------:|:---------------:|:-----------:|\n|                             | 5-shot     |  5-shot              |  5-shot         |  5-shot         | 5-shot      |\n| **GPT-4**                   | 59.32      | 77.16                | 80.28           | 74.58           | 72.51       |\n| **GPT-3.5 Turbo**           | 42.31      | 61.17                | 53.81           | 52.92           | 56.25       |\n| **LLaMA-13B**               | 27.54      | 35.14                | 28.83           | 23.38           | 39.52       |\n| **LLaMA2-13B**              | 34.08      | 47.42                | 35.04           | 29.74           | 42.12       |\n| **Vicuna-13B**              | 28.38      | 40.99                | 34.80           | 27.67           | 40.66       |\n| **Chinese-Alpaca-Plus-13B** | 35.32      | 46.31                | 27.49           | 32.66           | 35.87       |\n| **XVERSE-13B**              | 46.42      | 58.08                | 32.99           | 58.76           | 41.34       |\n| **Baichuan-13B-Base**       | 41.34      | 51.77                | 29.07           | 43.67           | 39.60       |\n| **Baichuan2-13B-Base**      | 47.40      | 59.33                | 40.38           | 61.62           | 42.86       |\n\n## æ•°å­¦ã€ä»£ç \n\næ•°å­¦é¢†åŸŸæˆ‘ä»¬ä½¿ç”¨ [OpenCompass](https://opencompass.org.cn/) è¯„ä¼°æ¡†æ¶ï¼Œå¯¹ [GSM8K](https://huggingface.co/datasets/gsm8k) å’Œ [MATH](https://huggingface.co/datasets/competition_math) æ•°æ®é›†è¿›è¡Œäº† 4-shot æµ‹è¯•ã€‚\n\n- GSM8K æ˜¯ç”± OpenAI å‘å¸ƒçš„ä¸€ä¸ªç”± 8.5K é«˜è´¨é‡çš„è¯­è¨€å¤šæ ·åŒ–çš„å°å­¦æ•°å­¦åº”ç”¨é¢˜ç»„æˆçš„æ•°æ®é›†ï¼Œè¦æ±‚æ ¹æ®ç»™å®šçš„åœºæ™¯å’Œä¸¤ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œé€‰æ‹©æœ€åˆç†çš„æ–¹æ¡ˆã€‚\n- MATH æ•°æ®é›†åŒ…å« 12,500 ä¸ªæ•°å­¦é—®é¢˜ï¼ˆå…¶ä¸­ 7500 ä¸ªå±äºè®­ç»ƒé›†ï¼Œ5000 ä¸ªå±äºæµ‹è¯•é›†ï¼‰ï¼Œè¿™äº›é—®é¢˜æ”¶é›†è‡ª AMC 10ã€AMC 12ã€AIME ç­‰æ•°å­¦ç«èµ›ã€‚\n\nä»£ç é¢†åŸŸåˆ™é‡‡ç”¨äº† [HumanEval](https://huggingface.co/datasets/openai_humaneval) å’Œ [MBPP](https://huggingface.co/datasets/mbpp) æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨ OpenCompassï¼Œå¯¹ HumanEval è¿›è¡Œäº† 0-shot æµ‹è¯•ï¼ŒMBPP æ•°æ®é›†è¿›è¡Œäº† 3-shot æµ‹è¯•ã€‚\n- HumanEval ä¸­çš„ç¼–ç¨‹ä»»åŠ¡åŒ…æ‹¬æ¨¡å‹è¯­è¨€ç†è§£ã€æ¨ç†ã€ç®—æ³•å’Œç®€å•æ•°å­¦ï¼Œä»¥è¯„ä¼°æ¨¡å‹åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶è¡¡é‡æ¨¡å‹çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚\n- MBPP åŒ…æ‹¬ 974 ä¸ª Python çŸ­å‡½æ•°ã€ç¨‹åºçš„æ–‡å­—æè¿°ä»¥åŠç”¨äºæ£€æŸ¥åŠŸèƒ½æ­£ç¡®æ€§çš„æµ‹è¯•ç”¨ä¾‹çš„æ•°æ®é›†ã€‚\n\n### 7B æ¨¡å‹ç»“æœ\n\n|                       | **GSM8K** | **MATH** | **HumanEval** | **MBPP** |\n|:---------------------:|:---------:|:--------:|:-------------:|:--------:|\n|                       |  4-shot   | 4-shot   |  0-shot       |  3-shot  |\n| **GPT-4**             |   89.99   | 40.20    | 69.51         |  63.60   |\n| **GPT-3.5 Turbo**     |   57.77   | 13.96    | 52.44         |  61.40   |\n| **LLaMA-7B**          |   9.78    | 3.02     | 11.59         |  14.00   |\n| **LLaMA2-7B**         |   16.22   | 3.24     | 12.80         |  14.80   |\n| **MPT-7B**            |   8.64    | 2.90     | 14.02         |  23.40   |\n| **Falcon-7B**         |   5.46    | 1.68     | -             |  10.20   |\n| **ChatGLM2-6B**       |   28.89   | 6.40     | 9.15          |   9.00   |\n| **Baichuan-7B**       |   9.17    | 2.54     | 9.20          |   6.60   |\n| **Baichuan2-7B-Base** |   24.49   | 5.58     | 18.29         |  24.20   |\n\n### 13B æ¨¡å‹ç»“æœ\n\n|                             | **GSM8K** | **MATH** | **HumanEval** | **MBPP** |\n|:---------------------------:|:---------:|:--------:|:-------------:|:--------:|\n|                             |  4-shot   | 4-shot   |  0-shot       |  3-shot  |\n| **GPT-4**                   |   89.99   | 40.20    | 69.51         |  63.60   |\n| **GPT-3.5 Turbo**           |   57.77   | 13.96    | 52.44         |  61.40   |\n| **LLaMA-13B**               |   20.55   | 3.68     | 15.24         |  21.40   |\n| **LLaMA2-13B**              |   28.89   | 4.96     | 15.24         |  27.00   |\n| **Vicuna-13B**              |   28.13   | 4.36     | 16.46         |  15.00   |\n| **Chinese-Alpaca-Plus-13B** |   11.98   | 2.50     | 16.46         |  20.00   |\n| **XVERSE-13B**              |   18.20   | 2.18     | 15.85         |  16.80   |\n| **Baichuan-13B-Base**       |   26.76   | 4.84     | 11.59         |  22.80   |\n| **Baichuan2-13B-Base**      |   52.77   | 10.08    | 17.07         |  30.20   |\n\n## å¤šè¯­è¨€ç¿»è¯‘\n\næˆ‘ä»¬é‡‡ç”¨äº† [Flores-101](https://huggingface.co/datasets/facebook/flores) æ•°æ®é›†æ¥è¯„ä¼°æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚Flores-101 æ¶µç›–äº†ä¸–ç•Œå„åœ°çš„ 101 ç§è¯­è¨€ã€‚å®ƒçš„æ•°æ®æ¥æºäºæ–°é—»ã€æ—…æ¸¸æŒ‡å—å’Œä¹¦ç±ç­‰å¤šä¸ªä¸åŒé¢†åŸŸã€‚æˆ‘ä»¬é€‰æ‹©äº†è”åˆå›½å®˜æ–¹è¯­è¨€ï¼ˆé˜¿æ‹‰ä¼¯æ–‡ã€ä¸­æ–‡ã€è‹±æ–‡ã€æ³•æ–‡ã€ä¿„æ–‡å’Œè¥¿ç­ç‰™æ–‡ï¼‰ä»¥åŠå¾·æ–‡å’Œæ—¥æ–‡ä½œä¸ºæµ‹è¯•è¯­ç§ã€‚æˆ‘ä»¬ä½¿ç”¨ OpenCompass å¯¹ Flores-101 ä¸­çš„ä¸­-è‹±ã€ä¸­-æ³•ã€ä¸­-è¥¿ç­ç‰™ã€ä¸­-é˜¿æ‹‰ä¼¯ã€ä¸­-ä¿„ã€ä¸­-æ—¥ã€ä¸­-å¾·ç­‰ä¸ƒä¸ªå­ä»»åŠ¡åˆ†åˆ«è¿›è¡Œäº† 8-shot æµ‹è¯•ã€‚\n\n### 7B æ¨¡å‹ç»“æœ\n\n|             | **CN-EN** | **CN-FR** | **CN-ES** | **CN-AR** | **CN-RU** | **CN-JP** | **CN-DE** | Average |\n|:---------------------:|:-------:|:-------:|:---------:|:---------:|:-------:|:-------:|:-------:|:-------:|\n| **GPT-4**             | 29.94   | 29.56   | 20.01     | 10.76     | 18.62   | 13.26   | 20.83   | 20.43   |\n| **GPT-3.5 Turbo**     | 27.67   | 26.15   | 19.58     | 10.73     | 17.45   | 1.82    | 19.70   | 17.59   |\n| **LLaMA-7B**          | 17.27   | 12.02   | 9.54      | 0.00      | 4.47    | 1.41    | 8.73    | 7.63    |\n| **LLaMA2-7B**         | 25.76   | 15.14   | 11.92     | 0.79      | 4.99    | 2.20    | 10.15   | 10.14   |\n| **MPT-7B**            | 20.77   | 9.53    | 8.96      | 0.10      | 3.54    | 2.91    | 6.54    | 7.48    |\n| **Falcon-7B**         | 22.13   | 15.67   | 9.28      | 0.11      | 1.35    | 0.41    | 6.41    | 7.91    |\n| **ChatGLM2-6B**       | 22.28   | 9.42    | 7.77      | 0.64      | 1.78    | 0.26    | 4.61    | 6.68    |\n| **Baichuan-7B**       | 25.07   | 16.51   | 12.72     | 0.41      | 6.66    | 2.24    | 9.86    | 10.50   |\n| **Baichuan2-7B-Base** | 27.27   | 20.87   | 16.17     | 1.39      | 11.21   | 3.11    | 12.76   | 13.25   |\n\n### 13B æ¨¡å‹ç»“æœ\n\n|                   | **CN-EN** | **CN-FR** | **CN-ES** | **CN-AR** | **CN-RU** | **CN-JP** | **CN-DE** | Average |\n|:---------------------------:|:-------:|:-------:|:---------:|:---------:|:-------:|:-------:|:-------:|:-------:|\n|          **GPT-4**          | 29.94   | 29.56   | 20.01     | 10.76     | 18.62   | 13.26   | 20.83   | 20.43   |\n|      **GPT-3.5 Turbo**      | 27.67   | 26.15   | 19.58     | 10.73     | 17.45   | 1.82    | 19.70   | 17.59   |\n|        **LLaMA-13B**        | 21.75   | 16.16   | 13.29     | 0.58      | 7.61    | 0.41    | 10.66   | 10.07   |\n|       **LLaMA2-13B**        | 25.44   | 19.25   | 17.49     | 1.38      | 10.34   | 0.13    | 11.13   | 12.17   |\n|       **Vicuna-13B**        | 22.63   | 18.04   | 14.67     | 0.70      | 9.27    | 3.59    | 10.25   | 11.31   |\n| **Chinese-Alpaca-Plus-13B** | 22.53   | 13.82   | 11.29     | 0.28      | 1.52    | 0.31    | 8.13    | 8.27    |\n|       **XVERSE-13B**        | 29.26   | 24.03   | 16.67     | 2.78      | 11.61   | 3.08    | 14.26   | 14.53   |\n|    **Baichuan-13B-Base**    | 30.24   | 20.90   | 15.92     | 0.98      | 9.65    | 2.64    | 12.00   | 13.19   |\n|    **Baichuan2-13B-Base**   | 30.61   | 22.11   | 17.27     | 2.39      | 14.17   | 11.58   | 14.53   | 16.09   |\n\n# æ¨ç†å’Œéƒ¨ç½²\n\næ¨ç†æ‰€éœ€çš„æ¨¡å‹æƒé‡ã€æºç ã€é…ç½®å·²å‘å¸ƒåœ¨ Hugging Faceï¼Œä¸‹è½½é“¾æ¥è§æœ¬æ–‡æ¡£æœ€å¼€å§‹çš„è¡¨æ ¼ã€‚æˆ‘ä»¬åœ¨æ­¤ç¤ºèŒƒå¤šç§æ¨ç†æ–¹å¼ã€‚ç¨‹åºä¼šè‡ªåŠ¨ä» Hugging Face ä¸‹è½½æ‰€éœ€èµ„æºã€‚\n\n## å®‰è£…ä¾èµ–\n```shell\npip install -r requirements.txt\n```\n\n## Python ä»£ç æ–¹å¼\n\n### Chat æ¨¡å‹æ¨ç†æ–¹æ³•ç¤ºèŒƒ\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> from transformers.generation.utils import GenerationConfig\n>>> tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\", use_fast=False, trust_remote_code=True)\n>>> model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n>>> model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\")\n>>> messages = []\n>>> messages.append({\"role\": \"user\", \"content\": \"è§£é‡Šä¸€ä¸‹â€œæ¸©æ•…è€ŒçŸ¥æ–°â€\"})\n>>> response = model.chat(tokenizer, messages)\n>>> print(response)\n\"æ¸©æ•…è€ŒçŸ¥æ–°\"æ˜¯ä¸€å¥ä¸­å›½å¤ä»£çš„æˆè¯­ï¼Œå‡ºè‡ªã€Šè®ºè¯­Â·ä¸ºæ”¿ã€‹ç¯‡ã€‚è¿™å¥è¯çš„æ„æ€æ˜¯ï¼šé€šè¿‡å›é¡¾è¿‡å»ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°æ–°çš„çŸ¥è¯†å’Œç†è§£ã€‚æ¢å¥è¯è¯´ï¼Œå­¦ä¹ å†å²å’Œç»éªŒå¯ä»¥è®©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ç°åœ¨å’Œæœªæ¥ã€‚\n\nè¿™å¥è¯é¼“åŠ±æˆ‘ä»¬åœ¨å­¦ä¹ å’Œç”Ÿæ´»ä¸­ä¸æ–­åœ°å›é¡¾å’Œåæ€è¿‡å»çš„ç»éªŒï¼Œä»è€Œè·å¾—æ–°çš„å¯ç¤ºå’Œæˆé•¿ã€‚é€šè¿‡é‡æ¸©æ—§çš„çŸ¥è¯†å’Œç»å†ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°æ–°çš„è§‚ç‚¹å’Œç†è§£ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹ä¸æ–­å˜åŒ–çš„ä¸–ç•Œå’ŒæŒ‘æˆ˜ã€‚\n```\n\n### Base æ¨¡å‹æ¨ç†æ–¹æ³•ç¤ºèŒƒ\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-13B-Base\", trust_remote_code=True)\n>>> model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-13B-Base\", device_map=\"auto\", trust_remote_code=True)\n>>> inputs = tokenizer('ç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\\nå¤œé›¨å¯„åŒ—->', return_tensors='pt')\n>>> inputs = inputs.to('cuda:0')\n>>> pred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\n>>> print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\nç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\nå¤œé›¨å¯„åŒ—->æå•†éš\n```\n\n> åœ¨ä¸Šè¿°ä¸¤æ®µä»£ç ä¸­ï¼Œæ¨¡å‹åŠ è½½æŒ‡å®š `device_map='auto'`ï¼Œä¼šä½¿ç”¨æ‰€æœ‰å¯ç”¨æ˜¾å¡ã€‚å¦‚éœ€æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡ï¼Œå¯ä»¥ä½¿ç”¨ç±»ä¼¼ `export CUDA_VISIBLE_DEVICES=0,1`ï¼ˆä½¿ç”¨äº†0ã€1å·æ˜¾å¡ï¼‰çš„æ–¹å¼æ§åˆ¶ã€‚\n\n## å‘½ä»¤è¡Œå·¥å…·æ–¹å¼\n\n```shell\npython cli_demo.py\n```\næœ¬å‘½ä»¤è¡Œå·¥å…·æ˜¯ä¸º Chat åœºæ™¯è®¾è®¡ï¼Œå› æ­¤æˆ‘ä»¬ä¸æ”¯æŒä½¿ç”¨è¯¥å·¥å…·è°ƒç”¨ Base æ¨¡å‹ã€‚\n\n## ç½‘é¡µ demo æ–¹å¼\n\nä¾é  streamlit è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä¼šåœ¨æœ¬åœ°å¯åŠ¨ä¸€ä¸ª web æœåŠ¡ï¼ŒæŠŠæ§åˆ¶å°ç»™å‡ºçš„åœ°å€æ”¾å…¥æµè§ˆå™¨å³å¯è®¿é—®ã€‚æœ¬ç½‘é¡µ demo å·¥å…·æ˜¯ä¸º Chat åœºæ™¯è®¾è®¡ï¼Œå› æ­¤æˆ‘ä»¬ä¸æ”¯æŒä½¿ç”¨è¯¥å·¥å…·è°ƒç”¨ Base æ¨¡å‹ã€‚\n\n```shell\nstreamlit run web_demo.py\n```\n\n## Baichuan2-13B-Chat ç¤ºä¾‹è¾“å‡º\n\n<details><summary><b>ä»£ç </b></summary>\n\n`````\nç”¨æˆ·ï¼šæ‰¾åˆ°ä¸‹åˆ—æ•°ç»„çš„ä¸­ä½æ•°[3.1,6.2,1.3,8.4,10.5,11.6,2.1]ï¼Œè¯·ç”¨pythonä»£ç å®Œæˆä»¥ä¸ŠåŠŸèƒ½\n\nBaichuan 2ï¼šdef find_median(numbers):\n    numbers.sort()\n    length = len(numbers)\n    if length % 2 == 0:\n        return (numbers[length // 2 - 1] + numbers[length // 2]) / 2\n    else:\n        return numbers[length // 2]\nnumbers = [3.1, 6.2, 1.3, 8.4, 10.5, 11.6, 2.1]\nprint(find_median(numbers))\n`````\n\n</details>\n\n<details><summary><b>æ•°å­¦</b></summary>\n\n`````\nç”¨æˆ·ï¼šé¸¡å’Œå…”åœ¨ä¸€ä¸ªç¬¼å­é‡Œï¼Œå…±æœ‰26ä¸ªå¤´ï¼Œ68åªè„šï¼Œé‚£ä¹ˆé¸¡æœ‰å¤šå°‘åªï¼Œå…”æœ‰å¤šå°‘åªï¼Ÿ\n\nBaichuan 2ï¼šæˆ‘ä»¬å¯ä»¥ç”¨ä»£æ•°æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å‡è®¾é¸¡çš„æ•°é‡ä¸ºxï¼Œå…”å­çš„æ•°é‡ä¸ºyã€‚æ ¹æ®é¢˜ç›®ç»™å‡ºçš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä»¥ä¸‹ä¸¤ä¸ªæ–¹ç¨‹ï¼š\n\n1. x + y = 26 (å¤´çš„æ€»æ•°)\n2. 2x + 4y = 68 (è„šçš„æ€»æ•°)\n\nç°åœ¨æˆ‘ä»¬éœ€è¦è§£è¿™ä¸ªäºŒå…ƒä¸€æ¬¡æ–¹ç¨‹ç»„ã€‚é¦–å…ˆä»ç¬¬ä¸€ä¸ªæ–¹ç¨‹ä¸­è§£å‡ºxï¼š\n\nx = 26 - y\n\nç„¶åå°†xçš„è¡¨è¾¾å¼ä»£å…¥ç¬¬äºŒä¸ªæ–¹ç¨‹ï¼š\n\n2(26 - y) + 4y = 68\n52 - 2y + 4y = 68\n2y = 16\n\næ¥ä¸‹æ¥æˆ‘ä»¬æ±‚è§£yçš„å€¼ï¼š\n\ny = 16 / 2\ny = 8\n\nç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†å…”å­çš„æ•°é‡æ˜¯8åªã€‚å°†yçš„å€¼ä»£å…¥xçš„è¡¨è¾¾å¼æ±‚å¾—é¸¡çš„æ•°é‡ï¼š\n\nx = 26 - 8\nx = 18\n\næ‰€ä»¥é¸¡æœ‰18åªï¼Œå…”å­æœ‰8åªã€‚\n`````\n\n</details>\n\n<details><summary><b>æŒ‡ä»¤è·Ÿéš</b></summary>\n\n`````\nç”¨æˆ·ï¼šä»ç°åœ¨å¼€å§‹ï¼Œè¯·ä½ å°†æˆ‘æ¥ä¸‹æ¥è¯´çš„æ¯ä¸€å¥è¯ç¿»è¯‘æˆè‹±è¯­ï¼Œå‡†å¤‡å¥½äº†å—ï¼Ÿ\n\nBaichuan 2ï¼šI'm ready to translate every word you say next into English.\n\nç”¨æˆ·ï¼šä½ å¥½\n\nBaichuan 2ï¼šHello\n\nç”¨æˆ·ï¼šæˆ‘æ˜¯Baichuan 2ï¼Œå¾ˆé«˜å…´è®¤è¯†å¤§å®¶\n\nBaichuan 2ï¼šI am Baichuan 2, and I am very happy to meet everyone.\n\nç”¨æˆ·ï¼šå¸Œæœ›é€šè¿‡å¤§å®¶çš„å…±åŒåŠªåŠ›ï¼Œæ—©æ—¥è¿æ¥AGIæ—¶ä»£çš„åˆ°æ¥\n\nBaichuan 2ï¼šI hope through the joint efforts of all of us, we can welcome the arrival of the AGI era as soon as possible.\n`````\n</details>\n\n<details><summary><b>è§£é¢˜</b></summary>\n\n`````\nç”¨æˆ·ï¼šä»¥ä¸‹ç‰©ç†å¸¸è¯†é¢˜ç›®ï¼Œå“ªä¸€ä¸ªæ˜¯é”™è¯¯çš„?A.åœ¨è‡ªç„¶ç¯å¢ƒä¸‹ï¼Œå£°éŸ³åœ¨å›ºä½“ä¸­ä¼ æ’­é€Ÿåº¦æœ€å¿«ã€‚B.ç‰›é¡¿ç¬¬ä¸€å®šå¾‹:ä¸€ä¸ªç‰©ä½“å¦‚æœä¸å—åŠ›ä½œç”¨ï¼Œå°†ä¿æŒé™æ­¢æˆ–åŒ€é€Ÿç›´çº¿è¿åŠ¨çš„çŠ¶æ€ã€‚C.ç‰›é¡¿ç¬¬ä¸‰å®šå¾‹:å¯¹äºæ¯ä¸ªä½œç”¨åŠ›ï¼Œéƒ½æœ‰ä¸€ä¸ªç›¸ç­‰è€Œåå‘çš„åä½œç”¨åŠ›ã€‚D.å£°éŸ³åœ¨ç©ºæ°”ä¸­çš„ä¼ æ’­é€Ÿåº¦ä¸º1000m/sã€‚\n\nBaichuan 2ï¼šæœ¬é¢˜è€ƒæŸ¥ç‰©ç†åŸºæœ¬æ¦‚å¿µå’ŒåŸç†çš„ç†è§£å’Œåº”ç”¨èƒ½åŠ›ã€‚\nAé¡¹ï¼šå£°éŸ³çš„ä¼ æ’­éœ€è¦ä»‹è´¨ï¼Œåœ¨å›ºä½“ã€æ¶²ä½“å’Œæ°”ä½“çš„ä¸‰ç§ä»‹è´¨ä¸­ï¼Œå£°éŸ³åœ¨å›ºä½“ä¸­çš„ä¼ æ’­é€Ÿåº¦æ˜¯æœ€å¿«çš„ï¼Œæ•…Aæ­£ç¡®ï¼› Bé¡¹ï¼šç‰›é¡¿ç¬¬ä¸€å®šå¾‹æ˜¯æƒ¯æ€§å®šå¾‹ï¼Œå…¶å†…å®¹æ˜¯ï¼šä¸€ä¸ªç‰©ä½“å¦‚æœæ²¡æœ‰å—åˆ°å¤–åŠ›çš„ä½œç”¨ï¼Œè¦ä¹ˆä¿æŒé™æ­¢çŠ¶æ€ï¼Œè¦ä¹ˆä¿æŒåŒ€é€Ÿç›´çº¿è¿åŠ¨çŠ¶æ€ï¼Œç›´åˆ°æœ‰å¤–åŠ›è¿«ä½¿å®ƒæ”¹å˜è¿åŠ¨çŠ¶æ€ä¸ºæ­¢ï¼Œæ•…Bæ­£ç¡®ï¼› Cé¡¹ï¼šç‰›é¡¿ç¬¬ä¸‰å®šå¾‹çš„å†…å®¹æ˜¯ï¼šå¯¹äºæ¯ä¸€ä¸ªä½œç”¨åŠ›ï¼Œéƒ½æœ‰ä¸€ä¸ªå¤§å°ç›¸ç­‰ã€æ–¹å‘ç›¸åçš„åä½œç”¨åŠ›ï¼Œè¿™ä¸¤ä¸ªåŠ›åˆ†åˆ«ä½œç”¨åœ¨ä¸¤ä¸ªç›¸äº’ä½œç”¨çš„ç‰©ä½“ä¸Šï¼Œæ•…Cé”™è¯¯ï¼› Dé¡¹ï¼šå£°éŸ³åœ¨ç©ºæ°”ä¸­ä»¥æ³¢çš„å½¢å¼ä¼ æ’­ï¼Œå…¶ä¼ æ’­é€Ÿåº¦ä¸ä»‹è´¨çš„æ¸©åº¦ã€æ¹¿åº¦ç­‰å› ç´ æœ‰å…³ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå£°éŸ³åœ¨ç©ºæ°”ä¸­çš„ä¼ æ’­é€Ÿåº¦çº¦ä¸º340m/så·¦å³ï¼Œè€Œä¸æ˜¯1000m/sï¼Œæ•…Dé”™è¯¯ã€‚ å› æ­¤ï¼Œæœ¬é¢˜çš„æ­£ç¡®ç­”æ¡ˆæ˜¯ D ã€‚\n`````\n`````\nç”¨æˆ·ï¼šI'm Mike ã€‚I am going to have a busy weekendã€‚On Saturdayï¼ŒI am going to learn how to swimã€‚I will go with my fatherã€‚Then we are going to have lunch in the restaurantã€‚In the afternoonï¼Œwe are going to the zooã€‚We are going to see the pandasã€‚On Sundayï¼ŒI will finish my homework in the morningã€‚In the afternoonï¼Œmy parents and I are going to visit my grandparentsã€‚We will have dinner togetherã€‚\né˜…è¯»çŸ­æ–‡ï¼Œåˆ¤æ–­æ­£ç¡®å’Œé”™è¯¯ï¼æ­£ç¡®è¯·å†™Tï¼Œé”™è¯¯è¯·å†™F:\n1.Mike is going to have a busy weekend()\n2.Mike is going to learn how to swim with his father()\n3.They are going to have lunch at home()\n\nBaichuan 2ï¼š1. Tï¼ˆæ­£ç¡®çš„ï¼‰\n2. Tï¼ˆæ­£ç¡®çš„ï¼‰\n3. Fï¼ˆé”™è¯¯çš„ï¼‰\n`````\n\n</details>\n\n## é‡åŒ–éƒ¨ç½²\n\nä¸ºäº†è®©ä¸åŒçš„ç”¨æˆ·ä»¥åŠä¸åŒçš„å¹³å°éƒ½èƒ½è¿è¡Œ Baichuan 2 æ¨¡å‹ï¼Œæˆ‘ä»¬é’ˆå¯¹ Baichuan 2 æ¨¡å‹åšäº†ç›¸åº”åœ°é‡åŒ–å·¥ä½œï¼ˆåŒ…æ‹¬ Baichuan2-7B-Chat å’Œ Baichuan2-13B-Chatï¼‰ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿé«˜æ•ˆåœ°åœ¨è‡ªå·±çš„å¹³å°éƒ¨ç½² Baichuan 2 æ¨¡å‹ã€‚\n\n### é‡åŒ–æ–¹æ³•\n\nBaichuan 2 çš„é‡‡ç”¨ç¤¾åŒºä¸»æµçš„é‡åŒ–æ–¹æ³•ï¼š[BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)ã€‚è¯¥æ–¹æ³•å¯ä»¥ä¿è¯é‡åŒ–åçš„æ•ˆæœåŸºæœ¬ä¸æ‰ç‚¹ï¼Œç›®å‰å·²ç»é›†æˆåˆ° transformers åº“é‡Œï¼Œå¹¶åœ¨ç¤¾åŒºå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚BitsAndBytes æ”¯æŒ 8bits å’Œ 4bits ä¸¤ç§é‡åŒ–ï¼Œå…¶ä¸­ 4bits æ”¯æŒ FP4 å’Œ NF4 ä¸¤ç§æ ¼å¼ï¼ŒBaichuan 2 é€‰ç”¨ NF4 ä½œä¸º 4bits é‡åŒ–çš„æ•°æ®ç±»å‹ã€‚  \n  \nåŸºäºè¯¥é‡åŒ–æ–¹æ³•ï¼ŒBaichuan 2 æ”¯æŒåœ¨çº¿é‡åŒ–å’Œç¦»çº¿é‡åŒ–ä¸¤ç§æ¨¡å¼ã€‚\n\n### åœ¨çº¿é‡åŒ–\n\nå¯¹äºåœ¨çº¿é‡åŒ–ï¼Œæˆ‘ä»¬æ”¯æŒ 8bits å’Œ 4bits é‡åŒ–ï¼Œä½¿ç”¨æ–¹å¼å’Œ [Baichuan-13B](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) é¡¹ç›®ä¸­çš„æ–¹å¼ç±»ä¼¼ï¼Œåªéœ€è¦å…ˆåŠ è½½æ¨¡å‹åˆ° CPU çš„å†…å­˜é‡Œï¼Œå†è°ƒç”¨`quantize()`æ¥å£é‡åŒ–ï¼Œæœ€åè°ƒç”¨ `cuda()`å‡½æ•°ï¼Œå°†é‡åŒ–åçš„æƒé‡æ‹·è´åˆ° GPU æ˜¾å­˜ä¸­ã€‚å®ç°æ•´ä¸ªæ¨¡å‹åŠ è½½çš„ä»£ç éå¸¸ç®€å•ï¼Œæˆ‘ä»¬ä»¥ Baichuan2-7B-Chat ä¸ºä¾‹ï¼š\n\n8bits åœ¨çº¿é‡åŒ–:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", torch_dtype=torch.float16, trust_remote_code=True)\nmodel = model.quantize(8).cuda() \n```\n4bits åœ¨çº¿é‡åŒ–:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", torch_dtype=torch.float16, trust_remote_code=True)\nmodel = model.quantize(4).cuda() \n```\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ç”¨ `from_pretrained` æ¥å£çš„æ—¶å€™ï¼Œç”¨æˆ·ä¸€èˆ¬ä¼šåŠ ä¸Š `device_map=\"auto\"`ï¼Œåœ¨ä½¿ç”¨åœ¨çº¿é‡åŒ–æ—¶ï¼Œéœ€è¦å»æ‰è¿™ä¸ªå‚æ•°ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚\n\n### ç¦»çº¿é‡åŒ–\n\nä¸ºäº†æ–¹ä¾¿ç”¨æˆ·çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†ç¦»çº¿é‡åŒ–å¥½çš„ 4bits çš„ç‰ˆæœ¬ [Baichuan2-7B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits/tree/main)ï¼Œä¾›ç”¨æˆ·ä¸‹è½½ã€‚\nç”¨æˆ·åŠ è½½ Baichuan2-7B-Chat-4bits æ¨¡å‹å¾ˆç®€å•ï¼Œåªéœ€è¦æ‰§è¡Œ:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat-4bits\", device_map=\"auto\", trust_remote_code=True)\n```\nå¯¹äº 8bits ç¦»çº¿é‡åŒ–ï¼Œæˆ‘ä»¬æ²¡æœ‰æä¾›ç›¸åº”çš„ç‰ˆæœ¬ï¼Œå› ä¸º Hugging Face transformers åº“æä¾›äº†ç›¸åº”çš„ API æ¥å£ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„å®ç° 8bits é‡åŒ–æ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½ã€‚ç”¨æˆ·å¯ä»¥è‡ªè¡ŒæŒ‰ç…§å¦‚ä¸‹æ–¹å¼å®ç° 8bits çš„æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ï¼š\n```python\n# Model saving: model_id is the original model directory, and quant8_saved_dir is the directory where the 8bits quantized model is saved.\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)\nmodel.save_pretrained(quant8_saved_dir)\nmodel = AutoModelForCausalLM.from_pretrained(quant8_saved_dir, device_map=\"auto\", trust_remote_code=True)\n```\n\n### é‡åŒ–æ•ˆæœ\n\né‡åŒ–å‰åæ˜¾å­˜å ç”¨å¯¹æ¯” (GPU Mem in GB)ï¼š\n| Precision   | Baichuan2-7B |Baichuan2-13B |\n|-------------|:------------:|:------------:|\n| bf16 / fp16 | 15.3         | 27.5         |\n| 8bits       | 8.0          | 16.1         |\n| 4bits       | 5.1          | 8.6          |\n\né‡åŒ–ååœ¨å„ä¸ª benchmark ä¸Šçš„ç»“æœå’ŒåŸå§‹ç‰ˆæœ¬å¯¹æ¯”å¦‚ä¸‹ï¼š\n\n| Model 5-shot           | C-Eval | MMLU | CMMLU |\n|------------------------|:------:|:----:|:-----:|\n| Baichuan2-13B-Chat      | 56.74  | 57.32| 59.68  |\n| Baichuan2-13B-Chat-4bits | 56.05   | 56.24 | 58.82  |\n| Baichuan2-7B-Chat       | 54.35   | 52.93 | 54.99  |\n| Baichuan2-7B-Chat-4bits | 53.04   | 51.72 | 52.84  |\n> C-Eval æ˜¯åœ¨å…¶ val set ä¸Šè¿›è¡Œçš„è¯„æµ‹\n\nå¯ä»¥çœ‹åˆ°ï¼Œ4bits ç›¸å¯¹ bfloat16 ç²¾åº¦æŸå¤±åœ¨ 1 - 2 ä¸ªç™¾åˆ†ç‚¹å·¦å³ã€‚\n\n## CPU éƒ¨ç½²\n\nBaichuan 2 æ¨¡å‹æ”¯æŒ CPU æ¨ç†ï¼Œä½†éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼ŒCPU çš„æ¨ç†é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚éœ€æŒ‰å¦‚ä¸‹æ–¹å¼ä¿®æ”¹æ¨¡å‹åŠ è½½çš„æ–¹å¼ï¼š\n```python\n# Taking Baichuan2-7B-Chat as an example\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", torch_dtype=torch.float32, trust_remote_code=True)\n```\n## å¯¹ Baichuan 1 çš„æ¨ç†ä¼˜åŒ–è¿ç§»åˆ° Baichuan 2\n\nç”±äºå¾ˆå¤šç”¨æˆ·åœ¨ Baichuan 1 (Baichuan-7B, Baichuan-13B)ä¸Šåšäº†å¾ˆå¤šä¼˜åŒ–çš„å·¥ä½œï¼Œä¾‹å¦‚ç¼–è¯‘ä¼˜åŒ–ã€é‡åŒ–ç­‰ï¼Œä¸ºäº†å°†è¿™äº›å·¥ä½œé›¶æˆæœ¬åœ°åº”ç”¨äº Baichuan 2ï¼Œç”¨æˆ·å¯ä»¥å¯¹ Baichuan 2 æ¨¡å‹åšä¸€ä¸ªç¦»çº¿è½¬æ¢ï¼Œè½¬æ¢åå°±å¯ä»¥å½“åš Baichuan 1 æ¨¡å‹æ¥ä½¿ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œç”¨æˆ·åªéœ€è¦åˆ©ç”¨ä»¥ä¸‹è„šæœ¬ç¦»çº¿å¯¹ Baichuan 2 æ¨¡å‹çš„æœ€åä¸€å±‚ lm_head åšå½’ä¸€åŒ–ï¼Œå¹¶æ›¿æ¢æ‰`lm_head.weight`å³å¯ã€‚æ›¿æ¢å®Œåï¼Œå°±å¯ä»¥åƒå¯¹ Baichuan 1 æ¨¡å‹ä¸€æ ·å¯¹è½¬æ¢åçš„æ¨¡å‹åšç¼–è¯‘ä¼˜åŒ–ç­‰å·¥ä½œäº†ã€‚\n```python\nimport torch\nimport os\nori_model_dir = 'your Baichuan 2 model directory'\n# To avoid overwriting the original model, it's best to save the converted model to another directory before replacing it\nnew_model_dir = 'your normalized lm_head weight Baichuan 2 model directory'\nmodel = torch.load(os.path.join(ori_model_dir, 'pytorch_model.bin'))\nlm_head_w = model['lm_head.weight']\nlm_head_w = torch.nn.functional.normalize(lm_head_w)\nmodel['lm_head.weight'] = lm_head_w\ntorch.save(model, os.path.join(new_model_dir, 'pytorch_model.bin'))\n```\n\n# æ¨¡å‹å¾®è°ƒ\n\n## ä¾èµ–å®‰è£…\n\n```shell\ngit clone https://github.com/baichuan-inc/Baichuan2.git\ncd Baichuan2/fine-tune\npip install -r requirements.txt\n```\n- å¦‚éœ€ä½¿ç”¨ LoRA ç­‰è½»é‡çº§å¾®è°ƒæ–¹æ³•éœ€é¢å¤–å®‰è£… [peft](https://github.com/huggingface/peft)\n- å¦‚éœ€ä½¿ç”¨ xFormers è¿›è¡Œè®­ç»ƒåŠ é€Ÿéœ€é¢å¤–å®‰è£… [xFormers](https://github.com/facebookresearch/xformers)\n\n## å•æœºè®­ç»ƒ\n\nä¸‹é¢æˆ‘ä»¬ç»™ä¸€ä¸ªå¾®è°ƒ Baichuan2-7B-Base çš„å•æœºè®­ç»ƒä¾‹å­ã€‚\n\nè®­ç»ƒæ•°æ®ï¼š`data/belle_chat_ramdon_10k.json`ï¼Œè¯¥æ ·ä¾‹æ•°æ®æ˜¯ä» [multiturn_chat_0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) é‡‡æ ·å‡º 1 ä¸‡æ¡ï¼Œå¹¶ä¸”åšäº†æ ¼å¼è½¬æ¢ã€‚ä¸»è¦æ˜¯å±•ç¤ºå¤šè½®æ•°æ®æ€ä¹ˆè®­ç»ƒï¼Œä¸ä¿è¯æ•ˆæœã€‚\n\n```shell\nhostfile=\"\"\ndeepspeed --hostfile=$hostfile fine-tune.py  \\\n    --report_to \"none\" \\\n    --data_path \"data/belle_chat_ramdon_10k.json\" \\\n    --model_name_or_path \"baichuan-inc/Baichuan2-7B-Base\" \\\n    --output_dir \"output\" \\\n    --model_max_length 512 \\\n    --num_train_epochs 4 \\\n    --per_device_train_batch_size 16 \\\n    --gradient_accumulation_steps 1 \\\n    --save_strategy epoch \\\n    --learning_rate 2e-5 \\\n    --lr_scheduler_type constant \\\n    --adam_beta1 0.9 \\\n    --adam_beta2 0.98 \\\n    --adam_epsilon 1e-8 \\\n    --max_grad_norm 1.0 \\\n    --weight_decay 1e-4 \\\n    --warmup_ratio 0.0 \\\n    --logging_steps 1 \\\n    --gradient_checkpointing True \\\n    --deepspeed ds_config.json \\\n    --bf16 True \\\n    --tf32 True\n```\n\n## å¤šæœºè®­ç»ƒ\n\nå¤šæœºè®­ç»ƒåªéœ€è¦ç»™ä¸€ä¸‹ hostfile ï¼Œå†…å®¹ç±»ä¼¼å¦‚ä¸‹ï¼š\n```\nip1 slots=8\nip2 slots=8\nip3 slots=8\nip4 slots=8\n....\n```\nåŒæ—¶åœ¨è®­ç»ƒè„šæœ¬é‡Œé¢æŒ‡å®š hosftfile çš„è·¯å¾„ï¼š\n```shell\nhostfile=\"/path/to/hostfile\"\ndeepspeed --hostfile=$hostfile fine-tune.py  \\\n    --report_to \"none\" \\\n    --data_path \"data/belle_chat_ramdon_10k.json\" \\\n    --model_name_or_path \"baichuan-inc/Baichuan2-7B-Base\" \\\n    --output_dir \"output\" \\\n    --model_max_length 512 \\\n    --num_train_epochs 4 \\\n    --per_device_train_batch_size 16 \\\n    --gradient_accumulation_steps 1 \\\n    --save_strategy epoch \\\n    --learning_rate 2e-5 \\\n    --lr_scheduler_type constant \\\n    --adam_beta1 0.9 \\\n    --adam_beta2 0.98 \\\n    --adam_epsilon 1e-8 \\\n    --max_grad_norm 1.0 \\\n    --weight_decay 1e-4 \\\n    --warmup_ratio 0.0 \\\n    --logging_steps 1 \\\n    --gradient_checkpointing True \\\n    --deepspeed ds_config.json \\\n    --bf16 True \\\n    --tf32 True\n```\n\n## è½»é‡åŒ–å¾®è°ƒ\n\nä»£ç å·²ç»æ”¯æŒè½»é‡åŒ–å¾®è°ƒå¦‚ LoRAï¼Œå¦‚éœ€ä½¿ç”¨ä»…éœ€åœ¨ä¸Šé¢çš„è„šæœ¬ä¸­åŠ å…¥ä»¥ä¸‹å‚æ•°ï¼š\n```shell\n--use_lora True\n```\nLoRA å…·ä½“çš„é…ç½®å¯è§ `fine-tune.py` è„šæœ¬ã€‚\n\nä½¿ç”¨ LoRA å¾®è°ƒåå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åŠ è½½æ¨¡å‹ï¼š\n```python\nfrom peft import AutoPeftModelForCausalLM\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"output\", trust_remote_code=True)\n```\n\n# ä¸­é—´ Checkpoints\n\né™¤äº†è®­ç»ƒäº† 2.6 ä¸‡äº¿ Tokens çš„ Baichuan2-7B-Base æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†åœ¨æ­¤ä¹‹å‰çš„å¦å¤– 11 ä¸ªä¸­é—´ checkpointsï¼ˆåˆ†åˆ«å¯¹åº”è®­ç»ƒäº†çº¦ 0.2 ~ 2.4 ä¸‡äº¿ Tokensï¼‰ä¾›ç¤¾åŒºç ”ç©¶ä½¿ç”¨ï¼ˆ[ä¸‹è½½åœ°å€](https://huggingface.co/baichuan-inc/Baichuan2-7B-Intermediate-Checkpoints)ï¼‰ã€‚ä¸‹å›¾ç»™å‡ºäº†è¿™äº› checkpoints åœ¨ C-Evalã€MMLUã€CMMLU ä¸‰ä¸ª benchmark ä¸Šçš„æ•ˆæœå˜åŒ–ï¼š\n\n<div align=\"center\">\n<img src=\"https://github.com/baichuan-inc/Baichuan2/blob/main/media/checkpoints.jpeg?raw=true\" width=50% />\n</div>\n\n# ç¤¾åŒºä¸ç”Ÿæ€\n\n**ğŸ“¢ğŸ“¢ğŸ“¢ æˆ‘ä»¬ä¼šåœ¨æ­¤æŒç»­æ›´æ–°ç¤¾åŒºå’Œç”Ÿæ€å¯¹ Baichuan 2 çš„æ”¯æŒ ğŸ˜€ğŸ˜€ğŸ˜€**\n\n## Intel é…·ç¿ Ultra å¹³å°è¿è¡Œç™¾å·å¤§æ¨¡å‹\n\n### ä½¿ç”¨æ–¹å¼\n\nä½¿ç”¨é…·ç¿â„¢/è‡³å¼ºÂ® å¯æ‰©å±•å¤„ç†å™¨æˆ–é…åˆé”ç‚«â„¢ GPUç­‰è¿›è¡Œéƒ¨ç½²BaiChuan2 - 7B/Chatï¼ŒBaiChuan2 - 13B/Chatæ¨¡å‹\n\næ¨èä½¿ç”¨ BigDL-LLMï¼ˆ[CPU](https://github.com/intel-analytics/BigDL/tree/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan2), [GPU](https://github.com/intel-analytics/BigDL/tree/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/baichuan2)ï¼‰ä»¥å‘æŒ¥æ›´å¥½æ¨ç†æ€§èƒ½ã€‚\n\n### å¸®åŠ©æ–‡æ¡£\n\n[ä¸­æ–‡æ“ä½œæ‰‹å†Œ](https://github.com/intel-analytics/bigdl-llm-tutorial/tree/main/Chinese_Version)ï¼ŒåŒ…æ‹¬ç”¨notebookæ”¯æŒ\n\n[åŠ è½½ï¼Œä¼˜åŒ–ï¼Œä¿å­˜æ–¹æ³•ç­‰](https://github.com/intel-analytics/bigdl-llm-tutorial/blob/main/Chinese_Version/ch_3_AppDev_Basic/3_BasicApp.ipynb)\n\n## åä¸ºæ˜‡è…¾\n\n### Pytorch æ¡†æ¶\n\næ¨¡å‹å¾®è°ƒï¼šBaichuan 2 ï¼ˆ7Bï¼‰å·²åŸç”Ÿæ”¯æŒåŸºäºæ˜‡è…¾ NPU çš„ PyTorchï¼ˆ2.1.0ï¼‰+ Transformersï¼ˆ4.36.0ï¼‰+ DeepSpeedï¼ˆ0.12.4ï¼‰+ Accelerateï¼ˆ0.25.0ï¼‰æ¨¡å‹å¾®è°ƒï¼Œæ— éœ€é¢å¤–é€‚é…å³å¯ä½¿ç”¨ã€‚\n\næ¨ç†éƒ¨ç½²ï¼šBaichuan 2 ï¼ˆ7Bï¼‰å·²åŸç”Ÿæ”¯æŒæ˜‡è…¾ NPU æ¨ç†ï¼Œæ— éœ€é¢å¤–é€‚é…å³å¯ä½¿ç”¨ã€‚\n\n### MindSpore æ¡†æ¶\n\n[MindFormers]( https://gitee.com/mindspore/mindformers) æ˜¯ä¸€ä¸ªåŸºäºæ˜‡æ€æ¡†æ¶ï¼ˆMindSporeï¼‰å¹¶æ”¯æŒå¤§æ¨¡å‹è®­ç»ƒã€å¾®è°ƒã€è¯„ä¼°ã€æ¨ç†ã€éƒ¨ç½²çš„å…¨æµç¨‹å¼€å‘å¥—ä»¶ï¼Œ[Baichuan2-7B / 13B]( https://gitee.com/mindspore/mindformers/tree/dev/research/baichuan2) å·²é›†æˆäºæ­¤å¥—ä»¶ï¼Œæ”¯æŒç”¨æˆ·è¿›è¡Œæ¨¡å‹å¾®è°ƒã€éƒ¨ç½²ï¼Œå…·ä½“ä½¿ç”¨æ–¹å¼å¯è§ [README]( https://gitee.com/mindspore/mindformers/tree/dev/research/baichuan2/baichuan2.md)ã€‚\n\n### å¤§æ¨¡å‹ä½“éªŒå¹³å°\n\n[æ˜‡æ€å¤§æ¨¡å‹å¹³å°](https://xihe.mindspore.cn) åŸºäºæ˜‡æ€ MindSpore AI æ¡†æ¶ã€MindFormers å¤§æ¨¡å‹å¼€å‘å¥—ä»¶ä¸æ˜‡è…¾ç¡¬ä»¶ç®—åŠ›ï¼Œå°† [Baichuan2-7B](https://xihe.mindspore.cn/modelzoo/baichuan2_7b_chat) å¤§æ¨¡å‹èƒ½åŠ›å¼€æ”¾ç»™å…¬ä¼—ï¼Œæ¬¢è¿å¤§å®¶åœ¨çº¿ä½“éªŒã€‚\n\n## LLaMA-Efficient-Tuning\n[LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) å·²æ”¯æŒå¯¹ Baichuan 2 æ¨¡å‹çš„å¾®è°ƒå’Œç»§ç»­è®­ç»ƒã€‚\n\n## å¤ªåˆå…ƒç¢\nBaichuan2ï¼ˆ7B/13Bï¼‰æ”¯æŒå¤ªåˆ T100 åŠ é€Ÿå¡æ¨ç†ï¼Œç°[è¯•ç”¨é€šé“](http://www.tecorigin.com/cn/developer.html)å·²æ­£å¼å¯¹å¤–å¼€å¯ã€‚\n\n# å£°æ˜ã€åè®®ã€å¼•ç”¨\n\n## å£°æ˜\n\næˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œæˆ‘ä»¬çš„å¼€å‘å›¢é˜Ÿå¹¶æœªåŸºäº Baichuan 2 æ¨¡å‹å¼€å‘ä»»ä½•åº”ç”¨ï¼Œæ— è®ºæ˜¯åœ¨ iOSã€Androidã€ç½‘é¡µæˆ–ä»»ä½•å…¶ä»–å¹³å°ã€‚æˆ‘ä»¬å¼ºçƒˆå‘¼åæ‰€æœ‰ä½¿ç”¨è€…ï¼Œä¸è¦åˆ©ç”¨ Baichuan 2 æ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°† Baichuan 2 æ¨¡å‹ç”¨äºæœªç»é€‚å½“å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„ä½¿ç”¨è€…éƒ½èƒ½éµå®ˆè¿™ä¸ªåŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€çš„å‘å±•èƒ½åœ¨è§„èŒƒå’Œåˆæ³•çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚\n\næˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨ Baichuan 2 å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n\n## åè®®\nç¤¾åŒºä½¿ç”¨ Baichuan 2 æ¨¡å‹éœ€è¦éµå¾ª [Apache 2.0](https://github.com/baichuan-inc/Baichuan2/blob/main/LICENSE) å’Œ[ã€ŠBaichuan 2 æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/resolve/main/Baichuan%202%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)ã€‚Baichuan 2 æ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå¦‚æœæ‚¨è®¡åˆ’å°† Baichuan 2 æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨äºå•†ä¸šç›®çš„ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„ä¸»ä½“ç¬¦åˆä»¥ä¸‹æƒ…å†µï¼š\n  1. æ‚¨æˆ–æ‚¨çš„å…³è”æ–¹çš„æœåŠ¡æˆ–äº§å“çš„æ—¥å‡ç”¨æˆ·æ´»è·ƒé‡ï¼ˆDAUï¼‰ä½äº100ä¸‡ã€‚\n  2. æ‚¨æˆ–æ‚¨çš„å…³è”æ–¹ä¸æ˜¯è½¯ä»¶æœåŠ¡æä¾›å•†ã€äº‘æœåŠ¡æä¾›å•†ã€‚\n  3. æ‚¨æˆ–æ‚¨çš„å…³è”æ–¹ä¸å­˜åœ¨å°†æˆäºˆæ‚¨çš„å•†ç”¨è®¸å¯ï¼Œæœªç»ç™¾å·è®¸å¯äºŒæ¬¡æˆæƒç»™å…¶ä»–ç¬¬ä¸‰æ–¹çš„å¯èƒ½ã€‚\n\nåœ¨ç¬¦åˆä»¥ä¸Šæ¡ä»¶çš„å‰æä¸‹ï¼Œæ‚¨éœ€è¦é€šè¿‡ä»¥ä¸‹è”ç³»é‚®ç®± opensource@baichuan-inc.com ï¼Œæäº¤ã€ŠBaichuan 2 æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹è¦æ±‚çš„ç”³è¯·ææ–™ã€‚å®¡æ ¸é€šè¿‡åï¼Œç™¾å·å°†ç‰¹æ­¤æˆäºˆæ‚¨ä¸€ä¸ªéæ’ä»–æ€§ã€å…¨çƒæ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯å†è®¸å¯ã€å¯æ’¤é”€çš„å•†ç”¨ç‰ˆæƒè®¸å¯ã€‚\n\n## å¼•ç”¨\nå¦‚éœ€å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·ä½¿ç”¨å¦‚ä¸‹ reference:\n```\n@article{baichuan2023baichuan2,\n  title={Baichuan 2: Open Large-scale Language Models},\n  author={Baichuan},\n  journal={arXiv preprint arXiv:2309.10305},\n  url={https://arxiv.org/abs/2309.10305},\n  year={2023}\n}\n```\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 39.521484375,
          "content": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n\n<div align=\"center\">\n<h1>\n  Baichuan 2\n</h1>\n</div>\n\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/baichuan-inc/\" target=\"_blank\">Hugging Face</a> â€¢ ğŸ¤– <a href=\"https://modelscope.cn/organization/baichuan-inc\" target=\"_blank\">ModelScope</a> â€¢ ğŸ’¬ <a href=\"https://github.com/baichuan-inc/Baichuan-7B/blob/main/media/wechat.jpeg?raw=true\" target=\"_blank\">WeChat</a>â€¢ ğŸ§© <a href=\"https://modelers.cn/spaces/Baichuan/Baichuan2-7B-Chat\" target=\"_blank\">Modelers</a>\n</p>\n\n<div align=\"center\">\n\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/Baichuan2/blob/main/LICENSE)\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/baichuan-inc/Baichuan2/blob/main/README.md\">ä¸­æ–‡</a>\n    <p>\n</h4>\n</div>\n\n# Table of Contents\n\n- [ğŸ“– Models Introduction](#Models-Introduction)\n- [ğŸ“Š Benchmark Results ğŸ¥‡ğŸ¥‡ğŸ”¥ğŸ”¥](#Benchmark-Results)\n- [âš™ï¸ Inference and Deployment](#Inference-and-Deployment)\n- [ğŸ› ï¸ Fine-tuning the Model](#Fine-tuning-the-Model)\n- [ğŸ’¾ Intermediate Checkpoints ğŸ”¥ğŸ”¥](#Intermediate-Checkpoints)\n- [ğŸ‘¥ Community and Ecosystem](#Community-and-Ecosystem)\n- [ğŸ“œ Disclaimer, License and Citation](#Disclaimer-License-and-Citation)\n\n# Update\n[2023.12.29] ğŸ‰ğŸ‰ğŸ‰ We have released **[Baichuan2-13B-Chat v2](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/tree/v2.0)** version. In this version:\n- Significantly improved the model's overall capabilities, especially in mathematics and logical reasoning, and complex instruction following.\n\n# Models Introduction\n\n- Baichuan 2 is the new generation of open-source large language models launched by Baichuan Intelligent Technology. It was trained on a high-quality corpus with **2.6 trillion** tokens.\n- Baichuan 2 achieved the best performance of its size on multiple authoritative Chinese, English, and multi-language general and domain-specific benchmarks.\n- This release includes **Base** and **Chat** versions for **7B** and **13B**, and a **4bits quantized** version for the Chat model.\n- All versions are fully open to academic research. Developers only need to apply via email and obtain official commercial permission to use it for free commercially.\n- For more information, welcome reading our technical report [Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305).\n\nThe specific released versions and download links are shown in the table below:\n\n|         | Base Models  | Aligned Models | Aligned Models 4bits Quantized |\n|:-------:|:-----------:|:-------------:|:-----------------------------:|\n| 7B      | ğŸ¤— [Baichuan2-7B-Base](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base) | ğŸ¤— [Baichuan2-7B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat) | ğŸ¤— [Baichuan2-7B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits) |\n| 13B     | ğŸ¤— [Baichuan2-13B-Base](https://huggingface.co/baichuan-inc/Baichuan2-13B-Base) | ğŸ¤— [Baichuan2-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat) | ğŸ¤— [Baichuan2-13B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat-4bits) |\n\n# Benchmark Results\n\nWe conducted extensive testing on authoritative Chinese, English and multi-language datasets across six domains: [general](#General-Domain), [legal](#Law-and-Medicine), [medical](#Law-and-Medicine), [mathematics](#Mathematics-and-Code), [code](#Mathematics-and-Code), and [multi-language translation](#Multilingual-Translation).\n\n## General Domain\n\nIn the general domain, we conducted 5-shot tests on the following datasets:\n- [C-Eval](https://cevalbenchmark.com/index.html#home) is a comprehensive Chinese basic model evaluation dataset, covering 52 disciplines and four levels of difficulty. We used the dev set of this dataset as the source for few-shot learning and tested on the test set. Our evaluation approach followed that of [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B/tree/main).\n- [MMLU](https://arxiv.org/abs/2009.03300) is an English evaluation dataset comprising 57 tasks, encompassing elementary math, American history, computer science, law, etc. The difficulty ranges from high school level to expert level. It's a mainstream LLM evaluation dataset. We used its [open-source](https://github.com/hendrycks/test) evaluation approach.\n- [CMMLU](https://github.com/haonan-li/CMMLU) is a comprehensive Chinese evaluation benchmark covering 67 topics, specifically designed to assess language models' knowledge and reasoning capabilities in a Chinese context. We adopted its [official](https://github.com/haonan-li/CMMLU) evaluation approach.\n- [Gaokao](https://github.com/OpenLMLab/GAOKAO-Bench) is a dataset utilizing China's college entrance examination questions to evaluate large language models' abilities, focusing on linguistic proficiency and logical reasoning. We retained only its single-choice questions and conducted random partitioning. Our evaluation method is similar to that of C-Eval.\n- [AGIEval](https://github.com/microsoft/AGIEval) aims to evaluate a model's general abilities in cognition and problem-solving related tasks. We retained only its four-option single-choice questions and did random partitioning. We used an evaluation scheme similar to C-Eval.\n- [BBH](https://huggingface.co/datasets/lukaemon/bbh) is a challenging task subset of Big-Bench. Big-Bench currently includes 204 tasks. Task themes involve linguistics, child development, mathematics, common sense reasoning, biology, physics, societal biases, software development, etc. BBH consists of benchmark tasks extracted from the 204 Big-Bench tasks in which large models did not perform well.\n\n### 7B Model Results\n\n|                       | **C-Eval** | **MMLU** | **CMMLU** | **Gaokao** | **AGIEval** | **BBH** |\n|:---------------------:|:----------:|:--------:|:---------:|:----------:|:-----------:|:-------:|\n|                       |  5-shot    |  5-shot  |  5-shot   | 5-shot     | 5-shot      | 3-shot  |\n| **GPT-4**             | 68.40      | 83.93    | 70.33     | 66.15      | 63.27       | 75.12   |\n| **GPT-3.5 Turbo**     | 51.10      | 68.54    | 54.06     | 47.07      | 46.13       | 61.59   |\n| **LLaMA-7B**          | 27.10      | 35.10    | 26.75     | 27.81      | 28.17       | 32.38   |\n| **LLaMA2-7B**         | 28.90      | 45.73    | 31.38     | 25.97      | 26.53       | 39.16   |\n| **MPT-7B**            | 27.15      | 27.93    | 26.00     | 26.54      | 24.83       | 35.20   |\n| **Falcon-7B**         | 24.23      | 26.03    | 25.66     | 24.24      | 24.10       | 28.77   |\n| **ChatGLM2-6B**       | 50.20      | 45.90    | 49.00     | 49.44      | 45.28       | 31.65   |\n| **Baichuan-7B**       | 42.80      | 42.30    | 44.02     | 36.34      | 34.44       | 32.48   |\n| **Baichuan2-7B-Base** | 54.00      | 54.16    | 57.07     | 47.47      | 42.73       | 41.56   |\n\n### 13B Model Results\n\n|                             | **C-Eval** | **MMLU** | **CMMLU** | **Gaokao** | **AGIEval** | **BBH** |\n|:---------------------------:|:----------:|:--------:|:---------:|:----------:|:-----------:|:-------:|\n|                             |  5-shot    |  5-shot  |  5-shot   | 5-shot     | 5-shot      | 3-shot  |\n| **GPT-4**                   | 68.40      | 83.93    | 70.33     | 66.15      | 63.27       | 75.12   |\n| **GPT-3.5 Turbo**           | 51.10      | 68.54    | 54.06     | 47.07      | 46.13       | 61.59   |\n| **LLaMA-13B**               | 28.50      | 46.30    | 31.15     | 28.23      | 28.22       | 37.89   |\n| **LLaMA2-13B**              | 35.80      | 55.09    | 37.99     | 30.83      | 32.29       | 46.98   |\n| **Vicuna-13B**              | 32.80      | 52.00    | 36.28     | 30.11      | 31.55       | 43.04   |\n| **Chinese-Alpaca-Plus-13B** | 38.80      | 43.90    | 33.43     | 34.78      | 35.46       | 28.94   |\n| **XVERSE-13B**              | 53.70      | 55.21    | 58.44     | 44.69      | 42.54       | 38.06   |\n| **Baichuan-13B-Base**       | 52.40      | 51.60    | 55.30     | 49.69      | 43.20       | 43.01   |\n| **Baichuan2-13B-Base**      | 58.10      | 59.17    | 61.97     | 54.33      | 48.17       | 48.78   |\n\n\n## Law and Medicine\n\nIn the legal domain, we used the [JEC-QA](https://jecqa.thunlp.org/) dataset. The JEC-QA dataset originates from China's National Judicial Examination. We retained only the multiple-choice questions from it. Our evaluation method was similar to that of C-Eval.\n\nIn the medical domain, we used medical-related subjects from general domain datasets (C-Eval, MMLU, CMMLU), as well as [MedQA](https://arxiv.org/abs/2009.13081) and [MedMCQA](https://medmcqa.github.io/). We followed an evaluation scheme similar to C-Eval.\n- For testing convenience, we used the val set from C-Eval for testing.\n- The MedQA dataset comes from medical exams in the US and China. We tested the USMLE and MCMLE subsets from the [MedQA dataset](https://huggingface.co/datasets/bigbio/med_qa), and used a version with five candidates.\n- The MedMCQA dataset originates from entrance exams of medical colleges in India. We retained only the multiple-choice questions. Since the test set doesn't have answers, we used the dev set for testing.\n- Medical-related subjects included in the general domain datasets are as follows:\n    - C-Eval: clinical_medicine, basic_medicine\n    - MMLU: clinical_knowledge, anatomy, college_medicine, college_biology, nutrition, virology, medical_genetics, professional_medicine\n    - CMMLU: anatomy, clinical_knowledge, college_medicine, genetics, nutrition, traditional_chinese_medicine, virology \n\nWe conducted 5-shot tests on the above datasets.\n\n### 7B Model Results\n\n|                       | **JEC-QA** | **CEval-MMLU-CMMLU** | **MedQA-USMLE** | **MedQA-MCMLE** | **MedMCQA** |\n|:---------------------:|:----------:|:--------------------:|:---------------:|:---------------:|:-----------:|\n|                       | 5-shot     | 5-shot               | 5-shot          | 5-shot          | 5-shot      |\n| **GPT-4**             | 59.32      | 77.16                | 80.28           | 74.58           | 72.51       |\n| **GPT-3.5 Turbo**     | 42.31      | 61.17                | 53.81           | 52.92           | 56.25       |\n| **LLaMA-7B**          | 27.45      | 33.34                | 24.12           | 21.72           | 27.45       |\n| **LLaMA2-7B**         | 29.20      | 36.75                | 27.49           | 24.78           | 37.93       |\n| **MPT-7B**            | 27.45      | 26.67                | 16.97           | 19.79           | 31.96       |\n| **Falcon-7B**         | 23.66      | 25.33                | 21.29           | 18.07           | 33.88       |\n| **ChatGLM2-6B**       | 40.76      | 44.54                | 26.24           | 45.53           | 30.22       |\n| **Baichuan-7B**       | 34.64      | 42.37                | 27.42           | 39.46           | 31.39       |\n| **Baichuan2-7B-Base** | 44.46      | 56.39                | 32.68           | 54.93           | 41.73       |\n\n### 13B Model Results\n\n|                             | **JEC-QA** | **CEval-MMLU-CMMLU** | **MedQA-USMLE** | **MedQA-MCMLE** | **MedMCQA** |\n|:---------------------------:|:----------:|:--------------------:|:---------------:|:---------------:|:-----------:|\n|                             | 5-shot     | 5-shot               | 5-shot          | 5-shot          | 5-shot      |\n| **GPT-4**                   | 59.32      | 77.16                | 80.28           | 74.58           | 72.51       |\n| **GPT-3.5 Turbo**           | 42.31      | 61.17                | 53.81           | 52.92           | 56.25       |\n| **LLaMA-13B**               | 27.54      | 35.14                | 28.83           | 23.38           | 39.52       |\n| **LLaMA2-13B**              | 34.08      | 47.42                | 35.04           | 29.74           | 42.12       |\n| **Vicuna-13B**              | 28.38      | 40.99                | 34.80           | 27.67           | 40.66       |\n| **Chinese-Alpaca-Plus-13B** | 35.32      | 46.31                | 27.49           | 32.66           | 35.87       |\n| **XVERSE-13B**              | 46.42      | 58.08                | 32.99           | 58.76           | 41.34       |\n| **Baichuan-13B-Base**       | 41.34      | 51.77                | 29.07           | 43.67           | 39.60       |\n| **Baichuan2-13B-Base**      | 47.40      | 59.33                | 40.38           | 61.62           | 42.86       |\n\n## Mathematics and Code\n\nIn the mathematics domain, we used the [OpenCompass](https://opencompass.org.cn/) evaluation framework and conducted 4-shot tests on the [GSM8K](https://huggingface.co/datasets/gsm8k) and [MATH](https://huggingface.co/datasets/competition_math) datasets.\n\n- GSM8K is a dataset released by OpenAI, consisting of 8.5K high-quality linguistically diverse elementary school math application questions. It requires selecting the most reasonable solution based on a given scenario and two possible solutions.\n- The MATH dataset contains 12,500 math problems (of which 7,500 belong to the training set and 5,000 to the test set). These problems are collected from math competitions like AMC 10, AMC 12, AIME.\n\nFor the code domain, we used the [HumanEval](https://huggingface.co/datasets/openai_humaneval) and [MBPP](https://huggingface.co/datasets/mbpp) datasets. Using OpenCompass, we performed a 0-shot test on HumanEval and a 3-shot test on the MBPP dataset.\n- Tasks in HumanEval include programming tasks encompassing language understanding, reasoning, algorithms, and basic math to evaluate the functional correctness of models and measure their problem-solving capability.\n- MBPP consists of a dataset with 974 Python short functions, textual descriptions of programs, and test cases to check their functional correctness.\n\n### 7B Model Results\n\n|                       | **GSM8K** | **MATH** | **HumanEval** | **MBPP** |\n|:---------------------:|:---------:|:--------:|:-------------:|:--------:|\n|                       |  4-shot   | 4-shot   |  0-shot       |  3-shot  |\n| **GPT-4**             |   89.99   | 40.20    | 69.51         |  63.60   |\n| **GPT-3.5 Turbo**     |   57.77   | 13.96    | 52.44         |  61.40   |\n| **LLaMA-7B**          |   9.78    | 3.02     | 11.59         |  14.00   |\n| **LLaMA2-7B**         |   16.22   | 3.24     | 12.80         |  14.80   |\n| **MPT-7B**            |   8.64    | 2.90     | 14.02         |  23.40   |\n| **Falcon-7B**         |   5.46    | 1.68     | -             |  10.20   |\n| **ChatGLM2-6B**       |   28.89   | 6.40     | 9.15          |   9.00   |\n| **Baichuan-7B**       |   9.17    | 2.54     | 9.20          |   6.60   |\n| **Baichuan2-7B-Base** |   24.49   | 5.58     | 18.29         |  24.20   |\n\n### 13B Model Results\n\n|                             | **GSM8K** | **MATH** | **HumanEval** | **MBPP** |\n|:---------------------------:|:---------:|:--------:|:-------------:|:--------:|\n|                             |  4-shot   | 4-shot   |  0-shot       |  3-shot  |\n| **GPT-4**                   |   89.99   | 40.20    | 69.51         |  63.60   |\n| **GPT-3.5 Turbo**           |   57.77   | 13.96    | 52.44         |  61.40   |\n| **LLaMA-13B**               |   20.55   | 3.68     | 15.24         |  21.40   |\n| **LLaMA2-13B**              |   28.89   | 4.96     | 15.24         |  27.00   |\n| **Vicuna-13B**              |   28.13   | 4.36     | 16.46         |  15.00   |\n| **Chinese-Alpaca-Plus-13B** |   11.98   | 2.50     | 16.46         |  20.00   |\n| **XVERSE-13B**              |   18.20   | 2.18     | 15.85         |  16.80   |\n| **Baichuan-13B-Base**       |   26.76   | 4.84     | 11.59         |  22.80   |\n| **Baichuan2-13B-Base**      |   52.77   | 10.08    | 17.07         |  30.20   |\n\n## Multilingual Translation\n\nWe used the [Flores-101](https://huggingface.co/datasets/facebook/flores) dataset to evaluate the multilingual capability of the models. Flores-101 covers 101 languages from around the world. Its data comes from various domains including news, travel guides, and books. We chose the official languages of the United Nations (Arabic, Chinese, English, French, Russian, and Spanish) as well as German and Japanese for testing. Using OpenCompass, we performed 8-shot tests on seven sub-tasks within Flores-101: Chinese-English, Chinese-French, Chinese-Spanish, Chinese-Arabic, Chinese-Russian, Chinese-Japanese, and Chinese-German.\n\n### 7B Model Results\n\n|             | **CN-EN** | **CN-FR** | **CN-ES** | **CN-AR** | **CN-RU** | **CN-JP** | **CN-DE** | Average |\n|:---------------------:|:-------:|:-------:|:---------:|:---------:|:-------:|:-------:|:-------:|:-------:|\n| **GPT-4**             | 29.94   | 29.56   | 20.01     | 10.76     | 18.62   | 13.26   | 20.83   | 20.43   |\n| **GPT-3.5 Turbo**     | 27.67   | 26.15   | 19.58     | 10.73     | 17.45   | 1.82    | 19.70   | 17.59   |\n| **LLaMA-7B**          | 17.27   | 12.02   | 9.54      | 0.00      | 4.47    | 1.41    | 8.73    | 7.63    |\n| **LLaMA2-7B**         | 25.76   | 15.14   | 11.92     | 0.79      | 4.99    | 2.20    | 10.15   | 10.14   |\n| **MPT-7B**            | 20.77   | 9.53    | 8.96      | 0.10      | 3.54    | 2.91    | 6.54    | 7.48    |\n| **Falcon-7B**         | 22.13   | 15.67   | 9.28      | 0.11      | 1.35    | 0.41    | 6.41    | 7.91    |\n| **ChatGLM2-6B**       | 22.28   | 9.42    | 7.77      | 0.64      | 1.78    | 0.26    | 4.61    | 6.68    |\n| **Baichuan-7B**       | 25.07   | 16.51   | 12.72     | 0.41      | 6.66    | 2.24    | 9.86    | 10.50   |\n| **Baichuan2-7B-Base** | 27.27   | 20.87   | 16.17     | 1.39      | 11.21   | 3.11    | 12.76   | 13.25   |\n\n### 13B Model Results\n\n|                   | **CN-EN** | **CN-FR** | **CN-ES** | **CN-AR** | **CN-RU** | **CN-JP** | **CN-DE** | Average |\n|:---------------------------:|:-------:|:-------:|:---------:|:---------:|:-------:|:-------:|:-------:|:-------:|\n|          **GPT-4**          | 29.94   | 29.56   | 20.01     | 10.76     | 18.62   | 13.26   | 20.83   | 20.43   |\n|      **GPT-3.5 Turbo**      | 27.67   | 26.15   | 19.58     | 10.73     | 17.45   | 1.82    | 19.70   | 17.59   |\n|        **LLaMA-13B**        | 21.75   | 16.16   | 13.29     | 0.58      | 7.61    | 0.41    | 10.66   | 10.07   |\n|       **LLaMA2-13B**        | 25.44   | 19.25   | 17.49     | 1.38      | 10.34   | 0.13    | 11.13   | 12.17   |\n|       **Vicuna-13B**        | 22.63   | 18.04   | 14.67     | 0.70      | 9.27    | 3.59    | 10.25   | 11.31   |\n| **Chinese-Alpaca-Plus-13B** | 22.53   | 13.82   | 11.29     | 0.28      | 1.52    | 0.31    | 8.13    | 8.27    |\n|       **XVERSE-13B**        | 29.26   | 24.03   | 16.67     | 2.78      | 11.61   | 3.08    | 14.26   | 14.53   |\n|    **Baichuan-13B-Base**    | 30.24   | 20.90   | 15.92     | 0.98      | 9.65    | 2.64    | 12.00   | 13.19   |\n|    **Baichuan2-13B-Base**   | 30.61   | 22.11   | 17.27     | 2.39      | 14.17   | 11.58   | 14.53   | 16.09   |\n\n# Inference and Deployment\n\nThe model weights, source code, and configuration needed for inference have been released on Hugging Face. Download links can be found in the table at the beginning of this document. Below, we demonstrate various inference methods using Baichuan2-13B-Chat as an example. The program will automatically download the required resources from Hugging Face.\n\n## Dependency Installation\n\n```shell\npip install -r requirements.txt\n```\n\n## Python Code Inference\n\n### Demonstration of Chat Model Inference\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> from transformers.generation.utils import GenerationConfig\n>>> tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\", use_fast=False, trust_remote_code=True)\n>>> model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n>>> model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-13B-Chat\")\n>>> messages = []\n>>> messages.append({\"role\": \"user\", \"content\": \"è§£é‡Šä¸€ä¸‹â€œæ¸©æ•…è€ŒçŸ¥æ–°â€\"})\n>>> response = model.chat(tokenizer, messages)\n>>> print(response)\n\"æ¸©æ•…è€ŒçŸ¥æ–°\"æ˜¯ä¸€å¥ä¸­å›½å¤ä»£çš„æˆè¯­ï¼Œå‡ºè‡ªã€Šè®ºè¯­Â·ä¸ºæ”¿ã€‹ç¯‡ã€‚è¿™å¥è¯çš„æ„æ€æ˜¯ï¼šé€šè¿‡å›é¡¾è¿‡å»ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°æ–°çš„çŸ¥è¯†å’Œç†è§£ã€‚æ¢å¥è¯è¯´ï¼Œå­¦ä¹ å†å²å’Œç»éªŒå¯ä»¥è®©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ç°åœ¨å’Œæœªæ¥ã€‚\n\nè¿™å¥è¯é¼“åŠ±æˆ‘ä»¬åœ¨å­¦ä¹ å’Œç”Ÿæ´»ä¸­ä¸æ–­åœ°å›é¡¾å’Œåæ€è¿‡å»çš„ç»éªŒï¼Œä»è€Œè·å¾—æ–°çš„å¯ç¤ºå’Œæˆé•¿ã€‚é€šè¿‡é‡æ¸©æ—§çš„çŸ¥è¯†å’Œç»å†ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°æ–°çš„è§‚ç‚¹å’Œç†è§£ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹ä¸æ–­å˜åŒ–çš„ä¸–ç•Œå’ŒæŒ‘æˆ˜ã€‚\n```\n\n### Demonstration of Base Model Inference\n\n```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-13B-Base\", trust_remote_code=True)\n>>> model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-13B-Base\", device_map=\"auto\", trust_remote_code=True)\n>>> inputs = tokenizer('ç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\\nå¤œé›¨å¯„åŒ—->', return_tensors='pt')\n>>> inputs = inputs.to('cuda:0')\n>>> pred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)\n>>> print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\nç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\nå¤œé›¨å¯„åŒ—->æå•†éš\n```\n\n> In the above code snippets, the model loading specifies `device_map='auto'`, which will use all available GPUs. If you need to specify the device(s) to use, you can control it in a way similar to `export CUDA_VISIBLE_DEVICES=0,1` (using the 0 and 1 graphics cards).\n\n## Command Line Tool Inference\n\n```shell\npython cli_demo.py\n```\nWe do not support using this tool for Base Model because it is designed for Chat scenario.\n\n## Web Demo Inference\n\nRun the following command using streamlit. It will start a web service locally. You can access it by entering the address provided in the console into your browser. We do not support using this tool for Base Model because it is designed for Chat scenario.\n\n```shell\nstreamlit run web_demo.py\n```\n\n## Demo Results of Baichuan2-13B-Chat\n\n<details><summary><b>Coding</b></summary>\n\n`````\nç”¨æˆ·ï¼šæ‰¾åˆ°ä¸‹åˆ—æ•°ç»„çš„ä¸­ä½æ•°[3.1,6.2,1.3,8.4,10.5,11.6,2.1]ï¼Œè¯·ç”¨pythonä»£ç å®Œæˆä»¥ä¸ŠåŠŸèƒ½\n\nBaichuan 2ï¼šdef find_median(numbers):\n    numbers.sort()\n    length = len(numbers)\n    if length % 2 == 0:\n        return (numbers[length // 2 - 1] + numbers[length // 2]) / 2\n    else:\n        return numbers[length // 2]\nnumbers = [3.1, 6.2, 1.3, 8.4, 10.5, 11.6, 2.1]\nprint(find_median(numbers))\n`````\n\n</details>\n\n<details><summary><b>Math</b></summary>\n\n`````\nç”¨æˆ·ï¼šé¸¡å’Œå…”åœ¨ä¸€ä¸ªç¬¼å­é‡Œï¼Œå…±æœ‰26ä¸ªå¤´ï¼Œ68åªè„šï¼Œé‚£ä¹ˆé¸¡æœ‰å¤šå°‘åªï¼Œå…”æœ‰å¤šå°‘åªï¼Ÿ\n\nBaichuan 2ï¼šæˆ‘ä»¬å¯ä»¥ç”¨ä»£æ•°æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å‡è®¾é¸¡çš„æ•°é‡ä¸ºxï¼Œå…”å­çš„æ•°é‡ä¸ºyã€‚æ ¹æ®é¢˜ç›®ç»™å‡ºçš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä»¥ä¸‹ä¸¤ä¸ªæ–¹ç¨‹ï¼š\n\n1. x + y = 26 (å¤´çš„æ€»æ•°)\n2. 2x + 4y = 68 (è„šçš„æ€»æ•°)\n\nç°åœ¨æˆ‘ä»¬éœ€è¦è§£è¿™ä¸ªäºŒå…ƒä¸€æ¬¡æ–¹ç¨‹ç»„ã€‚é¦–å…ˆä»ç¬¬ä¸€ä¸ªæ–¹ç¨‹ä¸­è§£å‡ºxï¼š\n\nx = 26 - y\n\nç„¶åå°†xçš„è¡¨è¾¾å¼ä»£å…¥ç¬¬äºŒä¸ªæ–¹ç¨‹ï¼š\n\n2(26 - y) + 4y = 68\n52 - 2y + 4y = 68\n2y = 16\n\næ¥ä¸‹æ¥æˆ‘ä»¬æ±‚è§£yçš„å€¼ï¼š\n\ny = 16 / 2\ny = 8\n\nç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†å…”å­çš„æ•°é‡æ˜¯8åªã€‚å°†yçš„å€¼ä»£å…¥xçš„è¡¨è¾¾å¼æ±‚å¾—é¸¡çš„æ•°é‡ï¼š\n\nx = 26 - 8\nx = 18\n\næ‰€ä»¥é¸¡æœ‰18åªï¼Œå…”å­æœ‰8åªã€‚\n`````\n\n</details>\n\n<details><summary><b>Instruction Following</b></summary>\n\n`````\nç”¨æˆ·ï¼šä»ç°åœ¨å¼€å§‹ï¼Œè¯·ä½ å°†æˆ‘æ¥ä¸‹æ¥è¯´çš„æ¯ä¸€å¥è¯ç¿»è¯‘æˆè‹±è¯­ï¼Œå‡†å¤‡å¥½äº†å—ï¼Ÿ\n\nBaichuan 2ï¼šI'm ready to translate every word you say next into English.\n\nç”¨æˆ·ï¼šä½ å¥½\n\nBaichuan 2ï¼šHello\n\nç”¨æˆ·ï¼šæˆ‘æ˜¯Baichuan 2ï¼Œå¾ˆé«˜å…´è®¤è¯†å¤§å®¶\n\nBaichuan 2ï¼šI am Baichuan 2, and I am very happy to meet everyone.\n\nç”¨æˆ·ï¼šå¸Œæœ›é€šè¿‡å¤§å®¶çš„å…±åŒåŠªåŠ›ï¼Œæ—©æ—¥è¿æ¥AGIæ—¶ä»£çš„åˆ°æ¥\n\nBaichuan 2ï¼šI hope through the joint efforts of all of us, we can welcome the arrival of the AGI era as soon as possible.\n`````\n</details>\n\n<details><summary><b>Solving Quizzes</b></summary>\n\n`````\nç”¨æˆ·ï¼šä»¥ä¸‹ç‰©ç†å¸¸è¯†é¢˜ç›®ï¼Œå“ªä¸€ä¸ªæ˜¯é”™è¯¯çš„?A.åœ¨è‡ªç„¶ç¯å¢ƒä¸‹ï¼Œå£°éŸ³åœ¨å›ºä½“ä¸­ä¼ æ’­é€Ÿåº¦æœ€å¿«ã€‚B.ç‰›é¡¿ç¬¬ä¸€å®šå¾‹:ä¸€ä¸ªç‰©ä½“å¦‚æœä¸å—åŠ›ä½œç”¨ï¼Œå°†ä¿æŒé™æ­¢æˆ–åŒ€é€Ÿç›´çº¿è¿åŠ¨çš„çŠ¶æ€ã€‚C.ç‰›é¡¿ç¬¬ä¸‰å®šå¾‹:å¯¹äºæ¯ä¸ªä½œç”¨åŠ›ï¼Œéƒ½æœ‰ä¸€ä¸ªç›¸ç­‰è€Œåå‘çš„åä½œç”¨åŠ›ã€‚D.å£°éŸ³åœ¨ç©ºæ°”ä¸­çš„ä¼ æ’­é€Ÿåº¦ä¸º1000m/sã€‚\n\nBaichuan 2ï¼šæœ¬é¢˜è€ƒæŸ¥ç‰©ç†åŸºæœ¬æ¦‚å¿µå’ŒåŸç†çš„ç†è§£å’Œåº”ç”¨èƒ½åŠ›ã€‚\nAé¡¹ï¼šå£°éŸ³çš„ä¼ æ’­éœ€è¦ä»‹è´¨ï¼Œåœ¨å›ºä½“ã€æ¶²ä½“å’Œæ°”ä½“çš„ä¸‰ç§ä»‹è´¨ä¸­ï¼Œå£°éŸ³åœ¨å›ºä½“ä¸­çš„ä¼ æ’­é€Ÿåº¦æ˜¯æœ€å¿«çš„ï¼Œæ•…Aæ­£ç¡®ï¼› Bé¡¹ï¼šç‰›é¡¿ç¬¬ä¸€å®šå¾‹æ˜¯æƒ¯æ€§å®šå¾‹ï¼Œå…¶å†…å®¹æ˜¯ï¼šä¸€ä¸ªç‰©ä½“å¦‚æœæ²¡æœ‰å—åˆ°å¤–åŠ›çš„ä½œç”¨ï¼Œè¦ä¹ˆä¿æŒé™æ­¢çŠ¶æ€ï¼Œè¦ä¹ˆä¿æŒåŒ€é€Ÿç›´çº¿è¿åŠ¨çŠ¶æ€ï¼Œç›´åˆ°æœ‰å¤–åŠ›è¿«ä½¿å®ƒæ”¹å˜è¿åŠ¨çŠ¶æ€ä¸ºæ­¢ï¼Œæ•…Bæ­£ç¡®ï¼› Cé¡¹ï¼šç‰›é¡¿ç¬¬ä¸‰å®šå¾‹çš„å†…å®¹æ˜¯ï¼šå¯¹äºæ¯ä¸€ä¸ªä½œç”¨åŠ›ï¼Œéƒ½æœ‰ä¸€ä¸ªå¤§å°ç›¸ç­‰ã€æ–¹å‘ç›¸åçš„åä½œç”¨åŠ›ï¼Œè¿™ä¸¤ä¸ªåŠ›åˆ†åˆ«ä½œç”¨åœ¨ä¸¤ä¸ªç›¸äº’ä½œç”¨çš„ç‰©ä½“ä¸Šï¼Œæ•…Cé”™è¯¯ï¼› Dé¡¹ï¼šå£°éŸ³åœ¨ç©ºæ°”ä¸­ä»¥æ³¢çš„å½¢å¼ä¼ æ’­ï¼Œå…¶ä¼ æ’­é€Ÿåº¦ä¸ä»‹è´¨çš„æ¸©åº¦ã€æ¹¿åº¦ç­‰å› ç´ æœ‰å…³ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå£°éŸ³åœ¨ç©ºæ°”ä¸­çš„ä¼ æ’­é€Ÿåº¦çº¦ä¸º340m/så·¦å³ï¼Œè€Œä¸æ˜¯1000m/sï¼Œæ•…Dé”™è¯¯ã€‚ å› æ­¤ï¼Œæœ¬é¢˜çš„æ­£ç¡®ç­”æ¡ˆæ˜¯ D ã€‚\n`````\n`````\nç”¨æˆ·ï¼šI'm Mike ã€‚I am going to have a busy weekendã€‚On Saturdayï¼ŒI am going to learn how to swimã€‚I will go with my fatherã€‚Then we are going to have lunch in the restaurantã€‚In the afternoonï¼Œwe are going to the zooã€‚We are going to see the pandasã€‚On Sundayï¼ŒI will finish my homework in the morningã€‚In the afternoonï¼Œmy parents and I are going to visit my grandparentsã€‚We will have dinner togetherã€‚\né˜…è¯»çŸ­æ–‡ï¼Œåˆ¤æ–­æ­£ç¡®å’Œé”™è¯¯ï¼æ­£ç¡®è¯·å†™Tï¼Œé”™è¯¯è¯·å†™F:\n1.Mike is going to have a busy weekend()\n2.Mike is going to learn how to swim with his father()\n3.They are going to have lunch at home()\n\nBaichuan 2ï¼š1. Tï¼ˆæ­£ç¡®çš„ï¼‰\n2. Tï¼ˆæ­£ç¡®çš„ï¼‰\n3. Fï¼ˆé”™è¯¯çš„ï¼‰\n`````\n\n</details>\n\n## Quantization Deployment\n\nTo allow different users and platforms to run the Baichuan 2 model, we have carried out quantization for the Baichuan 2 model (including Baichuan2-7B-Chat and Baichuan2-13B-Chat). This facilitates users to quickly and efficiently deploy the Baichuan 2 model on their respective platforms.\n\n### Quantization Method\n\nBaichuan 2 utilizes the widely-adopted community quantization method: the [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) method. This method ensures that the performance remains largely unchanged after quantization. It has now been integrated into the transformers library and is widely used in the community. BitsAndBytes supports both 4bits and 8bits quantization. Within the 4bits option, it provides FP4 and NF4 formats, with Baichuan 2 selecting NF4 as its 4bits quantization data type.\n\nBased on this quantization method, Baichuan 2 supports both online and offline quantization modes.\n\n### Online Quantization\n\nFor online quantization, we support both 8bits and 4bits. The usage is similar to the method described in the [Baichuan-13B](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) project. One simply needs to first load the model into the CPU memory, then invoke the `quantize()` method, and finally call the `cuda()` function to copy the quantized weights to the GPU memory. The code for loading the entire model is straightforward. Let's take Baichuan2-7B-Chat as an example:\n\n8bits online quantization:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", torch_dtype=torch.float16, trust_remote_code=True)\nmodel = model.quantize(8).cuda() \n```\n4bits online quantization:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", torch_dtype=torch.float16, trust_remote_code=True)\nmodel = model.quantize(4).cuda() \n```\nIt's worth noting that when using the `from_pretrained` interface, users typically add `device_map=\"auto\"`. However, when using online quantization, this parameter should be removed; otherwise, an error will occur.\n\n### Offline Quantization\n\nTo facilitate user adoption, we offer a pre-quantized 4bits version: [Baichuan2-7B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits/tree/main) for download. \nLoading the Baichuan2-7B-Chat-4bits model is straightforward, just execute:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat-4bits\", device_map=\"auto\", trust_remote_code=True)\n```\nFor 8bits offline quantization, we haven't provided a corresponding version since the Hugging Face transformers library offers the necessary API interfaces. This makes the saving and loading of 8bits quantized models very convenient. Users can implement the saving and loading of 8bits models in the following manner:\n```python\n# Model saving: model_id is the original model directory, and quant8_saved_dir is the directory where the 8bits quantized model is saved.\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)\nmodel.save_pretrained(quant8_saved_dir)\nmodel = AutoModelForCausalLM.from_pretrained(quant8_saved_dir, device_map=\"auto\", trust_remote_code=True)\n```\n\n### Quantization Effect\n\nComparison of memory usage before and after quantization (GPU Mem in GB):\n| Precision   | Baichuan2-7B |Baichuan2-13B |\n|-------------|:------------:|:------------:|\n| bf16 / fp16 | 14.0         | 25.9         |\n| 8bits       | 8.0          | 14.2         |\n| 4bits       | 5.1          | 8.6          |\n\nThe results on various benchmarks after quantization compared to the original version are as follows:\n\n| Model 5-shot           | C-Eval | MMLU | CMMLU |\n|------------------------|:------:|:----:|:-----:|\n| Baichuan2-13B-Chat      | 56.74  | 57.32| 59.68  |\n| Baichuan2-13B-Chat-4bits | 56.05   | 56.24 | 58.82  |\n| Baichuan2-7B-Chat       | 54.35   | 52.93 | 54.99  |\n| Baichuan2-7B-Chat-4bits | 53.04   | 51.72 | 52.84  |\n> C-Eval is tested on val set\n\nIt can be seen that the 4bits, compared to bfloat16, has a drop of around 1 ~ 2 percentage points.\n\n## CPU Deployment\n\nBaichuan-13B supports CPU inference, but it should be emphasized that the inference speed on CPU will be very slow. Modify the model loading logic as follows:\n```python\n# Taking BVaichuan2-7B-Chat as an example\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", torch_dtype=torch.float32, trust_remote_code=True)\n```\n\n## Migrating Inference Optimizations from Baichuan 1 to Baichuan 2\n\nGiven that many users have made various optimizations on Baichuan 1 (Baichuan-7B, Baichuan-13B), such as compilation optimizations, quantization, etc., to seamlessly apply these enhancements to Baichuan 2, users can perform an offline conversion on the Baichuan 2 model. After this conversion, it can be treated as a Baichuan 1 model. Specifically, users only need to use the script below to offline normalize the last `lm_head` layer of the Baichuan 2 model and replace the \"lm_head.weight\". Once replaced, optimizations such as compilation can be applied to the converted model just like with the Baichuan 1 model.\n```python\nimport torch\nimport os\nori_model_dir = 'your Baichuan 2 model directory'\n# To avoid overwriting the original model, it's best to save the converted model to another directory before replacing it\nnew_model_dir = 'your normalized lm_head weight Baichuan 2 model directory'\nmodel = torch.load(os.path.join(ori_model_dir, 'pytorch_model.bin'))\nlm_head_w = model['lm_head.weight']\nlm_head_w = torch.nn.functional.normalize(lm_head_w)\nmodel['lm_head.weight'] = lm_head_w\ntorch.save(model, os.path.join(new_model_dir, 'pytorch_model.bin'))\n```\n\n# Fine-tuning the Model\n\n## Dependency Installation\n\n```shell\ngit clone https://github.com/baichuan-inc/Baichuan2.git\ncd Baichuan2/fine-tune\npip install -r requirements.txt\n```\n- To use lightweight fine-tuning methods like LoRA, you must additionally install [peft](https://github.com/huggingface/peft).\n- To accelerate training with xFormers, you must additionally install [xFormers](https://github.com/facebookresearch/xformers).\n\n## Single Machine Training\n\nBelow, we provide an example of fine-tuning the Baichuan2-7B-Base on a single machine.\n\nTraining Data: `data/belle_chat_ramdon_10k.json`. This sample data was drawn from [multiturn_chat_0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M), consisting of a selection of 10,000 entries, and has been reformatted. The main purpose is to demonstrate how to train with multi-turn data, and effectiveness is not guaranteed.\n\n```shell\nhostfile=\"\"\ndeepspeed --hostfile=$hostfile fine-tune.py  \\\n    --report_to \"none\" \\\n    --data_path \"data/belle_chat_ramdon_10k.json\" \\\n    --model_name_or_path \"baichuan-inc/Baichuan2-7B-Base\" \\\n    --output_dir \"output\" \\\n    --model_max_length 512 \\\n    --num_train_epochs 4 \\\n    --per_device_train_batch_size 16 \\\n    --gradient_accumulation_steps 1 \\\n    --save_strategy epoch \\\n    --learning_rate 2e-5 \\\n    --lr_scheduler_type constant \\\n    --adam_beta1 0.9 \\\n    --adam_beta2 0.98 \\\n    --adam_epsilon 1e-8 \\\n    --max_grad_norm 1.0 \\\n    --weight_decay 1e-4 \\\n    --warmup_ratio 0.0 \\\n    --logging_steps 1 \\\n    --gradient_checkpointing True \\\n    --deepspeed ds_config.json \\\n    --bf16 True \\\n    --tf32 True\n```\n\n## Multi-machine Training\n\nFor multi-machine training, you only need to provide the hostfile, the content of which is similar to follows:\n```\nip1 slots=8\nip2 slots=8\nip3 slots=8\nip4 slots=8\n....\n```\n\nAt the same time, specify the path of the hostfile in the training script:\n```shell\nhostfile=\"/path/to/hostfile\"\ndeepspeed --hostfile=$hostfile fine-tune.py  \\\n    --report_to \"none\" \\\n    --data_path \"data/belle_chat_ramdon_10k.json\" \\\n    --model_name_or_path \"baichuan-inc/Baichuan2-7B-Base\" \\\n    --output_dir \"output\" \\\n    --model_max_length 512 \\\n    --num_train_epochs 4 \\\n    --per_device_train_batch_size 16 \\\n    --gradient_accumulation_steps 1 \\\n    --save_strategy epoch \\\n    --learning_rate 2e-5 \\\n    --lr_scheduler_type constant \\\n    --adam_beta1 0.9 \\\n    --adam_beta2 0.98 \\\n    --adam_epsilon 1e-8 \\\n    --max_grad_norm 1.0 \\\n    --weight_decay 1e-4 \\\n    --warmup_ratio 0.0 \\\n    --logging_steps 1 \\\n    --gradient_checkpointing True \\\n    --deepspeed ds_config.json \\\n    --bf16 True \\\n    --tf32 True\n```\n\n## Lightweight Fine-tuning\n\nThe code already supports lightweight fine-tuning such as LoRA. If you need to use it, simply add the following parameters to the script mentioned above.\n```shell\n--use_lora True\n```\nSpecific configurations for LoRA can be found in the fine-tune.py script.\n\nAfter fine-tuning with LoRA, you can load the model using the command below:\n```python\nfrom peft import AutoPeftModelForCausalLM\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"output\", trust_remote_code=True)\n```\n\n# Intermediate Checkpoints\n\nIn addition to the Baichuan2-7B-Base model with 2.6 trillion tokens, we also provide 11 intermediate checkpoints (ranging approximately from 0.2 to 2.4 trillion tokens) from before this for community research ([Download link](https://huggingface.co/baichuan-inc/Baichuan2-7B-Intermediate-Checkpoints)). The chart below shows the performance changes of these checkpoints on the C-Eval, MMLU, and CMMLU benchmarks:\n\n<div align=\"center\">\n<img src=\"https://github.com/baichuan-inc/Baichuan2/blob/main/media/checkpoints.jpeg?raw=true\" width=50% />\n</div>\n\n# Community and Ecosystem\n\n**ğŸ“¢ğŸ“¢ğŸ“¢ We will continuously update the support for Baichuan 2 from the community and ecosystem here ğŸ˜€ğŸ˜€ğŸ˜€**\n\n## Running BaiChuan LLM on IntelÂ® Coreâ„¢ Ultra\n\n### How to Use\n\nWhen deploy on Coreâ„¢/XeonÂ® Scalable Processors or with Arcâ„¢ GPU to deploy BaiChuan2 - 7B/Chat and BaiChuan2 - 13B/Chat model.\n\nBigDL-LLM to ([CPU](https://github.com/intel-analytics/BigDL/tree/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan2), [GPU](https://github.com/intel-analytics/BigDL/tree/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/baichuan2)) is recommended to take full advantage of better inference performance.\n\n## Huawei Ascend\n\n### Pytorch Framework\n\nModel Fine-tuning: Baichuan 2 (7B) already supports PyTorchï¼ˆ2.1.0ï¼‰+ Transformersï¼ˆ4.36.0ï¼‰+ DeepSpeedï¼ˆ0.12.4ï¼‰+ Accelerateï¼ˆ0.25.0ï¼‰model fine-tuning based on Ascend NPU natively, and can be used without additional adaptation.\n\nInference Deployment: Baichuan 2 (7B) already supports inference with the Ascend NPU natively, and can be used without additional adaptation.\n\n### MindSpore Framework\n\n[MindFormers](https://gitee.com/mindspore/mindformers) is a comprehensive development suite based on the MindSpore framework that supports large model training, fine-tuning, evaluation, inference, and deployment. [Baichuan2-7B / 13B](https://gitee.com/mindspore/mindformers/tree/dev/research/baichuan2) has been integrated into this suite, supporting users in model fine-tuning and deployment. For specific usage, please see the [README](https://gitee.com/mindspore/mindformers/tree/dev/research/baichuan2/baichuan2.md).\n\n### Large Model Experience Platform\n\n[Ascend Large Model Platform](https://xihe.mindspore.cn) based on Ascend's MindSpore AI framework, MindFormers large model development suite, and Ascend hardware computing power, has opened the capabilities of the [Baichuan2-7B](https://xihe.mindspore.cn/modelzoo/baichuan2_7b_chat) large model to the public. Everyone is welcome to experience it online.\n\n# Disclaimer, License and Citation\n\n## Disclaimer\nWe hereby declare that our team has not developed any applications based on Baichuan 2 models, not on iOS, Android, the web, or any other platform. We strongly call on all users not to use Baichuan 2 models for any activities that harm national / social security or violate the law. Also, we ask users not to use Baichuan 2 models for Internet services that have not undergone appropriate security reviews and filings. We hope that all users can abide by this principle and ensure that the development of technology proceeds in a regulated and legal environment.\n\nWe have done our best to ensure the compliance of the data used in the model training process. However, despite our considerable efforts, there may still be some unforeseeable issues due to the complexity of the model and data. Therefore, if any problems arise due to the use of Baichuan 2 open-source models, including but not limited to data security issues, public opinion risks, or any risks and problems brought about by the model being misled, abused, spread or improperly exploited, we will not assume any responsibility.\n\n## License\nThe community usage of Baichuan 2 model requires adherence to [Apache 2.0](https://github.com/baichuan-inc/Baichuan2/blob/main/LICENSE) and [Community License for Baichuan2 Model](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/resolve/main/Baichuan%202%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf). The Baichuan 2 model supports commercial use. If you plan to use the Baichuan 2 model or its derivatives for commercial purposes, please ensure that your entity meets the following conditions:\n\n  1. The Daily Active Users (DAU) of your or your affiliate's service or product is less than 1 million.\n  2. Neither you nor your affiliates are software service providers or cloud service providers.\n  3. There is no possibility for you or your affiliates to grant the commercial license given to you, to reauthorize it to other third parties without Baichuan's permission.\n\nUpon meeting the above conditions, you need to submit the application materials required by the Baichuan 2 Model Community License Agreement via the following contact email: opensource@baichuan-inc.com. Once approved, Baichuan will hereby grant you a non-exclusive, global, non-transferable, non-sublicensable, revocable commercial copyright license.\n\n## Citation\nIf you wish to cite our work, please use the following reference:\n```\n@article{baichuan2023baichuan2,\n  title={Baichuan 2: Open Large-scale Language Models},\n  author={Baichuan},\n  journal={arXiv preprint arXiv:2309.10305},\n  url={https://arxiv.org/abs/2309.10305},\n  year={2023}\n}\n```\n"
        },
        {
          "name": "cli_demo.py",
          "type": "blob",
          "size": 2.783203125,
          "content": "import os\nimport torch\nimport platform\nimport subprocess\nfrom colorama import Fore, Style\nfrom tempfile import NamedTemporaryFile\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\n\n\ndef init_model():\n    print(\"init model ...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        \"baichuan-inc/Baichuan2-13B-Chat\",\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    model.generation_config = GenerationConfig.from_pretrained(\n        \"baichuan-inc/Baichuan2-13B-Chat\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"baichuan-inc/Baichuan2-13B-Chat\",\n        use_fast=False,\n        trust_remote_code=True\n    )\n    return model, tokenizer\n\n\ndef clear_screen():\n    if platform.system() == \"Windows\":\n        os.system(\"cls\")\n    else:\n        os.system(\"clear\")\n    print(Fore.YELLOW + Style.BRIGHT + \"æ¬¢è¿ä½¿ç”¨ç™¾å·å¤§æ¨¡å‹ï¼Œè¾“å…¥è¿›è¡Œå¯¹è¯ï¼Œvim å¤šè¡Œè¾“å…¥ï¼Œclear æ¸…ç©ºå†å²ï¼ŒCTRL+C ä¸­æ–­ç”Ÿæˆï¼Œstream å¼€å…³æµå¼ç”Ÿæˆï¼Œexit ç»“æŸã€‚\")\n    return []\n\n\ndef vim_input():\n    with NamedTemporaryFile() as tempfile:\n        tempfile.close()\n        subprocess.call(['vim', '+star', tempfile.name])\n        text = open(tempfile.name).read()\n    return text\n\n\ndef main(stream=True):\n    model, tokenizer = init_model()\n    messages = clear_screen()\n    while True:\n        prompt = input(Fore.GREEN + Style.BRIGHT + \"\\nç”¨æˆ·ï¼š\" + Style.NORMAL)\n        if prompt.strip() == \"exit\":\n            break\n        if prompt.strip() == \"clear\":\n            messages = clear_screen()\n            continue\n        if prompt.strip() == 'vim':\n            prompt = vim_input()\n            print(prompt)\n        print(Fore.CYAN + Style.BRIGHT + \"\\nBaichuan 2ï¼š\" + Style.NORMAL, end='')\n        if prompt.strip() == \"stream\":\n            stream = not stream\n            print(Fore.YELLOW + \"({}æµå¼ç”Ÿæˆ)\\n\".format(\"å¼€å¯\" if stream else \"å…³é—­\"), end='')\n            continue\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        if stream:\n            position = 0\n            try:\n                for response in model.chat(tokenizer, messages, stream=True):\n                    print(response[position:], end='', flush=True)\n                    position = len(response)\n                    if torch.backends.mps.is_available():\n                        torch.mps.empty_cache()\n            except KeyboardInterrupt:\n                pass\n            print()\n        else:\n            response = model.chat(tokenizer, messages)\n            print(response)\n            if torch.backends.mps.is_available():\n                torch.mps.empty_cache()\n        messages.append({\"role\": \"assistant\", \"content\": response})\n    print(Style.RESET_ALL)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "fine-tune",
          "type": "tree",
          "content": null
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.111328125,
          "content": "accelerate\ncolorama\nbitsandbytes\nsentencepiece\nstreamlit\ntransformers_stream_generator\ncpm_kernels\nxformers\nscipy\n"
        },
        {
          "name": "web_demo.py",
          "type": "blob",
          "size": 2.20703125,
          "content": "import json\nimport torch\nimport streamlit as st\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation.utils import GenerationConfig\n\n\nst.set_page_config(page_title=\"Baichuan 2\")\nst.title(\"Baichuan 2\")\n\n\n@st.cache_resource\ndef init_model():\n    model = AutoModelForCausalLM.from_pretrained(\n        \"baichuan-inc/Baichuan2-13B-Chat\",\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    model.generation_config = GenerationConfig.from_pretrained(\n        \"baichuan-inc/Baichuan2-13B-Chat\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"baichuan-inc/Baichuan2-13B-Chat\",\n        use_fast=False,\n        trust_remote_code=True\n    )\n    return model, tokenizer\n\n\ndef clear_chat_history():\n    del st.session_state.messages\n\n\ndef init_chat_history():\n    with st.chat_message(\"assistant\", avatar='ğŸ¤–'):\n        st.markdown(\"æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾å·å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°\")\n\n    if \"messages\" in st.session_state:\n        for message in st.session_state.messages:\n            avatar = 'ğŸ§‘â€ğŸ’»' if message[\"role\"] == \"user\" else 'ğŸ¤–'\n            with st.chat_message(message[\"role\"], avatar=avatar):\n                st.markdown(message[\"content\"])\n    else:\n        st.session_state.messages = []\n\n    return st.session_state.messages\n\n\ndef main():\n    model, tokenizer = init_model()\n    messages = init_chat_history()\n\n    if prompt := st.chat_input(\"Shift + Enter æ¢è¡Œ, Enter å‘é€\"):\n        with st.chat_message(\"user\", avatar='ğŸ§‘â€ğŸ’»'):\n            st.markdown(prompt)\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        print(f\"[user] {prompt}\", flush=True)\n        with st.chat_message(\"assistant\", avatar='ğŸ¤–'):\n            placeholder = st.empty()\n            for response in model.chat(tokenizer, messages, stream=True):\n                placeholder.markdown(response)\n                if torch.backends.mps.is_available():\n                    torch.mps.empty_cache()\n        messages.append({\"role\": \"assistant\", \"content\": response})\n        print(json.dumps(messages, ensure_ascii=False), flush=True)\n\n        st.button(\"æ¸…ç©ºå¯¹è¯\", on_click=clear_chat_history)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}