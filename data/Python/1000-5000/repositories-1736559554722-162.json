{
  "metadata": {
    "timestamp": 1736559554722,
    "page": 162,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hhatto/autopep8",
      "stars": 4586,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".codecov.yml",
          "type": "blob",
          "size": 0.068359375,
          "content": "coverage:\n  status:\n    project:\n      default:\n        threshold: 1%\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.193359375,
          "content": ".eggs\n.idea/\n*.class\n*.egg-info/\n*.egg/\n*.pyc\n*.swo\n*.swp\n.tox\n.travis-solo/\n.build-*/\nREADME.html\n__pycache__\n.mypy_cache/\nbuild\n.coverage\ncoverage.xml\ndist\nhtmlcov\npep8.py\ntest/suite/out/*.py.err\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.091796875,
          "content": "repos:\n  - repo: https://github.com/pycqa/flake8\n    rev: 6.0.0\n    hooks:\n      - id: flake8\n"
        },
        {
          "name": ".pre-commit-hooks.yaml",
          "type": "blob",
          "size": 0.20703125,
          "content": "-   id: autopep8\n    name: autopep8\n    description: A tool that automatically formats Python code to conform to the PEP 8 style guide.\n    entry: autopep8\n    language: python\n    types: [python]\n    args: [-i]\n"
        },
        {
          "name": "AUTHORS.rst",
          "type": "blob",
          "size": 1.9560546875,
          "content": "Main contributors\n-----------------\n- Hideo Hattori (https://github.com/hhatto)\n- Steven Myint (https://github.com/myint)\n- Bill Wendling (https://github.com/gwelymernans)\n\nPatches\n-------\n- Fraser Tweedale (https://github.com/frasertweedale)\n- clach04 (https://github.com/clach04)\n- Marc Abramowitz (https://github.com/msabramo)\n- dellis23 (https://github.com/dellis23)\n- Sam Vilain (https://github.com/samv)\n- Florent Xicluna (https://github.com/florentx)\n- Andras Tim (https://github.com/andras-tim)\n- tomscytale (https://github.com/tomscytale)\n- Filip Noetzel (https://github.com/peritus)\n- Erik Bray (https://github.com/iguananaut)\n- Christopher Medrela (https://github.com/chrismedrela)\n- 小明 (https://github.com/dongweiming)\n- Andy Hayden (https://github.com/hayd)\n- Fabio Zadrozny (https://github.com/fabioz)\n- Alex Chernetz (https://github.com/achernet)\n- Marc Schlaich (https://github.com/schlamar)\n- E. M. Bray (https://github.com/embray)\n- Thomas Hisch (https://github.com/thisch)\n- Florian Best (https://github.com/spaceone)\n- Ian Clark (https://github.com/evenicoulddoit)\n- Khairi Hafsham (https://github.com/khairihafsham)\n- Neil Halelamien (https://github.com/neilsh)\n- Hashem Nasarat (https://github.com/Hnasar)\n- Hugo van Kemenade (https://github.com/hugovk)\n- gmbnomis (https://github.com/gmbnomis)\n- Samuel Lelièvre (https://github.com/slel)\n- bigredengineer (https://github.com/bigredengineer)\n- Kai Chen (https://github.com/kx-chen)\n- Anthony Sottile (https://github.com/asottile)\n- 秋葉 (https://github.com/Hanaasagi)\n- Christian Clauss (https://github.com/cclauss)\n- tobixx (https://github.com/tobixx)\n- bigredengineer (https://github.com/bigredengineer)\n- Bastien Gérard (https://github.com/bagerard)\n- nicolasbonifas (https://github.com/nicolasbonifas)\n- Andrii Yurchuk (https://github.com/Ch00k)\n- José M. Guisado (https://github.com/pvxe)\n- Dai Truong (https://github.com/NovaDev94)\n- jnozsc (https://github.com/jnozsc)\n- Edwin Shepherd (https://github.com/shardros)\n"
        },
        {
          "name": "CONTRIBUTING.rst",
          "type": "blob",
          "size": 1.04296875,
          "content": "============\nContributing\n============\n\nContributions are appreciated.\n\n\nIssues\n======\n\nWhen submitting a bug report, please provide the following:\n\n1. Does the ``pycodestyle`` tool behave correctly? If not, then the bug\n   report should be filed at the pycodestyle_ repository instead.\n2. ``autopep8 --version``\n3. ``pycodestyle --version``\n4. ``python --version``\n5. ``uname -a`` if on Unix.\n6. The example input that causes the bug.\n7. The ``autopep8`` command-line options used to cause the bug.\n8. The expected output.\n9. Does the bug happen with the latest version of autopep8? To upgrade::\n\n    $ pip install --upgrade git+https://github.com/hhatto/autopep8\n\n\nPull requests\n=============\n\nWhen submitting a pull request, please do the following.\n\n1. Does the ``pycodestyle`` tool behave correctly? If not, then a pull request\n   should be filed at the pycodestyle_ repository instead.\n2. Add a test case to ``test/test_autopep8.py`` that demonstrates what your\n   change does.\n3. Make sure all tests pass.\n\n.. _pycodestyle: https://github.com/PyCQA/pycodestyle\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1533203125,
          "content": "Copyright (C) 2010-2011 Hideo Hattori\nCopyright (C) 2011-2013 Hideo Hattori, Steven Myint\nCopyright (C) 2013-2016 Hideo Hattori, Steven Myint, Bill Wendling\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\nBE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\nACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.7001953125,
          "content": "include AUTHORS.rst\ninclude LICENSE\ninclude README.rst\ninclude test/__init__.py\ninclude test/acid.py\ninclude test/bad_encoding.py\ninclude test/bad_encoding2.py\ninclude test/e101_example.py\ninclude test/example\ninclude test/example/x.py\ninclude test/example.py\ninclude test/iso_8859_1.py\ninclude test/fake_configuration/.pep8\ninclude test/fake_pycodestyle_configuration/tox.ini\ninclude tox.ini\nrecursive-exclude test/suite *.py\nrecursive-exclude test/suite/out *.py\nexclude CONTRIBUTING.rst\nexclude Makefile\nexclude hooks\nexclude hooks/pre-push\nexclude install_hooks.bash\nexclude test/.gitignore\nexclude test/acid_pypi.py\nexclude test/suite\nexclude test/suite/out\nexclude test/vim_autopep8.py\nexclude update_readme.py\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.3525390625,
          "content": "all:\n\t@echo \"make test(test_basic, test_diff, test_unit)\"\n\t@echo \"make fasttest\"\n\t@echo \"make benchmark\"\n\t@echo \"make pypireg\"\n\t@echo \"make coverage\"\n\t@echo \"make check\"\n\t@echo \"make clean\"\n\nPYTHON?=python\nCOVERAGE?=coverage\n\nTEST_DIR=test\n\ntest: test_basic test_diff test_unit\nfasttest: test_fast\n\ntest_basic:\n\t@echo '--->  Running basic test'\n\t@${PYTHON} autopep8.py --aggressive test/example.py > .tmp.test.py\n\tpycodestyle --repeat .tmp.test.py\n\t@rm .tmp.test.py\n\ntest_diff:\n\t@echo '--->  Running --diff test'\n\t@cp test/example.py .tmp.example.py\n\t@${PYTHON} autopep8.py --aggressive --diff .tmp.example.py > .tmp.example.py.patch\n\tpatch < .tmp.example.py.patch\n\t@rm .tmp.example.py.patch\n\tpycodestyle --repeat .tmp.example.py && ${PYTHON} -m py_compile .tmp.example.py\n\t@rm .tmp.example.py\n\ntest_unit:\n\t@echo '--->  Running unit tests'\n\t@${PYTHON} test/test_autopep8.py\n\ntest_fast:\n\t@echo '[run]' > .pytest.coveragerc\n\t@echo 'branch = True' >> .pytest.coveragerc\n\t@echo 'omit = \"*/site-packages/*\"' >> .pytest.coveragerc\n\t@echo '[report]' >> .pytest.coveragerc\n\t@echo 'include = autopep8.py' >> .pytest.coveragerc\n\t@AUTOPEP8_COVERAGE=1 py.test -n4 --cov-config .pytest.coveragerc \\\n\t\t--cov-report term-missing --cov autopep8 test/test_autopep8.py\n\t@rm .pytest.coveragerc .coverage\n\ntest_ci:\n\tpytest --cov-report xml --cov=autopep8\n\t@${PYTHON} test/acid.py -aaa --experimental test/example.py\n\t@${PYTHON} test/acid.py --pycodestyle= -aaa --compare-bytecode --experimental test/example.py\n\t@${PYTHON} test/acid.py --pycodestyle= --aggressive --line-range 550 610 test/inspect_example.py\n\t@${PYTHON} test/acid.py --pycodestyle= --line-range 289 925 test/vectors_example.py\n\t@${PYTHON} test/test_suite.py\n\ncoverage:\n\t@coverage erase\n\t@AUTOPEP8_COVERAGE=1 ${COVERAGE} run --branch --parallel-mode --omit='*/site-packages/*' test/test_autopep8.py\n\t@${COVERAGE} combine\n\t@${COVERAGE} report --show-missing\n\t@${COVERAGE} xml --include=autopep8.py\n\nopen_coverage: coverage\n\t@${COVERAGE} html\n\t@python -m webbrowser -n \"file://${PWD}/htmlcov/index.html\"\n\nbenchmark:\n\t@echo '--->  Benchmark of autopep8.py test/example.py'\n\t@time ${PYTHON} autopep8.py --aggressive test/example.py > /dev/null\n\t@echo '--->  Benchmark of test_unit'\n\t@time ${PYTHON} test/test_autopep8.py > /dev/null\n\t@echo '--->  Benchmark of autopep8.py -d test/*.py'\n\t@time ${PYTHON} autopep8.py -d test/*.py > /dev/null\n\nreadme:\n\t@${PYTHON} update_readme.py\n\t@rstcheck README.rst\n\t@${PYTHON} -m doctest -v README.rst\n\nopen_readme: readme\n\t@python -m webbrowser -n \"file://${PWD}/README.html\"\n\ncheck:\n\tpycodestyle \\\n\t\t--ignore=E402,E226,E24,W50 \\\n\t\tautopep8.py setup.py test/acid.py test/acid_pypi.py update_readme.py\n\tpycodestyle \\\n\t\t--max-line-length=300 test/test_autopep8.py\n\tpylint \\\n\t\t--reports=no \\\n\t\t--msg-template='{path}:{line}: [{msg_id}({symbol}), {obj}] {msg}' \\\n\t\t--disable=bad-builtin \\\n\t\t--disable=bad-continuation \\\n\t\t--disable=fixme \\\n\t\t--disable=import-error \\\n\t\t--disable=invalid-name \\\n\t\t--disable=locally-disabled \\\n\t\t--disable=missing-docstring \\\n\t\t--disable=no-member \\\n\t\t--disable=no-self-use \\\n\t\t--disable=not-callable \\\n\t\t--disable=protected-access \\\n\t\t--disable=redefined-builtin \\\n\t\t--disable=star-args \\\n\t\t--disable=super-on-old-class \\\n\t\t--disable=too-few-public-methods \\\n\t\t--disable=too-many-arguments \\\n\t\t--disable=too-many-boolean-expressions \\\n\t\t--disable=too-many-branches \\\n\t\t--disable=too-many-instance-attributes \\\n\t\t--disable=too-many-lines \\\n\t\t--disable=too-many-locals \\\n\t\t--disable=too-many-nested-blocks \\\n\t\t--disable=too-many-public-methods \\\n\t\t--disable=too-many-statements \\\n\t\t--disable=undefined-loop-variable \\\n\t\t--rcfile=/dev/null autopep8.py setup.py update_readme.py\n\tpylint \\\n\t\t--reports=no \\\n\t\t--msg-template='{path}:{line}: [{msg_id}({symbol}), {obj}] {msg}' \\\n\t\t--errors-only \\\n\t\t--disable=no-member \\\n\t\t--rcfile=/dev/null \\\n\t\ttest/acid.py test/acid_pypi.py test/test_autopep8.py\n\t./autopep8.py --diff autopep8.py setup.py test/test_autopep8.py update_readme.py\n\nmutant:\n\t@mut.py --disable-operator RIL -t autopep8 -u test.test_autopep8 -mc\n\npypireg:\n\t${PYTHON} -m pip install twine build\n\t${PYTHON} -m build\n\t${PYTHON} -m twine upload dist/*\n\nclean:\n\trm -rf .tmp.test.py temp *.pyc *egg-info dist build \\\n\t\t__pycache__ */__pycache__ */*/__pycache__ \\\n\t\thtmlcov coverage.xml\n\n.PHONY: \\\n\tall clean mutant pypireg test_basic test_unit \\\n\tbenchmark coverage open_coverage readme test_diff \\\n\tcheck fasttest open_readme test test_fast test_ci\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 15.33203125,
          "content": "========\nautopep8\n========\n\n.. image:: https://img.shields.io/pypi/v/autopep8.svg\n    :target: https://pypi.org/project/autopep8/\n    :alt: PyPI Version\n\n.. image:: https://github.com/hhatto/autopep8/workflows/Python%20package/badge.svg\n    :target: https://github.com/hhatto/autopep8/actions\n    :alt: Build status\n\n.. image:: https://codecov.io/gh/hhatto/autopep8/branch/main/graph/badge.svg\n    :target: https://codecov.io/gh/hhatto/autopep8\n    :alt: Code Coverage\n\nautopep8 automatically formats Python code to conform to the `PEP 8`_ style\nguide. It uses the pycodestyle_ utility to determine what parts of the code\nneeds to be formatted. autopep8 is capable of fixing most of the formatting\nissues_ that can be reported by pycodestyle.\n\n.. _PEP 8: https://www.python.org/dev/peps/pep-0008/\n.. _issues: https://pycodestyle.readthedocs.org/en/latest/intro.html#error-codes\n\n.. contents::\n\n\nInstallation\n============\n\nFrom pip::\n\n    $ pip install --upgrade autopep8\n\nConsider using the ``--user`` option_.\n\n.. _option: https://pip.pypa.io/en/latest/user_guide/#user-installs\n\n\nRequirements\n============\n\nautopep8 requires pycodestyle_.\n\n.. _pycodestyle: https://github.com/PyCQA/pycodestyle\n\n\nUsage\n=====\n\nTo modify a file in place (with aggressive level 2)::\n\n    $ autopep8 --in-place --aggressive --aggressive <filename>\n\nBefore running autopep8.\n\n.. code-block:: python\n\n    import math, sys;\n\n    def example1():\n        ####This is a long comment. This should be wrapped to fit within 72 characters.\n        some_tuple=(   1,2, 3,'a'  );\n        some_variable={'long':'Long code lines should be wrapped within 79 characters.',\n        'other':[math.pi, 100,200,300,9876543210,'This is a long string that goes on'],\n        'more':{'inner':'This whole logical line should be wrapped.',some_tuple:[1,\n        20,300,40000,500000000,60000000000000000]}}\n        return (some_tuple, some_variable)\n    def example2(): return {'has_key() is deprecated':True}.has_key({'f':2}.has_key(''));\n    class Example3(   object ):\n        def __init__    ( self, bar ):\n         #Comments should have a space after the hash.\n         if bar : bar+=1;  bar=bar* bar   ; return bar\n         else:\n                        some_string = \"\"\"\n    \t\t           Indentation in multiline strings should not be touched.\n    Only actual code should be reindented.\n    \"\"\"\n                        return (sys.path, some_string)\n\nAfter running autopep8.\n\n.. code-block:: python\n\n    import math\n    import sys\n\n\n    def example1():\n        # This is a long comment. This should be wrapped to fit within 72\n        # characters.\n        some_tuple = (1, 2, 3, 'a')\n        some_variable = {\n            'long': 'Long code lines should be wrapped within 79 characters.',\n            'other': [\n                math.pi,\n                100,\n                200,\n                300,\n                9876543210,\n                'This is a long string that goes on'],\n            'more': {\n                'inner': 'This whole logical line should be wrapped.',\n                some_tuple: [\n                    1,\n                    20,\n                    300,\n                    40000,\n                    500000000,\n                    60000000000000000]}}\n        return (some_tuple, some_variable)\n\n\n    def example2(): return ('' in {'f': 2}) in {'has_key() is deprecated': True}\n\n\n    class Example3(object):\n        def __init__(self, bar):\n            # Comments should have a space after the hash.\n            if bar:\n                bar += 1\n                bar = bar * bar\n                return bar\n            else:\n                some_string = \"\"\"\n    \t\t           Indentation in multiline strings should not be touched.\n    Only actual code should be reindented.\n    \"\"\"\n                return (sys.path, some_string)\n\nOptions::\n\n    usage: autopep8 [-h] [--version] [-v] [-d] [-i] [--global-config filename]\n                    [--ignore-local-config] [-r] [-j n] [-p n] [-a]\n                    [--experimental] [--exclude globs] [--list-fixes]\n                    [--ignore errors] [--select errors] [--max-line-length n]\n                    [--line-range line line] [--hang-closing] [--exit-code]\n                    [files [files ...]]\n\n    Automatically formats Python code to conform to the PEP 8 style guide.\n\n    positional arguments:\n      files                 files to format or '-' for standard in\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      --version             show program's version number and exit\n      -v, --verbose         print verbose messages; multiple -v result in more\n                            verbose messages\n      -d, --diff            print the diff for the fixed source\n      -i, --in-place        make changes to files in place\n      --global-config filename\n                            path to a global pep8 config file; if this file does\n                            not exist then this is ignored (default:\n                            ~/.config/pep8)\n      --ignore-local-config\n                            don't look for and apply local config files; if not\n                            passed, defaults are updated with any config files in\n                            the project's root directory\n      -r, --recursive       run recursively over directories; must be used with\n                            --in-place or --diff\n      -j n, --jobs n        number of parallel jobs; match CPU count if value is\n                            less than 1\n      -p n, --pep8-passes n\n                            maximum number of additional pep8 passes (default:\n                            infinite)\n      -a, --aggressive      enable non-whitespace changes; multiple -a result in\n                            more aggressive changes\n      --experimental        enable experimental fixes\n      --exclude globs       exclude file/directory names that match these comma-\n                            separated globs\n      --list-fixes          list codes for fixes; used by --ignore and --select\n      --ignore errors       do not fix these errors/warnings (default:\n                            E226,E24,W50,W690)\n      --select errors       fix only these errors/warnings (e.g. E4,W)\n      --max-line-length n   set maximum allowed line length (default: 79)\n      --line-range line line, --range line line\n                            only fix errors found within this inclusive range of\n                            line numbers (e.g. 1 99); line numbers are indexed at\n                            1\n      --hang-closing        hang-closing option passed to pycodestyle\n      --exit-code           change to behavior of exit code. default behavior of\n                            return value, 0 is no differences, 1 is error exit.\n                            return 2 when add this option. 2 is exists\n                            differences.\n\n\nFeatures\n========\n\nautopep8 fixes the following issues_ reported by pycodestyle_::\n\n    E101 - Reindent all lines.\n    E11  - Fix indentation.\n    E121 - Fix indentation to be a multiple of four.\n    E122 - Add absent indentation for hanging indentation.\n    E123 - Align closing bracket to match opening bracket.\n    E124 - Align closing bracket to match visual indentation.\n    E125 - Indent to distinguish line from next logical line.\n    E126 - Fix over-indented hanging indentation.\n    E127 - Fix visual indentation.\n    E128 - Fix visual indentation.\n    E129 - Fix visual indentation.\n    E131 - Fix hanging indent for unaligned continuation line.\n    E133 - Fix missing indentation for closing bracket.\n    E20  - Remove extraneous whitespace.\n    E211 - Remove extraneous whitespace.\n    E22  - Fix extraneous whitespace around keywords.\n    E224 - Remove extraneous whitespace around operator.\n    E225 - Fix missing whitespace around operator.\n    E226 - Fix missing whitespace around arithmetic operator.\n    E227 - Fix missing whitespace around bitwise/shift operator.\n    E228 - Fix missing whitespace around modulo operator.\n    E231 - Add missing whitespace.\n    E241 - Fix extraneous whitespace around keywords.\n    E242 - Remove extraneous whitespace around operator.\n    E251 - Remove whitespace around parameter '=' sign.\n    E252 - Missing whitespace around parameter equals.\n    E26  - Fix spacing after comment hash for inline comments.\n    E265 - Fix spacing after comment hash for block comments.\n    E266 - Fix too many leading '#' for block comments.\n    E27  - Fix extraneous whitespace around keywords.\n    E301 - Add missing blank line.\n    E302 - Add missing 2 blank lines.\n    E303 - Remove extra blank lines.\n    E304 - Remove blank line following function decorator.\n    E305 - Expected 2 blank lines after end of function or class.\n    E306 - Expected 1 blank line before a nested definition.\n    E401 - Put imports on separate lines.\n    E402 - Fix module level import not at top of file\n    E501 - Try to make lines fit within --max-line-length characters.\n    E502 - Remove extraneous escape of newline.\n    E701 - Put colon-separated compound statement on separate lines.\n    E70  - Put semicolon-separated compound statement on separate lines.\n    E711 - Fix comparison with None.\n    E712 - Fix comparison with boolean.\n    E713 - Use 'not in' for test for membership.\n    E714 - Use 'is not' test for object identity.\n    E721 - Use \"isinstance()\" instead of comparing types directly.\n    E722 - Fix bare except.\n    E731 - Use a def when use do not assign a lambda expression.\n    W291 - Remove trailing whitespace.\n    W292 - Add a single newline at the end of the file.\n    W293 - Remove trailing whitespace on blank line.\n    W391 - Remove trailing blank lines.\n    W503 - Fix line break before binary operator.\n    W504 - Fix line break after binary operator.\n    W605 - Fix invalid escape sequence 'x'.\n\nautopep8 also fixes some issues not found by pycodestyle_.\n\n- Normalize files with mixed line endings.\n- Put a blank line between a class docstring and its first method\n  declaration. (Enabled with ``E301``.)\n- Remove blank lines between a function declaration and its docstring. (Enabled\n  with ``E303``.)\n\nautopep8 avoids fixing some issues found by pycodestyle_.\n\n- ``E112``/``E113`` for non comments are reports of bad indentation that break\n  syntax rules. These should not be modified at all.\n- ``E265``, which refers to spacing after comment hash, is ignored if the\n  comment looks like code. autopep8 avoids modifying these since they are not\n  real comments. If you really want to get rid of the pycodestyle_ warning,\n  consider just removing the commented-out code. (This can be automated via\n  eradicate_.)\n\n.. _eradicate: https://github.com/myint/eradicate\n\n\nMore advanced usage\n===================\n\nBy default autopep8 only makes whitespace changes. Thus, by default, it does\nnot fix ``E711`` and ``E712``. (Changing ``x == None`` to ``x is None`` may\nchange the meaning of the program if ``x`` has its ``__eq__`` method\noverridden.) Nor does it correct deprecated code ``W6``. To enable these\nmore aggressive fixes, use the ``--aggressive`` option::\n\n    $ autopep8 --aggressive <filename>\n\nUse multiple ``--aggressive`` to increase the aggressiveness level. For\nexample, ``E712`` requires aggressiveness level 2 (since ``x == True`` could be\nchanged to either ``x`` or ``x is True``, but autopep8 chooses the former).\n\n``--aggressive`` will also shorten lines more aggressively. It will also remove\ntrailing whitespace more aggressively. (Usually, we don't touch trailing\nwhitespace in docstrings and other multiline strings. And to do even more\naggressive changes to docstrings, use docformatter_.)\n\n.. _docformatter: https://github.com/myint/docformatter\n\nTo enable only a subset of the fixes, use the ``--select`` option. For example,\nto fix various types of indentation issues::\n\n    $ autopep8 --select=E1,W1 <filename>\n\nIf the file being fixed is large, you may want to enable verbose progress\nmessages::\n\n    $ autopep8 -v <filename>\n\nPassing in ``--experimental`` enables the following functionality:\n\n- Shortens code lines by taking its length into account\n\n::\n\n$ autopep8 --experimental <filename>\n\nDisabling line-by-line\n----------------------\n\nIt is possible to disable autopep8 until it is turned back on again in the file, using ``autopep8: off`` and then reenabling with ``autopep8: on``. \n\n.. code-block:: python\n\n    # autopep8: off\n        [\n            [23, 23, 13, 43],\n            [32, 34, 34, 34],\n            [56, 34, 34, 11],\n            [10, 10, 10, 10],\n        ]\n    # autopep8: on\n         \n``fmt: off`` and ``fmt: on`` are also valid.\n\nUse as a module\n===============\n\nThe simplest way of using autopep8 as a module is via the ``fix_code()``\nfunction:\n\n    >>> import autopep8\n    >>> autopep8.fix_code('x=       123\\n')\n    'x = 123\\n'\n\nOr with options:\n\n    >>> import autopep8\n    >>> autopep8.fix_code('print( 123 )\\n',\n    ...                   options={'ignore': ['E']})\n    'print( 123 )\\n'\n\n\nConfiguration\n=============\n\nBy default, if ``$HOME/.config/pycodestyle`` (``~\\.pycodestyle`` in Windows\nenvironment) exists, it will be used as global configuration file.\nAlternatively, you can specify the global configuration file with the\n``--global-config`` option.\n\nAlso, if ``setup.cfg``, ``tox.ini``, ``.pep8`` and ``.flake8`` files exist\nin the directory where the target file exists, it will be used as the\nconfiguration file.\n\n``pep8``, ``pycodestyle``, and ``flake8`` can be used as a section.\n\nconfiguration file example::\n\n    [pycodestyle]\n    max_line_length = 120\n    ignore = E501\n\npyproject.toml\n--------------\n\nautopep8 can also use ``pyproject.toml``.\nThe section must be ``[tool.autopep8]``, and ``pyproject.toml`` takes precedence\nover any other configuration files.\n\nconfiguration file example::\n\n    [tool.autopep8]\n    max_line_length = 120\n    ignore = \"E501,W6\"  # or [\"E501\", \"W6\"]\n    in-place = true\n    recursive = true\n    aggressive = 3\n\nUsage with pre-commit\n=====================\n\nautopep8 can be used as a hook for pre-commit_.\n\nTo add autopep8 as a plugin, add this repo definition to your configuration:\n\n.. code-block:: yaml\n\n    repos:\n    -   repo: https://github.com/hhatto/autopep8\n        rev: ...  # select the tag or revision you want, or run `pre-commit autoupdate`\n        hooks:\n        -   id: autopep8\n\n.. _`pre-commit`: https://pre-commit.com\n\n\nTesting\n=======\n\nTest cases are in ``test/test_autopep8.py``. They can be run directly via\n``python test/test_autopep8.py`` or via tox_. The latter is useful for\ntesting against multiple Python interpreters. (We currently test against\nCPython versions 3.8, 3.9, 3.10, 3.11 and 3.12. We also test against PyPy.)\n\n.. _`tox`: https://pypi.org/project/tox/\n\nBroad spectrum testing is available via ``test/acid.py``. This script runs\nautopep8 against Python code and checks for correctness and completeness of the\ncode fixes. It can check that the bytecode remains identical.\n``test/acid_pypi.py`` makes use of ``acid.py`` to test against the latest\nreleased packages on PyPI.\n\n\nTroubleshooting\n===============\n\n``pkg_resources.DistributionNotFound``\n--------------------------------------\n\nIf you are using an ancient version of ``setuptools``, you might encounter\n``pkg_resources.DistributionNotFound`` when trying to run ``autopep8``. Try\nupgrading ``setuptools`` to workaround this ``setuptools`` problem::\n\n    $ pip install --upgrade setuptools\n\nUse ``sudo`` if you are installing to the system.\n\n\nLinks\n=====\n\n* PyPI_\n* GitHub_\n* Codecov_\n\n.. _PyPI: https://pypi.org/project/autopep8/\n.. _GitHub: https://github.com/hhatto/autopep8\n.. _`Codecov`: https://app.codecov.io/gh/hhatto/autopep8\n"
        },
        {
          "name": "autopep8.py",
          "type": "blob",
          "size": 154.5771484375,
          "content": "#!/usr/bin/env python\n\n# Copyright (C) 2010-2011 Hideo Hattori\n# Copyright (C) 2011-2013 Hideo Hattori, Steven Myint\n# Copyright (C) 2013-2016 Hideo Hattori, Steven Myint, Bill Wendling\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# Copyright (C) 2006-2009 Johann C. Rocholl <johann@rocholl.net>\n# Copyright (C) 2009-2013 Florent Xicluna <florent.xicluna@gmail.com>\n#\n# Permission is hereby granted, free of charge, to any person\n# obtaining a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"Automatically formats Python code to conform to the PEP 8 style guide.\n\nFixes that only need be done once can be added by adding a function of the form\n\"fix_<code>(source)\" to this module. They should return the fixed source code.\nThese fixes are picked up by apply_global_fixes().\n\nFixes that depend on pycodestyle should be added as methods to FixPEP8. See the\nclass documentation for more information.\n\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport collections\nimport copy\nimport difflib\nimport fnmatch\nimport importlib\nimport inspect\nimport io\nimport itertools\nimport keyword\nimport locale\nimport os\nimport re\nimport signal\nimport sys\nimport textwrap\nimport token\nimport tokenize\nimport warnings\nimport ast\nfrom configparser import ConfigParser as SafeConfigParser, Error\n\nimport pycodestyle\n\n\n__version__ = '2.3.1'\n\n\nCR = '\\r'\nLF = '\\n'\nCRLF = '\\r\\n'\n\n\nPYTHON_SHEBANG_REGEX = re.compile(r'^#!.*\\bpython[23]?\\b\\s*$')\nLAMBDA_REGEX = re.compile(r'([\\w.]+)\\s=\\slambda\\s*([)(=\\w,\\s.]*):')\nCOMPARE_NEGATIVE_REGEX = re.compile(r'\\b(not)\\s+([^][)(}{]+?)\\s+(in|is)\\s')\nCOMPARE_NEGATIVE_REGEX_THROUGH = re.compile(r'\\b(not\\s+in|is\\s+not)\\s')\nBARE_EXCEPT_REGEX = re.compile(r'except\\s*:')\nSTARTSWITH_DEF_REGEX = re.compile(r'^(async\\s+def|def)\\s.*\\):')\nDOCSTRING_START_REGEX = re.compile(r'^u?r?(?P<kind>[\"\\']{3})')\nENABLE_REGEX = re.compile(r'# *(fmt|autopep8): *on')\nDISABLE_REGEX = re.compile(r'# *(fmt|autopep8): *off')\nENCODING_MAGIC_COMMENT = re.compile(\n    r'^[ \\t\\f]*#.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)'\n)\nCOMPARE_TYPE_REGEX = re.compile(\n    r'([=!]=)\\s+type(?:\\s*\\(\\s*([^)]*[^ )])\\s*\\))'\n    r'|\\btype(?:\\s*\\(\\s*([^)]*[^ )])\\s*\\))\\s+([=!]=)'\n)\nTYPE_REGEX = re.compile(r'(type\\s*\\(\\s*[^)]*?[^\\s)]\\s*\\))')\n\nEXIT_CODE_OK = 0\nEXIT_CODE_ERROR = 1\nEXIT_CODE_EXISTS_DIFF = 2\nEXIT_CODE_ARGPARSE_ERROR = 99\n\n# For generating line shortening candidates.\nSHORTEN_OPERATOR_GROUPS = frozenset([\n    frozenset([',']),\n    frozenset(['%']),\n    frozenset([',', '(', '[', '{']),\n    frozenset(['%', '(', '[', '{']),\n    frozenset([',', '(', '[', '{', '%', '+', '-', '*', '/', '//']),\n    frozenset(['%', '+', '-', '*', '/', '//']),\n])\n\n\nDEFAULT_IGNORE = 'E226,E24,W50,W690'    # TODO: use pycodestyle.DEFAULT_IGNORE\nDEFAULT_INDENT_SIZE = 4\n# these fixes conflict with each other, if the `--ignore` setting causes both\n# to be enabled, disable both of them\nCONFLICTING_CODES = ('W503', 'W504')\n\nif sys.platform == 'win32':  # pragma: no cover\n    DEFAULT_CONFIG = os.path.expanduser(r'~\\.pycodestyle')\nelse:\n    DEFAULT_CONFIG = os.path.join(os.getenv('XDG_CONFIG_HOME') or\n                                  os.path.expanduser('~/.config'),\n                                  'pycodestyle')\n# fallback, use .pep8\nif not os.path.exists(DEFAULT_CONFIG):  # pragma: no cover\n    if sys.platform == 'win32':\n        DEFAULT_CONFIG = os.path.expanduser(r'~\\.pep8')\n    else:\n        DEFAULT_CONFIG = os.path.join(os.path.expanduser('~/.config'), 'pep8')\nPROJECT_CONFIG = ('setup.cfg', 'tox.ini', '.pep8', '.flake8')\n\n\nMAX_PYTHON_FILE_DETECTION_BYTES = 1024\n\nIS_SUPPORT_TOKEN_FSTRING = False\nif sys.version_info >= (3, 12):  # pgrama: no cover\n    IS_SUPPORT_TOKEN_FSTRING = True\n\n\ndef _custom_formatwarning(message, category, _, __, line=None):\n    return f\"{category.__name__}: {message}\\n\"\n\n\ndef open_with_encoding(filename, mode='r', encoding=None, limit_byte_check=-1):\n    \"\"\"Return opened file with a specific encoding.\"\"\"\n    if not encoding:\n        encoding = detect_encoding(filename, limit_byte_check=limit_byte_check)\n\n    return io.open(filename, mode=mode, encoding=encoding,\n                   newline='')  # Preserve line endings\n\n\ndef _detect_encoding_from_file(filename: str):\n    try:\n        with open(filename) as input_file:\n            for idx, line in enumerate(input_file):\n                if idx == 0 and line[0] == '\\ufeff':\n                    return \"utf-8-sig\"\n                if idx >= 2:\n                    break\n                match = ENCODING_MAGIC_COMMENT.search(line)\n                if match:\n                    return match.groups()[0]\n    except Exception:\n        pass\n    # Python3's default encoding\n    return 'utf-8'\n\n\ndef detect_encoding(filename, limit_byte_check=-1):\n    \"\"\"Return file encoding.\"\"\"\n    encoding = _detect_encoding_from_file(filename)\n    if encoding == \"utf-8-sig\":\n        return encoding\n    try:\n        with open_with_encoding(filename, encoding=encoding) as test_file:\n            test_file.read(limit_byte_check)\n        return encoding\n    except (LookupError, SyntaxError, UnicodeDecodeError):\n        return 'latin-1'\n\n\ndef readlines_from_file(filename):\n    \"\"\"Return contents of file.\"\"\"\n    with open_with_encoding(filename) as input_file:\n        return input_file.readlines()\n\n\ndef extended_blank_lines(logical_line,\n                         blank_lines,\n                         blank_before,\n                         indent_level,\n                         previous_logical):\n    \"\"\"Check for missing blank lines after class declaration.\"\"\"\n    if previous_logical.startswith(('def ', 'async def ')):\n        if blank_lines and pycodestyle.DOCSTRING_REGEX.match(logical_line):\n            yield (0, 'E303 too many blank lines ({})'.format(blank_lines))\n    elif pycodestyle.DOCSTRING_REGEX.match(previous_logical):\n        # Missing blank line between class docstring and method declaration.\n        if (\n            indent_level and\n            not blank_lines and\n            not blank_before and\n            logical_line.startswith(('def ', 'async def ')) and\n            '(self' in logical_line\n        ):\n            yield (0, 'E301 expected 1 blank line, found 0')\n\n\ndef continued_indentation(logical_line, tokens, indent_level, hang_closing,\n                          indent_char, noqa):\n    \"\"\"Override pycodestyle's function to provide indentation information.\"\"\"\n    first_row = tokens[0][2][0]\n    nrows = 1 + tokens[-1][2][0] - first_row\n    if noqa or nrows == 1:\n        return\n\n    # indent_next tells us whether the next block is indented. Assuming\n    # that it is indented by 4 spaces, then we should not allow 4-space\n    # indents on the final continuation line. In turn, some other\n    # indents are allowed to have an extra 4 spaces.\n    indent_next = logical_line.endswith(':')\n\n    row = depth = 0\n    valid_hangs = (\n        (DEFAULT_INDENT_SIZE,)\n        if indent_char != '\\t' else (DEFAULT_INDENT_SIZE,\n                                     2 * DEFAULT_INDENT_SIZE)\n    )\n\n    # Remember how many brackets were opened on each line.\n    parens = [0] * nrows\n\n    # Relative indents of physical lines.\n    rel_indent = [0] * nrows\n\n    # For each depth, collect a list of opening rows.\n    open_rows = [[0]]\n    # For each depth, memorize the hanging indentation.\n    hangs = [None]\n\n    # Visual indents.\n    indent_chances = {}\n    last_indent = tokens[0][2]\n    indent = [last_indent[1]]\n\n    last_token_multiline = None\n    line = None\n    last_line = ''\n    last_line_begins_with_multiline = False\n    for token_type, text, start, end, line in tokens:\n\n        newline = row < start[0] - first_row\n        if newline:\n            row = start[0] - first_row\n            newline = (not last_token_multiline and\n                       token_type not in (tokenize.NL, tokenize.NEWLINE))\n            last_line_begins_with_multiline = last_token_multiline\n\n        if newline:\n            # This is the beginning of a continuation line.\n            last_indent = start\n\n            # Record the initial indent.\n            rel_indent[row] = pycodestyle.expand_indent(line) - indent_level\n\n            # Identify closing bracket.\n            close_bracket = (token_type == tokenize.OP and text in ']})')\n\n            # Is the indent relative to an opening bracket line?\n            for open_row in reversed(open_rows[depth]):\n                hang = rel_indent[row] - rel_indent[open_row]\n                hanging_indent = hang in valid_hangs\n                if hanging_indent:\n                    break\n            if hangs[depth]:\n                hanging_indent = (hang == hangs[depth])\n\n            visual_indent = (not close_bracket and hang > 0 and\n                             indent_chances.get(start[1]))\n\n            if close_bracket and indent[depth]:\n                # Closing bracket for visual indent.\n                if start[1] != indent[depth]:\n                    yield (start, 'E124 {}'.format(indent[depth]))\n            elif close_bracket and not hang:\n                # closing bracket matches indentation of opening bracket's line\n                if hang_closing:\n                    yield (start, 'E133 {}'.format(indent[depth]))\n            elif indent[depth] and start[1] < indent[depth]:\n                if visual_indent is not True:\n                    # Visual indent is broken.\n                    yield (start, 'E128 {}'.format(indent[depth]))\n            elif (hanging_indent or\n                  (indent_next and\n                   rel_indent[row] == 2 * DEFAULT_INDENT_SIZE)):\n                # Hanging indent is verified.\n                if close_bracket and not hang_closing:\n                    yield (start, 'E123 {}'.format(indent_level +\n                                                   rel_indent[open_row]))\n                hangs[depth] = hang\n            elif visual_indent is True:\n                # Visual indent is verified.\n                indent[depth] = start[1]\n            elif visual_indent in (text, str):\n                # Ignore token lined up with matching one from a previous line.\n                pass\n            else:\n                one_indented = (indent_level + rel_indent[open_row] +\n                                DEFAULT_INDENT_SIZE)\n                # Indent is broken.\n                if hang <= 0:\n                    error = ('E122', one_indented)\n                elif indent[depth]:\n                    error = ('E127', indent[depth])\n                elif not close_bracket and hangs[depth]:\n                    error = ('E131', one_indented)\n                elif hang > DEFAULT_INDENT_SIZE:\n                    error = ('E126', one_indented)\n                else:\n                    hangs[depth] = hang\n                    error = ('E121', one_indented)\n\n                yield (start, '{} {}'.format(*error))\n\n        # Look for visual indenting.\n        if (\n            parens[row] and\n            token_type not in (tokenize.NL, tokenize.COMMENT) and\n            not indent[depth]\n        ):\n            indent[depth] = start[1]\n            indent_chances[start[1]] = True\n        # Deal with implicit string concatenation.\n        elif (token_type in (tokenize.STRING, tokenize.COMMENT) or\n              text in ('u', 'ur', 'b', 'br')):\n            indent_chances[start[1]] = str\n        # Special case for the \"if\" statement because len(\"if (\") is equal to\n        # 4.\n        elif not indent_chances and not row and not depth and text == 'if':\n            indent_chances[end[1] + 1] = True\n        elif text == ':' and line[end[1]:].isspace():\n            open_rows[depth].append(row)\n\n        # Keep track of bracket depth.\n        if token_type == tokenize.OP:\n            if text in '([{':\n                depth += 1\n                indent.append(0)\n                hangs.append(None)\n                if len(open_rows) == depth:\n                    open_rows.append([])\n                open_rows[depth].append(row)\n                parens[row] += 1\n            elif text in ')]}' and depth > 0:\n                # Parent indents should not be more than this one.\n                prev_indent = indent.pop() or last_indent[1]\n                hangs.pop()\n                for d in range(depth):\n                    if indent[d] > prev_indent:\n                        indent[d] = 0\n                for ind in list(indent_chances):\n                    if ind >= prev_indent:\n                        del indent_chances[ind]\n                del open_rows[depth + 1:]\n                depth -= 1\n                if depth:\n                    indent_chances[indent[depth]] = True\n                for idx in range(row, -1, -1):\n                    if parens[idx]:\n                        parens[idx] -= 1\n                        break\n            assert len(indent) == depth + 1\n            if (\n                start[1] not in indent_chances and\n                # This is for purposes of speeding up E121 (GitHub #90).\n                not last_line.rstrip().endswith(',')\n            ):\n                # Allow to line up tokens.\n                indent_chances[start[1]] = text\n\n        last_token_multiline = (start[0] != end[0])\n        if last_token_multiline:\n            rel_indent[end[0] - first_row] = rel_indent[row]\n\n        last_line = line\n\n    if (\n        indent_next and\n        not last_line_begins_with_multiline and\n        pycodestyle.expand_indent(line) == indent_level + DEFAULT_INDENT_SIZE\n    ):\n        pos = (start[0], indent[0] + 4)\n        desired_indent = indent_level + 2 * DEFAULT_INDENT_SIZE\n        if visual_indent:\n            yield (pos, 'E129 {}'.format(desired_indent))\n        else:\n            yield (pos, 'E125 {}'.format(desired_indent))\n\n\n# NOTE: need reload with runpy and call twice\n#   see: https://github.com/hhatto/autopep8/issues/625\nimportlib.reload(pycodestyle)\ndel pycodestyle._checks['logical_line'][pycodestyle.continued_indentation]\npycodestyle.register_check(extended_blank_lines)\npycodestyle.register_check(continued_indentation)\n\n\nclass FixPEP8(object):\n\n    \"\"\"Fix invalid code.\n\n    Fixer methods are prefixed \"fix_\". The _fix_source() method looks for these\n    automatically.\n\n    The fixer method can take either one or two arguments (in addition to\n    self). The first argument is \"result\", which is the error information from\n    pycodestyle. The second argument, \"logical\", is required only for\n    logical-line fixes.\n\n    The fixer method can return the list of modified lines or None. An empty\n    list would mean that no changes were made. None would mean that only the\n    line reported in the pycodestyle error was modified. Note that the modified\n    line numbers that are returned are indexed at 1. This typically would\n    correspond with the line number reported in the pycodestyle error\n    information.\n\n    [fixed method list]\n        - e111,e114,e115,e116\n        - e121,e122,e123,e124,e125,e126,e127,e128,e129\n        - e201,e202,e203\n        - e211\n        - e221,e222,e223,e224,e225\n        - e231\n        - e251,e252\n        - e261,e262\n        - e271,e272,e273,e274,e275\n        - e301,e302,e303,e304,e305,e306\n        - e401,e402\n        - e502\n        - e701,e702,e703,e704\n        - e711,e712,e713,e714\n        - e721,e722\n        - e731\n        - w291\n        - w503,504\n\n    \"\"\"\n\n    def __init__(self, filename,\n                 options,\n                 contents=None,\n                 long_line_ignore_cache=None):\n        self.filename = filename\n        if contents is None:\n            self.source = readlines_from_file(filename)\n        else:\n            sio = io.StringIO(contents)\n            self.source = sio.readlines()\n        self.options = options\n        self.indent_word = _get_indentword(''.join(self.source))\n        self.original_source = copy.copy(self.source)\n\n        # collect imports line\n        self.imports = {}\n        for i, line in enumerate(self.source):\n            if (line.find(\"import \") == 0 or line.find(\"from \") == 0) and \\\n                    line not in self.imports:\n                # collect only import statements that first appeared\n                self.imports[line] = i\n\n        self.long_line_ignore_cache = (\n            set() if long_line_ignore_cache is None\n            else long_line_ignore_cache)\n\n        # Many fixers are the same even though pycodestyle categorizes them\n        # differently.\n        self.fix_e115 = self.fix_e112\n        self.fix_e121 = self._fix_reindent\n        self.fix_e122 = self._fix_reindent\n        self.fix_e123 = self._fix_reindent\n        self.fix_e124 = self._fix_reindent\n        self.fix_e126 = self._fix_reindent\n        self.fix_e127 = self._fix_reindent\n        self.fix_e128 = self._fix_reindent\n        self.fix_e129 = self._fix_reindent\n        self.fix_e133 = self.fix_e131\n        self.fix_e202 = self.fix_e201\n        self.fix_e203 = self.fix_e201\n        self.fix_e204 = self.fix_e201\n        self.fix_e211 = self.fix_e201\n        self.fix_e221 = self.fix_e271\n        self.fix_e222 = self.fix_e271\n        self.fix_e223 = self.fix_e271\n        self.fix_e226 = self.fix_e225\n        self.fix_e227 = self.fix_e225\n        self.fix_e228 = self.fix_e225\n        self.fix_e241 = self.fix_e271\n        self.fix_e242 = self.fix_e224\n        self.fix_e252 = self.fix_e225\n        self.fix_e261 = self.fix_e262\n        self.fix_e272 = self.fix_e271\n        self.fix_e273 = self.fix_e271\n        self.fix_e274 = self.fix_e271\n        self.fix_e275 = self.fix_e271\n        self.fix_e306 = self.fix_e301\n        self.fix_e501 = (\n            self.fix_long_line_logically if\n            options and (options.aggressive >= 2 or options.experimental) else\n            self.fix_long_line_physically)\n        self.fix_e703 = self.fix_e702\n        self.fix_w292 = self.fix_w291\n        self.fix_w293 = self.fix_w291\n\n    def _check_affected_anothers(self, result) -> bool:\n        \"\"\"Check if the fix affects the number of lines of another remark.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n        original_target = self.original_source[line_index]\n        return target != original_target\n\n    def _fix_source(self, results):\n        try:\n            (logical_start, logical_end) = _find_logical(self.source)\n            logical_support = True\n        except (SyntaxError, tokenize.TokenError):  # pragma: no cover\n            logical_support = False\n\n        completed_lines = set()\n        for result in sorted(results, key=_priority_key):\n            if result['line'] in completed_lines:\n                continue\n\n            fixed_methodname = 'fix_' + result['id'].lower()\n            if hasattr(self, fixed_methodname):\n                fix = getattr(self, fixed_methodname)\n\n                line_index = result['line'] - 1\n                original_line = self.source[line_index]\n\n                is_logical_fix = len(_get_parameters(fix)) > 2\n                if is_logical_fix:\n                    logical = None\n                    if logical_support:\n                        logical = _get_logical(self.source,\n                                               result,\n                                               logical_start,\n                                               logical_end)\n                        if logical and set(range(\n                            logical[0][0] + 1,\n                            logical[1][0] + 1)).intersection(\n                                completed_lines):\n                            continue\n\n                    if self._check_affected_anothers(result):\n                        continue\n                    modified_lines = fix(result, logical)\n                else:\n                    if self._check_affected_anothers(result):\n                        continue\n                    modified_lines = fix(result)\n\n                if modified_lines is None:\n                    # Force logical fixes to report what they modified.\n                    assert not is_logical_fix\n\n                    if self.source[line_index] == original_line:\n                        modified_lines = []\n\n                if modified_lines:\n                    completed_lines.update(modified_lines)\n                elif modified_lines == []:  # Empty list means no fix\n                    if self.options.verbose >= 2:\n                        print(\n                            '--->  Not fixing {error} on line {line}'.format(\n                                error=result['id'], line=result['line']),\n                            file=sys.stderr)\n                else:  # We assume one-line fix when None.\n                    completed_lines.add(result['line'])\n            else:\n                if self.options.verbose >= 3:\n                    print(\n                        \"--->  '{}' is not defined.\".format(fixed_methodname),\n                        file=sys.stderr)\n\n                    info = result['info'].strip()\n                    print('--->  {}:{}:{}:{}'.format(self.filename,\n                                                     result['line'],\n                                                     result['column'],\n                                                     info),\n                          file=sys.stderr)\n\n    def fix(self):\n        \"\"\"Return a version of the source code with PEP 8 violations fixed.\"\"\"\n        pep8_options = {\n            'ignore': self.options.ignore,\n            'select': self.options.select,\n            'max_line_length': self.options.max_line_length,\n            'hang_closing': self.options.hang_closing,\n        }\n        results = _execute_pep8(pep8_options, self.source)\n\n        if self.options.verbose:\n            progress = {}\n            for r in results:\n                if r['id'] not in progress:\n                    progress[r['id']] = set()\n                progress[r['id']].add(r['line'])\n            print('--->  {n} issue(s) to fix {progress}'.format(\n                n=len(results), progress=progress), file=sys.stderr)\n\n        if self.options.line_range:\n            start, end = self.options.line_range\n            results = [r for r in results\n                       if start <= r['line'] <= end]\n\n        self._fix_source(filter_results(source=''.join(self.source),\n                                        results=results,\n                                        aggressive=self.options.aggressive))\n\n        if self.options.line_range:\n            # If number of lines has changed then change line_range.\n            count = sum(sline.count('\\n')\n                        for sline in self.source[start - 1:end])\n            self.options.line_range[1] = start + count - 1\n\n        return ''.join(self.source)\n\n    def _fix_reindent(self, result):\n        \"\"\"Fix a badly indented line.\n\n        This is done by adding or removing from its initial indent only.\n\n        \"\"\"\n        num_indent_spaces = int(result['info'].split()[1])\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        self.source[line_index] = ' ' * num_indent_spaces + target.lstrip()\n\n    def fix_e112(self, result):\n        \"\"\"Fix under-indented comments.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        if not target.lstrip().startswith('#'):\n            # Don't screw with invalid syntax.\n            return []\n\n        self.source[line_index] = self.indent_word + target\n\n    def fix_e113(self, result):\n        \"\"\"Fix unexpected indentation.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n        indent = _get_indentation(target)\n        stripped = target.lstrip()\n        self.source[line_index] = indent[1:] + stripped\n\n    def fix_e116(self, result):\n        \"\"\"Fix over-indented comments.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        indent = _get_indentation(target)\n        stripped = target.lstrip()\n\n        if not stripped.startswith('#'):\n            # Don't screw with invalid syntax.\n            return []\n\n        self.source[line_index] = indent[1:] + stripped\n\n    def fix_e117(self, result):\n        \"\"\"Fix over-indented.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        indent = _get_indentation(target)\n        if indent == '\\t':\n            return []\n\n        stripped = target.lstrip()\n\n        self.source[line_index] = indent[1:] + stripped\n\n    def fix_e125(self, result):\n        \"\"\"Fix indentation undistinguish from the next logical line.\"\"\"\n        num_indent_spaces = int(result['info'].split()[1])\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        spaces_to_add = num_indent_spaces - len(_get_indentation(target))\n        indent = len(_get_indentation(target))\n        modified_lines = []\n\n        while len(_get_indentation(self.source[line_index])) >= indent:\n            self.source[line_index] = (' ' * spaces_to_add +\n                                       self.source[line_index])\n            modified_lines.append(1 + line_index)  # Line indexed at 1.\n            line_index -= 1\n\n        return modified_lines\n\n    def fix_e131(self, result):\n        \"\"\"Fix indentation undistinguish from the next logical line.\"\"\"\n        num_indent_spaces = int(result['info'].split()[1])\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        spaces_to_add = num_indent_spaces - len(_get_indentation(target))\n\n        indent_length = len(_get_indentation(target))\n        spaces_to_add = num_indent_spaces - indent_length\n        if num_indent_spaces == 0 and indent_length == 0:\n            spaces_to_add = 4\n\n        if spaces_to_add >= 0:\n            self.source[line_index] = (' ' * spaces_to_add +\n                                       self.source[line_index])\n        else:\n            offset = abs(spaces_to_add)\n            self.source[line_index] = self.source[line_index][offset:]\n\n    def fix_e201(self, result):\n        \"\"\"Remove extraneous whitespace.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n        offset = result['column'] - 1\n\n        fixed = fix_whitespace(target,\n                               offset=offset,\n                               replacement='')\n\n        self.source[line_index] = fixed\n\n    def fix_e224(self, result):\n        \"\"\"Remove extraneous whitespace around operator.\"\"\"\n        target = self.source[result['line'] - 1]\n        offset = result['column'] - 1\n        fixed = target[:offset] + target[offset:].replace('\\t', ' ')\n        self.source[result['line'] - 1] = fixed\n\n    def fix_e225(self, result):\n        \"\"\"Fix missing whitespace around operator.\"\"\"\n        target = self.source[result['line'] - 1]\n        offset = result['column'] - 1\n        fixed = target[:offset] + ' ' + target[offset:]\n\n        # Only proceed if non-whitespace characters match.\n        # And make sure we don't break the indentation.\n        if (\n            fixed.replace(' ', '') == target.replace(' ', '') and\n            _get_indentation(fixed) == _get_indentation(target)\n        ):\n            self.source[result['line'] - 1] = fixed\n            error_code = result.get('id', 0)\n            try:\n                ts = generate_tokens(fixed)\n            except (SyntaxError, tokenize.TokenError):\n                return\n            if not check_syntax(fixed.lstrip()):\n                return\n            try:\n                _missing_whitespace = (\n                    pycodestyle.missing_whitespace_around_operator\n                )\n            except AttributeError:\n                # pycodestyle >= 2.11.0\n                _missing_whitespace = pycodestyle.missing_whitespace\n            errors = list(_missing_whitespace(fixed, ts))\n            for e in reversed(errors):\n                if error_code != e[1].split()[0]:\n                    continue\n                offset = e[0][1]\n                fixed = fixed[:offset] + ' ' + fixed[offset:]\n            self.source[result['line'] - 1] = fixed\n        else:\n            return []\n\n    def fix_e231(self, result):\n        \"\"\"Add missing whitespace.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n        offset = result['column']\n        fixed = target[:offset].rstrip() + ' ' + target[offset:].lstrip()\n        self.source[line_index] = fixed\n\n    def fix_e251(self, result):\n        \"\"\"Remove whitespace around parameter '=' sign.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        # This is necessary since pycodestyle sometimes reports columns that\n        # goes past the end of the physical line. This happens in cases like,\n        # foo(bar\\n=None)\n        c = min(result['column'] - 1,\n                len(target) - 1)\n\n        if target[c].strip():\n            fixed = target\n        else:\n            fixed = target[:c].rstrip() + target[c:].lstrip()\n\n        # There could be an escaped newline\n        #\n        #     def foo(a=\\\n        #             1)\n        if fixed.endswith(('=\\\\\\n', '=\\\\\\r\\n', '=\\\\\\r')):\n            self.source[line_index] = fixed.rstrip('\\n\\r \\t\\\\')\n            self.source[line_index + 1] = self.source[line_index + 1].lstrip()\n            return [line_index + 1, line_index + 2]  # Line indexed at 1\n\n        self.source[result['line'] - 1] = fixed\n\n    def fix_e262(self, result):\n        \"\"\"Fix spacing after inline comment hash.\"\"\"\n        target = self.source[result['line'] - 1]\n        offset = result['column']\n\n        code = target[:offset].rstrip(' \\t#')\n        comment = target[offset:].lstrip(' \\t#')\n\n        fixed = code + ('  # ' + comment if comment.strip() else '\\n')\n\n        self.source[result['line'] - 1] = fixed\n\n    def fix_e265(self, result):\n        \"\"\"Fix spacing after block comment hash.\"\"\"\n        target = self.source[result['line'] - 1]\n\n        indent = _get_indentation(target)\n        line = target.lstrip(' \\t')\n        pos = next((index for index, c in enumerate(line) if c != '#'))\n        hashes = line[:pos]\n        comment = line[pos:].lstrip(' \\t')\n\n        # Ignore special comments, even in the middle of the file.\n        if comment.startswith('!'):\n            return\n\n        fixed = indent + hashes + (' ' + comment if comment.strip() else '\\n')\n\n        self.source[result['line'] - 1] = fixed\n\n    def fix_e266(self, result):\n        \"\"\"Fix too many block comment hashes.\"\"\"\n        target = self.source[result['line'] - 1]\n\n        # Leave stylistic outlined blocks alone.\n        if target.strip().endswith('#'):\n            return\n\n        indentation = _get_indentation(target)\n        fixed = indentation + '# ' + target.lstrip('# \\t')\n\n        self.source[result['line'] - 1] = fixed\n\n    def fix_e271(self, result):\n        \"\"\"Fix extraneous whitespace around keywords.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n        offset = result['column'] - 1\n\n        fixed = fix_whitespace(target,\n                               offset=offset,\n                               replacement=' ')\n\n        if fixed == target:\n            return []\n        else:\n            self.source[line_index] = fixed\n\n    def fix_e301(self, result):\n        \"\"\"Add missing blank line.\"\"\"\n        cr = '\\n'\n        self.source[result['line'] - 1] = cr + self.source[result['line'] - 1]\n\n    def fix_e302(self, result):\n        \"\"\"Add missing 2 blank lines.\"\"\"\n        add_linenum = 2 - int(result['info'].split()[-1])\n        offset = 1\n        if self.source[result['line'] - 2].strip() == \"\\\\\":\n            offset = 2\n        cr = '\\n' * add_linenum\n        self.source[result['line'] - offset] = (\n            cr + self.source[result['line'] - offset]\n        )\n\n    def fix_e303(self, result):\n        \"\"\"Remove extra blank lines.\"\"\"\n        delete_linenum = int(result['info'].split('(')[1].split(')')[0]) - 2\n        delete_linenum = max(1, delete_linenum)\n\n        # We need to count because pycodestyle reports an offset line number if\n        # there are comments.\n        cnt = 0\n        line = result['line'] - 2\n        modified_lines = []\n        while cnt < delete_linenum and line >= 0:\n            if not self.source[line].strip():\n                self.source[line] = ''\n                modified_lines.append(1 + line)  # Line indexed at 1\n                cnt += 1\n            line -= 1\n\n        return modified_lines\n\n    def fix_e304(self, result):\n        \"\"\"Remove blank line following function decorator.\"\"\"\n        line = result['line'] - 2\n        if not self.source[line].strip():\n            self.source[line] = ''\n\n    def fix_e305(self, result):\n        \"\"\"Add missing 2 blank lines after end of function or class.\"\"\"\n        add_delete_linenum = 2 - int(result['info'].split()[-1])\n        cnt = 0\n        offset = result['line'] - 2\n        modified_lines = []\n        if add_delete_linenum < 0:\n            # delete cr\n            add_delete_linenum = abs(add_delete_linenum)\n            while cnt < add_delete_linenum and offset >= 0:\n                if not self.source[offset].strip():\n                    self.source[offset] = ''\n                    modified_lines.append(1 + offset)  # Line indexed at 1\n                    cnt += 1\n                offset -= 1\n        else:\n            # add cr\n            cr = '\\n'\n            # check comment line\n            while True:\n                if offset < 0:\n                    break\n                line = self.source[offset].lstrip()\n                if not line:\n                    break\n                if line[0] != '#':\n                    break\n                offset -= 1\n            offset += 1\n            self.source[offset] = cr + self.source[offset]\n            modified_lines.append(1 + offset)   # Line indexed at 1.\n        return modified_lines\n\n    def fix_e401(self, result):\n        \"\"\"Put imports on separate lines.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n        offset = result['column'] - 1\n\n        if not target.lstrip().startswith('import'):\n            return []\n\n        indentation = re.split(pattern=r'\\bimport\\b',\n                               string=target, maxsplit=1)[0]\n        fixed = (target[:offset].rstrip('\\t ,') + '\\n' +\n                 indentation + 'import ' + target[offset:].lstrip('\\t ,'))\n        self.source[line_index] = fixed\n\n    def fix_e402(self, result):\n        (line_index, offset, target) = get_index_offset_contents(result,\n                                                                 self.source)\n        for i in range(1, 100):\n            line = \"\".join(self.source[line_index:line_index+i])\n            try:\n                generate_tokens(\"\".join(line))\n            except (SyntaxError, tokenize.TokenError):\n                continue\n            break\n        if not (target in self.imports and self.imports[target] != line_index):\n            mod_offset = get_module_imports_on_top_of_file(self.source,\n                                                           line_index)\n            self.source[mod_offset] = line + self.source[mod_offset]\n        for offset in range(i):\n            self.source[line_index+offset] = ''\n\n    def fix_long_line_logically(self, result, logical):\n        \"\"\"Try to make lines fit within --max-line-length characters.\"\"\"\n        if (\n            not logical or\n            len(logical[2]) == 1 or\n            self.source[result['line'] - 1].lstrip().startswith('#')\n        ):\n            return self.fix_long_line_physically(result)\n\n        start_line_index = logical[0][0]\n        end_line_index = logical[1][0]\n        logical_lines = logical[2]\n\n        previous_line = get_item(self.source, start_line_index - 1, default='')\n        next_line = get_item(self.source, end_line_index + 1, default='')\n\n        single_line = join_logical_line(''.join(logical_lines))\n\n        try:\n            fixed = self.fix_long_line(\n                target=single_line,\n                previous_line=previous_line,\n                next_line=next_line,\n                original=''.join(logical_lines))\n        except (SyntaxError, tokenize.TokenError):\n            return self.fix_long_line_physically(result)\n\n        if fixed:\n            for line_index in range(start_line_index, end_line_index + 1):\n                self.source[line_index] = ''\n            self.source[start_line_index] = fixed\n            return range(start_line_index + 1, end_line_index + 1)\n\n        return []\n\n    def fix_long_line_physically(self, result):\n        \"\"\"Try to make lines fit within --max-line-length characters.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        previous_line = get_item(self.source, line_index - 1, default='')\n        next_line = get_item(self.source, line_index + 1, default='')\n\n        try:\n            fixed = self.fix_long_line(\n                target=target,\n                previous_line=previous_line,\n                next_line=next_line,\n                original=target)\n        except (SyntaxError, tokenize.TokenError):\n            return []\n\n        if fixed:\n            self.source[line_index] = fixed\n            return [line_index + 1]\n\n        return []\n\n    def fix_long_line(self, target, previous_line,\n                      next_line, original):\n        cache_entry = (target, previous_line, next_line)\n        if cache_entry in self.long_line_ignore_cache:\n            return []\n\n        if target.lstrip().startswith('#'):\n            if self.options.aggressive:\n                # Wrap commented lines.\n                return shorten_comment(\n                    line=target,\n                    max_line_length=self.options.max_line_length,\n                    last_comment=not next_line.lstrip().startswith('#'))\n            return []\n\n        fixed = get_fixed_long_line(\n            target=target,\n            previous_line=previous_line,\n            original=original,\n            indent_word=self.indent_word,\n            max_line_length=self.options.max_line_length,\n            aggressive=self.options.aggressive,\n            experimental=self.options.experimental,\n            verbose=self.options.verbose)\n\n        if fixed and not code_almost_equal(original, fixed):\n            return fixed\n\n        self.long_line_ignore_cache.add(cache_entry)\n        return None\n\n    def fix_e502(self, result):\n        \"\"\"Remove extraneous escape of newline.\"\"\"\n        (line_index, _, target) = get_index_offset_contents(result,\n                                                            self.source)\n        self.source[line_index] = target.rstrip('\\n\\r \\t\\\\') + '\\n'\n\n    def fix_e701(self, result):\n        \"\"\"Put colon-separated compound statement on separate lines.\"\"\"\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n        c = result['column']\n\n        fixed_source = (target[:c] + '\\n' +\n                        _get_indentation(target) + self.indent_word +\n                        target[c:].lstrip('\\n\\r \\t\\\\'))\n        self.source[result['line'] - 1] = fixed_source\n        return [result['line'], result['line'] + 1]\n\n    def fix_e702(self, result, logical):\n        \"\"\"Put semicolon-separated compound statement on separate lines.\"\"\"\n        if not logical:\n            return []  # pragma: no cover\n        logical_lines = logical[2]\n\n        # Avoid applying this when indented.\n        # https://docs.python.org/reference/compound_stmts.html\n        for line in logical_lines:\n            if (\n                result['id'] == 'E702'\n                and ':' in line\n                and pycodestyle.STARTSWITH_INDENT_STATEMENT_REGEX.match(line)\n            ):\n                if self.options.verbose:\n                    print(\n                        '---> avoid fixing {error} with '\n                        'other compound statements'.format(error=result['id']),\n                        file=sys.stderr\n                    )\n                return []\n\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        if target.rstrip().endswith('\\\\'):\n            # Normalize '1; \\\\\\n2' into '1; 2'.\n            self.source[line_index] = target.rstrip('\\n \\r\\t\\\\')\n            self.source[line_index + 1] = self.source[line_index + 1].lstrip()\n            return [line_index + 1, line_index + 2]\n\n        if target.rstrip().endswith(';'):\n            self.source[line_index] = target.rstrip('\\n \\r\\t;') + '\\n'\n            return [line_index + 1]\n\n        offset = result['column'] - 1\n        first = target[:offset].rstrip(';').rstrip()\n        second = (_get_indentation(logical_lines[0]) +\n                  target[offset:].lstrip(';').lstrip())\n\n        # Find inline comment.\n        inline_comment = None\n        if target[offset:].lstrip(';').lstrip()[:2] == '# ':\n            inline_comment = target[offset:].lstrip(';')\n\n        if inline_comment:\n            self.source[line_index] = first + inline_comment\n        else:\n            self.source[line_index] = first + '\\n' + second\n        return [line_index + 1]\n\n    def fix_e704(self, result):\n        \"\"\"Fix multiple statements on one line def\"\"\"\n        (line_index, _, target) = get_index_offset_contents(result,\n                                                            self.source)\n        match = STARTSWITH_DEF_REGEX.match(target)\n        if match:\n            self.source[line_index] = '{}\\n{}{}'.format(\n                match.group(0),\n                _get_indentation(target) + self.indent_word,\n                target[match.end(0):].lstrip())\n\n    def fix_e711(self, result):\n        \"\"\"Fix comparison with None.\"\"\"\n        (line_index, offset, target) = get_index_offset_contents(result,\n                                                                 self.source)\n\n        right_offset = offset + 2\n        if right_offset >= len(target):\n            return []\n\n        left = target[:offset].rstrip()\n        center = target[offset:right_offset]\n        right = target[right_offset:].lstrip()\n\n        if center.strip() == '==':\n            new_center = 'is'\n        elif center.strip() == '!=':\n            new_center = 'is not'\n        else:\n            return []\n\n        self.source[line_index] = ' '.join([left, new_center, right])\n\n    def fix_e712(self, result):\n        \"\"\"Fix (trivial case of) comparison with boolean.\"\"\"\n        (line_index, offset, target) = get_index_offset_contents(result,\n                                                                 self.source)\n\n        # Handle very easy \"not\" special cases.\n        if re.match(r'^\\s*if [\\w.\"\\'\\[\\]]+ == False:$', target):\n            self.source[line_index] = re.sub(r'if ([\\w.\"\\'\\[\\]]+) == False:',\n                                             r'if not \\1:', target, count=1)\n        elif re.match(r'^\\s*if [\\w.\"\\'\\[\\]]+ != True:$', target):\n            self.source[line_index] = re.sub(r'if ([\\w.\"\\'\\[\\]]+) != True:',\n                                             r'if not \\1:', target, count=1)\n        else:\n            right_offset = offset + 2\n            if right_offset >= len(target):\n                return []\n\n            left = target[:offset].rstrip()\n            center = target[offset:right_offset]\n            right = target[right_offset:].lstrip()\n\n            # Handle simple cases only.\n            new_right = None\n            if center.strip() == '==':\n                if re.match(r'\\bTrue\\b', right):\n                    new_right = re.sub(r'\\bTrue\\b *', '', right, count=1)\n            elif center.strip() == '!=':\n                if re.match(r'\\bFalse\\b', right):\n                    new_right = re.sub(r'\\bFalse\\b *', '', right, count=1)\n\n            if new_right is None:\n                return []\n\n            if new_right[0].isalnum():\n                new_right = ' ' + new_right\n\n            self.source[line_index] = left + new_right\n\n    def fix_e713(self, result):\n        \"\"\"Fix (trivial case of) non-membership check.\"\"\"\n        (line_index, offset, target) = get_index_offset_contents(result,\n                                                                 self.source)\n\n        # to convert once 'not in' -> 'in'\n        before_target = target[:offset]\n        target = target[offset:]\n        match_notin = COMPARE_NEGATIVE_REGEX_THROUGH.search(target)\n        notin_pos_start, notin_pos_end = 0, 0\n        if match_notin:\n            notin_pos_start = match_notin.start(1)\n            notin_pos_end = match_notin.end()\n            target = '{}{} {}'.format(\n                target[:notin_pos_start], 'in', target[notin_pos_end:])\n\n        # fix 'not in'\n        match = COMPARE_NEGATIVE_REGEX.search(target)\n        if match:\n            if match.group(3) == 'in':\n                pos_start = match.start(1)\n                new_target = '{5}{0}{1} {2} {3} {4}'.format(\n                    target[:pos_start], match.group(2), match.group(1),\n                    match.group(3), target[match.end():], before_target)\n                if match_notin:\n                    # revert 'in' -> 'not in'\n                    pos_start = notin_pos_start + offset\n                    pos_end = notin_pos_end + offset - 4     # len('not ')\n                    new_target = '{}{} {}'.format(\n                        new_target[:pos_start], 'not in', new_target[pos_end:])\n                self.source[line_index] = new_target\n\n    def fix_e714(self, result):\n        \"\"\"Fix object identity should be 'is not' case.\"\"\"\n        (line_index, offset, target) = get_index_offset_contents(result,\n                                                                 self.source)\n\n        # to convert once 'is not' -> 'is'\n        before_target = target[:offset]\n        target = target[offset:]\n        match_isnot = COMPARE_NEGATIVE_REGEX_THROUGH.search(target)\n        isnot_pos_start, isnot_pos_end = 0, 0\n        if match_isnot:\n            isnot_pos_start = match_isnot.start(1)\n            isnot_pos_end = match_isnot.end()\n            target = '{}{} {}'.format(\n                target[:isnot_pos_start], 'in', target[isnot_pos_end:])\n\n        match = COMPARE_NEGATIVE_REGEX.search(target)\n        if match:\n            if match.group(3).startswith('is'):\n                pos_start = match.start(1)\n                new_target = '{5}{0}{1} {2} {3} {4}'.format(\n                    target[:pos_start], match.group(2), match.group(3),\n                    match.group(1), target[match.end():], before_target)\n                if match_isnot:\n                    # revert 'is' -> 'is not'\n                    pos_start = isnot_pos_start + offset\n                    pos_end = isnot_pos_end + offset - 4     # len('not ')\n                    new_target = '{}{} {}'.format(\n                        new_target[:pos_start], 'is not', new_target[pos_end:])\n                self.source[line_index] = new_target\n\n    def fix_e721(self, result):\n        \"\"\"fix comparison type\"\"\"\n        (line_index, _, target) = get_index_offset_contents(result,\n                                                            self.source)\n        match = COMPARE_TYPE_REGEX.search(target)\n        if match:\n            # NOTE: match objects\n            #  * type(a) == type(b)  -> (None, None, 'a', '==')\n            #  * str == type(b)      -> ('==', 'b', None, None)\n            #  * type(\"\") != type(b) -> (None, None, '\"\"', '!=')\n            start = match.start()\n            end = match.end()\n            _prefix = \"\"\n            _suffix = \"\"\n            first_match_type_obj = match.groups()[1]\n            if first_match_type_obj is None:\n                _target_obj = match.groups()[2]\n            else:\n                _target_obj = match.groups()[1]\n                _suffix = target[end:]\n\n            isinstance_stmt = \" isinstance\"\n            is_not_condition = (\n                match.groups()[0] == \"!=\" or match.groups()[3] == \"!=\"\n            )\n            if is_not_condition:\n                isinstance_stmt = \" not isinstance\"\n\n            _type_comp = f\"{_target_obj}, {target[:start]}\"\n\n            _prefix_tmp = target[:start].split()\n            if len(_prefix_tmp) >= 1:\n                _type_comp = f\"{_target_obj}, {target[:start]}\"\n                if first_match_type_obj is not None:\n                    _prefix = \" \".join(_prefix_tmp[:-1])\n                    _type_comp = f\"{_target_obj}, {_prefix_tmp[-1]}\"\n                else:\n                    _prefix = \" \".join(_prefix_tmp)\n\n            _suffix_tmp = target[end:]\n            _suffix_type_match = TYPE_REGEX.search(_suffix_tmp)\n            if len(_suffix_tmp.split()) >= 1 and _suffix_type_match:\n                if _suffix_type_match:\n                    type_match_end = _suffix_type_match.end()\n                    _suffix = _suffix_tmp[type_match_end:]\n            if _suffix_type_match:\n                cmp_b = _suffix_type_match.groups()[0]\n                _type_comp = f\"{_target_obj}, {cmp_b}\"\n\n            fix_line = f\"{_prefix}{isinstance_stmt}({_type_comp}){_suffix}\"\n            self.source[line_index] = fix_line\n\n    def fix_e722(self, result):\n        \"\"\"fix bare except\"\"\"\n        (line_index, _, target) = get_index_offset_contents(result,\n                                                            self.source)\n        match = BARE_EXCEPT_REGEX.search(target)\n        if match:\n            self.source[line_index] = '{}{}{}'.format(\n                target[:result['column'] - 1], \"except BaseException:\",\n                target[match.end():])\n\n    def fix_e731(self, result):\n        \"\"\"Fix do not assign a lambda expression check.\"\"\"\n        (line_index, _, target) = get_index_offset_contents(result,\n                                                            self.source)\n        match = LAMBDA_REGEX.search(target)\n        if match:\n            end = match.end()\n            self.source[line_index] = '{}def {}({}): return {}'.format(\n                target[:match.start(0)], match.group(1), match.group(2),\n                target[end:].lstrip())\n\n    def fix_w291(self, result):\n        \"\"\"Remove trailing whitespace.\"\"\"\n        fixed_line = self.source[result['line'] - 1].rstrip()\n        self.source[result['line'] - 1] = fixed_line + '\\n'\n\n    def fix_w391(self, _):\n        \"\"\"Remove trailing blank lines.\"\"\"\n        blank_count = 0\n        for line in reversed(self.source):\n            line = line.rstrip()\n            if line:\n                break\n            else:\n                blank_count += 1\n\n        original_length = len(self.source)\n        self.source = self.source[:original_length - blank_count]\n        return range(1, 1 + original_length)\n\n    def fix_w503(self, result):\n        (line_index, _, target) = get_index_offset_contents(result,\n                                                            self.source)\n        one_string_token = target.split()[0]\n        try:\n            ts = generate_tokens(one_string_token)\n        except (SyntaxError, tokenize.TokenError):\n            return\n        if not _is_binary_operator(ts[0][0], one_string_token):\n            return\n        # find comment\n        comment_index = 0\n        found_not_comment_only_line = False\n        comment_only_linenum = 0\n        for i in range(5):\n            # NOTE: try to parse code in 5 times\n            if (line_index - i) < 0:\n                break\n            from_index = line_index - i - 1\n            if from_index < 0 or len(self.source) <= from_index:\n                break\n            to_index = line_index + 1\n            strip_line = self.source[from_index].lstrip()\n            if (\n                not found_not_comment_only_line and\n                strip_line and strip_line[0] == '#'\n            ):\n                comment_only_linenum += 1\n                continue\n            found_not_comment_only_line = True\n            try:\n                ts = generate_tokens(\"\".join(self.source[from_index:to_index]))\n            except (SyntaxError, tokenize.TokenError):\n                continue\n            newline_count = 0\n            newline_index = []\n            for index, t in enumerate(ts):\n                if t[0] in (tokenize.NEWLINE, tokenize.NL):\n                    newline_index.append(index)\n                    newline_count += 1\n            if newline_count > 2:\n                tts = ts[newline_index[-3]:]\n            else:\n                tts = ts\n            old = []\n            for t in tts:\n                if t[0] in (tokenize.NEWLINE, tokenize.NL):\n                    newline_count -= 1\n                if newline_count <= 1:\n                    break\n                if tokenize.COMMENT == t[0] and old and old[0] != tokenize.NL:\n                    comment_index = old[3][1]\n                    break\n                old = t\n            break\n        i = target.index(one_string_token)\n        fix_target_line = line_index - 1 - comment_only_linenum\n        self.source[line_index] = '{}{}'.format(\n            target[:i], target[i + len(one_string_token):].lstrip())\n        nl = find_newline(self.source[fix_target_line:line_index])\n        before_line = self.source[fix_target_line]\n        bl = before_line.index(nl)\n        if comment_index:\n            self.source[fix_target_line] = '{} {} {}'.format(\n                before_line[:comment_index], one_string_token,\n                before_line[comment_index + 1:])\n        else:\n            if before_line[:bl].endswith(\"#\"):\n                # special case\n                # see: https://github.com/hhatto/autopep8/issues/503\n                self.source[fix_target_line] = '{}{} {}'.format(\n                    before_line[:bl-2], one_string_token, before_line[bl-2:])\n            else:\n                self.source[fix_target_line] = '{} {}{}'.format(\n                    before_line[:bl], one_string_token, before_line[bl:])\n\n    def fix_w504(self, result):\n        (line_index, _, target) = get_index_offset_contents(result,\n                                                            self.source)\n        # NOTE: is not collect pointed out in pycodestyle==2.4.0\n        comment_index = 0\n        operator_position = None    # (start_position, end_position)\n        for i in range(1, 6):\n            to_index = line_index + i\n            try:\n                ts = generate_tokens(\"\".join(self.source[line_index:to_index]))\n            except (SyntaxError, tokenize.TokenError):\n                continue\n            newline_count = 0\n            newline_index = []\n            for index, t in enumerate(ts):\n                if _is_binary_operator(t[0], t[1]):\n                    if t[2][0] == 1 and t[3][0] == 1:\n                        operator_position = (t[2][1], t[3][1])\n                elif t[0] == tokenize.NAME and t[1] in (\"and\", \"or\"):\n                    if t[2][0] == 1 and t[3][0] == 1:\n                        operator_position = (t[2][1], t[3][1])\n                elif t[0] in (tokenize.NEWLINE, tokenize.NL):\n                    newline_index.append(index)\n                    newline_count += 1\n            if newline_count > 2:\n                tts = ts[:newline_index[-3]]\n            else:\n                tts = ts\n            old = []\n            for t in tts:\n                if tokenize.COMMENT == t[0] and old:\n                    comment_row, comment_index = old[3]\n                    break\n                old = t\n            break\n        if not operator_position:\n            return\n        target_operator = target[operator_position[0]:operator_position[1]]\n\n        if comment_index and comment_row == 1:\n            self.source[line_index] = '{}{}'.format(\n                target[:operator_position[0]].rstrip(),\n                target[comment_index:])\n        else:\n            self.source[line_index] = '{}{}{}'.format(\n                target[:operator_position[0]].rstrip(),\n                target[operator_position[1]:].lstrip(),\n                target[operator_position[1]:])\n\n        next_line = self.source[line_index + 1]\n        next_line_indent = 0\n        m = re.match(r'\\s*', next_line)\n        if m:\n            next_line_indent = m.span()[1]\n        self.source[line_index + 1] = '{}{} {}'.format(\n            next_line[:next_line_indent], target_operator,\n            next_line[next_line_indent:])\n\n    def fix_w605(self, result):\n        (line_index, offset, target) = get_index_offset_contents(result,\n                                                                 self.source)\n        self.source[line_index] = '{}\\\\{}'.format(\n            target[:offset + 1], target[offset + 1:])\n\n\ndef get_module_imports_on_top_of_file(source, import_line_index):\n    \"\"\"return import or from keyword position\n\n    example:\n      > 0: import sys\n        1: import os\n        2:\n        3: def function():\n    \"\"\"\n    def is_string_literal(line):\n        if line[0] in 'uUbB':\n            line = line[1:]\n        if line and line[0] in 'rR':\n            line = line[1:]\n        return line and (line[0] == '\"' or line[0] == \"'\")\n\n    def is_future_import(line):\n        nodes = ast.parse(line)\n        for n in nodes.body:\n            if isinstance(n, ast.ImportFrom) and n.module == '__future__':\n                return True\n        return False\n\n    def has_future_import(source):\n        offset = 0\n        line = ''\n        for _, next_line in source:\n            for line_part in next_line.strip().splitlines(True):\n                line = line + line_part\n                try:\n                    return is_future_import(line), offset\n                except SyntaxError:\n                    continue\n            offset += 1\n        return False, offset\n\n    allowed_try_keywords = ('try', 'except', 'else', 'finally')\n    in_docstring = False\n    docstring_kind = '\"\"\"'\n    source_stream = iter(enumerate(source))\n    for cnt, line in source_stream:\n        if not in_docstring:\n            m = DOCSTRING_START_REGEX.match(line.lstrip())\n            if m is not None:\n                in_docstring = True\n                docstring_kind = m.group('kind')\n                remain = line[m.end(): m.endpos].rstrip()\n                if remain[-3:] == docstring_kind:  # one line doc\n                    in_docstring = False\n                continue\n        if in_docstring:\n            if line.rstrip()[-3:] == docstring_kind:\n                in_docstring = False\n            continue\n\n        if not line.rstrip():\n            continue\n        elif line.startswith('#'):\n            continue\n\n        if line.startswith('import '):\n            if cnt == import_line_index:\n                continue\n            return cnt\n        elif line.startswith('from '):\n            if cnt == import_line_index:\n                continue\n            hit, offset = has_future_import(\n                itertools.chain([(cnt, line)], source_stream)\n            )\n            if hit:\n                # move to the back\n                return cnt + offset + 1\n            return cnt\n        elif pycodestyle.DUNDER_REGEX.match(line):\n            return cnt\n        elif any(line.startswith(kw) for kw in allowed_try_keywords):\n            continue\n        elif is_string_literal(line):\n            return cnt\n        else:\n            return cnt\n    return 0\n\n\ndef get_index_offset_contents(result, source):\n    \"\"\"Return (line_index, column_offset, line_contents).\"\"\"\n    line_index = result['line'] - 1\n    return (line_index,\n            result['column'] - 1,\n            source[line_index])\n\n\ndef get_fixed_long_line(target, previous_line, original,\n                        indent_word='    ', max_line_length=79,\n                        aggressive=0, experimental=False, verbose=False):\n    \"\"\"Break up long line and return result.\n\n    Do this by generating multiple reformatted candidates and then\n    ranking the candidates to heuristically select the best option.\n\n    \"\"\"\n    indent = _get_indentation(target)\n    source = target[len(indent):]\n    assert source.lstrip() == source\n    assert not target.lstrip().startswith('#')\n\n    # Check for partial multiline.\n    tokens = list(generate_tokens(source))\n\n    candidates = shorten_line(\n        tokens, source, indent,\n        indent_word,\n        max_line_length,\n        aggressive=aggressive,\n        experimental=experimental,\n        previous_line=previous_line)\n\n    # Also sort alphabetically as a tie breaker (for determinism).\n    candidates = sorted(\n        sorted(set(candidates).union([target, original])),\n        key=lambda x: line_shortening_rank(\n            x,\n            indent_word,\n            max_line_length,\n            experimental=experimental))\n\n    if verbose >= 4:\n        print(('-' * 79 + '\\n').join([''] + candidates + ['']),\n              file=wrap_output(sys.stderr, 'utf-8'))\n\n    if candidates:\n        best_candidate = candidates[0]\n\n        # Don't allow things to get longer.\n        if longest_line_length(best_candidate) > longest_line_length(original):\n            return None\n\n        return best_candidate\n\n\ndef longest_line_length(code):\n    \"\"\"Return length of longest line.\"\"\"\n    if len(code) == 0:\n        return 0\n    return max(len(line) for line in code.splitlines())\n\n\ndef join_logical_line(logical_line):\n    \"\"\"Return single line based on logical line input.\"\"\"\n    indentation = _get_indentation(logical_line)\n\n    return indentation + untokenize_without_newlines(\n        generate_tokens(logical_line.lstrip())) + '\\n'\n\n\ndef untokenize_without_newlines(tokens):\n    \"\"\"Return source code based on tokens.\"\"\"\n    text = ''\n    last_row = 0\n    last_column = -1\n\n    for t in tokens:\n        token_string = t[1]\n        (start_row, start_column) = t[2]\n        (end_row, end_column) = t[3]\n\n        if start_row > last_row:\n            last_column = 0\n        if (\n            (start_column > last_column or token_string == '\\n') and\n            not text.endswith(' ')\n        ):\n            text += ' '\n\n        if token_string != '\\n':\n            text += token_string\n\n        last_row = end_row\n        last_column = end_column\n\n    return text.rstrip()\n\n\ndef _find_logical(source_lines):\n    # Make a variable which is the index of all the starts of lines.\n    logical_start = []\n    logical_end = []\n    last_newline = True\n    parens = 0\n    for t in generate_tokens(''.join(source_lines)):\n        if t[0] in [tokenize.COMMENT, tokenize.DEDENT,\n                    tokenize.INDENT, tokenize.NL,\n                    tokenize.ENDMARKER]:\n            continue\n        if not parens and t[0] in [tokenize.NEWLINE, tokenize.SEMI]:\n            last_newline = True\n            logical_end.append((t[3][0] - 1, t[2][1]))\n            continue\n        if last_newline and not parens:\n            logical_start.append((t[2][0] - 1, t[2][1]))\n            last_newline = False\n        if t[0] == tokenize.OP:\n            if t[1] in '([{':\n                parens += 1\n            elif t[1] in '}])':\n                parens -= 1\n    return (logical_start, logical_end)\n\n\ndef _get_logical(source_lines, result, logical_start, logical_end):\n    \"\"\"Return the logical line corresponding to the result.\n\n    Assumes input is already E702-clean.\n\n    \"\"\"\n    row = result['line'] - 1\n    col = result['column'] - 1\n    ls = None\n    le = None\n    for i in range(0, len(logical_start), 1):\n        assert logical_end\n        x = logical_end[i]\n        if x[0] > row or (x[0] == row and x[1] > col):\n            le = x\n            ls = logical_start[i]\n            break\n    if ls is None:\n        return None\n    original = source_lines[ls[0]:le[0] + 1]\n    return ls, le, original\n\n\ndef get_item(items, index, default=None):\n    if 0 <= index < len(items):\n        return items[index]\n\n    return default\n\n\ndef reindent(source, indent_size, leave_tabs=False):\n    \"\"\"Reindent all lines.\"\"\"\n    reindenter = Reindenter(source, leave_tabs)\n    return reindenter.run(indent_size)\n\n\ndef code_almost_equal(a, b):\n    \"\"\"Return True if code is similar.\n\n    Ignore whitespace when comparing specific line.\n\n    \"\"\"\n    split_a = split_and_strip_non_empty_lines(a)\n    split_b = split_and_strip_non_empty_lines(b)\n\n    if len(split_a) != len(split_b):\n        return False\n\n    for (index, _) in enumerate(split_a):\n        if ''.join(split_a[index].split()) != ''.join(split_b[index].split()):\n            return False\n\n    return True\n\n\ndef split_and_strip_non_empty_lines(text):\n    \"\"\"Return lines split by newline.\n\n    Ignore empty lines.\n\n    \"\"\"\n    return [line.strip() for line in text.splitlines() if line.strip()]\n\n\ndef find_newline(source):\n    \"\"\"Return type of newline used in source.\n\n    Input is a list of lines.\n\n    \"\"\"\n    assert not isinstance(source, str)\n\n    counter = collections.defaultdict(int)\n    for line in source:\n        if line.endswith(CRLF):\n            counter[CRLF] += 1\n        elif line.endswith(CR):\n            counter[CR] += 1\n        elif line.endswith(LF):\n            counter[LF] += 1\n\n    return (sorted(counter, key=counter.get, reverse=True) or [LF])[0]\n\n\ndef _get_indentword(source):\n    \"\"\"Return indentation type.\"\"\"\n    indent_word = '    '  # Default in case source has no indentation\n    try:\n        for t in generate_tokens(source):\n            if t[0] == token.INDENT:\n                indent_word = t[1]\n                break\n    except (SyntaxError, tokenize.TokenError):\n        pass\n    return indent_word\n\n\ndef _get_indentation(line):\n    \"\"\"Return leading whitespace.\"\"\"\n    if line.strip():\n        non_whitespace_index = len(line) - len(line.lstrip())\n        return line[:non_whitespace_index]\n\n    return ''\n\n\ndef get_diff_text(old, new, filename):\n    \"\"\"Return text of unified diff between old and new.\"\"\"\n    newline = '\\n'\n    diff = difflib.unified_diff(\n        old, new,\n        'original/' + filename,\n        'fixed/' + filename,\n        lineterm=newline)\n\n    text = ''\n    for line in diff:\n        text += line\n\n        # Work around missing newline (http://bugs.python.org/issue2142).\n        if text and not line.endswith(newline):\n            text += newline + r'\\ No newline at end of file' + newline\n\n    return text\n\n\ndef _priority_key(pep8_result):\n    \"\"\"Key for sorting PEP8 results.\n\n    Global fixes should be done first. This is important for things like\n    indentation.\n\n    \"\"\"\n    priority = [\n        # Fix multiline colon-based before semicolon based.\n        'e701',\n        # Break multiline statements early.\n        'e702',\n        # Things that make lines longer.\n        'e225', 'e231',\n        # Remove extraneous whitespace before breaking lines.\n        'e201',\n        # Shorten whitespace in comment before resorting to wrapping.\n        'e262'\n    ]\n    middle_index = 10000\n    lowest_priority = [\n        # We need to shorten lines last since the logical fixer can get in a\n        # loop, which causes us to exit early.\n        'e501',\n    ]\n    key = pep8_result['id'].lower()\n    try:\n        return priority.index(key)\n    except ValueError:\n        try:\n            return middle_index + lowest_priority.index(key) + 1\n        except ValueError:\n            return middle_index\n\n\ndef shorten_line(tokens, source, indentation, indent_word, max_line_length,\n                 aggressive=0, experimental=False, previous_line=''):\n    \"\"\"Separate line at OPERATOR.\n\n    Multiple candidates will be yielded.\n\n    \"\"\"\n    for candidate in _shorten_line(tokens=tokens,\n                                   source=source,\n                                   indentation=indentation,\n                                   indent_word=indent_word,\n                                   aggressive=aggressive,\n                                   previous_line=previous_line):\n        yield candidate\n\n    if aggressive:\n        for key_token_strings in SHORTEN_OPERATOR_GROUPS:\n            shortened = _shorten_line_at_tokens(\n                tokens=tokens,\n                source=source,\n                indentation=indentation,\n                indent_word=indent_word,\n                key_token_strings=key_token_strings,\n                aggressive=aggressive)\n\n            if shortened is not None and shortened != source:\n                yield shortened\n\n    if experimental:\n        for shortened in _shorten_line_at_tokens_new(\n                tokens=tokens,\n                source=source,\n                indentation=indentation,\n                max_line_length=max_line_length):\n\n            yield shortened\n\n\ndef _shorten_line(tokens, source, indentation, indent_word,\n                  aggressive=0, previous_line=''):\n    \"\"\"Separate line at OPERATOR.\n\n    The input is expected to be free of newlines except for inside multiline\n    strings and at the end.\n\n    Multiple candidates will be yielded.\n\n    \"\"\"\n    in_string = False\n    for (token_type,\n         token_string,\n         start_offset,\n         end_offset) in token_offsets(tokens):\n\n        if IS_SUPPORT_TOKEN_FSTRING:\n            if token_type == tokenize.FSTRING_START:\n                in_string = True\n            elif token_type == tokenize.FSTRING_END:\n                in_string = False\n        if in_string:\n            continue\n\n        if (\n            token_type == tokenize.COMMENT and\n            not is_probably_part_of_multiline(previous_line) and\n            not is_probably_part_of_multiline(source) and\n            not source[start_offset + 1:].strip().lower().startswith(\n                ('noqa', 'pragma:', 'pylint:'))\n        ):\n            # Move inline comments to previous line.\n            first = source[:start_offset]\n            second = source[start_offset:]\n            yield (indentation + second.strip() + '\\n' +\n                   indentation + first.strip() + '\\n')\n        elif token_type == token.OP and token_string != '=':\n            # Don't break on '=' after keyword as this violates PEP 8.\n\n            assert token_type != token.INDENT\n\n            first = source[:end_offset]\n\n            second_indent = indentation\n            if (first.rstrip().endswith('(') and\n                    source[end_offset:].lstrip().startswith(')')):\n                pass\n            elif first.rstrip().endswith('('):\n                second_indent += indent_word\n            elif '(' in first:\n                second_indent += ' ' * (1 + first.find('('))\n            else:\n                second_indent += indent_word\n\n            second = (second_indent + source[end_offset:].lstrip())\n            if (\n                not second.strip() or\n                second.lstrip().startswith('#')\n            ):\n                continue\n\n            # Do not begin a line with a comma\n            if second.lstrip().startswith(','):\n                continue\n            # Do end a line with a dot\n            if first.rstrip().endswith('.'):\n                continue\n            if token_string in '+-*/':\n                fixed = first + ' \\\\' + '\\n' + second\n            else:\n                fixed = first + '\\n' + second\n\n            # Only fix if syntax is okay.\n            if check_syntax(normalize_multiline(fixed)\n                            if aggressive else fixed):\n                yield indentation + fixed\n\n\ndef _is_binary_operator(token_type, text):\n    return ((token_type == tokenize.OP or text in ['and', 'or']) and\n            text not in '()[]{},:.;@=%~')\n\n\n# A convenient way to handle tokens.\nToken = collections.namedtuple('Token', ['token_type', 'token_string',\n                                         'spos', 'epos', 'line'])\n\n\nclass ReformattedLines(object):\n\n    \"\"\"The reflowed lines of atoms.\n\n    Each part of the line is represented as an \"atom.\" They can be moved\n    around when need be to get the optimal formatting.\n\n    \"\"\"\n\n    ###########################################################################\n    # Private Classes\n\n    class _Indent(object):\n\n        \"\"\"Represent an indentation in the atom stream.\"\"\"\n\n        def __init__(self, indent_amt):\n            self._indent_amt = indent_amt\n\n        def emit(self):\n            return ' ' * self._indent_amt\n\n        @property\n        def size(self):\n            return self._indent_amt\n\n    class _Space(object):\n\n        \"\"\"Represent a space in the atom stream.\"\"\"\n\n        def emit(self):\n            return ' '\n\n        @property\n        def size(self):\n            return 1\n\n    class _LineBreak(object):\n\n        \"\"\"Represent a line break in the atom stream.\"\"\"\n\n        def emit(self):\n            return '\\n'\n\n        @property\n        def size(self):\n            return 0\n\n    def __init__(self, max_line_length):\n        self._max_line_length = max_line_length\n        self._lines = []\n        self._bracket_depth = 0\n        self._prev_item = None\n        self._prev_prev_item = None\n        self._in_fstring = False\n\n    def __repr__(self):\n        return self.emit()\n\n    ###########################################################################\n    # Public Methods\n\n    def add(self, obj, indent_amt, break_after_open_bracket):\n        if isinstance(obj, Atom):\n            self._add_item(obj, indent_amt)\n            return\n\n        self._add_container(obj, indent_amt, break_after_open_bracket)\n\n    def add_comment(self, item):\n        num_spaces = 2\n        if len(self._lines) > 1:\n            if isinstance(self._lines[-1], self._Space):\n                num_spaces -= 1\n            if len(self._lines) > 2:\n                if isinstance(self._lines[-2], self._Space):\n                    num_spaces -= 1\n\n        while num_spaces > 0:\n            self._lines.append(self._Space())\n            num_spaces -= 1\n        self._lines.append(item)\n\n    def add_indent(self, indent_amt):\n        self._lines.append(self._Indent(indent_amt))\n\n    def add_line_break(self, indent):\n        self._lines.append(self._LineBreak())\n        self.add_indent(len(indent))\n\n    def add_line_break_at(self, index, indent_amt):\n        self._lines.insert(index, self._LineBreak())\n        self._lines.insert(index + 1, self._Indent(indent_amt))\n\n    def add_space_if_needed(self, curr_text, equal=False):\n        if (\n            not self._lines or isinstance(\n                self._lines[-1], (self._LineBreak, self._Indent, self._Space))\n        ):\n            return\n\n        prev_text = str(self._prev_item)\n        prev_prev_text = (\n            str(self._prev_prev_item) if self._prev_prev_item else '')\n\n        if (\n            # The previous item was a keyword or identifier and the current\n            # item isn't an operator that doesn't require a space.\n            ((self._prev_item.is_keyword or self._prev_item.is_string or\n              self._prev_item.is_name or self._prev_item.is_number) and\n             (curr_text[0] not in '([{.,:}])' or\n              (curr_text[0] == '=' and equal))) or\n\n            # Don't place spaces around a '.', unless it's in an 'import'\n            # statement.\n            ((prev_prev_text != 'from' and prev_text[-1] != '.' and\n              curr_text != 'import') and\n\n             # Don't place a space before a colon.\n             curr_text[0] != ':' and\n\n             # Don't split up ending brackets by spaces.\n             ((prev_text[-1] in '}])' and curr_text[0] not in '.,}])') or\n\n              # Put a space after a colon or comma.\n              prev_text[-1] in ':,' or\n\n              # Put space around '=' if asked to.\n              (equal and prev_text == '=') or\n\n              # Put spaces around non-unary arithmetic operators.\n              ((self._prev_prev_item and\n                (prev_text not in '+-' and\n                 (self._prev_prev_item.is_name or\n                  self._prev_prev_item.is_number or\n                  self._prev_prev_item.is_string)) and\n                prev_text in ('+', '-', '%', '*', '/', '//', '**', 'in')))))\n        ):\n            self._lines.append(self._Space())\n\n    def previous_item(self):\n        \"\"\"Return the previous non-whitespace item.\"\"\"\n        return self._prev_item\n\n    def fits_on_current_line(self, item_extent):\n        return self.current_size() + item_extent <= self._max_line_length\n\n    def current_size(self):\n        \"\"\"The size of the current line minus the indentation.\"\"\"\n        size = 0\n        for item in reversed(self._lines):\n            size += item.size\n            if isinstance(item, self._LineBreak):\n                break\n\n        return size\n\n    def line_empty(self):\n        return (self._lines and\n                isinstance(self._lines[-1],\n                           (self._LineBreak, self._Indent)))\n\n    def emit(self):\n        string = ''\n        for item in self._lines:\n            if isinstance(item, self._LineBreak):\n                string = string.rstrip()\n            string += item.emit()\n\n        return string.rstrip() + '\\n'\n\n    ###########################################################################\n    # Private Methods\n\n    def _add_item(self, item, indent_amt):\n        \"\"\"Add an item to the line.\n\n        Reflow the line to get the best formatting after the item is\n        inserted. The bracket depth indicates if the item is being\n        inserted inside of a container or not.\n\n        \"\"\"\n        if item.is_fstring_start:\n            self._in_fstring = True\n        elif self._prev_item and self._prev_item.is_fstring_end:\n            self._in_fstring = False\n\n        if self._prev_item and self._prev_item.is_string and item.is_string:\n            # Place consecutive string literals on separate lines.\n            self._lines.append(self._LineBreak())\n            self._lines.append(self._Indent(indent_amt))\n\n        item_text = str(item)\n        if self._lines and self._bracket_depth:\n            # Adding the item into a container.\n            self._prevent_default_initializer_splitting(item, indent_amt)\n\n            if item_text in '.,)]}':\n                self._split_after_delimiter(item, indent_amt)\n\n        elif self._lines and not self.line_empty():\n            # Adding the item outside of a container.\n            if self.fits_on_current_line(len(item_text)):\n                self._enforce_space(item)\n\n            else:\n                # Line break for the new item.\n                self._lines.append(self._LineBreak())\n                self._lines.append(self._Indent(indent_amt))\n\n        self._lines.append(item)\n        self._prev_item, self._prev_prev_item = item, self._prev_item\n\n        if item_text in '([{' and not self._in_fstring:\n            self._bracket_depth += 1\n\n        elif item_text in '}])' and not self._in_fstring:\n            self._bracket_depth -= 1\n            assert self._bracket_depth >= 0\n\n    def _add_container(self, container, indent_amt, break_after_open_bracket):\n        actual_indent = indent_amt + 1\n\n        if (\n            str(self._prev_item) != '=' and\n            not self.line_empty() and\n            not self.fits_on_current_line(\n                container.size + self._bracket_depth + 2)\n        ):\n\n            if str(container)[0] == '(' and self._prev_item.is_name:\n                # Don't split before the opening bracket of a call.\n                break_after_open_bracket = True\n                actual_indent = indent_amt + 4\n            elif (\n                break_after_open_bracket or\n                str(self._prev_item) not in '([{'\n            ):\n                # If the container doesn't fit on the current line and the\n                # current line isn't empty, place the container on the next\n                # line.\n                self._lines.append(self._LineBreak())\n                self._lines.append(self._Indent(indent_amt))\n                break_after_open_bracket = False\n        else:\n            actual_indent = self.current_size() + 1\n            break_after_open_bracket = False\n\n        if isinstance(container, (ListComprehension, IfExpression)):\n            actual_indent = indent_amt\n\n        # Increase the continued indentation only if recursing on a\n        # container.\n        container.reflow(self, ' ' * actual_indent,\n                         break_after_open_bracket=break_after_open_bracket)\n\n    def _prevent_default_initializer_splitting(self, item, indent_amt):\n        \"\"\"Prevent splitting between a default initializer.\n\n        When there is a default initializer, it's best to keep it all on\n        the same line. It's nicer and more readable, even if it goes\n        over the maximum allowable line length. This goes back along the\n        current line to determine if we have a default initializer, and,\n        if so, to remove extraneous whitespaces and add a line\n        break/indent before it if needed.\n\n        \"\"\"\n        if str(item) == '=':\n            # This is the assignment in the initializer. Just remove spaces for\n            # now.\n            self._delete_whitespace()\n            return\n\n        if (not self._prev_item or not self._prev_prev_item or\n                str(self._prev_item) != '='):\n            return\n\n        self._delete_whitespace()\n        prev_prev_index = self._lines.index(self._prev_prev_item)\n\n        if (\n            isinstance(self._lines[prev_prev_index - 1], self._Indent) or\n            self.fits_on_current_line(item.size + 1)\n        ):\n            # The default initializer is already the only item on this line.\n            # Don't insert a newline here.\n            return\n\n        # Replace the space with a newline/indent combo.\n        if isinstance(self._lines[prev_prev_index - 1], self._Space):\n            del self._lines[prev_prev_index - 1]\n\n        self.add_line_break_at(self._lines.index(self._prev_prev_item),\n                               indent_amt)\n\n    def _split_after_delimiter(self, item, indent_amt):\n        \"\"\"Split the line only after a delimiter.\"\"\"\n        self._delete_whitespace()\n\n        if self.fits_on_current_line(item.size):\n            return\n\n        last_space = None\n        for current_item in reversed(self._lines):\n            if (\n                last_space and\n                (not isinstance(current_item, Atom) or\n                 not current_item.is_colon)\n            ):\n                break\n            else:\n                last_space = None\n            if isinstance(current_item, self._Space):\n                last_space = current_item\n            if isinstance(current_item, (self._LineBreak, self._Indent)):\n                return\n\n        if not last_space:\n            return\n\n        self.add_line_break_at(self._lines.index(last_space), indent_amt)\n\n    def _enforce_space(self, item):\n        \"\"\"Enforce a space in certain situations.\n\n        There are cases where we will want a space where normally we\n        wouldn't put one. This just enforces the addition of a space.\n\n        \"\"\"\n        if isinstance(self._lines[-1],\n                      (self._Space, self._LineBreak, self._Indent)):\n            return\n\n        if not self._prev_item:\n            return\n\n        item_text = str(item)\n        prev_text = str(self._prev_item)\n\n        # Prefer a space around a '.' in an import statement, and between the\n        # 'import' and '('.\n        if (\n            (item_text == '.' and prev_text == 'from') or\n            (item_text == 'import' and prev_text == '.') or\n            (item_text == '(' and prev_text == 'import')\n        ):\n            self._lines.append(self._Space())\n\n    def _delete_whitespace(self):\n        \"\"\"Delete all whitespace from the end of the line.\"\"\"\n        while isinstance(self._lines[-1], (self._Space, self._LineBreak,\n                                           self._Indent)):\n            del self._lines[-1]\n\n\nclass Atom(object):\n\n    \"\"\"The smallest unbreakable unit that can be reflowed.\"\"\"\n\n    def __init__(self, atom):\n        self._atom = atom\n\n    def __repr__(self):\n        return self._atom.token_string\n\n    def __len__(self):\n        return self.size\n\n    def reflow(\n        self, reflowed_lines, continued_indent, extent,\n        break_after_open_bracket=False,\n        is_list_comp_or_if_expr=False,\n        next_is_dot=False\n    ):\n        if self._atom.token_type == tokenize.COMMENT:\n            reflowed_lines.add_comment(self)\n            return\n\n        total_size = extent if extent else self.size\n\n        if self._atom.token_string not in ',:([{}])':\n            # Some atoms will need an extra 1-sized space token after them.\n            total_size += 1\n\n        prev_item = reflowed_lines.previous_item()\n        if (\n            not is_list_comp_or_if_expr and\n            not reflowed_lines.fits_on_current_line(total_size) and\n            not (next_is_dot and\n                 reflowed_lines.fits_on_current_line(self.size + 1)) and\n            not reflowed_lines.line_empty() and\n            not self.is_colon and\n            not (prev_item and prev_item.is_name and\n                 str(self) == '(')\n        ):\n            # Start a new line if there is already something on the line and\n            # adding this atom would make it go over the max line length.\n            reflowed_lines.add_line_break(continued_indent)\n        else:\n            reflowed_lines.add_space_if_needed(str(self))\n\n        reflowed_lines.add(self, len(continued_indent),\n                           break_after_open_bracket)\n\n    def emit(self):\n        return self.__repr__()\n\n    @property\n    def is_keyword(self):\n        return keyword.iskeyword(self._atom.token_string)\n\n    @property\n    def is_string(self):\n        return self._atom.token_type == tokenize.STRING\n\n    @property\n    def is_fstring_start(self):\n        if not IS_SUPPORT_TOKEN_FSTRING:\n            return False\n        return self._atom.token_type == tokenize.FSTRING_START\n\n    @property\n    def is_fstring_end(self):\n        if not IS_SUPPORT_TOKEN_FSTRING:\n            return False\n        return self._atom.token_type == tokenize.FSTRING_END\n\n    @property\n    def is_name(self):\n        return self._atom.token_type == tokenize.NAME\n\n    @property\n    def is_number(self):\n        return self._atom.token_type == tokenize.NUMBER\n\n    @property\n    def is_comma(self):\n        return self._atom.token_string == ','\n\n    @property\n    def is_colon(self):\n        return self._atom.token_string == ':'\n\n    @property\n    def size(self):\n        return len(self._atom.token_string)\n\n\nclass Container(object):\n\n    \"\"\"Base class for all container types.\"\"\"\n\n    def __init__(self, items):\n        self._items = items\n\n    def __repr__(self):\n        string = ''\n        last_was_keyword = False\n\n        for item in self._items:\n            if item.is_comma:\n                string += ', '\n            elif item.is_colon:\n                string += ': '\n            else:\n                item_string = str(item)\n                if (\n                    string and\n                    (last_was_keyword or\n                     (not string.endswith(tuple('([{,.:}]) ')) and\n                      not item_string.startswith(tuple('([{,.:}])'))))\n                ):\n                    string += ' '\n                string += item_string\n\n            last_was_keyword = item.is_keyword\n        return string\n\n    def __iter__(self):\n        for element in self._items:\n            yield element\n\n    def __getitem__(self, idx):\n        return self._items[idx]\n\n    def reflow(self, reflowed_lines, continued_indent,\n               break_after_open_bracket=False):\n        last_was_container = False\n        for (index, item) in enumerate(self._items):\n            next_item = get_item(self._items, index + 1)\n\n            if isinstance(item, Atom):\n                is_list_comp_or_if_expr = (\n                    isinstance(self, (ListComprehension, IfExpression)))\n                item.reflow(reflowed_lines, continued_indent,\n                            self._get_extent(index),\n                            is_list_comp_or_if_expr=is_list_comp_or_if_expr,\n                            next_is_dot=(next_item and\n                                         str(next_item) == '.'))\n                if last_was_container and item.is_comma:\n                    reflowed_lines.add_line_break(continued_indent)\n                last_was_container = False\n            else:  # isinstance(item, Container)\n                reflowed_lines.add(item, len(continued_indent),\n                                   break_after_open_bracket)\n                last_was_container = not isinstance(item, (ListComprehension,\n                                                           IfExpression))\n\n            if (\n                break_after_open_bracket and index == 0 and\n                # Prefer to keep empty containers together instead of\n                # separating them.\n                str(item) == self.open_bracket and\n                (not next_item or str(next_item) != self.close_bracket) and\n                (len(self._items) != 3 or not isinstance(next_item, Atom))\n            ):\n                reflowed_lines.add_line_break(continued_indent)\n                break_after_open_bracket = False\n            else:\n                next_next_item = get_item(self._items, index + 2)\n                if (\n                    str(item) not in ['.', '%', 'in'] and\n                    next_item and not isinstance(next_item, Container) and\n                    str(next_item) != ':' and\n                    next_next_item and (not isinstance(next_next_item, Atom) or\n                                        str(next_item) == 'not') and\n                    not reflowed_lines.line_empty() and\n                    not reflowed_lines.fits_on_current_line(\n                        self._get_extent(index + 1) + 2)\n                ):\n                    reflowed_lines.add_line_break(continued_indent)\n\n    def _get_extent(self, index):\n        \"\"\"The extent of the full element.\n\n        E.g., the length of a function call or keyword.\n\n        \"\"\"\n        extent = 0\n        prev_item = get_item(self._items, index - 1)\n        seen_dot = prev_item and str(prev_item) == '.'\n        while index < len(self._items):\n            item = get_item(self._items, index)\n            index += 1\n\n            if isinstance(item, (ListComprehension, IfExpression)):\n                break\n\n            if isinstance(item, Container):\n                if prev_item and prev_item.is_name:\n                    if seen_dot:\n                        extent += 1\n                    else:\n                        extent += item.size\n\n                    prev_item = item\n                    continue\n            elif (str(item) not in ['.', '=', ':', 'not'] and\n                  not item.is_name and not item.is_string):\n                break\n\n            if str(item) == '.':\n                seen_dot = True\n\n            extent += item.size\n            prev_item = item\n\n        return extent\n\n    @property\n    def is_string(self):\n        return False\n\n    @property\n    def size(self):\n        return len(self.__repr__())\n\n    @property\n    def is_keyword(self):\n        return False\n\n    @property\n    def is_name(self):\n        return False\n\n    @property\n    def is_comma(self):\n        return False\n\n    @property\n    def is_colon(self):\n        return False\n\n    @property\n    def open_bracket(self):\n        return None\n\n    @property\n    def close_bracket(self):\n        return None\n\n\nclass Tuple(Container):\n\n    \"\"\"A high-level representation of a tuple.\"\"\"\n\n    @property\n    def open_bracket(self):\n        return '('\n\n    @property\n    def close_bracket(self):\n        return ')'\n\n\nclass List(Container):\n\n    \"\"\"A high-level representation of a list.\"\"\"\n\n    @property\n    def open_bracket(self):\n        return '['\n\n    @property\n    def close_bracket(self):\n        return ']'\n\n\nclass DictOrSet(Container):\n\n    \"\"\"A high-level representation of a dictionary or set.\"\"\"\n\n    @property\n    def open_bracket(self):\n        return '{'\n\n    @property\n    def close_bracket(self):\n        return '}'\n\n\nclass ListComprehension(Container):\n\n    \"\"\"A high-level representation of a list comprehension.\"\"\"\n\n    @property\n    def size(self):\n        length = 0\n        for item in self._items:\n            if isinstance(item, IfExpression):\n                break\n            length += item.size\n        return length\n\n\nclass IfExpression(Container):\n\n    \"\"\"A high-level representation of an if-expression.\"\"\"\n\n\ndef _parse_container(tokens, index, for_or_if=None):\n    \"\"\"Parse a high-level container, such as a list, tuple, etc.\"\"\"\n\n    # Store the opening bracket.\n    items = [Atom(Token(*tokens[index]))]\n    index += 1\n\n    num_tokens = len(tokens)\n    while index < num_tokens:\n        tok = Token(*tokens[index])\n\n        if tok.token_string in ',)]}':\n            # First check if we're at the end of a list comprehension or\n            # if-expression. Don't add the ending token as part of the list\n            # comprehension or if-expression, because they aren't part of those\n            # constructs.\n            if for_or_if == 'for':\n                return (ListComprehension(items), index - 1)\n\n            elif for_or_if == 'if':\n                return (IfExpression(items), index - 1)\n\n            # We've reached the end of a container.\n            items.append(Atom(tok))\n\n            # If not, then we are at the end of a container.\n            if tok.token_string == ')':\n                # The end of a tuple.\n                return (Tuple(items), index)\n\n            elif tok.token_string == ']':\n                # The end of a list.\n                return (List(items), index)\n\n            elif tok.token_string == '}':\n                # The end of a dictionary or set.\n                return (DictOrSet(items), index)\n\n        elif tok.token_string in '([{':\n            # A sub-container is being defined.\n            (container, index) = _parse_container(tokens, index)\n            items.append(container)\n\n        elif tok.token_string == 'for':\n            (container, index) = _parse_container(tokens, index, 'for')\n            items.append(container)\n\n        elif tok.token_string == 'if':\n            (container, index) = _parse_container(tokens, index, 'if')\n            items.append(container)\n\n        else:\n            items.append(Atom(tok))\n\n        index += 1\n\n    return (None, None)\n\n\ndef _parse_tokens(tokens):\n    \"\"\"Parse the tokens.\n\n    This converts the tokens into a form where we can manipulate them\n    more easily.\n\n    \"\"\"\n\n    index = 0\n    parsed_tokens = []\n\n    num_tokens = len(tokens)\n    while index < num_tokens:\n        tok = Token(*tokens[index])\n\n        assert tok.token_type != token.INDENT\n        if tok.token_type == tokenize.NEWLINE:\n            # There's only one newline and it's at the end.\n            break\n\n        if tok.token_string in '([{':\n            (container, index) = _parse_container(tokens, index)\n            if not container:\n                return None\n            parsed_tokens.append(container)\n        else:\n            parsed_tokens.append(Atom(tok))\n\n        index += 1\n\n    return parsed_tokens\n\n\ndef _reflow_lines(parsed_tokens, indentation, max_line_length,\n                  start_on_prefix_line):\n    \"\"\"Reflow the lines so that it looks nice.\"\"\"\n\n    if str(parsed_tokens[0]) == 'def':\n        # A function definition gets indented a bit more.\n        continued_indent = indentation + ' ' * 2 * DEFAULT_INDENT_SIZE\n    else:\n        continued_indent = indentation + ' ' * DEFAULT_INDENT_SIZE\n\n    break_after_open_bracket = not start_on_prefix_line\n\n    lines = ReformattedLines(max_line_length)\n    lines.add_indent(len(indentation.lstrip('\\r\\n')))\n\n    if not start_on_prefix_line:\n        # If splitting after the opening bracket will cause the first element\n        # to be aligned weirdly, don't try it.\n        first_token = get_item(parsed_tokens, 0)\n        second_token = get_item(parsed_tokens, 1)\n\n        if (\n            first_token and second_token and\n            str(second_token)[0] == '(' and\n            len(indentation) + len(first_token) + 1 == len(continued_indent)\n        ):\n            return None\n\n    for item in parsed_tokens:\n        lines.add_space_if_needed(str(item), equal=True)\n\n        save_continued_indent = continued_indent\n        if start_on_prefix_line and isinstance(item, Container):\n            start_on_prefix_line = False\n            continued_indent = ' ' * (lines.current_size() + 1)\n\n        item.reflow(lines, continued_indent, break_after_open_bracket)\n        continued_indent = save_continued_indent\n\n    return lines.emit()\n\n\ndef _shorten_line_at_tokens_new(tokens, source, indentation,\n                                max_line_length):\n    \"\"\"Shorten the line taking its length into account.\n\n    The input is expected to be free of newlines except for inside\n    multiline strings and at the end.\n\n    \"\"\"\n    # Yield the original source so to see if it's a better choice than the\n    # shortened candidate lines we generate here.\n    yield indentation + source\n\n    parsed_tokens = _parse_tokens(tokens)\n\n    if parsed_tokens:\n        # Perform two reflows. The first one starts on the same line as the\n        # prefix. The second starts on the line after the prefix.\n        fixed = _reflow_lines(parsed_tokens, indentation, max_line_length,\n                              start_on_prefix_line=True)\n        if fixed and check_syntax(normalize_multiline(fixed.lstrip())):\n            yield fixed\n\n        fixed = _reflow_lines(parsed_tokens, indentation, max_line_length,\n                              start_on_prefix_line=False)\n        if fixed and check_syntax(normalize_multiline(fixed.lstrip())):\n            yield fixed\n\n\ndef _shorten_line_at_tokens(tokens, source, indentation, indent_word,\n                            key_token_strings, aggressive):\n    \"\"\"Separate line by breaking at tokens in key_token_strings.\n\n    The input is expected to be free of newlines except for inside\n    multiline strings and at the end.\n\n    \"\"\"\n    offsets = []\n    for (index, _t) in enumerate(token_offsets(tokens)):\n        (token_type,\n         token_string,\n         start_offset,\n         end_offset) = _t\n\n        assert token_type != token.INDENT\n\n        if token_string in key_token_strings:\n            # Do not break in containers with zero or one items.\n            unwanted_next_token = {\n                '(': ')',\n                '[': ']',\n                '{': '}'}.get(token_string)\n            if unwanted_next_token:\n                if (\n                    get_item(tokens,\n                             index + 1,\n                             default=[None, None])[1] == unwanted_next_token or\n                    get_item(tokens,\n                             index + 2,\n                             default=[None, None])[1] == unwanted_next_token\n                ):\n                    continue\n\n            if (\n                index > 2 and token_string == '(' and\n                tokens[index - 1][1] in ',(%['\n            ):\n                # Don't split after a tuple start, or before a tuple start if\n                # the tuple is in a list.\n                continue\n\n            if end_offset < len(source) - 1:\n                # Don't split right before newline.\n                offsets.append(end_offset)\n        else:\n            # Break at adjacent strings. These were probably meant to be on\n            # separate lines in the first place.\n            previous_token = get_item(tokens, index - 1)\n            if (\n                token_type == tokenize.STRING and\n                previous_token and previous_token[0] == tokenize.STRING\n            ):\n                offsets.append(start_offset)\n\n    current_indent = None\n    fixed = None\n    for line in split_at_offsets(source, offsets):\n        if fixed:\n            fixed += '\\n' + current_indent + line\n\n            for symbol in '([{':\n                if line.endswith(symbol):\n                    current_indent += indent_word\n        else:\n            # First line.\n            fixed = line\n            assert not current_indent\n            current_indent = indent_word\n\n    assert fixed is not None\n\n    if check_syntax(normalize_multiline(fixed)\n                    if aggressive > 1 else fixed):\n        return indentation + fixed\n\n    return None\n\n\ndef token_offsets(tokens):\n    \"\"\"Yield tokens and offsets.\"\"\"\n    end_offset = 0\n    previous_end_row = 0\n    previous_end_column = 0\n    for t in tokens:\n        token_type = t[0]\n        token_string = t[1]\n        (start_row, start_column) = t[2]\n        (end_row, end_column) = t[3]\n\n        # Account for the whitespace between tokens.\n        end_offset += start_column\n        if previous_end_row == start_row:\n            end_offset -= previous_end_column\n\n        # Record the start offset of the token.\n        start_offset = end_offset\n\n        # Account for the length of the token itself.\n        end_offset += len(token_string)\n\n        yield (token_type,\n               token_string,\n               start_offset,\n               end_offset)\n\n        previous_end_row = end_row\n        previous_end_column = end_column\n\n\ndef normalize_multiline(line):\n    \"\"\"Normalize multiline-related code that will cause syntax error.\n\n    This is for purposes of checking syntax.\n\n    \"\"\"\n    if line.startswith(('def ', 'async def ')) and line.rstrip().endswith(':'):\n        return line + ' pass'\n    elif line.startswith('return '):\n        return 'def _(): ' + line\n    elif line.startswith('@'):\n        return line + 'def _(): pass'\n    elif line.startswith('class '):\n        return line + ' pass'\n    elif line.startswith(('if ', 'elif ', 'for ', 'while ')):\n        return line + ' pass'\n\n    return line\n\n\ndef fix_whitespace(line, offset, replacement):\n    \"\"\"Replace whitespace at offset and return fixed line.\"\"\"\n    # Replace escaped newlines too\n    left = line[:offset].rstrip('\\n\\r \\t\\\\')\n    right = line[offset:].lstrip('\\n\\r \\t\\\\')\n    if right.startswith('#'):\n        return line\n\n    return left + replacement + right\n\n\ndef _execute_pep8(pep8_options, source):\n    \"\"\"Execute pycodestyle via python method calls.\"\"\"\n    class QuietReport(pycodestyle.BaseReport):\n\n        \"\"\"Version of checker that does not print.\"\"\"\n\n        def __init__(self, options):\n            super(QuietReport, self).__init__(options)\n            self.__full_error_results = []\n\n        def error(self, line_number, offset, text, check):\n            \"\"\"Collect errors.\"\"\"\n            code = super(QuietReport, self).error(line_number,\n                                                  offset,\n                                                  text,\n                                                  check)\n            if code:\n                self.__full_error_results.append(\n                    {'id': code,\n                     'line': line_number,\n                     'column': offset + 1,\n                     'info': text})\n\n        def full_error_results(self):\n            \"\"\"Return error results in detail.\n\n            Results are in the form of a list of dictionaries. Each\n            dictionary contains 'id', 'line', 'column', and 'info'.\n\n            \"\"\"\n            return self.__full_error_results\n\n    checker = pycodestyle.Checker('', lines=source, reporter=QuietReport,\n                                  **pep8_options)\n    checker.check_all()\n    return checker.report.full_error_results()\n\n\ndef _remove_leading_and_normalize(line, with_rstrip=True):\n    # ignore FF in first lstrip()\n    if with_rstrip:\n        return line.lstrip(' \\t\\v').rstrip(CR + LF) + '\\n'\n    return line.lstrip(' \\t\\v')\n\n\nclass Reindenter(object):\n\n    \"\"\"Reindents badly-indented code to uniformly use four-space indentation.\n\n    Released to the public domain, by Tim Peters, 03 October 2000.\n\n    \"\"\"\n\n    def __init__(self, input_text, leave_tabs=False):\n        sio = io.StringIO(input_text)\n        source_lines = sio.readlines()\n\n        self.string_content_line_numbers = multiline_string_lines(input_text)\n\n        # File lines, rstripped & tab-expanded. Dummy at start is so\n        # that we can use tokenize's 1-based line numbering easily.\n        # Note that a line is all-blank iff it is a newline.\n        self.lines = []\n        for line_number, line in enumerate(source_lines, start=1):\n            # Do not modify if inside a multiline string.\n            if line_number in self.string_content_line_numbers:\n                self.lines.append(line)\n            else:\n                # Only expand leading tabs.\n                with_rstrip = line_number != len(source_lines)\n                if leave_tabs:\n                    self.lines.append(\n                        _get_indentation(line) +\n                        _remove_leading_and_normalize(line, with_rstrip)\n                    )\n                else:\n                    self.lines.append(\n                        _get_indentation(line).expandtabs() +\n                        _remove_leading_and_normalize(line, with_rstrip)\n                    )\n\n        self.lines.insert(0, None)\n        self.index = 1  # index into self.lines of next line\n        self.input_text = input_text\n\n    def run(self, indent_size=DEFAULT_INDENT_SIZE):\n        \"\"\"Fix indentation and return modified line numbers.\n\n        Line numbers are indexed at 1.\n\n        \"\"\"\n        if indent_size < 1:\n            return self.input_text\n\n        try:\n            stats = _reindent_stats(tokenize.generate_tokens(self.getline))\n        except (SyntaxError, tokenize.TokenError):\n            return self.input_text\n        # Remove trailing empty lines.\n        lines = self.lines\n        # Sentinel.\n        stats.append((len(lines), 0))\n        # Map count of leading spaces to # we want.\n        have2want = {}\n        # Program after transformation.\n        after = []\n        # Copy over initial empty lines -- there's nothing to do until\n        # we see a line with *something* on it.\n        i = stats[0][0]\n        after.extend(lines[1:i])\n        for i in range(len(stats) - 1):\n            thisstmt, thislevel = stats[i]\n            nextstmt = stats[i + 1][0]\n            have = _leading_space_count(lines[thisstmt])\n            want = thislevel * indent_size\n            if want < 0:\n                # A comment line.\n                if have:\n                    # An indented comment line. If we saw the same\n                    # indentation before, reuse what it most recently\n                    # mapped to.\n                    want = have2want.get(have, -1)\n                    if want < 0:\n                        # Then it probably belongs to the next real stmt.\n                        for j in range(i + 1, len(stats) - 1):\n                            jline, jlevel = stats[j]\n                            if jlevel >= 0:\n                                if have == _leading_space_count(lines[jline]):\n                                    want = jlevel * indent_size\n                                break\n                    # Maybe it's a hanging comment like this one,\n                    if want < 0:\n                        # in which case we should shift it like its base\n                        # line got shifted.\n                        for j in range(i - 1, -1, -1):\n                            jline, jlevel = stats[j]\n                            if jlevel >= 0:\n                                want = (have + _leading_space_count(\n                                        after[jline - 1]) -\n                                        _leading_space_count(lines[jline]))\n                                break\n                    if want < 0:\n                        # Still no luck -- leave it alone.\n                        want = have\n                else:\n                    want = 0\n            assert want >= 0\n            have2want[have] = want\n            diff = want - have\n            if diff == 0 or have == 0:\n                after.extend(lines[thisstmt:nextstmt])\n            else:\n                for line_number, line in enumerate(lines[thisstmt:nextstmt],\n                                                   start=thisstmt):\n                    if line_number in self.string_content_line_numbers:\n                        after.append(line)\n                    elif diff > 0:\n                        if line == '\\n':\n                            after.append(line)\n                        else:\n                            after.append(' ' * diff + line)\n                    else:\n                        remove = min(_leading_space_count(line), -diff)\n                        after.append(line[remove:])\n\n        return ''.join(after)\n\n    def getline(self):\n        \"\"\"Line-getter for tokenize.\"\"\"\n        if self.index >= len(self.lines):\n            line = ''\n        else:\n            line = self.lines[self.index]\n            self.index += 1\n        return line\n\n\ndef _reindent_stats(tokens):\n    \"\"\"Return list of (lineno, indentlevel) pairs.\n\n    One for each stmt and comment line. indentlevel is -1 for comment\n    lines, as a signal that tokenize doesn't know what to do about them;\n    indeed, they're our headache!\n\n    \"\"\"\n    find_stmt = 1  # Next token begins a fresh stmt?\n    level = 0  # Current indent level.\n    stats = []\n\n    for t in tokens:\n        token_type = t[0]\n        sline = t[2][0]\n        line = t[4]\n\n        if token_type == tokenize.NEWLINE:\n            # A program statement, or ENDMARKER, will eventually follow,\n            # after some (possibly empty) run of tokens of the form\n            #     (NL | COMMENT)* (INDENT | DEDENT+)?\n            find_stmt = 1\n\n        elif token_type == tokenize.INDENT:\n            find_stmt = 1\n            level += 1\n\n        elif token_type == tokenize.DEDENT:\n            find_stmt = 1\n            level -= 1\n\n        elif token_type == tokenize.COMMENT:\n            if find_stmt:\n                stats.append((sline, -1))\n                # But we're still looking for a new stmt, so leave\n                # find_stmt alone.\n\n        elif token_type == tokenize.NL:\n            pass\n\n        elif find_stmt:\n            # This is the first \"real token\" following a NEWLINE, so it\n            # must be the first token of the next program statement, or an\n            # ENDMARKER.\n            find_stmt = 0\n            if line:   # Not endmarker.\n                stats.append((sline, level))\n\n    return stats\n\n\ndef _leading_space_count(line):\n    \"\"\"Return number of leading spaces in line.\"\"\"\n    i = 0\n    while i < len(line) and line[i] == ' ':\n        i += 1\n    return i\n\n\ndef check_syntax(code):\n    \"\"\"Return True if syntax is okay.\"\"\"\n    try:\n        return compile(code, '<string>', 'exec', dont_inherit=True)\n    except (SyntaxError, TypeError, ValueError):\n        return False\n\n\ndef find_with_line_numbers(pattern, contents):\n    \"\"\"A wrapper around 're.finditer' to find line numbers.\n\n    Returns a list of line numbers where pattern was found in contents.\n    \"\"\"\n    matches = list(re.finditer(pattern, contents))\n    if not matches:\n        return []\n\n    end = matches[-1].start()\n\n    # -1 so a failed `rfind` maps to the first line.\n    newline_offsets = {\n        -1: 0\n    }\n    for line_num, m in enumerate(re.finditer(r'\\n', contents), 1):\n        offset = m.start()\n        if offset > end:\n            break\n        newline_offsets[offset] = line_num\n\n    def get_line_num(match, contents):\n        \"\"\"Get the line number of string in a files contents.\n\n        Failing to find the newline is OK, -1 maps to 0\n\n        \"\"\"\n        newline_offset = contents.rfind('\\n', 0, match.start())\n        return newline_offsets[newline_offset]\n\n    return [get_line_num(match, contents) + 1 for match in matches]\n\n\ndef get_disabled_ranges(source):\n    \"\"\"Returns a list of tuples representing the disabled ranges.\n\n    If disabled and no re-enable will disable for rest of file.\n\n    \"\"\"\n    enable_line_nums = find_with_line_numbers(ENABLE_REGEX, source)\n    disable_line_nums = find_with_line_numbers(DISABLE_REGEX, source)\n    total_lines = len(re.findall(\"\\n\", source)) + 1\n\n    enable_commands = {}\n    for num in enable_line_nums:\n        enable_commands[num] = True\n    for num in disable_line_nums:\n        enable_commands[num] = False\n\n    disabled_ranges = []\n    currently_enabled = True\n    disabled_start = None\n\n    for line, commanded_enabled in sorted(enable_commands.items()):\n        if commanded_enabled is False and currently_enabled is True:\n            disabled_start = line\n            currently_enabled = False\n        elif commanded_enabled is True and currently_enabled is False:\n            disabled_ranges.append((disabled_start, line))\n            currently_enabled = True\n\n    if currently_enabled is False:\n        disabled_ranges.append((disabled_start, total_lines))\n\n    return disabled_ranges\n\n\ndef filter_disabled_results(result, disabled_ranges):\n    \"\"\"Filter out reports based on tuple of disabled ranges.\n\n    \"\"\"\n    line = result['line']\n    for disabled_range in disabled_ranges:\n        if disabled_range[0] <= line <= disabled_range[1]:\n            return False\n    return True\n\n\ndef filter_results(source, results, aggressive):\n    \"\"\"Filter out spurious reports from pycodestyle.\n\n    If aggressive is True, we allow possibly unsafe fixes (E711, E712).\n\n    \"\"\"\n    non_docstring_string_line_numbers = multiline_string_lines(\n        source, include_docstrings=False)\n    all_string_line_numbers = multiline_string_lines(\n        source, include_docstrings=True)\n\n    commented_out_code_line_numbers = commented_out_code_lines(source)\n\n    # Filter out the disabled ranges\n    disabled_ranges = get_disabled_ranges(source)\n    if disabled_ranges:\n        results = [\n            result for result in results if filter_disabled_results(\n                result,\n                disabled_ranges,\n            )\n        ]\n\n    has_e901 = any(result['id'].lower() == 'e901' for result in results)\n\n    for r in results:\n        issue_id = r['id'].lower()\n\n        if r['line'] in non_docstring_string_line_numbers:\n            if issue_id.startswith(('e1', 'e501', 'w191')):\n                continue\n\n        if r['line'] in all_string_line_numbers:\n            if issue_id in ['e501']:\n                continue\n\n        # We must offset by 1 for lines that contain the trailing contents of\n        # multiline strings.\n        if not aggressive and (r['line'] + 1) in all_string_line_numbers:\n            # Do not modify multiline strings in non-aggressive mode. Remove\n            # trailing whitespace could break doctests.\n            if issue_id.startswith(('w29', 'w39')):\n                continue\n\n        if aggressive <= 0:\n            if issue_id.startswith(('e711', 'e72', 'w6')):\n                continue\n\n        if aggressive <= 1:\n            if issue_id.startswith(('e712', 'e713', 'e714')):\n                continue\n\n        if aggressive <= 2:\n            if issue_id.startswith(('e704')):\n                continue\n\n        if r['line'] in commented_out_code_line_numbers:\n            if issue_id.startswith(('e261', 'e262', 'e501')):\n                continue\n\n        # Do not touch indentation if there is a token error caused by\n        # incomplete multi-line statement. Otherwise, we risk screwing up the\n        # indentation.\n        if has_e901:\n            if issue_id.startswith(('e1', 'e7')):\n                continue\n\n        yield r\n\n\ndef multiline_string_lines(source, include_docstrings=False):\n    \"\"\"Return line numbers that are within multiline strings.\n\n    The line numbers are indexed at 1.\n\n    Docstrings are ignored.\n\n    \"\"\"\n    line_numbers = set()\n    previous_token_type = ''\n    _check_target_tokens = [tokenize.STRING]\n    if IS_SUPPORT_TOKEN_FSTRING:\n        _check_target_tokens.extend([\n            tokenize.FSTRING_START,\n            tokenize.FSTRING_MIDDLE,\n            tokenize.FSTRING_END,\n        ])\n    try:\n        for t in generate_tokens(source):\n            token_type = t[0]\n            start_row = t[2][0]\n            end_row = t[3][0]\n\n            if token_type in _check_target_tokens and start_row != end_row:\n                if (\n                    include_docstrings or\n                    previous_token_type != tokenize.INDENT\n                ):\n                    # We increment by one since we want the contents of the\n                    # string.\n                    line_numbers |= set(range(1 + start_row, 1 + end_row))\n\n            previous_token_type = token_type\n    except (SyntaxError, tokenize.TokenError):\n        pass\n\n    return line_numbers\n\n\ndef commented_out_code_lines(source):\n    \"\"\"Return line numbers of comments that are likely code.\n\n    Commented-out code is bad practice, but modifying it just adds even\n    more clutter.\n\n    \"\"\"\n    line_numbers = []\n    try:\n        for t in generate_tokens(source):\n            token_type = t[0]\n            token_string = t[1]\n            start_row = t[2][0]\n            line = t[4]\n\n            # Ignore inline comments.\n            if not line.lstrip().startswith('#'):\n                continue\n\n            if token_type == tokenize.COMMENT:\n                stripped_line = token_string.lstrip('#').strip()\n                with warnings.catch_warnings():\n                    # ignore SyntaxWarning in Python3.8+\n                    # refs:\n                    #   https://bugs.python.org/issue15248\n                    #   https://docs.python.org/3.8/whatsnew/3.8.html#other-language-changes\n                    warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n                    if (\n                        ' ' in stripped_line and\n                        '#' not in stripped_line and\n                        check_syntax(stripped_line)\n                    ):\n                        line_numbers.append(start_row)\n    except (SyntaxError, tokenize.TokenError):\n        pass\n\n    return line_numbers\n\n\ndef shorten_comment(line, max_line_length, last_comment=False):\n    \"\"\"Return trimmed or split long comment line.\n\n    If there are no comments immediately following it, do a text wrap.\n    Doing this wrapping on all comments in general would lead to jagged\n    comment text.\n\n    \"\"\"\n    assert len(line) > max_line_length\n    line = line.rstrip()\n\n    # PEP 8 recommends 72 characters for comment text.\n    indentation = _get_indentation(line) + '# '\n    max_line_length = min(max_line_length,\n                          len(indentation) + 72)\n\n    MIN_CHARACTER_REPEAT = 5\n    if (\n        len(line) - len(line.rstrip(line[-1])) >= MIN_CHARACTER_REPEAT and\n        not line[-1].isalnum()\n    ):\n        # Trim comments that end with things like ---------\n        return line[:max_line_length] + '\\n'\n    elif last_comment and re.match(r'\\s*#+\\s*\\w+', line):\n        split_lines = textwrap.wrap(line.lstrip(' \\t#'),\n                                    initial_indent=indentation,\n                                    subsequent_indent=indentation,\n                                    width=max_line_length,\n                                    break_long_words=False,\n                                    break_on_hyphens=False)\n        return '\\n'.join(split_lines) + '\\n'\n\n    return line + '\\n'\n\n\ndef normalize_line_endings(lines, newline):\n    \"\"\"Return fixed line endings.\n\n    All lines will be modified to use the most common line ending.\n    \"\"\"\n    line = [line.rstrip('\\n\\r') + newline for line in lines]\n    if line and lines[-1] == lines[-1].rstrip('\\n\\r'):\n        line[-1] = line[-1].rstrip('\\n\\r')\n    return line\n\n\ndef mutual_startswith(a, b):\n    return b.startswith(a) or a.startswith(b)\n\n\ndef code_match(code, select, ignore):\n    if ignore:\n        assert not isinstance(ignore, str)\n        for ignored_code in [c.strip() for c in ignore]:\n            if mutual_startswith(code.lower(), ignored_code.lower()):\n                return False\n\n    if select:\n        assert not isinstance(select, str)\n        for selected_code in [c.strip() for c in select]:\n            if mutual_startswith(code.lower(), selected_code.lower()):\n                return True\n        return False\n\n    return True\n\n\ndef fix_code(source, options=None, encoding=None, apply_config=False):\n    \"\"\"Return fixed source code.\n\n    \"encoding\" will be used to decode \"source\" if it is a byte string.\n\n    \"\"\"\n    options = _get_options(options, apply_config)\n    # normalize\n    options.ignore = [opt.upper() for opt in options.ignore]\n    options.select = [opt.upper() for opt in options.select]\n\n    # check ignore args\n    # NOTE: If W50x is not included, add W50x because the code\n    #       correction result is indefinite.\n    ignore_opt = options.ignore\n    if not {\"W50\", \"W503\", \"W504\"} & set(ignore_opt):\n        options.ignore.append(\"W50\")\n\n    if not isinstance(source, str):\n        source = source.decode(encoding or get_encoding())\n\n    sio = io.StringIO(source)\n    return fix_lines(sio.readlines(), options=options)\n\n\ndef _get_options(raw_options, apply_config):\n    \"\"\"Return parsed options.\"\"\"\n    if not raw_options:\n        return parse_args([''], apply_config=apply_config)\n\n    if isinstance(raw_options, dict):\n        options = parse_args([''], apply_config=apply_config)\n        for name, value in raw_options.items():\n            if not hasattr(options, name):\n                raise ValueError(\"No such option '{}'\".format(name))\n\n            # Check for very basic type errors.\n            expected_type = type(getattr(options, name))\n            if not isinstance(expected_type, (str, )):\n                if isinstance(value, (str, )):\n                    raise ValueError(\n                        \"Option '{}' should not be a string\".format(name))\n            setattr(options, name, value)\n    else:\n        options = raw_options\n\n    return options\n\n\ndef fix_lines(source_lines, options, filename=''):\n    \"\"\"Return fixed source code.\"\"\"\n    # Transform everything to line feed. Then change them back to original\n    # before returning fixed source code.\n    original_newline = find_newline(source_lines)\n    tmp_source = ''.join(normalize_line_endings(source_lines, '\\n'))\n\n    # Keep a history to break out of cycles.\n    previous_hashes = set()\n\n    if options.line_range:\n        # Disable \"apply_local_fixes()\" for now due to issue #175.\n        fixed_source = tmp_source\n    else:\n        # Apply global fixes only once (for efficiency).\n        fixed_source = apply_global_fixes(tmp_source,\n                                          options,\n                                          filename=filename)\n\n    passes = 0\n    long_line_ignore_cache = set()\n    while hash(fixed_source) not in previous_hashes:\n        if options.pep8_passes >= 0 and passes > options.pep8_passes:\n            break\n        passes += 1\n\n        previous_hashes.add(hash(fixed_source))\n\n        tmp_source = copy.copy(fixed_source)\n\n        fix = FixPEP8(\n            filename,\n            options,\n            contents=tmp_source,\n            long_line_ignore_cache=long_line_ignore_cache)\n\n        fixed_source = fix.fix()\n\n    sio = io.StringIO(fixed_source)\n    return ''.join(normalize_line_endings(sio.readlines(), original_newline))\n\n\ndef fix_file(filename, options=None, output=None, apply_config=False):\n    if not options:\n        options = parse_args([filename], apply_config=apply_config)\n\n    original_source = readlines_from_file(filename)\n\n    fixed_source = original_source\n\n    if options.in_place or options.diff or output:\n        encoding = detect_encoding(filename)\n\n    if output:\n        output = LineEndingWrapper(wrap_output(output, encoding=encoding))\n\n    fixed_source = fix_lines(fixed_source, options, filename=filename)\n\n    if options.diff:\n        new = io.StringIO(fixed_source)\n        new = new.readlines()\n        diff = get_diff_text(original_source, new, filename)\n        if output:\n            output.write(diff)\n            output.flush()\n        elif options.jobs > 1:\n            diff = diff.encode(encoding)\n        return diff\n    elif options.in_place:\n        original = \"\".join(original_source).splitlines()\n        fixed = fixed_source.splitlines()\n        original_source_last_line = (\n            original_source[-1].split(\"\\n\")[-1] if original_source else \"\"\n        )\n        fixed_source_last_line = fixed_source.split(\"\\n\")[-1]\n        if original != fixed or (\n            original_source_last_line != fixed_source_last_line\n        ):\n            with open_with_encoding(filename, 'w', encoding=encoding) as fp:\n                fp.write(fixed_source)\n            return fixed_source\n        return None\n    else:\n        if output:\n            output.write(fixed_source)\n            output.flush()\n    return fixed_source\n\n\ndef global_fixes():\n    \"\"\"Yield multiple (code, function) tuples.\"\"\"\n    for function in list(globals().values()):\n        if inspect.isfunction(function):\n            arguments = _get_parameters(function)\n            if arguments[:1] != ['source']:\n                continue\n\n            code = extract_code_from_function(function)\n            if code:\n                yield (code, function)\n\n\ndef _get_parameters(function):\n    # pylint: disable=deprecated-method\n    if sys.version_info.major >= 3:\n        # We need to match \"getargspec()\", which includes \"self\" as the first\n        # value for methods.\n        # https://bugs.python.org/issue17481#msg209469\n        if inspect.ismethod(function):\n            function = function.__func__\n\n        return list(inspect.signature(function).parameters)\n    else:\n        return inspect.getargspec(function)[0]\n\n\ndef apply_global_fixes(source, options, where='global', filename='',\n                       codes=None):\n    \"\"\"Run global fixes on source code.\n\n    These are fixes that only need be done once (unlike those in\n    FixPEP8, which are dependent on pycodestyle).\n\n    \"\"\"\n    if codes is None:\n        codes = []\n    if any(code_match(code, select=options.select, ignore=options.ignore)\n           for code in ['E101', 'E111']):\n        source = reindent(\n            source,\n            indent_size=options.indent_size,\n            leave_tabs=not (\n                code_match(\n                    'W191',\n                    select=options.select,\n                    ignore=options.ignore\n                )\n            )\n        )\n\n    for (code, function) in global_fixes():\n        if code_match(code, select=options.select, ignore=options.ignore):\n            if options.verbose:\n                print('--->  Applying {} fix for {}'.format(where,\n                                                            code.upper()),\n                      file=sys.stderr)\n            source = function(source,\n                              aggressive=options.aggressive)\n\n    return source\n\n\ndef extract_code_from_function(function):\n    \"\"\"Return code handled by function.\"\"\"\n    if not function.__name__.startswith('fix_'):\n        return None\n\n    code = re.sub('^fix_', '', function.__name__)\n    if not code:\n        return None\n\n    try:\n        int(code[1:])\n    except ValueError:\n        return None\n\n    return code\n\n\ndef _get_package_version():\n    packages = [\"pycodestyle: {}\".format(pycodestyle.__version__)]\n    return \", \".join(packages)\n\n\ndef create_parser():\n    \"\"\"Return command-line parser.\"\"\"\n    parser = argparse.ArgumentParser(description=docstring_summary(__doc__),\n                                     prog='autopep8')\n    parser.add_argument('--version', action='version',\n                        version='%(prog)s {} ({})'.format(\n                            __version__, _get_package_version()))\n    parser.add_argument('-v', '--verbose', action='count',\n                        default=0,\n                        help='print verbose messages; '\n                             'multiple -v result in more verbose messages')\n    parser.add_argument('-d', '--diff', action='store_true',\n                        help='print the diff for the fixed source')\n    parser.add_argument('-i', '--in-place', action='store_true',\n                        help='make changes to files in place')\n    parser.add_argument('--global-config', metavar='filename',\n                        default=DEFAULT_CONFIG,\n                        help='path to a global pep8 config file; if this file '\n                             'does not exist then this is ignored '\n                             '(default: {})'.format(DEFAULT_CONFIG))\n    parser.add_argument('--ignore-local-config', action='store_true',\n                        help=\"don't look for and apply local config files; \"\n                             'if not passed, defaults are updated with any '\n                             \"config files in the project's root directory\")\n    parser.add_argument('-r', '--recursive', action='store_true',\n                        help='run recursively over directories; '\n                             'must be used with --in-place or --diff')\n    parser.add_argument('-j', '--jobs', type=int, metavar='n', default=1,\n                        help='number of parallel jobs; '\n                             'match CPU count if value is less than 1')\n    parser.add_argument('-p', '--pep8-passes', metavar='n',\n                        default=-1, type=int,\n                        help='maximum number of additional pep8 passes '\n                             '(default: infinite)')\n    parser.add_argument('-a', '--aggressive', action='count', default=0,\n                        help='enable non-whitespace changes; '\n                             'multiple -a result in more aggressive changes')\n    parser.add_argument('--experimental', action='store_true',\n                        help='enable experimental fixes')\n    parser.add_argument('--exclude', metavar='globs',\n                        help='exclude file/directory names that match these '\n                             'comma-separated globs')\n    parser.add_argument('--list-fixes', action='store_true',\n                        help='list codes for fixes; '\n                        'used by --ignore and --select')\n    parser.add_argument('--ignore', metavar='errors', default='',\n                        help='do not fix these errors/warnings '\n                             '(default: {})'.format(DEFAULT_IGNORE))\n    parser.add_argument('--select', metavar='errors', default='',\n                        help='fix only these errors/warnings (e.g. E4,W)')\n    parser.add_argument('--max-line-length', metavar='n', default=79, type=int,\n                        help='set maximum allowed line length '\n                             '(default: %(default)s)')\n    parser.add_argument('--line-range', '--range', metavar='line',\n                        default=None, type=int, nargs=2,\n                        help='only fix errors found within this inclusive '\n                             'range of line numbers (e.g. 1 99); '\n                             'line numbers are indexed at 1')\n    parser.add_argument('--indent-size', default=DEFAULT_INDENT_SIZE,\n                        type=int, help=argparse.SUPPRESS)\n    parser.add_argument('--hang-closing', action='store_true',\n                        help='hang-closing option passed to pycodestyle')\n    parser.add_argument('--exit-code', action='store_true',\n                        help='change to behavior of exit code.'\n                             ' default behavior of return value, 0 is no '\n                             'differences, 1 is error exit. return 2 when'\n                             ' add this option. 2 is exists differences.')\n    parser.add_argument('files', nargs='*',\n                        help=\"files to format or '-' for standard in\")\n\n    return parser\n\n\ndef _expand_codes(codes, ignore_codes):\n    \"\"\"expand to individual E/W codes\"\"\"\n    ret = set()\n\n    is_conflict = False\n    if all(\n            any(\n                conflicting_code.startswith(code)\n                for code in codes\n            )\n            for conflicting_code in CONFLICTING_CODES\n    ):\n        is_conflict = True\n\n    is_ignore_w503 = \"W503\" in ignore_codes\n    is_ignore_w504 = \"W504\" in ignore_codes\n\n    for code in codes:\n        if code == \"W\":\n            if is_ignore_w503 and is_ignore_w504:\n                ret.update({\"W1\", \"W2\", \"W3\", \"W505\", \"W6\"})\n            elif is_ignore_w503:\n                ret.update({\"W1\", \"W2\", \"W3\", \"W504\", \"W505\", \"W6\"})\n            else:\n                ret.update({\"W1\", \"W2\", \"W3\", \"W503\", \"W505\", \"W6\"})\n        elif code in (\"W5\", \"W50\"):\n            if is_ignore_w503 and is_ignore_w504:\n                ret.update({\"W505\"})\n            elif is_ignore_w503:\n                ret.update({\"W504\", \"W505\"})\n            else:\n                ret.update({\"W503\", \"W505\"})\n        elif not (code in (\"W503\", \"W504\") and is_conflict):\n            ret.add(code)\n\n    return ret\n\n\ndef _parser_error_with_code(\n    parser: argparse.ArgumentParser, code: int, msg: str,\n) -> None:\n    \"\"\"wrap parser.error with exit code\"\"\"\n    parser.print_usage(sys.stderr)\n    parser.exit(code, f\"{msg}\\n\")\n\n\ndef parse_args(arguments, apply_config=False):\n    \"\"\"Parse command-line options.\"\"\"\n    parser = create_parser()\n    args = parser.parse_args(arguments)\n\n    if not args.files and not args.list_fixes:\n        _parser_error_with_code(\n            parser, EXIT_CODE_ARGPARSE_ERROR, 'incorrect number of arguments',\n        )\n\n    args.files = [decode_filename(name) for name in args.files]\n\n    if apply_config:\n        parser = read_config(args, parser)\n        # prioritize settings when exist pyproject.toml's tool.autopep8 section\n        try:\n            parser_with_pyproject_toml = read_pyproject_toml(args, parser)\n        except Exception:\n            parser_with_pyproject_toml = None\n        if parser_with_pyproject_toml:\n            parser = parser_with_pyproject_toml\n        args = parser.parse_args(arguments)\n        args.files = [decode_filename(name) for name in args.files]\n\n    if '-' in args.files:\n        if len(args.files) > 1:\n            _parser_error_with_code(\n                parser,\n                EXIT_CODE_ARGPARSE_ERROR,\n                'cannot mix stdin and regular files',\n            )\n\n        if args.diff:\n            _parser_error_with_code(\n                parser,\n                EXIT_CODE_ARGPARSE_ERROR,\n                '--diff cannot be used with standard input',\n            )\n\n        if args.in_place:\n            _parser_error_with_code(\n                parser,\n                EXIT_CODE_ARGPARSE_ERROR,\n                '--in-place cannot be used with standard input',\n            )\n\n        if args.recursive:\n            _parser_error_with_code(\n                parser,\n                EXIT_CODE_ARGPARSE_ERROR,\n                '--recursive cannot be used with standard input',\n            )\n\n    if len(args.files) > 1 and not (args.in_place or args.diff):\n        _parser_error_with_code(\n            parser,\n            EXIT_CODE_ARGPARSE_ERROR,\n            'autopep8 only takes one filename as argument '\n            'unless the \"--in-place\" or \"--diff\" args are used',\n        )\n\n    if args.recursive and not (args.in_place or args.diff):\n        _parser_error_with_code(\n            parser,\n            EXIT_CODE_ARGPARSE_ERROR,\n            '--recursive must be used with --in-place or --diff',\n        )\n\n    if args.in_place and args.diff:\n        _parser_error_with_code(\n            parser,\n            EXIT_CODE_ARGPARSE_ERROR,\n            '--in-place and --diff are mutually exclusive',\n        )\n\n    if args.max_line_length <= 0:\n        _parser_error_with_code(\n            parser,\n            EXIT_CODE_ARGPARSE_ERROR,\n            '--max-line-length must be greater than 0',\n        )\n\n    if args.indent_size <= 0:\n        _parser_error_with_code(\n            parser,\n            EXIT_CODE_ARGPARSE_ERROR,\n            '--indent-size must be greater than 0',\n        )\n\n    if args.select:\n        args.select = _expand_codes(\n            _split_comma_separated(args.select),\n            (_split_comma_separated(args.ignore) if args.ignore else [])\n        )\n\n    if args.ignore:\n        args.ignore = _split_comma_separated(args.ignore)\n        if all(\n                not any(\n                    conflicting_code.startswith(ignore_code)\n                    for ignore_code in args.ignore\n                )\n                for conflicting_code in CONFLICTING_CODES\n        ):\n            args.ignore.update(CONFLICTING_CODES)\n    elif not args.select:\n        if args.aggressive:\n            # Enable everything by default if aggressive.\n            args.select = {'E', 'W1', 'W2', 'W3', 'W6'}\n        else:\n            args.ignore = _split_comma_separated(DEFAULT_IGNORE)\n\n    if args.exclude:\n        args.exclude = _split_comma_separated(args.exclude)\n    else:\n        args.exclude = {}\n\n    if args.jobs < 1:\n        # Do not import multiprocessing globally in case it is not supported\n        # on the platform.\n        import multiprocessing\n        args.jobs = multiprocessing.cpu_count()\n\n    if args.jobs > 1 and not (args.in_place or args.diff):\n        _parser_error_with_code(\n            parser,\n            EXIT_CODE_ARGPARSE_ERROR,\n            'parallel jobs requires --in-place',\n        )\n\n    if args.line_range:\n        if args.line_range[0] <= 0:\n            _parser_error_with_code(\n                parser,\n                EXIT_CODE_ARGPARSE_ERROR,\n                '--range must be positive numbers',\n            )\n        if args.line_range[0] > args.line_range[1]:\n            _parser_error_with_code(\n                parser,\n                EXIT_CODE_ARGPARSE_ERROR,\n                'First value of --range should be less than or equal '\n                'to the second',\n            )\n\n    original_formatwarning = warnings.formatwarning\n    warnings.formatwarning = _custom_formatwarning\n    if args.experimental:\n        warnings.warn(\n            \"`experimental` option is deprecated and will be \"\n            \"removed in a future version.\",\n            DeprecationWarning,\n        )\n    warnings.formatwarning = original_formatwarning\n\n    return args\n\n\ndef _get_normalize_options(args, config, section, option_list):\n    for (k, v) in config.items(section):\n        norm_opt = k.lstrip('-').replace('-', '_')\n        if not option_list.get(norm_opt):\n            continue\n        opt_type = option_list[norm_opt]\n        if opt_type is int:\n            if v.strip() == \"auto\":\n                # skip to special case\n                if args.verbose:\n                    print(f\"ignore config: {k}={v}\")\n                continue\n            value = config.getint(section, k)\n        elif opt_type is bool:\n            value = config.getboolean(section, k)\n        else:\n            value = config.get(section, k)\n        yield norm_opt, k, value\n\n\ndef read_config(args, parser):\n    \"\"\"Read both user configuration and local configuration.\"\"\"\n    config = SafeConfigParser()\n\n    try:\n        if args.verbose and os.path.exists(args.global_config):\n            print(\"read config path: {}\".format(args.global_config))\n        config.read(args.global_config)\n\n        if not args.ignore_local_config:\n            parent = tail = args.files and os.path.abspath(\n                os.path.commonprefix(args.files))\n            while tail:\n                if config.read([os.path.join(parent, fn)\n                                for fn in PROJECT_CONFIG]):\n                    if args.verbose:\n                        for fn in PROJECT_CONFIG:\n                            config_file = os.path.join(parent, fn)\n                            if not os.path.exists(config_file):\n                                continue\n                            print(\n                                \"read config path: {}\".format(\n                                    os.path.join(parent, fn)\n                                )\n                            )\n                    break\n                (parent, tail) = os.path.split(parent)\n\n        defaults = {}\n        option_list = {o.dest: o.type or type(o.default)\n                       for o in parser._actions}\n\n        for section in ['pep8', 'pycodestyle', 'flake8']:\n            if not config.has_section(section):\n                continue\n            for norm_opt, k, value in _get_normalize_options(\n                args, config, section, option_list\n            ):\n                if args.verbose:\n                    print(\"enable config: section={}, key={}, value={}\".format(\n                        section, k, value))\n                defaults[norm_opt] = value\n\n        parser.set_defaults(**defaults)\n    except Error:\n        # Ignore for now.\n        pass\n\n    return parser\n\n\ndef read_pyproject_toml(args, parser):\n    \"\"\"Read pyproject.toml and load configuration.\"\"\"\n    if sys.version_info >= (3, 11):\n        import tomllib\n    else:\n        import tomli as tomllib\n\n    config = None\n\n    if os.path.exists(args.global_config):\n        with open(args.global_config, \"rb\") as fp:\n            config = tomllib.load(fp)\n\n    if not args.ignore_local_config:\n        parent = tail = args.files and os.path.abspath(\n            os.path.commonprefix(args.files))\n        while tail:\n            pyproject_toml = os.path.join(parent, \"pyproject.toml\")\n            if os.path.exists(pyproject_toml):\n                with open(pyproject_toml, \"rb\") as fp:\n                    config = tomllib.load(fp)\n                    break\n            (parent, tail) = os.path.split(parent)\n\n    if not config:\n        return None\n\n    if config.get(\"tool\", {}).get(\"autopep8\") is None:\n        return None\n\n    config = config.get(\"tool\", {}).get(\"autopep8\")\n\n    defaults = {}\n    option_list = {o.dest: o.type or type(o.default)\n                   for o in parser._actions}\n\n    TUPLED_OPTIONS = (\"ignore\", \"select\")\n    for (k, v) in config.items():\n        norm_opt = k.lstrip('-').replace('-', '_')\n        if not option_list.get(norm_opt):\n            continue\n        if type(v) in (list, tuple) and norm_opt in TUPLED_OPTIONS:\n            value = \",\".join(v)\n        else:\n            value = v\n        if args.verbose:\n            print(\"enable pyproject.toml config: \"\n                  \"key={}, value={}\".format(k, value))\n        defaults[norm_opt] = value\n\n    if defaults:\n        # set value when exists key-value in defaults dict\n        parser.set_defaults(**defaults)\n\n    return parser\n\n\ndef _split_comma_separated(string):\n    \"\"\"Return a set of strings.\"\"\"\n    return {text.strip() for text in string.split(',') if text.strip()}\n\n\ndef decode_filename(filename):\n    \"\"\"Return Unicode filename.\"\"\"\n    if isinstance(filename, str):\n        return filename\n\n    return filename.decode(sys.getfilesystemencoding())\n\n\ndef supported_fixes():\n    \"\"\"Yield pep8 error codes that autopep8 fixes.\n\n    Each item we yield is a tuple of the code followed by its\n    description.\n\n    \"\"\"\n    yield ('E101', docstring_summary(reindent.__doc__))\n\n    instance = FixPEP8(filename=None, options=None, contents='')\n    for attribute in dir(instance):\n        code = re.match('fix_([ew][0-9][0-9][0-9])', attribute)\n        if code:\n            yield (\n                code.group(1).upper(),\n                re.sub(r'\\s+', ' ',\n                       docstring_summary(getattr(instance, attribute).__doc__))\n            )\n\n    for (code, function) in sorted(global_fixes()):\n        yield (code.upper() + (4 - len(code)) * ' ',\n               re.sub(r'\\s+', ' ', docstring_summary(function.__doc__)))\n\n\ndef docstring_summary(docstring):\n    \"\"\"Return summary of docstring.\"\"\"\n    return docstring.split('\\n')[0] if docstring else ''\n\n\ndef line_shortening_rank(candidate, indent_word, max_line_length,\n                         experimental=False):\n    \"\"\"Return rank of candidate.\n\n    This is for sorting candidates.\n\n    \"\"\"\n    if not candidate.strip():\n        return 0\n\n    rank = 0\n    lines = candidate.rstrip().split('\\n')\n\n    offset = 0\n    if (\n        not lines[0].lstrip().startswith('#') and\n        lines[0].rstrip()[-1] not in '([{'\n    ):\n        for (opening, closing) in ('()', '[]', '{}'):\n            # Don't penalize empty containers that aren't split up. Things like\n            # this \"foo(\\n    )\" aren't particularly good.\n            opening_loc = lines[0].find(opening)\n            closing_loc = lines[0].find(closing)\n            if opening_loc >= 0:\n                if closing_loc < 0 or closing_loc != opening_loc + 1:\n                    offset = max(offset, 1 + opening_loc)\n\n    current_longest = max(offset + len(x.strip()) for x in lines)\n\n    rank += 4 * max(0, current_longest - max_line_length)\n\n    rank += len(lines)\n\n    # Too much variation in line length is ugly.\n    rank += 2 * standard_deviation(len(line) for line in lines)\n\n    bad_staring_symbol = {\n        '(': ')',\n        '[': ']',\n        '{': '}'}.get(lines[0][-1])\n\n    if len(lines) > 1:\n        if (\n            bad_staring_symbol and\n            lines[1].lstrip().startswith(bad_staring_symbol)\n        ):\n            rank += 20\n\n    for lineno, current_line in enumerate(lines):\n        current_line = current_line.strip()\n\n        if current_line.startswith('#'):\n            continue\n\n        for bad_start in ['.', '%', '+', '-', '/']:\n            if current_line.startswith(bad_start):\n                rank += 100\n\n            # Do not tolerate operators on their own line.\n            if current_line == bad_start:\n                rank += 1000\n\n        if (\n            current_line.endswith(('.', '%', '+', '-', '/')) and\n            \"': \" in current_line\n        ):\n            rank += 1000\n\n        if current_line.endswith(('(', '[', '{', '.')):\n            # Avoid lonely opening. They result in longer lines.\n            if len(current_line) <= len(indent_word):\n                rank += 100\n\n            # Avoid the ugliness of \", (\\n\".\n            if (\n                current_line.endswith('(') and\n                current_line[:-1].rstrip().endswith(',')\n            ):\n                rank += 100\n\n            # Avoid the ugliness of \"something[\\n\" and something[index][\\n.\n            if (\n                current_line.endswith('[') and\n                len(current_line) > 1 and\n                (current_line[-2].isalnum() or current_line[-2] in ']')\n            ):\n                rank += 300\n\n            # Also avoid the ugliness of \"foo.\\nbar\"\n            if current_line.endswith('.'):\n                rank += 100\n\n            if has_arithmetic_operator(current_line):\n                rank += 100\n\n        # Avoid breaking at unary operators.\n        if re.match(r'.*[(\\[{]\\s*[\\-\\+~]$', current_line.rstrip('\\\\ ')):\n            rank += 1000\n\n        if re.match(r'.*lambda\\s*\\*$', current_line.rstrip('\\\\ ')):\n            rank += 1000\n\n        if current_line.endswith(('%', '(', '[', '{')):\n            rank -= 20\n\n        # Try to break list comprehensions at the \"for\".\n        if current_line.startswith('for '):\n            rank -= 50\n\n        if current_line.endswith('\\\\'):\n            # If a line ends in \\-newline, it may be part of a\n            # multiline string. In that case, we would like to know\n            # how long that line is without the \\-newline. If it's\n            # longer than the maximum, or has comments, then we assume\n            # that the \\-newline is an okay candidate and only\n            # penalize it a bit.\n            total_len = len(current_line)\n            lineno += 1\n            while lineno < len(lines):\n                total_len += len(lines[lineno])\n\n                if lines[lineno].lstrip().startswith('#'):\n                    total_len = max_line_length\n                    break\n\n                if not lines[lineno].endswith('\\\\'):\n                    break\n\n                lineno += 1\n\n            if total_len < max_line_length:\n                rank += 10\n            else:\n                rank += 100 if experimental else 1\n\n        # Prefer breaking at commas rather than colon.\n        if ',' in current_line and current_line.endswith(':'):\n            rank += 10\n\n        # Avoid splitting dictionaries between key and value.\n        if current_line.endswith(':'):\n            rank += 100\n\n        rank += 10 * count_unbalanced_brackets(current_line)\n\n    return max(0, rank)\n\n\ndef standard_deviation(numbers):\n    \"\"\"Return standard deviation.\"\"\"\n    numbers = list(numbers)\n    if not numbers:\n        return 0\n    mean = sum(numbers) / len(numbers)\n    return (sum((n - mean) ** 2 for n in numbers) /\n            len(numbers)) ** .5\n\n\ndef has_arithmetic_operator(line):\n    \"\"\"Return True if line contains any arithmetic operators.\"\"\"\n    for operator in pycodestyle.ARITHMETIC_OP:\n        if operator in line:\n            return True\n\n    return False\n\n\ndef count_unbalanced_brackets(line):\n    \"\"\"Return number of unmatched open/close brackets.\"\"\"\n    count = 0\n    for opening, closing in ['()', '[]', '{}']:\n        count += abs(line.count(opening) - line.count(closing))\n\n    return count\n\n\ndef split_at_offsets(line, offsets):\n    \"\"\"Split line at offsets.\n\n    Return list of strings.\n\n    \"\"\"\n    result = []\n\n    previous_offset = 0\n    current_offset = 0\n    for current_offset in sorted(offsets):\n        if current_offset < len(line) and previous_offset != current_offset:\n            result.append(line[previous_offset:current_offset].strip())\n        previous_offset = current_offset\n\n    result.append(line[current_offset:])\n\n    return result\n\n\nclass LineEndingWrapper(object):\n\n    r\"\"\"Replace line endings to work with sys.stdout.\n\n    It seems that sys.stdout expects only '\\n' as the line ending, no matter\n    the platform. Otherwise, we get repeated line endings.\n\n    \"\"\"\n\n    def __init__(self, output):\n        self.__output = output\n\n    def write(self, s):\n        self.__output.write(s.replace('\\r\\n', '\\n').replace('\\r', '\\n'))\n\n    def flush(self):\n        self.__output.flush()\n\n\ndef match_file(filename, exclude):\n    \"\"\"Return True if file is okay for modifying/recursing.\"\"\"\n    base_name = os.path.basename(filename)\n\n    if base_name.startswith('.'):\n        return False\n\n    for pattern in exclude:\n        if fnmatch.fnmatch(base_name, pattern):\n            return False\n        if fnmatch.fnmatch(filename, pattern):\n            return False\n\n    if not os.path.isdir(filename) and not is_python_file(filename):\n        return False\n\n    return True\n\n\ndef find_files(filenames, recursive, exclude):\n    \"\"\"Yield filenames.\"\"\"\n    while filenames:\n        name = filenames.pop(0)\n        if recursive and os.path.isdir(name):\n            for root, directories, children in os.walk(name):\n                filenames += [os.path.join(root, f) for f in children\n                              if match_file(os.path.join(root, f),\n                                            exclude)]\n                directories[:] = [d for d in directories\n                                  if match_file(os.path.join(root, d),\n                                                exclude)]\n        else:\n            is_exclude_match = False\n            for pattern in exclude:\n                if fnmatch.fnmatch(name, pattern):\n                    is_exclude_match = True\n                    break\n            if not is_exclude_match:\n                yield name\n\n\ndef _fix_file(parameters):\n    \"\"\"Helper function for optionally running fix_file() in parallel.\"\"\"\n    if parameters[1].verbose:\n        print('[file:{}]'.format(parameters[0]), file=sys.stderr)\n    try:\n        return fix_file(*parameters)\n    except IOError as error:\n        print(str(error), file=sys.stderr)\n        raise error\n\n\ndef fix_multiple_files(filenames, options, output=None):\n    \"\"\"Fix list of files.\n\n    Optionally fix files recursively.\n\n    \"\"\"\n    results = []\n    filenames = find_files(filenames, options.recursive, options.exclude)\n    if options.jobs > 1:\n        import multiprocessing\n        pool = multiprocessing.Pool(options.jobs)\n        rets = []\n        for name in filenames:\n            ret = pool.apply_async(_fix_file, ((name, options),))\n            rets.append(ret)\n        pool.close()\n        pool.join()\n        if options.diff:\n            for r in rets:\n                sys.stdout.write(r.get().decode())\n                sys.stdout.flush()\n        results.extend([x.get() for x in rets if x is not None])\n    else:\n        for name in filenames:\n            ret = _fix_file((name, options, output))\n            if ret is None:\n                continue\n            if options.diff:\n                if ret != '':\n                    results.append(ret)\n            elif options.in_place:\n                results.append(ret)\n            else:\n                original_source = readlines_from_file(name)\n                if \"\".join(original_source).splitlines() != ret.splitlines():\n                    results.append(ret)\n    return results\n\n\ndef is_python_file(filename):\n    \"\"\"Return True if filename is Python file.\"\"\"\n    if filename.endswith('.py'):\n        return True\n\n    try:\n        with open_with_encoding(\n                filename,\n                limit_byte_check=MAX_PYTHON_FILE_DETECTION_BYTES) as f:\n            text = f.read(MAX_PYTHON_FILE_DETECTION_BYTES)\n            if not text:\n                return False\n            first_line = text.splitlines()[0]\n    except (IOError, IndexError):\n        return False\n\n    if not PYTHON_SHEBANG_REGEX.match(first_line):\n        return False\n\n    return True\n\n\ndef is_probably_part_of_multiline(line):\n    \"\"\"Return True if line is likely part of a multiline string.\n\n    When multiline strings are involved, pep8 reports the error as being\n    at the start of the multiline string, which doesn't work for us.\n\n    \"\"\"\n    return (\n        '\"\"\"' in line or\n        \"'''\" in line or\n        line.rstrip().endswith('\\\\')\n    )\n\n\ndef wrap_output(output, encoding):\n    \"\"\"Return output with specified encoding.\"\"\"\n    return codecs.getwriter(encoding)(output.buffer\n                                      if hasattr(output, 'buffer')\n                                      else output)\n\n\ndef get_encoding():\n    \"\"\"Return preferred encoding.\"\"\"\n    return locale.getpreferredencoding() or sys.getdefaultencoding()\n\n\ndef main(argv=None, apply_config=True):\n    \"\"\"Command-line entry.\"\"\"\n    if argv is None:\n        argv = sys.argv\n\n    try:\n        # Exit on broken pipe.\n        signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n    except AttributeError:  # pragma: no cover\n        # SIGPIPE is not available on Windows.\n        pass\n\n    try:\n        args = parse_args(argv[1:], apply_config=apply_config)\n\n        if args.list_fixes:\n            for code, description in sorted(supported_fixes()):\n                print('{code} - {description}'.format(\n                    code=code, description=description))\n            return EXIT_CODE_OK\n\n        if args.files == ['-']:\n            assert not args.in_place\n\n            encoding = sys.stdin.encoding or get_encoding()\n            read_stdin = sys.stdin.read()\n            fixed_stdin = fix_code(read_stdin, args, encoding=encoding)\n\n            # LineEndingWrapper is unnecessary here due to the symmetry between\n            # standard in and standard out.\n            wrap_output(sys.stdout, encoding=encoding).write(fixed_stdin)\n\n            if hash(read_stdin) != hash(fixed_stdin):\n                if args.exit_code:\n                    return EXIT_CODE_EXISTS_DIFF\n        else:\n            if args.in_place or args.diff:\n                args.files = list(set(args.files))\n            else:\n                assert len(args.files) == 1\n                assert not args.recursive\n\n            results = fix_multiple_files(args.files, args, sys.stdout)\n            if args.diff:\n                ret = any([len(ret) != 0 for ret in results])\n            else:\n                # with in-place option\n                ret = any([ret is not None for ret in results])\n            if args.exit_code and ret:\n                return EXIT_CODE_EXISTS_DIFF\n    except IOError:\n        return EXIT_CODE_ERROR\n    except KeyboardInterrupt:\n        return EXIT_CODE_ERROR  # pragma: no cover\n\n\nclass CachedTokenizer(object):\n\n    \"\"\"A one-element cache around tokenize.generate_tokens().\n\n    Original code written by Ned Batchelder, in coverage.py.\n\n    \"\"\"\n\n    def __init__(self):\n        self.last_text = None\n        self.last_tokens = None\n\n    def generate_tokens(self, text):\n        \"\"\"A stand-in for tokenize.generate_tokens().\"\"\"\n        if text != self.last_text:\n            string_io = io.StringIO(text)\n            self.last_tokens = list(\n                tokenize.generate_tokens(string_io.readline)\n            )\n            self.last_text = text\n        return self.last_tokens\n\n\n_cached_tokenizer = CachedTokenizer()\ngenerate_tokens = _cached_tokenizer.generate_tokens\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n"
        },
        {
          "name": "hooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "install_hooks.bash",
          "type": "blob",
          "size": 0.08984375,
          "content": "#!/bin/bash -ex\n\nreadonly root=$(dirname \"$0\")\ncd \"$root\"/.git/hooks\nln -fs ../../hooks/* .\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.4033203125,
          "content": "[project]\nname = \"autopep8\"\ndescription = \"A tool that automatically formats Python code to conform to the PEP 8 style guide\"\nlicense = {file = \"LICENSE.rst\"}\nauthors = [\n    {name = \"Hideo Hattori\", email = \"hhatto.jp@gmail.com\"},\n]\nreadme = \"README.rst\"\nkeywords = [\n    \"automation\",\n    \"pep8\",\n    \"format\",\n    \"pycodestyle\",\n]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Environment :: Console\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n    \"Topic :: Software Development :: Quality Assurance\",\n]\nrequires-python = \">=3.8\"\ndependencies = [\n    \"pycodestyle >= 2.12.0\",\n    \"tomli; python_version < '3.11'\",\n]\ndynamic = [\"version\"]\n\n[project.urls]\nRepository = \"https://github.com/hhatto/autopep8\"\n\n[project.scripts]\nautopep8 = \"autopep8:main\"\n\n[tool.setuptools]\npy-modules = [\"autopep8\"]\n\n[tool.setuptools.dynamic]\nversion = {attr = \"autopep8.__version__\"}\n\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.02734375,
          "content": "[bdist_wheel]\nuniversal = 1\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.2802734375,
          "content": "[tox]\nenvlist=py38,py39,py310,py311,py312\nskip_missing_interpreters=True\n\n[testenv]\ncommands=\n    python test/test_autopep8.py\n    python test/acid.py --aggressive test/example.py\n    python test/acid.py --compare-bytecode test/example.py\ndeps=\n    pycodestyle>=2.12.0\n    pydiff>=0.1.2\n"
        },
        {
          "name": "update_readme.py",
          "type": "blob",
          "size": 2.3408203125,
          "content": "#!/usr/bin/env python\n\n\"\"\"Update example in readme.\"\"\"\n\nimport io\nimport os\nimport sys\nimport textwrap\n\nimport pyflakes.api\nimport pyflakes.messages\nimport pyflakes.reporter\n\nimport autopep8\n\n\ndef split_readme(readme_path, before_key, after_key, options_key, end_key):\n    \"\"\"Return split readme.\"\"\"\n    with open(readme_path) as readme_file:\n        readme = readme_file.read()\n\n    top, rest = readme.split(before_key)\n    before, rest = rest.split(after_key)\n    _, rest = rest.split(options_key)\n    _, bottom = rest.split(end_key)\n\n    return (top.rstrip('\\n'),\n            before.strip('\\n'),\n            end_key + '\\n\\n' + bottom.lstrip('\\n'))\n\n\ndef indent_line(line):\n    \"\"\"Indent non-empty lines.\"\"\"\n    if line:\n        return 4 * ' ' + line\n\n    return line\n\n\ndef indent(text):\n    \"\"\"Indent text by four spaces.\"\"\"\n    return '\\n'.join(indent_line(line) for line in text.split('\\n'))\n\n\ndef help_message():\n    \"\"\"Return help output.\"\"\"\n    parser = autopep8.create_parser()\n    string_io = io.StringIO()\n    parser.print_help(string_io)\n    # Undo home directory expansion.\n    return string_io.getvalue().replace(os.path.expanduser('~'), '~')\n\n\ndef check(source):\n    \"\"\"Check code.\"\"\"\n    compile(source, '<string>', 'exec', dont_inherit=True)\n    reporter = pyflakes.reporter.Reporter(sys.stderr, sys.stderr)\n    pyflakes.api.check(source, filename='<string>', reporter=reporter)\n\n\ndef main():\n    readme_path = 'README.rst'\n    before_key = 'Before running autopep8.\\n\\n.. code-block:: python'\n    after_key = 'After running autopep8.\\n\\n.. code-block:: python'\n    options_key = 'Options::'\n\n    (top, before, bottom) = split_readme(readme_path,\n                                         before_key=before_key,\n                                         after_key=after_key,\n                                         options_key=options_key,\n                                         end_key='Features\\n========')\n\n    input_code = textwrap.dedent(before)\n\n    output_code = autopep8.fix_code(\n        input_code,\n        options={'aggressive': 2})\n\n    check(output_code)\n\n    new_readme = '\\n\\n'.join([\n        top,\n        before_key, before,\n        after_key, indent(output_code).rstrip(),\n        options_key, indent(help_message()),\n        bottom])\n\n    with open(readme_path, 'w') as output_file:\n        output_file.write(new_readme)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    }
  ]
}