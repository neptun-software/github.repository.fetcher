{
  "metadata": {
    "timestamp": 1736559609939,
    "page": 249,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Kr1s77/Python-crawler-tutorial-starts-from-zero",
      "stars": 4393,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.099609375,
          "content": "   *.js linguist-language=python\n   *.css linguist-language=python\n   *.html linguist-language=python\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1396484375,
          "content": "# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# pyenv\n.python-version\n\n# Mac os\n.DS_Store \n*/.DS_Store \n\n# pycharm\n.idea\n.env\n\n"
        },
        {
          "name": "01-豆瓣电影.md",
          "type": "blob",
          "size": 1.39453125,
          "content": "   * [豆瓣电影](#豆瓣电影)\n      * [1. 分析](#1-分析)\n         * [分析流程图](#分析流程图)\n         * [分析结果](#分析结果)\n         * [代码实现流程分析](#代码实现流程分析)\n         * [具体代码](#具体代码)\n\n# 豆瓣电影\n## 1. 分析\n### 分析流程图\n![](./images/豆瓣电影分析图.png)\n\n### 分析结果\n- 结果概要\n\n分析目标 | 分析结果\n------------- | -------------\n请求URL分析\t| \thttps://movie.douban.com/j/search_subjects\n请求方式分析 | GET\n请求参数分析 | 上图中所示请求参数\n请求头分析 | 上图中请求头\n\n- 注意:\n\n> 请求头一般都是先放``User-Agent``，如果爬取失败再补``Referer``，还是失败就再补``Cookie``，如果喜欢稳一点的，可以每次都加上\n\n- 请求参数分析\n\n参数KEY | 分析结果\n------------- | -------------\ntype\t| \tmovie（固定值不变）\ntag | 热门（固定值不变）\nsort | recommend（固定值不变）\npage_limit | 20（固定值不变，表示每页数量）\npage_start | 0（每次请求发生变化，每20页进行叠加，表示数据的偏移量）\n\n### 代码实现流程分析\n1. 先完成一次请求的抓取\n2. 再完成多次请求的爬取\n- 总结:\n\n> 循序渐进养成良好的习惯\n\n### 具体代码\n\n[查看代码请点击此处](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/code_demo/douban.py)\n "
        },
        {
          "name": "02-百度贴吧.md",
          "type": "blob",
          "size": 3.2900390625,
          "content": "   * [百度贴吧爬虫](#百度贴吧爬虫)\n      * [分析](#分析)\n         * [分析流程图](#分析流程图)\n         * [分析结果](#分析结果)\n            * [结果概要](#结果概要)\n         * [代码实现流程](#代码实现流程)\n      * [代码实现](#代码实现)\n\n# 百度贴吧爬虫\n## 分析\n### 分析流程图\n\n> 分析 ``url`` 的时候我们一般都是从第二页开始分析，可以看出 ``url`` 的变化\n\n![](./images/百度贴吧分析.jpg)\n\n### 分析结果\n#### 结果概要\n\n| 请求目标                           | 分析结果            |\n|-----------------------------------|-------------------|\n| 请求方式分析                        | GET                                 |\n| 请求参数分析                        | pn每页50发生变化，其他参数固定不变       |\n| 请求头分析                          | 只需要添加User-Agent                  |\n| 请求url分析                         | https://tieba.baidu.com/f?kw=英雄联盟&ie=utf-8&pn=50 \n\n### 代码实现流程\n1. 实现面向对象构建爬虫对象\n2. 爬虫流程四步骤\n\t1. 获取 url 列表\n\t2. 发送请求获取响应\n\t3. 从响应中提取数据\n\t4. 保存数据\n\n\n## 代码实现\n\n[详细代码请点击](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/code_demo/Tieba.py)\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\nimport requests\n\nclass TiebaSpider():\n\n    def __init__(self,kw,max_pn):\n        self.max_pn = max_pn\n        self.kw = kw\n        self.base_url = \"https://tieba.baidu.com/f?kw={}&ie=utf-8&pn={}\"\n        self.headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36\"\n        }\n        pass\n\n    def get_url_list(self):\n        '''\n        获取 url 列表\n        :return: \n        '''\n        # 写法一\n        '''\n        url_list = []\n\n        for pn in range(0,self.max_pn,50):\n            url = self.base_url.format(self.kw,pn)\n            url_list.append(url)\n\n        return url_list\n        '''\n        # 写法二\n        return [self.base_url.format(self.kw,pn) for pn in range(0,self.max_pn,50)]\n\n    def get_content(self,url):\n        '''\n        发送请求获取响应内容\n        :param url: \n        :return: \n        '''\n        response = requests.get(\n            url=url,\n            headers = self.headers\n        )\n\n        return response.content\n\n    def get_items(self,content,idx):\n        '''\n        从响应内容中提取数据\n        :param content: \n        :return: \n        '''\n        with open('08-{}.html'.format(idx),'wb') as f:\n            f.write(content)\n        return None\n\n    def save_items(self,items):\n        '''\n        保存数据\n        :param items: \n        :return: \n        '''\n        pass\n\n\n    def run(self):\n\n        # 1. 获取 url 列表\n        url_list = self.get_url_list()\n\n        for url in url_list:\n            # 2. 发送请求获取响应\n            content = self.get_content(url)\n\n            # 3. 从响应中提取数据\n            items = self.get_items(content,url_list.index(url) + 1)\n\n            # 4. 保存数据\n            self.save_items(items)\n\n        pass\n\nif __name__ == '__main__':\n    spider = TiebaSpider(\"英雄联盟\",150)\n    spider.run()\n```"
        },
        {
          "name": "03-百度翻译.md",
          "type": "blob",
          "size": 9.8173828125,
          "content": "   * [百度翻译爬虫](#百度翻译爬虫)\n      * [分析](#分析)\n         * [分析流程](#分析流程)\n         * [分析结果](#分析结果)\n            * [请求参数分析](#请求参数分析)\n         * [JS 逆向流程](#js-逆向流程)\n            * [chrome 调试技巧](#chrome-调试技巧)\n               * [JS 逆向流程](#js-逆向流程-1)\n            * [根据逆向流程查找我们所需的js代码](#根据逆向流程查找我们所需的js代码)\n         * [编写爬虫代码](#编写爬虫代码)\n            * [注意](#注意)\n            * [最后](#最后)\n         * [<a href=\"https://github.com/CriseLYJ/awesome-python-login-model/tree/master/baidu_translate\">完整代码</a>](#完整代码)\n\n# 百度翻译爬虫\n## 分析\n- 打开百度翻译\n\n> url : https://fanyi.baidu.com/#en/zh/\n\n### 分析流程\n\n1. 将浏览器切换至手机端，这样查看请求的参数可能会较少，点击下图中的按钮：\n\n![](./images/切换手机端.jpg)\n\n2. 查看请求接口\n\n![](./images/百度翻译请求.jpg)\n\n3. 查看请求参数是否有变化\n- 图一：\n![](./images/发送请求的数据.jpg)\n\n- 图二：\n![](./images/参数变化.jpg)\n\n> 经过分析，我们发现这里有两个参数发生变化，经过推断，这里的参数可能是使用js生成的，所以我们这里要对接口进行分析，破解，得到这里的值\n\n### 分析结果\n|--分析目标---|----分析结果-------------------|\n-------------|-------------------------------|\n|请求URL分析\t   |https://fanyi.baidu.com/basetrans|\n|请求方式分析\t| POST  |\n|请求参数分析 | \t参看请求参数分析|\n|请求头分析 |\t参看请求头分析|\n\n#### 请求参数分析\n\n参数KEY\t|分析结果\n-------|-------\n| query|\t翻译单词（变化）|\n|from\t|en（固定值不变）|\n|to\t |zh（固定值不变）|\n|token | 6f5c83b84d69ad3633abdf18abcb030d（经过请求测试，我们发现可以为固定值）|\n|sign |54706.276099|\n\n### JS 逆向流程\n\n> 注意： 在js逆向中，不是你要精通js才可以js逆向，你要懂一点js就可以做逆向，重要的逆向思维，对问题的思考方式\n\n#### chrome 调试技巧\n\n1. search 打开查询面板\n2. 查询面板可以通过关键字查找所有出现关键字地方的代码\n3. 点击跟踪代码并且可以把代码格式化\n4. 对格式化的代码进行设置断点\n5. 鼠标光标移动到上面可以查看当前运行代码变量值，函数原始代码地方等等\n\n##### JS 逆向流程\n\n1. 通过关键词切入到代码中，切入到发送请求的代码行，通过请求的url中提取关键字\n2. 在发送请求的代码添加断点，并且触发发送请求，确认寻找的代码是否正确\n3. 往上逆向，寻找目标参数以及生成逻辑\n4. 利用js2py模拟执行生成逻辑获取想要的内容\n\n\n#### 根据逆向流程查找我们所需的js代码\n\n1. 搜索关键字\n\n![](./images/查找百度翻译接口.jpg)\n\n2. 跟进代码，分析AJAX请求\n\n![](./images/跟进百度js.jpg)\n\n3. 查找我们需要的值\n\n![](./images/find_g.jpg)\n\n- 准确的找到我们需要的值\n\n![](./images/生成函数.png)\n\n- 复制我们需要的js代码\n\n![](./images/复制js代码.jpg)\n\n- 代码如下：\n\n```javascript\n        function a(r) {\n            var t = r.match(/[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]/g);\n            if (null === t) {\n                var a = r.length;\n                a > 30 && (r = \"\" + r.substr(0, 10) + r.substr(Math.floor(a / 2) - 5, 10) + r.substr(-10, 10))\n            } else {\n                for (var C = r.split(/[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]/), h = 0, f = C.length, u = []; f > h; h++)\n                    \"\" !== C[h] && u.push.apply(u, e(C[h].split(\"\"))),\n                    h !== f - 1 && u.push(t[h]);\n                var g = u.length;\n                g > 30 && (r = u.slice(0, 10).join(\"\") + u.slice(Math.floor(g / 2) - 5, Math.floor(g / 2) + 5).join(\"\") + u.slice(-10).join(\"\"))\n            }\n            var l = void 0\n              , d = \"\" + String.fromCharCode(103) + String.fromCharCode(116) + String.fromCharCode(107);\n            l = null !== i ? i : (i = o.common[d] || \"\") || \"\";\n            for (var m = l.split(\".\"), S = Number(m[0]) || 0, s = Number(m[1]) || 0, c = [], v = 0, F = 0; F < r.length; F++) {\n                var p = r.charCodeAt(F);\n                128 > p ? c[v++] = p : (2048 > p ? c[v++] = p >> 6 | 192 : (55296 === (64512 & p) && F + 1 < r.length && 56320 === (64512 & r.charCodeAt(F + 1)) ? (p = 65536 + ((1023 & p) << 10) + (1023 & r.charCodeAt(++F)),\n                c[v++] = p >> 18 | 240,\n                c[v++] = p >> 12 & 63 | 128) : c[v++] = p >> 12 | 224,\n                c[v++] = p >> 6 & 63 | 128),\n                c[v++] = 63 & p | 128)\n            }\n            for (var w = S, A = \"\" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(97) + (\"\" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(54)), b = \"\" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(51) + (\"\" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(98)) + (\"\" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(102)), D = 0; D < c.length; D++)\n                w += c[D],\n                w = n(w, A);\n            return w = n(w, b),\n            w ^= s,\n            0 > w && (w = (2147483647 & w) + 2147483648),\n            w %= 1e6,\n            w.toString() + \".\" + (w ^ S)\n        }\n```\n\n### 编写爬虫代码\n- 写代码之前，我们需要先了解一下 ``js2py`` 这个模块\n- ``js2py`` 模块的使用我会放在主要教程中，接下来我们就不浪费时间了，直接开始。\n\n```python\nimport requests\nimport js2py\n\ncontext = js2py.EvalJs()\n\n\nclass BaiDuTranslater(object):\n    \"\"\"\n    百度翻译爬虫\n    \"\"\"\n\n    def __init__(self, query):\n        # 初始化\n        self.url = \"https://fanyi.baidu.com/basetrans\"\n        self.query = query\n        self.headers = {\n            \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1\",\n            \"Referer\": \"https://fanyi.baidu.com/\",\n            \"Cookie\": \"BAIDUID=714BFAAF02DA927F583935C7A354949A:FG=1; BIDUPSID=714BFAAF02DA927F583935C7A354949A; PSTM=1553390486; delPer=0; PSINO=5; H_PS_PSSID=28742_1463_21125_18559_28723_28557_28697_28585_28640_28604_28626_22160; locale=zh; from_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; Hm_lvt_afd111fa62852d1f37001d1f980b6800=1553658863,1553766321,1553769980,1553770442; Hm_lpvt_afd111fa62852d1f37001d1f980b6800=1553770442; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1553766258,1553766321,1553769980,1553770442; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1553770442\"\n        }\n\n    def make_sign(self):\n        # js逆向获取sign的值\n        # 读取js文件\n        with open(\"translate.js\", \"r\", encoding=\"utf-8\") as f:\n        \t  # 添加至上下文\n            context.execute(f.read())\n\n        # 调用js中的函数生成sign\n        sign = context.a(self.query)\n        # 将sign加入到data中\n        return sign\n\n    def make_data(self, sign):\n        data = {\n            \"query\": self.query,\n            \"from\": \"en\",\n            \"to\": \"zh\",\n            \"token\": \"6f5c83b84d69ad3633abdf18abcb030d\",\n            \"sign\": sign\n        }\n        return data\n\n    def get_content(self, data):\n        # 发送请求获取响应\n        response = requests.post(\n            url=self.url,\n            headers=self.headers,\n            data=data\n        )\n        return response.json()[\"trans\"][0][\"dst\"]\n\n    def run(self):\n        \"\"\"运行程序\"\"\"\n        # 获取sign的值\n        sign = self.make_sign()\n        # 构建参数\n        data = self.make_data(sign)\n        # 获取翻译内容\n        content = self.get_content(data)\n        print(content)\n\n\nif __name__ == '__main__':\n    query = input(\"请输入您要翻译的内容:\")\n    translater = BaiDuTranslater(query)\n    translater.run()\n```\n\n#### 注意\n- 此时我们运行代码会报错误，会讲我们缺少 ``r`` 的值\n\n\n> 解决办法：回到浏览器，我们查找 ``r`` 的值,并将生成 ``r`` 的值的函数，加入到我们之前创建的 ``js`` 文件中，放在 `G` 函数的上面\n\n![](./images/find_r.jpg)\n\n- 生成r的代码,如下所示\n\n```javascript\nfunction n(r, o) {\n    for (var t = 0; t < o.length - 2; t += 3) {\n        var e = o.charAt(t + 2);\n        e = e >= \"a\" ? e.charCodeAt(0) - 87 : Number(e),\n            e = \"+\" === o.charAt(t + 1) ? r >>> e : r << e,\n            r = \"+\" === o.charAt(t) ? r + e & 4294967295 : r ^ e\n    }\n    return r\n}\n```\n\n- 我们重新运行代码，发现又会遇到一个错误，说是缺少 ``i`` 的值，所以我们的解决办法还是，继续进行 `js` 逆向，查找i的值\n\n- 逆向分析\n\n1. 我们回到 `function a()`中，找到需要使用 ``i``值的地方，打上断点\n\n![](./images/获取i打断点.jpg)\n\n2. 打上断点之后刷新一下页面,我们再次将鼠标放在 ``i``上，我们会看到一个浮点字符串，此时我们不确定 `i` 的值是否是变化的，所以我们换一个翻译的单词，刷新页面，再次查看 `i` 的值，我们发现 `i` 的值是固定的，所以我们可以在代码中，直接定义一个固定的`i` 值.\n\n![](./images/找到i的值.png)\n\n> 我们在自定义的 js 代码最上面一行写入：\n\n```javascript\nvar i = \"320305.131321201\"\n```\n\n#### 最后\n\n> 我们运行代码，yes 成功，这样我们就完成了一次简单的 js 逆向尝试。\n\n### [完整代码](https://github.com/CriseLYJ/awesome-python-login-model/tree/master/baidu_translate)\n\n\n"
        },
        {
          "name": "06 - 正则表达式 提取数据.md",
          "type": "blob",
          "size": 14.0927734375,
          "content": "# 正则表达式 提取数据\n\n## 正则表达式\n\n### 1. 正则表达式概念\n\n> ###### 正则表达式就是记录文本规则的代码\n\n### 2. 正则表达式的样子\n\n> 0\\d{2}-\\d{8} 这个就是一个正则表达式，表达的意思是匹配的是座机号码\n\n### 3. 正则表达式的特点\n\n- 正则表达式的语法很令人头疼，可读性差\n- 正则表达式通用行很强，能够适用于很多编程语言\n\n## 1. re模块的使用过程\n\n在Python中需要通过正则表达式对字符串进行匹配的时候，可以使用一个模块，名字为re\n\n```python\n    # 导入re模块\n    import re\n\n    # 使用match方法进行匹配操作\n    result = re.match(正则表达式,要匹配的字符串)\n\n    # 如果上一步匹配到数据的话，可以使用group方法来提取数据\n    result.group()\n```\n\n## 2. re模块示例\n\n```python\n    #coding=utf-8\n\n    import re\n\n    result = re.match(\"hello\",\"hello.cn\")\n\n    result.group()\n```\n\n运行结果为：\n\n```\nhello\n```\n\n\n\n#### 定义\n\n> 用事先定义好的一些特定字符、及这些特定字符的组合，组成一个规则字符串，这个规则字符串用来表达对字符串的一种过滤逻辑。\n\n#### 常见语法\n\n> 字符\n\n| 语法     | 说明                                                       | 表达式案例 | 完整匹配字符串    |\n| :------- | :--------------------------------------------------------- | :--------- | :---------------- |\n| 一般字符 | 匹配自身                                                   | abc        | abc               |\n| .        | 匹配任意除换行符`\\n`外的字符。在DOTALL模式中也能匹配换行符 | a.c        | abc               |\n| \\        | 转义字符，使后一个字符表示字符本身。                       | a.c        | a.c               |\n| [...]    | 选取字符范围                                               | a[bcd]e    | abe 或 ace 或 ade |\n\n> 预定义字符集（可以写在字符集[...]中）\n\n| 语法 | 说明                           | 表达式案例 | 完整匹配字符串 |\n| :--- | :----------------------------- | :--------- | :------------- |\n| \\d   | 数字:[0-9]                     | a\\dc       | a1c            |\n| \\D   | 非数字:[^0-9]                  | a\\Dc       | abc            |\n| \\s   | 空白字符:[<空格>\\t\\r\\n\\f\\v]    | a\\sc       | a c            |\n| \\S   | 非空白字符:[^<空格>\\t\\r\\n\\f\\v] | a\\Sc       | abc            |\n| \\w   | 单词字符:[A-Za-z0-9_]          | a\\wc       | abc            |\n| \\W   | 非单词字符:[^A-Za-z0-9_]       | a\\Wc       | a c            |\n\n> 数量词（用在字符或(...)之后）\n\n| 语法 | 说明                        | 表达式案例 | 完整匹配字符串 |\n| :--- | :-------------------------- | :--------- | :------------- |\n| *    | 匹配前一个字符0次或无限次。 | abc*       | abccc          |\n| +    | 匹配前一个字符1次或无限次。 | abc+       | abccc          |\n| ?    | 匹配前一个字符0次或1次。    | abc?       | abc 或 ab      |\n| {m}  | 匹配前一个字符m次。         | ab{2}c     | abbc           |\n\n## 匹配单个字符\n\n\n\n### 示例1： \n\n```python\n#coding=utf-8\n\nimport re\n\nret = re.match(\".\",\"M\")\nprint(ret.group())\n\nret = re.match(\"t.o\",\"too\")\nprint(ret.group())\n\nret = re.match(\"t.o\",\"two\")\nprint(ret.group())\n```\n\n运行结果：\n\n```python\nM\ntoo\ntwo\n```\n\n### 示例2：[]\n\n```python\nimport re\n\n# 如果hello的首字符小写，那么正则表达式需要小写的h\nret = re.match(\"h\",\"hello Python\") \nprint(ret.group())\n\n\n# 如果hello的首字符大写，那么正则表达式需要大写的H\nret = re.match(\"H\",\"Hello Python\") \nprint(ret.group())\n\n# 大小写h都可以的情况\nret = re.match(\"[hH]\",\"hello Python\")\nprint(ret.group())\nret = re.match(\"[hH]\",\"Hello Python\")\nprint(ret.group())\nret = re.match(\"[hH]ello Python\",\"Hello Python\")\nprint(ret.group())\n\n# 匹配0到9第一种写法\nret = re.match(\"[0123456789]Hello Python\",\"7Hello Python\")\nprint(ret.group())\n\n# 匹配0到9第二种写法\nret = re.match(\"[0-9]Hello Python\",\"7Hello Python\")\nprint(ret.group())\n\nret = re.match(\"[0-35-9]Hello Python\",\"7Hello Python\")\nprint(ret.group())\n\n# 下面这个正则不能够匹配到数字4，因此ret为None\nret = re.match(\"[0-35-9]Hello Python\",\"4Hello Python\")\n# print(ret.group())\n```\n\n运行结果：\n\n```python\nh\nH\nh\nH\nHello Python\n7Hello Python\n7Hello Python\n7Hello Python\n```\n\n### 示例3：\\d\n\n```python\nimport re\n\n# 普通的匹配方式\nret = re.match(\"嫦娥1号\",\"嫦娥1号发射成功\") \nprint(ret.group())\n\nret = re.match(\"嫦娥2号\",\"嫦娥2号发射成功\") \nprint(ret.group())\n\nret = re.match(\"嫦娥3号\",\"嫦娥3号发射成功\") \nprint(ret.group())\n\n# 使用\\d进行匹配\nret = re.match(\"嫦娥\\d号\",\"嫦娥1号发射成功\") \nprint(ret.group())\n\nret = re.match(\"嫦娥\\d号\",\"嫦娥2号发射成功\") \nprint(ret.group())\n\nret = re.match(\"嫦娥\\d号\",\"嫦娥3号发射成功\") \nprint(ret.group())\n```\n\n运行结果：\n\n```python\n嫦娥1号\n嫦娥2号\n嫦娥3号\n嫦娥1号\n嫦娥2号\n嫦娥3号\n```\n\n### 示例4：\\D\n\n```python\nimport re\n\nmatch_obj = re.match(\"\\D\", \"f\")\nif match_obj:\n    # 获取匹配结果\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n运行结果:\n\n```python\nf\n```\n\n### 示例5：\\s\n\n```python\nimport re\n\n# 空格属于空白字符\nmatch_obj = re.match(\"hello\\sworld\", \"hello world\")\nif match_obj:\n    result = match_obj.group()\n    print(result)\nelse:\n    print(\"匹配失败\")\n\n\n\n# \\t 属于空白字符\nmatch_obj = re.match(\"hello\\sworld\", \"hello\\tworld\")\nif match_obj:\n    result = match_obj.group()\n    print(result)\nelse:\n    print(\"匹配失败\")\n```\n\n运行结果:\n\n```python\nhello world\nhello world\n```\n\n### 示例6：\\S\n\n```python\nimport re\n\nmatch_obj = re.match(\"hello\\Sworld\", \"hello&world\")\nif match_obj:\n    result = match_obj.group()\n    print(result)\nelse:\n    print(\"匹配失败\")\n\n\n\nmatch_obj = re.match(\"hello\\Sworld\", \"hello$world\")\nif match_obj:\n    result = match_obj.group()\n    print(result)\nelse:\n    print(\"匹配失败\")\n```\n\n运行结果:\n\n```python\nhello&world  \nhello$world\n```\n\n### 示例7：\\w\n\n```python\nimport re\n\n# 匹配非特殊字符中的一位\nmatch_obj = re.match(\"\\w\", \"A\")\nif match_obj:\n    # 获取匹配结果\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n执行结果:\n\n```\nA\n```\n\n### 示例8：\\W\n\n```python\n# 匹配特殊字符中的一位\nmatch_obj = re.match(\"\\W\", \"&\")\nif match_obj:\n    # 获取匹配结果\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n执行结果:\n\n```\n&\n```\n\n### 思考\n\n匹配密码中的其中一位，密码是由字母、数字、下划线组成，请列举的方式匹配?\n\n## 匹配多个字符\n\n### 示例1：*\n\n需求：匹配出一个字符串第一个字母为大小字符，后面都是小写字母并且这些小写字母可 有可无\n\n```python\nimport re\n\nret = re.match(\"[A-Z][a-z]*\",\"M\")\nprint(ret.group())\n\nret = re.match(\"[A-Z][a-z]*\",\"MnnM\")\nprint(ret.group())\n\nret = re.match(\"[A-Z][a-z]*\",\"Aabcdef\")\nprint(ret.group())\n```\n\n运行结果：\n\n```python\nM\nMnn\nAabcdef\n```\n\n### 示例2：+\n\n需求：匹配一个字符串，第一个字符是t,最后一个字符串是o,中间至少有一个字符\n\n```python\nimport re\n\n\nmatch_obj = re.match(\"t.+o\", \"two\")\nif match_obj:\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n运行结果：\n\n```python\ntwo\n```\n\n### 示例3：?\n\n需求：匹配出这样的数据，但是https 这个s可能有，也可能是http 这个s没有\n\n```python\nimport re\n\nmatch_obj = re.match(\"https?\", \"http\")\nif match_obj:\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n运行结果：\n\n```python\nhttps\n```\n\n### 示例4：{m}、{m,n}\n\n需求：匹配出，8到20位的密码，可以是大小写英文字母、数字、下划线\n\n```python\nimport re\n\n\nret = re.match(\"[a-zA-Z0-9_]{6}\",\"12a3g45678\")\nprint(ret.group())\n\nret = re.match(\"[a-zA-Z0-9_]{8,20}\",\"1ad12f23s34455ff66\")\nprint(ret.group())\n```\n\n运行结果：\n\n```python\n12a3g4\n1ad12f23s34455ff66\n```\n\n## 思考\n\n如何使用正则表达式把qq:10567这样的数据匹配处理?\n\n## 匹配开头和结尾\n\n### 1. 匹配开头和结尾的正则表达式\n\n| 代码 | 功能           |\n| :--: | :------------- |\n|  ^   | 匹配字符串开头 |\n|  $   | 匹配字符串结尾 |\n\n### 示例1：^\n\n需求：匹配以数字开头的数据\n\n```python\nimport re\n\n# 匹配以数字开头的数据\nmatch_obj = re.match(\"^\\d.*\", \"3hello\")\nif match_obj:\n    # 获取匹配结果\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n运行结果:\n\n```python\n3hello\n```\n\n### 示例2：$\n\n需求: 匹配以数字结尾的数据\n\n```python\nimport re\n# 匹配以数字结尾的数据\nmatch_obj = re.match(\".*\\d$\", \"hello5\")\nif match_obj:\n    # 获取匹配结果\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n运行结果：\n\n```python\nhello5\n```\n\n### 示例3：^ 和 $\n\n需求: 匹配以数字开头中间内容不管以数字结尾\n\n```python\nmatch_obj = re.match(\"^\\d.*\\d$\", \"4hello4\")\nif match_obj:\n    # 获取匹配结果\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n运行结果:\n\n```python\n4hello4\n```\n\n### 2.除了指定字符以外都匹配\n\n> ###### [^指定字符]: 表示除了指定字符都匹配\n\n需求: 第一个字符除了aeiou的字符都匹配\n\n```python\nimport re\n\n\nmatch_obj = re.match(\"[^aeiou]\", \"h\")\nif match_obj:\n    # 获取匹配结果\n    print(match_obj.group())\nelse:\n    print(\"匹配失败\")\n```\n\n执行结果\n\n```\nh\n```\n\n### 思考\n\n- 1.匹配以数字开头中间内容不管以数字结尾, 结尾不要4或者7\n- 2.匹配出163的邮箱地址，且@符号之前有4到20位，例如hello@163.com\n- 3.匹配出11位手机号码\n- 4.匹配出微博中的话题, 比如: #幸福是奋斗出来的#\n\n## 高级匹配\n\n\n\n#### compile 方法\n\n##### 作用\n\n对正则表达式进行编译构建对象，后续不需要重新构建正则表达式上下文环境，从而提高效率\n\n##### 使用方式\n\n```python\n# 构建表达式对象\npattern = re.compile(r'表达式')\n# 通过表达式对象对数据进行操作\npattern.findall(string)\n```\n\n#### 贪婪模式和非贪婪模式区别\n\n##### 贪婪模式\n\n- `(.*)` 尽可能多的匹配\n\n##### 非贪婪模式\n\n- `(.*?)` 一旦匹配到就结束\n\n#### DOTALL模式\n\n> `.`符号匹配所有的字符包括换行符\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\nimport re\n\nstring = '''\n    abcd\n    abcd\n'''\n\n# 相当于\n# 1. 编译正则表达式\n# (.*)      贪婪匹配，尽可能多匹配直到无法匹配\n# (.*?)     非贪婪匹配，只要匹配到就返回\n#  . 符号默认不包含换行符，DOTALL模式表示让 . 符号匹配任何字符包括换行符\n# re.DOTALL == re.S == re.RegexFlag.DOTALL == re.RegexFlag.S\npattern = re.compile(r'a(.*)d',re.RegexFlag.S)\n\n# 2. 提取数据\nresult = pattern.findall(string)\nprint(result)\n```\n\n#### 忽略大小写模式\n\n> 正则匹配时忽略大小写\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\nimport re\n\n\nstring = '''\n    abcD\n    abcD\n'''\n\n# 相当于\n# 1. 编译正则表达式\n# (.*)      贪婪匹配，尽可能多匹配直到无法匹配\n# (.*?)     非贪婪匹配，只要匹配到就返回\n#  . 符号默认不包含换行符，DOTALL模式表示让 . 符号匹配任何字符包括换行符\n# re.DOTALL == re.S == re.RegexFlag.DOTALL == re.RegexFlag.S\n# 忽略大小写\n# re.IGNORECASE == re.I == re.RegexFlag.IGNORECASE == re.RegexFlag.I\n# 忽略大小写并且支持 DOTALL模式 使用 |\npattern = re.compile(r'a(.*)d',re.RegexFlag.IGNORECASE | re.DOTALL)\n\n# 2. 提取数据\nresult = pattern.findall(string)\nprint(result)\n```\n\n#### 原始字符串 r 的用法\n\n> 让字符串内部的转义字符没有任何意义。\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\nstring = r\"abc\\nd\"\nprint(string)\n```\n\n#### 四大检索方法\n\n- **match** 开头匹配，只匹配一次\n- **search** 全局匹配，只匹配一次\n- **findall** 匹配所有符号条件的数据，返回是 结果列表\n- **finditer** 迭代对象，迭代 Match 对象\n\n##### match 和 seach 区别\n\n> **match** 开头匹配，只匹配一次\n>\n> **search** 全局匹配，只匹配一次\n\n##### findall 和 finditer 区别\n\n> **findall** 优点：使用简单，缺点：必须把所有数据搜索返回再返回\n>\n> **finditer** 优点：找到就返回，可以边找边返回\n>\n> 如果数据量小 使用 **findall**\n>\n> 如果数据量大 使用 **finditer**\n\n##### 代码实现\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\nimport re\n\nstring = \"123abc123\"\n\n# match 开头匹配，只匹配一次\npattern = re.compile('\\d+')\n# result = pattern.match(string)\n\n# search 全局匹配，只匹配一次\n# result = pattern.search(string)\n\n'''\nfindall 优点：使用简单，缺点：必须把所有数据搜索返回再返回\nfinditer 优点：找到就返回，可以边找边返回\n如果数据量小 使用 findall\n\n如果数据量大 使用 finditer\n\n'''\n# findall 匹配所有符号条件的数据，返回是 结果列表\n# result = pattern.findall(string)\n\n# finditer 迭代对象，迭代 Match 对象\nresults = pattern.finditer(string)\nfor result in results:\n    print(result)\n\nprint(results)\n```\n\n#### 分组与替换方法\n\n##### 分组\n\n> 通过给定字符串进行对数据进行分组\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\nimport re\nstring = \"a;dj jkl,jj; j;sd\"\n# split 分组\npattern = re.compile(r'[; ,]+')\nresult = pattern.split(string)\nprint(result)\n```\n\n##### 替换\n\n> 通过给定的正则表达式和替换字符进行替换\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\nimport re\n\n# sub 交换\nstring = \"hello world;sjd;ssdjkls;sdjk;crise lyj\"\n# 带 空格的词组替换成 #\npattern = re.compile(r'(\\w+) (\\w+)')\n\n# 把 空格的词组 进行交换\nresult = pattern.sub(r\"\\2 \\1\",string)\n\nprint(result)\n```\n\n#### 常见提取方式\n\n##### 使用流程\n\n1. 拷贝原有字符串内容\n2. 把所要提取的数据使用 `(.*)` 或 `(.*?)` 进行替换\n3. 使用 `findall` 或 `finditer` 进行数据提取\n\n##### 代码实现\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\nimport re\n\nstring = '<input type=\"submit\" id=\"su\" value=\"百度一下\" class=\"bg s_btn\">'\n\npattern = re.compile(r'<input type=\"submit\" id=\"(.*?)\" value=\"(.*?)\" class=\"bg s_btn\">')\n\nresult = pattern.findall(string)\nprint(result)\n```\n"
        },
        {
          "name": "HTTP响应列表.md",
          "type": "blob",
          "size": 3.5791015625,
          "content": "# HTTP 响应头列表\n\n应答头  | 说明\n------------- | -------------\nAllow\t| 服务器支持哪些请求方法（如GET、POST等）\nContent-Encoding | 文档的编码（Encode）方法。只有在解码之后才可以得到Content-Type头指定的内容类型。利用gzip压缩文档能够显著地减少HTML文档的下载时间。Java的GZIPOutputStream可以很方便地进行gzip压缩，但只有Unix上的Netscape和Windows上的IE 4、IE 5才支持它。因此，Servlet应该通过查看Accept-Encoding头（即request.getHeader(\"Accept-Encoding\")）检查浏览器是否支持gzip，为支持gzip的浏览器返回经gzip压缩的HTML页面，为其他浏览器返回普通页面。\nContent-Length | 表示内容长度。只有当浏览器使用持久HTTP连接时才需要这个数据。如果你想要利用持久连接的优势，可以把输出文档写入 ByteArrayOutputStream，完成后查看其大小，然后把该值放入Content-Length头，最后通过byteArrayStream.writeTo(response.getOutputStream()发送内容。\nContent-Type | 表示后面的文档属于什么MIME类型。Servlet默认为text/plain，但通常需要显式地指定为text/html。由于经常要设置Content-Type，因此HttpServletResponse提供了一个专用的方法setContentType。\nDate | 当前的GMT时间。你可以用setDateHeader来设置这个头以避免转换时间格式的麻烦。\nExpires | 应该在什么时候认为文档已经过期，从而不再缓存它？\nLast-Modified | 文档的最后改动时间。客户可以通过If-Modified-Since请求头提供一个日期，该请求将被视为一个条件GET，只有改动时间迟于指定时间的文档才会返回，否则返回一个304（Not Modified）状态。Last-Modified也可用setDateHeader方法来设置。\nLocation | 表示客户应当到哪里去提取文档。Location通常不是直接设置的，而是通过HttpServletResponse的sendRedirect方法，该方法同时设置状态代码为302。\nRefresh\t| 表示浏览器应该在多少时间之后刷新文档，以秒计。除了刷新当前文档之外，你还可以通过setHeader(\"Refresh\", \"5; URL=http://host/path\")让浏览器读取指定的页面。 注意这种功能通常是通过设置HTML页面HEAD区的＜META HTTP-EQUIV=\"Refresh\" CONTENT=\"5;URL=http://host/path\"＞实现，这是因为，自动刷新或重定向对于那些不能使用CGI或Servlet的HTML编写者十分重要。但是，对于Servlet来说，直接设置Refresh头更加方便。 注意Refresh的意义是\"N秒之后刷新本页面或访问指定页面\"，而不是\"每隔N秒刷新本页面或访问指定页面\"。因此，连续刷新要求每次都发送一个Refresh头，而发送204状态代码则可以阻止浏览器继续刷新，不管是使用Refresh头还是＜META HTTP-EQUIV=\"Refresh\" ...＞。 注意Refresh头不属于HTTP 1.1正式规范的一部分，而是一个扩展，但Netscape和IE都支持它。\nServer | 服务器名字。Servlet一般不设置这个值，而是由Web服务器自己设置。\nSet-Cookie | 设置和页面关联的Cookie。Servlet不应使用response.setHeader(\"Set-Cookie\", ...)，而是应使用HttpServletResponse提供的专用方法addCookie。参见下文有关Cookie设置的讨论。\nWWW-Authenticate | 客户应该在Authorization头中提供什么类型的授权信息？在包含401（Unauthorized）状态行的应答中这个头是必需的。例如，response.setHeader(\"WWW-Authenticate\", \"BASIC realm=＼\"executives＼\"\")。 注意Servlet一般不进行这方面的处理，而是让Web服务器的专门机制来控制受密码保护页面的访问（例如.htaccess）。\n"
        },
        {
          "name": "HTTP请求列表.md",
          "type": "blob",
          "size": 7.3984375,
          "content": "# HTTP请求头列表\n## Host\n### 作用\n> 请求报头域主要用于指定被请求资源的Internet主机和端口号，它通常从HTTP URL中提取出来的\n\n### 例如\n> 我们在浏览器中输入：http://www.hzau.edu.cn\n> \n> 浏览器发送的请求消息中，就会包含Host请求报头域，如下：\n> \n> Host：www.hzau.edu.cn\n> \n> 此处使用缺省端口号80，若指定了端口号，则变成：Host：指定端口号\n\n## Connection\n### 作用\n> 表Connection表示是否需要持久连接。如果Servlet看到这里的值为“Keep-Alive”，或者看到请求使用的是HTTP 1.1（HTTP 1.1默认进行持久连接），它就可以利用持久连接的优点，当页面包含多个元素时（例如Applet，图片），显著地减少下载所需要的时间。要实现这一点，Servlet需要在应答中发送一个Content-Length头，最简单的实现方法是：先把内容写入ByteArrayOutputStream，然后在正式写出内容之前计算它的大小。\n\n### 例如\n> Connection: Keepalive 告诉WEB服务器或者代理服务器，在完成本次请求的响应后，保持连接，等待本次连接的后续请求。\n> \n> Connection: Keep-Alive 如果浏览器请求保持连接，则该头部表明希望 WEB 服务器保持连接多长时间（秒），如Keep-Alive：300。\n> \n> Connection: close 告诉WEB服务器或者代理服务器，在完成本次请求的响应后，断开连接，不要等待本次连接的后续请求了。\n> \n## Upgrade-Insecure-Requests\n### 作用\n> 则是告诉服务器，自己支持这种操作，也就是我能读懂你服务器发过来的上面这条信息，并且在以后发请求的时候不用http而用https\n\n## User-Agent\n### 作用\n> 告诉HTTP服务器， 客户端使用的操作系统和浏览器的名称和版本.\n\n> 我们上网登陆论坛的时候，往往会看到一些欢迎信息，其中列出了你的操作系统的名称和版本，你所使用的浏览器的名称和版本，这往往让很多人感到很神奇，实际上，服务器应用程序就是从User-Agent这个请求报头域中获取到这些信息User-Agent请求报头域允许客户端将它的操作系统、浏览器和其它属性告诉服务器。\n\n### 例如\n> User-Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; CIBA; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C; InfoPath.2; .NET4.0E)\n\n## Accept\n### 作用\n> 浏览器端可以接受的媒体类型\n\n### 例如\n> Accept: text/html 代表浏览器可以接受服务器回发的类型为 text/html 也就是我们常说的html文档,\n\n> 如果服务器无法返回 text/html 类型的数据,服务器应该返回一个406错误(non acceptable)\n\n> 通配符 * 代表任意类型\n\n> Accept: */* 代表浏览器可以处理所有类型,(一般浏览器发给服务器都是发这个)\n\n## Accept-Encoding\n### 作用\n> 浏览器申明自己接收的编码方法，通常指定压缩方法，是否支持压缩，支持什么压缩方法（gzip，deflate），（注意：这不是只字符编码）;\n\n### 例如\n> Accept-Encoding: zh-CN,zh;q=0.8\n\n## Referer\n### 作用\n> 当浏览器向web服务器发送请求的时候，一般会带上Referer，告诉服务器我是从哪个页面链接过来的，服务器籍此可以获得一些信息用于处理。比如从我主页上链接到一个朋友那里，他的服务器就能够从HTTP Referer中统计出每天有多少用户点击我主页上的链接访问他的网站。\n\n## Accept-Language\n### 作用\n> 浏览器申明自己接收的语言。 语言跟字符集的区别：中文是语言，中文有多种字符集，比如big5，gb2312，gbk等等；\n\n### 例如\n> Accept-Language: en-us\n\n## Content-Length\n### 作用\n> 表示请求消息正文的长度。\n\n## Cache-Control\n### 作用\n> 我们网页的缓存控制是由HTTP头中的“Cache-control”来实现的，常见值有private、no-cache、max-age、must-revalidate等，默认为private。这几种值的作用是根据重新查看某一页面时不同的方式来区分的：\n> \n> （1）、打开新窗口\n> \n> 值为private、no-cache、must-revalidate，那么打开新窗口访问时都会重新访问服务器。而如果指定了max-age值（单位为秒），那么在此值内的时间里就不会重新访问服务器，例如：\n> \n> Cache-control: max-age=5 (表示当访问此网页后的5秒内再次访问不会去服务器)\n> \n> （2）、在地址栏回车\n> \n> 值为 private 或 must-revalidate 则只有第一次访问时会访问服务器，以后就不再访问。\n> \n> 值为 no-cache ，那么每次都会访问。\n> \n> 值为 max-age ，则在过期之前不会重复访问。\n> \n> （3）、按后退按扭\n> \n> 值为 private、must-revalidate、max-age，则不会重访问，\n> \n> 值为 no-cache，则每次都重复访问\n> \n> （4）、按刷新按扭\n> \n> 无论为何值，都会重复访问\n\n## Cookie\n### 作用\n> Cookie是用来存储一些用户信息以便让服务器辨别用户身份的（大多数需要登录的网站上面会比较常见），比如cookie会存储一些用户的用户名和密码，当用户登录后就会在客户端产生一个cookie来存储相关信息，这样浏览器通过读取cookie的信息去服务器上验证并通过后会判定你是合法用户，从而允许查看相应网页。当然cookie里面的数据不仅仅是上述范围，还有很多信息可以存储是cookie里面，比如sessionid等。\n\n## Content-Type\n### 作用\n> 表示后面的文档属于什么MIME类型。Servlet默认为text/plain，但通常需要显式地指定为text/html。由于经常要设置Content-Type，因此HttpServletResponse提供了一个专用的方法setContentType。\n\n### 例如\n> Content-Type: text/html\n\n## If-Modified-Since\n### 作用\n> 把浏览器端缓存页面的最后修改时间发送到服务器去，服务器会把这个时间与服务器上实际文件的最后修改时间进行对比。如果时间一致，那么返回304，客户端就直接使用本地缓存文件。如果时间不一致，就会返回200和新的文件内容。客户端接到之后，会丢弃旧文件，把新文件缓存起来，并显示在浏览器中.\n\n### 例如\n> If-Modified-Since : Mon, 17 Aug 2015 12:03:33 GMT\n\n## If-None-Match\n### 作用\n> If-None-Match和ETag一起工作，工作原理是在HTTP Response中添加ETag信息。 当用户再次请求该资源时，将在HTTP Request 中加入If-None-Match信息(ETag的值)。如果服务器验证资源的ETag没有改变（该资源没有更新），将返回一个304状态告诉客户端使用本地缓存文件。否则将返回200状态和新的资源和Etag. 使用这样的机制将提高网站的性能\n\n### 例如\n> If-None-Match: W/\"3119-1437038474000\"\n\n## Authorization\n### 作用\n> 授权信息，通常出现在对服务器发送的WWW-Authenticate头的应答中。\n\n## Upgrade\n### 作用\n> 它可以指定另一种可能完全不同的协议，如HTTP/1.1客户端可以向服务器发送一条HTTP/1.0请求，其中包含值为“HTTP/1.1”的Update头部，这样客户端就可以测试一下服务器是否也使用HTTP/1.1了。\n\n## Proxy-Authenticate\n### 作用\n> 代理服务器响应浏览器，要求其提供代理身份验证信息。\n\n## Proxy-Authorization\n### 作用\n> 浏览器响应代理服务器的身份验证请求，提供自己的身份信息。\n\n## Range\n### 作用\n> 浏览器（比如 Flashget 多线程下载时）告诉 WEB 服务器自己想取对象的哪部分。\n\n### 例如\n> Range: bytes=1173546"
        },
        {
          "name": "JSON数据提取.md",
          "type": "blob",
          "size": 6.3828125,
          "content": "   * [JSON 数据提取](#josn-数据提取)\n      * [JSON 数据格式](#json-数据格式)\n         * [定义](#定义)\n         * [作用](#作用)\n         * [获取方式](#获取方式)\n         * [格式化方式](#格式化方式)\n      * [JSON 模块使用](#json-模块使用)\n         * [方法介绍](#方法介绍)\n         * [代码演练](#代码演练)\n      * [JsonPath](#jsonpath)\n         * [介绍](#介绍)\n         * [环境搭建](#环境搭建)\n            * [在线调试地址](#在线调试地址)\n            * [在线调试环境说明](#在线调试环境说明)\n            * [调试测试数据](#调试测试数据)\n         * [语法规则](#语法规则)\n         * [jsonpath 代码演练](#jsonpath-代码演练)\n            * [模块安装](#模块安装)\n            * [需求](#需求)\n            * [实现步骤](#实现步骤)\n         * [代码实现](#代码实现)\n      * [总结](#总结)\n\n# JSON 数据提取\n## JSON 数据格式\n### 定义\n\n> JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。\n\n### 作用\n![](./images/json和python的对应.png)\n\n### 获取方式\n- ajax 请求接口\n- 切换手机移动h5端\n- app抓包获取\n- 等等\n\n### 格式化方式\n- 安装 chrome 插件 JSONView\n- 在chrome中的请求详情中使用 preview 查看\n- 使用在线JSON格式化工具，例如：https://www.bejson.com/\n\n## JSON 模块使用\n### 方法介绍\n![](./images/json的方法.png)\n\n- json.loads json字符串 转 Python数据类型\n- json.dumps Python数据类型 转 json字符串\n- json.load json文件 转 Python数据类型\n- json.dump Python数据类型 转 json文件\n\t- ensure_ascii=False 实现让中文写入的时候保持为中文\n\t- indent=空格数 通过空格的数量进行缩紧\n\n### 代码演练\n```python3\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n# 导入模块\nimport json\n\n# json.loads json字符串 转 Python数据类型\njson_string = '''\n{\n    \"name\": \"crise\",\n    \"age\": 18,\n    \"parents\": {\n        \"monther\": \"妈妈\",\n        \"father\": \"爸爸\"\n    }\n}\n'''\nprint(\"json_string数据类型：\",type(json_string))\ndata = json.loads(json_string)\nprint(\"data数据类型：\",type(data))\nprint(data)\nprint(\"*\" * 100)\n# json.dumps Python数据类型 转 json字符串\ndata = {\n    \"name\": \"crise\",\n    \"age\": 18,\n    \"parents\": {\n        \"monther\": \"妈妈\",\n        \"father\": \"爸爸\"\n    }\n}\nprint(\"data数据类型：\",type(data))\njson_string = json.dumps(data)\nprint(\"json_string数据类型：\",type(json_string))\nprint(json_string)\n\nprint(\"*\"*100)\n# json.load json文件 转 Python数据类型\nwith open('data.json','r',encoding='utf-8') as f:\n    data = json.load(f)\n    print(\"data数据类型：\", type(data))\n    print(data)\n\nprint(\"*\"*100)\n# json.dump Python数据类型 转 json文件\ndata = {\n    \"name\": \"crise\",\n    \"age\": 18,\n    \"parents\": {\n        \"monther\": \"妈妈\",\n        \"father\": \"爸爸\"\n    }\n}\nwith open('data_out.json','w',encoding='utf-8') as f:\n    json.dump(data,f,ensure_ascii=False,indent=2)\n```\n\n## JsonPath\n### 介绍\n> 用来解析多层嵌套的json数据;JsonPath 是一种信息抽取类库，是从JSON文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。\n\n``本质``： 通过一种语法规则快速从 JSON 数据中提取数据。类似于 正则表达式 通过一定规则从 text 文本内容提取数据。\n\n### 环境搭建\n#### 在线调试地址\n- 使用在线调试环境 http://jsonpath.com/\n\n#### 在线调试环境说明\n![](./images/jsonpath调试环境使用.png)\n\n#### 调试测试数据\n```json\n{ \n  \"store\": {\n    \"book\": [ \n      { \"category\": \"reference\",\n        \"author\": \"Nigel Rees\",\n        \"title\": \"Sayings of the Century\",\n        \"price\": 8.95\n      },\n      { \"category\": \"fiction\",\n        \"author\": \"Evelyn Waugh\",\n        \"title\": \"Sword of Honour\",\n        \"price\": 12.99\n      },\n      { \"category\": \"fiction\",\n        \"author\": \"Herman Melville\",\n        \"title\": \"Moby Dick\",\n        \"isbn\": \"0-553-21311-3\",\n        \"price\": 8.99\n      },\n      { \"category\": \"fiction\",\n        \"author\": \"J. R. R. Tolkien\",\n        \"title\": \"The Lord of the Rings\",\n        \"isbn\": \"0-395-19395-8\",\n        \"price\": 22.99\n      }\n    ],\n    \"bicycle\": {\n      \"color\": \"red\",\n      \"price\": 19.95\n    }\n  }\n}\n```\n### 语法规则\n| 语法         | 描述               | 案例          |\n|-------------|-------------------|---------------|\n| $           | 根节点             |\n| @           | 现行节点           |\n| .           | 取子节点           | $.store.book  |\n| ..          | 取子孙节点         | $..book        |\n| []          | 设置筛选条件        | $..book[0]     |\n| [,]         | 支持多选选择内容    | $..book[1,3]     |\n| ()          | 支持表达式计算      | $..book[(@.length - 1)]  |\n| ?()         | 支持过滤操作        | $..book[?(@.price<10)]    |\n\n\n### jsonpath 代码演练\n#### 模块安装\n> pip install jsonpath\n\n\n#### 需求\n> 从 http://www.lagou.com/lbs/getAllCitySearchLabels.json 接口返回数据中提取所有的城市信息\n\n#### 实现步骤\n1. 网络获取数据\n2. 把响应数据转换成python数据类型\n3. 使用 jsonpath 提取数据\n\n### 代码实现\n```python3\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\n# 网络获取数据\nimport requests\nimport json\nimport jsonpath\n\nurl = \"http://www.lagou.com/lbs/getAllCitySearchLabels.json\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n}\n\nresponse = requests.get(url,headers=headers)\nhtml = response.text\nprint(html)\n# 把响应数据转换成python数据类型\ndata = json.loads(html)\n\n# 使用 jsonpath 提取数据\ncities = jsonpath.jsonpath(data,'$..name')\nprint(cities)\n```\n\n## 总结\n- JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式\n- JSON相关的方法\n\t- json.loads json字符串 转 Python数据类型\n\t- json.dumps Python数据类型 转 json字符串\n\t- json.load json文件 转 Python数据类型\n\t- json.dump Python数据类型 转 json文件\n- jsonpath 是一种语法规则快速从 JSON 数据中提取数据。\n- jsonpath 基本语法\n\t- $ 根节点\n\t- . 下一个节点\n\t- .. 子孙节点\n\t- [] 筛选条件，可以编写下标\n\t\n\n\n\n"
        },
        {
          "name": "NewVersion_PleaseReadIt",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.45703125,
          "content": "### 分布式爬虫从零开始\n\n+ 从零学习python爬虫\n\n- 欢迎在issues中留言，如果文章中有错别字可以向我提pr，感谢各位！\n\n\n# 目录\n\n- [引言 ](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/引言.md)\n- [01 - 爬虫预备知识](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/%E7%88%AC%E8%99%AB%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.md)\n- [02 - 请求分析流程](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/%E8%AF%B7%E6%B1%82%E5%88%86%E6%9E%90.md)\n- [03 - requests模块的使用](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8.md)\n- [04 - 数据提取概念和数据的分类](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E6%A6%82%E5%BF%B5%E5%92%8C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%86%E7%B1%BB.md)\n- [05 - JSON数据提取](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/JSON数据提取.md)\n- [06 - re 正则表达式提取数据](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/06%20-%20%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%20%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE.md)\n\n\n# 实例目录\n- [01 - 豆瓣热门爬虫-入门](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/01-%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1.md)\n- [02 - 百度贴吧爬虫-入门](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/02-%E7%99%BE%E5%BA%A6%E8%B4%B4%E5%90%A7.md)\n- [03 - 百度翻译爬虫-逆向](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/03-%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91.md)\n\n# 文献整理\n## http知识\n[维基百科](https://zh.wikipedia.org/wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE)\n[百度百科](https://baike.baidu.com/item/http)\n[来自简书](https://www.jianshu.com/p/a6d086a3997d)\n\n## Chrome控制台\n[官方教程](https://developers.google.com/web/tools/chrome-devtools/console/?hl=zh-cn)\n[来自知乎](https://zhuanlan.zhihu.com/p/39340856)\n\n## Requests使用\n[官方中文文档](http://docs.python-requests.org/zh_CN/latest/index.html)\n\n## JSON\n[菜鸟教程](http://www.runoob.com/json/json-tutorial.html)\n\n## XML\n[菜鸟教程](http://www.runoob.com/xml/xml-tutorial.html)\n"
        },
        {
          "name": "code_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "引言.md",
          "type": "blob",
          "size": 2.0126953125,
          "content": "# 引言\n\n> Life is fantastic\n\n### 有些问题\n\n1. 在学习爬虫之前，你是否有 python 基础，python 高级技能？\n2. 你对 http，tcp，ip了解多少？\n3. 你是否喜欢 python，喜欢爬虫，喜欢创造，喜欢钻研？\n4. 你对爬虫之前是否有过了解？\n5. 你是否能坚持下来？\n\n### 下面，你们将会知道爬虫到底是什么？\n\n#### 爬虫是什么？\n\n> 通俗的讲，爬虫可以比喻成是一台机器，能够帮你完成批量的工作。就像是早些时候我们去地里收庄稼一样，那个时候我们只能用刀一颗一颗得收，显得特别麻烦，而且特别浪费人力物力，现在就不一样了，现在我们拥有了机器，一个人控制一台机器，就可以很快的收完以前需要很多人很多时间的庄稼。或许现在你已经对爬虫有一点概念了吧！\n>\n> 爬虫无非就是模拟我们人的行为去各个网站抓取下来我们所需要的内容，而且爬虫的速度非常之快，将数据整理之后，数据也非常的整洁，这就是我们的爬虫。\n>\n> 其实在我们的身边已经布满了，各式各样的爬虫，举个例子来讲，百度搜索引擎其实就是一个特别庞大的爬虫系统，当然了，谷歌搜索也是，其实爬虫有甚多，但是它们的善恶不同，各怀心思，所以，爬虫也总是不被人待见。\n\n#### 爬虫爬取下来的数据有什么作用\n\n> 现在这个时间多的我就不用说了吧，总是流传那么一句话，掌握了数据，我们也就掌握了未来！\n>\n> 比如说，我们想要去租个房子，而我们又不知道去哪租房子，不知道哪里的房子价格便宜，也不知道哪里房子更好一点，哪里更适合自己，这个时候我们对数据的需求就来了，如果这个时候我们手里有租房数据，我们就可以使用，python 的其他库，来对这些数据分析，相信我，你会得到你需要的答案的。\n\n##### 知识是自己的，我写的也不一定是最好的，所以要好好把握！加油！\n\n\n\n"
        },
        {
          "name": "数据提取概念和数据的分类.md",
          "type": "blob",
          "size": 1.9853515625,
          "content": "   * [数据提取](#数据提取)\n      * [介绍](#介绍)\n      * [概要](#概要)\n      * [数据提取概念和数据的分类](#数据提取概念和数据的分类)\n         * [什么是数据提取](#什么是数据提取)\n         * [数据的种类](#数据的种类)\n            * [构化数据](#构化数据)\n            * [非结构化数据](#非结构化数据)\n         * [总结](#总结)\n\n# 数据提取\n## 介绍\n> 用网络获取的数据中提取出想要的数据。\n\n## 概要\n- 数据提取概念和数据的分类\n- 使用 `json` 模块提取数据\n- 使用正则表达式提取数据\n- 使用 `xpath` 提取数据\n- 使用 `beautifulsoup` 提取数据\n- `json`、`csv` 数据转换\n\n## 数据提取概念和数据的分类\n\n### 什么是数据提取\n> 简单的来说，数据提取就是从响应中获取我们想要的数据的过程\n\n### 数据的种类\n\n#### 构化数据\n- **数据类型**\n\n- json 格式数据\n```json\n{\n  \"name\":\"hello\",\n  \"age\":18,\n  \"parents\":{\n    \"mother\":\"妈妈\",\n    \"father\":\"爸爸\"\n  }\n}\n```\n- xml 格式数据\n```xml\n<bookstore>\n  <book category=\"COOKING\">\n    <title lang=\"en\">Everyday Italian</title> \n    <author>Giada De Laurentiis</author> \n    <year>2005</year> \n    <price>30.00</price> \n  </book>\n  <book category=\"CHILDREN\">\n    <title lang=\"en\">Harry Potter</title> \n    <author>J K. Rowling</author> \n    <year>2005</year> \n    <price>29.99</price> \n  </book>\n  <book category=\"WEB\">\n    <title lang=\"en\">Learning XML</title> \n    <author>Erik T. Ray</author> \n    <year>2003</year> \n    <price>39.95</price> \n  </book>\n</bookstore>\n```\n\n- 处理方式\n> 通过 json 模块等直接转成 Python 数据类型\n\n#### 非结构化数据\n- 数据类型\n\t- html 格式数据\n\t- word 格式数据\n\t- 等\n- **处理方式**\n> 通过 `正则表达式` 、 `xpath` 、`beautifulsoup` 等模块提取数据\n\n### 总结\n\n- 数据提取 从网络获取数据中提取想要的数据\n- 数据的种类\n\t- 结构化数据\n\t- json\n\t- xml\n- 非结构化数据\n\t- html\n\t- word"
        },
        {
          "name": "爬虫预备知识.md",
          "type": "blob",
          "size": 10.2275390625,
          "content": "   * [爬虫预备知识](#爬虫预备知识)\n      * [概要](#概要)\n         * [爬虫定义、分类和流程](#爬虫定义分类和流程)\n            * [爬虫定义](#爬虫定义)\n            * [爬虫的分类](#爬虫的分类)\n            * [爬虫的用途](#爬虫的用途)\n            * [爬虫的流程](#爬虫的流程)\n            * [robots 协议](#robots-协议)\n         * [总结](#总结)\n      * [HTTP 与 HTTPS](#http-与-https)\n         * [HTTP 与 HTTPS 的概念](#http-与-https-的概念)\n            * [HTTP](#http)\n               * [HTTP 请求流程](#http-请求流程)\n               * [五层网络模型](#五层网络模型)\n               * [HTTP协议结构图](#HTTP协议结构图)\n               * [网络模型对应关系](#网络模型对应关系)\n               * [url 地址格式](#url-地址格式)\n               * [HTTP 请求](#http-请求)\n               * [HTTP 响应](#http-响应)\n               * [HTTPS](#https)\n               \n# 爬虫预备知识\n> 爬虫课程中所需要的相关知识储备\n\n\n## 概要\n- 爬虫``定义``、``分类``和``流程``\n- ``http``和``https``\n\n### 爬虫定义、分类和流程\n\n#### 爬虫定义\n> 网络爬虫（又被称为网页蜘蛛，网络机器人）就是模拟浏览器发送网络请求，接收请求响应，一种按照一定的规则，自动地抓取互联网信息的程序。\n\n> 爬虫就是模拟浏览器的行为，越像越好，越像就越不容易被发现。 \n\n> 原则上,只要是浏览器(客户端)能做的事情，爬虫都能够做。\n\n#### 爬虫的分类\n1. ``通用爬虫``：通常指搜索引擎的爬虫\n2. ``聚焦爬虫``：针对特定网站的爬虫\n\n#### 爬虫的用途\n- 今日头条\n- 网易云音乐\n- 12306抢票\n- 网站自动投票\n- 短信轰炸\n- 等等\n\n#### 爬虫的流程\n![](./images/爬虫流程图.png)\n1. 向起始url发送请求，并获取响应\n2. 对响应进行提取\n3. 如果提取url，则继续发送请求获取响应\n4. 如果提取数据，则将数据进行保存\n\n#### robots 协议\n> Robots 协议：网站通过 Robots 协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取，但它仅仅是道德层面上的约束 例如：[淘宝的 robots 协议](https://www.taobao.com/robots.txt)\n\n### 总结\n- 爬虫概念：模拟浏览器发送网络请求，接收请求响应。只要是浏览器(客户端)能做的事情，爬虫都能够做。\n- 爬虫的分类：聚焦爬虫、通用爬虫\n- 爬虫的流程：\n\t1. 向起始 url 发送请求，并获取响应\n\t2. 对响应进行提取\n\t3. 如果提取 url，则继续发送请求获取响应\n\t4. 如果提取数据，则将数据进行保存\n\n\n## HTTP 与 HTTPS\n\n### HTTP 与 HTTPS 的概念\n\n#### HTTP\n> 概念：HTTP（超文本传输协议）是应用层上的一种客户端/服务端模型的通信协议,它由请求和响应构成，且是无状态的。\n> 协议：协议规定了通信双方必须遵守的数据传输格式，这样通信双方按照约定的格式才能准确的通信。\n> 无状态：无状态是指两次谅解通信之间是没有任何联系的，每次都是一个新的连接，服务端不会记录前后的请求信息。\n\n##### HTTP 请求流程\n\n![](./images/http.png)\n\n1. 浏览器通过域名解析服务器（DNS）获取IP地址\n2. 浏览器先向 IP 发起请求，并获取相应\n3. 在返回的响应内容（html）中，会带有 css、js、图片等 url 地址，以及 ajax 代码，浏览器按照响应内容中的顺序依次发送其他的请求，并获取相应的响应\n4. 浏览器每获取一个响应就对展示出的结果进行添加（加载），js，css 等内容会修改页面的内容，js也可以重新发送请求，获取响应\n5. 从获取第一个响应并在浏览器中展示，直到最终获取全部响应，并在展示的结果中添加内容或修改————这个过程叫做浏览器的渲染\n\n##### 五层网络模型\n\n![](./images/网络模型图.jpg)\n\n##### HTTP协议结构图\n\n![](./images/http协议结构图.jpg)\n\n##### 网络模型对应关系\n\n1. HTTP、RTSP、FTP -------> 应用层\n2. TCP、UDP -------> 传输层\n3. IP -------> 网络层\n4. 数据链路 -------> 数据链路层\n5. 物理介质 -------> 物理层\n\n##### url 地址格式\n\n![](./images/http的url规则.jpg)\n> 格式说明： scheme://host[:port]/path/…/[?query-string][#anchor]\n\n1. scheme：协议（例如：http, https, ftp）\n2. host：服务器的 IP 地址或者域名\n3. port：服务器的端口（如果是走协议默认端口，缺省端口80）\n4. path：访问资源的路径\n5. query-string：参数，发送给 http 服务器的数据\n6. anchor：锚（跳转到网页的指定锚点位置）\n\n#####  HTTP 请求\n- 请求格式\n![](./images/请求协议格式.png)\n\n- 案例\n![](./images/请求协议案例.jpg)\n\n- **请求方式**\n> 根据 HTTP 标准，HTTP 请求可以使用多种请求方法。\n>\n> HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD 方法。\n>\n> HTTP1.1 新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。\n\n| 请求方式 | 描述 |\n|---------|------------------------------------------------------------------------------------------------------------------------------------------|\n| GET     | 请求指定的页面信息，并返回实体主体。                                                                                                     |\n| HEAD    | 类似于 get 请求，只不过返回的响应中没有具体的内容，用于获取报头                                                                          |\n| POST    | 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 |\n| PUT     | 从客户端向服务器传送的数据取代指定的文档的内容                                                                                           |\n| DELETE  | 请求服务器删除指定的页面。                                                                                                               |\n| CONNECT | HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。                                                                                |\n| OPTIONS | 允许客户端查看服务器的性能。                                                                                                             |\n| TRACE   | 回显服务器收到的请求，主要用于测试或诊断。                                                                                               |\n\n- **常见请求头**\n\n| 请求头                            | 作用              |\n|-----------------------------------|-------------------|\n| **Cookie**                        | Cookie            |\n| **User-Agent**                    | 浏览器名称        |\n| **Referer**                       | 页面跳转处        |\n| Host                              | 主机和端口号      |\n| Connection                        | 链接类型          |\n| Upgrade-Insecure-Requests         | 升级为 HTTPS 请求 |\n| Accept                            | 传输文件类型      |\n| Accept-Encoding                   | 文件编解码格式    |\n| x-requested-with : XMLHttpRequest | ajax 请求         |\n\n[点击查看更多](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/HTTP%E8%AF%B7%E6%B1%82%E5%88%97%E8%A1%A8.md)\n\n##### HTTP 响应\n\n- **响应格式**\n\n> HTTP响应也由四个部分组成，分别是：状态行、消息报头、空行（回车符 + 换行符）和响应正文。\n![](./images/响应格式.jpg)\n\n- **响应头**\n\n\n| 响应头         | 作用                                           |\n|----------------|------------------------------------------------|\n| **Location**   | 这个头配合 302 状态码使用，告诉用户端找谁。    |\n| **Set-Cookie** | 设置和页面关联的 Cookie                        |\n| Content-Type   | 服务器通过这个头，回送数据的类型               |\n| Server         | 服务器通过这个头，告诉浏览器服务器的类型       |\n| Content-Length | 服务器通过这个头，告诉浏览器回送数据的长度     |\n| Connection     | 服务器通过这个头，响应完是保持链接还是关闭链接 |\n\n- **HTTP 状态码**\n\n>当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含 HTTP 状态码的信息头（server header）用以响应浏览器的请求。\n>\n> HTTP 状态码的英文为 HTTP Status Code。\n>\n> HTTP 状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，后两个数字没有分类的作用。HTTP 状态码共分为 5 种类型\n\n| 分类 | 分类描述                                       |\n|------|------------------------------------------------|\n| 1**  | 信息，服务器收到请求，需要请求者继续执行操作   |\n| 2**  | 成功，操作被成功接收并处理                     |\n| 3**  | 重定向，需要进一步的操作以完成请求             |\n| 4**  | 客户端错误，请求包含语法错误或无法完成请求     |\n| 5**  | 服务器错误，服务器在处理请求的过程中发生了错误 |\n\n- 常见的 HTTP 状态码：\n\t- 200 - 请求成功\n\t- 301 - 资源（网页等）被永久转移到其它 URL\n\t- 404 - 请求的资源（网页等）不存在\n\t- 500 - 内部服务器错误\n\n[点击查看更多](https://github.com/CriseLYJ/Python-crawler-tutorial-starts-from-zero/blob/master/HTTP%E5%93%8D%E5%BA%94%E5%88%97%E8%A1%A8.md)\n\n#### HTTPS\n\t- HTTP + SSL (安全套接字层)，即带有安全套接字层的超本文传输协议\n\t- 默认端口号：443\n\n- **HTTPS 作用**\n> 在传输过程中对数据进行加密，防止中间路由器、交换机等中间的路由设备对数据进行篡改。\n\n- **HTTP 与 HTTPS 优缺点**\n> HTTP 因为不需要对数据进行加密所以性能更高，但是安全性差。\n>\n>\n>HTTPS 虽然安全性高，但是因为浏览器和服务器端需要对数据进行加解密，所以占用服务器资源。\n\n- **当前形式**\n> 注意：目前 HTTPS 是未来主流，微信小程序，iOS 客户端，android 客户端的接口提供都需要 HTTPS 接口支持。\n"
        },
        {
          "name": "网络请求模块的使用.md",
          "type": "blob",
          "size": 11.486328125,
          "content": "   * [网络请求模块](#网络请求模块)\n      * [requests](#requests)\n         * [介绍](#介绍)\n         * [学习资料](#学习资料)\n      * [requests模块的安装](#requests模块的安装)\n         * [requests模块的使用](#requests模块的使用)\n            * [基本使用](#基本使用)\n            * [自定义请求头](#自定义请求头)\n            * [发送 GET 请求](#发送-get-请求)\n            * [发送 POST 请求](#发送-post-请求)\n            * [使用代理服务器](#使用代理服务器)\n            * [发送请求携带 Cookies](#发送请求携带-cookies)\n            * [错误证书处理](#错误证书处理)\n            * [超时处理](#超时处理)\n            * [重试处理](#重试处理)\n      * [urllib2](#urllib2)\n         * [使用流程](#使用流程)\n      * [urllib](#urllib)\n         * [urllib使用注意事项](#urllib使用注意事项)\n      \n# 网络请求模块\n## requests\n### 介绍\n> ``requests`` 模块是可以模仿浏览器发送请求获取响应\n> ``requests`` 模块在python2,与python3中通用\n> ``requests``模块能够自动帮助我们解压网页内容\n\n### 学习资料\n[中文文档](http://docs.python-requests.org/zh_CN/latest/index.html)\n\n## requests模块的安装\n> pip install requests\n> \n> 如果你本地有python2,和python3两个环境，你想装在python3中，建议使用下面这种方式安装\n\n> pip3 install requests\n\n### requests模块的使用\n#### 基本使用\n- 使用方式\n\n```\n# 导入模块\nimport requests\n# 定义请求地址\nurl = 'http://www.baidu.com'\n# 发送 GET 请求获取响应\nresponse = requests.get(url)\n# 获取响应的 html 内容\nhtml = response.text\n```\n\n- 代码讲解\n- response 常用属性\n\t- response.text 返回响应内容，响应内容为 str 类型\n\t- respones.content 返回响应内容,响应内容为 bytes 类型\n\t- response.status_code 返回响应状态码\n\t- response.request.headers 返回请求头\n\t- response.headers 返回响应头\n\t- response.cookies 返回响应的 RequestsCookieJar 对象\n- response.content 转换 str 类型\n\n```\n# 获取字节数据\ncontent = response.content\n# 转换成字符串类型\nhtml = content.decode('utf-8')\n```\n\n- response.cookies 操作\n\n```\n# 返回 RequestsCookieJar 对象\ncookies = response.cookies\n# RequestsCookieJar 转 dict\nrequests.utils.dict_from_cookiejar(cookies)\n# dict 转 RequestsCookieJar\nrequests.utils.cookiejar_from_dict()\n# 对cookie进行操作,把一个字典添加到cookiejar中\nrequests.utils.add_dict_to_cookiejar()\n```\n\n#### 自定义请求头\n- 使用方式\n\n```\n# 导入模块\nimport requests\n# 定义请求地址\nurl = 'http://www.baidu.com'\n# 定义自定义请求头\nheaders = {\n  \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n}\n# 发送自定义请求头\nresponse = requests.get(url,headers=headers)\n# 获取响应的 html 内容\nhtml = response.text\n```\n\n- 代码讲解\n> 发送请求时添加 headers 参数作为自定义请求头\n\n#### 发送 GET 请求\n- 使用方式\n\n```\n# 导入模块\nimport requests\n# 定义请求地址\nurl = 'http://www.baidu.com/s'\n# 定义自定义请求头\nheaders = {\n  \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n}\n# 定义 GET 请求参数\nparams = {\n  \"kw\":\"hello\"\n}\n# 使用 GET 请求参数发送请求\nresponse = requests.get(url,headers=headers,params=params)\n# 获取响应的 html 内容\nhtml = response.text\n```\n\n- 代码讲解\n> 发送请求时 params 参数作为 GET 请求参数\n\n#### 发送 POST 请求\n- 使用方式\n\n```\n# 导入模块\nimport requests\n# 定义请求地址\nurl = 'http://www.baidu.com'\n# 定义自定义请求头\nheaders = {\n  \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n}\n# 定义post请求参数\ndata = {\n  \"kw\":\"hello\"\n}\n\n# 使用 POST 请求参数发送请求\nresponse = requests.post(url,headers=headers,data=data)\n# 获取响应的 html 内容\nhtml = response.text\n```\n\n- 代码讲解\n> 发送请求时 data 参数作为 POST 请求参数\n\n####保存图片\n- 使用方式\n\n```\n# 导入模块\nimport requests\n# 下载图片地址\nurl = \"http://docs.python-requests.org/zh_CN/latest/_static/requests-sidebar.png\"\n# 发送请求获取响应\nresponse = requests.get(url)\n# 保存图片\nwith open('image.png','wb') as f:\n  f.write(response.content)\n``` \n\n- 代码讲解\n\n> 保存图片时后缀名和请求的后缀名一致\n> \n> 保存必须使用 response.content 进行保存文件\n\n\n#### 使用代理服务器\n- 作用\n\t- 让服务器以为不是同一个客户端在请求\n\t- 防止我们的真实地址被泄露，防止被追究\n- 使用代理的过程\n\n![](./images/使用代理的过程.png)\n\n- 代理分类\n- 透明代理(Transparent Proxy)：透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以查到你是谁。\n- 匿名代理(Anonymous Proxy)：匿名代理比透明代理进步了一点：别人只能知道你用了代理，无法知道你是谁。\n- 混淆代理(Distorting Proxies)：与匿名代理相同，如果使用了混淆代理，别人还是能知道你在用代理，但是会得到一个假的IP地址，伪装的更逼真\n- 高匿代理(Elite proxy或High Anonymity Proxy)：可以看出来，高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。\n> 在使用的使用，毫无疑问使用高匿代理效果最好\n> \n> 从使用的协议：代理ip可以分为http代理，https代理，socket代理等，使用的时候需要根据抓取网站的协议来选择\n\n- 使用方式\n\n```python\n# 导入模块\nimport requests\n# 定义请求地址\nurl = 'http://www.baidu.com'\n# 定义自定义请求头\nheaders = {\n  \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n}\n# 定义 代理服务器\nproxies = {\n  \"http\":\"http://IP地址:端口号\",\n  \"https\":\"https://IP地址:端口号\"\n}\n# 使用 POST 请求参数发送请求\nresponse = requests.get(url,headers=headers,proxies=proxies)\n# 获取响应的 html 内容\nhtml = response.text\n```\n\n- 代码讲解\n\n> 发送请求时 proxies 参数设置代理\n\n#### 发送请求携带 Cookies\n- 使用方式\n\n\n> 直接在自定义请求头中携带 Cookie\n> \n> 通过请求参数携带 Cookie 对象\n\n- 代码\n\n```python\n# 导入模块\nimport requests\n# 定义请求地址\nurl = 'http://www.baidu.com'\n# 定义自定义请求头\nheaders = {\n  \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n  # 方式一：直接在请求头中携带Cookie内容\n  \"Cookie\": \"Cookie值\"\n}\n# 方式二：定义 cookies 值\ncookies = {\n  \"xx\":\"yy\"\n}\n# 使用 POST 请求参数发送请求\nresponse = requests.get(url,headers=headers,cookies=cookies)\n# 获取响应的 html 内容\nhtml = response.text\n```\n\n- 代码讲解\n\n> 发送请求时 cookies 参数携带 Cookies\n\n#### 错误证书处理\n- 问题描述\n![](./images/12306ssl错误.png)\n\n- 使用方式\n\n```python\n# 导入模块\nimport requests\n\nurl = \"https://www.12306.cn/mormhweb/\"\n# 设置忽略证书\nresponse = requests.get(url,verify=False)\n```\n\n- 代码讲解\n\n\n> 发送请求时 verify 参数设置为 False 表示不验证CA证书\n\n#### 超时处理\n- 使用方式\n\n```python\n# 导入模块\nimport requests\n\nurl = \"https://www.baidu.com\"\n# 设置忽略证书\nresponse = requests.get(url,timeout=5)\n```\n\n- 代码讲解\n\n\n> 发送请求时 timeout 参数设置为超时秒数\n\n#### 重试处理\n- 使用方式\n\n```python\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n'''\n可以使用第三方模块 retrying 模块\n1. pip install retrying\n\n'''\nimport requests\n# 1. 导入模块\nfrom retrying import retry\n\n# 2. 使用装饰器进行重试设置\n# stop_max_attempt_number 表示重试次数\n@retry(stop_max_attempt_number=3)\ndef parse_url(url):\n    print(\"访问url:\",url)\n    headers = {\n        \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36\"\n    }\n    proxies = {\n        \"http\":\"http://124.235.135.210:80\"\n    }\n    # 设置超时参数\n    response = requests.get(url,headers=headers,proxies=proxies,timeout=5)\n    return response.text\n\nif __name__ == '__main__':\n    url = \"http://www.baidu.com\"\n    try:\n        html = parse_url(url)\n        print(html)\n    except Exception as e:\n        # 把 url 记录到日志文件中，未来进行手动分析，然后对url进行重新请求\n        print(e)\n        \n```\n\n- 代码讲解\n安装 ``retrying`` 模块\n\n> retrying 模块可以通过装饰器模式对某个函数进行监控，如果该函数引发异常就会触发重试操作\n\n> pip install retrying\n\n- 对需要重试的函数进行装饰器设置\n\n> 通过 ``@retry(stop_max_attempt_number=重试次数)`` 参数设置重试次数\n\n```python\n# 1. 导入模块\nfrom retrying import retry\n# 2. 装饰器设置重试函数\n@retry(stop_max_attempt_number=3)\ndef exec_func():\n    pass\n``` \n    \n    \n\n## urllib2\n> 在python2 中使用urllib2网络库\n\n### 使用流程\n1. 导入urllib2模块\n2. 发起请求获取响应数据\n\t1. 定义请求地址\n\t2. 自定义请求头\n\t3. 自定义请求对象\n\t4. 发送请求\n3. 处理响应内容\n\n```python2\n#!/usr/bin/python2\n# -*- coding: utf-8 -*-\n#1. 导入urllib2模块\nimport urllib2\n#2. 发起请求获取响应数据\n#2.1. 定义请求地址\nurl = \"https://github.com\"\n#2.2. 自定义请求头\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36\",\n    \"Referer\": \"https://github.com/\",\n    \"Host\": \"github.com\"\n}\n#2.3. 自定义请求对象\nreq = urllib2.Request(\n    url=url,\n    headers=headers\n)\n#2.4. 发送请求\nresp = urllib2.urlopen(req)\n#3. 处理响应内容\nwith open(\"text.txt\", \"wb\") as f:\n\tf.write(resp.read())\n\n\n```\n\n## urllib\n> python3 中使用urllib网络库\n\n```python3\n#!/usr/bin/python3\n# -*- coding: utf-8 -*-\nimport urllib.request\n\n# 2. 发起网络请求\n# 2.1. 定义请求地址\nurl = \"https://github.com\"\n# 2.2. 自定义请求头\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36\",\n    \"Referer\": \"https://github.com/\",\n    \"Host\": \"github.com\"\n}\n\n# 定义请求对象\nreq = urllib.request.Request(\n    url=url,\n    headers=headers\n)\n\n# 发送请求\nresp = urllib.request.urlopen(req)\n\n# 处理响应\nwith open('github.txt', 'wb') as f:\n    f.write(resp.read())\n```\n\n### urllib使用注意事项\n- 如果使用在URL中需要进行转义\n\n```python3\n #!/usr/bin/python3\n # -*- coding: utf-8 -*-\n\n # 1. 导入模块\n import urllib.request\n import urllib.parse\n\n # 2. 发起请求获取响应\n\n wd = input(\"请输入查询内容：\")\n\n # 2.1 定义请求地址\n url = \"https://www.baidu.com/s?wd=\"\n # 2.2 定义自定义请求头\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36\",\n    \"Referer\": \"https://github.com/\",\n    \"Host\": \"github.com\"\n}\n # 2.3 定义请求对象\n request = urllib.request.Request(\n     url=url + urllib.parse.quote(wd),\n     headers=headers\n )\n # 2.4 发送请求\n response = urllib.request.urlopen(request)\n\n # 3. 处理响应\n with open('02.html','wb') as f:\n     f.write(response.read())\nresponse.read() \n```\n- 返回值是字节串，获取字符串内容需要进行 decode\n\n```python3\n html = response.read().decode('utf-8')\n```\n \n \n"
        },
        {
          "name": "请求分析.md",
          "type": "blob",
          "size": 0.5986328125,
          "content": "  * [请求分析](#请求分析)\n    * [chrome调试面板的使用](#chrome调试面板的使用)\n    * [请求分析的流程图](#请求分析的流程图)\n\n# 请求分析\n## chrome调试面板的使用\n\n![](./images/chrom分析.jpg)\n\n\n\n## 请求分析的流程图\n\n![](./images/分析请求步骤jpg.jpg)\n\n1. 寻找指定的url\n2. 确定请求的方式\n3. 获取到请求的参数\n4. 获取请求头\n\n- 请求头使用\n> 模拟请求时，先在headers中加入``User-Agent``,如果还不可以请求再尝试加入``Referer``,还无法访问，在尝试加入``Cookie``,在最后可以尝试加入``Host``.\n"
        }
      ]
    }
  ]
}