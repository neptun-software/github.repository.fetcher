{
  "metadata": {
    "timestamp": 1736559714818,
    "page": 405,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "google-deepmind/learning-to-learn",
      "stars": 4061,
      "defaultBranch": "master",
      "files": [
        {
          "name": "CONTRIBUTING",
          "type": "blob",
          "size": 1.41796875,
          "content": "Want to contribute? Great! First, read this page (including the small print at the end).\n\n### Before you contribute\nBefore we can use your code, you must sign the\n[Google Individual Contributor License Agreement]\n(https://cla.developers.google.com/about/google-individual)\n(CLA), which you can do online. The CLA is necessary mainly because you own the\ncopyright to your changes, even after your contribution becomes part of our\ncodebase, so we need your permission to use and distribute your code. We also\nneed to be sure of various other thingsâ€”for instance that you'll tell us if you\nknow that your code infringes on other people's patents. You don't have to sign\nthe CLA until after you've submitted your code for review and a member has\napproved it, but you must do it before we can put your code into our codebase.\nBefore you start working on a larger contribution, you should get in touch with\nus first through the issue tracker with your idea so that we can help out and\npossibly guide you. Coordinating up front makes it much easier to avoid\nfrustration later on.\n\n### Code reviews\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose.\n\n### The small print\nContributions made by corporations are covered by a different agreement than\nthe one above, the\n[Software Grant and Corporate Contributor License Agreement]\n(https://cla.developers.google.com/about/google-corporate).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.8154296875,
          "content": "# [Learning to Learn](https://arxiv.org/abs/1606.04474) in TensorFlow\n\n\n## Dependencies\n\n* [TensorFlow >=1.0](https://www.tensorflow.org/)\n* [Sonnet >=1.0](https://github.com/deepmind/sonnet)\n\n\n## Training\n\n```\npython train.py --problem=mnist --save_path=./mnist\n```\n\nCommand-line flags:\n\n* `save_path`: If present, the optimizer will be saved to the specified path\n    every time the evaluation performance is improved.\n* `num_epochs`: Number of training epochs.\n* `log_period`: Epochs before mean performance and time is reported.\n* `evaluation_period`: Epochs before the optimizer is evaluated.\n* `evaluation_epochs`: Number of evaluation epochs.\n* `problem`: Problem to train on. See [Problems](#problems) section below.\n* `num_steps`: Number of optimization steps.\n* `unroll_length`: Number of unroll steps for the optimizer.\n* `learning_rate`: Learning rate.\n* `second_derivatives`: If `true`, the optimizer will try to compute second\n    derivatives through the loss function specified by the problem.\n\n\n## Evaluation\n\n```\npython evaluate.py --problem=mnist --optimizer=L2L --path=./mnist\n```\n\nCommand-line flags:\n\n* `optimizer`: `Adam` or `L2L`.\n* `path`: Path to saved optimizer, only relevant if using the `L2L` optimizer.\n* `learning_rate`: Learning rate, only relevant if using `Adam` optimizer.\n* `num_epochs`: Number of evaluation epochs.\n* `seed`: Seed for random number generation.\n* `problem`: Problem to evaluate on. See [Problems](#problems) section below.\n* `num_steps`: Number of optimization steps.\n\n\n## Problems\n\nThe training and evaluation scripts support the following problems (see\n`util.py` for more details):\n\n* `simple`: One-variable quadratic function.\n* `simple-multi`: Two-variable quadratic function, where one of the variables\n    is optimized using a learned optimizer and the other one using Adam.\n* `quadratic`: Batched ten-variable quadratic function.\n* `mnist`: Mnist classification using a two-layer fully connected network.\n* `cifar`: Cifar10 classification using a convolutional neural network.\n* `cifar-multi`: Cifar10 classification using a convolutional neural network,\n    where two independent learned optimizers are used. One to optimize\n    parameters from convolutional layers and the other one for parameters from\n    fully connected layers.\n\n\nNew problems can be implemented very easily. You can see in `train.py` that\nthe `meta_minimize` method from the `MetaOptimizer` class is given a function\nthat returns the TensorFlow operation that generates the loss function we want\nto minimize (see `problems.py` for an example).\n\nIt's important that all operations with Python side effects (e.g. queue\ncreation) must be done outside of the function passed to `meta_minimize`. The\n`cifar10` function in `problems.py` is a good example of a loss function that\nuses TensorFlow queues.\n\n\nDisclaimer: This is not an official Google product.\n"
        },
        {
          "name": "convergence_test.py",
          "type": "blob",
          "size": 2.0283203125,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for L2L TensorFlow implementation.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nimport tensorflow as tf\n\nimport meta\nimport problems\n\n\ndef train(sess, minimize_ops, num_epochs, num_unrolls):\n  \"\"\"L2L training.\"\"\"\n  step, update, reset, loss_last, x_last = minimize_ops\n\n  for _ in xrange(num_epochs):\n    sess.run(reset)\n    for _ in xrange(num_unrolls):\n      cost, final_x, unused_1, unused_2 = sess.run([loss_last, x_last,\n                                                    update, step])\n\n  return cost, final_x\n\n\nclass L2LTest(tf.test.TestCase):\n  \"\"\"Tests L2L TensorFlow implementation.\"\"\"\n\n  def testSimple(self):\n    \"\"\"Tests L2L applied to simple problem.\"\"\"\n    problem = problems.simple()\n    optimizer = meta.MetaOptimizer(net=dict(\n        net=\"CoordinateWiseDeepLSTM\",\n        net_options={\n            \"layers\": (),\n            # Initializing the network to zeros makes learning more stable.\n            \"initializer\": \"zeros\"\n        }))\n    minimize_ops = optimizer.meta_minimize(problem, 20, learning_rate=1e-2)\n    # L2L should solve the simple problem is less than 500 epochs.\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      cost, _ = train(sess, minimize_ops, 500, 5)\n    self.assertLess(cost, 1e-5)\n\n\nif __name__ == \"__main__\":\n  tf.test.main()\n"
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 3.0810546875,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Learning 2 Learn evaluation.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nimport tensorflow as tf\n\nfrom tensorflow.contrib.learn.python.learn import monitored_session as ms\n\nimport meta\nimport util\n\nflags = tf.flags\nlogging = tf.logging\n\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\"optimizer\", \"L2L\", \"Optimizer.\")\nflags.DEFINE_string(\"path\", None, \"Path to saved meta-optimizer network.\")\nflags.DEFINE_integer(\"num_epochs\", 100, \"Number of evaluation epochs.\")\nflags.DEFINE_integer(\"seed\", None, \"Seed for TensorFlow's RNG.\")\n\nflags.DEFINE_string(\"problem\", \"simple\", \"Type of problem.\")\nflags.DEFINE_integer(\"num_steps\", 100,\n                     \"Number of optimization steps per epoch.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate.\")\n\n\ndef main(_):\n  # Configuration.\n  num_unrolls = FLAGS.num_steps\n\n  if FLAGS.seed:\n    tf.set_random_seed(FLAGS.seed)\n\n  # Problem.\n  problem, net_config, net_assignments = util.get_config(FLAGS.problem,\n                                                         FLAGS.path)\n\n  # Optimizer setup.\n  if FLAGS.optimizer == \"Adam\":\n    cost_op = problem()\n    problem_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    problem_reset = tf.variables_initializer(problem_vars)\n\n    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n    optimizer_reset = tf.variables_initializer(optimizer.get_slot_names())\n    update = optimizer.minimize(cost_op)\n    reset = [problem_reset, optimizer_reset]\n  elif FLAGS.optimizer == \"L2L\":\n    if FLAGS.path is None:\n      logging.warning(\"Evaluating untrained L2L optimizer\")\n    optimizer = meta.MetaOptimizer(**net_config)\n    meta_loss = optimizer.meta_loss(problem, 1, net_assignments=net_assignments)\n    _, update, reset, cost_op, _ = meta_loss\n  else:\n    raise ValueError(\"{} is not a valid optimizer\".format(FLAGS.optimizer))\n\n  with ms.MonitoredSession() as sess:\n    # Prevent accidental changes to the graph.\n    tf.get_default_graph().finalize()\n\n    total_time = 0\n    total_cost = 0\n    for _ in xrange(FLAGS.num_epochs):\n      # Training.\n      time, cost = util.run_epoch(sess, cost_op, [update], reset,\n                                  num_unrolls)\n      total_time += time\n      total_cost += cost\n\n    # Results.\n    util.print_stats(\"Epoch {}\".format(FLAGS.num_epochs), total_cost,\n                     total_time, FLAGS.num_epochs)\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n"
        },
        {
          "name": "meta.py",
          "type": "blob",
          "size": 13.2529296875,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Learning to learn (meta) optimizer.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport os\n\nimport mock\nimport sonnet as snt\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.util import nest\n\nimport networks\n\n\ndef _nested_assign(ref, value):\n  \"\"\"Returns a nested collection of TensorFlow assign operations.\n\n  Args:\n    ref: Nested collection of TensorFlow variables.\n    value: Values to be assigned to the variables. Must have the same structure\n        as `ref`.\n\n  Returns:\n    Nested collection (same structure as `ref`) of TensorFlow assign operations.\n\n  Raises:\n    ValueError: If `ref` and `values` have different structures.\n  \"\"\"\n  if isinstance(ref, list) or isinstance(ref, tuple):\n    if len(ref) != len(value):\n      raise ValueError(\"ref and value have different lengths.\")\n    result = [_nested_assign(r, v) for r, v in zip(ref, value)]\n    if isinstance(ref, tuple):\n      return tuple(result)\n    return result\n  else:\n    return tf.assign(ref, value)\n\n\ndef _nested_variable(init, name=None, trainable=False):\n  \"\"\"Returns a nested collection of TensorFlow variables.\n\n  Args:\n    init: Nested collection of TensorFlow initializers.\n    name: Variable name.\n    trainable: Make variables trainable (`False` by default).\n\n  Returns:\n    Nested collection (same structure as `init`) of TensorFlow variables.\n  \"\"\"\n  if isinstance(init, list) or isinstance(init, tuple):\n    result = [_nested_variable(i, name, trainable) for i in init]\n    if isinstance(init, tuple):\n      return tuple(result)\n    return result\n  else:\n    return tf.Variable(init, name=name, trainable=trainable)\n\n\ndef _wrap_variable_creation(func, custom_getter):\n  \"\"\"Provides a custom getter for all variable creations.\"\"\"\n  original_get_variable = tf.get_variable\n  def custom_get_variable(*args, **kwargs):\n    if hasattr(kwargs, \"custom_getter\"):\n      raise AttributeError(\"Custom getters are not supported for optimizee \"\n                           \"variables.\")\n    return original_get_variable(*args, custom_getter=custom_getter, **kwargs)\n\n  # Mock the get_variable method.\n  with mock.patch(\"tensorflow.get_variable\", custom_get_variable):\n    return func()\n\n\ndef _get_variables(func):\n  \"\"\"Calls func, returning any variables created, but ignoring its return value.\n\n  Args:\n    func: Function to be called.\n\n  Returns:\n    A tuple (variables, constants) where the first element is a list of\n    trainable variables and the second is the non-trainable variables.\n  \"\"\"\n  variables = []\n  constants = []\n\n  def custom_getter(getter, name, **kwargs):\n    trainable = kwargs[\"trainable\"]\n    kwargs[\"trainable\"] = False\n    variable = getter(name, **kwargs)\n    if trainable:\n      variables.append(variable)\n    else:\n      constants.append(variable)\n    return variable\n\n  with tf.name_scope(\"unused_graph\"):\n    _wrap_variable_creation(func, custom_getter)\n\n  return variables, constants\n\n\ndef _make_with_custom_variables(func, variables):\n  \"\"\"Calls func and replaces any trainable variables.\n\n  This returns the output of func, but whenever `get_variable` is called it\n  will replace any trainable variables with the tensors in `variables`, in the\n  same order. Non-trainable variables will re-use any variables already\n  created.\n\n  Args:\n    func: Function to be called.\n    variables: A list of tensors replacing the trainable variables.\n\n  Returns:\n    The return value of func is returned.\n  \"\"\"\n  variables = collections.deque(variables)\n\n  def custom_getter(getter, name, **kwargs):\n    if kwargs[\"trainable\"]:\n      return variables.popleft()\n    else:\n      kwargs[\"reuse\"] = True\n      return getter(name, **kwargs)\n\n  return _wrap_variable_creation(func, custom_getter)\n\n\nMetaLoss = collections.namedtuple(\"MetaLoss\", \"loss, update, reset, fx, x\")\nMetaStep = collections.namedtuple(\"MetaStep\", \"step, update, reset, fx, x\")\n\n\ndef _make_nets(variables, config, net_assignments):\n  \"\"\"Creates the optimizer networks.\n\n  Args:\n    variables: A list of variables to be optimized.\n    config: A dictionary of network configurations, each of which will be\n        passed to networks.Factory to construct a single optimizer net.\n    net_assignments: A list of tuples where each tuple is of the form (netid,\n        variable_names) and is used to assign variables to networks. netid must\n        be a key in config.\n\n  Returns:\n    A tuple (nets, keys, subsets) where nets is a dictionary of created\n    optimizer nets such that the net with key keys[i] should be applied to the\n    subset of variables listed in subsets[i].\n\n  Raises:\n    ValueError: If net_assignments is None and the configuration defines more\n        than one network.\n  \"\"\"\n  # create a dictionary which maps a variable name to its index within the\n  # list of variables.\n  name_to_index = dict((v.name.split(\":\")[0], i)\n                       for i, v in enumerate(variables))\n\n  if net_assignments is None:\n    if len(config) != 1:\n      raise ValueError(\"Default net_assignments can only be used if there is \"\n                       \"a single net config.\")\n\n    with tf.variable_scope(\"vars_optimizer\"):\n      key = next(iter(config))\n      kwargs = config[key]\n      net = networks.factory(**kwargs)\n\n    nets = {key: net}\n    keys = [key]\n    subsets = [range(len(variables))]\n  else:\n    nets = {}\n    keys = []\n    subsets = []\n    with tf.variable_scope(\"vars_optimizer\"):\n      for key, names in net_assignments:\n        if key in nets:\n          raise ValueError(\"Repeated netid in net_assigments.\")\n        nets[key] = networks.factory(**config[key])\n        subset = [name_to_index[name] for name in names]\n        keys.append(key)\n        subsets.append(subset)\n        print(\"Net: {}, Subset: {}\".format(key, subset))\n\n  # subsets should be a list of disjoint subsets (as lists!) of the variables\n  # and nets should be a list of networks to apply to each subset.\n  return nets, keys, subsets\n\n\nclass MetaOptimizer(object):\n  \"\"\"Learning to learn (meta) optimizer.\n\n  Optimizer which has an internal RNN which takes as input, at each iteration,\n  the gradient of the function being minimized and returns a step direction.\n  This optimizer can then itself be optimized to learn optimization on a set of\n  tasks.\n  \"\"\"\n\n  def __init__(self, **kwargs):\n    \"\"\"Creates a MetaOptimizer.\n\n    Args:\n      **kwargs: A set of keyword arguments mapping network identifiers (the\n          keys) to parameters that will be passed to networks.Factory (see docs\n          for more info).  These can be used to assign different optimizee\n          parameters to different optimizers (see net_assignments in the\n          meta_loss method).\n    \"\"\"\n    self._nets = None\n\n    if not kwargs:\n      # Use a default coordinatewise network if nothing is given. this allows\n      # for no network spec and no assignments.\n      self._config = {\n          \"coordinatewise\": {\n              \"net\": \"CoordinateWiseDeepLSTM\",\n              \"net_options\": {\n                  \"layers\": (20, 20),\n                  \"preprocess_name\": \"LogAndSign\",\n                  \"preprocess_options\": {\"k\": 5},\n                  \"scale\": 0.01,\n              }}}\n    else:\n      self._config = kwargs\n\n  def save(self, sess, path=None):\n    \"\"\"Save meta-optimizer.\"\"\"\n    result = {}\n    for k, net in self._nets.items():\n      if path is None:\n        filename = None\n        key = k\n      else:\n        filename = os.path.join(path, \"{}.l2l\".format(k))\n        key = filename\n      net_vars = networks.save(net, sess, filename=filename)\n      result[key] = net_vars\n    return result\n\n  def meta_loss(self,\n                make_loss,\n                len_unroll,\n                net_assignments=None,\n                second_derivatives=False):\n    \"\"\"Returns an operator computing the meta-loss.\n\n    Args:\n      make_loss: Callable which returns the optimizee loss; note that this\n          should create its ops in the default graph.\n      len_unroll: Number of steps to unroll.\n      net_assignments: variable to optimizer mapping. If not None, it should be\n          a list of (k, names) tuples, where k is a valid key in the kwargs\n          passed at at construction time and names is a list of variable names.\n      second_derivatives: Use second derivatives (default is false).\n\n    Returns:\n      namedtuple containing (loss, update, reset, fx, x)\n    \"\"\"\n\n    # Construct an instance of the problem only to grab the variables. This\n    # loss will never be evaluated.\n    x, constants = _get_variables(make_loss)\n\n    print(\"Optimizee variables\")\n    print([op.name for op in x])\n    print(\"Problem variables\")\n    print([op.name for op in constants])\n\n    # Create the optimizer networks and find the subsets of variables to assign\n    # to each optimizer.\n    nets, net_keys, subsets = _make_nets(x, self._config, net_assignments)\n\n    # Store the networks so we can save them later.\n    self._nets = nets\n\n    # Create hidden state for each subset of variables.\n    state = []\n    with tf.name_scope(\"states\"):\n      for i, (subset, key) in enumerate(zip(subsets, net_keys)):\n        net = nets[key]\n        with tf.name_scope(\"state_{}\".format(i)):\n          state.append(_nested_variable(\n              [net.initial_state_for_inputs(x[j], dtype=tf.float32)\n               for j in subset],\n              name=\"state\", trainable=False))\n\n    def update(net, fx, x, state):\n      \"\"\"Parameter and RNN state update.\"\"\"\n      with tf.name_scope(\"gradients\"):\n        gradients = tf.gradients(fx, x)\n\n        # Stopping the gradient here corresponds to what was done in the\n        # original L2L NIPS submission. However it looks like things like\n        # BatchNorm, etc. don't support second-derivatives so we still need\n        # this term.\n        if not second_derivatives:\n          gradients = [tf.stop_gradient(g) for g in gradients]\n\n      with tf.name_scope(\"deltas\"):\n        deltas, state_next = zip(*[net(g, s) for g, s in zip(gradients, state)])\n        state_next = list(state_next)\n\n      return deltas, state_next\n\n    def time_step(t, fx_array, x, state):\n      \"\"\"While loop body.\"\"\"\n      x_next = list(x)\n      state_next = []\n\n      with tf.name_scope(\"fx\"):\n        fx = _make_with_custom_variables(make_loss, x)\n        fx_array = fx_array.write(t, fx)\n\n      with tf.name_scope(\"dx\"):\n        for subset, key, s_i in zip(subsets, net_keys, state):\n          x_i = [x[j] for j in subset]\n          deltas, s_i_next = update(nets[key], fx, x_i, s_i)\n\n          for idx, j in enumerate(subset):\n            x_next[j] += deltas[idx]\n          state_next.append(s_i_next)\n\n      with tf.name_scope(\"t_next\"):\n        t_next = t + 1\n\n      return t_next, fx_array, x_next, state_next\n\n    # Define the while loop.\n    fx_array = tf.TensorArray(tf.float32, size=len_unroll + 1,\n                              clear_after_read=False)\n    _, fx_array, x_final, s_final = tf.while_loop(\n        cond=lambda t, *_: t < len_unroll,\n        body=time_step,\n        loop_vars=(0, fx_array, x, state),\n        parallel_iterations=1,\n        swap_memory=True,\n        name=\"unroll\")\n\n    with tf.name_scope(\"fx\"):\n      fx_final = _make_with_custom_variables(make_loss, x_final)\n      fx_array = fx_array.write(len_unroll, fx_final)\n\n    loss = tf.reduce_sum(fx_array.stack(), name=\"loss\")\n\n    # Reset the state; should be called at the beginning of an epoch.\n    with tf.name_scope(\"reset\"):\n      variables = (nest.flatten(state) +\n                   x + constants)\n      # Empty array as part of the reset process.\n      reset = [tf.variables_initializer(variables), fx_array.close()]\n\n    # Operator to update the parameters and the RNN state after our loop, but\n    # during an epoch.\n    with tf.name_scope(\"update\"):\n      update = (nest.flatten(_nested_assign(x, x_final)) +\n                nest.flatten(_nested_assign(state, s_final)))\n\n    # Log internal variables.\n    for k, net in nets.items():\n      print(\"Optimizer '{}' variables\".format(k))\n      print([op.name for op in snt.get_variables_in_module(net)])\n\n    return MetaLoss(loss, update, reset, fx_final, x_final)\n\n  def meta_minimize(self, make_loss, len_unroll, learning_rate=0.01, **kwargs):\n    \"\"\"Returns an operator minimizing the meta-loss.\n\n    Args:\n      make_loss: Callable which returns the optimizee loss; note that this\n          should create its ops in the default graph.\n      len_unroll: Number of steps to unroll.\n      learning_rate: Learning rate for the Adam optimizer.\n      **kwargs: keyword arguments forwarded to meta_loss.\n\n    Returns:\n      namedtuple containing (step, update, reset, fx, x)\n    \"\"\"\n    info = self.meta_loss(make_loss, len_unroll, **kwargs)\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    step = optimizer.minimize(info.loss)\n    return MetaStep(step, *info[1:])\n"
        },
        {
          "name": "meta_test.py",
          "type": "blob",
          "size": 7.4619140625,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for L2L meta-optimizer.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\n\nfrom nose_parameterized import parameterized\nimport numpy as np\nfrom six.moves import xrange\nimport sonnet as snt\nimport tensorflow as tf\n\nimport meta\nimport problems\n\n\ndef train(sess, minimize_ops, num_epochs, num_unrolls):\n  \"\"\"L2L training.\"\"\"\n  step, update, reset, loss_last, x_last = minimize_ops\n\n  for _ in xrange(num_epochs):\n    sess.run(reset)\n    for _ in xrange(num_unrolls):\n      cost, final_x, unused_1, unused_2 = sess.run([loss_last, x_last,\n                                                    update, step])\n\n  return cost, final_x\n\n\nclass L2LTest(tf.test.TestCase):\n  \"\"\"Tests L2L meta-optimizer.\"\"\"\n\n  def testResults(self):\n    \"\"\"Tests reproducibility of Torch results.\"\"\"\n    problem = problems.simple()\n    optimizer = meta.MetaOptimizer(net=dict(\n        net=\"CoordinateWiseDeepLSTM\",\n        net_options={\n            \"layers\": (),\n            \"initializer\": \"zeros\"\n        }))\n    minimize_ops = optimizer.meta_minimize(problem, 5)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      cost, final_x = train(sess, minimize_ops, 1, 2)\n\n    # Torch results\n    torch_cost = 0.7325327\n    torch_final_x = 0.8559\n\n    self.assertAlmostEqual(cost, torch_cost, places=4)\n    self.assertAlmostEqual(final_x[0], torch_final_x, places=4)\n\n  @parameterized.expand([\n      # Shared optimizer.\n      (\n          None,\n          {\n              \"net\": {\n                  \"net\": \"CoordinateWiseDeepLSTM\",\n                  \"net_options\": {\"layers\": (1, 1,)}\n              }\n          }\n      ),\n      # Explicit sharing.\n      (\n          [(\"net\", [\"x_0\", \"x_1\"])],\n          {\n              \"net\": {\n                  \"net\": \"CoordinateWiseDeepLSTM\",\n                  \"net_options\": {\"layers\": (1,)}\n              }\n          }\n      ),\n      # Different optimizers.\n      (\n          [(\"net1\", [\"x_0\"]), (\"net2\", [\"x_1\"])],\n          {\n              \"net1\": {\n                  \"net\": \"CoordinateWiseDeepLSTM\",\n                  \"net_options\": {\"layers\": (1,)}\n              },\n              \"net2\": {\"net\": \"Adam\"}\n          }\n      ),\n      # Different optimizers for the same variable.\n      (\n          [(\"net1\", [\"x_0\"]), (\"net2\", [\"x_0\"])],\n          {\n              \"net1\": {\n                  \"net\": \"CoordinateWiseDeepLSTM\",\n                  \"net_options\": {\"layers\": (1,)}\n              },\n              \"net2\": {\n                  \"net\": \"CoordinateWiseDeepLSTM\",\n                  \"net_options\": {\"layers\": (1,)}\n              }\n          }\n      ),\n  ])\n  def testMultiOptimizer(self, net_assignments, net_config):\n    \"\"\"Tests different variable->net mappings in multi-optimizer problem.\"\"\"\n    problem = problems.simple_multi_optimizer(num_dims=2)\n    optimizer = meta.MetaOptimizer(**net_config)\n    minimize_ops = optimizer.meta_minimize(problem, 3,\n                                           net_assignments=net_assignments)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      train(sess, minimize_ops, 1, 2)\n\n  def testSecondDerivatives(self):\n    \"\"\"Tests second derivatives for simple problem.\"\"\"\n    problem = problems.simple()\n    optimizer = meta.MetaOptimizer(net=dict(\n        net=\"CoordinateWiseDeepLSTM\",\n        net_options={\"layers\": ()}))\n    minimize_ops = optimizer.meta_minimize(problem, 3,\n                                           second_derivatives=True)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      train(sess, minimize_ops, 1, 2)\n\n  def testConvolutional(self):\n    \"\"\"Tests L2L applied to problem with convolutions.\"\"\"\n    kernel_shape = 4\n    def convolutional_problem():\n      conv = snt.Conv2D(output_channels=1,\n                        kernel_shape=kernel_shape,\n                        stride=1,\n                        name=\"conv\")\n      output = conv(tf.random_normal((100, 100, 3, 10)))\n      return tf.reduce_sum(output)\n\n    net_config = {\n        \"conv\": {\n            \"net\": \"KernelDeepLSTM\",\n            \"net_options\": {\n                \"kernel_shape\": [kernel_shape] * 2,\n                \"layers\": (5,)\n            },\n        },\n    }\n    optimizer = meta.MetaOptimizer(**net_config)\n    minimize_ops = optimizer.meta_minimize(\n        convolutional_problem, 3,\n        net_assignments=[(\"conv\", [\"conv/w\"])]\n    )\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      train(sess, minimize_ops, 1, 2)\n\n  def testWhileLoopProblem(self):\n    \"\"\"Tests L2L applied to problem with while loop.\"\"\"\n    def while_loop_problem():\n      x = tf.get_variable(\"x\", shape=[], initializer=tf.ones_initializer())\n\n      # Strange way of squaring the variable.\n      _, x_squared = tf.while_loop(\n          cond=lambda t, _: t < 1,\n          body=lambda t, x: (t + 1, x * x),\n          loop_vars=(0, x),\n          name=\"loop\")\n      return x_squared\n\n    optimizer = meta.MetaOptimizer(net=dict(\n        net=\"CoordinateWiseDeepLSTM\",\n        net_options={\"layers\": ()}))\n    minimize_ops = optimizer.meta_minimize(while_loop_problem, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      train(sess, minimize_ops, 1, 2)\n\n  def testSaveAndLoad(self):\n    \"\"\"Tests saving and loading a meta-optimizer.\"\"\"\n    layers = (2, 3)\n    net_options = {\"layers\": layers, \"initializer\": \"zeros\"}\n    num_unrolls = 2\n    num_epochs = 1\n\n    problem = problems.simple()\n\n    # Original optimizer.\n    with tf.Graph().as_default() as g1:\n      optimizer = meta.MetaOptimizer(net=dict(\n          net=\"CoordinateWiseDeepLSTM\",\n          net_options=net_options))\n      minimize_ops = optimizer.meta_minimize(problem, 3)\n\n    with self.test_session(graph=g1) as sess:\n      sess.run(tf.global_variables_initializer())\n      train(sess, minimize_ops, 1, 2)\n\n      # Save optimizer.\n      tmp_dir = tempfile.mkdtemp()\n      save_result = optimizer.save(sess, path=tmp_dir)\n      net_path = next(iter(save_result))\n\n      # Retrain original optimizer.\n      cost, x = train(sess, minimize_ops, num_unrolls, num_epochs)\n\n    # Load optimizer and retrain in a new session.\n    with tf.Graph().as_default() as g2:\n      optimizer = meta.MetaOptimizer(net=dict(\n          net=\"CoordinateWiseDeepLSTM\",\n          net_options=net_options,\n          net_path=net_path))\n      minimize_ops = optimizer.meta_minimize(problem, 3)\n\n    with self.test_session(graph=g2) as sess:\n      sess.run(tf.global_variables_initializer())\n      cost_loaded, x_loaded = train(sess, minimize_ops, num_unrolls, num_epochs)\n\n    # The last cost should be the same.\n    self.assertAlmostEqual(cost, cost_loaded, places=3)\n    self.assertAlmostEqual(x[0], x_loaded[0], places=3)\n\n    # Cleanup.\n    os.remove(net_path)\n    os.rmdir(tmp_dir)\n\n\nif __name__ == \"__main__\":\n  tf.test.main()\n"
        },
        {
          "name": "networks.py",
          "type": "blob",
          "size": 11.5908203125,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Learning 2 Learn meta-optimizer networks.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\nimport sys\n\nimport dill as pickle\nimport numpy as np\nimport six\nimport sonnet as snt\nimport tensorflow as tf\n\nimport preprocess\n\n\ndef factory(net, net_options=(), net_path=None):\n  \"\"\"Network factory.\"\"\"\n\n  net_class = getattr(sys.modules[__name__], net)\n  net_options = dict(net_options)\n\n  if net_path:\n    with open(net_path, \"rb\") as f:\n      net_options[\"initializer\"] = pickle.load(f)\n\n  return net_class(**net_options)\n\n\ndef save(network, sess, filename=None):\n  \"\"\"Save the variables contained by a network to disk.\"\"\"\n  to_save = collections.defaultdict(dict)\n  variables = snt.get_variables_in_module(network)\n\n  for v in variables:\n    split = v.name.split(\":\")[0].split(\"/\")\n    module_name = split[-2]\n    variable_name = split[-1]\n    to_save[module_name][variable_name] = v.eval(sess)\n\n  if filename:\n    with open(filename, \"wb\") as f:\n      pickle.dump(to_save, f)\n\n  return to_save\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Network(snt.RNNCore):\n  \"\"\"Base class for meta-optimizer networks.\"\"\"\n\n  @abc.abstractmethod\n  def initial_state_for_inputs(self, inputs, **kwargs):\n    \"\"\"Initial state given inputs.\"\"\"\n    pass\n\n\ndef _convert_to_initializer(initializer):\n  \"\"\"Returns a TensorFlow initializer.\n\n  * Corresponding TensorFlow initializer when the argument is a string (e.g.\n  \"zeros\" -> `tf.zeros_initializer`).\n  * `tf.constant_initializer` when the argument is a `numpy` `array`.\n  * Identity when the argument is a TensorFlow initializer.\n\n  Args:\n    initializer: `string`, `numpy` `array` or TensorFlow initializer.\n\n  Returns:\n    TensorFlow initializer.\n  \"\"\"\n\n  if isinstance(initializer, str):\n    return getattr(tf, initializer + \"_initializer\")(dtype=tf.float32)\n  elif isinstance(initializer, np.ndarray):\n    return tf.constant_initializer(initializer)\n  else:\n    return initializer\n\n\ndef _get_initializers(initializers, fields):\n  \"\"\"Produces a nn initialization `dict` (see Linear docs for a example).\n\n  Grabs initializers for relevant fields if the first argument is a `dict` or\n  reuses the same initializer for all fields otherwise. All initializers are\n  processed using `_convert_to_initializer`.\n\n  Args:\n    initializers: Initializer or <variable, initializer> dictionary.\n    fields: Fields nn is expecting for module initialization.\n\n  Returns:\n    nn initialization dictionary.\n  \"\"\"\n\n  result = {}\n  for f in fields:\n    if isinstance(initializers, dict):\n      if f in initializers:\n        # Variable-specific initializer.\n        result[f] = _convert_to_initializer(initializers[f])\n    else:\n      # Common initiliazer for all variables.\n      result[f] = _convert_to_initializer(initializers)\n\n  return result\n\n\ndef _get_layer_initializers(initializers, layer_name, fields):\n  \"\"\"Produces a nn initialization dictionary for a layer.\n\n  Calls `_get_initializers using initializers[layer_name]` if `layer_name` is a\n  valid key or using initializers otherwise (reuses initializers between\n  layers).\n\n  Args:\n    initializers: Initializer, <variable, initializer> dictionary,\n        <layer, initializer> dictionary.\n    layer_name: Layer name.\n    fields: Fields nn is expecting for module initialization.\n\n  Returns:\n    nn initialization dictionary.\n  \"\"\"\n\n  # No initializers specified.\n  if initializers is None:\n    return None\n\n  # Layer-specific initializer.\n  if isinstance(initializers, dict) and layer_name in initializers:\n    return _get_initializers(initializers[layer_name], fields)\n\n  return _get_initializers(initializers, fields)\n\n\nclass StandardDeepLSTM(Network):\n  \"\"\"LSTM layers with a Linear layer on top.\"\"\"\n\n  def __init__(self, output_size, layers, preprocess_name=\"identity\",\n               preprocess_options=None, scale=1.0, initializer=None,\n               name=\"deep_lstm\"):\n    \"\"\"Creates an instance of `StandardDeepLSTM`.\n\n    Args:\n      output_size: Output sizes of the final linear layer.\n      layers: Output sizes of LSTM layers.\n      preprocess_name: Gradient preprocessing class name (in `l2l.preprocess` or\n          tf modules). Default is `tf.identity`.\n      preprocess_options: Gradient preprocessing options.\n      scale: Gradient scaling (default is 1.0).\n      initializer: Variable initializer for linear layer. See `snt.Linear` and\n          `snt.LSTM` docs for more info. This parameter can be a string (e.g.\n          \"zeros\" will be converted to tf.zeros_initializer).\n      name: Module name.\n    \"\"\"\n    super(StandardDeepLSTM, self).__init__(name=name)\n\n    self._output_size = output_size\n    self._scale = scale\n\n    if hasattr(preprocess, preprocess_name):\n      preprocess_class = getattr(preprocess, preprocess_name)\n      self._preprocess = preprocess_class(**preprocess_options)\n    else:\n      self._preprocess = getattr(tf, preprocess_name)\n\n    with tf.variable_scope(self._template.variable_scope):\n      self._cores = []\n      for i, size in enumerate(layers, start=1):\n        name = \"lstm_{}\".format(i)\n        init = _get_layer_initializers(initializer, name,\n                                       (\"w_gates\", \"b_gates\"))\n        self._cores.append(snt.LSTM(size, name=name, initializers=init))\n      self._rnn = snt.DeepRNN(self._cores, skip_connections=False,\n                              name=\"deep_rnn\")\n\n      init = _get_layer_initializers(initializer, \"linear\", (\"w\", \"b\"))\n      self._linear = snt.Linear(output_size, name=\"linear\", initializers=init)\n\n  def _build(self, inputs, prev_state):\n    \"\"\"Connects the `StandardDeepLSTM` module into the graph.\n\n    Args:\n      inputs: 2D `Tensor` ([batch_size, input_size]).\n      prev_state: `DeepRNN` state.\n\n    Returns:\n      `Tensor` shaped as `inputs`.\n    \"\"\"\n    # Adds preprocessing dimension and preprocess.\n    inputs = self._preprocess(tf.expand_dims(inputs, -1))\n    # Incorporates preprocessing into data dimension.\n    inputs = tf.reshape(inputs, [inputs.get_shape().as_list()[0], -1])\n    output, next_state = self._rnn(inputs, prev_state)\n    return self._linear(output) * self._scale, next_state\n\n  def initial_state_for_inputs(self, inputs, **kwargs):\n    batch_size = inputs.get_shape().as_list()[0]\n    return self._rnn.initial_state(batch_size, **kwargs)\n\n\nclass CoordinateWiseDeepLSTM(StandardDeepLSTM):\n  \"\"\"Coordinate-wise `DeepLSTM`.\"\"\"\n\n  def __init__(self, name=\"cw_deep_lstm\", **kwargs):\n    \"\"\"Creates an instance of `CoordinateWiseDeepLSTM`.\n\n    Args:\n      name: Module name.\n      **kwargs: Additional `DeepLSTM` args.\n    \"\"\"\n    super(CoordinateWiseDeepLSTM, self).__init__(1, name=name, **kwargs)\n\n  def _reshape_inputs(self, inputs):\n    return tf.reshape(inputs, [-1, 1])\n\n  def _build(self, inputs, prev_state):\n    \"\"\"Connects the CoordinateWiseDeepLSTM module into the graph.\n\n    Args:\n      inputs: Arbitrarily shaped `Tensor`.\n      prev_state: `DeepRNN` state.\n\n    Returns:\n      `Tensor` shaped as `inputs`.\n    \"\"\"\n    input_shape = inputs.get_shape().as_list()\n    reshaped_inputs = self._reshape_inputs(inputs)\n\n    build_fn = super(CoordinateWiseDeepLSTM, self)._build\n    output, next_state = build_fn(reshaped_inputs, prev_state)\n\n    # Recover original shape.\n    return tf.reshape(output, input_shape), next_state\n\n  def initial_state_for_inputs(self, inputs, **kwargs):\n    reshaped_inputs = self._reshape_inputs(inputs)\n    return super(CoordinateWiseDeepLSTM, self).initial_state_for_inputs(\n        reshaped_inputs, **kwargs)\n\n\nclass KernelDeepLSTM(StandardDeepLSTM):\n  \"\"\"`DeepLSTM` for convolutional filters.\n\n  The inputs are assumed to be shaped as convolutional filters with an extra\n  preprocessing dimension ([kernel_w, kernel_h, n_input_channels,\n  n_output_channels]).\n  \"\"\"\n\n  def __init__(self, kernel_shape, name=\"kernel_deep_lstm\", **kwargs):\n    \"\"\"Creates an instance of `KernelDeepLSTM`.\n\n    Args:\n      kernel_shape: Kernel shape (2D `tuple`).\n      name: Module name.\n      **kwargs: Additional `DeepLSTM` args.\n    \"\"\"\n    self._kernel_shape = kernel_shape\n    output_size = np.prod(kernel_shape)\n    super(KernelDeepLSTM, self).__init__(output_size, name=name, **kwargs)\n\n  def _reshape_inputs(self, inputs):\n    transposed_inputs = tf.transpose(inputs, perm=[2, 3, 0, 1])\n    return tf.reshape(transposed_inputs, [-1] + self._kernel_shape)\n\n  def _build(self, inputs, prev_state):\n    \"\"\"Connects the KernelDeepLSTM module into the graph.\n\n    Args:\n      inputs: 4D `Tensor` (convolutional filter).\n      prev_state: `DeepRNN` state.\n\n    Returns:\n      `Tensor` shaped as `inputs`.\n    \"\"\"\n    input_shape = inputs.get_shape().as_list()\n    reshaped_inputs = self._reshape_inputs(inputs)\n\n    build_fn = super(KernelDeepLSTM, self)._build\n    output, next_state = build_fn(reshaped_inputs, prev_state)\n    transposed_output = tf.transpose(output, [1, 0])\n\n    # Recover original shape.\n    return tf.reshape(transposed_output, input_shape), next_state\n\n  def initial_state_for_inputs(self, inputs, **kwargs):\n    \"\"\"Batch size given inputs.\"\"\"\n    reshaped_inputs = self._reshape_inputs(inputs)\n    return super(KernelDeepLSTM, self).initial_state_for_inputs(\n        reshaped_inputs, **kwargs)\n\n\nclass Sgd(Network):\n  \"\"\"Identity network which acts like SGD.\"\"\"\n\n  def __init__(self, learning_rate=0.001, name=\"sgd\"):\n    \"\"\"Creates an instance of the Identity optimizer network.\n\n    Args:\n      learning_rate: constant learning rate to use.\n      name: Module name.\n    \"\"\"\n    super(Sgd, self).__init__(name=name)\n    self._learning_rate = learning_rate\n\n  def _build(self, inputs, _):\n    return -self._learning_rate * inputs, []\n\n  def initial_state_for_inputs(self, inputs, **kwargs):\n    return []\n\n\ndef _update_adam_estimate(estimate, value, b):\n  return (b * estimate) + ((1 - b) * value)\n\n\ndef _debias_adam_estimate(estimate, b, t):\n  return estimate / (1 - tf.pow(b, t))\n\n\nclass Adam(Network):\n  \"\"\"Adam algorithm (https://arxiv.org/pdf/1412.6980v8.pdf).\"\"\"\n\n  def __init__(self, learning_rate=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8,\n               name=\"adam\"):\n    \"\"\"Creates an instance of Adam.\"\"\"\n    super(Adam, self).__init__(name=name)\n    self._learning_rate = learning_rate\n    self._beta1 = beta1\n    self._beta2 = beta2\n    self._epsilon = epsilon\n\n  def _build(self, g, prev_state):\n    \"\"\"Connects the Adam module into the graph.\"\"\"\n    b1 = self._beta1\n    b2 = self._beta2\n\n    g_shape = g.get_shape().as_list()\n    g = tf.reshape(g, (-1, 1))\n\n    t, m, v = prev_state\n\n    t_next = t + 1\n\n    m_next = _update_adam_estimate(m, g, b1)\n    m_hat = _debias_adam_estimate(m_next, b1, t_next)\n\n    v_next = _update_adam_estimate(v, tf.square(g), b2)\n    v_hat = _debias_adam_estimate(v_next, b2, t_next)\n\n    update = -self._learning_rate * m_hat / (tf.sqrt(v_hat) + self._epsilon)\n    return tf.reshape(update, g_shape), (t_next, m_next, v_next)\n\n  def initial_state_for_inputs(self, inputs, dtype=tf.float32, **kwargs):\n    batch_size = int(np.prod(inputs.get_shape().as_list()))\n    t = tf.zeros((), dtype=dtype)\n    m = tf.zeros((batch_size, 1), dtype=dtype)\n    v = tf.zeros((batch_size, 1), dtype=dtype)\n    return (t, m, v)\n"
        },
        {
          "name": "networks_test.py",
          "type": "blob",
          "size": 6.5439453125,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for L2L networks.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nose_parameterized import parameterized\nimport numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\nimport networks\n\n\nclass CoordinateWiseDeepLSTMTest(tf.test.TestCase):\n  \"\"\"Tests CoordinateWiseDeepLSTM network.\"\"\"\n\n  def testShape(self):\n    shape = [10, 5]\n    gradients = tf.random_normal(shape)\n    net = networks.CoordinateWiseDeepLSTM(layers=(1, 1))\n    state = net.initial_state_for_inputs(gradients)\n    update, _ = net(gradients, state)\n    self.assertEqual(update.get_shape().as_list(), shape)\n\n  def testTrainable(self):\n    \"\"\"Tests the network contains trainable variables.\"\"\"\n    shape = [10, 5]\n    gradients = tf.random_normal(shape)\n    net = networks.CoordinateWiseDeepLSTM(layers=(1,))\n    state = net.initial_state_for_inputs(gradients)\n    net(gradients, state)\n    # Weights and biases for two layers.\n    variables = snt.get_variables_in_module(net)\n    self.assertEqual(len(variables), 4)\n\n  @parameterized.expand([\n      [\"zeros\"],\n      [{\"w\": \"zeros\", \"b\": \"zeros\", \"bad\": \"bad\"}],\n      [{\"w\": tf.zeros_initializer(), \"b\": np.array([0])}],\n      [{\"linear\": {\"w\": tf.zeros_initializer(), \"b\": \"zeros\"}}]\n  ])\n  def testResults(self, initializer):\n    \"\"\"Tests zero updates when last layer is initialized to zero.\"\"\"\n    shape = [10]\n    gradients = tf.random_normal(shape)\n    net = networks.CoordinateWiseDeepLSTM(layers=(1, 1),\n                                          initializer=initializer)\n    state = net.initial_state_for_inputs(gradients)\n    update, _ = net(gradients, state)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      update_np = sess.run(update)\n      self.assertAllEqual(update_np, np.zeros(shape))\n\n\nclass KernelDeepLSTMTest(tf.test.TestCase):\n  \"\"\"Tests KernelDeepLSTMTest network.\"\"\"\n\n  def testShape(self):\n    kernel_shape = [5, 5]\n    shape = kernel_shape + [2, 2]  # The input has to be 4-dimensional.\n    gradients = tf.random_normal(shape)\n    net = networks.KernelDeepLSTM(layers=(1, 1), kernel_shape=kernel_shape)\n    state = net.initial_state_for_inputs(gradients)\n    update, _ = net(gradients, state)\n    self.assertEqual(update.get_shape().as_list(), shape)\n\n  def testTrainable(self):\n    \"\"\"Tests the network contains trainable variables.\"\"\"\n    kernel_shape = [5, 5]\n    shape = kernel_shape + [2, 2]  # The input has to be 4-dimensional.\n    gradients = tf.random_normal(shape)\n    net = networks.KernelDeepLSTM(layers=(1,), kernel_shape=kernel_shape)\n    state = net.initial_state_for_inputs(gradients)\n    net(gradients, state)\n    # Weights and biases for two layers.\n    variables = snt.get_variables_in_module(net)\n    self.assertEqual(len(variables), 4)\n\n  @parameterized.expand([\n      [\"zeros\"],\n      [{\"w\": \"zeros\", \"b\": \"zeros\", \"bad\": \"bad\"}],\n      [{\"w\": tf.zeros_initializer(), \"b\": np.array([0])}],\n      [{\"linear\": {\"w\": tf.zeros_initializer(), \"b\": \"zeros\"}}]\n  ])\n  def testResults(self, initializer):\n    \"\"\"Tests zero updates when last layer is initialized to zero.\"\"\"\n    kernel_shape = [5, 5]\n    shape = kernel_shape + [2, 2]  # The input has to be 4-dimensional.\n    gradients = tf.random_normal(shape)\n    net = networks.KernelDeepLSTM(layers=(1, 1),\n                                  kernel_shape=kernel_shape,\n                                  initializer=initializer)\n    state = net.initial_state_for_inputs(gradients)\n    update, _ = net(gradients, state)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      update_np = sess.run(update)\n      self.assertAllEqual(update_np, np.zeros(shape))\n\n\nclass SgdTest(tf.test.TestCase):\n  \"\"\"Tests Sgd network.\"\"\"\n\n  def testShape(self):\n    shape = [10, 5]\n    gradients = tf.random_normal(shape)\n    net = networks.Sgd()\n    state = net.initial_state_for_inputs(gradients)\n    update, _ = net(gradients, state)\n    self.assertEqual(update.get_shape().as_list(), shape)\n\n  def testNonTrainable(self):\n    \"\"\"Tests the network doesn't contain trainable variables.\"\"\"\n    shape = [10, 5]\n    gradients = tf.random_normal(shape)\n    net = networks.Sgd()\n    state = net.initial_state_for_inputs(gradients)\n    net(gradients, state)\n    variables = snt.get_variables_in_module(net)\n    self.assertEqual(len(variables), 0)\n\n  def testResults(self):\n    \"\"\"Tests network produces zero updates with learning rate equal to zero.\"\"\"\n    shape = [10]\n    learning_rate = 0.01\n    gradients = tf.random_normal(shape)\n    net = networks.Sgd(learning_rate=learning_rate)\n    state = net.initial_state_for_inputs(gradients)\n    update, _ = net(gradients, state)\n\n    with self.test_session() as sess:\n      gradients_np, update_np = sess.run([gradients, update])\n      self.assertAllEqual(update_np, -learning_rate * gradients_np)\n\n\nclass AdamTest(tf.test.TestCase):\n  \"\"\"Tests Adam network.\"\"\"\n\n  def testShape(self):\n    shape = [10, 5]\n    gradients = tf.random_normal(shape)\n    net = networks.Adam()\n    state = net.initial_state_for_inputs(gradients)\n    update, _ = net(gradients, state)\n    self.assertEqual(update.get_shape().as_list(), shape)\n\n  def testNonTrainable(self):\n    \"\"\"Tests the network doesn't contain trainable variables.\"\"\"\n    shape = [10, 5]\n    gradients = tf.random_normal(shape)\n    net = networks.Adam()\n    state = net.initial_state_for_inputs(gradients)\n    net(gradients, state)\n    variables = snt.get_variables_in_module(net)\n    self.assertEqual(len(variables), 0)\n\n  def testZeroLearningRate(self):\n    \"\"\"Tests network produces zero updates with learning rate equal to zero.\"\"\"\n    shape = [10]\n    gradients = tf.random_normal(shape)\n    net = networks.Adam(learning_rate=0)\n    state = net.initial_state_for_inputs(gradients)\n    update, _ = net(gradients, state)\n\n    with self.test_session() as sess:\n      update_np = sess.run(update)\n      self.assertAllEqual(update_np, np.zeros(shape))\n\n\nif __name__ == \"__main__\":\n  tf.test.main()\n"
        },
        {
          "name": "preprocess.py",
          "type": "blob",
          "size": 2.27734375,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Learning 2 Learn preprocessing modules.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\n\nclass Clamp(snt.AbstractModule):\n\n  def __init__(self, min_value=None, max_value=None, name=\"clamp\"):\n    super(Clamp, self).__init__(name=name)\n    self._min = min_value\n    self._max = max_value\n\n  def _build(self, inputs):\n    output = inputs\n    if self._min is not None:\n      output = tf.maximum(output, self._min)\n    if self._max is not None:\n      output = tf.minimum(output, self._max)\n    return output\n\n\nclass LogAndSign(snt.AbstractModule):\n  \"\"\"Log and sign preprocessing.\n\n  As described in https://arxiv.org/pdf/1606.04474v1.pdf (Appendix A).\n  \"\"\"\n\n  def __init__(self, k, name=\"preprocess_log\"):\n    super(LogAndSign, self).__init__(name=name)\n    self._k = k\n\n  def _build(self, gradients):\n    \"\"\"Connects the LogAndSign module into the graph.\n\n    Args:\n      gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.\n\n    Returns:\n      `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements\n      along the nth dimension correspond to the log output and the remaining\n      `d_n` elements to the sign output.\n    \"\"\"\n    eps = np.finfo(gradients.dtype.as_numpy_dtype).eps\n    ndims = gradients.get_shape().ndims\n\n    log = tf.log(tf.abs(gradients) + eps)\n    clamped_log = Clamp(min_value=-1.0)(log / self._k)  # pylint: disable=not-callable\n    sign = Clamp(min_value=-1.0, max_value=1.0)(gradients * np.exp(self._k))  # pylint: disable=not-callable\n\n    return tf.concat([clamped_log, sign], ndims - 1)\n"
        },
        {
          "name": "preprocess_test.py",
          "type": "blob",
          "size": 2.94921875,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for L2L preprocessors.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nimport preprocess\n\n\nclass ClampTest(tf.test.TestCase):\n  \"\"\"Tests Clamp module.\"\"\"\n\n  def testShape(self):\n    shape = [2, 3]\n    inputs = tf.random_normal(shape)\n    clamp = preprocess.Clamp(min_value=-1.0, max_value=1.0)\n    output = clamp(inputs)\n    self.assertEqual(output.get_shape().as_list(), shape)\n\n  def testMin(self):\n    shape = [100]\n    inputs = tf.random_normal(shape)\n    clamp = preprocess.Clamp(min_value=0.0)\n    output = clamp(inputs)\n\n    with self.test_session() as sess:\n      output_np = sess.run(output)\n      self.assertTrue(np.all(np.greater_equal(output_np, np.zeros(shape))))\n\n  def testMax(self):\n    shape = [100]\n    inputs = tf.random_normal(shape)\n    clamp = preprocess.Clamp(max_value=0.0)\n    output = clamp(inputs)\n\n    with self.test_session() as sess:\n      output_np = sess.run(output)\n      self.assertTrue(np.all(np.less_equal(output_np, np.zeros(shape))))\n\n  def testMinAndMax(self):\n    shape = [100]\n    inputs = tf.random_normal(shape)\n    clamp = preprocess.Clamp(min_value=0.0, max_value=0.0)\n    output = clamp(inputs)\n\n    with self.test_session() as sess:\n      output_np = sess.run(output)\n      self.assertAllEqual(output_np, np.zeros(shape))\n\n\nclass LogAndSignTest(tf.test.TestCase):\n  \"\"\"Tests LogAndSign module.\"\"\"\n\n  def testShape(self):\n    shape = [2, 3]\n    inputs = tf.random_normal(shape)\n    module = preprocess.LogAndSign(k=1)\n    output = module(inputs)\n    self.assertEqual(output.get_shape().as_list(), shape[:-1] + [shape[-1] * 2])\n\n  def testLogWithOnes(self):\n    shape = [1]\n    inputs = tf.ones(shape)\n    module = preprocess.LogAndSign(k=10)\n    output = module(inputs)\n\n    with self.test_session() as sess:\n      output_np = sess.run(output)\n      log_np = output_np[0]\n      self.assertAlmostEqual(log_np, 0.0)\n\n  def testSign(self):\n    shape = [2, 1]\n    inputs = tf.random_normal(shape)\n    module = preprocess.LogAndSign(k=1)\n    output = module(inputs)\n\n    with self.test_session() as sess:\n      inputs_np, output_np = sess.run([inputs, output])\n      sign_np = output_np[:, 1:]\n      self.assertAllEqual(np.sign(sign_np), np.sign(inputs_np))\n\n\nif __name__ == \"__main__\":\n  tf.test.main()\n"
        },
        {
          "name": "problems.py",
          "type": "blob",
          "size": 8.7509765625,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Learning 2 Learn problems.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tarfile\nimport sys\n\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport sonnet as snt\nimport tensorflow as tf\n\nfrom tensorflow.contrib.learn.python.learn.datasets import mnist as mnist_dataset\n\n\n_nn_initializers = {\n    \"w\": tf.random_normal_initializer(mean=0, stddev=0.01),\n    \"b\": tf.random_normal_initializer(mean=0, stddev=0.01),\n}\n\n\ndef simple():\n  \"\"\"Simple problem: f(x) = x^2.\"\"\"\n\n  def build():\n    \"\"\"Builds loss graph.\"\"\"\n    x = tf.get_variable(\n        \"x\",\n        shape=[],\n        dtype=tf.float32,\n        initializer=tf.ones_initializer())\n    return tf.square(x, name=\"x_squared\")\n\n  return build\n\n\ndef simple_multi_optimizer(num_dims=2):\n  \"\"\"Multidimensional simple problem.\"\"\"\n\n  def get_coordinate(i):\n    return tf.get_variable(\"x_{}\".format(i),\n                           shape=[],\n                           dtype=tf.float32,\n                           initializer=tf.ones_initializer())\n\n  def build():\n    coordinates = [get_coordinate(i) for i in xrange(num_dims)]\n    x = tf.concat([tf.expand_dims(c, 0) for c in coordinates], 0)\n    return tf.reduce_sum(tf.square(x, name=\"x_squared\"))\n\n  return build\n\n\ndef quadratic(batch_size=128, num_dims=10, stddev=0.01, dtype=tf.float32):\n  \"\"\"Quadratic problem: f(x) = ||Wx - y||.\"\"\"\n\n  def build():\n    \"\"\"Builds loss graph.\"\"\"\n\n    # Trainable variable.\n    x = tf.get_variable(\n        \"x\",\n        shape=[batch_size, num_dims],\n        dtype=dtype,\n        initializer=tf.random_normal_initializer(stddev=stddev))\n\n    # Non-trainable variables.\n    w = tf.get_variable(\"w\",\n                        shape=[batch_size, num_dims, num_dims],\n                        dtype=dtype,\n                        initializer=tf.random_uniform_initializer(),\n                        trainable=False)\n    y = tf.get_variable(\"y\",\n                        shape=[batch_size, num_dims],\n                        dtype=dtype,\n                        initializer=tf.random_uniform_initializer(),\n                        trainable=False)\n\n    product = tf.squeeze(tf.matmul(w, tf.expand_dims(x, -1)))\n    return tf.reduce_mean(tf.reduce_sum((product - y) ** 2, 1))\n\n  return build\n\n\ndef ensemble(problems, weights=None):\n  \"\"\"Ensemble of problems.\n\n  Args:\n    problems: List of problems. Each problem is specified by a dict containing\n        the keys 'name' and 'options'.\n    weights: Optional list of weights for each problem.\n\n  Returns:\n    Sum of (weighted) losses.\n\n  Raises:\n    ValueError: If weights has an incorrect length.\n  \"\"\"\n  if weights and len(weights) != len(problems):\n    raise ValueError(\"len(weights) != len(problems)\")\n\n  build_fns = [getattr(sys.modules[__name__], p[\"name\"])(**p[\"options\"])\n               for p in problems]\n\n  def build():\n    loss = 0\n    for i, build_fn in enumerate(build_fns):\n      with tf.variable_scope(\"problem_{}\".format(i)):\n        loss_p = build_fn()\n        if weights:\n          loss_p *= weights[i]\n        loss += loss_p\n    return loss\n\n  return build\n\n\ndef _xent_loss(output, labels):\n  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output,\n                                                        labels=labels)\n  return tf.reduce_mean(loss)\n\n\ndef mnist(layers,  # pylint: disable=invalid-name\n          activation=\"sigmoid\",\n          batch_size=128,\n          mode=\"train\"):\n  \"\"\"Mnist classification with a multi-layer perceptron.\"\"\"\n\n  if activation == \"sigmoid\":\n    activation_op = tf.sigmoid\n  elif activation == \"relu\":\n    activation_op = tf.nn.relu\n  else:\n    raise ValueError(\"{} activation not supported\".format(activation))\n\n  # Data.\n  data = mnist_dataset.load_mnist()\n  data = getattr(data, mode)\n  images = tf.constant(data.images, dtype=tf.float32, name=\"MNIST_images\")\n  images = tf.reshape(images, [-1, 28, 28, 1])\n  labels = tf.constant(data.labels, dtype=tf.int64, name=\"MNIST_labels\")\n\n  # Network.\n  mlp = snt.nets.MLP(list(layers) + [10],\n                     activation=activation_op,\n                     initializers=_nn_initializers)\n  network = snt.Sequential([snt.BatchFlatten(), mlp])\n\n  def build():\n    indices = tf.random_uniform([batch_size], 0, data.num_examples, tf.int64)\n    batch_images = tf.gather(images, indices)\n    batch_labels = tf.gather(labels, indices)\n    output = network(batch_images)\n    return _xent_loss(output, batch_labels)\n\n  return build\n\n\nCIFAR10_URL = \"http://www.cs.toronto.edu/~kriz\"\nCIFAR10_FILE = \"cifar-10-binary.tar.gz\"\nCIFAR10_FOLDER = \"cifar-10-batches-bin\"\n\n\ndef _maybe_download_cifar10(path):\n  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n  if not os.path.exists(path):\n    os.makedirs(path)\n  filepath = os.path.join(path, CIFAR10_FILE)\n  if not os.path.exists(filepath):\n    print(\"Downloading CIFAR10 dataset to {}\".format(filepath))\n    url = os.path.join(CIFAR10_URL, CIFAR10_FILE)\n    filepath, _ = urllib.request.urlretrieve(url, filepath)\n    statinfo = os.stat(filepath)\n    print(\"Successfully downloaded {} bytes\".format(statinfo.st_size))\n    tarfile.open(filepath, \"r:gz\").extractall(path)\n\n\ndef cifar10(path,  # pylint: disable=invalid-name\n            conv_channels=None,\n            linear_layers=None,\n            batch_norm=True,\n            batch_size=128,\n            num_threads=4,\n            min_queue_examples=1000,\n            mode=\"train\"):\n  \"\"\"Cifar10 classification with a convolutional network.\"\"\"\n\n  # Data.\n  _maybe_download_cifar10(path)\n\n  # Read images and labels from disk.\n  if mode == \"train\":\n    filenames = [os.path.join(path,\n                              CIFAR10_FOLDER,\n                              \"data_batch_{}.bin\".format(i))\n                 for i in xrange(1, 6)]\n  elif mode == \"test\":\n    filenames = [os.path.join(path, \"test_batch.bin\")]\n  else:\n    raise ValueError(\"Mode {} not recognised\".format(mode))\n\n  depth = 3\n  height = 32\n  width = 32\n  label_bytes = 1\n  image_bytes = depth * height * width\n  record_bytes = label_bytes + image_bytes\n  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n  _, record = reader.read(tf.train.string_input_producer(filenames))\n  record_bytes = tf.decode_raw(record, tf.uint8)\n\n  label = tf.cast(tf.slice(record_bytes, [0], [label_bytes]), tf.int32)\n  raw_image = tf.slice(record_bytes, [label_bytes], [image_bytes])\n  image = tf.cast(tf.reshape(raw_image, [depth, height, width]), tf.float32)\n  # height x width x depth.\n  image = tf.transpose(image, [1, 2, 0])\n  image = tf.div(image, 255)\n\n  queue = tf.RandomShuffleQueue(capacity=min_queue_examples + 3 * batch_size,\n                                min_after_dequeue=min_queue_examples,\n                                dtypes=[tf.float32, tf.int32],\n                                shapes=[image.get_shape(), label.get_shape()])\n  enqueue_ops = [queue.enqueue([image, label]) for _ in xrange(num_threads)]\n  tf.train.add_queue_runner(tf.train.QueueRunner(queue, enqueue_ops))\n\n  # Network.\n  def _conv_activation(x):  # pylint: disable=invalid-name\n    return tf.nn.max_pool(tf.nn.relu(x),\n                          ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1],\n                          padding=\"SAME\")\n\n  conv = snt.nets.ConvNet2D(output_channels=conv_channels,\n                            kernel_shapes=[5],\n                            strides=[1],\n                            paddings=[snt.SAME],\n                            activation=_conv_activation,\n                            activate_final=True,\n                            initializers=_nn_initializers,\n                            use_batch_norm=batch_norm)\n\n  if batch_norm:\n    linear_activation = lambda x: tf.nn.relu(snt.BatchNorm()(x))\n  else:\n    linear_activation = tf.nn.relu\n\n  mlp = snt.nets.MLP(list(linear_layers) + [10],\n                     activation=linear_activation,\n                     initializers=_nn_initializers)\n  network = snt.Sequential([conv, snt.BatchFlatten(), mlp])\n\n  def build():\n    image_batch, label_batch = queue.dequeue_many(batch_size)\n    label_batch = tf.reshape(label_batch, [batch_size])\n\n    output = network(image_batch)\n    return _xent_loss(output, label_batch)\n\n  return build\n"
        },
        {
          "name": "problems_test.py",
          "type": "blob",
          "size": 4.4365234375,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for L2L problems.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nimport tensorflow as tf\n\nfrom nose_parameterized import parameterized\n\nimport problems\n\n\nclass SimpleTest(tf.test.TestCase):\n  \"\"\"Tests simple problem.\"\"\"\n\n  def testShape(self):\n    problem = problems.simple()\n    f = problem()\n    self.assertEqual(f.get_shape().as_list(), [])\n\n  def testVariables(self):\n    problem = problems.simple()\n    problem()\n    variables = tf.trainable_variables()\n    self.assertEqual(len(variables), 1)\n    self.assertEqual(variables[0].get_shape().as_list(), [])\n\n  @parameterized.expand([(-1,), (0,), (1,), (10,)])\n  def testValues(self, value):\n    problem = problems.simple()\n    f = problem()\n\n    with self.test_session() as sess:\n      output = sess.run(f, feed_dict={\"x:0\": value})\n      self.assertEqual(output, value**2)\n\n\nclass SimpleMultiOptimizerTest(tf.test.TestCase):\n  \"\"\"Tests multi-optimizer simple problem.\"\"\"\n\n  def testShape(self):\n    num_dims = 3\n    problem = problems.simple_multi_optimizer(num_dims=num_dims)\n    f = problem()\n    self.assertEqual(f.get_shape().as_list(), [])\n\n  def testVariables(self):\n    num_dims = 3\n    problem = problems.simple_multi_optimizer(num_dims=num_dims)\n    problem()\n    variables = tf.trainable_variables()\n    self.assertEqual(len(variables), num_dims)\n    for v in variables:\n      self.assertEqual(v.get_shape().as_list(), [])\n\n  @parameterized.expand([(-1,), (0,), (1,), (10,)])\n  def testValues(self, value):\n    problem = problems.simple_multi_optimizer(num_dims=1)\n    f = problem()\n\n    with self.test_session() as sess:\n      output = sess.run(f, feed_dict={\"x_0:0\": value})\n      self.assertEqual(output, value**2)\n\n\nclass QuadraticTest(tf.test.TestCase):\n  \"\"\"Tests Quadratic problem.\"\"\"\n\n  def testShape(self):\n    problem = problems.quadratic()\n    f = problem()\n    self.assertEqual(f.get_shape().as_list(), [])\n\n  def testVariables(self):\n    batch_size = 5\n    num_dims = 3\n    problem = problems.quadratic(batch_size=batch_size, num_dims=num_dims)\n    problem()\n    variables = tf.trainable_variables()\n    self.assertEqual(len(variables), 1)\n    self.assertEqual(variables[0].get_shape().as_list(), [batch_size, num_dims])\n\n  @parameterized.expand([(-1,), (0,), (1,), (10,)])\n  def testValues(self, value):\n    problem = problems.quadratic(batch_size=1, num_dims=1)\n    f = problem()\n\n    w = 2.0\n    y = 3.0\n\n    with self.test_session() as sess:\n      output = sess.run(f, feed_dict={\"x:0\": [[value]],\n                                      \"w:0\": [[[w]]],\n                                      \"y:0\": [[y]]})\n      self.assertEqual(output, ((w * value) - y)**2)\n\n\nclass EnsembleTest(tf.test.TestCase):\n  \"\"\"Tests Ensemble problem.\"\"\"\n\n  def testShape(self):\n    num_dims = 3\n    problem_defs = [{\"name\": \"simple\", \"options\": {}} for _ in xrange(num_dims)]\n    ensemble = problems.ensemble(problem_defs)\n    f = ensemble()\n    self.assertEqual(f.get_shape().as_list(), [])\n\n  def testVariables(self):\n    num_dims = 3\n    problem_defs = [{\"name\": \"simple\", \"options\": {}} for _ in xrange(num_dims)]\n    ensemble = problems.ensemble(problem_defs)\n    ensemble()\n    variables = tf.trainable_variables()\n    self.assertEqual(len(variables), num_dims)\n    for v in variables:\n      self.assertEqual(v.get_shape().as_list(), [])\n\n  @parameterized.expand([(-1,), (0,), (1,), (10,)])\n  def testValues(self, value):\n    num_dims = 1\n    weight = 0.5\n    problem_defs = [{\"name\": \"simple\", \"options\": {}} for _ in xrange(num_dims)]\n    ensemble = problems.ensemble(problem_defs, weights=[weight])\n    f = ensemble()\n\n    with self.test_session() as sess:\n      output = sess.run(f, feed_dict={\"problem_0/x:0\": value})\n      self.assertEqual(output, weight * value**2)\n\n\nif __name__ == \"__main__\":\n  tf.test.main()\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.8828125,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Learning 2 Learn training.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom six.moves import xrange\nimport tensorflow as tf\n\nfrom tensorflow.contrib.learn.python.learn import monitored_session as ms\n\nimport meta\nimport util\n\nflags = tf.flags\nlogging = tf.logging\n\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\"save_path\", None, \"Path for saved meta-optimizer.\")\nflags.DEFINE_integer(\"num_epochs\", 10000, \"Number of training epochs.\")\nflags.DEFINE_integer(\"log_period\", 100, \"Log period.\")\nflags.DEFINE_integer(\"evaluation_period\", 1000, \"Evaluation period.\")\nflags.DEFINE_integer(\"evaluation_epochs\", 20, \"Number of evaluation epochs.\")\n\nflags.DEFINE_string(\"problem\", \"simple\", \"Type of problem.\")\nflags.DEFINE_integer(\"num_steps\", 100,\n                     \"Number of optimization steps per epoch.\")\nflags.DEFINE_integer(\"unroll_length\", 20, \"Meta-optimizer unroll length.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate.\")\nflags.DEFINE_boolean(\"second_derivatives\", False, \"Use second derivatives.\")\n\n\ndef main(_):\n  # Configuration.\n  num_unrolls = FLAGS.num_steps // FLAGS.unroll_length\n\n  if FLAGS.save_path is not None:\n    if os.path.exists(FLAGS.save_path):\n      raise ValueError(\"Folder {} already exists\".format(FLAGS.save_path))\n    else:\n      os.mkdir(FLAGS.save_path)\n\n  # Problem.\n  problem, net_config, net_assignments = util.get_config(FLAGS.problem)\n\n  # Optimizer setup.\n  optimizer = meta.MetaOptimizer(**net_config)\n  minimize = optimizer.meta_minimize(\n      problem, FLAGS.unroll_length,\n      learning_rate=FLAGS.learning_rate,\n      net_assignments=net_assignments,\n      second_derivatives=FLAGS.second_derivatives)\n  step, update, reset, cost_op, _ = minimize\n\n  with ms.MonitoredSession() as sess:\n    # Prevent accidental changes to the graph.\n    tf.get_default_graph().finalize()\n\n    best_evaluation = float(\"inf\")\n    total_time = 0\n    total_cost = 0\n    for e in xrange(FLAGS.num_epochs):\n      # Training.\n      time, cost = util.run_epoch(sess, cost_op, [update, step], reset,\n                                  num_unrolls)\n      total_time += time\n      total_cost += cost\n\n      # Logging.\n      if (e + 1) % FLAGS.log_period == 0:\n        util.print_stats(\"Epoch {}\".format(e + 1), total_cost, total_time,\n                         FLAGS.log_period)\n        total_time = 0\n        total_cost = 0\n\n      # Evaluation.\n      if (e + 1) % FLAGS.evaluation_period == 0:\n        eval_cost = 0\n        eval_time = 0\n        for _ in xrange(FLAGS.evaluation_epochs):\n          time, cost = util.run_epoch(sess, cost_op, [update], reset,\n                                      num_unrolls)\n          eval_time += time\n          eval_cost += cost\n\n        util.print_stats(\"EVALUATION\", eval_cost, eval_time,\n                         FLAGS.evaluation_epochs)\n\n        if FLAGS.save_path is not None and eval_cost < best_evaluation:\n          print(\"Removing previously saved meta-optimizer\")\n          for f in os.listdir(FLAGS.save_path):\n            os.remove(os.path.join(FLAGS.save_path, f))\n          print(\"Saving meta-optimizer to {}\".format(FLAGS.save_path))\n          optimizer.save(sess, FLAGS.save_path)\n          best_evaluation = eval_cost\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n"
        },
        {
          "name": "util.py",
          "type": "blob",
          "size": 4.373046875,
          "content": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Learning 2 Learn utils.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom timeit import default_timer as timer\n\nimport numpy as np\nfrom six.moves import xrange\n\nimport problems\n\n\ndef run_epoch(sess, cost_op, ops, reset, num_unrolls):\n  \"\"\"Runs one optimization epoch.\"\"\"\n  start = timer()\n  sess.run(reset)\n  for _ in xrange(num_unrolls):\n    cost = sess.run([cost_op] + ops)[0]\n  return timer() - start, cost\n\n\ndef print_stats(header, total_error, total_time, n):\n  \"\"\"Prints experiment statistics.\"\"\"\n  print(header)\n  print(\"Log Mean Final Error: {:.2f}\".format(np.log10(total_error / n)))\n  print(\"Mean epoch time: {:.2f} s\".format(total_time / n))\n\n\ndef get_net_path(name, path):\n  return None if path is None else os.path.join(path, name + \".l2l\")\n\n\ndef get_default_net_config(name, path):\n  return {\n      \"net\": \"CoordinateWiseDeepLSTM\",\n      \"net_options\": {\n          \"layers\": (20, 20),\n          \"preprocess_name\": \"LogAndSign\",\n          \"preprocess_options\": {\"k\": 5},\n          \"scale\": 0.01,\n      },\n      \"net_path\": get_net_path(name, path)\n  }\n\n\ndef get_config(problem_name, path=None):\n  \"\"\"Returns problem configuration.\"\"\"\n  if problem_name == \"simple\":\n    problem = problems.simple()\n    net_config = {\"cw\": {\n        \"net\": \"CoordinateWiseDeepLSTM\",\n        \"net_options\": {\"layers\": (), \"initializer\": \"zeros\"},\n        \"net_path\": get_net_path(\"cw\", path)\n    }}\n    net_assignments = None\n  elif problem_name == \"simple-multi\":\n    problem = problems.simple_multi_optimizer()\n    net_config = {\n        \"cw\": {\n            \"net\": \"CoordinateWiseDeepLSTM\",\n            \"net_options\": {\"layers\": (), \"initializer\": \"zeros\"},\n            \"net_path\": get_net_path(\"cw\", path)\n        },\n        \"adam\": {\n            \"net\": \"Adam\",\n            \"net_options\": {\"learning_rate\": 0.1}\n        }\n    }\n    net_assignments = [(\"cw\", [\"x_0\"]), (\"adam\", [\"x_1\"])]\n  elif problem_name == \"quadratic\":\n    problem = problems.quadratic(batch_size=128, num_dims=10)\n    net_config = {\"cw\": {\n        \"net\": \"CoordinateWiseDeepLSTM\",\n        \"net_options\": {\"layers\": (20, 20)},\n        \"net_path\": get_net_path(\"cw\", path)\n    }}\n    net_assignments = None\n  elif problem_name == \"mnist\":\n    mode = \"train\" if path is None else \"test\"\n    problem = problems.mnist(layers=(20,), mode=mode)\n    net_config = {\"cw\": get_default_net_config(\"cw\", path)}\n    net_assignments = None\n  elif problem_name == \"cifar\":\n    mode = \"train\" if path is None else \"test\"\n    problem = problems.cifar10(\"cifar10\",\n                               conv_channels=(16, 16, 16),\n                               linear_layers=(32,),\n                               mode=mode)\n    net_config = {\"cw\": get_default_net_config(\"cw\", path)}\n    net_assignments = None\n  elif problem_name == \"cifar-multi\":\n    mode = \"train\" if path is None else \"test\"\n    problem = problems.cifar10(\"cifar10\",\n                               conv_channels=(16, 16, 16),\n                               linear_layers=(32,),\n                               mode=mode)\n    net_config = {\n        \"conv\": get_default_net_config(\"conv\", path),\n        \"fc\": get_default_net_config(\"fc\", path)\n    }\n    conv_vars = [\"conv_net_2d/conv_2d_{}/w\".format(i) for i in xrange(3)]\n    fc_vars = [\"conv_net_2d/conv_2d_{}/b\".format(i) for i in xrange(3)]\n    fc_vars += [\"conv_net_2d/batch_norm_{}/beta\".format(i) for i in xrange(3)]\n    fc_vars += [\"mlp/linear_{}/w\".format(i) for i in xrange(2)]\n    fc_vars += [\"mlp/linear_{}/b\".format(i) for i in xrange(2)]\n    fc_vars += [\"mlp/batch_norm/beta\"]\n    net_assignments = [(\"conv\", conv_vars), (\"fc\", fc_vars)]\n  else:\n    raise ValueError(\"{} is not a valid problem\".format(problem_name))\n\n  return problem, net_config, net_assignments\n"
        }
      ]
    }
  ]
}