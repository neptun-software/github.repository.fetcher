{
  "metadata": {
    "timestamp": 1736559768696,
    "page": 483,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "googlefonts/noto-emoji",
      "stars": 3914,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.052734375,
          "content": "__pycache__/\n*.pyc\nwaveflag\nbuild/\nvenv/\nemojicompat/\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.35546875,
          "content": "# This is the official list of Noto authors for copyright\n# purposes.\n# This file is distinct from the CONTRIBUTORS files.\n# See the latter for an explanation.\n\n# Names should be added to this file as:\n# Name or Organization <email address>\n# The email address is not required for organizations.\n\nGoogle Inc.\nArjen Nienhuis <a.g.nienhuis@gmail.com>  # SVG cleanup\n"
        },
        {
          "name": "BUILD.md",
          "type": "blob",
          "size": 1.86328125,
          "content": "# Build instructions\n\nTypically build the CBDT then the COLRv1 as COLRv1 copies some information from CBDT.\n\n## Is this a Unicode rev?\n\n* Update https://github.com/notofonts/nototools, publish the new version\n   * Must be done by a Googler. See internal instructions.\n* Update emojicompat\n   * https://github.com/googlefonts/emojicompat?tab=readme-ov-file#support-new-unicode-sequences\n* Update artwork\n   * Must be done by a Googler. Work with the emoji design team using internal instructions.\n\n## Update version\n\nEdit `NotoColorEmoji.tmpl.ttx.tmpl`\n*   In `<head>` find `fontRevision`.\n    *   It should be of the form 2.xxx\n    *   Increment xxx by 1\n*   In `<name>` find `<namerecord nameID=\"5\" platformID=\"3\" platEncID=\"1\"\n    langID=\"0x409\">`\n    *   It should look like `Version\n        2.017;GOOG;noto-emoji:20180810:f1da3bc656f9`\n    *   Update Version to match `<head>` (`Version 2.017` in the example)\n    *   Update the date (`20180810` in the example)\n    *   Update the commit\n\n## Update new flags\n\n* Add new flags to list in Makefile ([example](https://github.com/googlefonts/noto-emoji-next/commit/21bdd6107fac60979737ac95c2655cb02824d144))\n* Update `third_party/region-flags`. For example, for CQ (Sark) update:\n   * `third_party/region-flags/png/CQ.png`\n      * This file can be highres, it will be resized by the CBDT build process\n      * This file should have the proportions of the flag\n   * `third_party/region-flags/svg/CQ.svg`\n      * This file is *not* required to have the `0 0 128 128` viewbox files in `/svg` have to have\n   * `third_party/region-flags/waved-svg/emoji_u1f1e8_1f1f6.svg`\n      * This file is produced using https://github.com/rsheeter/warp\n      * New flags are added to `wave_list.txt`\n         * To wave only the new flag delete other entries locally\n\n## Rebuild the fonts\n\n```bash\n# Build CBDT, COLR, flags-only, and emojicompat fonts\n$ ./full_rebuild.sh\n```\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.4169921875,
          "content": "Want to contribute? Great! First, read this page (including the small print\nat the end).\n\n### Before you contribute\nBefore we can use your code, you must sign the\n[Google Individual Contributor License\nAgreement](https://cla.developers.google.com/about/google-individual) \n(CLA), which you can do online. The CLA is necessary mainly because you own the\ncopyright to your changes, even after your contribution becomes part of our\ncodebase, so we need your permission to use and distribute your code. We also\nneed to be sure of various other thingsâ€”for instance that you'll tell us if you\nknow that your code infringes on other people's patents. You don't have to sign\nthe CLA until after you've submitted your code for review and a member has\napproved it, but you must do it before we can put your code into our codebase.\nBefore you start working on a larger contribution, you should get in touch with\nus first through the issue tracker with your idea so that we can help out and\npossibly guide you. Coordinating up front makes it much easier to avoid\nfrustration later on.\n\n### Code reviews\nAll submissions, including submissions by project members, require review.\nWe use Github pull requests for this purpose.\n\n### The small print\nContributions made by corporations are covered by a different agreement than\nthe one above, the [Software Grant and Corporate Contributor License\nAgreement](https://cla.developers.google.com/about/google-corporate).\n"
        },
        {
          "name": "CONTRIBUTORS",
          "type": "blob",
          "size": 0.609375,
          "content": "# People who have agreed to one of the CLAs and can contribute patches.\n# The AUTHORS file lists the copyright holders; this file\n# lists people.  For example, Google employees are listed here\n# but not in AUTHORS, because Google holds the copyright.\n#\n# https://developers.google.com/open-source/cla/individual\n# https://developers.google.com/open-source/cla/corporate\n#\n# Names should be added to this file as:\n#     Name <email address>\n\nArjen Nienhuis <a.g.nienhuis@gmail.com>\nBehdad Esfahbod <behdad@google.com>\nDoug Felt <dougfelt@google.com>\nRoozbeh Pournader <roozbeh@google.com>\nVictoria Lease <violets@google.com>\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 4.228515625,
          "content": "Copyright 2013 Google LLC\n\nThis Font Software is licensed under the SIL Open Font License, Version 1.1.\nThis license is copied below, and is also available with a FAQ at:\nhttps://scripts.sil.org/OFL\n\n\n-----------------------------------------------------------\nSIL OPEN FONT LICENSE Version 1.1 - 26 February 2007\n-----------------------------------------------------------\n\nPREAMBLE\nThe goals of the Open Font License (OFL) are to stimulate worldwide\ndevelopment of collaborative font projects, to support the font creation\nefforts of academic and linguistic communities, and to provide a free and\nopen framework in which fonts may be shared and improved in partnership\nwith others.\n\nThe OFL allows the licensed fonts to be used, studied, modified and\nredistributed freely as long as they are not sold by themselves. The\nfonts, including any derivative works, can be bundled, embedded, \nredistributed and/or sold with any software provided that any reserved\nnames are not used by derivative works. The fonts and derivatives,\nhowever, cannot be released under any other type of license. The\nrequirement for fonts to remain under this license does not apply\nto any document created using the fonts or their derivatives.\n\nDEFINITIONS\n\"Font Software\" refers to the set of files released by the Copyright\nHolder(s) under this license and clearly marked as such. This may\ninclude source files, build scripts and documentation.\n\n\"Reserved Font Name\" refers to any names specified as such after the\ncopyright statement(s).\n\n\"Original Version\" refers to the collection of Font Software components as\ndistributed by the Copyright Holder(s).\n\n\"Modified Version\" refers to any derivative made by adding to, deleting,\nor substituting -- in part or in whole -- any of the components of the\nOriginal Version, by changing formats or by porting the Font Software to a\nnew environment.\n\n\"Author\" refers to any designer, engineer, programmer, technical\nwriter or other person who contributed to the Font Software.\n\nPERMISSION & CONDITIONS\nPermission is hereby granted, free of charge, to any person obtaining\na copy of the Font Software, to use, study, copy, merge, embed, modify,\nredistribute, and sell modified and unmodified copies of the Font\nSoftware, subject to the following conditions:\n\n1) Neither the Font Software nor any of its individual components,\nin Original or Modified Versions, may be sold by itself.\n\n2) Original or Modified Versions of the Font Software may be bundled,\nredistributed and/or sold with any software, provided that each copy\ncontains the above copyright notice and this license. These can be\nincluded either as stand-alone text files, human-readable headers or\nin the appropriate machine-readable metadata fields within text or\nbinary files as long as those fields can be easily viewed by the user.\n\n3) No Modified Version of the Font Software may use the Reserved Font\nName(s) unless explicit written permission is granted by the corresponding\nCopyright Holder. This restriction only applies to the primary font name as\npresented to the users.\n\n4) The name(s) of the Copyright Holder(s) or the Author(s) of the Font\nSoftware shall not be used to promote, endorse or advertise any\nModified Version, except to acknowledge the contribution(s) of the\nCopyright Holder(s) and the Author(s) or with their explicit written\npermission.\n\n5) The Font Software, modified or unmodified, in part or in whole,\nmust be distributed entirely under this license, and must not be\ndistributed under any other license. The requirement for fonts to\nremain under this license does not apply to any document created\nusing the Font Software.\n\nTERMINATION\nThis license becomes null and void if any of the above conditions are\nnot met.\n\nDISCLAIMER\nTHE FONT SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT\nOF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nINCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL\nDAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM\nOTHER DEALINGS IN THE FONT SOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 8.3583984375,
          "content": "# Copyright 2014 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nEMOJI = NotoColorEmoji\nEMOJI_WINDOWS = NotoColorEmoji_WindowsCompatible\nall: $(EMOJI).ttf $(EMOJI_WINDOWS).ttf\n\nCFLAGS = -std=c99 -Wall -Wextra `pkg-config --cflags --libs cairo`\nLDFLAGS = -lm `pkg-config --libs cairo`\n\nPNGQUANT = pngquant\nPYTHON = python3\nPNGQUANTFLAGS = --speed 1 --skip-if-larger --quality 85-95 --force\nBODY_DIMENSIONS = 136x128\nIMOPS := -size $(BODY_DIMENSIONS) canvas:none -compose copy -gravity center\n\nZOPFLIPNG = zopflipng\nTTX = ttx\n\nEMOJI_BUILDER = third_party/color_emoji/emoji_builder.py\n# flag for emoji builder.  Default to legacy small metrics for the time being.\nSMALL_METRICS := -S\nADD_GLYPHS = add_glyphs.py\nADD_GLYPHS_FLAGS = -a emoji_aliases.txt\nPUA_ADDER = map_pua_emoji.py\nVS_ADDER = add_vs_cmap.py # from nototools\n\nEMOJI_SRC_DIR ?= png/128\nFLAGS_SRC_DIR := third_party/region-flags/png\n\nSEQUENCE_CHECK_PY = check_emoji_sequences.py\n\nBUILD_DIR := build\nEMOJI_DIR := $(BUILD_DIR)/emoji\nFLAGS_DIR := $(BUILD_DIR)/flags\nRESIZED_FLAGS_DIR := $(BUILD_DIR)/resized_flags\nRENAMED_FLAGS_DIR := $(BUILD_DIR)/renamed_flags\nQUANTIZED_DIR := $(BUILD_DIR)/quantized_pngs\nCOMPRESSED_DIR := $(BUILD_DIR)/compressed_pngs\n\n# Unknown flag is PUA fe82b\n# Note, we omit some flags below that we support via aliasing instead.\n\nLIMITED_FLAGS = CN DE ES FR GB IT JP KR RU US\nSELECTED_FLAGS = AC AD AE AF AG AI AL AM AO AQ AR AS AT AU AW AX AZ \\\n\tBA BB BD BE BF BG BH BI BJ BL BM BN BO BQ BR BS BT BW BY BZ \\\n\tCA CC CD CF CG CH CI CK CL CM CN CO CQ CR CU CV CW CX CY CZ \\\n\tDE DJ DK DM DO DZ \\\n\tEC EE EG EH ER ES ET EU \\\n\tFI FJ FK FM FO FR \\\n\tGA GB GD GE GF GG GH GI GL GM GN GP GQ GR GS GT GU GW GY \\\n\tHK HN HR HT HU \\\n\tIC ID IE IL IM IN IO IQ IR IS IT \\\n\tJE JM JO JP \\\n\tKE KG KH KI KM KN KP KR KW KY KZ \\\n\tLA LB LC LI LK LR LS LT LU LV LY \\\n\tMA MC MD ME MG MH MK ML MM MN MO MP MQ MR MS MT MU MV MW MX MY MZ \\\n\tNA NC NE NF NG NI NL NO NP NR NU NZ \\\n\tOM \\\n\tPA PE PF PG PH PK PL PM PN PR PS PT PW PY \\\n\tQA \\\n\tRE RO RS RU RW \\\n\tSA SB SC SD SE SG SH SI SK SL SM SN SO SR SS ST SV SX SY SZ \\\n\tTA TC TD TF TG TH TJ TK TL TM TN TO TR TT TV TW TZ \\\n\tUA UG UN US UY UZ \\\n\tVA VC VE VG VI VN VU \\\n\tWF WS \\\n\tXK \\\n\tYE YT \\\n\tZA ZM ZW \\\n        GB-ENG GB-SCT GB-WLS\n\nifeq (,$(shell which $(ZOPFLIPNG)))\n  ifeq (,$(wildcard $(ZOPFLIPNG)))\n    MISSING_ZOPFLI = fail\n  endif\nendif\n\nifndef VIRTUAL_ENV\n  MISSING_VENV = fail\nendif\n\nifeq (, $(shell which $(VS_ADDER)))\n  MISSING_PY_TOOLS = fail\nendif\nifeq (, $(shell which $(TTX)))\n  MISSING_PY_TOOLS = fail\nendif\n\nALL_FLAGS = $(basename $(notdir $(wildcard $(FLAGS_SRC_DIR)/*.png)))\n\nFLAGS = $(SELECTED_FLAGS)\n\nFLAG_NAMES = $(FLAGS:%=%.png)\nFLAG_FILES = $(addprefix $(FLAGS_DIR)/, $(FLAG_NAMES))\nRESIZED_FLAG_FILES = $(addprefix $(RESIZED_FLAGS_DIR)/, $(FLAG_NAMES))\n\nifndef MISSING_PY_TOOLS\nFLAG_GLYPH_NAMES = $(shell $(PYTHON) flag_glyph_name.py $(FLAGS))\nelse\nFLAG_GLYPH_NAMES =\nendif\nRENAMED_FLAG_NAMES = $(FLAG_GLYPH_NAMES:%=emoji_%.png)\nRENAMED_FLAG_FILES = $(addprefix $(RENAMED_FLAGS_DIR)/, $(RENAMED_FLAG_NAMES))\n\nEMOJI_NAMES = $(notdir $(wildcard $(EMOJI_SRC_DIR)/emoji_u*.png))\nEMOJI_FILES= $(addprefix $(EMOJI_DIR)/,$(EMOJI_NAMES)))\n\nALL_NAMES = $(EMOJI_NAMES) $(RENAMED_FLAG_NAMES)\n\nALL_QUANTIZED_FILES = $(addprefix $(QUANTIZED_DIR)/, $(ALL_NAMES))\nALL_COMPRESSED_FILES = $(addprefix $(COMPRESSED_DIR)/, $(ALL_NAMES))\n\n\nemoji: $(EMOJI_FILES)\n\nflags: $(FLAG_FILES)\n\nresized_flags: $(RESIZED_FLAG_FILES)\n\nrenamed_flags: $(RENAMED_FLAG_FILES)\n\nquantized: $(ALL_QUANTIZED_FILES)\n\ncompressed: $(ALL_COMPRESSED_FILES)\n\ncheck_tools:\nifdef MISSING_ZOPFLI\n\t$(error \"Missing $(ZOPFLIPNG). Try 'brew install zopfli' (Mac) or 'sudo apt-get install zopfli' (linux)\")\nendif\nifdef MISSING_VENV\n\t\t$(error \"Please start your virtual environment, and run: \"'pip install -r requirements.txt'\")\nendif\nifdef MISSING_PY_TOOLS\n\t\t$(error \"Missing tools; run: \"'pip install -r requirements.txt' in your virtual environment\")\nendif\n\n$(EMOJI_DIR) $(FLAGS_DIR) $(RESIZED_FLAGS_DIR) $(RENAMED_FLAGS_DIR) $(QUANTIZED_DIR) $(COMPRESSED_DIR):\n\tmkdir -p \"$@\"\n\n\nwaveflag: waveflag.c\n\t$(CC) $< -o $@ $(CFLAGS) $(LDFLAGS)\n\n\n# imagemagick's -extent operator munges the grayscale images in such a fashion\n# that while it can display them correctly using libpng12, chrome and gimp using\n# both libpng12 and libpng16 display the wrong gray levels.\n#\n# @convert \"$<\" -gravity center -background none -extent 136x128 \"$@\"\n#\n# We can get around the conversion to a gray colorspace in the version of\n# imagemagick packaged with ubuntu trusty (6.7.7-10) by using -composite.\n\n$(EMOJI_DIR)/%.png: $(EMOJI_SRC_DIR)/%.png | $(EMOJI_DIR)\n\t@convert $(IMOPS) \"$<\" -composite \"PNG32:$@\"\n\n$(FLAGS_DIR)/%.png: $(FLAGS_SRC_DIR)/%.png ./waveflag | $(FLAGS_DIR)\n\t@./waveflag $(FLAGS_DIR)/ \"$<\"\n\n$(RESIZED_FLAGS_DIR)/%.png: $(FLAGS_DIR)/%.png | $(RESIZED_FLAGS_DIR)\n\t@convert $(IMOPS) \"$<\" -composite \"PNG32:$@\"\n\nflag-symlinks: $(RESIZED_FLAG_FILES) | $(RENAMED_FLAGS_DIR)\n\t@$(subst ^, ,                                  \\\n\t  $(join                                       \\\n\t    $(FLAGS:%=ln^-fs^../resized_flags/%.png^), \\\n\t    $(RENAMED_FLAG_FILES:%=%; )                \\\n\t   )                                           \\\n\t )\n\n$(RENAMED_FLAG_FILES): | flag-symlinks\n\n$(QUANTIZED_DIR)/%.png: $(RENAMED_FLAGS_DIR)/%.png | $(QUANTIZED_DIR)\n\t@($(PNGQUANT) $(PNGQUANTFLAGS) -o \"$@\" \"$<\"; case \"$$?\" in \"98\"|\"99\") echo \"reuse $<\"; cp $< $@;; *) exit \"$$?\";; esac)\n\n$(QUANTIZED_DIR)/%.png: $(EMOJI_DIR)/%.png | $(QUANTIZED_DIR)\n\t@($(PNGQUANT) $(PNGQUANTFLAGS) -o \"$@\" \"$<\"; case \"$$?\" in \"98\"|\"99\") echo \"reuse $<\";cp $< $@;; *) exit \"$$?\";; esac)\n\n$(COMPRESSED_DIR)/%.png: $(QUANTIZED_DIR)/%.png | check_tools $(COMPRESSED_DIR)\n\t@$(ZOPFLIPNG) -y \"$<\" \"$@\" 1> /dev/null 2>&1\n\n# Make 3.81 can endless loop here if the target is missing but no\n# prerequisite is updated and make has been invoked with -j, e.g.:\n# File `font' does not exist.\n#      File `NotoColorEmoji.tmpl.ttx' does not exist.\n# File `font' does not exist.\n#      File `NotoColorEmoji.tmpl.ttx' does not exist.\n# ...\n# Run make without -j if this happens.\n\n$(EMOJI).tmpl.ttx: $(EMOJI).tmpl.ttx.tmpl $(ADD_GLYPHS) $(ALL_COMPRESSED_FILES)\n\t$(PYTHON) $(ADD_GLYPHS) -f \"$<\" -o \"$@\" -d \"$(COMPRESSED_DIR)\" $(ADD_GLYPHS_FLAGS)\n\n$(EMOJI_WINDOWS).tmpl.ttx: $(EMOJI).tmpl.ttx.tmpl $(ADD_GLYPHS) $(ALL_COMPRESSED_FILES)\n\t$(PYTHON) $(ADD_GLYPHS) --add_cmap4 --add_glyf -f \"$<\" -o \"$@\" -d \"$(COMPRESSED_DIR)\" $(ADD_GLYPHS_FLAGS)\n\n%.ttf: %.ttx\n\t@rm -f \"$@\"\n\tttx \"$<\"\n\n$(EMOJI).ttf: check_sequence $(EMOJI).tmpl.ttf $(EMOJI_BUILDER) $(PUA_ADDER) \\\n\t$(ALL_COMPRESSED_FILES) | check_tools\n\n\t@$(PYTHON) $(EMOJI_BUILDER) $(SMALL_METRICS) -V $(word 2,$^) \"$@\" \"$(COMPRESSED_DIR)/emoji_u\"\n\t@$(PYTHON) $(PUA_ADDER) \"$@\" \"$@-with-pua\"\n\t@$(VS_ADDER) -vs 2640 2642 2695 --dstdir '.' -o \"$@-with-pua-varsel\" \"$@-with-pua\"\n\t@mv \"$@-with-pua-varsel\" \"$@\"\n\t@rm \"$@-with-pua\"\n\n$(EMOJI_WINDOWS).ttf: check_sequence $(EMOJI_WINDOWS).tmpl.ttf $(EMOJI_BUILDER) $(PUA_ADDER) \\\n\t$(ALL_COMPRESSED_FILES) | check_tools\n\n\t@$(PYTHON) $(EMOJI_BUILDER) -O $(SMALL_METRICS) -V $(word 2,$^) \"$@\" \"$(COMPRESSED_DIR)/emoji_u\"\n\t@$(PYTHON) $(PUA_ADDER) \"$@\" \"$@-with-pua\"\n\t@$(VS_ADDER) -vs 2640 2642 2695 --dstdir '.' -o \"$@-with-pua-varsel\" \"$@-with-pua\"\n\t@mv \"$@-with-pua-varsel\" \"$@\"\n\t@rm \"$@-with-pua\"\n\n\ncheck_sequence:\nifdef BYPASS_SEQUENCE_CHECK\n\t@echo Bypassing the emoji sequence checks\nelse\n\t@$(PYTHON) $(SEQUENCE_CHECK_PY) -n $(ALL_NAMES) -c\nendif\n\nclean:\n\trm -f $(EMOJI).ttf $(EMOJI_WINDOWS).ttf $(EMOJI).tmpl.ttf $(EMOJI_WINDOWS).tmpl.ttf $(EMOJI).tmpl.ttx $(EMOJI_WINDOWS).tmpl.ttx\n\trm -f waveflag\n\trm -rf $(BUILD_DIR)\n\n# This prints the value of a Makefile variable: e.g. `make print-SELECTED_FLAGS`\n# will print the content of SELECTED_FLAGS.\n# Source: https://apprize.best/linux/gnu/3.html\nprint-%: ; @echo $* = $($*)\n\n.SECONDARY: $(EMOJI_FILES) $(FLAG_FILES) $(RESIZED_FLAG_FILES) $(RENAMED_FLAG_FILES) \\\n  $(ALL_QUANTIZED_FILES) $(ALL_COMPRESSED_FILES)\n\n.PHONY:\tclean flags emoji renamed_flags quantized compressed check_tools\n\n"
        },
        {
          "name": "NotoColorEmoji.tmpl.ttx.tmpl",
          "type": "blob",
          "size": 23.2861328125,
          "content": "<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?>\n<ttFont sfntVersion=\"\\x00\\x01\\x00\\x00\" ttLibVersion=\"3.6\">\n\n  <GlyphOrder>\n    <!-- The 'id' attribute is only for humans; it is ignored when parsed. -->\n    <GlyphID id=\"0\" name=\".notdef\"/>\n    <GlyphID id=\"1\" name=\"null\"/>\n    <GlyphID id=\"2\" name=\"nonmarkingreturn\"/>\n    <GlyphID id=\"3\" name=\"space\"/>\n    <GlyphID id=\"4\" name=\"uni200D\"/>\n    <GlyphID id=\"5\" name=\"uE0030\"/>\n    <GlyphID id=\"6\" name=\"uE0031\"/>\n    <GlyphID id=\"7\" name=\"uE0032\"/>\n    <GlyphID id=\"8\" name=\"uE0033\"/>\n    <GlyphID id=\"9\" name=\"uE0034\"/>\n    <GlyphID id=\"10\" name=\"uE0035\"/>\n    <GlyphID id=\"11\" name=\"uE0036\"/>\n    <GlyphID id=\"12\" name=\"uE0037\"/>\n    <GlyphID id=\"13\" name=\"uE0038\"/>\n    <GlyphID id=\"14\" name=\"uE0039\"/>\n    <GlyphID id=\"15\" name=\"uE0061\"/>\n    <GlyphID id=\"16\" name=\"uE0062\"/>\n    <GlyphID id=\"17\" name=\"uE0063\"/>\n    <GlyphID id=\"18\" name=\"uE0064\"/>\n    <GlyphID id=\"19\" name=\"uE0065\"/>\n    <GlyphID id=\"20\" name=\"uE0066\"/>\n    <GlyphID id=\"21\" name=\"uE0067\"/>\n    <GlyphID id=\"22\" name=\"uE0068\"/>\n    <GlyphID id=\"23\" name=\"uE0069\"/>\n    <GlyphID id=\"24\" name=\"uE006A\"/>\n    <GlyphID id=\"25\" name=\"uE006B\"/>\n    <GlyphID id=\"26\" name=\"uE006C\"/>\n    <GlyphID id=\"27\" name=\"uE006D\"/>\n    <GlyphID id=\"28\" name=\"uE006E\"/>\n    <GlyphID id=\"29\" name=\"uE006F\"/>\n    <GlyphID id=\"30\" name=\"uE0070\"/>\n    <GlyphID id=\"31\" name=\"uE0071\"/>\n    <GlyphID id=\"32\" name=\"uE0072\"/>\n    <GlyphID id=\"33\" name=\"uE0073\"/>\n    <GlyphID id=\"34\" name=\"uE0074\"/>\n    <GlyphID id=\"35\" name=\"uE0075\"/>\n    <GlyphID id=\"36\" name=\"uE0076\"/>\n    <GlyphID id=\"37\" name=\"uE0077\"/>\n    <GlyphID id=\"38\" name=\"uE0078\"/>\n    <GlyphID id=\"39\" name=\"uE0079\"/>\n    <GlyphID id=\"40\" name=\"uE007A\"/>\n    <GlyphID id=\"41\" name=\"uE007F\"/>\n    <GlyphID id=\"42\" name=\"u1F3F4\"/>\n    <GlyphID id=\"43\" name=\"uFE82B\"/>\n    <GlyphID id=\"44\" name=\"u1F1E6\"/>\n    <GlyphID id=\"45\" name=\"u1F1E7\"/>\n    <GlyphID id=\"46\" name=\"u1F1E8\"/>\n    <GlyphID id=\"47\" name=\"u1F1E9\"/>\n    <GlyphID id=\"48\" name=\"u1F1EA\"/>\n    <GlyphID id=\"49\" name=\"u1F1EB\"/>\n    <GlyphID id=\"50\" name=\"u1F1EC\"/>\n    <GlyphID id=\"51\" name=\"u1F1ED\"/>\n    <GlyphID id=\"52\" name=\"u1F1EE\"/>\n    <GlyphID id=\"53\" name=\"u1F1EF\"/>\n    <GlyphID id=\"54\" name=\"u1F1F0\"/>\n    <GlyphID id=\"55\" name=\"u1F1F1\"/>\n    <GlyphID id=\"56\" name=\"u1F1F2\"/>\n    <GlyphID id=\"57\" name=\"u1F1F3\"/>\n    <GlyphID id=\"58\" name=\"u1F1F4\"/>\n    <GlyphID id=\"59\" name=\"u1F1F5\"/>\n    <GlyphID id=\"60\" name=\"u1F1F6\"/>\n    <GlyphID id=\"61\" name=\"u1F1F7\"/>\n    <GlyphID id=\"62\" name=\"u1F1F8\"/>\n    <GlyphID id=\"63\" name=\"u1F1F9\"/>\n    <GlyphID id=\"64\" name=\"u1F1FA\"/>\n    <GlyphID id=\"65\" name=\"u1F1FB\"/>\n    <GlyphID id=\"66\" name=\"u1F1FC\"/>\n    <GlyphID id=\"67\" name=\"u1F1FD\"/>\n    <GlyphID id=\"68\" name=\"u1F1FE\"/>\n    <GlyphID id=\"69\" name=\"u1F1FF\"/>\n  </GlyphOrder>\n\n  <head>\n    <!-- Most of this table will be recalculated by the compiler -->\n    <tableVersion value=\"1.0\"/>\n    <fontRevision value=\"2.047\"/>\n    <checkSumAdjustment value=\"0x4d5a161a\"/>\n    <magicNumber value=\"0x5f0f3cf5\"/>\n    <flags value=\"00000000 00001011\"/>\n    <unitsPerEm value=\"2048\"/>\n    <created value=\"Wed May 22 20:00:43 2013\"/>\n    <modified value=\"Wed May 22 20:00:43 2013\"/>\n    <xMin value=\"0\"/>\n    <yMin value=\"-500\"/>\n    <xMax value=\"2550\"/>\n    <yMax value=\"1900\"/>\n    <macStyle value=\"00000000 00000000\"/>\n    <lowestRecPPEM value=\"8\"/>\n    <fontDirectionHint value=\"2\"/>\n    <indexToLocFormat value=\"0\"/>\n    <glyphDataFormat value=\"0\"/>\n  </head>\n\n  <hhea>\n    <tableVersion value=\"0x00010000\"/>\n    <ascent value=\"1900\"/>\n    <descent value=\"-500\"/>\n    <lineGap value=\"0\"/>\n    <advanceWidthMax value=\"2550\"/>\n    <minLeftSideBearing value=\"0\"/>\n    <minRightSideBearing value=\"0\"/>\n    <xMaxExtent value=\"2550\"/>\n    <caretSlopeRise value=\"1\"/>\n    <caretSlopeRun value=\"0\"/>\n    <caretOffset value=\"0\"/>\n    <reserved0 value=\"0\"/>\n    <reserved1 value=\"0\"/>\n    <reserved2 value=\"0\"/>\n    <reserved3 value=\"0\"/>\n    <metricDataFormat value=\"0\"/>\n    <numberOfHMetrics value=\"4\"/>\n  </hhea>\n\n  <maxp>\n    <!-- Most of this table will be recalculated by the compiler -->\n    <tableVersion value=\"0x10000\"/>\n    <numGlyphs value=\"70\"/>\n    <maxPoints value=\"8\"/>\n    <maxContours value=\"2\"/>\n    <maxCompositePoints value=\"0\"/>\n    <maxCompositeContours value=\"0\"/>\n    <maxZones value=\"2\"/>\n    <maxTwilightPoints value=\"0\"/>\n    <maxStorage value=\"1\"/>\n    <maxFunctionDefs value=\"1\"/>\n    <maxInstructionDefs value=\"0\"/>\n    <maxStackElements value=\"64\"/>\n    <maxSizeOfInstructions value=\"46\"/>\n    <maxComponentElements value=\"0\"/>\n    <maxComponentDepth value=\"0\"/>\n  </maxp>\n\n  <OS_2>\n    <version value=\"4\"/>\n    <xAvgCharWidth value=\"2550\"/>\n    <usWeightClass value=\"400\"/>\n    <usWidthClass value=\"5\"/>\n    <fsType value=\"00000000 00000000\"/>\n    <ySubscriptXSize value=\"1331\"/>\n    <ySubscriptYSize value=\"1433\"/>\n    <ySubscriptXOffset value=\"0\"/>\n    <ySubscriptYOffset value=\"286\"/>\n    <ySuperscriptXSize value=\"1331\"/>\n    <ySuperscriptYSize value=\"1433\"/>\n    <ySuperscriptXOffset value=\"0\"/>\n    <ySuperscriptYOffset value=\"983\"/>\n    <yStrikeoutSize value=\"102\"/>\n    <yStrikeoutPosition value=\"530\"/>\n    <sFamilyClass value=\"0\"/>\n    <panose>\n      <bFamilyType value=\"2\"/>\n      <bSerifStyle value=\"0\"/>\n      <bWeight value=\"6\"/>\n      <bProportion value=\"9\"/>\n      <bContrast value=\"0\"/>\n      <bStrokeVariation value=\"0\"/>\n      <bArmStyle value=\"0\"/>\n      <bLetterForm value=\"0\"/>\n      <bMidline value=\"0\"/>\n      <bXHeight value=\"0\"/>\n    </panose>\n    <ulUnicodeRange1 value=\"00000000 00000000 00000000 00000001\"/>\n    <ulUnicodeRange2 value=\"00000000 00000000 00000000 00000000\"/>\n    <ulUnicodeRange3 value=\"00000000 00000000 00000000 00000000\"/>\n    <ulUnicodeRange4 value=\"00000000 00000000 00000000 00000000\"/>\n    <achVendID value=\"GOOG\"/>\n    <fsSelection value=\"00000000 01000000\"/>\n    <usFirstCharIndex value=\"0\"/>\n    <usLastCharIndex value=\"65535\"/>\n    <sTypoAscender value=\"1900\"/>\n    <sTypoDescender value=\"-500\"/>\n    <sTypoLineGap value=\"0\"/>\n    <usWinAscent value=\"1900\"/>\n    <usWinDescent value=\"500\"/>\n    <ulCodePageRange1 value=\"00000000 00000000 00000000 00000001\"/>\n    <ulCodePageRange2 value=\"00000000 00000000 00000000 00000000\"/>\n    <sxHeight value=\"0\"/>\n    <sCapHeight value=\"1900\"/>\n    <usDefaultChar value=\"0\"/>\n    <usBreakChar value=\"32\"/>\n    <usMaxContext value=\"1\"/>\n  </OS_2>\n\n  <hmtx>\n    <mtx name=\".notdef\" width=\"2550\" lsb=\"0\"/>\n    <mtx name=\"null\" width=\"0\" lsb=\"0\"/>\n    <mtx name=\"nonmarkingreturn\" width=\"2550\" lsb=\"0\"/>\n    <mtx name=\"space\" width=\"2550\" lsb=\"0\"/>\n  </hmtx>\n\n  <vhea>\n    <tableVersion value=\"0x00010000\"/>\n    <ascent value=\"1275\"/>\n    <descent value=\"-1275\"/>\n    <lineGap value=\"0\"/>\n    <advanceHeightMax value=\"2500\"/>\n    <minTopSideBearing value=\"0\"/>\n    <minBottomSideBearing value=\"0\"/>\n    <yMaxExtent value=\"2400\"/>\n    <caretSlopeRise value=\"0\"/>\n    <caretSlopeRun value=\"1\"/>\n    <caretOffset value=\"0\"/>\n    <reserved1 value=\"0\"/>\n    <reserved2 value=\"0\"/>\n    <reserved3 value=\"0\"/>\n    <reserved4 value=\"0\"/>\n    <metricDataFormat value=\"0\"/>\n    <numberOfVMetrics value=\"1\"/>\n  </vhea>\n\n  <vmtx>\n    <mtx name=\".notdef\" height=\"2500\" tsb=\"0\"/>\n    <mtx name=\"null\" height=\"0\" tsb=\"0\"/>\n    <mtx name=\"nonmarkingreturn\" height=\"2500\" tsb=\"0\"/>\n    <mtx name=\"space\" height=\"2500\" tsb=\"0\"/>\n  </vmtx>\n\n  <cmap>\n    <tableVersion version=\"0\"/>\n    <cmap_format_12 platformID=\"3\" platEncID=\"10\" language=\"0\" format=\"12\" reserved=\"0\" length=\"1\" nGroups=\"1\">\n      <map code=\"0x0\" name=\"null\"/><!-- &lt;control> -->\n      <map code=\"0xd\" name=\"nonmarkingreturn\"/>\n      <map code=\"0x20\" name=\"space\"/>\n    </cmap_format_12>\n  </cmap>\n\n  <name>\n    <namerecord nameID=\"0\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Copyright 2022 Google Inc.\n    </namerecord>\n    <namerecord nameID=\"1\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Noto Color Emoji\n    </namerecord>\n    <namerecord nameID=\"2\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Regular\n    </namerecord>\n    <namerecord nameID=\"3\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Noto Color Emoji\n    </namerecord>\n    <namerecord nameID=\"4\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Noto Color Emoji\n    </namerecord>\n    <namerecord nameID=\"5\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Version 2.047;GOOG;noto-emoji:20240827:6c211821b8442ab3683a502f9a79b2034293fced\n    </namerecord>\n    <namerecord nameID=\"6\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      NotoColorEmoji\n    </namerecord>\n    <namerecord nameID=\"7\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Noto is a trademark of Google Inc.\n    </namerecord>\n    <namerecord nameID=\"8\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Google, Inc.\n    </namerecord>\n    <namerecord nameID=\"9\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Google, Inc.\n    </namerecord>\n    <namerecord nameID=\"10\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Color emoji font using CBDT glyph data.\n    </namerecord>\n    <namerecord nameID=\"11\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      http://www.google.com/get/noto/\n    </namerecord>\n    <namerecord nameID=\"12\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      http://www.google.com/get/noto/\n    </namerecord>\n    <namerecord nameID=\"13\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      This Font Software is licensed under the SIL Open Font License, Version 1.1. This Font Software is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the SIL Open Font License for the specific language, permissions and limitations governing your use of this Font Software.\n    </namerecord>\n    <namerecord nameID=\"14\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      http://scripts.sil.org/OFL\n    </namerecord>\n  </name>\n\n  <post>\n    <formatType value=\"3.0\"/>\n    <italicAngle value=\"0.0\"/>\n    <underlinePosition value=\"-1244\"/>\n    <underlineThickness value=\"131\"/>\n    <isFixedPitch value=\"1\"/>\n    <minMemType42 value=\"0\"/>\n    <maxMemType42 value=\"0\"/>\n    <minMemType1 value=\"0\"/>\n    <maxMemType1 value=\"0\"/>\n  </post>\n\n  <GSUB>\n    <Version value=\"0x00010000\"/>\n    <ScriptList>\n      <!-- ScriptCount=1 -->\n      <ScriptRecord index=\"0\">\n        <ScriptTag value=\"DFLT\"/>\n        <Script>\n          <DefaultLangSys>\n            <ReqFeatureIndex value=\"65535\"/>\n            <!-- FeatureCount=1 -->\n            <FeatureIndex index=\"0\" value=\"0\"/>\n          </DefaultLangSys>\n          <!-- LangSysCount=0 -->\n        </Script>\n      </ScriptRecord>\n    </ScriptList>\n    <FeatureList>\n      <!-- FeatureCount=1 -->\n      <FeatureRecord index=\"0\">\n        <FeatureTag value=\"ccmp\"/>\n        <Feature>\n          <!-- LookupCount=4 -->\n          <LookupListIndex index=\"0\" value=\"0\"/>\n          <LookupListIndex index=\"1\" value=\"2\"/>\n          <LookupListIndex index=\"2\" value=\"3\"/>\n          <LookupListIndex index=\"3\" value=\"4\"/>\n        </Feature>\n      </FeatureRecord>\n    </FeatureList>\n    <LookupList>\n      <!-- LookupCount=5 -->\n      <Lookup index=\"0\">\n        <LookupType value=\"4\"/>\n        <LookupFlag value=\"0\"/>\n        <!-- SubTableCount=1 -->\n        <LigatureSubst index=\"0\" Format=\"1\">\n        </LigatureSubst>\n      </Lookup>\n      <Lookup index=\"1\">\n        <LookupType value=\"2\"/>\n        <LookupFlag value=\"0\"/>\n        <!-- SubTableCount=1 -->\n        <MultipleSubst index=\"0\" Format=\"1\">\n          <Substitution in=\"u1F1E6\" out=\"\"/>\n          <Substitution in=\"u1F1E7\" out=\"\"/>\n          <Substitution in=\"u1F1E8\" out=\"\"/>\n          <Substitution in=\"u1F1E9\" out=\"\"/>\n          <Substitution in=\"u1F1EA\" out=\"\"/>\n          <Substitution in=\"u1F1EB\" out=\"\"/>\n          <Substitution in=\"u1F1EC\" out=\"\"/>\n          <Substitution in=\"u1F1ED\" out=\"\"/>\n          <Substitution in=\"u1F1EE\" out=\"\"/>\n          <Substitution in=\"u1F1EF\" out=\"\"/>\n          <Substitution in=\"u1F1F0\" out=\"\"/>\n          <Substitution in=\"u1F1F1\" out=\"\"/>\n          <Substitution in=\"u1F1F2\" out=\"\"/>\n          <Substitution in=\"u1F1F3\" out=\"\"/>\n          <Substitution in=\"u1F1F4\" out=\"\"/>\n          <Substitution in=\"u1F1F5\" out=\"\"/>\n          <Substitution in=\"u1F1F6\" out=\"\"/>\n          <Substitution in=\"u1F1F7\" out=\"\"/>\n          <Substitution in=\"u1F1F8\" out=\"\"/>\n          <Substitution in=\"u1F1F9\" out=\"\"/>\n          <Substitution in=\"u1F1FA\" out=\"\"/>\n          <Substitution in=\"u1F1FB\" out=\"\"/>\n          <Substitution in=\"u1F1FC\" out=\"\"/>\n          <Substitution in=\"u1F1FD\" out=\"\"/>\n          <Substitution in=\"u1F1FE\" out=\"\"/>\n          <Substitution in=\"u1F1FF\" out=\"\"/>\n          <Substitution in=\"uE0030\" out=\"\"/>\n          <Substitution in=\"uE0031\" out=\"\"/>\n          <Substitution in=\"uE0032\" out=\"\"/>\n          <Substitution in=\"uE0033\" out=\"\"/>\n          <Substitution in=\"uE0034\" out=\"\"/>\n          <Substitution in=\"uE0035\" out=\"\"/>\n          <Substitution in=\"uE0036\" out=\"\"/>\n          <Substitution in=\"uE0037\" out=\"\"/>\n          <Substitution in=\"uE0038\" out=\"\"/>\n          <Substitution in=\"uE0039\" out=\"\"/>\n          <Substitution in=\"uE0061\" out=\"\"/>\n          <Substitution in=\"uE0062\" out=\"\"/>\n          <Substitution in=\"uE0063\" out=\"\"/>\n          <Substitution in=\"uE0064\" out=\"\"/>\n          <Substitution in=\"uE0065\" out=\"\"/>\n          <Substitution in=\"uE0066\" out=\"\"/>\n          <Substitution in=\"uE0067\" out=\"\"/>\n          <Substitution in=\"uE0068\" out=\"\"/>\n          <Substitution in=\"uE0069\" out=\"\"/>\n          <Substitution in=\"uE006A\" out=\"\"/>\n          <Substitution in=\"uE006B\" out=\"\"/>\n          <Substitution in=\"uE006C\" out=\"\"/>\n          <Substitution in=\"uE006D\" out=\"\"/>\n          <Substitution in=\"uE006E\" out=\"\"/>\n          <Substitution in=\"uE006F\" out=\"\"/>\n          <Substitution in=\"uE0070\" out=\"\"/>\n          <Substitution in=\"uE0071\" out=\"\"/>\n          <Substitution in=\"uE0072\" out=\"\"/>\n          <Substitution in=\"uE0073\" out=\"\"/>\n          <Substitution in=\"uE0074\" out=\"\"/>\n          <Substitution in=\"uE0075\" out=\"\"/>\n          <Substitution in=\"uE0076\" out=\"\"/>\n          <Substitution in=\"uE0077\" out=\"\"/>\n          <Substitution in=\"uE0078\" out=\"\"/>\n          <Substitution in=\"uE0079\" out=\"\"/>\n          <Substitution in=\"uE007A\" out=\"\"/>\n        </MultipleSubst>\n      </Lookup>\n      <Lookup index=\"2\">\n        <LookupType value=\"6\"/>\n        <LookupFlag value=\"0\"/>\n        <!-- SubTableCount=1 -->\n        <ChainContextSubst index=\"0\" Format=\"2\">\n          <Coverage Format=\"2\">\n            <Glyph value=\"uE0030\"/>\n            <Glyph value=\"uE0031\"/>\n            <Glyph value=\"uE0032\"/>\n            <Glyph value=\"uE0033\"/>\n            <Glyph value=\"uE0034\"/>\n            <Glyph value=\"uE0035\"/>\n            <Glyph value=\"uE0036\"/>\n            <Glyph value=\"uE0037\"/>\n            <Glyph value=\"uE0038\"/>\n            <Glyph value=\"uE0039\"/>\n            <Glyph value=\"uE0061\"/>\n            <Glyph value=\"uE0062\"/>\n            <Glyph value=\"uE0063\"/>\n            <Glyph value=\"uE0064\"/>\n            <Glyph value=\"uE0065\"/>\n            <Glyph value=\"uE0066\"/>\n            <Glyph value=\"uE0067\"/>\n            <Glyph value=\"uE0068\"/>\n            <Glyph value=\"uE0069\"/>\n            <Glyph value=\"uE006A\"/>\n            <Glyph value=\"uE006B\"/>\n            <Glyph value=\"uE006C\"/>\n            <Glyph value=\"uE006D\"/>\n            <Glyph value=\"uE006E\"/>\n            <Glyph value=\"uE006F\"/>\n            <Glyph value=\"uE0070\"/>\n            <Glyph value=\"uE0071\"/>\n            <Glyph value=\"uE0072\"/>\n            <Glyph value=\"uE0073\"/>\n            <Glyph value=\"uE0074\"/>\n            <Glyph value=\"uE0075\"/>\n            <Glyph value=\"uE0076\"/>\n            <Glyph value=\"uE0077\"/>\n            <Glyph value=\"uE0078\"/>\n            <Glyph value=\"uE0079\"/>\n            <Glyph value=\"uE007A\"/>\n          </Coverage>\n          <BacktrackClassDef Format=\"1\">\n            <ClassDef glyph=\"u1F3F4\" class=\"1\"/>\n            <ClassDef glyph=\"uE007F\" class=\"1\"/>\n          </BacktrackClassDef>\n          <InputClassDef Format=\"2\">\n            <ClassDef glyph=\"uE0030\" class=\"2\"/>\n            <ClassDef glyph=\"uE0031\" class=\"2\"/>\n            <ClassDef glyph=\"uE0032\" class=\"2\"/>\n            <ClassDef glyph=\"uE0033\" class=\"2\"/>\n            <ClassDef glyph=\"uE0034\" class=\"2\"/>\n            <ClassDef glyph=\"uE0035\" class=\"2\"/>\n            <ClassDef glyph=\"uE0036\" class=\"2\"/>\n            <ClassDef glyph=\"uE0037\" class=\"2\"/>\n            <ClassDef glyph=\"uE0038\" class=\"2\"/>\n            <ClassDef glyph=\"uE0039\" class=\"2\"/>\n            <ClassDef glyph=\"uE0061\" class=\"2\"/>\n            <ClassDef glyph=\"uE0062\" class=\"2\"/>\n            <ClassDef glyph=\"uE0063\" class=\"2\"/>\n            <ClassDef glyph=\"uE0064\" class=\"2\"/>\n            <ClassDef glyph=\"uE0065\" class=\"2\"/>\n            <ClassDef glyph=\"uE0066\" class=\"2\"/>\n            <ClassDef glyph=\"uE0067\" class=\"2\"/>\n            <ClassDef glyph=\"uE0068\" class=\"2\"/>\n            <ClassDef glyph=\"uE0069\" class=\"2\"/>\n            <ClassDef glyph=\"uE006A\" class=\"2\"/>\n            <ClassDef glyph=\"uE006B\" class=\"2\"/>\n            <ClassDef glyph=\"uE006C\" class=\"2\"/>\n            <ClassDef glyph=\"uE006D\" class=\"2\"/>\n            <ClassDef glyph=\"uE006E\" class=\"2\"/>\n            <ClassDef glyph=\"uE006F\" class=\"2\"/>\n            <ClassDef glyph=\"uE0070\" class=\"2\"/>\n            <ClassDef glyph=\"uE0071\" class=\"2\"/>\n            <ClassDef glyph=\"uE0072\" class=\"2\"/>\n            <ClassDef glyph=\"uE0073\" class=\"2\"/>\n            <ClassDef glyph=\"uE0074\" class=\"2\"/>\n            <ClassDef glyph=\"uE0075\" class=\"2\"/>\n            <ClassDef glyph=\"uE0076\" class=\"2\"/>\n            <ClassDef glyph=\"uE0077\" class=\"2\"/>\n            <ClassDef glyph=\"uE0078\" class=\"2\"/>\n            <ClassDef glyph=\"uE0079\" class=\"2\"/>\n            <ClassDef glyph=\"uE007A\" class=\"2\"/>\n          </InputClassDef>\n          <LookAheadClassDef Format=\"2\">\n          </LookAheadClassDef>\n          <!-- ChainSubClassSetCount=3 -->\n          <ChainSubClassSet index=\"0\" empty=\"1\"/>\n          <ChainSubClassSet index=\"1\" empty=\"1\"/>\n          <ChainSubClassSet index=\"2\">\n            <!-- ChainSubClassRuleCount=1 -->\n            <ChainSubClassRule index=\"0\">\n              <!-- BacktrackGlyphCount=1 -->\n              <Backtrack index=\"0\" value=\"1\"/>\n              <!-- InputGlyphCount=1 -->\n              <!-- LookAheadGlyphCount=0 -->\n              <!-- SubstCount=1 -->\n              <SubstLookupRecord index=\"0\">\n                <SequenceIndex value=\"0\"/>\n                <LookupListIndex value=\"1\"/>\n              </SubstLookupRecord>\n            </ChainSubClassRule>\n          </ChainSubClassSet>\n        </ChainContextSubst>\n      </Lookup>\n      <Lookup index=\"3\">\n        <LookupType value=\"4\"/>\n        <LookupFlag value=\"0\"/>\n        <!-- SubTableCount=1 -->\n        <LigatureSubst index=\"0\" Format=\"1\">\n          <LigatureSet glyph=\"u1F3F4\">\n            <Ligature components=\"uE007F\" glyph=\"uFE82B\"/>\n          </LigatureSet>\n          <LigatureSet glyph=\"uE007F\">\n            <Ligature components=\"u1F3F4\" glyph=\"uFE82B\"/>\n          </LigatureSet>\n        </LigatureSubst>\n      </Lookup>\n      <Lookup index=\"4\">\n        <LookupType value=\"5\"/>\n        <LookupFlag value=\"0\"/>\n        <!-- SubTableCount=1 -->\n        <ContextSubst index=\"0\" Format=\"2\">\n          <Coverage Format=\"2\">\n            <Glyph value=\"u1F1E6\"/>\n            <Glyph value=\"u1F1E7\"/>\n            <Glyph value=\"u1F1E8\"/>\n            <Glyph value=\"u1F1E9\"/>\n            <Glyph value=\"u1F1EA\"/>\n            <Glyph value=\"u1F1EB\"/>\n            <Glyph value=\"u1F1EC\"/>\n            <Glyph value=\"u1F1ED\"/>\n            <Glyph value=\"u1F1EE\"/>\n            <Glyph value=\"u1F1EF\"/>\n            <Glyph value=\"u1F1F0\"/>\n            <Glyph value=\"u1F1F1\"/>\n            <Glyph value=\"u1F1F2\"/>\n            <Glyph value=\"u1F1F3\"/>\n            <Glyph value=\"u1F1F4\"/>\n            <Glyph value=\"u1F1F5\"/>\n            <Glyph value=\"u1F1F6\"/>\n            <Glyph value=\"u1F1F7\"/>\n            <Glyph value=\"u1F1F8\"/>\n            <Glyph value=\"u1F1F9\"/>\n            <Glyph value=\"u1F1FA\"/>\n            <Glyph value=\"u1F1FB\"/>\n            <Glyph value=\"u1F1FC\"/>\n            <Glyph value=\"u1F1FD\"/>\n            <Glyph value=\"u1F1FE\"/>\n            <Glyph value=\"u1F1FF\"/>\n          </Coverage>\n          <ClassDef Format=\"2\">\n            <ClassDef glyph=\"u1F1E6\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1E7\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1E8\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1E9\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1EA\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1EB\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1EC\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1ED\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1EE\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1EF\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F0\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F1\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F2\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F3\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F4\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F5\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F6\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F7\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F8\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1F9\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1FA\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1FB\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1FC\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1FD\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1FE\" class=\"1\"/>\n            <ClassDef glyph=\"u1F1FF\" class=\"1\"/>\n          </ClassDef>\n          <!-- SubClassSetCount=2 -->\n          <SubClassSet index=\"0\" empty=\"1\"/>\n          <SubClassSet index=\"1\">\n            <!-- SubClassRuleCount=1 -->\n            <SubClassRule index=\"0\">\n              <!-- GlyphCount=2 -->\n              <!-- SubstCount=2 -->\n              <Class index=\"0\" value=\"1\"/>\n              <SubstLookupRecord index=\"0\">\n                <SequenceIndex value=\"0\"/>\n                <LookupListIndex value=\"5\"/>\n              </SubstLookupRecord>\n              <SubstLookupRecord index=\"1\">\n                <SequenceIndex value=\"1\"/>\n                <LookupListIndex value=\"1\"/>\n              </SubstLookupRecord>\n            </SubClassRule>\n          </SubClassSet>\n        </ContextSubst>\n      </Lookup>\n      <Lookup index=\"5\">\n        <LookupType value=\"1\"/>\n        <LookupFlag value=\"0\"/>\n        <SingleSubst index=\"0\" Format=\"1\">\n          <Substitution in=\"u1F1E6\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1E7\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1E8\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1E9\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1EA\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1EB\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1EC\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1ED\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1EE\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1EF\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F0\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F1\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F2\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F3\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F4\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F5\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F6\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F7\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F8\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1F9\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1FA\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1FB\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1FC\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1FD\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1FE\" out=\"uFE82B\"/>\n          <Substitution in=\"u1F1FF\" out=\"uFE82B\"/>\n        </SingleSubst>\n      </Lookup>\n    </LookupList>\n  </GSUB>\n\n</ttFont>\n"
        },
        {
          "name": "NotoColorEmojiSvg.tmpl.ttx",
          "type": "blob",
          "size": 6.6689453125,
          "content": "<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?>\n<ttFont sfntVersion=\"\\x00\\x01\\x00\\x00\" ttLibVersion=\"2.3\">\n  <!-- noto svg emoji -->\n  <GlyphOrder>\n    <!-- The 'id' attribute is only for humans; it is ignored when parsed. -->\n    <GlyphID id=\"0\" name=\".notdef\"/>\n  </GlyphOrder>\n\n  <head>\n    <!-- Most of this table will be recalculated by the compiler -->\n    <tableVersion value=\"1.0\"/>\n    <fontRevision value=\"1.22\"/>\n    <checkSumAdjustment value=\"0x4d5a161a\"/>\n    <magicNumber value=\"0x5f0f3cf5\"/>\n    <flags value=\"00000000 00001011\"/>\n    <unitsPerEm value=\"2048\"/>\n    <created value=\"Wed May 22 20:00:43 2013\"/>\n    <modified value=\"Wed May 22 20:00:43 2013\"/>\n    <xMin value=\"0\"/>\n    <yMin value=\"-500\"/>\n    <xMax value=\"2550\"/>\n    <yMax value=\"2100\"/>\n    <macStyle value=\"00000000 00000000\"/>\n    <lowestRecPPEM value=\"8\"/>\n    <fontDirectionHint value=\"2\"/>\n    <indexToLocFormat value=\"0\"/>\n    <glyphDataFormat value=\"0\"/>\n  </head>\n\n  <hhea>\n    <tableVersion value=\"1.0\"/>\n    <ascent value=\"1900\"/>\n    <descent value=\"-500\"/>\n    <lineGap value=\"256\"/>\n    <advanceWidthMax value=\"2550\"/>\n    <minLeftSideBearing value=\"0\"/>\n    <minRightSideBearing value=\"0\"/>\n    <xMaxExtent value=\"2550\"/>\n    <caretSlopeRise value=\"1\"/>\n    <caretSlopeRun value=\"0\"/>\n    <caretOffset value=\"0\"/>\n    <reserved0 value=\"0\"/>\n    <reserved1 value=\"0\"/>\n    <reserved2 value=\"0\"/>\n    <reserved3 value=\"0\"/>\n    <metricDataFormat value=\"0\"/>\n    <numberOfHMetrics value=\"1\"/>\n  </hhea>\n\n  <maxp>\n    <!-- Most of this table will be recalculated by the compiler -->\n    <tableVersion value=\"0x10000\"/>\n    <numGlyphs value=\"1\"/>\n    <maxPoints value=\"8\"/>\n    <maxContours value=\"2\"/>\n    <maxCompositePoints value=\"0\"/>\n    <maxCompositeContours value=\"0\"/>\n    <maxZones value=\"2\"/>\n    <maxTwilightPoints value=\"0\"/>\n    <maxStorage value=\"1\"/>\n    <maxFunctionDefs value=\"1\"/>\n    <maxInstructionDefs value=\"0\"/>\n    <maxStackElements value=\"64\"/>\n    <maxSizeOfInstructions value=\"46\"/>\n    <maxComponentElements value=\"0\"/>\n    <maxComponentDepth value=\"0\"/>\n  </maxp>\n\n  <OS_2>\n    <version value=\"4\"/>\n    <xAvgCharWidth value=\"2550\"/>\n    <usWeightClass value=\"500\"/>\n    <usWidthClass value=\"5\"/>\n    <fsType value=\"00000000 00000000\"/>\n    <ySubscriptXSize value=\"1331\"/>\n    <ySubscriptYSize value=\"1433\"/>\n    <ySubscriptXOffset value=\"0\"/>\n    <ySubscriptYOffset value=\"286\"/>\n    <ySuperscriptXSize value=\"1331\"/>\n    <ySuperscriptYSize value=\"1433\"/>\n    <ySuperscriptXOffset value=\"0\"/>\n    <ySuperscriptYOffset value=\"983\"/>\n    <yStrikeoutSize value=\"102\"/>\n    <yStrikeoutPosition value=\"530\"/>\n    <sFamilyClass value=\"0\"/>\n    <panose>\n      <bFamilyType value=\"2\"/>\n      <bSerifStyle value=\"0\"/>\n      <bWeight value=\"6\"/>\n      <bProportion value=\"9\"/>\n      <bContrast value=\"0\"/>\n      <bStrokeVariation value=\"0\"/>\n      <bArmStyle value=\"0\"/>\n      <bLetterForm value=\"0\"/>\n      <bMidline value=\"0\"/>\n      <bXHeight value=\"0\"/>\n    </panose>\n    <ulUnicodeRange1 value=\"00000000 00000000 00000000 00000001\"/>\n    <ulUnicodeRange2 value=\"00000000 00000000 00000000 00000000\"/>\n    <ulUnicodeRange3 value=\"00000000 00000000 00000000 00000000\"/>\n    <ulUnicodeRange4 value=\"00000000 00000000 00000000 00000000\"/>\n    <achVendID value=\"GOOG\"/>\n    <fsSelection value=\"00000000 11000000\"/>\n    <fsFirstCharIndex value=\"0\"/>\n    <fsLastCharIndex value=\"90\"/>\n    <sTypoAscender value=\"1900\"/>\n    <sTypoDescender value=\"-500\"/>\n    <sTypoLineGap value=\"256\"/>\n    <usWinAscent value=\"1900\"/>\n    <usWinDescent value=\"500\"/>\n    <ulCodePageRange1 value=\"00000000 00000000 00000000 00000001\"/>\n    <ulCodePageRange2 value=\"00000000 00000000 00000000 00000000\"/>\n    <sxHeight value=\"0\"/>\n    <sCapHeight value=\"1900\"/>\n    <usDefaultChar value=\"0\"/>\n    <usBreakChar value=\"32\"/>\n    <usMaxContex value=\"1\"/>\n  </OS_2>\n\n  <hmtx>\n    <mtx name=\".notdef\" width=\"2550\" lsb=\"0\"/>\n  </hmtx>\n\n  <cmap>\n    <tableVersion version=\"0\"/>\n    <cmap_format_12 platformID=\"3\" platEncID=\"10\" language=\"0\" format=\"12\" reserved=\"0\" length=\"1\" nGroups=\"1\">\n      <map code=\"0x0\" name=\".notdef\"/><!-- &lt;control> -->\n    </cmap_format_12>\n  </cmap>\n\n  <name>\n    <namerecord nameID=\"0\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Copyright 2015 Google Inc.\n    </namerecord>\n    <namerecord nameID=\"1\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Noto Color Emoji SVG\n    </namerecord>\n    <namerecord nameID=\"2\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Regular\n    </namerecord>\n    <namerecord nameID=\"3\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Noto Color Emoji SVG\n    </namerecord>\n    <namerecord nameID=\"4\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Noto Color Emoji SVG\n    </namerecord>\n    <namerecord nameID=\"5\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Version 1.22\n    </namerecord>\n    <namerecord nameID=\"6\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      NotoColorEmojiSVG\n    </namerecord>\n    <namerecord nameID=\"7\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      Noto is a trademark of Google Inc. and may be registered in certain jurisdictions.\n    </namerecord>\n    <namerecord nameID=\"11\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      http://code.google.com/p/noto/\n    </namerecord>\n    <namerecord nameID=\"13\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      This Font Software is licensed under the SIL Open Font License, Version 1.1. This Font Software is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the SIL Open Font License for the specific language, permissions and limitations governing your use of this Font Software.\n    </namerecord>\n    <namerecord nameID=\"14\" platformID=\"3\" platEncID=\"1\" langID=\"0x409\">\n      http://scripts.sil.org/OFL\n    </namerecord>\n  </name>\n\n  <post>\n    <formatType value=\"3.0\"/>\n    <italicAngle value=\"0.0\"/>\n    <underlinePosition value=\"-1244\"/>\n    <underlineThickness value=\"131\"/>\n    <isFixedPitch value=\"1\"/>\n    <minMemType42 value=\"0\"/>\n    <maxMemType42 value=\"0\"/>\n    <minMemType1 value=\"0\"/>\n    <maxMemType1 value=\"0\"/>\n  </post>\n\n  <loca>\n  </loca>\n\n  <glyf>\n    <!-- The xMin, yMin, xMax and yMax values\n         will be recalculated by the compiler. -->\n\n    <TTGlyph name=\".notdef\" xMin=\"675\" yMin=\"-269\" xMax=\"1825\" yMax=\"1731\">\n      <contour>\n        <pt x=\"817\" y=\"1589\" on=\"1\"/>\n        <pt x=\"817\" y=\"-127\" on=\"1\"/>\n        <pt x=\"1683\" y=\"-127\" on=\"1\"/>\n        <pt x=\"1683\" y=\"1589\" on=\"1\"/>\n      </contour>\n      <contour>\n        <pt x=\"675\" y=\"-269\" on=\"1\"/>\n        <pt x=\"675\" y=\"1731\" on=\"1\"/>\n        <pt x=\"1825\" y=\"1731\" on=\"1\"/>\n        <pt x=\"1825\" y=\"-269\" on=\"1\"/>\n      </contour>\n      <instructions><assembly>\n        </assembly></instructions>\n    </TTGlyph>\n  </glyf>\n\n</ttFont>\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.4306640625,
          "content": "![Noto](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab4b4276-9bb0-42a6-a675-510fcb6055df_1940x1088.png)\n# Noto Emoji\nNoto Emoji (Stands for No Tofu) is an open source (Open Font License 1.1) emoji library that provides standard Unicode emoji support and tools for working with them including:\n\n- A Unicode compliant color emoji [font](https://github.com/googlefonts/noto-emoji/raw/main/fonts/NotoColorEmoji.ttf).\n- A full library of Noto color emoji font files including vector svgs and pngs\n- [Metadata](https://github.com/googlefonts/emoji-metadata) for Emoji Input (including shortcodes, emoji ordering, ascii equivalents)\n\n## Color Font\n\nThe latest font file is found [here](https://github.com/googlefonts/noto-emoji/raw/main/fonts/NotoColorEmoji.ttf). If you want to download a specific version, please look at the gh-pages branch, where you will find the built assets for both our latest and older versions. \n\n## Monochrome Font\n\nThe black-and-white emoji font is back under active development and is available as a [variable font](https://fonts.google.com/noto/specimen/Noto+Emoji)\n\n## Using NotoColorEmoji\n\nNotoColorEmoji uses the CBDT/CBLC color font format, which is supported by Android\nand Chrome/Chromium OS.  Windows supports it starting with Windows 10 Anniversary\nUpdate in Chrome and Edge.  On macOS, only Chrome supports it, while on Linux it will\nsupport it with some fontconfig tweaking, see [issue #36](https://github.com/googlei18n/noto-emoji/issues/36). Currently we do not build other color font formats.\n\n## A note about PNGs\n\nThe assets provided in the repo are all those used to build the NotoColorEmoji font. With one exception: the flag images in the font are PNG images to which transforms have been applied to standardize the size and generate the wave and border shadow. We do not have SVG versions that reflect these transforms.\n\n## License\n\nEmoji fonts (under the fonts subdirectory) are under the\n[SIL Open Font License, version 1.1](fonts/LICENSE).<br/>\nTools and most image resources are under the [Apache license, version 2.0](./LICENSE).\nFlag images under third_party/region-flags are in the public domain or\notherwise exempt from copyright ([more info](third_party/region-flags/LICENSE)).\n\n## Contributing\n\nPlease read [CONTRIBUTING](CONTRIBUTING.md) if you are thinking of contributing to this project.\n"
        },
        {
          "name": "about_fonts.py",
          "type": "blob",
          "size": 1.0048828125,
          "content": "\"\"\"Prints info about emoji fonts.\"\"\"\n\nfrom fontTools import ttLib\nfrom pathlib import Path\nimport sys\n\n\nNAME_ID_VERSION = 5\n\n\ndef name(font, name_id):\n    return \",\".join(n.toUnicode() for n in font[\"name\"].names if n.isUnicode() and n.nameID == name_id)\n\n\ndef main():\n    font_files = sorted(p for p in (Path(__file__).parent / \"fonts\").iterdir() if p.suffix == \".ttf\")\n    max_name_len = max(len(p.name) for p in font_files)\n\n    for font_file in font_files:\n        font = ttLib.TTFont(font_file)\n\n        font_type = []\n        if \"CBDT\" in font:\n            font_type.append(\"CBDT\")\n        if \"COLR\" in font:\n            font_type.append(\"COLR\")\n        if \"meta\" in font and \"Emji\" in font[\"meta\"].data:\n            font_type.append(\"EmojiCompat\")\n        font_type.append(f\"fontRevision:{font['head'].fontRevision:.3f}\")\n        font_type.append(name(font, NAME_ID_VERSION))\n        font_type = \", \".join(font_type)\n\n        print(f\"{font_file.name:{max_name_len + 1}} {font_type}\")\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "add_aliases.py",
          "type": "blob",
          "size": 7.244140625,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2017 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport glob\nimport os\nfrom os import path\nimport shutil\nimport sys\n\nfrom nototools import unicode_data\n\n\"\"\"Create aliases in target directory.\n\nIn addition to links/copies named with aliased sequences, this can also\ncreate canonically named aliases/copies, if requested.\"\"\"\n\n\nDATA_ROOT = path.dirname(path.abspath(__file__))\n\ndef str_to_seq(seq_str):\n  res = [int(s, 16) for s in seq_str.split('_')]\n  if 0xfe0f in res:\n    print('0xfe0f in file name: %s' % seq_str)\n    res = [x for x in res if x != 0xfe0f]\n  return tuple(res)\n\n\ndef seq_to_str(seq):\n  return '_'.join('%04x' % cp for cp in seq)\n\n\ndef read_default_unknown_flag_aliases():\n  unknown_flag_path = path.join(DATA_ROOT, 'unknown_flag_aliases.txt')\n  return read_emoji_aliases(unknown_flag_path)\n\n\ndef read_default_emoji_aliases():\n  alias_path = path.join(DATA_ROOT, 'emoji_aliases.txt')\n  return read_emoji_aliases(alias_path)\n\n\ndef read_emoji_aliases(filename):\n  result = {}\n\n  with open(filename, 'r') as f:\n    for line in f:\n      ix = line.find('#')\n      if (ix > -1):\n        line = line[:ix]\n      line = line.strip()\n      if not line:\n        continue\n      als, trg = (s.strip() for s in line.split(';'))\n      try:\n        als_seq = tuple([int(x, 16) for x in als.split('_')])\n        trg_seq = tuple([int(x, 16) for x in trg.split('_')])\n      except:\n        print('cannot process alias %s -> %s' % (als, trg))\n        continue\n      result[als_seq] = trg_seq\n  return result\n\n\ndef add_aliases(\n    srcdir, dstdir, aliasfile, prefix, ext, replace=False, copy=False,\n    canonical_names=False, dry_run=False):\n  \"\"\"Use aliasfile to create aliases of files in srcdir matching prefix/ext in\n  dstdir.  If dstdir is null, use srcdir as dstdir.  If replace is false\n  and a file already exists in dstdir, report and do nothing.  If copy is false\n  create a symlink, else create a copy.\n\n  If canonical_names is true, check all source files and generate aliases/copies\n  using the canonical name if different from the existing name.\n\n  If dry_run is true, report what would be done.  Dstdir will be created if\n  necessary, even if dry_run is true.\"\"\"\n\n  if not path.isdir(srcdir):\n    print('%s is not a directory' % srcdir, file=sys.stderr)\n    return\n\n  if not dstdir:\n    dstdir = srcdir\n  elif not path.isdir(dstdir):\n    os.makedirs(dstdir)\n\n  prefix_len = len(prefix)\n  suffix_len = len(ext) + 1\n  filenames = [path.basename(f)\n               for f in glob.glob(path.join(srcdir, '%s*.%s' % (prefix, ext)))]\n  seq_to_file = {\n      str_to_seq(name[prefix_len:-suffix_len]) : name\n      for name in filenames}\n\n  aliases = read_emoji_aliases(aliasfile)\n  aliases_to_create = {}\n  aliases_to_replace = []\n  alias_exists = False\n\n  def check_alias_seq(seq):\n    alias_str = seq_to_str(seq)\n    alias_name = '%s%s.%s' % (prefix, alias_str, ext)\n    alias_path = path.join(dstdir, alias_name)\n    if path.exists(alias_path):\n      if replace:\n        aliases_to_replace.append(alias_name)\n      else:\n        print('alias %s exists' % alias_str, file=sys.stderr)\n        alias_exists = True\n        return None\n    return alias_name\n\n  canonical_to_file = {}\n  for als, trg in sorted(aliases.items()):\n    if trg not in seq_to_file:\n      print('target %s for %s does not exist' % (\n          seq_to_str(trg), seq_to_str(als)), file=sys.stderr)\n      continue\n    alias_name = check_alias_seq(als)\n    if alias_name:\n      target_file = seq_to_file[trg]\n      aliases_to_create[alias_name] = target_file\n      if canonical_names:\n        canonical_seq = unicode_data.get_canonical_emoji_sequence(als)\n        if canonical_seq and canonical_seq != als:\n          canonical_alias_name = check_alias_seq(canonical_seq)\n          if canonical_alias_name:\n            canonical_to_file[canonical_alias_name] = target_file\n\n  if canonical_names:\n    print('adding %d canonical aliases' % len(canonical_to_file))\n    for seq, f in seq_to_file.iteritems():\n      canonical_seq = unicode_data.get_canonical_emoji_sequence(seq)\n      if canonical_seq and canonical_seq != seq:\n        alias_name = check_alias_seq(canonical_seq)\n        if alias_name:\n          canonical_to_file[alias_name] = f\n\n    print('adding %d total canonical sequences' % len(canonical_to_file))\n    aliases_to_create.update(canonical_to_file)\n\n  if replace:\n    if not dry_run:\n      for k in sorted(aliases_to_replace):\n        os.remove(path.join(dstdir, k))\n    print('replacing %d files' % len(aliases_to_replace))\n  elif alias_exists:\n    print('aborting, aliases exist.', file=sys.stderr)\n    return\n\n  for k, v in sorted(aliases_to_create.items()):\n    if dry_run:\n      msg = 'replace ' if k in aliases_to_replace else ''\n      print('%s%s -> %s' % (msg, k, v))\n    else:\n      try:\n        if copy:\n          shutil.copy2(path.join(srcdir, v), path.join(dstdir, k))\n        else:\n          # fix this to create relative symlinks\n          if srcdir == dstdir:\n            os.symlink(v, path.join(dstdir, k))\n          else:\n            raise Exception('can\\'t create cross-directory symlinks yet')\n      except Exception as e:\n        print('failed to create %s -> %s' % (k, v), file=sys.stderr)\n        raise Exception('oops, ' + str(e))\n  print('created %d %s' % (\n      len(aliases_to_create), 'copies' if copy else 'symlinks'))\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-s', '--srcdir', help='directory containing files to alias',\n      required=True, metavar='dir')\n  parser.add_argument(\n      '-d', '--dstdir', help='directory to write aliases, default srcdir',\n      metavar='dir')\n  parser.add_argument(\n      '-a', '--aliasfile', help='alias file (default emoji_aliases.txt)',\n      metavar='file', default='emoji_aliases.txt')\n  parser.add_argument(\n      '-p', '--prefix', help='file name prefix (default emoji_u)',\n      metavar='pfx', default='emoji_u')\n  parser.add_argument(\n      '-e', '--ext', help='file name extension (default png)',\n      choices=['ai', 'png', 'svg'], default='png')\n  parser.add_argument(\n      '-r', '--replace', help='replace existing files/aliases',\n      action='store_true')\n  parser.add_argument(\n      '-c', '--copy', help='create a copy of the file, not a symlink',\n      action='store_true')\n  parser.add_argument(\n      '--canonical_names', help='include extra copies with canonical names '\n      '(including fe0f emoji presentation character)', action='store_true');\n  parser.add_argument(\n      '-n', '--dry_run', help='print out aliases to create only',\n      action='store_true')\n  args = parser.parse_args()\n\n  add_aliases(\n      args.srcdir, args.dstdir, args.aliasfile, args.prefix, args.ext,\n      args.replace, args.copy, args.canonical_names, args.dry_run)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "add_emoji_gsub.py",
          "type": "blob",
          "size": 5.8818359375,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2014 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Modify the Noto Color Emoji font to use GSUB rules for flags and keycaps.\"\"\"\n\n__author__ = \"roozbeh@google.com (Roozbeh Pournader)\"\n\nimport sys\n\nfrom fontTools import agl\nfrom fontTools.ttLib.tables import otTables\nfrom fontTools import ttLib\nfrom nototools import font_data\n\n\ndef create_script_list(script_tag='DFLT'):\n    \"\"\"Create a ScriptList for the GSUB table.\"\"\"\n    def_lang_sys = otTables.DefaultLangSys()\n    def_lang_sys.ReqFeatureIndex = 0xFFFF\n    def_lang_sys.FeatureCount = 1\n    def_lang_sys.FeatureIndex = [0]\n    def_lang_sys.LookupOrder = None\n\n    script_record = otTables.ScriptRecord()\n    script_record.ScriptTag = script_tag\n    script_record.Script = otTables.Script()\n    script_record.Script.DefaultLangSys = def_lang_sys\n    script_record.Script.LangSysCount = 0\n    script_record.Script.LangSysRecord = []\n\n    script_list = otTables.ScriptList()\n    script_list.ScriptCount = 1\n    script_list.ScriptRecord = [script_record]\n\n    return script_list\n\n\ndef create_feature_list(feature_tag, lookup_count):\n    \"\"\"Create a FeatureList for the GSUB table.\"\"\"\n    feature_record = otTables.FeatureRecord()\n    feature_record.FeatureTag = feature_tag\n    feature_record.Feature = otTables.Feature()\n    feature_record.Feature.LookupCount = lookup_count\n    feature_record.Feature.LookupListIndex = range(lookup_count)\n    feature_record.Feature.FeatureParams = None\n\n    feature_list = otTables.FeatureList()\n    feature_list.FeatureCount = 1\n    feature_list.FeatureRecord = [feature_record]\n\n    return feature_list\n\n\ndef create_lookup_list(lookups):\n    \"\"\"Create a LookupList for the GSUB table.\"\"\"\n    lookup_list = otTables.LookupList()\n    lookup_list.LookupCount = len(lookups)\n    lookup_list.Lookup = lookups\n\n    return lookup_list\n\n\ndef get_glyph_name_or_create(char, font):\n    \"\"\"Return the glyph name for a character, creating if it doesn't exist.\"\"\"\n    cmap = font_data.get_cmap(font)\n    if char in cmap:\n        return cmap[char]\n\n    glyph_name = agl.UV2AGL[char]\n    assert glyph_name not in font.glyphOrder\n\n    font['hmtx'].metrics[glyph_name] = [0, 0]\n    cmap[char] = glyph_name\n\n    if 'glyf' in font:\n        from fontTools.ttLib.tables import _g_l_y_f\n        empty_glyph = _g_l_y_f.Glyph()\n        font['glyf'].glyphs[glyph_name] = empty_glyph\n\n    font.glyphOrder.append(glyph_name)\n    return glyph_name\n\n\ndef create_lookup(table, font, flag=0):\n    \"\"\"Create a Lookup based on mapping table.\"\"\"\n    cmap = font_data.get_cmap(font)\n\n    ligatures = {}\n    for output, (ch1, ch2) in table.iteritems():\n        output = cmap[output]\n        ch1 = get_glyph_name_or_create(ch1, font)\n        ch2 = get_glyph_name_or_create(ch2, font)\n\n        ligature = otTables.Ligature()\n        ligature.CompCount = 2\n        ligature.Component = [ch2]\n        ligature.LigGlyph = output\n\n        try:\n            ligatures[ch1].append(ligature)\n        except KeyError:\n            ligatures[ch1] = [ligature]\n\n    ligature_subst = otTables.LigatureSubst()\n    ligature_subst.ligatures = ligatures\n\n    lookup = otTables.Lookup()\n    lookup.LookupType = 4\n    lookup.LookupFlag = flag\n    lookup.SubTableCount = 1\n    lookup.SubTable = [ligature_subst]\n\n    return lookup\n\n\ndef create_simple_gsub(lookups, script='DFLT', feature='ccmp'):\n    \"\"\"Create a simple GSUB table.\"\"\"\n    gsub_class = ttLib.getTableClass('GSUB')\n    gsub = gsub_class('GSUB')\n\n    gsub.table = otTables.GSUB()\n    gsub.table.Version = 1.0\n    gsub.table.ScriptList = create_script_list(script)\n    gsub.table.FeatureList = create_feature_list(feature, len(lookups))\n    gsub.table.LookupList = create_lookup_list(lookups)\n    return gsub\n\n\ndef reg_indicator(letter):\n    \"\"\"Return a regional indicator character from corresponding capital letter.\n    \"\"\"\n    return 0x1F1E6 + ord(letter) - ord('A')\n\n\nEMOJI_FLAGS = {\n    0xFE4E5: (reg_indicator('J'), reg_indicator('P')),  # Japan\n    0xFE4E6: (reg_indicator('U'), reg_indicator('S')),  # United States\n    0xFE4E7: (reg_indicator('F'), reg_indicator('R')),  # France\n    0xFE4E8: (reg_indicator('D'), reg_indicator('E')),  # Germany\n    0xFE4E9: (reg_indicator('I'), reg_indicator('T')),  # Italy\n    0xFE4EA: (reg_indicator('G'), reg_indicator('B')),  # United Kingdom\n    0xFE4EB: (reg_indicator('E'), reg_indicator('S')),  # Spain\n    0xFE4EC: (reg_indicator('R'), reg_indicator('U')),  # Russia\n    0xFE4ED: (reg_indicator('C'), reg_indicator('N')),  # China\n    0xFE4EE: (reg_indicator('K'), reg_indicator('R')),  # Korea\n}\n\nKEYCAP = 0x20E3\n\nEMOJI_KEYCAPS = {\n    0xFE82C: (ord('#'), KEYCAP),\n    0xFE82E: (ord('1'), KEYCAP),\n    0xFE82F: (ord('2'), KEYCAP),\n    0xFE830: (ord('3'), KEYCAP),\n    0xFE831: (ord('4'), KEYCAP),\n    0xFE832: (ord('5'), KEYCAP),\n    0xFE833: (ord('6'), KEYCAP),\n    0xFE834: (ord('7'), KEYCAP),\n    0xFE835: (ord('8'), KEYCAP),\n    0xFE836: (ord('9'), KEYCAP),\n    0xFE837: (ord('0'), KEYCAP),\n}\n\ndef main(argv):\n    \"\"\"Modify all the fonts given in the command line.\"\"\"\n    for font_name in argv[1:]:\n        font = ttLib.TTFont(font_name)\n\n        assert 'GSUB' not in font\n        font['GSUB'] = create_simple_gsub([\n            create_lookup(EMOJI_KEYCAPS, font),\n            create_lookup(EMOJI_FLAGS, font)])\n\n        font_data.delete_from_cmap(\n            font, EMOJI_FLAGS.keys() + EMOJI_KEYCAPS.keys())\n\n        font.save(font_name+'-fixed')\n\nif __name__ == '__main__':\n    main(sys.argv)\n"
        },
        {
          "name": "add_glyphs.py",
          "type": "blob",
          "size": 14.7666015625,
          "content": "#!/usr/bin/env python3\n\n\"\"\"Extend a ttx file with additional data.\n\nTakes a ttx file and one or more directories containing image files named\nafter sequences of codepoints, extends the cmap, hmtx, GSUB, and GlyphOrder\ntables in the source ttx file based on these sequences, and writes out a new\nttx file.\n\nThis can also apply aliases from an alias file.\"\"\"\n\nimport argparse\nimport collections\nimport os\nfrom os import path\nimport re\nimport sys\n\nfrom fontTools import ttx\nfrom fontTools.ttLib.tables import otTables\nfrom fontTools.pens.ttGlyphPen import TTGlyphPen\nfrom fontTools.ttLib.tables._c_m_a_p import CmapSubtable\nfrom fontTools.ttLib import newTable\n\nimport add_emoji_gsub\nimport add_aliases\n\nsys.path.append(\n    path.join(os.path.dirname(__file__), 'third_party', 'color_emoji'))\nfrom png import PNG\n\n\ndef get_seq_to_file(image_dir, prefix, suffix):\n  \"\"\"Return a mapping from codepoint sequences to files in the given directory,\n  for files that match the prefix and suffix.  File names with this prefix and\n  suffix should consist of codepoints in hex separated by underscore.  'fe0f'\n  (the codepoint of the emoji presentation variation selector) is stripped from\n  the sequence.\n  \"\"\"\n  start = len(prefix)\n  limit = -len(suffix)\n  seq_to_file = {}\n  for name in os.listdir(image_dir):\n    if not (name.startswith(prefix) and name.endswith(suffix)):\n      continue\n    try:\n      cps = [int(s, 16) for s in name[start:limit].split('_')]\n      seq = tuple(cp for cp in cps if cp != 0xfe0f)\n    except:\n      raise Exception('could not parse \"%s\"' % name)\n    for cp in cps:\n      if not (0 <= cp <= 0x10ffff):\n        raise Exception('bad codepoint(s) in \"%s\"' % name)\n    if seq in seq_to_file:\n      raise Exception('duplicate sequence for \"%s\" in %s' % (name, image_dir))\n    seq_to_file[seq] = path.join(image_dir, name)\n  return seq_to_file\n\n\ndef collect_seq_to_file(image_dirs, prefix, suffix):\n  \"\"\"Return a sequence to file mapping by calling get_seq_to_file on a list\n  of directories.  When sequences for files in later directories match those\n  from earlier directories, the later file replaces the earlier one.\n  \"\"\"\n  seq_to_file = {}\n  for image_dir in image_dirs:\n    seq_to_file.update(get_seq_to_file(image_dir, prefix, suffix))\n  return seq_to_file\n\n\ndef remap_values(seq_to_file, map_fn):\n  return {k: map_fn(v) for k, v in seq_to_file.items()}\n\n\ndef get_png_file_to_advance_mapper(lineheight):\n  def map_fn(filename):\n    wid, ht = PNG(filename).get_size()\n    return int(round(float(lineheight) * wid / ht))\n  return map_fn\n\n\ndef cp_name(cp):\n  \"\"\"return uniXXXX or uXXXXX(X) as a name for the glyph mapped to this cp.\"\"\"\n  return '%s%04X' % ('u' if cp > 0xffff else 'uni', cp)\n\n\ndef seq_name(seq):\n  \"\"\"Sequences of length one get the cp_name.  Others start with 'u' followed by\n  two or more 4-to-6-digit hex strings separated by underscore.\"\"\"\n  if len(seq) == 1:\n    return cp_name(seq[0])\n  return 'u' + '_'.join('%04X' % cp for cp in seq)\n\n\ndef collect_cps(seqs):\n  cps = set()\n  for seq in seqs:\n    cps.update(seq)\n  return cps\n\n\ndef get_glyphorder_cps_and_truncate(glyphOrder):\n  \"\"\"This scans glyphOrder for names that correspond to a single codepoint\n  using the 'u(ni)XXXXXX' syntax.  All names that don't match are moved\n  to the front the glyphOrder list in their original order, and the\n  list is truncated.  The ones that do match are returned as a set of\n  codepoints.\"\"\"\n  glyph_name_re = re.compile(r'^u(?:ni)?([0-9a-fA-F]{4,6})$')\n  cps = set()\n  write_ix = 0\n  for ix, name in enumerate(glyphOrder):\n    m = glyph_name_re.match(name)\n    if m:\n      cps.add(int(m.group(1), 16))\n    else:\n      glyphOrder[write_ix] = name\n      write_ix += 1\n  del glyphOrder[write_ix:]\n  return cps\n\n\ndef get_all_seqs(font, seq_to_advance):\n  \"\"\"Copies the sequences from seq_to_advance and extends it with single-\n  codepoint sequences from the GlyphOrder table as well as those internal\n  to sequences in seq_to_advance.  Reduces the GlyphOrder table. \"\"\"\n\n  all_seqs = set(seq_to_advance.keys())\n  # using collect_cps includes cps internal to a seq\n  cps = collect_cps(all_seqs)\n  glyphOrder = font.getGlyphOrder()\n  # extract cps in glyphOrder and reduce glyphOrder to only those that remain\n  glyphOrder_cps = get_glyphorder_cps_and_truncate(glyphOrder)\n  cps.update(glyphOrder_cps)\n  # add new single codepoint sequences from glyphOrder and sequences\n  all_seqs.update((cp,) for cp in cps)\n  return all_seqs\n\n\ndef get_font_cmap(font):\n  \"\"\"Return the first cmap in the font, we assume it exists and is a unicode\n  cmap.\"\"\"\n  return font['cmap'].tables[0].cmap\n\n\ndef add_glyph_data(font, seqs, seq_to_advance, vadvance, add_glyf):\n  \"\"\"Add hmtx and GlyphOrder data for all sequences in seqs, and ensures there's\n  a cmap entry for each single-codepoint sequence.  Seqs not in seq_to_advance\n  will get a zero advance.\"\"\"\n\n  # We allow the template cmap to omit mappings for single-codepoint glyphs\n  # defined in the template's GlyphOrder table.  Similarly, the hmtx table can\n  # omit advances.  We assume glyphs named 'uniXXXX' or 'uXXXXX(X)' in the\n  # GlyphOrder table correspond to codepoints based on the name; we don't\n  # attempt to handle other types of names and these must occur in the cmap and\n  # hmtx tables in the template.\n  #\n  # seq_to_advance maps sequences (including single codepoints) to advances.\n  # All codepoints in these sequences will be added to the cmap.  Some cps\n  # in these sequences have no corresponding single-codepoint sequence, they\n  # will also get added.\n  #\n  # The added codepoints have no advance information, so will get a zero\n  # advance.\n\n  cmap = get_font_cmap(font)\n  hmtx = font['hmtx'].metrics\n  vmtx = font['vmtx'].metrics\n\n  # Add glyf table so empty glyphs will be added to ensure compatibility\n  # with systems requiring a glyf table, like Windows 10.\n  if add_glyf:\n    pen = TTGlyphPen(None)\n    empty_glyph = pen.glyph()\n    font['loca'] = newTable(\"loca\")\n    font['glyf'] = glyf_table = newTable(\"glyf\")\n    glyf_table.glyphOrder = font.getGlyphOrder()\n    glyf_table.glyphs = {g: empty_glyph for g in glyf_table.glyphOrder}\n\n  # We don't expect sequences to be in the glyphOrder, since we removed all the\n  # single-cp sequences from it and don't expect it to already contain names\n  # corresponding to multiple-cp sequencess.  But just in case, we use\n  # reverseGlyphMap to avoid duplicating names accidentally.\n\n  updatedGlyphOrder = False\n  reverseGlyphMap = font.getReverseGlyphMap()\n\n  # Order the glyphs by grouping all the single-codepoint sequences first,\n  # then order by sequence so that related sequences are together.  We group\n  # by single-codepoint sequence first in order to keep these glyphs together--\n  # they're used in the coverage tables for some of the substitutions, and\n  # those tables can be more compact this way.\n  for seq in sorted(seqs, key=lambda s: (0 if len(s) == 1 else 1, s)):\n    name = seq_name(seq)\n    if len(seq) == 1:\n      cmap[seq[0]] = name\n    advance = seq_to_advance.get(seq, 0)\n    hmtx[name] = [advance, 0]\n    vmtx[name] = [vadvance, 0]\n    if name not in reverseGlyphMap:\n      font.glyphOrder.append(name)\n      updatedGlyphOrder=True\n    if add_glyf:\n      glyf_table[name] = empty_glyph\n\n  if updatedGlyphOrder:\n    delattr(font, '_reverseGlyphOrderDict')\n\ndef add_aliases_to_cmap(font, aliases):\n  \"\"\"Some aliases might map a single codepoint to some other sequence.  These\n  should map directly to the glyph for that sequence in the cmap.  (Others will\n  map via GSUB).\n  \"\"\"\n  if not aliases:\n    return\n\n  cp_aliases = [seq for seq in aliases if len(seq) == 1]\n  if not cp_aliases:\n    return\n\n  cmap = get_font_cmap(font)\n  for src_seq in cp_aliases:\n    cp = src_seq[0]\n    name = seq_name(aliases[src_seq])\n    cmap[cp] = name\n\n\ndef get_rtl_seq(seq):\n  \"\"\"Return the rtl variant of the sequence, if it has one, else the empty\n  sequence.\n  \"\"\"\n  # Sequences with ZWJ in them will reflect.  Fitzpatrick modifiers\n  # however do not, so if we reflect we make a pass to swap them back into their\n  # logical order.\n  # Used to check for TAG_END 0xe007f as well but Android fontchain_lint\n  # dislikes the resulting mangling of flags for England, Scotland, Wales.\n\n  ZWJ = 0x200d\n  def is_fitzpatrick(cp):\n    return 0x1f3fb <= cp <= 0x1f3ff\n\n  if ZWJ not in seq:\n    return ()\n\n  rev_seq = list(seq)\n  rev_seq.reverse()\n  for i in range(1, len(rev_seq)):\n    if is_fitzpatrick(rev_seq[i-1]):\n      tmp = rev_seq[i]\n      rev_seq[i] = rev_seq[i-1]\n      rev_seq[i-1] = tmp\n  return tuple(rev_seq)\n\n\ndef get_gsub_ligature_lookup(font):\n  \"\"\"If the font does not have a GSUB table, create one with a ligature\n  substitution lookup.  If it does, ensure the first lookup is a properly\n  initialized ligature substitution lookup.  Return the lookup.\"\"\"\n\n  # The template might include more lookups after lookup 0, if it has a\n  # GSUB table.\n  if 'GSUB' not in font:\n    ligature_subst = otTables.LigatureSubst()\n    ligature_subst.ligatures = {}\n\n    lookup = otTables.Lookup()\n    lookup.LookupType = 4\n    lookup.LookupFlag = 0\n    lookup.SubTableCount = 1\n    lookup.SubTable = [ligature_subst]\n\n    font['GSUB'] = add_emoji_gsub.create_simple_gsub([lookup])\n  else:\n    lookup = font['GSUB'].table.LookupList.Lookup[0]\n    assert lookup.LookupFlag == 0\n\n    # importXML doesn't fully init GSUB structures, so help it out\n    st = lookup.SubTable[0]\n    if not hasattr(lookup, 'LookupType'):\n      assert st.LookupType == 4\n      setattr(lookup, 'LookupType', 4)\n\n    if not hasattr(st, 'ligatures'):\n      setattr(st, 'ligatures', {})\n\n  return lookup\n\n\ndef add_ligature_sequences(font, seqs, aliases):\n  \"\"\"Add ligature sequences.\"\"\"\n\n  seq_to_target_name = {\n      seq: seq_name(seq) for seq in seqs if len(seq) > 1}\n  if aliases:\n    seq_to_target_name.update({\n        seq: seq_name(aliases[seq]) for seq in aliases if len(seq) > 1})\n  if not seq_to_target_name:\n    return\n\n  rtl_seq_to_target_name = {\n      get_rtl_seq(seq): name for seq, name in seq_to_target_name.items()}\n  seq_to_target_name.update(rtl_seq_to_target_name)\n  # sequences that don't have rtl variants get mapped to the empty sequence,\n  # delete it.\n  if () in seq_to_target_name:\n    del seq_to_target_name[()]\n\n  # organize by first codepoint in sequence\n  keyed_ligatures = collections.defaultdict(list)\n  for t in seq_to_target_name.items():\n    first_cp = t[0][0]\n    keyed_ligatures[first_cp].append(t)\n\n  def add_ligature(lookup, cmap, seq, name):\n    # The sequences consist of codepoints, but the entries in the ligature table\n    # are glyph names.  Aliasing can give single codepoints names based on\n    # sequences (e.g. 'guardsman' with 'male guardsman') so we map the\n    # codepoints through the cmap to get the glyph names.\n    glyph_names = [cmap[cp] for cp in seq]\n\n    lig = otTables.Ligature()\n    lig.CompCount = len(seq)\n    lig.Component = glyph_names[1:]\n    lig.LigGlyph = name\n\n    ligatures = lookup.SubTable[0].ligatures\n    first_name = glyph_names[0]\n    try:\n      ligatures[first_name].append(lig)\n    except KeyError:\n      ligatures[first_name] = [lig]\n\n  lookup = get_gsub_ligature_lookup(font)\n  cmap = get_font_cmap(font)\n  for first_cp in sorted(keyed_ligatures):\n    pairs = keyed_ligatures[first_cp]\n\n    # Sort longest first, this ensures longer sequences with common prefixes\n    # are handled before shorter ones.  The secondary sort is a standard\n    # sort on the codepoints in the sequence.\n    pairs.sort(key = lambda pair: (-len(pair[0]), pair[0]))\n    for seq, name in pairs:\n      add_ligature(lookup, cmap, seq, name)\n\ndef add_cmap_format_4(font):\n  \"\"\"Add cmap format 4 table for Windows support, based on the\n  format 12 cmap.\"\"\"\n\n  cmap = get_font_cmap(font)\n\n  newtable = CmapSubtable.newSubtable(4)\n  newtable.platformID = 3\n  newtable.platEncID = 1\n  newtable.language = 0\n\n  # Format 4 only has unicode values 0x0000 to 0xFFFF\n  newtable.cmap = {cp: name for cp, name in cmap.items() if cp <= 0xFFFF}\n\n  font['cmap'].tables.append(newtable)\n\ndef update_font_data(font, seq_to_advance, vadvance, aliases, add_cmap4, add_glyf):\n  \"\"\"Update the font's cmap, hmtx, GSUB, and GlyphOrder tables.\"\"\"\n  seqs = get_all_seqs(font, seq_to_advance)\n  add_glyph_data(font, seqs, seq_to_advance, vadvance, add_glyf)\n  add_aliases_to_cmap(font, aliases)\n  add_ligature_sequences(font, seqs, aliases)\n  if add_cmap4:\n    add_cmap_format_4(font)\n\ndef apply_aliases(seq_dict, aliases):\n  \"\"\"Aliases is a mapping from sequence to replacement sequence.  We can use\n  an alias if the target is a key in the dictionary.  Furthermore, if the\n  source is a key in the dictionary, we can delete it.  This updates the\n  dictionary and returns the usable aliases.\"\"\"\n  usable_aliases = {}\n  for k, v in aliases.items():\n    if v in seq_dict:\n      usable_aliases[k] = v\n      if k in seq_dict:\n        del seq_dict[k]\n  return usable_aliases\n\n\ndef update_ttx(in_file, out_file, image_dirs, prefix, ext, aliases_file, add_cmap4, add_glyf):\n  if ext != '.png':\n    raise Exception('extension \"%s\" not supported' % ext)\n\n  seq_to_file = collect_seq_to_file(image_dirs, prefix, ext)\n  if not seq_to_file:\n    raise ValueError(\n        'no sequences with prefix \"%s\" and extension \"%s\" in %s' % (\n            prefix, ext, ', '.join(image_dirs)))\n\n  aliases = None\n  if aliases_file:\n    aliases = add_aliases.read_emoji_aliases(aliases_file)\n    aliases = apply_aliases(seq_to_file, aliases)\n\n  font = ttx.TTFont()\n  font.importXML(in_file)\n\n  lineheight = font['hhea'].ascent - font['hhea'].descent\n  map_fn = get_png_file_to_advance_mapper(lineheight)\n  seq_to_advance = remap_values(seq_to_file, map_fn)\n\n  vadvance = font['vhea'].advanceHeightMax if 'vhea' in font else lineheight\n\n  update_font_data(font, seq_to_advance, vadvance, aliases, add_cmap4, add_glyf)\n\n  font.saveXML(out_file)\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-f', '--in_file', help='ttx input file', metavar='file', required=True)\n  parser.add_argument(\n      '-o', '--out_file', help='ttx output file', metavar='file', required=True)\n  parser.add_argument(\n      '-d', '--image_dirs', help='directories containing image files',\n      nargs='+', metavar='dir', required=True)\n  parser.add_argument(\n      '-p', '--prefix', help='file prefix (default \"emoji_u\")',\n      metavar='pfx', default='emoji_u')\n  parser.add_argument(\n      '-e', '--ext', help='file extension (default \".png\", currently only '\n      '\".png\" is supported',  metavar='ext', default='.png')\n  parser.add_argument(\n      '-a', '--aliases', help='process alias table', const='emoji_aliases.txt',\n      nargs='?', metavar='file')\n  parser.add_argument(\n      '--add_cmap4', help='add cmap format 4 table', dest='add_cmap4', action='store_true')\n  parser.add_argument(\n      '--add_glyf', help='add glyf and loca tables', dest='add_glyf', action='store_true')\n  args = parser.parse_args()\n\n  update_ttx(\n      args.in_file, args.out_file, args.image_dirs, args.prefix, args.ext,\n      args.aliases, args.add_cmap4, args.add_glyf)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "add_svg_glyphs.py",
          "type": "blob",
          "size": 10.0498046875,
          "content": "#!/usr/bin/env python3\n# Copyright 2015 Google, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Google Author(s): Doug Felt\n\n\"\"\"Tool to update GSUB, hmtx, cmap, glyf tables with svg image glyphs.\"\"\"\nfrom __future__ import print_function\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport re\nimport sys\n\nfrom fontTools.ttLib.tables import otTables\nfrom fontTools.ttLib.tables import _g_l_y_f\nfrom fontTools.ttLib.tables import S_V_G_ as SVG\nfrom fontTools import ttx\n\nfrom nototools import tool_utils\n\nimport add_emoji_gsub\nimport svg_builder\n\n\nclass FontBuilder(object):\n  \"\"\"A utility for mutating a ttx font.  This maintains glyph_order, cmap, and\n  hmtx tables, and optionally GSUB, glyf, and SVN tables as well.\"\"\"\n\n  def __init__(self, font):\n    self.font = font;\n    self.glyph_order = font.getGlyphOrder()\n    self.cmap = font['cmap'].tables[0].cmap\n    self.hmtx = font['hmtx'].metrics\n\n  def init_gsub(self):\n    \"\"\"Call this if you are going to add ligatures to the font.  Creates a GSUB\n    table if there isn't one already.\"\"\"\n\n    if hasattr(self, 'ligatures'):\n      return\n    font = self.font\n    if 'GSUB' not in font:\n      ligature_subst = otTables.LigatureSubst()\n      ligature_subst.ligatures = {}\n\n      lookup = otTables.Lookup()\n      lookup.LookupType = 4\n      lookup.LookupFlag = 0\n      lookup.SubTableCount = 1\n      lookup.SubTable = [ligature_subst]\n\n      font['GSUB'] = add_emoji_gsub.create_simple_gsub([lookup])\n    else:\n      lookup = font['GSUB'].table.LookupList.Lookup[0]\n      assert lookup.LookupType == 4\n      assert lookup.LookupFlag == 0\n    self.ligatures = lookup.SubTable[0].ligatures\n\n  def init_glyf(self):\n    \"\"\"Call this if you need to create empty glyf entries in the font when you\n    add a new glyph.\"\"\"\n\n    if hasattr(self, 'glyphs'):\n      return\n    font = self.font\n    if 'glyf' not in font:\n      glyf_table = _g_l_y_f.table__g_l_y_f()\n      glyf_table.glyphs = {}\n      glyf_table.glyphOrder = self.glyph_order\n      font['glyf'] = glyf_table\n    self.glyphs = font['glyf'].glyphs\n\n  def init_svg(self):\n    \"\"\"Call this if you expect to add SVG images in the font. This calls\n    init_glyf since SVG support currently requires fallback glyf records for\n    each SVG image.\"\"\"\n\n    if hasattr(self, 'svgs'):\n      return\n\n    # svg requires glyf\n    self.init_glyf()\n\n    font = self.font\n    if 'SVG ' not in font:\n      svg_table = SVG.table_S_V_G_()\n      svg_table.docList = []\n      svg_table.colorPalettes = None\n      font['SVG '] = svg_table\n    self.svgs = font['SVG '].docList\n\n  def glyph_name(self, string):\n    return \"_\".join([\"u%04X\" % ord(char) for char in string])\n\n  def glyph_name_to_index(self, name):\n    return self.glyph_order.index(name) if name in self.glyph_order else -1;\n\n  def glyph_index_to_name(self, glyph_index):\n    if glyph_index < len(self.glyph_order):\n      return self.glyph_order[glyph_index]\n    return ''\n\n  def have_glyph(self, name):\n    return self.name_to_glyph_index >= 0\n\n  def _add_ligature(self, glyphstr):\n    lig = otTables.Ligature()\n    lig.CompCount = len(glyphstr)\n    lig.Component = [self.glyph_name(ch) for ch in glyphstr[1:]]\n    lig.LigGlyph = self.glyph_name(glyphstr)\n\n    first = self.glyph_name(glyphstr[0])\n    try:\n      self.ligatures[first].append(lig)\n    except KeyError:\n      self.ligatures[first] = [lig]\n\n  def _add_empty_glyph(self, glyphstr, name):\n    \"\"\"Create an empty glyph. If glyphstr is not a ligature, add a cmap entry\n    for it.\"\"\"\n    if len(glyphstr) == 1:\n      self.cmap[ord(glyphstr)] = name\n    self.hmtx[name] = [0, 0]\n    self.glyph_order.append(name)\n    if hasattr(self, 'glyphs'):\n      self.glyphs[name] = _g_l_y_f.Glyph()\n\n  def add_components_and_ligature(self, glyphstr):\n    \"\"\"Convert glyphstr to a name and check if it already exists. If not, check\n    if it is a ligature (longer than one codepoint), and if it is, generate\n    empty glyphs with cmap entries for any missing ligature components and add a\n    ligature record.  Then generate an empty glyph for the name.  Return a tuple\n    with the name, index, and a bool indicating whether the glyph already\n    existed.\"\"\"\n\n    name = self.glyph_name(glyphstr)\n    index = self.glyph_name_to_index(name)\n    exists = index >= 0\n    if not exists:\n      if len(glyphstr) > 1:\n        for char in glyphstr:\n          if ord(char) not in self.cmap:\n            char_name = self.glyph_name(char)\n            self._add_empty_glyph(char, char_name)\n        self._add_ligature(glyphstr)\n      index = len(self.glyph_order)\n      self._add_empty_glyph(glyphstr, name)\n    return name, index, exists\n\n  def add_svg(self, doc, hmetrics, name, index):\n    \"\"\"Add an svg table entry. If hmetrics is not None, update the hmtx table.\n    This expects the glyph has already been added.\"\"\"\n    # sanity check to make sure name and index correspond.\n    assert name == self.glyph_index_to_name(index)\n    if hmetrics:\n      self.hmtx[name] = hmetrics\n    svg_record = (doc, index, index) # startGlyphId, endGlyphId are the same\n    self.svgs.append(svg_record)\n\n\ndef collect_glyphstr_file_pairs(prefix, ext, include=None, exclude=None, verbosity=1):\n  \"\"\"Scan files with the given prefix and extension, and return a list of\n  (glyphstr, filename) where glyphstr is the character or ligature, and filename\n  is the image file associated with it.  The glyphstr is formed by decoding the\n  filename (exclusive of the prefix) as a sequence of hex codepoints separated\n  by underscore. Include, if defined, is a regex string to include only matched\n  filenames. Exclude, if defined, is a regex string to exclude matched\n  filenames, and is applied after include.\"\"\"\n\n  image_files = {}\n  glob_pat = \"%s*.%s\" % (prefix, ext)\n  leading = len(prefix)\n  trailing = len(ext) + 1 # include dot\n  logging.info(\"Looking for images matching '%s'.\", glob_pat)\n  ex_count = 0\n  ex = re.compile(exclude) if exclude else None\n  inc = re.compile(include) if include else None\n  if inc:\n    logging.info(\"Including images matching '%s'.\", include)\n  if ex:\n    logging.info(\"Excluding images matching '%s'.\", exclude)\n\n  for image_file in glob.glob(glob_pat):\n    if inc and not inc.search(image_file):\n      continue\n\n    if ex and ex.search(image_file):\n      if verbosity > 1:\n        print(\"Exclude %s\" % image_file)\n      ex_count += 1\n      continue\n\n    codes = image_file[leading:-trailing]\n    if \"_\" in codes:\n      pieces = codes.split (\"_\")\n      u = \"\".join ([unichr(int(code, 16)) for code in pieces])\n    else:\n      u = unichr(int(codes, 16))\n    image_files[u] = image_file\n\n  if ex_count:\n    logging.info(\"Excluded %d files.\", ex_count)\n  if not image_files:\n    raise Exception (\"No image files matching '%s'.\", glob_pat)\n  logging.info(\"Matched %s files.\", len(image_files))\n  return image_files.items()\n\n\ndef sort_glyphstr_tuples(glyphstr_tuples):\n  \"\"\"The list contains tuples whose first element is a string representing a\n  character or ligature.  It is sorted with shorter glyphstrs first, then\n  alphabetically. This ensures that ligature components are added to the font\n  before any ligatures that contain them.\"\"\"\n  glyphstr_tuples.sort(key=lambda t: (len(t[0]), t[0]))\n\n\ndef add_image_glyphs(in_file, out_file, pairs):\n  \"\"\"Add images from pairs (glyphstr, filename) to .ttx file in_file and write\n  to .ttx file out_file.\"\"\"\n\n  font = ttx.TTFont()\n  font.importXML(in_file)\n\n  sort_glyphstr_tuples(pairs)\n\n  font_builder = FontBuilder(font)\n  # we've already sorted by length, so the longest glyphstrs are at the end. To\n  # see if we have ligatures, we just need to check the last one.\n  if len(pairs[-1][0]) > 1:\n    font_builder.init_gsub()\n\n  img_builder = svg_builder.SvgBuilder(font_builder)\n  for glyphstr, filename in pairs:\n    logging.debug(\"Adding glyph for U+%s\", \",\".join(\n          [\"%04X\" % ord(char) for char in glyphstr]))\n    img_builder.add_from_filename(glyphstr, filename)\n\n  font.saveXML(out_file)\n  logging.info(\"Added %s images to %s\", len(pairs), out_file)\n\n\ndef main(argv):\n  usage = \"\"\"This will search for files that have image_prefix followed by one\n  or more hex numbers (separated by underscore if more than one), and end in\n  \".svg\". For example, if image_prefix is \"icons/u\", then files with names like\n  \"icons/u1F4A9.svg\" or \"icons/u1F1EF_1F1F5.svg\" will be loaded.  The script\n  then adds cmap, htmx, and potentially GSUB entries for the Unicode characters\n  found.  The advance width will be chosen based on image aspect ratio.  If\n  Unicode values outside the BMP are desired, the existing cmap table should be\n  of the appropriate (format 12) type.  Only the first cmap table and the first\n  GSUB lookup (if existing) are modified.\"\"\"\n\n  parser = argparse.ArgumentParser(\n      description='Update cmap, glyf, GSUB, and hmtx tables from image glyphs.',\n      epilog=usage)\n  parser.add_argument(\n      'in_file', help='Input ttx file name.', metavar='fname')\n  parser.add_argument(\n      'out_file', help='Output ttx file name.', metavar='fname')\n  parser.add_argument(\n      'image_prefix', help='Location and prefix of image files.',\n      metavar='path')\n  parser.add_argument(\n      '-i', '--include', help='include files whoses name matches this regex',\n      metavar='regex')\n  parser.add_argument(\n      '-e', '--exclude', help='exclude files whose name matches this regex',\n      metavar='regex')\n  parser.add_argument(\n      '-l', '--loglevel', help='log level name', default='warning')\n  args = parser.parse_args(argv)\n\n  tool_utils.setup_logging(args.loglevel)\n\n  pairs = collect_glyphstr_file_pairs(\n      args.image_prefix, 'svg', include=args.include, exclude=args.exclude)\n  add_image_glyphs(args.in_file, args.out_file, pairs)\n\n\nif __name__ == '__main__':\n  main(sys.argv[1:])\n"
        },
        {
          "name": "annotations_u11.txt",
          "type": "blob",
          "size": 7.3623046875,
          "content": "# new unicode 11 emoji\n# generated using unicode_data and alpha UCD11 data.\n# emoji sort order\nannotation: ok\n1f970 # smiling face with 3 hearts\n1f975 # hot face\n1f976 # cold face\n1f973 # partying face\n1f974 # woozy face\n1f97a # pleading face\n1f468 200d 1f9b0 # man, red haired\n1f468 1f3fb 200d 1f9b0 # man, red haired: light skin tone\n1f468 1f3fc 200d 1f9b0 # man, red haired: medium-light skin tone\n1f468 1f3fd 200d 1f9b0 # man, red haired: medium skin tone\n1f468 1f3fe 200d 1f9b0 # man, red haired: medium-dark skin tone\n1f468 1f3ff 200d 1f9b0 # man, red haired: dark skin tone\n1f469 200d 1f9b0 # woman, red haired\n1f469 1f3fb 200d 1f9b0 # woman, red haired: light skin tone\n1f469 1f3fc 200d 1f9b0 # woman, red haired: medium-light skin tone\n1f469 1f3fd 200d 1f9b0 # woman, red haired: medium skin tone\n1f469 1f3fe 200d 1f9b0 # woman, red haired: medium-dark skin tone\n1f469 1f3ff 200d 1f9b0 # woman, red haired: dark skin tone\n1f468 200d 1f9b1 # man, curly haired\n1f468 1f3fb 200d 1f9b1 # man, curly haired: light skin tone\n1f468 1f3fc 200d 1f9b1 # man, curly haired: medium-light skin tone\n1f468 1f3fd 200d 1f9b1 # man, curly haired: medium skin tone\n1f468 1f3fe 200d 1f9b1 # man, curly haired: medium-dark skin tone\n1f468 1f3ff 200d 1f9b1 # man, curly haired: dark skin tone\n1f469 200d 1f9b1 # woman, curly haired\n1f469 1f3fb 200d 1f9b1 # woman, curly haired: light skin tone\n1f469 1f3fc 200d 1f9b1 # woman, curly haired: medium-light skin tone\n1f469 1f3fd 200d 1f9b1 # woman, curly haired: medium skin tone\n1f469 1f3fe 200d 1f9b1 # woman, curly haired: medium-dark skin tone\n1f469 1f3ff 200d 1f9b1 # woman, curly haired: dark skin tone\n1f468 200d 1f9b2 # man, bald\n1f468 1f3fb 200d 1f9b2 # man, bald: light skin tone\n1f468 1f3fc 200d 1f9b2 # man, bald: medium-light skin tone\n1f468 1f3fd 200d 1f9b2 # man, bald: medium skin tone\n1f468 1f3fe 200d 1f9b2 # man, bald: medium-dark skin tone\n1f468 1f3ff 200d 1f9b2 # man, bald: dark skin tone\n1f469 200d 1f9b2 # woman, bald\n1f469 1f3fb 200d 1f9b2 # woman, bald: light skin tone\n1f469 1f3fc 200d 1f9b2 # woman, bald: medium-light skin tone\n1f469 1f3fd 200d 1f9b2 # woman, bald: medium skin tone\n1f469 1f3fe 200d 1f9b2 # woman, bald: medium-dark skin tone\n1f469 1f3ff 200d 1f9b2 # woman, bald: dark skin tone\n1f468 200d 1f9b3 # man, white haired\n1f468 1f3fb 200d 1f9b3 # man, white haired: light skin tone\n1f468 1f3fc 200d 1f9b3 # man, white haired: medium-light skin tone\n1f468 1f3fd 200d 1f9b3 # man, white haired: medium skin tone\n1f468 1f3fe 200d 1f9b3 # man, white haired: medium-dark skin tone\n1f468 1f3ff 200d 1f9b3 # man, white haired: dark skin tone\n1f469 200d 1f9b3 # woman, white haired\n1f469 1f3fb 200d 1f9b3 # woman, white haired: light skin tone\n1f469 1f3fc 200d 1f9b3 # woman, white haired: medium-light skin tone\n1f469 1f3fd 200d 1f9b3 # woman, white haired: medium skin tone\n1f469 1f3fe 200d 1f9b3 # woman, white haired: medium-dark skin tone\n1f469 1f3ff 200d 1f9b3 # woman, white haired: dark skin tone\n1f9b8 # superhero\n1f9b8 1f3fb # superhero: light skin tone\n1f9b8 1f3fc # superhero: medium-light skin tone\n1f9b8 1f3fd # superhero: medium skin tone\n1f9b8 1f3fe # superhero: medium-dark skin tone\n1f9b8 1f3ff # superhero: dark skin tone\n1f9b8 200d 2640 fe0f # woman superhero\n1f9b8 1f3fb 200d 2640 fe0f # woman superhero: light skin tone\n1f9b8 1f3fc 200d 2640 fe0f # woman superhero: medium-light skin tone\n1f9b8 1f3fd 200d 2640 fe0f # woman superhero: medium skin tone\n1f9b8 1f3fe 200d 2640 fe0f # woman superhero: medium-dark skin tone\n1f9b8 1f3ff 200d 2640 fe0f # woman superhero: dark skin tone\n1f9b8 200d 2642 fe0f # man superhero\n1f9b8 1f3fb 200d 2642 fe0f # man superhero: light skin tone\n1f9b8 1f3fc 200d 2642 fe0f # man superhero: medium-light skin tone\n1f9b8 1f3fd 200d 2642 fe0f # man superhero: medium skin tone\n1f9b8 1f3fe 200d 2642 fe0f # man superhero: medium-dark skin tone\n1f9b8 1f3ff 200d 2642 fe0f # man superhero: dark skin tone\n1f9b9 # supervillain\n1f9b9 1f3fb # supervillain: light skin tone\n1f9b9 1f3fc # supervillain: medium-light skin tone\n1f9b9 1f3fd # supervillain: medium skin tone\n1f9b9 1f3fe # supervillain: medium-dark skin tone\n1f9b9 1f3ff # supervillain: dark skin tone\n1f9b9 200d 2640 fe0f # woman supervillain\n1f9b9 1f3fb 200d 2640 fe0f # woman supervillain: light skin tone\n1f9b9 1f3fc 200d 2640 fe0f # woman supervillain: medium-light skin tone\n1f9b9 1f3fd 200d 2640 fe0f # woman supervillain: medium skin tone\n1f9b9 1f3fe 200d 2640 fe0f # woman supervillain: medium-dark skin tone\n1f9b9 1f3ff 200d 2640 fe0f # woman supervillain: dark skin tone\n1f9b9 200d 2642 fe0f # man supervillain\n1f9b9 1f3fb 200d 2642 fe0f # man supervillain: light skin tone\n1f9b9 1f3fc 200d 2642 fe0f # man supervillain: medium-light skin tone\n1f9b9 1f3fd 200d 2642 fe0f # man supervillain: medium skin tone\n1f9b9 1f3fe 200d 2642 fe0f # man supervillain: medium-dark skin tone\n1f9b9 1f3ff 200d 2642 fe0f # man supervillain: dark skin tone\n1f9b5 # leg\n1f9b5 1f3fb # leg: light skin tone\n1f9b5 1f3fc # leg: medium-light skin tone\n1f9b5 1f3fd # leg: medium skin tone\n1f9b5 1f3fe # leg: medium-dark skin tone\n1f9b5 1f3ff # leg: dark skin tone\n1f9b6 # foot\n1f9b6 1f3fb # foot: light skin tone\n1f9b6 1f3fc # foot: medium-light skin tone\n1f9b6 1f3fd # foot: medium skin tone\n1f9b6 1f3fe # foot: medium-dark skin tone\n1f9b6 1f3ff # foot: dark skin tone\n1f9b0 # red-haired\n1f9b1 # curly-haired\n1f9b2 # bald\n1f9b3 # white-haired\n1f9b4 # bone\n1f9b7 # tooth\n1f97d # goggles\n1f97c # lab coat\n1f97e # hiking boot\n1f97f # womanâ€™s flat shoe\n1f99d # raccoon\n1f999 # llama\n1f99b # hippopotamus\n1f998 # kangaroo\n1f9a1 # badger\n1f9a2 # swan\n1f99a # peacock\n1f99c # parrot\n1f99e # lobster\n1f99f # mosquito\n1f9a0 # microbe\n1f96d # mango\n1f96c # leafy green\n1f96f # bagel\n1f9c2 # salt\n1f96e # moon cake\n1f9c1 # cupcake\n1f9ed # compass\n1f9f1 # bricks\n1f6f9 # skateboard\n1f9f3 # luggage\n1f9e8 # firecracker\n1f9e7 # red envelope\n1f94e # softball\n1f94f # flying disc\n1f94d # lacrosse\n1f9ff # nazar amulet\n1f9e9 # jigsaw\n1f9f8 # teddy bear\n1f9f5 # thread\n1f9f6 # yarn\n1f9ee # abacus\n1f9fe # receipt\n1f9f0 # toolbox\n1f9f2 # magnet\n1f9ea # test tube\n1f9eb # petri dish\n1f9ec # dna\n1f9f4 # lotion bottle\n1f9f7 # safety pin\n1f9f9 # broom\n1f9fa # basket\n1f9fb # roll of paper\n1f9fc # soap\n1f9fd # sponge\n1f9ef # fire extinguisher\n\n# bugfixes, codepoint order\nannotation: warning\n0023\n002a\n0030\n0031\n0032\n0033\n0034\n0035\n0036\n0037\n0038\n0039\n1f311\n1f312\n1f313\n1f314\n1f315\n1f316\n1f317\n1f318\n1f319\n# 1f31a # new moon face\n1f31b\n1f31c\n# 1f31d # full moon face\n1f31e # sun with face\n1f3bb\n1f3e5\n1f3f9\n1f410\n1f415\n# 1f41d # honeybee\n1f422\n1f42a\n1f42b\n1f436\n1f46a\n1f482 1f3fb 200d 2640\n1f482 1f3fb 200d 2642\n1f482 1f3fc 200d 2640\n1f482 1f3fc 200d 2642\n1f482 1f3fd 200d 2640\n1f482 1f3fd 200d 2642\n1f482 1f3fe 200d 2640\n1f482 1f3fe 200d 2642\n1f482 1f3ff 200d 2640\n1f482 1f3ff 200d 2642\n1f482 200d 2640\n1f482 200d 2642\n1f491\n1f4a3\n1f4be\n1f52a\n1f52b\n1f5e1\n1f62a\n1f634\n1f641\n1f643\n1f691\n1f699\n1f912\n1f925\n1f92a\n# 1f939 200d 2642 # man juggling\n1f94c\n1f953\n1f957\n1f992\n1f997\n1f9c0\n1f9d9 1f3fd 200d 2640\n2139\n2694\n\nannotation: ok\n# 'new' in that they now have emoji presentation\n265f # chess pawn\n267e # infinity\n\n# I believe this is new but u11 emoji-zwj-sequences.txt lists this as 7.0\n1f3f4 200d 2620  # pirate flag\n\n# flags\nannotation: ok\n1f1e9 1f1ec # DG  -- alias IO\n1f1ea 1f1e6 # EA  -- alias ES\n1f1ea 1f1ed # EH\n1f1eb 1f1f0 # FK\n1f1ec 1f1eb # GF\n1f1ec 1f1f5 # GP\n1f1ec 1f1f8 # GS\n1f1f2 1f1eb # MF  -- alias FR\n1f1f3 1f1e8 # NC\n1f1f5 1f1f2 # PM\n1f1fc 1f1eb # WF\n1f1fd 1f1f0 # XK\n1f1fe 1f1f9 # YT\n\n"
        },
        {
          "name": "check_emoji_sequences.py",
          "type": "blob",
          "size": 15.654296875,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2016 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Compare emoji image file namings against unicode property data.\nThe intent of this script is to check if the resulting font will pass\nthe Android linter:\nhttps://android.googlesource.com/platform/frameworks/base/+/master/tools/fonts/fontchain_linter.py\n\"\"\"\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport glob\nimport os\nfrom os import path\nimport re\nimport sys\n\nfrom nototools import unicode_data\nimport add_aliases\n\nZWJ = 0x200d\nEMOJI_VS = 0xfe0f\n\nEND_TAG = 0xe007f\n\ndef _make_tag_set():\n  tag_set = set()\n  tag_set |= set(range(0xe0030, 0xe003a))  # 0-9\n  tag_set |= set(range(0xe0061, 0xe007b))  # a-z\n  tag_set.add(END_TAG)\n  return tag_set\n\nTAG_SET = _make_tag_set()\n\n_namedata = None\n\ndef seq_name(seq):\n  global _namedata\n\n  if not _namedata:\n    def strip_vs_map(seq_map):\n      return {\n          unicode_data.strip_emoji_vs(k): v\n          for k, v in seq_map.items()}\n    _namedata = [\n        strip_vs_map(unicode_data.get_emoji_combining_sequences()),\n        strip_vs_map(unicode_data.get_emoji_flag_sequences()),\n        strip_vs_map(unicode_data.get_emoji_modifier_sequences()),\n        strip_vs_map(unicode_data.get_emoji_zwj_sequences()),\n        ]\n\n  if len(seq) == 1:\n    return unicode_data.name(seq[0], None)\n\n  for data in _namedata:\n    if seq in data:\n      return data[seq]\n  if EMOJI_VS in seq:\n    non_vs_seq = unicode_data.strip_emoji_vs(seq)\n    for data in _namedata:\n      if non_vs_seq in data:\n        return data[non_vs_seq]\n\n  return None\n\n\ndef _check_no_vs(sorted_seq_to_filepath):\n  \"\"\"Our image data does not use emoji presentation variation selectors.\"\"\"\n  for seq, fp in sorted_seq_to_filepath.items():\n    if EMOJI_VS in seq:\n      print(f'check no VS: {EMOJI_VS} in path: {fp}')\n\n\ndef _check_valid_emoji_cps(sorted_seq_to_filepath, unicode_version):\n  \"\"\"Ensure all cps in these sequences are valid emoji cps or specific cps\n  used in forming emoji sequences.  This is a 'pre-check' that reports\n  this specific problem.\"\"\"\n\n  coverage_pass = True\n\n  valid_cps = set(unicode_data.get_emoji())\n  if unicode_version is None or unicode_version >= unicode_data.PROPOSED_EMOJI_AGE:\n    valid_cps |= unicode_data.proposed_emoji_cps()\n  else:\n    valid_cps = set(\n        cp for cp in valid_cps if unicode_data.age(cp) <= unicode_version)\n  valid_cps.add(0x200d)  # ZWJ\n  valid_cps.add(0x20e3)  # combining enclosing keycap\n  valid_cps.add(0xfe0f)  # variation selector (emoji presentation)\n  valid_cps.add(0xfe82b)  # PUA value for unknown flag\n  valid_cps |= TAG_SET  # used in subregion tag sequences\n\n  not_emoji = {}\n  for seq, fp in sorted_seq_to_filepath.items():\n    for cp in seq:\n      if cp not in valid_cps:\n        if cp not in not_emoji:\n          not_emoji[cp] = []\n        not_emoji[cp].append(fp)\n\n  if len(not_emoji):\n    print(\n        f'check valid emoji cps: {len(not_emoji)} non-emoji cp found', file=sys.stderr)\n    for cp in sorted(not_emoji):\n      fps = not_emoji[cp]\n      print(\n          f'check the following cp: {cp} - {not_emoji.get(cp)[0]} (in {len(fps)} sequences)', file=sys.stderr)\n    coverage_pass = False\n\n  if not coverage_pass:\n    exit(\"Please fix the problems mentioned above or run: make BYPASS_SEQUENCE_CHECK='True'\")\n\n\ndef _check_zwj(sorted_seq_to_filepath):\n  \"\"\"Ensure zwj is only between two appropriate emoji.  This is a 'pre-check'\n  that reports this specific problem.\"\"\"\n\n  for seq, fp in sorted_seq_to_filepath.items():\n    if ZWJ not in seq:\n      continue\n    if seq[0] == ZWJ:\n      print(f'check zwj: zwj at head of sequence in {fp}', file=sys.stderr)\n    if len(seq) == 1:\n      continue\n    if seq[-1] == ZWJ:\n      print(f'check zwj: zwj at end of sequence in {fp}', file=sys.stderr)\n    for i, cp in enumerate(seq):\n      if cp == ZWJ:\n        if i > 0:\n          pcp = seq[i-1]\n          if pcp != EMOJI_VS and not unicode_data.is_emoji(pcp):\n            print(\n                f'check zwj: non-emoji {pcp} precedes ZWJ in {fp}',\n                file=sys.stderr)\n        if i < len(seq) - 1:\n          fcp = seq[i+1]\n          if not unicode_data.is_emoji(fcp):\n            print(\n                f'check zwj: non-emoji {fcp} follows ZWJ in {fp}',\n                file=sys.stderr)\n\n\ndef _check_flags(sorted_seq_to_filepath):\n  \"\"\"Ensure regional indicators are only in sequences of one or two, and\n  never mixed.\"\"\"\n  for seq, fp in sorted_seq_to_filepath.items():\n    have_reg = None\n    for cp in seq:\n      is_reg = unicode_data.is_regional_indicator(cp)\n      if have_reg == None:\n        have_reg = is_reg\n      elif have_reg != is_reg:\n        print(\n            f'check flags: mix of regional and non-regional in {fp}',\n            file=sys.stderr)\n    if have_reg and len(seq) > 2:\n      # We provide dummy glyphs for regional indicators, so there are sequences\n      # with single regional indicator symbols, the len check handles this.\n      print(\n          f'check flags: regional indicator sequence length != 2 in {fp}',\n          file=sys.stderr)\n\ndef _check_tags(sorted_seq_to_filepath):\n  \"\"\"Ensure tag sequences (for subregion flags) conform to the spec.  We don't\n  validate against CLDR, just that there's a sequence of 2 or more tags starting\n  and ending with the appropriate codepoints.\"\"\"\n\n  BLACK_FLAG = 0x1f3f4\n  BLACK_FLAG_SET = set([BLACK_FLAG])\n  for seq, fp in sorted_seq_to_filepath.items():\n    seq_set = set(cp for cp in seq)\n    overlap_set = seq_set & TAG_SET\n    if not overlap_set:\n      continue\n    if seq[0] != BLACK_FLAG:\n      print(f'check tags: bad start tag in {fp}')\n    elif seq[-1] != END_TAG:\n      print(f'check tags: bad end tag in {fp}')\n    elif len(seq) < 4:\n      print(f'check tags: sequence too short in {fp}')\n    elif seq_set - TAG_SET != BLACK_FLAG_SET:\n      print(f'check tags: non-tag items in {fp}')\n\n\ndef _check_skintone(sorted_seq_to_filepath):\n  \"\"\"Ensure skin tone modifiers are not applied to emoji that are not defined\n  to take them.  May appear standalone, though.  Also check that emoji that take\n  skin tone modifiers have a complete set.\"\"\"\n  base_to_modifiers = collections.defaultdict(set)\n  for seq, fp in sorted_seq_to_filepath.items():\n    for i, cp in enumerate(seq):\n      if unicode_data.is_skintone_modifier(cp):\n        if i == 0:\n          if len(seq) > 1:\n            print(\n                f'check skintone: skin color selector first in sequence {fp}',\n                file=sys.stderr)\n          # standalone are ok\n          continue\n        pcp = seq[i-1]\n        if not unicode_data.is_emoji_modifier_base(pcp):\n          print(\n              f'check skintone: emoji skintone modifier applied to non-base at {i}: {fp}',\n              file=sys.stderr)\n        else:\n          if pcp not in base_to_modifiers:\n            base_to_modifiers[pcp] = set()\n          base_to_modifiers[pcp].add(cp)\n\n  for cp, modifiers in sorted(base_to_modifiers.items()):\n    if len(modifiers) != 5:\n      print(\n          'check skintone: base %04x has %d modifiers defined (%s) in %s' % (\n              cp, len(modifiers),\n              ', '.join('%04x' % cp for cp in sorted(modifiers)), fp),\n          file=sys.stderr)\n\n\ndef _check_zwj_sequences(sorted_seq_to_filepath, unicode_version):\n  \"\"\"Verify that zwj sequences are valid for the given unicode version.\"\"\"\n  for seq, fp in sorted_seq_to_filepath.items():\n    if ZWJ not in seq:\n      continue\n    age = unicode_data.get_emoji_sequence_age(seq)\n    if age is None or unicode_version is not None and age > unicode_version:\n      print(f'check zwj sequences: undefined sequence {fp}')\n\n\ndef _check_no_alias_sources(sorted_seq_to_filepath):\n  \"\"\"Check that we don't have sequences that we expect to be aliased to\n  some other sequence.\"\"\"\n  aliases = add_aliases.read_default_emoji_aliases()\n  for seq, fp in sorted_seq_to_filepath.items():\n    if seq in aliases:\n      print(f'check no alias sources: aliased sequence {fp}')\n\n\ndef _check_coverage(seq_to_filepath, unicode_version):\n  \"\"\"Ensure we have all and only the cps and sequences that we need for the\n  font as of this version.\"\"\"\n\n  coverage_pass = True\n  age = unicode_version\n\n  non_vs_to_canonical = {}\n  for k in seq_to_filepath:\n    if EMOJI_VS in k:\n      non_vs = unicode_data.strip_emoji_vs(k)\n      non_vs_to_canonical[non_vs] = k\n\n  aliases = add_aliases.read_default_emoji_aliases()\n  for k, v in sorted(aliases.items()):\n    if v not in seq_to_filepath and v not in non_vs_to_canonical:\n      alias_str = unicode_data.seq_to_string(k)\n      target_str = unicode_data.seq_to_string(v)\n      print(f'coverage: alias {alias_str} missing target {target_str}')\n      coverage_pass = False\n      continue\n    if k in seq_to_filepath or k in non_vs_to_canonical:\n      alias_str = unicode_data.seq_to_string(k)\n      target_str = unicode_data.seq_to_string(v)\n      print(f'coverage: alias {alias_str} already exists as {target_str} ({seq_name(v)})')\n      coverage_pass = False\n      continue\n    filename = seq_to_filepath.get(v) or seq_to_filepath[non_vs_to_canonical[v]]\n    seq_to_filepath[k] = 'alias:' + filename\n\n  # check single emoji, this includes most of the special chars\n  emoji = sorted(unicode_data.get_emoji())\n  for cp in emoji:\n    if tuple([cp]) not in seq_to_filepath:\n      print(\n          f'coverage: missing single {cp} ({unicode_data.name(cp)})')\n      coverage_pass = False\n\n  # special characters\n  # all but combining enclosing keycap are currently marked as emoji\n  for cp in [ord('*'), ord('#'), ord(u'\\u20e3')] + list(range(0x30, 0x3a)):\n    if cp not in emoji and tuple([cp]) not in seq_to_filepath:\n      print(f'coverage: missing special {cp} ({unicode_data.name(cp)})')\n      coverage_pass = False\n\n  # combining sequences\n  comb_seq_to_name = sorted(\n      unicode_data._emoji_sequence_data.items())\n  for seq, name in comb_seq_to_name:\n    if seq not in seq_to_filepath:\n      # strip vs and try again\n      non_vs_seq = unicode_data.strip_emoji_vs(seq)\n      if non_vs_seq not in seq_to_filepath:\n        print(f'coverage: missing combining sequence {unicode_data.seq_to_string(seq)} ({name})')\n        coverage_pass = False\n\n  # check for 'unknown flag'\n  # this is either emoji_ufe82b or 'unknown_flag', but we filter out things that\n  # don't start with our prefix so 'unknown_flag' would be excluded by default.\n  if tuple([0xfe82b]) not in seq_to_filepath:\n    print('coverage: missing unknown flag PUA fe82b')\n    coverage_pass = False\n\n  if not coverage_pass:\n    exit(\"Please fix the problems mentioned above or run: make BYPASS_SEQUENCE_CHECK='True'\")\n\n\ndef check_sequence_to_filepath(seq_to_filepath, unicode_version, coverage):\n  sorted_seq_to_filepath = collections.OrderedDict(\n      sorted(seq_to_filepath.items()))\n  _check_no_vs(sorted_seq_to_filepath)\n  _check_valid_emoji_cps(sorted_seq_to_filepath, unicode_version)\n  _check_zwj(sorted_seq_to_filepath)\n  _check_flags(sorted_seq_to_filepath)\n  _check_tags(sorted_seq_to_filepath)\n  _check_skintone(sorted_seq_to_filepath)\n  _check_zwj_sequences(sorted_seq_to_filepath, unicode_version)\n  _check_no_alias_sources(sorted_seq_to_filepath)\n  if coverage:\n    _check_coverage(sorted_seq_to_filepath, unicode_version)\n\n\ndef create_sequence_to_filepath(name_to_dirpath, prefix, suffix):\n  \"\"\"Check names, and convert name to sequences for names that are ok,\n  returning a sequence to file path mapping.  Reports bad segments\n  of a name to stderr.\"\"\"\n  segment_re = re.compile(r'^[0-9a-f]{4,6}$')\n  result = {}\n  for name, dirname in name_to_dirpath.items():\n    if not name.startswith(prefix):\n      print(f'expected prefix \"{prefix}\" for \"{name}\"')\n      continue\n\n    segments = name[len(prefix): -len(suffix)].split('_')\n    segfail = False\n    seq = []\n    for s in segments:\n      if not segment_re.match(s):\n        print(f'bad codepoint name \"{s}\" in {dirname}/{name}')\n        segfail = True\n        continue\n      n = int(s, 16)\n      if n > 0x10ffff:\n        print(f'codepoint \"{s}\" out of range in {dirname}/{name}')\n        segfail = True\n        continue\n      seq.append(n)\n    if not segfail:\n      result[tuple(seq)] = path.join(dirname, name)\n  return result\n\n\ndef collect_name_to_dirpath(directory, prefix, suffix, exclude=None):\n  \"\"\"Return a mapping from filename to path rooted at directory, ignoring files\n  that don't match suffix, and subtrees with names in exclude.  Report when a\n  filename appears in more than one subdir; the first path found is kept.\"\"\"\n  result = {}\n  for dirname, dirs, files in os.walk(directory, topdown=True):\n    if exclude:\n      dirs[:] = [d for d in dirs if d not in exclude]\n\n    if directory != '.':\n      dirname = directory\n    for f in files:\n      if not f.endswith(suffix):\n        continue\n      if f in result:\n        print('duplicate file \"%s\" in %s and %s ' % (\n            f, dirname, result[f]), file=sys.stderr)\n        continue\n      result[f] = dirname\n  return result\n\n\ndef collect_name_to_dirpath_with_override(dirs, prefix, suffix, exclude=None):\n  \"\"\"Return a mapping from filename to a directory path rooted at a directory\n  in dirs, using collect_name_to_filepath.  The last directory is retained. This\n  does not report an error if a file appears under more than one root directory,\n  so lets later root directories override earlier ones.  Use 'exclude' to\n  name subdirectories (of any root) whose subtree you wish to skip.\"\"\"\n  result = {}\n  for d in dirs:\n    result.update(collect_name_to_dirpath(d, prefix, suffix, exclude))\n  return result\n\n\ndef run_check(dirs, names, prefix, suffix, exclude, unicode_version, coverage):\n  msg = ''\n  if unicode_version:\n    msg = ' (%3.1f)' % unicode_version\n\n  if (names and dirs):\n    sys.exit(\"Please only provide a directory or a list of names\")\n  elif names:\n    name_to_dirpath = {}\n    for name in names:\n      name_to_dirpath[name] = \"\"\n  elif dirs:\n    print(f'Checking files with prefix \"{prefix}\" and suffix \"{suffix}\"{msg} in: {dirs}')\n    name_to_dirpath = collect_name_to_dirpath_with_override(dirs, prefix=prefix, suffix=suffix, exclude=exclude)\n\n  print(f'checking {len(name_to_dirpath)} names')\n  seq_to_filepath = create_sequence_to_filepath(name_to_dirpath, prefix, suffix)\n  print(f'checking {len(seq_to_filepath)} sequences')\n  check_sequence_to_filepath(seq_to_filepath, unicode_version, coverage)\n  print('Done running checks')\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-d', '--dirs', help='directory roots containing emoji images',\n      metavar='dir', nargs='+')\n  parser.add_argument(\n      '-n', '--names', help='list with expected emoji',\n      metavar='names', nargs='+')\n  parser.add_argument(\n      '-e', '--exclude', help='names of source subdirs to exclude',\n      metavar='dir', nargs='+')\n  parser.add_argument(\n      '-c', '--coverage', help='test for complete coverage',\n      action='store_true')\n  parser.add_argument(\n      '-p', '--prefix', help='prefix to match, default \"emoji_u\"',\n      metavar='pfx', default='emoji_u')\n  parser.add_argument(\n      '-s', '--suffix', help='suffix to match, default \".png\"', metavar='sfx',\n      default='.png')\n  parser.add_argument(\n      '-u', '--unicode_version', help='limit to this unicode version or before',\n      metavar='version', type=float)\n  args = parser.parse_args()\n  run_check(\n      args.dirs, args.names, args.prefix, args.suffix, args.exclude, args.unicode_version,\n      args.coverage)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "collect_emoji_svg.py",
          "type": "blob",
          "size": 5.341796875,
          "content": "#!/usr/bin/env python3\n# Copyright 2015 Google, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Google Author(s): Doug Felt\n\n\"\"\"Tool to collect emoji svg glyphs into one directory for processing\nby add_svg_glyphs.  There are two sources, noto/color_emoji/svg and\nnoto/third_party/region-flags/svg.  The add_svg_glyphs file expects\nthe file names to contain the character string that represents it\nrepresented as a sequence of hex-encoded codepoints separated by\nunderscore.  The files in noto/color_emoji/svg do this, and have the\nprefix 'emoji_u', but the files in region-flags/svg just have the\ntwo-letter code.\n\nWe create a directory and copy the files into it with the required\nnaming convention. First we do this for region-flags/svg, converting\nthe names, and then we do this for color_emoji/svg, so any duplicates\nwill be overwritten by what we assume are the preferred svg.  We use\ncopies instead of symlinks so we can continue to optimize or modify\nthe files without messing with the originals.\"\"\"\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport os.path\nimport re\nimport shutil\nimport sys\n\nfrom nototools import tool_utils\n\ndef _is_svg(f):\n  return f.endswith('.svg')\n\n\ndef _is_svg_and_startswith_emoji(f):\n  return f.endswith('.svg') and f.startswith('emoji_u')\n\n\ndef _flag_rename(f):\n  \"\"\"Converts a file name from two-letter upper-case ASCII to our expected\n  'emoji_uXXXXX_XXXXX form, mapping each character to the corresponding\n  regional indicator symbol.\"\"\"\n\n  cp_strs = []\n  name, ext = os.path.splitext(f)\n  if len(name) != 2:\n    raise ValueError('illegal flag name \"%s\"' % f)\n  for cp in name:\n    if not ('A' <= cp <= 'Z'):\n      raise ValueError('illegal flag name \"%s\"' % f)\n    ncp = 0x1f1e6 - 0x41 + ord(cp)\n    cp_strs.append(\"%04x\" % ncp)\n  return 'emoji_u%s%s' % ('_'.join(cp_strs), ext)\n\n\ndef copy_with_rename(src_dir, dst_dir, accept_pred=None, rename=None):\n  \"\"\"Copy files from src_dir to dst_dir that match accept_pred (all if None) and\n  rename using rename (if not None), replacing existing files.  accept_pred\n  takes the filename and returns True if the file should be copied, rename takes\n  the filename and returns a new file name.\"\"\"\n\n  count = 0\n  replace_count = 0\n  for src_filename in os.listdir(src_dir):\n    if accept_pred and not accept_pred(src_filename):\n      continue\n    dst_filename = rename(src_filename) if rename else src_filename\n    src = os.path.join(src_dir, src_filename)\n    dst = os.path.join(dst_dir, dst_filename)\n    if os.path.exists(dst):\n      logging.debug('Replacing existing file %s', dst)\n      os.unlink(dst)\n      replace_count += 1\n    shutil.copy2(src, dst)\n    logging.debug('cp -p %s %s', src, dst)\n    count += 1\n  if logging.getLogger().getEffectiveLevel() <= logging.INFO:\n    src_short = tool_utils.short_path(src_dir)\n    dst_short = tool_utils.short_path(dst_dir)\n    logging.info('Copied %d files (replacing %d) from %s to %s',\n        count, replace_count, src_short, dst_short)\n\n\ndef build_svg_dir(dst_dir, clean=False, emoji_dir='', flags_dir=''):\n  \"\"\"Copies/renames files from emoji_dir and then flags_dir, giving them the\n  standard format and prefix ('emoji_u' followed by codepoints expressed in hex\n  separated by underscore).  If clean, removes the target dir before proceeding.\n  If either emoji_dir or flags_dir are empty, skips them.\"\"\"\n\n  dst_dir = tool_utils.ensure_dir_exists(dst_dir, clean=clean)\n\n  if not emoji_dir and not flags_dir:\n    logging.warning('Nothing to do.')\n    return\n\n  if emoji_dir:\n    copy_with_rename(\n        emoji_dir, dst_dir, accept_pred=_is_svg_and_startswith_emoji)\n\n  if flags_dir:\n     copy_with_rename(\n        flags_dir, dst_dir, accept_pred=_is_svg, rename=_flag_rename)\n\n\ndef main(argv):\n  DEFAULT_EMOJI_DIR = '[emoji]/svg'\n  DEFAULT_FLAGS_DIR = '[emoji]/third_party/region-flags/svg'\n\n  parser = argparse.ArgumentParser(\n      description='Collect svg files into target directory with prefix.')\n  parser.add_argument(\n      'dst_dir', help='Directory to hold copied files.', metavar='dir')\n  parser.add_argument(\n      '--clean', '-c', help='Replace target directory', action='store_true')\n  parser.add_argument(\n      '--flags_dir', '-f', metavar='dir', help='directory containing flag svg, '\n      'default %s' % DEFAULT_FLAGS_DIR, default=DEFAULT_FLAGS_DIR)\n  parser.add_argument(\n      '--emoji_dir', '-e', metavar='dir',\n      help='directory containing emoji svg, default %s' % DEFAULT_EMOJI_DIR,\n      default=DEFAULT_EMOJI_DIR)\n  parser.add_argument(\n      '-l', '--loglevel', help='log level name/value', default='warning')\n  args = parser.parse_args(argv)\n\n  tool_utils.setup_logging(args.loglevel)\n\n  args.flags_dir = tool_utils.resolve_path(args.flags_dir)\n  args.emoji_dir = tool_utils.resolve_path(args.emoji_dir)\n  build_svg_dir(\n      args.dst_dir, clean=args.clean, emoji_dir=args.emoji_dir,\n      flags_dir=args.flags_dir)\n\n\nif __name__ == '__main__':\n  main(sys.argv[1:])\n"
        },
        {
          "name": "colrv1",
          "type": "tree",
          "content": null
        },
        {
          "name": "colrv1_add_soft_light_to_flags.py",
          "type": "blob",
          "size": 5.388671875,
          "content": "\"\"\"Utility to add soft-light effect to NotoColorEmoji-COLRv1 region flags.\"\"\"\nimport sys\nimport subprocess\nfrom fontTools import ttLib\nfrom fontTools.ttLib.tables import otTables as ot\nfrom fontTools.ttLib.tables.C_P_A_L_ import Color\nfrom fontTools.colorLib.builder import LayerListBuilder\nfrom add_aliases import read_default_emoji_aliases\nfrom flag_glyph_name import flag_code_to_glyph_name\nfrom map_pua_emoji import get_glyph_name_from_gsub\n\n\nREGIONAL_INDICATOR_RANGE = range(0x1F1E6, 0x1F1FF + 1)\nBLACK_FLAG = 0x1F3F4\nCANCEL_TAG = 0xE007F\nTAG_RANGE = range(0xE0000, CANCEL_TAG + 1)\n\n\ndef is_flag(sequence):\n    # regular region flags are comprised of regional indicators\n    if all(cp in REGIONAL_INDICATOR_RANGE for cp in sequence):\n        return True\n\n    # subdivision flags start with  black flag, contain some tag characters and end with\n    # a cancel tag\n    if (\n        len(sequence) > 2\n        and sequence[0] == BLACK_FLAG\n        and sequence[-1] == CANCEL_TAG\n        and all(cp in TAG_RANGE for cp in sequence[1:-1])\n    ):\n        return True\n\n    return False\n\n\ndef read_makefile_variable(var_name):\n    # see `print-%` command in Makefile\n    value = subprocess.run(\n        [\"make\", f\"print-{var_name}\"], capture_output=True, check=True\n    ).stdout.decode(\"utf-8\")\n    return value[len(var_name) + len(\" = \") :].strip()\n\n\ndef flag_code_to_sequence(flag_code):\n    # I use the existing code to first convert from flag code to glyph name,\n    # and then convert names back to integer codepoints since it already\n    # handles both the region indicators and subdivision tags.\n    name = flag_code_to_glyph_name(flag_code)\n    assert name.startswith(\"u\")\n    return tuple(int(v, 16) for v in name[1:].split(\"_\"))\n\n\ndef all_flag_sequences():\n    \"\"\"Return the set of all noto-emoji's region and subdivision flag sequences.\n    These include those in SELECTED_FLAGS Makefile variable plus those listed\n    in the 'emoji_aliases.txt' file.\n    \"\"\"\n    result = {\n        flag_code_to_sequence(flag_code)\n        for flag_code in read_makefile_variable(\"SELECTED_FLAGS\").split()\n    }\n    result.update(seq for seq in read_default_emoji_aliases() if is_flag(seq))\n    return result\n\n\n_builder = LayerListBuilder()\n\n\ndef _build_paint(source):\n    return _builder.buildPaint(source)\n\n\ndef _paint_composite(source, mode, backdrop):\n    return _build_paint(\n        {\n            \"Format\": ot.PaintFormat.PaintComposite,\n            \"SourcePaint\": source,\n            \"CompositeMode\": mode,\n            \"BackdropPaint\": backdrop,\n        }\n    )\n\n\ndef _palette_index(cpal, color):\n    assert len(cpal.palettes) == 1\n    palette = cpal.palettes[0]\n    try:\n        i = palette.index(color)\n    except ValueError:\n        i = len(palette)\n        palette.append(color)\n        cpal.numPaletteEntries += 1\n        assert len(palette) == cpal.numPaletteEntries\n    return i\n\n\nWHITE = Color.fromHex(\"#FFFFFFFF\")\nGRAY = Color.fromHex(\"#808080FF\")\nBLACK = Color.fromHex(\"#000000FF\")\n\n\ndef _soft_light_gradient(cpal):\n    return _build_paint(\n        {\n            \"Format\": ot.PaintFormat.PaintLinearGradient,\n            \"ColorLine\": {\n                \"Extend\": \"pad\",\n                \"ColorStop\": [\n                    {\n                        \"StopOffset\": 0.0,\n                        \"PaletteIndex\": _palette_index(cpal, WHITE),\n                        \"Alpha\": 0.5,\n                    },\n                    {\n                        \"StopOffset\": 0.5,\n                        \"PaletteIndex\": _palette_index(cpal, GRAY),\n                        \"Alpha\": 0.5,\n                    },\n                    {\n                        \"StopOffset\": 1.0,\n                        \"PaletteIndex\": _palette_index(cpal, BLACK),\n                        \"Alpha\": 0.5,\n                    },\n                ],\n            },\n            \"x0\": 47,\n            \"y0\": 790,\n            \"x1\": 890,\n            \"y1\": -342,\n            \"x2\": -1085,\n            \"y2\": -53,\n        },\n    )\n\n\ndef flag_ligature_glyphs(font):\n    \"\"\"Yield ligature glyph names for all the region/subdivision flags in the font.\"\"\"\n    for flag_sequence in all_flag_sequences():\n        flag_name = get_glyph_name_from_gsub(flag_sequence, font)\n        if flag_name is not None:\n            yield flag_name\n\n\ndef add_soft_light_to_flags(font, flag_glyph_names=None):\n    \"\"\"Add soft-light effect to region and subdivision flags in CORLv1 font.\"\"\"\n    if flag_glyph_names is None:\n        flag_glyph_names = flag_ligature_glyphs(font)\n\n    colr_glyphs = {\n        rec.BaseGlyph: rec\n        for rec in font[\"COLR\"].table.BaseGlyphList.BaseGlyphPaintRecord\n    }\n    cpal = font[\"CPAL\"]\n\n    for flag_name in flag_glyph_names:\n        flag = colr_glyphs[flag_name]\n        flag.Paint = _paint_composite(\n            source=_paint_composite(\n                source=_soft_light_gradient(cpal),\n                mode=ot.CompositeMode.SRC_IN,\n                backdrop=flag.Paint,\n            ),\n            mode=ot.CompositeMode.SOFT_LIGHT,\n            backdrop=flag.Paint,\n        )\n\n\ndef main():\n    try:\n        input_file, output_file = sys.argv[1:]\n    except ValueError:\n        print(\"usage: colrv1_add_soft_light_to_flags.py INPUT_FONT OUTPUT_FONT\")\n        return 2\n\n    font = ttLib.TTFont(input_file)\n\n    if \"COLR\" not in font or font[\"COLR\"].version != 1:\n        print(\"error: missing required COLRv1 table\")\n        return 1\n\n    add_soft_light_to_flags(font)\n\n    font.save(output_file)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"
        },
        {
          "name": "colrv1_generate_configs.py",
          "type": "blob",
          "size": 3.0654296875,
          "content": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Generate config files for Noto-COLRv1\n\nfrom nanoemoji.util import rel\nfrom pathlib import Path\n\n\n_NOTO_FAMILY_NAME = \"Noto Color Emoji\"\n_NOTO_SVG_DIR = Path(\"svg\")\n_REGIONAL_INDICATORS = {\n    Path(_NOTO_SVG_DIR / f\"emoji_u{i:x}.svg\") for i in range(0x1F1E6, 0x1F1FF + 1)\n}\n_NOTO_WAVED_FLAG_SVG_DIR = Path(\"third_party/region-flags/waved-svg\")\n_NOTO_SUBDIVISION_FLAGS = (\n    _NOTO_WAVED_FLAG_SVG_DIR / \"emoji_u1f3f4_e0067_e0062_e0065_e006e_e0067_e007f.svg\",\n    _NOTO_WAVED_FLAG_SVG_DIR / \"emoji_u1f3f4_e0067_e0062_e0073_e0063_e0074_e007f.svg\",\n    _NOTO_WAVED_FLAG_SVG_DIR / \"emoji_u1f3f4_e0067_e0062_e0077_e006c_e0073_e007f.svg\",\n)\n_CONFIG_DIR = Path(\"colrv1\")\n\n\ndef _write_config(config_name, output_file, svg_files):\n    svg_files = [rel(_CONFIG_DIR, Path(f)) for f in svg_files]\n    config_file = f\"{config_name}.toml\"\n    for svg_file in svg_files:\n        assert _CONFIG_DIR.joinpath(\n            svg_file\n        ).is_file(), f\"{svg_file} not found relative to {_CONFIG_DIR}\"\n    svg_list = \",\\n    \".join(f'\"{f}\"' for f in sorted(svg_files)).rstrip()\n    with open(_CONFIG_DIR / config_file, \"w\") as f:\n        f.write(\n            f\"\"\"\nfamily = \"{_NOTO_FAMILY_NAME}\"\noutput_file = \"{output_file}\"\ncolor_format = \"glyf_colr_1\"\nclipbox_quantization = 32\n\n[axis.wght]\nname = \"Weight\"\ndefault = 400\n\n[master.regular]\nstyle_name = \"Regular\"\nsrcs = [\n    {svg_list},\n]\n\n[master.regular.position]\nwght = 400\n\"\"\"\n        )\n\n\ndef _write_all_noto_configs():\n    # includes all of noto-emoji svgs plus all the waved region flags\n    regular = tuple(_NOTO_SVG_DIR.glob(\"*.svg\"))\n    flags = tuple(_NOTO_WAVED_FLAG_SVG_DIR.glob(\"*.svg\"))\n\n    dups = {p.name for p in regular} & {p.name for p in flags}\n    if dups:\n        raise ValueError(f\"Flags *and* regular have {dups}\")\n\n    svgs = regular + flags\n    _write_config(\"all\", \"NotoColorEmoji.ttf\", svgs)\n\n\ndef _write_noto_noflag_configs():\n    # Does not contain regional indicators and the region flags that are\n    # composed with them. It still includes the England, Scotland and Wales\n    # 'subdivision' flags, as those are not composed with Unicode regional\n    # indicators, but use sequences of Unicode Tag letters prefixed with\n    # the Black Flag and ending with a Cancel Tag.\n    svgs = (\n        tuple(p for p in _NOTO_SVG_DIR.glob(\"*.svg\") if p not in _REGIONAL_INDICATORS)\n        + _NOTO_SUBDIVISION_FLAGS\n    )\n    _write_config(\"noflags\", \"NotoColorEmoji-noflags.ttf\", svgs)\n\n\ndef main():\n    _write_all_noto_configs()\n    _write_noto_noflag_configs()\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "colrv1_postproc.py",
          "type": "blob",
          "size": 11.392578125,
          "content": "\"\"\"\nPost-nanoemoji processing of the Noto COLRv1 Emoji files.\n\nAdds additional sequences to properly support Safari, corrects 'name', etc.\n\nFor now substantially based on copying from a correct bitmap build.\n\"\"\"\nfrom absl import app\nimport functools\nfrom fontTools.feaLib.builder import addOpenTypeFeaturesFromString\nfrom fontTools import ttLib\nfrom fontTools.ttLib.tables import _g_l_y_f as glyf\nfrom fontTools.ttLib.tables import otTables as ot\nimport map_pua_emoji\nfrom nototools import add_vs_cmap\nfrom nototools import font_data\nfrom nototools import unicode_data\nfrom pathlib import Path\nimport re\n\nfrom colrv1_add_soft_light_to_flags import add_soft_light_to_flags\n\n\n_CBDT_FILE = Path(\"fonts/NotoColorEmoji.ttf\")\n_COLR_FILES = {\n    Path(\"fonts/Noto-COLRv1-noflags.ttf\"),\n    Path(\"fonts/Noto-COLRv1.ttf\"),\n}\n\n\ndef _is_colrv1(font):\n    return \"COLR\" in font and font[\"COLR\"].version == 1\n\n\ndef _is_cbdt(font):\n    return \"CBDT\" in font\n\n\ndef _set_name(name_table, nameID):\n    name_table.getName(value, nameID, 3, 1, 0x409)\n\n\ndef _set_name(name_table, nameID, value):\n    name_table.setName(value, nameID, 3, 1, 0x409)\n\n\ndef _copy_names(colr_font, cbdt_font):\n    colr_font[\"name\"] = cbdt_font[\"name\"]\n    name_table = colr_font[\"name\"]\n    assert all(\n        (n.platformID, n.platEncID, n.langID) == (3, 1, 0x409) for n in name_table.names\n    ), \"Should only have names Android uses\"\n\n    # Amendments\n    _set_name(name_table, 10, \"Color emoji font using COLRv1.\")\n    _set_name(name_table, 11, \"https://github.com/googlefonts/noto-emoji\")\n    _set_name(name_table, 12, \"https://github.com/googlefonts/noto-emoji\")\n\n\n# CBDT build step: @$(VS_ADDER) -vs 2640 2642 2695 --dstdir '.' -o \"$@-with-pua-varsel\" \"$@-with-pua\"\ndef _add_vs_cmap(colr_font):\n    emoji_variants = unicode_data.get_unicode_emoji_variants() | {\n        0x2640,\n        0x2642,\n        0x2695,\n    }\n    add_vs_cmap.modify_font(\"COLRv1 Emoji\", colr_font, \"emoji\", emoji_variants)\n\n\ndef _is_variation_selector_cmap_table(table):\n    assert table.format in {4, 12, 14}\n    return table.format == 14\n\n\ndef _lookup_in_cmap(colr_font, codepoint):\n    result = set()\n    for table in colr_font[\"cmap\"].tables:\n        if _is_variation_selector_cmap_table(table):\n            continue\n        assert codepoint in table.cmap\n        result.add(table.cmap[codepoint])\n    assert len(result) == 1, f\"Ambiguous mapping for {codepoint}: {result}\"\n    return next(iter(result))\n\n\ndef _add_cmap_entries(colr_font, codepoint, glyph_name):\n    for table in colr_font[\"cmap\"].tables:\n        if _is_variation_selector_cmap_table(table):\n            continue\n        if not _is_bmp(codepoint) and table.format == 4:\n            continue\n        table.cmap[codepoint] = glyph_name\n        #print(f\"Map 0x{codepoint:04x} to {glyph_name}, format {table.format}\")\n\n\nFLAG_TAGS = set(range(0xE0030, 0xE0039 + 1)) | set(range(0xE0061, 0xE007A + 1))\nCANCEL_TAG = 0xE007F\n\n\ndef _map_missing_flag_tag_chars_to_empty_glyphs(colr_font):\n    # Add all tag characters used in flags + cancel tag\n    tag_cps = FLAG_TAGS | {CANCEL_TAG}\n\n    # Anything already cmap'd is fine\n    tag_cps -= set(_Cmap(colr_font).keys())\n\n    # CBDT maps these to blank glyphs\n    glyf_table = colr_font[\"glyf\"]\n    hmtx_table = colr_font[\"hmtx\"]\n    glyph_order_size = len(glyf_table.glyphOrder)\n    for cp in tag_cps:\n        #print(f\"Map 0x{cp:04x} to a blank glyf\")\n        glyph_name = f\"u{cp:04X}\"\n        assert glyph_name not in glyf_table, f\"{glyph_name} already in glyf\"\n        assert glyph_name not in hmtx_table.metrics, f\"{glyph_name} already in hmtx\"\n        glyf_table[glyph_name] = glyf.Glyph()\n        hmtx_table[glyph_name] = (0, 0)\n\n        _add_cmap_entries(colr_font, cp, glyph_name)\n\n\ndef _is_bmp(cp):\n    return cp in range(0x0000, 0xFFFF + 1)\n\n\ndef _ligaset_for_glyph(lookup_list, glyph_name):\n    for lookup in lookup_list.Lookup:\n        if lookup.LookupType != 4:\n            continue\n        for liga_set in lookup.SubTable:\n            if glyph_name in liga_set.ligatures:\n                return liga_set.ligatures[glyph_name]\n    return None\n\n\ndef _Cmap(ttfont):\n    def _Reducer(acc, u):\n        acc.update(u)\n        return acc\n\n    unicode_cmaps = (t.cmap for t in ttfont[\"cmap\"].tables if t.isUnicode())\n    return functools.reduce(_Reducer, unicode_cmaps, {})\n\n\ndef _add_vertical_layout_tables(cbdt_font, colr_font):\n    upem_scale = colr_font[\"head\"].unitsPerEm / cbdt_font[\"head\"].unitsPerEm\n\n    vhea = colr_font[\"vhea\"] = ttLib.newTable(\"vhea\")\n    vhea.tableVersion = 0x00010000\n    vhea.ascent = round(cbdt_font[\"vhea\"].ascent * upem_scale)\n    vhea.descent = round(cbdt_font[\"vhea\"].descent * upem_scale)\n    vhea.lineGap = 0\n    # most of the stuff below is recalculated by the compiler, but still needs to be\n    # initialized... Â¯\\_(ãƒ„)_/Â¯\n    vhea.advanceHeightMax = 0\n    vhea.minTopSideBearing = 0\n    vhea.minBottomSideBearing = 0\n    vhea.yMaxExtent = 0\n    vhea.caretSlopeRise = 0\n    vhea.caretSlopeRun = 0\n    vhea.caretOffset = 0\n    vhea.reserved0 = 0\n    vhea.reserved1 = 0\n    vhea.reserved2 = 0\n    vhea.reserved3 = 0\n    vhea.reserved4 = 0\n    vhea.metricDataFormat = 0\n    vhea.numberOfVMetrics = 0\n\n    # emoji font is monospaced -- except for an odd uni0000 (NULL) glyph which happens\n    # to have height=0; but colrv1 font doesn't have that anyway, so I just skip it\n    cbdt_heights = set(h for h, _ in cbdt_font[\"vmtx\"].metrics.values() if h != 0)\n    assert len(cbdt_heights) == 1, \"NotoColorEmoji CBDT font should be monospaced!\"\n    height = round(cbdt_heights.pop() * upem_scale)\n    vmtx = colr_font[\"vmtx\"] = ttLib.newTable(\"vmtx\")\n    vmtx.metrics = {}\n    for gn in colr_font.getGlyphOrder():\n        vmtx.metrics[gn] = height, 0\n\n\nUNKNOWN_FLAG_PUA = 0xFE82B\nBLACK_FLAG = 0x1F3F4\nREGIONAL_INDICATORS = set(range(0x1F1E6, 0x1F1FF + 1))\n\n\ndef _add_fallback_subs_for_unknown_flags(colr_font):\n    \"\"\"Add GSUB lookups to replace unsupported flag sequences with the 'unknown flag'.\n\n    In order to locate the unknown flag, the glyph must be mapped to 0xFE82B PUA code;\n    the latter is removed from the cmap table after the GSUB has been updated.\n    \"\"\"\n    cmap = _Cmap(colr_font)\n    unknown_flag = cmap[UNKNOWN_FLAG_PUA]\n    black_flag = cmap[BLACK_FLAG]\n    cancel_tag = cmap[CANCEL_TAG]\n    flag_tags = sorted(cmap[cp] for cp in FLAG_TAGS)\n    # in the *-noflags.ttf font there are no region flags thus this list is empty\n    regional_indicators = sorted(cmap[cp] for cp in REGIONAL_INDICATORS if cp in cmap)\n\n    classes = f'@FLAG_TAGS = [{\" \".join(flag_tags)}];\\n'\n    if regional_indicators:\n        classes += f\"\"\"\n            @REGIONAL_INDICATORS = [{\" \".join(regional_indicators)}];\n            @UNKNOWN_FLAG = [{\" \".join([unknown_flag] * len(regional_indicators))}];\n        \"\"\"\n    lookups = (\n        # the first lookup is a dummy that stands for the emoji sequences ligatures\n        # from the destination font; we only use it to ensure the lookup indices match.\n        # We can't leave it empty otherwise feaLib optimizes it away.\n        f\"\"\"\n        lookup placeholder {{\n            sub {unknown_flag} {unknown_flag} by {unknown_flag};\n        }} placeholder;\n        \"\"\"\n        + \"\\n\".join(\n            [\"lookup delete_glyph {\"]\n            + [f\"    sub {g} by NULL;\" for g in sorted(regional_indicators + flag_tags)]\n            + [\"} delete_glyph;\"]\n        )\n        + (\n            \"\"\"\n            lookup replace_with_unknown_flag {\n                sub @REGIONAL_INDICATORS by @UNKNOWN_FLAG;\n            } replace_with_unknown_flag;\n            \"\"\"\n            if regional_indicators\n            else \"\\n\"\n        )\n    )\n    features = (\n        \"languagesystem DFLT dflt;\\n\"\n        + classes\n        + lookups\n        + \"feature ccmp {\"\n        + f\"\"\"\n            lookup placeholder;\n            sub {black_flag} @FLAG_TAGS' lookup delete_glyph;\n            sub {black_flag} {cancel_tag} by {unknown_flag};\n        \"\"\"\n        + (\n            \"\"\"\n            sub @REGIONAL_INDICATORS' lookup replace_with_unknown_flag\n                @REGIONAL_INDICATORS' lookup delete_glyph;\n            \"\"\"\n            if regional_indicators\n            else \"\"\n        )\n        + \"} ccmp;\"\n    )\n    # feaLib always builds a new GSUB table (can't update one in place) so we have to\n    # use an empty TTFont and then update our GSUB with the newly built lookups\n    temp_font = ttLib.TTFont()\n    temp_font.setGlyphOrder(colr_font.getGlyphOrder())\n\n    addOpenTypeFeaturesFromString(temp_font, features)\n\n    temp_gsub = temp_font[\"GSUB\"].table\n    # sanity check\n    assert len(temp_gsub.FeatureList.FeatureRecord) == 1\n    assert temp_gsub.FeatureList.FeatureRecord[0].FeatureTag == \"ccmp\"\n    temp_ccmp = temp_gsub.FeatureList.FeatureRecord[0].Feature\n\n    colr_gsub = colr_font[\"GSUB\"].table\n    ccmps = [\n        r.Feature for r in colr_gsub.FeatureList.FeatureRecord if r.FeatureTag == \"ccmp\"\n    ]\n    assert len(ccmps) == 1, f\"expected only 1 'ccmp' feature record, found {len(ccmps)}\"\n    colr_ccmp = ccmps[0]\n\n    colr_lookups = colr_gsub.LookupList.Lookup\n    assert (\n        len(colr_lookups) == 1\n    ), f\"expected only 1 lookup in COLRv1's GSUB.LookupList, found {len(colr_lookups)}\"\n    assert (\n        colr_lookups[0].LookupType == 4\n    ), f\"expected Lookup[0] of type 4 in COLRv1, found {colr_lookups[0].LookupType}\"\n\n    colr_lookups.extend(temp_gsub.LookupList.Lookup[1:])\n    colr_gsub.LookupList.LookupCount = len(colr_lookups)\n    colr_ccmp.LookupListIndex = temp_ccmp.LookupListIndex\n    colr_ccmp.LookupCount = len(colr_ccmp.LookupListIndex)\n\n    # get rid of the Unknown Flag private codepoint as no longer needed\n    font_data.delete_from_cmap(colr_font, [UNKNOWN_FLAG_PUA])\n\n\ndef _set_no_font_embedding_restrictions(colr_font):\n    # The CBDT/CBLC NotoColorEmoji has OS/2.fsType = 0 (i.e. no embedding restrictions)\n    # so the COLRv1 variant must also have no such restrictions.\n    # https://github.com/notofonts/noto-fonts/issues/2408\n    # https://github.com/google/fonts/issues/5729\n    colr_font[\"OS/2\"].fsType = 0\n\n\ndef _set_head_version_to_name_version(colr_font):\n    # head.fontRevision and the version on name 5 should match\n    name_version = colr_font['name'].getName(5, 3, 1, 0x409)\n    assert name_version is not None, \"No version found in 'name'\"\n    name_version = name_version.toUnicode()\n    match = re.match(r'^Version (\\d+[.]\\d+);GOOG;', name_version)\n    assert match is not None, f\"Unable to parse version from '{name_version}'\"\n    colr_font[\"head\"].fontRevision = float(match.group(1))\n\n\ndef _font(path, check_fn, check_fail_str):\n    assert path.is_file(), path\n    font = ttLib.TTFont(path)\n    if not check_fn(font):\n        raise ValueError(path + check_fail_str)\n    return font\n\n\ndef main(_):\n    cbdt_font = _font(_CBDT_FILE, _is_cbdt, \" must be a CBDT font\")\n\n    for colr_file in _COLR_FILES:\n        colr_font = _font(colr_file, _is_colrv1, \" must be a COLRv1 font\")\n\n        print(f\"Updating {colr_file} from {_CBDT_FILE}\")\n\n        _copy_names(colr_font, cbdt_font)\n\n        # CBDT build step: @$(PYTHON) $(PUA_ADDER) \"$@\" \"$@-with-pua\"\n        map_pua_emoji.add_pua_cmap_to_font(colr_font)\n\n        _add_vs_cmap(colr_font)\n\n        _map_missing_flag_tag_chars_to_empty_glyphs(colr_font)\n\n        add_soft_light_to_flags(colr_font)\n\n        _add_vertical_layout_tables(cbdt_font, colr_font)\n\n        _add_fallback_subs_for_unknown_flags(colr_font)\n\n        _set_no_font_embedding_restrictions(colr_font)\n\n        _set_head_version_to_name_version(colr_font)\n\n        print(\"Writing\", colr_file)\n        colr_font.save(colr_file)\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n"
        },
        {
          "name": "drop_flags.py",
          "type": "blob",
          "size": 1.2939453125,
          "content": "\"\"\"Removes regional indicators from a font.\"\"\"\n\nfrom fontTools import subset\nfrom fontTools import ttLib\nimport functools\nfrom pathlib import Path\nimport sys\nfrom typing import Set\n\n\ndef codepoints(font: ttLib.TTFont) -> Set[int]:\n    unicode_cmaps = (t.cmap.keys() for t in font['cmap'].tables if t.isUnicode())\n    return functools.reduce(lambda acc, u: acc | u, unicode_cmaps, set())\n\n\ndef is_regional_indicator(cp: int) -> bool:\n    return 0x1F1E6 <= cp <= 0x1F1FF\n\n\ndef main(argv):\n    for font_file in sorted(argv[1:]):\n        font_file = Path(font_file)\n        assert font_file.is_file(), font_file\n        noflags_file = font_file.with_stem(font_file.stem + \"-noflags\")\n\n        if noflags_file.is_file():\n            print(font_file, \"already has\", noflags_file, \"; nop\")\n            continue\n\n        font = ttLib.TTFont(font_file)\n\n        cps = codepoints(font)\n        cps_without_flags = {cp for cp in cps if not is_regional_indicator(cp)}\n\n        if cps == cps_without_flags:\n            print(font_file, \"has no regional indicators\")\n            continue\n\n        subsetter = subset.Subsetter()\n        subsetter.populate(unicodes=cps_without_flags)\n        subsetter.subset(font)\n\n        font.save(noflags_file)\n        print(font_file, \"=>\" , noflags_file)\n\n\nif __name__ == '__main__':\n  main(sys.argv)\n"
        },
        {
          "name": "emoji_aliases.txt",
          "type": "blob",
          "size": 0.462890625,
          "content": "# alias table\n# from;to\n# the 'from' sequence should be represented by the image for the 'to' sequence\n# 'fe0f' is not in these sequences\nfe82b;unknown_flag # no name -> no name\n\n# flag aliases\n1f1e7_1f1fb;1f1f3_1f1f4 # BV -> NO\n1f1e8_1f1f5;1f1eb_1f1f7 # CP -> FR\n1f1e9_1f1ec;1f1ee_1f1f4 # DG -> IO\n1f1ea_1f1e6;1f1ea_1f1f8 # EA -> ES\n1f1ed_1f1f2;1f1e6_1f1fa # HM -> AU\n1f1f2_1f1eb;1f1eb_1f1f7 # MF -> FR\n1f1f8_1f1ef;1f1f3_1f1f4 # SJ -> NO\n1f1fa_1f1f2;1f1fa_1f1f8 # UM -> US\n"
        },
        {
          "name": "emoji_annotations.txt",
          "type": "blob",
          "size": 10.1748046875,
          "content": "# annotations\n###\n### aliases\n###\nannotation: ok\n1f3c3 # RUNNER -> man running\n1f3c3 1f3fb # light skin tone\n1f3c3 1f3fc # medium-light skin tone\n1f3c3 1f3fd # medium skin tone\n1f3c3 1f3fe # medium-dark skin tone\n1f3c3 1f3ff # dark skin tone\n1f3c4 # SURFER -> man surfing\n1f3c4 1f3fb # light skin tone\n1f3c4 1f3fc # medium-light skin tone\n1f3c4 1f3fd # medium skin tone\n1f3c4 1f3fe # medium-dark skin tone\n1f3c4 1f3ff # dark skin tone\n1f3ca # SWIMMER -> man swimming\n1f3ca 1f3fb # light skin tone\n1f3ca 1f3fc # medium-light skin tone\n1f3ca 1f3fd # medium skin tone\n1f3ca 1f3fe # medium-dark skin tone\n1f3ca 1f3ff # dark skin tone\n1f3cb # WEIGHT LIFTER -> man lifting weights\n1f3cb 1f3fb # light skin tone\n1f3cb 1f3fc # medium-light skin tone\n1f3cb 1f3fd # medium skin tone\n1f3cb 1f3fe # medium-dark skin tone\n1f3cb 1f3ff # dark skin tone\n1f3cc # GOLFER -> man golfing\n1f3cc 1f3fb # light skin tone\n1f3cc 1f3fc # medium-light skin tone\n1f3cc 1f3fd # medium skin tone\n1f3cc 1f3fe # medium-dark skin tone\n1f3cc 1f3ff # dark skin tone\n1f46a # FAMILY -> family: man, woman, boy\n1f46e # POLICE OFFICER -> man police officer\n1f46e 1f3fb # light skin tone\n1f46e 1f3fc # medium-light skin tone\n1f46e 1f3fd # medium skin tone\n1f46e 1f3fe # medium-dark skin tone\n1f46e 1f3ff # dark skin tone\n1f46f # WOMAN WITH BUNNY EARS -> women with bunny ears partying\n1f471 # PERSON WITH BLOND HAIR -> blond-haired man\n1f471 1f3fb # light skin tone\n1f471 1f3fc # medium-light skin tone\n1f471 1f3fd # medium skin tone\n1f471 1f3fe # medium-dark skin tone\n1f471 1f3ff # dark skin tone\n1f473 # MAN WITH TURBAN -> man wearing turban\n1f473 1f3fb # light skin tone\n1f473 1f3fc # medium-light skin tone\n1f473 1f3fd # medium skin tone\n1f473 1f3fe # medium-dark skin tone\n1f473 1f3ff # dark skin tone\n1f477 # CONSTRUCTION WORKER -> man construction worker\n1f477 1f3fb # light skin tone\n1f477 1f3fc # medium-light skin tone\n1f477 1f3fd # medium skin tone\n1f477 1f3fe # medium-dark skin tone\n1f477 1f3ff # dark skin tone\n1f481 # INFORMATION DESK PERSON -> woman tipping hand\n1f481 1f3fb # light skin tone\n1f481 1f3fc # medium-light skin tone\n1f481 1f3fd # medium skin tone\n1f481 1f3fe # medium-dark skin tone\n1f481 1f3ff # dark skin tone\n1f482 # GUARDSMAN -> man guard\n1f482 1f3fb # light skin tone\n1f482 1f3fc # medium-light skin tone\n1f482 1f3fd # medium skin tone\n1f482 1f3fe # medium-dark skin tone\n1f482 1f3ff # dark skin tone\n1f486 # FACE MASSAGE -> woman getting massage\n1f486 1f3fb # light skin tone\n1f486 1f3fc # medium-light skin tone\n1f486 1f3fd # medium skin tone\n1f486 1f3fe # medium-dark skin tone\n1f486 1f3ff # dark skin tone\n1f487 # HAIRCUT -> woman getting haircut\n1f487 1f3fb # light skin tone\n1f487 1f3fc # medium-light skin tone\n1f487 1f3fd # medium skin tone\n1f487 1f3fe # medium-dark skin tone\n1f487 1f3ff # dark skin tone\n1f48f # KISS -> kiss: woman, man\n1f491 # COUPLE WITH HEART -> couple with heart: woman, man\n1f575 # SLEUTH OR SPY -> man detective\n1f575 1f3fb # light skin tone\n1f575 1f3fc # medium-light skin tone\n1f575 1f3fd # medium skin tone\n1f575 1f3fe # medium-dark skin tone\n1f575 1f3ff # dark skin tone\n1f645 # FACE WITH NO GOOD GESTURE -> woman gesturing NO\n1f645 1f3fb # light skin tone\n1f645 1f3fc # medium-light skin tone\n1f645 1f3fd # medium skin tone\n1f645 1f3fe # medium-dark skin tone\n1f645 1f3ff # dark skin tone\n1f646 # FACE WITH OK GESTURE -> woman gesturing OK\n1f646 1f3fb # light skin tone\n1f646 1f3fc # medium-light skin tone\n1f646 1f3fd # medium skin tone\n1f646 1f3fe # medium-dark skin tone\n1f646 1f3ff # dark skin tone\n1f647 # PERSON BOWING DEEPLY -> man bowing\n1f647 1f3fb # light skin tone\n1f647 1f3fc # medium-light skin tone\n1f647 1f3fd # medium skin tone\n1f647 1f3fe # medium-dark skin tone\n1f647 1f3ff # dark skin tone\n1f64b # HAPPY PERSON RAISING ONE HAND -> woman raising hand\n1f64b 1f3fb # light skin tone\n1f64b 1f3fc # medium-light skin tone\n1f64b 1f3fd # medium skin tone\n1f64b 1f3fe # medium-dark skin tone\n1f64b 1f3ff # dark skin tone\n1f64d # PERSON FROWNING -> woman frowning\n1f64d 1f3fb # light skin tone\n1f64d 1f3fc # medium-light skin tone\n1f64d 1f3fd # medium skin tone\n1f64d 1f3fe # medium-dark skin tone\n1f64d 1f3ff # dark skin tone\n1f64e # PERSON WITH POUTING FACE -> woman pouting\n1f64e 1f3fb # light skin tone\n1f64e 1f3fc # medium-light skin tone\n1f64e 1f3fd # medium skin tone\n1f64e 1f3fe # medium-dark skin tone\n1f64e 1f3ff # dark skin tone\n1f6a3 # ROWBOAT -> man rowing boat\n1f6a3 1f3fb # light skin tone\n1f6a3 1f3fc # medium-light skin tone\n1f6a3 1f3fd # medium skin tone\n1f6a3 1f3fe # medium-dark skin tone\n1f6a3 1f3ff # dark skin tone\n1f6b4 # BICYCLIST -> man biking\n1f6b4 1f3fb # light skin tone\n1f6b4 1f3fc # medium-light skin tone\n1f6b4 1f3fd # medium skin tone\n1f6b4 1f3fe # medium-dark skin tone\n1f6b4 1f3ff # dark skin tone\n1f6b5 # MOUNTAIN BICYCLIST -> man mountain biking\n1f6b5 1f3fb # light skin tone\n1f6b5 1f3fc # medium-light skin tone\n1f6b5 1f3fd # medium skin tone\n1f6b5 1f3fe # medium-dark skin tone\n1f6b5 1f3ff # dark skin tone\n1f6b6 # PEDESTRIAN -> man walking\n1f6b6 1f3fb # light skin tone\n1f6b6 1f3fc # medium-light skin tone\n1f6b6 1f3fd # medium skin tone\n1f6b6 1f3fe # medium-dark skin tone\n1f6b6 1f3ff # dark skin tone\n1f926 # FACE PALM -> woman facepalming\n1f926 1f3fb # light skin tone\n1f926 1f3fc # medium-light skin tone\n1f926 1f3fd # medium skin tone\n1f926 1f3fe # medium-dark skin tone\n1f926 1f3ff # dark skin tone\n1f937 # SHRUG -> woman shrugging\n1f937 1f3fb # light skin tone\n1f937 1f3fc # medium-light skin tone\n1f937 1f3fd # medium skin tone\n1f937 1f3fe # medium-dark skin tone\n1f937 1f3ff # dark skin tone\n1f938 # PERSON DOING CARTWHEEL -> man cartwheeling\n1f938 1f3fb # light skin tone\n1f938 1f3fc # medium-light skin tone\n1f938 1f3fd # medium skin tone\n1f938 1f3fe # medium-dark skin tone\n1f938 1f3ff # dark skin tone\n1f939 # JUGGLING -> man juggling\n1f939 1f3fb # light skin tone\n1f939 1f3fc # medium-light skin tone\n1f939 1f3fd # medium skin tone\n1f939 1f3fe # medium-dark skin tone\n1f939 1f3ff # dark skin tone\n1f93c # WRESTLERS -> men wrestling\n1f93d # WATER POLO -> man playing water polo\n1f93d 1f3fb # light skin tone\n1f93d 1f3fc # medium-light skin tone\n1f93d 1f3fd # medium skin tone\n1f93d 1f3fe # medium-dark skin tone\n1f93d 1f3ff # dark skin tone\n1f93e # HANDBALL -> man playing handball\n1f93e 1f3fb # light skin tone\n1f93e 1f3fc # medium-light skin tone\n1f93e 1f3fd # medium skin tone\n1f93e 1f3fe # medium-dark skin tone\n1f93e 1f3ff # dark skin tone\n26f9 # PERSON WITH BALL -> man bouncing ball\n26f9 1f3fb # light skin tone\n26f9 1f3fc # medium-light skin tone\n26f9 1f3fd # medium skin tone\n26f9 1f3fe # medium-dark skin tone\n26f9 1f3ff # dark skin tone\nfe82b # no name -> no name\n\n# flag aliases\n1f1e7 1f1fb # BV -> NO\n1f1e8 1f1f5 # CP -> FR\n1f1ed 1f1f2 # HM -> AU\n1f1f8 1f1ef # SJ -> NO\n1f1fa 1f1f2 # UM -> US\n\n###\n### unwanted flags\n###\nannotation: error\n1f1e7 1f1f1\n1f1e7 1f1f6\n1f1e9 1f1ec\n1f1ea 1f1e6\n1f1ea 1f1ed\n1f1eb 1f1f0\n1f1ec 1f1eb\n1f1ec 1f1f5\n1f1ec 1f1f8\n1f1f2 1f1eb\n1f1f2 1f1f6\n1f1f3 1f1e8\n1f1f5 1f1f2\n1f1f7 1f1ea\n1f1f9 1f1eb\n1f1fc 1f1eb\n1f1fd 1f1f0\n1f1fe 1f1f9\n\n###\n### new emoji\n###\nannotation: warning\n1f6f7\n1f6f8\n1f91f\n1f91f 1f3fb\n1f91f 1f3fc\n1f91f 1f3fd\n1f91f 1f3fe\n1f91f 1f3ff\n1f928\n1f929\n1f92a\n1f92b\n1f92c\n1f92d\n1f92e\n1f92f\n1f931\n1f931 1f3fb\n1f931 1f3fc\n1f931 1f3fd\n1f931 1f3fe\n1f931 1f3ff\n1f932\n1f932 1f3fb\n1f932 1f3fc\n1f932 1f3fd\n1f932 1f3fe\n1f932 1f3ff\n1f94c\n1f961\n1f962\n1f964\n1f965\n1f966\n1f995\n1f996\n1f997\n1f9d0\n1f9d1\n1f9d1 1f3fb\n1f9d1 1f3fc\n1f9d1 1f3fd\n1f9d1 1f3fe\n1f9d1 1f3ff\n1f9d2\n1f9d2 1f3fb\n1f9d2 1f3fc\n1f9d2 1f3fd\n1f9d2 1f3fe\n1f9d2 1f3ff\n1f9d3\n1f9d3 1f3fb\n1f9d3 1f3fc\n1f9d3 1f3fd\n1f9d3 1f3fe\n1f9d3 1f3ff\n1f9d4\n1f9d4 1f3fb\n1f9d4 1f3fc\n1f9d4 1f3fd\n1f9d4 1f3fe\n1f9d4 1f3ff\n1f9d5\n1f9d5 1f3fb\n1f9d5 1f3fc\n1f9d5 1f3fd\n1f9d5 1f3fe\n1f9d5 1f3ff\n1f9d6\n1f9d6 1f3fb\n1f9d6 1f3fc\n1f9d6 1f3fd\n1f9d6 1f3fe\n1f9d6 1f3ff\n1f9d6 200d 2640\n1f9d6 1f3fb 200d 2640\n1f9d6 1f3fc 200d 2640\n1f9d6 1f3fd 200d 2640\n1f9d6 1f3fe 200d 2640\n1f9d6 1f3ff 200d 2640\n1f9d6 200d 2642\n1f9d6 1f3fb 200d 2642\n1f9d6 1f3fc 200d 2642\n1f9d6 1f3fd 200d 2642\n1f9d6 1f3fe 200d 2642\n1f9d6 1f3ff 200d 2642\n1f9d7\n1f9d7 1f3fb\n1f9d7 1f3fc\n1f9d7 1f3fd\n1f9d7 1f3fe\n1f9d7 1f3ff\n1f9d7 200d 2640\n1f9d7 1f3fb 200d 2640\n1f9d7 1f3fc 200d 2640\n1f9d7 1f3fd 200d 2640\n1f9d7 1f3fe 200d 2640\n1f9d7 1f3ff 200d 2640\n1f9d7 200d 2642\n1f9d7 1f3fb 200d 2642\n1f9d7 1f3fc 200d 2642\n1f9d7 1f3fd 200d 2642\n1f9d7 1f3fe 200d 2642\n1f9d7 1f3ff 200d 2642\n1f9d8\n1f9d8 1f3fb\n1f9d8 1f3fc\n1f9d8 1f3fd\n1f9d8 1f3fe\n1f9d8 1f3ff\n1f9d8 200d 2640\n1f9d8 1f3fb 200d 2640\n1f9d8 1f3fc 200d 2640\n1f9d8 1f3fd 200d 2640\n1f9d8 1f3fe 200d 2640\n1f9d8 1f3ff 200d 2640\n1f9d8 200d 2642\n1f9d8 1f3fb 200d 2642\n1f9d8 1f3fc 200d 2642\n1f9d8 1f3fd 200d 2642\n1f9d8 1f3fe 200d 2642\n1f9d8 1f3ff 200d 2642\n1f9d9\n1f9d9 1f3fb\n1f9d9 1f3fc\n1f9d9 1f3fd\n1f9d9 1f3fe\n1f9d9 1f3ff\n1f9d9 200d 2640\n1f9d9 1f3fb 200d 2640\n1f9d9 1f3fc 200d 2640\n1f9d9 1f3fd 200d 2640\n1f9d9 1f3fe 200d 2640\n1f9d9 1f3ff 200d 2640\n1f9d9 200d 2642\n1f9d9 1f3fb 200d 2642\n1f9d9 1f3fc 200d 2642\n1f9d9 1f3fd 200d 2642\n1f9d9 1f3fe 200d 2642\n1f9d9 1f3ff 200d 2642\n1f9da\n1f9da 1f3fb\n1f9da 1f3fc\n1f9da 1f3fd\n1f9da 1f3fe\n1f9da 1f3ff\n1f9da 200d 2640\n1f9da 1f3fb 200d 2640\n1f9da 1f3fc 200d 2640\n1f9da 1f3fd 200d 2640\n1f9da 1f3fe 200d 2640\n1f9da 1f3ff 200d 2640\n1f9da 200d 2642\n1f9da 1f3fb 200d 2642\n1f9da 1f3fc 200d 2642\n1f9da 1f3fd 200d 2642\n1f9da 1f3fe 200d 2642\n1f9da 1f3ff 200d 2642\n1f9db\n1f9db 1f3fb\n1f9db 1f3fc\n1f9db 1f3fd\n1f9db 1f3fe\n1f9db 1f3ff\n1f9db 200d 2640\n1f9db 1f3fb 200d 2640\n1f9db 1f3fc 200d 2640\n1f9db 1f3fd 200d 2640\n1f9db 1f3fe 200d 2640\n1f9db 1f3ff 200d 2640\n1f9db 200d 2642\n1f9db 1f3fb 200d 2642\n1f9db 1f3fc 200d 2642\n1f9db 1f3fd 200d 2642\n1f9db 1f3fe 200d 2642\n1f9db 1f3ff 200d 2642\n1f9dc\n1f9dc 1f3fb\n1f9dc 1f3fc\n1f9dc 1f3fd\n1f9dc 1f3fe\n1f9dc 1f3ff\n1f9dc 200d 2640\n1f9dc 1f3fb 200d 2640\n1f9dc 1f3fc 200d 2640\n1f9dc 1f3fd 200d 2640\n1f9dc 1f3fe 200d 2640\n1f9dc 1f3ff 200d 2640\n1f9dc 200d 2642\n1f9dc 1f3fb 200d 2642\n1f9dc 1f3fc 200d 2642\n1f9dc 1f3fd 200d 2642\n1f9dc 1f3fe 200d 2642\n1f9dc 1f3ff 200d 2642\n1f9dd\n1f9dd 1f3fb\n1f9dd 1f3fc\n1f9dd 1f3fd\n1f9dd 1f3fe\n1f9dd 1f3ff\n1f9dd 200d 2640\n1f9dd 1f3fb 200d 2640\n1f9dd 1f3fc 200d 2640\n1f9dd 1f3fd 200d 2640\n1f9dd 1f3fe 200d 2640\n1f9dd 1f3ff 200d 2640\n1f9dd 200d 2642\n1f9dd 1f3fb 200d 2642\n1f9dd 1f3fc 200d 2642\n1f9dd 1f3fd 200d 2642\n1f9dd 1f3fe 200d 2642\n1f9dd 1f3ff 200d 2642\n1f9de\n1f9de 200d 2640\n1f9de 200d 2642\n1f9df\n1f9df 200d 2640\n1f9df 200d 2642\n1f9e0\n1f9e1\n1f9e2\n1f9e3\n1f9e4\n1f9e5\n1f9e6\n\n"
        },
        {
          "name": "fix_colr_font_revision.py",
          "type": "blob",
          "size": 0.9130859375,
          "content": "\"\"\"Set COLRv1 fontRevision from CBDT.\n\nUsed for bugfix, should fix to set properly on build instead.\n\"\"\"\n\nfrom fontTools import ttLib\nfrom pathlib import Path\nimport sys\n\n\nNAME_ID_VERSION = 5\n\n\ndef name(font, name_id):\n    return \",\".join(n.toUnicode() for n in font[\"name\"].names if n.isUnicode() and n.nameID == name_id)\n\n\ndef main():\n    colr_font_files = sorted(p for p in (Path(__file__).parent / \"fonts\").iterdir() if p.name.startswith(\"Noto-COLRv1\"))\n\n    for colr_font_file in colr_font_files:\n        cbdt_font_file = colr_font_file.with_stem(colr_font_file.stem.replace(\"Noto-COLRv1\", \"NotoColorEmoji\"))\n\n        colr_font = ttLib.TTFont(colr_font_file)\n        cbdt_font = ttLib.TTFont(cbdt_font_file)\n\n        assert \"CBDT\" in cbdt_font\n        assert \"COLR\" in colr_font\n\n        colr_font[\"head\"].fontRevision = cbdt_font[\"head\"].fontRevision\n\n        colr_font.save(colr_font_file)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "flag_glyph_name.py",
          "type": "blob",
          "size": 1.81640625,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2014 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Generate a glyph name for flag emojis.\"\"\"\nfrom __future__ import print_function\n\n__author__ = 'roozbeh@google.com (Roozbeh Pournader)'\n\nimport re\nimport sys\n\ntry:\n  import add_emoji_gsub\nexcept ImportError as e:\n    print(e, file=sys.stderr)\n    sys.exit('Python environment is not setup right')\n\ndef two_letter_code_to_glyph_name(region_code):\n    return 'u%04x_%04x' % (\n        add_emoji_gsub.reg_indicator(region_code[0]),\n        add_emoji_gsub.reg_indicator(region_code[1]))\n\n\nsubcode_re = re.compile(r'[0-9a-z]{2}-[0-9a-z]+$')\ndef hyphenated_code_to_glyph_name(sub_code):\n  # Hyphenated codes use tag sequences, not regional indicator symbol pairs.\n  sub_code = sub_code.lower()\n  if not subcode_re.match(sub_code):\n    raise Exception('%s is not a valid flag subcode' % sub_code)\n  cps = ['u1f3f4']\n  cps.extend('e00%02x' % ord(cp) for cp in sub_code if cp != '-')\n  cps.append('e007f')\n  return '_'.join(cps)\n\n\ndef flag_code_to_glyph_name(flag_code):\n  if '-' in flag_code:\n    return hyphenated_code_to_glyph_name(flag_code)\n  return two_letter_code_to_glyph_name(flag_code)\n\n\ndef main():\n    print(' '.join([\n        flag_code_to_glyph_name(flag_code) for flag_code in sys.argv[1:]]))\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "flag_info.py",
          "type": "blob",
          "size": 2.3271484375,
          "content": "#!/usr/bin/python3\n#\n# Copyright 2016 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Quick tool to display count/ids of flag images in a directory named\neither using ASCII upper case pairs or the emoji_u+codepoint_sequence\nnames.\"\"\"\nfrom __future__ import print_function\n\nimport argparse\nimport re\nimport glob\nimport os\nfrom os import path\n\ndef _flag_names_from_emoji_file_names(src):\n  def _flag_char(char_str):\n    return unichr(ord('A') + int(char_str, 16) - 0x1f1e6)\n  flag_re = re.compile('emoji_u(1f1[0-9a-f]{2})_(1f1[0-9a-f]{2}).png')\n  flags = set()\n  for f in glob.glob(path.join(src, 'emoji_u*.png')):\n    m = flag_re.match(path.basename(f))\n    if not m:\n      continue\n    flag_short_name = _flag_char(m.group(1)) + _flag_char(m.group(2))\n    flags.add(flag_short_name)\n  return flags\n\n\ndef _flag_names_from_file_names(src):\n  flag_re = re.compile('([A-Z]{2}).png')\n  flags = set()\n  for f in glob.glob(path.join(src, '*.png')):\n    m = flag_re.match(path.basename(f))\n    if not m:\n      print('no match')\n      continue\n    flags.add(m.group(1))\n  return flags\n\n\ndef _dump_flag_info(names):\n  prev = None\n  print('%d flags' % len(names))\n  for n in sorted(names):\n    if n[0] != prev:\n      if prev:\n        print()\n      prev = n[0]\n    print(n, end=' ')\n  print()\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-s', '--srcdir', help='location of files', metavar='dir',\n      required=True)\n  parser.add_argument(\n      '-n', '--name_type', help='type of names', metavar='type',\n      choices=['ascii', 'codepoint'], required=True)\n  args = parser.parse_args()\n\n  if args.name_type == 'ascii':\n    names = _flag_names_from_file_names(args.srcdir)\n  else:\n    names = _flag_names_from_emoji_file_names(args.srcdir)\n  print(args.srcdir)\n  _dump_flag_info(names)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "flags-only-unicodes.txt",
          "type": "blob",
          "size": 0.2802734375,
          "content": "U+1f1e6\nU+1f1e7\nU+1f1e8\nU+1f1e9\nU+1f1ea\nU+1f1eb\nU+1f1ec\nU+1f1ed\nU+1f1ee\nU+1f1ef\nU+1f1f0\nU+1f1f1\nU+1f1f2\nU+1f1f3\nU+1f1f4\nU+1f1f5\nU+1f1f6\nU+1f1f7\nU+1f1f8\nU+1f1f9\nU+1f1fa\nU+1f1fb\nU+1f1fc\nU+1f1fd\nU+1f1fe\nU+1f1ff\nU+fe4e5\nU+fe4e6\nU+fe4e7\nU+fe4e8\nU+fe4e9\nU+fe4ea\nU+fe4eb\nU+fe4ec\nU+fe4ed\nU+fe4ee"
        },
        {
          "name": "fonts",
          "type": "tree",
          "content": null
        },
        {
          "name": "full_rebuild.sh",
          "type": "blob",
          "size": 1.5869140625,
          "content": "#!/usr/bin/env bash\n\nset -e\nset -v\n\n# We have to have hb-subset on PATH\nwhich hb-subset\n\n# Build the CBDT font\n\nrm -rf venv  # in case you have an old borked venv!\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\nrm -rf emojicompat\ngit clone git@github.com:googlefonts/emojicompat.git\npip install emojicompat/\n\n# Validation\npython size_check.py\nrm -rf build/ && time make -j 48\n# Should take 2-3 minutes to create noto-emoji/NotoColorEmoji.ttf\n\nmv *.ttf fonts/\n\n# make noflags CBDT font\nrm fonts/NotoColorEmoji-noflags.ttf\npython drop_flags.py fonts/NotoColorEmoji.ttf\n\n# Build the COLRv1 font (slow)\n\npython colrv1_generate_configs.py\ngit diff colrv1/*.toml\n\n# Compile the fonts\n# Should take ~20 minutes\n(cd colrv1 && rm -rf build/ && time nanoemoji *.toml)\ncp colrv1/build/NotoColorEmoji.ttf fonts/Noto-COLRv1.ttf\ncp colrv1/build/NotoColorEmoji-noflags.ttf fonts/Noto-COLRv1-noflags.ttf\n\n# Post-process them\npython colrv1_postproc.py\n\n# Produce emojicompat variants\n# Add support for new sequences per https://github.com/googlefonts/emojicompat#support-new-unicode-sequences\n\npushd fonts\ncp NotoColorEmoji.ttf NotoColorEmoji-emojicompat.ttf\ncp Noto-COLRv1.ttf Noto-COLRv1-emojicompat.ttf\nemojicompat --op setup --font NotoColorEmoji-emojicompat.ttf\nemojicompat --op setup --font Noto-COLRv1-emojicompat.ttf\nemojicompat --op check --font NotoColorEmoji-emojicompat.ttf\nemojicompat --op check --font Noto-COLRv1-emojicompat.ttf\npopd\n\nhb-subset --unicodes-file=flags-only-unicodes.txt \\\n   --output-file=fonts/NotoColorEmoji-flagsonly.ttf \\\n   fonts/NotoColorEmoji.ttf\npython update_flag_name.py"
        },
        {
          "name": "gen_version.py",
          "type": "blob",
          "size": 6.5068359375,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2015 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Generate version string for NotoColorEmoji.\n\nThis parses the color emoji template file and updates the lines\ncontaining version string info, writing a new file.\n\nThe nameID 5 field in the emoji font should reflect the commit/date\nof the repo it was built from.  This will build a string of the following\nformat:\n  Version 1.39;GOOG;noto-emoji:20170220:a8a215d2e889'\n\nThis is intended to indicate that it was built by Google from noto-emoji\nat commit a8a215d2e889 and date 20170220 (since dates are a bit easier\nto locate in time than commit hashes).\n\nFor building with external data we don't include the commit id as we\nmight be using different resources.  Instead the version string is:\n  Version 1.39;GOOG;noto-emoji:20170518;BETA <msg>\n\nHere the date is the current date, and the message after 'BETA ' is\nprovided using the '-b' flag.  There's no commit hash.  This also\nbypasses some checks about the state of the repo.\n\nThe release number should have 2 or 3 minor digits.  Right now we've been\nusing 2 but at the next major release we probably want to use 3.  This\nsupports both.  It will bump the version number if none is provided,\nmaintaining the minor digit length.\n\"\"\"\n\nimport argparse\nimport datetime\nimport re\n\nfrom nototools import tool_utils\n\n# These are not very lenient, we expect to be applied to the noto color\n# emoji template ttx file which matches these.  Why then require the\n# input argument, you ask?  Um... testing?\n_nameid_re = re.compile(r'\\s*<namerecord nameID=\"5\"')\n_version_re = re.compile(r'\\s*Version\\s(\\d+.\\d{2,3})')\n_headrev_re = re.compile(r'\\s*<fontRevision value=\"(\\d+.\\d{2,3})\"/>')\n\ndef _get_existing_version(lines):\n  \"\"\"Scan lines for all existing version numbers, and ensure they match.\n  Return the matched version number string.\"\"\"\n\n  version = None\n  def check_version(new_version):\n    if version is not None and new_version != version:\n      raise Exception(\n          'version %s and namerecord version %s do not match' % (\n              version, new_version))\n    return new_version\n\n  saw_nameid = False\n  for line in lines:\n    if saw_nameid:\n      saw_nameid = False\n      m = _version_re.match(line)\n      if not m:\n        raise Exception('could not match line \"%s\" in namerecord' % line)\n      version = check_version(m.group(1))\n    elif _nameid_re.match(line):\n      saw_nameid = True\n    else:\n      m = _headrev_re.match(line)\n      if m:\n        version = check_version(m.group(1))\n  return version\n\n\ndef _version_to_mm(version):\n  majs, mins = version.split('.')\n  minor_len = len(mins)\n  return int(majs), int(mins), minor_len\n\n\ndef _mm_to_version(major, minor, minor_len):\n  fmt = '%%d.%%0%dd' % minor_len\n  return fmt % (major, minor)\n\n\ndef _version_compare(lhs, rhs):\n  lmaj, lmin, llen = _version_to_mm(lhs)\n  rmaj, rmin, rlen = _version_to_mm(rhs)\n  # if major versions differ, we don't care about the minor length, else\n  # they should be the same\n  if lmaj != rmaj:\n    return lmaj - rmaj\n  if llen != rlen:\n    raise Exception('minor version lengths differ: \"%s\" and \"%s\"' % (lhs, rhs))\n  return lmin - rmin\n\n\ndef _version_bump(version):\n  major, minor, minor_len = _version_to_mm(version)\n  minor = (minor + 1) % (10 ** minor_len)\n  if minor == 0:\n    raise Exception('cannot bump version \"%s\", requires new major' % version)\n  return _mm_to_version(major, minor, minor_len)\n\n\ndef _get_repo_version_str(beta):\n  \"\"\"See above for description of this string.\"\"\"\n  if beta is not None:\n    date_str = datetime.date.today().strftime('%Y%m%d')\n    return 'GOOG;noto-emoji:%s;BETA %s' % (date_str, beta)\n\n  p = tool_utils.resolve_path('[emoji]')\n  commit, date, _ = tool_utils.git_head_commit(p)\n  if not tool_utils.git_check_remote_commit(p, commit):\n    raise Exception('emoji not on upstream master branch')\n  date_re = re.compile(r'(\\d{4})-(\\d{2})-(\\d{2})')\n  m = date_re.match(date)\n  if not m:\n    raise Exception('could not match \"%s\" with \"%s\"' % (date, date_re.pattern))\n  ymd = ''.join(m.groups())\n  return 'GOOG;noto-emoji:%s:%s' % (ymd, commit[:12])\n\n\ndef _replace_existing_version(lines, version, version_str):\n  \"\"\"Update lines with new version strings in appropriate places.\"\"\"\n  saw_nameid = False\n  for i in range(len(lines)):\n    line = lines[i]\n    if saw_nameid:\n      saw_nameid = False\n      # preserve indentation\n      lead_ws = len(line) - len(line.lstrip())\n      lines[i] = line[:lead_ws] + version_str + '\\n'\n    elif _nameid_re.match(line):\n      saw_nameid = True\n    elif _headrev_re.match(line):\n      lead_ws = len(line) - len(line.lstrip())\n      lines[i] = line[:lead_ws] + '<fontRevision value=\"%s\"/>\\n' % version\n\n\ndef update_version(srcfile, dstfile, version, beta):\n  \"\"\"Update version in srcfile and write to dstfile.  If version is None,\n  bumps the current version, else version must be greater than the\n  current version.\"\"\"\n\n  with open(srcfile, 'r') as f:\n    lines = f.readlines()\n  current_version = _get_existing_version(lines)\n  if not version:\n    version = _version_bump(current_version)\n  elif version and _version_compare(version, current_version) <= 0:\n    raise Exception('new version %s is <= current version %s' % (\n        version, current_version))\n  version_str = 'Version %s;%s' % (version, _get_repo_version_str(beta))\n  _replace_existing_version(lines, version, version_str)\n  with open(dstfile, 'w') as f:\n    for line in lines:\n      f.write(line)\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-v', '--version', help='version number, default bumps the current '\n      'version', metavar='ver')\n  parser.add_argument(\n      '-s', '--src', help='ttx file with name and head tables',\n      metavar='file', required=True)\n  parser.add_argument(\n      '-d', '--dst', help='name of edited ttx file to write',\n      metavar='file', required=True)\n  parser.add_argument(\n      '-b', '--beta', help='beta tag if font is built using external resources')\n  args = parser.parse_args()\n\n  update_version(args.src, args.dst, args.version, args.beta)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "generate_emoji_html.py",
          "type": "blob",
          "size": 21.361328125,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2016 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Build an html page showing emoji images.\n\nThis takes a list of directories containing emoji image files, and\nbuilds an html page presenting the images along with their composition\n(for sequences) and unicode names (for individual emoji).\"\"\"\nfrom __future__ import print_function\n\nimport argparse\nimport codecs\nimport collections\nimport datetime\nimport glob\nimport os\nfrom os import path\nimport re\nimport shutil\nimport string\nimport sys\n\nfrom nototools import tool_utils\nfrom nototools import unicode_data\n\nimport add_aliases\n\n_default_dir = 'png/128'\n_default_ext = 'png'\n_default_prefix = 'emoji_u'\n_default_title = 'Emoji List'\n\n# DirInfo represents information about a directory of file names.\n# - directory is the directory path\n# - title is the title to use for this directory\n# - filemap is a dict mapping from a tuple of codepoints to the name of\n#   a file in the directory.\nDirInfo = collections.namedtuple('DirInfo', 'directory, title, filemap')\n\n\ndef _merge_keys(dicts):\n  \"\"\"Return the union of the keys in the list of dicts.\"\"\"\n  keys = []\n  for d in dicts:\n    keys.extend(d.keys())\n  return frozenset(keys)\n\n\ndef _generate_row_cells(\n    key, font, aliases, excluded, dir_infos, basepaths, colors):\n  CELL_PREFIX = '<td>'\n  indices = range(len(basepaths))\n  def _cell(info, basepath):\n    if key in info.filemap:\n      return '<img src=\"%s\">' % path.join(basepath, info.filemap[key])\n    if key in aliases:\n      return 'alias'\n    if key in excluded:\n      return 'exclude'\n    return 'missing'\n\n  def _text_cell(text_dir):\n    text = ''.join(chr(cp) for cp in key)\n    return '<span class=\"efont\" dir=\"%s\">%s</span>' % (text_dir, text)\n\n  if font:\n    row_cells = [\n        CELL_PREFIX + _text_cell(text_dir)\n        for text_dir in ('ltr', 'rtl')]\n  else:\n    row_cells = []\n  row_cells.extend(\n      [CELL_PREFIX + _cell(dir_infos[i], basepaths[i])\n       for i in indices])\n  if len(colors) > 1:\n    ix = indices[-1]\n    extension = CELL_PREFIX + _cell(dir_infos[ix], basepaths[ix])\n    row_cells.extend([extension] * (len(colors) - 1))\n  return row_cells\n\n\ndef _get_desc(key_tuple, aliases, dir_infos, basepaths):\n  CELL_PREFIX = '<td>'\n  def _get_filepath(cp):\n    def get_key_filepath(key):\n      for i in range(len(dir_infos)):\n        info = dir_infos[i]\n        if key in info.filemap:\n          basepath = basepaths[i]\n          return path.join(basepath, info.filemap[key])\n      return None\n\n    cp_key = tuple([cp])\n    cp_key = unicode_data.get_canonical_emoji_sequence(cp_key) or cp_key\n    fp = get_key_filepath(cp_key)\n    if not fp:\n      if cp_key in aliases:\n        fp = get_key_filepath(aliases[cp_key])\n      else:\n        print('no alias for %s' % unicode_data.seq_to_string(cp_key))\n    if not fp:\n      print('no part for %s in %s' % (\n          unicode_data.seq_to_string(cp_key),\n          unicode_data.seq_to_string(key_tuple)))\n    return fp\n\n  def _get_part(cp):\n    if cp == 0x200d:  # zwj, common so replace with '+'\n      return '+'\n    if unicode_data.is_regional_indicator(cp):\n      return unicode_data.regional_indicator_to_ascii(cp)\n    if unicode_data.is_tag(cp):\n      return unicode_data.tag_character_to_ascii(cp)\n    fname = _get_filepath(cp)\n    if fname:\n      return '<img src=\"%s\">' % fname\n    raise Exception()\n\n  if len(key_tuple) == 1:\n    desc = '%04x' % key_tuple\n  else:\n    desc = ' '.join('%04x' % cp for cp in key_tuple)\n    if len(unicode_data.strip_emoji_vs(key_tuple)) > 1:\n      try:\n        desc += ' (%s)' % ''.join(\n            _get_part(cp) for cp in key_tuple if cp != 0xfe0f)\n      except:\n        pass\n  return CELL_PREFIX + desc\n\n\ndef _get_name(key_tuple, annotations):\n  annotation = None if annotations is None else annotations.get(key_tuple)\n  CELL_PREFIX = '<td%s>' % (\n      '' if annotation is None else ' class=\"%s\"' % annotation)\n\n  seq_name = unicode_data.get_emoji_sequence_name(key_tuple)\n  if seq_name == None:\n    if key_tuple == (0x20e3,):\n      seq_name = '(combining enlosing keycap)'\n    elif key_tuple == (0xfe82b,):\n      seq_name = '(unknown flag PUA codepoint)'\n    else:\n      print('no name for %s' % unicode_data.seq_to_string(key_tuple))\n      seq_name = '(oops)'\n  return CELL_PREFIX + seq_name\n\n\ndef _collect_aux_info(dir_infos, keys):\n  \"\"\"Returns a map from dir_info_index to a set of keys of additional images\n  that we will take from the directory at that index.\"\"\"\n\n  target_key_to_info_index = {}\n  for key in keys:\n    if len(key) == 1:\n      continue\n    for cp in key:\n      target_key = tuple([cp])\n      if target_key in keys or target_key in target_key_to_info_index:\n        continue\n      for i, info in enumerate(dir_infos):\n        if target_key in info.filemap:\n          target_key_to_info_index[target_key] = i\n          break\n      if target_key not in target_key_to_info_index:\n        # we shouldn't try to use it in the description.  maybe report this?\n        pass\n\n  # now we need to invert the map\n  aux_info = collections.defaultdict(set)\n  for key, index in target_key_to_info_index.items():\n    aux_info[index].add(key)\n\n  return aux_info\n\n\ndef _generate_content(\n    basedir, font, dir_infos, keys, aliases, excluded, annotations, standalone,\n    colors):\n  \"\"\"Generate an html table for the infos.  Basedir is the parent directory of\n  the content, filenames will be made relative to this if underneath it, else\n  absolute. If font is not none, generate columns for the text rendered in the\n  font before other columns.  Dir_infos is the list of DirInfos in column\n  order.  Keys is the list of canonical emoji sequences in row order.  Aliases\n  and excluded indicate images we expect to not be present either because\n  they are aliased or specifically excluded.  If annotations is not none,\n  highlight sequences that appear in this map based on their map values ('ok',\n  'error', 'warning').  If standalone is true, the image data and font (if used)\n  will be copied under the basedir to make a completely stand-alone page.\n  Colors is the list of background colors, the last DirInfo column will be\n  repeated against each of these backgrounds.\n  \"\"\"\n\n  basedir = path.abspath(path.expanduser(basedir))\n  if not path.isdir(basedir):\n    os.makedirs(basedir)\n\n  basepaths = []\n\n  if standalone:\n    # auxiliary images are used in the decomposition of multi-part emoji but\n    # aren't part of main set.  e.g. if we have female basketball player\n    # color-3 we want female, basketball player, and color-3 images available\n    # even if they aren't part of the target set.\n    aux_info = _collect_aux_info(dir_infos, keys)\n\n    # create image subdirectories in target dir, copy image files to them,\n    # and adjust paths\n    for i, info in enumerate(dir_infos):\n      subdir = '%02d' % i\n      dstdir = path.join(basedir, subdir)\n      if not path.isdir(dstdir):\n        os.mkdir(dstdir)\n\n      copy_keys = set(keys) | aux_info[i]\n      srcdir = info.directory\n      filemap = info.filemap\n      for key in copy_keys:\n        if key in filemap:\n          filename = filemap[key]\n          srcfile = path.join(srcdir, filename)\n          dstfile = path.join(dstdir, filename)\n          shutil.copy2(srcfile, dstfile)\n      basepaths.append(subdir)\n  else:\n    for srcdir, _, _ in dir_infos:\n      abs_srcdir = path.abspath(path.expanduser(srcdir))\n      if abs_srcdir == basedir:\n        dirspec = ''\n      elif abs_srcdir.startswith(basedir):\n        dirspec = abs_srcdir[len(basedir) + 1:]\n      else:\n        dirspec = abs_srcdir\n      basepaths.append(dirspec)\n\n  lines = ['<table>']\n  header_row = ['']\n  if font:\n    header_row.extend(['Emoji ltr', 'Emoji rtl'])\n  header_row.extend([info.title for info in dir_infos])\n  if len(colors) > 1:\n    header_row.extend([dir_infos[-1].title] * (len(colors) - 1))\n  header_row.extend(['Sequence', 'Name'])\n  lines.append('<th>'.join(header_row))\n\n  for key in keys:\n    row = _generate_row_cells(\n        key, font, aliases, excluded, dir_infos, basepaths, colors)\n    row.append(_get_desc(key, aliases, dir_infos, basepaths))\n    row.append(_get_name(key, annotations))\n    lines.append(''.join(row))\n\n  return '\\n  <tr>'.join(lines) + '\\n</table>'\n\n\ndef _get_image_data(image_dir, ext, prefix):\n  \"\"\"Return a map from a canonical tuple of cp sequences to a filename.\n\n  This filters by file extension, and expects the rest of the files\n  to match the prefix followed by a sequence of hex codepoints separated\n  by underscore.  Files that don't match, duplicate sequences (because\n  of casing), and out_of_range or empty codepoints raise an error.\"\"\"\n\n  fails = []\n  result = {}\n  expect_re = re.compile(r'%s([0-9A-Fa-f_]+).%s' % (prefix, ext))\n  for f in sorted(glob.glob(path.join(image_dir, '*.%s' % ext))):\n    filename = path.basename(f)\n    m = expect_re.match(filename)\n    if not m:\n      if filename.startswith('unknown_flag.') or filename.startswith('p4p_'):\n        continue\n      fails.append('\"%s\" did not match: \"%s\"' % (expect_re.pattern, filename))\n      continue\n    seq = m.group(1)\n    this_failed = False\n    try:\n      cps = tuple(int(s, 16) for s in seq.split('_'))\n      for cp in cps:\n        if (cp > 0x10ffff):\n          fails.append('cp out of range: ' + filename)\n          this_failed = True\n          break\n      if this_failed:\n        continue\n      canonical_cps = unicode_data.get_canonical_emoji_sequence(cps)\n      if canonical_cps:\n        # if it is unrecognized, just leave it alone, else replace with\n        # canonical sequence.\n        cps = canonical_cps\n    except:\n      fails.append('bad cp sequence: ' + filename)\n      continue\n    if cps in result:\n      fails.append('duplicate sequence: %s and %s' (result[cps], filename))\n      continue\n    result[cps] = filename\n  if fails:\n    print('get_image_data failed (%s, %s, %s):\\n  %s' % (\n        image_dir, ext, prefix, '\\n  '.join(fails)), file=sys.stderr)\n    raise ValueError('get image data failed')\n  return result\n\n\ndef _get_dir_infos(\n    image_dirs, exts=None, prefixes=None, titles=None,\n    default_ext=_default_ext, default_prefix=_default_prefix):\n  \"\"\"Return a list of DirInfos for the image_dirs.  When defined,\n  exts, prefixes, and titles should be the same length as image_dirs.\n  Titles default to using the last segments of the image_dirs,\n  exts and prefixes default to the corresponding default values.\"\"\"\n\n  count = len(image_dirs)\n  if not titles:\n    titles = [None] * count\n  elif len(titles) != count:\n      raise ValueError('have %d image dirs but %d titles' % (\n          count, len(titles)))\n  if not exts:\n    exts = [default_ext] * count\n  elif len(exts) != count:\n    raise ValueError('have %d image dirs but %d extensions' % (\n        count, len(exts)))\n  if not prefixes:\n    prefixes = [default_prefix] * count\n  elif len(prefixes) != count:\n    raise ValueError('have %d image dirs but %d prefixes' % (\n        count, len(prefixes)))\n\n  infos = []\n  for i in range(count):\n    image_dir = image_dirs[i]\n    title = titles[i] or path.basename(path.abspath(image_dir))\n    ext = exts[i] or default_ext\n    prefix = prefixes[i] or default_prefix\n    filemap = _get_image_data(image_dir, ext, prefix)\n    infos.append(DirInfo(image_dir, title, filemap))\n  return infos\n\n\ndef _add_aliases(keys, aliases):\n  for k, v in sorted(aliases.items()):\n    k_str = unicode_data.seq_to_string(k)\n    v_str = unicode_data.seq_to_string(v)\n    if k in keys:\n      msg = '' if v in keys else ' but it\\'s not present'\n      print('have alias image %s, should use %s%s' % (k_str, v_str, msg))\n    elif v not in keys:\n      print('can\\'t use alias %s, no image matching %s' % (k_str, v_str))\n  to_add = {k for k, v in aliases.items() if k not in keys and v in keys}\n  return keys | to_add\n\n\ndef _get_keys(dir_infos, aliases, limit, all_emoji, emoji_sort, ignore_missing):\n  \"\"\"Return a list of the key tuples to display.  If all_emoji is\n  true, start with all emoji sequences, else the sequences available\n  in dir_infos (limited to the first dir_info if limit is True).\n  If ignore_missing is true and all_emoji is false, ignore sequences\n  that are not valid (e.g. skin tone variants of wrestlers).  If\n  ignore_missing is true and all_emoji is true, ignore sequences\n  for which we have no assets (e.g. newly defined emoji).  If not using\n  all_emoji, aliases are included if we have a target for them.\n  The result is in emoji order if emoji_sort is true, else in\n  unicode codepoint order.\"\"\"\n\n  if all_emoji or ignore_missing:\n    all_keys = unicode_data.get_emoji_sequences()\n  if not all_emoji or ignore_missing:\n    if len(dir_infos) == 1 or limit:\n      avail_keys = frozenset(dir_infos[0].filemap.keys())\n    else:\n      avail_keys = _merge_keys([info.filemap for info in dir_infos])\n    if aliases:\n      avail_keys = _add_aliases(avail_keys, aliases)\n\n  if not ignore_missing:\n    keys = all_keys if all_emoji else avail_keys\n  else:\n    keys = set(all_keys) & avail_keys\n\n  if emoji_sort:\n    sorted_keys = unicode_data.get_sorted_emoji_sequences(keys)\n  else:\n    sorted_keys = sorted(keys)\n  return sorted_keys\n\n\ndef _generate_info_text(args):\n  lines = ['%s: %r' % t for t in sorted(args.__dict__.items())]\n  lines.append('generated by %s on %s' % (\n      path.basename(__file__), datetime.datetime.now()))\n  return '\\n  '.join(lines)\n\n\ndef _parse_annotation_file(afile):\n  \"\"\"Parse file and return a map from sequences to one of 'ok', 'warning',\n  or 'error'.\n\n  The file format consists of two kinds of lines.  One defines the annotation\n  to apply, it consists of the text 'annotation:' followed by one of 'ok',\n  'warning', or 'error'.  The other defines a sequence that should get the most\n  recently defined annotation, this is a series of codepoints expressed in hex\n  separated by spaces.  The initial default annotation is 'error'.  '#' starts\n  a comment to end of line, blank lines are ignored.\n  \"\"\"\n\n  annotations = {}\n  line_re = re.compile(r'annotation:\\s*(ok|warning|error)|([0-9a-f ]+)')\n  annotation = 'error'\n  with open(afile, 'r') as f:\n    for line in f:\n      line = line.strip()\n      if not line or line[0] == '#':\n        continue\n      m = line_re.match(line)\n      if not m:\n        raise Exception('could not parse annotation \"%s\"' % line)\n      new_annotation = m.group(1)\n      if new_annotation:\n        annotation = new_annotation\n      else:\n        seq = tuple([int(s, 16) for s in m.group(2).split()])\n        canonical_seq = unicode_data.get_canonical_emoji_sequence(seq)\n        if canonical_seq:\n          seq = canonical_seq\n        if seq in annotations:\n          raise Exception(\n              'duplicate sequence %s in annotations' %\n              unicode_data.seq_to_string(seq))\n        annotations[seq] = annotation\n  return annotations\n\n\ndef _instantiate_template(template, arg_dict):\n  id_regex = re.compile(r'\\$([a-zA-Z0-9_]+)')\n  ids = set(m.group(1) for m in id_regex.finditer(template))\n  keyset = set(arg_dict.keys())\n  extra_args = keyset - ids\n  if extra_args:\n    print((\n        'the following %d args are unused:\\n%s' %\n        (len(extra_args), ', '.join(sorted(extra_args)))), file=sys.stderr)\n  return string.Template(template).substitute(arg_dict)\n\n\nTEMPLATE = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\">\n    <title>$title</title>$fontFaceStyle\n    <style>$style</style>\n  </head>\n  <body>\n  <!--\n  $info\n  -->\n  <h3>$title</h3>\n  $content\n  </body>\n</html>\n\"\"\"\n\nSTYLE = \"\"\"\n      tbody { background-color: rgb(110, 110, 110) }\n      th { background-color: rgb(210, 210, 210) }\n      td img { width: 64px; height: 64px }\n      td:nth-last-of-type(2) {\n         font-size: 18pt; font-weight: regular; background-color: rgb(210, 210, 210)\n      }\n      td:nth-last-of-type(2) img {\n         vertical-align: bottom; width: 32px; height: 32px\n      }\n      td:last-of-type { background-color: white }\n      td.error { background-color: rgb(250, 65, 75) }\n      td.warning { background-color: rgb(240, 245, 50) }\n      td.ok { background-color: rgb(10, 200, 60) }\n\"\"\"\n\ndef write_html_page(\n    filename, page_title, font, dir_infos, keys, aliases, excluded, annotations,\n    standalone, colors, info):\n\n  out_dir = path.dirname(filename)\n  if font:\n    if standalone:\n      # the assumption with standalone is that the source data and\n      # output directory don't overlap, this should probably be checked...\n\n      rel_fontpath = path.join('font', path.basename(font))\n      new_font = path.join(out_dir, rel_fontpath)\n      tool_utils.ensure_dir_exists(path.dirname(new_font))\n      shutil.copy2(font, new_font)\n      font = rel_fontpath\n    else:\n      common_prefix, (rel_dir, rel_font) = tool_utils.commonpathprefix(\n          [out_dir, font])\n      if rel_dir == '':\n        # font is in a subdirectory of the target, so just use the relative\n        # path\n        font = rel_font\n      else:\n        # use the absolute path\n        font = path.normpath(path.join(common_prefix, rel_font))\n\n  content = _generate_content(\n      path.dirname(filename), font, dir_infos, keys, aliases, excluded,\n      annotations, standalone, colors)\n  N_STYLE = STYLE\n  if font:\n    FONT_FACE_STYLE = \"\"\"\n    <style>@font-face {\n      font-family: \"Emoji\"; src: local(\"Noto Color Emoji\"), url(\"%s\");\n    }</style>\"\"\" % font\n    N_STYLE += '      span.efont { font-family: \"Emoji\"; font-size:32pt }\\n'\n  else:\n    FONT_FACE_STYLE = ''\n  num_final_cols = len(colors)\n  col_colors = ['']\n  for i, color in enumerate(colors):\n    col_colors.append(\n        \"\"\"td:nth-last-of-type(%d) { background-color: #%s }\\n\"\"\" % (\n            2 + num_final_cols - i, color))\n  N_STYLE += '       '.join(col_colors)\n  text = _instantiate_template(\n      TEMPLATE, {\n          'title': page_title, 'fontFaceStyle': FONT_FACE_STYLE,\n          'style': N_STYLE, 'content': content, 'info':info})\n  with codecs.open(filename, 'w', 'utf-8') as f:\n    f.write(text)\n\n\ndef _get_canonical_aliases():\n  def canon(seq):\n    return unicode_data.get_canonical_emoji_sequence(seq) or seq\n  aliases = add_aliases.read_default_emoji_aliases()\n  return {canon(k): canon(v) for k, v in aliases.items()}\n\ndef _get_canonical_excluded():\n  def canon(seq):\n    return unicode_data.get_canonical_emoji_sequence(seq) or seq\n  aliases = add_aliases.read_default_unknown_flag_aliases()\n  return frozenset([canon(k) for k in aliases.keys()])\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-o', '--outfile', help='path to output file', metavar='file',\n      required=True)\n  parser.add_argument(\n      '--page_title', help='page title', metavar='title', default='Emoji Table')\n  parser.add_argument(\n      '-d', '--image_dirs', help='image directories', metavar='dir',\n      nargs='+')\n  parser.add_argument(\n      '-e', '--exts', help='file extension, one per image dir', metavar='ext',\n      nargs='*')\n  parser.add_argument(\n      '-p', '--prefixes', help='file name prefix, one per image dir',\n      metavar='prefix', nargs='*')\n  parser.add_argument(\n      '-t', '--titles', help='title, one per image dir', metavar='title',\n      nargs='*'),\n  parser.add_argument(\n      '-l', '--limit', help='limit to only sequences supported by first set',\n      action='store_true')\n  parser.add_argument(\n      '-de', '--default_ext', help='default extension', metavar='ext',\n      default=_default_ext)\n  parser.add_argument(\n      '-dp', '--default_prefix', help='default prefix', metavar='prefix',\n      default=_default_prefix)\n  parser.add_argument(\n      '-f', '--font', help='emoji font', metavar='font')\n  parser.add_argument(\n      '-a', '--annotate', help='file listing sequences to annotate',\n      metavar='file')\n  parser.add_argument(\n      '-s', '--standalone', help='copy resources used by html under target dir',\n      action='store_true')\n  parser.add_argument(\n      '-c', '--colors', help='list of colors for background', nargs='*',\n      metavar='hex')\n  parser.add_argument(\n      '--all_emoji', help='use all emoji sequences', action='store_true')\n  parser.add_argument(\n      '--emoji_sort', help='use emoji sort order', action='store_true')\n  parser.add_argument(\n      '--ignore_missing', help='do not include missing emoji',\n      action='store_true')\n\n  args = parser.parse_args()\n  file_parts = path.splitext(args.outfile)\n  if file_parts[1] != '.html':\n    args.outfile = file_parts[0] + '.html'\n    print('added .html extension to filename:\\n%s' % args.outfile)\n\n  if args.annotate:\n    annotations = _parse_annotation_file(args.annotate)\n  else:\n    annotations = None\n\n  if args.colors == None:\n    args.colors = ['6e6e6e']\n  elif not args.colors:\n    args.colors = \"\"\"eceff1 f5f5f5 e4e7e9 d9dbdd 080808 263238 21272b 3c474c\n    4db6ac 80cbc4 5e35b1\"\"\".split()\n\n  dir_infos = _get_dir_infos(\n      args.image_dirs, args.exts, args.prefixes, args.titles,\n      args.default_ext, args.default_prefix)\n\n  aliases = _get_canonical_aliases()\n  keys = _get_keys(\n      dir_infos, aliases, args.limit, args.all_emoji, args.emoji_sort,\n      args.ignore_missing)\n\n  excluded = _get_canonical_excluded()\n\n  info = _generate_info_text(args)\n\n  write_html_page(\n      args.outfile, args.page_title, args.font, dir_infos, keys, aliases,\n      excluded, annotations, args.standalone, args.colors, info)\n\n\nif __name__ == \"__main__\":\n  main()\n"
        },
        {
          "name": "generate_emoji_name_data.py",
          "type": "blob",
          "size": 14.0693359375,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-#\n#\n# Copyright 2015 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Generate name data for emoji resources. Currently in json format.\"\"\"\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport glob\nimport json\nimport os\nfrom os import path\nimport re\nimport sys\n\nimport generate_emoji_html\n\nfrom nototools import tool_utils\nfrom nototools import unicode_data\n\ndef _create_custom_gendered_seq_names():\n  \"\"\"The names have detail that is adequately represented by the image.\"\"\"\n\n  BOY = 0x1f466\n  GIRL = 0x1f467\n  MAN = 0x1f468\n  WOMAN = 0x1f469\n  HEART = 0x2764  # Heavy Black Heart\n  KISS_MARK = 0x1f48b\n  return {\n      (MAN, HEART, KISS_MARK, MAN): 'Kiss',\n      (WOMAN, HEART, KISS_MARK, WOMAN): 'Kiss',\n      (WOMAN, HEART, KISS_MARK, MAN): 'Kiss',\n      (WOMAN, HEART, MAN): 'Couple with Heart',\n      (MAN, HEART, MAN): 'Couple with Heart',\n      (WOMAN, HEART, WOMAN): 'Couple with Heart',\n      (MAN, GIRL): 'Family',\n      (MAN, GIRL, GIRL): 'Family',\n      (MAN, GIRL, BOY): 'Family',\n      (MAN, BOY): 'Family',\n      (MAN, BOY, BOY): 'Family',\n      (MAN, WOMAN, GIRL): 'Family',\n      (MAN, WOMAN, GIRL, GIRL): 'Family',\n      (MAN, WOMAN, GIRL, BOY): 'Family',\n      (MAN, WOMAN, BOY): 'Family',\n      (MAN, WOMAN, BOY, BOY): 'Family',\n      (MAN, MAN, GIRL): 'Family',\n      (MAN, MAN, GIRL, GIRL): 'Family',\n      (MAN, MAN, GIRL, BOY): 'Family',\n      (MAN, MAN, BOY): 'Family',\n      (MAN, MAN, BOY, BOY): 'Family',\n      (WOMAN, GIRL): 'Family',\n      (WOMAN, GIRL, GIRL): 'Family',\n      (WOMAN, GIRL, BOY): 'Family',\n      (WOMAN, BOY): 'Family',\n      (WOMAN, BOY, BOY): 'Family',\n      (WOMAN, WOMAN, GIRL): 'Family',\n      (WOMAN, WOMAN, GIRL, GIRL): 'Family',\n      (WOMAN, WOMAN, GIRL, BOY): 'Family',\n      (WOMAN, WOMAN, BOY): 'Family',\n      (WOMAN, WOMAN, BOY, BOY): 'Family' }\n\ndef _create_custom_seq_names():\n  \"\"\"These have names that often are of the form 'Person xyz-ing' or 'Man Xyz.'\n  We opt to simplify the former to an activity name or action, and the latter to\n  drop the gender.  This also generally makes the names shorter.\"\"\"\n\n  EYE = 0x1f441\n  SPEECH = 0x1f5e8\n  WHITE_FLAG = 0x1f3f3\n  RAINBOW = 0x1f308\n  return {\n      (EYE, SPEECH): 'I Witness',\n      (WHITE_FLAG, RAINBOW): 'Rainbow Flag',\n      (0x2695,): 'Health Worker',\n      (0x2696,): 'Judge',\n      (0x26f7,): 'Skiing',\n      (0x26f9,): 'Bouncing a Ball',\n      (0x2708,): 'Pilot',\n      (0x1f33e,): 'Farmer',\n      (0x1f373,): 'Cook',\n      (0x1f393,): 'Student',\n      (0x1f3a4,): 'Singer',\n      (0x1f3a8,): 'Artist',\n      (0x1f3c2,): 'Snowboarding',\n      (0x1f3c3,): 'Running',\n      (0x1f3c4,): 'Surfing',\n      (0x1f3ca,): 'Swimming',\n      (0x1f3cb,): 'Weight Lifting',\n      (0x1f3cc,): 'Golfing',\n      (0x1f3eb,): 'Teacher',\n      (0x1f3ed,): 'Factory Worker',\n      (0x1f46e,): 'Police Officer',\n      (0x1f46f,): 'Partying',\n      (0x1f471,): 'Person with Blond Hair',\n      (0x1f473,): 'Person Wearing Turban',\n      (0x1f477,): 'Construction Worker',\n      (0x1f481,): 'Tipping Hand',\n      (0x1f482,): 'Guard',\n      (0x1f486,): 'Face Massage',\n      (0x1f487,): 'Haircut',\n      (0x1f4bb,): 'Technologist',\n      (0x1f4bc,): 'Office Worker',\n      (0x1f527,): 'Mechanic',\n      (0x1f52c,): 'Scientist',\n      (0x1f575,): 'Detective',\n      (0x1f645,): 'No Good Gesture',\n      (0x1f646,): 'OK Gesture',\n      (0x1f647,): 'Bowing Deeply',\n      (0x1f64b,): 'Raising Hand',\n      (0x1f64d,): 'Frowning',\n      (0x1f64e,): 'Pouting',\n      (0x1f680,): 'Astronaut',\n      (0x1f692,): 'Firefighter',\n      (0x1f6a3,): 'Rowing',\n      (0x1f6b4,): 'Bicycling',\n      (0x1f6b5,): 'Mountain Biking',\n      (0x1f6b6,): 'Walking',\n      (0x1f926,): 'Face Palm',\n      (0x1f937,): 'Shrug',\n      (0x1f938,): 'Doing a Cartwheel',\n      (0x1f939,): 'Juggling',\n      (0x1f93c,): 'Wrestling',\n      (0x1f93d,): 'Water Polo',\n      (0x1f93e,): 'Playing Handball',\n      (0x1f9d6,): 'Person in Steamy Room',\n      (0x1f9d7,): 'Climbing',\n      (0x1f9d8,): 'Person in Lotus Position',\n      (0x1f9d9,): 'Mage',\n      (0x1f9da,): 'Fairy',\n      (0x1f9db,): 'Vampire',\n      (0x1f9dd,): 'Elf',\n      (0x1f9de,): 'Genie',\n      (0x1f9df,): 'Zombie',\n  }\n\n_CUSTOM_GENDERED_SEQ_NAMES = _create_custom_gendered_seq_names()\n_CUSTOM_SEQ_NAMES = _create_custom_seq_names()\n\n# Fixes for unusual capitalization or cases we don't care to handle in code.\n# Also prevents titlecasing 'S' after apostrophe in posessives.  Note we _do_\n# want titlecasing after apostrophe in some cases, e.g. O'Clock.\n_CUSTOM_CAPS_NAMES = {\n    (0x26d1,): 'Rescue Workerâ€™s Helmet',\n    (0x1f170,): 'A Button (blood type)',  # a Button (Blood Type)\n    (0x1f171,): 'B Button (blood type)',  # B Button (Blood Type)\n    (0x1f17e,): 'O Button (blood type)',  # O Button (Blood Type)\n    (0x1f18e,): 'AB Button (blood type)',  # Ab Button (Blood Type)\n    (0x1f191,): 'CL Button',  # Cl Button\n    (0x1f192,): 'COOL Button',  # Cool Button\n    (0x1f193,): 'FREE Button',  # Free Button\n    (0x1f194,): 'ID Button',  # Id Button\n    (0x1f195,): 'NEW Button',  # New Button\n    (0x1f196,): 'NG Button',  # Ng Button\n    (0x1f197,): 'OK Button',  # Ok Button\n    (0x1f198,): 'SOS Button',  # Sos Button\n    (0x1f199,): 'UP! Button',  # Up! Button\n    (0x1f19a,): 'VS Button',  # Vs Button\n    (0x1f3e7,): 'ATM Sign',  # Atm Sign\n    (0x1f44C,): 'OK Hand',  # Ok Hand\n    (0x1f452,): 'Womanâ€™s Hat',\n    (0x1f45a,): 'Womanâ€™s Clothes',\n    (0x1f45e,): 'Manâ€™s Shoe',\n    (0x1f461,): 'Womanâ€™s Sandal',\n    (0x1f462,): 'Womanâ€™s Boot',\n    (0x1f519,): 'BACK Arrow',  # Back Arrow\n    (0x1f51a,): 'END Arrow',  # End Arrow\n    (0x1f51b,): 'ON! Arrow',  # On! Arrow\n    (0x1f51c,): 'SOON Arrow',  # Soon Arrow\n    (0x1f51d,): 'TOP Arrow',  # Top Arrow\n    (0x1f6b9,): 'Menâ€™s Room',\n    (0x1f6ba,): 'Womenâ€™s Room',\n}\n\n# For the custom sequences we ignore ZWJ, the emoji variation selector\n# and skin tone modifiers.  We can't always ignore gender  because\n# the gendered sequences match against them, but we ignore gender in other\n# cases so we define a separate set of gendered emoji to remove.\n\n_NON_GENDER_CPS_TO_STRIP = frozenset(\n    [0xfe0f, 0x200d] +\n    range(unicode_data._FITZ_START, unicode_data._FITZ_END + 1))\n\n_GENDER_CPS_TO_STRIP = frozenset([0x2640, 0x2642, 0x1f468, 0x1f469])\n\ndef _custom_name(seq):\n  \"\"\"Apply three kinds of custom names, based on the sequence.\"\"\"\n\n  seq = tuple([cp for cp in seq if cp not in _NON_GENDER_CPS_TO_STRIP])\n  name = _CUSTOM_CAPS_NAMES.get(seq)\n  if name:\n    return name\n\n  # Single characters that participate in sequences (e.g. fire truck in the\n  # firefighter sequences) should not get converted.  Single characters\n  # are in the custom caps names set but not the other sets.\n  if len(seq) == 1:\n    return None\n\n  name = _CUSTOM_GENDERED_SEQ_NAMES.get(seq)\n  if name:\n    return name\n\n  seq = tuple([cp for cp in seq if cp not in _GENDER_CPS_TO_STRIP])\n  name = _CUSTOM_SEQ_NAMES.get(seq)\n\n  return name\n\n\ndef _standard_name(seq):\n  \"\"\"Use the standard emoji name, with some algorithmic modifications.\n\n  We want to ignore skin-tone modifiers (but of course if the sequence _is_\n  the skin-tone modifier itself we keep that).  So we strip these so we can\n  start with the generic name ignoring skin tone.\n\n  Non-emoji that are turned into emoji using the emoji VS have '(emoji) '\n  prepended to them, so strip that.\n\n  Regional indicator symbol names are a bit long, so shorten them.\n\n  Regional sequences are assumed to be ok as-is in terms of capitalization and\n  punctuation, so no modifications are applied to them.\n\n  After title-casing we make some English articles/prepositions lower-case\n  again.  We also replace '&' with 'and'; Unicode seems rather fond of\n  ampersand.\"\"\"\n\n  if not unicode_data.is_skintone_modifier(seq[0]):\n    seq = tuple([cp for cp in seq if not unicode_data.is_skintone_modifier(cp)])\n  name = unicode_data.get_emoji_sequence_name(seq)\n\n  if name.startswith('(emoji) '):\n    name = name[8:]\n\n  if len(seq) == 1 and unicode_data.is_regional_indicator(seq[0]):\n    return 'Regional Symbol ' + unicode_data.regional_indicator_to_ascii(seq[0])\n\n  if (unicode_data.is_regional_indicator_seq(seq) or\n      unicode_data.is_regional_tag_seq(seq)):\n    return name\n\n  name = name.title()\n  # Require space delimiting just in case...\n  name = re.sub(r'\\s&\\s', ' and ', name)\n  name = re.sub(\n      # not \\b at start because we retain capital at start of phrase\n      r'(\\s(:?A|And|From|In|Of|With|For))\\b', lambda s: s.group(1).lower(),\n      name)\n\n  return name\n\n\ndef _name_data(seq, seq_file):\n  name = _custom_name(seq) or _standard_name(seq)\n  # we don't need canonical sequences\n  sequence = ''.join('&#x%x;' % cp for cp in seq if cp != 0xfe0f)\n  fname = path.basename(seq_file)\n  return fname, sequence, name\n\n\ndef generate_names(\n    src_dir, dst_dir, skip_limit=20, omit_groups=None, pretty_print=False,\n    verbose=False):\n  srcdir = tool_utils.resolve_path(src_dir)\n  if not path.isdir(srcdir):\n    print('%s is not a directory' % src_dir, file=sys.stderr)\n    return\n\n  if omit_groups:\n    unknown_groups = set(omit_groups) - set(unicode_data.get_emoji_groups())\n    if unknown_groups:\n      print('did not recognize %d group%s: %s' % (\n          len(unknown_groups), '' if len(unknown_groups) == 1 else 's',\n          ', '.join('\"%s\"' % g for g in omit_groups if g in unknown_groups)), file=sys.stderr)\n      print('valid groups are:\\n  %s' % (\n          '\\n  '.join(g for g in unicode_data.get_emoji_groups())), file=sys.stderr)\n      return\n    print('omitting %d group%s: %s' % (\n        len(omit_groups), '' if len(omit_groups) == 1 else 's',\n        ', '.join('\"%s\"' % g for g in omit_groups)))\n  else:\n    # might be None\n    print('keeping all groups')\n    omit_groups = []\n\n  # make sure the destination exists\n  dstdir = tool_utils.ensure_dir_exists(\n      tool_utils.resolve_path(dst_dir))\n\n  # _get_image_data returns canonical cp sequences\n  print('src dir:', srcdir)\n  seq_to_file = generate_emoji_html._get_image_data(srcdir, 'png', 'emoji_u')\n  print('seq to file has %d sequences' % len(seq_to_file))\n\n  # Aliases add non-gendered versions using gendered images for the most part.\n  # But when we display the images, we don't distinguish genders in the\n  # naming, we rely on the images-- so these look redundant. So we\n  # intentionally don't generate images for these.\n  # However, the alias file also includes the flag aliases, which we do want,\n  # and it also fails to exclude the unknown flag pua (since it doesn't\n  # map to anything), so we need to adjust for this.\n  canonical_aliases = generate_emoji_html._get_canonical_aliases()\n\n  aliases = set([\n      cps for cps in canonical_aliases.keys()\n      if not unicode_data.is_regional_indicator_seq(cps)])\n  aliases.add((0xfe82b,))  # unknown flag PUA\n  excluded = aliases | generate_emoji_html._get_canonical_excluded()\n\n  # The flag aliases have distinct names, so we _do_ want to show them\n  # multiple times.\n  to_add = {}\n  for seq in canonical_aliases:\n    if unicode_data.is_regional_indicator_seq(seq):\n      replace_seq = canonical_aliases[seq]\n      if seq in seq_to_file:\n        print('warning, alias %s has file %s' % (\n            unicode_data.regional_indicator_seq_to_string(seq),\n            seq_to_file[seq]))\n        continue\n      replace_file = seq_to_file.get(replace_seq)\n      if replace_file:\n        to_add[seq] = replace_file\n  seq_to_file.update(to_add)\n\n  data = []\n  last_skipped_group = None\n  skipcount = 0\n  for group in unicode_data.get_emoji_groups():\n    if group in omit_groups:\n      continue\n    name_data = []\n    for seq in unicode_data.get_emoji_in_group(group):\n      if seq in excluded:\n        continue\n      seq_file = seq_to_file.get(seq, None)\n      if seq_file is None:\n        skipcount += 1\n        if verbose:\n          if group != last_skipped_group:\n            print('group %s' % group)\n            last_skipped_group = group\n          print('  %s (%s)' % (\n              unicode_data.seq_to_string(seq),\n              ', '.join(unicode_data.name(cp, 'x') for cp in seq)))\n        if skip_limit >= 0 and skipcount > skip_limit:\n          raise Exception('skipped too many items')\n      else:\n        name_data.append(_name_data(seq, seq_file))\n    data.append({'category': group, 'emojis': name_data})\n\n  outfile = path.join(dstdir, 'data.json')\n  with open(outfile, 'w') as f:\n    indent = 2 if pretty_print else None\n    separators = None if pretty_print else (',', ':')\n    json.dump(data, f, indent=indent, separators=separators)\n  print('wrote %s' % outfile)\n\n\ndef main():\n  DEFAULT_DSTDIR = '[emoji]/emoji'\n  DEFAULT_IMAGEDIR = '[emoji]/build/compressed_pngs'\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-s', '--srcdir', help='directory containing images (default %s)' %\n      DEFAULT_IMAGEDIR,  metavar='dir', default=DEFAULT_IMAGEDIR)\n  parser.add_argument(\n      '-d', '--dstdir', help='name of destination directory (default %s)' %\n      DEFAULT_DSTDIR, metavar='fname', default=DEFAULT_DSTDIR)\n  parser.add_argument(\n      '-p', '--pretty_print', help='pretty-print json file',\n      action='store_true')\n  parser.add_argument(\n      '-m', '--missing_limit', help='number of missing images before failure '\n      '(default 20), use -1 for no limit', metavar='n', default=20)\n  parser.add_argument(\n      '--omit_groups', help='names of groups to omit (default \"Misc, Flags\")',\n      metavar='name', default=['Misc', 'Flags'], nargs='*')\n  parser.add_argument(\n      '-v', '--verbose', help='print progress information to stdout',\n      action='store_true')\n  args = parser.parse_args()\n  generate_names(\n      args.srcdir, args.dstdir, args.missing_limit, args.omit_groups,\n      pretty_print=args.pretty_print, verbose=args.verbose)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "generate_emoji_placeholders.py",
          "type": "blob",
          "size": 2.2998046875,
          "content": "from __future__ import print_function\nimport os\nfrom os import path\nimport subprocess\n\nOUTPUT_DIR = '/tmp/placeholder_emoji'\n\ndef generate_image(name, text):\n  print(name, text.replace('\\n', '_'))\n  subprocess.check_call(\n      ['convert', '-size', '100x100', 'label:%s' % text,\n       '%s/%s' % (OUTPUT_DIR, name)])\n\ndef is_color_patch(cp):\n  return cp >= 0x1f3fb and cp <= 0x1f3ff\n\ndef has_color_patch(values):\n  for v in values:\n    if is_color_patch(v):\n      return True\n  return False\n\ndef regional_to_ascii(cp):\n  return unichr(ord('A') + cp - 0x1f1e6)\n\ndef is_flag_sequence(values):\n  if len(values) != 2:\n    return False\n  for v in values:\n    v -= 0x1f1e6\n    if v < 0 or v > 25:\n      return False\n  return True\n\ndef is_keycap_sequence(values):\n  return len(values) == 2 and values[1] == 0x20e3\n\ndef get_keycap_text(values):\n  return '-%c-' % unichr(values[0]) # convert gags on '['\n\nchar_map = {\n    0x1f468: 'M',\n    0x1f469: 'W',\n    0x1f466: 'B',\n    0x1f467: 'G',\n    0x2764: 'H', # heavy black heart, no var sel\n    0x1f48b: 'K', # kiss mark\n    0x200D: '-', # zwj placeholder\n    0xfe0f: '-', # variation selector placeholder\n    0x1f441: 'I', # Eye\n    0x1f5e8: 'W', # 'witness' (left speech bubble)\n}\n\ndef get_combining_text(values):\n  chars = []\n  for v in values:\n    char = char_map.get(v, None)\n    if not char:\n      return None\n    if char != '-':\n      chars.append(char)\n  return ''.join(chars)\n\n\nif not path.isdir(OUTPUT_DIR):\n  os.makedirs(OUTPUT_DIR)\n\nwith open('sequences.txt', 'r') as f:\n  for seq in f:\n    seq = seq.strip()\n    text = None\n    values = [int(code, 16) for code in seq.split('_')]\n    if len(values) == 1:\n      val = values[0]\n      text = '%04X' % val # ensure upper case format\n    elif is_flag_sequence(values):\n      text = ''.join(regional_to_ascii(cp) for cp in values)\n    elif has_color_patch(values):\n      print('skipping color patch sequence %s' % seq)\n    elif is_keycap_sequence(values):\n      text = get_keycap_text(values)\n    else:\n      text = get_combining_text(values)\n      if not text:\n        print('missing %s' % seq)\n\n    if text:\n      if len(text) > 3:\n        if len(text) == 4:\n          hi = text[:2]\n          lo = text[2:]\n        else:\n          hi = text[:-3]\n          lo = text[-3:]\n        text = '%s\\n%s' % (hi, lo)\n      generate_image('emoji_u%s.png' % seq, text)\n"
        },
        {
          "name": "generate_emoji_thumbnails.py",
          "type": "blob",
          "size": 5.0966796875,
          "content": "#!/usr/bin/env python3\n# Copyright 2017 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Generate 72x72 thumbnails including aliases.\n\nTakes a source directory of images named using our emoji filename\nconventions and writes thumbnails of them into the destination\ndirectory.  If a file is a target of one or more aliases, creates\ncopies named for the aliases.\"\"\"\n\n\nimport argparse\nimport collections\nimport logging\nimport os\nfrom os import path\nimport shutil\nimport subprocess\n\nimport add_aliases\n\nfrom nototools import tool_utils\nfrom nototools import unicode_data\n\nlogger = logging.getLogger('emoji_thumbnails')\n\ndef create_thumbnail(src_path, dst_path, crop):\n  # Uses imagemagik\n  # We need images exactly 72x72 in size, with transparent background.\n  # Remove 4-pixel LR margins from 136x128 source images if we crop.\n  if crop:\n    cmd = [\n        'convert', src_path, '-crop', '128x128+4+0!', '-thumbnail', '72x72',\n        'PNG32:' + dst_path]\n  else:\n    cmd = [\n        'convert', '-thumbnail', '72x72', '-gravity', 'center', '-background',\n        'none', '-extent', '72x72', src_path, 'PNG32:' + dst_path]\n  subprocess.check_call(cmd)\n\n\ndef get_inv_aliases():\n  \"\"\"Return a mapping from target to list of sources for all alias\n  targets in either the default alias table or the unknown_flag alias\n  table.\"\"\"\n\n  inv_aliases = collections.defaultdict(list)\n\n  standard_aliases = add_aliases.read_default_emoji_aliases()\n  for k, v in standard_aliases.iteritems():\n    inv_aliases[v].append(k)\n\n  unknown_flag_aliases = add_aliases.read_emoji_aliases(\n      'unknown_flag_aliases.txt')\n  for k, v in unknown_flag_aliases.iteritems():\n    inv_aliases[v].append(k)\n\n  return inv_aliases\n\n\ndef filename_to_sequence(filename, prefix, suffix):\n  if not filename.startswith(prefix) and filename.endswith(suffix):\n    raise ValueError('bad prefix or suffix: \"%s\"' % filename)\n  seq_str = filename[len(prefix): -len(suffix)]\n  seq = unicode_data.string_to_seq(seq_str)\n  if not unicode_data.is_cp_seq(seq):\n    raise ValueError('sequence includes non-codepoint: \"%s\"' % filename)\n  return seq\n\n\ndef sequence_to_filename(seq, prefix, suffix):\n  return ''.join((prefix, unicode_data.seq_to_string(seq), suffix))\n\n\ndef create_thumbnails_and_aliases(src_dir, dst_dir, crop, dst_prefix):\n  \"\"\"Creates thumbnails in dst_dir based on sources in src.dir, using\n  dst_prefix. Assumes the source prefix is 'emoji_u' and the common suffix\n  is '.png'.\"\"\"\n\n  src_dir = tool_utils.resolve_path(src_dir)\n  if not path.isdir(src_dir):\n    raise ValueError('\"%s\" is not a directory')\n\n  dst_dir = tool_utils.ensure_dir_exists(tool_utils.resolve_path(dst_dir))\n\n  src_prefix = 'emoji_u'\n  suffix = '.png'\n\n  inv_aliases = get_inv_aliases()\n\n  for src_file in os.listdir(src_dir):\n    try:\n      seq = unicode_data.strip_emoji_vs(\n          filename_to_sequence(src_file, src_prefix, suffix))\n    except ValueError as ve:\n      logger.warning('Error (%s), skipping' % ve)\n      continue\n\n    src_path = path.join(src_dir, src_file)\n\n    dst_file = sequence_to_filename(seq, dst_prefix, suffix)\n    dst_path = path.join(dst_dir, dst_file)\n\n    create_thumbnail(src_path, dst_path, crop)\n    logger.info('wrote thumbnail%s: %s' % (\n        ' with crop' if crop else '', dst_file))\n\n    for alias_seq in inv_aliases.get(seq, ()):\n      alias_file = sequence_to_filename(alias_seq, dst_prefix, suffix)\n      alias_path = path.join(dst_dir, alias_file)\n      shutil.copy2(dst_path, alias_path)\n      logger.info('wrote alias: %s' % alias_file)\n\n\ndef main():\n  SRC_DEFAULT = '[emoji]/build/compressed_pngs'\n  PREFIX_DEFAULT = 'android_'\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-s', '--src_dir', help='source images (default \\'%s\\')' % SRC_DEFAULT,\n      default=SRC_DEFAULT, metavar='dir')\n  parser.add_argument(\n      '-d', '--dst_dir', help='destination directory', metavar='dir',\n      required=True)\n  parser.add_argument(\n      '-p', '--prefix', help='prefix for thumbnail (default \\'%s\\')' %\n      PREFIX_DEFAULT, default=PREFIX_DEFAULT, metavar='str')\n  parser.add_argument(\n      '-c', '--crop', help='crop images (will automatically crop if '\n      'src dir is the default)', action='store_true')\n  parser.add_argument(\n      '-v', '--verbose', help='write log output', metavar='level',\n      choices='warning info debug'.split(), const='info',\n      nargs='?')\n  args = parser.parse_args()\n\n  if args.verbose is not None:\n    logging.basicConfig(level=getattr(logging, args.verbose.upper()))\n\n  crop = args.crop or (args.src_dir == SRC_DEFAULT)\n  create_thumbnails_and_aliases(\n      args.src_dir, args.dst_dir, crop, args.prefix)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "generate_test_html.py",
          "type": "blob",
          "size": 7.1337890625,
          "content": "#!/usr/bin/env python3\n# Copyright 2015 Google, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Google Author(s): Doug Felt\n\nfrom __future__ import print_function\nimport argparse\nimport os\nimport os.path\nimport re\nimport sys\n\nfrom fontTools import ttx\n\nimport add_svg_glyphs\n\ndef do_generate_test_html(font_basename, pairs, glyph=None, verbosity=1):\n  header = r\"\"\"<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"utf-8\">\n<style type=\"text/css\">\n@font-face { font-family: svgfont; src: url(\"%s\") }\nbody { font-family: sans-serif; font-size: 24px }\n#emoji span { font-family: svgfont, sans-serif }\n#panel { font-family: svgfont, sans-serif; font-size: 256px }\n#paneltitle { font-family: sans-serif; font-size: 36px }\n</style>\n<script type=\"text/javascript\">\nfunction hexify(text) {\n  var surr_offset = 0x10000 - (0xd800 << 10) - 0xdc00\n  var str = text.trim()\n  var len = str.length\n  var result = \"\"\n  for (var i = 0; i < len; ++i) {\n    var cp = str.charCodeAt(i)\n    if (cp >= 0xd800 && cp < 0xdc00 && i < len - 1) {\n      ncp = str.charCodeAt(i+1)\n      if (ncp >= 0xdc00 && ncp < 0xe000) {\n        cp = (cp << 10) + ncp + surr_offset\n        ++i;\n      }\n    }\n    result += \" 0x\" + cp.toString(16)\n  }\n  return result\n};\n\nfunction showText(event) {\n  var text = event.target.textContent\n  var p = document.getElementById('panel')\n  p.textContent = text\n  p = document.getElementById('paneltitle')\n  p.textContent = hexify(text)\n};\n\nfunction setup() {\n  var t = document.getElementById('emoji')\n  var tdlist = t.getElementsByTagName('span')\n  for (var i = 0; i < tdlist.length; ++i) {\n    var e = tdlist[i]\n    e.onmouseover = showText\n  }\n};\n</script>\n</head>\"\"\"\n\n  body_head = r\"\"\"<body onload=\"setup();\">\n<p>Test for SVG glyphs in %(font)s.  It uses the proposed\n<a href=\"http://lists.w3.org/Archives/Public/public-svgopentype/2013Jul/0003.html\">SVG-in-OpenType format</a>.\nView using Firefox&nbsp;26 and later.\n<div style=\"float:left; text-align:center; margin:0 10px; width:40%%\">\n<div id='panel' style=\"margin-left:auto; margin-right:auto\">%(glyph)s</div>\n<div id='paneltitle' style=\"margin-left:auto; margin-right:auto\">%(glyph_hex)s</div>\n</div>\n<div id='emoji'><p>\"\"\"\n\n  body_tail = r\"\"\"</div>\n</body>\n</html>\n\"\"\"\n\n  font_name = font_basename + \".woff\"\n  html_name = font_basename + \"_test.html\"\n\n  found_initial_glyph = False\n  initial_glyph_str = None;\n  initial_glyph_hex = None;\n  text_parts = []\n  for glyphstr, _ in pairs:\n    name_parts = []\n    hex_parts = []\n    for cp in glyphstr:\n      hex_str = hex(ord(cp))\n      name_parts.append('&#x%s;' % hex_str[2:])\n      hex_parts.append(hex_str)\n    glyph_str = ''.join(name_parts)\n\n    if not found_initial_glyph:\n      if not glyph or glyph_str == glyph:\n        initial_glyph_str = glyph_str\n        initial_glyph_hex = ' '.join(hex_parts)\n        found_initial_glyph = True\n      elif not initial_glyph_str:\n        initial_glyph_str = glyph_str\n        initial_glyph_hex = ' '.join(hex_parts)\n\n    text = '<span>%s</span>' % glyph_str\n    text_parts.append(text)\n\n  if verbosity and glyph and not found_initial_glyph:\n    print(\"Did not find glyph '%s', using initial glyph '%s'\" % (glyph, initial_glyph_str))\n  elif verbosity > 1 and not glyph:\n    print(\"Using initial glyph '%s'\" % initial_glyph_str)\n\n  lines = [header % font_name]\n  lines.append(body_head % {'font':font_name, 'glyph':initial_glyph_str,\n                            'glyph_hex':initial_glyph_hex})\n  lines.extend(text_parts) # we'll end up with space between each emoji\n  lines.append(body_tail)\n  output = '\\n'.join(lines)\n  with open(html_name, 'w') as fp:\n    fp.write(output)\n  if verbosity:\n    print('Wrote ' + html_name)\n\n\ndef do_generate_fonts(template_file, font_basename, pairs, reuse=0, verbosity=1):\n  out_woff = font_basename + '.woff'\n  if reuse > 1 and os.path.isfile(out_woff) and os.access(out_woff, os.R_OK):\n    if verbosity:\n      print('Reusing ' + out_woff)\n    return\n\n  out_ttx = font_basename + '.ttx'\n  if reuse == 0:\n    add_svg_glyphs.add_image_glyphs(template_file, out_ttx, pairs, verbosity=verbosity)\n  elif verbosity:\n    print('Reusing ' + out_ttx)\n\n  quiet=verbosity < 2\n  font = ttx.TTFont(flavor='woff', quiet=quiet)\n  font.importXML(out_ttx, quiet=quiet)\n  font.save(out_woff)\n  if verbosity:\n    print('Wrote ' + out_woff)\n\n\ndef main(argv):\n  usage = \"\"\"This will search for files that have image_prefix followed by one or more\n      hex numbers (separated by underscore if more than one), and end in \".svg\".\n      For example, if image_prefix is \"icons/u\", then files with names like\n      \"icons/u1F4A9.svg\" or \"icons/u1F1EF_1F1F5.svg\" will be found. It generates\n      an SVG font from this, converts it to woff, and also generates an html test\n      page containing text for all the SVG glyphs.\"\"\"\n\n  parser = argparse.ArgumentParser(\n      description='Generate font and html test file.', epilog=usage)\n  parser.add_argument('template_file', help='name of template .ttx file')\n  parser.add_argument('image_prefix', help='location and prefix of image files')\n  parser.add_argument('-i', '--include', help='include files whoses name matches this regex')\n  parser.add_argument('-e', '--exclude', help='exclude files whose name matches this regex')\n  parser.add_argument('-o', '--out_basename', help='base name of (ttx, woff, html) files to generate, '\n                      'defaults to the template base name')\n  parser.add_argument('-g', '--glyph', help='set the initial glyph text (html encoded string), '\n                      'defaults to first glyph')\n  parser.add_argument('-rt', '--reuse_ttx_font', dest='reuse_font', help='use existing ttx font',\n                      default=0, const=1, action='store_const')\n  parser.add_argument('-r', '--reuse_font', dest='reuse_font', help='use existing woff font',\n                      const=2, action='store_const')\n  parser.add_argument('-q', '--quiet', dest='v', help='quiet operation', default=1,\n                      action='store_const', const=0)\n  parser.add_argument('-v', '--verbose', dest='v', help='verbose operation',\n                      action='store_const', const=2)\n  args = parser.parse_args(argv)\n\n  pairs = add_svg_glyphs.collect_glyphstr_file_pairs(\n    args.image_prefix, 'svg', include=args.include, exclude=args.exclude, verbosity=args.v)\n  add_svg_glyphs.sort_glyphstr_tuples(pairs)\n\n  out_basename = args.out_basename\n  if not out_basename:\n    out_basename = args.template_file.split('.')[0] # exclude e.g. '.tmpl.ttx'\n    if args.v:\n      print(\"Output basename is %s.\" % out_basename)\n  do_generate_fonts(args.template_file, out_basename, pairs, reuse=args.reuse_font, verbosity=args.v)\n  do_generate_test_html(out_basename, pairs, glyph=args.glyph, verbosity=args.v)\n\nif __name__ == '__main__':\n  main(sys.argv[1:])\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "map_pua_emoji.py",
          "type": "blob",
          "size": 2.2744140625,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2014 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Modify an emoji font to map legacy PUA characters to standard ligatures.\"\"\"\n\n__author__ = 'roozbeh@google.com (Roozbeh Pournader)'\n\nimport sys\nimport itertools\n\nfrom fontTools import ttLib\n\nfrom nototools import font_data\n\nimport add_emoji_gsub\n\n\ndef get_glyph_name_from_gsub(char_seq, font):\n    \"\"\"Find the glyph name for ligature of a given character sequence from GSUB.\n    \"\"\"\n    cmap = font_data.get_cmap(font)\n    # FIXME: So many assumptions are made here.\n    try:\n        first_glyph = cmap[char_seq[0]]\n        rest_of_glyphs = [cmap[ch] for ch in char_seq[1:]]\n    except KeyError:\n        return None\n\n    for lookup in font['GSUB'].table.LookupList.Lookup:\n        ligatures = lookup.SubTable[0].ligatures\n        try:\n            for ligature in ligatures[first_glyph]:\n                if ligature.Component == rest_of_glyphs:\n                    return ligature.LigGlyph\n        except KeyError:\n            continue\n    return None\n\n\ndef add_pua_cmap_to_font(font):\n    cmap = font_data.get_cmap(font)\n    for pua, (ch1, ch2) in itertools.chain(\n        add_emoji_gsub.EMOJI_KEYCAPS.items(), add_emoji_gsub.EMOJI_FLAGS.items()\n    ):\n        if pua not in cmap:\n            glyph_name = get_glyph_name_from_gsub([ch1, ch2], font)\n            if glyph_name is not None:\n                cmap[pua] = glyph_name\n\n\ndef add_pua_cmap(source_file, target_file):\n    \"\"\"Add PUA characters to the cmap of the first font and save as second.\"\"\"\n    font = ttLib.TTFont(source_file)\n    add_pua_cmap_to_font(font)\n    font.save(target_file)\n\n\ndef main(argv):\n    \"\"\"Save the first font given to the second font.\"\"\"\n    add_pua_cmap(argv[1], argv[2])\n\n\nif __name__ == '__main__':\n    main(sys.argv)\n\n"
        },
        {
          "name": "materialize_emoji_images.py",
          "type": "blob",
          "size": 3.98046875,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2016 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Create a copy of the emoji images that instantiates aliases, etc. as\nsymlinks.\"\"\"\nfrom __future__ import print_function\n\nimport argparse\nimport glob\nimport os\nfrom os import path\nimport re\nimport shutil\n\nfrom nototools import tool_utils\n\n# copied from third_party/color_emoji/add_glyphs.py\n\nEXTRA_SEQUENCES = {\n    'u1F46A': '1F468_200D_1F469_200D_1F466', # MWB\n    'u1F491': '1F469_200D_2764_FE0F_200D_1F468', # WHM\n    'u1F48F': '1F469_200D_2764_FE0F_200D_1F48B_200D_1F468', # WHKM\n}\n\n# Flag aliases - from: to\nFLAG_ALIASES = {\n    'BV': 'NO',\n    'CP': 'FR',\n    'HM': 'AU',\n    'SJ': 'NO',\n    'UM': 'US',\n}\n\nOMITTED_FLAGS = set(\n    'BL BQ DG EA EH FK GF GP GS MF MQ NC PM RE TF WF XK YT'.split())\n\ndef _flag_str(ris_pair):\n  return '_'.join('%04x' % (ord(cp) - ord('A') +  0x1f1e6)\n                  for cp in ris_pair)\n\ndef _copy_files(src, dst):\n  \"\"\"Copies files named 'emoji_u*.png' from dst to src, and return a set of\n  the names with 'emoji_u' and the extension stripped.\"\"\"\n  code_strings = set()\n  tool_utils.check_dir_exists(src)\n  dst = tool_utils.ensure_dir_exists(dst, clean=True)\n  for f in glob.glob(path.join(src, 'emoji_u*.png')):\n    shutil.copy(f, dst)\n    code_strings.add(path.splitext(path.basename(f))[0][7:])\n  return code_strings\n\n\ndef _alias_people(code_strings, dst):\n  \"\"\"Create aliases for people in dst, based on code_strings.\"\"\"\n  for src, ali in sorted(EXTRA_SEQUENCES.items()):\n    if src[1:].lower() in code_strings:\n      src_name = 'emoji_%s.png' % src.lower()\n      ali_name = 'emoji_u%s.png' % ali.lower()\n      print('creating symlink %s -> %s' % (ali_name, src_name))\n      os.symlink(path.join(dst, src_name), path.join(dst, ali_name))\n    else:\n      print('people image %s not found' % src, file=os.stderr)\n\n\ndef _alias_flags(code_strings, dst):\n  for ali, src in sorted(FLAG_ALIASES.items()):\n    src_str = _flag_str(src)\n    if src_str in code_strings:\n      src_name = 'emoji_u%s.png' % src_str\n      ali_name = 'emoji_u%s.png' % _flag_str(ali)\n      print('creating symlink %s (%s) -> %s (%s)' % (ali_name, ali, src_name, src))\n      os.symlink(path.join(dst, src_name), path.join(dst, ali_name))\n    else:\n      print('flag image %s (%s) not found' % (src_name, src), file=os.stderr)\n\n\ndef _alias_omitted_flags(code_strings, dst):\n  UNKNOWN_FLAG = 'fe82b'\n  if UNKNOWN_FLAG not in code_strings:\n    print('unknown flag missing', file=os.stderr)\n    return\n  dst_name = 'emoji_u%s.png' % UNKNOWN_FLAG\n  dst_path = path.join(dst, dst_name)\n  for ali in sorted(OMITTED_FLAGS):\n    ali_str = _flag_str(ali)\n    if ali_str in code_strings:\n      print('omitted flag %s has image %s' % (ali, ali_str), file=os.stderr)\n      continue\n    ali_name = 'emoji_u%s.png' % ali_str\n    print('creating symlink %s (%s) -> unknown_flag (%s)' % (\n        ali_str, ali, dst_name))\n    os.symlink(dst_path, path.join(dst, ali_name))\n\n\ndef materialize_images(src, dst):\n  code_strings = _copy_files(src, dst)\n  _alias_people(code_strings, dst)\n  _alias_flags(code_strings, dst)\n  _alias_omitted_flags(code_strings, dst)\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-s', '--srcdir', help='path to input sources', metavar='dir',\n      default = 'build/compressed_pngs')\n  parser.add_argument(\n      '-d', '--dstdir', help='destination for output images', metavar='dir')\n  args = parser.parse_args()\n  materialize_images(args.srcdir, args.dstdir)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "png",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0693359375,
          "content": "fonttools>=4.7.0\nnotofonttools>=0.2.17\nnanoemoji >= 0.14.3\npytest>=7.4\n"
        },
        {
          "name": "scour_svg.sh",
          "type": "blob",
          "size": 0.8603515625,
          "content": "#!/bin/bash\n\nSRC_DIR=\"\"\nDST_DIR=\"\"\n\nSCOUR_ARGS=\"--strip-xml-prolog --enable-viewboxing --enable-id-stripping --enable-comment-stripping --shorten-ids --no-line-breaks --strip-xml-space\"\n\nwhile [ $# != 0 ]; do\n  case \"$1\" in\n    -s) SRC_DIR=${2}\n        shift\n        shift\n        ;;\n    -d) DST_DIR=${2}\n        shift\n        shift\n        ;;\n    *) echo \"unrecognized arg $1\"\n       exit 1\n       ;;\n  esac\ndone\n\nif [ -z \"$SRC_DIR\" ]; then\n  echo \"missing source directory\"\n  exit 1;\nfi\n\nif [ ! -d \"$SRC_DIR\" ]; then\n  echo \"source directory '$SRC_DIR' does not exist\"\n  exit 1;\nfi\n\nif [ -z \"$DST_DIR\" ]; then\n  echo \"missing destination directory\"\n  exit 1\nfi\n\nif [ ! -d \"$DST_DIR\" ]; then\n  echo \"creating destination directory '$DST_DIR'\"\n  mkdir -p \"$DST_DIR\"\nfi\n\nfor file in \"$SRC_DIR\"/*.svg; do\n  dst=\"${file##*/}\"\n  scour $SCOUR_ARGS -i \"$file\" -o \"$DST_DIR/$dst\"\ndone\n  \n"
        },
        {
          "name": "size_check.py",
          "type": "blob",
          "size": 1.6494140625,
          "content": "\"\"\"Sanity check image sizes and svg viewboxes.\"\"\"\nfrom PIL import Image\nfrom pathlib import Path\nfrom lxml import etree\nimport sys\n\n\ndef _check_image(base_dir, image_dir):\n\tassert image_dir.is_dir()\n\texpected_size = (int(image_dir.name), int(image_dir.name))\n\n\tnum_bad = 0\n\tnum_good = 0\n\tfor image_file in image_dir.iterdir():\n\t\twith Image.open(image_file) as image:\n\t\t\tactual_size = image.size\n\t\tif expected_size != actual_size:\n\t\t\tprint(f\"bad_dim {image_file.relative_to(base_dir)} actual {actual_size} expected {expected_size}\")\n\t\t\tnum_bad += 1\n\t\telse:\n\t\t\tnum_good += 1\n\treturn num_bad, num_good\n\ndef _check_svg(base_dir, svg_dir):\n\texpected_viewbox = (0.0, 0.0, 128.0, 128.0)\n\tnum_bad = 0\n\tnum_good = 0\n\tfor svg_file in svg_dir.iterdir():\n\t\tif not svg_file.name.startswith(\"emoji_u\"):\n\t\t\tcontinue\n\t\tassert svg_file.is_file()\n\t\twith open(svg_file) as f:\n\t\t\tactual_viewbox = etree.parse(f).getroot().attrib[\"viewBox\"]\n\t\tactual_viewbox = tuple(float(s) for s in actual_viewbox.split(\" \"))\n\t\tif expected_viewbox != actual_viewbox:\n\t\t\tprint(f\"bad_dim {svg_file.relative_to(base_dir)} actual {actual_viewbox} expected {expected_viewbox}\")\n\t\t\tnum_bad += 1\n\t\telse:\n\t\t\tnum_good += 1\n\treturn num_bad, num_good\n\ndef main():\n\tbase_dir = Path(__file__).parent\n\timage_dir = base_dir / \"png\"\n\tsvg_dir = base_dir / \"svg\"\n\n\tassert image_dir.is_dir()\n\tassert svg_dir.is_dir()\n\n\tfor size_dir in image_dir.iterdir():\n\t\tnum_bad, num_good = _check_image(base_dir, size_dir)\n\t\tprint(f\"{num_bad}/{num_bad+num_good} issues with {size_dir}\")\n\tnum_bad, num_good = _check_svg(base_dir, svg_dir)\n\tprint(f\"{num_bad}/{num_bad+num_good} issues with {svg_dir}\")\n\tsys.exit(num_bad)\n\nif __name__ == \"__main__\":\n   main()"
        },
        {
          "name": "strip_vs_from_filenames.py",
          "type": "blob",
          "size": 2.61328125,
          "content": "#!/usr/bin/env python3\n#\n# Copyright 2017 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport glob\nimport os\nfrom os import path\nimport sys\n\n\"\"\"Rename image files based on codepoints to remove the emoji variation\nselector from the name.  For our emoji image data, this codepoint is not\nrelevant.\"\"\"\n\nEMOJI_VS = 0xfe0f\n\n\ndef str_to_seq(seq_str):\n  return tuple([int(s, 16) for s in seq_str.split('_')])\n\n\ndef seq_to_str(seq):\n  return '_'.join('%04x' % cp for cp in seq)\n\n\ndef strip_vs(seq):\n  return tuple([cp for cp in seq if cp != EMOJI_VS])\n\n\ndef strip_vs_from_filenames(imagedir, prefix, ext, dry_run=False):\n  prefix_len = len(prefix)\n  suffix_len = len(ext) + 1\n  names = [path.basename(f)\n           for f in glob.glob(\n               path.join(imagedir, '%s*.%s' % (prefix, ext)))]\n  renames = {}\n  for name in names:\n    seq = str_to_seq(name[prefix_len:-suffix_len])\n    if seq and EMOJI_VS in seq:\n      newname = '%s%s.%s' % (prefix, seq_to_str(strip_vs(seq)), ext)\n      if newname in names:\n        print('%s non-vs name %s already exists.' % (\n            name, newname), file=sys.stderr)\n        return\n      renames[name] = newname\n\n  for k, v in renames.iteritems():\n    if dry_run:\n      print('%s -> %s' % (k, v))\n    else:\n      os.rename(path.join(imagedir, k), path.join(imagedir, v))\n  print('renamed %d files in %s' % (len(renames), imagedir))\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '-d', '--imagedir', help='directory containing images to rename',\n      metavar='dir', required=True)\n  parser.add_argument(\n      '-e', '--ext', help='image filename extension (default png)',\n      choices=['ai', 'png', 'svg'], default='png')\n  parser.add_argument(\n      '-p', '--prefix', help='image filename prefix (default emoji_u)',\n      default='emoji_u', metavar='pfx')\n  parser.add_argument(\n      '-n', '--dry_run', help='compute renames and list only',\n      action='store_true')\n\n  args = parser.parse_args()\n  strip_vs_from_filenames(args.imagedir, args.prefix, args.ext, args.dry_run)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "svg",
          "type": "tree",
          "content": null
        },
        {
          "name": "svg_builder.py",
          "type": "blob",
          "size": 8.24609375,
          "content": "# Copyright 2015 Google, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Google Author(s): Doug Felt\n\nimport math\nimport random\nimport re\nimport string\n\nimport svg_cleaner\n\nclass SvgBuilder(object):\n  \"\"\"Modifies a font to add SVG glyphs from a document or string.  Once built you\n  can call add_from_filename or add_from_doc multiple times to add SVG\n  documents, which should contain a single root svg element representing the glyph.\n  This element must have width and height attributes (in px), these are used to\n  determine how to scale the glyph.  The svg should be designed to fit inside\n  this bounds and have its origin at the top left.  Adding the svg generates a\n  transform to scale and position the glyph, so the svg element should not have\n  a transform attribute since it will be overwritten.  Any id attribute on the\n  glyph is also overwritten.\n\n  Adding a glyph can generate additional default glyphs for components of a\n  ligature that are not already present.\n\n  It is possible to add SVG images to a font that already has corresponding\n  glyphs.  If a glyph exists already, then its hmtx advance is assumed valid.\n  Otherwise we will generate an advance based on the image's width and scale\n  factor.  Callers should ensure that glyphs for components of ligatures are\n  added before the ligatures themselves, otherwise glyphs generated for missing\n  ligature components will be assigned zero metrics metrics that will not be\n  overridden later.\"\"\"\n\n  def __init__(self, font_builder):\n    font_builder.init_svg()\n\n    self.font_builder = font_builder\n    self.cleaner = svg_cleaner.SvgCleaner()\n\n    font = font_builder.font\n    self.font_ascent = font['hhea'].ascent\n    self.font_height = self.font_ascent - font['hhea'].descent\n    self.font_upem = font['head'].unitsPerEm\n\n  def add_from_filename(self, ustr, filename):\n    with open(filename, \"r\") as fp:\n      return self.add_from_doc(ustr, fp.read(), filename=filename)\n\n  def _strip_px(self, val):\n    return float(val[:-2] if val.endswith('px') else val)\n\n  def add_from_doc(self, ustr, svgdoc, filename=None):\n    \"\"\"Cleans the svg doc, tweaks the root svg element's\n    attributes, then updates the font.  ustr is the character or ligature\n    string, svgdoc is the svg document xml.  The doc must have a single\n    svg root element.\"\"\"\n\n    # The svg element must have an id attribute of the form 'glyphNNN' where NNN\n    # is the glyph id.  We capture the index of the glyph we're adding and write\n    # it into the svg.\n    #\n    # We generate a transform that places the origin at the top left of the\n    # ascent and uniformly scales it to fit both the font height (ascent -\n    # descent) and glyph advance if it is already present.  The initial viewport\n    # is 1000x1000. When present, viewBox scales to fit this and uses default\n    # values for preserveAspectRatio that center the viewBox in this viewport\n    # ('xMidyMid meet'), and ignores the width and height.  If viewBox is not\n    # present, width and height cause a (possibly non-uniform) scale to be\n    # applied that map the extent to the viewport.  This is unfortunate for us,\n    # since we want to preserve the aspect ratio, and the image is likely\n    # designed for a viewport with the width and height it requested.\n    #\n    # If we have an advance, we want to replicate the behavior of viewBox,\n    # except using a 'viewport' of advance, ascent+descent. If we don't have\n    # an advance, we scale the height and compute the advance from the scaled\n    # width.\n    #\n    # Lengths using percentage units map 100% to the width/height/diagonal\n    # of the viewBox, or if it is not defined, the viewport.  Since we can't\n    # define the viewport, we must always have a viewBox.\n\n    cleaner = self.cleaner\n    fbuilder = self.font_builder\n\n    tree = cleaner.tree_from_text(svgdoc)\n\n    name, index, exists = fbuilder.add_components_and_ligature(ustr)\n\n    advance = 0\n    if exists:\n      advance = fbuilder.hmtx[name][0]\n\n    vb = tree.attrs.get('viewBox')\n    if vb:\n      x, y, w, h = map(self._strip_px, re.split('\\s*,\\s*|\\s+', vb))\n    else:\n      wid = tree.attrs.get('width')\n      ht = tree.attrs.get('height')\n      if not (wid and ht):\n        raise ValueError(\n            'missing viewBox and width or height attrs (%s)' % filename)\n      x, y, w, h = 0, 0, self._strip_px(wid), self._strip_px(ht)\n\n    # We're going to assume default values for preserveAspectRatio for now,\n    # this preserves aspect ratio and centers in the viewport.\n    #\n    # The viewport is 0,0 1000x1000. First compute the scaled extent and\n    # translations that center the image rect in the viewport, then scale and\n    # translate the result to fit our true 'viewport', which has an origin at\n    # 0,-ascent and an extent of advance (if defined) x font_height.  We won't\n    # try to optimize this, it's clearer what we're doing this way.\n\n    # Since the viewport is square, we can just compare w and h to determine\n    # which to fit to the viewport extent.  Get our position and extent in\n    # the viewport.\n    if w > h:\n        scale_to_viewport = 1000.0 / w\n        h_in_viewport = scale_to_viewport * h\n        y_in_viewport = (1000 - h_in_viewport) / 2\n        w_in_viewport = 1000.0\n        x_in_viewport = 0.0\n    else:\n        scale_to_viewport = 1000.0 / h\n        h_in_viewport = 1000.0\n        y_in_viewport = 0.0\n        w_in_viewport = scale_to_viewport * w\n        x_in_viewport = (1000 - w_in_viewport) / 2\n\n    # Now, compute the scale and translations that fit this rectangle to our\n    # true 'viewport'.  The true viewport is not square so we need to choose the\n    # smaller of the scales that fit its height or width.  We start with height,\n    # if there's no advance then we're done, otherwise we might have to fit the\n    # advance.\n    scale = self.font_height / h_in_viewport\n    fit_height = True\n    if advance and scale * w_in_viewport > advance:\n      scale = advance / w_in_viewport\n      fit_height = False\n\n    # Compute transforms that put the top left of the image where we want it.\n    ty = -self.font_ascent - scale * y_in_viewport\n    tx = -scale * x_in_viewport\n\n    # Adjust them to center the image horizontally if we fit the full height,\n    # vertically otherwise.\n    if fit_height and advance:\n      tx += (advance - scale * w_in_viewport) / 2\n    else:\n      ty += (self.font_height - scale * h_in_viewport) / 2\n\n    cleaner.clean_tree(tree)\n\n    tree.attrs['id'] = 'glyph%s' % index\n\n    transform = 'translate(%g, %g) scale(%g)' % (tx, ty, scale)\n    tree.attrs['transform'] = transform\n\n    tree.attrs['viewBox'] = '%g %g %g %g' % (x, y, w, h)\n\n    # In order to clip, we need to create a path and reference it.  You'd think\n    # establishing a rectangular clip would be simpler...  Aaaaand... as it\n    # turns out, in FF the clip on the outer svg element is only relative to the\n    # initial viewport, and is not affected by the viewBox or transform on the\n    # svg element.  Unlike chrome.  So either we apply an inverse transform, or\n    # insert a group with the clip between the svg and its children.  The latter\n    # seems cleaner, ultimately.\n    clip_id = 'clip_' + ''.join(\n        random.choice(string.ascii_lowercase) for i in range(8))\n    clip_text = ('<g clip-path=\"url(#%s)\"><clipPath id=\"%s\">'\n      '<path d=\"M%g %gh%gv%gh%gz\"/></clipPath></g>' % (\n          clip_id, clip_id, x, y, w, h, -w))\n    clip_tree = cleaner.tree_from_text(clip_text)\n    clip_tree.contents.extend(tree.contents)\n    tree.contents = [clip_tree]\n\n    svgdoc = cleaner.tree_to_text(tree)\n\n    hmetrics = None\n    if not exists:\n      # There was no advance to fit, so no horizontal centering. The image advance is\n      # all there is.\n      # hmetrics is horiz advance and lsb\n      advance = scale * w_in_viewport\n      hmetrics = [int(round(advance)), 0]\n\n    fbuilder.add_svg(svgdoc, hmetrics, name, index)\n"
        },
        {
          "name": "svg_cleaner.py",
          "type": "blob",
          "size": 12.0546875,
          "content": "#!/usr/bin/env python3\n# Copyright 2015 Google, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Google Author(s): Doug Felt\n\n\"\"\"Clean SVG.\n\nsvgo could do this, but we're fussy.  Also, emacs doesn't understand\nthat 'style' defaults to 'text/css' and svgo strips this out by\ndefault.\n\nThe files we're getting that are exported from AI contain lots of extra\ndata so that it can reimport the svg, and we don't need it.\"\"\"\n\n\nimport argparse\nimport codecs\nimport logging\nimport os\nfrom os import path\nimport re\nimport sys\n\nfrom nototools import tool_utils\n\nfrom xml.parsers import expat\nfrom xml.sax import saxutils\n\n# Expat doesn't allow me to identify empty tags (in particular, with an\n# empty tag the parse location for the start and end is not the same) so I\n# have to take a dom-like approach if I want to identify them. There are a\n# lot of empty tags in svg.  This way I can do some other kinds of cleanup\n# as well (remove unnecessary 'g' elements, for instance).\n\n# Use nodes instead of tuples and strings because it's easier to mutate\n# a tree of these, and cleaner will want to do this.\n\nclass _Elem_Node(object):\n  def __init__(self, name, attrs, contents):\n    self.name = name\n    self.attrs = attrs\n    self.contents = contents\n\n  def __repr__(self):\n    line = [\"elem(name: '%s'\" % self.name]\n    if self.attrs:\n      line.append(\" attrs: '%s'\" % self.attrs)\n    if self.contents:\n      line.append(\" contents[%s]: '%s'\" % (len(self.contents), self.contents))\n    line.append(')')\n    return ''.join(line)\n\nclass _Text_Node(object):\n  def __init__(self, text):\n    self.text = text\n\n  def __repr__(self):\n    return \"text('%s')\" % self.text\n\nclass SvgCleaner(object):\n  \"\"\"Strip out unwanted parts of an svg file, primarily the xml declaration and\n  doctype lines, comments, and some attributes of the outermost <svg> element.\n  The id will be replaced when it is inserted into the font. (viewBox causes\n  unwanted scaling when used in a font and its effect is difficult to\n  predict, but for outside a font we need to keep it sometimes so we keep it).\n  version is unneeded, xml:space is ignored (we're processing spaces\n  so a request to maintain them has no effect).  enable-background appears to\n  have no effect.  x and y on the outermost svg element have no effect.  We\n  keep width and height, and will elsewhere assume these are the dimensions\n  used for the character box.\"\"\"\n\n  def __init__(self, strip=False):\n    self.reader = SvgCleaner._Reader()\n    self.cleaner = SvgCleaner._Cleaner()\n    self.writer = SvgCleaner._Writer(strip)\n\n  class _Reader(object):\n    \"\"\"Loosely based on fonttools's XMLReader.  This generates a tree of nodes,\n    either element nodes or text nodes.  Successive text content is merged\n    into one node, so contents will never contain more than one _Text_Node in\n    a row.  This drops comments, xml declarations, and doctypes.\"\"\"\n\n    def _reset(self, parser):\n      self._stack = []\n      self._textbuf = []\n\n    def _start_element(self, name, attrs):\n      self._flush_textbuf()\n      node = _Elem_Node(name, attrs, [])\n      if len(self._stack):\n        self._stack[-1].contents.append(node)\n      self._stack.append(node)\n\n    def _end_element(self, name):\n      self._flush_textbuf()\n      if len(self._stack) > 1:\n        self._stack = self._stack[:-1]\n\n    def _character_data(self, data):\n      if len(self._stack):\n        self._textbuf.append(data)\n\n    def _flush_textbuf(self):\n      if self._textbuf:\n        node = _Text_Node(''.join(self._textbuf))\n        self._stack[-1].contents.append(node)\n        self._textbuf = []\n\n    def from_text(self, data):\n      \"\"\"Return the root node of a tree representing the svg data.\"\"\"\n\n      parser = expat.ParserCreate()\n      parser.StartElementHandler = self._start_element\n      parser.EndElementHandler = self._end_element\n      parser.CharacterDataHandler = self._character_data\n      self._reset(parser)\n      parser.Parse(data)\n      return self._stack[0]\n\n  class _Cleaner(object):\n    def _clean_elem(self, node):\n      viewBox, x, y, width, height = None, None, None, None, None\n      nattrs = {}\n      for k, v in node.attrs.items():\n        if node.name == 'svg' and k in [\n            'x', 'y', 'id', 'version', 'viewBox', 'width', 'height',\n            'enable-background', 'xml:space', 'xmlns:graph', 'xmlns:i',\n            'xmlns:x']:\n          if k == 'viewBox':\n            viewBox = v\n          elif k == 'width':\n            width = v\n          elif k == 'height':\n            height = v\n          elif k.startswith('xmlns:') and 'ns.adobe.com' not in v:\n            # keep if not an adobe namespace\n            logging.debug('keep \"%s\" = \"%s\"' % (k, v))\n            nattrs[k] = v\n          logging.debug('removing %s=%s' % (k, v))\n          continue\n        v = re.sub('\\s+', ' ', v)\n        nattrs[k] = v\n\n      if node.name == 'svg':\n        if viewBox:\n          x, y, width, height = viewBox.split()\n        if not width or not height:\n          if not viewBox:\n            raise ValueError('no viewBox, width, or height')\n        nattrs['width'] = width\n        nattrs['height'] = height\n        # keep for svg use outside of font\n        if viewBox and (int(x) != 0 or int(y) != 0):\n          logging.warn('viewbox \"%s\" x: %s y: %s' % (viewBox, x, y));\n          nattrs['viewBox'] = viewBox\n      node.attrs = nattrs\n\n      # if display:none, skip this and its children\n      style = node.attrs.get('style')\n      if (style and 'display:none' in style) or node.attrs.get('display') == 'none':\n          node.contents = []\n          return\n\n      # scan contents. remove any empty text nodes, or empty 'g' element nodes.\n      # if a 'g' element has no attrs and only one subnode, replace it with the\n      # subnode.\n      wpos = 0\n      for n in node.contents:\n        if isinstance(n, _Text_Node):\n          if not n.text:\n            continue\n        elif n.name == 'g':\n          if not n.contents:\n            continue\n          if 'i:extraneous' in n.attrs:\n            del n.attrs['i:extraneous']\n          if not n.attrs and len(n.contents) == 1:\n            n = n.contents[0]\n        elif n.name == 'i:pgf' or n.name == 'foreignObject':\n          continue\n        elif n.name =='switch' and len(n.contents) == 1:\n          n = n.contents[0]\n        elif n.name == 'style':\n          # some emacsen don't default 'style' properly, so leave this in.\n          if False and n.attrs.get('type') == 'text/css':\n            del n.attrs['type']\n\n        node.contents[wpos] = n\n        wpos += 1\n      if wpos < len(node.contents):\n        node.contents = node.contents[:wpos]\n\n    def _clean_text(self, node):\n      text = node.text.strip()\n      # common case is text is empty (line endings between elements)\n      if text:\n        # main goal here is to leave linefeeds in for style elements\n        text = re.sub(r'[ \\t]*\\n+[ \\t]*', '\\n', text)\n        text = re.sub(r'[ \\t]+', ' ', text)\n      node.text = text\n\n    def clean(self, node):\n      if isinstance(node, _Text_Node):\n        self._clean_text(node)\n      else:\n        # do contents first, so we can check for empty subnodes after\n        for n in node.contents:\n          self.clean(n)\n        self._clean_elem(node)\n\n  class _Writer(object):\n    \"\"\"For text nodes, replaces sequences of whitespace with a single space.\n    For elements, replaces sequences of whitespace in attributes, and\n    removes unwanted attributes from <svg> elements.\"\"\"\n    def __init__(self, strip):\n      logging.warning('writer strip: %s' % strip);\n      self._strip = strip\n\n    def _write_node(self, node, lines, indent):\n      \"\"\"Node is a node generated by _Reader, either a TextNode or an\n      ElementNode. Lines is a list to collect the lines of output.  Indent is\n      the indentation level for this node.\"\"\"\n\n      if isinstance(node, _Text_Node):\n        if node.text:\n          lines.append(node.text)\n      else:\n        margin = '' if self._strip else '  ' * indent\n        line = [margin]\n        line.append('<%s' % node.name)\n        # custom sort attributes of svg, yes this is a hack\n        if node.name == 'svg':\n          def svgsort(k):\n            if k == 'width': return (0, None)\n            elif k == 'height': return (1, None)\n            else: return (2, k)\n          ks = sorted(node.attrs.keys(), key=svgsort)\n        else:\n          def defsort(k):\n            if k == 'id': return (0, None)\n            elif k == 'class': return (1, None)\n            else: return (2, k)\n          ks = sorted(node.attrs.keys(), key=defsort)\n        for k in ks:\n          v = node.attrs[k]\n          line.append(' %s=%s' % (k, saxutils.quoteattr(v)))\n        if node.contents:\n          line.append('>')\n          lines.append(''.join(line))\n          for elem in node.contents:\n            self._write_node(elem, lines, indent + 1)\n          line = [margin]\n          line.append('</%s>' % node.name)\n          lines.append(''.join(line))\n        else:\n          line.append('/>')\n          lines.append(''.join(line))\n\n    def to_text(self, root):\n      # set up lines for recursive calls, let them append lines, then return\n      # the result.\n      lines = []\n      self._write_node(root, lines, 0)\n      return ''.join(lines) if self._strip else '\\n'.join(lines)\n\n  def tree_from_text(self, svg_text):\n    return self.reader.from_text(svg_text)\n\n  def clean_tree(self, svg_tree):\n    self.cleaner.clean(svg_tree)\n\n  def tree_to_text(self, svg_tree):\n    return self.writer.to_text(svg_tree)\n\n  def clean_svg(self, svg_text):\n    \"\"\"Return the cleaned svg_text.\"\"\"\n    tree = self.tree_from_text(svg_text)\n    self.clean_tree(tree)\n    return self.tree_to_text(tree)\n\n\ndef clean_svg_files(in_dir, out_dir, match_pat=None, clean=False, strip=False):\n  regex = re.compile(match_pat) if match_pat else None\n  count = 0\n\n  if clean and path.samefile(in_dir, out_dir):\n    logging.error('Cannot clean %s (same as in_dir)', out_dir)\n    return\n\n  out_dir = tool_utils.ensure_dir_exists(out_dir, clean=clean)\n\n  cleaner = SvgCleaner(strip)\n  for file_name in os.listdir(in_dir):\n    if regex and not regex.match(file_name):\n      continue\n    in_path = os.path.join(in_dir, file_name)\n    logging.debug('read: %s', in_path)\n    with open(in_path) as in_fp:\n      result = cleaner.clean_svg(in_fp.read())\n    out_path = os.path.join(out_dir, file_name)\n    with codecs.open(out_path, 'w', 'utf-8') as out_fp:\n      logging.debug('write: %s', out_path)\n      out_fp.write(result)\n      count += 1\n  if not count:\n    logging.warning('Failed to match any files')\n  else:\n    logging.info('Wrote %s files to %s', count, out_dir)\n\n\ndef main():\n  parser = argparse.ArgumentParser(\n      description=\"Generate 'cleaned' svg files.\")\n  parser.add_argument(\n      'in_dir', help='Input directory.', metavar='dir')\n  parser.add_argument(\n      '-o', '--out_dir', help='Output directory, defaults to sibling of in_dir',\n      metavar='dir')\n  parser.add_argument(\n      '-c', '--clean', help='Clean output directory', action='store_true')\n  parser.add_argument(\n      '-r', '--regex', help='Regex to select files, default matches all files.',\n      metavar='regex', default=None)\n  parser.add_argument(\n      '-l', '--loglevel', help='log level name/value', default='warning')\n  parser.add_argument(\n      '-w', '--strip_whitespace', help='remove newlines and indentation',\n      action='store_true')\n  args = parser.parse_args()\n\n  tool_utils.setup_logging(args.loglevel)\n\n  if not args.out_dir:\n    if args.in_dir.endswith('/'):\n      args.in_dir = args.in_dir[:-1]\n    args.out_dir = args.in_dir + '_clean'\n    logging.info('Writing output to %s', args.out_dir)\n\n  clean_svg_files(\n      args.in_dir, args.out_dir, match_pat=args.regex, clean=args.clean,\n      strip=args.strip_whitespace)\n\n\nif __name__ == '__main__':\n  main()\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "unknown_flag_aliases.txt",
          "type": "blob",
          "size": 0.2646484375,
          "content": "# alias table\n# from;to\n# the 'from' sequence should be represented by the image for the 'to' sequence\n# these are flags we explicitly omit\n\n# flag aliases\n1f1e7_1f1f1;fe82b # BL\n1f1e7_1f1f6;fe82b # BQ\n1f1f2_1f1f6;fe82b # MQ\n1f1f7_1f1ea;fe82b # RE\n1f1f9_1f1eb;fe82b # TF\n"
        },
        {
          "name": "update_flag_name.py",
          "type": "blob",
          "size": 0.8212890625,
          "content": "\"\"\"Updates the name table for the CBDT flagsonly font.\"\"\"\n\nfrom fontTools import subset\nfrom fontTools import ttLib\nimport functools\nfrom pathlib import Path\nimport sys\nfrom typing import Set\n\n\nNAME_ID_FAMILY = 1\nNAME_ID_UNIQUE_ID = 3\nNAME_ID_FULLNAME = 4\nNAME_ID_POSTSCRIPT_NAME = 6\n\n\n_NAME_VALUES = [\n    (NAME_ID_FAMILY, \"Noto Color Emoji Flags\"),\n    (NAME_ID_UNIQUE_ID, \"Noto Color Emoji Flags\"),\n    (NAME_ID_FULLNAME, \"Noto Color Emoji Flags\"),\n    (NAME_ID_POSTSCRIPT_NAME, \"NotoColorEmojiFlags\"),\n]\n\n\ndef main(argv):\n    font_file = \"fonts/NotoColorEmoji-flagsonly.ttf\"\n    font = ttLib.TTFont(font_file)\n    name_table = font[\"name\"]\n    for (name_id, value) in _NAME_VALUES:\n        name = name_table.getName(name_id, 3, 1, 0x409)\n        name.string = value\n    font.save(font_file)\n\n\nif __name__ == '__main__':\n  main(sys.argv)\n"
        },
        {
          "name": "waveflag.c",
          "type": "blob",
          "size": 11.869140625,
          "content": "/*\n * Copyright 2014 Google Inc. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * Google contributors: Behdad Esfahbod\n */\n\n#include <cairo.h>\n#include <libgen.h> // basename\n#include <math.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <assert.h>\n#include <string.h>\n\n\n#define SCALE 8\n#define SIZE 128\n#define MARGIN (debug ? 4 : 0)\n\nstatic unsigned int debug;\n\n#define std_aspect (5./3.)\n#define top 21\n#define bot 128-top\n#define B 21\n#define C 4\nstatic struct { double x, y; } mesh_points[] =\n{\n  {  1, top+C},\n  { 43, top-B+C},\n  { 85, top+B-C},\n  {127, top-C},\n  {127, bot-C},\n  { 85, bot+B-C},\n  { 43, bot-B+C},\n  {  1, bot+C},\n};\n#define M(i) \\\n\tx_aspect (mesh_points[i].x, aspect), \\\n\ty_aspect (mesh_points[i].y, aspect)\n\nstatic inline double x_aspect (double v, double aspect)\n{\n\treturn aspect >= 1. ? v : (v - 64) * aspect + 64;\n}\nstatic inline double y_aspect (double v, double aspect)\n{\n\treturn aspect <= 1. ? v : (v - 64) / aspect + 64;\n}\n\nstatic cairo_path_t *\nwave_path_create (double aspect)\n{\n\tcairo_surface_t *surface = cairo_image_surface_create (CAIRO_FORMAT_ARGB32, 0,0);\n\tcairo_t *cr = cairo_create (surface);\n\tcairo_path_t *path;\n\n\tcairo_scale (cr, SIZE/128.*SCALE, SIZE/128.*SCALE);\n\n\tcairo_line_to(cr,   M(0));\n\tcairo_curve_to(cr,  M(1), M(2), M(3));\n\tcairo_line_to(cr,   M(4));\n\tcairo_curve_to(cr,  M(5), M(6), M(7));\n\tcairo_close_path (cr);\n\n\tcairo_identity_matrix (cr);\n\tpath = cairo_copy_path (cr);\n\tcairo_destroy (cr);\n\tcairo_surface_destroy (surface);\n\n\treturn path;\n}\n\nstatic cairo_pattern_t *\nwave_mesh_create (double aspect, int alpha)\n{\n\tcairo_pattern_t *pattern = cairo_pattern_create_mesh();\n\tcairo_matrix_t scale_matrix = {128./SIZE/SCALE, 0, 0, 128./SIZE/SCALE, 0, 0};\n\tcairo_pattern_set_matrix (pattern, &scale_matrix);\n\tcairo_mesh_pattern_begin_patch(pattern);\n\n\tcairo_mesh_pattern_line_to(pattern,   M(0));\n\tcairo_mesh_pattern_curve_to(pattern,  M(1), M(2), M(3));\n\tcairo_mesh_pattern_line_to(pattern,   M(4));\n\tcairo_mesh_pattern_curve_to(pattern,  M(5), M(6), M(7));\n\n\tif (alpha)\n\t{\n\t\tcairo_mesh_pattern_set_corner_color_rgba(pattern, 0, 1, 1, 1, .5);\n\t\tcairo_mesh_pattern_set_corner_color_rgba(pattern, 1,.5,.5,.5, .5);\n\t\tcairo_mesh_pattern_set_corner_color_rgba(pattern, 2, 0, 0, 0, .5);\n\t\tcairo_mesh_pattern_set_corner_color_rgba(pattern, 3,.5,.5,.5, .5);\n\t}\n\telse\n\t{\n\t\tcairo_mesh_pattern_set_corner_color_rgb(pattern, 0, 0, 0, .5);\n\t\tcairo_mesh_pattern_set_corner_color_rgb(pattern, 1, 1, 0, .5);\n\t\tcairo_mesh_pattern_set_corner_color_rgb(pattern, 2, 1, 1, .5);\n\t\tcairo_mesh_pattern_set_corner_color_rgb(pattern, 3, 0, 1, .5);\n\t}\n\n\tcairo_mesh_pattern_end_patch(pattern);\n\n\treturn pattern;\n}\n\nstatic cairo_surface_t *\nscale_flag (cairo_surface_t *flag)\n{\n\tunsigned int w = cairo_image_surface_get_width  (flag);\n\tunsigned int h = cairo_image_surface_get_height (flag);\n\tcairo_surface_t *scaled = cairo_image_surface_create (CAIRO_FORMAT_ARGB32, 256,256);\n\tcairo_t *cr = cairo_create (scaled);\n\n\tcairo_scale (cr, 256./w, 256./h);\n\n\tcairo_set_source_surface (cr, flag, 0, 0);\n\tcairo_pattern_set_filter (cairo_get_source (cr), CAIRO_FILTER_BEST);\n\tcairo_pattern_set_extend (cairo_get_source (cr), CAIRO_EXTEND_PAD);\n\tcairo_paint (cr);\n\n\tcairo_destroy (cr);\n\treturn scaled;\n}\n\nstatic cairo_surface_t *\nload_scaled_flag (const char *filename, double *aspect)\n{\n\tcairo_surface_t *flag = cairo_image_surface_create_from_png (filename);\n\tcairo_surface_t *scaled = scale_flag (flag);\n\t*aspect = (double) cairo_image_surface_get_width (flag) /\n\t\t  (double) cairo_image_surface_get_height (flag);\n\tcairo_surface_destroy (flag);\n\treturn scaled;\n}\n\nstatic int\nis_transparent (uint32_t pix)\n{\n\treturn ((pix>>24) < 0xff);\n}\n\nstatic int\nborder_is_transparent (cairo_surface_t *scaled_flag)\n{\n\t/* Some flags might have a border already.  As such, skip\n\t * a few pixels on each side... */\n\tconst unsigned int skip = 5;\n\tuint32_t *s = (uint32_t *) cairo_image_surface_get_data (scaled_flag);\n\tunsigned int width  = cairo_image_surface_get_width (scaled_flag);\n\tunsigned int height = cairo_image_surface_get_height (scaled_flag);\n\tunsigned int sstride = cairo_image_surface_get_stride (scaled_flag) / 4;\n\n\tint transparent = 0;\n\n\tassert (width > 2 * skip && height > 2 * skip);\n\n\n\tfor (unsigned int x = skip; x < width - skip; x++)\n\t\ttransparent |= is_transparent (s[x]);\n\ts += sstride;\n\tfor (unsigned int y = 1 + skip; y < height - 1 - skip; y++)\n\t{\n\t\ttransparent |= is_transparent (s[skip]);\n\t\ttransparent |= is_transparent (s[width - 1 - skip]);\n\t\ts += sstride;\n\t}\n\tfor (unsigned int x = skip; x < width - skip; x++)\n\t\ttransparent |= is_transparent (s[x]);\n\n\treturn transparent;\n}\n\nstatic cairo_t *\ncreate_image (void)\n{\n\tcairo_surface_t *surface = cairo_image_surface_create (CAIRO_FORMAT_ARGB32,\n\t\t\t\t\t\t\t       (SIZE+2*MARGIN)*SCALE,\n\t\t\t\t\t\t\t       (SIZE+2*MARGIN)*SCALE);\n\tcairo_t *cr = cairo_create (surface);\n\tcairo_surface_destroy (surface);\n\treturn cr;\n}\n\nstatic cairo_surface_t *\nwave_surface_create (double aspect)\n{\n\tcairo_t *cr = create_image ();\n\tcairo_surface_t *surface = cairo_surface_reference (cairo_get_target (cr));\n\tcairo_pattern_t *mesh = wave_mesh_create (aspect, 0);\n\tcairo_set_source (cr, mesh);\n\tcairo_paint (cr);\n\tcairo_pattern_destroy (mesh);\n\tcairo_destroy (cr);\n\treturn surface;\n}\n\nstatic cairo_surface_t *\ntexture_map (cairo_surface_t *src, cairo_surface_t *tex)\n{\n\tuint32_t *s = (uint32_t *) cairo_image_surface_get_data (src);\n\tunsigned int width  = cairo_image_surface_get_width (src);\n\tunsigned int height = cairo_image_surface_get_height (src);\n\tunsigned int sstride = cairo_image_surface_get_stride (src) / 4;\n\n\tcairo_surface_t *dst = cairo_image_surface_create (CAIRO_FORMAT_ARGB32, width, height);\n\tuint32_t *d = (uint32_t *) cairo_image_surface_get_data (dst);\n\tunsigned int dstride = cairo_image_surface_get_stride (dst) / 4;\n\n\tuint32_t *t = (uint32_t *) cairo_image_surface_get_data (tex);\n\tunsigned int twidth  = cairo_image_surface_get_width (tex);\n\tunsigned int theight = cairo_image_surface_get_height (tex);\n\tunsigned int tstride = cairo_image_surface_get_stride (tex) / 4;\n\n\tassert (twidth == 256 && theight == 256);\n\n\tfor (unsigned int y = 0; y < height; y++)\n\t{\n\t\tfor (unsigned int x = 0; x < width; x++)\n\t\t{\n\t\t\tunsigned int pix = s[x];\n\t\t\tunsigned int sa = pix >> 24;\n\t\t\tunsigned int sr = (pix >> 16) & 0xFF;\n\t\t\tunsigned int sg = (pix >>  8) & 0xFF;\n\t\t\tunsigned int sb = (pix      ) & 0xFF;\n\t\t\tif (sa == 0)\n\t\t\t{\n\t\t\t\td[x] = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (sa != 255)\n\t\t\t{\n\t\t\t\tsr = sr * 255 / sa;\n\t\t\t\tsg = sg * 255 / sa;\n\t\t\t\tsb = sb * 255 / sa;\n\t\t\t}\n\t\t\tassert (sb >= 127 && sb <= 129);\n\t\t\td[x] = t[tstride * sg + sr];\n\t\t}\n\t\ts += sstride;\n\t\td += dstride;\n\t}\n\tcairo_surface_mark_dirty (dst);\n\n\treturn dst;\n}\n\nstatic void\nwave_flag (const char *filename, const char *out_prefix)\n{\n\tstatic cairo_path_t *standard_wave_path;\n\tstatic cairo_surface_t *standard_wave_surface;\n\tcairo_path_t *wave_path;\n\tcairo_surface_t *wave_surface;\n\tint border_transparent;\n\tchar out[1000];\n\tdouble aspect = 0;\n\n\tcairo_surface_t *scaled_flag, *waved_flag;\n\tcairo_t *cr;\n\n\tif (debug) printf (\"Processing %s\\n\", filename);\n\n\tscaled_flag = load_scaled_flag (filename, &aspect);\n\n\taspect /= std_aspect;\n\taspect = sqrt (aspect); // Discount the effect\n\tif (.9 <= aspect && aspect <= 1.1)\n\t{\n\t\tif (debug) printf (\"Standard aspect ratio\\n\");\n\t\taspect = 1.;\n\t}\n\n\tif (aspect == 1.)\n\t{\n\t\tif (!standard_wave_path)\n\t\t\tstandard_wave_path = wave_path_create (aspect);\n\t\tif (!standard_wave_surface)\n\t\t\tstandard_wave_surface = wave_surface_create (aspect);\n\t\twave_path = standard_wave_path;\n\t\twave_surface = standard_wave_surface;\n\t}\n\telse\n\t{\n\t\twave_path = wave_path_create (aspect);\n\t\twave_surface = wave_surface_create (aspect);\n\t}\n\n\n\tborder_transparent = border_is_transparent (scaled_flag);\n\twaved_flag = texture_map (wave_surface, scaled_flag);\n\tcairo_surface_destroy (scaled_flag);\n\n\tcr = create_image ();\n\tcairo_translate (cr, SCALE * MARGIN, SCALE * MARGIN);\n\n\t// Paint waved flag\n\tcairo_set_source_surface (cr, waved_flag, 0, 0);\n\tcairo_append_path (cr, wave_path);\n\tif (!debug)\n\t\tcairo_clip_preserve (cr);\n\tcairo_paint (cr);\n\n\t// Paint border\n\tif (!border_transparent)\n\t{\n\t\tdouble border_alpha = .2;\n\t\tdouble border_width = 4 * SCALE;\n\t\tdouble border_gray = 0x42/255.;\n\t\tif (debug)\n\t\t\tprintf (\"Border: alpha %g width %g gray %g\\n\",\n\t\t\t\tborder_alpha, border_width/SCALE, border_gray);\n\n\t\tcairo_save (cr);\n\t\tcairo_set_source_rgba (cr,\n\t\t\t\t       border_gray * border_alpha,\n\t\t\t\t       border_gray * border_alpha,\n\t\t\t\t       border_gray * border_alpha,\n\t\t\t\t       border_alpha);\n\t\tcairo_set_line_width (cr, 2*border_width);\n\t\tif (!debug)\n\t\t\tcairo_set_operator (cr, CAIRO_OPERATOR_MULTIPLY);\n\t\tcairo_stroke (cr);\n\t\tcairo_restore (cr);\n\t}\n\telse\n\t{\n\t\tif (debug) printf (\"Transparent border\\n\");\n\t\tcairo_new_path (cr);\n\t}\n\n\t// Paint shade gradient\n\t{\n\t\tcairo_pattern_t *gradient = wave_mesh_create (aspect, 1);\n\t\tcairo_pattern_t *w = cairo_pattern_create_for_surface (waved_flag);\n\n\t\tcairo_save (cr);\n\t\tcairo_set_source (cr, gradient);\n\n\t\tcairo_set_operator (cr, CAIRO_OPERATOR_SOFT_LIGHT);\n\t\tcairo_mask (cr, w);\n\n\t\tcairo_restore (cr);\n\n\t\tcairo_pattern_destroy (w);\n\t}\n\n\tif (debug)\n\t{\n\t\t/* Draw mesh points. */\n\t\tcairo_save (cr);\n\t\tcairo_scale (cr, SIZE/128.*SCALE, SIZE/128.*SCALE);\n\t\tcairo_set_source_rgba (cr, .5,.0,.0,.9);\n\t\tcairo_set_line_cap (cr, CAIRO_LINE_CAP_ROUND);\n\t\tfor (unsigned int i = 0; i < sizeof (mesh_points) / sizeof (mesh_points[0]); i++)\n\t\t{\n\t\t\tcairo_move_to (cr, M(i));\n\t\t\tcairo_rel_line_to (cr, 0, 0);\n\t\t}\n\t\tcairo_set_line_width (cr, 2);\n\t\tcairo_stroke (cr);\n\t\tfor (unsigned int i = 0; i < 4; i++)\n\t\t{\n\t\t\tcairo_move_to (cr, M(2*i));\n\t\t\tcairo_line_to (cr, M(2*i+1));\n\t\t\tcairo_move_to (cr, M(2*i));\n\t\t\tcairo_line_to (cr, M(7 - 2*i));\n\t\t}\n\t\tcairo_set_line_width (cr, .5);\n\t\tcairo_stroke (cr);\n\t\tcairo_restore (cr);\n\t}\n\n\tif (!debug)\n\t{\n\t\t/* Scale down, 2x at a time, to get best downscaling, because cairo's\n\t\t * downscaling is crap... :( */\n\t\tunsigned int scale = SCALE;\n\t\twhile (scale > 1)\n\t\t{\n\t\t\tcairo_surface_t *old_surface, *new_surface;\n\n\t\t\told_surface = cairo_surface_reference (cairo_get_target (cr));\n\t\t\tassert (scale % 2 == 0);\n\t\t\tscale /= 2;\n\t\t\tcairo_destroy (cr);\n\t\t\tnew_surface = cairo_image_surface_create (CAIRO_FORMAT_ARGB32, (SIZE+2*MARGIN)*scale, (SIZE+2*MARGIN)*scale);\n\t\t\tcr = cairo_create (new_surface);\n\t\t\tcairo_scale (cr, .5, .5);\n\t\t\tcairo_set_source_surface (cr, old_surface, 0, 0);\n\t\t\tcairo_paint (cr);\n\t\t\tcairo_surface_destroy (old_surface);\n\t\t\tcairo_surface_destroy (new_surface);\n\t\t}\n\t}\n\n\t*out = '\\0';\n\tstrcat (out, out_prefix);\n        // diff from upstream. we call this a bit differently, filename might not be in cwd.\n\n        // basename wants a non-const argument.  The problem here is paths that end in a\n        // slash, POSIX basename removes them while GNU just returns a pointer to that\n        // slash.  Since this is supposed to be a filename such input is illegal for us.\n        // We're already not checking for overflow of the output buffer anyway...\n\tstrcat (out, basename((char *) filename));\n\n\tcairo_surface_write_to_png (cairo_get_target (cr), out);\n\tcairo_destroy (cr);\n\tif (wave_path != standard_wave_path)\n\t\tcairo_path_destroy (wave_path);\n\tif (wave_surface != standard_wave_surface)\n\t\tcairo_surface_destroy (wave_surface);\n}\n\nint\nmain (int argc, char **argv)\n{\n\tconst char *out_prefix;\n\n\tif (argc < 3)\n\t{\n\t  fprintf (stderr, \"Usage: waveflag [-debug] out-prefix [in.png]...\\n\");\n\t  return 1;\n\t}\n\n\tif (!strcmp (argv[1], \"-debug\"))\n\t{\n\t  debug = 1;\n\t  argc--, argv++;\n\t}\n\n\tout_prefix = argv[1];\n\targc--, argv++;\n\n\tfor (argc--, argv++; argc; argc--, argv++)\n\t\twave_flag (*argv, out_prefix);\n\n\treturn 0;\n}\n"
        }
      ]
    }
  ]
}