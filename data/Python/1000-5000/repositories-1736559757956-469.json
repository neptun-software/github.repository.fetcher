{
  "metadata": {
    "timestamp": 1736559757956,
    "page": 469,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "joblib/joblib",
      "stars": 3934,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".codecov.yml",
          "type": "blob",
          "size": 0.212890625,
          "content": "codecov:\n  token: 1b7eb264-fd77-469a-829a-e9cd5efd7cef\ncoverage:\n  status:\n    project:\n      default:\n        # Allow coverage to drop by up to 1% in a PR before marking it as\n        # failed\n        threshold: '1%'\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.6796875,
          "content": "*.py[oc]\n*.so\n# setup.py working directory\nbuild\n# setup.py dist directory\ndist\n# Editor temporary/working/backup files\n*$\n.*.sw[nop]\n.sw[nop]\n*~\n[#]*#\n.#*\n*.bak\n*.tmp\n*.tgz\n*.rej\n*.org\n.project\n*.diff\n.settings/\n*.svn/\n# Egg metadata\n*.egg-info\n# The shelf plugin uses this dir\n./.shelf\n# Some IDEs add this directory\n.idea\n.vscode\n\n# Mac droppings\n.DS_Store\ndoc/documentation.zip\ndoc/generated\ndoc/CHANGES.rst\ndoc/README.rst\n# Coverage report\n.coverage\n\n# pytest cache on failure\n.cache\n.pytest_cache\n\n# Generated ctags\ntags\n\n# Generated by sphinx\ndoc/auto_examples/\ndoc/_build\n\n# Dask worker space directory in generated examples\ndask-worker-space/\n\n# Binary files created by benchmarks\n*.npy\n"
        },
        {
          "name": ".mailmap",
          "type": "blob",
          "size": 0.4560546875,
          "content": "Gael Varoquaux <gael.varoquaux@normalesup.org> Gael Varoquaux <gael.varoquaux@normalesup.org>\nGael Varoquaux <gael.varoquaux@normalesup.org> Gael varoquaux <gael.varoquaux@normalesup.org>\nGael Varoquaux <gael.varoquaux@normalesup.org> GaelVaroquaux <gael.varoquaux@normalesup.org>\nGael Varoquaux <gael.varoquaux@normalesup.org> Gael VAROQUAUX <gv985687@is206780.intra.cea.fr>\nGael Varoquaux <gael.varoquaux@normalesup.org> gvaroquaux <gael.varoquaux@normalesup.org>\n\n"
        },
        {
          "name": ".readthedocs-requirements.txt",
          "type": "blob",
          "size": 0.0966796875,
          "content": "sphinx\ndocutils<0.18\nnumpy\nmatplotlib\npillow\nsphinx-gallery\nnumpydoc\npandas\nlz4\ndistributed\npsutil\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.3408203125,
          "content": "version: 2\n\n# Set the version of Python and OS\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n\npython:\n  # make sure joblib is installed in the virtualenv (it is imported in\n  # conf.py)\n  install:\n    - requirements: .readthedocs-requirements.txt\n    - method: pip\n      path: .\n\nsphinx:\n  fail_on_warning: true\n  configuration: doc/conf.py\n"
        },
        {
          "name": "CHANGES.rst",
          "type": "blob",
          "size": 40.10546875,
          "content": "Latest changes\n==============\n\nIn development\n--------------\n\n- Enforce ``age_limit`` is a positive timedelta for ``Memory.reduce_size``,\n  to avoid silently ignoring it.\n  https://github.com/joblib/joblib/pull/1613\n\n- Remove deprecated ``bytes_limit`` argument for ``Memory``, which should\n  be passed directly to ``Memory.reduce_size``.\n  https://github.com/joblib/joblib/pull/1569\n\n- Extend functionality of the ``check_call_in_cache`` method to now also\n  check against cache validity. Before, it would only check for a given call\n  if it is in cache memory.\n  https://github.com/joblib/joblib/pull/1584\n\n- Support for Python 3.13 free-threaded has been added.\n  https://github.com/joblib/joblib/pull/1589\n\n\nRelease 1.4.2 -- 2024/05/02\n---------------------------\n\nDue to maintenance issues, 1.4.1 was not valid and we bumped the version to 1.4.2\n\n\n- Fix a backward incompatible change in ``MemorizedFunc.call`` which needs to\n  return the metadata. Also make sure that ``NotMemorizedFunc.call`` return\n  an empty dict for metadata for consistency.\n  https://github.com/joblib/joblib/pull/1576\n\n\nRelease 1.4.0 -- 2024/04/08\n---------------------------\n\n- Allow caching co-routines with `Memory.cache`.\n  https://github.com/joblib/joblib/pull/894\n\n- Try to cast ``n_jobs`` to int in parallel and raise an error if\n  it fails. This means that ``n_jobs=2.3`` will now result in\n  ``effective_n_jobs=2`` instead of failing.\n  https://github.com/joblib/joblib/pull/1539\n\n- Ensure that errors in the task generator given to Parallel's call\n  are raised in the results consumming thread.\n  https://github.com/joblib/joblib/pull/1491\n\n- Adjust codebase to NumPy 2.0 by changing ``np.NaN`` to ``np.nan``\n  and importing ``byte_bounds`` from ``np.lib.array_utils``.\n  https://github.com/joblib/joblib/pull/1501\n\n- The parameter ``return_as`` in ``joblib.Parallel`` can now be set to\n  ``generator_unordered``. In this case the results will be returned in the\n  order of task completion rather than the order of submission.\n  https://github.com/joblib/joblib/pull/1463\n\n- dask backend now supports ``return_as=generator`` and\n  ``return_as=generator_unordered``.\n  https://github.com/joblib/joblib/pull/1520\n\n- Vendor cloudpickle 3.0.0 and end support for Python 3.7 which has\n  reached end of life.\n  https://github.com/joblib/joblib/pull/1487\n  https://github.com/joblib/joblib/pull/1515\n\nRelease 1.3.2 -- 2023/08/08\n---------------------------\n\n- Fix a regression in ``joblib.Parallel`` introduced in 1.3.0 where\n  explicitly setting ``n_jobs=None`` was not interpreted as \"unset\".\n  https://github.com/joblib/joblib/pull/1475\n\n- Fix a regression in ``joblib.Parallel`` introduced in 1.3.0 where\n  ``joblib.Parallel`` logging methods exposed from inheritance to\n  ``joblib.Logger`` didn't work because of missing logger\n  initialization.\n  https://github.com/joblib/joblib/pull/1494\n\n- Various maintenance updates to the doc, the ci and the test.\n  https://github.com/joblib/joblib/pull/1480,\n  https://github.com/joblib/joblib/pull/1481,\n  https://github.com/joblib/joblib/pull/1476,\n  https://github.com/joblib/joblib/pull/1492\n\nRelease 1.3.1 -- 2023/06/29\n---------------------------\n\n- Fix compatibility with python 3.7 by vendor loky 3.4.1\n  which is compatible with this version.\n  https://github.com/joblib/joblib/pull/1472\n\n\nRelease 1.3.0 -- 2023/06/28\n---------------------------\n\n- Ensure native byte order for memmap arrays in ``joblib.load``.\n  https://github.com/joblib/joblib/issues/1353\n\n- Add ability to change default Parallel backend in tests by setting the\n  ``JOBLIB_TESTS_DEFAULT_PARALLEL_BACKEND`` environment variable.\n  https://github.com/joblib/joblib/pull/1356\n\n- Fix temporary folder creation in `joblib.Parallel` on Linux subsystems on Windows\n  which do have `/dev/shm` but don't have the `os.statvfs` function\n  https://github.com/joblib/joblib/issues/1353\n\n- Drop runtime dependency on ``distutils``. ``distutils`` is going away\n  in Python 3.12 and is deprecated from Python 3.10 onwards. This import\n  was kept around to avoid breaking scikit-learn, however it's now been\n  long enough since scikit-learn deployed a fixed (version 1.1 was released\n  in May 2022) that it should be safe to remove this.\n  https://github.com/joblib/joblib/pull/1361\n\n- A warning is raised when a pickling error occurs during caching operations.\n  In version 1.5, this warning will be turned into an error. For all other\n  errors, a new warning has been introduced: ``joblib.memory.CacheWarning``.\n  https://github.com/joblib/joblib/pull/1359\n\n- Avoid (module, name) collisions when caching nested functions. This fix\n  changes the module name of nested functions, invalidating caches from\n  previous versions of Joblib.\n  https://github.com/joblib/joblib/pull/1374\n\n- Add ``cache_validation_callback`` in :meth:`joblib.Memory.cache`, to allow\n  custom cache invalidation based on the metadata of the function call.\n  https://github.com/joblib/joblib/pull/1149\n\n- Add a ``return_as`` parameter for ``Parallel``, that enables consuming\n  results asynchronously.\n  https://github.com/joblib/joblib/pull/1393,\n  https://github.com/joblib/joblib/pull/1458\n\n- Improve the behavior of ``joblib`` for ``n_jobs=1``, with simplified\n  tracebacks and more efficient running time.\n  https://github.com/joblib/joblib/pull/1393\n\n- Add the ``parallel_config`` context manager to allow for more fine-grained\n  control over the backend configuration. It should be used in place of the\n  ``parallel_backend`` context manager. In particular, it has the advantage\n  of not requiring to set a specific backend in the context manager.\n  https://github.com/joblib/joblib/pull/1392,\n  https://github.com/joblib/joblib/pull/1457\n\n- Add ``items_limit`` and ``age_limit`` in :meth:`joblib.Memory.reduce_size`\n  to make it easy to limit the number of items and remove items that have\n  not been accessed for a long time in the cache.\n  https://github.com/joblib/joblib/pull/1200\n\n- Deprecate ``bytes_limit`` in ``Memory`` as this is not automatically enforced,\n  the limit can be directly passed to :meth:`joblib.Memory.reduce_size` which\n  needs to be called to actually enforce the limit.\n  https://github.com/joblib/joblib/pull/1447\n\n- Vendor ``loky`` 3.4.0 which includes various fixes.\n  https://github.com/joblib/joblib/pull/1422\n\n- Various updates to the documentation and to benchmarking tools.\n  https://github.com/joblib/joblib/pull/1343,\n  https://github.com/joblib/joblib/pull/1348,\n  https://github.com/joblib/joblib/pull/1411,\n  https://github.com/joblib/joblib/pull/1451,\n  https://github.com/joblib/joblib/pull/1427,\n  https://github.com/joblib/joblib/pull/1400\n\n- Move project metadata to ``pyproject.toml``.\n  https://github.com/joblib/joblib/pull/1382,\n  https://github.com/joblib/joblib/pull/1433\n\n- Add more tests to improve python ``nogil`` support.\n  https://github.com/joblib/joblib/pull/1394,\n  https://github.com/joblib/joblib/pull/1395\n\n\nRelease 1.2.0\n-------------\n\n- Fix a security issue where ``eval(pre_dispatch)`` could potentially run\n  arbitrary code. Now only basic numerics are supported.\n  https://github.com/joblib/joblib/pull/1327\n\n- Make sure that joblib works even when multiprocessing is not available,\n  for instance with Pyodide\n  https://github.com/joblib/joblib/pull/1256\n\n- Avoid unnecessary warnings when workers and main process delete\n  the temporary memmap folder contents concurrently.\n  https://github.com/joblib/joblib/pull/1263\n\n- Fix memory alignment bug for pickles containing numpy arrays.\n  This is especially important when loading the pickle with\n  ``mmap_mode != None`` as the resulting ``numpy.memmap`` object\n  would not be able to correct the misalignment without performing\n  a memory copy.\n  This bug would cause invalid computation and segmentation faults\n  with native code that would directly access the underlying data\n  buffer of a numpy array, for instance C/C++/Cython code compiled\n  with older GCC versions or some old OpenBLAS written in platform\n  specific assembly.\n  https://github.com/joblib/joblib/pull/1254\n\n- Vendor cloudpickle 2.2.0 which adds support for PyPy 3.8+.\n\n- Vendor loky 3.3.0 which fixes several bugs including:\n\n  - robustly forcibly terminating worker processes in case of a crash\n    (https://github.com/joblib/joblib/pull/1269);\n\n  - avoiding leaking worker processes in case of nested loky parallel\n    calls;\n\n  - reliability spawn the correct number of reusable workers.\n\nRelease 1.1.1\n-------------\n\n- Fix a security issue where ``eval(pre_dispatch)`` could potentially run\n  arbitrary code. Now only basic numerics are supported.\n  https://github.com/joblib/joblib/pull/1327\n\nRelease 1.1.0\n--------------\n\n- Fix byte order inconsistency issue during deserialization using joblib.load\n  in cross-endian environment: the numpy arrays are now always loaded to\n  use the system byte order, independently of the byte order of the system\n  that serialized the pickle.\n  https://github.com/joblib/joblib/pull/1181\n\n- Fix joblib.Memory bug with the ``ignore`` parameter when the cached function\n  is a decorated function.\n  https://github.com/joblib/joblib/pull/1165\n\n- Fix `joblib.Memory` to properly handle caching for functions defined\n  interactively in a IPython session or in Jupyter notebook cell.\n  https://github.com/joblib/joblib/pull/1214\n\n- Update vendored loky (from version 2.9 to 3.0) and cloudpickle (from\n  version 1.6 to 2.0)\n  https://github.com/joblib/joblib/pull/1218\n\nRelease 1.0.1\n-------------\n\n- Add check_call_in_cache method to check cache without calling function.\n  https://github.com/joblib/joblib/pull/820\n\n- dask: avoid redundant scattering of large arguments to make a more\n  efficient use of the network resources and avoid crashing dask with\n  \"OSError: [Errno 55] No buffer space available\"\n  or \"ConnectionResetError: [Errno 104] connection reset by peer\".\n  https://github.com/joblib/joblib/pull/1133\n\nRelease 1.0.0\n-------------\n\n- Make `joblib.hash` and `joblib.Memory` caching system compatible with `numpy\n  >= 1.20.0`. Also make it explicit in the documentation that users should now\n  expect to have their `joblib.Memory` cache invalidated when either `joblib`\n  or a third party library involved in the cached values definition is\n  upgraded.  In particular, users updating `joblib` to a release that includes\n  this fix will see their previous cache invalidated if they contained\n  reference to `numpy` objects.\n  https://github.com/joblib/joblib/pull/1136\n\n- Remove deprecated `check_pickle` argument in `delayed`.\n  https://github.com/joblib/joblib/pull/903\n\nRelease 0.17.0\n--------------\n\n- Fix a spurious invalidation of `Memory.cache`'d functions called with\n  `Parallel` under Jupyter or IPython.\n  https://github.com/joblib/joblib/pull/1093\n\n- Bump vendored loky to 2.9.0 and cloudpickle to 1.6.0. In particular\n  this fixes a problem to add compat for Python 3.9.\n\nRelease 0.16.0\n--------------\n\n- Fix a problem in the constructors of Parallel backends classes that\n  inherit from the `AutoBatchingMixin` that prevented the dask backend to\n  properly batch short tasks.\n  https://github.com/joblib/joblib/pull/1062\n\n- Fix a problem in the way the joblib dask backend batches calls that would\n  badly interact with the dask callable pickling cache and lead to wrong\n  results or errors.\n  https://github.com/joblib/joblib/pull/1055\n\n- Prevent a dask.distributed bug from surfacing in joblib's dask backend\n  during nested Parallel calls (due to joblib's auto-scattering feature)\n  https://github.com/joblib/joblib/pull/1061\n\n- Workaround for a race condition after Parallel calls with the dask backend\n  that would cause low level warnings from asyncio coroutines:\n  https://github.com/joblib/joblib/pull/1078\n\nRelease 0.15.1\n--------------\n\n- Make joblib work on Python 3 installation that do not ship with the lzma\n  package in their standard library.\n\nRelease 0.15.0\n--------------\n\n- Drop support for Python 2 and Python 3.5. All objects in\n  ``joblib.my_exceptions`` and ``joblib.format_stack`` are now deprecated and\n  will be removed in joblib 0.16. Note that no deprecation warning will be\n  raised for these objects Python < 3.7.\n  https://github.com/joblib/joblib/pull/1018\n\n- Fix many bugs related to the temporary files and folder generated when\n  automatically memory mapping large numpy arrays for efficient inter-process\n  communication. In particular, this would cause `PermissionError` exceptions\n  to be raised under Windows and large leaked files in `/dev/shm` under Linux\n  in case of crash.\n  https://github.com/joblib/joblib/pull/966\n\n- Make the dask backend collect results as soon as they complete\n  leading to a performance improvement:\n  https://github.com/joblib/joblib/pull/1025\n\n- Fix the number of jobs reported by ``effective_n_jobs`` when ``n_jobs=None``\n  called in a parallel backend context.\n  https://github.com/joblib/joblib/pull/985\n\n- Upgraded vendored cloupickle to 1.4.1 and loky to 2.8.0. This allows for\n  Parallel calls of dynamically defined functions with type annotations\n  in particular.\n\n\nRelease 0.14.1\n--------------\n\n- Configure the loky workers' environment to mitigate oversubsription with\n  nested multi-threaded code in the following case:\n\n  - allow for a suitable number of threads for numba (``NUMBA_NUM_THREADS``);\n\n  - enable Interprocess Communication for scheduler coordination when the\n    nested code uses Threading Building Blocks (TBB) (``ENABLE_IPC=1``)\n\n  https://github.com/joblib/joblib/pull/951\n\n- Fix a regression where the loky backend was not reusing previously\n  spawned workers.\n  https://github.com/joblib/joblib/pull/968\n\n- Revert https://github.com/joblib/joblib/pull/847 to avoid using\n  `pkg_resources` that introduced a performance regression under Windows:\n  https://github.com/joblib/joblib/issues/965\n\nRelease 0.14.0\n--------------\n\n- Improved the load balancing between workers to avoid stranglers caused by an\n  excessively large batch size when the task duration is varying significantly\n  (because of the combined use of ``joblib.Parallel`` and ``joblib.Memory``\n  with a partially warmed cache for instance).\n  https://github.com/joblib/joblib/pull/899\n\n- Add official support for Python 3.8: fixed protocol number in `Hasher`\n  and updated tests.\n\n- Fix a deadlock when using the dask backend (when scattering large numpy\n  arrays).\n  https://github.com/joblib/joblib/pull/914\n\n- Warn users that they should never use `joblib.load` with files from\n  untrusted sources. Fix security related API change introduced in numpy\n  1.6.3 that would prevent using joblib with recent numpy versions.\n  https://github.com/joblib/joblib/pull/879\n\n- Upgrade to cloudpickle 1.1.1 that add supports for the upcoming\n  Python 3.8 release among other things.\n  https://github.com/joblib/joblib/pull/878\n\n- Fix semaphore availability checker to avoid spawning resource trackers\n  on module import.\n  https://github.com/joblib/joblib/pull/893\n\n- Fix the oversubscription protection to only protect against nested\n  `Parallel` calls. This allows `joblib` to be run in background threads.\n  https://github.com/joblib/joblib/pull/934\n\n- Fix `ValueError` (negative dimensions) when pickling large numpy arrays on\n  Windows.\n  https://github.com/joblib/joblib/pull/920\n\n- Upgrade to loky 2.6.0 that add supports for the setting environment variables\n  in child before loading any module.\n  https://github.com/joblib/joblib/pull/940\n\n- Fix the oversubscription protection for native libraries using threadpools\n  (OpenBLAS, MKL, Blis and OpenMP runtimes).\n  The maximal number of threads is can now be set in children using the\n  ``inner_max_num_threads`` in ``parallel_backend``. It defaults to\n  ``cpu_count() // n_jobs``.\n  https://github.com/joblib/joblib/pull/940\n\n\nRelease 0.13.2\n--------------\n\nPierre Glaser\n\n   Upgrade to cloudpickle 0.8.0\n\n   Add a non-regression test related to joblib issues #836 and #833, reporting\n   that cloudpickle versions between 0.5.4 and 0.7 introduced a bug where\n   global variables changes in a parent process between two calls to\n   joblib.Parallel would not be propagated into the workers\n\n\nRelease 0.13.1\n--------------\n\nPierre Glaser\n\n   Memory now accepts pathlib.Path objects as ``location`` parameter.\n   Also, a warning is raised if the returned backend is None while\n   ``location`` is not None.\n\nOlivier Grisel\n\n   Make ``Parallel`` raise an informative ``RuntimeError`` when the\n   active parallel backend has zero worker.\n\n   Make the ``DaskDistributedBackend`` wait for workers before trying to\n   schedule work. This is useful in particular when the workers are\n   provisionned dynamically but provisionning is not immediate (for\n   instance using Kubernetes, Yarn or an HPC job queue).\n\n\nRelease 0.13.0\n--------------\n\nThomas Moreau\n\n   Include loky 2.4.2 with default serialization with ``cloudpickle``.\n   This can be tweaked with the environment variable ``LOKY_PICKLER``.\n\nThomas Moreau\n\n   Fix nested backend in SequentialBackend to avoid changing the default\n   backend to Sequential. (#792)\n\nThomas Moreau, Olivier Grisel\n\n    Fix nested_backend behavior to avoid setting the default number of\n    workers to -1 when the backend is not dask. (#784)\n\nRelease 0.12.5\n--------------\n\nThomas Moreau, Olivier Grisel\n\n    Include loky 2.3.1 with better error reporting when a worker is\n    abruptly terminated. Also fixes spurious debug output.\n\n\nPierre Glaser\n\n    Include cloudpickle 0.5.6. Fix a bug with the handling of global\n    variables by locally defined functions.\n\n\nRelease 0.12.4\n--------------\n\nThomas Moreau, Pierre Glaser, Olivier Grisel\n\n    Include loky 2.3.0 with many bugfixes, notably w.r.t. when setting\n    non-default multiprocessing contexts. Also include improvement on\n    memory management of long running worker processes and fixed issues\n    when using the loky backend under PyPy.\n\n\nMaxime Weyl\n\n    Raises a more explicit exception when a corrupted MemorizedResult is loaded.\n\nMaxime Weyl\n\n    Loading a corrupted cached file with mmap mode enabled would\n    recompute the results and return them without memory mapping.\n\n\nRelease 0.12.3\n--------------\n\nThomas Moreau\n\n    Fix joblib import setting the global start_method for multiprocessing.\n\nAlexandre Abadie\n\n    Fix MemorizedResult not picklable (#747).\n\nLoïc Estève\n\n    Fix Memory, MemorizedFunc and MemorizedResult round-trip pickling +\n    unpickling (#746).\n\nJames Collins\n\n    Fixed a regression in Memory when positional arguments are called as\n    kwargs several times with different values (#751).\n\nThomas Moreau and Olivier Grisel\n\n    Integration of loky 2.2.2 that fixes issues with the selection of the\n    default start method and improve the reporting when calling functions\n    with arguments that raise an exception when unpickling.\n\n\nMaxime Weyl\n\n    Prevent MemorizedFunc.call_and_shelve from loading cached results to\n    RAM when not necessary. Results in big performance improvements\n\n\nRelease 0.12.2\n--------------\n\nOlivier Grisel\n\n   Integrate loky 2.2.0 to fix regression with unpicklable arguments and\n   functions reported by users (#723, #643).\n\n   Loky 2.2.0 also provides a protection against memory leaks long running\n   applications when psutil is installed (reported as #721).\n\n   Joblib now includes the code for the dask backend which has been updated\n   to properly handle nested parallelism and data scattering at the same\n   time (#722).\n\nAlexandre Abadie and Olivier Grisel\n\n   Restored some private API attribute and arguments\n   (`MemorizedResult.argument_hash` and `BatchedCalls.__init__`'s\n   `pickle_cache`) for backward compat. (#716, #732).\n\n\nJoris Van den Bossche\n\n   Fix a deprecation warning message (for `Memory`'s `cachedir`) (#720).\n\n\nRelease 0.12.1\n--------------\n\nThomas Moreau\n\n    Make sure that any exception triggered when serializing jobs in the queue\n    will be wrapped as a PicklingError as in past versions of joblib.\n\nNoam Hershtig\n\n    Fix kwonlydefaults key error in filter_args (#715)\n\n\nRelease 0.12\n------------\n\nThomas Moreau\n\n    Implement the ``'loky'`` backend with @ogrisel. This backend relies on\n    a robust implementation of ``concurrent.futures.ProcessPoolExecutor``\n    with spawned processes that can be reused across the ``Parallel``\n    calls. This fixes the bad integration with third paty libraries relying on\n    thread pools, described in https://pythonhosted.org/joblib/parallel.html#bad-interaction-of-multiprocessing-and-third-party-libraries\n\n    Limit the number of threads used in worker processes by C-libraries that\n    relies on threadpools. This functionality works for MKL, OpenBLAS, OpenMP\n    and Accelerated.\n\nElizabeth Sander\n\n    Prevent numpy arrays with the same shape and data from hashing to\n    the same memmap, to prevent jobs with preallocated arrays from\n    writing over each other.\n\nOlivier Grisel\n\n    Reduce overhead of automatic memmap by removing the need to hash the\n    array.\n\n    Make ``Memory.cache`` robust to ``PermissionError (errno 13)`` under\n    Windows when run in combination with ``Parallel``.\n\n    The automatic array memory mapping feature of ``Parallel`` does no longer\n    use ``/dev/shm`` if it is too small (less than 2 GB). In particular in\n    docker containers ``/dev/shm`` is only 64 MB by default which would cause\n    frequent failures when running joblib in Docker containers.\n\n    Make it possible to hint for thread-based parallelism with\n    ``prefer='threads'`` or enforce shared-memory semantics with\n    ``require='sharedmem'``.\n\n    Rely on the built-in exception nesting system of Python 3 to preserve\n    traceback information when an exception is raised on a remote worker\n    process. This avoid verbose and redundant exception reports under\n    Python 3.\n\n    Preserve exception type information when doing nested Parallel calls\n    instead of mapping the exception to the generic ``JoblibException`` type.\n\n\nAlexandre Abadie\n\n    Introduce the concept of 'store' and refactor the ``Memory`` internal\n    storage implementation to make it accept extra store backends for caching\n    results. ``backend`` and ``backend_options`` are the new options added to\n    ``Memory`` to specify and configure a store backend.\n\n    Add the ``register_store_backend`` function to extend the store backend\n    used by default with Memory. This default store backend is named 'local'\n    and corresponds to the local filesystem.\n\n    The store backend API is experimental and thus is subject to change in the\n    future without deprecation.\n\n    The ``cachedir`` parameter of ``Memory`` is now marked as deprecated, use\n    ``location`` instead.\n\n    Add support for LZ4 compression if ``lz4`` package is installed.\n\n    Add ``register_compressor`` function for extending available compressors.\n\n    Allow passing a string to ``compress`` parameter in ``dump`` function. This\n    string should correspond to the compressor used (e.g. zlib, gzip, lz4,\n    etc). The default compression level is used in this case.\n\nMatthew Rocklin\n\n    Allow ``parallel_backend`` to be used globally instead of only as a context\n    manager.\n    Support lazy registration of external parallel backends\n\nRelease 0.11\n------------\n\nAlexandre Abadie\n\n    Remove support for python 2.6\n\nAlexandre Abadie\n\n    Remove deprecated `format_signature`, `format_call` and `load_output`\n    functions from Memory API.\n\nLoïc Estève\n\n    Add initial implementation of LRU cache cleaning. You can specify\n    the size limit of a ``Memory`` object via the ``bytes_limit``\n    parameter and then need to clean explicitly the cache via the\n    ``Memory.reduce_size`` method.\n\nOlivier Grisel\n\n    Make the multiprocessing backend work even when the name of the main\n    thread is not the Python default. Thanks to Roman Yurchak for the\n    suggestion.\n\nKaran Desai\n\n    pytest is used to run the tests instead of nosetests.\n    ``python setup.py test`` or ``python setup.py nosetests`` do not work\n    anymore, run ``pytest joblib`` instead.\n\nLoïc Estève\n\n    An instance of ``joblib.ParallelBackendBase`` can be passed into\n    the ``parallel`` argument in ``joblib.Parallel``.\n\n\nLoïc Estève\n\n    Fix handling of memmap objects with offsets greater than\n    mmap.ALLOCATIONGRANULARITY in ``joblib.Parallel``. See\n    https://github.com/joblib/joblib/issues/451 for more details.\n\nLoïc Estève\n\n    Fix performance regression in ``joblib.Parallel`` with\n    n_jobs=1. See https://github.com/joblib/joblib/issues/483 for more\n    details.\n\nLoïc Estève\n\n    Fix race condition when a function cached with\n    ``joblib.Memory.cache`` was used inside a ``joblib.Parallel``. See\n    https://github.com/joblib/joblib/issues/490 for more details.\n\nRelease 0.10.3\n--------------\n\nLoïc Estève\n\n    Fix tests when multiprocessing is disabled via the\n    JOBLIB_MULTIPROCESSING environment variable.\n\nharishmk\n\n    Remove warnings in nested Parallel objects when the inner Parallel\n    has n_jobs=1. See https://github.com/joblib/joblib/pull/406 for\n    more details.\n\nRelease 0.10.2\n--------------\n\nLoïc Estève\n\n    FIX a bug in stack formatting when the error happens in a compiled\n    extension. See https://github.com/joblib/joblib/pull/382 for more\n    details.\n\nVincent Latrouite\n\n    FIX a bug in the constructor of BinaryZlibFile that would throw an\n    exception when passing unicode filename (Python 2 only).\n    See https://github.com/joblib/joblib/pull/384 for more details.\n\nOlivier Grisel\n\n    Expose :class:`joblib.parallel.ParallelBackendBase` and\n    :class:`joblib.parallel.AutoBatchingMixin` in the public API to\n    make them officially re-usable by backend implementers.\n\n\nRelease 0.10.0\n--------------\n\nAlexandre Abadie\n\n    ENH: joblib.dump/load now accept file-like objects besides filenames.\n    https://github.com/joblib/joblib/pull/351 for more details.\n\nNiels Zeilemaker and Olivier Grisel\n\n    Refactored joblib.Parallel to enable the registration of custom\n    computational backends.\n    https://github.com/joblib/joblib/pull/306\n    Note the API to register custom backends is considered experimental\n    and subject to change without deprecation.\n\nAlexandre Abadie\n\n    Joblib pickle format change: joblib.dump always create a single pickle file\n    and joblib.dump/joblib.save never do any memory copy when writing/reading\n    pickle files. Reading pickle files generated with joblib versions prior\n    to 0.10 will be supported for a limited amount of time, we advise to\n    regenerate them from scratch when convenient.\n    joblib.dump and joblib.load also support pickle files compressed using\n    various strategies: zlib, gzip, bz2, lzma and xz. Note that lzma and xz are\n    only available with python >= 3.3.\n    https://github.com/joblib/joblib/pull/260 for more details.\n\nAntony Lee\n\n    ENH: joblib.dump/load now accept pathlib.Path objects as filenames.\n    https://github.com/joblib/joblib/pull/316 for more details.\n\nOlivier Grisel\n\n    Workaround for \"WindowsError: [Error 5] Access is denied\" when trying to\n    terminate a multiprocessing pool under Windows:\n    https://github.com/joblib/joblib/issues/354\n\n\nRelease 0.9.4\n-------------\n\nOlivier Grisel\n\n    FIX a race condition that could cause a joblib.Parallel to hang\n    when collecting the result of a job that triggers an exception.\n    https://github.com/joblib/joblib/pull/296\n\nOlivier Grisel\n\n    FIX a bug that caused joblib.Parallel to wrongly reuse previously\n    memmapped arrays instead of creating new temporary files.\n    https://github.com/joblib/joblib/pull/294 for more details.\n\nLoïc Estève\n\n    FIX for raising non inheritable exceptions in a Parallel call. See\n    https://github.com/joblib/joblib/issues/269 for more details.\n\nAlexandre Abadie\n\n    FIX joblib.hash error with mixed types sets and dicts containing mixed\n    types keys when using Python 3.\n    see https://github.com/joblib/joblib/issues/254\n\nLoïc Estève\n\n    FIX joblib.dump/load for big numpy arrays with dtype=object. See\n    https://github.com/joblib/joblib/issues/220 for more details.\n\nLoïc Estève\n\n    FIX joblib.Parallel hanging when used with an exhausted\n    iterator. See https://github.com/joblib/joblib/issues/292 for more\n    details.\n\nRelease 0.9.3\n-------------\n\nOlivier Grisel\n\n    Revert back to the ``fork`` start method (instead of\n    ``forkserver``) as the latter was found to cause crashes in\n    interactive Python sessions.\n\nRelease 0.9.2\n-------------\n\nLoïc Estève\n\n    Joblib hashing now uses the default pickle protocol (2 for Python\n    2 and 3 for Python 3). This makes it very unlikely to get the same\n    hash for a given object under Python 2 and Python 3.\n\n    In particular, for Python 3 users, this means that the output of\n    joblib.hash changes when switching from joblib 0.8.4 to 0.9.2 . We\n    strive to ensure that the output of joblib.hash does not change\n    needlessly in future versions of joblib but this is not officially\n    guaranteed.\n\nLoïc Estève\n\n    Joblib pickles generated with Python 2 can not be loaded with\n    Python 3 and the same applies for joblib pickles generated with\n    Python 3 and loaded with Python 2.\n\n    During the beta period 0.9.0b2 to 0.9.0b4, we experimented with\n    a joblib serialization that aimed to make pickles serialized with\n    Python 3 loadable under Python 2. Unfortunately this serialization\n    strategy proved to be too fragile as far as the long-term\n    maintenance was concerned (For example see\n    https://github.com/joblib/joblib/pull/243). That means that joblib\n    pickles generated with joblib 0.9.0bN can not be loaded under\n    joblib 0.9.2. Joblib beta testers, who are the only ones likely to\n    be affected by this, are advised to delete their joblib cache when\n    they upgrade from 0.9.0bN to 0.9.2.\n\nArthur Mensch\n\n    Fixed a bug with ``joblib.hash`` that used to return unstable values for\n    strings and numpy.dtype instances depending on interning states.\n\nOlivier Grisel\n\n    Make joblib use the 'forkserver' start method by default under Python 3.4+\n    to avoid causing crash with 3rd party libraries (such as Apple vecLib /\n    Accelerate or the GCC OpenMP runtime) that use an internal thread pool that\n    is not reinitialized when a ``fork`` system call happens.\n\nOlivier Grisel\n\n    New context manager based API (``with`` block) to re-use\n    the same pool of workers across consecutive parallel calls.\n\nVlad Niculae and Olivier Grisel\n\n    Automated batching of fast tasks into longer running jobs to\n    hide multiprocessing dispatching overhead when possible.\n\nOlivier Grisel\n\n    FIX make it possible to call ``joblib.load(filename, mmap_mode='r')``\n    on pickled objects that include a mix of arrays of both\n    memory memmapable dtypes and object dtype.\n\n\nRelease 0.8.4\n-------------\n\n2014-11-20\nOlivier Grisel\n\n    OPTIM use the C-optimized pickler under Python 3\n\n    This makes it possible to efficiently process parallel jobs that deal with\n    numerous Python objects such as large dictionaries.\n\n\nRelease 0.8.3\n-------------\n\n2014-08-19\nOlivier Grisel\n\n    FIX disable memmapping for object arrays\n\n2014-08-07\nLars Buitinck\n\n    MAINT NumPy 1.10-safe version comparisons\n\n\n2014-07-11\nOlivier Grisel\n\n    FIX #146: Heisen test failure caused by thread-unsafe Python lists\n\n    This fix uses a queue.Queue datastructure in the failing test. This\n    datastructure is thread-safe thanks to an internal Lock. This Lock instance\n    not picklable hence cause the picklability check of delayed to check fail.\n\n    When using the threading backend, picklability is no longer required, hence\n    this PRs give the user the ability to disable it on a case by case basis.\n\n\nRelease 0.8.2\n-------------\n\n2014-06-30\nOlivier Grisel\n\n    BUG: use mmap_mode='r' by default in Parallel and MemmappingPool\n\n    The former default of mmap_mode='c' (copy-on-write) caused\n    problematic use of the paging file under Windows.\n\n2014-06-27\nOlivier Grisel\n\n    BUG: fix usage of the /dev/shm folder under Linux\n\n\nRelease 0.8.1\n-------------\n\n2014-05-29\nGael Varoquaux\n\n    BUG: fix crash with high verbosity\n\n\nRelease 0.8.0\n-------------\n\n2014-05-14\nOlivier Grisel\n\n   Fix a bug in exception reporting under Python 3\n\n2014-05-10\nOlivier Grisel\n\n   Fixed a potential segfault when passing non-contiguous memmap\n   instances.\n\n2014-04-22\nGael Varoquaux\n\n    ENH: Make memory robust to modification of source files while the\n    interpreter is running. Should lead to less spurious cache flushes\n    and recomputations.\n\n\n2014-02-24\nPhilippe Gervais\n\n   New ``Memory.call_and_shelve`` API to handle memoized results by\n   reference instead of by value.\n\n\nRelease 0.8.0a3\n---------------\n\n2014-01-10\nOlivier Grisel & Gael Varoquaux\n\n   FIX #105: Race condition in task iterable consumption when\n   pre_dispatch != 'all' that could cause crash with error messages \"Pools\n   seems closed\" and \"ValueError: generator already executing\".\n\n2014-01-12\nOlivier Grisel\n\n   FIX #72: joblib cannot persist \"output_dir\" keyword argument.\n\n\nRelease 0.8.0a2\n---------------\n\n2013-12-23\nOlivier Grisel\n\n    ENH: set default value of Parallel's max_nbytes to 100MB\n\n    Motivation: avoid introducing disk latency on medium sized\n    parallel workload where memory usage is not an issue.\n\n    FIX: properly handle the JOBLIB_MULTIPROCESSING env variable\n\n    FIX: timeout test failures under windows\n\n\nRelease 0.8.0a\n--------------\n\n2013-12-19\nOlivier Grisel\n\n    FIX: support the new Python 3.4 multiprocessing API\n\n\n2013-12-05\nOlivier Grisel\n\n    ENH: make Memory respect mmap_mode at first call too\n\n    ENH: add a threading based backend to Parallel\n\n    This is low overhead alternative backend to the default multiprocessing\n    backend that is suitable when calling compiled extensions that release\n    the GIL.\n\n\nAuthor: Dan Stahlke <dan@stahlke.org>\nDate:   2013-11-08\n\n    FIX: use safe_repr to print arg vals in trace\n\n    This fixes a problem in which extremely long (and slow) stack traces would\n    be produced when function parameters are large numpy arrays.\n\n\n2013-09-10\nOlivier Grisel\n\n    ENH: limit memory copy with Parallel by leveraging numpy.memmap when\n    possible\n\n\nRelease 0.7.1\n---------------\n\n2013-07-25\nGael Varoquaux\n\n    MISC: capture meaningless argument (n_jobs=0) in Parallel\n\n2013-07-09\nLars Buitinck\n\n    ENH Handles tuples, sets and Python 3's dict_keys type the same as\n    lists. in pre_dispatch\n\n2013-05-23\nMartin Luessi\n\n    ENH: fix function caching for IPython\n\nRelease 0.7.0\n---------------\n\n**This release drops support for Python 2.5 in favor of support for\nPython 3.0**\n\n2013-02-13\nGael Varoquaux\n\n    BUG: fix nasty hash collisions\n\n2012-11-19\nGael Varoquaux\n\n    ENH: Parallel: Turn of pre-dispatch for already expanded lists\n\n\nGael Varoquaux\n2012-11-19\n\n    ENH: detect recursive sub-process spawning, as when people do not\n    protect the __main__ in scripts under Windows, and raise a useful\n    error.\n\n\nGael Varoquaux\n2012-11-16\n\n    ENH: Full python 3 support\n\nRelease 0.6.5\n---------------\n\n2012-09-15\nYannick Schwartz\n\n    BUG: make sure that sets and dictionaries give reproducible hashes\n\n\n2012-07-18\nMarek Rudnicki\n\n    BUG: make sure that object-dtype numpy array hash correctly\n\n2012-07-12\nGaelVaroquaux\n\n    BUG: Bad default n_jobs for Parallel\n\nRelease 0.6.4\n---------------\n\n2012-05-07\nVlad Niculae\n\n    ENH: controlled randomness in tests and doctest fix\n\n2012-02-21\nGaelVaroquaux\n\n    ENH: add verbosity in memory\n\n2012-02-21\nGaelVaroquaux\n\n    BUG: non-reproducible hashing: order of kwargs\n\n    The ordering of a dictionary is random. As a result the function hashing\n    was not reproducible. Pretty hard to test\n\nRelease 0.6.3\n---------------\n\n2012-02-14\nGaelVaroquaux\n\n    BUG: fix joblib Memory pickling\n\n2012-02-11\nGaelVaroquaux\n\n    BUG: fix hasher with Python 3\n\n2012-02-09\nGaelVaroquaux\n\n    API: filter_args:  `*args, **kwargs -> args, kwargs`\n\nRelease 0.6.2\n---------------\n\n2012-02-06\nGael Varoquaux\n\n    BUG: make sure Memory pickles even if cachedir=None\n\nRelease 0.6.1\n---------------\n\nBugfix release because of a merge error in release 0.6.0\n\nRelease 0.6.0\n---------------\n\n**Beta 3**\n\n2012-01-11\nGael Varoquaux\n\n    BUG: ensure compatibility with old numpy\n\n    DOC: update installation instructions\n\n    BUG: file semantic to work under Windows\n\n2012-01-10\nYaroslav Halchenko\n\n    BUG: a fix toward 2.5 compatibility\n\n**Beta 2**\n\n2012-01-07\nGael Varoquaux\n\n    ENH: hash: bugware to be able to hash objects defined interactively\n    in IPython\n\n2012-01-07\nGael Varoquaux\n\n    ENH: Parallel: warn and not fail for nested loops\n\n    ENH: Parallel: n_jobs=-2 now uses all CPUs but one\n\n2012-01-01\nJuan Manuel Caicedo Carvajal and Gael Varoquaux\n\n    ENH: add verbosity levels in Parallel\n\nRelease 0.5.7\n---------------\n\n2011-12-28\nGael varoquaux\n\n    API: zipped -> compress\n\n2011-12-26\nGael varoquaux\n\n    ENH: Add a zipped option to Memory\n\n    API: Memory no longer accepts save_npy\n\n2011-12-22\nKenneth C. Arnold and Gael varoquaux\n\n    BUG: fix numpy_pickle for array subclasses\n\n2011-12-21\nGael varoquaux\n\n    ENH: add zip-based pickling\n\n2011-12-19\nFabian Pedregosa\n\n    Py3k: compatibility fixes.\n    This makes run fine the tests test_disk and test_parallel\n\nRelease 0.5.6\n---------------\n\n2011-12-11\nLars Buitinck\n\n    ENH: Replace os.path.exists before makedirs with exception check\n    New disk.mkdirp will fail with other errnos than EEXIST.\n\n2011-12-10\nBala Subrahmanyam Varanasi\n\n    MISC: pep8 compliant\n\n\nRelease 0.5.5\n---------------\n\n2011-19-10\nFabian Pedregosa\n\n    ENH: Make joblib installable under Python 3.X\n\nRelease 0.5.4\n---------------\n\n2011-09-29\nJon Olav Vik\n\n    BUG: Make mangling path to filename work on Windows\n\n2011-09-25\nOlivier Grisel\n\n    FIX: doctest heisenfailure on execution time\n\n2011-08-24\nRalf Gommers\n\n    STY: PEP8 cleanup.\n\n\nRelease 0.5.3\n---------------\n\n2011-06-25\nGael varoquaux\n\n   API: All the useful symbols in the __init__\n\n\nRelease 0.5.2\n---------------\n\n2011-06-25\nGael varoquaux\n\n    ENH: Add cpu_count\n\n2011-06-06\nGael varoquaux\n\n    ENH: Make sure memory hash in a reproducible way\n\n\nRelease 0.5.1\n---------------\n\n2011-04-12\nGael varoquaux\n\n    TEST: Better testing of parallel and pre_dispatch\n\nYaroslav Halchenko\n2011-04-12\n\n    DOC: quick pass over docs -- trailing spaces/spelling\n\nYaroslav Halchenko\n2011-04-11\n\n    ENH: JOBLIB_MULTIPROCESSING env var to disable multiprocessing from the\n    environment\n\nAlexandre Gramfort\n2011-04-08\n\n    ENH : adding log message to know how long it takes to load from disk the\n    cache\n\n\nRelease 0.5.0\n---------------\n\n2011-04-01\nGael varoquaux\n\n    BUG: pickling MemoizeFunc does not store timestamp\n\n2011-03-31\nNicolas Pinto\n\n    TEST: expose hashing bug with cached method\n\n2011-03-26...2011-03-27\nPietro Berkes\n\n    BUG: fix error management in rm_subdirs\n    BUG: fix for race condition during tests in mem.clear()\n\nGael varoquaux\n2011-03-22...2011-03-26\n\n    TEST: Improve test coverage and robustness\n\nGael varoquaux\n2011-03-19\n\n    BUG: hashing functions with only \\*var \\**kwargs\n\nGael varoquaux\n2011-02-01... 2011-03-22\n\n    BUG: Many fixes to capture interprocess race condition when mem.cache\n    is used by several processes on the same cache.\n\nFabian Pedregosa\n2011-02-28\n\n    First work on Py3K compatibility\n\nGael varoquaux\n2011-02-27\n\n    ENH: pre_dispatch in parallel: lazy generation of jobs in parallel\n    for to avoid drowning memory.\n\nGaelVaroquaux\n2011-02-24\n\n    ENH: Add the option of overloading the arguments of the mother\n    'Memory' object in the cache method that is doing the decoration.\n\nGael varoquaux\n2010-11-21\n\n    ENH: Add a verbosity level for more verbosity\n\nRelease 0.4.6\n----------------\n\nGael varoquaux\n2010-11-15\n\n    ENH: Deal with interruption in parallel\n\nGael varoquaux\n2010-11-13\n\n    BUG: Exceptions raised by Parallel when n_job=1 are no longer captured.\n\nGael varoquaux\n2010-11-13\n\n    BUG: Capture wrong arguments properly (better error message)\n\n\nRelease 0.4.5\n----------------\n\nPietro Berkes\n2010-09-04\n\n    BUG: Fix Windows peculiarities with path separators and file names\n    BUG: Fix more windows locking bugs\n\nGael varoquaux\n2010-09-03\n\n    ENH: Make sure that exceptions raised in Parallel also inherit from\n    the original exception class\n    ENH: Add a shadow set of exceptions\n\nFabian Pedregosa\n2010-09-01\n\n    ENH: Clean up the code for parallel. Thanks to Fabian Pedregosa for\n    the patch.\n\n\nRelease 0.4.4\n----------------\n\nGael varoquaux\n2010-08-23\n\n    BUG: Fix Parallel on computers with only one CPU, for n_jobs=-1.\n\nGael varoquaux\n2010-08-02\n\n    BUG: Fix setup.py for extra setuptools args.\n\nGael varoquaux\n2010-07-29\n\n    MISC: Silence tests (and hopefully Yaroslav :P)\n\nRelease 0.4.3\n----------------\n\nGael Varoquaux\n2010-07-22\n\n    BUG: Fix hashing for function with a side effect modifying their input\n    argument. Thanks to Pietro Berkes for reporting the bug and proving the\n    patch.\n\nRelease 0.4.2\n----------------\n\nGael Varoquaux\n2010-07-16\n\n    BUG: Make sure that joblib still works with Python2.5. => release 0.4.2\n\nRelease 0.4.1\n----------------\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.4912109375,
          "content": "BSD 3-Clause License\n\nCopyright (c) 2008-2021, The joblib developers.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.125,
          "content": "include *.txt *.py\nrecursive-include joblib *.rst *.py\ngraft doc\ngraft doc/_static\ngraft doc/_templates\nglobal-exclude *~ *.swp\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.3544921875,
          "content": ".PHONY: all test test-no-multiprocessing test-doc doc doc-clean\n\nall: test\n\ntest:\n\tpytest joblib --timeout 30 -vl\n\ntest-no-multiprocessing:\n\texport JOBLIB_MULTIPROCESSING=0 && pytest joblib\n\ntest-doc:\n\tpytest $(shell find doc -name '*.rst' | sort)\n\n# generate html documentation using sphinx\ndoc:\n\tmake -C doc\n\n# clean documentation\ndoc-clean:\n\tmake -C doc clean\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 4.09375,
          "content": "|PyPi| |CIStatus| |ReadTheDocs| |Codecov| \n\n.. |PyPi| image:: https://badge.fury.io/py/joblib.svg\n   :target: https://badge.fury.io/py/joblib\n   :alt: Joblib version\n\n.. |CIStatus| image:: https://github.com/joblib/joblib/actions/workflows/test.yml/badge.svg\n   :target: https://github.com/joblib/joblib/actions/workflows/test.yml?query=branch%3Amain\n   :alt: CI status\n\n.. |ReadTheDocs| image:: https://readthedocs.org/projects/joblib/badge/?version=latest\n    :target: https://joblib.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n\n.. |Codecov| image:: https://codecov.io/gh/joblib/joblib/branch/main/graph/badge.svg\n   :target: https://codecov.io/gh/joblib/joblib\n   :alt: Codecov coverage\n\n\nThe homepage of joblib with user documentation is located on:\n\nhttps://joblib.readthedocs.io\n\nGetting the latest code\n=======================\n\nTo get the latest code using git, simply type::\n\n    git clone https://github.com/joblib/joblib.git\n\nIf you don't have git installed, you can download a zip\nof the latest code: https://github.com/joblib/joblib/archive/refs/heads/main.zip\n\nInstalling\n==========\n\nYou can use `pip` to install joblib::\n\n    pip install joblib\n\nfrom any directory or::\n\n    python setup.py install\n\nfrom the source directory.\n\nDependencies\n============\n\n- Joblib has no mandatory dependencies besides Python (supported versions are\n  3.8+).\n- Joblib has an optional dependency on Numpy (at least version 1.6.1) for array\n  manipulation.\n- Joblib includes its own vendored copy of\n  `loky <https://github.com/tomMoral/loky>`_ for process management.\n- Joblib can efficiently dump and load numpy arrays but does not require numpy\n  to be installed.\n- Joblib has an optional dependency on\n  `python-lz4 <https://pypi.python.org/pypi/lz4>`_ as a faster alternative to\n  zlib and gzip for compressed serialization.\n- Joblib has an optional dependency on psutil to mitigate memory leaks in\n  parallel worker processes.\n- Some examples require external dependencies such as pandas. See the\n  instructions in the `Building the docs`_ section for details.\n\nWorkflow to contribute\n======================\n\nTo contribute to joblib, first create an account on `github\n<https://github.com/>`_. Once this is done, fork the `joblib repository\n<https://github.com/joblib/joblib>`_ to have your own repository,\nclone it using 'git clone' on the computers where you want to work. Make\nyour changes in your clone, push them to your github account, test them\non several computers, and when you are happy with them, send a pull\nrequest to the main repository.\n\nRunning the test suite\n======================\n\nTo run the test suite, you need the pytest (version >= 3) and coverage modules.\nRun the test suite using::\n\n    pytest joblib\n\nfrom the root of the project.\n\nBuilding the docs\n=================\n\nTo build the docs you need to have sphinx (>=1.4) and some dependencies\ninstalled::\n\n    pip install -U -r .readthedocs-requirements.txt\n\nThe docs can then be built with the following command::\n\n    make doc\n\nThe html docs are located in the ``doc/_build/html`` directory.\n\n\nMaking a source tarball\n=======================\n\nTo create a source tarball, eg for packaging or distributing, run the\nfollowing command::\n\n    python setup.py sdist\n\nThe tarball will be created in the `dist` directory. This command will\ncompile the docs, and the resulting tarball can be installed with\nno extra dependencies than the Python standard library. You will need\nsetuptool and sphinx.\n\nMaking a release and uploading it to PyPI\n=========================================\n\nThis command is only run by project manager, to make a release, and\nupload in to PyPI::\n\n    python setup.py sdist bdist_wheel\n    twine upload dist/*\n\n\nNote that the documentation should automatically get updated at each git\npush. If that is not the case, try building th doc locally and resolve\nany doc build error (in particular when running the examples).\n\nUpdating the changelog\n======================\n\nChanges are listed in the CHANGES.rst file. They must be manually updated\nbut, the following git command may be used to generate the lines::\n\n    git log --abbrev-commit --date=short --no-merges --sparse\n\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.5595703125,
          "content": "# Security Policy\n\n## Supported Versions\n\n| Version       | Supported          |\n| ------------- | ------------------ |\n| 1.4.2         | :white_check_mark: |\n| < 1.4.2       | :x:                |\n\n## Reporting a Vulnerability\n\nPlease report security vulnerabilities by email to `joblib-security@scikit-learn.org`.\nThis email is an alias to a subset of the joblib maintainers' team.\n\nIf the security vulnerability is accepted, a patch will be crafted privately\nin order to prepare a dedicated bugfix release as timely as possible (depending\non the complexity of the fix).\n"
        },
        {
          "name": "TODO.rst",
          "type": "blob",
          "size": 1.6708984375,
          "content": "Tasks at hand on joblib, in increasing order of difficulty.\n\n* Add a changelog!\n\n* In parallel: need to deal with return arguments that don't pickle.\n\n* Improve test coverage and documentation \n\n* Store a repr of the arguments for each call in the corresponding\n  cachedir\n\n* Try to use Mike McKerns's Dill pickling module in Parallel:\n  Implementation idea: \n    * Create a new function that is wrapped and takes Dillo pickles as \n      inputs as output, feed this one to multiprocessing\n    * pickle everything using Dill in the Parallel object.\n      http://dev.danse.us/trac/pathos/browser/dill\n\n* Make a sensible error message when wrong keyword arguments are given,\n  currently we have::\n\n    from joblib import Memory\n    mem = Memory(cachedir='cache')\n\n    def f(a=0, b=2):\n\treturn a, b\n\n    g = mem.cache(f)\n    g(c=2) \n\n    /home/varoquau/dev/joblib/joblib/func_inspect.pyc in filter_args(func,\n\t\tignore_lst, *args, **kwargs), line 168\n\n\t    TypeError: Ignore list for diffusion_reorder() contains and\n\t\t\tunexpected keyword argument 'cachedir'\n\n* add a 'depends' keyword argument to memory.cache, to be able to\n  specify that a function depends on other functions, and thus that the\n  cache should be cleared.\n\n* add a 'argument_hash' keyword argument to Memory.cache, to be able to\n  replace the hashing logic of memory for the input arguments. It should\n  accept as an input the dictionary of arguments, as returned in\n  func_inspect, and return a string.\n\n* add a sqlite db for provenance tracking. Store computation time and usage \n  timestamps, to be able to do 'garbage-collection-like' cleaning of\n  unused results, based on a cost function balancing computation cost and\n  frequency of use.\n\n\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 3.4375,
          "content": "import os\nimport sys\nimport logging\nimport faulthandler\n\nimport pytest\nfrom _pytest.doctest import DoctestItem\n\nfrom joblib.parallel import mp, ParallelBackendBase\nfrom joblib.backports import LooseVersion\nfrom joblib import Memory\ntry:\n    import lz4\nexcept ImportError:\n    lz4 = None\ntry:\n    from distributed.utils_test import loop, loop_in_thread\nexcept ImportError:\n    loop = None\n    loop_in_thread = None\n\n\ndef pytest_collection_modifyitems(config, items):\n    skip_doctests = True\n\n    # We do not want to run the doctests if multiprocessing is disabled\n    # e.g. via the JOBLIB_MULTIPROCESSING env variable\n    if mp is not None:\n        try:\n            # Only run doctests for numpy >= 2 and Python >= 3.10 to avoid\n            # formatting changes\n            import numpy as np\n            if (LooseVersion(np.__version__) >= LooseVersion('2')\n                    and sys.version_info[:2] >= (3, 10)):\n                skip_doctests = False\n        except ImportError:\n            pass\n\n    if skip_doctests:\n        reason = (\n            'doctests are only run in some conditions, '\n            'see conftest.py for more details'\n        )\n        skip_marker = pytest.mark.skip(reason=reason)\n\n        for item in items:\n            if isinstance(item, DoctestItem):\n                item.add_marker(skip_marker)\n\n    if lz4 is None:\n        for item in items:\n            if item.name == 'persistence.rst':\n                item.add_marker(pytest.mark.skip(reason='lz4 is missing'))\n\n\ndef pytest_configure(config):\n    \"\"\"Setup multiprocessing logging for the tests\"\"\"\n    if mp is not None:\n        log = mp.util.log_to_stderr(logging.DEBUG)\n        log.handlers[0].setFormatter(logging.Formatter(\n            '[%(levelname)s:%(processName)s:%(threadName)s] %(message)s'))\n\n    # Some CI runs failed with hanging processes that were not terminated\n    # with the timeout. To make sure we always get a proper trace, set a large\n    # enough dump_traceback_later to kill the process with a report.\n    faulthandler.dump_traceback_later(30 * 60, exit=True)\n\n    DEFAULT_BACKEND = os.environ.get(\n        \"JOBLIB_TESTS_DEFAULT_PARALLEL_BACKEND\", None\n    )\n    if DEFAULT_BACKEND is not None:\n        print(\n            f\"Setting joblib parallel default backend to {DEFAULT_BACKEND} \"\n            \"from JOBLIB_TESTS_DEFAULT_PARALLEL_BACKEND environment variable\"\n        )\n        from joblib import parallel\n        parallel.DEFAULT_BACKEND = DEFAULT_BACKEND\n\n\ndef pytest_unconfigure(config):\n\n    # Setup a global traceback printer callback to debug deadlocks that\n    # would happen once pytest has completed: for instance in atexit\n    # finalizers. At this point the stdout/stderr capture of pytest\n    # should be disabled. Note that we cancel the global dump_traceback_later\n    # to waiting for too long.\n    faulthandler.cancel_dump_traceback_later()\n\n    # Note that we also use a shorter timeout for the per-test callback\n    # configured via the pytest-timeout extension.\n    faulthandler.dump_traceback_later(60, exit=True)\n\n\n@pytest.fixture(scope='function')\ndef memory(tmp_path):\n    \"Fixture to get an independent and self-cleaning Memory\"\n    mem = Memory(location=tmp_path, verbose=0)\n    yield mem\n    mem.clear()\n\n\n@pytest.fixture(scope='function', autouse=True)\ndef avoid_env_var_leakage():\n    \"Fixture to avoid MAX_NUM_THREADS env vars leakage between tests\"\n    yield\n    assert all(\n        os.environ.get(k) is None\n        for k in ParallelBackendBase.MAX_NUM_THREADS_VARS\n    )\n"
        },
        {
          "name": "continuous_integration",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "joblib",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.333984375,
          "content": "[build-system]\nrequires = [\"setuptools>=61.2\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"joblib\"\nauthors = [{name = \"Gael Varoquaux\", email = \"gael.varoquaux@normalesup.org\"}]\nlicense = {text = \"BSD 3-Clause\"}\ndescription = \"Lightweight pipelining with Python functions\"\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Environment :: Console\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Intended Audience :: Education\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: Utilities\",\n    \"Topic :: Software Development :: Libraries\",\n]\nrequires-python = \">=3.8\"\ndynamic = [\"version\"]\n\n[project.readme]\nfile = \"README.rst\"\ncontent-type = \"text/x-rst\"\n\n[project.urls]\nHomepage = \"https://joblib.readthedocs.io\"\nSource = \"https://github.com/joblib/joblib\"\n\n[tool.setuptools]\npackages = [\n    \"joblib\",\n    \"joblib.test\",\n    \"joblib.test.data\",\n    \"joblib.externals\",\n    \"joblib.externals.cloudpickle\",\n    \"joblib.externals.loky\",\n    \"joblib.externals.loky.backend\",\n]\nplatforms = [\"any\"]\ninclude-package-data = false\n\n[tool.setuptools.package-data]\n\"joblib.test\" = [\n    \"data/*.gz\",\n    \"data/*.gzip\",\n    \"data/*.bz2\",\n    \"data/*.xz\",\n    \"data/*.lzma\",\n    \"data/*.pkl\",\n    \"data/*.npy\",\n    \"data/*.npy.z\",\n]\n\n[tool.setuptools.dynamic]\nversion = {attr = \"joblib.__version__\"}\n\n\n[aliases]\nrelease = \"egg_info -RDb ''\"\n# Make sure the docs are uploaded when we do an upload\nupload = \"upload upload_docs --upload-dir doc/_build/html\"\n\n[bdist_rpm]\ndoc_files = \"doc\"\n\n\n[tool.pytest.ini_options]\ndoctest_optionflags = [\n    \"NORMALIZE_WHITESPACE\",\n    \"ELLIPSIS\"\n]\naddopts = [\n    \"--doctest-glob='doc/*.rst'\",\n    \"--doctest-modules\",\n    \"--color=yes\",\n]\ntestpaths = \"joblib\"\nnorecursedirs = \"joblib/externals/*\"\n\n[tool.coverage.run]\nsource = [\n    \"joblib\"\n]\nomit = [\n    \"joblib/test/data/*\",\n    \"joblib/test/_openmp_test_helper/setup.py\",\n    \"*/joblib/externals/*\",\n]\nrelative_files = true\n\n[tool.coverage.report]\nshow_missing = true\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.287109375,
          "content": "# The prefered config file is pyproject.toml. The use of setup.cfg is\n# mostly for compatibility with flake8 so it should not be used if possible.\n\n[flake8]\nexclude=\n    joblib/externals/*,\n    doc/auto_examples,\n    doc/_build,\n\nper-file-ignores =\n    examples/*: E402,\n    doc/conf.py: E402,\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.08984375,
          "content": "#!/usr/bin/env python\n\nfrom setuptools import setup\n\nif __name__ == \"__main__\":\n    setup()\n"
        }
      ]
    }
  ]
}