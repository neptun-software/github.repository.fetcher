{
  "metadata": {
    "timestamp": 1736560395495,
    "page": 950,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "bijection/sistine",
      "stars": 3251,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0263671875,
          "content": "*.jpg\n*.pyc\n*.pickle\n*.mov\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.134765625,
          "content": "The MIT License (MIT)\n=====================\n\n**Copyright (c) 2016-2018 Anish Athalye, Kevin Kwok, Guillermo Webster, and Logan Engstrom**\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.3623046875,
          "content": "# Project Sistine\n\n![Sistine * 3/2](splash.png)\n\nWe turned a MacBook into a touchscreen using only $1 of hardware and a little bit of computer vision. The proof-of-concept, dubbed “Project Sistine” after our [recreation](https://www.anishathalye.com/media/2018/04/03/thumbnail.jpg) of the famous [painting](https://en.wikipedia.org/wiki/The_Creation_of_Adam) in the Sistine Chapel, was prototyped by [Anish Athalye](https://www.anishathalye.com/), [Kevin Kwok](https://twitter.com/antimatter15), [Guillermo Webster](https://twitter.com/biject), and [Logan Engstrom](https://github.com/lengstrom) in about 16 hours.\n\n## Basic Principle\n\nThe basic principle behind Sistine is simple. Surfaces viewed from an angle tend to look shiny, and you can tell if a finger is touching the surface by checking if it’s touching its own reflection.\n\n![Hover versus touch](https://www.anishathalye.com/media/2018/04/03/explanation.png)\n\nKevin, back in middle school, noticed this phenomenon and built [ShinyTouch](https://antimatter15.com/project/shinytouch/), utilizing an external webcam to build a touch input system requiring virtually no setup. We wanted to see if we could miniaturize the idea and make it work without an external webcam. Our idea was to retrofit a small mirror in front of a MacBook’s built-in webcam, so that the webcam would be looking down at the computer screen at a sharp angle. The camera would be able to see fingers hovering over or touching the screen, and we’d be able to translate the video feed into touch events using computer vision.\n\n(Read the rest of our blog post, including a video demo and a high-level explanation of the algorithm, [here](https://www.anishathalye.com/2018/04/03/macbook-touchscreen/))\n\n## Installation (with Homebrew Python)\n\n* First, make sure you have [Mac Homebrew](https://brew.sh/) installed on your computer. If not, you can install it by running `/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"`\n\n* Install Python 2 via Homebrew with `brew install python2`\n\n* Install OpenCV 3 via Homebrew with `brew install opencv3`\n\n* Install PyObjC via Pip with `pip2 install pyobjc`\n\n## Running\n\nRun `python2 sistine.py`\n\n## License\n\nCopyright (c) 2016-2018 Anish Athalye, Kevin Kwok, Guillermo Webster, and Logan\nEngstrom. Released under the MIT License. See [LICENSE.md][license] for\ndetails.\n\n[license]: LICENSE.md\n"
        },
        {
          "name": "README_WINDOWS.MD",
          "type": "blob",
          "size": 2.3076171875,
          "content": "# Project Windows-Touch\n\n![Sistine * 3/2](splash.png)\n\nTurn a Windows-PC into a touchscreen using only $1.0 of hardware. The proof-of-concept, dubbed “Project Sistine” after our [recreation](https://www.anishathalye.com/media/2018/04/03/thumbnail.jpg) of the famous [painting](https://en.wikipedia.org/wiki/The_Creation_of_Adam) in the Sistine Chapel, was prototyped by [Anish Athalye](https://www.anishathalye.com/), [Kevin Kwok](https://twitter.com/antimatter15), [Guillermo Webster](https://twitter.com/biject), and [Logan Engstrom](https://github.com/lengstrom) in about 16 hours. It was thus mordified by [Ashish Gupta](https://github.com/ashishgupta1350) to work from mac/linux to windows.\n\n## Basic Principle\n\nThe basic principle behind Sistine is simple. Surfaces viewed from an angle tend to look shiny, and you can tell if a finger is touching the surface by checking if it’s touching its own reflection.\n\n![Hover versus touch](https://www.anishathalye.com/media/2018/04/03/explanation.png)\n\nKevin, back in middle school, noticed this phenomenon and built [ShinyTouch](https://antimatter15.com/project/shinytouch/), utilizing an external webcam to build a touch input system requiring virtually no setup. We wanted to see if we could miniaturize the idea and make it work without an external webcam. Our idea was to retrofit a small mirror in front of a MacBook’s built-in webcam, so that the webcam would be looking down at the computer screen at a sharp angle. The camera would be able to see fingers hovering over or touching the screen, and we’d be able to translate the video feed into touch events using computer vision.\n\n(Read the rest of our blog post, including a video demo and a high-level explanation of the algorithm, (https://www.anishathalye.com/2018/04/03/macbook-touchscreen/)\n\n## Installation (with pip Python)\n\nDownload Python 3 and then pip.\n\n* Install Python 3 via webisite https://www.python.org/download/releases/3.0/ \n\n* Download and Install pip. Follow this video : https://www.youtube.com/watch?v=AVCcFyYynQY\n\n* Install OpenCV 3 via pip with `pip install opencv-python`\n\n* Install pyautogui via pip with `pip install pyautogui`\n\n* Install Numpy via pip with `pip install numpy`\n\n\n## Running\n\nRun `python WindowsTouch.py` via cmd or compile using python3. Don't forget to add python 3 to path. \nPython 2 is supported.\n"
        },
        {
          "name": "actually_good_calib.pickle",
          "type": "blob",
          "size": 5.0546875,
          "content": "(dp0\nS'hom'\np1\ncnumpy.core.multiarray\n_reconstruct\np2\n(cnumpy\nndarray\np3\n(I0\ntp4\nS'b'\np5\ntp6\nRp7\n(I1\n(I3\nI3\ntp8\ncnumpy\ndtype\np9\n(S'f8'\np10\nI0\nI1\ntp11\nRp12\n(I3\nS'<'\np13\nNNNI-1\nI-1\nI0\ntp14\nbI00\nS'^Il\\x92\\xbb\\x82\\xb1?L+\\xe8R\\xf0\\x04\\xf3\\xbf\\x19#\\xaa\\x1a\\x93\\xcb\\x81@\\xed>}]\\xdf\\xe9T\\xbf\\xf0H\\xf0e\\xb8\\x8c\\xd1?\\xf8\\xdc\\x8d\\x92\\xe9OP\\xc0\\x08Q6=\\xeb\\x03\\xe3\\xbe<<\\xa3\\xe6\\xe3\\xc6_\\xbf\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?'\np15\ntp16\nbsS'realPts'\np17\n(lp18\n(I384\nI503\ntp19\na(I384\nI648\ntp20\na(I640\nI360\ntp21\na(I640\nI503\ntp22\na(I640\nI648\ntp23\na(I896\nI503\ntp24\na(I896\nI648\ntp25\na(I0\nI0\ntp26\nag26\nasS'calibrationPts'\np27\n(lp28\n(lp29\n(I225\nI456\ntp30\na(I223\nI456\ntp31\na(I206\nI458\ntp32\na(I223\nI456\ntp33\na(I224\nI457\ntp34\na(I224\nI458\ntp35\na(I224\nI458\ntp36\na(I225\nI458\ntp37\na(I224\nI456\ntp38\na(I224\nI457\ntp39\na(I225\nI457\ntp40\na(I224\nI457\ntp41\na(I224\nI457\ntp42\na(I224\nI457\ntp43\na(I224\nI457\ntp44\na(I225\nI457\ntp45\na(I224\nI457\ntp46\na(I224\nI456\ntp47\na(I225\nI456\ntp48\na(I224\nI457\ntp49\na(I225\nI457\ntp50\na(I225\nI456\ntp51\na(I225\nI456\ntp52\na(I226\nI457\ntp53\na(I226\nI458\ntp54\na(I224\nI457\ntp55\na(I226\nI458\ntp56\na(I226\nI458\ntp57\na(I224\nI457\ntp58\na(I225\nI457\ntp59\na(I225\nI457\ntp60\na(I226\nI458\ntp61\naa(lp62\n(I279\nI464\ntp63\na(I279\nI464\ntp64\na(I281\nI465\ntp65\na(I293\nI467\ntp66\na(I291\nI465\ntp67\na(I293\nI466\ntp68\na(I293\nI466\ntp69\na(I291\nI465\ntp70\na(I292\nI466\ntp71\na(I292\nI465\ntp72\na(I292\nI466\ntp73\na(I292\nI466\ntp74\na(I292\nI465\ntp75\na(I292\nI466\ntp76\na(I292\nI466\ntp77\na(I293\nI465\ntp78\na(I292\nI465\ntp79\na(I293\nI466\ntp80\na(I292\nI466\ntp81\na(I292\nI465\ntp82\na(I292\nI465\ntp83\na(I293\nI466\ntp84\na(I292\nI465\ntp85\na(I292\nI466\ntp86\na(I292\nI466\ntp87\na(I293\nI465\ntp88\na(I292\nI466\ntp89\na(I292\nI466\ntp90\na(I293\nI466\ntp91\na(I293\nI464\ntp92\na(I292\nI466\ntp93\na(I292\nI465\ntp94\naa(lp95\n(I680\nI434\ntp96\na(I641\nI436\ntp97\na(I640\nI436\ntp98\na(I645\nI436\ntp99\na(I647\nI435\ntp100\na(I646\nI438\ntp101\na(I645\nI434\ntp102\na(I649\nI436\ntp103\na(I645\nI434\ntp104\na(I643\nI438\ntp105\na(I649\nI434\ntp106\na(I652\nI433\ntp107\na(I651\nI439\ntp108\na(I649\nI438\ntp109\na(I644\nI433\ntp110\na(I646\nI439\ntp111\na(I647\nI437\ntp112\na(I641\nI436\ntp113\na(I637\nI436\ntp114\na(I644\nI438\ntp115\na(I650\nI438\ntp116\na(I650\nI435\ntp117\na(I640\nI436\ntp118\na(I646\nI436\ntp119\na(I645\nI434\ntp120\na(I647\nI436\ntp121\na(I639\nI433\ntp122\na(I645\nI434\ntp123\na(I649\nI433\ntp124\na(I646\nI432\ntp125\na(I648\nI438\ntp126\na(I647\nI438\ntp127\na(I648\nI436\ntp128\na(I644\nI435\ntp129\na(I645\nI435\ntp130\na(I643\nI434\ntp131\na(I641\nI436\ntp132\na(I648\nI433\ntp133\na(I650\nI439\ntp134\naa(lp135\n(I605\nI450\ntp136\na(I609\nI453\ntp137\na(I603\nI450\ntp138\na(I629\nI452\ntp139\na(I630\nI452\ntp140\na(I631\nI451\ntp141\na(I631\nI452\ntp142\na(I631\nI452\ntp143\na(I630\nI453\ntp144\na(I631\nI454\ntp145\na(I630\nI453\ntp146\na(I631\nI454\ntp147\na(I630\nI453\ntp148\na(I630\nI452\ntp149\na(I630\nI453\ntp150\na(I629\nI453\ntp151\na(I631\nI454\ntp152\na(I629\nI453\ntp153\na(I629\nI453\ntp154\na(I629\nI453\ntp155\na(I629\nI453\ntp156\na(I630\nI452\ntp157\na(I630\nI453\ntp158\na(I630\nI452\ntp159\na(I631\nI454\ntp160\na(I631\nI453\ntp161\na(I630\nI453\ntp162\na(I630\nI453\ntp163\na(I630\nI454\ntp164\na(I630\nI454\ntp165\na(I629\nI453\ntp166\na(I629\nI453\ntp167\na(I629\nI453\ntp168\na(I629\nI453\ntp169\na(I630\nI453\ntp170\na(I630\nI453\ntp171\na(I629\nI453\ntp172\na(I629\nI453\ntp173\na(I629\nI452\ntp174\na(I629\nI453\ntp175\na(I630\nI453\ntp176\naa(lp177\n(I610\nI463\ntp178\na(I612\nI463\ntp179\na(I611\nI464\ntp180\na(I622\nI462\ntp181\na(I622\nI464\ntp182\na(I621\nI463\ntp183\na(I622\nI465\ntp184\na(I622\nI462\ntp185\na(I622\nI464\ntp186\na(I622\nI463\ntp187\na(I623\nI463\ntp188\na(I622\nI463\ntp189\na(I623\nI463\ntp190\na(I622\nI464\ntp191\na(I623\nI464\ntp192\na(I621\nI464\ntp193\na(I622\nI463\ntp194\na(I622\nI464\ntp195\na(I621\nI463\ntp196\na(I622\nI463\ntp197\na(I622\nI463\ntp198\na(I622\nI463\ntp199\na(I622\nI464\ntp200\na(I622\nI464\ntp201\na(I622\nI463\ntp202\na(I622\nI463\ntp203\na(I622\nI464\ntp204\na(I622\nI464\ntp205\na(I622\nI464\ntp206\na(I622\nI464\ntp207\na(I622\nI464\ntp208\na(I624\nI464\ntp209\na(I623\nI464\ntp210\na(I622\nI464\ntp211\na(I623\nI463\ntp212\na(I623\nI463\ntp213\na(I623\nI463\ntp214\na(I622\nI463\ntp215\na(I622\nI464\ntp216\na(I624\nI462\ntp217\na(I623\nI461\ntp218\naa(lp219\n(I1009\nI451\ntp220\na(I1009\nI453\ntp221\na(I1008\nI452\ntp222\na(I1024\nI452\ntp223\na(I1024\nI452\ntp224\na(I1025\nI452\ntp225\na(I1023\nI451\ntp226\na(I1024\nI451\ntp227\na(I1024\nI452\ntp228\na(I1025\nI451\ntp229\na(I1025\nI452\ntp230\na(I1025\nI452\ntp231\na(I1024\nI452\ntp232\na(I1024\nI450\ntp233\na(I1024\nI452\ntp234\na(I1024\nI452\ntp235\na(I1025\nI451\ntp236\na(I1025\nI452\ntp237\na(I1025\nI453\ntp238\na(I1024\nI452\ntp239\na(I1024\nI452\ntp240\na(I1024\nI453\ntp241\na(I1025\nI452\ntp242\na(I1025\nI453\ntp243\na(I1024\nI452\ntp244\na(I1024\nI452\ntp245\na(I1024\nI452\ntp246\na(I1024\nI452\ntp247\na(I1024\nI453\ntp248\na(I1024\nI452\ntp249\na(I1024\nI453\ntp250\naa(lp251\n(I944\nI463\ntp252\na(I940\nI462\ntp253\na(I943\nI463\ntp254\na(I953\nI462\ntp255\na(I953\nI461\ntp256\na(I953\nI463\ntp257\na(I953\nI463\ntp258\na(I953\nI464\ntp259\na(I952\nI462\ntp260\na(I954\nI460\ntp261\na(I953\nI462\ntp262\na(I953\nI462\ntp263\na(I952\nI461\ntp264\na(I953\nI464\ntp265\na(I953\nI463\ntp266\na(I953\nI463\ntp267\na(I953\nI462\ntp268\na(I953\nI464\ntp269\na(I953\nI462\ntp270\na(I953\nI462\ntp271\na(I953\nI463\ntp272\na(I954\nI461\ntp273\na(I954\nI462\ntp274\na(I952\nI462\ntp275\na(I953\nI464\ntp276\na(I953\nI462\ntp277\na(I952\nI462\ntp278\na(I953\nI463\ntp279\na(I953\nI465\ntp280\na(I954\nI463\ntp281\na(I953\nI464\ntp282\na(I952\nI461\ntp283\na(I953\nI463\ntp284\na(I953\nI464\ntp285\na(I953\nI464\ntp286\naa(lp287\na(lp288\nasS'orp'\np289\ng18\ns."
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "good_calib.pickle",
          "type": "blob",
          "size": 5.2890625,
          "content": "(dp0\nS'hom'\np1\ncnumpy.core.multiarray\n_reconstruct\np2\n(cnumpy\nndarray\np3\n(I0\ntp4\nS'b'\np5\ntp6\nRp7\n(I1\n(I3\nI3\ntp8\ncnumpy\ndtype\np9\n(S'f8'\np10\nI0\nI1\ntp11\nRp12\n(I3\nS'<'\np13\nNNNI-1\nI-1\nI0\ntp14\nbI00\nS'\\xf9\\x9b\\x18\\x14rD\\xac?D\\xba\\xdf\\x8e\\x1b\\xf3\\xf4\\xbf\\x1c\\xa6\\xcce\\\\+\\x83@6?b\\xb0\\x12U\\x14? \\xc0\\xa1L\\r\\x02\\xbd?\\xa6\\x1b3\\xcfb\\xc7\\x1b\\xc00\\xae\\xc3\\xc9H\\xf9\\x81>\\x96U\\x85\\x0b=\\x87`\\xbf\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?'\np15\ntp16\nbsS'realPts'\np17\n(lp18\n(I384\nI503\ntp19\na(I384\nI648\ntp20\na(I640\nI360\ntp21\na(I640\nI503\ntp22\na(I640\nI648\ntp23\na(I896\nI503\ntp24\na(I896\nI648\ntp25\nasS'calibrationPts'\np26\n(lp27\n(lp28\n(I239\nI460\ntp29\na(I239\nI457\ntp30\na(I239\nI458\ntp31\na(I240\nI458\ntp32\na(I240\nI457\ntp33\na(I239\nI457\ntp34\na(I240\nI456\ntp35\na(I241\nI456\ntp36\na(I241\nI456\ntp37\na(I239\nI457\ntp38\na(I239\nI455\ntp39\na(I240\nI456\ntp40\na(I240\nI456\ntp41\na(I240\nI456\ntp42\na(I240\nI457\ntp43\na(I241\nI456\ntp44\na(I240\nI457\ntp45\na(I241\nI456\ntp46\na(I239\nI456\ntp47\na(I240\nI458\ntp48\na(I241\nI457\ntp49\na(I240\nI457\ntp50\na(I241\nI456\ntp51\na(I241\nI456\ntp52\na(I241\nI457\ntp53\na(I240\nI456\ntp54\na(I239\nI457\ntp55\na(I240\nI457\ntp56\na(I239\nI456\ntp57\na(I238\nI455\ntp58\na(I240\nI456\ntp59\na(I240\nI456\ntp60\na(I239\nI456\ntp61\na(I239\nI455\ntp62\na(I240\nI456\ntp63\naa(lp64\n(I286\nI465\ntp65\na(I303\nI468\ntp66\na(I303\nI467\ntp67\na(I303\nI467\ntp68\na(I299\nI462\ntp69\na(I302\nI465\ntp70\na(I303\nI466\ntp71\na(I303\nI466\ntp72\na(I303\nI463\ntp73\na(I302\nI464\ntp74\na(I302\nI465\ntp75\na(I301\nI465\ntp76\na(I302\nI467\ntp77\na(I302\nI466\ntp78\na(I302\nI466\ntp79\na(I301\nI465\ntp80\na(I302\nI465\ntp81\na(I302\nI465\ntp82\na(I303\nI466\ntp83\na(I301\nI466\ntp84\na(I303\nI466\ntp85\na(I302\nI464\ntp86\na(I302\nI464\ntp87\na(I302\nI468\ntp88\na(I302\nI465\ntp89\na(I301\nI466\ntp90\na(I301\nI466\ntp91\na(I301\nI466\ntp92\na(I301\nI465\ntp93\na(I302\nI466\ntp94\na(I302\nI465\ntp95\na(I302\nI465\ntp96\naa(lp97\n(I592\nI435\ntp98\na(I592\nI436\ntp99\na(I633\nI435\ntp100\na(I626\nI436\ntp101\na(I629\nI436\ntp102\na(I626\nI436\ntp103\na(I627\nI436\ntp104\na(I632\nI435\ntp105\na(I627\nI436\ntp106\na(I632\nI437\ntp107\na(I632\nI437\ntp108\na(I629\nI437\ntp109\na(I624\nI439\ntp110\na(I623\nI437\ntp111\na(I623\nI437\ntp112\na(I626\nI438\ntp113\na(I627\nI440\ntp114\na(I627\nI438\ntp115\na(I626\nI439\ntp116\na(I627\nI437\ntp117\na(I629\nI438\ntp118\na(I626\nI438\ntp119\na(I628\nI438\ntp120\na(I625\nI438\ntp121\na(I624\nI439\ntp122\na(I627\nI436\ntp123\na(I625\nI439\ntp124\na(I626\nI437\ntp125\na(I629\nI438\ntp126\na(I630\nI441\ntp127\na(I624\nI437\ntp128\na(I634\nI437\ntp129\na(I622\nI437\ntp130\na(I625\nI436\ntp131\na(I628\nI438\ntp132\na(I627\nI437\ntp133\na(I626\nI437\ntp134\na(I628\nI439\ntp135\na(I626\nI436\ntp136\na(I624\nI437\ntp137\naa(lp138\n(I631\nI452\ntp139\na(I613\nI452\ntp140\na(I612\nI451\ntp141\na(I633\nI451\ntp142\na(I633\nI451\ntp143\na(I633\nI452\ntp144\na(I632\nI452\ntp145\na(I632\nI452\ntp146\na(I632\nI452\ntp147\na(I633\nI452\ntp148\na(I634\nI450\ntp149\na(I632\nI453\ntp150\na(I633\nI453\ntp151\na(I632\nI452\ntp152\na(I632\nI452\ntp153\na(I632\nI452\ntp154\na(I632\nI454\ntp155\na(I631\nI452\ntp156\na(I630\nI453\ntp157\na(I630\nI452\ntp158\na(I632\nI451\ntp159\na(I630\nI453\ntp160\na(I630\nI453\ntp161\na(I630\nI452\ntp162\na(I630\nI452\ntp163\na(I629\nI452\ntp164\na(I629\nI454\ntp165\na(I631\nI452\ntp166\na(I630\nI454\ntp167\na(I631\nI452\ntp168\na(I629\nI453\ntp169\na(I630\nI454\ntp170\na(I630\nI453\ntp171\na(I630\nI453\ntp172\na(I630\nI452\ntp173\na(I630\nI453\ntp174\na(I628\nI452\ntp175\na(I630\nI453\ntp176\na(I630\nI454\ntp177\na(I630\nI454\ntp178\na(I630\nI453\ntp179\naa(lp180\n(I652\nI462\ntp181\na(I612\nI462\ntp182\na(I633\nI462\ntp183\na(I634\nI462\ntp184\na(I633\nI462\ntp185\na(I633\nI463\ntp186\na(I633\nI463\ntp187\na(I633\nI461\ntp188\na(I633\nI461\ntp189\na(I634\nI463\ntp190\na(I634\nI463\ntp191\na(I633\nI461\ntp192\na(I634\nI461\ntp193\na(I632\nI460\ntp194\na(I634\nI462\ntp195\na(I634\nI462\ntp196\na(I634\nI462\ntp197\na(I633\nI463\ntp198\na(I633\nI461\ntp199\na(I631\nI462\ntp200\na(I633\nI462\ntp201\na(I632\nI461\ntp202\na(I633\nI462\ntp203\na(I633\nI461\ntp204\na(I633\nI461\ntp205\na(I634\nI462\ntp206\na(I633\nI462\ntp207\na(I633\nI463\ntp208\na(I633\nI461\ntp209\na(I632\nI464\ntp210\na(I633\nI461\ntp211\na(I634\nI462\ntp212\na(I633\nI461\ntp213\na(I633\nI461\ntp214\na(I632\nI462\ntp215\na(I632\nI462\ntp216\na(I633\nI462\ntp217\naa(lp218\n(I1044\nI451\ntp219\na(I1041\nI452\ntp220\na(I1041\nI451\ntp221\na(I1041\nI451\ntp222\na(I1043\nI452\ntp223\na(I1042\nI450\ntp224\na(I1041\nI451\ntp225\na(I1041\nI451\ntp226\na(I1041\nI451\ntp227\na(I1040\nI451\ntp228\na(I1040\nI451\ntp229\na(I1038\nI451\ntp230\na(I1040\nI452\ntp231\na(I1039\nI450\ntp232\na(I1041\nI451\ntp233\na(I1041\nI453\ntp234\na(I1040\nI452\ntp235\na(I1040\nI453\ntp236\na(I1040\nI450\ntp237\na(I1040\nI452\ntp238\na(I1040\nI452\ntp239\na(I1040\nI452\ntp240\na(I1040\nI452\ntp241\na(I1040\nI452\ntp242\na(I1040\nI452\ntp243\na(I1040\nI452\ntp244\na(I1040\nI452\ntp245\na(I1040\nI451\ntp246\na(I1040\nI452\ntp247\na(I1039\nI452\ntp248\na(I1039\nI452\ntp249\na(I1040\nI452\ntp250\na(I1040\nI452\ntp251\na(I1039\nI451\ntp252\na(I1040\nI452\ntp253\na(I1040\nI452\ntp254\na(I1040\nI451\ntp255\na(I1039\nI450\ntp256\na(I1039\nI452\ntp257\naa(lp258\n(I941\nI463\ntp259\na(I943\nI463\ntp260\na(I950\nI461\ntp261\na(I959\nI462\ntp262\na(I959\nI461\ntp263\na(I958\nI459\ntp264\na(I957\nI459\ntp265\na(I976\nI366\ntp266\na(I960\nI461\ntp267\na(I958\nI460\ntp268\na(I959\nI461\ntp269\na(I960\nI461\ntp270\na(I977\nI371\ntp271\na(I959\nI461\ntp272\na(I960\nI461\ntp273\na(I959\nI461\ntp274\na(I959\nI461\ntp275\na(I958\nI461\ntp276\na(I957\nI463\ntp277\na(I958\nI461\ntp278\na(I958\nI462\ntp279\na(I958\nI462\ntp280\na(I958\nI461\ntp281\na(I958\nI462\ntp282\na(I958\nI461\ntp283\na(I958\nI462\ntp284\na(I959\nI461\ntp285\na(I959\nI461\ntp286\na(I958\nI462\ntp287\na(I957\nI461\ntp288\na(I958\nI461\ntp289\na(I957\nI462\ntp290\na(I957\nI462\ntp291\na(I957\nI460\ntp292\na(I957\nI460\ntp293\na(I957\nI463\ntp294\na(I956\nI461\ntp295\na(I957\nI460\ntp296\na(I957\nI461\ntp297\na(I957\nI461\ntp298\na(I957\nI462\ntp299\naa(lp300\na(lp301\nasS'orp'\np302\ng18\ns."
        },
        {
          "name": "index.html",
          "type": "blob",
          "size": 1.1337890625,
          "content": "<!DOCTYPE html>\n<html>\n<head>\n\t<title>SISTINE | BETTER THAN SLICED BREAD</title>\n\t<style type=\"text/css\">\n\t\tbody, html {\n\t\t\tposition: absolute;\n\t\t\theight: 100%;\n\t\t\twidth: 100%;\n\t\t\tmargin: 0;\n\t\t}\n\t\t#main {\n\t\t\theight: 100%;\n\t\t\tbackground: url(sistine.jpg);\n\t\t\ttransition: background-position 0.1s linear;\n\t\t}\n\t</style>\n</head>\n<body>\n<div id='main'></div>\n<script type=\"text/javascript\">\n\n\tvar scale = .8\n\n\tvar bgsize = 3548\n\n\tvar xoffset = 2210;\n\tvar yoffset = 900;\n\n\tvar clicked = false\n\n\tvar mousex = 0\n\tvar mousey = 0\n\n\tdocument.addEventListener('mousemove', function(e){\n\t\tconsole.log(bgsize * scale)\n\t\tmousex = e.clientX\n\t\tmousey = e.clientY\n\t})\n\n\tdocument.addEventListener('mousedown', function(e){\n\t\tclicked = true\n\t})\n\n\tdocument.addEventListener('mouseup', function(e){\n\t\tclicked = false\n\t})\n\n\trequestAnimationFrame(function tick(){\n\t\trequestAnimationFrame(tick)\n\t\t\n\t\tif(!clicked){\n\t\t\tscale = Math.max(scale*.95, .8)\n\t\t} else {\n\t\t\tscale = Math.min(scale*1.05, 3)\n\t\t}\n\t\tmain.style['background-position'] = (mousex + xoffset* scale)+'px '+(mousey + yoffset* scale)+'px'\n\t\tmain.style['background-size'] = (bgsize* scale)+'px'\n\n\t})\n\n</script>\n</body>\n</html>"
        },
        {
          "name": "simulate.py",
          "type": "blob",
          "size": 0.775390625,
          "content": "from Quartz.CoreGraphics import CGEventCreateMouseEvent\nfrom Quartz.CoreGraphics import CGEventPost\nfrom Quartz.CoreGraphics import kCGEventMouseMoved\nfrom Quartz.CoreGraphics import kCGEventLeftMouseDown\nfrom Quartz.CoreGraphics import kCGEventLeftMouseDown\nfrom Quartz.CoreGraphics import kCGEventLeftMouseUp\nfrom Quartz.CoreGraphics import kCGMouseButtonLeft\nfrom Quartz.CoreGraphics import kCGHIDEventTap\n\ndef mouseEvent(type, posx, posy):\n    theEvent = CGEventCreateMouseEvent(None, type, (posx,posy), kCGMouseButtonLeft)\n    CGEventPost(kCGHIDEventTap, theEvent)\n\ndef mousemove(posx,posy):\n    mouseEvent(kCGEventMouseMoved, posx,posy);\n\ndef mousedown(posx,posy):\n    mouseEvent(kCGEventLeftMouseDown, posx,posy);\n\ndef mouseup(posx,posy):\n    mouseEvent(kCGEventLeftMouseUp, posx,posy);\n"
        },
        {
          "name": "simulate_windows.py",
          "type": "blob",
          "size": 0.1982421875,
          "content": "import pyautogui \r\n\r\ndef mousemove(posx,posy):\r\n    pyautogui.moveTo(posx,posy)\r\ndef mousedown(posx,posy):\r\n    pyautogui.mouseDown(posx,posy)\r\ndef mouseup(posx,posy):\r\n    pyautogui.mouseUp(posx,posy)\r\n"
        },
        {
          "name": "sistine.jpg",
          "type": "blob",
          "size": 167.861328125,
          "content": null
        },
        {
          "name": "sistine.py",
          "type": "blob",
          "size": 15.263671875,
          "content": "import cv2\nimport numpy as np\nimport sys, pdb\nimport pickle\nimport simulate\n\n# dont change parameters\nCOMP_DIMENSION_X = 1440\nCOMP_DIMENSION_Y = 900\n\n# parameters\nMIDPOINT_DETECTION_SKIP_ZONE = 0.08\nMIDPOINT_DETECTION_IGNORE_ZONE = 0.1\nFINGER_COLOR_LOW = 90 # b in Lab space\nFINGER_COLOR_HIGH = 110 # b in Lab space\nMIN_FINGER_SIZE = 7000 # pixels\nREFLECTION_MIN_RATIO = 0.1\nFINGER_WIDTH_LOCATION_RATIO = 0.5 # percent of way down from point to dead space\nMOVING_AVERAGE_WEIGHT = 0.5\n\nCAPTURE_DIMENSION_X = 1280\nCAPTURE_DIMENSION_Y = 720\n\nWINDOW_SHIFT_X = (COMP_DIMENSION_X - CAPTURE_DIMENSION_X)/2\nWINDOW_SHIFT_Y = (COMP_DIMENSION_Y - CAPTURE_DIMENSION_Y)/2\n\nCALIBRATION_X_COORDS = [.3,.5,.7]\nCALIBRATION_Y_COORDS = [.5,.7,.9]\n\nVERT_STAGE_SETUP_TIME = 3\nVERT_STAGE_TIME = 6\n\n# unimportant parameters\nLINE_WIDTH = 2\nLINE_HEIGHT = 100\nCIRCLE_RADIUS = 6\nFINGER_RADIUS = 40\nPURPLE = (255, 0, 255)\nCYAN = (255, 255, 0)\nBLUE = (255, 0, 0)\nGREEN = (0, 255, 0)\nYELLOW = (0, 255, 255)\nRED = (0, 0, 255)\nCALIB_CIRCLE_RADIUS = 10\n\ndef segmentImage(image):\n    # this is kinda wrong cause image is actually BGR\n    # but apparently it works??\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    image = cv2.inRange(image[:,:,2], FINGER_COLOR_LOW, FINGER_COLOR_HIGH)\n    return image\n\ndef opencv2system(ox, oy):\n    return (ox + WINDOW_SHIFT_X, oy + WINDOW_SHIFT_Y)\n\ndef findTouchPoint(contour, x, y, w, h):\n    buf = np.zeros((h, w))\n    cv2.drawContours(buf, [contour], -1, 255, 1, offset=(-x, -y))\n    thiny, thinx, width = None, None, float('inf')\n    topstart = int(round(h * MIDPOINT_DETECTION_SKIP_ZONE))\n    bottomstop = int(round(h * (1 - MIDPOINT_DETECTION_SKIP_ZONE)))\n    for row in range(topstart, bottomstop + 1):\n        left = 0\n        for i in range(w):\n            if buf[row][i] == 255:\n                left = i\n                break\n        right = w-1\n        for i in range(w-1, -1, -1):\n            if buf[row][i] == 255:\n                right = i\n                break\n        diff = right - left\n        if diff < width:\n            width = diff\n            thiny = row\n            thinx = int(left + diff / 2.0)\n    cv2.circle(buf, (thinx, thiny), CIRCLE_RADIUS, PURPLE, -1)\n    validstart = int(round(h * MIDPOINT_DETECTION_IGNORE_ZONE))\n    validstop = int(round(h * (1 - MIDPOINT_DETECTION_IGNORE_ZONE)))\n    if not (validstart < thiny < validstop):\n        return None, None, None, None\n\n    width_row = int(thiny + FINGER_WIDTH_LOCATION_RATIO * (validstop - thiny))\n    left = 0\n    for i in range(w):\n        if buf[width_row][i] == 255:\n            left = i\n            break\n    right = w-1\n    for i in range(w-1, -1, -1):\n        if buf[width_row][i] == 255:\n            right = i\n            break\n    widthloc = x + left\n    width = right - left\n    return thinx + x, thiny + y, widthloc, width\n\n\ndef findHoverPoint(\n        contour_big,\n        x1,\n        y1,\n        w1,\n        h1,\n        contour_small,\n        x2,\n        y2,\n        w2,\n        h2):\n    # this can probably be done more efficiently...\n    buf1 = np.zeros((h1, w1))\n    cv2.drawContours(buf1, [contour_big], -1, 255, 1, offset=(-x1, -y1))\n    left1 = 0\n    for i in range(w1):\n        if buf1[0][i] == 255:\n            left1 = i\n            break\n    right1 = w1 - 1\n    for i in range(w1-1, -1, -1):\n        if buf1[0][i] == 255:\n            right1 = i\n            break\n    mid1 = left1 + (right1 - left1) / 2.0\n\n    buf2 = np.zeros((h2, w2))\n    cv2.drawContours(buf2, [contour_big], -2, 255, 2, offset=(-x2, -y2))\n    left2 = 0\n    for i in range(w2):\n        if buf2[-1][i] == 255:\n            left2 = i\n            break\n    right2 = w2 - 1\n    for i in range(w2-1, -1, -1):\n        if buf2[-1][i] == 255:\n            right2 = i\n            break\n    mid2 = left2 + (right2 - left2) / 2.0\n\n    mid_y = ((y1) + (y2 + h2)) / 2.0\n    mid_x = ((x1 + mid1) + (x2 + mid2)) / 2.0\n    return int(mid_x), int(mid_y)\n\n\n# find finger and touch / hover points in an image\n# debugframe is the thing to draw on\n# returns x, y, touch\n# x and y and touch are none if nothing is found\n# touch is true if it's a touch, otherwise it's false\ndef find(segmented_image, debugframe=None, options={}):\n    found_x, found_y, touch = None, None, None\n    if cv2.__version__.startswith('4'):\n        cnts, _ = cv2.findContours(segmented_image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    else:\n        _, cnts, _ = cv2.findContours(segmented_image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    byarea = []\n    for c in cnts:\n        area = cv2.contourArea(c)\n        byarea.append((area, c))\n    byarea.sort(key=lambda i: i[0])\n    if len(byarea) > 2:\n        # is there a finger?\n        largest_contour = byarea[-1][1]\n        x1, y1, w1, h1 = cv2.boundingRect(largest_contour)\n        largest_area = byarea[-1][0]\n        if largest_area > MIN_FINGER_SIZE:\n            # see if there's a reflection\n            smaller_contour = byarea[-2][1]\n            x2, y2, w2, h2 = cv2.boundingRect(smaller_contour)\n            smaller_area = byarea[-2][0]\n            # if they overlap in X and the smaller one is above the larger one\n            if (not (x1 + w1 < x2 or x2 + w2 < x1)) and y2 + h2 < y1 and \\\n                    smaller_area / largest_area >= REFLECTION_MIN_RATIO:\n                # hover\n                if debugframe is not None:\n                    if not options['nocontour'] and not options['nodemodebug']:\n                        cv2.drawContours(debugframe, [largest_contour], -1, GREEN, LINE_WIDTH)\n                        cv2.drawContours(debugframe, [smaller_contour], -1, GREEN, LINE_WIDTH)\n                    if not options['nobox'] and not options['nodemodebug']:\n                        cv2.rectangle(debugframe, (x1, y1), (x1 + w1, y1 + h1), RED, LINE_WIDTH)\n                        cv2.rectangle(debugframe, (x2, y2), (x2 + w2, y2 + h2), RED, LINE_WIDTH)\n                hover_x, hover_y = findHoverPoint(largest_contour, x1, y1, w1, h1,\n                        smaller_contour, x2, y2, w2, h2)\n                return hover_x, hover_y, False\n            else:\n                # touch\n                # find the touch point height\n                touch_x, touch_y, wloc, width = findTouchPoint(largest_contour, x1, y1, w1, h1)\n                if touch_y is not None:\n                    if debugframe is not None:\n                        if not options['nocontour'] and not options['nodemodebug']:\n                            cv2.drawContours(debugframe, [largest_contour], -1, GREEN, LINE_WIDTH)\n                        if not options['nobox'] and not options['nodemodebug']:\n                            cv2.rectangle(debugframe, (x1, y1), (x1 + w1, y1 + h1),\n                                    RED, LINE_WIDTH)\n                        if not options['nowidth']:\n                            cv2.line(debugframe, (wloc, touch_y + LINE_HEIGHT), (wloc, touch_y - LINE_HEIGHT),\n                                    BLUE, LINE_WIDTH)\n                            cv2.line(debugframe, (wloc + width, touch_y + LINE_HEIGHT),\n                                    (wloc + width, touch_y - LINE_HEIGHT), BLUE, LINE_WIDTH)\n                    return touch_x, touch_y, True\n    return None, None, None\n\ndef calibration(ind):\n    rows,cols,_ = (720, 1280, 3) # frame.shape\n    col = cols/2\n\n    pts = []\n    for i in range(len(CALIBRATION_X_COORDS)):\n        x_frac = CALIBRATION_X_COORDS[i]\n        for j in range(len(CALIBRATION_Y_COORDS)):\n            if j == 0 and i != 1:\n                continue\n            y_frac = CALIBRATION_Y_COORDS[j]\n            x = int(x_frac * CAPTURE_DIMENSION_X)\n            y = int(y_frac * CAPTURE_DIMENSION_Y)\n            pt = (x,y)\n            pts.append(pt)\n\n    pt = pts[ind]\n    x_calib, y_calib = pt\n\n    def _calibration(segmented, debugframe, options, ticks, drawframe, calib, state):\n        font = cv2.FONT_HERSHEY_SIMPLEX\n\n        if ticks > VERT_STAGE_SETUP_TIME:\n            cv2.putText(drawframe, 'Keep touching the dot', (10,50), font, 1, (255,255,255),2,cv2.LINE_AA)\n            cv2.circle(drawframe, (x_calib, y_calib), CALIB_CIRCLE_RADIUS, RED, -1)\n            x, y, touch = find(segmented, debugframe=drawframe, options=options)\n            if touch is not None:\n                cv2.circle(drawframe, (x,y), CIRCLE_RADIUS, PURPLE, -1)\n                calib['calibrationPts'][ind].append((x,y))\n        else:\n            cv2.putText(drawframe, 'Move your finger to the dot', (10,50), font, 1, (255,255,255),2,cv2.LINE_AA)\n            cv2.circle(drawframe, (x_calib, y_calib), CALIB_CIRCLE_RADIUS, GREEN, -1)\n        if ticks > VERT_STAGE_TIME:\n            # cleanup\n            calib['realPts'][ind] = pt\n            return False\n        return True\n\n    return _calibration\n\ndef mainLoop(segmented, debugframe, options, ticks, drawframe, calib, state):\n    if 'initialized' not in state:\n        nnn = (None, None, None)\n        state['last'] = [nnn, nnn, nnn] # last 3 results\n        state['last_drawn'] = None # a pair (x, y)\n        state['initialized'] = True\n        state['md'] = False\n        state['usemouse'] = False\n\n    x, y, touch = find(segmented, debugframe=drawframe, options=options)\n    state['last'].append((x, y, touch))\n    state['last'].pop(0)\n    if 'hom' not in calib:\n        webcam_points = calib['calibrationPts']\n        real_points = calib['realPts']\n        calib['orp'] = real_points\n        screen_points = []\n        for i in range(len(real_points)):\n            for _ in range(len(webcam_points[i])):\n                screen_points.append(real_points[i])\n\n        webcam_points = [i for s in webcam_points for i in s]\n        hom = findTransform(webcam_points, screen_points)\n        calib['hom'] = hom\n        if not ('nocalib' in sys.argv):\n            pickle.dump(calib, open('previous.pickle','w+'))\n\n    if not options['nocalib']:\n        for i, j in calib['orp']:\n            i_, j_ = applyTransform(i, j, np.linalg.inv(calib['hom']))\n            cv2.circle(drawframe, (i, j), CIRCLE_RADIUS, RED, -1)\n            cv2.line(drawframe, (i, j), (i_, j_), RED, LINE_WIDTH)\n    \n    shouldMouse = state['usemouse']\n    font = cv2.FONT_HERSHEY_SIMPLEX\n\n    if shouldMouse:\n        cv2.putText(drawframe, 'Mouse on', (10,50), font, 1, (255,255,255),2,cv2.LINE_AA)\n    else:\n        cv2.putText(drawframe, 'Mouse off', (10,50), font, 1, (255,255,255),2,cv2.LINE_AA)\n\n    if touch is not None:\n        if not options['demo']:\n            cv2.circle(drawframe, (x, y), CIRCLE_RADIUS, PURPLE, -1)\n        x_, y_ = applyTransform(x, y, calib['hom'])\n        if state['last_drawn'] is not None:\n            x_ = int(x_ * MOVING_AVERAGE_WEIGHT + (1 - MOVING_AVERAGE_WEIGHT) * state['last_drawn'][0])\n            y_ = int(y_ * MOVING_AVERAGE_WEIGHT + (1 - MOVING_AVERAGE_WEIGHT) * state['last_drawn'][1])\n        state['last_drawn'] = (x_, y_)\n        cv2.circle(drawframe, (x_, y_), FINGER_RADIUS, CYAN, -1)\n        \n        mx, my = opencv2system(x_,y_)\n        if shouldMouse:\n            simulate.mousemove(mx, my)\n        if touch:\n            if not state['md'] and shouldMouse:\n                simulate.mousedown(mx, my)\n                state['md'] = True\n            cv2.circle(drawframe, (x_, y_), FINGER_RADIUS, YELLOW, -1)\n        else:\n            if state['md'] and shouldMouse:\n                simulate.mouseup(mx, my)\n                state['md'] = False\n            cv2.circle(drawframe, (x_, y_), FINGER_RADIUS, CYAN, -1)\n        cv2.circle(drawframe, (x_, y_), CIRCLE_RADIUS, GREEN, -1)\n    else:\n        state['last_drawn'] = None\n\n    return True\n\n# points are in the format [(x, y)]\ndef findTransform(webcam_points, screen_points):\n    print webcam_points\n    print screen_points\n    webcam_points = np.array(webcam_points).astype(np.float)\n    screen_points = np.array(screen_points).astype(np.float)\n    hom, mask = cv2.findHomography(webcam_points, screen_points, method=cv2.RANSAC)\n    return hom\n\n\n# returns the transformed (x, y) as a pair\ndef applyTransform(x, y, homography):\n    inp = np.array([[[x, y]]], dtype=np.float)\n    res = cv2.perspectiveTransform(inp, homography)\n    x_, y_ = res[0,0]\n    return int(round(x_)), int(round(y_))\n\n\nCALIBRATION_MESSAGE = \"\"\"\nDuring the calibration process, you will be \nshown a series of 9 dots on your screen.\n\nTouch the dot with your finger and hold your\nfinger at that position until the next dot \nappears.\n\nWhen the dot is green, it is not capturing\nyour finger position. When the dot is green,\nit is recording information.\n\nEnable/disable mouse control with \"k\" key.\n\nWhen you are ready to begin, press space bar.\n\"\"\"\n\ndef waitSetup(segmented, debugframe, options, ticks, drawframe, calib, state):\n    font = cv2.FONT_HERSHEY_SIMPLEX\n\n    cv2.putText(drawframe, 'Sistine Calibration', (10,50), font, 2, (255,255,255),2,cv2.LINE_AA)\n    for i, line in enumerate(CALIBRATION_MESSAGE.split('\\n')):\n        cv2.putText(drawframe, line, (10,150 + i * 30), font, 1, (255,255,255),2,cv2.LINE_AA)\n\n    return True\n\n\ndef main():\n    cv2.ocl.setUseOpenCL(False) # some stuff dies if you don't do this\n\n    initialStageTicks = cv2.getTickCount()\n    calib = {\n        \"calibrationPts\":[[] for i in range(9)],\n        \"realPts\":[(0,0)] * 7\n    }\n\n    if 'nocalib' in sys.argv:\n        with open('previous.pickle') as f:\n            calib = pickle.load(f)\n        stages = [mainLoop]\n    else:\n        stages = [waitSetup] + [calibration(i) for i in range(7)] + [mainLoop]\n\n    currStage = stages.pop(0)\n\n    # settings\n    options = {}\n    if 'test' in sys.argv:\n        cap = cv2.VideoCapture('cv/fingers/fingers.mov')\n    else:\n        cap = cv2.VideoCapture(0)\n    options['orig'] = 'orig' in sys.argv\n    options['nobox'] = 'nobox' in sys.argv\n    options['nocontour'] = 'nocontour' in sys.argv\n    options['nowidth'] = 'nowidth' in sys.argv\n    options['nocalib'] = 'nocalib' in sys.argv\n    options['demo'] = 'demo' in sys.argv\n    options['nodemodebug'] = 'nodemodebug' in sys.argv\n    if options['demo']:\n        options['nocontour'] = True\n        options['nowidth'] = True\n        options['nobox'] = True\n        options['nocalib'] = True\n\n    debugframe = None\n    # main loop\n    state = {}\n    while True:\n        key = cv2.waitKey(1)\n        if key & 0xff == ord('q'):\n            break\n        elif key & 0xff == ord('k'):\n            state['usemouse'] = not state['usemouse']\n        elif key & 0xff == ord(' ') and currStage == waitSetup:\n            currStage = stages.pop(0)\n            initialStageTicks = cv2.getTickCount()\n\n        # frame by frame capture\n        # I think there's a callback-based way to do this as well, but I think\n        # this way works fine for us\n        ret, frame = cap.read()\n        if frame is None:\n            break\n        frame = cv2.flip(frame, 1) # unmirror left to right\n        segmented = segmentImage(frame)\n\n        # only matters for debugging\n        if options['orig']:\n            drawframe = frame\n        elif options['demo']:\n            drawframe = np.zeros_like(frame)\n        else:\n            drawframe = cv2.cvtColor(segmented, cv2.COLOR_GRAY2BGR)\n\n        ticks = (cv2.getTickCount() - initialStageTicks)/cv2.getTickFrequency()\n        if not currStage(segmented, debugframe, options, ticks, drawframe, calib, state):\n            currStage = stages.pop(0)\n            initialStageTicks = cv2.getTickCount()\n        \n        cv2.imshow('drawframe', drawframe)\n        cv2.moveWindow('drawframe', WINDOW_SHIFT_X, WINDOW_SHIFT_Y)\n\n    # release everything\n    cap.release()\n    cv2.destroyAllWindows()\n    #if state['md']:\n    #    simulate.mouseup(mx, my)\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "sistine_windows.py",
          "type": "blob",
          "size": 14.21875,
          "content": "# Adding changes to sistine\n# Changes Type: Enabled sistine for Windows.\n# Changes: Changed reference from simulate to simulate_windows.py, Changed COMP_DIMENSION to standard resolution for windows, Added install instructions for windows(read me).\n# Author: Ashish Gupta(@https://github.com/ashishgupta1350/)\n# Date: 24 June 2018\n\n\nimport cv2\nimport numpy as np\nimport sys, pdb\nimport pickle\nimport simulate_windows\n\n# [Code Change] Standard Windows Res changed\nCOMP_DIMENSION_X = 1366\nCOMP_DIMENSION_Y = 768\n\n# parameters\nMIDPOINT_DETECTION_SKIP_ZONE = 0.08\nMIDPOINT_DETECTION_IGNORE_ZONE = 0.1\nFINGER_COLOR_LOW = 90 # b in Lab space\nFINGER_COLOR_HIGH = 110 # b in Lab space\nMIN_FINGER_SIZE = 7000 # pixels\nREFLECTION_MIN_RATIO = 0.05\nFINGER_WIDTH_LOCATION_RATIO = 0.5 # percent of way down from point to dead space\nMOVING_AVERAGE_WEIGHT = 0.5\n\n# [Code Change] Optimal Width and Height [1366 * 768].\nCAPTURE_DIMENSION_X = 300\nCAPTURE_DIMENSION_Y = 450\n\nWINDOW_SHIFT_X = (COMP_DIMENSION_X - CAPTURE_DIMENSION_X)/2\nWINDOW_SHIFT_Y = (COMP_DIMENSION_Y - CAPTURE_DIMENSION_Y)/2\n\n# [Code Change] Optimal Points for detection( May be optimised)\nCALIBRATION_X_COORDS = [.1,.5,.9]\nCALIBRATION_Y_COORDS = [.2,.6,.95]\n\nVERT_STAGE_SETUP_TIME = 3\nVERT_STAGE_TIME = 6\n\n# unimportant parameters\nLINE_WIDTH = 2\nLINE_HEIGHT = 100\nCIRCLE_RADIUS = 6\nFINGER_RADIUS = 40\nPURPLE = (255, 0, 255)\nCYAN = (255, 255, 0)\nBLUE = (255, 0, 0)\nGREEN = (0, 255, 0)\nYELLOW = (0, 255, 255)\nRED = (0, 0, 255)\nCALIB_CIRCLE_RADIUS = 10\n\ndef segmentImage(image):\n    # this is kinda wrong cause image is actually BGR\n    # but apparently it works??\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    image = cv2.inRange(image[:,:,2], FINGER_COLOR_LOW, FINGER_COLOR_HIGH)\n    return image\n\ndef opencv2system(ox, oy):\n    return (ox + WINDOW_SHIFT_X, oy + WINDOW_SHIFT_Y)\n\ndef findTouchPoint(contour, x, y, w, h):\n    buf = np.zeros((h, w))\n    cv2.drawContours(buf, [contour], -1, 255, 1, offset=(-x, -y))\n    thiny, thinx, width = None, None, float('inf')\n    topstart = int(round(h * MIDPOINT_DETECTION_SKIP_ZONE))\n    bottomstop = int(round(h * (1 - MIDPOINT_DETECTION_SKIP_ZONE)))\n    for row in range(topstart, bottomstop + 1):\n        left = 0\n        for i in range(w):\n            if buf[row][i] == 255:\n                left = i\n                break\n        right = w-1\n        for i in range(w-1, -1, -1):\n            if buf[row][i] == 255:\n                right = i\n                break\n        diff = right - left\n        if diff < width:\n            width = diff\n            thiny = row\n            thinx = int(left + diff / 2.0)\n    cv2.circle(buf, (thinx, thiny), CIRCLE_RADIUS, PURPLE, -1)\n    validstart = int(round(h * MIDPOINT_DETECTION_IGNORE_ZONE))\n    validstop = int(round(h * (1 - MIDPOINT_DETECTION_IGNORE_ZONE)))\n    if not (validstart < thiny < validstop):\n        return None, None, None, None\n\n    width_row = int(thiny + FINGER_WIDTH_LOCATION_RATIO * (validstop - thiny))\n    left = 0\n    for i in range(w):\n        if buf[width_row][i] == 255:\n            left = i\n            break\n    right = w-1\n    for i in range(w-1, -1, -1):\n        if buf[width_row][i] == 255:\n            right = i\n            break\n    widthloc = x + left\n    width = right - left\n    return thinx + x, thiny + y, widthloc, width\n\n\ndef findHoverPoint(\n        contour_big,\n        x1,\n        y1,\n        w1,\n        h1,\n        contour_small,\n        x2,\n        y2,\n        w2,\n        h2):\n    # this can probably be done more efficiently...\n    buf1 = np.zeros((h1, w1))\n    cv2.drawContours(buf1, [contour_big], -1, 255, 1, offset=(-x1, -y1))\n    left1 = 0\n    for i in range(w1):\n        if buf1[0][i] == 255:\n            left1 = i\n            break\n    right1 = w1 - 1\n    for i in range(w1-1, -1, -1):\n        if buf1[0][i] == 255:\n            right1 = i\n            break\n    mid1 = left1 + (right1 - left1) / 2.0\n\n    buf2 = np.zeros((h2, w2))\n    cv2.drawContours(buf2, [contour_big], -2, 255, 2, offset=(-x2, -y2))\n    left2 = 0\n    for i in range(w2):\n        if buf2[-1][i] == 255:\n            left2 = i\n            break\n    right2 = w2 - 1\n    for i in range(w2-1, -1, -1):\n        if buf2[-1][i] == 255:\n            right2 = i\n            break\n    mid2 = left2 + (right2 - left2) / 2.0\n\n    mid_y = ((y1) + (y2 + h2)) / 2.0\n    mid_x = ((x1 + mid1) + (x2 + mid2)) / 2.0\n    return int(mid_x), int(mid_y)\n\n\n# find finger and touch / hover points in an image\n# debugframe is the thing to draw on\n# returns x, y, touch\n# x and y and touch are none if nothing is found\n# touch is true if it's a touch, otherwise it's false\ndef find(segmented_image, debugframe=None, options={}):\n    found_x, found_y, touch = None, None, None\n    _, cnts, _ = cv2.findContours(segmented_image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    byarea = []\n    for c in cnts:\n        area = cv2.contourArea(c)\n        byarea.append((area, c))\n    byarea.sort(key=lambda i: i[0])\n    if len(byarea) > 2:\n        # is there a finger?\n        largest_contour = byarea[-1][1]\n        x1, y1, w1, h1 = cv2.boundingRect(largest_contour)\n        largest_area = byarea[-1][0]\n        if largest_area > MIN_FINGER_SIZE:\n            # see if there's a reflection\n            smaller_contour = byarea[-2][1]\n            x2, y2, w2, h2 = cv2.boundingRect(smaller_contour)\n            smaller_area = byarea[-2][0]\n            # if they overlap in X and the smaller one is above the larger one\n            if (not (x1 + w1 < x2 or x2 + w2 < x1)) and y2 + h2 < y1 and \\\n                    smaller_area / largest_area >= REFLECTION_MIN_RATIO:\n                # hover\n                if debugframe is not None:\n                    if not options['nocontour'] and not options['nodemodebug']:\n                        cv2.drawContours(debugframe, [largest_contour], -1, GREEN, LINE_WIDTH)\n                        cv2.drawContours(debugframe, [smaller_contour], -1, GREEN, LINE_WIDTH)\n                    if not options['nobox'] and not options['nodemodebug']:\n                        cv2.rectangle(debugframe, (x1, y1), (x1 + w1, y1 + h1), RED, LINE_WIDTH)\n                        cv2.rectangle(debugframe, (x2, y2), (x2 + w2, y2 + h2), RED, LINE_WIDTH)\n                hover_x, hover_y = findHoverPoint(largest_contour, x1, y1, w1, h1,\n                        smaller_contour, x2, y2, w2, h2)\n                return hover_x, hover_y, False\n            else:\n                # touch\n                # find the touch point height\n                touch_x, touch_y, wloc, width = findTouchPoint(largest_contour, x1, y1, w1, h1)\n                if touch_y is not None:\n                    if debugframe is not None:\n                        if not options['nocontour'] and not options['nodemodebug']:\n                            cv2.drawContours(debugframe, [largest_contour], -1, GREEN, LINE_WIDTH)\n                        if not options['nobox'] and not options['nodemodebug']:\n                            cv2.rectangle(debugframe, (x1, y1), (x1 + w1, y1 + h1),\n                                    RED, LINE_WIDTH)\n                        if not options['nowidth']:\n                            cv2.line(debugframe, (wloc, touch_y + LINE_HEIGHT), (wloc, touch_y - LINE_HEIGHT),\n                                    BLUE, LINE_WIDTH)\n                            cv2.line(debugframe, (wloc + width, touch_y + LINE_HEIGHT),\n                                    (wloc + width, touch_y - LINE_HEIGHT), BLUE, LINE_WIDTH)\n                    return touch_x, touch_y, True\n    return None, None, None\n\ndef calibration(ind):\n    rows,cols,_ = (720, 1280, 3) # frame.shape\n    col = cols/2\n\n    pts = []\n    for i in range(len(CALIBRATION_X_COORDS)):\n        x_frac = CALIBRATION_X_COORDS[i]\n        for j in range(len(CALIBRATION_Y_COORDS)):\n            if j == 0 and i != 1:\n                continue\n            y_frac = CALIBRATION_Y_COORDS[j]\n            x = int(x_frac * CAPTURE_DIMENSION_X)\n            y = int(y_frac * CAPTURE_DIMENSION_Y)\n            pt = (x,y)\n            pts.append(pt)\n\n    pt = pts[ind]\n    x_calib, y_calib = pt\n\n    def _calibration(segmented, debugframe, options, ticks, drawframe, calib, state):\n        if ticks > VERT_STAGE_SETUP_TIME:\n            cv2.circle(drawframe, (x_calib, y_calib), CALIB_CIRCLE_RADIUS, RED, -1)\n            x, y, touch = find(segmented, debugframe=drawframe, options=options)\n            if touch is not None:\n                cv2.circle(drawframe, (x,y), CIRCLE_RADIUS, PURPLE, -1)\n                calib['calibrationPts'][ind].append((x,y))\n\n        else:\n            cv2.circle(drawframe, (x_calib, y_calib), CALIB_CIRCLE_RADIUS, GREEN, -1)\n        if ticks > VERT_STAGE_TIME:\n            # cleanup\n            calib['realPts'][ind] = pt\n            return False\n        return True\n\n    return _calibration\n\ndef mainLoop(segmented, debugframe, options, ticks, drawframe, calib, state):\n    if 'initialized' not in state:\n        nnn = (None, None, None)\n        state['last'] = [nnn, nnn, nnn] # last 3 results\n        state['last_drawn'] = None # a pair (x, y)\n        state['initialized'] = True\n        state['md'] = False\n        state['usemouse'] = False\n\n    x, y, touch = find(segmented, debugframe=drawframe, options=options)\n    state['last'].append((x, y, touch))\n    state['last'].pop(0)\n    if 'hom' not in calib:\n        webcam_points = calib['calibrationPts']\n        real_points = calib['realPts']\n        calib['orp'] = real_points\n        screen_points = []\n        for i in range(len(real_points)):\n            for _ in range(len(webcam_points[i])):\n                screen_points.append(real_points[i])\n\n        webcam_points = [i for s in webcam_points for i in s]\n        hom = findTransform(webcam_points, screen_points)\n        calib['hom'] = hom\n        if not ('nocalib' in sys.argv):\n            pickle.dump(calib, open('previous.pickle','wb+'))\n\n    if not options['nocalib']:\n        for i, j in calib['orp']:\n            i_, j_ = applyTransform(i, j, np.linalg.inv(calib['hom']))\n            cv2.circle(drawframe, (i, j), CIRCLE_RADIUS, RED, -1)\n            cv2.line(drawframe, (i, j), (i_, j_), RED, LINE_WIDTH)\n\n    if touch is not None:\n        if not options['demo']:\n            cv2.circle(drawframe, (x, y), CIRCLE_RADIUS, PURPLE, -1)\n        x_, y_ = applyTransform(x, y, calib['hom'])\n        if state['last_drawn'] is not None:\n            x_ = int(x_ * MOVING_AVERAGE_WEIGHT + (1 - MOVING_AVERAGE_WEIGHT) * state['last_drawn'][0])\n            y_ = int(y_ * MOVING_AVERAGE_WEIGHT + (1 - MOVING_AVERAGE_WEIGHT) * state['last_drawn'][1])\n        state['last_drawn'] = (x_, y_)\n        cv2.circle(drawframe, (x_, y_), FINGER_RADIUS, CYAN, -1)\n        shouldMouse = True #state['usemouse']\n        mx, my = opencv2system(x_,y_)\n        # [Code Change] Changing import reference for simulate_windows.\n        if shouldMouse:\n            simulate_windows.mousemove(mx, my)\n        if touch:\n            if not state['md'] and shouldMouse:\n                simulate_windows.mousedown(mx, my)\n                state['md'] = True\n            cv2.circle(drawframe, (x_, y_), FINGER_RADIUS, YELLOW, -1)\n        else:\n            if state['md'] and shouldMouse:\n                simulate_windows.mouseup(mx, my)\n                state['md'] = False\n            cv2.circle(drawframe, (x_, y_), FINGER_RADIUS, CYAN, -1)\n        cv2.circle(drawframe, (x_, y_), CIRCLE_RADIUS, GREEN, -1)\n    else:\n        state['last_drawn'] = None\n\n    return True\n\n# points are in the format [(x, y)]\ndef findTransform(webcam_points, screen_points):\n    print(webcam_points)\n    print(screen_points)\n    webcam_points = np.array(webcam_points).astype(np.float)\n    screen_points = np.array(screen_points).astype(np.float)\n    hom, mask = cv2.findHomography(webcam_points, screen_points, method=cv2.RANSAC)\n    return hom\n\n\n# returns the transformed (x, y) as a pair\ndef applyTransform(x, y, homography):\n    inp = np.array([[[x, y]]], dtype=np.float)\n    res = cv2.perspectiveTransform(inp, homography)\n    x_, y_ = res[0,0]\n    return int(round(x_)), int(round(y_))\n\n\ndef main():\n    cv2.ocl.setUseOpenCL(False) # some stuff dies if you don't do this\n\n    initialStageTicks = cv2.getTickCount()\n    calib = {\n        \"calibrationPts\":[[] for i in range(9)],\n        \"realPts\":[(0,0)] * 7\n    }\n\n    if 'nocalib' in sys.argv:\n        with open('previous.pickle') as f:\n            calib = pickle.load(f)\n        stages = [mainLoop]\n    else:\n        stages = [calibration(i) for i in range(7)] + [mainLoop]\n\n    currStage = stages.pop(0)\n\n    # settings\n    options = {}\n    if 'test' in sys.argv:\n        cap = cv2.VideoCapture('cv/fingers/fingers.mov')\n    else:\n        cap = cv2.VideoCapture(0)\n    options['orig'] = 'orig' in sys.argv\n    options['nobox'] = 'nobox' in sys.argv\n    options['nocontour'] = 'nocontour' in sys.argv\n    options['nowidth'] = 'nowidth' in sys.argv\n    options['nocalib'] = 'nocalib' in sys.argv\n    options['demo'] = 'demo' in sys.argv\n    options['nodemodebug'] = 'nodemodebug' in sys.argv\n    if options['demo']:\n        options['nocontour'] = True\n        options['nowidth'] = True\n        options['nobox'] = True\n        options['nocalib'] = True\n\n    debugframe = None\n    # main loop\n    state = {}\n    while True:\n        key = cv2.waitKey(1)\n        if key & 0xff == ord('q'):\n            break\n        elif key & 0xff == ord('k'):\n            state['usemouse'] = False\n\n        # frame by frame capture\n        # I think there's a callback-based way to do this as well, but I think\n        # this way works fine for us\n        ret, frame = cap.read()\n        if frame is None:\n            break\n        frame = cv2.flip(frame, 1) # unmirror left to right\n        segmented = segmentImage(frame)\n\n        # only matters for debugging\n        if options['orig']:\n            drawframe = frame\n        elif options['demo']:\n            drawframe = np.zeros_like(frame)\n        else:\n            drawframe = cv2.cvtColor(segmented, cv2.COLOR_GRAY2BGR)\n\n        ticks = (cv2.getTickCount() - initialStageTicks)/cv2.getTickFrequency()\n        if not currStage(segmented, debugframe, options, ticks, drawframe, calib, state):\n            currStage = stages.pop(0)\n            initialStageTicks = cv2.getTickCount()\n        \n        cv2.imshow('drawframe', drawframe)\n        cv2.moveWindow('drawframe', int(WINDOW_SHIFT_X), int(WINDOW_SHIFT_Y))\n\n    # release everything\n    cap.release()\n    cv2.destroyAllWindows()\n    #if state['md']:\n    #    simulate_windows.mouseup(mx, my)\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "splash.png",
          "type": "blob",
          "size": 1530.8291015625,
          "content": null
        },
        {
          "name": "submission.md",
          "type": "blob",
          "size": 1.0341796875,
          "content": "# Project Name: Sistine\n\n## Tagline: \"Download a touchscreen\" on any laptop\n\n## Public URL: https://youtu.be/1SrF__brJMo\n\n## Description\n\nSistine is a touchscreen that you can download to your laptop. With about $1 of\nhardware paired with our fancy computer vision algorithms, you can turn any\nlaptop screen into a touchscreen.\n\nYou can click links or draw pictures or play the piano on your screen!\n\nTouch it to believe it: https://www.youtube.com/watch?v=1SrF__brJMo\n\n(Your grandma can finally click the screen with her finger.)\n\n## Project Information\n\n### Technologies\n\nSoftware: Python, OpenCV, all hand-written detector code.\n\nHardware: Paper plates, mirror shards, hot glue, angle brackets, a door hinge,\nand a Leatherman. (We burned our fingers a nonzero number of times in the making\nof this hack.)\n\n### Presentation Details\n\nWe're presenting on a MacBook, customized with $1 worth of hand-built hardware.\nThis is a hack that's most effectively presented on the MacBook screen itself,\nnot a projector. We do want a projector to show a video, though.\n"
        }
      ]
    }
  ]
}