{
  "metadata": {
    "timestamp": 1736559725404,
    "page": 422,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVlabs/imaginaire",
      "stars": 4028,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0458984375,
          "content": ".git\ndocs\ndataset\ncheckpoints\nlogs\nprojects\nhub"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.1923828125,
          "content": "checkpoints/\n.idea/\nlogs/\nresults/\n\n.vscode\n*.ipynb\nDockerfile\n\ndocs/_build/\nnsys_profile*\n\n# Byte-compiled / optimized / DLL files\n__pycache__\n*.py[cod]\n*$py.class\n*.pyc\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/source/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# static files generated from Django application using `collectstatic`\nmedia\nstatic\n\n"
        },
        {
          "name": "Dockerfile.base",
          "type": "blob",
          "size": 0.42578125,
          "content": "ARG DEBIAN_FRONTEND=noninteractive\n\n# Install basics\nRUN apt-get update && apt-get install -y --allow-downgrades --allow-change-held-packages --no-install-recommends \\\n        build-essential \\\n        cmake \\\n        git \\\n        curl \\\n        vim \\\n        tmux \\\n        wget \\\n        bzip2 \\\n        unzip \\\n        g++ \\\n        ca-certificates \\\n        ffmpeg \\\n        libx264-dev \\\n        imagemagick \\\n        libnss3-dev\n"
        },
        {
          "name": "INSTALL.md",
          "type": "blob",
          "size": 2.6767578125,
          "content": "<img src=\"imaginaire_logo.svg\" alt=\"imaginaire_logo.svg\" height=\"360\"/>\n\n# Imaginaire\n### [Docs](http://imaginaire.cc/docs) | [License](LICENSE.md) | [Installation](INSTALL.md) | [Model Zoo](MODELZOO.md)\n\n# Installation Guide\n\nOur library is developed using an Ubuntu 18.04 machine. We have not yet tested our library on other operating systems.\n\n## Prerequisite\n- [Anaconda3](https://www.anaconda.com/products/individual)\n- [cuda11.1](https://developer.nvidia.com/cuda-toolkit)\n- [cudnn](https://developer.nvidia.com/cudnn)\n\nWe provide three different ways of installing Imaginaire.\n\n### 1. Installation (default)\nNote: sudo privilege is required.\n```bash\ngit clone https://github.com/nvlabs/imaginaire\ncd imaginaire\nbash scripts/install.sh\nbash scripts/test_training.sh\n```\nIf installation is not successful, error message will be prompted.\n\n### 2. Installation with Docker\nWe use NVIDIA docker image. We provide two ways to build the docker image.\n1. Build a target docker image\n```bash\nbash scripts/build_docker.sh 21.06\n```\n\n2. Launch an interactive docker container and test the imaginaire repo.\n```bash\ncd scripts\nbash start_local_docker.sh 21.06\ncd ${IMAGINAIRE_ROOT}\nbash scripts/test_training.sh\n```\n\n### 3. Installation with Conda\nSet up the Conda environment and install packages with\n```bash\nconda env create --file scripts/requirements_conda.yaml\n# install third-party libraries\nexport CUDA_VERSION=$(nvcc --version| grep -Po \"(\\d+\\.)+\\d+\" | head -1)\nCURRENT=$(pwd)\nfor p in correlation channelnorm resample2d bias_act upfirdn2d; do\n    cd imaginaire/third_party/${p};\n    rm -rf build dist *info;\n    python setup.py install;\n    cd ${CURRENT};\ndone\n```\nTo activate the environment and test the repo:\n```bash\nconda activate imaginaire\nbash scripts/test_training.sh\n```\n\n## Flake8\nWe follow the PEP8 style using flake8. To follow our practice, please do\n```bash\npip install flake8\nflake8 --install-hook git\ngit config --bool flake8.strict true\n```\nWe set the maximum line length to 80. To avoid error messages due to different line length, create a file `~/.config/flake8` with the following content:\n```\n[flake8]\nmax-line-length = 200\n```\n\n## Windows Installation [Out-dated]\n\n- Install [git for windows](https://gitforwindows.org/)\n- Install [Microsoft C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)\n- Install [Anaconda3](https://repo.anaconda.com/archive/Anaconda3-2020.02-Windows-x86_64.exe)\n- Install [CUDA11.1](https://developer.nvidia.com/cuda-11.1-download-archive)\n- Install [cudnn](https://developer.nvidia.com/cudnn)\n- Open an anaconda prompt.\n```\ncd https://github.com/NVlabs/imaginaire\n.\\scripts\\install.bat\n```\n\nPowershell\n```\n$env:PYTHONPATH = pwd\nGet-ChildItem Env:PYTHONPATH\n```\n\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 4.3486328125,
          "content": "# NVIDIA Source Code License for Imaginaire \n\n## 1. Definitions \n \n- “Licensor” means any person or entity that distributes its Work. \n\n-  “Software” means the original work of authorship made available under this License. \n\n-  “Work” means the Software and any additions to or derivative works of the Software that are made available under this License. \n\n-  “NVIDIA Processors” means any central processing unit (CPU), graphics processing unit (GPU), field-programmable gate array (FPGA), application-specific integrated circuit (ASIC) or any combination thereof designed, made, sold, or provided by NVIDIA or its affiliates. \n\n-  The terms “reproduce,” “reproduction,” “derivative works,” and “distribution” have the meaning as provided under U.S. copyright law; provided, however, that for the purposes of this License, derivative works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work. \n\n-  Works, including the Software, are “made available” under this License by including in or with the Work either (a) a copyright notice referencing the applicability of this License to the Work, or (b) a copy of this License. \n\n## 2. License Grant \n\n### 2.1 Copyright Grant. \n\nSubject to the terms and conditions of this License, each Licensor grants to you a perpetual, worldwide, non-exclusive, royalty-free, copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense and distribute its Work and any resulting derivative works in any form.\n\n## 3. Limitations\n\n### 3.1 Redistribution. \n\nYou may reproduce or distribute the Work only if (a) you do so under this License, (b) you include a complete copy of this License with your distribution, and (c) you retain without modification any copyright, patent, trademark, or attribution notices that are present in the Work.\n\n### 3.2 Derivative Works.\n\nYou may specify that additional or different terms apply to the use, reproduction, and distribution of your derivative works of the Work (“Your Terms”) only if (a) Your Terms provide that the use limitation in Section 3.3 applies to your derivative works, and (b) you identify the specific derivative works that are subject to Your Terms. Notwithstanding Your Terms, this License (including the redistribution requirements in Section 3.1) will continue to apply to the Work itself.\n \n### 3.3 Use Limitation.\n\nThe Work and any derivative works thereof only may be used or intended for use non-commercially and with NVIDIA Processors. Notwithstanding the foregoing, NVIDIA and its affiliates may use the Work and any derivative works commercially. As used herein, “non-commercially” means for research or evaluation purposes only.\n\n### 3.4 Patent Claims.\n\nIf you bring or threaten to bring a patent claim against any Licensor (including any claim, cross-claim or counterclaim in a lawsuit) to enforce any patents that you allege are infringed by any Work, then your rights under this License from such Licensor (including the grant in Section 2.1) will terminate immediately.\n\n### 3.5 Trademarks.\n\nThis License does not grant any rights to use any Licensor’s or its affiliates’ names, logos, or trademarks, except as necessary to reproduce the notices described in this License.\n\n### 3.6 Termination.\n\nIf you violate any term of this License, then your rights under this License (including the grant in Section 2.1) will terminate immediately.\n\n## 4. Disclaimer of Warranty. \n\nTHE WORK IS PROVIDED “AS IS” WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF M ERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER THIS LICENSE.  \n\n## 5. Limitation of Liability. \n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE SHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF OR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK (INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION, LOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER COMM ERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. \n"
        },
        {
          "name": "MODELZOO.md",
          "type": "blob",
          "size": 4.9794921875,
          "content": "<img src=\"imaginaire_logo.svg\" alt=\"imaginaire_logo.svg\" height=\"360\"/>\n\n# Imaginaire\n### [Docs](http://imaginaire.cc/docs) | [License](LICENSE.md) | [Installation](INSTALL.md) | [Model Zoo](MODELZOO.md)\n\n# Model Zoo\n\n## Introduction\n\nWe provide a wide range of pretrained imaginaire models for different tasks. All the models were trained using an NVIDIA DGX 1 machine with 8 32GB V100 using NVIDIA PyTorch docker 20.06 or later.\n\n\n|Algorithm Name                               | Task                                                                                                            | Model        |  Resolution |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------|--------------|------------:|\n|[pix2pixHD](projects/pix2pixHD/README.md)     | Cityscapes, segmentation to image                                                                               |[download](https://drive.google.com/file/d/1B3bXpQQzidJW0G3oCjYSWYEn2zd8h9dg/view?usp=sharing)  | 1024x512    |\n|[SPADE](projects/spade/README.md)             | COCO-Stuff, segmentation to image                                                                               |[download](https://drive.google.com/file/d/1R27Zk9zlj8HitW_bOsQmbT2LOQNCKJJL/view?usp=sharing)  |  256x256    |\n\n\n### Unsupervised Image-to-Image Translation\n\n\n|Algorithm Name                               | Task                                                                                                            | Model        |  Resolution |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------|--------------|------------:|\n|[UNIT](projects/unit/README.md)               | Winter <-> Summer                                                                                               |[download](https://drive.google.com/file/d/1y1FJT_kRq80Se6ASCU3LFZwrrJKzHr4I/view?usp=sharing)  | 256x256     |\n|[MUNIT](projects/munit/README.md)             | AFHQ Dog <-> Cat                                                                                                |[download](https://drive.google.com/file/d/1XCqHFD1pN7Vlp0RWI0oYKpH4USSKdGqo/view?usp=sharing)  | 256x256     |\n|[FUNIT](projects/funit/README.md)             | AniamlFaces                                                                                                     |[download](https://drive.google.com/file/d/1Tbq0zaaH_Omv_0IPfX8LIvh0sVmusqE-/view?usp=sharing)  | 256x256     |\n|[COCO-FUNIT](projects/coco_funit/README.md)   | AniamlFaces                                                                                                     |[download](https://drive.google.com/file/d/1ODlwSfgauWyOSxj-aPCbFMOGUn2INiQT/view?usp=sharing)  | 256x256     |\n|[COCO-FUNIT](projects/coco_funit/README.md)   | Mammals, Full body animal translation                                                                           |[download](https://drive.google.com/file/d/1Wf0BhcIpVJgHQunipdt8r-KtQ9mRvKxt/view?usp=sharing)  | 256x256     |\n\n\n### Video-to-video Translation\n\n\n|Algorithm Name                               | Task                                                                                                            | Model        |  Resolution |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------|--------------|------------:|\n|[vid2vid](projects/vid2vid/README.md)         | Cityscapes, segmentation to video                                                                               |[download](https://drive.google.com/file/d/1b2M5rU740vBurLQ9iDP2kb4sP5HAb-Jx/view?usp=sharing)  | 1024x512    |\n|[fs-vid2vid](projects/fs_vid2vid/README.md)   | FaceForensics, landmarks to video                                                                               |[download](https://drive.google.com/file/d/1F_22ctFmo553nRHy1d_BX7aorc9zk9cF/view?usp=sharing)  | 512x512     |\n\n\n\n### World-to-world Translation\n\n\n|Algorithm Name                               | Task                                                                                                            | Model        |  Resolution |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------|--------------|------------:|\n|[wc-vid2vid](projects/wc_vid2vid/README.md)   | Cityscapes, segmentation to video                                                                               |[download](https://drive.google.com/file/d/1CvRBok210WWQHF6VuZvU4Vuzdd05ItYB/view?usp=sharing)  | 1024x512    |\n|[GANcraft](projects/gancraft/README.md)   | Semantic block worlds to realistic worlds                                                                               |[download](https://drive.google.com/file/d/1T1GeItHXwa0dpDPMLP6EaYQ-hsawuNXS/view?usp=sharing)  | arbitrary    |\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.994140625,
          "content": "<img src=\"imaginaire_logo.svg\" alt=\"imaginaire_logo.svg\" height=\"360\"/>\n\n# Imaginaire\n### [Docs](http://deepimagination.cc/) | [License](LICENSE.md) | [Installation](INSTALL.md) | [Model Zoo](MODELZOO.md)\n\nImaginaire is a [pytorch](https://pytorch.org/) library that contains\noptimized implementation of several image and video synthesis methods developed at [NVIDIA](https://www.nvidia.com/en-us/).\n\n## License\n\nImaginaire is released under [NVIDIA Software license](LICENSE.md).\nFor commercial use, please consult [NVIDIA Research Inquiries](https://www.nvidia.com/en-us/research/inquiries/).\n\n## What's inside?\n\n[![IMAGE ALT TEXT](http://img.youtube.com/vi/jgTX5OnAsYQ/0.jpg)](http://www.youtube.com/watch?v=jgTX5OnAsYQ \"Imaginaire\")\n\nWe have a tutorial for each model. Click on the model name, and your browser should take you to the tutorial page for the project.\n\n### Supervised Image-to-Image Translation\n\n|Algorithm Name                               | Feature                                                                                                         | Publication                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------:|\n|[pix2pixHD](projects/pix2pixhd/README.md)     | Learn a mapping that converts a semantic image to a high-resolution photorealistic image.                       |    [Wang et. al. CVPR 2018](https://arxiv.org/abs/1711.11585) |\n|[SPADE](projects/spade/README.md)             | Improve pix2pixHD on handling diverse input labels and delivering better output quality.                        |    [Park et. al. CVPR 2019](https://arxiv.org/abs/1903.07291) |\n\n\n### Unsupervised Image-to-Image Translation\n\n\n|Algorithm Name                               | Feature                                                                                                         | Publication                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------:|\n|[UNIT](projects/unit/README.md)               | Learn a one-to-one mapping between two visual domains.                                                          |    [Liu et. al. NeurIPS 2017](https://arxiv.org/abs/1703.00848) |\n|[MUNIT](projects/munit/README.md)             | Learn a many-to-many mapping between two visual domains.                                                        |    [Huang et. al. ECCV 2018](https://arxiv.org/abs/1804.04732) |\n|[FUNIT](projects/funit/README.md)             | Learn a style-guided image translation model that can generate translations in unseen domains.                  |    [Liu et. al. ICCV 2019](https://arxiv.org/abs/1905.01723) |\n|[COCO-FUNIT](projects/coco_funit/README.md)   | Improve FUNIT with a content-conditioned style encoding scheme for style code computation.                      |    [Saito et. al. ECCV 2020](https://arxiv.org/abs/2007.07431) |\n\n\n### Video-to-video Translation\n\n\n|Algorithm Name                               | Feature                                                                                                         | Publication                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------:|\n|[vid2vid](projects/vid2vid/README.md)         | Learn a mapping that converts a semantic video to a photorealistic video.                                       |    [Wang et. al. NeurIPS 2018](https://arxiv.org/abs/1808.06601) |\n|[fs-vid2vid](projects/fs_vid2vid/README.md)   | Learn a subject-agnostic mapping that converts a semantic video and an example image to a photoreslitic video.  |    [Wang et. al. NeurIPS 2019](https://arxiv.org/abs/1808.06601) |\n\n\n### World-to-world Translation\n\n\n|Algorithm Name                               | Feature                                                                                                         | Publication                                                   |\n|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------:|\n|[wc-vid2vid](projects/wc_vid2vid/README.md)   | Improve vid2vid on view consistency and long-term consistency.                                                  |    [Mallya et. al. ECCV 2020](https://arxiv.org/abs/2007.08509) |\n|[GANcraft](projects/gancraft/README.md)   | Convert semantic block worlds to realistic-looking worlds.                                                  |    [Hao et. al. ICCV 2021](https://arxiv.org/abs/2104.07659) |\n\n\n\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 5.2119140625,
          "content": "# Copyright (C) 2021 NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, check out LICENSE.md\nimport argparse\nimport glob\nimport os\n\nimport wandb\n\nimport imaginaire.config\nfrom imaginaire.config import Config\nfrom imaginaire.utils.cudnn import init_cudnn\nfrom imaginaire.utils.dataset import get_train_and_val_dataloader\nfrom imaginaire.utils.distributed import init_dist, is_master\nfrom imaginaire.utils.distributed import master_only_print as print\nfrom imaginaire.utils.gpu_affinity import set_affinity\nfrom imaginaire.utils.logging import init_logging, make_logging_dir\nfrom imaginaire.utils.trainer import (get_model_optimizer_and_scheduler,\n                                      get_trainer, set_random_seed)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Training')\n    parser.add_argument('--config',\n                        help='Path to the training config file.', required=True)\n    parser.add_argument('--logdir', help='Dir for saving evaluation results.')\n    parser.add_argument('--checkpoint', default='', help='Checkpoint path.')\n    parser.add_argument('--checkpoint_logdir', help='Dir for loading models.')\n    parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n    parser.add_argument('--local_rank', type=int, default=os.getenv('LOCAL_RANK', 0))\n    parser.add_argument('--start_iter', type=int, default=0)\n    parser.add_argument('--end_iter', type=int, default=1000000000)\n    parser.add_argument('--single_gpu', action='store_true')\n    parser.add_argument('--debug', action='store_true')\n    parser.add_argument('--use_jit', action='store_true')\n    parser.add_argument('--wandb', action='store_true')\n    parser.add_argument('--wandb_name', default='default', type=str)\n    parser.add_argument('--wandb_id', type=str)\n    parser.add_argument('--num_workers', type=int)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    set_affinity(args.local_rank)\n    set_random_seed(args.seed, by_rank=True)\n    cfg = Config(args.config)\n\n    # If args.single_gpu is set to True,\n    # we will disable distributed data parallel\n    if not args.single_gpu:\n        cfg.local_rank = args.local_rank\n        init_dist(cfg.local_rank)\n\n    # Global arguments.\n    imaginaire.config.DEBUG = args.debug\n    imaginaire.config.USE_JIT = args.use_jit\n\n    # Override the number of data loading workers if necessary\n    if args.num_workers is not None:\n        cfg.data.num_workers = args.num_workers\n\n    # Create log directory for storing training results.\n    cfg.date_uid, cfg.logdir = init_logging(args.config, args.logdir)\n    make_logging_dir(cfg.logdir)\n\n    # Initialize cudnn.\n    init_cudnn(cfg.cudnn.deterministic, cfg.cudnn.benchmark)\n\n    # Initialize data loaders and models.\n    train_data_loader, val_data_loader = get_train_and_val_dataloader(cfg)\n    net_G, net_D, opt_G, opt_D, sch_G, sch_D = \\\n        get_model_optimizer_and_scheduler(cfg, seed=args.seed)\n    trainer = get_trainer(cfg, net_G, net_D,\n                          opt_G, opt_D,\n                          sch_G, sch_D,\n                          train_data_loader, val_data_loader)\n\n    # Initialize Wandb.\n    if is_master():\n        if args.wandb_id is not None:\n            wandb_id = args.wandb_id\n        else:\n            if os.path.exists(os.path.join(cfg.logdir, 'wandb_id.txt')):\n                with open(os.path.join(cfg.logdir, 'wandb_id.txt'), 'r+') as f:\n                    wandb_id = f.read()\n            else:\n                wandb_id = wandb.util.generate_id()\n                with open(os.path.join(cfg.logdir, 'wandb_id.txt'), 'w+') as f:\n                    f.write(wandb_id)\n        wandb_mode = \"disabled\" if (args.debug or not args.wandb) else \"online\"\n        wandb.init(id=wandb_id,\n                   project=args.wandb_name,\n                   config=cfg,\n                   name=os.path.basename(cfg.logdir),\n                   resume=\"allow\",\n                   settings=wandb.Settings(start_method=\"fork\"),\n                   mode=wandb_mode)\n        wandb.config.update({'dataset': cfg.data.name})\n        wandb.watch(trainer.net_G_module)\n        wandb.watch(trainer.net_D.module)\n\n    # Start evaluation.\n    if args.checkpoint is not None:\n        checkpoint = args.checkpoint\n        _, current_epoch, current_iteration = trainer.load_checkpoint(cfg, checkpoint, resume=True)\n        trainer.current_epoch = current_epoch\n        trainer.current_iteration = current_iteration\n        trainer.write_metrics()\n    else:\n        checkpoints = sorted(glob.glob('{}/*.pt'.format(args.checkpoint_logdir)))\n        for checkpoint in checkpoints:\n            # current_iteration = int(os.path.basename(checkpoint).split('_')[3])\n            if args.start_iter <= current_iteration <= args.end_iter:\n                print(f\"Evaluating the model at iteration {current_iteration}.\")\n                _, current_epoch, current_iteration = trainer.load_checkpoint(cfg, checkpoint, resume=True)\n                trainer.current_epoch = current_epoch\n                trainer.current_iteration = current_iteration\n                trainer.write_metrics()\n    print('Done with evaluation!!!')\n    return\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "imaginaire",
          "type": "tree",
          "content": null
        },
        {
          "name": "imaginaire_logo.svg",
          "type": "blob",
          "size": 5.58203125,
          "content": "<svg width=\"384\" height=\"384\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" overflow=\"hidden\"><defs><clipPath id=\"clip0\"><path d=\"M0 0 384 0 384 384 0 384Z\" fill-rule=\"evenodd\" clip-rule=\"evenodd\"/></clipPath></defs><g clip-path=\"url(#clip0)\"><rect x=\"0\" y=\"0\" width=\"384\" height=\"384\" fill=\"#1A1918\"/><path d=\"M0 0 136.703 75.9328\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\" transform=\"matrix(-1 -8.74228e-08 -8.74228e-08 1 297.203 133.5)\"/><path d=\"M0 0 127.88 74.2261\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\" transform=\"matrix(-1 -8.74228e-08 -8.74228e-08 1 266.38 120.5)\"/><path d=\"M0 0 130.622 78.1951\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\" transform=\"matrix(-1 -8.74228e-08 -8.74228e-08 1 260.122 86.5001)\"/><path d=\"M129.5 164.5 130.961 179.015\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M139.5 194.5 160.799 210.616\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M104.5 197.5 106.941 210.111\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M0 0 77.7866 19.5144\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\" transform=\"matrix(1 0 0 -1 84.5001 229.014)\"/><path d=\"M0 0 45.6073 63.6821\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\" transform=\"matrix(1 0 0 -1 84.5001 226.182)\"/><path d=\"M106.5 210.5 116.81 219.613\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M132.5 180.5 138.812 193.815\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M0 0 133.36 75.3849\" stroke=\"#7AB547\" stroke-width=\"3\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\" transform=\"matrix(-1 -8.74228e-08 -8.74228e-08 1 263.86 102.5)\"/><text fill=\"#FFFFFF\" font-family=\"Berlin Sans FB,Berlin Sans FB_MSFontService,sans-serif\" font-weight=\"600\" font-size=\"48\" transform=\"translate(79.301 271)\">I<tspan font-size=\"37\" x=\"14.3333\" y=\"0\">MAGINAIRE</tspan></text><path d=\"M99.5001 198.5C99.5001 195.739 101.515 193.5 104 193.5 106.485 193.5 108.5 195.739 108.5 198.5 108.5 201.261 106.485 203.5 104 203.5 101.515 203.5 99.5001 201.261 99.5001 198.5Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M82.5001 226C82.5001 223.515 84.5148 221.5 87.0001 221.5 89.4853 221.5 91.5001 223.515 91.5001 226 91.5001 228.485 89.4853 230.5 87.0001 230.5 84.5148 230.5 82.5001 228.485 82.5001 226Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M103.5 210.5C103.5 207.739 105.739 205.5 108.5 205.5 111.261 205.5 113.5 207.739 113.5 210.5 113.5 213.261 111.261 215.5 108.5 215.5 105.739 215.5 103.5 213.261 103.5 210.5Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M124.5 164.5C124.5 161.739 126.739 159.5 129.5 159.5 132.261 159.5 134.5 161.739 134.5 164.5 134.5 167.261 132.261 169.5 129.5 169.5 126.739 169.5 124.5 167.261 124.5 164.5Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M126.5 177.5C126.5 174.739 128.739 172.5 131.5 172.5 134.261 172.5 136.5 174.739 136.5 177.5 136.5 180.261 134.261 182.5 131.5 182.5 128.739 182.5 126.5 180.261 126.5 177.5Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M134.5 194C134.5 191.515 136.739 189.5 139.5 189.5 142.261 189.5 144.5 191.515 144.5 194 144.5 196.485 142.261 198.5 139.5 198.5 136.739 198.5 134.5 196.485 134.5 194Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M113.5 220.5C113.5 217.739 115.739 215.5 118.5 215.5 121.261 215.5 123.5 217.739 123.5 220.5 123.5 223.261 121.261 225.5 118.5 225.5 115.739 225.5 113.5 223.261 113.5 220.5Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M156.5 209C156.5 206.515 158.739 204.5 161.5 204.5 164.261 204.5 166.5 206.515 166.5 209 166.5 211.485 164.261 213.5 161.5 213.5 158.739 213.5 156.5 211.485 156.5 209Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M255.5 86.0001C255.5 83.5148 257.739 81.5001 260.5 81.5001 263.261 81.5001 265.5 83.5148 265.5 86.0001 265.5 88.4853 263.261 90.5001 260.5 90.5001 257.739 90.5001 255.5 88.4853 255.5 86.0001Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M257.5 104C257.5 101.515 259.739 99.5001 262.5 99.5001 265.261 99.5001 267.5 101.515 267.5 104 267.5 106.485 265.261 108.5 262.5 108.5 259.739 108.5 257.5 106.485 257.5 104Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M261.5 121.5C261.5 118.739 263.515 116.5 266 116.5 268.485 116.5 270.5 118.739 270.5 121.5 270.5 124.261 268.485 126.5 266 126.5 263.515 126.5 261.5 124.261 261.5 121.5Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/><path d=\"M294.5 133.5C294.5 130.739 296.515 128.5 299 128.5 301.485 128.5 303.5 130.739 303.5 133.5 303.5 136.261 301.485 138.5 299 138.5 296.515 138.5 294.5 136.261 294.5 133.5Z\" stroke=\"#FFFFFF\" stroke-width=\"0.666667\" stroke-miterlimit=\"8\" fill=\"#7AB547\" fill-rule=\"evenodd\"/></g></svg>"
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 3.50390625,
          "content": "# Copyright (C) 2021 NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, check out LICENSE.md\nimport argparse\n\nfrom imaginaire.config import Config\nfrom imaginaire.utils.cudnn import init_cudnn\nfrom imaginaire.utils.dataset import get_test_dataloader\nfrom imaginaire.utils.distributed import init_dist\nfrom imaginaire.utils.gpu_affinity import set_affinity\nfrom imaginaire.utils.io import get_checkpoint as get_checkpoint\nfrom imaginaire.utils.logging import init_logging\nfrom imaginaire.utils.trainer import \\\n    (get_model_optimizer_and_scheduler, get_trainer, set_random_seed)\nimport imaginaire.config\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Training')\n    parser.add_argument('--config', required=True,\n                        help='Path to the training config file.')\n    parser.add_argument('--checkpoint', default='',\n                        help='Checkpoint path.')\n    parser.add_argument('--output_dir', required=True,\n                        help='Location to save the image outputs')\n    parser.add_argument('--logdir',\n                        help='Dir for saving logs and models.')\n    parser.add_argument('--seed', type=int, default=0,\n                        help='Random seed.')\n    parser.add_argument('--local_rank', type=int, default=0)\n    parser.add_argument('--single_gpu', action='store_true')\n    parser.add_argument('--num_workers', type=int)\n    parser.add_argument('--debug', action='store_true')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    set_affinity(args.local_rank)\n    set_random_seed(args.seed, by_rank=True)\n    cfg = Config(args.config)\n    imaginaire.config.DEBUG = args.debug\n\n    if not hasattr(cfg, 'inference_args'):\n        cfg.inference_args = None\n\n    # If args.single_gpu is set to True,\n    # we will disable distributed data parallel.\n    if not args.single_gpu:\n        cfg.local_rank = args.local_rank\n        init_dist(cfg.local_rank)\n\n    # Override the number of data loading workers if necessary\n    if args.num_workers is not None:\n        cfg.data.num_workers = args.num_workers\n\n    # Create log directory for storing training results.\n    cfg.date_uid, cfg.logdir = init_logging(args.config, args.logdir)\n\n    # Initialize cudnn.\n    init_cudnn(cfg.cudnn.deterministic, cfg.cudnn.benchmark)\n\n    # Initialize data loaders and models.\n    test_data_loader = get_test_dataloader(cfg)\n    net_G, net_D, opt_G, opt_D, sch_G, sch_D = \\\n        get_model_optimizer_and_scheduler(cfg, seed=args.seed)\n    trainer = get_trainer(cfg, net_G, net_D,\n                          opt_G, opt_D,\n                          sch_G, sch_D,\n                          None, test_data_loader)\n\n    if args.checkpoint == '':\n        # Download pretrained weights.\n        pretrained_weight_url = cfg.pretrained_weight\n        if pretrained_weight_url == '':\n            print('link to the pretrained weight is not specified.')\n            raise\n        default_checkpoint_path = args.config.split('.yaml')[0] + '-' + cfg.pretrained_weight + '.pt'\n        args.checkpoint = get_checkpoint(default_checkpoint_path, pretrained_weight_url)\n        print('Checkpoint downloaded to', args.checkpoint)\n\n    # Load checkpoint.\n    trainer.load_checkpoint(cfg, args.checkpoint)\n\n    # Do inference.\n    trainer.current_epoch = -1\n    trainer.current_iteration = -1\n    trainer.test(test_data_loader, args.output_dir, cfg.inference_args)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "projects",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 6.8447265625,
          "content": "# Copyright (C) 2021 NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# This work is made available under the Nvidia Source Code License-NC.\n# To view a copy of this license, check out LICENSE.md\nimport argparse\nimport os\nimport sys\nimport random\n\nimport torch.autograd.profiler as profiler\nimport wandb\n\nimport imaginaire.config\nfrom imaginaire.config import Config\nfrom imaginaire.utils.cudnn import init_cudnn\nfrom imaginaire.utils.dataset import get_train_and_val_dataloader\nfrom imaginaire.utils.distributed import init_dist, is_master, get_world_size\nfrom imaginaire.utils.distributed import master_only_print as print\nfrom imaginaire.utils.gpu_affinity import set_affinity\nfrom imaginaire.utils.misc import slice_tensor\nfrom imaginaire.utils.logging import init_logging, make_logging_dir\nfrom imaginaire.utils.trainer import (get_model_optimizer_and_scheduler,\n                                      get_trainer, set_random_seed)\n\nsys.path.append(os.environ.get('SUBMIT_SCRIPTS', '.'))\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Training')\n    parser.add_argument('--config',\n                        help='Path to the training config file.', required=True)\n    parser.add_argument('--logdir', help='Dir for saving logs and models.')\n    parser.add_argument('--checkpoint', default='', help='Checkpoint path.')\n    parser.add_argument('--seed', type=int, default=2, help='Random seed.')\n    parser.add_argument('--randomized_seed', action='store_true', help='Use a random seed between 0-10000.')\n    parser.add_argument('--local_rank', type=int, default=os.getenv('LOCAL_RANK', 0))\n    parser.add_argument('--single_gpu', action='store_true')\n    parser.add_argument('--debug', action='store_true')\n    parser.add_argument('--use_jit', action='store_true')\n    parser.add_argument('--profile', action='store_true')\n    parser.add_argument('--wandb', action='store_true')\n    parser.add_argument('--wandb_name', default='default', type=str)\n    parser.add_argument('--wandb_id', type=str)\n    parser.add_argument('--resume', type=int)\n    parser.add_argument('--num_workers', type=int)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    set_affinity(args.local_rank)\n    if args.randomized_seed:\n        args.seed = random.randint(0, 10000)\n    set_random_seed(args.seed, by_rank=True)\n    cfg = Config(args.config)\n    try:\n        from userlib.auto_resume import AutoResume\n        AutoResume.init()\n    except:  # noqa\n        pass\n\n    # If args.single_gpu is set to True,\n    # we will disable distributed data parallel\n    if not args.single_gpu:\n        cfg.local_rank = args.local_rank\n        init_dist(cfg.local_rank)\n    print(f\"Training with {get_world_size()} GPUs.\")\n\n    # Global arguments.\n    imaginaire.config.DEBUG = args.debug\n    imaginaire.config.USE_JIT = args.use_jit\n\n    # Override the number of data loading workers if necessary\n    if args.num_workers is not None:\n        cfg.data.num_workers = args.num_workers\n\n    # Create log directory for storing training results.\n    cfg.date_uid, cfg.logdir = init_logging(args.config, args.logdir)\n    make_logging_dir(cfg.logdir)\n\n    # Initialize cudnn.\n    init_cudnn(cfg.cudnn.deterministic, cfg.cudnn.benchmark)\n\n    # Initialize data loaders and models.\n    batch_size = cfg.data.train.batch_size\n    total_step = max(cfg.trainer.dis_step, cfg.trainer.gen_step)\n    cfg.data.train.batch_size *= total_step\n    train_data_loader, val_data_loader = get_train_and_val_dataloader(cfg, args.seed)\n    net_G, net_D, opt_G, opt_D, sch_G, sch_D = \\\n        get_model_optimizer_and_scheduler(cfg, seed=args.seed)\n    trainer = get_trainer(cfg, net_G, net_D,\n                          opt_G, opt_D,\n                          sch_G, sch_D,\n                          train_data_loader, val_data_loader)\n    resumed, current_epoch, current_iteration = trainer.load_checkpoint(cfg, args.checkpoint, args.resume)\n\n    # Initialize Wandb.\n    if is_master():\n        if args.wandb_id is not None:\n            wandb_id = args.wandb_id\n        else:\n            if resumed and os.path.exists(os.path.join(cfg.logdir, 'wandb_id.txt')):\n                with open(os.path.join(cfg.logdir, 'wandb_id.txt'), 'r+') as f:\n                    wandb_id = f.read()\n            else:\n                wandb_id = wandb.util.generate_id()\n                with open(os.path.join(cfg.logdir, 'wandb_id.txt'), 'w+') as f:\n                    f.write(wandb_id)\n        wandb_mode = \"disabled\" if (args.debug or not args.wandb) else \"online\"\n        wandb.init(id=wandb_id,\n                   project=args.wandb_name,\n                   config=cfg,\n                   name=os.path.basename(cfg.logdir),\n                   resume=\"allow\",\n                   settings=wandb.Settings(start_method=\"fork\"),\n                   mode=wandb_mode)\n        wandb.config.update({'dataset': cfg.data.name})\n        wandb.watch(trainer.net_G_module)\n        wandb.watch(trainer.net_D.module)\n\n    # Start training.\n    for epoch in range(current_epoch, cfg.max_epoch):\n        print('Epoch {} ...'.format(epoch))\n        if not args.single_gpu:\n            train_data_loader.sampler.set_epoch(current_epoch)\n        trainer.start_of_epoch(current_epoch)\n        for it, data in enumerate(train_data_loader):\n            with profiler.profile(enabled=args.profile,\n                                  use_cuda=True,\n                                  profile_memory=True,\n                                  record_shapes=True) as prof:\n                data = trainer.start_of_iteration(data, current_iteration)\n\n                for i in range(cfg.trainer.dis_step):\n                    trainer.dis_update(\n                        slice_tensor(data, i * batch_size,\n                                     (i + 1) * batch_size))\n                for i in range(cfg.trainer.gen_step):\n                    trainer.gen_update(\n                        slice_tensor(data, i * batch_size,\n                                     (i + 1) * batch_size))\n\n                current_iteration += 1\n                trainer.end_of_iteration(data, current_epoch, current_iteration)\n                if current_iteration >= cfg.max_iter:\n                    print('Done with training!!!')\n                    return\n            if args.profile:\n                print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n                prof.export_chrome_trace(os.path.join(cfg.logdir, \"trace.json\"))\n            try:\n                if AutoResume.termination_requested():\n                    trainer.save_checkpoint(current_epoch, current_iteration)\n                    AutoResume.request_resume()\n                    print(\"Training terminated. Returning\")\n                    return 0\n            except:  # noqa\n                pass\n\n        current_epoch += 1\n        trainer.end_of_epoch(data, current_epoch, current_iteration)\n    print('Done with training!!!')\n    return\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}