{
  "metadata": {
    "timestamp": 1736559770842,
    "page": 486,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hunkim/PyTorchZeroToAll",
      "stars": 3909,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.12109375,
          "content": ".idea\n*.key\n.nsml*\n*.pt\nX*\nmnist_data\nppts\n.ipynb_checkpoints\nclient_secret.json\n__pycache__/\n.py*\ntmp\ntemplate.pdf\n*.ipynb\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.3291015625,
          "content": "# code below is taken from https://github.com/fchollet/keras/blob/master/.travis.yml\nsudo: required\ndist: trusty\nlanguage: python\npython: # Only two versions for now\n  - \"2.7\"\n  - \"3.6\"\n# command to install dependencies\ninstall: \"pip install -r requirements.txt\"\n\nscript:\n  - python -m compileall .\n  - ls ??_*.py|xargs -n 1 -P 3 python\n"
        },
        {
          "name": "01_basics.py",
          "type": "blob",
          "size": 1.0263671875,
          "content": "import numpy as np\nimport matplotlib.pyplot as plt\n\nx_data = [1.0, 2.0, 3.0]\ny_data = [2.0, 4.0, 6.0]\n\n\n# our model for the forward pass\ndef forward(x):\n    return x * w\n\n\n# Loss function\ndef loss(x, y):\n    y_pred = forward(x)\n    return (y_pred - y) * (y_pred - y)\n\n# List of weights/Mean square Error (Mse) for each input\nw_list = []\nmse_list = []\n\nfor w in np.arange(0.0, 4.1, 0.1):\n    # Print the weights and initialize the lost\n    print(\"w=\", w)\n    l_sum = 0\n\n    for x_val, y_val in zip(x_data, y_data):\n        # For each input and output, calculate y_hat\n        # Compute the total loss and add to the total error\n        y_pred_val = forward(x_val)\n        l = loss(x_val, y_val)\n        l_sum += l\n        print(\"\\t\", x_val, y_val, y_pred_val, l)\n    # Now compute the Mean squared error (mse) of each\n    # Aggregate the weight/mse from this run\n    print(\"MSE=\", l_sum / len(x_data))\n    w_list.append(w)\n    mse_list.append(l_sum / len(x_data))\n\n# Plot it all\nplt.plot(w_list, mse_list)\nplt.ylabel('Loss')\nplt.xlabel('w')\nplt.show()\n"
        },
        {
          "name": "02_manual_gradient.py",
          "type": "blob",
          "size": 0.9423828125,
          "content": "# Training Data\nx_data = [1.0, 2.0, 3.0]\ny_data = [2.0, 4.0, 6.0]\n\nw = 1.0  # a random guess: random value\n\n\n# our model forward pass\ndef forward(x):\n    return x * w\n\n\n# Loss function\ndef loss(x, y):\n    y_pred = forward(x)\n    return (y_pred - y) * (y_pred - y)\n\n\n# compute gradient\ndef gradient(x, y):  # d_loss/d_w\n    return 2 * x * (x * w - y)\n\n\n# Before training\nprint(\"Prediction (before training)\",  4, forward(4))\n\n# Training loop\nfor epoch in range(10):\n    for x_val, y_val in zip(x_data, y_data):\n        # Compute derivative w.r.t to the learned weights\n        # Update the weights\n        # Compute the loss and print progress\n        grad = gradient(x_val, y_val)\n        w = w - 0.01 * grad\n        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n        l = loss(x_val, y_val)\n    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n\n# After training\nprint(\"Predicted score (after training)\",  \"4 hours of studying: \", forward(4))\n"
        },
        {
          "name": "03_auto_gradient.py",
          "type": "blob",
          "size": 0.8779296875,
          "content": "import torch\nimport pdb\n\nx_data = [1.0, 2.0, 3.0]\ny_data = [2.0, 4.0, 6.0]\nw = torch.tensor([1.0], requires_grad=True)\n\n# our model forward pass\ndef forward(x):\n    return x * w\n\n# Loss function\ndef loss(y_pred, y_val):\n    return (y_pred - y_val) ** 2\n\n# Before training\nprint(\"Prediction (before training)\",  4, forward(4).item())\n\n# Training loop\nfor epoch in range(10):\n    for x_val, y_val in zip(x_data, y_data):\n        y_pred = forward(x_val) # 1) Forward pass\n        l = loss(y_pred, y_val) # 2) Compute loss\n        l.backward() # 3) Back propagation to update weights\n        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n        w.data = w.data - 0.01 * w.grad.item()\n\n        # Manually zero the gradients after updating weights\n        w.grad.data.zero_()\n\n    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n\n# After training\nprint(\"Prediction (after training)\",  4, forward(4).item())\n"
        },
        {
          "name": "05_linear_regression.py",
          "type": "blob",
          "size": 1.5517578125,
          "content": "from torch import nn\nimport torch\nfrom torch import tensor\n\nx_data = tensor([[1.0], [2.0], [3.0]])\ny_data = tensor([[2.0], [4.0], [6.0]])\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate two nn.Linear module\n        \"\"\"\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Variable of input data and we must return\n        a Variable of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Variables.\n        \"\"\"\n        y_pred = self.linear(x)\n        return y_pred\n\n\n# our model\nmodel = Model()\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters of the two\n# nn.Linear modules which are members of the model.\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nfor epoch in range(500):\n    # 1) Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x_data)\n\n    # 2) Compute and print loss\n    loss = criterion(y_pred, y_data)\n    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n\n# After training\nhour_var = tensor([[4.0]])\ny_pred = model(hour_var)\nprint(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())\n"
        },
        {
          "name": "06_logistic_regression.py",
          "type": "blob",
          "size": 1.783203125,
          "content": "from torch import tensor\nfrom torch import nn\nfrom torch import sigmoid\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Training data and ground truth\nx_data = tensor([[1.0], [2.0], [3.0], [4.0]])\ny_data = tensor([[0.], [0.], [1.], [1.]])\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate nn.Linear module\n        \"\"\"\n        super(Model, self).__init__()\n        self.linear = nn.Linear(1, 1)  # One in and one out\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Variable of input data and we must return\n        a Variable of output data.\n        \"\"\"\n        y_pred = sigmoid(self.linear(x))\n        return y_pred\n\n\n# our model\nmodel = Model()\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters of the two\n# nn.Linear modules which are members of the model.\ncriterion = nn.BCELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nfor epoch in range(1000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x_data)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y_data)\n    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n# After training\nprint(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\nhour_var = model(tensor([[1.0]]))\nprint(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\nhour_var = model(tensor([[7.0]]))\nprint(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')\n"
        },
        {
          "name": "07_diabets_logistic.py",
          "type": "blob",
          "size": 1.693359375,
          "content": "from torch import nn, optim, from_numpy\nimport numpy as np\n\nxy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\nx_data = from_numpy(xy[:, 0:-1])\ny_data = from_numpy(xy[:, [-1]])\nprint(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate two nn.Linear module\n        \"\"\"\n        super(Model, self).__init__()\n        self.l1 = nn.Linear(8, 6)\n        self.l2 = nn.Linear(6, 4)\n        self.l3 = nn.Linear(4, 1)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Variable of input data and we must return\n        a Variable of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Variables.\n        \"\"\"\n        out1 = self.sigmoid(self.l1(x))\n        out2 = self.sigmoid(self.l2(out1))\n        y_pred = self.sigmoid(self.l3(out2))\n        return y_pred\n\n\n# our model\nmodel = Model()\n\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters of the two\n# nn.Linear modules which are members of the model.\ncriterion = nn.BCELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# Training loop\nfor epoch in range(100):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x_data)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y_data)\n    print(f'Epoch: {epoch + 1}/100 | Loss: {loss.item():.4f}')\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n"
        },
        {
          "name": "08_1_dataset_loader.py",
          "type": "blob",
          "size": 1.287109375,
          "content": "# References\n# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import from_numpy, tensor\nimport numpy as np\n\nclass DiabetesDataset(Dataset):\n    \"\"\" Diabetes dataset.\"\"\"\n\n    # Initialize your data, download, etc.\n    def __init__(self):\n        xy = np.loadtxt('./data/diabetes.csv.gz',\n                        delimiter=',', dtype=np.float32)\n        self.len = xy.shape[0]\n        self.x_data = from_numpy(xy[:, 0:-1])\n        self.y_data = from_numpy(xy[:, [-1]])\n\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    def __len__(self):\n        return self.len\n\n\ndataset = DiabetesDataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=32,\n                          shuffle=True,\n                          num_workers=2)\n\nfor epoch in range(2):\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # wrap them in Variable\n        inputs, labels = tensor(inputs), tensor(labels)\n\n        # Run your training process\n        print(f'Epoch: {i} | Inputs {inputs.data} | Labels {labels.data}')\n"
        },
        {
          "name": "08_2_dataset_loade_logistic.py",
          "type": "blob",
          "size": 2.5458984375,
          "content": "# References\n# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, from_numpy, optim\nimport numpy as np\n\n\nclass DiabetesDataset(Dataset):\n    \"\"\" Diabetes dataset.\"\"\"\n    # Initialize your data, download, etc.\n    def __init__(self):\n        xy = np.loadtxt('./data/diabetes.csv.gz',\n                        delimiter=',', dtype=np.float32)\n        self.len = xy.shape[0]\n        self.x_data = from_numpy(xy[:, 0:-1])\n        self.y_data = from_numpy(xy[:, [-1]])\n\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    def __len__(self):\n        return self.len\n\n\ndataset = DiabetesDataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=32,\n                          shuffle=True,\n                          num_workers=2)\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate two nn.Linear module\n        \"\"\"\n        super(Model, self).__init__()\n        self.l1 = nn.Linear(8, 6)\n        self.l2 = nn.Linear(6, 4)\n        self.l3 = nn.Linear(4, 1)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Variable of input data and we must return\n        a Variable of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Variables.\n        \"\"\"\n        out1 = self.sigmoid(self.l1(x))\n        out2 = self.sigmoid(self.l2(out1))\n        y_pred = self.sigmoid(self.l3(out2))\n        return y_pred\n\n\n# our model\nmodel = Model()\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters of the two\n# nn.Linear modules which are members of the model.\ncriterion = nn.BCELoss(reduction='sum')\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# Training loop\nfor epoch in range(2):\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # Forward pass: Compute predicted y by passing x to the model\n        y_pred = model(inputs)\n\n        # Compute and print loss\n        loss = criterion(y_pred, labels)\n        print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n\n        # Zero gradients, perform a backward pass, and update the weights.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n"
        },
        {
          "name": "09_01_softmax_loss.py",
          "type": "blob",
          "size": 1.47265625,
          "content": "from torch import nn, tensor, max\nimport numpy as np\n\n# Cross entropy example\n# One hot\n# 0: 1 0 0\n# 1: 0 1 0\n# 2: 0 0 1\nY = np.array([1, 0, 0])\nY_pred1 = np.array([0.7, 0.2, 0.1])\nY_pred2 = np.array([0.1, 0.3, 0.6])\nprint(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\nprint(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')\n\n# Softmax + CrossEntropy (logSoftmax + NLLLoss)\nloss = nn.CrossEntropyLoss()\n\n# target is of size nBatch\n# each element in target has to have 0 <= value < nClasses (0-2)\n# Input is class, not one-hot\nY = tensor([0], requires_grad=False)\n\n# input is of size nBatch x nClasses = 1 x 4\n# Y_pred are logits (not softmax)\nY_pred1 = tensor([[2.0, 1.0, 0.1]])\nY_pred2 = tensor([[0.5, 2.0, 0.3]])\n\nl1 = loss(Y_pred1, Y)\nl2 = loss(Y_pred2, Y)\n\nprint(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\nprint(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}')\nprint(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')\n\n# target is of size nBatch\n# each element in target has to have 0 <= value < nClasses (0-2)\n# Input is class, not one-hot\nY = tensor([2, 0, 1], requires_grad=False)\n\n# input is of size nBatch x nClasses = 2 x 4\n# Y_pred are logits (not softmax)\nY_pred1 = tensor([[0.1, 0.2, 0.9],\n                  [1.1, 0.1, 0.2],\n                  [0.2, 2.1, 0.1]])\n\nY_pred2 = tensor([[0.8, 0.2, 0.3],\n                  [0.2, 0.3, 0.5],\n                  [0.2, 0.2, 0.5]])\n\nl1 = loss(Y_pred1, Y)\nl2 = loss(Y_pred2, Y)\nprint(f'Batch Loss1:  {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}')\n"
        },
        {
          "name": "09_2_softmax_mnist.py",
          "type": "blob",
          "size": 3.451171875,
          "content": "# https://github.com/pytorch/examples/blob/master/mnist/main.py\nfrom __future__ import print_function\nfrom torch import nn, optim, cuda\nfrom torch.utils import data\nfrom torchvision import datasets, transforms\nimport torch.nn.functional as F\nimport time\n\n# Training settings\nbatch_size = 64\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/',\n                               train=True,\n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='./mnist_data/',\n                              train=False,\n                              transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.l1 = nn.Linear(784, 520)\n        self.l2 = nn.Linear(520, 320)\n        self.l3 = nn.Linear(320, 240)\n        self.l4 = nn.Linear(240, 120)\n        self.l5 = nn.Linear(120, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n        x = F.relu(self.l1(x))\n        x = F.relu(self.l2(x))\n        x = F.relu(self.l3(x))\n        x = F.relu(self.l4(x))\n        return self.l5(x)\n\n\nmodel = Net()\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        data, target = data.to(device), target.to(device)\n        output = model(data)\n        # sum up batch loss\n        test_loss += criterion(output, target).item()\n        # get the index of the max\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n\n\nif __name__ == '__main__':\n    since = time.time()\n    for epoch in range(1, 10):\n        epoch_start = time.time()\n        train(epoch)\n        m, s = divmod(time.time() - epoch_start, 60)\n        print(f'Training time: {m:.0f}m {s:.0f}s')\n        test()\n        m, s = divmod(time.time() - epoch_start, 60)\n        print(f'Testing time: {m:.0f}m {s:.0f}s')\n\n    m, s = divmod(time.time() - since, 60)\n    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n\n"
        },
        {
          "name": "10_1_cnn_mnist.py",
          "type": "blob",
          "size": 2.947265625,
          "content": "# https://github.com/pytorch/examples/blob/master/mnist/main.py\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# Training settings\nbatch_size = 64\n\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./data/',\n                               train=True,\n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='./data/',\n                              train=False,\n                              transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.mp = nn.MaxPool2d(2)\n        self.fc = nn.Linear(320, 10)\n\n    def forward(self, x):\n        in_size = x.size(0)\n        x = F.relu(self.mp(self.conv1(x)))\n        x = F.relu(self.mp(self.conv2(x)))\n        x = x.view(in_size, -1)  # flatten the tensor\n        x = self.fc(x)\n        return F.log_softmax(x)\n\n\nmodel = Net()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        # sum up batch loss\n        test_loss += F.nll_loss(output, target, size_average=False).data\n        # get the index of the max log-probability\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nfor epoch in range(1, 10):\n    train(epoch)\n    test()\n"
        },
        {
          "name": "11_1_toy_inception_mnist.py",
          "type": "blob",
          "size": 4.23828125,
          "content": "# https://github.com/pytorch/examples/blob/master/mnist/main.py\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# Training settings\nbatch_size = 64\n\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./data/',\n                               train=True,\n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='./data/',\n                              train=False,\n                              transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionA, self).__init__()\n        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n\n        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n\n        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n\n        self.incept1 = InceptionA(in_channels=10)\n        self.incept2 = InceptionA(in_channels=20)\n\n        self.mp = nn.MaxPool2d(2)\n        self.fc = nn.Linear(1408, 10)\n\n    def forward(self, x):\n        in_size = x.size(0)\n        x = F.relu(self.mp(self.conv1(x)))\n        x = self.incept1(x)\n        x = F.relu(self.mp(self.conv2(x)))\n        x = self.incept2(x)\n        x = x.view(in_size, -1)  # flatten the tensor\n        x = self.fc(x)\n        return F.log_softmax(x)\n\n\nmodel = Net()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        # sum up batch loss\n        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n        # get the index of the max log-probability\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nfor epoch in range(1, 10):\n    train(epoch)\n    test()\n"
        },
        {
          "name": "12_1_rnn_basics.py",
          "type": "blob",
          "size": 2.0107421875,
          "content": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# One hot encoding for each char in 'hello'\nh = [1, 0, 0, 0]\ne = [0, 1, 0, 0]\nl = [0, 0, 1, 0]\no = [0, 0, 0, 1]\n\n# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\ncell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n\n# (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\nhidden = Variable(torch.randn(1, 1, 2))\n\n# Propagate input through RNN\n# Input: (batch, seq_len, input_size) when batch_first=True\ninputs = Variable(torch.Tensor([h, e, l, l, o]))\nfor one in inputs:\n    one = one.view(1, 1, -1)\n    # Input: (batch, seq_len, input_size) when batch_first=True\n    out, hidden = cell(one, hidden)\n    print(\"one input size\", one.size(), \"out size\", out.size())\n\n# We can do the whole at once\n# Propagate input through RNN\n# Input: (batch, seq_len, input_size) when batch_first=True\ninputs = inputs.view(1, 5, -1)\nout, hidden = cell(inputs, hidden)\nprint(\"sequence input size\", inputs.size(), \"out size\", out.size())\n\n\n# hidden : (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\nhidden = Variable(torch.randn(1, 3, 2))\n\n# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n# 3 batches 'hello', 'eolll', 'lleel'\n# rank = (3, 5, 4)\ninputs = Variable(torch.Tensor([[h, e, l, l, o],\n                                [e, o, l, l, l],\n                                [l, l, e, e, l]]))\n\n# Propagate input through RNN\n# Input: (batch, seq_len, input_size) when batch_first=True\n# B x S x I\nout, hidden = cell(inputs, hidden)\nprint(\"batch input size\", inputs.size(), \"out size\", out.size())\n\n\n# One cell RNN input_dim (4) -> output_dim (2)\ncell = nn.RNN(input_size=4, hidden_size=2)\n\n# The given dimensions dim0 and dim1 are swapped.\ninputs = inputs.transpose(dim0=0, dim1=1)\n# Propagate input through RNN\n# Input: (seq_len, batch_size, input_size) when batch_first=False (default)\n# S x B x I\nout, hidden = cell(inputs, hidden)\nprint(\"batch input size\", inputs.size(), \"out size\", out.size())\n"
        },
        {
          "name": "12_2_hello_rnn.py",
          "type": "blob",
          "size": 2.466796875,
          "content": "# Lab 12 RNN\nimport sys\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\ntorch.manual_seed(777)  # reproducibility\n#            0    1    2    3    4\nidx2char = ['h', 'i', 'e', 'l', 'o']\n\n# Teach hihell -> ihello\nx_data = [0, 1, 0, 2, 3, 3]   # hihell\none_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n                  [0, 1, 0, 0, 0],  # 1\n                  [0, 0, 1, 0, 0],  # 2\n                  [0, 0, 0, 1, 0],  # 3\n                  [0, 0, 0, 0, 1]]  # 4\n\ny_data = [1, 0, 2, 3, 3, 4]    # ihello\nx_one_hot = [one_hot_lookup[x] for x in x_data]\n\n# As we have one batch of samples, we will change them to variables only once\ninputs = Variable(torch.Tensor(x_one_hot))\nlabels = Variable(torch.LongTensor(y_data))\n\nnum_classes = 5\ninput_size = 5  # one-hot size\nhidden_size = 5  # output from the RNN. 5 to directly predict one-hot\nbatch_size = 1   # one sentence\nsequence_length = 1  # One by one\nnum_layers = 1  # one-layer rnn\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n        self.rnn = nn.RNN(input_size=input_size,\n                          hidden_size=hidden_size, batch_first=True)\n\n    def forward(self, hidden, x):\n        # Reshape input (batch first)\n        x = x.view(batch_size, sequence_length, input_size)\n\n        # Propagate input through RNN\n        # Input: (batch, seq_len, input_size)\n        # hidden: (num_layers * num_directions, batch, hidden_size)\n        out, hidden = self.rnn(x, hidden)\n        return hidden, out.view(-1, num_classes)\n\n    def init_hidden(self):\n        # Initialize hidden and cell states\n        # (num_layers * num_directions, batch, hidden_size)\n        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n\n\n# Instantiate RNN model\nmodel = Model()\nprint(model)\n\n# Set loss and optimizer function\n# CrossEntropyLoss = LogSoftmax + NLLLoss\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n# Train the model\nfor epoch in range(100):\n    optimizer.zero_grad()\n    loss = 0\n    hidden = model.init_hidden()\n\n    sys.stdout.write(\"predicted string: \")\n    for input, label in zip(inputs, labels):\n        # print(input.size(), label.size())\n        hidden, output = model(hidden, input)\n        val, idx = output.max(1)\n        sys.stdout.write(idx2char[idx.data[0]])\n        loss += criterion(output, torch.LongTensor([label]))\n\n    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss))\n\n    loss.backward()\n    optimizer.step()\n\nprint(\"Learning finished!\")\n"
        },
        {
          "name": "12_3_hello_rnn_seq.py",
          "type": "blob",
          "size": 2.5048828125,
          "content": "# Lab 12 RNN\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\ntorch.manual_seed(777)  # reproducibility\n\n\nidx2char = ['h', 'i', 'e', 'l', 'o']\n\n# Teach hihell -> ihello\nx_data = [[0, 1, 0, 2, 3, 3]]   # hihell\nx_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n              [0, 1, 0, 0, 0],   # i 1\n              [1, 0, 0, 0, 0],   # h 0\n              [0, 0, 1, 0, 0],   # e 2\n              [0, 0, 0, 1, 0],   # l 3\n              [0, 0, 0, 1, 0]]]  # l 3\n\ny_data = [1, 0, 2, 3, 3, 4]    # ihello\n\n# As we have one batch of samples, we will change them to variables only once\ninputs = Variable(torch.Tensor(x_one_hot))\nlabels = Variable(torch.LongTensor(y_data))\n\nnum_classes = 5\ninput_size = 5  # one-hot size\nhidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\nbatch_size = 1   # one sentence\nsequence_length = 6  # |ihello| == 6\nnum_layers = 1  # one-layer rnn\n\n\nclass RNN(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(RNN, self).__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.sequence_length = sequence_length\n\n        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n\n    def forward(self, x):\n        # Initialize hidden and cell states\n        # (num_layers * num_directions, batch, hidden_size) for batch_first=True\n        h_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size))\n\n        # Reshape input\n        x.view(x.size(0), self.sequence_length, self.input_size)\n\n        # Propagate input through RNN\n        # Input: (batch, seq_len, input_size)\n        # h_0: (num_layers * num_directions, batch, hidden_size)\n\n        out, _ = self.rnn(x, h_0)\n        return out.view(-1, num_classes)\n\n\n# Instantiate RNN model\nrnn = RNN(num_classes, input_size, hidden_size, num_layers)\nprint(rnn)\n\n# Set loss and optimizer function\n# CrossEntropyLoss = LogSoftmax + NLLLoss\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n\n# Train the model\nfor epoch in range(100):\n    outputs = rnn(inputs)\n    optimizer.zero_grad()\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    _, idx = outputs.max(1)\n    idx = idx.data.numpy()\n    result_str = [idx2char[c] for c in idx.squeeze()]\n    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n    print(\"Predicted string: \", ''.join(result_str))\n\nprint(\"Learning finished!\")\n"
        },
        {
          "name": "12_4_hello_rnn_emb.py",
          "type": "blob",
          "size": 2.291015625,
          "content": "# Lab 12 RNN\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\ntorch.manual_seed(777)  # reproducibility\n\n\nidx2char = ['h', 'i', 'e', 'l', 'o']\n\n# Teach hihell -> ihello\nx_data = [[0, 1, 0, 2, 3, 3]]   # hihell\ny_data = [1, 0, 2, 3, 3, 4]    # ihello\n\n# As we have one batch of samples, we will change them to variables only once\ninputs = Variable(torch.LongTensor(x_data))\nlabels = Variable(torch.LongTensor(y_data))\n\nnum_classes = 5\ninput_size = 5\nembedding_size = 10  # embedding size\nhidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\nbatch_size = 1   # one sentence\nsequence_length = 6  # |ihello| == 6\nnum_layers = 1  # one-layer rnn\n\n\nclass Model(nn.Module):\n\n    def __init__(self, num_layers, hidden_size):\n        super(Model, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.RNN(input_size=embedding_size,\n                          hidden_size=5, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        # Initialize hidden and cell states\n        # (num_layers * num_directions, batch, hidden_size)\n        h_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size))\n\n        emb = self.embedding(x)\n        emb = emb.view(batch_size, sequence_length, -1)\n\n        # Propagate embedding through RNN\n        # Input: (batch, seq_len, embedding_size)\n        # h_0: (num_layers * num_directions, batch, hidden_size)\n        out, _ = self.rnn(emb, h_0)\n        return self.fc(out.view(-1, num_classes))\n\n\n# Instantiate RNN model\nmodel = Model(num_layers, hidden_size)\nprint(model)\n\n# Set loss and optimizer function\n# CrossEntropyLoss = LogSoftmax + NLLLoss\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n# Train the model\nfor epoch in range(100):\n    outputs = model(inputs)\n    optimizer.zero_grad()\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    _, idx = outputs.max(1)\n    idx = idx.data.numpy()\n    result_str = [idx2char[c] for c in idx.squeeze()]\n    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n    print(\"Predicted string: \", ''.join(result_str))\n\nprint(\"Learning finished!\")\n"
        },
        {
          "name": "13_1_rnn_classification_basics.py",
          "type": "blob",
          "size": 2.931640625,
          "content": "# Original code is from https://github.com/spro/practical-pytorch\nimport time\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom name_dataset import NameDataset\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# Parameters and DataLoaders\nHIDDEN_SIZE = 100\nN_CHARS = 128  # ASCII\nN_CLASSES = 18\n\n\nclass RNNClassifier(nn.Module):\n\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input):\n        # Note: we run this all at once (over the whole input sequence)\n\n        # input = B x S . size(0) = B\n        batch_size = input.size(0)\n\n        # input:  B x S  -- (transpose) --> S x B\n        input = input.t()\n\n        # Embedding S x B -> S x B x I (embedding size)\n        print(\"  input\", input.size())\n        embedded = self.embedding(input)\n        print(\"  embedding\", embedded.size())\n\n        # Make a hidden\n        hidden = self._init_hidden(batch_size)\n\n        output, hidden = self.gru(embedded, hidden)\n        print(\"  gru hidden output\", hidden.size())\n        # Use the last layer output as FC's input\n        # No need to unpack, since we are going to use hidden\n        fc_output = self.fc(hidden)\n        print(\"  fc output\", fc_output.size())\n        return fc_output\n\n    def _init_hidden(self, batch_size):\n        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n        return Variable(hidden)\n\n# Help functions\n\n\ndef str2ascii_arr(msg):\n    arr = [ord(c) for c in msg]\n    return arr, len(arr)\n\n# pad sequences and sort the tensor\ndef pad_sequences(vectorized_seqs, seq_lengths):\n    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n    return seq_tensor\n\n# Create necessary variables, lengths, and target\ndef make_variables(names):\n    sequence_and_length = [str2ascii_arr(name) for name in names]\n    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n    return pad_sequences(vectorized_seqs, seq_lengths)\n\n\nif __name__ == '__main__':\n    names = ['adylov', 'solan', 'hard', 'san']\n    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASSES)\n\n    for name in names:\n        arr, _ = str2ascii_arr(name)\n        inp = Variable(torch.LongTensor([arr]))\n        out = classifier(inp)\n        print(\"in\", inp.size(), \"out\", out.size())\n\n\n    inputs = make_variables(names)\n    out = classifier(inputs)\n    print(\"batch in\", inputs.size(), \"batch out\", out.size())\n\n\n"
        },
        {
          "name": "13_2_rnn_classification.py",
          "type": "blob",
          "size": 6.4990234375,
          "content": "# Original code is from https://github.com/spro/practical-pytorch\nimport time\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom name_dataset import NameDataset\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# Parameters and DataLoaders\nHIDDEN_SIZE = 100\nN_LAYERS = 2\nBATCH_SIZE = 256\nN_EPOCHS = 100\n\ntest_dataset = NameDataset(is_train_set=False)\ntest_loader = DataLoader(dataset=test_dataset,\n                         batch_size=BATCH_SIZE, shuffle=True)\n\n\ntrain_dataset = NameDataset(is_train_set=True)\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=BATCH_SIZE, shuffle=True)\n\nN_COUNTRIES = len(train_dataset.get_countries())\nprint(N_COUNTRIES, \"countries\")\nN_CHARS = 128  # ASCII\n\n\n# Some utility functions\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef create_variable(tensor):\n    # Do cuda() before wrapping with variable\n    if torch.cuda.is_available():\n        return Variable(tensor.cuda())\n    else:\n        return Variable(tensor)\n\n\n# pad sequences and sort the tensor\ndef pad_sequences(vectorized_seqs, seq_lengths, countries):\n    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n\n    # Sort tensors by their length\n    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n    seq_tensor = seq_tensor[perm_idx]\n\n    # Also sort the target (countries) in the same order\n    target = countries2tensor(countries)\n    if len(countries):\n        target = target[perm_idx]\n\n    # Return variables\n    # DataParallel requires everything to be a Variable\n    return create_variable(seq_tensor), \\\n        create_variable(seq_lengths), \\\n        create_variable(target)\n\n\n# Create necessary variables, lengths, and target\ndef make_variables(names, countries):\n    sequence_and_length = [str2ascii_arr(name) for name in names]\n    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n\n\ndef str2ascii_arr(msg):\n    arr = [ord(c) for c in msg]\n    return arr, len(arr)\n\n\ndef countries2tensor(countries):\n    country_ids = [train_dataset.get_country_id(\n        country) for country in countries]\n    return torch.LongTensor(country_ids)\n\n\nclass RNNClassifier(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.n_directions = int(bidirectional) + 1\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          bidirectional=bidirectional)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input, seq_lengths):\n        # Note: we run this all at once (over the whole input sequence)\n        # input shape: B x S (input size)\n        # transpose to make S(sequence) x B (batch)\n        input = input.t()\n        batch_size = input.size(1)\n\n        # Make a hidden\n        hidden = self._init_hidden(batch_size)\n\n        # Embedding S x B -> S x B x I (embedding size)\n        embedded = self.embedding(input)\n\n        # Pack them up nicely\n        gru_input = pack_padded_sequence(\n            embedded, seq_lengths.data.cpu().numpy())\n\n        # To compact weights again call flatten_parameters().\n        self.gru.flatten_parameters()\n        output, hidden = self.gru(gru_input, hidden)\n\n        # Use the last layer output as FC's input\n        # No need to unpack, since we are going to use hidden\n        fc_output = self.fc(hidden[-1])\n        return fc_output\n\n    def _init_hidden(self, batch_size):\n        hidden = torch.zeros(self.n_layers * self.n_directions,\n                             batch_size, self.hidden_size)\n        return create_variable(hidden)\n\n\n# Train cycle\ndef train():\n    total_loss = 0\n\n    for i, (names, countries) in enumerate(train_loader, 1):\n        input, seq_lengths, target = make_variables(names, countries)\n        output = classifier(input, seq_lengths)\n\n        loss = criterion(output, target)\n        total_loss += loss.data[0]\n\n        classifier.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i % 10 == 0:\n            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n                time_since(start), epoch,  i *\n                len(names), len(train_loader.dataset),\n                100. * i * len(names) / len(train_loader.dataset),\n                total_loss / i * len(names)))\n\n    return total_loss\n\n\n# Testing cycle\ndef test(name=None):\n    # Predict for a given name\n    if name:\n        input, seq_lengths, target = make_variables([name], [])\n        output = classifier(input, seq_lengths)\n        pred = output.data.max(1, keepdim=True)[1]\n        country_id = pred.cpu().numpy()[0][0]\n        print(name, \"is\", train_dataset.get_country(country_id))\n        return\n\n    print(\"evaluating trained model ...\")\n    correct = 0\n    train_data_size = len(test_loader.dataset)\n\n    for names, countries in test_loader:\n        input, seq_lengths, target = make_variables(names, countries)\n        output = classifier(input, seq_lengths)\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        correct, train_data_size, 100. * correct / train_data_size))\n\n\nif __name__ == '__main__':\n\n    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n        classifier = nn.DataParallel(classifier)\n\n    if torch.cuda.is_available():\n        classifier.cuda()\n\n    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    start = time.time()\n    print(\"Training for %d epochs...\" % N_EPOCHS)\n    for epoch in range(1, N_EPOCHS + 1):\n        # Train cycle\n        train()\n\n        # Testing\n        test()\n\n        # Testing several samples\n        test(\"Sung\")\n        test(\"Jungwoo\")\n        test(\"Soojin\")\n        test(\"Nako\")\n"
        },
        {
          "name": "13_3_char_rnn.py",
          "type": "blob",
          "size": 4.287109375,
          "content": "# https://github.com/spro/practical-pytorch\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom text_loader import TextDataset\n\nhidden_size = 100\nn_layers = 3\nbatch_size = 1\nn_epochs = 100\nn_characters = 128  # ASCII\n\n\nclass RNN(nn.Module):\n\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    # This runs this one step at a time\n    # It's extremely slow, and please do not use in practice.\n    # We need to use (1) batch and (2) data parallelism\n    def forward(self, input, hidden):\n        embed = self.embedding(input.view(1, -1))  # S(=1) x I\n        embed = embed.view(1, 1, -1)  # S(=1) x B(=1) x I (embedding size)\n        output, hidden = self.gru(embed, hidden)\n        output = self.linear(output.view(1, -1))  # S(=1) x I\n        return output, hidden\n\n    def init_hidden(self):\n        if torch.cuda.is_available():\n            hidden = torch.zeros(self.n_layers, 1, self.hidden_size).cuda()\n        else:\n            hidden = torch.zeros(self.n_layers, 1, self.hidden_size)\n\n        return Variable(hidden)\n\n\ndef str2tensor(string):\n    tensor = [ord(c) for c in string]\n    tensor = torch.LongTensor(tensor)\n\n    if torch.cuda.is_available():\n        tensor = tensor.cuda()\n\n    return Variable(tensor)\n\n\ndef generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n    hidden = decoder.init_hidden()\n    prime_input = str2tensor(prime_str)\n    predicted = prime_str\n\n    # Use priming string to \"build up\" hidden state\n    for p in range(len(prime_str) - 1):\n        _, hidden = decoder(prime_input[p], hidden)\n\n    inp = prime_input[-1]\n\n    for p in range(predict_len):\n        output, hidden = decoder(inp, hidden)\n\n        # Sample from the network as a multinomial distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n\n        # Add predicted character to string and use as next input\n        predicted_char = chr(top_i)\n        predicted += predicted_char\n        inp = str2tensor(predicted_char)\n\n    return predicted\n\n# Train for a given src and target\n# It feeds single string to demonstrate seq2seq\n# It's extremely slow, and we need to use (1) batch and (2) data parallelism\n# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\n\n\ndef train_teacher_forching(line):\n    input = str2tensor(line[:-1])\n    target = str2tensor(line[1:])\n\n    hidden = decoder.init_hidden()\n    loss = 0\n\n    for c in range(len(input)):\n        output, hidden = decoder(input[c], hidden)\n        loss += criterion(output, target[c])\n\n    decoder.zero_grad()\n    loss.backward()\n    decoder_optimizer.step()\n\n    return loss.data[0] / len(input)\n\n\ndef train(line):\n    input = str2tensor(line[:-1])\n    target = str2tensor(line[1:])\n\n    hidden = decoder.init_hidden()\n    decoder_in = input[0]\n    loss = 0\n\n    for c in range(len(input)):\n        output, hidden = decoder(decoder_in, hidden)\n        loss += criterion(output, target[c])\n        decoder_in = output.max(1)[1]\n\n    decoder.zero_grad()\n    loss.backward()\n    decoder_optimizer.step()\n\n    return loss.data[0] / len(input)\n\nif __name__ == '__main__':\n\n    decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n    if torch.cuda.is_available():\n        decoder.cuda()\n\n    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(dataset=TextDataset(),\n                              batch_size=batch_size,\n                              shuffle=True)\n\n    print(\"Training for %d epochs...\" % n_epochs)\n    for epoch in range(1, n_epochs + 1):\n        for i, (lines, _) in enumerate(train_loader):\n            loss = train(lines[0])  # Batch size is 1\n\n            if i % 100 == 0:\n                print('[(%d %d%%) loss: %.4f]' %\n                      (epoch, epoch / n_epochs * 100, loss))\n                print(generate(decoder, 'Wh', 100), '\\n')\n"
        },
        {
          "name": "13_4_pack_pad.py",
          "type": "blob",
          "size": 2.2841796875,
          "content": "# Original source from\n# https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e\n# torch\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport torch.nn.functional as F\nimport numpy as np\nimport itertools\n\n\ndef flatten(l):\n    return list(itertools.chain.from_iterable(l))\n\nseqs = ['ghatmasala', 'nicela', 'chutpakodas']\n\n# make <pad> idx 0\nvocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n\n# make model\nembedding_size = 3\nembed = nn.Embedding(len(vocab), embedding_size)\nlstm = nn.LSTM(embedding_size, 5)\n\nvectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\nprint(\"vectorized_seqs\", vectorized_seqs)\n\nprint([x for x in map(len, vectorized_seqs)])\n# get the length of each seq in your batch\nseq_lengths = torch.LongTensor([x for x in map(len, vectorized_seqs)])\n\n# dump padding everywhere, and place seqs on the left.\n# NOTE: you only need a tensor as big as your longest sequence\nseq_tensor = Variable(torch.zeros(\n    (len(vectorized_seqs), seq_lengths.max()))).long()\nfor idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n\nprint(\"seq_tensor\", seq_tensor)\n\n# SORT YOUR TENSORS BY LENGTH!\nseq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\nseq_tensor = seq_tensor[perm_idx]\n\nprint(\"seq_tensor after sorting\", seq_tensor)\n\n# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n# Otherwise, give (L,B,D) tensors\nseq_tensor = seq_tensor.transpose(0, 1)  # (B,L,D) -> (L,B,D)\nprint(\"seq_tensor after transposing\", seq_tensor.size(), seq_tensor.data)\n\n# embed your sequences\nembeded_seq_tensor = embed(seq_tensor)\nprint(\"seq_tensor after embeding\", embeded_seq_tensor.size(), seq_tensor.data)\n\n# pack them up nicely\npacked_input = pack_padded_sequence(\n    embeded_seq_tensor, seq_lengths.cpu().numpy())\n\n# throw them through your LSTM (remember to give batch_first=True here if\n# you packed with it)\npacked_output, (ht, ct) = lstm(packed_input)\n\n# unpack your output if required\noutput, _ = pad_packed_sequence(packed_output)\nprint(\"Lstm output\", output.size(), output.data)\n\n# Or if you just want the final hidden state?\nprint(\"Last output\", ht[-1].size(), ht[-1].data)\n"
        },
        {
          "name": "14_1_seq2seq.py",
          "type": "blob",
          "size": 3.6767578125,
          "content": "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom text_loader import TextDataset\nimport seq2seq_models as sm\nfrom seq2seq_models import str2tensor, EOS_token, SOS_token\n\nHIDDEN_SIZE = 100\nN_LAYERS = 1\nBATCH_SIZE = 1\nN_EPOCH = 100\nN_CHARS = 128  # ASCII\n\n\n# Simple test to show how our network works\ndef test():\n    encoder_hidden = encoder.init_hidden()\n    word_input = str2tensor('hello')\n    encoder_outputs, encoder_hidden = encoder(word_input, encoder_hidden)\n    print(encoder_outputs)\n\n    decoder_hidden = encoder_hidden\n\n    word_target = str2tensor('pytorch')\n    for c in range(len(word_target)):\n        decoder_output, decoder_hidden = decoder(\n            word_target[c], decoder_hidden)\n        print(decoder_output.size(), decoder_hidden.size())\n\n\n# Train for a given src and target\n# To demonstrate seq2seq, We don't handle batch in the code,\n# and our encoder runs this one step at a time\n# It's extremely slow, and please do not use in practice.\n# We need to use (1) batch and (2) data parallelism\n# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\ndef train(src, target):\n    src_var = str2tensor(src)\n    target_var = str2tensor(target, eos=True)  # Add the EOS token\n\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n\n    hidden = encoder_hidden\n    loss = 0\n\n    for c in range(len(target_var)):\n        # First, we feed SOS\n        # Others, we use teacher forcing\n        token = target_var[c - 1] if c else str2tensor(SOS_token)\n        output, hidden = decoder(token, hidden)\n        loss += criterion(output, target_var[c])\n\n    encoder.zero_grad()\n    decoder.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss.data[0] / len(target_var)\n\n\n# Translate the given input\ndef translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n    input_var = str2tensor(enc_input)\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n\n    hidden = encoder_hidden\n\n    predicted = ''\n    dec_input = str2tensor(SOS_token)\n    for c in range(predict_len):\n        output, hidden = decoder(dec_input, hidden)\n\n        # Sample from the network as a multi nominal distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n\n        # Stop at the EOS\n        if top_i is EOS_token:\n            break\n\n        predicted_char = chr(top_i)\n        predicted += predicted_char\n\n        dec_input = str2tensor(predicted_char)\n\n    return enc_input, predicted\n\n\nencoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\ndecoder = sm.DecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n\nif torch.cuda.is_available():\n    decoder.cuda()\n    encoder.cuda()\nprint(encoder, decoder)\ntest()\n\nparams = list(encoder.parameters()) + list(decoder.parameters())\noptimizer = torch.optim.Adam(params, lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n\ntrain_loader = DataLoader(dataset=TextDataset(),\n                          batch_size=BATCH_SIZE,\n                          shuffle=True,\n                          num_workers=2)\n\nprint(\"Training for %d epochs...\" % N_EPOCH)\nfor epoch in range(1, N_EPOCH + 1):\n    # Get srcs and targets from data loader\n    for i, (srcs, targets) in enumerate(train_loader):\n        train_loss = train(srcs[0], targets[0])  # Batch is 1\n\n        if i % 100 is 0:\n            print('[(%d %d%%) %.4f]' %\n                  (epoch, epoch / N_EPOCH * 100, train_loss))\n            print(translate(srcs[0]), '\\n')\n            print(translate(), '\\n')\n"
        },
        {
          "name": "14_2_seq2seq_att.py",
          "type": "blob",
          "size": 4.650390625,
          "content": "# Original code from\n# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n\n#import matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader\nfrom text_loader import TextDataset\nimport seq2seq_models as sm\nfrom seq2seq_models import cuda_variable, str2tensor, EOS_token, SOS_token\n\n\nN_LAYERS = 1\nBATCH_SIZE = 1\nN_EPOCH = 100\nN_CHARS = 128  # ASCII\nHIDDEN_SIZE = N_CHARS\n\n\n# Simple test to show how our train works\ndef test():\n    encoder_test = sm.EncoderRNN(10, 10, 2)\n    decoder_test = sm.AttnDecoderRNN(10, 10, 2)\n\n    if torch.cuda.is_available():\n        encoder_test.cuda()\n        decoder_test.cuda()\n\n    encoder_hidden = encoder_test.init_hidden()\n    word_input = cuda_variable(torch.LongTensor([1, 2, 3]))\n    encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n    print(encoder_outputs.size())\n\n    word_target = cuda_variable(torch.LongTensor([1, 2, 3]))\n    decoder_attns = torch.zeros(1, 3, 3)\n    decoder_hidden = encoder_hidden\n\n    for c in range(len(word_target)):\n        decoder_output, decoder_hidden, decoder_attn = \\\n            decoder_test(word_target[c],\n                         decoder_hidden, encoder_outputs)\n        print(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())\n        decoder_attns[0, c] = decoder_attn.squeeze(0).cpu().data\n\n\n# Train for a given src and target\n# To demonstrate seq2seq, We don't handle batch in the code,\n# and our encoder runs this one step at a time\n# It's extremely slow, and please do not use in practice.\n# We need to use (1) batch and (2) data parallelism\n# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\ndef train(src, target):\n    loss = 0\n\n    src_var = str2tensor(src)\n    target_var = str2tensor(target, eos=True)  # Add the EOS token\n\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n\n    hidden = encoder_hidden\n\n    for c in range(len(target_var)):\n        # First, we feed SOS. Others, we use teacher forcing.\n        token = target_var[c - 1] if c else str2tensor(SOS_token)\n        output, hidden, attention = decoder(token, hidden, encoder_outputs)\n        loss += criterion(output, target_var[c])\n\n    encoder.zero_grad()\n    decoder.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss.data[0] / len(target_var)\n\n\n# Translate the given input\ndef translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n    input_var = str2tensor(enc_input)\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n\n    hidden = encoder_hidden\n\n    predicted = ''\n    dec_input = str2tensor(SOS_token)\n    attentions = []\n    for c in range(predict_len):\n        output, hidden, attention = decoder(dec_input, hidden, encoder_outputs)\n        # Sample from the network as a multi nominal distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        attentions.append(attention.view(-1).data.cpu().numpy().tolist())\n\n        # Stop at the EOS\n        if top_i is EOS_token:\n            break\n\n        predicted_char = chr(top_i)\n        predicted += predicted_char\n\n        dec_input = str2tensor(predicted_char)\n\n    return predicted, attentions\n\n\nif __name__ == '__main__':\n    encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n    decoder = sm.AttnDecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n\n    if torch.cuda.is_available():\n        decoder.cuda()\n        encoder.cuda()\n    print(encoder, decoder)\n    # test()\n\n    params = list(encoder.parameters()) + list(decoder.parameters())\n    optimizer = torch.optim.Adam(params, lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(dataset=TextDataset(),\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              num_workers=2)\n\n    print(\"Training for %d epochs...\" % N_EPOCH)\n    for epoch in range(1, N_EPOCH + 1):\n        # Get srcs and targets from data loader\n        for i, (srcs, targets) in enumerate(train_loader):\n            train_loss = train(srcs[0], targets[0])\n\n            if i % 1000 is 0:\n                print('[(%d/%d %d%%) %.4f]' %\n                      (epoch, N_EPOCH, i * len(srcs) * 100 / len(train_loader), train_loss))\n                output, _ = translate(srcs[0])\n                print(srcs[0], output, '\\n')\n\n                output, attentions = translate()\n                print('thisissungkim.iloveyou.', output, '\\n')\n\n        # plt.matshow(attentions)\n        # plt.show()\n        # print(attentions)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 0.6484375,
          "content": "[![Build Status](https://travis-ci.org/hunkim/PythonZeroToAll.svg?branch=master)](https://travis-ci.org/hunkim/PythonZeroToAll)\n\n# PyTorchZeroToAll\nQuick 3~4 day lecture materials for HKUST students.\n\n## Video Lectures: (RNN TBA)\n* [Youtube](http://bit.ly/PyTorchVideo)\n* [Bilibili](https://www.bilibili.com/video/av15823922/)\n\n## Slides\n* [Lecture Slides @GoogleDrive](http://bit.ly/PyTorchZeroAll)\n\nIf you cannot access the GoogleDoc for somehow, please check out pdf files in slides. However, slides in GoogleDrive are always latest. We really appreciate your comments.\n\n## Previous Lectures \n* cf., http://bit.ly/TF_HKUST (3 day crash course using TensorFlow)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "name_dataset.py",
          "type": "blob",
          "size": 1.767578125,
          "content": "# References\n# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport csv\nimport gzip\n\n\nclass NameDataset(Dataset):\n    \"\"\" Diabetes dataset.\"\"\"\n\n    # Initialize your data, download, etc.\n    def __init__(self, is_train_set=False):\n        filename = './data/names_train.csv.gz' if is_train_set else './data/names_test.csv.gz'\n        with gzip.open(filename, \"rt\") as f:\n            reader = csv.reader(f)\n            rows = list(reader)\n\n        self.names = [row[0] for row in rows]\n        self.countries = [row[1] for row in rows]\n        self.len = len(self.countries)\n\n        self.country_list = list(sorted(set(self.countries)))\n\n    def __getitem__(self, index):\n        return self.names[index], self.countries[index]\n\n    def __len__(self):\n        return self.len\n\n    def get_countries(self):\n        return self.country_list\n\n    def get_country(self, id):\n        return self.country_list[id]\n\n    def get_country_id(self, country):\n        return self.country_list.index(country)\n\n# Test the loader\nif __name__ == \"__main__\":\n    dataset = NameDataset(False)\n    print(dataset.get_countries())\n    print(dataset.get_country(3))\n    print(dataset.get_country_id('Korean'))\n\n    train_loader = DataLoader(dataset=dataset,\n                              batch_size=10,\n                              shuffle=True)\n\n    print(len(train_loader.dataset))\n    for epoch in range(2):\n        for i, (names, countries) in enumerate(train_loader):\n            # Run your training process\n            print(epoch, i, \"names\", names, \"countries\", countries)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1494140625,
          "content": "#nonsml: digitalgenius/ubuntu-pytorch\n#varunagrawal/pytorch\nhttplib2==0.18.0\nmatplotlib==2.0.0\nnumpy==1.13.3\ntorch\ntorchvision==0.1.9\nUnidecode==0.04.21\n"
        },
        {
          "name": "seq2seq_models.py",
          "type": "blob",
          "size": 5.060546875,
          "content": "# Original code from\n# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nMAX_LENGTH = 100\n\nSOS_token = chr(0)\nEOS_token = 1\n\n# Helper function to create Variable based on\n# the cuda availability\n\n\ndef cuda_variable(tensor):\n    # Do cuda() before wrapping with variable\n    if torch.cuda.is_available():\n        return Variable(tensor.cuda())\n    else:\n        return Variable(tensor)\n\n\n# Sting to char tensor\ndef str2tensor(msg, eos=False):\n    tensor = [ord(c) for c in msg]\n    if eos:\n        tensor.append(EOS_token)\n\n    return cuda_variable(torch.LongTensor(tensor))\n\n\n# To demonstrate seq2seq, We don't handle batch in the code,\n# and our encoder runs this one step at a time\n# It's extremely slow, and please do not use in practice.\n# We need to use (1) batch and (2) data parallelism\n# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\n\nclass EncoderRNN(nn.Module):\n\n    def __init__(self, input_size, hidden_size, n_layers=1):\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        super(EncoderRNN, self).__init__()\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n\n    def forward(self, word_inputs, hidden):\n        # Note: we run this all at once (over the whole input sequence)\n        seq_len = len(word_inputs)\n        # input shape: S x B (=1) x I (input size)\n        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n        output, hidden = self.gru(embedded, hidden)\n        return output, hidden\n\n    def init_hidden(self):\n        # (num_layers * num_directions, batch, hidden_size)\n        return cuda_variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n\n\nclass DecoderRNN(nn.Module):\n\n    def __init__(self, hidden_size, output_size, n_layers=1):\n        super(DecoderRNN, self).__init__()\n\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input, hidden):\n        # input shape: S(=1) x B (=1) x I (input size)\n        # Note: we run this one step at a time. (Sequence size = 1)\n        output = self.embedding(input).view(1, 1, -1)\n        output, hidden = self.gru(output, hidden)\n        output = self.out(output[0])\n        # No need softmax, since we are using CrossEntropyLoss\n        return output, hidden\n\n    def init_hidden(self):\n        # (num_layers * num_directions, batch, hidden_size)\n        return cuda_variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n\n\nclass AttnDecoderRNN(nn.Module):\n\n    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n        super(AttnDecoderRNN, self).__init__()\n\n        # Linear for attention\n        self.attn = nn.Linear(hidden_size, hidden_size)\n\n        # Define layers\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size,\n                          n_layers, dropout=dropout_p)\n        self.out = nn.Linear(hidden_size * 2, output_size)\n\n    def forward(self, word_input, last_hidden, encoder_hiddens):\n        # Note: we run this one step (S=1) at a time\n        # Get the embedding of the current input word (last output word)\n        rnn_input = self.embedding(word_input).view(1, 1, -1)  # S=1 x B x I\n        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n\n        # Calculate attention from current RNN state and all encoder outputs;\n        # apply to encoder outputs\n        attn_weights = self.get_att_weight(\n            rnn_output.squeeze(0), encoder_hiddens)\n        context = attn_weights.bmm(\n            encoder_hiddens.transpose(0, 1))  # B x S(=1) x I\n\n        # Final output layer (next word prediction) using the RNN hidden state\n        # and context vector\n        rnn_output = rnn_output.squeeze(0)  # S(=1) x B x I -> B x I\n        context = context.squeeze(1)  # B x S(=1) x I -> B x I\n        output = self.out(torch.cat((rnn_output, context), 1))\n\n        # Return final output, hidden state, and attention weights (for\n        # visualization)\n        return output, hidden, attn_weights\n\n    def get_att_weight(self, hidden, encoder_hiddens):\n        seq_len = len(encoder_hiddens)\n\n        # Create variable to store attention energies\n        attn_scores = cuda_variable(torch.zeros(seq_len))  # B x 1 x S\n\n        # Calculate energies for each encoder hidden\n        for i in range(seq_len):\n            attn_scores[i] = self.get_att_score(hidden, encoder_hiddens[i])\n\n        # Normalize scores to weights in range 0 to 1,\n        # resize to 1 x 1 x seq_len\n        # print(\"att_scores\", attn_scores.size())\n        return F.softmax(attn_scores).view(1, 1, -1)\n\n    # score = h^T W h^e = h dot (W h^e)\n    # TODO: We need to implement different score models\n    def get_att_score(self, hidden, encoder_hidden):\n        score = self.attn(encoder_hidden)\n        return torch.dot(hidden.view(-1), score.view(-1))\n"
        },
        {
          "name": "slides",
          "type": "tree",
          "content": null
        },
        {
          "name": "text_loader.py",
          "type": "blob",
          "size": 1.1337890625,
          "content": "# References\n# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\nimport gzip\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass TextDataset(Dataset):\n    # Initialize your data, download, etc.\n\n    def __init__(self, filename=\"./data/shakespeare.txt.gz\"):\n        self.len = 0\n        with gzip.open(filename, 'rt') as f:\n            self.targetLines = [x.strip() for x in f if x.strip()]\n            self.srcLines = [x.lower().replace(' ', '')\n                             for x in self.targetLines]\n            self.len = len(self.srcLines)\n\n    def __getitem__(self, index):\n        return self.srcLines[index], self.targetLines[index]\n\n    def __len__(self):\n        return self.len\n\n\n# Test the loader\nif __name__ == \"__main__\":\n    dataset = TextDataset()\n    train_loader = DataLoader(dataset=dataset,\n                              batch_size=3,\n                              shuffle=True,\n                              num_workers=2)\n\n    for i, (src, target) in enumerate(train_loader):\n        print(i, \"data\", src)\n"
        }
      ]
    }
  ]
}