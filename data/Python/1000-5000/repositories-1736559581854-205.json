{
  "metadata": {
    "timestamp": 1736559581854,
    "page": 205,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Tianxiaomo/pytorch-YOLOv4",
      "stars": 4496,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1279296875,
          "content": "ttest\n*.weights\n*.pth\n*.onnx\n*.engine\n*.pyc\n*.infer\n*.npy\n\nz_demo_*\n\n__pycache__\n.idea\n.vscode\nruns\nlog\n\n*.jpg\n*.json\ndata/outcome\n"
        },
        {
          "name": "DeepStream",
          "type": "tree",
          "content": null
        },
        {
          "name": "License.txt",
          "type": "blob",
          "size": 11.2890625,
          "content": "\r\n                                 Apache License\r\n                           Version 2.0, January 2004\r\n                        http://www.apache.org/licenses/\r\n\r\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\r\n\r\n   1. Definitions.\r\n\r\n      \"License\" shall mean the terms and conditions for use, reproduction,\r\n      and distribution as defined by Sections 1 through 9 of this document.\r\n\r\n      \"Licensor\" shall mean the copyright owner or entity authorized by\r\n      the copyright owner that is granting the License.\r\n\r\n      \"Legal Entity\" shall mean the union of the acting entity and all\r\n      other entities that control, are controlled by, or are under common\r\n      control with that entity. For the purposes of this definition,\r\n      \"control\" means (i) the power, direct or indirect, to cause the\r\n      direction or management of such entity, whether by contract or\r\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\r\n      outstanding shares, or (iii) beneficial ownership of such entity.\r\n\r\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\r\n      exercising permissions granted by this License.\r\n\r\n      \"Source\" form shall mean the preferred form for making modifications,\r\n      including but not limited to software source code, documentation\r\n      source, and configuration files.\r\n\r\n      \"Object\" form shall mean any form resulting from mechanical\r\n      transformation or translation of a Source form, including but\r\n      not limited to compiled object code, generated documentation,\r\n      and conversions to other media types.\r\n\r\n      \"Work\" shall mean the work of authorship, whether in Source or\r\n      Object form, made available under the License, as indicated by a\r\n      copyright notice that is included in or attached to the work\r\n      (an example is provided in the Appendix below).\r\n\r\n      \"Derivative Works\" shall mean any work, whether in Source or Object\r\n      form, that is based on (or derived from) the Work and for which the\r\n      editorial revisions, annotations, elaborations, or other modifications\r\n      represent, as a whole, an original work of authorship. For the purposes\r\n      of this License, Derivative Works shall not include works that remain\r\n      separable from, or merely link (or bind by name) to the interfaces of,\r\n      the Work and Derivative Works thereof.\r\n\r\n      \"Contribution\" shall mean any work of authorship, including\r\n      the original version of the Work and any modifications or additions\r\n      to that Work or Derivative Works thereof, that is intentionally\r\n      submitted to Licensor for inclusion in the Work by the copyright owner\r\n      or by an individual or Legal Entity authorized to submit on behalf of\r\n      the copyright owner. For the purposes of this definition, \"submitted\"\r\n      means any form of electronic, verbal, or written communication sent\r\n      to the Licensor or its representatives, including but not limited to\r\n      communication on electronic mailing lists, source code control systems,\r\n      and issue tracking systems that are managed by, or on behalf of, the\r\n      Licensor for the purpose of discussing and improving the Work, but\r\n      excluding communication that is conspicuously marked or otherwise\r\n      designated in writing by the copyright owner as \"Not a Contribution.\"\r\n\r\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\r\n      on behalf of whom a Contribution has been received by Licensor and\r\n      subsequently incorporated within the Work.\r\n\r\n   2. Grant of Copyright License. Subject to the terms and conditions of\r\n      this License, each Contributor hereby grants to You a perpetual,\r\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\r\n      copyright license to reproduce, prepare Derivative Works of,\r\n      publicly display, publicly perform, sublicense, and distribute the\r\n      Work and such Derivative Works in Source or Object form.\r\n\r\n   3. Grant of Patent License. Subject to the terms and conditions of\r\n      this License, each Contributor hereby grants to You a perpetual,\r\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\r\n      (except as stated in this section) patent license to make, have made,\r\n      use, offer to sell, sell, import, and otherwise transfer the Work,\r\n      where such license applies only to those patent claims licensable\r\n      by such Contributor that are necessarily infringed by their\r\n      Contribution(s) alone or by combination of their Contribution(s)\r\n      with the Work to which such Contribution(s) was submitted. If You\r\n      institute patent litigation against any entity (including a\r\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\r\n      or a Contribution incorporated within the Work constitutes direct\r\n      or contributory patent infringement, then any patent licenses\r\n      granted to You under this License for that Work shall terminate\r\n      as of the date such litigation is filed.\r\n\r\n   4. Redistribution. You may reproduce and distribute copies of the\r\n      Work or Derivative Works thereof in any medium, with or without\r\n      modifications, and in Source or Object form, provided that You\r\n      meet the following conditions:\r\n\r\n      (a) You must give any other recipients of the Work or\r\n          Derivative Works a copy of this License; and\r\n\r\n      (b) You must cause any modified files to carry prominent notices\r\n          stating that You changed the files; and\r\n\r\n      (c) You must retain, in the Source form of any Derivative Works\r\n          that You distribute, all copyright, patent, trademark, and\r\n          attribution notices from the Source form of the Work,\r\n          excluding those notices that do not pertain to any part of\r\n          the Derivative Works; and\r\n\r\n      (d) If the Work includes a \"NOTICE\" text file as part of its\r\n          distribution, then any Derivative Works that You distribute must\r\n          include a readable copy of the attribution notices contained\r\n          within such NOTICE file, excluding those notices that do not\r\n          pertain to any part of the Derivative Works, in at least one\r\n          of the following places: within a NOTICE text file distributed\r\n          as part of the Derivative Works; within the Source form or\r\n          documentation, if provided along with the Derivative Works; or,\r\n          within a display generated by the Derivative Works, if and\r\n          wherever such third-party notices normally appear. The contents\r\n          of the NOTICE file are for informational purposes only and\r\n          do not modify the License. You may add Your own attribution\r\n          notices within Derivative Works that You distribute, alongside\r\n          or as an addendum to the NOTICE text from the Work, provided\r\n          that such additional attribution notices cannot be construed\r\n          as modifying the License.\r\n\r\n      You may add Your own copyright statement to Your modifications and\r\n      may provide additional or different license terms and conditions\r\n      for use, reproduction, or distribution of Your modifications, or\r\n      for any such Derivative Works as a whole, provided Your use,\r\n      reproduction, and distribution of the Work otherwise complies with\r\n      the conditions stated in this License.\r\n\r\n   5. Submission of Contributions. Unless You explicitly state otherwise,\r\n      any Contribution intentionally submitted for inclusion in the Work\r\n      by You to the Licensor shall be under the terms and conditions of\r\n      this License, without any additional terms or conditions.\r\n      Notwithstanding the above, nothing herein shall supersede or modify\r\n      the terms of any separate license agreement you may have executed\r\n      with Licensor regarding such Contributions.\r\n\r\n   6. Trademarks. This License does not grant permission to use the trade\r\n      names, trademarks, service marks, or product names of the Licensor,\r\n      except as required for reasonable and customary use in describing the\r\n      origin of the Work and reproducing the content of the NOTICE file.\r\n\r\n   7. Disclaimer of Warranty. Unless required by applicable law or\r\n      agreed to in writing, Licensor provides the Work (and each\r\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\r\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\r\n      implied, including, without limitation, any warranties or conditions\r\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\r\n      PARTICULAR PURPOSE. You are solely responsible for determining the\r\n      appropriateness of using or redistributing the Work and assume any\r\n      risks associated with Your exercise of permissions under this License.\r\n\r\n   8. Limitation of Liability. In no event and under no legal theory,\r\n      whether in tort (including negligence), contract, or otherwise,\r\n      unless required by applicable law (such as deliberate and grossly\r\n      negligent acts) or agreed to in writing, shall any Contributor be\r\n      liable to You for damages, including any direct, indirect, special,\r\n      incidental, or consequential damages of any character arising as a\r\n      result of this License or out of the use or inability to use the\r\n      Work (including but not limited to damages for loss of goodwill,\r\n      work stoppage, computer failure or malfunction, or any and all\r\n      other commercial damages or losses), even if such Contributor\r\n      has been advised of the possibility of such damages.\r\n\r\n   9. Accepting Warranty or Additional Liability. While redistributing\r\n      the Work or Derivative Works thereof, You may choose to offer,\r\n      and charge a fee for, acceptance of support, warranty, indemnity,\r\n      or other liability obligations and/or rights consistent with this\r\n      License. However, in accepting such obligations, You may act only\r\n      on Your own behalf and on Your sole responsibility, not on behalf\r\n      of any other Contributor, and only if You agree to indemnify,\r\n      defend, and hold each Contributor harmless for any liability\r\n      incurred by, or claims asserted against, such Contributor by reason\r\n      of your accepting any such warranty or additional liability.\r\n\r\n   END OF TERMS AND CONDITIONS\r\n\r\n   APPENDIX: How to apply the Apache License to your work.\r\n\r\n      To apply the Apache License to your work, attach the following\r\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\r\n      replaced with your own identifying information. (Don't include\r\n      the brackets!)  The text should be enclosed in the appropriate\r\n      comment syntax for the file format. We also recommend that a\r\n      file or class name and description of purpose be included on the\r\n      same \"printed page\" as the copyright notice for easier\r\n      identification within third-party archives.\r\n\r\n   Copyright [yyyy] [name of copyright owner]\r\n\r\n   Licensed under the Apache License, Version 2.0 (the \"License\");\r\n   you may not use this file except in compliance with the License.\r\n   You may obtain a copy of the License at\r\n\r\n       http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n   Unless required by applicable law or agreed to in writing, software\r\n   distributed under the License is distributed on an \"AS IS\" BASIS,\r\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n   See the License for the specific language governing permissions and\r\n   limitations under the License.\r\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.2890625,
          "content": "# Pytorch-YOLOv4\n\n![](https://img.shields.io/static/v1?label=python&message=3.6|3.7&color=blue)\n![](https://img.shields.io/static/v1?label=pytorch&message=1.4&color=<COLOR>)\n[![](https://img.shields.io/static/v1?label=license&message=Apache2&color=green)](./License.txt)\n\nA minimal PyTorch implementation of YOLOv4.\n- Paper Yolo v4: https://arxiv.org/abs/2004.10934\n- Source code:https://github.com/AlexeyAB/darknet\n- More details: http://pjreddie.com/darknet/yolo/\n\n\n- [x] Inference\n- [x] Train\n    - [x] Mosaic\n\n```\n├── README.md\n├── dataset.py            dataset\n├── demo.py               demo to run pytorch --> tool/darknet2pytorch\n├── demo_darknet2onnx.py  tool to convert into onnx --> tool/darknet2pytorch\n├── demo_pytorch2onnx.py  tool to convert into onnx\n├── models.py             model for pytorch\n├── train.py              train models.py\n├── cfg.py                cfg.py for train\n├── cfg                   cfg --> darknet2pytorch\n├── data            \n├── weight                --> darknet2pytorch\n├── tool\n│   ├── camera.py           a demo camera\n│   ├── coco_annotation.py       coco dataset generator\n│   ├── config.py\n│   ├── darknet2pytorch.py\n│   ├── region_loss.py\n│   ├── utils.py\n│   └── yolo_layer.py\n```\n\n![image](https://user-gold-cdn.xitu.io/2020/4/26/171b5a6c8b3bd513?w=768&h=576&f=jpeg&s=78882)\n\n# 0. Weights Download\n\n## 0.1 darknet\n- baidu(https://pan.baidu.com/s/1dAGEW8cm-dqK14TbhhVetA     Extraction code:dm5b)\n- google(https://drive.google.com/open?id=1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT)\n\n## 0.2 pytorch\nyou can use darknet2pytorch to convert it yourself, or download my converted model.\n\n- baidu\n    - yolov4.pth(https://pan.baidu.com/s/1ZroDvoGScDgtE1ja_QqJVw Extraction code:xrq9) \n    - yolov4.conv.137.pth(https://pan.baidu.com/s/1ovBie4YyVQQoUrC3AY0joA Extraction code:kcel)\n- google\n    - yolov4.pth(https://drive.google.com/open?id=1wv_LiFeCRYwtpkqREPeI13-gPELBDwuJ)\n    - yolov4.conv.137.pth(https://drive.google.com/open?id=1fcbR0bWzYfIEdLJPzOsn4R5mlvR6IQyA)\n\n# 1. Train\n\n[use yolov4 to train your own data](Use_yolov4_to_train_your_own_data.md)\n\n1. Download weight\n2. Transform data\n\n    For coco dataset,you can use tool/coco_annotation.py.\n    ```\n    # train.txt\n    image_path1 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\n    image_path2 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\n    ...\n    ...\n    ```\n3. Train\n\n    you can set parameters in cfg.py.\n    ```\n     python train.py -g [GPU_ID] -dir [Dataset direction] ...\n    ```\n\n# 2. Inference\n\n## 2.1 Performance on MS COCO dataset (using pretrained DarknetWeights from <https://github.com/AlexeyAB/darknet>)\n\n**ONNX and TensorRT models are converted from Pytorch (TianXiaomo): Pytorch->ONNX->TensorRT.**\nSee following sections for more details of conversions.\n\n- val2017 dataset (input size: 416x416)\n\n| Model type          | AP          | AP50        | AP75        |  APS        | APM         | APL         |\n| ------------------- | ----------: | ----------: | ----------: | ----------: | ----------: | ----------: |\n| DarkNet (YOLOv4 paper)|     0.471 |       0.710 |       0.510 |       0.278 |       0.525 |       0.636 |\n| Pytorch (TianXiaomo)|       0.466 |       0.704 |       0.505 |       0.267 |       0.524 |       0.629 |\n| TensorRT FP32 + BatchedNMSPlugin | 0.472| 0.708 |       0.511 |       0.273 |       0.530 |       0.637 |\n| TensorRT FP16 + BatchedNMSPlugin | 0.472| 0.708 |       0.511 |       0.273 |       0.530 |       0.636 |\n\n- testdev2017 dataset (input size: 416x416)\n\n| Model type          | AP          | AP50        | AP75        |  APS        | APM         | APL         |\n| ------------------- | ----------: | ----------: | ----------: | ----------: | ----------: | ----------: |\n| DarkNet (YOLOv4 paper)|     0.412 |       0.628 |       0.443 |       0.204 |       0.444 |       0.560 |\n| Pytorch (TianXiaomo)|       0.404 |       0.615 |       0.436 |       0.196 |       0.438 |       0.552 |\n| TensorRT FP32 + BatchedNMSPlugin | 0.412| 0.625 |       0.445 |       0.200 |       0.446 |       0.564 |\n| TensorRT FP16 + BatchedNMSPlugin | 0.412| 0.625 |       0.445 |       0.200 |       0.446 |       0.563 |\n\n\n## 2.2 Image input size for inference\n\nImage input size is NOT restricted in `320 * 320`, `416 * 416`, `512 * 512` and `608 * 608`.\nYou can adjust your input sizes for a different input ratio, for example: `320 * 608`.\nLarger input size could help detect smaller targets, but may be slower and GPU memory exhausting.\n\n```py\nheight = 320 + 96 * n, n in {0, 1, 2, 3, ...}\nwidth  = 320 + 96 * m, m in {0, 1, 2, 3, ...}\n```\n\n## 2.3 **Different inference options**\n\n- Load the pretrained darknet model and darknet weights to do the inference (image size is configured in cfg file already)\n\n    ```sh\n    python demo.py -cfgfile <cfgFile> -weightfile <weightFile> -imgfile <imgFile>\n    ```\n\n- Load pytorch weights (pth file) to do the inference\n\n    ```sh\n    python models.py <num_classes> <weightfile> <imgfile> <IN_IMAGE_H> <IN_IMAGE_W> <namefile(optional)>\n    ```\n    \n- Load converted ONNX file to do inference (See section 3 and 4)\n\n- Load converted TensorRT engine file to do inference (See section 5)\n\n## 2.4 Inference output\n\nThere are 2 inference outputs.\n- One is locations of bounding boxes, its shape is  `[batch, num_boxes, 1, 4]` which represents x1, y1, x2, y2 of each bounding box.\n- The other one is scores of bounding boxes which is of shape `[batch, num_boxes, num_classes]` indicating scores of all classes for each bounding box.\n\nUntil now, still a small piece of post-processing including NMS is required. We are trying to minimize time and complexity of post-processing.\n\n\n# 3. Darknet2ONNX\n\n- **This script is to convert the official pretrained darknet model into ONNX**\n\n- **Pytorch version Recommended:**\n\n    - Pytorch 1.4.0 for TensorRT 7.0 and higher\n    - Pytorch 1.5.0 and 1.6.0 for TensorRT 7.1.2 and higher\n\n- **Install onnxruntime**\n\n    ```sh\n    pip install onnxruntime\n    ```\n\n- **Run python script to generate ONNX model and run the demo**\n\n    ```sh\n    python demo_darknet2onnx.py <cfgFile> <namesFile> <weightFile> <imageFile> <batchSize>\n    ```\n\n## 3.1 Dynamic or static batch size\n\n- **Positive batch size will generate ONNX model of static batch size, otherwise, batch size will be dynamic**\n    - Dynamic batch size will generate only one ONNX model\n    - Static batch size will generate 2 ONNX models, one is for running the demo (batch_size=1)\n\n# 4. Pytorch2ONNX\n\n- **You can convert your trained pytorch model into ONNX using this script**\n\n- **Pytorch version Recommended:**\n\n    - Pytorch 1.4.0 for TensorRT 7.0 and higher\n    - Pytorch 1.5.0 and 1.6.0 for TensorRT 7.1.2 and higher\n\n- **Install onnxruntime**\n\n    ```sh\n    pip install onnxruntime\n    ```\n\n- **Run python script to generate ONNX model and run the demo**\n\n    ```sh\n    python demo_pytorch2onnx.py <weight_file> <image_path> <batch_size> <n_classes> <IN_IMAGE_H> <IN_IMAGE_W>\n    ```\n\n    For example:\n\n    ```sh\n    python demo_pytorch2onnx.py yolov4.pth dog.jpg 8 80 416 416\n    ```\n\n## 4.1 Dynamic or static batch size\n\n- **Positive batch size will generate ONNX model of static batch size, otherwise, batch size will be dynamic**\n    - Dynamic batch size will generate only one ONNX model\n    - Static batch size will generate 2 ONNX models, one is for running the demo (batch_size=1)\n\n\n# 5. ONNX2TensorRT\n\n- **TensorRT version Recommended: 7.0, 7.1**\n\n## 5.1 Convert from ONNX of static Batch size\n\n- **Run the following command to convert YOLOv4 ONNX model into TensorRT engine**\n\n    ```sh\n    trtexec --onnx=<onnx_file> --explicitBatch --saveEngine=<tensorRT_engine_file> --workspace=<size_in_megabytes> --fp16\n    ```\n    - Note: If you want to use int8 mode in conversion, extra int8 calibration is needed.\n\n## 5.2 Convert from ONNX of dynamic Batch size\n\n- **Run the following command to convert YOLOv4 ONNX model into TensorRT engine**\n\n    ```sh\n    trtexec --onnx=<onnx_file> \\\n    --minShapes=input:<shape_of_min_batch> --optShapes=input:<shape_of_opt_batch> --maxShapes=input:<shape_of_max_batch> \\\n    --workspace=<size_in_megabytes> --saveEngine=<engine_file> --fp16\n    ```\n- For example:\n\n    ```sh\n    trtexec --onnx=yolov4_-1_3_320_512_dynamic.onnx \\\n    --minShapes=input:1x3x320x512 --optShapes=input:4x3x320x512 --maxShapes=input:8x3x320x512 \\\n    --workspace=2048 --saveEngine=yolov4_-1_3_320_512_dynamic.engine --fp16\n    ```\n\n## 5.3 Run the demo\n\n```sh\npython demo_trt.py <tensorRT_engine_file> <input_image> <input_H> <input_W>\n```\n\n- This demo here only works when batchSize is dynamic (1 should be within dynamic range) or batchSize=1, but you can update this demo a little for other dynamic or static batch sizes.\n    \n- Note1: input_H and input_W should agree with the input size in the original ONNX file.\n    \n- Note2: extra NMS operations are needed for the tensorRT output. This demo uses python NMS code from `tool/utils.py`.\n\n\n# 6. ONNX2Tensorflow\n\n- **First:Conversion to ONNX**\n\n    tensorflow >=2.0\n    \n    1: Thanks:github:https://github.com/onnx/onnx-tensorflow\n    \n    2: Run git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow\n    Run pip install -e .\n    \n    Note:Errors will occur when using \"pip install onnx-tf\", at least for me,it is recommended to use source code installation\n\n# 7. ONNX2TensorRT and DeepStream Inference\n  \n  1. Compile the DeepStream Nvinfer Plugin \n  \n  ```\n      cd DeepStream\n      make \n  ```\n  2. Build a TRT Engine.\n  \n   For single batch, \n   ```\n   trtexec --onnx=<onnx_file> --explicitBatch --saveEngine=<tensorRT_engine_file> --workspace=<size_in_megabytes> --fp16\n   ```\n   \n   For multi-batch, \n  ```\n  trtexec --onnx=<onnx_file> --explicitBatch --shapes=input:Xx3xHxW --optShapes=input:Xx3xHxW --maxShapes=input:Xx3xHxW --minShape=input:1x3xHxW --saveEngine=<tensorRT_engine_file> --fp16\n  ```\n  \n  Note :The maxShapes could not be larger than model original shape.\n  \n  3. Write the deepstream config file for the TRT Engine.\n  \n  \n   \nReference:\n- https://github.com/eriklindernoren/PyTorch-YOLOv3\n- https://github.com/marvis/pytorch-caffe-darknet-convert\n- https://github.com/marvis/pytorch-yolo3\n\n```\n@article{yolov4,\n  title={YOLOv4: YOLOv4: Optimal Speed and Accuracy of Object Detection},\n  author={Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao},\n  journal = {arXiv},\n  year={2020}\n}\n```\n"
        },
        {
          "name": "Use_yolov4_to_train_your_own_data.md",
          "type": "blob",
          "size": 3.357421875,
          "content": "The release of yolov4 has attracted a lot of attention, but because darknet is written in big brother c language, there are many unchanged reading of the code, so the weekend wrote a pytorch version (to rub a wave of heat). Although pytorch - yolov4 write good has been a while, but for a variety of reasons have not been validated (mainly lazy), people raised many questions to help fix many bugs, there are big brothers together to add new features, thank you for your help. These days the highest call is how to how to use their own data for training, and yesterday was the weekend, so the thing that has dragged on for a long time to do. It is not like using a lot of data, so I made a simple dataset myself\n\n\n# 1. Code Preparation\n\ngithub Cloning Code\n```\ngit clone https://github.com/Tianxiaomo/pytorch-YOLOv4.git\n```\n# 2. Data Preparation\n\nPrepare train.txt, which contains the image name and box in the following format\n\n```\nimage_path1 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\nimage_path2 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\n...\n```\n- image_path : Image Name\n- x1,y1 : Coordinates of the upper left corner\n- x2,y2 : Coordinates of the lower right corner\n- id : Object Class\n\nI use their own data is their own production of a small data set to detect a variety of coins (also 1 yuan, 50 cents, 10 cents three), why not use other things to produce data sets, no ah, only these coins on hand feel more appropriate, relatively simple compared to other things。\n\n![UTOOLS1590383513325.png](https://user-gold-cdn.xitu.io/2020/5/25/1724a3e953909b1b?w=1649&h=791&f=png&s=1290382)\n\nA total of a few prepared。\n\n# 3. Parameter Setting\n\nWhen I started training, I directly used the original parameters, batch size set to 64, ran a few epochs found that it is not right, my data is only a total of more than 20. After modifying the network update strategy, not in accordance with the step of each epoch update, using the total steps update, observe the loss seems to be able to train, so sleep, tomorrow to see how the training (the ghost knows what I changed)\n\nToday, I opened my computer and saw that what xx,loss converged to 2.e+4, which must be strange again, so I killed it. So I set the batch size to 4 directly, and can train normally。\n\n```\nCfg.batch = 4\nCfg.subdivisions = 1\n```\n\n# 4. Start training\n\n```\n python train.py -l 0.001 -g 4 -pretrained ./yolov4.conv.137.pth -classes 3 -dir /home/OCR/coins\n\n-l learning rate\n-g gpu id\n-pretrained Pre-trained backbone network, converted from yolov4.conv.137 of darknet given by AlexeyAB\n-classes NO. of classes\n-dir Training image dir\n```\n\n\nLook at the loss curve\n```\ntensorboard --logdir log --host 192.168.212.75 --port 6008\n```\n![UTOOLS1590386319240.png](https://user-gold-cdn.xitu.io/2020/5/25/1724a696148d13f3?w=1357&h=795&f=png&s=151465)\n\n# 5. Inference\n\n```\npython model.py 3 weight/Yolov4_epoch166_coins.pth data/coin2.jpg data/coins.names\n\npython model.py num_classes weightfile imagepath namefile\n```\ncoins.names\n```\n1yuan\n5jiao\n1jiao\n\n```\n\n![UTOOLS1590386705468.png](https://user-gold-cdn.xitu.io/2020/5/25/1724a6f46e826bb8?w=774&h=1377&f=png&s=1191048)\n\nThe results were poor (only 3 types of coins were available for the training data).\n\n# Attachment\n\n- coins dataset (link: https://pan.baidu.com/s/1y701NRKSdpj6UKDIH-GpqA) \n(Extraction code: j09s)\n- yolov4.conv.137.pth (Link: https://pan.baidu.com/s/1ovBie4YyVQQoUrC3AY0joA Extraction code: kcel)\n"
        },
        {
          "name": "cfg.py",
          "type": "blob",
          "size": 1.5361328125,
          "content": "# -*- coding: utf-8 -*-\n'''\n@Time          : 2020/05/06 21:05\n@Author        : Tianxiaomo\n@File          : Cfg.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n\n'''\nimport os\nfrom easydict import EasyDict\n\n\n_BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\nCfg = EasyDict()\n\nCfg.use_darknet_cfg = True\nCfg.cfgfile = os.path.join(_BASE_DIR, 'cfg', 'yolov4.cfg')\n\nCfg.batch = 64\nCfg.subdivisions = 16\nCfg.width = 608\nCfg.height = 608\nCfg.channels = 3\nCfg.momentum = 0.949\nCfg.decay = 0.0005\nCfg.angle = 0\nCfg.saturation = 1.5\nCfg.exposure = 1.5\nCfg.hue = .1\n\nCfg.learning_rate = 0.00261\nCfg.burn_in = 1000\nCfg.max_batches = 500500\nCfg.steps = [400000, 450000]\nCfg.policy = Cfg.steps\nCfg.scales = .1, .1\n\nCfg.cutmix = 0\nCfg.mosaic = 1\n\nCfg.letter_box = 0\nCfg.jitter = 0.2\nCfg.classes = 80\nCfg.track = 0\nCfg.w = Cfg.width\nCfg.h = Cfg.height\nCfg.flip = 1\nCfg.blur = 0\nCfg.gaussian = 0\nCfg.boxes = 60  # box num\nCfg.TRAIN_EPOCHS = 300\nCfg.train_label = os.path.join(_BASE_DIR, 'data', 'train.txt')\nCfg.val_label = os.path.join(_BASE_DIR, 'data' ,'val.txt')\nCfg.TRAIN_OPTIMIZER = 'adam'\n'''\nimage_path1 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\nimage_path2 x1,y1,x2,y2,id x1,y1,x2,y2,id x1,y1,x2,y2,id ...\n...\n'''\n\nif Cfg.mosaic and Cfg.cutmix:\n    Cfg.mixup = 4\nelif Cfg.cutmix:\n    Cfg.mixup = 2\nelif Cfg.mosaic:\n    Cfg.mixup = 3\n\nCfg.checkpoints = os.path.join(_BASE_DIR, 'checkpoints')\nCfg.TRAIN_TENSORBOARD_DIR = os.path.join(_BASE_DIR, 'log')\n\nCfg.iou_type = 'iou'  # 'giou', 'diou', 'ciou'\n\nCfg.keep_checkpoint_max = 10\n"
        },
        {
          "name": "cfg",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset.py",
          "type": "blob",
          "size": 16.779296875,
          "content": "# -*- coding: utf-8 -*-\n'''\n@Time          : 2020/05/06 21:09\n@Author        : Tianxiaomo\n@File          : dataset.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n\n'''\nimport os\nimport random\nimport sys\n\nimport cv2\nimport numpy as np\n\nimport torch\nfrom torch.utils.data.dataset import Dataset\n\n\ndef rand_uniform_strong(min, max):\n    if min > max:\n        swap = min\n        min = max\n        max = swap\n    return random.random() * (max - min) + min\n\n\ndef rand_scale(s):\n    scale = rand_uniform_strong(1, s)\n    if random.randint(0, 1) % 2:\n        return scale\n    return 1. / scale\n\n\ndef rand_precalc_random(min, max, random_part):\n    if max < min:\n        swap = min\n        min = max\n        max = swap\n    return (random_part * (max - min)) + min\n\n\ndef fill_truth_detection(bboxes, num_boxes, classes, flip, dx, dy, sx, sy, net_w, net_h):\n    if bboxes.shape[0] == 0:\n        return bboxes, 10000\n    np.random.shuffle(bboxes)\n    bboxes[:, 0] -= dx\n    bboxes[:, 2] -= dx\n    bboxes[:, 1] -= dy\n    bboxes[:, 3] -= dy\n\n    bboxes[:, 0] = np.clip(bboxes[:, 0], 0, sx)\n    bboxes[:, 2] = np.clip(bboxes[:, 2], 0, sx)\n\n    bboxes[:, 1] = np.clip(bboxes[:, 1], 0, sy)\n    bboxes[:, 3] = np.clip(bboxes[:, 3], 0, sy)\n\n    out_box = list(np.where(((bboxes[:, 1] == sy) & (bboxes[:, 3] == sy)) |\n                            ((bboxes[:, 0] == sx) & (bboxes[:, 2] == sx)) |\n                            ((bboxes[:, 1] == 0) & (bboxes[:, 3] == 0)) |\n                            ((bboxes[:, 0] == 0) & (bboxes[:, 2] == 0)))[0])\n    list_box = list(range(bboxes.shape[0]))\n    for i in out_box:\n        list_box.remove(i)\n    bboxes = bboxes[list_box]\n\n    if bboxes.shape[0] == 0:\n        return bboxes, 10000\n\n    bboxes = bboxes[np.where((bboxes[:, 4] < classes) & (bboxes[:, 4] >= 0))[0]]\n\n    if bboxes.shape[0] > num_boxes:\n        bboxes = bboxes[:num_boxes]\n\n    min_w_h = np.array([bboxes[:, 2] - bboxes[:, 0], bboxes[:, 3] - bboxes[:, 1]]).min()\n\n    bboxes[:, 0] *= (net_w / sx)\n    bboxes[:, 2] *= (net_w / sx)\n    bboxes[:, 1] *= (net_h / sy)\n    bboxes[:, 3] *= (net_h / sy)\n\n    if flip:\n        temp = net_w - bboxes[:, 0]\n        bboxes[:, 0] = net_w - bboxes[:, 2]\n        bboxes[:, 2] = temp\n\n    return bboxes, min_w_h\n\n\ndef rect_intersection(a, b):\n    minx = max(a[0], b[0])\n    miny = max(a[1], b[1])\n\n    maxx = min(a[2], b[2])\n    maxy = min(a[3], b[3])\n    return [minx, miny, maxx, maxy]\n\n\ndef image_data_augmentation(mat, w, h, pleft, ptop, swidth, sheight, flip, dhue, dsat, dexp, gaussian_noise, blur,\n                            truth):\n    try:\n        img = mat\n        oh, ow, _ = img.shape\n        pleft, ptop, swidth, sheight = int(pleft), int(ptop), int(swidth), int(sheight)\n        # crop\n        src_rect = [pleft, ptop, swidth + pleft, sheight + ptop]  # x1,y1,x2,y2\n        img_rect = [0, 0, ow, oh]\n        new_src_rect = rect_intersection(src_rect, img_rect)  # 交集\n\n        dst_rect = [max(0, -pleft), max(0, -ptop), max(0, -pleft) + new_src_rect[2] - new_src_rect[0],\n                    max(0, -ptop) + new_src_rect[3] - new_src_rect[1]]\n        # cv2.Mat sized\n\n        if (src_rect[0] == 0 and src_rect[1] == 0 and src_rect[2] == img.shape[0] and src_rect[3] == img.shape[1]):\n            sized = cv2.resize(img, (w, h), cv2.INTER_LINEAR)\n        else:\n            cropped = np.zeros([sheight, swidth, 3])\n            cropped[:, :, ] = np.mean(img, axis=(0, 1))\n\n            cropped[dst_rect[1]:dst_rect[3], dst_rect[0]:dst_rect[2]] = \\\n                img[new_src_rect[1]:new_src_rect[3], new_src_rect[0]:new_src_rect[2]]\n\n            # resize\n            sized = cv2.resize(cropped, (w, h), cv2.INTER_LINEAR)\n\n        # flip\n        if flip:\n            # cv2.Mat cropped\n            sized = cv2.flip(sized, 1)  # 0 - x-axis, 1 - y-axis, -1 - both axes (x & y)\n\n        # HSV augmentation\n        # cv2.COLOR_BGR2HSV, cv2.COLOR_RGB2HSV, cv2.COLOR_HSV2BGR, cv2.COLOR_HSV2RGB\n        if dsat != 1 or dexp != 1 or dhue != 0:\n            if img.shape[2] >= 3:\n                hsv_src = cv2.cvtColor(sized.astype(np.float32), cv2.COLOR_RGB2HSV)  # RGB to HSV\n                hsv = cv2.split(hsv_src)\n                hsv[1] *= dsat\n                hsv[2] *= dexp\n                hsv[0] += 179 * dhue\n                hsv_src = cv2.merge(hsv)\n                sized = np.clip(cv2.cvtColor(hsv_src, cv2.COLOR_HSV2RGB), 0, 255)  # HSV to RGB (the same as previous)\n            else:\n                sized *= dexp\n\n        if blur:\n            if blur == 1:\n                dst = cv2.GaussianBlur(sized, (17, 17), 0)\n                # cv2.bilateralFilter(sized, dst, 17, 75, 75)\n            else:\n                ksize = (blur / 2) * 2 + 1\n                dst = cv2.GaussianBlur(sized, (ksize, ksize), 0)\n\n            if blur == 1:\n                img_rect = [0, 0, sized.cols, sized.rows]\n                for b in truth:\n                    left = (b.x - b.w / 2.) * sized.shape[1]\n                    width = b.w * sized.shape[1]\n                    top = (b.y - b.h / 2.) * sized.shape[0]\n                    height = b.h * sized.shape[0]\n                    roi(left, top, width, height)\n                    roi = roi & img_rect\n                    dst[roi[0]:roi[0] + roi[2], roi[1]:roi[1] + roi[3]] = sized[roi[0]:roi[0] + roi[2],\n                                                                          roi[1]:roi[1] + roi[3]]\n\n            sized = dst\n\n        if gaussian_noise:\n            noise = np.array(sized.shape)\n            gaussian_noise = min(gaussian_noise, 127)\n            gaussian_noise = max(gaussian_noise, 0)\n            cv2.randn(noise, 0, gaussian_noise)  # mean and variance\n            sized = sized + noise\n    except:\n        print(\"OpenCV can't augment image: \" + str(w) + \" x \" + str(h))\n        sized = mat\n\n    return sized\n\n\ndef filter_truth(bboxes, dx, dy, sx, sy, xd, yd):\n    bboxes[:, 0] -= dx\n    bboxes[:, 2] -= dx\n    bboxes[:, 1] -= dy\n    bboxes[:, 3] -= dy\n\n    bboxes[:, 0] = np.clip(bboxes[:, 0], 0, sx)\n    bboxes[:, 2] = np.clip(bboxes[:, 2], 0, sx)\n\n    bboxes[:, 1] = np.clip(bboxes[:, 1], 0, sy)\n    bboxes[:, 3] = np.clip(bboxes[:, 3], 0, sy)\n\n    out_box = list(np.where(((bboxes[:, 1] == sy) & (bboxes[:, 3] == sy)) |\n                            ((bboxes[:, 0] == sx) & (bboxes[:, 2] == sx)) |\n                            ((bboxes[:, 1] == 0) & (bboxes[:, 3] == 0)) |\n                            ((bboxes[:, 0] == 0) & (bboxes[:, 2] == 0)))[0])\n    list_box = list(range(bboxes.shape[0]))\n    for i in out_box:\n        list_box.remove(i)\n    bboxes = bboxes[list_box]\n\n    bboxes[:, 0] += xd\n    bboxes[:, 2] += xd\n    bboxes[:, 1] += yd\n    bboxes[:, 3] += yd\n\n    return bboxes\n\n\ndef blend_truth_mosaic(out_img, img, bboxes, w, h, cut_x, cut_y, i_mixup,\n                       left_shift, right_shift, top_shift, bot_shift):\n    left_shift = min(left_shift, w - cut_x)\n    top_shift = min(top_shift, h - cut_y)\n    right_shift = min(right_shift, cut_x)\n    bot_shift = min(bot_shift, cut_y)\n\n    if i_mixup == 0:\n        bboxes = filter_truth(bboxes, left_shift, top_shift, cut_x, cut_y, 0, 0)\n        out_img[:cut_y, :cut_x] = img[top_shift:top_shift + cut_y, left_shift:left_shift + cut_x]\n    if i_mixup == 1:\n        bboxes = filter_truth(bboxes, cut_x - right_shift, top_shift, w - cut_x, cut_y, cut_x, 0)\n        out_img[:cut_y, cut_x:] = img[top_shift:top_shift + cut_y, cut_x - right_shift:w - right_shift]\n    if i_mixup == 2:\n        bboxes = filter_truth(bboxes, left_shift, cut_y - bot_shift, cut_x, h - cut_y, 0, cut_y)\n        out_img[cut_y:, :cut_x] = img[cut_y - bot_shift:h - bot_shift, left_shift:left_shift + cut_x]\n    if i_mixup == 3:\n        bboxes = filter_truth(bboxes, cut_x - right_shift, cut_y - bot_shift, w - cut_x, h - cut_y, cut_x, cut_y)\n        out_img[cut_y:, cut_x:] = img[cut_y - bot_shift:h - bot_shift, cut_x - right_shift:w - right_shift]\n\n    return out_img, bboxes\n\n\ndef draw_box(img, bboxes):\n    for b in bboxes:\n        img = cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 255, 0), 2)\n    return img\n\n\nclass Yolo_dataset(Dataset):\n    def __init__(self, label_path, cfg, train=True):\n        super(Yolo_dataset, self).__init__()\n        if cfg.mixup == 2:\n            print(\"cutmix=1 - isn't supported for Detector\")\n            raise\n        elif cfg.mixup == 2 and cfg.letter_box:\n            print(\"Combination: letter_box=1 & mosaic=1 - isn't supported, use only 1 of these parameters\")\n            raise\n\n        self.cfg = cfg\n        self.train = train\n\n        truth = {}\n        f = open(label_path, 'r', encoding='utf-8')\n        for line in f.readlines():\n            data = line.split(\" \")\n            truth[data[0]] = []\n            for i in data[1:]:\n                truth[data[0]].append([int(float(j)) for j in i.split(',')])\n\n        self.truth = truth\n        self.imgs = list(self.truth.keys())\n\n    def __len__(self):\n        return len(self.truth.keys())\n\n    def __getitem__(self, index):\n        if not self.train:\n            return self._get_val_item(index)\n        img_path = self.imgs[index]\n        bboxes = np.array(self.truth.get(img_path), dtype=np.float)\n        img_path = os.path.join(self.cfg.dataset_dir, img_path)\n        use_mixup = self.cfg.mixup\n        if random.randint(0, 1):\n            use_mixup = 0\n\n        if use_mixup == 3:\n            min_offset = 0.2\n            cut_x = random.randint(int(self.cfg.w * min_offset), int(self.cfg.w * (1 - min_offset)))\n            cut_y = random.randint(int(self.cfg.h * min_offset), int(self.cfg.h * (1 - min_offset)))\n\n        r1, r2, r3, r4, r_scale = 0, 0, 0, 0, 0\n        dhue, dsat, dexp, flip, blur = 0, 0, 0, 0, 0\n        gaussian_noise = 0\n\n        out_img = np.zeros([self.cfg.h, self.cfg.w, 3])\n        out_bboxes = []\n\n        for i in range(use_mixup + 1):\n            if i != 0:\n                img_path = random.choice(list(self.truth.keys()))\n                bboxes = np.array(self.truth.get(img_path), dtype=np.float)\n                img_path = os.path.join(self.cfg.dataset_dir, img_path)\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            if img is None:\n                continue\n            oh, ow, oc = img.shape\n            dh, dw, dc = np.array(np.array([oh, ow, oc]) * self.cfg.jitter, dtype=np.int)\n\n            dhue = rand_uniform_strong(-self.cfg.hue, self.cfg.hue)\n            dsat = rand_scale(self.cfg.saturation)\n            dexp = rand_scale(self.cfg.exposure)\n\n            pleft = random.randint(-dw, dw)\n            pright = random.randint(-dw, dw)\n            ptop = random.randint(-dh, dh)\n            pbot = random.randint(-dh, dh)\n\n            flip = random.randint(0, 1) if self.cfg.flip else 0\n\n            if (self.cfg.blur):\n                tmp_blur = random.randint(0, 2)  # 0 - disable, 1 - blur background, 2 - blur the whole image\n                if tmp_blur == 0:\n                    blur = 0\n                elif tmp_blur == 1:\n                    blur = 1\n                else:\n                    blur = self.cfg.blur\n\n            if self.cfg.gaussian and random.randint(0, 1):\n                gaussian_noise = self.cfg.gaussian\n            else:\n                gaussian_noise = 0\n\n            if self.cfg.letter_box:\n                img_ar = ow / oh\n                net_ar = self.cfg.w / self.cfg.h\n                result_ar = img_ar / net_ar\n                # print(\" ow = %d, oh = %d, w = %d, h = %d, img_ar = %f, net_ar = %f, result_ar = %f \\n\", ow, oh, w, h, img_ar, net_ar, result_ar);\n                if result_ar > 1:  # sheight - should be increased\n                    oh_tmp = ow / net_ar\n                    delta_h = (oh_tmp - oh) / 2\n                    ptop = ptop - delta_h\n                    pbot = pbot - delta_h\n                    # print(\" result_ar = %f, oh_tmp = %f, delta_h = %d, ptop = %f, pbot = %f \\n\", result_ar, oh_tmp, delta_h, ptop, pbot);\n                else:  # swidth - should be increased\n                    ow_tmp = oh * net_ar\n                    delta_w = (ow_tmp - ow) / 2\n                    pleft = pleft - delta_w\n                    pright = pright - delta_w\n                    # printf(\" result_ar = %f, ow_tmp = %f, delta_w = %d, pleft = %f, pright = %f \\n\", result_ar, ow_tmp, delta_w, pleft, pright);\n\n            swidth = ow - pleft - pright\n            sheight = oh - ptop - pbot\n\n            truth, min_w_h = fill_truth_detection(bboxes, self.cfg.boxes, self.cfg.classes, flip, pleft, ptop, swidth,\n                                                  sheight, self.cfg.w, self.cfg.h)\n            if (min_w_h / 8) < blur and blur > 1:  # disable blur if one of the objects is too small\n                blur = min_w_h / 8\n\n            ai = image_data_augmentation(img, self.cfg.w, self.cfg.h, pleft, ptop, swidth, sheight, flip,\n                                         dhue, dsat, dexp, gaussian_noise, blur, truth)\n\n            if use_mixup == 0:\n                out_img = ai\n                out_bboxes = truth\n            if use_mixup == 1:\n                if i == 0:\n                    old_img = ai.copy()\n                    old_truth = truth.copy()\n                elif i == 1:\n                    out_img = cv2.addWeighted(ai, 0.5, old_img, 0.5)\n                    out_bboxes = np.concatenate([old_truth, truth], axis=0)\n            elif use_mixup == 3:\n                if flip:\n                    tmp = pleft\n                    pleft = pright\n                    pright = tmp\n\n                left_shift = int(min(cut_x, max(0, (-int(pleft) * self.cfg.w / swidth))))\n                top_shift = int(min(cut_y, max(0, (-int(ptop) * self.cfg.h / sheight))))\n\n                right_shift = int(min((self.cfg.w - cut_x), max(0, (-int(pright) * self.cfg.w / swidth))))\n                bot_shift = int(min(self.cfg.h - cut_y, max(0, (-int(pbot) * self.cfg.h / sheight))))\n\n                out_img, out_bbox = blend_truth_mosaic(out_img, ai, truth.copy(), self.cfg.w, self.cfg.h, cut_x,\n                                                       cut_y, i, left_shift, right_shift, top_shift, bot_shift)\n                out_bboxes.append(out_bbox)\n                # print(img_path)\n        if use_mixup == 3:\n            out_bboxes = np.concatenate(out_bboxes, axis=0)\n        out_bboxes1 = np.zeros([self.cfg.boxes, 5])\n        out_bboxes1[:min(out_bboxes.shape[0], self.cfg.boxes)] = out_bboxes[:min(out_bboxes.shape[0], self.cfg.boxes)]\n        return out_img, out_bboxes1\n\n    def _get_val_item(self, index):\n        \"\"\"\n        \"\"\"\n        img_path = self.imgs[index]\n        bboxes_with_cls_id = np.array(self.truth.get(img_path), dtype=np.float)\n        img = cv2.imread(os.path.join(self.cfg.dataset_dir, img_path))\n        # img_height, img_width = img.shape[:2]\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        # img = cv2.resize(img, (self.cfg.w, self.cfg.h))\n        # img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n        num_objs = len(bboxes_with_cls_id)\n        target = {}\n        # boxes to coco format\n        boxes = bboxes_with_cls_id[...,:4]\n        boxes[..., 2:] = boxes[..., 2:] - boxes[..., :2]  # box width, box height\n        target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n        target['labels'] = torch.as_tensor(bboxes_with_cls_id[...,-1].flatten(), dtype=torch.int64)\n        target['image_id'] = torch.tensor([get_image_id(img_path)])\n        target['area'] = (target['boxes'][:,3])*(target['boxes'][:,2])\n        target['iscrowd'] = torch.zeros((num_objs,), dtype=torch.int64)\n        return img, target\n\n\ndef get_image_id(filename:str) -> int:\n    \"\"\"\n    Convert a string to a integer.\n    Make sure that the images and the `image_id`s are in one-one correspondence.\n    There are already `image_id`s in annotations of the COCO dataset,\n    in which case this function is unnecessary.\n    For creating one's own `get_image_id` function, one can refer to\n    https://github.com/google/automl/blob/master/efficientdet/dataset/create_pascal_tfrecord.py#L86\n    or refer to the following code (where the filenames are like 'level1_123.jpg')\n    >>> lv, no = os.path.splitext(os.path.basename(filename))[0].split(\"_\")\n    >>> lv = lv.replace(\"level\", \"\")\n    >>> no = f\"{int(no):04d}\"\n    >>> return int(lv+no)\n    \"\"\"\n    # raise NotImplementedError(\"Create your own 'get_image_id' function\")\n    # lv, no = os.path.splitext(os.path.basename(filename))[0].split(\"_\")\n    # lv = lv.replace(\"level\", \"\")\n    # no = f\"{int(no):04d}\"\n    # return int(lv+no)\n\n    print(\"You could also create your own 'get_image_id' function.\")\n    # print(filename)\n    parts = filename.split('/')\n    id = int(parts[-1][0:-4])\n    # print(id)\n    return id\n\n\nif __name__ == \"__main__\":\n    from cfg import Cfg\n    import matplotlib.pyplot as plt\n\n    random.seed(2020)\n    np.random.seed(2020)\n    Cfg.dataset_dir = '/mnt/e/Dataset'\n    dataset = Yolo_dataset(Cfg.train_label, Cfg)\n    for i in range(100):\n        out_img, out_bboxes = dataset.__getitem__(i)\n        a = draw_box(out_img.copy(), out_bboxes.astype(np.int32))\n        plt.imshow(a.astype(np.int32))\n        plt.show()\n"
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 4.681640625,
          "content": "# -*- coding: utf-8 -*-\n'''\n@Time          : 20/04/25 15:49\n@Author        : huguanghao\n@File          : demo.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n'''\n\n# import sys\n# import time\n# from PIL import Image, ImageDraw\n# from models.tiny_yolo import TinyYoloNet\nfrom tool.utils import *\nfrom tool.torch_utils import *\nfrom tool.darknet2pytorch import Darknet\nimport torch\nimport argparse\n\n\"\"\"hyper parameters\"\"\"\nuse_cuda = True\n\ndef detect_cv2(cfgfile, weightfile, imgfile):\n    import cv2\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print('Loading weights from %s... Done!' % (weightfile))\n\n    if use_cuda:\n        m.cuda()\n\n    num_classes = m.num_classes\n    if num_classes == 20:\n        namesfile = 'data/voc.names'\n    elif num_classes == 80:\n        namesfile = 'data/coco.names'\n    else:\n        namesfile = 'data/x.names'\n    class_names = load_class_names(namesfile)\n\n    img = cv2.imread(imgfile)\n    sized = cv2.resize(img, (m.width, m.height))\n    sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n\n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.4, 0.6, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print('%s: Predicted in %f seconds.' % (imgfile, (finish - start)))\n\n    plot_boxes_cv2(img, boxes[0], savename='predictions.jpg', class_names=class_names)\n\n\ndef detect_cv2_camera(cfgfile, weightfile):\n    import cv2\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    if args.torch:\n        m.load_state_dict(torch.load(weightfile))\n    else:\n        m.load_weights(weightfile)\n    print('Loading weights from %s... Done!' % (weightfile))\n\n    if use_cuda:\n        m.cuda()\n\n    cap = cv2.VideoCapture(0)\n    # cap = cv2.VideoCapture(\"./test.mp4\")\n    cap.set(3, 1280)\n    cap.set(4, 720)\n    print(\"Starting the YOLO loop...\")\n\n    num_classes = m.num_classes\n    if num_classes == 20:\n        namesfile = 'data/voc.names'\n    elif num_classes == 80:\n        namesfile = 'data/coco.names'\n    else:\n        namesfile = 'data/x.names'\n    class_names = load_class_names(namesfile)\n\n    while True:\n        ret, img = cap.read()\n        sized = cv2.resize(img, (m.width, m.height))\n        sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n\n        start = time.time()\n        boxes = do_detect(m, sized, 0.4, 0.6, use_cuda)\n        finish = time.time()\n        print('Predicted in %f seconds.' % (finish - start))\n\n        result_img = plot_boxes_cv2(img, boxes[0], savename=None, class_names=class_names)\n\n        cv2.imshow('Yolo demo', result_img)\n        cv2.waitKey(1)\n\n    cap.release()\n\n\ndef detect_skimage(cfgfile, weightfile, imgfile):\n    from skimage import io\n    from skimage.transform import resize\n    m = Darknet(cfgfile)\n\n    m.print_network()\n    m.load_weights(weightfile)\n    print('Loading weights from %s... Done!' % (weightfile))\n\n    if use_cuda:\n        m.cuda()\n\n    num_classes = m.num_classes\n    if num_classes == 20:\n        namesfile = 'data/voc.names'\n    elif num_classes == 80:\n        namesfile = 'data/coco.names'\n    else:\n        namesfile = 'data/x.names'\n    class_names = load_class_names(namesfile)\n\n    img = io.imread(imgfile)\n    sized = resize(img, (m.width, m.height)) * 255\n\n    for i in range(2):\n        start = time.time()\n        boxes = do_detect(m, sized, 0.4, 0.4, use_cuda)\n        finish = time.time()\n        if i == 1:\n            print('%s: Predicted in %f seconds.' % (imgfile, (finish - start)))\n\n    plot_boxes_cv2(img, boxes, savename='predictions.jpg', class_names=class_names)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser('Test your image or video by trained model.')\n    parser.add_argument('-cfgfile', type=str, default='./cfg/yolov4.cfg',\n                        help='path of cfg file', dest='cfgfile')\n    parser.add_argument('-weightfile', type=str,\n                        default='./checkpoints/Yolov4_epoch1.pth',\n                        help='path of trained model.', dest='weightfile')\n    parser.add_argument('-imgfile', type=str,\n                        default='./data/mscoco2017/train2017/190109_180343_00154162.jpg',\n                        help='path of your image file.', dest='imgfile')\n    parser.add_argument('-torch', type=bool, default=false,\n                        help='use torch weights')\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == '__main__':\n    args = get_args()\n    if args.imgfile:\n        detect_cv2(args.cfgfile, args.weightfile, args.imgfile)\n        # detect_imges(args.cfgfile, args.weightfile)\n        # detect_cv2(args.cfgfile, args.weightfile, args.imgfile)\n        # detect_skimage(args.cfgfile, args.weightfile, args.imgfile)\n    else:\n        detect_cv2_camera(args.cfgfile, args.weightfile)\n"
        },
        {
          "name": "demo_darknet2onnx.py",
          "type": "blob",
          "size": 2.0673828125,
          "content": "import sys\nimport onnx\nimport os\nimport argparse\nimport numpy as np\nimport cv2\nimport onnxruntime\n\nfrom tool.utils import *\nfrom tool.darknet2onnx import *\n\n\ndef main(cfg_file, namesfile, weight_file, image_path, batch_size):\n\n    if batch_size <= 0:\n        onnx_path_demo = transform_to_onnx(cfg_file, weight_file, batch_size)\n    else:\n        # Transform to onnx as specified batch size\n        transform_to_onnx(cfg_file, weight_file, batch_size)\n        # Transform to onnx as demo\n        onnx_path_demo = transform_to_onnx(cfg_file, weight_file, 1)\n\n    session = onnxruntime.InferenceSession(onnx_path_demo)\n    # session = onnx.load(onnx_path)\n    print(\"The model expects input shape: \", session.get_inputs()[0].shape)\n\n    image_src = cv2.imread(image_path)\n    detect(session, image_src, namesfile)\n\n\n\ndef detect(session, image_src, namesfile):\n    IN_IMAGE_H = session.get_inputs()[0].shape[2]\n    IN_IMAGE_W = session.get_inputs()[0].shape[3]\n\n    # Input\n    resized = cv2.resize(image_src, (IN_IMAGE_W, IN_IMAGE_H), interpolation=cv2.INTER_LINEAR)\n    img_in = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_in = np.transpose(img_in, (2, 0, 1)).astype(np.float32)\n    img_in = np.expand_dims(img_in, axis=0)\n    img_in /= 255.0\n    print(\"Shape of the network input: \", img_in.shape)\n\n    # Compute\n    input_name = session.get_inputs()[0].name\n\n    outputs = session.run(None, {input_name: img_in})\n\n    boxes = post_processing(img_in, 0.4, 0.6, outputs)\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(image_src, boxes[0], savename='predictions_onnx.jpg', class_names=class_names)\n\n\n\nif __name__ == '__main__':\n    print(\"Converting to onnx and running demo ...\")\n    if len(sys.argv) == 6:\n        cfg_file = sys.argv[1]\n        namesfile = sys.argv[2]\n        weight_file = sys.argv[3]\n        image_path = sys.argv[4]\n        batch_size = int(sys.argv[5])\n        main(cfg_file, namesfile, weight_file, image_path, batch_size)\n    else:\n        print('Please run this way:\\n')\n        print('  python demo_onnx.py <cfgFile> <namesFile> <weightFile> <imageFile> <batchSize>')\n"
        },
        {
          "name": "demo_pytorch2onnx.py",
          "type": "blob",
          "size": 3.375,
          "content": "import sys\nimport onnx\nimport os\nimport argparse\nimport numpy as np\nimport cv2\nimport onnxruntime\nimport torch\n\nfrom tool.utils import *\nfrom models import Yolov4\nfrom demo_darknet2onnx import detect\n\n\ndef transform_to_onnx(weight_file, batch_size, n_classes, IN_IMAGE_H, IN_IMAGE_W):\n    \n    model = Yolov4(n_classes=n_classes, inference=True)\n\n    pretrained_dict = torch.load(weight_file, map_location=torch.device('cuda'))\n    model.load_state_dict(pretrained_dict)\n\n    input_names = [\"input\"]\n    output_names = ['boxes', 'confs']\n\n    dynamic = False\n    if batch_size <= 0:\n        dynamic = True\n\n    if dynamic:\n        x = torch.randn((1, 3, IN_IMAGE_H, IN_IMAGE_W), requires_grad=True)\n        onnx_file_name = \"yolov4_-1_3_{}_{}_dynamic.onnx\".format(IN_IMAGE_H, IN_IMAGE_W)\n        dynamic_axes = {\"input\": {0: \"batch_size\"}, \"boxes\": {0: \"batch_size\"}, \"confs\": {0: \"batch_size\"}}\n        # Export the model\n        print('Export the onnx model ...')\n        torch.onnx.export(model,\n                          x,\n                          onnx_file_name,\n                          export_params=True,\n                          opset_version=11,\n                          do_constant_folding=True,\n                          input_names=input_names, output_names=output_names,\n                          dynamic_axes=dynamic_axes)\n\n        print('Onnx model exporting done')\n        return onnx_file_name\n\n    else:\n        x = torch.randn((batch_size, 3, IN_IMAGE_H, IN_IMAGE_W), requires_grad=True)\n        onnx_file_name = \"yolov4_{}_3_{}_{}_static.onnx\".format(batch_size, IN_IMAGE_H, IN_IMAGE_W)\n        # Export the model\n        print('Export the onnx model ...')\n        torch.onnx.export(model,\n                          x,\n                          onnx_file_name,\n                          export_params=True,\n                          opset_version=11,\n                          do_constant_folding=True,\n                          input_names=input_names, output_names=output_names,\n                          dynamic_axes=None)\n\n        print('Onnx model exporting done')\n        return onnx_file_name\n    \n\n\ndef main(weight_file, image_path, batch_size, n_classes, IN_IMAGE_H, IN_IMAGE_W):\n\n    if batch_size <= 0:\n        onnx_path_demo = transform_to_onnx(weight_file, batch_size, n_classes, IN_IMAGE_H, IN_IMAGE_W)\n    else:\n        # Transform to onnx as specified batch size\n        transform_to_onnx(weight_file, batch_size, n_classes, IN_IMAGE_H, IN_IMAGE_W)\n        # Transform to onnx for demo\n        onnx_path_demo = transform_to_onnx(weight_file, 1, n_classes, IN_IMAGE_H, IN_IMAGE_W)\n\n    session = onnxruntime.InferenceSession(onnx_path_demo)\n    # session = onnx.load(onnx_path)\n    print(\"The model expects input shape: \", session.get_inputs()[0].shape)\n\n    image_src = cv2.imread(image_path)\n    detect(session, image_src)\n\n\n\nif __name__ == '__main__':\n    print(\"Converting to onnx and running demo ...\")\n    if len(sys.argv) == 7:\n        \n        weight_file = sys.argv[1]\n        image_path = sys.argv[2]\n        batch_size = int(sys.argv[3])\n        n_classes = int(sys.argv[4])\n        IN_IMAGE_H = int(sys.argv[5])\n        IN_IMAGE_W = int(sys.argv[6])\n\n        main(weight_file, image_path, batch_size, n_classes, IN_IMAGE_H, IN_IMAGE_W)\n    else:\n        print('Please run this way:\\n')\n        print('  python demo_onnx.py <weight_file> <image_path> <batch_size> <n_classes> <IN_IMAGE_H> <IN_IMAGE_W>')\n"
        },
        {
          "name": "demo_tensorflow.py",
          "type": "blob",
          "size": 2.904296875,
          "content": "import sys\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\n\nimport cv2\nfrom tool.utils import post_processing, load_class_names, plot_boxes_cv2\n\n\ndef demo_tensorflow(tfpb_file=\"./weight/yolov4.pb\", image_path=None, print_sensor_name=False):\n    graph_name = 'yolov4'\n    tf.compat.v1.disable_eager_execution()\n    with tf.compat.v1.Session() as persisted_sess:\n        print(\"loading graph...\")\n        with gfile.FastGFile(tfpb_file, 'rb') as f:\n            graph_def = tf.compat.v1.GraphDef()\n            graph_def.ParseFromString(f.read())\n\n        persisted_sess.graph.as_default()\n        tf.import_graph_def(graph_def, name=graph_name)\n\n        # print all sensor_name\n        if print_sensor_name:\n            tensor_name_list = [tensor.name for tensor in tf.compat.v1.get_default_graph().as_graph_def().node]\n            for tensor_name in tensor_name_list:\n                print(tensor_name)\n\n        inp = persisted_sess.graph.get_tensor_by_name(graph_name + '/' + 'input:0')\n        print(inp.shape)\n        out1 = persisted_sess.graph.get_tensor_by_name(graph_name + '/' + 'output_1:0')\n        out2 = persisted_sess.graph.get_tensor_by_name(graph_name + '/' + 'output_2:0')\n        out3 = persisted_sess.graph.get_tensor_by_name(graph_name + '/' + 'output_3:0')\n        print(out1.shape, out2.shape, out3.shape)\n\n        # image_src = np.random.rand(1, 3, 608, 608).astype(np.float32)  # input image\n        # Input\n        image_src = cv2.imread(image_path)\n        resized = cv2.resize(image_src, (inp.shape[2], inp.shape[3]), interpolation=cv2.INTER_LINEAR)\n        img_in = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n        img_in = np.transpose(img_in, (2, 0, 1)).astype(np.float32)\n        img_in = np.expand_dims(img_in, axis=0)\n        img_in /= 255.0\n        print(\"Shape of the network input: \", img_in.shape)\n\n        feed_dict = {inp: img_in}\n\n        outputs = persisted_sess.run([out1, out2, out3], feed_dict)\n        print(outputs[0].shape)\n        print(outputs[1].shape)\n        print(outputs[2].shape)\n\n        boxes = post_processing(img_in, 0.4, outputs)\n\n        num_classes = 80\n        if num_classes == 20:\n            namesfile = 'data/voc.names'\n        elif num_classes == 80:\n            namesfile = 'data/coco.names'\n        else:\n            namesfile = 'data/names'\n\n        class_names = load_class_names(namesfile)\n        result = plot_boxes_cv2(image_src, boxes, savename=None, class_names=class_names)\n        cv2.imshow(\"tensorflow predicted\", result)\n        cv2.waitKey()\n\n\nif __name__ == '__main__':\n    if len(sys.argv) == 1:\n        sys.argv.append('weight/yolov4.pb')\n        sys.argv.append('data/dog.jpg')\n    if len(sys.argv) == 3:\n        tfpbfile = sys.argv[1]\n        image_path = sys.argv[2]\n        demo_tensorflow(tfpbfile, image_path)\n    else:\n        print('Please execute this script this way:\\n')\n        print('  python demo_tensorflow.py <tfpbfile> <imageFile>')\n"
        },
        {
          "name": "demo_trt.py",
          "type": "blob",
          "size": 7.0224609375,
          "content": "import sys\nimport os\nimport time\nimport argparse\nimport numpy as np\nimport cv2\n# from PIL import Image\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\n\nfrom tool.utils import *\n\ntry:\n    # Sometimes python2 does not understand FileNotFoundError\n    FileNotFoundError\nexcept NameError:\n    FileNotFoundError = IOError\n\ndef GiB(val):\n    return val * 1 << 30\n\ndef find_sample_data(description=\"Runs a TensorRT Python sample\", subfolder=\"\", find_files=[]):\n    '''\n    Parses sample arguments.\n    Args:\n        description (str): Description of the sample.\n        subfolder (str): The subfolder containing data relevant to this sample\n        find_files (str): A list of filenames to find. Each filename will be replaced with an absolute path.\n    Returns:\n        str: Path of data directory.\n    Raises:\n        FileNotFoundError\n    '''\n\n    # Standard command-line arguments for all samples.\n    kDEFAULT_DATA_ROOT = os.path.join(os.sep, \"usr\", \"src\", \"tensorrt\", \"data\")\n    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"-d\", \"--datadir\", help=\"Location of the TensorRT sample data directory.\", default=kDEFAULT_DATA_ROOT)\n    args, unknown_args = parser.parse_known_args()\n\n    # If data directory is not specified, use the default.\n    data_root = args.datadir\n    # If the subfolder exists, append it to the path, otherwise use the provided path as-is.\n    subfolder_path = os.path.join(data_root, subfolder)\n    data_path = subfolder_path\n    if not os.path.exists(subfolder_path):\n        print(\"WARNING: \" + subfolder_path + \" does not exist. Trying \" + data_root + \" instead.\")\n        data_path = data_root\n\n    # Make sure data directory exists.\n    if not (os.path.exists(data_path)):\n        raise FileNotFoundError(data_path + \" does not exist. Please provide the correct data path with the -d option.\")\n\n    # Find all requested files.\n    for index, f in enumerate(find_files):\n        find_files[index] = os.path.abspath(os.path.join(data_path, f))\n        if not os.path.exists(find_files[index]):\n            raise FileNotFoundError(find_files[index] + \" does not exist. Please provide the correct data path with the -d option.\")\n\n    return data_path, find_files\n\n# Simple helper data class that's a little nicer to use than a 2-tuple.\nclass HostDeviceMem(object):\n    def __init__(self, host_mem, device_mem):\n        self.host = host_mem\n        self.device = device_mem\n\n    def __str__(self):\n        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n\n    def __repr__(self):\n        return self.__str__()\n\n# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\ndef allocate_buffers(engine, batch_size):\n    inputs = []\n    outputs = []\n    bindings = []\n    stream = cuda.Stream()\n    for binding in engine:\n\n        size = trt.volume(engine.get_binding_shape(binding)) * batch_size\n        dims = engine.get_binding_shape(binding)\n        \n        # in case batch dimension is -1 (dynamic)\n        if dims[0] < 0:\n            size *= -1\n        \n        dtype = trt.nptype(engine.get_binding_dtype(binding))\n        # Allocate host and device buffers\n        host_mem = cuda.pagelocked_empty(size, dtype)\n        device_mem = cuda.mem_alloc(host_mem.nbytes)\n        # Append the device buffer to device bindings.\n        bindings.append(int(device_mem))\n        # Append to the appropriate list.\n        if engine.binding_is_input(binding):\n            inputs.append(HostDeviceMem(host_mem, device_mem))\n        else:\n            outputs.append(HostDeviceMem(host_mem, device_mem))\n    return inputs, outputs, bindings, stream\n\n# This function is generalized for multiple inputs/outputs.\n# inputs and outputs are expected to be lists of HostDeviceMem objects.\ndef do_inference(context, bindings, inputs, outputs, stream):\n    # Transfer input data to the GPU.\n    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n    # Run inference.\n    context.execute_async(bindings=bindings, stream_handle=stream.handle)\n    # Transfer predictions back from the GPU.\n    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n    # Synchronize the stream\n    stream.synchronize()\n    # Return only the host outputs.\n    return [out.host for out in outputs]\n\n\nTRT_LOGGER = trt.Logger()\n\ndef main(engine_path, image_path, image_size):\n    with get_engine(engine_path) as engine, engine.create_execution_context() as context:\n        buffers = allocate_buffers(engine, 1)\n        IN_IMAGE_H, IN_IMAGE_W = image_size\n        context.set_binding_shape(0, (1, 3, IN_IMAGE_H, IN_IMAGE_W))\n\n        image_src = cv2.imread(image_path)\n\n        num_classes = 80\n\n        for i in range(2):  # This 'for' loop is for speed check\n                            # Because the first iteration is usually longer\n            boxes = detect(context, buffers, image_src, image_size, num_classes)\n\n        if num_classes == 20:\n            namesfile = 'data/voc.names'\n        elif num_classes == 80:\n            namesfile = 'data/coco.names'\n        else:\n            namesfile = 'data/names'\n\n        class_names = load_class_names(namesfile)\n        plot_boxes_cv2(image_src, boxes[0], savename='predictions_trt.jpg', class_names=class_names)\n\n\ndef get_engine(engine_path):\n    # If a serialized engine exists, use it instead of building an engine.\n    print(\"Reading engine from file {}\".format(engine_path))\n    with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n        return runtime.deserialize_cuda_engine(f.read())\n\n\n\ndef detect(context, buffers, image_src, image_size, num_classes):\n    IN_IMAGE_H, IN_IMAGE_W = image_size\n\n    ta = time.time()\n    # Input\n    resized = cv2.resize(image_src, (IN_IMAGE_W, IN_IMAGE_H), interpolation=cv2.INTER_LINEAR)\n    img_in = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_in = np.transpose(img_in, (2, 0, 1)).astype(np.float32)\n    img_in = np.expand_dims(img_in, axis=0)\n    img_in /= 255.0\n    img_in = np.ascontiguousarray(img_in)\n    print(\"Shape of the network input: \", img_in.shape)\n    # print(img_in)\n\n    inputs, outputs, bindings, stream = buffers\n    print('Length of inputs: ', len(inputs))\n    inputs[0].host = img_in\n\n    trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n\n    print('Len of outputs: ', len(trt_outputs))\n\n    trt_outputs[0] = trt_outputs[0].reshape(1, -1, 1, 4)\n    trt_outputs[1] = trt_outputs[1].reshape(1, -1, num_classes)\n\n    tb = time.time()\n\n    print('-----------------------------------')\n    print('    TRT inference time: %f' % (tb - ta))\n    print('-----------------------------------')\n\n    boxes = post_processing(img_in, 0.4, 0.6, trt_outputs)\n\n    return boxes\n\n\n\nif __name__ == '__main__':\n    engine_path = sys.argv[1]\n    image_path = sys.argv[2]\n    \n    if len(sys.argv) < 4:\n        image_size = (416, 416)\n    elif len(sys.argv) < 5:\n        image_size = (int(sys.argv[3]), int(sys.argv[3]))\n    else:\n        image_size = (int(sys.argv[3]), int(sys.argv[4]))\n    \n    main(engine_path, image_path, image_size)\n"
        },
        {
          "name": "evaluate_on_coco.py",
          "type": "blob",
          "size": 11.537109375,
          "content": "\"\"\"\nA script to evaluate the model's performance using pre-trained weights using COCO API.\nExample usage: python evaluate_on_coco.py -dir D:\\cocoDataset\\val2017\\val2017 -gta D:\\cocoDataset\\annotatio\nns_trainval2017\\annotations\\instances_val2017.json -c cfg/yolov4-smaller-input.cfg -g 0\nExplanation: set where your images can be found using -dir, then use -gta to point to the ground truth annotations file\nand finally -c to point to the config file you want to use to load the network using.\n\"\"\"\n\nimport argparse\nimport datetime\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom PIL import Image, ImageDraw\nfrom easydict import EasyDict as edict\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom cfg import Cfg\nfrom tool.darknet2pytorch import Darknet\nfrom tool.utils import load_class_names\nfrom tool.torch_utils import do_detect\n\n\ndef get_class_name(cat):\n    class_names = load_class_names(\"./data/coco.names\")\n    if cat >= 1 and cat <= 11:\n        cat = cat - 1\n    elif cat >= 13 and cat <= 25:\n        cat = cat - 2\n    elif cat >= 27 and cat <= 28:\n        cat = cat - 3\n    elif cat >= 31 and cat <= 44:\n        cat = cat - 5\n    elif cat >= 46 and cat <= 65:\n        cat = cat - 6\n    elif cat == 67:\n        cat = cat - 7\n    elif cat == 70:\n        cat = cat - 9\n    elif cat >= 72 and cat <= 82:\n        cat = cat - 10\n    elif cat >= 84 and cat <= 90:\n        cat = cat - 11\n    return class_names[cat]\n\ndef convert_cat_id_and_reorientate_bbox(single_annotation):\n    cat = single_annotation['category_id']\n    bbox = single_annotation['bbox']\n    x, y, w, h = bbox\n    x1, y1, x2, y2 = x - w / 2, y - h / 2, x + w / 2, y + h / 2\n    if 0 <= cat <= 10:\n        cat = cat + 1\n    elif 11 <= cat <= 23:\n        cat = cat + 2\n    elif 24 <= cat <= 25:\n        cat = cat + 3\n    elif 26 <= cat <= 39:\n        cat = cat + 5\n    elif 40 <= cat <= 59:\n        cat = cat + 6\n    elif cat == 60:\n        cat = cat + 7\n    elif cat == 61:\n        cat = cat + 9\n    elif 62 <= cat <= 72:\n        cat = cat + 10\n    elif 73 <= cat <= 79:\n        cat = cat + 11\n    single_annotation['category_id'] = cat\n    single_annotation['bbox'] = [x1, y1, w, h]\n    return single_annotation\n\n\n\ndef myconverter(obj):\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, datetime.datetime):\n        return obj.__str__()\n    else:\n        return obj\n\ndef evaluate_on_coco(cfg, resFile):\n    annType = \"bbox\"  # specify type here\n    with open(resFile, 'r') as f:\n        unsorted_annotations = json.load(f)\n    sorted_annotations = list(sorted(unsorted_annotations, key=lambda single_annotation: single_annotation[\"image_id\"]))\n    sorted_annotations = list(map(convert_cat_id_and_reorientate_bbox, sorted_annotations))\n    reshaped_annotations = defaultdict(list)\n    for annotation in sorted_annotations:\n        reshaped_annotations[annotation['image_id']].append(annotation)\n\n    with open('temp.json', 'w') as f:\n        json.dump(sorted_annotations, f)\n\n    cocoGt = COCO(cfg.gt_annotations_path)\n    cocoDt = cocoGt.loadRes('temp.json')\n\n    with open(cfg.gt_annotations_path, 'r') as f:\n        gt_annotation_raw = json.load(f)\n        gt_annotation_raw_images = gt_annotation_raw[\"images\"]\n        gt_annotation_raw_labels = gt_annotation_raw[\"annotations\"]\n\n    rgb_label = (255, 0, 0)\n    rgb_pred = (0, 255, 0)\n\n    for i, image_id in enumerate(reshaped_annotations):\n        image_annotations = reshaped_annotations[image_id]\n        gt_annotation_image_raw = list(filter(\n            lambda image_json: image_json['id'] == image_id, gt_annotation_raw_images\n        ))\n        gt_annotation_labels_raw = list(filter(\n            lambda label_json: label_json['image_id'] == image_id, gt_annotation_raw_labels\n        ))\n        if len(gt_annotation_image_raw) == 1:\n            image_path = os.path.join(cfg.dataset_dir, gt_annotation_image_raw[0][\"file_name\"])\n            actual_image = Image.open(image_path).convert('RGB')\n            draw = ImageDraw.Draw(actual_image)\n\n            for annotation in image_annotations:\n                x1_pred, y1_pred, w, h = annotation['bbox']\n                x2_pred, y2_pred = x1_pred + w, y1_pred + h\n                cls_id = annotation['category_id']\n                label = get_class_name(cls_id)\n                draw.text((x1_pred, y1_pred), label, fill=rgb_pred)\n                draw.rectangle([x1_pred, y1_pred, x2_pred, y2_pred], outline=rgb_pred)\n            for annotation in gt_annotation_labels_raw:\n                x1_truth, y1_truth, w, h = annotation['bbox']\n                x2_truth, y2_truth = x1_truth + w, y1_truth + h\n                cls_id = annotation['category_id']\n                label = get_class_name(cls_id)\n                draw.text((x1_truth, y1_truth), label, fill=rgb_label)\n                draw.rectangle([x1_truth, y1_truth, x2_truth, y2_truth], outline=rgb_label)\n            actual_image.save(\"./data/outcome/predictions_{}\".format(gt_annotation_image_raw[0][\"file_name\"]))\n        else:\n            print('please check')\n            break\n        if (i + 1) % 100 == 0: # just see first 100\n            break\n\n    imgIds = sorted(cocoGt.getImgIds())\n    cocoEval = COCOeval(cocoGt, cocoDt, annType)\n    cocoEval.params.imgIds = imgIds\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\ndef test(model, annotations, cfg):\n    if not annotations[\"images\"]:\n        print(\"Annotations do not have 'images' key\")\n        return\n    images = annotations[\"images\"]\n    # images = images[:10]\n    resFile = 'data/coco_val_outputs.json'\n\n    if torch.cuda.is_available():\n        use_cuda = 1\n    else:\n        use_cuda = 0\n\n    # do one forward pass first to circumvent cold start\n    throwaway_image = Image.open('data/dog.jpg').convert('RGB').resize((model.width, model.height))\n    do_detect(model, throwaway_image, 0.5, 80, 0.4, use_cuda)\n    boxes_json = []\n\n    for i, image_annotation in enumerate(images):\n        logging.info(\"currently on image: {}/{}\".format(i + 1, len(images)))\n        image_file_name = image_annotation[\"file_name\"]\n        image_id = image_annotation[\"id\"]\n        image_height = image_annotation[\"height\"]\n        image_width = image_annotation[\"width\"]\n\n        # open and resize each image first\n        img = Image.open(os.path.join(cfg.dataset_dir, image_file_name)).convert('RGB')\n        sized = img.resize((model.width, model.height))\n\n        if use_cuda:\n            model.cuda()\n\n        start = time.time()\n        boxes = do_detect(model, sized, 0.0, 80, 0.4, use_cuda)\n        finish = time.time()\n        if type(boxes) == list:\n            for box in boxes:\n                box_json = {}\n                category_id = box[-1]\n                score = box[-2]\n                bbox_normalized = box[:4]\n                box_json[\"category_id\"] = int(category_id)\n                box_json[\"image_id\"] = int(image_id)\n                bbox = []\n                for i, bbox_coord in enumerate(bbox_normalized):\n                    modified_bbox_coord = float(bbox_coord)\n                    if i % 2:\n                        modified_bbox_coord *= image_height\n                    else:\n                        modified_bbox_coord *= image_width\n                    modified_bbox_coord = round(modified_bbox_coord, 2)\n                    bbox.append(modified_bbox_coord)\n                box_json[\"bbox_normalized\"] = list(map(lambda x: round(float(x), 2), bbox_normalized))\n                box_json[\"bbox\"] = bbox\n                box_json[\"score\"] = round(float(score), 2)\n                box_json[\"timing\"] = float(finish - start)\n                boxes_json.append(box_json)\n                # print(\"see box_json: \", box_json)\n                with open(resFile, 'w') as outfile:\n                    json.dump(boxes_json, outfile, default=myconverter)\n        else:\n            print(\"warning: output from model after postprocessing is not a list, ignoring\")\n            return\n\n        # namesfile = 'data/coco.names'\n        # class_names = load_class_names(namesfile)\n        # plot_boxes(img, boxes, 'data/outcome/predictions_{}.jpg'.format(image_id), class_names)\n\n    with open(resFile, 'w') as outfile:\n        json.dump(boxes_json, outfile, default=myconverter)\n\n    evaluate_on_coco(cfg, resFile)\n\n\ndef get_args(**kwargs):\n    cfg = kwargs\n    parser = argparse.ArgumentParser(description='Test model on test dataset',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('-f', '--load', dest='load', type=str, default=None,\n                        help='Load model from a .pth file')\n    parser.add_argument('-g', '--gpu', metavar='G', type=str, default='-1',\n                        help='GPU', dest='gpu')\n    parser.add_argument('-dir', '--data-dir', type=str, default=None,\n                        help='dataset dir', dest='dataset_dir')\n    parser.add_argument('-gta', '--ground_truth_annotations', type=str, default='instances_val2017.json',\n                        help='ground truth annotations file', dest='gt_annotations_path')\n    parser.add_argument('-w', '--weights_file', type=str, default='weights/yolov4.weights',\n                        help='weights file to load', dest='weights_file')\n    parser.add_argument('-c', '--model_config', type=str, default='cfg/yolov4.cfg',\n                        help='model config file to load', dest='model_config')\n    args = vars(parser.parse_args())\n\n    for k in args.keys():\n        cfg[k] = args.get(k)\n    return edict(cfg)\n\n\ndef init_logger(log_file=None, log_dir=None, log_level=logging.INFO, mode='w', stdout=True):\n    \"\"\"\n    log_dir: 日志文件的文件夹路径\n    mode: 'a', append; 'w', 覆盖原文件写入.\n    \"\"\"\n    import datetime\n    def get_date_str():\n        now = datetime.datetime.now()\n        return now.strftime('%Y-%m-%d_%H-%M-%S')\n\n    fmt = '%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s: %(message)s'\n    if log_dir is None:\n        log_dir = '~/temp/log/'\n    if log_file is None:\n        log_file = 'log_' + get_date_str() + '.txt'\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    log_file = os.path.join(log_dir, log_file)\n    # 此处不能使用logging输出\n    print('log file path:' + log_file)\n\n    logging.basicConfig(level=logging.DEBUG,\n                        format=fmt,\n                        filename=log_file,\n                        filemode=mode)\n\n    if stdout:\n        console = logging.StreamHandler(stream=sys.stdout)\n        console.setLevel(log_level)\n        formatter = logging.Formatter(fmt)\n        console.setFormatter(formatter)\n        logging.getLogger('').addHandler(console)\n\n    return logging\n\n\nif __name__ == \"__main__\":\n    logging = init_logger(log_dir='log')\n    cfg = get_args(**Cfg)\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfg.gpu\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logging.info(f'Using device {device}')\n\n    model = Darknet(cfg.model_config)\n\n    model.print_network()\n    model.load_weights(cfg.weights_file)\n    model.eval()  # set model away from training\n\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n\n    model.to(device=device)\n\n    annotations_file_path = cfg.gt_annotations_path\n    with open(annotations_file_path) as annotations_file:\n        try:\n            annotations = json.load(annotations_file)\n        except:\n            print(\"annotations file not a json\")\n            exit()\n    test(model=model,\n         annotations=annotations,\n         cfg=cfg, )\n"
        },
        {
          "name": "models.py",
          "type": "blob",
          "size": 16.6611328125,
          "content": "import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom tool.torch_utils import *\nfrom tool.yolo_layer import YoloLayer\n\n\nclass Mish(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        x = x * (torch.tanh(torch.nn.functional.softplus(x)))\n        return x\n\n\nclass Upsample(nn.Module):\n    def __init__(self):\n        super(Upsample, self).__init__()\n\n    def forward(self, x, target_size, inference=False):\n        assert (x.data.dim() == 4)\n        # _, _, tH, tW = target_size\n\n        if inference:\n\n            #B = x.data.size(0)\n            #C = x.data.size(1)\n            #H = x.data.size(2)\n            #W = x.data.size(3)\n\n            return x.view(x.size(0), x.size(1), x.size(2), 1, x.size(3), 1).\\\n                    expand(x.size(0), x.size(1), x.size(2), target_size[2] // x.size(2), x.size(3), target_size[3] // x.size(3)).\\\n                    contiguous().view(x.size(0), x.size(1), target_size[2], target_size[3])\n        else:\n            return F.interpolate(x, size=(target_size[2], target_size[3]), mode='nearest')\n\n\nclass Conv_Bn_Activation(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, activation, bn=True, bias=False):\n        super().__init__()\n        pad = (kernel_size - 1) // 2\n\n        self.conv = nn.ModuleList()\n        if bias:\n            self.conv.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad))\n        else:\n            self.conv.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad, bias=False))\n        if bn:\n            self.conv.append(nn.BatchNorm2d(out_channels))\n        if activation == \"mish\":\n            self.conv.append(Mish())\n        elif activation == \"relu\":\n            self.conv.append(nn.ReLU(inplace=True))\n        elif activation == \"leaky\":\n            self.conv.append(nn.LeakyReLU(0.1, inplace=True))\n        elif activation == \"linear\":\n            pass\n        else:\n            print(\"activate error !!! {} {} {}\".format(sys._getframe().f_code.co_filename,\n                                                       sys._getframe().f_code.co_name, sys._getframe().f_lineno))\n\n    def forward(self, x):\n        for l in self.conv:\n            x = l(x)\n        return x\n\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    Sequential residual blocks each of which consists of \\\n    two convolution layers.\n    Args:\n        ch (int): number of input and output channels.\n        nblocks (int): number of residual blocks.\n        shortcut (bool): if True, residual tensor addition is enabled.\n    \"\"\"\n\n    def __init__(self, ch, nblocks=1, shortcut=True):\n        super().__init__()\n        self.shortcut = shortcut\n        self.module_list = nn.ModuleList()\n        for i in range(nblocks):\n            resblock_one = nn.ModuleList()\n            resblock_one.append(Conv_Bn_Activation(ch, ch, 1, 1, 'mish'))\n            resblock_one.append(Conv_Bn_Activation(ch, ch, 3, 1, 'mish'))\n            self.module_list.append(resblock_one)\n\n    def forward(self, x):\n        for module in self.module_list:\n            h = x\n            for res in module:\n                h = res(h)\n            x = x + h if self.shortcut else h\n        return x\n\n\nclass DownSample1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(3, 32, 3, 1, 'mish')\n\n        self.conv2 = Conv_Bn_Activation(32, 64, 3, 2, 'mish')\n        self.conv3 = Conv_Bn_Activation(64, 64, 1, 1, 'mish')\n        # [route]\n        # layers = -2\n        self.conv4 = Conv_Bn_Activation(64, 64, 1, 1, 'mish')\n\n        self.conv5 = Conv_Bn_Activation(64, 32, 1, 1, 'mish')\n        self.conv6 = Conv_Bn_Activation(32, 64, 3, 1, 'mish')\n        # [shortcut]\n        # from=-3\n        # activation = linear\n\n        self.conv7 = Conv_Bn_Activation(64, 64, 1, 1, 'mish')\n        # [route]\n        # layers = -1, -7\n        self.conv8 = Conv_Bn_Activation(128, 64, 1, 1, 'mish')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x2)\n        # route -2\n        x4 = self.conv4(x2)\n        x5 = self.conv5(x4)\n        x6 = self.conv6(x5)\n        # shortcut -3\n        x6 = x6 + x4\n\n        x7 = self.conv7(x6)\n        # [route]\n        # layers = -1, -7\n        x7 = torch.cat([x7, x3], dim=1)\n        x8 = self.conv8(x7)\n        return x8\n\n\nclass DownSample2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(64, 128, 3, 2, 'mish')\n        self.conv2 = Conv_Bn_Activation(128, 64, 1, 1, 'mish')\n        # r -2\n        self.conv3 = Conv_Bn_Activation(128, 64, 1, 1, 'mish')\n\n        self.resblock = ResBlock(ch=64, nblocks=2)\n\n        # s -3\n        self.conv4 = Conv_Bn_Activation(64, 64, 1, 1, 'mish')\n        # r -1 -10\n        self.conv5 = Conv_Bn_Activation(128, 128, 1, 1, 'mish')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x1)\n\n        r = self.resblock(x3)\n        x4 = self.conv4(r)\n\n        x4 = torch.cat([x4, x2], dim=1)\n        x5 = self.conv5(x4)\n        return x5\n\n\nclass DownSample3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(128, 256, 3, 2, 'mish')\n        self.conv2 = Conv_Bn_Activation(256, 128, 1, 1, 'mish')\n        self.conv3 = Conv_Bn_Activation(256, 128, 1, 1, 'mish')\n\n        self.resblock = ResBlock(ch=128, nblocks=8)\n        self.conv4 = Conv_Bn_Activation(128, 128, 1, 1, 'mish')\n        self.conv5 = Conv_Bn_Activation(256, 256, 1, 1, 'mish')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x1)\n\n        r = self.resblock(x3)\n        x4 = self.conv4(r)\n\n        x4 = torch.cat([x4, x2], dim=1)\n        x5 = self.conv5(x4)\n        return x5\n\n\nclass DownSample4(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(256, 512, 3, 2, 'mish')\n        self.conv2 = Conv_Bn_Activation(512, 256, 1, 1, 'mish')\n        self.conv3 = Conv_Bn_Activation(512, 256, 1, 1, 'mish')\n\n        self.resblock = ResBlock(ch=256, nblocks=8)\n        self.conv4 = Conv_Bn_Activation(256, 256, 1, 1, 'mish')\n        self.conv5 = Conv_Bn_Activation(512, 512, 1, 1, 'mish')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x1)\n\n        r = self.resblock(x3)\n        x4 = self.conv4(r)\n\n        x4 = torch.cat([x4, x2], dim=1)\n        x5 = self.conv5(x4)\n        return x5\n\n\nclass DownSample5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv_Bn_Activation(512, 1024, 3, 2, 'mish')\n        self.conv2 = Conv_Bn_Activation(1024, 512, 1, 1, 'mish')\n        self.conv3 = Conv_Bn_Activation(1024, 512, 1, 1, 'mish')\n\n        self.resblock = ResBlock(ch=512, nblocks=4)\n        self.conv4 = Conv_Bn_Activation(512, 512, 1, 1, 'mish')\n        self.conv5 = Conv_Bn_Activation(1024, 1024, 1, 1, 'mish')\n\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x1)\n\n        r = self.resblock(x3)\n        x4 = self.conv4(r)\n\n        x4 = torch.cat([x4, x2], dim=1)\n        x5 = self.conv5(x4)\n        return x5\n\n\nclass Neck(nn.Module):\n    def __init__(self, inference=False):\n        super().__init__()\n        self.inference = inference\n\n        self.conv1 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n        self.conv2 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n        self.conv3 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n        # SPP\n        self.maxpool1 = nn.MaxPool2d(kernel_size=5, stride=1, padding=5 // 2)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=9, stride=1, padding=9 // 2)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=13, stride=1, padding=13 // 2)\n\n        # R -1 -3 -5 -6\n        # SPP\n        self.conv4 = Conv_Bn_Activation(2048, 512, 1, 1, 'leaky')\n        self.conv5 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n        self.conv6 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n        self.conv7 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n        # UP\n        self.upsample1 = Upsample()\n        # R 85\n        self.conv8 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n        # R -1 -3\n        self.conv9 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n        self.conv10 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n        self.conv11 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n        self.conv12 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n        self.conv13 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n        self.conv14 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n        # UP\n        self.upsample2 = Upsample()\n        # R 54\n        self.conv15 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n        # R -1 -3\n        self.conv16 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n        self.conv17 = Conv_Bn_Activation(128, 256, 3, 1, 'leaky')\n        self.conv18 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n        self.conv19 = Conv_Bn_Activation(128, 256, 3, 1, 'leaky')\n        self.conv20 = Conv_Bn_Activation(256, 128, 1, 1, 'leaky')\n\n    def forward(self, input, downsample4, downsample3, inference=False):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x2)\n        # SPP\n        m1 = self.maxpool1(x3)\n        m2 = self.maxpool2(x3)\n        m3 = self.maxpool3(x3)\n        spp = torch.cat([m3, m2, m1, x3], dim=1)\n        # SPP end\n        x4 = self.conv4(spp)\n        x5 = self.conv5(x4)\n        x6 = self.conv6(x5)\n        x7 = self.conv7(x6)\n        # UP\n        up = self.upsample1(x7, downsample4.size(), self.inference)\n        # R 85\n        x8 = self.conv8(downsample4)\n        # R -1 -3\n        x8 = torch.cat([x8, up], dim=1)\n\n        x9 = self.conv9(x8)\n        x10 = self.conv10(x9)\n        x11 = self.conv11(x10)\n        x12 = self.conv12(x11)\n        x13 = self.conv13(x12)\n        x14 = self.conv14(x13)\n\n        # UP\n        up = self.upsample2(x14, downsample3.size(), self.inference)\n        # R 54\n        x15 = self.conv15(downsample3)\n        # R -1 -3\n        x15 = torch.cat([x15, up], dim=1)\n\n        x16 = self.conv16(x15)\n        x17 = self.conv17(x16)\n        x18 = self.conv18(x17)\n        x19 = self.conv19(x18)\n        x20 = self.conv20(x19)\n        return x20, x13, x6\n\n\nclass Yolov4Head(nn.Module):\n    def __init__(self, output_ch, n_classes, inference=False):\n        super().__init__()\n        self.inference = inference\n\n        self.conv1 = Conv_Bn_Activation(128, 256, 3, 1, 'leaky')\n        self.conv2 = Conv_Bn_Activation(256, output_ch, 1, 1, 'linear', bn=False, bias=True)\n\n        self.yolo1 = YoloLayer(\n                                anchor_mask=[0, 1, 2], num_classes=n_classes,\n                                anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n                                num_anchors=9, stride=8)\n\n        # R -4\n        self.conv3 = Conv_Bn_Activation(128, 256, 3, 2, 'leaky')\n\n        # R -1 -16\n        self.conv4 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n        self.conv5 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n        self.conv6 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n        self.conv7 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n        self.conv8 = Conv_Bn_Activation(512, 256, 1, 1, 'leaky')\n        self.conv9 = Conv_Bn_Activation(256, 512, 3, 1, 'leaky')\n        self.conv10 = Conv_Bn_Activation(512, output_ch, 1, 1, 'linear', bn=False, bias=True)\n        \n        self.yolo2 = YoloLayer(\n                                anchor_mask=[3, 4, 5], num_classes=n_classes,\n                                anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n                                num_anchors=9, stride=16)\n\n        # R -4\n        self.conv11 = Conv_Bn_Activation(256, 512, 3, 2, 'leaky')\n\n        # R -1 -37\n        self.conv12 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n        self.conv13 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n        self.conv14 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n        self.conv15 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n        self.conv16 = Conv_Bn_Activation(1024, 512, 1, 1, 'leaky')\n        self.conv17 = Conv_Bn_Activation(512, 1024, 3, 1, 'leaky')\n        self.conv18 = Conv_Bn_Activation(1024, output_ch, 1, 1, 'linear', bn=False, bias=True)\n        \n        self.yolo3 = YoloLayer(\n                                anchor_mask=[6, 7, 8], num_classes=n_classes,\n                                anchors=[12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n                                num_anchors=9, stride=32)\n\n    def forward(self, input1, input2, input3):\n        x1 = self.conv1(input1)\n        x2 = self.conv2(x1)\n\n        x3 = self.conv3(input1)\n        # R -1 -16\n        x3 = torch.cat([x3, input2], dim=1)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)\n        x6 = self.conv6(x5)\n        x7 = self.conv7(x6)\n        x8 = self.conv8(x7)\n        x9 = self.conv9(x8)\n        x10 = self.conv10(x9)\n\n        # R -4\n        x11 = self.conv11(x8)\n        # R -1 -37\n        x11 = torch.cat([x11, input3], dim=1)\n\n        x12 = self.conv12(x11)\n        x13 = self.conv13(x12)\n        x14 = self.conv14(x13)\n        x15 = self.conv15(x14)\n        x16 = self.conv16(x15)\n        x17 = self.conv17(x16)\n        x18 = self.conv18(x17)\n        \n        if self.inference:\n            y1 = self.yolo1(x2)\n            y2 = self.yolo2(x10)\n            y3 = self.yolo3(x18)\n\n            return get_region_boxes([y1, y2, y3])\n        \n        else:\n            return [x2, x10, x18]\n\n\nclass Yolov4(nn.Module):\n    def __init__(self, yolov4conv137weight=None, n_classes=80, inference=False):\n        super().__init__()\n\n        output_ch = (4 + 1 + n_classes) * 3\n\n        # backbone\n        self.down1 = DownSample1()\n        self.down2 = DownSample2()\n        self.down3 = DownSample3()\n        self.down4 = DownSample4()\n        self.down5 = DownSample5()\n        # neck\n        self.neck = Neck(inference)\n        # yolov4conv137\n        if yolov4conv137weight:\n            _model = nn.Sequential(self.down1, self.down2, self.down3, self.down4, self.down5, self.neck)\n            pretrained_dict = torch.load(yolov4conv137weight)\n\n            model_dict = _model.state_dict()\n            # 1. filter out unnecessary keys\n            pretrained_dict = {k1: v for (k, v), k1 in zip(pretrained_dict.items(), model_dict)}\n            # 2. overwrite entries in the existing state dict\n            model_dict.update(pretrained_dict)\n            _model.load_state_dict(model_dict)\n        \n        # head\n        self.head = Yolov4Head(output_ch, n_classes, inference)\n\n\n    def forward(self, input):\n        d1 = self.down1(input)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n\n        x20, x13, x6 = self.neck(d5, d4, d3)\n\n        output = self.head(x20, x13, x6)\n        return output\n\n\nif __name__ == \"__main__\":\n    import sys\n    import cv2\n\n    namesfile = None\n    if len(sys.argv) == 6:\n        n_classes = int(sys.argv[1])\n        weightfile = sys.argv[2]\n        imgfile = sys.argv[3]\n        height = int(sys.argv[4])\n        width = int(sys.argv[5])\n    elif len(sys.argv) == 7:\n        n_classes = int(sys.argv[1])\n        weightfile = sys.argv[2]\n        imgfile = sys.argv[3]\n        height = int(sys.argv[4])\n        width = int(sys.argv[5])\n        namesfile = sys.argv[6]\n    else:\n        print('Usage: ')\n        print('  python models.py num_classes weightfile imgfile namefile')\n\n    model = Yolov4(yolov4conv137weight=None, n_classes=n_classes, inference=True)\n\n    pretrained_dict = torch.load(weightfile, map_location=torch.device('cuda'))\n    model.load_state_dict(pretrained_dict)\n\n    use_cuda = True\n    if use_cuda:\n        model.cuda()\n\n    img = cv2.imread(imgfile)\n\n    # Inference input size is 416*416 does not mean training size is the same\n    # Training size could be 608*608 or even other sizes\n    # Optional inference sizes:\n    #   Hight in {320, 416, 512, 608, ... 320 + 96 * n}\n    #   Width in {320, 416, 512, 608, ... 320 + 96 * m}\n    sized = cv2.resize(img, (width, height))\n    sized = cv2.cvtColor(sized, cv2.COLOR_BGR2RGB)\n\n    from tool.utils import load_class_names, plot_boxes_cv2\n    from tool.torch_utils import do_detect\n\n    for i in range(2):  # This 'for' loop is for speed check\n                        # Because the first iteration is usually longer\n        boxes = do_detect(model, sized, 0.4, 0.6, use_cuda)\n\n    if namesfile == None:\n        if n_classes == 20:\n            namesfile = 'data/voc.names'\n        elif n_classes == 80:\n            namesfile = 'data/coco.names'\n        else:\n            print(\"please give namefile\")\n\n    class_names = load_class_names(namesfile)\n    plot_boxes_cv2(img, boxes[0], 'predictions.jpg', class_names)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1474609375,
          "content": "numpy==1.18.2\ntorch==1.4.0\ntensorboardX==2.0\nscikit_image==0.16.2\nmatplotlib==2.2.3\ntqdm==4.43.0\neasydict==1.9\nPillow==7.1.2\nopencv_python\npycocotools\n"
        },
        {
          "name": "tool",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 27.6865234375,
          "content": "# -*- coding: utf-8 -*-\n'''\n@Time          : 2020/05/06 15:07\n@Author        : Tianxiaomo\n@File          : train.py\n@Noice         :\n@Modificattion :\n    @Author    :\n    @Time      :\n    @Detail    :\n\n'''\nimport time\nimport logging\nimport os, sys, math\nimport argparse\nfrom collections import deque\nimport datetime\n\nimport cv2\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom tensorboardX import SummaryWriter\nfrom easydict import EasyDict as edict\n\nfrom dataset import Yolo_dataset\nfrom cfg import Cfg\nfrom models import Yolov4\nfrom tool.darknet2pytorch import Darknet\n\nfrom tool.tv_reference.utils import collate_fn as val_collate\nfrom tool.tv_reference.coco_utils import convert_to_coco_api\nfrom tool.tv_reference.coco_eval import CocoEvaluator\n\n\ndef bboxes_iou(bboxes_a, bboxes_b, xyxy=True, GIoU=False, DIoU=False, CIoU=False):\n    \"\"\"Calculate the Intersection of Unions (IoUs) between bounding boxes.\n    IoU is calculated as a ratio of area of the intersection\n    and area of the union.\n\n    Args:\n        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n            :math:`N` is the number of bounding boxes.\n            The dtype should be :obj:`numpy.float32`.\n        bbox_b (array): An array similar to :obj:`bbox_a`,\n            whose shape is :math:`(K, 4)`.\n            The dtype should be :obj:`numpy.float32`.\n    Returns:\n        array:\n        An array whose shape is :math:`(N, K)`. \\\n        An element at index :math:`(n, k)` contains IoUs between \\\n        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n        box in :obj:`bbox_b`.\n\n    from: https://github.com/chainer/chainercv\n    https://github.com/ultralytics/yolov3/blob/eca5b9c1d36e4f73bf2f94e141d864f1c2739e23/utils/utils.py#L262-L282\n    \"\"\"\n    if bboxes_a.shape[1] != 4 or bboxes_b.shape[1] != 4:\n        raise IndexError\n\n    if xyxy:\n        # intersection top left\n        tl = torch.max(bboxes_a[:, None, :2], bboxes_b[:, :2])\n        # intersection bottom right\n        br = torch.min(bboxes_a[:, None, 2:], bboxes_b[:, 2:])\n        # convex (smallest enclosing box) top left and bottom right\n        con_tl = torch.min(bboxes_a[:, None, :2], bboxes_b[:, :2])\n        con_br = torch.max(bboxes_a[:, None, 2:], bboxes_b[:, 2:])\n        # centerpoint distance squared\n        rho2 = ((bboxes_a[:, None, 0] + bboxes_a[:, None, 2]) - (bboxes_b[:, 0] + bboxes_b[:, 2])) ** 2 / 4 + (\n                (bboxes_a[:, None, 1] + bboxes_a[:, None, 3]) - (bboxes_b[:, 1] + bboxes_b[:, 3])) ** 2 / 4\n\n        w1 = bboxes_a[:, 2] - bboxes_a[:, 0]\n        h1 = bboxes_a[:, 3] - bboxes_a[:, 1]\n        w2 = bboxes_b[:, 2] - bboxes_b[:, 0]\n        h2 = bboxes_b[:, 3] - bboxes_b[:, 1]\n\n        area_a = torch.prod(bboxes_a[:, 2:] - bboxes_a[:, :2], 1)\n        area_b = torch.prod(bboxes_b[:, 2:] - bboxes_b[:, :2], 1)\n    else:\n        # intersection top left\n        tl = torch.max((bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2),\n                       (bboxes_b[:, :2] - bboxes_b[:, 2:] / 2))\n        # intersection bottom right\n        br = torch.min((bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2),\n                       (bboxes_b[:, :2] + bboxes_b[:, 2:] / 2))\n\n        # convex (smallest enclosing box) top left and bottom right\n        con_tl = torch.min((bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2),\n                           (bboxes_b[:, :2] - bboxes_b[:, 2:] / 2))\n        con_br = torch.max((bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2),\n                           (bboxes_b[:, :2] + bboxes_b[:, 2:] / 2))\n        # centerpoint distance squared\n        rho2 = ((bboxes_a[:, None, :2] - bboxes_b[:, :2]) ** 2 / 4).sum(dim=-1)\n\n        w1 = bboxes_a[:, 2]\n        h1 = bboxes_a[:, 3]\n        w2 = bboxes_b[:, 2]\n        h2 = bboxes_b[:, 3]\n\n        area_a = torch.prod(bboxes_a[:, 2:], 1)\n        area_b = torch.prod(bboxes_b[:, 2:], 1)\n    en = (tl < br).type(tl.type()).prod(dim=2)\n    area_i = torch.prod(br - tl, 2) * en  # * ((tl < br).all())\n    area_u = area_a[:, None] + area_b - area_i\n    iou = area_i / area_u\n\n    if GIoU or DIoU or CIoU:\n        if GIoU:  # Generalized IoU https://arxiv.org/pdf/1902.09630.pdf\n            area_c = torch.prod(con_br - con_tl, 2)  # convex area\n            return iou - (area_c - area_u) / area_c  # GIoU\n        if DIoU or CIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n            # convex diagonal squared\n            c2 = torch.pow(con_br - con_tl, 2).sum(dim=2) + 1e-16\n            if DIoU:\n                return iou - rho2 / c2  # DIoU\n            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w1 / h1).unsqueeze(1) - torch.atan(w2 / h2), 2)\n                with torch.no_grad():\n                    alpha = v / (1 - iou + v)\n                return iou - (rho2 / c2 + v * alpha)  # CIoU\n    return iou\n\n\nclass Yolo_loss(nn.Module):\n    def __init__(self, n_classes=80, n_anchors=3, device=None, batch=2):\n        super(Yolo_loss, self).__init__()\n        self.device = device\n        self.strides = [8, 16, 32]\n        image_size = 608\n        self.n_classes = n_classes\n        self.n_anchors = n_anchors\n\n        self.anchors = [[12, 16], [19, 36], [40, 28], [36, 75], [76, 55], [72, 146], [142, 110], [192, 243], [459, 401]]\n        self.anch_masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n        self.ignore_thre = 0.5\n\n        self.masked_anchors, self.ref_anchors, self.grid_x, self.grid_y, self.anchor_w, self.anchor_h = [], [], [], [], [], []\n\n        for i in range(3):\n            all_anchors_grid = [(w / self.strides[i], h / self.strides[i]) for w, h in self.anchors]\n            masked_anchors = np.array([all_anchors_grid[j] for j in self.anch_masks[i]], dtype=np.float32)\n            ref_anchors = np.zeros((len(all_anchors_grid), 4), dtype=np.float32)\n            ref_anchors[:, 2:] = np.array(all_anchors_grid, dtype=np.float32)\n            ref_anchors = torch.from_numpy(ref_anchors)\n            # calculate pred - xywh obj cls\n            fsize = image_size // self.strides[i]\n            grid_x = torch.arange(fsize, dtype=torch.float).repeat(batch, 3, fsize, 1).to(device)\n            grid_y = torch.arange(fsize, dtype=torch.float).repeat(batch, 3, fsize, 1).permute(0, 1, 3, 2).to(device)\n            anchor_w = torch.from_numpy(masked_anchors[:, 0]).repeat(batch, fsize, fsize, 1).permute(0, 3, 1, 2).to(\n                device)\n            anchor_h = torch.from_numpy(masked_anchors[:, 1]).repeat(batch, fsize, fsize, 1).permute(0, 3, 1, 2).to(\n                device)\n\n            self.masked_anchors.append(masked_anchors)\n            self.ref_anchors.append(ref_anchors)\n            self.grid_x.append(grid_x)\n            self.grid_y.append(grid_y)\n            self.anchor_w.append(anchor_w)\n            self.anchor_h.append(anchor_h)\n\n    def build_target(self, pred, labels, batchsize, fsize, n_ch, output_id):\n        # target assignment\n        tgt_mask = torch.zeros(batchsize, self.n_anchors, fsize, fsize, 4 + self.n_classes).to(device=self.device)\n        obj_mask = torch.ones(batchsize, self.n_anchors, fsize, fsize).to(device=self.device)\n        tgt_scale = torch.zeros(batchsize, self.n_anchors, fsize, fsize, 2).to(self.device)\n        target = torch.zeros(batchsize, self.n_anchors, fsize, fsize, n_ch).to(self.device)\n\n        # labels = labels.cpu().data\n        nlabel = (labels.sum(dim=2) > 0).sum(dim=1)  # number of objects\n\n        truth_x_all = (labels[:, :, 2] + labels[:, :, 0]) / (self.strides[output_id] * 2)\n        truth_y_all = (labels[:, :, 3] + labels[:, :, 1]) / (self.strides[output_id] * 2)\n        truth_w_all = (labels[:, :, 2] - labels[:, :, 0]) / self.strides[output_id]\n        truth_h_all = (labels[:, :, 3] - labels[:, :, 1]) / self.strides[output_id]\n        truth_i_all = truth_x_all.to(torch.int16).cpu().numpy()\n        truth_j_all = truth_y_all.to(torch.int16).cpu().numpy()\n\n        for b in range(batchsize):\n            n = int(nlabel[b])\n            if n == 0:\n                continue\n            truth_box = torch.zeros(n, 4).to(self.device)\n            truth_box[:n, 2] = truth_w_all[b, :n]\n            truth_box[:n, 3] = truth_h_all[b, :n]\n            truth_i = truth_i_all[b, :n]\n            truth_j = truth_j_all[b, :n]\n\n            # calculate iou between truth and reference anchors\n            anchor_ious_all = bboxes_iou(truth_box.cpu(), self.ref_anchors[output_id], CIoU=True)\n\n            # temp = bbox_iou(truth_box.cpu(), self.ref_anchors[output_id])\n\n            best_n_all = anchor_ious_all.argmax(dim=1)\n            best_n = best_n_all % 3\n            best_n_mask = ((best_n_all == self.anch_masks[output_id][0]) |\n                           (best_n_all == self.anch_masks[output_id][1]) |\n                           (best_n_all == self.anch_masks[output_id][2]))\n\n            if sum(best_n_mask) == 0:\n                continue\n\n            truth_box[:n, 0] = truth_x_all[b, :n]\n            truth_box[:n, 1] = truth_y_all[b, :n]\n\n            pred_ious = bboxes_iou(pred[b].view(-1, 4), truth_box, xyxy=False)\n            pred_best_iou, _ = pred_ious.max(dim=1)\n            pred_best_iou = (pred_best_iou > self.ignore_thre)\n            pred_best_iou = pred_best_iou.view(pred[b].shape[:3])\n            # set mask to zero (ignore) if pred matches truth\n            obj_mask[b] = ~ pred_best_iou\n\n            for ti in range(best_n.shape[0]):\n                if best_n_mask[ti] == 1:\n                    i, j = truth_i[ti], truth_j[ti]\n                    a = best_n[ti]\n                    obj_mask[b, a, j, i] = 1\n                    tgt_mask[b, a, j, i, :] = 1\n                    target[b, a, j, i, 0] = truth_x_all[b, ti] - truth_x_all[b, ti].to(torch.int16).to(torch.float)\n                    target[b, a, j, i, 1] = truth_y_all[b, ti] - truth_y_all[b, ti].to(torch.int16).to(torch.float)\n                    target[b, a, j, i, 2] = torch.log(\n                        truth_w_all[b, ti] / torch.Tensor(self.masked_anchors[output_id])[best_n[ti], 0] + 1e-16)\n                    target[b, a, j, i, 3] = torch.log(\n                        truth_h_all[b, ti] / torch.Tensor(self.masked_anchors[output_id])[best_n[ti], 1] + 1e-16)\n                    target[b, a, j, i, 4] = 1\n                    target[b, a, j, i, 5 + labels[b, ti, 4].to(torch.int16).cpu().numpy()] = 1\n                    tgt_scale[b, a, j, i, :] = torch.sqrt(2 - truth_w_all[b, ti] * truth_h_all[b, ti] / fsize / fsize)\n        return obj_mask, tgt_mask, tgt_scale, target\n\n    def forward(self, xin, labels=None):\n        loss, loss_xy, loss_wh, loss_obj, loss_cls, loss_l2 = 0, 0, 0, 0, 0, 0\n        for output_id, output in enumerate(xin):\n            batchsize = output.shape[0]\n            fsize = output.shape[2]\n            n_ch = 5 + self.n_classes\n\n            output = output.view(batchsize, self.n_anchors, n_ch, fsize, fsize)\n            output = output.permute(0, 1, 3, 4, 2)  # .contiguous()\n\n            # logistic activation for xy, obj, cls\n            output[..., np.r_[:2, 4:n_ch]] = torch.sigmoid(output[..., np.r_[:2, 4:n_ch]])\n\n            pred = output[..., :4].clone()\n            pred[..., 0] += self.grid_x[output_id]\n            pred[..., 1] += self.grid_y[output_id]\n            pred[..., 2] = torch.exp(pred[..., 2]) * self.anchor_w[output_id]\n            pred[..., 3] = torch.exp(pred[..., 3]) * self.anchor_h[output_id]\n\n            obj_mask, tgt_mask, tgt_scale, target = self.build_target(pred, labels, batchsize, fsize, n_ch, output_id)\n\n            # loss calculation\n            output[..., 4] *= obj_mask\n            output[..., np.r_[0:4, 5:n_ch]] *= tgt_mask\n            output[..., 2:4] *= tgt_scale\n\n            target[..., 4] *= obj_mask\n            target[..., np.r_[0:4, 5:n_ch]] *= tgt_mask\n            target[..., 2:4] *= tgt_scale\n\n            loss_xy += F.binary_cross_entropy(input=output[..., :2], target=target[..., :2],\n                                              weight=tgt_scale * tgt_scale, reduction='sum')\n            loss_wh += F.mse_loss(input=output[..., 2:4], target=target[..., 2:4], reduction='sum') / 2\n            loss_obj += F.binary_cross_entropy(input=output[..., 4], target=target[..., 4], reduction='sum')\n            loss_cls += F.binary_cross_entropy(input=output[..., 5:], target=target[..., 5:], reduction='sum')\n            loss_l2 += F.mse_loss(input=output, target=target, reduction='sum')\n\n        loss = loss_xy + loss_wh + loss_obj + loss_cls\n\n        return loss, loss_xy, loss_wh, loss_obj, loss_cls, loss_l2\n\n\ndef collate(batch):\n    images = []\n    bboxes = []\n    for img, box in batch:\n        images.append([img])\n        bboxes.append([box])\n    images = np.concatenate(images, axis=0)\n    images = images.transpose(0, 3, 1, 2)\n    images = torch.from_numpy(images).div(255.0)\n    bboxes = np.concatenate(bboxes, axis=0)\n    bboxes = torch.from_numpy(bboxes)\n    return images, bboxes\n\n\ndef train(model, device, config, epochs=5, batch_size=1, save_cp=True, log_step=20, img_scale=0.5):\n    train_dataset = Yolo_dataset(config.train_label, config, train=True)\n    val_dataset = Yolo_dataset(config.val_label, config, train=False)\n\n    n_train = len(train_dataset)\n    n_val = len(val_dataset)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch // config.subdivisions, shuffle=True,\n                              num_workers=8, pin_memory=True, drop_last=True, collate_fn=collate)\n\n    val_loader = DataLoader(val_dataset, batch_size=config.batch // config.subdivisions, shuffle=True, num_workers=8,\n                            pin_memory=True, drop_last=True, collate_fn=val_collate)\n\n    writer = SummaryWriter(log_dir=config.TRAIN_TENSORBOARD_DIR,\n                           filename_suffix=f'OPT_{config.TRAIN_OPTIMIZER}_LR_{config.learning_rate}_BS_{config.batch}_Sub_{config.subdivisions}_Size_{config.width}',\n                           comment=f'OPT_{config.TRAIN_OPTIMIZER}_LR_{config.learning_rate}_BS_{config.batch}_Sub_{config.subdivisions}_Size_{config.width}')\n    # writer.add_images('legend',\n    #                   torch.from_numpy(train_dataset.label2colorlegend2(cfg.DATA_CLASSES).transpose([2, 0, 1])).to(\n    #                       device).unsqueeze(0))\n    max_itr = config.TRAIN_EPOCHS * n_train\n    # global_step = cfg.TRAIN_MINEPOCH * n_train\n    global_step = 0\n    logging.info(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {config.batch}\n        Subdivisions:    {config.subdivisions}\n        Learning rate:   {config.learning_rate}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Checkpoints:     {save_cp}\n        Device:          {device.type}\n        Images size:     {config.width}\n        Optimizer:       {config.TRAIN_OPTIMIZER}\n        Dataset classes: {config.classes}\n        Train label path:{config.train_label}\n        Pretrained:\n    ''')\n\n    # learning rate setup\n    def burnin_schedule(i):\n        if i < config.burn_in:\n            factor = pow(i / config.burn_in, 4)\n        elif i < config.steps[0]:\n            factor = 1.0\n        elif i < config.steps[1]:\n            factor = 0.1\n        else:\n            factor = 0.01\n        return factor\n\n    if config.TRAIN_OPTIMIZER.lower() == 'adam':\n        optimizer = optim.Adam(\n            model.parameters(),\n            lr=config.learning_rate / config.batch,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n        )\n    elif config.TRAIN_OPTIMIZER.lower() == 'sgd':\n        optimizer = optim.SGD(\n            params=model.parameters(),\n            lr=config.learning_rate / config.batch,\n            momentum=config.momentum,\n            weight_decay=config.decay,\n        )\n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, burnin_schedule)\n\n    criterion = Yolo_loss(device=device, batch=config.batch // config.subdivisions, n_classes=config.classes)\n    # scheduler = ReduceLROnPlateau(optimizer, mode='max', verbose=True, patience=6, min_lr=1e-7)\n    # scheduler = CosineAnnealingWarmRestarts(optimizer, 0.001, 1e-6, 20)\n\n    save_prefix = 'Yolov4_epoch'\n    saved_models = deque()\n    model.train()\n    for epoch in range(epochs):\n        # model.train()\n        epoch_loss = 0\n        epoch_step = 0\n\n        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img', ncols=50) as pbar:\n            for i, batch in enumerate(train_loader):\n                global_step += 1\n                epoch_step += 1\n                images = batch[0]\n                bboxes = batch[1]\n\n                images = images.to(device=device, dtype=torch.float32)\n                bboxes = bboxes.to(device=device)\n\n                bboxes_pred = model(images)\n                loss, loss_xy, loss_wh, loss_obj, loss_cls, loss_l2 = criterion(bboxes_pred, bboxes)\n                # loss = loss / config.subdivisions\n                loss.backward()\n\n                epoch_loss += loss.item()\n\n                if global_step % config.subdivisions == 0:\n                    optimizer.step()\n                    scheduler.step()\n                    model.zero_grad()\n\n                if global_step % (log_step * config.subdivisions) == 0:\n                    writer.add_scalar('train/Loss', loss.item(), global_step)\n                    writer.add_scalar('train/loss_xy', loss_xy.item(), global_step)\n                    writer.add_scalar('train/loss_wh', loss_wh.item(), global_step)\n                    writer.add_scalar('train/loss_obj', loss_obj.item(), global_step)\n                    writer.add_scalar('train/loss_cls', loss_cls.item(), global_step)\n                    writer.add_scalar('train/loss_l2', loss_l2.item(), global_step)\n                    writer.add_scalar('lr', scheduler.get_lr()[0] * config.batch, global_step)\n                    pbar.set_postfix(**{'loss (batch)': loss.item(), 'loss_xy': loss_xy.item(),\n                                        'loss_wh': loss_wh.item(),\n                                        'loss_obj': loss_obj.item(),\n                                        'loss_cls': loss_cls.item(),\n                                        'loss_l2': loss_l2.item(),\n                                        'lr': scheduler.get_lr()[0] * config.batch\n                                        })\n                    logging.debug('Train step_{}: loss : {},loss xy : {},loss wh : {},'\n                                  'loss obj : {}，loss cls : {},loss l2 : {},lr : {}'\n                                  .format(global_step, loss.item(), loss_xy.item(),\n                                          loss_wh.item(), loss_obj.item(),\n                                          loss_cls.item(), loss_l2.item(),\n                                          scheduler.get_lr()[0] * config.batch))\n\n                pbar.update(images.shape[0])\n\n            if cfg.use_darknet_cfg:\n                eval_model = Darknet(cfg.cfgfile, inference=True)\n            else:\n                eval_model = Yolov4(cfg.pretrained, n_classes=cfg.classes, inference=True)\n            # eval_model = Yolov4(yolov4conv137weight=None, n_classes=config.classes, inference=True)\n            if torch.cuda.device_count() > 1:\n                eval_model.load_state_dict(model.module.state_dict())\n            else:\n                eval_model.load_state_dict(model.state_dict())\n            eval_model.to(device)\n            evaluator = evaluate(eval_model, val_loader, config, device)\n            del eval_model\n\n            stats = evaluator.coco_eval['bbox'].stats\n            writer.add_scalar('train/AP', stats[0], global_step)\n            writer.add_scalar('train/AP50', stats[1], global_step)\n            writer.add_scalar('train/AP75', stats[2], global_step)\n            writer.add_scalar('train/AP_small', stats[3], global_step)\n            writer.add_scalar('train/AP_medium', stats[4], global_step)\n            writer.add_scalar('train/AP_large', stats[5], global_step)\n            writer.add_scalar('train/AR1', stats[6], global_step)\n            writer.add_scalar('train/AR10', stats[7], global_step)\n            writer.add_scalar('train/AR100', stats[8], global_step)\n            writer.add_scalar('train/AR_small', stats[9], global_step)\n            writer.add_scalar('train/AR_medium', stats[10], global_step)\n            writer.add_scalar('train/AR_large', stats[11], global_step)\n\n            if save_cp:\n                try:\n                    # os.mkdir(config.checkpoints)\n                    os.makedirs(config.checkpoints, exist_ok=True)\n                    logging.info('Created checkpoint directory')\n                except OSError:\n                    pass\n                save_path = os.path.join(config.checkpoints, f'{save_prefix}{epoch + 1}.pth')\n                if isinstance(model, torch.nn.DataParallel):\n                    torch.save(model.moduel,state_dict(), save_path)\n                else:\n                    torch.save(model.state_dict(), save_path)\n                logging.info(f'Checkpoint {epoch + 1} saved !')\n                saved_models.append(save_path)\n                if len(saved_models) > config.keep_checkpoint_max > 0:\n                    model_to_remove = saved_models.popleft()\n                    try:\n                        os.remove(model_to_remove)\n                    except:\n                        logging.info(f'failed to remove {model_to_remove}')\n\n    writer.close()\n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, cfg, device, logger=None, **kwargs):\n    \"\"\" finished, tested\n    \"\"\"\n    # cpu_device = torch.device(\"cpu\")\n    model.eval()\n    # header = 'Test:'\n\n    coco = convert_to_coco_api(data_loader.dataset, bbox_fmt='coco')\n    coco_evaluator = CocoEvaluator(coco, iou_types = [\"bbox\"], bbox_fmt='coco')\n\n    for images, targets in data_loader:\n        model_input = [[cv2.resize(img, (cfg.w, cfg.h))] for img in images]\n        model_input = np.concatenate(model_input, axis=0)\n        model_input = model_input.transpose(0, 3, 1, 2)\n        model_input = torch.from_numpy(model_input).div(255.0)\n        model_input = model_input.to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        model_time = time.time()\n        outputs = model(model_input)\n\n        # outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        # outputs = outputs.cpu().detach().numpy()\n        res = {}\n        # for img, target, output in zip(images, targets, outputs):\n        for img, target, boxes, confs in zip(images, targets, outputs[0], outputs[1]):\n            img_height, img_width = img.shape[:2]\n            # boxes = output[...,:4].copy()  # output boxes in yolo format\n            boxes = boxes.squeeze(2).cpu().detach().numpy()\n            boxes[...,2:] = boxes[...,2:] - boxes[...,:2] # Transform [x1, y1, x2, y2] to [x1, y1, w, h]\n            boxes[...,0] = boxes[...,0]*img_width\n            boxes[...,1] = boxes[...,1]*img_height\n            boxes[...,2] = boxes[...,2]*img_width\n            boxes[...,3] = boxes[...,3]*img_height\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            # confs = output[...,4:].copy()\n            confs = confs.cpu().detach().numpy()\n            labels = np.argmax(confs, axis=1).flatten()\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n            scores = np.max(confs, axis=1).flatten()\n            scores = torch.as_tensor(scores, dtype=torch.float32)\n            res[target[\"image_id\"].item()] = {\n                \"boxes\": boxes,\n                \"scores\": scores,\n                \"labels\": labels,\n            }\n        evaluator_time = time.time()\n        coco_evaluator.update(res)\n        evaluator_time = time.time() - evaluator_time\n\n    # gather the stats from all processes\n    coco_evaluator.synchronize_between_processes()\n\n    # accumulate predictions from all images\n    coco_evaluator.accumulate()\n    coco_evaluator.summarize()\n\n    return coco_evaluator\n\n\ndef get_args(**kwargs):\n    cfg = kwargs\n    parser = argparse.ArgumentParser(description='Train the Model on images and target masks',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # parser.add_argument('-b', '--batch-size', metavar='B', type=int, nargs='?', default=2,\n    #                     help='Batch size', dest='batchsize')\n    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=0.001,\n                        help='Learning rate', dest='learning_rate')\n    parser.add_argument('-f', '--load', dest='load', type=str, default=None,\n                        help='Load model from a .pth file')\n    parser.add_argument('-g', '--gpu', metavar='G', type=str, default='-1',\n                        help='GPU', dest='gpu')\n    parser.add_argument('-dir', '--data-dir', type=str, default=None,\n                        help='dataset dir', dest='dataset_dir')\n    parser.add_argument('-pretrained', type=str, default=None, help='pretrained yolov4.conv.137')\n    parser.add_argument('-classes', type=int, default=80, help='dataset classes')\n    parser.add_argument('-train_label_path', dest='train_label', type=str, default='train.txt', help=\"train label path\")\n    parser.add_argument(\n        '-optimizer', type=str, default='adam',\n        help='training optimizer',\n        dest='TRAIN_OPTIMIZER')\n    parser.add_argument(\n        '-iou-type', type=str, default='iou',\n        help='iou type (iou, giou, diou, ciou)',\n        dest='iou_type')\n    parser.add_argument(\n        '-keep-checkpoint-max', type=int, default=10,\n        help='maximum number of checkpoints to keep. If set 0, all checkpoints will be kept',\n        dest='keep_checkpoint_max')\n    args = vars(parser.parse_args())\n\n    # for k in args.keys():\n    #     cfg[k] = args.get(k)\n    cfg.update(args)\n\n    return edict(cfg)\n\n\ndef init_logger(log_file=None, log_dir=None, log_level=logging.INFO, mode='w', stdout=True):\n    \"\"\"\n    log_dir: 日志文件的文件夹路径\n    mode: 'a', append; 'w', 覆盖原文件写入.\n    \"\"\"\n    def get_date_str():\n        now = datetime.datetime.now()\n        return now.strftime('%Y-%m-%d_%H-%M-%S')\n\n    fmt = '%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s: %(message)s'\n    if log_dir is None:\n        log_dir = '~/temp/log/'\n    if log_file is None:\n        log_file = 'log_' + get_date_str() + '.txt'\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    log_file = os.path.join(log_dir, log_file)\n    # 此处不能使用logging输出\n    print('log file path:' + log_file)\n\n    logging.basicConfig(level=logging.DEBUG,\n                        format=fmt,\n                        filename=log_file,\n                        filemode=mode)\n\n    if stdout:\n        console = logging.StreamHandler(stream=sys.stdout)\n        console.setLevel(log_level)\n        formatter = logging.Formatter(fmt)\n        console.setFormatter(formatter)\n        logging.getLogger('').addHandler(console)\n\n    return logging\n\n\ndef _get_date_str():\n    now = datetime.datetime.now()\n    return now.strftime('%Y-%m-%d_%H-%M')\n\n\nif __name__ == \"__main__\":\n    logging = init_logger(log_dir='log')\n    cfg = get_args(**Cfg)\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfg.gpu\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logging.info(f'Using device {device}')\n\n    if cfg.use_darknet_cfg:\n        model = Darknet(cfg.cfgfile)\n    else:\n        model = Yolov4(cfg.pretrained, n_classes=cfg.classes)\n\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n    model.to(device=device)\n\n    try:\n        train(model=model,\n              config=cfg,\n              epochs=cfg.TRAIN_EPOCHS,\n              device=device, )\n    except KeyboardInterrupt:\n        if isinstance(model, torch.nn.DataParallel):\n            torch.save(model.module.state_dict(), 'INTERRUPTED.pth')\n        else:\n            torch.save(model.state_dict(), 'INTERRUPTED.pth')\n        logging.info('Saved interrupt')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\n"
        }
      ]
    }
  ]
}