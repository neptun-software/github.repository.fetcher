{
  "metadata": {
    "timestamp": 1736559943139,
    "page": 715,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "net4people/bbs",
      "stars": 3536,
      "defaultBranch": "master",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.9013671875,
          "content": "# [BBS](https://github.com/net4people/bbs/issues)\n\n### Net4People BBS\nThe BBS is an inclusive and multilingual forum for public discussion about Internet censorship circumvention. It is a place for **developers and researchers** to discuss and share information, techniques, and research. Feel free to write in your own language; we will translate. To start a discussion topic, [open a new issue](https://github.com/net4people/bbs/issues/new).\n\n### Net4People论坛\n本BBS是一个包容的多语种论坛，用于公开讨论规避互联网审查的话题。欢迎各位**开发者和研究人员**讨论和分享有关互联网封锁的信息、技术及研究。欢迎你使用自己的语言，我们会翻译的。要发起一个讨论话题，请[创建一个新的issue](https://github.com/net4people/bbs/issues/new)。\n\n### Net4People BBS\nEl BBS es un servicio inclusivo y multilingüe para la discusión pública acerca de las formas de elusión de la censura en Internet. Es un espacio para que **desarrolladores e investigadores** conversen y compartan información, técnicas y resultados. Si prefieres, escribe en tu propio idioma y lo trataremos de traducir. Para iniciar un nuevo tema de discusión, por favor [crea una nueva \"issue\"](https://github.com/net4people/bbs/issues/new).\n\n### Net4People serwis BBS\nTen BBS jest otwartym i wielojęzycznym forum dla publicznej dyskusji na temat obchodzenia cenzury Internetowej. To miejsce, gdzie **programiści i badacze** mogą rozmawiać i dzielić informacje, sposoby, i wyniki badawcze. Jeśli wolisz, proszę pisz po swoim języku, a przetłumaczymy. Aby rozpocząć temat dyskusyjny, proszę [otwórz nowy issue](https://github.com/net4people/bbs/issues/new).\n\n### Net4People BBS\nDas BBS ist ein inklusives und vielsprachiges Forum für öffentliche Diskussion um Internetzensur und Zensurumgehung. Es ist ein Ort für **Entwickler und Forscher**, um Informationen, Techniken und Forschung zu teilen. Schreibe gerne in deiner Sprache; wir werden übersetzen. Um eine Diskussion zu starten, [starte ein \"issue\"](https://github.com/net4people/bbs/issues/new).\n\n### &rlm;انجمن Net4People&zwnj;\n\n&rlm;BBS یک انجمن فراگیر و چند زبانه برای بحث و گفتگوی عمومی در مورد دور زدن سانسور اینترنت است. این مکانی برای **توسعه دهندگان و محققان** است تا بحث کنند و اطلاعات، فنون و تحقیقات را به اشتراک بگذارند. با خیال راحت به زبان خود بنویسید؛ ما ترجمه خواهیم کرد. برای شروع یک موضوع بحث، [یک مسئله ی جدید ایجاد کنید](https://github.com/net4people/bbs/issues/new).&zwnj;\n\n### Net4People BBS\nO BBS é um forum inclusivo e multilíngue para discussão pública sobre como se evadir da censura na Internet. É um lugar para **desenvolvedores e pesquisadores** discutirem e compartilharem informações, técnicas e pesquisas. Sinta-se à vontade para escrever em seu próprio idioma, pois nós traduziremos. Para iniciar um tópico de discussão, [abra um novo problema](https://github.com/net4people/bbs/issues/new).\n\n### Net4People BBS\nBBS adalah forum inklusif dan multibahasa untuk diskusi publik tentang pengelakan sensor internet. Forum ini merupakan tempat bagi para **pengembang dan peneliti** untuk berdiskusi dan berbagi informasi, teknik, dan penelitian. Jangan ragu untuk menulis dalam bahasamu sendiri; kami akan menerjemahkannya. Untuk memulai topik diskusi, [buka isu baru](https://github.com/net4people/bbs/issues/new).\n\n### Net4People ဘီဘီအက်စ်\nဘီဘီအက်စ်ဆိုသည်မှာ အင်တာနက်ဆင်ဆာပိတ်ဆို့မှုများအား ကျော်ဖြတ်ခြင်းအတွက် ဆွေးနွေးနိုင်သည့် ဖိုရမ်တစ်ခုဖြစ်ပါသည်။ **သုတေသီတွေနဲ့ ဒီဗလိုပါတွေ** သတင်းအချက်အလက်၊ နည်းစနစ်နဲ့ စာတမ်းတွေ မျှဝေနိုင်\nသည့်နေရာတစ်ခုလည်းဖြစ်ပါသည်။သင်နားလည်တဲ့ ဘာသာစကားနဲ့ဝင်ရောက်ဆွေးနွေးနိုင်ပါသည်။ ကျွန်ုပ်တို့မှ ဘာသာပြန်ပေးပါမည်။\nအောက်က လင့်ကို နှိပ်ပြီးဆွေးနွေးမှုတစ်ခုစတင်နိုင်ပါသည်။\n[open a new issue](https://github.com/net4people/bbs/issues/new)\n\n### &rlm;منتدى Net4People&zwnj;\n&rlm;هَذَا الْمُنْتَدَى مَسَّاحَةٌ شَامِلَةٌ وَمُتَعَدِّدَةُ اللُّغَاتِ لِلنِّقَاشِ الْعَامِّ حَوْلَ تَجَاوُزِ رَقَابَةِ الإنترنت. يُمْكِنُ **لِلْمُطَوِّرِينَ وَالْبَاحِثِينَ** مُنَاقَشَةُ وَمُشَارَكَةُ الْمَعْلُومَاتِ، وَالتِّقْنِيَّاتِ، وَالْأَبْحَاثِ هُنَا. لَا تَتَرَدَّدْ/ي فِي الْكُتَّابَةِ بَلَغَتِك؛ سَنَقُومُ بِالتَّرْجَمَةِ. لِفَتْحِ نِقَاشِ جَديدٍ، [اِفْتَحْ/ي مُشَكَّلَةَ جَديدَةٍ](https://github.com/net4people/bbs/issues/new).&zwnj;\n\n----\n\n[Archives of this forum](https://archive.org/search.php?query=source%3A%22https%3A%2F%2Fgithub.com%2Fnet4people%2Fbbs%22&sort=-date), made using the [backup.py](backup.py) script. To make your own backup, [create a personal access token](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) and run:\n<pre><code>./backup.py -u <var>username</var>:<var>token</var> net4people/bbs net4people_bbs.zip</code></pre>\n"
        },
        {
          "name": "backup.py",
          "type": "blob",
          "size": 15.1376953125,
          "content": "#!/usr/bin/env python3\n\n# Usage: ./backup.py -u username:token net4people/bbs bbs-20201231.zip\n#\n# Downloads GitHub issues, comments, and labels using the GitHub REST API\n# (https://docs.github.com/en/free-pro-team@latest/rest). Saves output to a zip\n# file.\n#\n# The -u option controls authentication. You don't have to use it, but if you\n# don't, you will be limited to 60 API requests per hour. When you are\n# authenticated, you get 5000 API requests per hour. The \"token\" part is a\n# Personal Access Token, created at https://github.com/settings/tokens.\n# https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token\n# You don't have to enable any scopes for the token.\n\nimport datetime\nimport getopt\nimport itertools\nimport json\nimport os\nimport os.path\nimport sys\nimport tempfile\nimport time\nimport urllib.parse\nimport zipfile\n\nimport mistune\nimport requests\n\nBASE_URL = \"https://api.github.com/\"\n\n# https://docs.github.com/en/free-pro-team@latest/rest/overview/media-types\nMEDIATYPE = \"application/vnd.github.v3+json\"\n# https://docs.github.com/en/free-pro-team@latest/rest/reference/issues#list-repository-issues-preview-notices\nMEDIATYPE_REACTIONS = \"application/vnd.github.squirrel-girl-preview+json\"\n\nUNSET_ZIPINFO_DATE_TIME = zipfile.ZipInfo(\"\").date_time\n\ndef url_origin(url):\n    components = urllib.parse.urlparse(url)\n    return (components.scheme, components.netloc)\n\ndef check_url_origin(base, url):\n    assert url_origin(base) == url_origin(url), (base, url)\n\ndef datetime_to_zip_time(d):\n    return (d.year, d.month, d.day, d.hour, d.minute, d.second)\n\ndef timestamp_to_zip_time(timestamp):\n    return datetime_to_zip_time(datetime.datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%SZ\"))\n\ndef http_date_to_zip_time(timestamp):\n    # https://tools.ietf.org/html/rfc7231#section-7.1.1.1\n    # We only support the IMF-fixdate format.\n    return datetime_to_zip_time(datetime.datetime.strptime(timestamp, \"%a, %d %b %Y %H:%M:%S GMT\"))\n\n# https://docs.github.com/en/free-pro-team@latest/rest/overview/resources-in-the-rest-api#rate-limiting\n# Returns a datetime at which the rate limit will be reset, or None if not\n# currently rate limited.\ndef rate_limit_reset(r):\n    # A rate-limited response is one that has status code 403, an\n    # x-ratelimit-remaining header with a value of 0, and an x-ratelimit-reset\n    # header.\n    if r.status_code != 403:\n        return None\n\n    remaining = r.headers.get(\"x-ratelimit-remaining\")\n    if remaining is None:\n        return None\n    try:\n        if int(remaining) > 0:\n            return None\n    except ValueError:\n        return None\n\n    # If x-ratelimit-remaining is set, assume x-ratelimit-reset is set.\n    reset = r.headers[\"x-ratelimit-reset\"]\n    return datetime.datetime.utcfromtimestamp(int(r.headers[\"x-ratelimit-reset\"]))\n\ndef response_datetime(r):\n    dt = r.headers.get(\"date\")\n    return datetime.datetime.strptime(dt, \"%a, %d %b %Y %X %Z\")\n\ndef get(sess, url, mediatype, params={}):\n    # TODO: warn on 301 redirect? https://docs.github.com/en/free-pro-team@latest/rest/overview/resources-in-the-rest-api#http-redirects\n\n    while True:\n        print(url, end=\"\", flush=True)\n        try:\n            headers = {}\n            if mediatype is not None:\n                headers[\"Accept\"] = mediatype\n            r = sess.get(url, params=params, headers=headers)\n        except Exception as e:\n            print(f\" => {str(type(e))}\", flush=True)\n            raise\n\n        print(f\" => {r.status_code} {r.reason} {r.headers.get('x-ratelimit-used', '-')}/{r.headers.get('x-ratelimit-limit', '-')}\", flush=True)\n        reset = rate_limit_reset(r)\n        if reset is not None:\n            reset_seconds = (reset - response_datetime(r)).total_seconds()\n            print(f\"waiting {reset_seconds:.0f} s for rate limit, will resume at {reset.strftime('%Y-%m-%d %H:%M:%S')}\", flush=True)\n            time.sleep(reset_seconds)\n        else:\n            r.raise_for_status()\n            return r\n\n# https://docs.github.com/en/free-pro-team@latest/rest/overview/resources-in-the-rest-api#pagination\n# https://docs.github.com/en/free-pro-team@latest/guides/traversing-with-pagination\ndef get_paginated(sess, url, mediatype, params={}):\n    params = params.copy()\n    try:\n        del params[\"page\"]\n    except KeyError:\n        pass\n    params[\"per_page\"] = \"100\"\n\n    while True:\n        r = get(sess, url, mediatype, params)\n        yield r\n\n        next_link = r.links.get(\"next\")\n        if next_link is None:\n            break\n        next_url = next_link[\"url\"]\n        # The API documentation instructs us to follow the \"next\" link without\n        # interpretation, but at least ensure it refers to the same scheme and\n        # host.\n        check_url_origin(url, next_url)\n\n        url = next_url\n\n# If zi.date_time is UNSET_ZIPINFO_DATE_TIME, then it will be replaced with the\n# value of the HTTP response's Last-Modified header, if present.\ndef get_to_zipinfo(sess, url, z, zi, mediatype, params={}):\n    r = get(sess, url, mediatype, params)\n\n    if zi.date_time == UNSET_ZIPINFO_DATE_TIME:\n        last_modified = r.headers.get(\"Last-Modified\")\n        if last_modified is not None:\n            zi.date_time = http_date_to_zip_time(last_modified)\n\n    with z.open(zi, mode=\"w\") as f:\n        for chunk in r.iter_content(4096):\n            f.write(chunk)\n\n# Converts a list of path components into a string path, raising an exception if\n# any component contains a slash, is \".\" or \"..\", or is empty; or if the whole\n# path is empty. The checks are to prevent any file writes outside the\n# destination directory when the zip file is extracted. We rely on the\n# assumption that no other files in the zip file are symbolic links, which is\n# true because this program does not create symbolic links.\ndef make_zip_file_path(*components):\n    for component in components:\n        if \"/\" in component:\n            raise ValueError(\"path component contains a slash\")\n        if component == \"\":\n            raise ValueError(\"path component is empty\")\n        if component == \".\":\n            raise ValueError(\"path component is a self directory reference\")\n        if component == \"..\":\n            raise ValueError(\"path component is a parent directory reference\")\n    if not components:\n        raise ValueError(\"path is empty\")\n    return \"/\".join(components)\n\n# Fallback to mistune 1.0 renderer if mistune 2.0 is not installed\ntry:\n    mistuneRenderer = mistune.HTMLRenderer\nexcept AttributeError:\n    mistuneRenderer = mistune.Renderer\n# Custom mistune.Renderer that stores a list of all links encountered.\nclass LinkExtractionRenderer(mistuneRenderer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.links = []\n\n    def autolink(self, link, is_email=False):\n        self.links.append(link)\n        return super().autolink(link, is_email)\n\n    def image(self, src, title, alt_text):\n        self.links.append(src)\n        return super().image(src, title, alt_text)\n\n    def link(self, link, title, content=None):\n        self.links.append(link)\n        return super().link(link, title, content)\n\ndef markdown_extract_links(markdown):\n    renderer = LinkExtractionRenderer()\n    mistune.Markdown(renderer=renderer)(markdown) # Discard HTML output.\n    return renderer.links\n\n# Return seq with prefix stripped if it has such a prefix, or else None.\ndef strip_prefix(seq, prefix):\n    if len(seq) < len(prefix):\n        return None\n    for a, b in zip(seq, prefix):\n        if a != b:\n            return None\n    return seq[len(prefix):]\n\ndef split_url_path(path):\n    return tuple(urllib.parse.unquote(component) for component in path.split(\"/\"))\n\ndef strip_url_path_prefix(path, prefix):\n    return strip_prefix(split_url_path(path), split_url_path(prefix))\n\n# If url is one we want to download, return a list of path components for the\n# path we want to store it at.\ndef link_is_wanted(url):\n    try:\n        components = urllib.parse.urlparse(url)\n    except ValueError:\n        return None\n\n    if components.scheme == \"https\" and components.netloc == \"user-images.githubusercontent.com\":\n        subpath = strip_url_path_prefix(components.path, \"\")\n        if subpath is not None:\n            # Inline image.\n            return (\"user-images.githubusercontent.com\", *subpath)\n    if components.scheme == \"https\" and components.netloc == \"github.com\":\n        for prefix in (f\"/{owner}/{repo}/files\", \"/user-attachments/files\"):\n            subpath = strip_url_path_prefix(components.path, prefix)\n            if subpath is not None:\n                # File attachment.\n                return (\"files\", *subpath)\n    if components.scheme == \"https\" and components.netloc == \"avatars.githubusercontent.com\":\n        path = components.path\n        if components.query:\n            # Avatar URLs often differ in the presence or absence of query\n            # parameters. Save the query string with the path, just in case they\n            # differ.\n            path += \"?\" + components.query\n        subpath = strip_url_path_prefix(path, \"\")\n        if subpath is not None:\n            # Avatar image.\n            return (\"avatars.githubusercontent.com\", *subpath)\n\ndef backup(owner, repo, z, username, token):\n    paths_seen = set()\n    # Calls make_zip_file_path, and additionally raises an exception if the path\n    # has already been used.\n    def check_path(*components):\n        path = make_zip_file_path(*components)\n        if path in paths_seen:\n            raise ValueError(f\"duplicate filename {path!a}\")\n        paths_seen.add(path)\n        return path\n\n    # Escape owner and repo suitably for use in a URL.\n    owner = urllib.parse.quote(owner, safe=\"\")\n    repo = urllib.parse.quote(repo, safe=\"\")\n\n    now = datetime.datetime.utcnow()\n    z.writestr(check_path(\"README\"), f\"\"\"\\\nArchive of the GitHub repository https://github.com/{owner}/{repo}/\nmade {now.strftime(\"%Y-%m-%d %H:%M:%S\")}.\n\"\"\")\n\n    file_urls = set()\n\n    # HTTP Basic authentication for API.\n    sess = requests.Session()\n    sess.auth = requests.auth.HTTPBasicAuth(username, token)\n\n    # https://docs.github.com/en/free-pro-team@latest/rest/reference/issues#list-repository-issues\n    issues_url = urllib.parse.urlparse(BASE_URL)._replace(\n        path=f\"/repos/{owner}/{repo}/issues\",\n    ).geturl()\n    for r in get_paginated(sess, issues_url, MEDIATYPE_REACTIONS, {\"sort\": \"created\", \"direction\": \"asc\"}):\n        for issue in r.json():\n            check_url_origin(BASE_URL, issue[\"url\"])\n            zi = zipfile.ZipInfo(check_path(\"issues\", str(issue[\"id\"]) + \".json\"), timestamp_to_zip_time(issue[\"created_at\"]))\n            get_to_zipinfo(sess, issue[\"url\"], z, zi, MEDIATYPE_REACTIONS)\n\n            # Re-open the JSON file we just wrote, to parse it for links.\n            with z.open(zi) as f:\n                data = json.load(f)\n                for link in itertools.chain(markdown_extract_links(data[\"body\"] or \"\"), [data[\"user\"][\"avatar_url\"]]):\n                    link = urllib.parse.urlunparse(urllib.parse.urlparse(link)._replace(fragment = None)) # Discard fragment.\n                    dest = link_is_wanted(link)\n                    if dest is not None:\n                        file_urls.add((dest, link))\n\n            # There's no API for getting all reactions in a repository, so get\n            # them per issue and per comment.\n            # https://docs.github.com/en/free-pro-team@latest/rest/reference/reactions#list-reactions-for-an-issue\n            reactions_url = issue[\"reactions\"][\"url\"]\n            check_url_origin(BASE_URL, reactions_url)\n            for r2 in get_paginated(sess, reactions_url, MEDIATYPE_REACTIONS):\n                for reaction in r2.json():\n                    zi = zipfile.ZipInfo(check_path(\"issues\", str(issue[\"id\"]), \"reactions\", str(reaction[\"id\"]) + \".json\"), timestamp_to_zip_time(reaction[\"created_at\"]))\n                    with z.open(zi, mode=\"w\") as f:\n                        f.write(json.dumps(reaction).encode(\"utf-8\"))\n\n    # https://docs.github.com/en/free-pro-team@latest/rest/reference/issues#list-issue-comments-for-a-repository\n    # Comments are linked to their parent issue via the issue_url field.\n    comments_url = urllib.parse.urlparse(BASE_URL)._replace(\n        path=f\"/repos/{owner}/{repo}/issues/comments\",\n    ).geturl()\n    for r in get_paginated(sess, comments_url, MEDIATYPE_REACTIONS):\n        for comment in r.json():\n            check_url_origin(BASE_URL, comment[\"url\"])\n            zi = zipfile.ZipInfo(check_path(\"issues\", \"comments\", str(comment[\"id\"]) + \".json\"), timestamp_to_zip_time(comment[\"created_at\"]))\n            get_to_zipinfo(sess, comment[\"url\"], z, zi, MEDIATYPE_REACTIONS)\n\n            # Re-open the JSON file we just wrote, to parse it for links.\n            with z.open(zi) as f:\n                data = json.load(f)\n                for link in itertools.chain(markdown_extract_links(data[\"body\"] or \"\"), [data[\"user\"][\"avatar_url\"]]):\n                    link = urllib.parse.urlunparse(urllib.parse.urlparse(link)._replace(fragment = None)) # Discard fragment.\n                    dest = link_is_wanted(link)\n                    if dest is not None:\n                        file_urls.add((dest, link))\n\n            # There's no API for getting all reactions in a repository, so get\n            # them per issue and per comment.\n            # https://docs.github.com/en/free-pro-team@latest/rest/reference/reactions#list-reactions-for-an-issue-comment\n            reactions_url = comment[\"reactions\"][\"url\"]\n            check_url_origin(BASE_URL, reactions_url)\n            for r2 in get_paginated(sess, reactions_url, MEDIATYPE_REACTIONS):\n                for reaction in r2.json():\n                    zi = zipfile.ZipInfo(check_path(\"issues\", \"comments\", str(comment[\"id\"]), \"reactions\", str(reaction[\"id\"]) + \".json\"), timestamp_to_zip_time(reaction[\"created_at\"]))\n                    with z.open(zi, mode=\"w\") as f:\n                        f.write(json.dumps(reaction).encode(\"utf-8\"))\n\n            # TODO: comment edit history (if possible)\n\n    labels_url = urllib.parse.urlparse(BASE_URL)._replace(\n        path=f\"/repos/{owner}/{repo}/labels\",\n    ).geturl()\n    for r in get_paginated(sess, labels_url, MEDIATYPE):\n        for label in r.json():\n            check_url_origin(BASE_URL, label[\"url\"])\n            zi = zipfile.ZipInfo(check_path(\"labels\", str(label[\"id\"]) + \".json\"))\n            get_to_zipinfo(sess, label[\"url\"], z, zi, MEDIATYPE)\n\n    # A new session, without Basic auth, for downloading plain files.\n    sess = requests.Session()\n\n    for dest, url in sorted(file_urls):\n        zi = zipfile.ZipInfo(check_path(*dest))\n        get_to_zipinfo(sess, url, z, zi, None)\n\nif __name__ == \"__main__\":\n    opts, (repo, zip_filename) = getopt.gnu_getopt(sys.argv[1:], \"u:\")\n    for o, a in opts:\n        if o == \"-u\":\n            username, token = a.split(\":\", 1)\n        elif o in (\"-h\", \"--help\"):\n            pass\n\n    owner, repo = repo.split(\"/\", 1)\n\n    # Write to a temporary file, then rename to the requested name when\n    # finished.\n    with tempfile.NamedTemporaryFile(dir=os.path.dirname(zip_filename), suffix=\".zip\", delete=False) as f:\n        try:\n            with zipfile.ZipFile(f, mode=\"w\") as z:\n                backup(owner, repo, z, username, token)\n            os.rename(f.name, zip_filename)\n        except:\n            # Delete output zip file on error.\n            os.remove(f.name)\n            raise\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.029296875,
          "content": "mistune >=2.0\nrequests >=2.25\n"
        }
      ]
    }
  ]
}