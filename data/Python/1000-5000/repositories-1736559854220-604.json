{
  "metadata": {
    "timestamp": 1736559854220,
    "page": 604,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OpenRLHF/OpenRLHF",
      "stars": 3690,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.921875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/.build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# IDE\n.idea/\n.vscode/\n\n# macos\n*.DS_Store\n#data/\n\ndocs/.build\n\n# pytorch checkpoint\n*.pt\n\ncore\n*/ckpt/*\n.vscode\n.nfs*\n*jianh*\n*test_scripts*\n*/checkpoint/*"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.8779296875,
          "content": "default_language_version:\n  python: python3\n\nci:\n  autofix_prs: true\n  autoupdate_commit_msg: '[pre-commit.ci] pre-commit suggestions'\n  autoupdate_schedule: quarterly\n\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: check-yaml\n      - id: check-case-conflict\n      - id: detect-private-key\n      - id: check-added-large-files\n        args: ['--maxkb=1000']\n      - id: requirements-txt-fixer\n\n  - repo: https://github.com/PyCQA/autoflake\n    rev: v2.0.2\n    hooks:\n      - id: autoflake\n        args: [--remove-all-unused-imports, --in-place]\n\n  - repo: https://github.com/PyCQA/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n        name: Format imports\n        exclude: docs/\n\n  - repo: https://github.com/psf/black\n    rev: 24.3.0\n    hooks:\n      - id: black\n        name: Format code\n        additional_dependencies: ['click==8.0.2']\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.1416015625,
          "content": "# Contributing to OpenRLHF\n\nAfter cloning the repository, please install pre-commit hooks with:\n```\npip install pre-commit\npre-commit install\n```"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08984375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.865234375,
          "content": "<div align=\"center\">\n    <img alt=\"OpenRLHF logo\" src=\"./docs/logo.png\" style=\"height: 140px;\" />\n</div>\n<div align=\"center\">\n<p align=\"center\">\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n        <img alt=\"GitHub Contributors\" src=\"https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/issues\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/discussions\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/pulls\">\n        <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff\" />\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/stargazers\">\n        <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf\" />\n      </a>\n      <br>\n      <em>Open-source / Comprehensive / Lightweight / Easy-to-use</em>\n    </p>\n</p>\n</div>\n\n<hr>\n\n<span>[ English | <a href=\"README_zh.md\">中文</a> | <a href=\"README_ja.md\">日本語</a> ]</span>\n\nOpenRLHF is a high-performance RLHF framework built on Ray, DeepSpeed and HF Transformers:\n\n- **Simple and easy to use**: OpenRLHF is one of the simplest high-performance RLHF libraries currently available, and seamlessly compatible with Huggingface models and datasets.\n- **High performance**: RLHF training spends 80% of the time on the sample generation stage. Thanks to the ability to use a large inference batch size with Ray and Packing Samples and vLLM generation acceleration, the performance of OpenRLHF 3~4x+ that of Optimized DeepSpeedChat with Hybrid Engine.\n- **Distributed RLHF**:  OpenRLHF distribute the Actor, Reward, Reference, and Critic models onto separate GPUs using Ray, while placing the Adam optimizer on the CPU. This enables full-scale fine-tuning of 70B+ models with multiple A100 80G GPUs and vLLM and 7B models across multiple 24GB RTX 4090 GPUs.\n- **PPO Implementation Optimization**: We integrated the implementation tricks for PPO to improve the training stability, referencing [Zhihu](https://zhuanlan.zhihu.com/p/622134699) and the [Notion blog](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361).\n\nMore details are in [Slides](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [Technical Report](https://arxiv.org/abs/2405.11143) | [Documents](https://openrlhf.readthedocs.io/)\n\n## News\n- [2024/12] We \"proposed\" 😊 the [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS).\n- [2024/12] We analyzed the PPO, REINFORCE++, GRPO and RLOO in the [Notion Blogpost](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05).\n\n\n## Features\n\n- Distributed [PPO](./examples/scripts/train_ppo_llama_ray.sh) and [REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray.sh) implementations based on Ray.  \n- Full RLHF fine-tuning support for models with [over 70 billion parameters](./examples/scripts/train_ppo_llama_ray_70b.sh).  \n- Integration with vLLM for accelerated generation in RLHF tasks (`--vllm_num_engines`).  \n- Support for multiple reward models (`--reward_pretrain model1,model2...`) and remote reward models (`--remote_rm_url`).  \n- Implementation of [DPO (Direct Preference Optimization)/IPO/cDPO](./examples/scripts/train_dpo_llama.sh) and [Kahneman-Tversky Optimization (KTO)](./examples/scripts/train_kto_llama.sh).  \n- Support for [Iterative DPO](./examples/scripts/train_iterative_dpo_llama.sh) ([GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)).  \n- Support for [Rejection Sampling](./examples/scripts/train_rejection_sampling_llama.sh).  \n- Implementation of [Conditional SFT](./examples/scripts/train_conditional_llama.sh) ([arXiv:2308.12050](https://arxiv.org/abs/2308.12050)).  \n- Support for [Knowledge Distillation](./examples/scripts/train_knowledge_distillation.sh) ([Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)).  \n- Integration of [Process Reward Model (PRM)](./examples/scripts/train_prm_mistral.sh).  \n- Packing of training samples for SFT, DPO, RM, PRM, and PPO (`--packing_samples`).  \n- Implementation of [RingAttention](./examples/scripts/train_dpo_ring_llama.sh) (`--ring_attn_size`, `--ring_head_stride`).  \n- Support for [Mixture of Experts (MoE)](./examples/test_scripts/train_sft_mixtral_lora.sh) (`--aux_loss_coef`).  \n- Integration of FlashAttention2 (`--flash_attn`).  \n- Support for QLoRA (`--load_in_4bit`) and [LoRA](./examples/scripts/train_sft_mixtral_lora.sh) (`--lora_rank`, `--target_modules`).  \n- Compatibility with HuggingFace's `tokenizer.apply_chat_template` for datasets (`--apply_chat_template` and `--input_key`).  \n- Logging support with Wandb (`--use_wandb`) and TensorBoard (`--use_tensorboard`).  \n- Checkpoint recovery functionality (`--load_checkpoint` and `--save_steps`).  \n- Provided multi-node training scripts, such as [DPO](./examples/scripts/train_llama_slurm.sh) and [Ray PPO](./examples/scripts/train_ppo_llama_ray_slurm.sh).\n\n\n### PPO Support Matrix\n\n| Feature | OpenRLHF | DSChat | CAIChat | TRL |\n| ------------- |:-------------:| :-------------:| :-------------:| :-------------:|\n| 70B+ Full Tuning with 16 A100-80GB      | ✅ | ❌ | ❌ | ❌ |\n| 7B Full Tuning with 4 RTX4090 | ✅      |    ❌ | ❌ | ❌ |\n| 34B DPO Full Tuning with 8 A100-80GB | ✅      |    ❌ | ❌ | ❌ |  \n| Inference Engine in PPO | ✅      |    ✅ | ❌ | ❌ |  \n| PPO Implementation Tricks | ✅      |    ❌ | ❌ | ✅ |\n| Support QLoRA | ✅      |    ❌ | ❌ | ✅ | \n| Support Mixtral 8*7b | ✅      |    ❌ | ❌ | ❌ |  \n| Support Unmerged Actor-Critic | ✅     |   ✅ | ✅ | ❌ | \n| Support Multiple Reward Models | ✅      |    ❌ | ❌ | ❌ |   \n| Support Huggingface Models | ✅      |    ✅ | ✅ | ✅ | \n| Easy-to-use | ✅      |   ❌ (HybridEngine bugs) | ✅ | ✅ | \n\n\n## Quick Start\n\n### Installation\n\nTo use OpenRLHF, first launch the docker container (**Recommended**) and `pip install` openrlhf inside the docker container:\n\n```bash\n# Launch the docker container\ndocker run --runtime=nvidia -it --rm --shm-size=\"10g\" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash\nsudo pip uninstall xgboost transformer_engine flash_attn -y\n\n# pip install\npip install openrlhf\n\n# If you want to use vLLM acceleration (Install vLLM 0.6.5)\npip install openrlhf[vllm]\n# latest vLLM is also supported\npip install openrlhf[vllm_latest]\n\n# pip install the latest version\npip install git+https://github.com/OpenRLHF/OpenRLHF.git\n\n# Or git clone\ngit clone https://github.com/OpenRLHF/OpenRLHF.git\ncd OpenRLHF\npip install -e .\n```\n\n> [!NOTE]\n>We recommend using vLLM 0.6.4 or higher. Other versions (vLLM >= 0.4.2) may require weight synchronization via Gloo (`--vllm_sync_backend gloo`).\n>We also provided the [Dockerfiles for vLLM](./dockerfile/) and [One-Click Installation Script of Nvidia-Docker](./examples/scripts/nvidia_docker_install.sh).\n\n### Prepare Datasets\nOpenRLHF provides multiple data processing methods in our dataset classes.\nSuch as in the [Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6):\n\n```python\ndef preprocess_data(data, input_template=None, input_key=\"input\", apply_chat_template=None) -> str:\n    if apply_chat_template:\n        chat = data[input_key]\n        if isinstance(chat, str):\n            chat = [{\"role\": \"user\", \"content\": chat}]\n        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    else:\n        prompt = data[input_key]\n        if input_template:\n            prompt = input_template.format(prompt)\n    return prompt\n```\n\n- We can use `--input_key` to specify the `JSON key name` of the input datasets `--prompt_data {name or path}` (PPO) or `--dataset {name or path}`, and use `--apply_chat_template` to utilize the `chat_template` from the [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating).\n- If you don't want to use `--apply_chat_template`, you can use `--input_template` instead, or preprocess the datasets offline in advance.\n- OpenRLHF also support mixing multiple datasets using `--prompt_data_probs 0.1,0.4,0.5` (PPO) or `--dataset_probs 0.1,0.4,0.5`.\n\nHow Chat Templating Works:\n\n```python\ndataset = [{\"input_key\": [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]}]\n\ntokenizer.apply_chat_template(dataset[0][\"input_key\"], tokenize=False)\n\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\nHow to specify training and test datasets ?\n\nYou can specify it using the `data_type@data_dir` format. For example, the dataset can be set as `--dataset json@./data`.\n\n```\ndata\n├── test.jsonl\n└── train.jsonl\n```\n\n> [!NOTE]\n> By default, we use `train` and `test` as splits to distinguish training and testing datasets from Huggingface.\n> The ``JSON key`` options depends on the specific datasets. See [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) and [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)\n\n### Supervised Fine-tuning\n\nOpenRLHF's model checkpoint is fully compatible with HuggingFace models. You can specify the model name or path using `--pretrain  {name or path}`, `--reward_pretrain  {name or path}` and `--critic_pretrain  {name or path}`. We have provided some pre-trained checkpoints and datasets on [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF).\n\nThen you can use the startup scripts we provide in the [examples/scripts](./examples/scripts/) directory, or start the training using the following commands.\n\n```bash \ndeepspeed --module openrlhf.cli.train_sft \\\n   --max_len 4096 \\\n   --dataset Open-Orca/OpenOrca \\\n   --input_key question \\\n   --output_key response \\\n   --input_template $'User: {}\\nAssistant: ' \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 2 \\\n   --max_samples 500000 \\\n   --pretrain meta-llama/Meta-Llama-3-8B \\\n   --save_path ./checkpoint/llama3-8b-sft \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --zero_stage 2 \\\n   --max_epochs 1 \\\n   --packing_samples \\\n   --bf16 \\\n   --flash_attn \\\n   --learning_rate 5e-6 \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n# Support HF tokenizer.apply_chat_template\n# --apply_chat_template \n# --tokenizer_chat_template {HF Chat Template}\n\n# Support RingAttention\n# pip install ring_flash_attn\n#   --ring_attn_size 2 \\\n#   --ring_head_stride 2 \\\n\n# Can also be used for continued pre-training\n# --pretrain_mode\n```\n\n> [!NOTE]\n> OpenRLHF SFT/DPO/RewardModel/PPO trainers support `--packing_samples` [based on `--flash_attn`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)\n\n\n### Reward Model Training\n```bash\ndeepspeed --module openrlhf.cli.train_rm \\\n   --save_path ./checkpoint/llama3-8b-rm \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 1 \\\n   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n   --bf16 \\\n   --max_epochs 1 \\\n   --max_len 8192 \\\n   --zero_stage 3 \\\n   --learning_rate 9e-6 \\\n   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \\\n   --apply_chat_template \\\n   --chosen_key chosen \\\n   --rejected_key rejected \\\n   --flash_attn \\\n   --packing_samples \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n```\n\nIt is recommended to set the `--value_prefix_head` option of the Reward Model to `score`, so that we can load the model using `AutoModelForSequenceClassification`:\n\n```python\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n              reward_model_path,\n              num_labels=1,\n              torch_dtype=torch.bfloat16,\n              attn_implementation=\"flash_attention_2\",\n              use_cache=False,\n          )\ninputs = xxxx (Left Padding Input Tokens)\nreward = reward_model.model(*inputs).last_hidden_state\nreward = reward_model.score(reward)[:, -1]\n```\n\n### PPO without Ray\n\n```bash\ndeepspeed --module openrlhf.cli.train_ppo \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path ./checkpoint/llama-3-8b-rlhf \\\n  --save_steps -1 \\\n  --logging_steps 1 \\\n  --eval_steps -1 \\\n  --micro_train_batch_size 2 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 4 \\\n  --rollout_batch_size 1024 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 2 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --max_samples 100000 \\\n  --normalize_reward \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --use_wandb {wandb_token}\n\n# Support remote reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n```\n\n### PPO/REINFORCE++ with Ray and vLLM\n\nTo improve RLHF training speed or support 70B models, we can use the PPO with Ray and vLLM acceleration\n\n```bash\n# launch the master node of ray in container\nray start --head --node-ip-address 0.0.0.0 --num-gpus 8\n\n# if you want to launch ray on more nodes, use\nray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8\n\nray job submit --address=\"http://127.0.0.1:8265\" \\\n  --runtime-env-json='{\"working_dir\": \"/openrlhf\"}' \\\n  -- python3 -m openrlhf.cli.train_ppo_ray \\\n  --ref_num_nodes 1 \\\n  --ref_num_gpus_per_node 2 \\\n  --reward_num_nodes 1 \\\n  --reward_num_gpus_per_node 2 \\\n  --critic_num_nodes 1 \\\n  --critic_num_gpus_per_node 2 \\\n  --actor_num_nodes 1 \\\n  --actor_num_gpus_per_node 2 \\\n  --vllm_num_engines 2 \\\n  --vllm_tensor_parallel_size 2 \\\n  --colocate_critic_reward \\\n  --colocate_actor_ref \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path /openrlhf/examples/checkpoint/llama3-8b-rlhf \\\n  --micro_train_batch_size 8 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 16 \\\n  --rollout_batch_size 1024 \\\n  --max_samples 100000 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 3 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --normalize_reward \\\n  --packing_samples \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --use_wandb {wandb_token}\n\n# Support REINFORCE++  | RLOO\n# --advantage_estimator reinforce | rloo\n\n# Support remote reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n\n# Support N samples\n# --n_samples_per_prompt 4\n```\n> [!NOTE]\n> Do not set `--vllm_num_engines` means not using the vLLM engine.\n> You can also use ``setup_commands`` to let Ray automatically deploy the environment, such as `--runtime-env-json='{\"setup_commands\": [\"pip install openrlhf[vllm]\"]}'`.\n\n> [!NOTE]\n> RLOO in OPENRLHF is a modification based on REINFORCE++, differing from the original version.\n\n> [!NOTE]\n> If you you encounter an error related to index out of range when deepspeed sets up the GPU devices, you can try to set the environment variable [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py) as a workaround.\n>   ```bash\n>   # For NVIDIA GPUs:\n>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1\n>   ```\n\nThe launch scripts and documents for supported algorithms are in [example/scripts](./examples/scripts/) and [Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)\n\n### LoRA\nIf you use `LoRA (Low-Rank Adaptation)`, `OpenRLHF` will not save the full weights by default instead of `LoRA Adapter`. To continue in your task normally, you should combine the `Adapter` with weights of your base model\n\n```bash\npython -m openrlhf.cli.lora_combiner \\\n    --model_path meta-llama/Meta-Llama-3-8B \\\n    --lora_path ./checkpoint/llama3-8b-rm \\\n    --output_path ./checkpoint/llama-3-8b-rm-combined \\\n    --is_rm \\\n    --bf16\n```\n\n## Performance\n\nWe optimized DSChat's performance to the greatest extent possible by employing techniques such as enabling Adam offload, along with reward model (RM) and reference model (Ref) offload to increase the micro-batch size during the inference stage and avoid out-of-memory issues. We even fixed some bugs in DSChat to enable the Hybrid Engine (HE) for LLaMA2. The average time (seconds) it took to train 1024 prompts with 1 PPO epoch using the Optimized DSChat and OpenRLHF:\n\n| **Size** | **NVIDIA A800-80GB GPUs** | **Optimized DSChat (with  Hybrid Engine)** | **OpenRLHF** | **Speedup** |\n| :---: | :---: | :---: | :---: | :---: |\n| 7B | 16 | 855.09 | 471.11 | 1.82x |\n| 13B | 32 | 1528.93 | 608.93 | 2.5x |\n| 34B | 32 | 3634.98 | 1526.4 | 2.4x |\n| 70B | 32 | 10407.0 | 4488.53 | 2.3x |\n\n> [!NOTE]\n> The data is outdated; please refer to the performance tuning section for re-testing.\n\n### Performance Tuning Guide\n\nTo achieve optimal performance, we recommend allocating more nodes to the vLLM Engine. For example, for a 70B model with 32 A100 GPUs, it is advised to allocate 16 A100 GPUs to the vLLM Engine, 8 GPUs to the Actor model, and the remaining 8 GPUs to the Critic model. Additionally, enable the `--colocate_critic_reward`, `--colocate_actor_ref` options to merge nodes. Finally, you should increase the `rollout_micro_batch_size` (and minimize the TP size of vLLM engine) as much as possible. During the training phase, a larger `--micro_train_batch_size` is better and enable `--packing_samples`. When there are enough GPUs, please disable `--adam_offload` and enable `--overlap_comm`. For multi-nodes RLHF, please use `--vllm_sync_backend nccl` with vLLM 0.6.4+.\n\n## Companies and Organizations using OpenRLHF\n\n- Google\n- ByteDance\n- Tencent\n- Alibaba\n- Baidu\n- China Telecom\n- Vivo\n- Allen AI\n- NexusFlow\n- Jülich Supercomputing Centre (JSC)\n- Berkeley Starling Team\n- M-A-P\n- ...\n\n## Join Us\n\n**How to Join?**\n\n1. Email us at janhu9527@gmail.com or join [GitHub Organization](https://github.com/OpenRLHF). Please include the following details:\n   - Your name\n   - Your GitHub username\n   - Your areas of interest\n   - Your skills and experience related to NLP and/or AI\n1. You can also join us through the official GitHub [OpenRLHF ↗](https://github.com/OpenRLHF/OpenRLHF) project page. Just create an issue about your interest to contribute and we will get back to you.\n\n**What can you do?**\n\n1. Join the team and participate in the development of the OpenRLHF project.\n1. Contribute to the project by submitting pull requests.\n1. Help improve documentation, fix bugs, or create new features.\n1. Share the project and help us grow the community.\n\n## Sponsor Us\n\nYour sponsorship can help us maintain and improve OpenRLHF. If you find this project useful, please consider sponsoring us. You can sponsor us on [Open Collective ↗](https://opencollective.com/OpenRLHF).\n\n## Starchart\n\n[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)\n\n## Contributors\n\nA big thank you to all our contributors! If you want to contribute, feel free to make a pull request or create an issue.\n\n<a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF\" />\n</a>\n\n## References & Acknowledgements\n\nWe would like to express our gratitude to the following projects and organizations for their contributions to the field of AI and NLP:\n\n- [Hugging Face Transformers ↗](https://github.com/huggingface/transformers)\n- [OpenAI GPT ↗](https://github.com/openai/gpt-3)\n- [LLaMA ↗](https://llama.meta.com/)\n- [DeepSpeed ↗](https://github.com/microsoft/DeepSpeed)\n- [Ray ↗](https://github.com/ray-project/ray)\n\nOur project would also like to thank [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) and [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat). In the early stages of the project, we referred to their code design. \n\n(2024/7) Our GitHub organization has changed from OpenLLMAI to OpenRLHF.\n\n## Citation\n```\n@article{hu2024openrlhf,\n  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},\n  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},\n  journal={arXiv preprint arXiv:2405.11143},\n  year={2024}\n}\n```\n\n______________________________________________________________________\n\n*OpenRLHF © 2025 OpenRLHF. All Rights Reserved.*\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 22.958984375,
          "content": "<div align=\"center\">\n    <img alt=\"OpenRLHF logo\" src=\"./docs/logo.png\" style=\"height: 140px;\" />\n</div>\n<div align=\"center\">\n<p align=\"center\">\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n        <img alt=\"GitHub Contributors\" src=\"https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/issues\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/discussions\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/pulls\">\n        <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff\" />\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/stargazers\">\n        <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf\" />\n      </a>\n      <br>\n      <em>オープンソース / 包括的 / 軽量 / 使いやすい</em>\n    </p>\n</p>\n</div>\n\n<hr>\n\n<span>[ <a href=\"README.md\">English</a> | <a href=\"README_zh.md\">中文</a> | 日本語 ]</span>\n\nOpenRLHFは、Ray、DeepSpeed、およびHF Transformersを基盤とした高性能なRLHFフレームワークです：\n\n- **シンプルで使いやすい**: OpenRLHFは現在利用可能な最もシンプルな高性能RLHFライブラリの一つであり、Huggingfaceのモデルとデータセットとシームレスに互換性があります。\n- **高性能**: RLHFトレーニングの80％の時間はサンプル生成段階に費やされます。RayとPacking SamplesおよびvLLM生成加速の能力を活用することで、OpenRLHFのパフォーマンスはOptimized DeepSpeedChat with Hybrid Engineの3〜4倍以上です。\n- **分散RLHF**: OpenRLHFは、Actor、Reward、Reference、およびCriticモデルをRayを使用して別々のGPUに分散し、AdamオプティマイザをCPUに配置します。これにより、複数のA100 80G GPUとvLLMを使用して70B+モデルのフルスケールの微調整が可能になり、複数の24GB RTX 4090 GPUで7Bモデルを微調整できます。\n- **PPO実装の最適化**: トレーニングの安定性を向上させるために、PPOの実装トリックを統合しました。詳細は[Zhihu](https://zhuanlan.zhihu.com/p/622134699)および[Notionブログ](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361)を参照してください。\n\n詳細は[スライド](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [技術報告](https://arxiv.org/abs/2405.11143) | [ドキュメント](https://openrlhf.readthedocs.io/)をご覧ください。\n\n## ニュース\n- [2024/12] 私たちは😊 [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS)を「提案」しました。\n- [2024/12] [Notionブログ](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05)でPPO、REINFORCE++、GRPO、およびRLOOを分析しました。\n\n## 特徴\n\n- Rayに基づく分散[ PPO](./examples/scripts/train_ppo_llama_ray.sh)および[REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray.sh)の実装。\n- [70億以上のパラメータを持つモデル](./examples/scripts/train_ppo_llama_ray_70b.sh)の完全なRLHF微調整のサポート。\n- RLHFタスクでの生成を加速するためのvLLMの統合（`--vllm_num_engines`）。\n- 複数の報酬モデル（`--reward_pretrain model1,model2...`）およびリモート報酬モデル（`--remote_rm_url`）のサポート。\n- [DPO（直接選好最適化）/IPO/cDPO](./examples/scripts/train_dpo_llama.sh)および[Kahneman-Tversky Optimization（KTO）](./examples/scripts/train_kto_llama.sh)の実装。\n- [反復DPO](./examples/scripts/train_iterative_dpo_llama.sh)（[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)）のサポート。\n- [拒否サンプリング](./examples/scripts/train_rejection_sampling_llama.sh)のサポート。\n- [条件付きSFT](./examples/scripts/train_conditional_llama.sh)（[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)）の実装。\n- [知識蒸留](./examples/scripts/train_knowledge_distillation.sh)（[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)）のサポート。\n- [プロセス報酬モデル（PRM）](./examples/scripts/train_prm_mistral.sh)の統合。\n- SFT、DPO、RM、PRM、およびPPOのトレーニングサンプルのパッキング（`--packing_samples`）。\n- [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)の実装（`--ring_attn_size`、`--ring_head_stride`）。\n- [専門家の混合モデル（MoE）](./examples/test_scripts/train_sft_mixtral_lora.sh)のサポート（`--aux_loss_coef`）。\n- FlashAttention2の統合（`--flash_attn`）。\n- QLoRA（`--load_in_4bit`）および[LoRA](./examples/scripts/train_sft_mixtral_lora.sh)（`--lora_rank`、`--target_modules`）のサポート。\n- HuggingFaceの`tokenizer.apply_chat_template`との互換性（`--apply_chat_template`および`--input_key`）。\n- Wandb（`--use_wandb`）およびTensorBoard（`--use_tensorboard`）によるログ記録のサポート。\n- チェックポイントの回復機能（`--load_checkpoint`および`--save_steps`）。\n- [DPO](./examples/scripts/train_llama_slurm.sh)および[Ray PPO](./examples/scripts/train_ppo_llama_ray_slurm.sh)などのマルチノードトレーニングスクリプトを提供。\n\n### PPOサポートマトリックス\n\n| 特徴 | OpenRLHF | DSChat | CAIChat | TRL |\n| ------------- |:-------------:| :-------------:| :-------------:| :-------------:|\n| 16 A100-80GBで70B+のフルチューニング      | ✅ | ❌ | ❌ | ❌ |\n| 4 RTX4090で7Bのフルチューニング | ✅      |    ❌ | ❌ | ❌ |\n| 8 A100-80GBで34B DPOのフルチューニング | ✅      |    ❌ | ❌ | ❌ |  \n| PPOでの推論エンジンのサポート | ✅      |    ✅ | ❌ | ❌ |  \n| PPO実装のトリック | ✅      |    ❌ | ❌ | ✅ |\n| QLoRAのサポート | ✅      |    ❌ | ❌ | ✅ | \n| Mixtral 8*7bのサポート | ✅      |    ❌ | ❌ | ❌ |  \n| 未結合のActor-Criticのサポート | ✅     |   ✅ | ✅ | ❌ | \n| 複数の報酬モデルのサポート | ✅      |    ❌ | ❌ | ❌ |   \n| Huggingfaceモデルのサポート | ✅      |    ✅ | ✅ | ✅ | \n| 使いやすさ | ✅      |   ❌ (HybridEngineのバグ) | ✅ | ✅ | \n\n## クイックスタート\n\n### インストール\n\nOpenRLHFを使用するには、まずDockerコンテナを起動し（**推奨**）、Dockerコンテナ内で`pip install`を実行してopenrlhfをインストールします：\n\n```bash\n# Dockerコンテナを起動\ndocker run --runtime=nvidia -it --rm --shm-size=\"10g\" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash\nsudo pip uninstall xgboost transformer_engine flash_attn -y\n\n# pip install\npip install openrlhf\n\n# vLLM加速を使用する場合（vLLM 0.6.5をインストール）\npip install openrlhf[vllm]\n# 最新のvLLMもサポートされています\npip install openrlhf[vllm_latest]\n\n# 最新バージョンをpip install\npip install git+https://github.com/OpenRLHF/OpenRLHF.git\n\n# またはgit clone\ngit clone https://github.com/OpenRLHF/OpenRLHF.git\ncd OpenRLHF\npip install -e .\n```\n\n> [!NOTE]\n>vLLM 0.6.4以降の使用をお勧めします。他のバージョン（vLLM >= 0.4.2）は、Glooを介して重みの同期が必要な場合があります（`--vllm_sync_backend gloo`）。\n>また、[vLLM用のDockerfile](./dockerfile/)および[Nvidia-Dockerのワンクリックインストールスクリプト](./examples/scripts/nvidia_docker_install.sh)も提供しています。\n\n### データセットの準備\nOpenRLHFは、データセットクラス内で複数のデータ処理方法を提供しています。\n例えば、[Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6)では：\n\n```python\ndef preprocess_data(data, input_template=None, input_key=\"input\", apply_chat_template=None) -> str:\n    if apply_chat_template:\n        chat = data[input_key]\n        if isinstance(chat, str):\n            chat = [{\"role\": \"user\", \"content\": chat}]\n        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    else:\n        prompt = data[input_key]\n        if input_template:\n            prompt = input_template.format(prompt)\n    return prompt\n```\n\n- `--input_key`を使用して、入力データセットの`JSON key name`を指定し、`--prompt_data {name or path}`（PPO）または`--dataset {name or path}`を使用し、`--apply_chat_template`を使用して[Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating)の`chat_template`を利用できます。\n- `--apply_chat_template`を使用したくない場合は、代わりに`--input_template`を使用するか、事前にデータセットをオフラインで前処理することができます。\n- OpenRLHFは、`--prompt_data_probs 0.1,0.4,0.5`（PPO）または`--dataset_probs 0.1,0.4,0.5`を使用して複数のデータセットを混合することもサポートしています。\n\nChat Templatingの動作方法：\n\n```python\ndataset = [{\"input_key\": [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]}]\n\ntokenizer.apply_chat_template(dataset[0][\"input_key\"], tokenize=False)\n\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\nトレーニングおよびテストデータセットの指定方法：\n\n`data_type@data_dir`形式を使用して指定できます。例えば、データセットは`--dataset json@./data`として設定できます。\n\n```\ndata\n├── test.jsonl\n└── train.jsonl\n```\n\n> [!NOTE]\n> デフォルトでは、`train`および`test`を使用してHuggingfaceのトレーニングおよびテストデータセットを区別します。\n> `JSON key`オプションは特定のデータセットに依存します。詳細は[Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10)および[SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)を参照してください。\n\n### 教師あり微調整\n\nOpenRLHFのモデルチェックポイントはHuggingFaceモデルと完全に互換性があります。`--pretrain  {name or path}`、`--reward_pretrain  {name or path}`、および`--critic_pretrain  {name or path}`を使用してモデル名またはパスを指定できます。いくつかの事前トレーニング済みチェックポイントとデータセットを[HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF)で提供しています。\n\n次に、[examples/scripts](./examples/scripts/)ディレクトリに提供されている起動スクリプトを使用するか、以下のコマンドを使用してトレーニングを開始できます。\n\n```bash \ndeepspeed --module openrlhf.cli.train_sft \\\n   --max_len 4096 \\\n   --dataset Open-Orca/OpenOrca \\\n   --input_key question \\\n   --output_key response \\\n   --input_template $'User: {}\\nAssistant: ' \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 2 \\\n   --max_samples 500000 \\\n   --pretrain meta-llama/Meta-Llama-3-8B \\\n   --save_path ./checkpoint/llama3-8b-sft \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --zero_stage 2 \\\n   --max_epochs 1 \\\n   --packing_samples \\\n   --bf16 \\\n   --flash_attn \\\n   --learning_rate 5e-6 \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n# HF tokenizer.apply_chat_templateのサポート\n# --apply_chat_template \n# --tokenizer_chat_template {HF Chat Template}\n\n# RingAttentionのサポート\n# pip install ring_flash_attn\n#   --ring_attn_size 2 \\\n#   --ring_head_stride 2 \\\n\n# 継続的な事前トレーニングにも使用できます\n# --pretrain_mode\n```\n\n> [!NOTE]\n> OpenRLHF SFT/DPO/RewardModel/PPOトレーナーは`--packing_samples`をサポートしています [`--flash_attn`に基づく](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)\n\n### 報酬モデルのトレーニング\n```bash\ndeepspeed --module openrlhf.cli.train_rm \\\n   --save_path ./checkpoint/llama3-8b-rm \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 1 \\\n   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n   --bf16 \\\n   --max_epochs 1 \\\n   --max_len 8192 \\\n   --zero_stage 3 \\\n   --learning_rate 9e-6 \\\n   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \\\n   --apply_chat_template \\\n   --chosen_key chosen \\\n   --rejected_key rejected \\\n   --flash_attn \\\n   --packing_samples \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n```\n\n報酬モデルの`--value_prefix_head`オプションを`score`に設定することをお勧めします。これにより、`AutoModelForSequenceClassification`を使用してモデルをロードできます：\n\n```python\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n              reward_model_path,\n              num_labels=1,\n              torch_dtype=torch.bfloat16,\n              attn_implementation=\"flash_attention_2\",\n              use_cache=False,\n          )\ninputs = xxxx (Left Padding Input Tokens)\nreward = reward_model.model(*inputs).last_hidden_state\nreward = reward_model.score(reward)[:, -1]\n```\n\n### Rayを使用しないPPO\n\n```bash\ndeepspeed --module openrlhf.cli.train_ppo \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path ./checkpoint/llama-3-8b-rlhf \\\n  --save_steps -1 \\\n  --logging_steps 1 \\\n  --eval_steps -1 \\\n  --micro_train_batch_size 2 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 4 \\\n  --rollout_batch_size 1024 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 2 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --max_samples 100000 \\\n  --normalize_reward \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --use_wandb {wandb_token}\n\n# リモート報酬モデルのサポート（HTTP）\n# --remote_rm_url http://localhost:5000/get_reward\n```\n\n### RayとvLLMを使用したPPO/REINFORCE++\n\nRLHFトレーニング速度を向上させるか、70Bモデルをサポートするために、RayとvLLM加速を使用したPPOを使用できます\n\n```bash\n# コンテナ内でRayのマスターノードを起動\nray start --head --node-ip-address 0.0.0.0 --num-gpus 8\n\n# さらに多くのノードでRayを起動する場合は\nray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8\n\nray job submit --address=\"http://127.0.0.1:8265\" \\\n  --runtime-env-json='{\"working_dir\": \"/openrlhf\"}' \\\n  -- python3 -m openrlhf.cli.train_ppo_ray \\\n  --ref_num_nodes 1 \\\n  --ref_num_gpus_per_node 2 \\\n  --reward_num_nodes 1 \\\n  --reward_num_gpus_per_node 2 \\\n  --critic_num_nodes 1 \\\n  --critic_num_gpus_per_node 2 \\\n  --actor_num_nodes 1 \\\n  --actor_num_gpus_per_node 2 \\\n  --vllm_num_engines 2 \\\n  --vllm_tensor_parallel_size 2 \\\n  --colocate_critic_reward \\\n  --colocate_actor_ref \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path /openrlhf/examples/checkpoint/llama3-8b-rlhf \\\n  --micro_train_batch_size 8 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 16 \\\n  --rollout_batch_size 1024 \\\n  --max_samples 100000 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 3 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --normalize_reward \\\n  --packing_samples \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --use_wandb {wandb_token}\n\n# REINFORCE++ | RLOOのサポート\n# --advantage_estimator reinforce | rloo\n\n# リモート報酬モデルのサポート（HTTP）\n# --remote_rm_url http://localhost:5000/get_reward\n\n\n# Nサンプルのサポート\n# --n_samples_per_prompt 4\n```\n> [!NOTE]\n> `--vllm_num_engines`を設定しない場合は、vLLMエンジンを使用しないことを意味します。\n> `setup_commands`を使用してRayが自動的に環境をデプロイすることもできます。例えば、`--runtime-env-json='{\"setup_commands\": [\"pip install openrlhf[vllm]\"]}'`。\n\n[!NOTE]\nOPENRLHFのRLOOは、REINFORCE++を基に改良されたものであり、オリジナル版とは異なります。\n\n> [!NOTE]\n> deepspeedがGPUデバイスを設定する際にインデックスが範囲外に関連するエラーが発生した場合、環境変数[`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py)を設定して回避策を試すことができます。\n>   ```bash\n>   # NVIDIA GPUの場合:\n>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1\n>   ```\n\nサポートされているアルゴリズムの起動スクリプトとドキュメントは[example/scripts](./examples/scripts/)および[Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)にあります。\n\n## パフォーマンス\n\nAdamオフロードの有効化、報酬モデル（RM）および参照モデル（Ref）オフロードなどの技術を使用して、DSChatのパフォーマンスを最大限に最適化し、推論段階でのマイクロバッチサイズを増やし、メモリ不足の問題を回避しました。LLaMA2のハイブリッドエンジン（HE）を有効にするために、DSChatのいくつかのバグも修正しました。Optimized DSChatとOpenRLHFを使用して1024のプロンプトを1つのPPOエポックでトレーニングするのにかかる平均時間（秒）は次のとおりです：\n\n| **サイズ** | **NVIDIA A800-80GB GPU** | **Optimized DSChat（ハイブリッドエンジン付き）** | **OpenRLHF** | **スピードアップ** |\n| :---: | :---: | :---: | :---: | :---: |\n| 7B | 16 | 855.09 | 471.11 | 1.82x |\n| 13B | 32 | 1528.93 | 608.93 | 2.5x |\n| 34B | 32 | 3634.98 | 1526.4 | 2.4x |\n| 70B | 32 | 10407.0 | 4488.53 | 2.3x |\n\n> [!NOTE]\n> データは古くなっています。再テストのためにパフォーマンスチューニングセクションを参照してください。\n\n### パフォーマンスチューニングガイド\n\n最適なパフォーマンスを達成するために、vLLMエンジンにより多くのノードを割り当てることをお勧めします。例えば、32個のA100 GPUを持つ70Bモデルの場合、16個のA100 GPUをvLLMエンジンに割り当て、8個のGPUをActorモデルに、残りの8個のGPUをCriticモデルに割り当てることをお勧めします。さらに、`--colocate_critic_reward`、`--colocate_actor_ref`オプションを有効にしてノードをマージします。最後に、`rollout_micro_batch_size`（およびvLLMエンジンのTPサイズを最小化）を可能な限り増やすべきです。トレーニングフェーズでは、より大きな`--micro_train_batch_size`が望ましく、`--packing_samples`を有効にします。十分なGPUがある場合、`--adam_offload`を無効にし、`--overlap_comm`を有効にします。マルチノードRLHFの場合、vLLM 0.6.4+で`--vllm_sync_backend nccl`を使用してください。\n\n## OpenRLHFを使用している企業と組織\n\n- Google\n- ByteDance\n- Tencent\n- Alibaba\n- Baidu\n- China Telecom\n- Vivo\n- Allen AI\n- NexusFlow\n- Jülich Supercomputing Centre (JSC)\n- Berkeley Starling Team\n- M-A-P\n- ...\n\n## 参加方法\n\n**参加方法**\n\n1. janhu9527@gmail.comにメールを送るか、[GitHub Organization](https://github.com/OpenRLHF)に参加してください。以下の詳細を含めてください：\n   - あなたの名前\n   - あなたのGitHubユーザー名\n   - あなたの興味のある分野\n   - NLPおよび/またはAIに関連するスキルと経験\n1. 公式GitHub[OpenRLHF ↗](https://github.com/OpenRLHF/OpenRLHF)プロジェクトページを通じて参加することもできます。貢献したい興味についてのissueを作成するだけで、私たちが連絡します。\n\n**何ができるか**\n\n1. チームに参加し、OpenRLHFプロジェクトの開発に参加します。\n1. プロジェクトに貢献するためにプルリクエストを提出します。\n1. ドキュメントの改善、バグの修正、新機能の作成を手伝います。\n1. プロジェクトを共有し、コミュニティの成長を支援します。\n\n## スポンサー\n\nスポンサーシップは、OpenRLHFの維持と改善に役立ちます。このプロジェクトが役立つと感じた場合は、スポンサーを検討してください。[Open Collective ↗](https://opencollective.com/OpenRLHF)でスポンサーになることができます。\n\n## スター履歴\n\n[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)\n\n## 貢献者\n\nすべての貢献者に感謝します！貢献したい場合は、プルリクエストを作成するか、issueを作成してください。\n\n<a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF\" />\n</a>\n\n## 参考文献と謝辞\n\nAIおよびNLP分野への貢献に対して、以下のプロジェクトおよび組織に感謝します：\n\n- [Hugging Face Transformers ↗](https://github.com/huggingface/transformers)\n- [OpenAI GPT ↗](https://github.com/openai/gpt-3)\n- [LLaMA ↗](https://llama.meta.com/)\n- [DeepSpeed ↗](https://github.com/microsoft/DeepSpeed)\n- [Ray ↗](https://github.com/ray-project/ray)\n\n私たちのプロジェクトは、[ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)および[DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)にも感謝します。プロジェクトの初期段階で、彼らのコードデザインを参考にしました。\n\n(2024/7) 私たちのGitHub組織はOpenLLMAIからOpenRLHFに変更されました。\n\n## 引用\n```\n@article{hu2024openrlhf,\n  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},\n  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},\n  journal={arXiv preprint arXiv:2405.11143},\n  year={2024}\n}\n```\n\n______________________________________________________________________\n\n*OpenRLHF © 2025 OpenRLHF. All Rights Reserved.*\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 20.1875,
          "content": "<div align=\"center\">\n<p align=\"center\">\n<img alt=\"\" src=\"./docs/logo.png\" style=\"display: inline-block; height: 140px\" />\n</p>\n</div>\n\n<div align=\"center\">\n<p align=\"center\">\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n        <img alt=\"GitHub Contributors\" src=\"https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/issues\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/discussions\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/pulls\">\n        <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff\" />\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/stargazers\">\n        <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf\" />\n      </a>\n      <br>\n      <em>开源 / 全面 / 轻量级 / 易用</em>\n    </p>\n</p>\n</div>\n\n<hr>\n\n<span>[ <a href=\"README.md\">English</a> | 中文 | <a href=\"README_ja.md\">日本語</a> ]</span>\n\nOpenRLHF 是一个基于 Ray、DeepSpeed 和 HF Transformers 构建的高性能 RLHF 框架：\n\n- **简单易用**: OpenRLHF 是目前可用的最简单的高性能 RLHF 库之一，无缝兼容 Huggingface 模型和数据集。\n- **高性能**: RLHF 训练中 80% 的时间用于样本生成阶段。得益于使用 Ray, Packing Samples 以及 vLLM 生成加速的能力，OpenRLHF 的性能是极致优化的 DeepSpeedChat with Hybrid Engine 的3~4倍以上。\n- **分布式 RLHF**:  OpenRLHF 使用 Ray 将 Actor、Reward、Reference 和 Critic 模型分布到不同的 GPU 上，同时将 Adam 优化器放在 CPU 上。这使得使用多个 A100 80G GPU 和 vLLM 可以全面微调超过 70B+ 的模型 以及在多个 24GB RTX 4090 GPU 上微调 7B 模型。\n- **PPO 实现技巧**: 我们集成了 PPO 的实现技巧以提高训练稳定性，详情参考 [知乎](https://zhuanlan.zhihu.com/p/622134699) 和 [Notion blog](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361).\n\n更多细节请参考 [PPT](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [技术报告](https://arxiv.org/abs/2405.11143) | [使用文档](https://openrlhf.readthedocs.io/)\n\n\n## 新闻  \n- [2024/12] 我们\"提出\"了 😊 [REINFORCE++ 对齐算法](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS).\n- [2024/12] 在 [Notion Blog](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05) 中，我们对 PPO、REINFORCE++、GRPO 和 RLOO 进行了分析。  \n\n## 特性  \n\n- 基于 Ray 的分布式 [PPO](./examples/scripts/train_ppo_llama_ray.sh) 和 [REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray.sh) 实现。  \n- 支持对 [超过 700 亿参数的模型](./examples/scripts/train_ppo_llama_ray_70b.sh) 进行完整的 RLHF 微调。  \n- 集成 vLLM，加速 RLHF 任务中的样本生成（`--vllm_num_engines`）。  \n- 支持多个奖励模型（`--reward_pretrain model1,model2...`）和远程奖励模型（`--remote_rm_url`）。  \n- 实现 [DPO（直接偏好优化）/IPO/cDPO](./examples/scripts/train_dpo_llama.sh) 和 [Kahneman-Tversky Optimization（KTO）](./examples/scripts/train_kto_llama.sh)。  \n- 支持 [迭代 DPO](./examples/scripts/train_iterative_dpo_llama.sh)（[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)）。  \n- 支持 [拒绝采样](./examples/scripts/train_rejection_sampling_llama.sh)。  \n- 实现 [条件 SFT](./examples/scripts/train_conditional_llama.sh)（[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)）。  \n- 支持 [知识蒸馏](./examples/scripts/train_knowledge_distillation.sh)（[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)）。  \n- 集成 [过程奖励模型（PRM）](./examples/scripts/train_prm_mistral.sh)。  \n- 支持 SFT、DPO、RM、PRM 和 PPO 的训练样本打包（`--packing_samples`）。  \n- 实现 [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)（`--ring_attn_size`，`--ring_head_stride`）。  \n- 支持 [专家混合模型（MoE）](./examples/test_scripts/train_sft_mixtral_lora.sh)（`--aux_loss_coef`）。  \n- 集成 FlashAttention2（`--flash_attn`）。  \n- 支持 QLoRA（`--load_in_4bit`）和 [LoRA](./examples/scripts/train_sft_mixtral_lora.sh)（`--lora_rank`，`--target_modules`）。  \n- 兼容 HuggingFace 的 `tokenizer.apply_chat_template` 数据集格式（`--apply_chat_template` 和 `--input_key`）。  \n- 支持使用 Wandb（`--use_wandb`）和 TensorBoard（`--use_tensorboard`）进行日志记录。  \n- 支持从检查点恢复训练（`--load_checkpoint` 和 `--save_steps`）。  \n- 提供了多节点训练脚本, 比如 [DPO](./examples/scripts/train_llama_slurm.sh) 和 [RLHF](./examples/scripts/train_ppo_llama_ray_slurm.sh)\n\n\n### PPO 支持矩阵\n\n| 特性 | OpenRLHF | DSChat | CAIChat | TRL |\n| ------------- |:-------------:| :-------------:| :-------------:| :-------------:| \n| 使用 16 个 A100 完成 70B+ 全微调      | ✅ | ❌ | ❌ | ❌ ||\n| 使用 4 个 RTX4090 完成 7B 全微调 | ✅      |    ❌ | ❌ | ❌ | \n| 使用 8 个 A100 完成 34B DPO 全微调 | ✅      |    ❌ | ❌ | ❌ |   \n| 支持推理引擎加速 | ✅      |    ✅ | ❌ | ❌ |  \n| PPO 实现技巧 | ✅      |    ❌ | ❌ | ✅ | \n| 支持 QLoRA | ✅      |    ❌ | ❌ | ✅ | \n| 支持 Mixtral 8*7b | ✅      |    ❌ | ❌ | ❌ | \n| 支持未合并的 Actor-Critic | ✅     |   ✅ | ✅ | ❌ | \n| 支持多个奖励模型 | ✅      |    ❌ | ❌ | ❌ |   \n| 支持 Huggingface 模型 | ✅      |    ✅ | ✅ | ✅ | \n| 易于使用 | ✅      |   ❌ (HybridEngine bugs) | ✅ | ✅ | \n\n## 快速开始\n\n### 安装\n\n要使用 OpenRLHF，首先启动 Docker 容器（**推荐**）然后执行 `pip install` 安装 `openrlhf`：\n\n```bash\n# 启动 docker container\ndocker run --runtime=nvidia -it --rm --shm-size=\"10g\" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash\nsudo pip uninstall xgboost transformer_engine flash_attn -y\n\n# pip install\npip install openrlhf\n\n# 如果你需要使用 vLLM 加速 (安装 vLLM 0.6.5)\npip install openrlhf[vllm]\n# 最新的 vLLM 也是支持的\npip install openrlhf[vllm_latest]\n\n# pip install GitHub 上的最新版\npip install git+https://github.com/OpenRLHF/OpenRLHF.git\n\n# 或者 git clone\ngit clone https://github.com/OpenRLHF/OpenRLHF.git\ncd OpenRLHF\npip install -e .\n```\n\n> [!NOTE]\n>我们推荐使用 vLLM 0.6.4+，其他版本 (vLLM >= 0.4.2) 可能需要通过 Gloo 进行权重同步（`--vllm_sync_backend gloo`）。\n>我们也提供了 [Dockerfiles for vLLM](./dockerfile/) 和 [Nvidia-Docker 一键安装脚本](./examples/scripts/nvidia_docker_install.sh)。\n\n### 准备数据集\nOpenRLHF 在其数据集类中提供了多种数据处理方法。\n例如在 [Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6) 中：\n\n```python\ndef preprocess_data(data, input_template=None, input_key=\"input\", apply_chat_template=None) -> str:\n    if apply_chat_template:\n        chat = data[input_key]\n        if isinstance(chat, str):\n            chat = [{\"role\": \"user\", \"content\": chat}]\n        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    else:\n        prompt = data[input_key]\n        if input_template:\n            prompt = input_template.format(prompt)\n    return prompt\n```\n\n- 我们可以使用 `--input_key` 指定 `JSON key name` 为输入数据集 `--prompt_data {name or path}` (PPO) 或 `--dataset {name or path}`，并使用 `--apply_chat_template` 利用 [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating) 中的 `chat_template`。\n- 如果不想使用 `--apply_chat_template`，可以改用 `--input_template`，或预先离线处理数据集。\n- OpenRLHF 还支持使用 `--prompt_data_probs 0.1,0.4,0.5` (PPO) 或 `--dataset_probs 0.1,0.4,0.5` 混合多个数据集。\n\nChat Templating 的工作原理如下:\n\n```python\ndataset = [{\"input_key\": [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]}]\n\ntokenizer.apply_chat_template(dataset[0][\"input_key\"], tokenize=False)\n\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\n如何指定训练和测试数据分区 ?\n\n你可以使用 `data_type@data_dir` 的方式指定, 比如下面的数据集可以设置为 `--dataset json@./data`\n\n```\ndata\n├── test.jsonl\n└── train.jsonl\n```\n\n> [!NOTE]\n>默认情况下我们使用 `train` 和 `test` 作为 split 区分 Huggingface 的训练/测试数据。\n>`JSON key` 选项取决于具体的数据集。请参阅 [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) 和 [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/mai\n\n\n### Supervised Fine-tuning\n\nOpenRLHF 的模型检查点完全兼容 HuggingFace 模型。您可以使用 `--pretrain  {name or path}`、`--reward_pretrain  {name or path}` 和 `--critic_pretrain  {name or path}` 指定模型名称或路径。我们在 [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF) 上提供了一些预训练的检查点和数据集。\n\n然后您可以使用我们在 [examples/scripts](./examples/scripts/) 目录中提供的启动脚本，或者使用以下命令启动训练：\n\n```bash \ndeepspeed --module openrlhf.cli.train_sft \\\n   --max_len 4096 \\\n   --dataset Open-Orca/OpenOrca \\\n   --input_key question \\\n   --output_key response \\\n   --input_template $'User: {}\\nAssistant: ' \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 2 \\\n   --max_samples 500000 \\\n   --pretrain meta-llama/Meta-Llama-3-8B \\\n   --save_path ./checkpoint/llama3-8b-sft \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --zero_stage 2 \\\n   --max_epochs 1 \\\n   --bf16 \\\n   --flash_attn \\\n   --learning_rate 5e-6 \\\n   --gradient_checkpointing \\\n   --packing_samples \\\n   --load_checkpoint \\\n   --use_wandb {wandb_token}\n\n# 支持 HF tokenizer.apply_chat_template\n# --apply_chat_template \n# --tokenizer_chat_template {HF Chat Template}\n\n# 支持 RingAttention\n# pip install ring_flash_attn\n#   --ring_attn_size 2 \\\n#   --ring_head_stride 2 \\\n\n# 也可用于 continued pre-training\n# --pretrain_mode\n```\n\n> [!NOTE]\n> OpenRLHF SFT/DPO/RewardModel/PPO 训练支持 `--packing_samples` [基于 `--flash_attn`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)\n\n### Reward Model Training\n```bash\ndeepspeed --module openrlhf.cli.train_rm \\\n   --save_path ./checkpoint/llama3-8b-rm \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 1 \\\n   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n   --bf16 \\\n   --max_epochs 1 \\\n   --max_len 8192 \\\n   --zero_stage 3 \\\n   --learning_rate 9e-6 \\\n   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \\\n   --apply_chat_template \\\n   --chosen_key chosen \\\n   --rejected_key rejected \\\n   --flash_attn \\\n   --packing_samples \\\n   --gradient_checkpointing \\\n   --load_checkpoint \\\n   --use_wandb {wandb_token}\n\n```\n\n推荐设置 Reward Model 的 `--value_prefix_head` 选项为 `score`, 这样使得我们可以用 `AutoModelForSequenceClassification` 加载模型:\n\n```python\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n              reward_model_path,\n              num_labels=1,\n              torch_dtype=torch.bfloat16,\n              attn_implementation=\"flash_attention_2\",\n              use_cache=False,\n          )\ninputs = xxxx (Left Padding Input Tokens)\nreward = reward_model.model(*inputs).last_hidden_state\nreward = reward_model.score(reward)[:, -1]\n```\n\n### 不使用 Ray 的 PPO\n\n```bash\ndeepspeed --module openrlhf.cli.train_ppo \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path ./checkpoint/llama-3-8b-rlhf \\\n  --save_steps -1 \\\n  --logging_steps 1 \\\n  --eval_steps -1 \\\n  --micro_train_batch_size 2 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 4 \\\n  --rollout_batch_size 1024 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 2 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --max_samples 100000 \\\n  --normalize_reward \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --load_checkpoint \\\n  --use_wandb {wandb_token}\n\n# 支持远程 reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n```\n\n### 使用 Ray 和 vLLM 的 PPO/REINFORCE++\n\n为了提高 RLHF 训练速度或支持 70B 模型，我们可以使用 Ray 和 vLLM 加速的 PPO\n\n```bash\n# 在容器中启动 Ray 的主节点\nray start --head --node-ip-address 0.0.0.0 --num-gpus 8\n\n# 如果要在更多节点上启动 Ray，请使用\nray start --address {MASTER-NODE-ADDRESS}:6379 --num-gpus 8\n\nray job submit --address=\"http://127.0.0.1:8265\" \\\n  --runtime-env-json='{\"working_dir\": \"/openrlhf\"}' \\\n  -- python3 -m openrlhf.cli.train_ppo_ray \\\n  --ref_num_nodes 1 \\\n  --ref_num_gpus_per_node 2 \\\n  --reward_num_nodes 1 \\\n  --reward_num_gpus_per_node 2 \\\n  --critic_num_nodes 1 \\\n  --critic_num_gpus_per_node 2 \\\n  --actor_num_nodes 1 \\\n  --actor_num_gpus_per_node 2 \\\n  --vllm_num_engines 2 \\\n  --vllm_tensor_parallel_size 2 \\\n  --colocate_critic_reward \\\n  --colocate_actor_ref \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path /openrlhf/examples/checkpoint/llama3-8b-rlhf \\\n  --micro_train_batch_size 8 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 32 \\\n  --rollout_batch_size 1024 \\\n  --max_samples 100000 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 3 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --normalize_reward \\\n  --packing_samples \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --load_checkpoint \\\n  --use_wandb {wandb_token}\n\n# 支持 REINFORCE++  | RLOO \n# --advantage_estimator reinforce | rloo\n\n# 支持远程 reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n\n# 支持 N 倍采样\n# --n_samples_per_prompt 4\n```\n\n> [!NOTE]\n> 不设置 `--vllm_num_engines` 则是不使用 vLLM engine。\n> 您也可以通过 ``setup_commands`` 让 Ray 自动初始化环境, 比如 `--runtime-env-json='{\"setup_commands\": [\"pip install openrlhf[vllm]\"]}'`\n\n> [!NOTE]\n> OPENRLHF's RLOO 基于 REINFORCE++ 修改而来, 和原版的实现不同.\n\n> [!NOTE]\n> 如果您由于某种原因，在 deepspeed 设置显卡设备时遇到与索引超出范围相关的错误，您可以尝试设置环境变量 [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py)。\n> ```bash\n> # 对于 NVIDIA 显卡:\n> export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1\n> ```\n\n所有支持算法的启动脚本和文档在 [example/scripts](./examples/scripts/) 和 [Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)\n\n\n### 使用 LoRA\n如果您使用了 `LoRA (Low-Rank Adaptation)`，默认保存下来的文件**并非**完整模型权重，而是 `LoRA Adapter`，若想按完整权重的方式进行后续任务，您需要将 `Adapter` 与训练前的模型权重进行合并\n\n```bash\npython -m openrlhf.cli.lora_combiner \\\n    --model_path meta-llama/Meta-Llama-3-8B \\\n    --lora_path ./checkpoint/llama3-8b-rm \\\n    --output_path ./checkpoint/llama-3-8b-rm-combined \\\n    --is_rm \\\n    --bf16\n```\n\n## 性能\n我们通过启用Adam卸载、奖励模型(RM)和参考模型(Ref)卸载等技术,尽可能优化了DSChat的性能,从而在推理阶段增加小批量大小并避免内存不足问题。我们甚至修复了DSChat中的一些bug,以启用LLaMA2的混合引擎(HE)。使用优化后的DSChat和OpenRLHF训练1024个提示需要1个PPO轮次的平均时间(秒)如下:\n\n| **Size** | **NVIDIA A800 GPUs** | **Optimized DSChat (with  Hybrid Engine)** | **OpenRLHF** | **Speedup** |\n| :---: | :---: | :---: | :---: | :---: |\n| 7B | 16 | 855.09 | 471.11 | 1.82x |\n| 13B | 32 | 1528.93 | 608.93 | 2.5x |\n| 34B | 32 | 3634.98 | 1526.4 | 2.4x |\n| 70B | 32 | 10407.0 | 4488.53 | 2.3x |\n\n\n> [!NOTE]\n> 数据已经过时; 请参考后面的调优指南重新测试\n\n## 调优指南\n为了获得最佳的性能，我们建议您分配更多的节点给 vLLM Engine。例如，对于 70B 模型以及 32 张 A100，建议分配 16 张以上 A100 给 vLLM Engine，8 张给 Actor 模型，以及最后 8 张给 Critic 模型，同时开启 `--colocate_critic_reward`, `--colocate_actor_ref` 或者 `--ref_reward_offload (可选)` 选项合并部分节点。最后您应该尽可能增大 `--rollout_micro_batch_size` ，以及减小 vLLM 的 TP 切分数量。训练阶段的 `micro_train_batch_size` 也是越大越好，请同时使用 `--packing_samples` 。当 GPU 数量足够时请关闭 `--adam_offload` 以及启用 `--overlap_comm`. 对于多节点 RLHF, 请使用 `--vllm_sync_backend nccl` with vLLM 0.6.4+.\n\n## 使用 OpenRLHF 的公司和组织\n\n- Google\n- ByteDance\n- Tencent\n- Alibaba\n- Baidu\n- China Telecom\n- Allen AI\n- Vivo\n- NexusFlow\n- Jülich Supercomputing Centre (JSC)\n- Berkeley Starling Team\n- M-A-P\n- ...\n\n\n## 加入我们\n\n**如何加入？**\n\n1. 通过联系邮箱 janhu9527@gmail.com 或者加入 [GitHub Organization](https://github.com/OpenRLHF)。请包含以下信息：\n   - 您的姓名\n   - 您的 GitHub 用户名\n   - 您感兴趣的领域\n   - 您在 NLP 和/或 AI 相关的技能和经验\n2. 您也可以通过官方 GitHub [OpenRLHF ↗](https://github.com/OpenRLHF/OpenRLHF) 项目页面加入我们。只需创建一个关于您想要贡献的兴趣的 issue，我们会与您联系。\n\n**您能做什么？**\n\n1. 加入团队，参与 OpenRLHF 项目的开发。\n2. 通过提交 pull 请求来为项目做出贡献。\n3. 帮助改进文档，修复 bugs 或创建新功能。\n4. 分享项目并帮助我们发展社区。\n\n## 赞助我们\n\n您的赞助可以帮助我们维护和改进 OpenRLHF。如果您觉得这个项目有用，请考虑赞助我们。您可以在 [Open Collective ↗](https://opencollective.com/OpenRLHF) 上赞助我们。\n\n## 星图\n\n[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)\n\n## 贡献者\n\n非常感谢所有的贡献者！如果您想贡献，请随时创建 pull 请求或创建 issue。\n\n<a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF\" />\n</a>\n\n## 引用与致谢\n\n我们想要对以下项目和组织在 AI 和 NLP 领域的贡献表示感谢：\n\n- [Hugging Face Transformers ↗](https://github.com/huggingface/transformers)\n- [OpenAI GPT ↗](https://github.com/openai/gpt-3)\n- [LLaMA ↗](https://llama.meta.com/)\n- [DeepSpeed ↗](https://github.com/microsoft/DeepSpeed)\n- [Ray ↗](https://github.com/ray-project/ray)\n\n我们的项目还想要感谢 [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) 和 [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)。在项目的早期阶段，我们参考了他们的代码设计。\n\n(2024/7) 我们的 GitHub 组织从 OpenLLMAI 迁移到了 OpenRLHF.\n\n## 引用\n```\n@article{hu2024openrlhf,\n  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},\n  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},\n  journal={arXiv preprint arXiv:2405.11143},\n  year={2024}\n}\n```\n\n\n______________________________________________________________________\n\n*OpenRLHF © 2025 OpenRLHF. 版权所有。*\n"
        },
        {
          "name": "dockerfile",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "openrlhf",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.09765625,
          "content": "[build-system]\nrequires = [\n    \"packaging\",\n    \"setuptools >= 49.4.0\",\n    \"wheel\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.isort]\nprofile = \"black\"  # black-compatible\nline_length = 119  # should match black parameters\nignore_whitespace = true  # ignore whitespace for compatibility with the initial style\npy_version = 310  # python 3.10 as a target version\nsections = [\"FUTURE\", \"STDLIB\", \"THIRDPARTY\", \"FIRSTPARTY\", \"LOCALFOLDER\"]\ndefault_section = \"THIRDPARTY\"\nextend_skip = [\"setup.py\", \"docs/source/conf.py\"]\n\n\n[tool.black]\nline_length = 119\n\n[tool.ruff]\nline-length = 119\n\n[tool.pytest.ini_options]\n# durations=0 will display all tests execution time, sorted in ascending order starting from from the slowest one.\n# -vv will also display tests with durration = 0.00s\naddopts = \"--verbose --pyargs --durations=0 --strict-markers\"  # always add these arguments to pytest\ntestpaths = [\"./tests\"]  # must be an explicit path to avoid importing another \"tests\" module\n# directories to ignore when discovering tests\nnorecursedirs = [\n    \"external\",\n    \"examples\",\n    \"docs\",\n    \"scripts\",\n    \"tools\",\n    \"tutorials\",\n    \"*.egg\",\n    \".*\",\n    \"_darcs\",\n    \"build\",\n    \"CVS\",\n    \"dist\",\n    \"venv\",\n    \"{arch}\",\n]\n# markers to select tests, use `pytest --markers` to see all available markers, `pytest -m \"<marker>\"` to select tests\nmarkers = [\n    \"unit: marks unit test, i.e. testing a single, well isolated functionality (deselect with '-m \\\"not unit\\\"')\",\n    \"integration: marks test checking the elements when integrated into subsystems (deselect with '-m \\\"not integration\\\"')\",\n    \"system: marks test working at the highest integration level (deselect with '-m \\\"not system\\\"')\",\n    \"acceptance: marks test checking whether the developed product/model passes the user defined acceptance criteria (deselect with '-m \\\"not acceptance\\\"')\",\n    \"docs: mark tests related to documentation (deselect with '-m \\\"not docs\\\"')\",\n    \"skipduringci: marks tests that are skipped ci as they are addressed by Jenkins jobs but should be run to test user setups\",\n    \"pleasefixme: marks tests that are broken and need fixing\",\n]\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2431640625,
          "content": "accelerate\nbitsandbytes\ndatasets\ndeepspeed==0.15.0\neinops\nflash-attn==2.7.0.post2\nisort\njsonlines\nloralib\noptimum\npackaging\npeft\nray[default]==2.12.0\ntensorboard\ntorch\ntorchmetrics\ntqdm\ntransformers==4.46.3\ntransformers_stream_generator\nwandb\nwheel\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.2607421875,
          "content": "import os\nimport sys\nimport platform\n\nfrom datetime import datetime\nfrom setuptools import find_packages, setup\nfrom wheel.bdist_wheel import bdist_wheel as _bdist_wheel\n\n_build_mode = os.getenv(\"OPENRLHF_BUILD_MODE\", \"\")\n\n\ndef _is_nightly():\n    return _build_mode.lower() == \"nightly\"\n\n\ndef _fetch_requirements(path):\n    with open(path, \"r\") as fd:\n        return [r.strip() for r in fd.readlines()]\n\n\ndef _fetch_readme():\n    with open(\"README.md\", encoding=\"utf-8\") as f:\n        return f.read()\n\n\ndef _fetch_version():\n    with open(\"version.txt\", \"r\") as f:\n        version = f.read().strip()\n\n    if _is_nightly():\n        now = datetime.now()\n        date_str = now.strftime(\"%Y%m%d\")\n        version += f\".dev{date_str}\"\n\n    return version\n\n\ndef _fetch_package_name():\n    return \"openrlhf-nightly\" if _is_nightly() else \"openrlhf\"\n\n\n# Custom wheel class to modify the wheel name\nclass bdist_wheel(_bdist_wheel):\n    def finalize_options(self):\n        _bdist_wheel.finalize_options(self)\n        self.root_is_pure = False\n\n    def get_tag(self):\n        python_version = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n        abi_tag = f\"{python_version}\"\n\n        if platform.system() == \"Linux\":\n            platform_tag = \"manylinux1_x86_64\"\n        else:\n            platform_tag = platform.system().lower()\n\n        return python_version, abi_tag, platform_tag\n\n\n# Setup configuration\nsetup(\n    author=\"OpenRLHF Team\",\n    name=_fetch_package_name(),\n    version=_fetch_version(),\n    packages=find_packages(\n        exclude=(\n            \"data\",\n            \"docs\",\n            \"examples\",\n        )\n    ),\n    description=\"A Ray-based High-performance RLHF framework.\",\n    long_description=_fetch_readme(),\n    long_description_content_type=\"text/markdown\",\n    install_requires=_fetch_requirements(\"requirements.txt\"),\n    extras_require={\n        \"vllm\": [\"vllm==0.6.5\"],\n        \"vllm_latest\": [\"vllm>0.6.5\"],\n    },\n    python_requires=\">=3.10\",\n    classifiers=[\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Environment :: GPU :: NVIDIA CUDA\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: System :: Distributed Computing\",\n    ],\n    cmdclass={\"bdist_wheel\": bdist_wheel},\n)\n"
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.0048828125,
          "content": "0.5.6"
        }
      ]
    }
  ]
}