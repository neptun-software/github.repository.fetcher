{
  "metadata": {
    "timestamp": 1736559854220,
    "page": 604,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OpenRLHF/OpenRLHF",
      "stars": 3690,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.921875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/.build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# IDE\n.idea/\n.vscode/\n\n# macos\n*.DS_Store\n#data/\n\ndocs/.build\n\n# pytorch checkpoint\n*.pt\n\ncore\n*/ckpt/*\n.vscode\n.nfs*\n*jianh*\n*test_scripts*\n*/checkpoint/*"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.8779296875,
          "content": "default_language_version:\n  python: python3\n\nci:\n  autofix_prs: true\n  autoupdate_commit_msg: '[pre-commit.ci] pre-commit suggestions'\n  autoupdate_schedule: quarterly\n\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: check-yaml\n      - id: check-case-conflict\n      - id: detect-private-key\n      - id: check-added-large-files\n        args: ['--maxkb=1000']\n      - id: requirements-txt-fixer\n\n  - repo: https://github.com/PyCQA/autoflake\n    rev: v2.0.2\n    hooks:\n      - id: autoflake\n        args: [--remove-all-unused-imports, --in-place]\n\n  - repo: https://github.com/PyCQA/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n        name: Format imports\n        exclude: docs/\n\n  - repo: https://github.com/psf/black\n    rev: 24.3.0\n    hooks:\n      - id: black\n        name: Format code\n        additional_dependencies: ['click==8.0.2']\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.1416015625,
          "content": "# Contributing to OpenRLHF\n\nAfter cloning the repository, please install pre-commit hooks with:\n```\npip install pre-commit\npre-commit install\n```"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08984375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.865234375,
          "content": "<div align=\"center\">\n    <img alt=\"OpenRLHF logo\" src=\"./docs/logo.png\" style=\"height: 140px;\" />\n</div>\n<div align=\"center\">\n<p align=\"center\">\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n        <img alt=\"GitHub Contributors\" src=\"https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/issues\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/discussions\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/pulls\">\n        <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff\" />\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/stargazers\">\n        <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf\" />\n      </a>\n      <br>\n      <em>Open-source / Comprehensive / Lightweight / Easy-to-use</em>\n    </p>\n</p>\n</div>\n\n<hr>\n\n<span>[ English | <a href=\"README_zh.md\">‰∏≠Êñá</a> | <a href=\"README_ja.md\">Êó•Êú¨Ë™û</a> ]</span>\n\nOpenRLHF is a high-performance RLHF framework built on Ray, DeepSpeed and HF Transformers:\n\n- **Simple and easy to use**: OpenRLHF is one of the simplest high-performance RLHF libraries currently available, and seamlessly compatible with Huggingface models and datasets.\n- **High performance**: RLHF training spends 80% of the time on the sample generation stage. Thanks to the ability to use a large inference batch size with Ray and Packing Samples and vLLM generation acceleration, the performance of OpenRLHF 3~4x+ that of Optimized DeepSpeedChat with Hybrid Engine.\n- **Distributed RLHF**:  OpenRLHF distribute the Actor, Reward, Reference, and Critic models onto separate GPUs using Ray, while placing the Adam optimizer on the CPU. This enables full-scale fine-tuning of 70B+ models with multiple A100 80G GPUs and vLLM and 7B models across multiple 24GB RTX 4090 GPUs.\n- **PPO Implementation Optimization**: We integrated the implementation tricks for PPO to improve the training stability, referencing [Zhihu](https://zhuanlan.zhihu.com/p/622134699) and the [Notion blog](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361).\n\nMore details are in [Slides](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [Technical Report](https://arxiv.org/abs/2405.11143) | [Documents](https://openrlhf.readthedocs.io/)\n\n## News\n- [2024/12] We \"proposed\" üòä the [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS).\n- [2024/12] We analyzed the PPO, REINFORCE++, GRPO and RLOO in the [Notion Blogpost](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05).\n\n\n## Features\n\n- Distributed [PPO](./examples/scripts/train_ppo_llama_ray.sh) and [REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray.sh) implementations based on Ray.  \n- Full RLHF fine-tuning support for models with [over 70 billion parameters](./examples/scripts/train_ppo_llama_ray_70b.sh).  \n- Integration with vLLM for accelerated generation in RLHF tasks (`--vllm_num_engines`).  \n- Support for multiple reward models (`--reward_pretrain model1,model2...`) and remote reward models (`--remote_rm_url`).  \n- Implementation of [DPO (Direct Preference Optimization)/IPO/cDPO](./examples/scripts/train_dpo_llama.sh) and [Kahneman-Tversky Optimization (KTO)](./examples/scripts/train_kto_llama.sh).  \n- Support for [Iterative DPO](./examples/scripts/train_iterative_dpo_llama.sh) ([GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)).  \n- Support for [Rejection Sampling](./examples/scripts/train_rejection_sampling_llama.sh).  \n- Implementation of [Conditional SFT](./examples/scripts/train_conditional_llama.sh) ([arXiv:2308.12050](https://arxiv.org/abs/2308.12050)).  \n- Support for [Knowledge Distillation](./examples/scripts/train_knowledge_distillation.sh) ([Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)).  \n- Integration of [Process Reward Model (PRM)](./examples/scripts/train_prm_mistral.sh).  \n- Packing of training samples for SFT, DPO, RM, PRM, and PPO (`--packing_samples`).  \n- Implementation of [RingAttention](./examples/scripts/train_dpo_ring_llama.sh) (`--ring_attn_size`, `--ring_head_stride`).  \n- Support for [Mixture of Experts (MoE)](./examples/test_scripts/train_sft_mixtral_lora.sh) (`--aux_loss_coef`).  \n- Integration of FlashAttention2 (`--flash_attn`).  \n- Support for QLoRA (`--load_in_4bit`) and [LoRA](./examples/scripts/train_sft_mixtral_lora.sh) (`--lora_rank`, `--target_modules`).  \n- Compatibility with HuggingFace's `tokenizer.apply_chat_template` for datasets (`--apply_chat_template` and `--input_key`).  \n- Logging support with Wandb (`--use_wandb`) and TensorBoard (`--use_tensorboard`).  \n- Checkpoint recovery functionality (`--load_checkpoint` and `--save_steps`).  \n- Provided multi-node training scripts, such as [DPO](./examples/scripts/train_llama_slurm.sh) and [Ray PPO](./examples/scripts/train_ppo_llama_ray_slurm.sh).\n\n\n### PPO Support Matrix\n\n| Feature | OpenRLHF | DSChat | CAIChat | TRL |\n| ------------- |:-------------:| :-------------:| :-------------:| :-------------:|\n| 70B+ Full Tuning with 16 A100-80GB      | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |\n| 7B Full Tuning with 4 RTX4090 | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |\n| 34B DPO Full Tuning with 8 A100-80GB | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |  \n| Inference Engine in PPO | ‚úÖ      |    ‚úÖ | ‚ùå | ‚ùå |  \n| PPO Implementation Tricks | ‚úÖ      |    ‚ùå | ‚ùå | ‚úÖ |\n| Support QLoRA | ‚úÖ      |    ‚ùå | ‚ùå | ‚úÖ | \n| Support Mixtral 8*7b | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |  \n| Support Unmerged Actor-Critic | ‚úÖ     |   ‚úÖ | ‚úÖ | ‚ùå | \n| Support Multiple Reward Models | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |   \n| Support Huggingface Models | ‚úÖ      |    ‚úÖ | ‚úÖ | ‚úÖ | \n| Easy-to-use | ‚úÖ      |   ‚ùå (HybridEngine bugs) | ‚úÖ | ‚úÖ | \n\n\n## Quick Start\n\n### Installation\n\nTo use OpenRLHF, first launch the docker container (**Recommended**) and `pip install` openrlhf inside the docker container:\n\n```bash\n# Launch the docker container\ndocker run --runtime=nvidia -it --rm --shm-size=\"10g\" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash\nsudo pip uninstall xgboost transformer_engine flash_attn -y\n\n# pip install\npip install openrlhf\n\n# If you want to use vLLM acceleration (Install vLLM 0.6.5)\npip install openrlhf[vllm]\n# latest vLLM is also supported\npip install openrlhf[vllm_latest]\n\n# pip install the latest version\npip install git+https://github.com/OpenRLHF/OpenRLHF.git\n\n# Or git clone\ngit clone https://github.com/OpenRLHF/OpenRLHF.git\ncd OpenRLHF\npip install -e .\n```\n\n> [!NOTE]\n>We recommend using vLLM 0.6.4 or higher. Other versions (vLLM >= 0.4.2) may require weight synchronization via Gloo (`--vllm_sync_backend gloo`).\n>We also provided the [Dockerfiles for vLLM](./dockerfile/) and [One-Click Installation Script of Nvidia-Docker](./examples/scripts/nvidia_docker_install.sh).\n\n### Prepare Datasets\nOpenRLHF provides multiple data processing methods in our dataset classes.\nSuch as in the [Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6):\n\n```python\ndef preprocess_data(data, input_template=None, input_key=\"input\", apply_chat_template=None) -> str:\n    if apply_chat_template:\n        chat = data[input_key]\n        if isinstance(chat, str):\n            chat = [{\"role\": \"user\", \"content\": chat}]\n        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    else:\n        prompt = data[input_key]\n        if input_template:\n            prompt = input_template.format(prompt)\n    return prompt\n```\n\n- We can use `--input_key` to specify the `JSON key name` of the input datasets `--prompt_data {name or path}` (PPO) or `--dataset {name or path}`, and use `--apply_chat_template` to utilize the `chat_template` from the [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating).\n- If you don't want to use `--apply_chat_template`, you can use `--input_template` instead, or preprocess the datasets offline in advance.\n- OpenRLHF also support mixing multiple datasets using `--prompt_data_probs 0.1,0.4,0.5` (PPO) or `--dataset_probs 0.1,0.4,0.5`.\n\nHow Chat Templating Works:\n\n```python\ndataset = [{\"input_key\": [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]}]\n\ntokenizer.apply_chat_template(dataset[0][\"input_key\"], tokenize=False)\n\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\nHow to specify training and test datasets ?\n\nYou can specify it using the `data_type@data_dir` format. For example, the dataset can be set as `--dataset json@./data`.\n\n```\ndata\n‚îú‚îÄ‚îÄ test.jsonl\n‚îî‚îÄ‚îÄ train.jsonl\n```\n\n> [!NOTE]\n> By default, we use `train` and `test` as splits to distinguish training and testing datasets from Huggingface.\n> The ``JSON key`` options depends on the specific datasets. See [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) and [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)\n\n### Supervised Fine-tuning\n\nOpenRLHF's model checkpoint is fully compatible with HuggingFace models. You can specify the model name or path using `--pretrain  {name or path}`, `--reward_pretrain  {name or path}` and `--critic_pretrain  {name or path}`. We have provided some pre-trained checkpoints and datasets on [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF).\n\nThen you can use the startup scripts we provide in the [examples/scripts](./examples/scripts/) directory, or start the training using the following commands.\n\n```bash \ndeepspeed --module openrlhf.cli.train_sft \\\n   --max_len 4096 \\\n   --dataset Open-Orca/OpenOrca \\\n   --input_key question \\\n   --output_key response \\\n   --input_template $'User: {}\\nAssistant: ' \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 2 \\\n   --max_samples 500000 \\\n   --pretrain meta-llama/Meta-Llama-3-8B \\\n   --save_path ./checkpoint/llama3-8b-sft \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --zero_stage 2 \\\n   --max_epochs 1 \\\n   --packing_samples \\\n   --bf16 \\\n   --flash_attn \\\n   --learning_rate 5e-6 \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n# Support HF tokenizer.apply_chat_template\n# --apply_chat_template \n# --tokenizer_chat_template {HF Chat Template}\n\n# Support RingAttention\n# pip install ring_flash_attn\n#   --ring_attn_size 2 \\\n#   --ring_head_stride 2 \\\n\n# Can also be used for continued pre-training\n# --pretrain_mode\n```\n\n> [!NOTE]\n> OpenRLHF SFT/DPO/RewardModel/PPO trainers support `--packing_samples` [based on `--flash_attn`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)\n\n\n### Reward Model Training\n```bash\ndeepspeed --module openrlhf.cli.train_rm \\\n   --save_path ./checkpoint/llama3-8b-rm \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 1 \\\n   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n   --bf16 \\\n   --max_epochs 1 \\\n   --max_len 8192 \\\n   --zero_stage 3 \\\n   --learning_rate 9e-6 \\\n   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \\\n   --apply_chat_template \\\n   --chosen_key chosen \\\n   --rejected_key rejected \\\n   --flash_attn \\\n   --packing_samples \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n```\n\nIt is recommended to set the `--value_prefix_head` option of the Reward Model to `score`, so that we can load the model using `AutoModelForSequenceClassification`:\n\n```python\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n              reward_model_path,\n              num_labels=1,\n              torch_dtype=torch.bfloat16,\n              attn_implementation=\"flash_attention_2\",\n              use_cache=False,\n          )\ninputs = xxxx (Left Padding Input Tokens)\nreward = reward_model.model(*inputs).last_hidden_state\nreward = reward_model.score(reward)[:, -1]\n```\n\n### PPO without Ray\n\n```bash\ndeepspeed --module openrlhf.cli.train_ppo \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path ./checkpoint/llama-3-8b-rlhf \\\n  --save_steps -1 \\\n  --logging_steps 1 \\\n  --eval_steps -1 \\\n  --micro_train_batch_size 2 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 4 \\\n  --rollout_batch_size 1024 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 2 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --max_samples 100000 \\\n  --normalize_reward \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --use_wandb {wandb_token}\n\n# Support remote reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n```\n\n### PPO/REINFORCE++ with Ray and vLLM\n\nTo improve RLHF training speed or support 70B models, we can use the PPO with Ray and vLLM acceleration\n\n```bash\n# launch the master node of ray in container\nray start --head --node-ip-address 0.0.0.0 --num-gpus 8\n\n# if you want to launch ray on more nodes, use\nray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8\n\nray job submit --address=\"http://127.0.0.1:8265\" \\\n  --runtime-env-json='{\"working_dir\": \"/openrlhf\"}' \\\n  -- python3 -m openrlhf.cli.train_ppo_ray \\\n  --ref_num_nodes 1 \\\n  --ref_num_gpus_per_node 2 \\\n  --reward_num_nodes 1 \\\n  --reward_num_gpus_per_node 2 \\\n  --critic_num_nodes 1 \\\n  --critic_num_gpus_per_node 2 \\\n  --actor_num_nodes 1 \\\n  --actor_num_gpus_per_node 2 \\\n  --vllm_num_engines 2 \\\n  --vllm_tensor_parallel_size 2 \\\n  --colocate_critic_reward \\\n  --colocate_actor_ref \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path /openrlhf/examples/checkpoint/llama3-8b-rlhf \\\n  --micro_train_batch_size 8 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 16 \\\n  --rollout_batch_size 1024 \\\n  --max_samples 100000 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 3 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --normalize_reward \\\n  --packing_samples \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --use_wandb {wandb_token}\n\n# Support REINFORCE++  | RLOO\n# --advantage_estimator reinforce | rloo\n\n# Support remote reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n\n# Support N samples\n# --n_samples_per_prompt 4\n```\n> [!NOTE]\n> Do not set `--vllm_num_engines` means not using the vLLM engine.\n> You can also use ``setup_commands`` to let Ray automatically deploy the environment, such as `--runtime-env-json='{\"setup_commands\": [\"pip install openrlhf[vllm]\"]}'`.\n\n> [!NOTE]\n> RLOO in OPENRLHF is a modification based on REINFORCE++, differing from the original version.\n\n> [!NOTE]\n> If you you encounter an error related to index out of range when deepspeed sets up the GPU devices, you can try to set the environment variable [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py) as a workaround.\n>   ```bash\n>   # For NVIDIA GPUs:\n>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1\n>   ```\n\nThe launch scripts and documents for supported algorithms are in [example/scripts](./examples/scripts/) and [Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)\n\n### LoRA\nIf you use `LoRA (Low-Rank Adaptation)`, `OpenRLHF` will not save the full weights by default instead of `LoRA Adapter`. To continue in your task normally, you should combine the `Adapter` with weights of your base model\n\n```bash\npython -m openrlhf.cli.lora_combiner \\\n    --model_path meta-llama/Meta-Llama-3-8B \\\n    --lora_path ./checkpoint/llama3-8b-rm \\\n    --output_path ./checkpoint/llama-3-8b-rm-combined \\\n    --is_rm \\\n    --bf16\n```\n\n## Performance\n\nWe optimized DSChat's performance to the greatest extent possible by employing techniques such as enabling Adam offload, along with reward model (RM) and reference model (Ref) offload to increase the micro-batch size during the inference stage and avoid out-of-memory issues. We even fixed some bugs in DSChat to enable the Hybrid Engine (HE) for LLaMA2. The average time (seconds) it took to train 1024 prompts with 1 PPO epoch using the Optimized DSChat and OpenRLHF:\n\n| **Size** | **NVIDIA A800-80GB GPUs** | **Optimized DSChat (with  Hybrid Engine)** | **OpenRLHF** | **Speedup** |\n| :---: | :---: | :---: | :---: | :---: |\n| 7B | 16 | 855.09 | 471.11 | 1.82x |\n| 13B | 32 | 1528.93 | 608.93 | 2.5x |\n| 34B | 32 | 3634.98 | 1526.4 | 2.4x |\n| 70B | 32 | 10407.0 | 4488.53 | 2.3x |\n\n> [!NOTE]\n> The data is outdated; please refer to the performance tuning section for re-testing.\n\n### Performance Tuning Guide\n\nTo achieve optimal performance, we recommend allocating more nodes to the vLLM Engine. For example, for a 70B model with 32 A100 GPUs, it is advised to allocate 16 A100 GPUs to the vLLM Engine, 8 GPUs to the Actor model, and the remaining 8 GPUs to the Critic model. Additionally, enable the `--colocate_critic_reward`, `--colocate_actor_ref` options to merge nodes. Finally, you should increase the `rollout_micro_batch_size` (and minimize the TP size of vLLM engine) as much as possible. During the training phase, a larger `--micro_train_batch_size` is better and enable `--packing_samples`. When there are enough GPUs, please disable `--adam_offload` and enable `--overlap_comm`. For multi-nodes RLHF, please use `--vllm_sync_backend nccl` with vLLM 0.6.4+.\n\n## Companies and Organizations using OpenRLHF\n\n- Google\n- ByteDance\n- Tencent\n- Alibaba\n- Baidu\n- China Telecom\n- Vivo\n- Allen AI\n- NexusFlow\n- J√ºlich Supercomputing Centre (JSC)\n- Berkeley Starling Team\n- M-A-P\n- ...\n\n## Join Us\n\n**How to Join?**\n\n1. Email us at janhu9527@gmail.com or join [GitHub Organization](https://github.com/OpenRLHF). Please include the following details:\n   - Your name\n   - Your GitHub username\n   - Your areas of interest\n   - Your skills and experience related to NLP and/or AI\n1. You can also join us through the official GitHub [OpenRLHF ‚Üó](https://github.com/OpenRLHF/OpenRLHF) project page. Just create an issue about your interest to contribute and we will get back to you.\n\n**What can you do?**\n\n1. Join the team and participate in the development of the OpenRLHF project.\n1. Contribute to the project by submitting pull requests.\n1. Help improve documentation, fix bugs, or create new features.\n1. Share the project and help us grow the community.\n\n## Sponsor Us\n\nYour sponsorship can help us maintain and improve OpenRLHF. If you find this project useful, please consider sponsoring us. You can sponsor us on [Open Collective ‚Üó](https://opencollective.com/OpenRLHF).\n\n## Starchart\n\n[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)\n\n## Contributors\n\nA big thank you to all our contributors! If you want to contribute, feel free to make a pull request or create an issue.\n\n<a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF\" />\n</a>\n\n## References & Acknowledgements\n\nWe would like to express our gratitude to the following projects and organizations for their contributions to the field of AI and NLP:\n\n- [Hugging Face Transformers ‚Üó](https://github.com/huggingface/transformers)\n- [OpenAI GPT ‚Üó](https://github.com/openai/gpt-3)\n- [LLaMA ‚Üó](https://llama.meta.com/)\n- [DeepSpeed ‚Üó](https://github.com/microsoft/DeepSpeed)\n- [Ray ‚Üó](https://github.com/ray-project/ray)\n\nOur project would also like to thank [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) and [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat). In the early stages of the project, we referred to their code design. \n\n(2024/7) Our GitHub organization has changed from OpenLLMAI to OpenRLHF.\n\n## Citation\n```\n@article{hu2024openrlhf,\n  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},\n  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},\n  journal={arXiv preprint arXiv:2405.11143},\n  year={2024}\n}\n```\n\n______________________________________________________________________\n\n*OpenRLHF ¬© 2025 OpenRLHF. All Rights Reserved.*\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 22.958984375,
          "content": "<div align=\"center\">\n    <img alt=\"OpenRLHF logo\" src=\"./docs/logo.png\" style=\"height: 140px;\" />\n</div>\n<div align=\"center\">\n<p align=\"center\">\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n        <img alt=\"GitHub Contributors\" src=\"https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/issues\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/discussions\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/pulls\">\n        <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff\" />\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/stargazers\">\n        <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf\" />\n      </a>\n      <br>\n      <em>„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ / ÂåÖÊã¨ÁöÑ / ËªΩÈáè / ‰Ωø„ÅÑ„ÇÑ„Åô„ÅÑ</em>\n    </p>\n</p>\n</div>\n\n<hr>\n\n<span>[ <a href=\"README.md\">English</a> | <a href=\"README_zh.md\">‰∏≠Êñá</a> | Êó•Êú¨Ë™û ]</span>\n\nOpenRLHF„ÅØ„ÄÅRay„ÄÅDeepSpeed„ÄÅ„Åä„Çà„Å≥HF Transformers„ÇíÂü∫Áõ§„Å®„Åó„ÅüÈ´òÊÄßËÉΩ„Å™RLHF„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„ÅôÔºö\n\n- **„Ç∑„É≥„Éó„É´„Åß‰Ωø„ÅÑ„ÇÑ„Åô„ÅÑ**: OpenRLHF„ÅØÁèæÂú®Âà©Áî®ÂèØËÉΩ„Å™ÊúÄ„ÇÇ„Ç∑„É≥„Éó„É´„Å™È´òÊÄßËÉΩRLHF„É©„Ç§„Éñ„É©„É™„ÅÆ‰∏Ä„Å§„Åß„ÅÇ„Çä„ÄÅHuggingface„ÅÆ„É¢„Éá„É´„Å®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®„Ç∑„Éº„É†„É¨„Çπ„Å´‰∫íÊèõÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n- **È´òÊÄßËÉΩ**: RLHF„Éà„É¨„Éº„Éã„É≥„Ç∞„ÅÆ80ÔºÖ„ÅÆÊôÇÈñì„ÅØ„Çµ„É≥„Éó„É´ÁîüÊàêÊÆµÈöé„Å´Ë≤ª„ÇÑ„Åï„Çå„Åæ„Åô„ÄÇRay„Å®Packing Samples„Åä„Çà„Å≥vLLMÁîüÊàêÂä†ÈÄü„ÅÆËÉΩÂäõ„ÇíÊ¥ªÁî®„Åô„Çã„Åì„Å®„Åß„ÄÅOpenRLHF„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅØOptimized DeepSpeedChat with Hybrid Engine„ÅÆ3„Äú4ÂÄç‰ª•‰∏ä„Åß„Åô„ÄÇ\n- **ÂàÜÊï£RLHF**: OpenRLHF„ÅØ„ÄÅActor„ÄÅReward„ÄÅReference„ÄÅ„Åä„Çà„Å≥Critic„É¢„Éá„É´„ÇíRay„Çí‰ΩøÁî®„Åó„Å¶Âà•„ÄÖ„ÅÆGPU„Å´ÂàÜÊï£„Åó„ÄÅAdam„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„ÇíCPU„Å´ÈÖçÁΩÆ„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅË§áÊï∞„ÅÆA100 80G GPU„Å®vLLM„Çí‰ΩøÁî®„Åó„Å¶70B+„É¢„Éá„É´„ÅÆ„Éï„É´„Çπ„Ç±„Éº„É´„ÅÆÂæÆË™øÊï¥„ÅåÂèØËÉΩ„Å´„Å™„Çä„ÄÅË§áÊï∞„ÅÆ24GB RTX 4090 GPU„Åß7B„É¢„Éá„É´„ÇíÂæÆË™øÊï¥„Åß„Åç„Åæ„Åô„ÄÇ\n- **PPOÂÆüË£Ö„ÅÆÊúÄÈÅ©Âåñ**: „Éà„É¨„Éº„Éã„É≥„Ç∞„ÅÆÂÆâÂÆöÊÄß„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„ÄÅPPO„ÅÆÂÆüË£Ö„Éà„É™„ÉÉ„ÇØ„ÇíÁµ±Âêà„Åó„Åæ„Åó„Åü„ÄÇË©≥Á¥∞„ÅØ[Zhihu](https://zhuanlan.zhihu.com/p/622134699)„Åä„Çà„Å≥[Notion„Éñ„É≠„Ç∞](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361)„ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\nË©≥Á¥∞„ÅØ[„Çπ„É©„Ç§„Éâ](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [ÊäÄË°ìÂ†±Âëä](https://arxiv.org/abs/2405.11143) | [„Éâ„Ç≠„É•„É°„É≥„Éà](https://openrlhf.readthedocs.io/)„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n## „Éã„É•„Éº„Çπ\n- [2024/12] ÁßÅ„Åü„Å°„ÅØüòä [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS)„Çí„ÄåÊèêÊ°à„Äç„Åó„Åæ„Åó„Åü„ÄÇ\n- [2024/12] [Notion„Éñ„É≠„Ç∞](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05)„ÅßPPO„ÄÅREINFORCE++„ÄÅGRPO„ÄÅ„Åä„Çà„Å≥RLOO„ÇíÂàÜÊûê„Åó„Åæ„Åó„Åü„ÄÇ\n\n## ÁâπÂæ¥\n\n- Ray„Å´Âü∫„Å•„ÅèÂàÜÊï£[ PPO](./examples/scripts/train_ppo_llama_ray.sh)„Åä„Çà„Å≥[REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray.sh)„ÅÆÂÆüË£Ö„ÄÇ\n- [70ÂÑÑ‰ª•‰∏ä„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíÊåÅ„Å§„É¢„Éá„É´](./examples/scripts/train_ppo_llama_ray_70b.sh)„ÅÆÂÆåÂÖ®„Å™RLHFÂæÆË™øÊï¥„ÅÆ„Çµ„Éù„Éº„Éà„ÄÇ\n- RLHF„Çø„Çπ„ÇØ„Åß„ÅÆÁîüÊàê„ÇíÂä†ÈÄü„Åô„Çã„Åü„ÇÅ„ÅÆvLLM„ÅÆÁµ±ÂêàÔºà`--vllm_num_engines`Ôºâ„ÄÇ\n- Ë§áÊï∞„ÅÆÂ†±ÈÖ¨„É¢„Éá„É´Ôºà`--reward_pretrain model1,model2...`Ôºâ„Åä„Çà„Å≥„É™„É¢„Éº„ÉàÂ†±ÈÖ¨„É¢„Éá„É´Ôºà`--remote_rm_url`Ôºâ„ÅÆ„Çµ„Éù„Éº„Éà„ÄÇ\n- [DPOÔºàÁõ¥Êé•ÈÅ∏Â•ΩÊúÄÈÅ©ÂåñÔºâ/IPO/cDPO](./examples/scripts/train_dpo_llama.sh)„Åä„Çà„Å≥[Kahneman-Tversky OptimizationÔºàKTOÔºâ](./examples/scripts/train_kto_llama.sh)„ÅÆÂÆüË£Ö„ÄÇ\n- [ÂèçÂæ©DPO](./examples/scripts/train_iterative_dpo_llama.sh)Ôºà[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)Ôºâ„ÅÆ„Çµ„Éù„Éº„Éà„ÄÇ\n- [ÊãíÂê¶„Çµ„É≥„Éó„É™„É≥„Ç∞](./examples/scripts/train_rejection_sampling_llama.sh)„ÅÆ„Çµ„Éù„Éº„Éà„ÄÇ\n- [Êù°‰ª∂‰ªò„ÅçSFT](./examples/scripts/train_conditional_llama.sh)Ôºà[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)Ôºâ„ÅÆÂÆüË£Ö„ÄÇ\n- [Áü•Ë≠òËí∏Áïô](./examples/scripts/train_knowledge_distillation.sh)Ôºà[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)Ôºâ„ÅÆ„Çµ„Éù„Éº„Éà„ÄÇ\n- [„Éó„É≠„Çª„ÇπÂ†±ÈÖ¨„É¢„Éá„É´ÔºàPRMÔºâ](./examples/scripts/train_prm_mistral.sh)„ÅÆÁµ±Âêà„ÄÇ\n- SFT„ÄÅDPO„ÄÅRM„ÄÅPRM„ÄÅ„Åä„Çà„Å≥PPO„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„Çµ„É≥„Éó„É´„ÅÆ„Éë„ÉÉ„Ç≠„É≥„Ç∞Ôºà`--packing_samples`Ôºâ„ÄÇ\n- [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)„ÅÆÂÆüË£ÖÔºà`--ring_attn_size`„ÄÅ`--ring_head_stride`Ôºâ„ÄÇ\n- [Â∞ÇÈñÄÂÆ∂„ÅÆÊ∑∑Âêà„É¢„Éá„É´ÔºàMoEÔºâ](./examples/test_scripts/train_sft_mixtral_lora.sh)„ÅÆ„Çµ„Éù„Éº„ÉàÔºà`--aux_loss_coef`Ôºâ„ÄÇ\n- FlashAttention2„ÅÆÁµ±ÂêàÔºà`--flash_attn`Ôºâ„ÄÇ\n- QLoRAÔºà`--load_in_4bit`Ôºâ„Åä„Çà„Å≥[LoRA](./examples/scripts/train_sft_mixtral_lora.sh)Ôºà`--lora_rank`„ÄÅ`--target_modules`Ôºâ„ÅÆ„Çµ„Éù„Éº„Éà„ÄÇ\n- HuggingFace„ÅÆ`tokenizer.apply_chat_template`„Å®„ÅÆ‰∫íÊèõÊÄßÔºà`--apply_chat_template`„Åä„Çà„Å≥`--input_key`Ôºâ„ÄÇ\n- WandbÔºà`--use_wandb`Ôºâ„Åä„Çà„Å≥TensorBoardÔºà`--use_tensorboard`Ôºâ„Å´„Çà„Çã„É≠„Ç∞Ë®òÈå≤„ÅÆ„Çµ„Éù„Éº„Éà„ÄÇ\n- „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅÆÂõûÂæ©Ê©üËÉΩÔºà`--load_checkpoint`„Åä„Çà„Å≥`--save_steps`Ôºâ„ÄÇ\n- [DPO](./examples/scripts/train_llama_slurm.sh)„Åä„Çà„Å≥[Ray PPO](./examples/scripts/train_ppo_llama_ray_slurm.sh)„Å™„Å©„ÅÆ„Éû„É´„ÉÅ„Éé„Éº„Éâ„Éà„É¨„Éº„Éã„É≥„Ç∞„Çπ„ÇØ„É™„Éó„Éà„ÇíÊèê‰æõ„ÄÇ\n\n### PPO„Çµ„Éù„Éº„Éà„Éû„Éà„É™„ÉÉ„ÇØ„Çπ\n\n| ÁâπÂæ¥ | OpenRLHF | DSChat | CAIChat | TRL |\n| ------------- |:-------------:| :-------------:| :-------------:| :-------------:|\n| 16 A100-80GB„Åß70B+„ÅÆ„Éï„É´„ÉÅ„É•„Éº„Éã„É≥„Ç∞      | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |\n| 4 RTX4090„Åß7B„ÅÆ„Éï„É´„ÉÅ„É•„Éº„Éã„É≥„Ç∞ | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |\n| 8 A100-80GB„Åß34B DPO„ÅÆ„Éï„É´„ÉÅ„É•„Éº„Éã„É≥„Ç∞ | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |  \n| PPO„Åß„ÅÆÊé®Ë´ñ„Ç®„É≥„Ç∏„É≥„ÅÆ„Çµ„Éù„Éº„Éà | ‚úÖ      |    ‚úÖ | ‚ùå | ‚ùå |  \n| PPOÂÆüË£Ö„ÅÆ„Éà„É™„ÉÉ„ÇØ | ‚úÖ      |    ‚ùå | ‚ùå | ‚úÖ |\n| QLoRA„ÅÆ„Çµ„Éù„Éº„Éà | ‚úÖ      |    ‚ùå | ‚ùå | ‚úÖ | \n| Mixtral 8*7b„ÅÆ„Çµ„Éù„Éº„Éà | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |  \n| Êú™ÁµêÂêà„ÅÆActor-Critic„ÅÆ„Çµ„Éù„Éº„Éà | ‚úÖ     |   ‚úÖ | ‚úÖ | ‚ùå | \n| Ë§áÊï∞„ÅÆÂ†±ÈÖ¨„É¢„Éá„É´„ÅÆ„Çµ„Éù„Éº„Éà | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |   \n| Huggingface„É¢„Éá„É´„ÅÆ„Çµ„Éù„Éº„Éà | ‚úÖ      |    ‚úÖ | ‚úÖ | ‚úÖ | \n| ‰Ωø„ÅÑ„ÇÑ„Åô„Åï | ‚úÖ      |   ‚ùå (HybridEngine„ÅÆ„Éê„Ç∞) | ‚úÖ | ‚úÖ | \n\n## „ÇØ„Ç§„ÉÉ„ÇØ„Çπ„Çø„Éº„Éà\n\n### „Ç§„É≥„Çπ„Éà„Éº„É´\n\nOpenRLHF„Çí‰ΩøÁî®„Åô„Çã„Å´„ÅØ„ÄÅ„Åæ„ÅöDocker„Ç≥„É≥„ÉÜ„Éä„ÇíËµ∑Âãï„ÅóÔºà**Êé®Â•®**Ôºâ„ÄÅDocker„Ç≥„É≥„ÉÜ„ÉäÂÜÖ„Åß`pip install`„ÇíÂÆüË°å„Åó„Å¶openrlhf„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Åæ„ÅôÔºö\n\n```bash\n# Docker„Ç≥„É≥„ÉÜ„Éä„ÇíËµ∑Âãï\ndocker run --runtime=nvidia -it --rm --shm-size=\"10g\" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash\nsudo pip uninstall xgboost transformer_engine flash_attn -y\n\n# pip install\npip install openrlhf\n\n# vLLMÂä†ÈÄü„Çí‰ΩøÁî®„Åô„ÇãÂ†¥ÂêàÔºàvLLM 0.6.5„Çí„Ç§„É≥„Çπ„Éà„Éº„É´Ôºâ\npip install openrlhf[vllm]\n# ÊúÄÊñ∞„ÅÆvLLM„ÇÇ„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„Åæ„Åô\npip install openrlhf[vllm_latest]\n\n# ÊúÄÊñ∞„Éê„Éº„Ç∏„Éß„É≥„Çípip install\npip install git+https://github.com/OpenRLHF/OpenRLHF.git\n\n# „Åæ„Åü„ÅØgit clone\ngit clone https://github.com/OpenRLHF/OpenRLHF.git\ncd OpenRLHF\npip install -e .\n```\n\n> [!NOTE]\n>vLLM 0.6.4‰ª•Èôç„ÅÆ‰ΩøÁî®„Çí„ÅäÂãß„ÇÅ„Åó„Åæ„Åô„ÄÇ‰ªñ„ÅÆ„Éê„Éº„Ç∏„Éß„É≥ÔºàvLLM >= 0.4.2Ôºâ„ÅØ„ÄÅGloo„Çí‰ªã„Åó„Å¶Èáç„Åø„ÅÆÂêåÊúü„ÅåÂøÖË¶Å„Å™Â†¥Âêà„Åå„ÅÇ„Çä„Åæ„ÅôÔºà`--vllm_sync_backend gloo`Ôºâ„ÄÇ\n>„Åæ„Åü„ÄÅ[vLLMÁî®„ÅÆDockerfile](./dockerfile/)„Åä„Çà„Å≥[Nvidia-Docker„ÅÆ„ÉØ„É≥„ÇØ„É™„ÉÉ„ÇØ„Ç§„É≥„Çπ„Éà„Éº„É´„Çπ„ÇØ„É™„Éó„Éà](./examples/scripts/nvidia_docker_install.sh)„ÇÇÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n### „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊ∫ñÂÇô\nOpenRLHF„ÅØ„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇØ„É©„ÇπÂÜÖ„ÅßË§áÊï∞„ÅÆ„Éá„Éº„ÇøÂá¶ÁêÜÊñπÊ≥ï„ÇíÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n‰æã„Åà„Å∞„ÄÅ[Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6)„Åß„ÅØÔºö\n\n```python\ndef preprocess_data(data, input_template=None, input_key=\"input\", apply_chat_template=None) -> str:\n    if apply_chat_template:\n        chat = data[input_key]\n        if isinstance(chat, str):\n            chat = [{\"role\": \"user\", \"content\": chat}]\n        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    else:\n        prompt = data[input_key]\n        if input_template:\n            prompt = input_template.format(prompt)\n    return prompt\n```\n\n- `--input_key`„Çí‰ΩøÁî®„Åó„Å¶„ÄÅÂÖ•Âäõ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ`JSON key name`„ÇíÊåáÂÆö„Åó„ÄÅ`--prompt_data {name or path}`ÔºàPPOÔºâ„Åæ„Åü„ÅØ`--dataset {name or path}`„Çí‰ΩøÁî®„Åó„ÄÅ`--apply_chat_template`„Çí‰ΩøÁî®„Åó„Å¶[Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating)„ÅÆ`chat_template`„ÇíÂà©Áî®„Åß„Åç„Åæ„Åô„ÄÇ\n- `--apply_chat_template`„Çí‰ΩøÁî®„Åó„Åü„Åè„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅ‰ª£„Çè„Çä„Å´`--input_template`„Çí‰ΩøÁî®„Åô„Çã„Åã„ÄÅ‰∫ãÂâç„Å´„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„Ç™„Éï„É©„Ç§„É≥„ÅßÂâçÂá¶ÁêÜ„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\n- OpenRLHF„ÅØ„ÄÅ`--prompt_data_probs 0.1,0.4,0.5`ÔºàPPOÔºâ„Åæ„Åü„ÅØ`--dataset_probs 0.1,0.4,0.5`„Çí‰ΩøÁî®„Åó„Å¶Ë§áÊï∞„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÊ∑∑Âêà„Åô„Çã„Åì„Å®„ÇÇ„Çµ„Éù„Éº„Éà„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nChat Templating„ÅÆÂãï‰ΩúÊñπÊ≥ïÔºö\n\n```python\ndataset = [{\"input_key\": [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]}]\n\ntokenizer.apply_chat_template(dataset[0][\"input_key\"], tokenize=False)\n\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\n„Éà„É¨„Éº„Éã„É≥„Ç∞„Åä„Çà„Å≥„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊåáÂÆöÊñπÊ≥ïÔºö\n\n`data_type@data_dir`ÂΩ¢Âºè„Çí‰ΩøÁî®„Åó„Å¶ÊåáÂÆö„Åß„Åç„Åæ„Åô„ÄÇ‰æã„Åà„Å∞„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ`--dataset json@./data`„Å®„Åó„Å¶Ë®≠ÂÆö„Åß„Åç„Åæ„Åô„ÄÇ\n\n```\ndata\n‚îú‚îÄ‚îÄ test.jsonl\n‚îî‚îÄ‚îÄ train.jsonl\n```\n\n> [!NOTE]\n> „Éá„Éï„Ç©„É´„Éà„Åß„ÅØ„ÄÅ`train`„Åä„Çà„Å≥`test`„Çí‰ΩøÁî®„Åó„Å¶Huggingface„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„Åä„Çà„Å≥„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂå∫Âà•„Åó„Åæ„Åô„ÄÇ\n> `JSON key`„Ç™„Éó„Ç∑„Éß„É≥„ÅØÁâπÂÆö„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´‰æùÂ≠ò„Åó„Åæ„Åô„ÄÇË©≥Á¥∞„ÅØ[Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10)„Åä„Çà„Å≥[SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)„ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n### ÊïôÂ∏´„ÅÇ„ÇäÂæÆË™øÊï¥\n\nOpenRLHF„ÅÆ„É¢„Éá„É´„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅØHuggingFace„É¢„Éá„É´„Å®ÂÆåÂÖ®„Å´‰∫íÊèõÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ`--pretrain  {name or path}`„ÄÅ`--reward_pretrain  {name or path}`„ÄÅ„Åä„Çà„Å≥`--critic_pretrain  {name or path}`„Çí‰ΩøÁî®„Åó„Å¶„É¢„Éá„É´Âêç„Åæ„Åü„ÅØ„Éë„Çπ„ÇíÊåáÂÆö„Åß„Åç„Åæ„Åô„ÄÇ„ÅÑ„Åè„Å§„Åã„ÅÆ‰∫ãÂâç„Éà„É¨„Éº„Éã„É≥„Ç∞Ê∏à„Åø„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Å®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí[HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF)„ÅßÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nÊ¨°„Å´„ÄÅ[examples/scripts](./examples/scripts/)„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´Êèê‰æõ„Åï„Çå„Å¶„ÅÑ„ÇãËµ∑Âãï„Çπ„ÇØ„É™„Éó„Éà„Çí‰ΩøÁî®„Åô„Çã„Åã„ÄÅ‰ª•‰∏ã„ÅÆ„Ç≥„Éû„É≥„Éâ„Çí‰ΩøÁî®„Åó„Å¶„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíÈñãÂßã„Åß„Åç„Åæ„Åô„ÄÇ\n\n```bash \ndeepspeed --module openrlhf.cli.train_sft \\\n   --max_len 4096 \\\n   --dataset Open-Orca/OpenOrca \\\n   --input_key question \\\n   --output_key response \\\n   --input_template $'User: {}\\nAssistant: ' \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 2 \\\n   --max_samples 500000 \\\n   --pretrain meta-llama/Meta-Llama-3-8B \\\n   --save_path ./checkpoint/llama3-8b-sft \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --zero_stage 2 \\\n   --max_epochs 1 \\\n   --packing_samples \\\n   --bf16 \\\n   --flash_attn \\\n   --learning_rate 5e-6 \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n# HF tokenizer.apply_chat_template„ÅÆ„Çµ„Éù„Éº„Éà\n# --apply_chat_template \n# --tokenizer_chat_template {HF Chat Template}\n\n# RingAttention„ÅÆ„Çµ„Éù„Éº„Éà\n# pip install ring_flash_attn\n#   --ring_attn_size 2 \\\n#   --ring_head_stride 2 \\\n\n# Á∂ôÁ∂öÁöÑ„Å™‰∫ãÂâç„Éà„É¨„Éº„Éã„É≥„Ç∞„Å´„ÇÇ‰ΩøÁî®„Åß„Åç„Åæ„Åô\n# --pretrain_mode\n```\n\n> [!NOTE]\n> OpenRLHF SFT/DPO/RewardModel/PPO„Éà„É¨„Éº„Éä„Éº„ÅØ`--packing_samples`„Çí„Çµ„Éù„Éº„Éà„Åó„Å¶„ÅÑ„Åæ„Åô [`--flash_attn`„Å´Âü∫„Å•„Åè](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)\n\n### Â†±ÈÖ¨„É¢„Éá„É´„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞\n```bash\ndeepspeed --module openrlhf.cli.train_rm \\\n   --save_path ./checkpoint/llama3-8b-rm \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 1 \\\n   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n   --bf16 \\\n   --max_epochs 1 \\\n   --max_len 8192 \\\n   --zero_stage 3 \\\n   --learning_rate 9e-6 \\\n   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \\\n   --apply_chat_template \\\n   --chosen_key chosen \\\n   --rejected_key rejected \\\n   --flash_attn \\\n   --packing_samples \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n```\n\nÂ†±ÈÖ¨„É¢„Éá„É´„ÅÆ`--value_prefix_head`„Ç™„Éó„Ç∑„Éß„É≥„Çí`score`„Å´Ë®≠ÂÆö„Åô„Çã„Åì„Å®„Çí„ÅäÂãß„ÇÅ„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ`AutoModelForSequenceClassification`„Çí‰ΩøÁî®„Åó„Å¶„É¢„Éá„É´„Çí„É≠„Éº„Éâ„Åß„Åç„Åæ„ÅôÔºö\n\n```python\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n              reward_model_path,\n              num_labels=1,\n              torch_dtype=torch.bfloat16,\n              attn_implementation=\"flash_attention_2\",\n              use_cache=False,\n          )\ninputs = xxxx (Left Padding Input Tokens)\nreward = reward_model.model(*inputs).last_hidden_state\nreward = reward_model.score(reward)[:, -1]\n```\n\n### Ray„Çí‰ΩøÁî®„Åó„Å™„ÅÑPPO\n\n```bash\ndeepspeed --module openrlhf.cli.train_ppo \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path ./checkpoint/llama-3-8b-rlhf \\\n  --save_steps -1 \\\n  --logging_steps 1 \\\n  --eval_steps -1 \\\n  --micro_train_batch_size 2 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 4 \\\n  --rollout_batch_size 1024 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 2 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --max_samples 100000 \\\n  --normalize_reward \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --use_wandb {wandb_token}\n\n# „É™„É¢„Éº„ÉàÂ†±ÈÖ¨„É¢„Éá„É´„ÅÆ„Çµ„Éù„Éº„ÉàÔºàHTTPÔºâ\n# --remote_rm_url http://localhost:5000/get_reward\n```\n\n### Ray„Å®vLLM„Çí‰ΩøÁî®„Åó„ÅüPPO/REINFORCE++\n\nRLHF„Éà„É¨„Éº„Éã„É≥„Ç∞ÈÄüÂ∫¶„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åã„ÄÅ70B„É¢„Éá„É´„Çí„Çµ„Éù„Éº„Éà„Åô„Çã„Åü„ÇÅ„Å´„ÄÅRay„Å®vLLMÂä†ÈÄü„Çí‰ΩøÁî®„Åó„ÅüPPO„Çí‰ΩøÁî®„Åß„Åç„Åæ„Åô\n\n```bash\n# „Ç≥„É≥„ÉÜ„ÉäÂÜÖ„ÅßRay„ÅÆ„Éû„Çπ„Çø„Éº„Éé„Éº„Éâ„ÇíËµ∑Âãï\nray start --head --node-ip-address 0.0.0.0 --num-gpus 8\n\n# „Åï„Çâ„Å´Â§ö„Åè„ÅÆ„Éé„Éº„Éâ„ÅßRay„ÇíËµ∑Âãï„Åô„ÇãÂ†¥Âêà„ÅØ\nray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8\n\nray job submit --address=\"http://127.0.0.1:8265\" \\\n  --runtime-env-json='{\"working_dir\": \"/openrlhf\"}' \\\n  -- python3 -m openrlhf.cli.train_ppo_ray \\\n  --ref_num_nodes 1 \\\n  --ref_num_gpus_per_node 2 \\\n  --reward_num_nodes 1 \\\n  --reward_num_gpus_per_node 2 \\\n  --critic_num_nodes 1 \\\n  --critic_num_gpus_per_node 2 \\\n  --actor_num_nodes 1 \\\n  --actor_num_gpus_per_node 2 \\\n  --vllm_num_engines 2 \\\n  --vllm_tensor_parallel_size 2 \\\n  --colocate_critic_reward \\\n  --colocate_actor_ref \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path /openrlhf/examples/checkpoint/llama3-8b-rlhf \\\n  --micro_train_batch_size 8 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 16 \\\n  --rollout_batch_size 1024 \\\n  --max_samples 100000 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 3 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --normalize_reward \\\n  --packing_samples \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --use_wandb {wandb_token}\n\n# REINFORCE++ | RLOO„ÅÆ„Çµ„Éù„Éº„Éà\n# --advantage_estimator reinforce | rloo\n\n# „É™„É¢„Éº„ÉàÂ†±ÈÖ¨„É¢„Éá„É´„ÅÆ„Çµ„Éù„Éº„ÉàÔºàHTTPÔºâ\n# --remote_rm_url http://localhost:5000/get_reward\n\n\n# N„Çµ„É≥„Éó„É´„ÅÆ„Çµ„Éù„Éº„Éà\n# --n_samples_per_prompt 4\n```\n> [!NOTE]\n> `--vllm_num_engines`„ÇíË®≠ÂÆö„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅvLLM„Ç®„É≥„Ç∏„É≥„Çí‰ΩøÁî®„Åó„Å™„ÅÑ„Åì„Å®„ÇíÊÑèÂë≥„Åó„Åæ„Åô„ÄÇ\n> `setup_commands`„Çí‰ΩøÁî®„Åó„Å¶Ray„ÅåËá™ÂãïÁöÑ„Å´Áí∞Â¢É„Çí„Éá„Éó„É≠„Ç§„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇ‰æã„Åà„Å∞„ÄÅ`--runtime-env-json='{\"setup_commands\": [\"pip install openrlhf[vllm]\"]}'`„ÄÇ\n\n[!NOTE]\nOPENRLHF„ÅÆRLOO„ÅØ„ÄÅREINFORCE++„ÇíÂü∫„Å´ÊîπËâØ„Åï„Çå„Åü„ÇÇ„ÅÆ„Åß„ÅÇ„Çä„ÄÅ„Ç™„É™„Ç∏„Éä„É´Áâà„Å®„ÅØÁï∞„Å™„Çä„Åæ„Åô„ÄÇ\n\n> [!NOTE]\n> deepspeed„ÅåGPU„Éá„Éê„Ç§„Çπ„ÇíË®≠ÂÆö„Åô„ÇãÈöõ„Å´„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅåÁØÑÂõ≤Â§ñ„Å´Èñ¢ÈÄ£„Åô„Çã„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„ÅüÂ†¥Âêà„ÄÅÁí∞Â¢ÉÂ§âÊï∞[`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py)„ÇíË®≠ÂÆö„Åó„Å¶ÂõûÈÅøÁ≠ñ„ÇíË©¶„Åô„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\n>   ```bash\n>   # NVIDIA GPU„ÅÆÂ†¥Âêà:\n>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1\n>   ```\n\n„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„Çã„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆËµ∑Âãï„Çπ„ÇØ„É™„Éó„Éà„Å®„Éâ„Ç≠„É•„É°„É≥„Éà„ÅØ[example/scripts](./examples/scripts/)„Åä„Çà„Å≥[Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ\n\n## „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ\n\nAdam„Ç™„Éï„É≠„Éº„Éâ„ÅÆÊúâÂäπÂåñ„ÄÅÂ†±ÈÖ¨„É¢„Éá„É´ÔºàRMÔºâ„Åä„Çà„Å≥ÂèÇÁÖß„É¢„Éá„É´ÔºàRefÔºâ„Ç™„Éï„É≠„Éº„Éâ„Å™„Å©„ÅÆÊäÄË°ì„Çí‰ΩøÁî®„Åó„Å¶„ÄÅDSChat„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÊúÄÂ§ßÈôê„Å´ÊúÄÈÅ©Âåñ„Åó„ÄÅÊé®Ë´ñÊÆµÈöé„Åß„ÅÆ„Éû„Ç§„ÇØ„É≠„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÇíÂ¢ó„ÇÑ„Åó„ÄÅ„É°„É¢„É™‰∏çË∂≥„ÅÆÂïèÈ°å„ÇíÂõûÈÅø„Åó„Åæ„Åó„Åü„ÄÇLLaMA2„ÅÆ„Éè„Ç§„Éñ„É™„ÉÉ„Éâ„Ç®„É≥„Ç∏„É≥ÔºàHEÔºâ„ÇíÊúâÂäπ„Å´„Åô„Çã„Åü„ÇÅ„Å´„ÄÅDSChat„ÅÆ„ÅÑ„Åè„Å§„Åã„ÅÆ„Éê„Ç∞„ÇÇ‰øÆÊ≠£„Åó„Åæ„Åó„Åü„ÄÇOptimized DSChat„Å®OpenRLHF„Çí‰ΩøÁî®„Åó„Å¶1024„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí1„Å§„ÅÆPPO„Ç®„Éù„ÉÉ„ÇØ„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åô„Çã„ÅÆ„Å´„Åã„Åã„ÇãÂπ≥ÂùáÊôÇÈñìÔºàÁßíÔºâ„ÅØÊ¨°„ÅÆ„Å®„Åä„Çä„Åß„ÅôÔºö\n\n| **„Çµ„Ç§„Ç∫** | **NVIDIA A800-80GB GPU** | **Optimized DSChatÔºà„Éè„Ç§„Éñ„É™„ÉÉ„Éâ„Ç®„É≥„Ç∏„É≥‰ªò„ÅçÔºâ** | **OpenRLHF** | **„Çπ„Éî„Éº„Éâ„Ç¢„ÉÉ„Éó** |\n| :---: | :---: | :---: | :---: | :---: |\n| 7B | 16 | 855.09 | 471.11 | 1.82x |\n| 13B | 32 | 1528.93 | 608.93 | 2.5x |\n| 34B | 32 | 3634.98 | 1526.4 | 2.4x |\n| 70B | 32 | 10407.0 | 4488.53 | 2.3x |\n\n> [!NOTE]\n> „Éá„Éº„Çø„ÅØÂè§„Åè„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÂÜç„ÉÜ„Çπ„Éà„ÅÆ„Åü„ÇÅ„Å´„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n### „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Ç¨„Ç§„Éâ\n\nÊúÄÈÅ©„Å™„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÈÅîÊàê„Åô„Çã„Åü„ÇÅ„Å´„ÄÅvLLM„Ç®„É≥„Ç∏„É≥„Å´„Çà„ÇäÂ§ö„Åè„ÅÆ„Éé„Éº„Éâ„ÇíÂâ≤„ÇäÂΩì„Å¶„Çã„Åì„Å®„Çí„ÅäÂãß„ÇÅ„Åó„Åæ„Åô„ÄÇ‰æã„Åà„Å∞„ÄÅ32ÂÄã„ÅÆA100 GPU„ÇíÊåÅ„Å§70B„É¢„Éá„É´„ÅÆÂ†¥Âêà„ÄÅ16ÂÄã„ÅÆA100 GPU„ÇívLLM„Ç®„É≥„Ç∏„É≥„Å´Ââ≤„ÇäÂΩì„Å¶„ÄÅ8ÂÄã„ÅÆGPU„ÇíActor„É¢„Éá„É´„Å´„ÄÅÊÆã„Çä„ÅÆ8ÂÄã„ÅÆGPU„ÇíCritic„É¢„Éá„É´„Å´Ââ≤„ÇäÂΩì„Å¶„Çã„Åì„Å®„Çí„ÅäÂãß„ÇÅ„Åó„Åæ„Åô„ÄÇ„Åï„Çâ„Å´„ÄÅ`--colocate_critic_reward`„ÄÅ`--colocate_actor_ref`„Ç™„Éó„Ç∑„Éß„É≥„ÇíÊúâÂäπ„Å´„Åó„Å¶„Éé„Éº„Éâ„Çí„Éû„Éº„Ç∏„Åó„Åæ„Åô„ÄÇÊúÄÂæå„Å´„ÄÅ`rollout_micro_batch_size`Ôºà„Åä„Çà„Å≥vLLM„Ç®„É≥„Ç∏„É≥„ÅÆTP„Çµ„Ç§„Ç∫„ÇíÊúÄÂ∞èÂåñÔºâ„ÇíÂèØËÉΩ„Å™Èôê„ÇäÂ¢ó„ÇÑ„Åô„Åπ„Åç„Åß„Åô„ÄÇ„Éà„É¨„Éº„Éã„É≥„Ç∞„Éï„Çß„Éº„Ç∫„Åß„ÅØ„ÄÅ„Çà„ÇäÂ§ß„Åç„Å™`--micro_train_batch_size`„ÅåÊúõ„Åæ„Åó„Åè„ÄÅ`--packing_samples`„ÇíÊúâÂäπ„Å´„Åó„Åæ„Åô„ÄÇÂçÅÂàÜ„Å™GPU„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ`--adam_offload`„ÇíÁÑ°Âäπ„Å´„Åó„ÄÅ`--overlap_comm`„ÇíÊúâÂäπ„Å´„Åó„Åæ„Åô„ÄÇ„Éû„É´„ÉÅ„Éé„Éº„ÉâRLHF„ÅÆÂ†¥Âêà„ÄÅvLLM 0.6.4+„Åß`--vllm_sync_backend nccl`„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n## OpenRLHF„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Çã‰ºÅÊ•≠„Å®ÁµÑÁπî\n\n- Google\n- ByteDance\n- Tencent\n- Alibaba\n- Baidu\n- China Telecom\n- Vivo\n- Allen AI\n- NexusFlow\n- J√ºlich Supercomputing Centre (JSC)\n- Berkeley Starling Team\n- M-A-P\n- ...\n\n## ÂèÇÂä†ÊñπÊ≥ï\n\n**ÂèÇÂä†ÊñπÊ≥ï**\n\n1. janhu9527@gmail.com„Å´„É°„Éº„É´„ÇíÈÄÅ„Çã„Åã„ÄÅ[GitHub Organization](https://github.com/OpenRLHF)„Å´ÂèÇÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ª•‰∏ã„ÅÆË©≥Á¥∞„ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑÔºö\n   - „ÅÇ„Å™„Åü„ÅÆÂêçÂâç\n   - „ÅÇ„Å™„Åü„ÅÆGitHub„É¶„Éº„Ç∂„ÉºÂêç\n   - „ÅÇ„Å™„Åü„ÅÆËààÂë≥„ÅÆ„ÅÇ„ÇãÂàÜÈáé\n   - NLP„Åä„Çà„Å≥/„Åæ„Åü„ÅØAI„Å´Èñ¢ÈÄ£„Åô„Çã„Çπ„Ç≠„É´„Å®ÁµåÈ®ì\n1. ÂÖ¨ÂºèGitHub[OpenRLHF ‚Üó](https://github.com/OpenRLHF/OpenRLHF)„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Éö„Éº„Ç∏„ÇíÈÄö„Åò„Å¶ÂèÇÂä†„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇË≤¢ÁåÆ„Åó„Åü„ÅÑËààÂë≥„Å´„Å§„ÅÑ„Å¶„ÅÆissue„Çí‰ΩúÊàê„Åô„Çã„Å†„Åë„Åß„ÄÅÁßÅ„Åü„Å°„ÅåÈÄ£Áµ°„Åó„Åæ„Åô„ÄÇ\n\n**‰Ωï„Åå„Åß„Åç„Çã„Åã**\n\n1. „ÉÅ„Éº„É†„Å´ÂèÇÂä†„Åó„ÄÅOpenRLHF„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆÈñãÁô∫„Å´ÂèÇÂä†„Åó„Åæ„Åô„ÄÇ\n1. „Éó„É≠„Ç∏„Çß„ÇØ„Éà„Å´Ë≤¢ÁåÆ„Åô„Çã„Åü„ÇÅ„Å´„Éó„É´„É™„ÇØ„Ç®„Çπ„Éà„ÇíÊèêÂá∫„Åó„Åæ„Åô„ÄÇ\n1. „Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆÊîπÂñÑ„ÄÅ„Éê„Ç∞„ÅÆ‰øÆÊ≠£„ÄÅÊñ∞Ê©üËÉΩ„ÅÆ‰ΩúÊàê„ÇíÊâã‰ºù„ÅÑ„Åæ„Åô„ÄÇ\n1. „Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÂÖ±Êúâ„Åó„ÄÅ„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„ÅÆÊàêÈï∑„ÇíÊîØÊè¥„Åó„Åæ„Åô„ÄÇ\n\n## „Çπ„Éù„É≥„Çµ„Éº\n\n„Çπ„Éù„É≥„Çµ„Éº„Ç∑„ÉÉ„Éó„ÅØ„ÄÅOpenRLHF„ÅÆÁ∂≠ÊåÅ„Å®ÊîπÂñÑ„Å´ÂΩπÁ´ã„Å°„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅåÂΩπÁ´ã„Å§„Å®ÊÑü„Åò„ÅüÂ†¥Âêà„ÅØ„ÄÅ„Çπ„Éù„É≥„Çµ„Éº„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ[Open Collective ‚Üó](https://opencollective.com/OpenRLHF)„Åß„Çπ„Éù„É≥„Çµ„Éº„Å´„Å™„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\n\n## „Çπ„Çø„ÉºÂ±•Ê≠¥\n\n[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)\n\n## Ë≤¢ÁåÆËÄÖ\n\n„Åô„Åπ„Å¶„ÅÆË≤¢ÁåÆËÄÖ„Å´ÊÑüË¨ù„Åó„Åæ„ÅôÔºÅË≤¢ÁåÆ„Åó„Åü„ÅÑÂ†¥Âêà„ÅØ„ÄÅ„Éó„É´„É™„ÇØ„Ç®„Çπ„Éà„Çí‰ΩúÊàê„Åô„Çã„Åã„ÄÅissue„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n<a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF\" />\n</a>\n\n## ÂèÇËÄÉÊñáÁåÆ„Å®Ë¨ùËæû\n\nAI„Åä„Çà„Å≥NLPÂàÜÈáé„Å∏„ÅÆË≤¢ÁåÆ„Å´ÂØæ„Åó„Å¶„ÄÅ‰ª•‰∏ã„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Åä„Çà„Å≥ÁµÑÁπî„Å´ÊÑüË¨ù„Åó„Åæ„ÅôÔºö\n\n- [Hugging Face Transformers ‚Üó](https://github.com/huggingface/transformers)\n- [OpenAI GPT ‚Üó](https://github.com/openai/gpt-3)\n- [LLaMA ‚Üó](https://llama.meta.com/)\n- [DeepSpeed ‚Üó](https://github.com/microsoft/DeepSpeed)\n- [Ray ‚Üó](https://github.com/ray-project/ray)\n\nÁßÅ„Åü„Å°„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØ„ÄÅ[ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)„Åä„Çà„Å≥[DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)„Å´„ÇÇÊÑüË¨ù„Åó„Åæ„Åô„ÄÇ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆÂàùÊúüÊÆµÈöé„Åß„ÄÅÂΩº„Çâ„ÅÆ„Ç≥„Éº„Éâ„Éá„Ç∂„Ç§„É≥„ÇíÂèÇËÄÉ„Å´„Åó„Åæ„Åó„Åü„ÄÇ\n\n(2024/7) ÁßÅ„Åü„Å°„ÅÆGitHubÁµÑÁπî„ÅØOpenLLMAI„Åã„ÇâOpenRLHF„Å´Â§âÊõ¥„Åï„Çå„Åæ„Åó„Åü„ÄÇ\n\n## ÂºïÁî®\n```\n@article{hu2024openrlhf,\n  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},\n  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},\n  journal={arXiv preprint arXiv:2405.11143},\n  year={2024}\n}\n```\n\n______________________________________________________________________\n\n*OpenRLHF ¬© 2025 OpenRLHF. All Rights Reserved.*\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 20.1875,
          "content": "<div align=\"center\">\n<p align=\"center\">\n<img alt=\"\" src=\"./docs/logo.png\" style=\"display: inline-block; height: 140px\" />\n</p>\n</div>\n\n<div align=\"center\">\n<p align=\"center\">\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n        <img alt=\"GitHub Contributors\" src=\"https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/issues\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/discussions\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/pulls\">\n        <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff\" />\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/stargazers\">\n        <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf\" />\n      </a>\n      <br>\n      <em>ÂºÄÊ∫ê / ÂÖ®Èù¢ / ËΩªÈáèÁ∫ß / ÊòìÁî®</em>\n    </p>\n</p>\n</div>\n\n<hr>\n\n<span>[ <a href=\"README.md\">English</a> | ‰∏≠Êñá | <a href=\"README_ja.md\">Êó•Êú¨Ë™û</a> ]</span>\n\nOpenRLHF ÊòØ‰∏Ä‰∏™Âü∫‰∫é Ray„ÄÅDeepSpeed Âíå HF Transformers ÊûÑÂª∫ÁöÑÈ´òÊÄßËÉΩ RLHF Ê°ÜÊû∂Ôºö\n\n- **ÁÆÄÂçïÊòìÁî®**: OpenRLHF ÊòØÁõÆÂâçÂèØÁî®ÁöÑÊúÄÁÆÄÂçïÁöÑÈ´òÊÄßËÉΩ RLHF Â∫ì‰πã‰∏ÄÔºåÊó†ÁºùÂÖºÂÆπ Huggingface Ê®°ÂûãÂíåÊï∞ÊçÆÈõÜ„ÄÇ\n- **È´òÊÄßËÉΩ**: RLHF ËÆ≠ÁªÉ‰∏≠ 80% ÁöÑÊó∂Èó¥Áî®‰∫éÊ†∑Êú¨ÁîüÊàêÈò∂ÊÆµ„ÄÇÂæóÁõä‰∫é‰ΩøÁî® Ray, Packing Samples ‰ª•Âèä vLLM ÁîüÊàêÂä†ÈÄüÁöÑËÉΩÂäõÔºåOpenRLHF ÁöÑÊÄßËÉΩÊòØÊûÅËá¥‰ºòÂåñÁöÑ DeepSpeedChat with Hybrid Engine ÁöÑ3~4ÂÄç‰ª•‰∏ä„ÄÇ\n- **ÂàÜÂ∏ÉÂºè RLHF**:  OpenRLHF ‰ΩøÁî® Ray Â∞Ü Actor„ÄÅReward„ÄÅReference Âíå Critic Ê®°ÂûãÂàÜÂ∏ÉÂà∞‰∏çÂêåÁöÑ GPU ‰∏äÔºåÂêåÊó∂Â∞Ü Adam ‰ºòÂåñÂô®ÊîæÂú® CPU ‰∏ä„ÄÇËøô‰ΩøÂæó‰ΩøÁî®Â§ö‰∏™ A100 80G GPU Âíå vLLM ÂèØ‰ª•ÂÖ®Èù¢ÂæÆË∞ÉË∂ÖËøá 70B+ ÁöÑÊ®°Âûã ‰ª•ÂèäÂú®Â§ö‰∏™ 24GB RTX 4090 GPU ‰∏äÂæÆË∞É 7B Ê®°Âûã„ÄÇ\n- **PPO ÂÆûÁé∞ÊäÄÂ∑ß**: Êàë‰ª¨ÈõÜÊàê‰∫Ü PPO ÁöÑÂÆûÁé∞ÊäÄÂ∑ß‰ª•ÊèêÈ´òËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÔºåËØ¶ÊÉÖÂèÇËÄÉ [Áü•‰πé](https://zhuanlan.zhihu.com/p/622134699) Âíå [Notion blog](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361).\n\nÊõ¥Â§öÁªÜËäÇËØ∑ÂèÇËÄÉ [PPT](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [ÊäÄÊúØÊä•Âëä](https://arxiv.org/abs/2405.11143) | [‰ΩøÁî®ÊñáÊ°£](https://openrlhf.readthedocs.io/)\n\n\n## Êñ∞Èóª  \n- [2024/12] Êàë‰ª¨\"ÊèêÂá∫\"‰∫Ü üòä [REINFORCE++ ÂØπÈΩêÁÆóÊ≥ï](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS).\n- [2024/12] Âú® [Notion Blog](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05) ‰∏≠ÔºåÊàë‰ª¨ÂØπ PPO„ÄÅREINFORCE++„ÄÅGRPO Âíå RLOO ËøõË°å‰∫ÜÂàÜÊûê„ÄÇ  \n\n## ÁâπÊÄß  \n\n- Âü∫‰∫é Ray ÁöÑÂàÜÂ∏ÉÂºè [PPO](./examples/scripts/train_ppo_llama_ray.sh) Âíå [REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray.sh) ÂÆûÁé∞„ÄÇ  \n- ÊîØÊåÅÂØπ [Ë∂ÖËøá 700 ‰∫øÂèÇÊï∞ÁöÑÊ®°Âûã](./examples/scripts/train_ppo_llama_ray_70b.sh) ËøõË°åÂÆåÊï¥ÁöÑ RLHF ÂæÆË∞É„ÄÇ  \n- ÈõÜÊàê vLLMÔºåÂä†ÈÄü RLHF ‰ªªÂä°‰∏≠ÁöÑÊ†∑Êú¨ÁîüÊàêÔºà`--vllm_num_engines`Ôºâ„ÄÇ  \n- ÊîØÊåÅÂ§ö‰∏™Â•ñÂä±Ê®°ÂûãÔºà`--reward_pretrain model1,model2...`ÔºâÂíåËøúÁ®ãÂ•ñÂä±Ê®°ÂûãÔºà`--remote_rm_url`Ôºâ„ÄÇ  \n- ÂÆûÁé∞ [DPOÔºàÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºâ/IPO/cDPO](./examples/scripts/train_dpo_llama.sh) Âíå [Kahneman-Tversky OptimizationÔºàKTOÔºâ](./examples/scripts/train_kto_llama.sh)„ÄÇ  \n- ÊîØÊåÅ [Ëø≠‰ª£ DPO](./examples/scripts/train_iterative_dpo_llama.sh)Ôºà[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)Ôºâ„ÄÇ  \n- ÊîØÊåÅ [ÊãíÁªùÈááÊ†∑](./examples/scripts/train_rejection_sampling_llama.sh)„ÄÇ  \n- ÂÆûÁé∞ [Êù°‰ª∂ SFT](./examples/scripts/train_conditional_llama.sh)Ôºà[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)Ôºâ„ÄÇ  \n- ÊîØÊåÅ [Áü•ËØÜËí∏È¶è](./examples/scripts/train_knowledge_distillation.sh)Ôºà[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)Ôºâ„ÄÇ  \n- ÈõÜÊàê [ËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâ](./examples/scripts/train_prm_mistral.sh)„ÄÇ  \n- ÊîØÊåÅ SFT„ÄÅDPO„ÄÅRM„ÄÅPRM Âíå PPO ÁöÑËÆ≠ÁªÉÊ†∑Êú¨ÊâìÂåÖÔºà`--packing_samples`Ôºâ„ÄÇ  \n- ÂÆûÁé∞ [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)Ôºà`--ring_attn_size`Ôºå`--ring_head_stride`Ôºâ„ÄÇ  \n- ÊîØÊåÅ [‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºàMoEÔºâ](./examples/test_scripts/train_sft_mixtral_lora.sh)Ôºà`--aux_loss_coef`Ôºâ„ÄÇ  \n- ÈõÜÊàê FlashAttention2Ôºà`--flash_attn`Ôºâ„ÄÇ  \n- ÊîØÊåÅ QLoRAÔºà`--load_in_4bit`ÔºâÂíå [LoRA](./examples/scripts/train_sft_mixtral_lora.sh)Ôºà`--lora_rank`Ôºå`--target_modules`Ôºâ„ÄÇ  \n- ÂÖºÂÆπ HuggingFace ÁöÑ `tokenizer.apply_chat_template` Êï∞ÊçÆÈõÜÊ†ºÂºèÔºà`--apply_chat_template` Âíå `--input_key`Ôºâ„ÄÇ  \n- ÊîØÊåÅ‰ΩøÁî® WandbÔºà`--use_wandb`ÔºâÂíå TensorBoardÔºà`--use_tensorboard`ÔºâËøõË°åÊó•ÂøóËÆ∞ÂΩï„ÄÇ  \n- ÊîØÊåÅ‰ªéÊ£ÄÊü•ÁÇπÊÅ¢Â§çËÆ≠ÁªÉÔºà`--load_checkpoint` Âíå `--save_steps`Ôºâ„ÄÇ  \n- Êèê‰æõ‰∫ÜÂ§öËäÇÁÇπËÆ≠ÁªÉËÑöÊú¨, ÊØîÂ¶Ç [DPO](./examples/scripts/train_llama_slurm.sh) Âíå [RLHF](./examples/scripts/train_ppo_llama_ray_slurm.sh)\n\n\n### PPO ÊîØÊåÅÁü©Èòµ\n\n| ÁâπÊÄß | OpenRLHF | DSChat | CAIChat | TRL |\n| ------------- |:-------------:| :-------------:| :-------------:| :-------------:| \n| ‰ΩøÁî® 16 ‰∏™ A100 ÂÆåÊàê 70B+ ÂÖ®ÂæÆË∞É      | ‚úÖ | ‚ùå | ‚ùå | ‚ùå ||\n| ‰ΩøÁî® 4 ‰∏™ RTX4090 ÂÆåÊàê 7B ÂÖ®ÂæÆË∞É | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå | \n| ‰ΩøÁî® 8 ‰∏™ A100 ÂÆåÊàê 34B DPO ÂÖ®ÂæÆË∞É | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |   \n| ÊîØÊåÅÊé®ÁêÜÂºïÊìéÂä†ÈÄü | ‚úÖ      |    ‚úÖ | ‚ùå | ‚ùå |  \n| PPO ÂÆûÁé∞ÊäÄÂ∑ß | ‚úÖ      |    ‚ùå | ‚ùå | ‚úÖ | \n| ÊîØÊåÅ QLoRA | ‚úÖ      |    ‚ùå | ‚ùå | ‚úÖ | \n| ÊîØÊåÅ Mixtral 8*7b | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå | \n| ÊîØÊåÅÊú™ÂêàÂπ∂ÁöÑ Actor-Critic | ‚úÖ     |   ‚úÖ | ‚úÖ | ‚ùå | \n| ÊîØÊåÅÂ§ö‰∏™Â•ñÂä±Ê®°Âûã | ‚úÖ      |    ‚ùå | ‚ùå | ‚ùå |   \n| ÊîØÊåÅ Huggingface Ê®°Âûã | ‚úÖ      |    ‚úÖ | ‚úÖ | ‚úÖ | \n| Êòì‰∫é‰ΩøÁî® | ‚úÖ      |   ‚ùå (HybridEngine bugs) | ‚úÖ | ‚úÖ | \n\n## Âø´ÈÄüÂºÄÂßã\n\n### ÂÆâË£Ö\n\nË¶Å‰ΩøÁî® OpenRLHFÔºåÈ¶ñÂÖàÂêØÂä® Docker ÂÆπÂô®Ôºà**Êé®Ëçê**ÔºâÁÑ∂ÂêéÊâßË°å `pip install` ÂÆâË£Ö `openrlhf`Ôºö\n\n```bash\n# ÂêØÂä® docker container\ndocker run --runtime=nvidia -it --rm --shm-size=\"10g\" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash\nsudo pip uninstall xgboost transformer_engine flash_attn -y\n\n# pip install\npip install openrlhf\n\n# Â¶ÇÊûú‰Ω†ÈúÄË¶Å‰ΩøÁî® vLLM Âä†ÈÄü (ÂÆâË£Ö vLLM 0.6.5)\npip install openrlhf[vllm]\n# ÊúÄÊñ∞ÁöÑ vLLM ‰πüÊòØÊîØÊåÅÁöÑ\npip install openrlhf[vllm_latest]\n\n# pip install GitHub ‰∏äÁöÑÊúÄÊñ∞Áâà\npip install git+https://github.com/OpenRLHF/OpenRLHF.git\n\n# ÊàñËÄÖ git clone\ngit clone https://github.com/OpenRLHF/OpenRLHF.git\ncd OpenRLHF\npip install -e .\n```\n\n> [!NOTE]\n>Êàë‰ª¨Êé®Ëçê‰ΩøÁî® vLLM 0.6.4+ÔºåÂÖ∂‰ªñÁâàÊú¨ (vLLM >= 0.4.2) ÂèØËÉΩÈúÄË¶ÅÈÄöËøá Gloo ËøõË°åÊùÉÈáçÂêåÊ≠•Ôºà`--vllm_sync_backend gloo`Ôºâ„ÄÇ\n>Êàë‰ª¨‰πüÊèê‰æõ‰∫Ü [Dockerfiles for vLLM](./dockerfile/) Âíå [Nvidia-Docker ‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨](./examples/scripts/nvidia_docker_install.sh)„ÄÇ\n\n### ÂáÜÂ§áÊï∞ÊçÆÈõÜ\nOpenRLHF Âú®ÂÖ∂Êï∞ÊçÆÈõÜÁ±ª‰∏≠Êèê‰æõ‰∫ÜÂ§öÁßçÊï∞ÊçÆÂ§ÑÁêÜÊñπÊ≥ï„ÄÇ\n‰æãÂ¶ÇÂú® [Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6) ‰∏≠Ôºö\n\n```python\ndef preprocess_data(data, input_template=None, input_key=\"input\", apply_chat_template=None) -> str:\n    if apply_chat_template:\n        chat = data[input_key]\n        if isinstance(chat, str):\n            chat = [{\"role\": \"user\", \"content\": chat}]\n        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    else:\n        prompt = data[input_key]\n        if input_template:\n            prompt = input_template.format(prompt)\n    return prompt\n```\n\n- Êàë‰ª¨ÂèØ‰ª•‰ΩøÁî® `--input_key` ÊåáÂÆö `JSON key name` ‰∏∫ËæìÂÖ•Êï∞ÊçÆÈõÜ `--prompt_data {name or path}` (PPO) Êàñ `--dataset {name or path}`ÔºåÂπ∂‰ΩøÁî® `--apply_chat_template` Âà©Áî® [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating) ‰∏≠ÁöÑ `chat_template`„ÄÇ\n- Â¶ÇÊûú‰∏çÊÉ≥‰ΩøÁî® `--apply_chat_template`ÔºåÂèØ‰ª•ÊîπÁî® `--input_template`ÔºåÊàñÈ¢ÑÂÖàÁ¶ªÁ∫øÂ§ÑÁêÜÊï∞ÊçÆÈõÜ„ÄÇ\n- OpenRLHF ËøòÊîØÊåÅ‰ΩøÁî® `--prompt_data_probs 0.1,0.4,0.5` (PPO) Êàñ `--dataset_probs 0.1,0.4,0.5` Ê∑∑ÂêàÂ§ö‰∏™Êï∞ÊçÆÈõÜ„ÄÇ\n\nChat Templating ÁöÑÂ∑•‰ΩúÂéüÁêÜÂ¶Ç‰∏ã:\n\n```python\ndataset = [{\"input_key\": [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]}]\n\ntokenizer.apply_chat_template(dataset[0][\"input_key\"], tokenize=False)\n\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\nÂ¶Ç‰ΩïÊåáÂÆöËÆ≠ÁªÉÂíåÊµãËØïÊï∞ÊçÆÂàÜÂå∫ ?\n\n‰Ω†ÂèØ‰ª•‰ΩøÁî® `data_type@data_dir` ÁöÑÊñπÂºèÊåáÂÆö, ÊØîÂ¶Ç‰∏ãÈù¢ÁöÑÊï∞ÊçÆÈõÜÂèØ‰ª•ËÆæÁΩÆ‰∏∫ `--dataset json@./data`\n\n```\ndata\n‚îú‚îÄ‚îÄ test.jsonl\n‚îî‚îÄ‚îÄ train.jsonl\n```\n\n> [!NOTE]\n>ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÊàë‰ª¨‰ΩøÁî® `train` Âíå `test` ‰Ωú‰∏∫ split Âå∫ÂàÜ Huggingface ÁöÑËÆ≠ÁªÉ/ÊµãËØïÊï∞ÊçÆ„ÄÇ\n>`JSON key` ÈÄâÈ°πÂèñÂÜ≥‰∫éÂÖ∑‰ΩìÁöÑÊï∞ÊçÆÈõÜ„ÄÇËØ∑ÂèÇÈòÖ [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) Âíå [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/mai\n\n\n### Supervised Fine-tuning\n\nOpenRLHF ÁöÑÊ®°ÂûãÊ£ÄÊü•ÁÇπÂÆåÂÖ®ÂÖºÂÆπ HuggingFace Ê®°Âûã„ÄÇÊÇ®ÂèØ‰ª•‰ΩøÁî® `--pretrain  {name or path}`„ÄÅ`--reward_pretrain  {name or path}` Âíå `--critic_pretrain  {name or path}` ÊåáÂÆöÊ®°ÂûãÂêçÁß∞ÊàñË∑ØÂæÑ„ÄÇÊàë‰ª¨Âú® [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF) ‰∏äÊèê‰æõ‰∫Ü‰∏Ä‰∫õÈ¢ÑËÆ≠ÁªÉÁöÑÊ£ÄÊü•ÁÇπÂíåÊï∞ÊçÆÈõÜ„ÄÇ\n\nÁÑ∂ÂêéÊÇ®ÂèØ‰ª•‰ΩøÁî®Êàë‰ª¨Âú® [examples/scripts](./examples/scripts/) ÁõÆÂΩï‰∏≠Êèê‰æõÁöÑÂêØÂä®ËÑöÊú¨ÔºåÊàñËÄÖ‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®ËÆ≠ÁªÉÔºö\n\n```bash \ndeepspeed --module openrlhf.cli.train_sft \\\n   --max_len 4096 \\\n   --dataset Open-Orca/OpenOrca \\\n   --input_key question \\\n   --output_key response \\\n   --input_template $'User: {}\\nAssistant: ' \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 2 \\\n   --max_samples 500000 \\\n   --pretrain meta-llama/Meta-Llama-3-8B \\\n   --save_path ./checkpoint/llama3-8b-sft \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --zero_stage 2 \\\n   --max_epochs 1 \\\n   --bf16 \\\n   --flash_attn \\\n   --learning_rate 5e-6 \\\n   --gradient_checkpointing \\\n   --packing_samples \\\n   --load_checkpoint \\\n   --use_wandb {wandb_token}\n\n# ÊîØÊåÅ HF tokenizer.apply_chat_template\n# --apply_chat_template \n# --tokenizer_chat_template {HF Chat Template}\n\n# ÊîØÊåÅ RingAttention\n# pip install ring_flash_attn\n#   --ring_attn_size 2 \\\n#   --ring_head_stride 2 \\\n\n# ‰πüÂèØÁî®‰∫é continued pre-training\n# --pretrain_mode\n```\n\n> [!NOTE]\n> OpenRLHF SFT/DPO/RewardModel/PPO ËÆ≠ÁªÉÊîØÊåÅ `--packing_samples` [Âü∫‰∫é `--flash_attn`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)\n\n### Reward Model Training\n```bash\ndeepspeed --module openrlhf.cli.train_rm \\\n   --save_path ./checkpoint/llama3-8b-rm \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 1 \\\n   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n   --bf16 \\\n   --max_epochs 1 \\\n   --max_len 8192 \\\n   --zero_stage 3 \\\n   --learning_rate 9e-6 \\\n   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \\\n   --apply_chat_template \\\n   --chosen_key chosen \\\n   --rejected_key rejected \\\n   --flash_attn \\\n   --packing_samples \\\n   --gradient_checkpointing \\\n   --load_checkpoint \\\n   --use_wandb {wandb_token}\n\n```\n\nÊé®ËçêËÆæÁΩÆ Reward Model ÁöÑ `--value_prefix_head` ÈÄâÈ°π‰∏∫ `score`, ËøôÊ†∑‰ΩøÂæóÊàë‰ª¨ÂèØ‰ª•Áî® `AutoModelForSequenceClassification` Âä†ËΩΩÊ®°Âûã:\n\n```python\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n              reward_model_path,\n              num_labels=1,\n              torch_dtype=torch.bfloat16,\n              attn_implementation=\"flash_attention_2\",\n              use_cache=False,\n          )\ninputs = xxxx (Left Padding Input Tokens)\nreward = reward_model.model(*inputs).last_hidden_state\nreward = reward_model.score(reward)[:, -1]\n```\n\n### ‰∏ç‰ΩøÁî® Ray ÁöÑ PPO\n\n```bash\ndeepspeed --module openrlhf.cli.train_ppo \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path ./checkpoint/llama-3-8b-rlhf \\\n  --save_steps -1 \\\n  --logging_steps 1 \\\n  --eval_steps -1 \\\n  --micro_train_batch_size 2 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 4 \\\n  --rollout_batch_size 1024 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 2 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --max_samples 100000 \\\n  --normalize_reward \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --load_checkpoint \\\n  --use_wandb {wandb_token}\n\n# ÊîØÊåÅËøúÁ®ã reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n```\n\n### ‰ΩøÁî® Ray Âíå vLLM ÁöÑ PPO/REINFORCE++\n\n‰∏∫‰∫ÜÊèêÈ´ò RLHF ËÆ≠ÁªÉÈÄüÂ∫¶ÊàñÊîØÊåÅ 70B Ê®°ÂûãÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî® Ray Âíå vLLM Âä†ÈÄüÁöÑ PPO\n\n```bash\n# Âú®ÂÆπÂô®‰∏≠ÂêØÂä® Ray ÁöÑ‰∏ªËäÇÁÇπ\nray start --head --node-ip-address 0.0.0.0 --num-gpus 8\n\n# Â¶ÇÊûúË¶ÅÂú®Êõ¥Â§öËäÇÁÇπ‰∏äÂêØÂä® RayÔºåËØ∑‰ΩøÁî®\nray start --address {MASTER-NODE-ADDRESS}:6379 --num-gpus 8\n\nray job submit --address=\"http://127.0.0.1:8265\" \\\n  --runtime-env-json='{\"working_dir\": \"/openrlhf\"}' \\\n  -- python3 -m openrlhf.cli.train_ppo_ray \\\n  --ref_num_nodes 1 \\\n  --ref_num_gpus_per_node 2 \\\n  --reward_num_nodes 1 \\\n  --reward_num_gpus_per_node 2 \\\n  --critic_num_nodes 1 \\\n  --critic_num_gpus_per_node 2 \\\n  --actor_num_nodes 1 \\\n  --actor_num_gpus_per_node 2 \\\n  --vllm_num_engines 2 \\\n  --vllm_tensor_parallel_size 2 \\\n  --colocate_critic_reward \\\n  --colocate_actor_ref \\\n  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \\\n  --save_path /openrlhf/examples/checkpoint/llama3-8b-rlhf \\\n  --micro_train_batch_size 8 \\\n  --train_batch_size 128 \\\n  --micro_rollout_batch_size 32 \\\n  --rollout_batch_size 1024 \\\n  --max_samples 100000 \\\n  --max_epochs 1 \\\n  --prompt_max_len 1024 \\\n  --generate_max_len 1024 \\\n  --zero_stage 3 \\\n  --bf16 \\\n  --actor_learning_rate 5e-7 \\\n  --critic_learning_rate 9e-6 \\\n  --init_kl_coef 0.01 \\\n  --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n  --input_key context_messages \\\n  --apply_chat_template \\\n  --normalize_reward \\\n  --packing_samples \\\n  --adam_offload \\\n  --flash_attn \\\n  --gradient_checkpointing \\\n  --load_checkpoint \\\n  --use_wandb {wandb_token}\n\n# ÊîØÊåÅ REINFORCE++  | RLOO \n# --advantage_estimator reinforce | rloo\n\n# ÊîØÊåÅËøúÁ®ã reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n\n# ÊîØÊåÅ N ÂÄçÈááÊ†∑\n# --n_samples_per_prompt 4\n```\n\n> [!NOTE]\n> ‰∏çËÆæÁΩÆ `--vllm_num_engines` ÂàôÊòØ‰∏ç‰ΩøÁî® vLLM engine„ÄÇ\n> ÊÇ®‰πüÂèØ‰ª•ÈÄöËøá ``setup_commands`` ËÆ© Ray Ëá™Âä®ÂàùÂßãÂåñÁéØÂ¢É, ÊØîÂ¶Ç `--runtime-env-json='{\"setup_commands\": [\"pip install openrlhf[vllm]\"]}'`\n\n> [!NOTE]\n> OPENRLHF's RLOO Âü∫‰∫é REINFORCE++ ‰øÆÊîπËÄåÊù•, ÂíåÂéüÁâàÁöÑÂÆûÁé∞‰∏çÂêå.\n\n> [!NOTE]\n> Â¶ÇÊûúÊÇ®Áî±‰∫éÊüêÁßçÂéüÂõ†ÔºåÂú® deepspeed ËÆæÁΩÆÊòæÂç°ËÆæÂ§áÊó∂ÈÅáÂà∞‰∏éÁ¥¢ÂºïË∂ÖÂá∫ËåÉÂõ¥Áõ∏ÂÖ≥ÁöÑÈîôËØØÔºåÊÇ®ÂèØ‰ª•Â∞ùËØïËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py)„ÄÇ\n> ```bash\n> # ÂØπ‰∫é NVIDIA ÊòæÂç°:\n> export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1\n> ```\n\nÊâÄÊúâÊîØÊåÅÁÆóÊ≥ïÁöÑÂêØÂä®ËÑöÊú¨ÂíåÊñáÊ°£Âú® [example/scripts](./examples/scripts/) Âíå [Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)\n\n\n### ‰ΩøÁî® LoRA\nÂ¶ÇÊûúÊÇ®‰ΩøÁî®‰∫Ü `LoRA (Low-Rank Adaptation)`ÔºåÈªòËÆ§‰øùÂ≠ò‰∏ãÊù•ÁöÑÊñá‰ª∂**Âπ∂Èùû**ÂÆåÊï¥Ê®°ÂûãÊùÉÈáçÔºåËÄåÊòØ `LoRA Adapter`ÔºåËã•ÊÉ≥ÊåâÂÆåÊï¥ÊùÉÈáçÁöÑÊñπÂºèËøõË°åÂêéÁª≠‰ªªÂä°ÔºåÊÇ®ÈúÄË¶ÅÂ∞Ü `Adapter` ‰∏éËÆ≠ÁªÉÂâçÁöÑÊ®°ÂûãÊùÉÈáçËøõË°åÂêàÂπ∂\n\n```bash\npython -m openrlhf.cli.lora_combiner \\\n    --model_path meta-llama/Meta-Llama-3-8B \\\n    --lora_path ./checkpoint/llama3-8b-rm \\\n    --output_path ./checkpoint/llama-3-8b-rm-combined \\\n    --is_rm \\\n    --bf16\n```\n\n## ÊÄßËÉΩ\nÊàë‰ª¨ÈÄöËøáÂêØÁî®AdamÂç∏ËΩΩ„ÄÅÂ•ñÂä±Ê®°Âûã(RM)ÂíåÂèÇËÄÉÊ®°Âûã(Ref)Âç∏ËΩΩÁ≠âÊäÄÊúØ,Â∞ΩÂèØËÉΩ‰ºòÂåñ‰∫ÜDSChatÁöÑÊÄßËÉΩ,‰ªéËÄåÂú®Êé®ÁêÜÈò∂ÊÆµÂ¢ûÂä†Â∞èÊâπÈáèÂ§ßÂ∞èÂπ∂ÈÅøÂÖçÂÜÖÂ≠ò‰∏çË∂≥ÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁîöËá≥‰øÆÂ§ç‰∫ÜDSChat‰∏≠ÁöÑ‰∏Ä‰∫õbug,‰ª•ÂêØÁî®LLaMA2ÁöÑÊ∑∑ÂêàÂºïÊìé(HE)„ÄÇ‰ΩøÁî®‰ºòÂåñÂêéÁöÑDSChatÂíåOpenRLHFËÆ≠ÁªÉ1024‰∏™ÊèêÁ§∫ÈúÄË¶Å1‰∏™PPOËΩÆÊ¨°ÁöÑÂπ≥ÂùáÊó∂Èó¥(Áßí)Â¶Ç‰∏ã:\n\n| **Size** | **NVIDIA A800 GPUs** | **Optimized DSChat (with  Hybrid Engine)** | **OpenRLHF** | **Speedup** |\n| :---: | :---: | :---: | :---: | :---: |\n| 7B | 16 | 855.09 | 471.11 | 1.82x |\n| 13B | 32 | 1528.93 | 608.93 | 2.5x |\n| 34B | 32 | 3634.98 | 1526.4 | 2.4x |\n| 70B | 32 | 10407.0 | 4488.53 | 2.3x |\n\n\n> [!NOTE]\n> Êï∞ÊçÆÂ∑≤ÁªèËøáÊó∂; ËØ∑ÂèÇËÄÉÂêéÈù¢ÁöÑË∞É‰ºòÊåáÂçóÈáçÊñ∞ÊµãËØï\n\n## Ë∞É‰ºòÊåáÂçó\n‰∏∫‰∫ÜËé∑ÂæóÊúÄ‰Ω≥ÁöÑÊÄßËÉΩÔºåÊàë‰ª¨Âª∫ËÆÆÊÇ®ÂàÜÈÖçÊõ¥Â§öÁöÑËäÇÁÇπÁªô vLLM Engine„ÄÇ‰æãÂ¶ÇÔºåÂØπ‰∫é 70B Ê®°Âûã‰ª•Âèä 32 Âº† A100ÔºåÂª∫ËÆÆÂàÜÈÖç 16 Âº†‰ª•‰∏ä A100 Áªô vLLM EngineÔºå8 Âº†Áªô Actor Ê®°ÂûãÔºå‰ª•ÂèäÊúÄÂêé 8 Âº†Áªô Critic Ê®°ÂûãÔºåÂêåÊó∂ÂºÄÂêØ `--colocate_critic_reward`, `--colocate_actor_ref` ÊàñËÄÖ `--ref_reward_offload (ÂèØÈÄâ)` ÈÄâÈ°πÂêàÂπ∂ÈÉ®ÂàÜËäÇÁÇπ„ÄÇÊúÄÂêéÊÇ®Â∫îËØ•Â∞ΩÂèØËÉΩÂ¢ûÂ§ß `--rollout_micro_batch_size` Ôºå‰ª•ÂèäÂáèÂ∞è vLLM ÁöÑ TP ÂàáÂàÜÊï∞Èáè„ÄÇËÆ≠ÁªÉÈò∂ÊÆµÁöÑ `micro_train_batch_size` ‰πüÊòØË∂äÂ§ßË∂äÂ•ΩÔºåËØ∑ÂêåÊó∂‰ΩøÁî® `--packing_samples` „ÄÇÂΩì GPU Êï∞ÈáèË∂≥Â§üÊó∂ËØ∑ÂÖ≥Èó≠ `--adam_offload` ‰ª•ÂèäÂêØÁî® `--overlap_comm`. ÂØπ‰∫éÂ§öËäÇÁÇπ RLHF, ËØ∑‰ΩøÁî® `--vllm_sync_backend nccl` with vLLM 0.6.4+.\n\n## ‰ΩøÁî® OpenRLHF ÁöÑÂÖ¨Âè∏ÂíåÁªÑÁªá\n\n- Google\n- ByteDance\n- Tencent\n- Alibaba\n- Baidu\n- China Telecom\n- Allen AI\n- Vivo\n- NexusFlow\n- J√ºlich Supercomputing Centre (JSC)\n- Berkeley Starling Team\n- M-A-P\n- ...\n\n\n## Âä†ÂÖ•Êàë‰ª¨\n\n**Â¶Ç‰ΩïÂä†ÂÖ•Ôºü**\n\n1. ÈÄöËøáËÅîÁ≥ªÈÇÆÁÆ± janhu9527@gmail.com ÊàñËÄÖÂä†ÂÖ• [GitHub Organization](https://github.com/OpenRLHF)„ÄÇËØ∑ÂåÖÂê´‰ª•‰∏ã‰ø°ÊÅØÔºö\n   - ÊÇ®ÁöÑÂßìÂêç\n   - ÊÇ®ÁöÑ GitHub Áî®Êà∑Âêç\n   - ÊÇ®ÊÑüÂÖ¥Ë∂£ÁöÑÈ¢ÜÂüü\n   - ÊÇ®Âú® NLP Âíå/Êàñ AI Áõ∏ÂÖ≥ÁöÑÊäÄËÉΩÂíåÁªèÈ™å\n2. ÊÇ®‰πüÂèØ‰ª•ÈÄöËøáÂÆòÊñπ GitHub [OpenRLHF ‚Üó](https://github.com/OpenRLHF/OpenRLHF) È°πÁõÆÈ°µÈù¢Âä†ÂÖ•Êàë‰ª¨„ÄÇÂè™ÈúÄÂàõÂª∫‰∏Ä‰∏™ÂÖ≥‰∫éÊÇ®ÊÉ≥Ë¶ÅË¥°ÁåÆÁöÑÂÖ¥Ë∂£ÁöÑ issueÔºåÊàë‰ª¨‰ºö‰∏éÊÇ®ËÅîÁ≥ª„ÄÇ\n\n**ÊÇ®ËÉΩÂÅö‰ªÄ‰πàÔºü**\n\n1. Âä†ÂÖ•Âõ¢ÈòüÔºåÂèÇ‰∏é OpenRLHF È°πÁõÆÁöÑÂºÄÂèë„ÄÇ\n2. ÈÄöËøáÊèê‰∫§ pull ËØ∑Ê±ÇÊù•‰∏∫È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆ„ÄÇ\n3. Â∏ÆÂä©ÊîπËøõÊñáÊ°£Ôºå‰øÆÂ§ç bugs ÊàñÂàõÂª∫Êñ∞ÂäüËÉΩ„ÄÇ\n4. ÂàÜ‰∫´È°πÁõÆÂπ∂Â∏ÆÂä©Êàë‰ª¨ÂèëÂ±ïÁ§æÂå∫„ÄÇ\n\n## ËµûÂä©Êàë‰ª¨\n\nÊÇ®ÁöÑËµûÂä©ÂèØ‰ª•Â∏ÆÂä©Êàë‰ª¨Áª¥Êä§ÂíåÊîπËøõ OpenRLHF„ÄÇÂ¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÊúâÁî®ÔºåËØ∑ËÄÉËôëËµûÂä©Êàë‰ª¨„ÄÇÊÇ®ÂèØ‰ª•Âú® [Open Collective ‚Üó](https://opencollective.com/OpenRLHF) ‰∏äËµûÂä©Êàë‰ª¨„ÄÇ\n\n## ÊòüÂõæ\n\n[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)\n\n## Ë¥°ÁåÆËÄÖ\n\nÈùûÂ∏∏ÊÑüË∞¢ÊâÄÊúâÁöÑË¥°ÁåÆËÄÖÔºÅÂ¶ÇÊûúÊÇ®ÊÉ≥Ë¥°ÁåÆÔºåËØ∑ÈöèÊó∂ÂàõÂª∫ pull ËØ∑Ê±ÇÊàñÂàõÂª∫ issue„ÄÇ\n\n<a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF\" />\n</a>\n\n## ÂºïÁî®‰∏éËá¥Ë∞¢\n\nÊàë‰ª¨ÊÉ≥Ë¶ÅÂØπ‰ª•‰∏ãÈ°πÁõÆÂíåÁªÑÁªáÂú® AI Âíå NLP È¢ÜÂüüÁöÑË¥°ÁåÆË°®Á§∫ÊÑüË∞¢Ôºö\n\n- [Hugging Face Transformers ‚Üó](https://github.com/huggingface/transformers)\n- [OpenAI GPT ‚Üó](https://github.com/openai/gpt-3)\n- [LLaMA ‚Üó](https://llama.meta.com/)\n- [DeepSpeed ‚Üó](https://github.com/microsoft/DeepSpeed)\n- [Ray ‚Üó](https://github.com/ray-project/ray)\n\nÊàë‰ª¨ÁöÑÈ°πÁõÆËøòÊÉ≥Ë¶ÅÊÑüË∞¢ [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) Âíå [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)„ÄÇÂú®È°πÁõÆÁöÑÊó©ÊúüÈò∂ÊÆµÔºåÊàë‰ª¨ÂèÇËÄÉ‰∫Ü‰ªñ‰ª¨ÁöÑ‰ª£Á†ÅËÆæËÆ°„ÄÇ\n\n(2024/7) Êàë‰ª¨ÁöÑ GitHub ÁªÑÁªá‰ªé OpenLLMAI ËøÅÁßªÂà∞‰∫Ü OpenRLHF.\n\n## ÂºïÁî®\n```\n@article{hu2024openrlhf,\n  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},\n  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},\n  journal={arXiv preprint arXiv:2405.11143},\n  year={2024}\n}\n```\n\n\n______________________________________________________________________\n\n*OpenRLHF ¬© 2025 OpenRLHF. ÁâàÊùÉÊâÄÊúâ„ÄÇ*\n"
        },
        {
          "name": "dockerfile",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "openrlhf",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.09765625,
          "content": "[build-system]\nrequires = [\n    \"packaging\",\n    \"setuptools >= 49.4.0\",\n    \"wheel\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.isort]\nprofile = \"black\"  # black-compatible\nline_length = 119  # should match black parameters\nignore_whitespace = true  # ignore whitespace for compatibility with the initial style\npy_version = 310  # python 3.10 as a target version\nsections = [\"FUTURE\", \"STDLIB\", \"THIRDPARTY\", \"FIRSTPARTY\", \"LOCALFOLDER\"]\ndefault_section = \"THIRDPARTY\"\nextend_skip = [\"setup.py\", \"docs/source/conf.py\"]\n\n\n[tool.black]\nline_length = 119\n\n[tool.ruff]\nline-length = 119\n\n[tool.pytest.ini_options]\n# durations=0 will display all tests execution time, sorted in ascending order starting from from the slowest one.\n# -vv will also display tests with durration = 0.00s\naddopts = \"--verbose --pyargs --durations=0 --strict-markers\"  # always add these arguments to pytest\ntestpaths = [\"./tests\"]  # must be an explicit path to avoid importing another \"tests\" module\n# directories to ignore when discovering tests\nnorecursedirs = [\n    \"external\",\n    \"examples\",\n    \"docs\",\n    \"scripts\",\n    \"tools\",\n    \"tutorials\",\n    \"*.egg\",\n    \".*\",\n    \"_darcs\",\n    \"build\",\n    \"CVS\",\n    \"dist\",\n    \"venv\",\n    \"{arch}\",\n]\n# markers to select tests, use `pytest --markers` to see all available markers, `pytest -m \"<marker>\"` to select tests\nmarkers = [\n    \"unit: marks unit test, i.e. testing a single, well isolated functionality (deselect with '-m \\\"not unit\\\"')\",\n    \"integration: marks test checking the elements when integrated into subsystems (deselect with '-m \\\"not integration\\\"')\",\n    \"system: marks test working at the highest integration level (deselect with '-m \\\"not system\\\"')\",\n    \"acceptance: marks test checking whether the developed product/model passes the user defined acceptance criteria (deselect with '-m \\\"not acceptance\\\"')\",\n    \"docs: mark tests related to documentation (deselect with '-m \\\"not docs\\\"')\",\n    \"skipduringci: marks tests that are skipped ci as they are addressed by Jenkins jobs but should be run to test user setups\",\n    \"pleasefixme: marks tests that are broken and need fixing\",\n]\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2431640625,
          "content": "accelerate\nbitsandbytes\ndatasets\ndeepspeed==0.15.0\neinops\nflash-attn==2.7.0.post2\nisort\njsonlines\nloralib\noptimum\npackaging\npeft\nray[default]==2.12.0\ntensorboard\ntorch\ntorchmetrics\ntqdm\ntransformers==4.46.3\ntransformers_stream_generator\nwandb\nwheel\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.2607421875,
          "content": "import os\nimport sys\nimport platform\n\nfrom datetime import datetime\nfrom setuptools import find_packages, setup\nfrom wheel.bdist_wheel import bdist_wheel as _bdist_wheel\n\n_build_mode = os.getenv(\"OPENRLHF_BUILD_MODE\", \"\")\n\n\ndef _is_nightly():\n    return _build_mode.lower() == \"nightly\"\n\n\ndef _fetch_requirements(path):\n    with open(path, \"r\") as fd:\n        return [r.strip() for r in fd.readlines()]\n\n\ndef _fetch_readme():\n    with open(\"README.md\", encoding=\"utf-8\") as f:\n        return f.read()\n\n\ndef _fetch_version():\n    with open(\"version.txt\", \"r\") as f:\n        version = f.read().strip()\n\n    if _is_nightly():\n        now = datetime.now()\n        date_str = now.strftime(\"%Y%m%d\")\n        version += f\".dev{date_str}\"\n\n    return version\n\n\ndef _fetch_package_name():\n    return \"openrlhf-nightly\" if _is_nightly() else \"openrlhf\"\n\n\n# Custom wheel class to modify the wheel name\nclass bdist_wheel(_bdist_wheel):\n    def finalize_options(self):\n        _bdist_wheel.finalize_options(self)\n        self.root_is_pure = False\n\n    def get_tag(self):\n        python_version = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n        abi_tag = f\"{python_version}\"\n\n        if platform.system() == \"Linux\":\n            platform_tag = \"manylinux1_x86_64\"\n        else:\n            platform_tag = platform.system().lower()\n\n        return python_version, abi_tag, platform_tag\n\n\n# Setup configuration\nsetup(\n    author=\"OpenRLHF Team\",\n    name=_fetch_package_name(),\n    version=_fetch_version(),\n    packages=find_packages(\n        exclude=(\n            \"data\",\n            \"docs\",\n            \"examples\",\n        )\n    ),\n    description=\"A Ray-based High-performance RLHF framework.\",\n    long_description=_fetch_readme(),\n    long_description_content_type=\"text/markdown\",\n    install_requires=_fetch_requirements(\"requirements.txt\"),\n    extras_require={\n        \"vllm\": [\"vllm==0.6.5\"],\n        \"vllm_latest\": [\"vllm>0.6.5\"],\n    },\n    python_requires=\">=3.10\",\n    classifiers=[\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Environment :: GPU :: NVIDIA CUDA\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: System :: Distributed Computing\",\n    ],\n    cmdclass={\"bdist_wheel\": bdist_wheel},\n)\n"
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.0048828125,
          "content": "0.5.6"
        }
      ]
    }
  ]
}