{
  "metadata": {
    "timestamp": 1736559501205,
    "page": 79,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "senguptaumd/Background-Matting",
      "stars": 4789,
      "defaultBranch": "master",
      "files": [
        {
          "name": "Data_adobe",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.2041015625,
          "content": "# Background Matting: The World is Your Green Screen\n![alt text](https://homes.cs.washington.edu/~soumya91/paper_thumbnails/matting.png)\n\nBy Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steve Seitz, and Ira Kemelmacher-Shlizerman\n\nThis paper will be presented in IEEE CVPR 2020.\n\n## [**Project Page**](http://grail.cs.washington.edu/projects/background-matting/)\n\nGo to Project page for additional details and results.\n\n## [Paper (Arxiv)](https://arxiv.org/abs/2004.00626)\n\n## [Blog Post](https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635?source=friends_link&sk=03e1a2de548367b22139568a7c798180&gi=85b436f7c556)\n\n## [**Background Matting v2.0**](http://grail.cs.washington.edu/projects/background-matting-v2/)\nWe recently released a brand new background matting project: better quality and REAL-TIME performance (30fps at 4K and 60fps at FHD)!\nYou can now use this with Zoom! Much better quality! We tried this on a Linux machine with a GPU.\n\n[Check out the code!](https://github.com/PeterL1n/BackgroundMattingV2)\n\n\n## Project members ##\n\n* [Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/), University of Washington\n* [Vivek Jayaram](http://www.vivekjayaram.com/research), University of Washington\n* [Brian Curless](https://homes.cs.washington.edu/~curless/), University of Washington\n* [Steve Seitz](https://homes.cs.washington.edu/~seitz/), University of Washington\n* [Ira Kemelmacher-Shlizerman](https://homes.cs.washington.edu/~kemelmi/), University of Washington\n\nAcknowledgement: [Andrey Ryabtsev](http://www.andreyryabtsev.com/#projects), University of Washington\n\n### License ###\nThis work is licensed under the [Creative Commons Attribution NonCommercial ShareAlike 4.0 License](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n## Summary ##\n- [Updates](#updates) \n- [Getting Started](#getting-started)\n- [Inference Code on images](#run-the-inference-code-on-sample-images)\n- [Inference Code on videos](#run-the-inference-code-on-sample-videos)\n- [Notes on capturing images](#notes-on-capturing-images)\n- [Training code on synthetic-composite Adobe dataset](#training-on-synthetic-composite-adobe-dataset)\n- [Training code on unlabeled real videos](#training-on-unlabeled-real-videos)\n- [Captured Data](#dataset)\n- [Inference in Google Colab](#google-colab)\n- [Citations](#citation)\n- [Related Implementations](#related-implementations)\n\n## **Updates** ##\nApril 21, 2020:\n- New features:\n\t- Training code for [supervised training on synthetic-composite Adobe dataset](#training-on-synthetic-composite-adobe-dataset) and [self-supervised learning on unlabeled real videos](#training-on-unlabeled-real-videos).\n\nApril 20,2020\n- New features:\n\t- [Google Colab for inference](https://gist.github.com/andreyryabtsev/243aa3eefa6e06891dda7b1583d1d08f), thanks to Andrey Ryabtsev, University of Washington.\n\t- [Captured data released for research purposes.](https://drive.google.com/open?id=1j3BMrRFhFpfzJAe6P2WDtfanoeSCLPiq)\n\nApril 9, 2020\n- Issues:\n\t- Updated alignment function in pre-processing code. Python version uses AKAZE features (SIFT and SURF is not available with opencv3), MATLAB version also provided uses SURF features.\n- New features:\n\t- [Testing code to replace background for videos](#run-the-inference-code-on-sample-videos)\n\nApril 8, 2020\n- Issues:\n\t- Turning off adjustExposure() for bias-gain correction in test_pre_processing.py. (Bug found, need to be fixed)\n\t- Incorporating 'uncropping' operation in test_background-matting_image.py. (Output will be of same resolution and aspect-ratio as input)\n\n\n\n## Getting Started \n\nClone repository: \n```\ngit clone https://github.com/senguptaumd/Background-Matting.git\n```\n\nPlease use Python 3. Create an [Anaconda](https://www.anaconda.com/distribution/) environment and install the dependencies. Our code is tested with Pytorch=1.1.0, Tensorflow=1.14 with cuda10.0\n\n```\nconda create --name back-matting python=3.6\nconda activate back-matting\n```\nMake sure CUDA 10.0 is your default cuda. If your CUDA 10.0 is installed in `/usr/local/cuda-10.0`, apply\n```\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64\nexport PATH=$PATH:/usr/local/cuda-10.0/bin\n``` \nInstall PyTorch, Tensorflow (needed for segmentation) and dependencies\n```\nconda install pytorch=1.1.0 torchvision cudatoolkit=10.0 -c pytorch\npip install tensorflow-gpu==1.14.0\npip install -r requirements.txt\n\n```\n\nNote: The code is likely to work on other PyTorch and Tensorflow versions compatible with your system CUDA. If you already have a working environment with PyTorch and Tensorflow, only install dependencies with `pip install -r requirements.txt`. If our code fails due to different versions, then you need to install specific CUDA, PyTorch and Tensorflow versions.\n\n## Run the inference code on sample images\n\n### Data\n\nTo perform Background Matting based green-screening, you need to capture:\n- (a) Image with the subject (use `_img.png` extension)\n- (b) Image of the background without the subject (use `_back.png` extension)\n- (c) Target background to insert the subject (place in `data/background`)\n\nUse `sample_data/` folder for testing and prepare your own data based on that. This data was collected with a hand-held camera.\n\n### Pre-trained model\n\nPlease download the pre-trained models from [Google Drive](https://drive.google.com/drive/folders/1WLDBC_Q-cA72QC8bB-Rdj53UB2vSPnXv?usp=sharing) and place `Models/` folder inside `Background-Matting/`.\n\nNote: `syn-comp-adobe-trainset` model was trained on the training set of the Adobe dataset. This was the model used for numerical evaluation on Adobe dataset.\n\n\n### Pre-processing\n\n1. Segmentation\n\nBackground Matting needs a segmentation mask for the subject. We use tensorflow version of [Deeplabv3+](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\n```\ncd Background-Matting/\ngit clone https://github.com/tensorflow/models.git\ncd models/research/\nexport PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\ncd ../..\npython test_segmentation_deeplab.py -i sample_data/input\n```\n\nYou can replace Deeplabv3+ with any segmentation network of your choice. Save the segmentation results with extension `_masksDL.png`.\n\n2. Alignment\n\nSkip this step, if your data is captured with fixed-camera.\n\n- For hand-held camera, we need to align the background with the input image as a part of pre-processing. We apply simple hoomography based alignment.\n- We ask users to **disable the auto-focus and auto-exposure** of the camera while capturing the pair of images. This can be easily done in iPhone cameras (tap and hold for a while).\n\nRun `python test_pre_process.py -i sample_data/input` for pre-processing. It aligns the background image `_back.png` and changes its bias-gain to match the input image `_img.png`\n\nWe used AKAZE features python code (since SURF and SIFT unavilable in opencv3) for alignment. We also provide an alternate MATLAB code (`test_pre_process.m`), which uses SURF features. MATLAB code also provides a way to visualize feature matching and alignment. Bad alignment will produce bad matting output.\nBias-gain adjustment is turned off in the Python code due to a bug, but it is present in MATLAB code. If there are significant exposure changes between the captured image and the captured background, use bias-gain adjustment to account for that.\n\nFeel free to write your own alignment code, choose your favorite feature detector, feature matching and alignment.\n\n### Background Matting\n\n```bash\npython test_background-matting_image.py -m real-hand-held -i sample_data/input/ -o sample_data/output/ -tb sample_data/background/0001.png\n```\nFor images taken with fixed camera (with a tripod), choose `-m real-fixed-cam` for best results. `-m syn-comp-adobe` lets you use the model trained on synthetic-composite Adobe dataset, without real data (worse performance).\n\n## Run the inference code on sample videos\n\nThis is almost exactly similar as that of the image with few small changes.\n\n### Data\n\nTo perform Background Matting based green-screening, you need to capture:\n- (a) Video with the subject (`teaser.mov`)\n- (b) Image of the background without the subject (use `teaser_back.png` extension)\n- (c) Target background to insert the subject (place in `target_back.mov`)\n\nWe provide `sample_video/` captured with hand-held camera and `sample_video_fixed/` captured with fixed camera for testing. Please [download the data](https://drive.google.com/open?id=1C_fLlL_WUP7A_ZcdKxbYVcF_T1uy1SRK) and place both folders under `Background-Matting`. Prepare your own data based on that.\n\n### Pre-processing\n\n1. Frame extraction:\n\n```\ncd Background-Matting/sample_video\nmkdir input background\nffmpeg -i teaser.mov input/%04d_img.png -hide_banner\nffmpeg -i target_back.mov background/%04d.png -hide_banner\n```\n\nRepeat the same for `sample_video_fixed`\n\n2. Segmentation\n```\ncd Background-Matting/models/research/\nexport PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\ncd ../..\npython test_segmentation_deeplab.py -i sample_video/input\n```\n\nRepeat the same for `sample_video_fixed`\n\n3. Alignment\n\nNo need to run alignment for `sample_video_fixed` or videos captured with fixed-camera.\n\nRun `python test_pre_process_video.py -i sample_video/input -v_name sample_video/teaser_back.png` for pre-processing. Alternately you can also use `test_pre_process_video.m` in MATLAB.\n\n### Background Matting\n\nFor hand-held videos, like `sample_video`:\n\n```bash\npython test_background-matting_image.py -m real-hand-held -i sample_video/input/ -o sample_video/output/ -tb sample_video/background/\n```\n\nFor fixed-camera videos, like `sample_video_fixed`:\n\n```bash\npython test_background-matting_image.py -m real-fixed-cam -i sample_video_fixed/input/ -o sample_video_fixed/output/ -tb sample_video_fixed/background/ -b sample_video_fixed/teaser_back.png\n```\n\nTo obtain the video from the output frames, run:\n```\ncd Background-Matting/sample_video\nffmpeg -r 60 -f image2 -i output/%04d_matte.png -vcodec libx264 -crf 15 -s 1280x720 -pix_fmt yuv420p teaser_matte.mp4\nffmpeg -r 60 -f image2 -i output/%04d_compose.png -vcodec libx264 -crf 15 -s 1280x720 -pix_fmt yuv420p teaser_compose.mp4\n```\n\nRepeat same for `sample_video_fixed`\n\n## Notes on capturing images\n\nFor best results capture images following these guidelines:\n- Choose a background that is mostly static, can be both indoor and outdoor.\n- Avoid casting any shadows of the subject on the background.\n\t- place the subject atleast few feets away from the background.\n\t- if possible adjust the lighting to avoid strong shadows on the background.\n- Avoid large color coincidences between subject and background. (e.g. Do not wear a white shirt in front of a white wall background.)\n- Lock AE/AF (Auto-exposure and Auto-focus) of the camera.\n- For hand-held capture, you need to:\n\t- allow only small camera motion by continuing to holding the camera as the subject exists the scene.\n\t- avoid backgrounds that has two perpendicular planes (homography based alignment will fail) or use a background very far away.\n\t- The above restirctions do not apply for images captured with fixed camera (on a tripod)\n\n\t \n\n## Training on synthetic-composite Adobe dataset ##\n\n### Data ###\n\n- Download original Adobe matting dataset: [Follow instructions.](https://sites.google.com/view/deepimagematting)\n- Separate human images: Use `test_data_list.txt` and `train_data_list.txt` in `Data_adobe` to copy only human subjects from Adobe dataset. Create folders `fg_train`, `fg_test`, `mask_train`, `mask_test` to copy foreground and alpha matte for test and train data separately. (The train test split is same as the original dataset.) You can run the following to accomplish this:\n```bash\ncd Data_adobe\n./prepare.sh /path/to/adobe/Combined_Dataset\n```\n- Download background images: Download MS-COCO images and place it in [`bg_train`](http://images.cocodataset.org/zips/test2017.zip) and in [`bg_test`](http://images.cocodataset.org/zips/val2017.zip).\n- Compose Adobe foregrounds onto COCO for the train and test sets. This saves the composed result as `_comp` and the background as `_back` under `merged_train` and `merged_test`. It will also create a CSV to be used by the training dataloader. You can pass `--workers 8` to use e.g. 8 threads, though it will use only one by default.\n```bash\npython compose.py --fg_path fg_train --mask_path mask_train --bg_path bg_train --out_path merged_train --out_csv Adobe_train_data.csv\npython compose.py --fg_path fg_test --mask_path mask_test --bg_path bg_test --out_path merged_test\n```\n\n\n### Training ###\n\nChange number of GPU and required batch-size, depending on your platform. We trained the model with 512x512 input (`-res` flag).\n\n```bash\nCUDA_VISIBLE_DEVICES=0,1 python train_adobe.py -n Adobe_train -bs 4 -res 512\n```\n\nNotes:\n- 512x512 is the maximum input resolution we recommend for training\n- If you decreasing training resolution to 256x256, change `-res 256`, but we also recommend using lesser residual blocks. Use: `-n_blocks1 5 -n_blocks2 2`.\n\nCheers to the [unofficial Deep Image Matting repo.](https://github.com/foamliu/Deep-Image-Matting-PyTorch)\n\n## Training on unlabeled real videos ##\n\n### Data ###\n\n[Please download our captured videos.](https://drive.google.com/drive/folders/1j3BMrRFhFpfzJAe6P2WDtfanoeSCLPiq?usp=sharing). We will show next how to finetune your model on `fixed-camera` captured videos. It will be similar for `hand-held` cameras, except you will need to align the captured background image to each frame of the video separately. (Take a hint from `test_pre_process.py` and use `alignImages()`.) \n\nData Pre-processing: \n- Extract frames for each video: `ffmpeg -i $NAME.mp4 $NAME/%04d_img.png -hide_banner`\n- Run Segmentation (follow instructions on Deeplabv3+) : `python test_segmentation_deeplab.py -i $NAME`\n- Target background for composition. For self-supervised learning we need some target backgrounds that has roughly similar lighting as the original videos. Either capture few videos of indoor/outdoor scenes without humans or use our captured background in the `background` folder.\n- Create a .csv file `Video_data_train.csv` with each row as: `$image;$captured_back;$segmentation;$image+20frames;$image+2*20frames;$image+3*20frames;$image+4*20frames;$target_back`.\nThe process is automated by `prepare_real.py` -- take a look inside and change `background_path` and `path` before running.\n\n### Training ###\n\nChange number of GPU and required batch-size, depending on your platform. We trained the model with 512x512 input (`-res` flag).\n\n```bash\nCUDA_VISIBLE_DEVICES=0,1 python train_real_fixed.py -n Real_fixed -bs 4 -res 512 -init_model Models/syn-comp-adobe-trainset/net_epoch_64.pth\n```\n\n\n\n## Dataset\n\nWe captured videos with both fixed and hand-held camera in indoor and outdoor settings. We release this data to encourage future research on improving background matting. The data is released for research purposes only.\n\n[Download data](https://drive.google.com/open?id=1j3BMrRFhFpfzJAe6P2WDtfanoeSCLPiq)\n\n## Google Colab\n\nThanks to Andrey Ryabstev for creating Google Colab version for easy inference on images and videos of your choice.\n\n[Google Colab](https://gist.github.com/andreyryabtsev/243aa3eefa6e06891dda7b1583d1d08f)\n\n## Notes\n\nWe are eager to hear how our algorithm works on your images/videos. If the algorithm fails on your data, please feel free to share it with us at soumya91@cs.washington.edu. This will help us in improving our algorithm for future research. Also, feel free to share any cool results.\n\n## Citation\nIf you use this code for your research, please consider citing:\n```\n@InProceedings{BMSengupta20,\n  title={Background Matting: The World is Your Green Screen},\n  author = {Soumyadip Sengupta and Vivek Jayaram and Brian Curless and Steve Seitz and Ira Kemelmacher-Shlizerman},\n  booktitle={Computer Vision and Pattern Regognition (CVPR)},\n  year={2020}\n}\n```\n\n## Related Implementations\n[Microsoft Virtual Stage](https://github.com/microsoft/ailab/tree/master/VirtualStage): Using our background matting technology along with depth sensing with Kinect, Microsoft opensourced this amazing code for virtual staging. [Follow this link for details of their technique.](https://www.microsoft.com/en-us/ai/ai-lab-virtual-stage)\n\n[Weights & Biases](https://app.wandb.ai/stacey/greenscreen/reports/Two-Shots-to-Green-Screen%3A-Collage-with-Deep-Learning--VmlldzoxMDc4MjY): Great presentation and detailed discussions and insights on pre-processing and training our model. Check out [Two Minutes Paper's](https://www.youtube.com/watch?v=sTe_-YOccdM) take on our work.\n"
        },
        {
          "name": "data_loader.py",
          "type": "blob",
          "size": 10.1748046875,
          "content": "from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nimport skimage\nfrom skimage import io\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pdb, random\nfrom torch.utils.data import Dataset, DataLoader\nimport random, os, cv2\n\nunknown_code=128\n\nclass VideoData(Dataset):\n\tdef __init__(self,csv_file,data_config,transform=None):\n\t\tself.frames = pd.read_csv(csv_file,sep=';')\n\t\tself.transform = transform\n\t\tself.resolution=data_config['reso']\n\t\t\n\tdef __len__(self):\n\t\treturn len(self.frames)\n\n\tdef __getitem__(self,idx):\n\t\timg = io.imread(self.frames.iloc[idx, 0])\n\t\tback = io.imread(self.frames.iloc[idx, 1])\n\t\tseg = io.imread(self.frames.iloc[idx, 2])\n\t\t\n\t\tfr1 = cv2.cvtColor(io.imread(self.frames.iloc[idx, 3]), cv2.COLOR_BGR2GRAY)\n\t\tfr2 = cv2.cvtColor(io.imread(self.frames.iloc[idx, 4]), cv2.COLOR_BGR2GRAY)\n\t\tfr3 = cv2.cvtColor(io.imread(self.frames.iloc[idx, 5]), cv2.COLOR_BGR2GRAY)\n\t\tfr4 = cv2.cvtColor(io.imread(self.frames.iloc[idx, 6]), cv2.COLOR_BGR2GRAY)\n\n\t\tback_rnd = io.imread(self.frames.iloc[idx, 7])\n\t\t\n\t\tsz=self.resolution\n\n\t\tif np.random.random_sample() > 0.5:\n\t\t\timg = cv2.flip(img,1)\n\t\t\tseg = cv2.flip(seg,1)\n\t\t\tback = cv2.flip(back,1)\n\t\t\tback_rnd = cv2.flip(back_rnd,1)\n\t\t\tfr1=cv2.flip(fr1,1); fr2=cv2.flip(fr2,1); fr3=cv2.flip(fr3,1); fr4=cv2.flip(fr4,1)\n\n\t\t#make frames together\n\t\tmulti_fr=np.zeros((img.shape[0],img.shape[1],4))\n\t\tmulti_fr[...,0]=fr1; multi_fr[...,1]=fr2; multi_fr[...,2]=fr3; multi_fr[...,3]=fr4;\n\t\t\t\n\t\t\n\t\t#allow random cropping centered on the segmentation map\n\t\tbbox=create_bbox(seg,seg.shape[0],seg.shape[1])\n\t\timg=apply_crop(img,bbox,self.resolution)\n\t\tseg=apply_crop(seg,bbox,self.resolution)\n\t\tback=apply_crop(back,bbox,self.resolution)\n\t\tback_rnd=apply_crop(back_rnd,bbox,self.resolution)\n\t\tmulti_fr=apply_crop(multi_fr,bbox,self.resolution)\n\n\t\t#convert seg to guidance map\n\t\t#segg=create_seg_guide(seg,self.resolution)\n\n\t\tsample = {'image': to_tensor(img), 'seg': to_tensor(create_seg_guide(seg,self.resolution)), 'bg': to_tensor(back), 'multi_fr': to_tensor(multi_fr), 'seg-gt':to_tensor(seg), 'back-rnd': to_tensor(back_rnd)}\n\n\t\tif self.transform:\n\t\t\tsample = self.transform(sample)\n\t\treturn sample\n\n\nclass AdobeDataAffineHR(Dataset):\n\tdef __init__(self,csv_file,data_config,transform=None):\n\t\tself.frames = pd.read_csv(csv_file,sep=';')\n\t\tself.transform = transform\n\t\tself.resolution=data_config['reso']\n\t\tself.trimapK=data_config['trimapK']\n\t\tself.noise=data_config['noise']\n\t\t\n\tdef __len__(self):\n\t\treturn len(self.frames)\n\n\tdef __getitem__(self,idx):\n\t\ttry:\n\t\t\t#load\n\t\t\tfg = io.imread(self.frames.iloc[idx, 0])\n\t\t\talpha = io.imread(self.frames.iloc[idx, 1])\n\t\t\timage = io.imread(self.frames.iloc[idx, 2])\n\t\t\tback = io.imread(self.frames.iloc[idx, 3])\n\n\t\t\tfg = cv2.resize(fg, dsize=(800,800))\n\t\t\talpha = cv2.resize(alpha, dsize=(800,800))\n\t\t\tback = cv2.resize(back, dsize=(800,800))\n\t\t\timage = cv2.resize(image, dsize=(800,800))\n\n\n\t\t\tsz=self.resolution\n\n\t\t\t#random flip\n\t\t\tif np.random.random_sample() > 0.5:\n\t\t\t\talpha = cv2.flip(alpha,1)\n\t\t\t\tfg = cv2.flip(fg,1)\n\t\t\t\tback = cv2.flip(back,1)\n\t\t\t\timage = cv2.flip(image,1)\n\n\t\t\ttrimap=generate_trimap(alpha,self.trimapK[0],self.trimapK[1],False)\n\n\n\t\t\t#randcom crop+scale\n\t\t\tdifferent_sizes = [(576,576),(608,608),(640,640),(672,672),(704,704),(736,736),(768,768),(800,800)]\n\t\t\tcrop_size = random.choice(different_sizes)\n\n\t\t\tx, y = random_choice(trimap, crop_size)\n\n\t\t\tfg = safe_crop(fg, x, y, crop_size,sz)\n\t\t\talpha = safe_crop(alpha, x, y, crop_size,sz)\n\t\t\timage = safe_crop(image, x, y, crop_size,sz)\n\t\t\tback = safe_crop(back, x, y, crop_size,sz)\n\t\t\ttrimap = safe_crop(trimap, x, y, crop_size,sz)\n\n\t\t\t#Perturb Background: random noise addition or gamma change\n\t\t\tif self.noise:\n\t\t\t\tif np.random.random_sample() > 0.6:\n\t\t\t\t\tsigma=np.random.randint(low=2, high=6)\n\t\t\t\t\tmu=np.random.randint(low=0, high=14)-7\n\t\t\t\t\tback_tr=add_noise(back,mu,sigma)\n\t\t\t\telse:\n\t\t\t\t\tback_tr=skimage.exposure.adjust_gamma(back,np.random.normal(1,0.12))\n\n\n\t\t\t#Create motion cues: transform foreground and create 4 additional images\n\t\t\taffine_fr=np.zeros((fg.shape[0],fg.shape[1],4))\n\t\t\tfor t in range(0,4):\n\t\t\t\tT=np.random.normal(0,5,(2,1)); theta=np.random.normal(0,7);\n\t\t\t\tR=np.array([[np.cos(np.deg2rad(theta)), -np.sin(np.deg2rad(theta))],[np.sin(np.deg2rad(theta)), np.cos(np.deg2rad(theta))]])\n\t\t\t\tsc=np.array([[1+np.random.normal(0,0.05), 0],[0,1]]); sh=np.array([[1, np.random.normal(0,0.05)*(np.random.random_sample() > 0.5)],[np.random.normal(0,0.05)*(np.random.random_sample() > 0.5), 1]]);\n\t\t\t\tA=np.concatenate((sc*sh*R, T), axis=1);\n\n\t\t\t\tfg_tr = cv2.warpAffine(fg.astype(np.uint8),A,(fg.shape[1],fg.shape[0]),flags=cv2.INTER_LINEAR,borderMode=cv2.BORDER_REFLECT)\n\t\t\t\talpha_tr = cv2.warpAffine(alpha.astype(np.uint8),A,(fg.shape[1],fg.shape[0]),flags=cv2.INTER_NEAREST,borderMode=cv2.BORDER_REFLECT)\n\n\t\t\t\tsigma=np.random.randint(low=2, high=6)\n\t\t\t\tmu=np.random.randint(low=0, high=14)-7\n\t\t\t\tback_tr0=add_noise(back,mu,sigma)\n\n\t\t\t\taffine_fr[...,t]=cv2.cvtColor(composite(fg_tr,back_tr0,alpha_tr), cv2.COLOR_BGR2GRAY)\n\n\n\n\t\t\tsample = {'image': to_tensor(image), 'fg': to_tensor(fg), 'alpha': to_tensor(alpha), 'bg': to_tensor(back), 'trimap': to_tensor(trimap), 'bg_tr': to_tensor(back_tr), 'seg': to_tensor(create_seg(alpha,trimap)), 'multi_fr': to_tensor(affine_fr)}\n\n\n\n\t\t\tif self.transform:\n\t\t\t\tsample = self.transform(sample)\n\t\t\treturn sample\n\t\texcept Exception as e:\n\t\t\tprint(\"Error loading: \" + self.frames.iloc[idx, 0])\n\t\t\tprint(e)\n\n\n#Functions\n\ndef create_seg_guide(rcnn,reso):\n\tkernel_er = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n\tkernel_dil = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n\trcnn=rcnn.astype(np.float32)/255; rcnn[rcnn>0.2]=1;\n\tK=25\n\n\tzero_id=np.nonzero(np.sum(rcnn,axis=1)==0)\n\tdel_id=zero_id[0][zero_id[0]>250]\n\tif len(del_id)>0:\n\t\tdel_id=[del_id[0]-2,del_id[0]-1,*del_id]\n\t\trcnn=np.delete(rcnn,del_id,0)\n\trcnn = cv2.copyMakeBorder( rcnn, 0, K + len(del_id), 0, 0, cv2.BORDER_REPLICATE)\n\n\trcnn = cv2.erode(rcnn, kernel_er, iterations=np.random.randint(10,20))\n\trcnn = cv2.dilate(rcnn, kernel_dil, iterations=np.random.randint(3,7))\n\tk_size_list=[(21,21),(31,31),(41,41)]\n\trcnn=cv2.GaussianBlur(rcnn.astype(np.float32),random.choice(k_size_list),0)\n\trcnn=(255*rcnn).astype(np.uint8)\n\trcnn=np.delete(rcnn, range(reso[0],reso[0]+K), 0)\n\n\treturn rcnn\n\ndef crop_holes(img,cx,cy,crop_size):\n\timg[cy:cy+crop_size[0],cx:cx+crop_size[1]]=0\n\treturn img\n\ndef create_seg(alpha,trimap):\n\t#old\n\tnum_holes=np.random.randint(low=0, high=3)\n\tcrop_size_list=[(15,15),(25,25),(35,35),(45,45)]\n\tkernel_er = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n\tkernel_dil = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n\tseg = (alpha>0.5).astype(np.float32)\n\t#print('Before %.4f max: %.4f' %(seg.sum(),seg.max()))\n\t#old\n\tseg = cv2.erode(seg, kernel_er, iterations=np.random.randint(low=10,high=20))\n\tseg = cv2.dilate(seg, kernel_dil, iterations=np.random.randint(low=15,high=30))\n\t#print('After %.4f max: %.4f' %(seg.sum(),seg.max()))\n\tseg=seg.astype(np.float32)\n\tseg=(255*seg).astype(np.uint8)\n\tfor i in range(num_holes):\n\t\tcrop_size=random.choice(crop_size_list)\n\t\tcx,cy = random_choice(trimap,crop_size)\n\t\tseg=crop_holes(seg,cx,cy,crop_size)\n\t\ttrimap=crop_holes(trimap,cx,cy,crop_size)\n\tk_size_list=[(21,21),(31,31),(41,41)]\n\tseg=cv2.GaussianBlur(seg.astype(np.float32),random.choice(k_size_list),0)\n\treturn seg.astype(np.uint8)\n\n\ndef apply_crop(img,bbox,reso):\n\timg_crop=img[bbox[0]:bbox[0]+bbox[2],bbox[1]:bbox[1]+bbox[3],...]; \n\timg_crop=cv2.resize(img_crop,reso)\n\treturn img_crop\n\ndef create_bbox(mask,R,C):\n\twhere = np.array(np.where(mask))\n\tx1, y1 = np.amin(where, axis=1)\n\tx2, y2 = np.amax(where, axis=1)\n\n\tw=np.maximum(y2-y1,x2-x1);\n\tbd=np.random.uniform(0.1,0.4)\n\tx1=x1-np.round(bd*w)\n\ty1=y1-np.round(bd*w)\n\ty2=y2+np.round(bd*w)\n\t\n\tif x1<0: x1=0\n\tif y1<0: y1=0\n\tif y2>=C: y2=C\n\tif x2>=R: x2=R-1\n\t\n\tbbox=np.around([x1,y1,x2-x1,y2-y1]).astype('int')\n\n\treturn bbox\n\ndef composite(fg, bg, a):\n\tfg = fg.astype(np.float32); bg=bg.astype(np.float32); a=a.astype(np.float32);\n\talpha= np.expand_dims(a / 255,axis=2)\n\tim = alpha * fg + (1 - alpha) * bg\n\tim = im.astype(np.uint8)\n\treturn im\n\ndef add_noise(back,mean,sigma):\n\tback=back.astype(np.float32)\n\trow,col,ch= back.shape\n\tgauss = np.random.normal(mean,sigma,(row,col,ch))\n\tgauss = gauss.reshape(row,col,ch)\n\t#gauss = np.repeat(gauss[:, :, np.newaxis], ch, axis=2)\n\tnoisy = back + gauss\n\n\tnoisy[noisy<0]=0; noisy[noisy>255]=255;\n\n\treturn noisy.astype(np.uint8)\n\ndef safe_crop(mat, x, y, crop_size,img_size,cubic=True):\n\timg_rows, img_cols = img_size\n\tcrop_height, crop_width = crop_size\n\tif len(mat.shape) == 2:\n\t\tret = np.zeros((crop_height, crop_width), np.float32)\n\telse:\n\t\tret = np.zeros((crop_height, crop_width, 3), np.float32)\n\tcrop = mat[y:y + crop_height, x:x + crop_width]\n\th, w = crop.shape[:2]\n\tret[0:h, 0:w] = crop\n\tif crop_size != (img_rows, img_cols):\n\t\tif cubic:\n\t\t\tret = cv2.resize(ret, dsize=(img_rows, img_cols))\n\t\telse:\n\t\t\tret = cv2.resize(ret, dsize=(img_rows, img_cols), interpolation=cv2.INTER_NEAREST)\n\treturn ret\n\ndef generate_trimap(alpha,K1,K2,train_mode):\n\tkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n\tfg = np.array(np.equal(alpha, 255).astype(np.float32))\n\tif train_mode:\n\t\tK=np.random.randint(K1,K2)\n\telse:\n\t\tK=np.round((K1+K2)/2).astype('int')\n\n\tfg = cv2.erode(fg, kernel, iterations=K)\n\tunknown = np.array(np.not_equal(alpha, 0).astype(np.float32))\n\tunknown = cv2.dilate(unknown, kernel, iterations=2*K)\n\ttrimap = fg * 255 + (unknown - fg) * 128\n\treturn trimap.astype(np.uint8)\n\n\ndef random_choice(trimap, crop_size=(320, 320)):\n\timg_height, img_width = trimap.shape[0:2]\n\tcrop_height, crop_width = crop_size\n\n\tval_idx=np.zeros((img_height,img_width))\n\tval_idx[int(crop_height/2):int(img_height-crop_height/2),int(crop_width/2):int(img_width-crop_width/2)]=1\n\n\ty_indices, x_indices = np.where(np.logical_and(trimap == unknown_code,val_idx==1))\n\tnum_unknowns = len(y_indices)\n\tx, y = 0, 0\n\tif num_unknowns > 0:\n\t\tix = np.random.choice(range(num_unknowns))\n\t\tcenter_x = x_indices[ix]\n\t\tcenter_y = y_indices[ix]\n\t\tx = max(0, center_x - int(crop_width / 2))\n\t\ty = max(0, center_y - int(crop_height / 2))\n\n\t#added extra\n\n\treturn x, y\n\ndef to_tensor(pic):\n\tif len(pic.shape)>=3:\n\t\timg = torch.from_numpy(pic.transpose((2, 0, 1)))\n\telse:\n\t\timg=torch.from_numpy(pic)\n\t\timg=img.unsqueeze(0)\n\t# backward compatibility\n\n\n\treturn 2*(img.float().div(255))-1\n"
        },
        {
          "name": "functions.py",
          "type": "blob",
          "size": 2.8203125,
          "content": "import numpy as np\nimport torch\nimport torchvision\nimport cv2, pdb\n\n\ndef composite4(fg, bg, a):\n\tfg = np.array(fg, np.float32)\n\talpha= np.expand_dims(a / 255,axis=2)\n\tim = alpha * fg + (1 - alpha) * bg\n\tim = im.astype(np.uint8)\n\treturn im\n\ndef compose_image_withshift(alpha_pred,fg_pred,bg,seg):\n\n    image_sh=torch.zeros(fg_pred.shape).cuda()\n\n    for t in range(0,fg_pred.shape[0]):\n        al_tmp=to_image(seg[t,...]).squeeze(2)\n        where = np.array(np.where((al_tmp>0.1).astype(np.float32)))\n        x1, y1 = np.amin(where, axis=1)\n        x2, y2 = np.amax(where, axis=1)\n\n        #select shift\n        n=np.random.randint(-(y1-10),al_tmp.shape[1]-y2-10)\n        #n positive indicates shift to right\n        alpha_pred_sh=torch.cat((alpha_pred[t,:,:,-n:],alpha_pred[t,:,:,:-n]),dim=2)\n        fg_pred_sh=torch.cat((fg_pred[t,:,:,-n:],fg_pred[t,:,:,:-n]),dim=2)\n\n        alpha_pred_sh=(alpha_pred_sh+1)/2\n\n        image_sh[t,...]=fg_pred_sh*alpha_pred_sh + (1-alpha_pred_sh)*bg[t,...]\n\n    return torch.autograd.Variable(image_sh.cuda())\n\ndef get_bbox(mask,R,C):\n    where = np.array(np.where(mask))\n    x1, y1 = np.amin(where, axis=1)\n    x2, y2 = np.amax(where, axis=1)\n\n    bbox_init=[x1,y1,np.maximum(x2-x1,y2-y1),np.maximum(x2-x1,y2-y1)]\n\n\n    bbox=create_bbox(bbox_init,(R,C))\n\n    return bbox\n\ndef crop_images(crop_list,reso,bbox):\n\n    for i in range(0,len(crop_list)):\n        img=crop_list[i]\n        if img.ndim>=3:\n            img_crop=img[bbox[0]:bbox[0]+bbox[2],bbox[1]:bbox[1]+bbox[3],...]; img_crop=cv2.resize(img_crop,reso)\n        else:\n            img_crop=img[bbox[0]:bbox[0]+bbox[2],bbox[1]:bbox[1]+bbox[3]]; img_crop=cv2.resize(img_crop,reso)\n        crop_list[i]=img_crop\n\n    return crop_list\n\ndef create_bbox(bbox_init,sh):\n\n    w=np.maximum(bbox_init[2],bbox_init[3])\n\n    x1=bbox_init[0]-0.1*w\n    y1=bbox_init[1]-0.1*w\n\n    x2=bbox_init[0]+1.1*w\n    y2=bbox_init[1]+1.1*w\n\n    if x1<0: x1=0\n    if y1<0: y1=0\n    if x2>=sh[0]: x2=sh[0]-1\n    if y2>=sh[1]: y2=sh[1]-1\n\n    bbox=np.around([x1,y1,x2-x1,y2-y1]).astype('int')\n\n    return bbox\n\ndef uncrop(alpha,bbox,R=720,C=1280):\n\n\n    alpha=cv2.resize(alpha,(bbox[3],bbox[2]))\n\n    if alpha.ndim==2:\n        alpha_uncrop=np.zeros((R,C))\n        alpha_uncrop[bbox[0]:bbox[0]+bbox[2],bbox[1]:bbox[1]+bbox[3]]=alpha\n    else:\n        alpha_uncrop=np.zeros((R,C,3))\n        alpha_uncrop[bbox[0]:bbox[0]+bbox[2],bbox[1]:bbox[1]+bbox[3],:]=alpha\n\n\n    return alpha_uncrop.astype(np.uint8)\n\n\ndef to_image(rec0):\n    rec0=((rec0.data).cpu()).numpy()\n    rec0=(rec0+1)/2\n    rec0=rec0.transpose((1,2,0))\n    rec0[rec0>1]=1\n    rec0[rec0<0]=0\n    return rec0\n\ndef write_tb_log(image,tag,log_writer,i):\n    # image1\n    output_to_show = image.cpu().data[0:4,...]\n    output_to_show = (output_to_show + 1)/2.0\n    grid = torchvision.utils.make_grid(output_to_show,nrow=4)\n\n    log_writer.add_image(tag, grid, i + 1)\n\n"
        },
        {
          "name": "loss_functions.py",
          "type": "blob",
          "size": 2.421875,
          "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n#import matplotlib.pyplot as plt\nimport pdb\nfrom torch.nn.modules.loss import _Loss\nfrom torch.autograd import Function, Variable\n#import scipy.io as sio\n\n\nclass alpha_loss(_Loss):\n\tdef __init__(self):\n\t\tsuper(alpha_loss,self).__init__()\n\n\tdef forward(self,alpha,alpha_pred,mask):\n\t\treturn normalized_l1_loss(alpha,alpha_pred,mask)\n\n\t\t\n\n\nclass compose_loss(_Loss):\n\tdef __init__(self):\n\t\tsuper(compose_loss,self).__init__()\n\n\tdef forward(self,image,alpha_pred,fg,bg,mask):\n\n\t\talpha_pred=(alpha_pred+1)/2\n\n\t\tcomp=fg*alpha_pred + (1-alpha_pred)*bg\n\n\t\treturn normalized_l1_loss(image,comp,mask)\n\nclass alpha_gradient_loss(_Loss):\n\tdef __init__(self):\n\t\tsuper(alpha_gradient_loss,self).__init__()\n\n\tdef forward(self,alpha,alpha_pred,mask):\n\n\t\tfx = torch.Tensor([[1, 0, -1],[2, 0, -2],[1, 0, -1]]); fx=fx.view((1,1,3,3)); fx=Variable(fx.cuda())\n\t\tfy = torch.Tensor([[1, 2, 1],[0, 0, 0],[-1, -2, -1]]); fy=fy.view((1,1,3,3)); fy=Variable(fy.cuda())\n\n\t\tG_x = F.conv2d(alpha,fx,padding=1); G_y = F.conv2d(alpha,fy,padding=1)\n\t\tG_x_pred = F.conv2d(alpha_pred,fx,padding=1); G_y_pred = F.conv2d(alpha_pred,fy,padding=1)\n\n\t\tloss=normalized_l1_loss(G_x,G_x_pred,mask) + normalized_l1_loss(G_y,G_y_pred,mask)\n\n\t\treturn loss\n\nclass alpha_gradient_reg_loss(_Loss):\n\tdef __init__(self):\n\t\tsuper(alpha_gradient_reg_loss,self).__init__()\n\n\tdef forward(self,alpha,mask):\n\n\t\tfx = torch.Tensor([[1, 0, -1],[2, 0, -2],[1, 0, -1]]); fx=fx.view((1,1,3,3)); fx=Variable(fx.cuda())\n\t\tfy = torch.Tensor([[1, 2, 1],[0, 0, 0],[-1, -2, -1]]); fy=fy.view((1,1,3,3)); fy=Variable(fy.cuda())\n\n\t\tG_x = F.conv2d(alpha,fx,padding=1); G_y = F.conv2d(alpha,fy,padding=1)\n\n\t\tloss=(torch.sum(torch.abs(G_x))+torch.sum(torch.abs(G_y)))/torch.sum(mask)\n\n\t\treturn loss\n\n\nclass GANloss(_Loss):\n\tdef __init__(self):\n\t\tsuper(GANloss,self).__init__()\n\n\tdef forward(self,pred,label_type):\n\t\tMSE=nn.MSELoss()\n\n\t\tloss=0\n\t\tfor i in range(0,len(pred)):\n\t\t\tif label_type:\n\t\t\t\tlabels=torch.ones(pred[i][0].shape)\n\t\t\telse:\n\t\t\t\tlabels=torch.zeros(pred[i][0].shape)\n\t\t\tlabels=Variable(labels.cuda())\n\n\t\t\tloss += MSE(pred[i][0],labels)\n\n\t\treturn loss/len(pred)\n\n\n\ndef normalized_l1_loss(alpha,alpha_pred,mask):\n\tloss=0; eps=1e-6;\n\tfor i in range(alpha.shape[0]):\n\t\tif mask[i,...].sum()>0:\n\t\t\tloss = loss + torch.sum(torch.abs(alpha[i,...]*mask[i,...]-alpha_pred[i,...]*mask[i,...]))/(torch.sum(mask[i,...])+eps)\n\tloss=loss/alpha.shape[0]\n\t\n\treturn loss"
        },
        {
          "name": "networks.py",
          "type": "blob",
          "size": 11.8046875,
          "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport numpy as np\n\n\nclass ResnetConditionHR(nn.Module):\n\tdef __init__(self, input_nc, output_nc, ngf=64, nf_part=64,norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks1=7, n_blocks2=3, padding_type='reflect'):\n\t\tassert(n_blocks1 >= 0); assert(n_blocks2 >= 0)\n\t\tsuper(ResnetConditionHR, self).__init__()\n\t\tself.input_nc = input_nc\n\t\tself.output_nc = output_nc\n\t\tself.ngf = ngf\n\t\tuse_bias=True\n\n\t\t#main encoder output 256xW/4xH/4\n\t\tmodel_enc1 = [nn.ReflectionPad2d(3),nn.Conv2d(input_nc[0], ngf, kernel_size=7, padding=0,bias=use_bias),norm_layer(ngf),nn.ReLU(True)]\n\t\tmodel_enc1 += [nn.Conv2d(ngf , ngf * 2, kernel_size=3,stride=2, padding=1, bias=use_bias),norm_layer(ngf * 2),nn.ReLU(True)]\n\t\tmodel_enc2 = [nn.Conv2d(ngf*2 , ngf * 4, kernel_size=3,stride=2, padding=1, bias=use_bias),norm_layer(ngf * 4),nn.ReLU(True)]\n\t\t\n\n\t\t#back encoder output 256xW/4xH/4\n\t\tmodel_enc_back = [nn.ReflectionPad2d(3),nn.Conv2d(input_nc[1], ngf, kernel_size=7, padding=0,bias=use_bias),norm_layer(ngf),nn.ReLU(True)]\n\t\tn_downsampling = 2\n\t\tfor i in range(n_downsampling):\n\t\t\tmult = 2**i\n\t\t\tmodel_enc_back += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,stride=2, padding=1, bias=use_bias),norm_layer(ngf * mult * 2),nn.ReLU(True)]\n\n\t\t#seg encoder output 256xW/4xH/4\n\t\tmodel_enc_seg = [nn.ReflectionPad2d(3),nn.Conv2d(input_nc[2], ngf, kernel_size=7, padding=0,bias=use_bias),norm_layer(ngf),nn.ReLU(True)]\n\t\tn_downsampling = 2\n\t\tfor i in range(n_downsampling):\n\t\t\tmult = 2**i\n\t\t\tmodel_enc_seg += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,stride=2, padding=1, bias=use_bias),norm_layer(ngf * mult * 2),nn.ReLU(True)]\n\n\t\tmult = 2**n_downsampling\n\n\t\t# #motion encoder output 256xW/4xH/4\n\t\tmodel_enc_multi = [nn.ReflectionPad2d(3),nn.Conv2d(input_nc[3], ngf, kernel_size=7, padding=0,bias=use_bias),norm_layer(ngf),nn.ReLU(True)]\n\t\tn_downsampling = 2\n\t\tfor i in range(n_downsampling):\n\t\t\tmult = 2**i\n\t\t\tmodel_enc_multi += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,stride=2, padding=1, bias=use_bias),norm_layer(ngf * mult * 2),nn.ReLU(True)]\n\n\n\t\tself.model_enc1 = nn.Sequential(*model_enc1)\n\t\tself.model_enc2 = nn.Sequential(*model_enc2)\n\t\tself.model_enc_back = nn.Sequential(*model_enc_back)\n\t\tself.model_enc_seg = nn.Sequential(*model_enc_seg)\n\t\tself.model_enc_multi = nn.Sequential(*model_enc_multi)\n\n\t\tmult = 2**n_downsampling\n\t\tself.comb_back=nn.Sequential(nn.Conv2d(ngf * mult*2,nf_part,kernel_size=1,stride=1,padding=0,bias=False),norm_layer(ngf),nn.ReLU(True))\n\t\tself.comb_seg=nn.Sequential(nn.Conv2d(ngf * mult*2,nf_part,kernel_size=1,stride=1,padding=0,bias=False),norm_layer(ngf),nn.ReLU(True))\n\t\tself.comb_multi=nn.Sequential(nn.Conv2d(ngf * mult*2,nf_part,kernel_size=1,stride=1,padding=0,bias=False),norm_layer(ngf),nn.ReLU(True))\n\n\t\t#decoder\n\t\tmodel_res_dec=[nn.Conv2d(ngf * mult +3*nf_part,ngf*mult,kernel_size=1,stride=1,padding=0,bias=False),norm_layer(ngf*mult),nn.ReLU(True)]\n\t\tfor i in range(n_blocks1):\n\t\t\tmodel_res_dec += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n\t\tmodel_res_dec_al=[]\n\t\tfor i in range(n_blocks2):\n\t\t\tmodel_res_dec_al += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n\t\tmodel_res_dec_fg=[]\n\t\tfor i in range(n_blocks2):\n\t\t\tmodel_res_dec_fg += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n\t\tmodel_dec_al=[]\n\t\tfor i in range(n_downsampling):\n\t\t\tmult = 2**(n_downsampling - i)\n\t\t\t#model_dec_al += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),kernel_size=3, stride=2,padding=1, output_padding=1,bias=use_bias),norm_layer(int(ngf * mult / 2)),nn.ReLU(True)]\n\t\t\tmodel_dec_al += [nn.Upsample(scale_factor=2,mode='bilinear',align_corners = True),nn.Conv2d(ngf * mult, int(ngf * mult / 2), 3, stride=1,padding=1),norm_layer(int(ngf * mult / 2)),nn.ReLU(True)]\n\t\tmodel_dec_al += [nn.ReflectionPad2d(3),nn.Conv2d(ngf, 1, kernel_size=7, padding=0),nn.Tanh()]\n\n\n\t\tmodel_dec_fg1=[nn.Upsample(scale_factor=2,mode='bilinear',align_corners = True),nn.Conv2d(ngf * 4, int(ngf * 2), 3, stride=1,padding=1),norm_layer(int(ngf * 2)),nn.ReLU(True)]\n\t\tmodel_dec_fg2=[nn.Upsample(scale_factor=2,mode='bilinear',align_corners = True),nn.Conv2d(ngf * 4, ngf, 3, stride=1,padding=1),norm_layer(ngf),nn.ReLU(True),nn.ReflectionPad2d(3),nn.Conv2d(ngf, output_nc-1, kernel_size=7, padding=0)]\n\n\t\tself.model_res_dec = nn.Sequential(*model_res_dec)\n\t\tself.model_res_dec_al=nn.Sequential(*model_res_dec_al)\n\t\tself.model_res_dec_fg=nn.Sequential(*model_res_dec_fg)\n\t\tself.model_al_out=nn.Sequential(*model_dec_al)\n\n\t\tself.model_dec_fg1=nn.Sequential(*model_dec_fg1)\n\t\tself.model_fg_out = nn.Sequential(*model_dec_fg2)\n\t\t\n\n\tdef forward(self, image,back,seg,multi):\n\t\timg_feat1=self.model_enc1(image)\n\t\timg_feat=self.model_enc2(img_feat1)\n\n\t\tback_feat=self.model_enc_back(back)\n\t\tseg_feat=self.model_enc_seg(seg)\n\t\tmulti_feat=self.model_enc_multi(multi)\n\n\t\toth_feat=torch.cat([self.comb_back(torch.cat([img_feat,back_feat],dim=1)),self.comb_seg(torch.cat([img_feat,seg_feat],dim=1)),self.comb_multi(torch.cat([img_feat,back_feat],dim=1))],dim=1)\n\n\t\tout_dec=self.model_res_dec(torch.cat([img_feat,oth_feat],dim=1))\n\n\t\tout_dec_al=self.model_res_dec_al(out_dec)\n\t\tal_out=self.model_al_out(out_dec_al)\n\n\t\tout_dec_fg=self.model_res_dec_fg(out_dec)\n\t\tout_dec_fg1=self.model_dec_fg1(out_dec_fg)\n\t\tfg_out=self.model_fg_out(torch.cat([out_dec_fg1,img_feat1],dim=1))\n\n\n\t\treturn al_out, fg_out\n\n############################## part ##################################\n\n\n\ndef conv_init(m):\n\tclassname = m.__class__.__name__\n\tif classname.find('Conv') != -1:\n\t\tinit.xavier_uniform(m.weight, gain=np.sqrt(2))\n\t\t#init.normal(m.weight)\n\t\tif m.bias is not None:\n\t\t\tinit.constant(m.bias, 0)\n\n\tif classname.find('Linear') != -1:\n\t\tinit.normal(m.weight)\n\t\tinit.constant(m.bias,1)\n\n\tif classname.find('BatchNorm2d') != -1:\n\t\tinit.normal(m.weight.data, 1.0, 0.2)\n\t\tinit.constant(m.bias.data, 0.0)\n\nclass conv3x3(nn.Module):\n\t'''(conv => BN => ReLU)'''\n\tdef __init__(self, in_ch, out_ch):\n\t\tsuper(conv3x3, self).__init__()\n\t\tself.conv = nn.Sequential(\n\t\t\tnn.Conv2d(in_ch, out_ch, 3, stride=2,padding=1),\n\t\t\tnn.BatchNorm2d(out_ch),\n\t\t\tnn.LeakyReLU(0.2,inplace=True),\n\t\t)\n\n\tdef forward(self, x):\n\t\tx = self.conv(x)\n\t\treturn x\n\nclass conv3x3s1(nn.Module):\n\t'''(conv => BN => ReLU)'''\n\tdef __init__(self, in_ch, out_ch):\n\t\tsuper(conv3x3s1, self).__init__()\n\t\tself.conv = nn.Sequential(\n\t\t\tnn.Conv2d(in_ch, out_ch, 3, stride=1,padding=1),\n\t\t\tnn.BatchNorm2d(out_ch),\n\t\t\tnn.LeakyReLU(0.2,inplace=True),\n\t\t)\n\n\tdef forward(self, x):\n\t\tx = self.conv(x)\n\t\treturn x\n\n\n\n\nclass conv1x1(nn.Module):\n\t'''(conv => BN => ReLU)'''\n\tdef __init__(self, in_ch, out_ch):\n\t\tsuper(conv1x1, self).__init__()\n\t\tself.conv = nn.Sequential(\n\t\t\tnn.Conv2d(in_ch, out_ch, 1, stride=1,padding=0),\n\t\t\tnn.BatchNorm2d(out_ch),\n\t\t\tnn.LeakyReLU(0.2,inplace=True),\n\t\t)\n\n\tdef forward(self, x):\n\t\tx = self.conv(x)\n\t\treturn x\n\n\n\nclass upconv3x3(nn.Module):\n\tdef __init__(self, in_ch, out_ch):\n\t\tsuper(upconv3x3, self).__init__()\n\t\tself.conv = nn.Sequential(\n\t\t\tnn.Upsample(scale_factor=2,mode='bilinear'),\n\t\t\tnn.Conv2d(in_ch, out_ch, 3, stride=1,padding=1),\n\t\t\tnn.BatchNorm2d(out_ch),\n\t\t\tnn.ReLU(inplace=True),\n\t\t)\n\tdef forward(self, x):\n\t\tx=self.conv(x)\n\t\treturn x\n\nclass fc(nn.Module):\n\tdef __init__(self,in_ch,out_ch):\n\t\tsuper(fc,self).__init__()\n\t\tself.fullc = nn.Sequential(\n\t\t\tnn.Linear(in_ch,out_ch),\n\t\t\tnn.ReLU(inplace=True),\n\t\t)\n\tdef forward(self,x):\n\t\tx=self.fullc(x)\n\t\treturn x\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n\tdef __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n\t\tsuper(ResnetBlock, self).__init__()\n\t\tself.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n\tdef build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n\t\tconv_block = []\n\t\tp = 0\n\t\tif padding_type == 'reflect':\n\t\t\tconv_block += [nn.ReflectionPad2d(1)]\n\t\telif padding_type == 'replicate':\n\t\t\tconv_block += [nn.ReplicationPad2d(1)]\n\t\telif padding_type == 'zero':\n\t\t\tp = 1\n\t\telse:\n\t\t\traise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n\t\tconv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n\t\t\t\t\t   norm_layer(dim),\n\t\t\t\t\t   nn.ReLU(True)]\n\t\tif use_dropout:\n\t\t\tconv_block += [nn.Dropout(0.5)]\n\n\t\tp = 0\n\t\tif padding_type == 'reflect':\n\t\t\tconv_block += [nn.ReflectionPad2d(1)]\n\t\telif padding_type == 'replicate':\n\t\t\tconv_block += [nn.ReplicationPad2d(1)]\n\t\telif padding_type == 'zero':\n\t\t\tp = 1\n\t\telse:\n\t\t\traise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\t\tconv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n\t\t\t\t\t   norm_layer(dim)]\n\n\t\treturn nn.Sequential(*conv_block)\n\n\tdef forward(self, x):\n\t\tout = x + self.conv_block(x)\n\t\treturn out\n\n\n##################################### Discriminators ####################################################\n\nclass MultiscaleDiscriminator(nn.Module):\n\tdef __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n\t\t\t\t use_sigmoid=False, num_D=3, getIntermFeat=False):\n\t\tsuper(MultiscaleDiscriminator, self).__init__()\n\t\tself.num_D = num_D\n\t\tself.n_layers = n_layers\n\t\tself.getIntermFeat = getIntermFeat\n\t \n\t\tfor i in range(num_D):\n\t\t\tnetD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n\t\t\tif getIntermFeat:                                \n\t\t\t\tfor j in range(n_layers+2):\n\t\t\t\t\tsetattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n\t\t\telse:\n\t\t\t\tsetattr(self, 'layer'+str(i), netD.model)\n\n\t\tself.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n\tdef singleD_forward(self, model, input):\n\t\tif self.getIntermFeat:\n\t\t\tresult = [input]\n\t\t\tfor i in range(len(model)):\n\t\t\t\tresult.append(model[i](result[-1]))\n\t\t\treturn result[1:]\n\t\telse:\n\t\t\treturn [model(input)]\n\n\tdef forward(self, input):        \n\t\tnum_D = self.num_D\n\t\tresult = []\n\t\tinput_downsampled = input\n\t\tfor i in range(num_D):\n\t\t\tif self.getIntermFeat:\n\t\t\t\tmodel = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n\t\t\telse:\n\t\t\t\tmodel = getattr(self, 'layer'+str(num_D-1-i))\n\t\t\tresult.append(self.singleD_forward(model, input_downsampled))\n\t\t\tif i != (num_D-1):\n\t\t\t\tinput_downsampled = self.downsample(input_downsampled)\n\t\treturn result\n\t\t\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n\tdef __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n\t\tsuper(NLayerDiscriminator, self).__init__()\n\t\tself.getIntermFeat = getIntermFeat\n\t\tself.n_layers = n_layers\n\n\t\tkw = 4\n\t\tpadw = int(np.ceil((kw-1.0)/2))\n\t\tsequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n\n\t\tnf = ndf\n\t\tfor n in range(1, n_layers):\n\t\t\tnf_prev = nf\n\t\t\tnf = min(nf * 2, 512)\n\t\t\tsequence += [[\n\t\t\t\tnn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n\t\t\t\tnorm_layer(nf), nn.LeakyReLU(0.2, True)\n\t\t\t]]\n\n\t\tnf_prev = nf\n\t\tnf = min(nf * 2, 512)\n\t\tsequence += [[\n\t\t\tnn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n\t\t\tnorm_layer(nf),\n\t\t\tnn.LeakyReLU(0.2, True)\n\t\t]]\n\n\t\tsequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n\n\t\tif use_sigmoid:\n\t\t\tsequence += [[nn.Sigmoid()]]\n\n\t\tif getIntermFeat:\n\t\t\tfor n in range(len(sequence)):\n\t\t\t\tsetattr(self, 'model'+str(n), nn.Sequential(*sequence[n]))\n\t\telse:\n\t\t\tsequence_stream = []\n\t\t\tfor n in range(len(sequence)):\n\t\t\t\tsequence_stream += sequence[n]\n\t\t\tself.model = nn.Sequential(*sequence_stream)\n\n\tdef forward(self, input):\n\t\tif self.getIntermFeat:\n\t\t\tres = [input]\n\t\t\tfor n in range(self.n_layers+2):\n\t\t\t\tmodel = getattr(self, 'model'+str(n))\n\t\t\t\tres.append(model(res[-1]))\n\t\t\treturn res[1:]\n\t\telse:\n\t\t\treturn self.model(input)        \n\n\n"
        },
        {
          "name": "prepare_real.py",
          "type": "blob",
          "size": 2.80859375,
          "content": "#######################################\n# Prepares training data. Takes a path to a directory of videos + captured backgrounds, dumps frames, extracts human\n# segmentations. Also takes a path of background videos. Creates a training CSV file with lines of the following format,\n# by using all but the last 80 frames of each video and iterating repeatedly over the background frames as needed.\n\n#$image;$captured_back;$segmentation;$image+20frames;$image+2*20frames;$image+3*20frames;$image+4*20frames;$target_back\n\npath = \"/path/to/Captured_Data/fixed-camera/train\"\nbackground_path = \"/path/to/Captured_Data/background\"\noutput_csv = \"Video_data_train.csv\"\n\n#######################################\n\nimport os\nfrom itertools import cycle\nfrom tqdm import tqdm\n\nvideos = [os.path.join(path, f[:-4]) for f in os.listdir(path) if f.endswith(\".mp4\")]\nbackgrounds = [os.path.join(background_path, f[:-4]) for f in os.listdir(background_path) if f.endswith(\".MOV\")]\n\nprint(f\"Dumping frames and segmenting {len(videos)} input videos\")\nfor i, video in enumerate(tqdm(videos)):\n    os.makedirs(video, exist_ok=True)\n    code = os.system(f\"ffmpeg -i {video}.mp4 {video}/%04d_img.png -hide_banner > prepare_real_logs.txt 2>&1\")\n    if code != 0:\n        exit(code)\n    print(f\"Dumped frames for {video} ({i+1}/{len(videos)})\")\n    code = os.system(f\"python test_segmentation_deeplab.py -i {video} > prepare_real_logs.txt 2>&1\")\n    if code != 0:\n        exit(code)\n    print(f\"Segmented {video} ({i+1}/{len(videos)})\")\n\nprint(f\"Dumping frames for {background_path} background videos\")\nfor i, background in enumerate(tqdm(backgrounds)):\n    os.makedirs(background, exist_ok=True)\n    code = os.system(f\"ffmpeg -i {background}.MOV {background}/%04d_img.png -hide_banner > /dev/null 2>&1\")\n    if code != 0:\n        exit(code)\n    print(f\"Dumped frames for {background} ({i+1}/{len(videos)})\")\n\nprint(f\"Creating CSV\")\nbackground_frames = []\nfor background in backgrounds:\n    background_frames.extend([os.path.join(background, f) for f in sorted(os.listdir(background))])\nbackground_stream = cycle(background_frames)\n\nwith open(output_csv, \"w\") as f:\n    for i, video in enumerate(videos):\n        n = len(os.listdir(video))\n        assert n % 2 == 0\n        n //= 2\n        for j in range(1, n + 1 - 80):\n            img_name = video + \"/%04d_img.png\" % j\n            captured_back = video + \".png\"\n            seg_name = video + \"/%04d_masksDL.png\" % j\n            mc1 = video + \"/%04d_img.png\" % (j + 20)\n            mc2 = video + \"/%04d_img.png\" % (j + 40)\n            mc3 = video + \"/%04d_img.png\" % (j + 60)\n            mc4 = video + \"/%04d_img.png\" % (j + 80)\n            target_back = next(background_stream)\n            csv_line = f\"{img_name};{captured_back};{seg_name};{mc1};{mc2};{mc3};{mc4};{target_back}\\n\"\n            f.write(csv_line)\n\nprint(f\"Done, written to {output_csv}\")\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1064453125,
          "content": "numpy==1.17.0\nopencv-python==3.4.5.20\npandas\nPillow==6.1\nscikit-image==0.14.2\nscipy==1.2.1\ntqdm\ntensorboardX\n"
        },
        {
          "name": "sample_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_background-matting_image.py",
          "type": "blob",
          "size": 7.162109375,
          "content": "from __future__ import print_function\n\n\nimport os, glob, time, argparse, pdb, cv2\n#import matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.measure import label\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\n\nfrom functions import *\nfrom networks import ResnetConditionHR\n\ntorch.set_num_threads(1)\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\nprint('CUDA Device: ' + os.environ[\"CUDA_VISIBLE_DEVICES\"])\n\n\n\"\"\"Parses arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Background Matting.')\nparser.add_argument('-m', '--trained_model', type=str, default='real-fixed-cam',choices=['real-fixed-cam', 'real-hand-held', 'syn-comp-adobe'],help='Trained background matting model')\nparser.add_argument('-o', '--output_dir', type=str, required=True,help='Directory to save the output results. (required)')\nparser.add_argument('-i', '--input_dir', type=str, required=True,help='Directory to load input images. (required)')\nparser.add_argument('-tb', '--target_back', type=str,help='Directory to load the target background.')\nparser.add_argument('-b', '--back', type=str,default=None,help='Captured background image. (only use for inference on videos with fixed camera')\n\n\nargs=parser.parse_args()\n\n#input model\nmodel_main_dir='Models/' + args.trained_model + '/';\n#input data path\ndata_path=args.input_dir\n\nif os.path.isdir(args.target_back):\n\targs.video=True\n\tprint('Using video mode')\nelse:\n\targs.video=False\n\tprint('Using image mode')\n\t#target background path\n\tback_img10=cv2.imread(args.target_back); back_img10=cv2.cvtColor(back_img10,cv2.COLOR_BGR2RGB);\n\t#Green-screen background\n\tback_img20=np.zeros(back_img10.shape); back_img20[...,0]=120; back_img20[...,1]=255; back_img20[...,2]=155;\n\n\n\n#initialize network\nfo=glob.glob(model_main_dir + 'netG_epoch_*')\nmodel_name1=fo[0]\nnetM=ResnetConditionHR(input_nc=(3,3,1,4),output_nc=4,n_blocks1=7,n_blocks2=3)\nnetM=nn.DataParallel(netM)\nnetM.load_state_dict(torch.load(model_name1))\nnetM.cuda(); netM.eval()\ncudnn.benchmark=True\nreso=(512,512) #input reoslution to the network\n\n#load captured background for video mode, fixed camera\nif args.back is not None:\n\tbg_im0=cv2.imread(args.back); bg_im0=cv2.cvtColor(bg_im0,cv2.COLOR_BGR2RGB);\n\n\n#Create a list of test images\ntest_imgs = [f for f in os.listdir(data_path) if\n\t\t\t   os.path.isfile(os.path.join(data_path, f)) and f.endswith('_img.png')]\ntest_imgs.sort()\n\n#output directory\nresult_path=args.output_dir\n\nif not os.path.exists(result_path):\n\tos.makedirs(result_path)\n\nfor i in range(0,len(test_imgs)):\n\tfilename = test_imgs[i]\t\n\t#original image\n\tbgr_img = cv2.imread(os.path.join(data_path, filename)); bgr_img=cv2.cvtColor(bgr_img,cv2.COLOR_BGR2RGB);\n\n\tif args.back is None:\n\t\t#captured background image\n\t\tbg_im0=cv2.imread(os.path.join(data_path, filename.replace('_img','_back'))); bg_im0=cv2.cvtColor(bg_im0,cv2.COLOR_BGR2RGB);\n\n\t#segmentation mask\n\trcnn = cv2.imread(os.path.join(data_path, filename.replace('_img','_masksDL')),0);\n\n\tif args.video: #if video mode, load target background frames\n\t\t#target background path\n\t\tback_img10=cv2.imread(os.path.join(args.target_back,filename.replace('_img.png','.png'))); back_img10=cv2.cvtColor(back_img10,cv2.COLOR_BGR2RGB);\n\t\t#Green-screen background\n\t\tback_img20=np.zeros(back_img10.shape); back_img20[...,0]=120; back_img20[...,1]=255; back_img20[...,2]=155;\n\n\t\t#create multiple frames with adjoining frames\n\t\tgap=20\n\t\tmulti_fr_w=np.zeros((bgr_img.shape[0],bgr_img.shape[1],4))\n\t\tidx=[i-2*gap,i-gap,i+gap,i+2*gap]\n\t\tfor t in range(0,4):\n\t\t\tif idx[t]<0:\n\t\t\t\tidx[t]=len(test_imgs)+idx[t]\n\t\t\telif idx[t]>=len(test_imgs):\n\t\t\t\tidx[t]=idx[t]-len(test_imgs)\n\n\t\t\tfile_tmp=test_imgs[idx[t]]\n\t\t\tbgr_img_mul = cv2.imread(os.path.join(data_path, file_tmp));\n\t\t\tmulti_fr_w[...,t]=cv2.cvtColor(bgr_img_mul,cv2.COLOR_BGR2GRAY);\n\n\telse:\n\t\t## create the multi-frame\n\t\tmulti_fr_w=np.zeros((bgr_img.shape[0],bgr_img.shape[1],4))\n\t\tmulti_fr_w[...,0] = cv2.cvtColor(bgr_img,cv2.COLOR_BGR2GRAY);\n\t\tmulti_fr_w[...,1] = multi_fr_w[...,0]\n\t\tmulti_fr_w[...,2] = multi_fr_w[...,0]\n\t\tmulti_fr_w[...,3] = multi_fr_w[...,0]\n\n\t\t\n\t#crop tightly\n\tbgr_img0=bgr_img;\n\tbbox=get_bbox(rcnn,R=bgr_img0.shape[0],C=bgr_img0.shape[1])\n\n\tcrop_list=[bgr_img,bg_im0,rcnn,back_img10,back_img20,multi_fr_w]\n\tcrop_list=crop_images(crop_list,reso,bbox)\n\tbgr_img=crop_list[0]; bg_im=crop_list[1]; rcnn=crop_list[2]; back_img1=crop_list[3]; back_img2=crop_list[4]; multi_fr=crop_list[5]\n\n\t#process segmentation mask\n\tkernel_er = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n\tkernel_dil = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n\trcnn=rcnn.astype(np.float32)/255; rcnn[rcnn>0.2]=1;\n\tK=25\n\n\tzero_id=np.nonzero(np.sum(rcnn,axis=1)==0)\n\tdel_id=zero_id[0][zero_id[0]>250]\n\tif len(del_id)>0:\n\t\tdel_id=[del_id[0]-2,del_id[0]-1,*del_id]\n\t\trcnn=np.delete(rcnn,del_id,0)\n\trcnn = cv2.copyMakeBorder( rcnn, 0, K + len(del_id), 0, 0, cv2.BORDER_REPLICATE)\n\n\n\trcnn = cv2.erode(rcnn, kernel_er, iterations=10)\n\trcnn = cv2.dilate(rcnn, kernel_dil, iterations=5)\n\trcnn=cv2.GaussianBlur(rcnn.astype(np.float32),(31,31),0)\n\trcnn=(255*rcnn).astype(np.uint8)\n\trcnn=np.delete(rcnn, range(reso[0],reso[0]+K), 0)\n\n\n\t#convert to torch\n\timg=torch.from_numpy(bgr_img.transpose((2, 0, 1))).unsqueeze(0); img=2*img.float().div(255)-1\n\tbg=torch.from_numpy(bg_im.transpose((2, 0, 1))).unsqueeze(0); bg=2*bg.float().div(255)-1\n\trcnn_al=torch.from_numpy(rcnn).unsqueeze(0).unsqueeze(0); rcnn_al=2*rcnn_al.float().div(255)-1\n\tmulti_fr=torch.from_numpy(multi_fr.transpose((2, 0, 1))).unsqueeze(0); multi_fr=2*multi_fr.float().div(255)-1\n\n\n\twith torch.no_grad():\n\t\timg,bg,rcnn_al, multi_fr =Variable(img.cuda()),  Variable(bg.cuda()), Variable(rcnn_al.cuda()), Variable(multi_fr.cuda())\n\t\tinput_im=torch.cat([img,bg,rcnn_al,multi_fr],dim=1)\n\t\t\n\t\talpha_pred,fg_pred_tmp=netM(img,bg,rcnn_al,multi_fr)\n\t\t\n\t\tal_mask=(alpha_pred>0.95).type(torch.cuda.FloatTensor)\n\n\t\t# for regions with alpha>0.95, simply use the image as fg\n\t\tfg_pred=img*al_mask + fg_pred_tmp*(1-al_mask)\n\n\t\talpha_out=to_image(alpha_pred[0,...]); \n\n\t\t#refine alpha with connected component\n\t\tlabels=label((alpha_out>0.05).astype(int))\n\t\ttry:\n\t\t\tassert( labels.max() != 0 )\n\t\texcept:\n\t\t\tcontinue\n\t\tlargestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n\t\talpha_out=alpha_out*largestCC\n\n\t\talpha_out=(255*alpha_out[...,0]).astype(np.uint8)\t\t\t\t\n\n\t\tfg_out=to_image(fg_pred[0,...]); fg_out=fg_out*np.expand_dims((alpha_out.astype(float)/255>0.01).astype(float),axis=2); fg_out=(255*fg_out).astype(np.uint8)\n\n\t\t#Uncrop\n\t\tR0=bgr_img0.shape[0];C0=bgr_img0.shape[1]\n\t\talpha_out0=uncrop(alpha_out,bbox,R0,C0)\n\t\tfg_out0=uncrop(fg_out,bbox,R0,C0)\n\n\t#compose\n\tback_img10=cv2.resize(back_img10,(C0,R0)); back_img20=cv2.resize(back_img20,(C0,R0))\n\tcomp_im_tr1=composite4(fg_out0,back_img10,alpha_out0)\n\tcomp_im_tr2=composite4(fg_out0,back_img20,alpha_out0)\n\n\tcv2.imwrite(result_path+'/'+filename.replace('_img','_out'), alpha_out0)\n\tcv2.imwrite(result_path+'/'+filename.replace('_img','_fg'), cv2.cvtColor(fg_out0,cv2.COLOR_BGR2RGB))\n\tcv2.imwrite(result_path+'/'+filename.replace('_img','_compose'), cv2.cvtColor(comp_im_tr1,cv2.COLOR_BGR2RGB))\n\tcv2.imwrite(result_path+'/'+filename.replace('_img','_matte').format(i), cv2.cvtColor(comp_im_tr2,cv2.COLOR_BGR2RGB))\n\n\t\n\tprint('Done: ' + str(i+1) + '/' + str(len(test_imgs)))\n\n"
        },
        {
          "name": "test_pre_process.m",
          "type": "blob",
          "size": 3.1953125,
          "content": "clc;clear;\n\nname='sample_data/input/';\n\n\nimg_list=dir([name '*_img.png']);\n\nbias=zeros(length(img_list),3); gain=ones(length(img_list),3);\nfor i=1:30:length(img_list)\n    img=double(imread([name img_list(i).name]))/255;\n    mask=double(imread([name strrep(img_list(i).name,'_img','_masksDL')]))/255;\n\n    ref=double(imread([name strrep(img_list(i).name,'_img','_back')]))/255;\n        \n        for t=1:10\n            mask = imdilate(mask,[0 1 0; 1 1 1; 0 1 0]);\n        end\n        \n        mask1=mask;\n        for t=1:300\n            mask1 = imdilate(mask1,[0 1 0; 1 1 1; 0 1 0]);\n        end\n        \n        [~,biasR,gainR]=bias_gain_corr(img(:,:,1),ref(:,:,1),mask1-mask);\n        [~,biasG,gainG]=bias_gain_corr(img(:,:,2),ref(:,:,2),mask1-mask);\n        [~,biasB,gainB]=bias_gain_corr(img(:,:,2),ref(:,:,3),mask1-mask);\n             \n        bias(i,1)=biasR; bias(i,2)=biasG; bias(i,3)=biasB;\n        gain(i,1)=gainR; gain(i,2)=gainG; gain(i,3)=gainB;\n\n    B=median(bias,1);\n    G=median(gain,1);\n\n\n    ref_tran(:,:,1)=ref(:,:,1)*G(1)+B(1);\n    ref_tran(:,:,2)=ref(:,:,2)*G(2)+B(2);\n    ref_tran(:,:,3)=ref(:,:,3)*G(3)+B(3);\n\n\n    [ref_tr,A]=align_tr_projective(img,ref_tran,mask); ref_tr=double(ref_tr)/255;\n    \n    imwrite(ref_tr,[name strrep(img_list(i).name,'_img','_back')]);\n   \nend\n\n%% functions %%\n\n\nfunction [cap_tran,biasR,gainR]=bias_gain_corr(orgR,capR,cap_mask)\n    cap_mask(cap_mask~=1)=0;\n\n    xR=capR(logical(cap_mask));\n    yR=orgR(logical(cap_mask));\n\n    gainR=nanstd(yR)/nanstd(xR);\n    biasR=nanmean(yR)-gainR*nanmean(xR);\n\n    cap_tran=capR*gainR+biasR;\n\nend\n\nfunction [recovered,A] = align_tr_projective(thumb0001_col,thumb0001_back_col,thumb0001_maskDL)\n\nthumb0001_col=uint8(255*thumb0001_col);\nthumb0001_back_col=uint8(255*thumb0001_back_col);\nthumb0001_maskDL=uint8(255*thumb0001_maskDL);\nmask=double(thumb0001_maskDL)==255;\n\nthumb0001=rgb2gray(thumb0001_col); thumb0001_back=rgb2gray(thumb0001_back_col);\n\nptsOriginal  = detectSURFFeatures(thumb0001);\nptsDistorted = detectSURFFeatures(thumb0001_back);\n\n[featuresOriginal,  validPtsOriginal]  = extractFeatures(thumb0001,  ptsOriginal);\n[featuresDistorted, validPtsDistorted] = extractFeatures(thumb0001_back, ptsDistorted);\n\nindexPairs = matchFeatures(featuresOriginal, featuresDistorted);\n\nmatchedOriginal  = validPtsOriginal(indexPairs(:,1));\nmatchedDistorted = validPtsDistorted(indexPairs(:,2));\n\n% figure;\n% showMatchedFeatures(thumb0001,thumb0001_back,matchedOriginal,matchedDistorted);\n% title('Putatively matched points (including outliers)');\n\n[tform, inlierDistorted, inlierOriginal] = estimateGeometricTransform(...\n    matchedDistorted, matchedOriginal, 'projective');\n\n% figure;\n% showMatchedFeatures(thumb0001,thumb0001_back,inlierOriginal,inlierDistorted);\n% title('Matching points (inliers only)');\n% legend('ptsOriginal','ptsDistorted');\n\noutputView = imref2d(size(thumb0001));\nrecovered  = imwarp(thumb0001_back_col,tform,'OutputView',outputView);\n\n% figure, imagesc(sum(abs(double(thumb0001_col)-double(recovered)),3)/3);\n\nmask=sum(double(recovered),3)==0;\nrecovered(repmat(mask,[1 1 3]))=thumb0001_col(repmat(mask,[1 1 3]));\n\nrecovered(repmat(mask&(double(thumb0001_maskDL)/255),[1 1 3]))=thumb0001_back_col(repmat(mask&(double(thumb0001_maskDL)/255),[1 1 3]));\n\nend\n\n"
        },
        {
          "name": "test_pre_process.py",
          "type": "blob",
          "size": 3.095703125,
          "content": "import numpy as np\nimport cv2, pdb, glob, argparse\n\nMAX_FEATURES = 500\nGOOD_MATCH_PERCENT = 0.15\n\n\ndef alignImages(im1, im2,masksDL):\n\n\t# Convert images to grayscale\n\tim1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n\tim2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n\n\takaze = cv2.AKAZE_create()\n\tkeypoints1, descriptors1 = akaze.detectAndCompute(im1, None)\n\tkeypoints2, descriptors2 = akaze.detectAndCompute(im2, None)\n\t\n\t# Match features.\n\tmatcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE)\n\tmatches = matcher.match(descriptors1, descriptors2, None)\n\t\n\t# Sort matches by score\n\tmatches.sort(key=lambda x: x.distance, reverse=False)\n\n\t# Remove not so good matches\n\tnumGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n\tmatches = matches[:numGoodMatches]\n\t\n\t# Extract location of good matches\n\tpoints1 = np.zeros((len(matches), 2), dtype=np.float32)\n\tpoints2 = np.zeros((len(matches), 2), dtype=np.float32)\n\n\tfor i, match in enumerate(matches):\n\t\tpoints1[i, :] = keypoints1[match.queryIdx].pt\n\t\tpoints2[i, :] = keypoints2[match.trainIdx].pt\n\t\n\t# Find homography\n\th, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n\n\t# Use homography\n\theight, width, channels = im2.shape\n\tim1Reg = cv2.warpPerspective(im1, h, (width, height))\n\t# copy image in the empty region, unless it is a foreground. Then copy background\n\n\tmask_rep=(np.sum(im1Reg.astype('float32'),axis=2)==0)\n\n\tim1Reg[mask_rep,0]=im2[mask_rep,0]\n\tim1Reg[mask_rep,1]=im2[mask_rep,1]\n\tim1Reg[mask_rep,2]=im2[mask_rep,2]\n\n\tmask_rep1=np.logical_and(mask_rep , masksDL[...,0]==255)\n\n\tim1Reg[mask_rep1,0]=im1[mask_rep1,0]\n\tim1Reg[mask_rep1,1]=im1[mask_rep1,1]\n\tim1Reg[mask_rep1,2]=im1[mask_rep1,2]\n\n\n\treturn im1Reg\n\n\ndef adjustExposure(img,back,mask):\n\tkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n\tmask = cv2.dilate(mask, kernel, iterations=10)\n\tmask1 = cv2.dilate(mask, kernel, iterations=300)\n\tmsk=mask1.astype(np.float32)/255-mask.astype(np.float32)/255; msk=msk.astype(np.bool)\n\n\tback_tr=back\n\tback_tr[...,0]=bias_gain(img[...,0],back[...,0],msk)\n\tback_tr[...,1]=bias_gain(img[...,1],back[...,1],msk)\n\tback_tr[...,2]=bias_gain(img[...,2],back[...,2],msk)\n\n\treturn back_tr\n\n\ndef bias_gain(orgR,capR,cap_mask):\n\tcapR=capR.astype('float32')\n\torgR=orgR.astype('float32')\n\n\txR=capR[cap_mask]\n\tyR=orgR[cap_mask]\n\n\tgainR=np.nanstd(yR)/np.nanstd(xR);\n\tbiasR=np.nanmean(yR)-gainR*np.nanmean(xR);\n\n\tcap_tran=capR*gainR+biasR;\n\n\treturn cap_tran.astype('float32')\n\n\nparser = argparse.ArgumentParser(description='Deeplab Segmentation')\nparser.add_argument('-i', '--input_dir', type=str, required=True,help='Directory to save the output results. (required)')\nargs=parser.parse_args()\n\n\ndir_name=args.input_dir\n\nlist_im=glob.glob(dir_name + '/*_img.png'); list_im.sort()\n\n\nfor i in range(0,len(list_im)):\n\n\timage = cv2.imread(list_im[i],cv2.IMREAD_COLOR)\n\tback = cv2.imread(list_im[i].replace('img','back'),cv2.IMREAD_COLOR)\n\tmask = cv2.imread(list_im[i].replace('img','masksDL'))\n\n\t#back_new = adjustExposure(image,back,mask[...,0])\n\n\tback_align = alignImages(back, image,mask)\n\n\tcv2.imwrite(list_im[i].replace('img','back'),back_align)\n\nstr_msg='\\nDone: ' + dir_name\nprint(str_msg)"
        },
        {
          "name": "test_pre_process_video.m",
          "type": "blob",
          "size": 3.443359375,
          "content": "clc;clear;\n\nname='sample_video/input/'; %input file location\nback_name='sample_video/teaser_back.png'; %captured background image\n\n\n\nimg_list=dir([name '*_img.png']);\n\n%% bias-gain adjustment\n\nref=double(imread(back_name))/255;\n\nbias=zeros(length(img_list),3); gain=ones(length(img_list),3);\nfor i=1:30:length(img_list)\n    img=double(imread([name img_list(i).name]))/255;\n    mask=double(imread([name strrep(img_list(i).name,'_img','_masksDL')]))/255;\n        \n        for t=1:10\n            mask = imdilate(mask,[0 1 0; 1 1 1; 0 1 0]);\n        end\n        \n        mask1=mask;\n        for t=1:300\n            mask1 = imdilate(mask1,[0 1 0; 1 1 1; 0 1 0]);\n        end\n        \n        [~,biasR,gainR]=bias_gain_corr(img(:,:,1),ref(:,:,1),mask1-mask);\n        [~,biasG,gainG]=bias_gain_corr(img(:,:,2),ref(:,:,2),mask1-mask);\n        [~,biasB,gainB]=bias_gain_corr(img(:,:,2),ref(:,:,3),mask1-mask);\n             \n        bias(i,1)=biasR; bias(i,2)=biasG; bias(i,3)=biasB;\n        gain(i,1)=gainR; gain(i,2)=gainG; gain(i,3)=gainB;\nend\n\n    B=median(bias,1);\n    G=median(gain,1);\n\n\n    ref_tran(:,:,1)=ref(:,:,1)*G(1)+B(1);\n    ref_tran(:,:,2)=ref(:,:,2)*G(2)+B(2);\n    ref_tran(:,:,3)=ref(:,:,3)*G(3)+B(3);\n\n\n%% alignment %%\n\nfor i=1:length(img_list)\n    img=double(imread([name img_list(i).name]))/255;\n    mask=double(imread([name strrep(img_list(i).name,'_img','_masksDL')]))/255;\n\n    [ref_tr,A]=align_tr_projective(img,ref_tran,mask); ref_tr=double(ref_tr)/255;\n    \n    imwrite(ref_tr,[name strrep(img_list(i).name,'_img','_back')]);\n\nend\n\n%% functions %%\n\n\nfunction [cap_tran,biasR,gainR]=bias_gain_corr(orgR,capR,cap_mask)\n    cap_mask(cap_mask~=1)=0;\n\n    xR=capR(logical(cap_mask));\n    yR=orgR(logical(cap_mask));\n\n    gainR=nanstd(yR)/nanstd(xR);\n    biasR=nanmean(yR)-gainR*nanmean(xR);\n\n    cap_tran=capR*gainR+biasR;\n\nend\n\nfunction [recovered,A] = align_tr_projective(thumb0001_col,thumb0001_back_col,thumb0001_maskDL)\n\nthumb0001_col=uint8(255*thumb0001_col);\nthumb0001_back_col=uint8(255*thumb0001_back_col);\nthumb0001_maskDL=uint8(255*thumb0001_maskDL);\nmask=double(thumb0001_maskDL)==255;\n\nthumb0001=rgb2gray(thumb0001_col); thumb0001_back=rgb2gray(thumb0001_back_col);\n\nptsOriginal  = detectSURFFeatures(thumb0001);\nptsDistorted = detectSURFFeatures(thumb0001_back);\n\n[featuresOriginal,  validPtsOriginal]  = extractFeatures(thumb0001,  ptsOriginal);\n[featuresDistorted, validPtsDistorted] = extractFeatures(thumb0001_back, ptsDistorted);\n\nindexPairs = matchFeatures(featuresOriginal, featuresDistorted);\n\nmatchedOriginal  = validPtsOriginal(indexPairs(:,1));\nmatchedDistorted = validPtsDistorted(indexPairs(:,2));\n\n% figure;\n% showMatchedFeatures(thumb0001,thumb0001_back,matchedOriginal,matchedDistorted);\n% title('Putatively matched points (including outliers)');\n\n[tform, inlierDistorted, inlierOriginal] = estimateGeometricTransform(...\n    matchedDistorted, matchedOriginal, 'projective');\n\n\n% figure;\n% showMatchedFeatures(thumb0001,thumb0001_back,inlierOriginal,inlierDistorted);\n% title('Matching points (inliers only)');\n% legend('ptsOriginal','ptsDistorted');\n\noutputView = imref2d(size(thumb0001));\nrecovered  = imwarp(thumb0001_back_col,tform,'OutputView',outputView);\n\n% figure, imagesc(sum(abs(double(thumb0001_col)-double(recovered)),3)/3);\n\nmask=sum(double(recovered),3)==0;\nrecovered(repmat(mask,[1 1 3]))=thumb0001_col(repmat(mask,[1 1 3]));\n\nrecovered(repmat(mask&(double(thumb0001_maskDL)/255),[1 1 3]))=thumb0001_back_col(repmat(mask&(double(thumb0001_maskDL)/255),[1 1 3]));\n\nend\n\n"
        },
        {
          "name": "test_pre_process_video.py",
          "type": "blob",
          "size": 3.6552734375,
          "content": "import numpy as np\nimport cv2, pdb, glob, argparse\n\nMAX_FEATURES = 500\nGOOD_MATCH_PERCENT = 0.15\n\n\ndef alignImages(im1, im2,masksDL):\n\n\t# Convert images to grayscale\n\tim1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n\tim2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n\n\takaze = cv2.AKAZE_create()\n\tkeypoints1, descriptors1 = akaze.detectAndCompute(im1, None)\n\tkeypoints2, descriptors2 = akaze.detectAndCompute(im2, None)\n\t\n\t# Match features.\n\tmatcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE)\n\tmatches = matcher.match(descriptors1, descriptors2, None)\n\t\n\t# Sort matches by score\n\tmatches.sort(key=lambda x: x.distance, reverse=False)\n\n\t# Remove not so good matches\n\tnumGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n\tmatches = matches[:numGoodMatches]\n\t\n\t# Extract location of good matches\n\tpoints1 = np.zeros((len(matches), 2), dtype=np.float32)\n\tpoints2 = np.zeros((len(matches), 2), dtype=np.float32)\n\n\tfor i, match in enumerate(matches):\n\t\tpoints1[i, :] = keypoints1[match.queryIdx].pt\n\t\tpoints2[i, :] = keypoints2[match.trainIdx].pt\n\t\n\t# Find homography\n\th, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n\n\t# Use homography\n\theight, width, channels = im2.shape\n\tim1Reg = cv2.warpPerspective(im1, h, (width, height))\n\t# copy image in the empty region, unless it is a foreground. Then copy background\n\n\tmask_rep=(np.sum(im1Reg.astype('float32'),axis=2)==0)\n\n\tim1Reg[mask_rep,0]=im2[mask_rep,0]\n\tim1Reg[mask_rep,1]=im2[mask_rep,1]\n\tim1Reg[mask_rep,2]=im2[mask_rep,2]\n\n\tmask_rep1=np.logical_and(mask_rep , masksDL[...,0]==255)\n\n\tim1Reg[mask_rep1,0]=im1[mask_rep1,0]\n\tim1Reg[mask_rep1,1]=im1[mask_rep1,1]\n\tim1Reg[mask_rep1,2]=im1[mask_rep1,2]\n\n\n\treturn im1Reg\n\ndef adjustExposure(img,back,mask):\n\tkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n\tmask = cv2.dilate(mask, kernel, iterations=10)\n\tmask1 = cv2.dilate(mask, kernel, iterations=300)\n\tmsk=mask1.astype(np.float32)/255-mask.astype(np.float32)/255; msk=msk.astype(np.bool)\n\n\tbias=np.zeros((1,3)); gain=np.ones((1,3))\n\n\tbias[0,0],gain[0,0]=bias_gain(img[...,0],back[...,0],msk)\n\tbias[0,1],gain[0,1]=bias_gain(img[...,1],back[...,1],msk)\n\tbias[0,2],gain[0,2]=bias_gain(img[...,2],back[...,2],msk)\n\n\treturn bias,gain\n\n\ndef bias_gain(orgR,capR,cap_mask):\n\n\txR=capR[cap_mask]\n\tyR=orgR[cap_mask]\n\n\tgainR=np.nanstd(yR)/np.nanstd(xR);\n\tbiasR=np.nanmean(yR)-gainR*np.nanmean(xR);\n\n\treturn biasR,gainR\n\n\nparser = argparse.ArgumentParser(description='Deeplab Segmentation')\nparser.add_argument('-i', '--input_dir', type=str, required=True,help='Directory to save the output results. (required)')\nparser.add_argument('-v_name','--video_name',type=str, default=None,help='Name of the video')\nargs=parser.parse_args()\n\n\ndir_name=args.input_dir\n\nlist_im=glob.glob(dir_name + '/*_img.png'); list_im.sort()\n\n\nback=cv2.imread(args.video_name);\n\n\n# back=back.astype('float32')/255\n# #adjust bias-gain\n# bias=[]; gain=[]\n# for i in range(0,len(list_im),30):\n\n# \timage = cv2.imread(list_im[i]); image=image.astype('float32')/255\n# \tmask = cv2.imread(list_im[i].replace('img','masksDL'))\n\n# \tb,g=adjustExposure(image,back,mask[...,0])\n# \tbias.append(b); gain.append(g)\n# Bias=np.median(np.asarray(bias),axis=0).squeeze(0); \n# Gain=np.median(np.asarray(gain),axis=0).squeeze(0)\n# back_new=back\n# back_new[...,0]=Gain[0]*back[...,0]+Bias[0]\n# back_new[...,1]=Gain[1]*back[...,1]+Bias[1]\n# back_new[...,2]=Gain[2]*back[...,2]+Bias[2]\n# back_new=(255*back_new).astype(np.uint8)\n\nfor i in range(0,len(list_im)):\n\n\timage = cv2.imread(list_im[i])\n\tmask = cv2.imread(list_im[i].replace('img','masksDL'))\n\n\tback_align = alignImages(back, image,mask)\n\n\tcv2.imwrite(list_im[i].replace('img','back'),back_align)\n\n\tprint('Done: ' + str(i+1) + '/' + str(len(list_im)))\n\n\n\t\n"
        },
        {
          "name": "test_segmentation_deeplab.py",
          "type": "blob",
          "size": 4.98828125,
          "content": "import os\nfrom io import BytesIO\nimport tarfile\nimport tempfile\nfrom six.moves import urllib\n\nimport numpy as np\nfrom PIL import Image\nimport cv2, pdb, glob, argparse\n\nimport tensorflow as tf\n\n\n\nclass DeepLabModel(object):\n\t\"\"\"Class to load deeplab model and run inference.\"\"\"\n\n\tINPUT_TENSOR_NAME = 'ImageTensor:0'\n\tOUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n\tINPUT_SIZE = 513\n\tFROZEN_GRAPH_NAME = 'frozen_inference_graph'\n\n\tdef __init__(self, tarball_path):\n\t\t#\"\"\"Creates and loads pretrained deeplab model.\"\"\"\n\t\tself.graph = tf.Graph()\n\t\tgraph_def = None\n\t\t# Extract frozen graph from tar archive.\n\t\ttar_file = tarfile.open(tarball_path)\n\t\tfor tar_info in tar_file.getmembers():\n\t\t\tif self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n\t\t\t\tfile_handle = tar_file.extractfile(tar_info)\n\t\t\t\tgraph_def = tf.GraphDef.FromString(file_handle.read())\n\t\t\t\tbreak\n\n\t\ttar_file.close()\n\n\t\tif graph_def is None:\n\t\t\traise RuntimeError('Cannot find inference graph in tar archive.')\n\n\t\twith self.graph.as_default():\n\t\t\ttf.import_graph_def(graph_def, name='')\n\n\t\tself.sess = tf.Session(graph=self.graph)\n\n\tdef run(self, image):\n\t\t\"\"\"Runs inference on a single image.\n\n\t\tArgs:\n\t\t  image: A PIL.Image object, raw input image.\n\n\t\tReturns:\n\t\t  resized_image: RGB image resized from original input image.\n\t\t  seg_map: Segmentation map of `resized_image`.\n\t\t\"\"\"\n\t\twidth, height = image.size\n\t\tresize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n\t\ttarget_size = (int(resize_ratio * width), int(resize_ratio * height))\n\t\tresized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n\t\tbatch_seg_map = self.sess.run(\n\t\t\tself.OUTPUT_TENSOR_NAME,\n\t\t\tfeed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n\t\tseg_map = batch_seg_map[0]\n\t\treturn resized_image, seg_map\n\ndef create_pascal_label_colormap():\n\t\"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n\n\tReturns:\n\tA Colormap for visualizing segmentation results.\n\t\"\"\"\n\tcolormap = np.zeros((256, 3), dtype=int)\n\tind = np.arange(256, dtype=int)\n\n\tfor shift in reversed(range(8)):\n\t\tfor channel in range(3):\n\t\t  colormap[:, channel] |= ((ind >> channel) & 1) << shift\n\t\tind >>= 3\n\n\treturn colormap\n\ndef label_to_color_image(label):\n\t\"\"\"Adds color defined by the dataset colormap to the label.\n\n\tArgs:\n\tlabel: A 2D array with integer type, storing the segmentation label.\n\n\tReturns:\n\tresult: A 2D array with floating type. The element of the array\n\t  is the color indexed by the corresponding element in the input label\n\t  to the PASCAL color map.\n\n\tRaises:\n\tValueError: If label is not of rank 2 or its value is larger than color\n\t  map maximum entry.\n\t\"\"\"\n\tif label.ndim != 2:\n\t\traise ValueError('Expect 2-D input label')\n\n\tcolormap = create_pascal_label_colormap()\n\n\tif np.max(label) >= len(colormap):\n\t\traise ValueError('label value too large.')\n\n\treturn colormap[label]\n\n\n\nparser = argparse.ArgumentParser(description='Deeplab Segmentation')\nparser.add_argument('-i', '--input_dir', type=str, required=True,help='Directory to save the output results. (required)')\nargs=parser.parse_args()\n\ndir_name=args.input_dir;\n\n\n## setup ####################\n\nLABEL_NAMES = np.asarray([\n\t'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n\t'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n\t'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n])\n\nFULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\nFULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)\n\n\nMODEL_NAME = 'xception_coco_voctrainval'  # @param ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n\n_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n_MODEL_URLS = {\n\t'mobilenetv2_coco_voctrainaug':\n\t\t'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n\t'mobilenetv2_coco_voctrainval':\n\t\t'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n\t'xception_coco_voctrainaug':\n\t\t'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n\t'xception_coco_voctrainval':\n\t\t'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n}\n_TARBALL_NAME = _MODEL_URLS[MODEL_NAME]\n\nmodel_dir = 'deeplab_model'\nif not os.path.exists(model_dir):\n  tf.gfile.MakeDirs(model_dir)\n\ndownload_path = os.path.join(model_dir, _TARBALL_NAME)\nif not os.path.exists(download_path):\n  print('downloading model to %s, this might take a while...' % download_path)\n  urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], \n\t\t\t     download_path)\n  print('download completed! loading DeepLab model...')\n\nMODEL = DeepLabModel(download_path)\nprint('model loaded successfully!')\n\n#######################################################################################\n\n\nlist_im=glob.glob(dir_name + '/*_img.png'); list_im.sort()\n\n\nfor i in range(0,len(list_im)):\n\n\timage = Image.open(list_im[i])\n\n\tres_im,seg=MODEL.run(image)\n\n\tseg=cv2.resize(seg.astype(np.uint8),image.size)\n\n\tmask_sel=(seg==15).astype(np.float32)\n\n\n\tname=list_im[i].replace('img','masksDL')\n\tcv2.imwrite(name,(255*mask_sel).astype(np.uint8))\n\nstr_msg='\\nDone: ' + dir_name\nprint(str_msg)\n\n\n"
        },
        {
          "name": "train_adobe.py",
          "type": "blob",
          "size": 5.6552734375,
          "content": "from __future__ import print_function\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter\n\n\nimport os\nimport time\nimport argparse\n\nfrom data_loader import AdobeDataAffineHR\nfrom functions import *\nfrom networks import ResnetConditionHR, conv_init\nfrom loss_functions import alpha_loss, compose_loss, alpha_gradient_loss\n\n\n#CUDA\n\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\nprint('CUDA Device: ' + os.environ[\"CUDA_VISIBLE_DEVICES\"])\n\n\n\"\"\"Parses arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Training Background Matting on Adobe Dataset.')\nparser.add_argument('-n', '--name', type=str, help='Name of tensorboard and model saving folders.')\nparser.add_argument('-bs', '--batch_size', type=int, help='Batch Size.')\nparser.add_argument('-res', '--reso', type=int, help='Input image resolution')\n\nparser.add_argument('-epoch', '--epoch', type=int, default=60,help='Maximum Epoch')\nparser.add_argument('-n_blocks1', '--n_blocks1', type=int, default=7,help='Number of residual blocks after Context Switching.')\nparser.add_argument('-n_blocks2', '--n_blocks2', type=int, default=3,help='Number of residual blocks for Fg and alpha each.')\n\nargs=parser.parse_args()\n\n\n##Directories\ntb_dir='TB_Summary/' + args.name\nmodel_dir='Models/' + args.name\n\nif not os.path.exists(model_dir):\n\tos.makedirs(model_dir)\n\nif not os.path.exists(tb_dir):\n\tos.makedirs(tb_dir)\n\n## Input list\ndata_config_train = {'reso': [args.reso,args.reso], 'trimapK': [5,5], 'noise': True}  # choice for data loading parameters\n\n# DATA LOADING\nprint('\\n[Phase 1] : Data Preparation')\n\ndef collate_filter_none(batch):\n\tbatch = list(filter(lambda x: x is not None, batch))\n\treturn torch.utils.data.dataloader.default_collate(batch)\n\n#Original Data\ntraindata =  AdobeDataAffineHR(csv_file='Data_adobe/Adobe_train_data.csv',data_config=data_config_train,transform=None)  #Write a dataloader function that can read the database provided by .csv file\n\ntrain_loader = torch.utils.data.DataLoader(traindata, batch_size=args.batch_size, shuffle=True, num_workers=args.batch_size, collate_fn=collate_filter_none)\n\n\nprint('\\n[Phase 2] : Initialization')\n\nnet=ResnetConditionHR(input_nc=(3,3,1,4), output_nc=4, n_blocks1=7, n_blocks2=3, norm_layer=nn.BatchNorm2d)\nnet.apply(conv_init)\nnet=nn.DataParallel(net)\n#net.load_state_dict(torch.load(model_dir + 'net_epoch_X')) #uncomment this if you are initializing your model\nnet.cuda()\ntorch.backends.cudnn.benchmark=True\n\n#Loss\nl1_loss=alpha_loss()\nc_loss=compose_loss()\ng_loss=alpha_gradient_loss()\n\noptimizer = optim.Adam(net.parameters(), lr=1e-4)\n#optimizer.load_state_dict(torch.load(model_dir + 'optim_epoch_X')) #uncomment this if you are initializing your model\n\n\n\nlog_writer=SummaryWriter(tb_dir)\n\nprint('Starting Training')\nstep=50 #steps to visualize training images in tensorboard\n\nKK=len(train_loader)\n\nfor epoch in range(0,args.epoch):\n\n\tnet.train(); \n\n\tnetL, alL, fgL, fg_cL, al_fg_cL, elapse_run, elapse=0,0,0,0,0,0,0\n\n\tt0=time.time();\n\ttestL=0; ct_tst=0;\n\tfor i,data in enumerate(train_loader):\n\t\t#Initiating\n\n\t\tfg, bg, alpha, image, seg, bg_tr, multi_fr = data['fg'], data['bg'], data['alpha'], data['image'], data['seg'], data['bg_tr'], data['multi_fr']\n\n\t\tfg, bg, alpha, image, seg, bg_tr, multi_fr = Variable(fg.cuda()), Variable(bg.cuda()), Variable(alpha.cuda()), Variable(image.cuda()), Variable(seg.cuda()), Variable(bg_tr.cuda()), Variable(multi_fr.cuda())\n\n\n\t\tmask=(alpha>-0.99).type(torch.cuda.FloatTensor)\n\t\tmask0=Variable(torch.ones(alpha.shape).cuda())\n\n\t\ttr0=time.time()\n\n\t\talpha_pred,fg_pred=net(image,bg_tr,seg,multi_fr)\n\n\t\t## Put needed loss here\n\t\tal_loss=l1_loss(alpha,alpha_pred,mask0)\n\t\tfg_loss=l1_loss(fg,fg_pred,mask)\n\n\t\tal_mask=(alpha_pred>0.95).type(torch.cuda.FloatTensor)\n\t\tfg_pred_c=image*al_mask + fg_pred*(1-al_mask)\n\t\t\n\t\tfg_c_loss= c_loss(image,alpha_pred,fg_pred_c,bg,mask0)\n\n\t\tal_fg_c_loss=g_loss(alpha,alpha_pred,mask0)\n\n\t\tloss=al_loss + 2*fg_loss + fg_c_loss + al_fg_c_loss\n\n\t\toptimizer.zero_grad()\n\t\tloss.backward()\n\n\t\toptimizer.step()\n\n\t\tnetL += loss.data\n\t\talL += al_loss.data\n\t\tfgL += fg_loss.data\n\t\tfg_cL += fg_c_loss.data\n\t\tal_fg_cL += al_fg_c_loss.data\n\n\n\t\tlog_writer.add_scalar('training_loss', loss.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('alpha_loss', al_loss.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('fg_loss', fg_loss.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('comp_loss', fg_c_loss.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('alpha_gradient_loss', al_fg_c_loss.data, epoch*KK + i + 1)\n\n\n\t\tt1=time.time()\n\n\t\telapse +=t1 -t0\n\t\telapse_run += t1-tr0\n\n\t\tt0=t1\n\n\t\ttestL+=loss.data\n\t\tct_tst+=1\n\n\t\tif i % step == (step-1):\n\t\t\tprint('[%d, %5d] Total-loss:  %.4f Alpha-loss: %.4f Fg-loss: %.4f Comp-loss: %.4f Alpha-gradient-loss: %.4f Time-all: %.4f Time-fwbw: %.4f' % (epoch + 1, i + 1, netL/step, alL/step, fgL/step, fg_cL/step, al_fg_cL/step, elapse/step, elapse_run/step))\n\t\t\tnetL, alL, fgL, fg_cL, al_fg_cL, elapse_run, elapse=0,0,0,0,0,0,0\n\n\t\t\twrite_tb_log(image,'image',log_writer,i)\n\t\t\twrite_tb_log(seg,'seg',log_writer,i)\n\t\t\twrite_tb_log(alpha,'alpha',log_writer,i)\n\t\t\twrite_tb_log(alpha_pred,'alpha_pred',log_writer,i)\n\t\t\twrite_tb_log(fg*mask,'fg',log_writer,i)\n\t\t\twrite_tb_log(fg_pred*mask,'fg_pred',log_writer,i)\n\t\t\twrite_tb_log(multi_fr[0:4,0,...].unsqueeze(1),'multi_fr',log_writer,i)\n\n\t\t\t#composition\n\t\t\talpha_pred=(alpha_pred+1)/2\n\t\t\tcomp=fg_pred*alpha_pred + (1-alpha_pred)*bg\n\t\t\twrite_tb_log(comp,'composite',log_writer,i)\n\n\t\t\tdel comp\n\n\t\tdel fg, bg, alpha, image, alpha_pred, fg_pred, seg, multi_fr\n\n\n\t#Saving\n\ttorch.save(net.state_dict(), model_dir + 'net_epoch_%d_%.4f.pth' %(epoch,testL/ct_tst))\n\ttorch.save(optimizer.state_dict(), model_dir + 'optim_epoch_%d_%.4f.pth' %(epoch,testL/ct_tst))\n\n\n"
        },
        {
          "name": "train_real_fixed.py",
          "type": "blob",
          "size": 7.7958984375,
          "content": "from __future__ import print_function\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter\n\n\nimport os\nimport time\nimport argparse\nimport numpy as np\n\nfrom data_loader import VideoData\nfrom functions import *\nfrom networks import ResnetConditionHR, MultiscaleDiscriminator, conv_init\nfrom loss_functions import alpha_loss, compose_loss, alpha_gradient_loss, GANloss\n\n\n#CUDA\n\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\nprint('CUDA Device: ' + os.environ[\"CUDA_VISIBLE_DEVICES\"])\n\n\n\"\"\"Parses arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Training Background Matting on Adobe Dataset.')\nparser.add_argument('-n', '--name', type=str, help='Name of tensorboard and model saving folders.')\nparser.add_argument('-bs', '--batch_size', type=int, help='Batch Size.')\nparser.add_argument('-res', '--reso', type=int, help='Input image resolution')\nparser.add_argument('-init_model', '--init_model', type=str, help='Initial model file')\n\nparser.add_argument('-epoch', '--epoch', type=int, default=15,help='Maximum Epoch')\nparser.add_argument('-n_blocks1', '--n_blocks1', type=int, default=7,help='Number of residual blocks after Context Switching.')\nparser.add_argument('-n_blocks2', '--n_blocks2', type=int, default=3,help='Number of residual blocks for Fg and alpha each.')\n\nargs=parser.parse_args()\n\n##Directories\ntb_dir='TB_Summary/' + args.name\nmodel_dir='Models/' + args.name\n\n\nif not os.path.exists(model_dir):\n\tos.makedirs(model_dir)\n\nif not os.path.exists(tb_dir):\n\tos.makedirs(tb_dir)\n\n## Input list\ndata_config_train = {'reso': (args.reso,args.reso)}  #if trimap is true, rcnn is used\n\n# DATA LOADING\nprint('\\n[Phase 1] : Data Preparation')\n\ndef collate_filter_none(batch):\n\tbatch = list(filter(lambda x: x is not None, batch))\n\treturn torch.utils.data.dataloader.default_collate(batch)\n\n#Original Data\ntraindata =  VideoData(csv_file='Video_data_train.csv',data_config=data_config_train,transform=None)  #Write a dataloader function that can read the database provided by .csv file\ntrain_loader = torch.utils.data.DataLoader(traindata, batch_size=args.batch_size, shuffle=True, num_workers=args.batch_size, collate_fn=collate_filter_none)\n\n\nprint('\\n[Phase 2] : Initialization')\n\nnetB=ResnetConditionHR(input_nc=(3,3,1,4),output_nc=4,n_blocks1=args.n_blocks1,n_blocks2=args.n_blocks2)\nnetB=nn.DataParallel(netB)\nnetB.load_state_dict(torch.load(args.init_model))\nnetB.cuda(); netB.eval()\nfor param in netB.parameters(): #freeze netD\n\tparam.requires_grad = False\n\nnetG=ResnetConditionHR(input_nc=(3,3,1,4),output_nc=4,n_blocks1=args.n_blocks1,n_blocks2=args.n_blocks2)\nnetG.apply(conv_init)\nnetG=nn.DataParallel(netG)\nnetG.cuda()\ntorch.backends.cudnn.benchmark=True\n\nnetD=MultiscaleDiscriminator(input_nc=3,num_D=1,norm_layer=nn.InstanceNorm2d,ndf=64)\nnetD.apply(conv_init)\nnetD=nn.DataParallel(netD)\nnetD.cuda()\n\n#Loss\nl1_loss=alpha_loss()\nc_loss=compose_loss()\ng_loss=alpha_gradient_loss()\nGAN_loss=GANloss()\n\noptimizerG = optim.Adam(netG.parameters(), lr=1e-4)\noptimizerD = optim.Adam(netD.parameters(), lr=1e-5)\n\n\n\nlog_writer=SummaryWriter(tb_dir)\n\nprint('Starting Training')\nstep=50\n\nKK=len(train_loader)\n\nwt=1\nfor epoch in range(0,args.epoch):\n\n\tnetG.train(); netD.train()\n\n\tlG, lD, GenL, DisL_r, DisL_f, alL, fgL, compL, elapse_run, elapse=0,0,0,0,0,0,0,0,0,0\n\n\tt0=time.time();\n\n\n\tfor i,data in enumerate(train_loader):\n\t\t#Initiating\n\n\t\tbg, image, seg, multi_fr, seg_gt, back_rnd =  data['bg'], data['image'], data['seg'], data['multi_fr'], data['seg-gt'], data['back-rnd']\n\n\t\tbg, image, seg, multi_fr, seg_gt, back_rnd = Variable(bg.cuda()), Variable(image.cuda()), Variable(seg.cuda()), Variable(multi_fr.cuda()), Variable(seg_gt.cuda()), Variable(back_rnd.cuda())\n\n\t\tmask0=Variable(torch.ones(seg.shape).cuda())\n\n\t\ttr0=time.time()\n\n\t\t#pseudo-supervision\n\t\talpha_pred_sup,fg_pred_sup=netB(image,bg,seg,multi_fr)\n\t\tmask=(alpha_pred_sup>-0.98).type(torch.cuda.FloatTensor)\n\n\t\tmask1=(seg_gt>0.95).type(torch.cuda.FloatTensor)\n\n\t\t## Train Generator\n\n\t\talpha_pred,fg_pred=netG(image,bg,seg,multi_fr)\n\n\t\t##pseudo-supervised losses\n\t\tal_loss=l1_loss(alpha_pred_sup,alpha_pred,mask0)+0.5*g_loss(alpha_pred_sup,alpha_pred,mask0)\n\t\tfg_loss=l1_loss(fg_pred_sup,fg_pred,mask)\n\n\t\t#compose into same background\n\t\tcomp_loss= c_loss(image,alpha_pred,fg_pred,bg,mask1)\n\n\t\t#randomly permute the background\n\t\tperm=torch.LongTensor(np.random.permutation(bg.shape[0]))\n\t\tbg_sh=bg[perm,:,:,:]\n\n\t\tal_mask=(alpha_pred>0.95).type(torch.cuda.FloatTensor)\n\n\t\t#Choose the target background for composition\n\t\t#back_rnd: contains separate set of background videos captured\n\t\t#bg_sh: contains randomly permuted captured background from the same minibatch\n\t\tif np.random.random_sample() > 0.5:\n\t\t\tbg_sh=back_rnd\n\n\t\timage_sh=compose_image_withshift(alpha_pred,image*al_mask + fg_pred*(1-al_mask),bg_sh,seg) \n\n\t\tfake_response=netD(image_sh)\n\n\t\tloss_ganG=GAN_loss(fake_response,label_type=True)\n\n\t\tlossG= loss_ganG + wt*(0.05*comp_loss+0.05*al_loss+0.05*fg_loss)\n\n\t\toptimizerG.zero_grad()\n\n\t\tlossG.backward()\n\t\toptimizerG.step()\n\n\t\t##Train Discriminator\n\t\t\n\t\tfake_response=netD(image_sh); real_response=netD(image)\n\n\t\tloss_ganD_fake=GAN_loss(fake_response,label_type=False)\n\t\tloss_ganD_real=GAN_loss(real_response,label_type=True)\n\n\t\tlossD=(loss_ganD_real+loss_ganD_fake)*0.5\n\n\t\t# Update discriminator for every 5 generator update\n\t\tif i%5 ==0:\n\t\t\toptimizerD.zero_grad()\n\t\t\tlossD.backward()\n\t\t\toptimizerD.step()\n\n\n\t\tlG += lossG.data\n\t\tlD += lossD.data\n\t\tGenL += loss_ganG.data\n\t\tDisL_r += loss_ganD_real.data\n\t\tDisL_f += loss_ganD_fake.data\n\n\t\talL += al_loss.data\n\t\tfgL += fg_loss.data\n\t\tcompL += comp_loss.data\n\n\n\t\tlog_writer.add_scalar('Generator Loss', lossG.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('Discriminator Loss', lossD.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('Generator Loss: Fake', loss_ganG.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('Discriminator Loss: Real', loss_ganD_real.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('Discriminator Loss: Fake', loss_ganD_fake.data, epoch*KK + i + 1)\n\n\t\tlog_writer.add_scalar('Generator Loss: Alpha', al_loss.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('Generator Loss: Fg', fg_loss.data, epoch*KK + i + 1)\n\t\tlog_writer.add_scalar('Generator Loss: Comp', comp_loss.data, epoch*KK + i + 1)\n\n\n\n\t\tt1=time.time()\n\n\t\telapse +=t1 -t0\n\t\telapse_run += t1-tr0\n\t\tt0=t1\n\n\t\tif i % step == (step-1):\n\t\t\tprint('[%d, %5d] Gen-loss:  %.4f Disc-loss: %.4f Alpha-loss: %.4f Fg-loss: %.4f Comp-loss: %.4f Time-all: %.4f Time-fwbw: %.4f' %(epoch + 1, i + 1, lG/step,lD/step,alL/step,fgL/step,compL/step,elapse/step,elapse_run/step))\n\t\t\tlG, lD, GenL, DisL_r, DisL_f, alL, fgL, compL, elapse_run, elapse=0,0,0,0,0,0,0,0,0,0\n\n\t\t\twrite_tb_log(image,'image',log_writer,i)\n\t\t\twrite_tb_log(seg,'seg',log_writer,i)\n\t\t\twrite_tb_log(alpha_pred_sup,'alpha-sup',log_writer,i)\n\t\t\twrite_tb_log(alpha_pred,'alpha_pred',log_writer,i)\n\t\t\twrite_tb_log(fg_pred_sup*mask,'fg-pred-sup',log_writer,i)\n\t\t\twrite_tb_log(fg_pred*mask,'fg_pred',log_writer,i)\n\n\t\t\t#composition\n\t\t\talpha_pred=(alpha_pred+1)/2\n\t\t\tcomp=fg_pred*alpha_pred + (1-alpha_pred)*bg\n\t\t\twrite_tb_log(comp,'composite-same',log_writer,i)\n\t\t\twrite_tb_log(image_sh,'composite-diff',log_writer,i)\n\n\t\t\t\n\t\t\tdel comp\n\n\t\tdel mask, back_rnd, mask0, seg_gt, mask1, bg, alpha_pred, alpha_pred_sup, image, fg_pred_sup, fg_pred, seg, multi_fr,image_sh, bg_sh, fake_response, real_response, al_loss, fg_loss, comp_loss, lossG, lossD, loss_ganD_real, loss_ganD_fake, loss_ganG\n\n\n\t\n\tif (epoch%2 == 0):\n\t\ttorch.save(netG.state_dict(), model_dir + 'netG_epoch_%d.pth' %(epoch))\n\t\ttorch.save(optimizerG.state_dict(), model_dir + 'optimG_epoch_%d.pth' %(epoch))\n\t\ttorch.save(netD.state_dict(), model_dir + 'netD_epoch_%d.pth' %(epoch))\n\t\ttorch.save(optimizerD.state_dict(), model_dir + 'optimD_epoch_%d.pth' %(epoch))\n\n\t\t#Change weight every 2 epoch to put more stress on discriminator weight and less on pseudo-supervision\n\t\twt=wt/2\n"
        }
      ]
    }
  ]
}