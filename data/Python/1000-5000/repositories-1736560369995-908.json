{
  "metadata": {
    "timestamp": 1736560369995,
    "page": 908,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "tensorforce/tensorforce",
      "stars": 3300,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.24609375,
          "content": "*__pycache__*\n*.DS_Store\n*.egg\n*.pyc\n*.swp\n*.vscode\n\n*/.ipynb_checkpoints/\n\n/CONTRIBUTING.html\n/PROJECTS.html\n/README.html\n/UPDATE_NOTES.html\n\n/.eggs/\n/.pytest_cache/\n/_vizdoom/\n/build/\n/dist/\n/docs/_*\n/examples/vizdoom/\n/tensorforce.egg-info/\n\n/_old*\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.3251953125,
          "content": "language: python\npython:\n  - \"3.7\"\n\nbranches:\n  only:\n    - master\n\nbefore_install:\n  - sudo apt-get -y install swig\n\ninstall:\n  - pip install .[tfa,tune,ale,gym,retro]\n  - pip install pytest\n\nscript:\n  - pytest\n\nnotifications:\n  email:\n    recipients:\n      - tensorforce.team@gmail.com\n    on_success: never\n    on_failure: always\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.4541015625,
          "content": "# Contribution guide\n\nPlease always get in touch on [Gitter](https://gitter.im/tensorforce/community) before start working on a pull request, unless it is a smaller bug fix involving only a few lines of code.\n\n\n### Code style\n\n- [Google Python style guide](https://google.github.io/styleguide/pyguide.html)\n- Maximum line length: 100 characters; tab size: 4 spaces\n- There should be no PEP8 warnings (apart from E501 regarding line length)\n- If arguments, when initializing objects / calling functions / specifying lists/dicts / etc, do not fit into the same line, should be in one (or multiple) separate tab-indented line(s), like this:\n\n```python\nsuper().__init__(\n    states=states, actions=actions, l2_regularization=l2_regularization,\n    parallel_interactions=parallel_interactions, config=config, saver=saver, summarizer=summarizer\n)\n```\n\n- TensorFlow as well as Tensorforce-internal function calls should use named arguments wherever possible\n- Binary operators should always be surrounded by a single space, so `z = x + y` instead of `z=x+y`\n- Numbers should always be specified according to their intended type, so `1.0` as opposed to `1` in the case of floats, and vice versa for integers. For clarity, floats should furthermore add single leading/trailing zeros where necessary, so `1.0` instead of `1.` and `0.1` instead of `.1`.\n- Line comments should generally be in a separate line preceding the line(s) they are commenting on, and not be added after the code as a suffix.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.134765625,
          "content": "Copyright 2018 Tensorforce Team. All Rights Reserved.\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2018 Tensorforce Team.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "PROJECTS.md",
          "type": "blob",
          "size": 6.7353515625,
          "content": "# Projects using Tensorforce\n\nPlease [get in touch](mailto:tensorforce.team@gmail.com) if you would like your project to be listed here.\n\n\n\n\n### Active flow control\n<img width=400 style=\"float: right;\" src=\"data/active_flow_control.gif\">\n\nTensorforce has been used within fluid mechanics to perform active flow control. Active flow control is known to be challenging due the combination of non linearity, high dimensionality, and time dependence implied by fluid mechanics, and therefore DRL is a promising new tool within this research field.\n\nThis project performs flow control of the 2D Kármán Vortex Street with Deep Reinforcement Learning. The simulations are done with FEniCS, while the Reinforcement Learning is performed with the help of the library Tensorforce. You will need Fenics, Tensorflow, Tensorforce and Gmsh available on your system in order to be able to run the code.\n\n[Paper](https://arxiv.org/abs/1808.07664)&nbsp;&nbsp;|&nbsp;&nbsp;[GitHub Project](https://github.com/jerabaul29/Cylinder2DFlowControlDRL)\n\n\n\n\n### DeepCrawl\n<img width=400 style=\"float: right;\" src=\"data/deepcrawl.gif\">\n\nDeepCrawl is a turn-based strategy game for mobile platforms, where all the enemies are trained with Deep Reinforcement Learning algorithms. The game is designed to be hard, yet fair: the player will have to explore the dungeons and defeat all the guardians of the rooms, paying attention to every moves the AI does!\n\nThe game was developed in Unity, while the AI was built through Tensorforce and Unity ML-Agents.\n\nThe project was part of a Master thesis in Computer Engineering at Università degli Studi di Firenze, with title *\"DeepCrawl: Deep Reinforcement Learning for turn-based strategy games\"*.\n\n[GitHub Project](https://github.com/SestoAle/DeepCrawl)\n\n\n\n\n### SimPyFab\n<img width=400 style=\"float: right;\" src=\"https://github.com/AndreasKuhnle/SimRLFab/blob/master/docu/layout.png?raw=true\">\n\nComplex job shop manufacturing systems are motivated by the manufacturing characteristics of the semiconductor wafer fabrication. A job shop consists of several machines (processing resources) that process jobs (products, orders) based on a defined list or process steps. After every process, the job is dispatched and transported to the next processing machine. Machines are usually grouped in sub-areas by the type processing type, i.e. similar processing capabilities are next to each other.\n\nThis framework provides an integrated simulation and reinforcement learning model to investigate the potential of data-driven reinforcement learning in production planning and control of complex job shop systems. The simulation model allows parametrization of a broad range of job shop-like manufacturing systems. Furthermore, performance statistics and logging of performance indicators are provided. Reinforcement learning is implemented to control the order dispatching and several dispatchin heuristics provide benchmarks that are used in practice.\n\n[GitHub Project](https://github.com/AndreasKuhnle/SimRLFab)\n\n\n\n\n### Navbot: Using RGB Image as Visual Input for Mapless Robot Navigation\n<img width=400 style=\"float: right;\" src=\"data/navbot.gif\">\n\nA collection for mapless robot navigation using RGB image as visual input. It contains the test environment and motion planners, aiming at realizing all the three levels of mapless navigation:\n\n1. memorizing efficiently; \n2. from memorizing to reasoning; \n3. more powerful reasoning\n\n[GitHub Project](https://github.com/marooncn/navbot)\n\n\n\n\n### Adaptive Behavior Generation for Autonomous Driving\n<img width=400 style=\"float: right;\" src=\"data/adaptive_behavior_generation_for_autonomous_driving.png\">\n\nMaking the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes\nof desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations\n\n[Paper](https://arxiv.org/abs/1809.03214)\n\n\n\n\n### Bitcoin trading bot\n<img width=400 style=\"float: right;\" src=\"data/bitcoin_trading_bot.png\">\n\nThis project is a Tensorforce-based Bitcoin trading bot (algo-trader). It uses deep reinforcement learning to automatically buy/sell/hold BTC based on what it learns about BTC price history. Most blogs / tutorials / boilerplate BTC trading-bots you'll find out there use supervised machine learning, likely an LTSM. That's well and good - supervised learning learns what makes a time-series tick so it can predict the next-step future. But that's where it stops. It says \"the price will go up next\", but it doesn't tell you what to do. Well that's simple, buy, right? Ah, buy low, sell high - it's not that simple. Thousands of lines of code go into trading rules, \"if this then that\" style. Reinforcement learning takes supervised to the next level - it embeds supervised within its architecture, and then decides what to do. It's beautiful stuff!\n\nThis project goes with Episode 26+ of [Machine Learning Guide](http://ocdevel.com/mlg). Those episodes are tutorial for this project; including an intro to Deep RL, hyperparameter decisions, etc.\n\n[GitHub Project](https://github.com/lefnire/tforce_btc_trader)\n\n\n\n\n### TensorTrade: Trade Efficiently with Reinforcement Learning\n<img width=400 style=\"float: right;\" src=\"https://raw.githubusercontent.com/notadamking/tensortrade/master/docs/source/_static/logo.jpg\">\n\nTensorTrade is an open source Python framework for building, training, evaluating, and deploying robust trading algorithms using reinforcement learning. The framework focuses on being highly composable and extensible, to allow the system to scale from simple trading strategies on a single CPU, to complex investment strategies run on a distribution of HPC machines.\n\nUnder the hood, the framework uses many of the APIs from existing machine learning libraries to maintain high quality data pipelines and learning models. One of the main goals of TensorTrade is to enable fast experimentation with algorithmic trading strategies, by leveraging the existing tools and pipelines provided by numpy, pandas, gym, keras, and tensorflow.\n\n[GitHub Project](https://github.com/notadamking/tensortrade)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.4345703125,
          "content": "# Tensorforce: a TensorFlow library for applied reinforcement learning\n\n[![Docs](https://readthedocs.org/projects/tensorforce/badge)](http://tensorforce.readthedocs.io/en/latest/)\n[![Gitter](https://badges.gitter.im/tensorforce/community.svg)](https://gitter.im/tensorforce/community)\n[![Build Status](https://travis-ci.com/tensorforce/tensorforce.svg?branch=master)](https://travis-ci.com/tensorforce/tensorforce)\n[![pypi version](https://img.shields.io/pypi/v/tensorforce)](https://pypi.org/project/Tensorforce/)\n[![python version](https://img.shields.io/pypi/pyversions/tensorforce)](https://pypi.org/project/Tensorforce/)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/tensorforce/tensorforce/blob/master/LICENSE)\n[![Donate](https://img.shields.io/badge/donate-GitHub_Sponsors-yellow)](https://github.com/sponsors/AlexKuhnle)\n[![Donate](https://img.shields.io/badge/donate-Liberapay-yellow)](https://liberapay.com/TensorforceTeam/donate)\n\n\n**This project is not maintained any longer!**\n\n\n#### Introduction\n\nTensorforce is an open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice. Tensorforce is built on top of [Google's TensorFlow framework](https://www.tensorflow.org/) and requires Python 3.\n\nTensorforce follows a set of high-level design choices which differentiate it from other similar libraries:\n\n- **Modular component-based design**: Feature implementations, above all, strive to be as generally applicable and configurable as possible, potentially at some cost of faithfully resembling details of the introducing paper.\n- **Separation of RL algorithm and application**: Algorithms are agnostic to the type and structure of inputs (states/observations) and outputs (actions/decisions), as well as the interaction with the application environment.\n- **Full-on TensorFlow models**: The entire reinforcement learning logic, including control flow, is implemented in TensorFlow, to enable portable computation graphs independent of application programming language, and to facilitate the deployment of models.\n\n\n\n#### Quicklinks\n\n- [Documentation](http://tensorforce.readthedocs.io) and [update notes](https://github.com/tensorforce/tensorforce/blob/master/UPDATE_NOTES.md)\n- [Contact](mailto:tensorforce.team@gmail.com) and [Gitter channel](https://gitter.im/tensorforce/community)\n- [Benchmarks](https://github.com/tensorforce/tensorforce/blob/master/benchmarks) and [projects using Tensorforce](https://github.com/tensorforce/tensorforce/blob/master/PROJECTS.md)\n- [Roadmap](https://github.com/tensorforce/tensorforce/blob/master/ROADMAP.md) and [contribution guidelines](https://github.com/tensorforce/tensorforce/blob/master/CONTRIBUTING.md)\n- [GitHub Sponsors](https://github.com/sponsors/AlexKuhnle) and [Liberapay](https://liberapay.com/TensorforceTeam/donate)\n\n\n\n#### Table of content\n\n- [Installation](#installation)\n- [Quickstart example code](#quickstart-example-code)\n- [Command line usage](#command-line-usage)\n- [Features](#features)\n- [Environment adapters](#environment-adapters)\n- [Support, feedback and donating](#support-feedback-and-donating)\n- [Core team and contributors](#core-team-and-contributors)\n- [Cite Tensorforce](#cite-tensorforce)\n\n\n\n## Installation\n\nA stable version of Tensorforce is periodically updated on PyPI and installed as follows:\n\n```bash\npip3 install tensorforce\n```\n\nTo always use the latest version of Tensorforce, install the GitHub version instead:\n\n```bash\ngit clone https://github.com/tensorforce/tensorforce.git\npip3 install -e tensorforce\n```\n\n**Note on installation on M1 Macs:** At the moment Tensorflow, which is a core dependency of Tensorforce, cannot be installed on M1 Macs directly. Follow the [\"M1 Macs\" section](https://tensorforce.readthedocs.io/en/latest/basics/installation.html) in the documentation for a workaround.\n\nEnvironments require additional packages for which there are setup options available (`ale`, `gym`, `retro`, `vizdoom`, `carla`; or `envs` for all environments), however, some require additional tools to be installed separately (see [environments documentation](http://tensorforce.readthedocs.io)). Other setup options include `tfa` for [TensorFlow Addons](https://www.tensorflow.org/addons) and `tune` for [HpBandSter](https://github.com/automl/HpBandSter) required for the `tune.py` script.\n\n**Note on GPU usage:** Different from (un)supervised deep learning, RL does not always benefit from running on a GPU, depending on environment and agent configuration. In particular for environments with low-dimensional state spaces (i.e., no images), it is hence worth trying to run on CPU only.\n\n\n\n## Quickstart example code\n\n```python\nfrom tensorforce import Agent, Environment\n\n# Pre-defined or custom environment\nenvironment = Environment.create(\n    environment='gym', level='CartPole', max_episode_timesteps=500\n)\n\n# Instantiate a Tensorforce agent\nagent = Agent.create(\n    agent='tensorforce',\n    environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n    memory=10000,\n    update=dict(unit='timesteps', batch_size=64),\n    optimizer=dict(type='adam', learning_rate=3e-4),\n    policy=dict(network='auto'),\n    objective='policy_gradient',\n    reward_estimation=dict(horizon=20)\n)\n\n# Train for 300 episodes\nfor _ in range(300):\n\n    # Initialize episode\n    states = environment.reset()\n    terminal = False\n\n    while not terminal:\n        # Episode timestep\n        actions = agent.act(states=states)\n        states, terminal, reward = environment.execute(actions=actions)\n        agent.observe(terminal=terminal, reward=reward)\n\nagent.close()\nenvironment.close()\n```\n\n\n\n## Command line usage\n\nTensorforce comes with a range of [example configurations](https://github.com/tensorforce/tensorforce/tree/master/benchmarks/configs) for different popular reinforcement learning environments. For instance, to run Tensorforce's implementation of the popular [Proximal Policy Optimization (PPO) algorithm](https://arxiv.org/abs/1707.06347) on the [OpenAI Gym CartPole environment](https://gym.openai.com/envs/CartPole-v1/), execute the following line:\n\n```bash\npython3 run.py --agent benchmarks/configs/ppo.json --environment gym \\\n    --level CartPole-v1 --episodes 100\n```\n\nFor more information check out the [documentation](http://tensorforce.readthedocs.io).\n\n\n\n## Features\n\n- **Network layers**: Fully-connected, 1- and 2-dimensional convolutions, embeddings, pooling, RNNs, dropout, normalization, and more; *plus* support of Keras layers.\n- **Network architecture**: Support for multi-state inputs and layer (block) reuse, simple definition of directed acyclic graph structures via register/retrieve layer, plus support for arbitrary architectures.\n- **Memory types**: Simple batch buffer memory, random replay memory.\n- **Policy distributions**: Bernoulli distribution for boolean actions, categorical distribution for (finite) integer actions, Gaussian distribution for continuous actions, Beta distribution for range-constrained continuous actions, multi-action support.\n- **Reward estimation**: Configuration options for estimation horizon, future reward discount, state/state-action/advantage estimation, and for whether to consider terminal and horizon states.\n- **Training objectives**: (Deterministic) policy gradient, state-(action-)value approximation.\n- **Optimization algorithms**: Various gradient-based optimizers provided by TensorFlow like Adam/AdaDelta/RMSProp/etc, evolutionary optimizer, natural-gradient-based optimizer, plus a range of meta-optimizers.\n- **Exploration**: Randomized actions, sampling temperature, variable noise.\n- **Preprocessing**: Clipping, deltafier, sequence, image processing.\n- **Regularization**: L2 and entropy regularization.\n- **Execution modes**: Parallelized execution of multiple environments based on Python's `multiprocessing` and `socket`.\n- **Optimized act-only SavedModel extraction**.\n- **TensorBoard support**.\n\nBy combining these modular components in different ways, a variety of popular deep reinforcement learning models/features can be replicated:\n\n- Q-learning: [Deep Q-learning](https://www.nature.com/articles/nature14236), [Double-DQN](https://arxiv.org/abs/1509.06461), [Dueling DQN](https://arxiv.org/abs/1511.06581), [n-step DQN](https://arxiv.org/abs/1602.01783), [Normalised Advantage Function (NAF)](https://arxiv.org/abs/1603.00748)\n- Policy gradient: [vanilla policy-gradient / REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf), [Actor-critic and A3C](https://arxiv.org/abs/1602.01783), [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347), [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477), [Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971)\n\nNote that in general the replication is not 100% faithful, since the models as described in the corresponding paper often involve additional minor tweaks and modifications which are hard to support with a modular design (and, arguably, also questionable whether it is important/desirable to support them). On the upside, these models are just a few examples from the multitude of module combinations supported by Tensorforce.\n\n\n\n## Environment adapters\n\n- [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment), a simple object-oriented framework that allows researchers and hobbyists to develop AI agents for Atari 2600 games.\n- [CARLA](https://github.com/carla-simulator/carla), is an open-source simulator for autonomous driving research.\n- [OpenAI Gym](https://gym.openai.com/), a toolkit for developing and comparing reinforcement learning algorithms which supports teaching agents everything from walking to playing games like Pong or Pinball.\n- [OpenAI Retro](https://github.com/openai/retro), lets you turn classic video games into Gym environments for reinforcement learning and comes with integrations for ~1000 games.\n- [OpenSim](http://osim-rl.stanford.edu/), reinforcement learning with musculoskeletal models.\n- [PyGame Learning Environment](https://github.com/ntasfi/PyGame-Learning-Environment/), learning environment which allows a quick start to Reinforcement Learning in Python.\n- [ViZDoom](https://github.com/mwydmuch/ViZDoom), allows developing AI bots that play Doom using only the visual information.\n\n\n## Support, feedback and donating\n\nPlease get in touch via [mail](mailto:tensorforce.team@gmail.com) or on [Gitter](https://gitter.im/tensorforce/community) if you have questions, feedback, ideas for features/collaboration, or if you seek support for applying Tensorforce to your problem.\n\nIf you want to support the Tensorforce core team (see below), please also consider donating: [GitHub Sponsors](https://github.com/sponsors/AlexKuhnle) or [Liberapay](https://liberapay.com/TensorforceTeam/donate).\n\n\n\n## Core team and contributors\n\nTensorforce is currently developed and maintained by [Alexander Kuhnle](https://github.com/AlexKuhnle).\n\nEarlier versions of Tensorforce (<= 0.4.2) were developed by [Michael Schaarschmidt](https://github.com/michaelschaarschmidt), [Alexander Kuhnle](https://github.com/AlexKuhnle) and [Kai Fricke](https://github.com/krfricke).\n\nThe advanced parallel execution functionality was originally contributed by Jean Rabault (@jerabaul29) and Vincent Belus (@vbelus). Moreover, the pretraining feature was largely developed in collaboration with Hongwei Tang (@thw1021) and Jean Rabault (@jerabaul29).\n\nThe CARLA environment wrapper is currently developed by Luca Anzalone (@luca96).\n\nWe are very grateful for our open-source contributors (listed according to Github, updated periodically):\n\nIslandman93, sven1977, Mazecreator, wassname, lefnire, daggertye, trickmeyer, mkempers,\nmryellow, ImpulseAdventure,\njanislavjankov, andrewekhalel,\nHassamSheikh, skervim,\nbeflix, coord-e,\nbenelot, tms1337, vwxyzjn, erniejunior,\nDeathn0t, petrbel, nrhodes, batu, yellowbee686, tgianko,\nAdamStelmaszczyk, BorisSchaeling, christianhidber, Davidnet, ekerazha, gitter-badger, kborozdin, Kismuz, mannsi, milesmcc, nagachika, neitzal, ngoodger, perara, sohakes, tomhennigan.\n\n\n\n## Cite Tensorforce\n\nPlease cite the framework as follows:\n\n```\n@misc{tensorforce,\n  author       = {Kuhnle, Alexander and Schaarschmidt, Michael and Fricke, Kai},\n  title        = {Tensorforce: a TensorFlow library for applied reinforcement learning},\n  howpublished = {Web page},\n  url          = {https://github.com/tensorforce/tensorforce},\n  year         = {2017}\n}\n```\n\nIf you use the [parallel execution functionality](https://github.com/tensorforce/tensorforce/tree/master/tensorforce/contrib), please additionally cite it as follows:\n\n```\n@article{rabault2019accelerating,\n  title        = {Accelerating deep reinforcement learning strategies of flow control through a multi-environment approach},\n  author       = {Rabault, Jean and Kuhnle, Alexander},\n  journal      = {Physics of Fluids},\n  volume       = {31},\n  number       = {9},\n  pages        = {094105},\n  year         = {2019},\n  publisher    = {AIP Publishing}\n}\n```\n\nIf you use Tensorforce in your research, you may additionally consider citing the following paper:\n\n```\n@article{lift-tensorforce,\n  author       = {Schaarschmidt, Michael and Kuhnle, Alexander and Ellis, Ben and Fricke, Kai and Gessert, Felix and Yoneki, Eiko},\n  title        = {{LIFT}: Reinforcement Learning in Computer Systems by Learning From Demonstrations},\n  journal      = {CoRR},\n  volume       = {abs/1808.07903},\n  year         = {2018},\n  url          = {http://arxiv.org/abs/1808.07903},\n  archivePrefix = {arXiv},\n  eprint       = {1808.07903}\n}\n```\n"
        },
        {
          "name": "ROADMAP.md",
          "type": "blob",
          "size": 0.7421875,
          "content": "# Roadmap\n\n1. RNN policies\n    - Extend configurability\n    - Allow recurrent baselines\n    - More RNN modules, incl Transformer\n2. Reward estimation extensions\n    - Auxiliary losses\n    - Curiosity\n    - Imitation learning\n    - Distributional perspective\n3. State/action modeling\n    - Sequence states/actions\n    - State-dependent actions\n    - Conditional/hierarchical actions\n4. Memory architecture\n    - Optimize retrieval of sequences\n    - Use TensorArray\n    - Improve other limitations\n5. CARLA environment\n    - Docs and assertions \n    - World's map loading (e.g. random, specific, etc.)\n    - Weather support\n    - Pretraining and Free play (e.g. for data collection)\n    - State space with a temporal component.\n    - ...\n6. To be determined...\n"
        },
        {
          "name": "UPDATE_NOTES.md",
          "type": "blob",
          "size": 18.572265625,
          "content": "# Update notes\n\nThis file records all major updates and new features, starting from version 0.5. As Tensorforce is still developing, updates and bug fixes for the internal architecture are continuously being implemented, which will not be tracked here in detail.\n\n\n\n### Latest changes\n\n\n---\n\n\n### Version 0.6.5\n\n##### Agents:\n- Renamed agent argument `reward_preprocessing` to `reward_processing`, and in case of Tensorforce agent moved to `reward_estimation[reward_processing]`\n\n##### Distributions:\n- New `categorical` distribution argument `skip_linear` to not add the implicit linear logits layer\n\n##### Environments:\n- Support for multi-actor parallel environments via new function `Environment.num_actors()`\n    - `Runner` uses multi-actor parallelism by default if environment is multi-actor\n- New optional `Environment` function `episode_return()` which returns the true return of the last episode, if cumulative sum of environment rewards is not a good metric for runner display\n\n##### Examples:\n- New `vectorized_environment.py` and `multiactor_environment.py` script to illustrate how to setup a vectorized/multi-actor environment.\n\n\n---\n\n\n### Version 0.6.4\n\n##### Agents:\n- Agent argument `update_frequency` / `update[frequency]` now supports float values > 0.0, which specify the update-frequency relative to the batch-size\n- Changed default value for argument `update_frequency` from `1.0` to `0.25` for DQN, DoubleDQN, DuelingDQN agents\n- New argument `return_processing` and `advantage_processing` (where applicable) for all agent sub-types\n- New function `Agent.get_specification()` which returns the agent specification as dictionary\n- New function `Agent.get_architecture()` which returns a string representation of the network layer architecture\n\n##### Modules:\n- Improved and simplified module specification, for instance: `network=my_module` instead of `network=my_module.TestNetwork`, or `environment=envs.custom_env` instead of `environment=envs.custom_env.CustomEnvironment` (module file needs to be in the same directory or a sub-directory)\n\n##### Networks:\n- New argument `single_output=True` for some policy types which, if `False`, allows the specification of additional network outputs for some/all actions via registered tensors\n- `KerasNetwork` argument `model` now supports arbitrary functions as long as they return a `tf.keras.Model`\n\n##### Layers:\n- New layer type `SelfAttention` (specification key: `self_attention`)\n\n##### Parameters:\n- Support tracking of non-constant parameter values\n\n##### Runner:\n- Rename attribute `episode_rewards` as `episode_returns`, and TQDM status `reward` as `return`\n- Extend argument `agent` to support `Agent.load()` keyword arguments to load an existing agent instead of creating a new one.\n\n##### Examples:\n- Added `action_masking.py` example script to illustrate an environment implementation with built-in action masking.\n\n##### Buxfixes:\n- Customized device placement was not applied to most tensors\n\n\n---\n\n\n### Version 0.6.3\n\n##### Agents:\n- New agent argument `tracking` and corresponding function `tracked_tensors()` to track and retrieve the current value of predefined tensors, similar to `summarizer` for TensorBoard summaries\n- New experimental value `trace_decay` and `gae_decay` for Tensorforce agent argument `reward_estimation`, soon for other agent types as well\n- New options `\"early\"` and `\"late\"` for value `estimate_advantage` of Tensorforce agent argument `reward_estimation`\n- Changed default value for `Agent.act()` argument `deterministic` from `False` to `True`\n\n##### Networks:\n- New network type `KerasNetwork` (specification key: `keras`) as wrapper for networks specified as Keras model\n- Passing a Keras model class/object as policy/network argument is automatically interpreted as `KerasNetwork`\n\n##### Distributions:\n- Changed `Gaussian` distribution argument `global_stddev=False` to `stddev_mode='predicted'`\n- New `Categorical` distribution argument `temperature_mode=None`\n\n##### Layers:\n- New option for `Function` layer argument `function` to pass string function expression with argument \"x\", e.g. \"(x+1.0)/2.0\"\n\n##### Summarizer:\n- New summary `episode-length` recorded as part of summary label \"reward\"\n\n##### Environments:\n- Support for vectorized parallel environments via new function `Environment.is_vectorizable()` and new argument `num_parallel` for `Environment.reset()`\n    - See `tensorforce/environments.cartpole.py` for a vectorizable environment example\n    - `Runner` uses vectorized parallelism by default if `num_parallel > 1`, `remote=None` and environment supports vectorization\n    - See `examples/act_observe_vectorized.py` for more details on act-observe interaction\n- New extended and vectorizable custom CartPole environment via key `custom_cartpole` (work in progress)\n- New environment argument `reward_shaping` to provide a simple way to modify/shape rewards of an environment, can be specified either as callable or string function expression\n\n##### run.py script:\n- New option for command line arguments `--checkpoints` and `--summaries` to add comma-separated checkpoint/summary filename in addition to directory\n- Added episode lengths to logging plot besides episode returns\n\n##### Buxfixes:\n- Temporal horizon handling of RNN layers\n- Critical bugfix for late horizon value prediction (including DQN variants and DPG agent) in combination with baseline RNN\n- GPU problems with scatter operations\n\n\n---\n\n\n### Version 0.6.2\n\n##### Buxfixes:\n- Critical bugfix for DQN variants and DPG agent\n\n\n---\n\n\n### Version 0.6.1\n\n##### Agents:\n- Removed default value `\"adam\"` for Tensorforce agent argument `optimizer` (since default optimizer argument `learning_rate` removed, see below)\n- Removed option `\"minimum\"` for Tensorforce agent argument `memory`, use `None` instead\n- Changed default value for `dqn`/`double_dqn`/`dueling_dqn` agent argument `huber_loss` from `0.0` to `None`\n\n##### Layers:\n- Removed default value `0.999` for `exponential_normalization` layer argument `decay`\n- Added new layer `batch_normalization` (generally should only be used for the agent arguments `reward_processing[return_processing]` and `reward_processing[advantage_processing]`)\n- Added `exponential/instance_normalization` layer argument `only_mean` with default `False`\n- Added `exponential/instance_normalization` layer argument `min_variance` with default `1e-4`\n\n##### Optimizers:\n- Removed default value `1e-3` for optimizer argument `learning_rate`\n- Changed default value for optimizer argument `gradient_norm_clipping` from `1.0` to `None` (no gradient clipping)\n- Added new optimizer `doublecheck_step` and corresponding argument `doublecheck_update` for optimizer wrapper\n- Removed `linesearch_step` optimizer argument `accept_ratio`\n- Removed `natural_gradient` optimizer argument `return_improvement_estimate`\n\n##### Saver:\n- Added option to specify agent argument `saver` as string, which is interpreted as `saver[directory]` with otherwise default values\n- Added default value for agent argument `saver[frequency]` as `10` (save model every 10 updates by default)\n- Changed default value of agent argument `saver[max_checkpoints]` from `5` to `10`\n\n##### Summarizer:\n- Added option to specify agent argument `summarizer` as string, which is interpreted as `summarizer[directory]` with otherwise default values\n- Renamed option of agent argument `summarizer` from `summarizer[labels]` to `summarizer[summaries]` (use of the term \"label\" due to earlier version, outdated and confusing by now)\n- Changed interpretation of agent argument `summarizer[summaries] = \"all\"` to include only numerical summaries, so all summaries except \"graph\"\n- Changed default value of agent argument `summarizer[summaries]` from `[\"graph\"]` to `\"all\"`\n- Changed default value of agent argument `summarizer[max_summaries]` from `5` to `7` (number of different colors in TensorBoard)\n- Added option `summarizer[filename]` to agent argument `summarizer`\n\n##### Recorder:\n- Added option to specify agent argument `recorder` as string, which is interpreted as `recorder[directory]` with otherwise default values\n\n##### run.py script:\n- Added `--checkpoints`/`--summaries`/`--recordings` command line argument to enable saver/summarizer/recorder agent argument specification separate from core agent configuration\n\n##### Examples:\n- Added `save_load_agent.py` example script to illustrate regular agent saving and loading\n\n##### Buxfixes:\n- Fixed problem with optimizer argument `gradient_norm_clipping` not being applied correctly\n- Fixed problem with `exponential_normalization` layer not updating moving mean and variance correctly\n- Fixed problem with `recent` memory for timestep-based updates sometimes sampling invalid memory indices\n\n\n---\n\n\n### Version 0.6\n\n- Removed agent arguments `execution`, `buffer_observe`, `seed`\n- Renamed agent arguments `baseline_policy`/`baseline_network`/`critic_network` to `baseline`/`critic`\n- Renamed agent `reward_estimation` arguments `estimate_horizon` to `predict_horizon_values`, `estimate_actions` to `predict_action_values`, `estimate_terminal` to `predict_terminal_values`\n- Renamed agent argument `preprocessing` to `state_preprocessing`\n- Default agent preprocessing `linear_normalization`\n- Moved agent arguments for reward/return/advantage processing from `preprocessing` to `reward_preprocessing` and `reward_estimation[return_/advantage_processing]`\n- New agent argument `config` with values `buffer_observe`, `enable_int_action_masking`, `seed`\n- Renamed PPO/TRPO/DPG argument `critic_network`/`_optimizer` to `baseline`/`baseline_optimizer`\n- Renamed PPO argument `optimization_steps` to `multi_step`\n- New TRPO argument `subsampling_fraction`\n- Changed agent argument `use_beta_distribution` default to false\n- Added double DQN agent (`double_dqn`)\n- Removed `Agent.act()` argument `evaluation`\n- Removed agent function arguments `query` (functionality removed)\n- Agent saver functionality changed (Checkpoint/SavedModel instead of Saver/Protobuf): `save`/`load` functions and `saver` argument changed\n- Default behavior when specifying `saver` is not to load agent, unless agent is created via `Agent.load`\n- Agent summarizer functionality changed: `summarizer` argument changed, some summary labels and other options removed\n- Renamed RNN layers `internal_{rnn/lstm/gru}` to `rnn/lstm/gru` and `rnn/lstm/gru` to `input_{rnn/lstm/gru}`\n- Renamed `auto` network argument `internal_rnn` to `rnn`\n- Renamed `(internal_)rnn/lstm/gru` layer argument `length` to `horizon`\n- Renamed `update_modifier_wrapper` to `optimizer_wrapper`\n- Renamed `optimizing_step` to `linesearch_step`, and `UpdateModifierWrapper` argument `optimizing_iterations` to `linesearch_iterations`\n- Optimizer `subsampling_step` accepts both absolute (int) and relative (float) fractions\n- Objective `policy_gradient` argument `ratio_based` renamed to `importance_sampling`\n- Added objectives `state_value` and `action_value`\n- Added `Gaussian` distribution arguments `global_stddev` and `bounded_transform` (for improved bounded action space handling)\n- Changed default memory `device` argument to `CPU:0`\n- Renamed rewards summaries\n- `Agent.create()` accepts act-function as `agent` argument for recording\n- Singleton states and actions are now consistently handled as singletons\n- Major change to policy handling and defaults, in particular `parametrized_distributions`, new default policies `parametrized_state/action_value`\n- Combined `long` and `int` type\n- Always wrap environment in `EnvironmentWrapper` class\n- Changed `tune.py` arguments\n\n\n---\n\n\n### Version 0.5.5\n\n- Changed independent mode of `agent.act` to use final values of dynamic hyperparameters and avoid TensorFlow conditions\n- Extended `\"tensorflow\"` format of `agent.save` to include an optimized Protobuf model with an act-only graph as `.pb` file, and `Agent.load` format `\"pb-actonly\"` to load act-only agent based on Protobuf model\n- Support for custom summaries via new `summarizer` argument value `custom` to specify summary type, and `Agent.summarize(...)` to record summary values\n- Added min/max-bounds for dynamic hyperparameters min/max-bounds to assert valid range and infer other arguments\n- Argument `batch_size` now mandatory for all agent classes\n- Removed `Estimator` argument `capacity`, now always automatically inferred\n- Internal changes related to agent arguments `memory`, `update` and `reward_estimation`\n- Changed the default `bias` and `activation` argument of some layers\n- Fixed issues with `sequence` preprocessor\n- DQN and dueling DQN properly constrained to `int` actions only\n- Added `use_beta_distribution` argument with default `True` to many agents and `ParametrizedDistributions` policy, so default can be changed\n\n\n---\n\n\n### Version 0.5.4\n\n- DQN/DuelingDQN/DPG argument `memory` now required to be specified explicitly, plus `update_frequency` default changed\n- Removed (temporarily) `conv1d/conv2d_transpose` layers due to TensorFlow gradient problems\n- `Agent`, `Environment` and `Runner` can now be imported via `from tensorforce import ...`\n- New generic reshape layer available as `reshape`\n- Support for batched version of `Agent.act` and `Agent.observe`\n- Support for parallelized remote environments based on Python's `multiprocessing` and `socket` (replacing `tensorforce/contrib/socket_remote_env/` and `tensorforce/environments/environment_process_wrapper.py`), available via `Environment.create(...)`, `Runner(...)` and `run.py`\n- Removed `ParallelRunner` and merged functionality with `Runner`\n- Changed `run.py` arguments\n- Changed independent mode for `Agent.act`: additional argument `internals` and corresponding return value, initial internals via `Agent.initial_internals()`, `Agent.reset()` not required anymore\n- Removed `deterministic` argument for `Agent.act` unless independent mode\n- Added `format` argument to `save`/`load`/`restore` with supported formats `tensorflow`, `numpy` and `hdf5`\n- Changed `save` argument `append_timestep` to `append` with default `None` (instead of `'timesteps'`)\n- Added `get_variable` and `assign_variable` agent functions\n\n\n---\n\n\n### Version 0.5.3\n\n- Added optional `memory` argument to various agents\n- Improved summary labels, particularly `\"entropy\"` and `\"kl-divergence\"`\n- `linear` layer now accepts tensors of rank 1 to 3\n- Network output / distribution input does not need to be a vector anymore\n- Transposed convolution layers (`conv1d/2d_transpose`)\n- Parallel execution functionality contributed by @jerabaul29, currently under `tensorforce/contrib/`\n- Accept string for runner `save_best_agent` argument to specify best model directory different from `saver` configuration\n- `saver` argument `steps` removed and `seconds` renamed to `frequency`\n- Moved `Parallel/Runner` argument `max_episode_timesteps` from `run(...)` to constructor\n- New `Environment.create(...)` argument `max_episode_timesteps`\n- TensorFlow 2.0 support\n- Improved Tensorboard summaries recording\n- Summary labels `graph`, `variables` and `variables-histogram` temporarily not working\n- TF-optimizers updated to TensorFlow 2.0 Keras optimizers\n- Added TensorFlow Addons dependency, and support for TFA optimizers\n- Changed unit of `target_sync_frequency` from timesteps to updates for `dqn` and `dueling_dqn` agent\n\n\n---\n\n\n### Version 0.5.2\n\n- Improved unittest performance\n- Added `updates` and renamed `timesteps`/`episodes` counter for agents and runners\n- Renamed `critic_{network,optimizer}` argument to `baseline_{network,optimizer}`\n- Added Actor-Critic (`ac`), Advantage Actor-Critic (`a2c`) and Dueling DQN (`dueling_dqn`) agents\n- Improved \"same\" baseline optimizer mode and added optional weight specification\n- Reuse layer now global for parameter sharing across modules\n- New block layer type (`block`) for easier sharing of layer blocks\n- Renamed `PolicyAgent/-Model` to `TensorforceAgent/-Model`\n- New `Agent.load(...)` function, saving includes agent specification\n- Removed `PolicyAgent` argument `(baseline-)network`\n- Added policy argument `temperature`\n- Removed `\"same\"` and `\"equal\"` options for `baseline_*` arguments and changed internal baseline handling\n- Combined `state/action_value` to `value` objective with argument `value` either `\"state\"` or `\"action\"`\n\n\n---\n\n\n### Version 0.5.1\n\n- Fixed setup.py packages value\n\n\n---\n\n\n### Version 0.5.0\n\n##### Agent:\n\n- DQFDAgent removed (temporarily)\n- DQNNstepAgent and NAFAgent part of DQNAgent\n- Agents need to be initialized via `agent.initialize()` before application\n- States/actions of type `int` require an entry `num_values` (instead of `num_actions`)\n- `Agent.from_spec()` changed and renamed to `Agent.create()`\n- `Agent.act()` argument `fetch_tensors` changed and renamed to `query`, `index` renamed to `parallel`, `buffered` removed\n- `Agent.observe()` argument `index` renamed to `parallel`\n- `Agent.atomic_observe()` removed\n- `Agent.save/restore_model()` renamed to `Agent.save/restore()`\n\n##### Agent arguments:\n\n- `update_mode` renamed to `update`\n- `states_preprocessing` and `reward_preprocessing` changed and combined to `preprocessing`\n- `actions_exploration` changed and renamed to `exploration`\n- `execution` entry `num_parallel` replaced by a separate argument `parallel_interactions`\n- `batched_observe` and `batching_capacity` replaced by argument `buffer_observe`\n- `scope` renamed to `name`\n\n##### DQNAgent arguments:\n\n- `update_mode` replaced by `batch_size`, `update_frequency` and `start_updating`\n- `optimizer` removed, implicitly defined as `'adam'`, `learning_rate` added\n- `memory` defines capacity of implicitly defined memory `'replay'`\n- `double_q_model` removed (temporarily)\n\n##### Policy gradient agent arguments:\n\n- New mandatory argument `max_episode_timesteps`\n- `update_mode` replaced by `batch_size` and `update_frequency`\n- `memory` removed\n- `baseline_mode` removed\n- `baseline` argument changed and renamed to `critic_network`\n- `baseline_optimizer` renamed to `critic_optimizer`\n- `gae_lambda` removed (temporarily)\n\n##### PPOAgent arguments:\n\n- `step_optimizer` removed, implicitly defined as `'adam'`, `learning_rate` added\n\n##### TRPOAgent arguments:\n\n- `cg_*` and `ls_*` arguments removed\n\n##### VPGAgent arguments:\n\n- `optimizer` removed, implicitly defined as `'adam'`, `learning_rate` added\n\n##### Environment:\n\n- Environment properties `states` and `actions` are now functions `states()` and `actions()`\n- States/actions of type `int` require an entry `num_values` (instead of `num_actions`)\n- New function `Environment.max_episode_timesteps()`\n\n##### Contrib environments:\n\n- ALE, MazeExp, OpenSim, Gym, Retro, PyGame and ViZDoom moved to `tensorforce.environments`\n- Other environment implementations removed (may be upgraded in the future)\n\n##### Runners:\n\n- Improved `run()` API for `Runner` and `ParallelRunner`\n- `ThreadedRunner` removed\n\n##### Other:\n\n- `examples` folder (including `configs`) removed, apart from `quickstart.py`\n- New `benchmarks` folder to replace parts of old `examples` folder\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements-all.txt",
          "type": "blob",
          "size": 0.40234375,
          "content": "gym >= 0.21.0, <0.23\nh5py >= 3.6.0\nmatplotlib >= 3.5.1\nmsgpack >= 1.0.3\nmsgpack-numpy >= 0.4.7.1\nnumpy ~= 1.21.5\nPillow >= 9.0.0\ntensorflow == 2.12.1\ntqdm >= 4.62.3\ntensorflow-addons >= 0.15.0\nhpbandster >= 0.7.4\nale-py >= 0.7.3\ngym[box2d,classic_control] >= 0.21.0\nbox2d >= 2.3.10\ngym-retro >= 0.8.0\nvizdoom >= 1.1.11\nm2r >= 0.2.1\nrecommonmark >= 0.7.1\nsphinx >= 4.3.2\nsphinx-rtd-theme >= 1.0.0\npytest >= 6.2.5\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1611328125,
          "content": "gym >= 0.21.0, <0.23\nh5py >= 3.6.0\nmatplotlib >= 3.5.1\nmsgpack >= 1.0.3\nmsgpack-numpy >= 0.4.7.1\nnumpy ~= 1.21.5\nPillow >= 9.0.0\ntensorflow == 2.12.1\ntqdm >= 4.62.3\n"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 9.978515625,
          "content": "# Copyright 2020 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport importlib\nimport json\nimport os\n\nimport matplotlib\nimport numpy as np\n\nfrom tensorforce import Environment, Runner\n\nmatplotlib.use('Agg')\n\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Tensorforce runner')\n    # Agent arguments\n    parser.add_argument(\n        '-a', '--agent', type=str, default=None,\n        help='Agent (name, configuration JSON file, or library module)'\n    )\n    parser.add_argument(\n        '-c', '--checkpoints', type=str, default=None,\n        help='TensorFlow checkpoints directory, plus optional comma-separated filename'\n    )\n    parser.add_argument(\n        '-s', '--summaries', type=str, default=None,\n        help='TensorBoard summaries directory, plus optional comma-separated filename'\n    )\n    parser.add_argument(\n        '--recordings', type=str, default=None, help='Traces recordings directory'\n    )\n    # Environment arguments\n    parser.add_argument(\n        '-e', '--environment', type=str, default=None,\n        help='Environment (name, configuration JSON file, or library module)'\n    )\n    parser.add_argument(\n        '-l', '--level', type=str, default=None,\n        help='Level or game id, like `CartPole-v1`, if supported'\n    )\n    parser.add_argument(\n        '-m', '--max-episode-timesteps', type=int, default=None,\n        help='Maximum number of timesteps per episode'\n    )\n    parser.add_argument(\n        '--visualize', action='store_true',\n        help='Visualize agent--environment interaction, if supported'\n    )\n    parser.add_argument(\n        '--visualize-directory', type=str, default=None,\n        help='Directory to store videos of agent--environment interaction, if supported'\n    )\n    parser.add_argument(\n        '--import-modules', type=str, default=None,\n        help='Import comma-separated modules required for environment'\n    )\n    # Parallel execution arguments\n    parser.add_argument(\n        '--num-parallel', type=int, default=None,\n        help='Number of environment instances to execute in parallel'\n    )\n    parser.add_argument(\n        '--batch-agent-calls', action='store_true',\n        help='Batch agent calls for parallel environment execution'\n    )\n    parser.add_argument(\n        '--sync-timesteps', action='store_true',\n        help='Synchronize parallel environment execution on timestep-level'\n    )\n    parser.add_argument(\n        '--sync-episodes', action='store_true',\n        help='Synchronize parallel environment execution on episode-level'\n    )\n    parser.add_argument(\n        '--remote', type=str, choices=('multiprocessing', 'socket-client', 'socket-server'),\n        default=None, help='Communication mode for remote environment execution of parallelized'\n                           'environment execution'\n    )\n    parser.add_argument(\n        '--blocking', action='store_true', help='Remote environments should be blocking'\n    )\n    parser.add_argument(\n        '--host', type=str, default=None,\n        help='Socket server hostname(s) or IP address(es), single value or comma-separated list'\n    )\n    parser.add_argument(\n        '--port', type=str, default=None,\n        help='Socket server port(s), single value or comma-separated list, increasing sequence if'\n             'single host and port given'\n    )\n    # Runner arguments\n    parser.add_argument(\n        '-v', '--evaluation', action='store_true',\n        help='Run environment (last if multiple) in evaluation mode'\n    )\n    parser.add_argument('-n', '--episodes', type=int, default=None, help='Number of episodes')\n    parser.add_argument('-t', '--timesteps', type=int, default=None, help='Number of timesteps')\n    parser.add_argument('-u', '--updates', type=int, default=None, help='Number of agent updates')\n    parser.add_argument(\n        '--mean-horizon', type=int, default=1,\n        help='Number of episodes progress bar values and evaluation score are averaged over'\n    )\n    parser.add_argument(\n        '--save-best-agent', type=str, default=None,\n        help='Directory to save the best version of the agent according to the evaluation score'\n    )\n    # Logging arguments\n    parser.add_argument('-r', '--repeat', type=int, default=1, help='Number of repetitions')\n    parser.add_argument(\n        '--path', type=str, default=None,\n        help='Logging path, directory plus filename without extension'\n    )\n    parser.add_argument('--seaborn', action='store_true', help='Use seaborn')\n    args = parser.parse_args()\n\n    if args.import_modules is not None:\n        for module in args.import_modules.split(','):\n            importlib.import_module(name=module)\n\n    if args.path is None:\n        callback = None\n\n    else:\n        assert os.path.splitext(args.path)[1] == ''\n        assert args.episodes is not None and args.visualize is not None\n        rewards = [list() for _ in range(args.episodes)]\n        timesteps = [list() for _ in range(args.episodes)]\n        seconds = [list() for _ in range(args.episodes)]\n        agent_seconds = [list() for _ in range(args.episodes)]\n\n        def callback(r, p):\n            rewards[r.episodes - 1].append(float(r.episode_returns[-1]))\n            timesteps[r.episodes - 1].append(int(r.episode_timesteps[-1]))\n            seconds[r.episodes - 1].append(float(r.episode_seconds[-1]))\n            agent_seconds[r.episodes - 1].append(float(r.episode_agent_seconds[-1]))\n            return True\n\n    if args.environment is None:\n        environment = None\n    else:\n        environment = dict(environment=args.environment)\n    if args.level is not None:\n        environment['level'] = args.level\n    if args.visualize:\n        environment['visualize'] = True\n    if args.visualize_directory is not None:\n        environment['visualize_directory'] = args.visualize_directory\n\n    if args.host is not None and ',' in args.host:\n        args.host = args.host.split(',')\n    if args.port is not None and ',' in args.port:\n        args.port = [int(x) for x in args.port.split(',')]\n    elif args.port is not None:\n        args.port = int(args.port)\n\n    if args.remote == 'socket-server':\n        Environment.create(\n            environment=environment, max_episode_timesteps=args.max_episode_timesteps,\n            remote=args.remote, port=args.port\n        )\n        return\n\n    if args.agent is None:\n        assert args.saver is None and args.summarizer is None and args.recorder is None\n        agent = None\n    else:\n        agent = dict(agent=args.agent)\n        if args.checkpoints is not None:\n            assert 'saver' not in agent\n            if ',' in args.checkpoints:\n                directory, filename = args.checkpoints.split(',')\n                agent['saver'] = dict(directory=directory, filename=filename)\n            else:\n                agent['saver'] = args.checkpoints\n        if args.summaries is not None:\n            assert 'summarizer' not in agent\n            if ',' in args.summaries:\n                directory, filename = args.summaries.split(',')\n                agent['summarizer'] = dict(directory=directory, filename=filename)\n            else:\n                agent['summarizer'] = args.summaries\n        if args.recordings is not None:\n            assert 'recorder' not in agent\n            agent['recorder'] = args.recordings\n\n    for _ in range(args.repeat):\n        runner = Runner(\n            agent=agent, environment=environment, max_episode_timesteps=args.max_episode_timesteps,\n            evaluation=args.evaluation, num_parallel=args.num_parallel, remote=args.remote,\n            blocking=args.blocking, host=args.host, port=args.port\n        )\n        runner.run(\n            num_episodes=args.episodes, num_timesteps=args.timesteps, num_updates=args.updates,\n            batch_agent_calls=args.batch_agent_calls, sync_timesteps=args.sync_timesteps,\n            sync_episodes=args.sync_episodes, callback=callback, mean_horizon=args.mean_horizon,\n            save_best_agent=args.save_best_agent\n        )\n        runner.close()\n\n    if args.path is not None:\n        directory = os.path.split(args.path)[0]\n        if directory != '' and not os.path.isdir(directory):\n            os.makedirs(directory, exist_ok=True)\n\n        with open(args.path + '.json', 'w') as filehandle:\n            filehandle.write(\n                json.dumps(dict(\n                    rewards=rewards, timesteps=timesteps, seconds=seconds,\n                    agent_seconds=agent_seconds\n                ))\n            )\n\n        if args.seaborn:\n            import seaborn as sns\n            sns.set()\n\n        xs = np.arange(len(rewards))\n        figure, axis1 = plt.subplots()\n        axis1.set_xlabel('episodes')\n        axis2 = axis1.twinx()\n\n        min_timesteps = np.amin(timesteps, axis=1)\n        max_timesteps = np.amax(timesteps, axis=1)\n        median_timesteps = np.median(timesteps, axis=1)\n        axis2.plot(xs, median_timesteps, color='blue', linewidth=2.0)\n        axis2.fill_between(xs, min_timesteps, max_timesteps, color='blue', alpha=0.4)\n        axis2.set_ylabel('episode length', color='blue')\n\n        min_rewards = np.amin(rewards, axis=1)\n        max_rewards = np.amax(rewards, axis=1)\n        median_rewards = np.median(rewards, axis=1)\n        axis1.plot(xs, median_rewards, color='green', linewidth=2.0)\n        axis1.fill_between(xs, min_rewards, max_rewards, color='green', alpha=0.4)\n        axis1.set_ylabel('episode return', color='green')\n\n        figure.tight_layout()\n        plt.savefig(fname=(args.path + '.png'))\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 5.5380859375,
          "content": "# Copyright 2020 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nfrom setuptools import find_packages, setup\nimport sys\n\n\n\"\"\"\ncd docs; make html; cd ..;\n\npip install --upgrade -r requirements-all.txt\n# ... update requirements.txt and setup.py ...\n\n# Check \"before update\" notes\n\n# Update __version__ in tensorforce/__init__.py, and UPDATE_NOTES.md\n\nrm -r build\nrm -r dist\nrm -r docs/_*\npip install --upgrade pip setuptools wheel twine\npython setup.py sdist bdist_wheel\n\ntwine upload --repository-url https://test.pypi.org/legacy/ dist/Tensorforce-0.6.X*\n\ndeactivate\ncd ..\nsource [XYZ]\n\npip install --upgrade -r tensorforce/requirements.txt\npip install --upgrade --index-url https://test.pypi.org/simple/ tensorforce\npython\n  > import tensorforce\n  > print(tensorforce.__version__)\n  > quit()\npython tensorforce/examples/quickstart.py\n\ndeactivate\nsource [XYZ]\ncd tensorforce\n\ngit status\ngit add -u\ngit commit -m \"Fix PyPI version 0.6.X\"\ngit push origin master\n\n# Wait for a while to check Travis is not failing right away (installation, first tests)\n\ntwine upload dist/Tensorforce-0.6.X*\n\n# Fix Github release\n\"\"\"\n\nif sys.version_info.major != 3:\n    raise NotImplementedError(\"Tensorforce is only compatible with Python 3.\")\n\ntensorforce_directory = os.path.abspath(os.path.dirname(__file__))\n\n# Extract version from tensorforce/__init__.py\nversion = None\nwith open(os.path.join(tensorforce_directory, 'tensorforce', '__init__.py'), 'r') as filehandle:\n    for line in filehandle:\n        if line.startswith('__version__ = \\'') and line.endswith('\\'\\n'):\n            version = line[15:-2]\nassert version is not None\n\n# Extract long_description from README.md introduction\nlong_description = list()\nwith open(os.path.join(tensorforce_directory, 'README.md'), 'r') as filehandle:\n    lines = iter(filehandle)\n    line = next(lines)\n    if not line.startswith('# Tensorforce:'):\n        raise NotImplementedError\n    long_description.append(line)\n    for line in lines:\n        if line == '#### Introduction\\n':\n            break\n    if next(lines) != '\\n':\n        raise NotImplementedError\n    while True:\n        line = next(lines)\n        if line == '\\n':\n            line = next(lines)\n            if line == '\\n':\n                break\n            else:\n                long_description.append('\\n')\n                long_description.append(line)\n        else:\n            long_description.append(line)\n    while line == '\\n':\n        line = next(lines)\n    if not line.startswith('#### '):\n        raise NotImplementedError\nassert len(long_description) > 0\nlong_description.append('\\n')\nlong_description.append('For more information, see the [GitHub project page](https://github.com/ten'\n                        'sorforce/tensorforce) and [ReadTheDocs documentation](https://tensorforce.'\n                        'readthedocs.io/en/latest/).\\n')\nlong_description = ''.join(long_description)\n\n# Find packages\npackages = find_packages(exclude=('test',))\nassert all(package.startswith('tensorforce') for package in packages)\n\n# Extract install_requires from requirements.txt\ninstall_requires = list()\nwith open(os.path.join(tensorforce_directory, 'requirements.txt'), 'r') as filehandle:\n    for line in filehandle:\n        line = line.strip()\n        if line:\n            install_requires.append(line)\nassert len(install_requires) > 0\n\n# Readthedocs requires Sphinx extensions to be specified as part of install_requires.\nif os.environ.get('READTHEDOCS', None) == 'True':\n    install_requires.append('recommonmark')\n\nsetup(\n    name='Tensorforce',\n    version=version,\n    description='Tensorforce: a TensorFlow library for applied reinforcement learning',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    author='Alexander Kuhnle',\n    author_email='tensorforce.team@gmail.com',\n    url='http://github.com/tensorforce/tensorforce',\n    packages=packages,\n    download_url='https://github.com/tensorforce/tensorforce/archive/{}.tar.gz'.format(version),\n    license='Apache 2.0',\n    python_requires='>=3.7',\n    classifiers=[\n        'Natural Language :: English',\n        'Topic :: Scientific/Engineering',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8'\n    ],\n    install_requires=install_requires,\n    extras_require=dict(\n        tfa=['tensorflow-addons >= 0.15.0'],\n        tune=['hpbandster >= 0.7.4'],\n        envs=[\n            'ale-py >= 0.7.3', 'gym[atari,box2d,classic_control] >= 0.21.0', 'box2d >= 2.3.10',\n            'gym-retro >= 0.8.0', 'vizdoom == 1.1.11'\n        ],\n        ale=['ale-py >= 0.7.3'],\n        gym=['gym[box2d,classic_control] >= 0.21.0', 'box2d >= 2.3.10'],\n        retro=['gym-retro >= 0.8.0'],\n        vizdoom=['vizdoom >= 1.1.11'],\n        carla=['pygame', 'opencv-python'],\n        docs=[\n            'm2r >= 0.2.1', 'recommonmark >= 0.7.1', 'sphinx >= 4.3.2', 'sphinx-rtd-theme >= 1.0.0'\n        ],\n        tests=['pytest >= 6.2.5']\n    ),\n    zip_safe=False\n)\n"
        },
        {
          "name": "tensorforce",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tune.py",
          "type": "blob",
          "size": 14.3271484375,
          "content": "# Copyright 2020 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport importlib\nimport math\nimport os\nimport pickle\n\nimport ConfigSpace as cs\nfrom hpbandster.core.nameserver import NameServer, nic_name_to_host\nfrom hpbandster.core.result import json_result_logger, logged_results_to_HBS_result\nfrom hpbandster.core.worker import Worker\nfrom hpbandster.optimizers import BOHB\nimport numpy as np\n\nfrom tensorforce import Runner, util\n\n\nclass TensorforceWorker(Worker):\n\n    def __init__(\n        self, *args, environment, num_episodes, base, runs_per_round, max_episode_timesteps=None,\n        num_parallel=None, **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.environment = environment\n        self.max_episode_timesteps = max_episode_timesteps\n        self.num_episodes = num_episodes\n        self.base = base\n        self.runs_per_round = runs_per_round\n        self.num_parallel = num_parallel\n\n    def compute(self, config_id, config, budget, working_directory):\n        budget = math.log(budget, self.base)\n        assert abs(budget - round(budget)) < util.epsilon\n        budget = round(budget)\n        assert budget < len(self.runs_per_round)\n        num_runs = self.runs_per_round[budget]\n\n        update = dict(unit='episodes', batch_size=config['batch_size'], frequency=1)\n        policy = dict(network=dict(type='auto', size=64, depth=2, rnn=False))\n        optimizer = dict(\n            optimizer='adam', learning_rate=config['learning_rate'],\n            multi_step=config['multi_step'], linesearch_iterations=5  # , subsampling_fraction=256\n        )\n\n        if config['clipping_value'] > 1.0:\n            objective = dict(\n                type='policy_gradient',\n                importance_sampling=(config['importance_sampling'] == 'yes')\n            )\n        else:\n            objective = dict(\n                type='policy_gradient',\n                importance_sampling=(config['importance_sampling'] == 'yes'),\n                clipping_value=config['clipping_value']\n            )\n\n        if config['baseline'] == 'no':\n            predict_horizon_values = False\n            estimate_advantage = False\n            predict_action_values = False\n            baseline = None\n            baseline_optimizer = None\n            baseline_objective = None\n\n        elif config['baseline'] == 'same':\n            predict_horizon_values = 'early'\n            estimate_advantage = (config['estimate_advantage'] == 'yes')\n            predict_action_values = False\n            baseline = None\n            baseline_optimizer = config['baseline_weight']\n            baseline_objective = dict(type='value', value='state')\n\n        elif config['baseline'] == 'yes':\n            predict_horizon_values = 'early'\n            estimate_advantage = (config['estimate_advantage'] == 'yes')\n            predict_action_values = False\n            baseline = dict(network=dict(type='auto', size=64, depth=2, rnn=False))\n            baseline_optimizer = config['baseline_weight']\n            baseline_objective = dict(type='value', value='state')\n\n        else:\n            assert False\n\n        reward_estimation = dict(\n            horizon=config['horizon'], discount=config['discount'],\n            predict_horizon_values=predict_horizon_values, estimate_advantage=estimate_advantage,\n            predict_action_values=predict_action_values\n        )\n\n        if config['entropy_regularization'] < 1e-5:\n            entropy_regularization = 0.0\n        else:\n            entropy_regularization = config['entropy_regularization']\n\n        agent = dict(\n            policy=policy, memory='recent', update=update, optimizer=optimizer, objective=objective,\n            reward_estimation=reward_estimation, baseline=baseline,\n            baseline_optimizer=baseline_optimizer, baseline_objective=baseline_objective,\n            entropy_regularization=entropy_regularization\n        )\n\n        average_reward = list()\n        final_reward = list()\n        rewards = list()\n\n        for n in range(num_runs):\n            if self.num_parallel is None:\n                runner = Runner(\n                    agent=agent, environment=self.environment,\n                    max_episode_timesteps=self.max_episode_timesteps\n                )\n                runner.run(num_episodes=self.num_episodes, use_tqdm=False)\n            else:\n                runner = Runner(\n                    agent=agent, environment=self.environment,\n                    max_episode_timesteps=self.max_episode_timesteps,\n                    num_parallel=min(self.num_parallel, config['batch_size']),\n                    remote='multiprocessing'\n                )\n                runner.run(\n                    num_episodes=self.num_episodes, batch_agent_calls=True, sync_episodes=True,\n                    use_tqdm=False\n                )\n            runner.close()\n\n            average_reward.append(float(np.mean(runner.episode_returns, axis=0)))\n            final_reward.append(float(np.mean(runner.episode_returns[-20:], axis=0)))\n            rewards.append(list(runner.episode_returns))\n\n        mean_average_reward = float(np.mean(average_reward, axis=0))\n        mean_final_reward = float(np.mean(final_reward, axis=0))\n        loss = -(mean_average_reward + mean_final_reward)\n\n        return dict(loss=loss, info=dict(rewards=rewards))\n\n    @staticmethod\n    def get_configspace():\n        configspace = cs.ConfigurationSpace()\n\n        batch_size = cs.hyperparameters.UniformIntegerHyperparameter(\n            name='batch_size', lower=1, upper=20, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=batch_size)\n\n        learning_rate = cs.hyperparameters.UniformFloatHyperparameter(\n            name='learning_rate', lower=1e-5, upper=1e-1, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=learning_rate)\n\n        multi_step = cs.hyperparameters.UniformIntegerHyperparameter(\n            name='multi_step', lower=1, upper=20, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=multi_step)\n\n        horizon = cs.hyperparameters.UniformIntegerHyperparameter(\n            name='horizon', lower=1, upper=100, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=horizon)\n\n        discount = cs.hyperparameters.UniformFloatHyperparameter(\n            name='discount', lower=0.8, upper=1.0, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=discount)\n\n        importance_sampling = cs.hyperparameters.CategoricalHyperparameter(\n            name='importance_sampling', choices=('no', 'yes')\n        )\n        configspace.add_hyperparameter(hyperparameter=importance_sampling)\n\n        # > 1.0: off (ln(1.3) roughly 1/10 of ln(5e-2))\n        clipping_value = cs.hyperparameters.UniformFloatHyperparameter(\n            name='clipping_value', lower=5e-2, upper=1.3, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=clipping_value)\n\n        baseline = cs.hyperparameters.CategoricalHyperparameter(\n            name='baseline', choices=('no', 'same', 'yes')\n        )\n        configspace.add_hyperparameter(hyperparameter=baseline)\n\n        baseline_weight = cs.hyperparameters.UniformFloatHyperparameter(\n            name='baseline_weight', lower=1e-2, upper=1e2\n        )\n        configspace.add_hyperparameter(hyperparameter=baseline_weight)\n\n        estimate_advantage = cs.hyperparameters.CategoricalHyperparameter(\n            name='estimate_advantage', choices=('no', 'yes')\n        )\n        configspace.add_hyperparameter(hyperparameter=estimate_advantage)\n\n        # < 1e-5: off (ln(3e-6) roughly 1/10 of ln(1e-5))\n        entropy_regularization = cs.hyperparameters.UniformFloatHyperparameter(\n            name='entropy_regularization', lower=3e-6, upper=1.0, log=True\n        )\n        configspace.add_hyperparameter(hyperparameter=entropy_regularization)\n\n        # configspace.add_condition(condition=cs.EqualsCondition(\n        #     child=clipping_value, parent=importance_sampling, value='yes'\n        # ))\n        configspace.add_condition(condition=cs.NotEqualsCondition(\n            child=estimate_advantage, parent=baseline, value='no'\n        ))\n        configspace.add_condition(condition=cs.NotEqualsCondition(\n            child=baseline_weight, parent=baseline, value='no'\n        ))\n\n        return configspace\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Tensorforce hyperparameter tuner, using BOHB optimizer (Bayesian Optimization '\n                    'and Hyperband)'\n    )\n    # Environment arguments (from run.py)\n    parser.add_argument(\n        '-e', '--environment', type=str,\n        help='Environment (name, configuration JSON file, or library module)'\n    )\n    parser.add_argument(\n        '-l', '--level', type=str, default=None,\n        help='Level or game id, like `CartPole-v1`, if supported'\n    )\n    parser.add_argument(\n        '-m', '--max-episode-timesteps', type=int, default=None,\n        help='Maximum number of timesteps per episode'\n    )\n    parser.add_argument(\n        '--import-modules', type=str, default=None,\n        help='Import comma-separated modules required for environment'\n    )\n    # Runner arguments (from run.py)\n    parser.add_argument('-n', '--episodes', type=int, help='Number of episodes')\n    parser.add_argument(\n        '-p', '--num-parallel', type=int, default=None,\n        help='Number of environment instances to execute in parallel'\n    )\n    # Tuner arguments\n    parser.add_argument(\n        '-r', '--runs-per-round', type=str, default='1,2,5,10',\n        help='Comma-separated number of runs per optimization round, each with a successively '\n             'smaller number of candidates'\n    )\n    parser.add_argument(\n        '-s', '--selection-factor', type=int, default=3,\n        help='Selection factor n, meaning that one out of n candidates in each round advances to '\n             'the next optimization round'\n    )\n    parser.add_argument(\n        '-i', '--num-iterations', type=int, default=1,\n        help='Number of optimization iterations, each consisting of a series of optimization '\n             'rounds with an increasingly reduced candidate pool'\n    )\n    parser.add_argument(\n        '-d', '--directory', type=str, default='tuner', help='Output directory'\n    )\n    parser.add_argument(\n        '--restore', type=str, default=None, help='Restore from given directory'\n    )\n    parser.add_argument('--id', type=str, default='worker', help='Unique worker id')\n    args = parser.parse_args()\n\n    if args.import_modules is not None:\n        for module in args.import_modules.split(','):\n            importlib.import_module(name=module)\n\n    environment = dict(environment=args.environment)\n    if args.level is not None:\n        environment['level'] = args.level\n\n    if False:\n        host = nic_name_to_host(nic_name=None)\n        port = 123\n    else:\n        host = 'localhost'\n        port = None\n\n    runs_per_round = tuple(int(x) for x in args.runs_per_round.split(','))\n    print('Bayesian Optimization and Hyperband optimization')\n    print(f'{args.num_iterations} iterations of each {len(runs_per_round)} rounds:')\n    for n, num_runs in enumerate(runs_per_round, start=1):\n        num_candidates = round(math.pow(args.selection_factor, len(runs_per_round) - n))\n        print(f'round {n}: {num_candidates} candidates, each {num_runs} runs')\n    print()\n\n    server = NameServer(run_id=args.id, working_directory=args.directory, host=host, port=port)\n    nameserver, nameserver_port = server.start()\n\n    worker = TensorforceWorker(\n        environment=environment, max_episode_timesteps=args.max_episode_timesteps,\n        num_episodes=args.episodes, base=args.selection_factor, runs_per_round=runs_per_round,\n        num_parallel=args.num_parallel, run_id=args.id, nameserver=nameserver,\n        nameserver_port=nameserver_port, host=host\n    )\n    worker.run(background=True)\n\n    if args.restore is None:\n        previous_result = None\n    else:\n        previous_result = logged_results_to_HBS_result(directory=args.restore)\n\n    result_logger = json_result_logger(directory=args.directory, overwrite=True)\n\n    optimizer = BOHB(\n        configspace=worker.get_configspace(), eta=args.selection_factor, min_budget=0.9,\n        max_budget=math.pow(args.selection_factor, len(runs_per_round) - 1), run_id=args.id,\n        working_directory=args.directory, nameserver=nameserver, nameserver_port=nameserver_port,\n        host=host, result_logger=result_logger, previous_result=previous_result\n    )\n    # BOHB(configspace=None, eta=3, min_budget=0.01, max_budget=1, min_points_in_model=None,\n    # top_n_percent=15, num_samples=64, random_fraction=1 / 3, bandwidth_factor=3,\n    # min_bandwidth=1e-3, **kwargs)\n    # Master(run_id, config_generator, working_directory='.', ping_interval=60,\n    # nameserver='127.0.0.1', nameserver_port=None, host=None, shutdown_workers=True,\n    # job_queue_sizes=(-1,0), dynamic_queue_size=True, logger=None, result_logger=None,\n    # previous_result = None)\n    # logger: logging.logger like object, the logger to output some (more or less meaningful)\n    # information\n\n    results = optimizer.run(n_iterations=args.num_iterations)\n    # optimizer.run(n_iterations=1, min_n_workers=1, iteration_kwargs={})\n    # min_n_workers: int, minimum number of workers before starting the run\n\n    optimizer.shutdown(shutdown_workers=True)\n    server.shutdown()\n\n    with open(os.path.join(args.directory, 'results.pkl'), 'wb') as filehandle:\n        pickle.dump(results, filehandle)\n\n    print('Best found configuration: {}'.format(\n        results.get_id2config_mapping()[results.get_incumbent_id()]['config']\n    ))\n    print('Runs:', results.get_runs_by_id(config_id=results.get_incumbent_id()))\n    print('A total of {} unique configurations where sampled.'.format(\n        len(results.get_id2config_mapping())\n    ))\n    print('A total of {} runs where executed.'.format(len(results.get_all_runs())))\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    }
  ]
}