{
  "metadata": {
    "timestamp": 1736559827001,
    "page": 566,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "GerbenJavado/LinkFinder",
      "stars": 3782,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0185546875,
          "content": "Dockerfile\nLICENCE\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.2294921875,
          "content": "### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n### Other ###\n\n# Generated HTML files\n*.html\n\n# .DS_Store\n.DS_Store\n\n# PyCharm's configuration files\n.idea/"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.3408203125,
          "content": "FROM python:3.7.3-alpine3.9\n\nRUN apk add --no-cache shadow bash && \\\n    mkdir -p /linkfinder/output && \\\n    useradd --create-home --shell /sbin/nologin linkfinder\n\nCOPY . /linkfinder/\n\nWORKDIR /linkfinder/\n\nRUN chown -R linkfinder:linkfinder /linkfinder && \\\n    python3 setup.py install\n\nUSER linkfinder\n\nENTRYPOINT [\"/linkfinder/linkfinder.py\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0556640625,
          "content": "MIT License\n\nCopyright (c) 2019 Gerben Janssen van Doorn\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.90625,
          "content": "<img src=\"https://user-images.githubusercontent.com/18099289/62728809-f98b0900-ba1c-11e9-8dd8-67111263a21f.png\" width=650px>\n\n## About LinkFinder\n\nLinkFinder is a python script written to discover endpoints and their parameters in JavaScript files. This way penetration testers and bug hunters are able to gather new, hidden endpoints on the websites they are testing. Resulting in new testing ground, possibility containing new vulnerabilities. It does so by using [jsbeautifier](https://github.com/beautify-web/js-beautify) for python in combination with a fairly large regular expression. The regular expressions consists of four small regular expressions. These are responsible for finding:\n\n- Full URLs (`https://example.com/*`)\n- Absolute URLs or dotted URLs (`/\\*` or `../*`)\n- Relative URLs with at least one slash (`text/test.php`)\n- Relative URLs without a slash (`test.php`)\n\nThe output is given in HTML or plaintext. [@karel_origin](https://twitter.com/karel_origin) has written a Chrome extension for LinkFinder which can be found [here](https://github.com/GerbenJavado/LinkFinder/tree/chrome_extension).\n\n## Screenshots\n\n![LinkFinder](https://i.imgur.com/JfcpYok.png \"LinkFinder in action\")\n\n## Installation\n\nLinkFinder supports **Python 3**.\n\n```\n$ git clone https://github.com/GerbenJavado/LinkFinder.git\n$ cd LinkFinder\n$ python setup.py install\n```\n\n## Dependencies\n\nLinkFinder depends on the `argparse` and `jsbeautifier` Python modules. These dependencies can all be installed using [pip](https://pypi.python.org/pypi/pip).\n\n```\n$ pip3 install -r requirements.txt\n```\n\n## Usage\n\nShort Form    | Long Form     | Description\n------------- | ------------- |-------------\n-i            | --input       | Input a: URL, file or folder. For folders a wildcard can be used (e.g. '/*.js').\n-o            | --output      | \"cli\" to print to STDOUT, otherwise where to save the HTML file Default: output.html\n-r            | --regex       | RegEx for filtering purposes against found endpoints (e.g. ^/api/)\n-d            | --domain      | Toggle to use when analyzing an entire domain. Enumerates over all found JS files.\n-b            | --burp        | Toggle to use when inputting a Burp 'Save selected' file containing multiple JS files\n-c            | --cookies     | Add cookies to the request\n-h            | --help        | show the help message and exit\n\n### Examples\n\n* Most basic usage to find endpoints in an online JavaScript file and output the HTML results to results.html:\n\n`python linkfinder.py -i https://example.com/1.js -o results.html`\n\n* CLI/STDOUT output (doesn't use jsbeautifier, which makes it very fast):\n\n`python linkfinder.py -i https://example.com/1.js -o cli`\n\n* Analyzing an entire domain and its JS files:\n\n`python linkfinder.py -i https://example.com -d`\n\n* Burp input (select in target the files you want to save, right click, `Save selected items`, feed that file as input):\n\n`python linkfinder.py -i burpfile -b`\n\n* Enumerating an entire folder for JavaScript files, while looking for endpoints starting with /api/ and finally saving the results to results.html:\n\n`python linkfinder.py -i 'Desktop/*.js' -r ^/api/ -o results.html`\n\n## Docker\n\n* Build the Docker image:\n\n  ``` docker build -t linkfinder```\n\n* Run with Docker\n\n  ` docker run --rm -v $(pwd):/linkfinder/output linkfinder -i http://example.com/1.js -o /linkfinder/output/output.html`\n\n  Make sure to use the path `/linkfinder/output` in your output path, or the output will be lost when the container exits.\n\n## Unit-test\n\n* Require pytest\n\n`pytest test_parser.py`\n\n## Final remarks\n- This is the first time I publicly release a tool. Contributions are much appreciated!\n- LinkFinder is published under the [MIT License](https://github.com/GerbenJavado/LinkFinder/blob/master/LICENSE).\n- Thanks to [@jackhcable](https://twitter.com/jackhcable) for providing me with feedback.\n- Special thanks [@edoverflow](https://twitter.com/edoverflow) for making this project a lot cleaner and awesome.\n"
        },
        {
          "name": "linkfinder.py",
          "type": "blob",
          "size": 13.689453125,
          "content": "#!/usr/bin/env python\n# Python 3\n# LinkFinder\n# By Gerben_Javado\n\n# Fix webbrowser bug for MacOS\nimport os\nos.environ[\"BROWSER\"] = \"open\"\n\n# Import libraries\nimport re, sys, glob, html, argparse, jsbeautifier, webbrowser, subprocess, base64, ssl, xml.etree.ElementTree\n\nfrom gzip import GzipFile\nfrom string import Template\n\ntry:\n    from StringIO import StringIO\n    readBytesCustom = StringIO\nexcept ImportError:\n    from io import BytesIO\n    readBytesCustom = BytesIO\n\ntry:\n    from urllib.request import Request, urlopen\nexcept ImportError:\n    from urllib2 import Request, urlopen\n\n# Regex used\nregex_str = r\"\"\"\n\n  (?:\"|')                               # Start newline delimiter\n\n  (\n    ((?:[a-zA-Z]{1,10}://|//)           # Match a scheme [a-Z]*1-10 or //\n    [^\"'/]{1,}\\.                        # Match a domainname (any character + dot)\n    [a-zA-Z]{2,}[^\"']{0,})              # The domainextension and/or path\n\n    |\n\n    ((?:/|\\.\\./|\\./)                    # Start with /,../,./\n    [^\"'><,;| *()(%%$^/\\\\\\[\\]]          # Next character can't be...\n    [^\"'><,;|()]{1,})                   # Rest of the characters can't be\n\n    |\n\n    ([a-zA-Z0-9_\\-/]{1,}/               # Relative endpoint with /\n    [a-zA-Z0-9_\\-/.]{1,}                # Resource name\n    \\.(?:[a-zA-Z]{1,4}|action)          # Rest + extension (length 1-4 or action)\n    (?:[\\?|#][^\"|']{0,}|))              # ? or # mark with parameters\n\n    |\n\n    ([a-zA-Z0-9_\\-/]{1,}/               # REST API (no extension) with /\n    [a-zA-Z0-9_\\-/]{3,}                 # Proper REST endpoints usually have 3+ chars\n    (?:[\\?|#][^\"|']{0,}|))              # ? or # mark with parameters\n\n    |\n\n    ([a-zA-Z0-9_\\-]{1,}                 # filename\n    \\.(?:php|asp|aspx|jsp|json|\n         action|html|js|txt|xml)        # . + extension\n    (?:[\\?|#][^\"|']{0,}|))              # ? or # mark with parameters\n\n  )\n\n  (?:\"|')                               # End newline delimiter\n\n\"\"\"\n\ncontext_delimiter_str = \"\\n\"\n\ndef parser_error(errmsg):\n    '''\n    Error Messages\n    '''\n    print(\"Usage: python %s [Options] use -h for help\" % sys.argv[0])\n    print(\"Error: %s\" % errmsg)\n    sys.exit()\n\n\ndef parser_input(input):\n    '''\n    Parse Input\n    '''\n\n    # Method 1 - URL\n    if input.startswith(('http://', 'https://',\n                         'file://', 'ftp://', 'ftps://')):\n        return [input]\n\n    # Method 2 - URL Inspector Firefox\n    if input.startswith('view-source:'):\n        return [input[12:]]\n\n    # Method 3 - Burp file\n    if args.burp:\n        jsfiles = []\n        items = xml.etree.ElementTree.fromstring(open(args.input, \"r\").read())\n\n        for item in items:\n            jsfiles.append({\"js\":base64.b64decode(item.find('response').text).decode('utf-8',\"replace\"), \"url\":item.find('url').text})\n        return jsfiles\n\n    # Method 4 - Folder with a wildcard\n    if \"*\" in input:\n        paths = glob.glob(os.path.abspath(input))\n        file_paths = [p for p in paths if os.path.isfile(p)]\n        for index, path in enumerate(file_paths):\n            file_paths[index] = \"file://%s\" % path\n        return (file_paths if len(file_paths) > 0 else parser_error('Input with wildcard does \\\n        not match any files.'))\n\n    # Method 5 - Local file\n    path = \"file://%s\" % os.path.abspath(input)\n    return [path if os.path.exists(input) else parser_error(\"file could not \\\nbe found (maybe you forgot to add http/https).\")]\n\n\ndef send_request(url):\n    '''\n    Send requests with Requests\n    '''\n    q = Request(url)\n\n    q.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36')\n    q.add_header('Accept', 'text/html,\\\n        application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8')\n    q.add_header('Accept-Language', 'en-US,en;q=0.8')\n    q.add_header('Accept-Encoding', 'gzip')\n    q.add_header('Cookie', args.cookies)\n\n    try:\n        sslcontext = ssl.create_default_context()\n        response = urlopen(q, timeout=args.timeout, context=sslcontext)\n    except:\n        sslcontext = ssl.create_default_context()\n        response = urlopen(q, timeout=args.timeout, context=sslcontext)\n\n    if response.info().get('Content-Encoding') == 'gzip':\n        data = GzipFile(fileobj=readBytesCustom(response.read())).read()\n    elif response.info().get('Content-Encoding') == 'deflate':\n        data = response.read().read()\n    else:\n        data = response.read()\n\n    return data.decode('utf-8', 'replace')\n\ndef getContext(list_matches, content, include_delimiter=0, context_delimiter_str=\"\\n\"):\n    '''\n    Parse Input\n    list_matches:       list of tuple (link, start_index, end_index)\n    content:            content to search for the context\n    include_delimiter   Set 1 to include delimiter in context\n    '''\n    items = []\n    for m in list_matches:\n        match_str = m[0]\n        match_start = m[1]\n        match_end = m[2]\n        context_start_index = match_start\n        context_end_index = match_end\n        delimiter_len = len(context_delimiter_str)\n        content_max_index = len(content) - 1\n\n        while content[context_start_index] != context_delimiter_str and context_start_index > 0:\n            context_start_index = context_start_index - 1\n\n        while content[context_end_index] != context_delimiter_str and context_end_index < content_max_index:\n            context_end_index = context_end_index + 1\n\n        if include_delimiter:\n            context = content[context_start_index: context_end_index]\n        else:\n            context = content[context_start_index + delimiter_len: context_end_index]\n\n        item = {\n            \"link\": match_str,\n            \"context\": context\n        }\n        items.append(item)\n\n    return items\n\ndef parser_file(content, regex_str, mode=1, more_regex=None, no_dup=1):\n    '''\n    Parse Input\n    content:    string of content to be searched\n    regex_str:  string of regex (The link should be in the group(1))\n    mode:       mode of parsing. Set 1 to include surrounding contexts in the result\n    more_regex: string of regex to filter the result\n    no_dup:     remove duplicated link (context is NOT counted)\n\n    Return the list of [\"link\": link, \"context\": context]\n    The context is optional if mode=1 is provided.\n    '''\n    global context_delimiter_str\n\n    if mode == 1:\n        # Beautify\n        if len(content) > 1000000:\n            content = content.replace(\";\",\";\\r\\n\").replace(\",\",\",\\r\\n\")\n        else:\n            content = jsbeautifier.beautify(content)\n\n    regex = re.compile(regex_str, re.VERBOSE)\n\n    if mode == 1:\n        all_matches = [(m.group(1), m.start(0), m.end(0)) for m in re.finditer(regex, content)]\n        items = getContext(all_matches, content, context_delimiter_str=context_delimiter_str)\n    else:\n        items = [{\"link\": m.group(1)} for m in re.finditer(regex, content)]\n\n    if no_dup:\n        # Remove duplication\n        all_links = set()\n        no_dup_items = []\n        for item in items:\n            if item[\"link\"] not in all_links:\n                all_links.add(item[\"link\"])\n                no_dup_items.append(item)\n        items = no_dup_items\n\n    # Match Regex\n    filtered_items = []\n    for item in items:\n        # Remove other capture groups from regex results\n        if more_regex:\n            if re.search(more_regex, item[\"link\"]):\n                filtered_items.append(item)\n        else:\n            filtered_items.append(item)\n\n    return filtered_items\n\ndef cli_output(endpoints):\n    '''\n    Output to CLI\n    '''\n    for endpoint in endpoints:\n        print(html.escape(endpoint[\"link\"]).encode(\n            'ascii', 'ignore').decode('utf8'))\n\ndef html_save(html):\n    '''\n    Save as HTML file and open in the browser\n    '''\n    hide = os.dup(1)\n    os.close(1)\n    os.open(os.devnull, os.O_RDWR)\n    try:\n        s = Template(open('%s/template.html' % sys.path[0], 'r').read())\n\n        text_file = open(args.output, \"wb\")\n        text_file.write(s.substitute(content=html).encode('utf8'))\n        text_file.close()\n\n        print(\"URL to access output: file://%s\" % os.path.abspath(args.output))\n        file = \"file:///%s\" % os.path.abspath(args.output)\n        if sys.platform == 'linux' or sys.platform == 'linux2':\n            subprocess.call([\"xdg-open\", file])\n        else:\n            webbrowser.open(file)\n    except Exception as e:\n        print(\"Output can't be saved in %s \\\n            due to exception: %s\" % (args.output, e))\n    finally:\n        os.dup2(hide, 1)\n\ndef check_url(url):\n    nopelist = [\"node_modules\", \"jquery.js\"]\n    if url[-3:] == \".js\":\n        words = url.split(\"/\")\n        for word in words:\n            if word in nopelist:\n                return False\n        if url[:2] == \"//\":\n            url = \"https:\" + url\n        if url[:4] != \"http\":\n            if url[:1] == \"/\":\n                url = args.input + url\n            else:\n                url = args.input + \"/\" + url\n        return url\n    else:\n        return False\n\nif __name__ == \"__main__\":\n    # Parse command line\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-d\", \"--domain\",\n                        help=\"Input a domain to recursively parse all javascript located in a page\",\n                        action=\"store_true\")\n    parser.add_argument(\"-i\", \"--input\",\n                        help=\"Input a: URL, file or folder. \\\n                        For folders a wildcard can be used (e.g. '/*.js').\",\n                        required=\"True\", action=\"store\")\n    parser.add_argument(\"-o\", \"--output\",\n                        help=\"Where to save the file, \\\n                        including file name. Default: output.html\",\n                        action=\"store\", default=\"output.html\")\n    parser.add_argument(\"-r\", \"--regex\",\n                        help=\"RegEx for filtering purposes \\\n                        against found endpoint (e.g. ^/api/)\",\n                        action=\"store\")\n    parser.add_argument(\"-b\", \"--burp\",\n                        help=\"\",\n                        action=\"store_true\")\n    parser.add_argument(\"-c\", \"--cookies\",\n                        help=\"Add cookies for authenticated JS files\",\n                        action=\"store\", default=\"\")\n    default_timeout = 10\n    parser.add_argument(\"-t\", \"--timeout\",\n                        help=\"How many seconds to wait for the server to send data before giving up (default: \" + str(default_timeout) + \" seconds)\",\n                        default=default_timeout, type=int, metavar=\"<seconds>\")\n    args = parser.parse_args()\n\n    if args.input[-1:] == \"/\":\n        args.input = args.input[:-1]\n\n    mode = 1\n    if args.output == \"cli\":\n        mode = 0\n\n    # Convert input to URLs or JS files\n    urls = parser_input(args.input)\n\n    # Convert URLs to JS\n    output = ''\n    for url in urls:\n        if not args.burp:\n            try:\n                file = send_request(url)\n            except Exception as e:\n                parser_error(\"invalid input defined or SSL error: %s\" % e)\n        else:\n            file = url['js']\n            url = url['url']\n\n        endpoints = parser_file(file, regex_str, mode, args.regex)\n        if args.domain:\n            for endpoint in endpoints:\n                endpoint = html.escape(endpoint[\"link\"]).encode('ascii', 'ignore').decode('utf8')\n                endpoint = check_url(endpoint)\n                if endpoint is False:\n                    continue\n                print(\"Running against: \" + endpoint)\n                print(\"\")\n                try:\n                    file = send_request(endpoint)\n                    new_endpoints = parser_file(file, regex_str, mode, args.regex)\n                    if args.output == 'cli':\n                        cli_output(new_endpoints)\n                    else:\n                        output += '''\n                        <h1>File: <a href=\"%s\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">%s</a></h1>\n                        ''' % (html.escape(endpoint), html.escape(endpoint))\n\n                        for endpoint2 in new_endpoints:\n                            url = html.escape(endpoint2[\"link\"])\n                            header = \"<div><a href='%s' class='text'>%s\" % (\n                                html.escape(url),\n                                html.escape(url)\n                            )\n                            body = \"</a><div class='container'>%s</div></div>\" % html.escape(\n                                endpoint2[\"context\"]\n                            )\n                            body = body.replace(\n                                html.escape(endpoint2[\"link\"]),\n                                \"<span style='background-color:yellow'>%s</span>\" %\n                                html.escape(endpoint2[\"link\"])\n                            )\n                            output += header + body\n                except Exception as e:\n                    print(\"Invalid input defined or SSL error for: \" + endpoint)\n                    continue\n\n        if args.output == 'cli':\n            cli_output(endpoints)\n        else:\n            output += '''\n                <h1>File: <a href=\"%s\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">%s</a></h1>\n                ''' % (html.escape(url), html.escape(url))\n\n            for endpoint in endpoints:\n                url = html.escape(endpoint[\"link\"])\n                header = \"<div><a href='%s' class='text'>%s\" % (\n                    html.escape(url),\n                    html.escape(url)\n                )\n                body = \"</a><div class='container'>%s</div></div>\" % html.escape(\n                    endpoint[\"context\"]\n                )\n                body = body.replace(\n                    html.escape(endpoint[\"link\"]),\n                    \"<span style='background-color:yellow'>%s</span>\" %\n                    html.escape(endpoint[\"link\"])\n                )\n\n                output += header + body\n\n    if args.output != 'cli':\n        html_save(output)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0126953125,
          "content": "jsbeautifier\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.4140625,
          "content": "#!/usr/bin/env python\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='LinkFinder',\n    packages=find_packages(),\n    version='1.0',\n    description=\"A python script that finds endpoints in JavaScript files.\",\n    long_description=open('README.md').read(),\n    author='Gerben Javado',\n    url='https://github.com/GerbenJavado/LinkFinder',\n    py_modules=['linkfinder'],\n    install_requires=['jsbeautifier'],\n)\n"
        },
        {
          "name": "template.html",
          "type": "blob",
          "size": 2.0390625,
          "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"UTF-8\">\n  <style>\n       h1 {\n          font-family: sans-serif;\n       }\n       a {\n          color: #000;\n       }\n       .text {\n          font-size: 16px;\n          font-family: Helvetica, sans-serif;\n          color: #323232;\n          background-color: white;\n       }\n       .container {\n          background-color: #e9e9e9;\n          padding: 10px;\n          margin: 10px 0;\n          font-family: helvetica;\n          font-size: 13px;\n          border-width: 1px;\n          border-style: solid;\n          border-color: #8a8a8a;\n          color: #323232;\n          margin-bottom: 15px;\n       }\n       .button {\n          padding: 17px 60px;\n          margin: 10px 10px 10px 0;\n          display: inline-block;\n          background-color: #f4f4f4;\n          border-radius: .25rem;\n          text-decoration: none;\n          -webkit-transition: .15s ease-in-out;\n          transition: .15s ease-in-out;\n          color: #333;\n          position: relative;\n       }\n       .button:hover {\n          background-color: #eee;\n          text-decoration: none;\n       }\n       .github-icon {\n          line-height: 0;\n          position: absolute;\n          top: 14px;\n          left: 24px;\n          opacity: 0.7;\n       }\n  </style>\n  <title>LinkFinder Output</title>\n</head>\n<body contenteditable=\"true\">\n  $content\n  \n  <a class='button' contenteditable='false' href='https://github.com/GerbenJavado/LinkFinder/issues/new' rel='nofollow noopener noreferrer' target='_blank'><span class='github-icon'><svg height=\"24\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\">\n  <path d=\"M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22\" fill=\"none\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\"></path></svg></span> Report an issue.</a>\n</body>\n</html>\n"
        },
        {
          "name": "test_parser.py",
          "type": "blob",
          "size": 3.3681640625,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport pytest\nfrom linkfinder import regex_str, parser_file\n\n# Imitate cli_output function\ndef get_parse_cli(str):\n    endpoints = parser_file(str, regex_str, mode=0)\n    ret = []\n    for endpoint in endpoints:\n        ret.append(endpoint[\"link\"])\n    return ret\n\ndef test_parser_cli():\n    assert get_parse_cli(\"\\\"http://example.com\\\"\") == [\"http://example.com\"]\n    assert get_parse_cli(\"\\\"smb://example.com\\\"\") == [\"smb://example.com\"]\n    assert get_parse_cli(\"\\\"https://www.example.co.us\\\"\") == [\"https://www.example.co.us\"]\n\n    assert get_parse_cli(\"\\\"/path/to/file\\\"\") == [\"/path/to/file\"]\n    assert get_parse_cli(\"\\\"../path/to/file\\\"\") == [\"../path/to/file\"]\n    assert get_parse_cli(\"\\\"./path/to/file\\\"\") == [\"./path/to/file\"]\n    assert get_parse_cli(\"\\\"/user/create.action?user=Test\\\"\") == [\"/user/create.action?user=Test\"]\n    assert get_parse_cli(\"\\\"/api/create.php?user=test&pass=test#home\\\"\") == [\"/api/create.php?user=test&pass=test#home\"]\n    assert get_parse_cli(\"\\\"/wrong/file/test<>b\\\"\") == []\n\n    assert get_parse_cli(\"\\\"api/create.php\\\"\") == [\"api/create.php\"]\n    assert get_parse_cli(\"\\\"api/create.php?user=test\\\"\") == [\"api/create.php?user=test\"]\n    assert get_parse_cli(\"\\\"api/create.php?user=test&pass=test\\\"\") == [\"api/create.php?user=test&pass=test\"]\n    assert get_parse_cli(\"\\\"api/create.php?user=test#home\\\"\") == [\"api/create.php?user=test#home\"]\n    assert get_parse_cli(\"\\\"user/create.action?user=Test\\\"\") == [\"user/create.action?user=Test\"]\n    assert get_parse_cli(\"\\\"user/create.notaext?user=Test\\\"\") == []\n\n    assert get_parse_cli(\"\\\"/path/to/file\\\"\") == [\"/path/to/file\"]\n    assert get_parse_cli(\"\\\"../path/to/file\\\"\") == [\"../path/to/file\"]\n    assert get_parse_cli(\"\\\"./path/to/file\\\"\") == [\"./path/to/file\"]\n    assert get_parse_cli(\"\\\"/wrong/file/test<>b\\\"\") == []\n\n    # REST API (no extension)\n    assert get_parse_cli(\"\\\"api/user\\\"\") == [\"api/user\"]\n    assert get_parse_cli(\"\\\"v1/create\\\"\") == [\"v1/create\"]\n    assert get_parse_cli(\"\\\"api/v1/user/2\\\"\") == [\"api/v1/user/2\"]\n    assert get_parse_cli(\"\\\"api/v1/search?text=Test Hello\\\"\") == [\"api/v1/search?text=Test Hello\"]\n\n    assert get_parse_cli(\"\\\"test_1.json\\\"\") == [\"test_1.json\"]\n    assert get_parse_cli(\"\\\"test2.aspx?arg1=tmp1+tmp2&arg2=tmp3\\\"\") == [\"test2.aspx?arg1=tmp1+tmp2&arg2=tmp3\"]\n    assert get_parse_cli(\"\\\"addUser.action\\\"\") == [\"addUser.action\"]\n    assert get_parse_cli(\"\\\"main.js\\\"\") == [\"main.js\"]\n    assert get_parse_cli(\"\\\"index.html\\\"\") == [\"index.html\"]\n    assert get_parse_cli(\"\\\"robots.txt\\\"\") == [\"robots.txt\"]\n    assert get_parse_cli(\"\\\"users.xml\\\"\") == [\"users.xml\"]\n    assert get_parse_cli(\"\\\"UserModel.name\\\"\") == []\n\n    assert get_parse_cli(\"\\\"app/admin/admin.controller.js\\\"\") == [\"app/admin/admin.controller.js\"]\n    assert get_parse_cli(\"\\\"services/customer.services.js\\\"\") == [\"services/customer.services.js\"]\n\ndef test_parser_cli_multi():\n    assert set(get_parse_cli(\"href=\\\"http://example.com\\\";href=\\\"/api/create.php\\\"\")) == set([\"http://example.com\", \"/api/create.php\"])\n\ndef test_parser_unique():\n    '''\n    Should return only unique link\n    '''\n    assert get_parse_cli(\"href=\\\"http://example.com\\\";document.window.location=\\\"http://example.com\\\"\") == [\"http://example.com\"]\n    assert set(get_parse_cli(\"href=\\\"http://example.com\\\";<img src=\\\"http://example.com\\\">;href=\\\"/api/create.php\\\"\")) == set([\"http://example.com\", \"/api/create.php\"])\n"
        }
      ]
    }
  ]
}