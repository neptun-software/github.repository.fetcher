{
  "metadata": {
    "timestamp": 1736559829231,
    "page": 569,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ostris/ai-toolkit",
      "stars": 3776,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.251953125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\n/env.sh\n/models\n/custom/*\n!/custom/.gitkeep\n/.tmp\n/venv.bkp\n/venv.*\n/config/*\n!/config/examples\n!/config/_PUT_YOUR_CONFIGS_HERE).txt\n/output/*\n!/output/.gitkeep\n/extensions/*\n!/extensions/example\n/temp\n/wandb\n.vscode/settings.json\n.DS_Store\n._.DS_Store"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.4580078125,
          "content": "[submodule \"repositories/sd-scripts\"]\n\tpath = repositories/sd-scripts\n\turl = https://github.com/kohya-ss/sd-scripts.git\n[submodule \"repositories/leco\"]\n\tpath = repositories/leco\n\turl = https://github.com/p1atdev/LECO\n[submodule \"repositories/batch_annotator\"]\n\tpath = repositories/batch_annotator\n\turl = https://github.com/ostris/batch-annotator\n[submodule \"repositories/ipadapter\"]\n\tpath = repositories/ipadapter\n\turl = https://github.com/tencent-ailab/IP-Adapter.git\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 0.1630859375,
          "content": "# FAQ\n\nWIP. Will continue to add things as they are needed.\n\n## FLUX.1 Training\n\n#### How much VRAM is required to train a lora on FLUX.1?\n\n24GB minimum is required.\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2024 Ostris, LLC\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 19.9453125,
          "content": "# AI Toolkit by Ostris\n\n## IMPORTANT NOTE - READ THIS\nThis is my research repo. I do a lot of experiments in it and it is possible that I will break things.\nIf something breaks, checkout an earlier commit. This repo can train a lot of things, and it is\nhard to keep up with all of them.\n\n## Support my work\n\n<a href=\"https://glif.app\" target=\"_blank\">\n<img alt=\"glif.app\" src=\"https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/glif.svg?v=1\" width=\"256\" height=\"auto\">\n</a>\n\n\nMy work on this project would not be possible without the amazing support of [Glif](https://glif.app/) and everyone on the \nteam. If you want to support me, support Glif. [Join the site](https://glif.app/), \n[Join us on Discord](https://discord.com/invite/nuR9zZ2nsh), [follow us on Twitter](https://x.com/heyglif)\nand come make some cool stuff with us\n\n## Installation\n\nRequirements:\n- python >3.10\n- Nvidia GPU with enough ram to do what you need\n- python venv\n- git\n\n\n\nLinux:\n```bash\ngit clone https://github.com/ostris/ai-toolkit.git\ncd ai-toolkit\ngit submodule update --init --recursive\npython3 -m venv venv\nsource venv/bin/activate\n# .\\venv\\Scripts\\activate on windows\n# install torch first\npip3 install torch\npip3 install -r requirements.txt\n```\n\nWindows:\n```bash\ngit clone https://github.com/ostris/ai-toolkit.git\ncd ai-toolkit\ngit submodule update --init --recursive\npython -m venv venv\n.\\venv\\Scripts\\activate\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\npip install -r requirements.txt\n```\n\n## FLUX.1 Training\n\n### Tutorial\n\nTo get started quickly, check out [@araminta_k](https://x.com/araminta_k) tutorial on [Finetuning Flux Dev on a 3090](https://www.youtube.com/watch?v=HzGW_Kyermg) with 24GB VRAM.\n\n\n### Requirements\nYou currently need a GPU with **at least 24GB of VRAM** to train FLUX.1. If you are using it as your GPU to control \nyour monitors, you probably need to set the flag `low_vram: true` in the config file under `model:`. This will quantize\nthe model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL,\nbut there are some reports of a bug when running on windows natively. \nI have only tested on linux for now. This is still extremely experimental\nand a lot of quantizing and tricks had to happen to get it to fit on 24GB at all. \n\n### FLUX.1-dev\n\nFLUX.1-dev has a non-commercial license. Which means anything you train will inherit the\nnon-commercial license. It is also a gated model, so you need to accept the license on HF before using it.\nOtherwise, this will fail. Here are the required steps to setup a license.\n\n1. Sign into HF and accept the model access here [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n2. Make a file named `.env` in the root on this folder\n3. [Get a READ key from huggingface](https://huggingface.co/settings/tokens/new?) and add it to the `.env` file like so `HF_TOKEN=your_key_here`\n\n### FLUX.1-schnell\n\nFLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF_TOKEN to train.\nHowever, it does require a special adapter to train with it, [ostris/FLUX.1-schnell-training-adapter](https://huggingface.co/ostris/FLUX.1-schnell-training-adapter).\nIt is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.\n\nTo use it, You just need to add the assistant to the `model` section of your config file like so:\n\n```yaml\n      model:\n        name_or_path: \"black-forest-labs/FLUX.1-schnell\"\n        assistant_lora_path: \"ostris/FLUX.1-schnell-training-adapter\"\n        is_flux: true\n        quantize: true\n```\n\nYou also need to adjust your sample steps since schnell does not require as many\n\n```yaml\n      sample:\n        guidance_scale: 1  # schnell does not do guidance\n        sample_steps: 4  # 1 - 4 works well\n```\n\n### Training\n1. Copy the example config file located at `config/examples/train_lora_flux_24gb.yaml` (`config/examples/train_lora_flux_schnell_24gb.yaml` for schnell) to the `config` folder and rename it to `whatever_you_want.yml`\n2. Edit the file following the comments in the file\n3. Run the file like so `python run.py config/whatever_you_want.yml`\n\nA folder with the name and the training folder from the config file will be created when you start. It will have all \ncheckpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up\nfrom the last checkpoint.\n\nIMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving\n\n### Need help?\n\nPlease do not open a bug report unless it is a bug in the code. You are welcome to [Join my Discord](https://discord.gg/VXmU2f5WEU)\nand ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord\nand I will answer when I can.\n\n## Gradio UI\n\nTo get started training locally with a with a custom UI, once you followed the steps above and `ai-toolkit` is installed:\n\n```bash\ncd ai-toolkit #in case you are not yet in the ai-toolkit folder\nhuggingface-cli login #provide a `write` token to publish your LoRA at the end\npython flux_train_ui.py\n```\n\nYou will instantiate a UI that will let you upload your images, caption them, train and publish your LoRA\n![image](assets/lora_ease_ui.png)\n\n\n## Training in RunPod\nExample RunPod template: **runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04**\n> You need a minimum of 24GB VRAM, pick a GPU by your preference.\n\n#### Example config ($0.5/hr):\n- 1x A40 (48 GB VRAM)\n- 19 vCPU 100 GB RAM\n\n#### Custom overrides (you need some storage to clone FLUX.1, store datasets, store trained models and samples):\n- ~120 GB Disk\n- ~120 GB Pod Volume\n- Start Jupyter Notebook\n\n### 1. Setup\n```\ngit clone https://github.com/ostris/ai-toolkit.git\ncd ai-toolkit\ngit submodule update --init --recursive\npython -m venv venv\nsource venv/bin/activate\npip install torch\npip install -r requirements.txt\npip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues\n```\n### 2. Upload your dataset\n- Create a new folder in the root, name it `dataset` or whatever you like.\n- Drag and drop your .jpg, .jpeg, or .png images and .txt files inside the newly created dataset folder.\n\n### 3. Login into Hugging Face with an Access Token\n- Get a READ token from [here](https://huggingface.co/settings/tokens) and request access to Flux.1-dev model from [here](https://huggingface.co/black-forest-labs/FLUX.1-dev).\n- Run ```huggingface-cli login``` and paste your token.\n\n### 4. Training\n- Copy an example config file located at ```config/examples``` to the config folder and rename it to ```whatever_you_want.yml```.\n- Edit the config following the comments in the file.\n- Change ```folder_path: \"/path/to/images/folder\"``` to your dataset path like ```folder_path: \"/workspace/ai-toolkit/your-dataset\"```.\n- Run the file: ```python run.py config/whatever_you_want.yml```.\n\n### Screenshot from RunPod\n<img width=\"1728\" alt=\"RunPod Training Screenshot\" src=\"https://github.com/user-attachments/assets/53a1b8ef-92fa-4481-81a7-bde45a14a7b5\">\n\n## Training in Modal\n\n### 1. Setup\n#### ai-toolkit:\n```\ngit clone https://github.com/ostris/ai-toolkit.git\ncd ai-toolkit\ngit submodule update --init --recursive\npython -m venv venv\nsource venv/bin/activate\npip install torch\npip install -r requirements.txt\npip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues\n```\n#### Modal:\n- Run `pip install modal` to install the modal Python package.\n- Run `modal setup` to authenticate (if this doesn’t work, try `python -m modal setup`).\n\n#### Hugging Face:\n- Get a READ token from [here](https://huggingface.co/settings/tokens) and request access to Flux.1-dev model from [here](https://huggingface.co/black-forest-labs/FLUX.1-dev).\n- Run `huggingface-cli login` and paste your token.\n\n### 2. Upload your dataset\n- Drag and drop your dataset folder containing the .jpg, .jpeg, or .png images and .txt files in `ai-toolkit`.\n\n### 3. Configs\n- Copy an example config file located at ```config/examples/modal``` to the `config` folder and rename it to ```whatever_you_want.yml```.\n- Edit the config following the comments in the file, **<ins>be careful and follow the example `/root/ai-toolkit` paths</ins>**.\n\n### 4. Edit run_modal.py\n- Set your entire local `ai-toolkit` path at `code_mount = modal.Mount.from_local_dir` like:\n  \n   ```\n   code_mount = modal.Mount.from_local_dir(\"/Users/username/ai-toolkit\", remote_path=\"/root/ai-toolkit\")\n   ```\n- Choose a `GPU` and `Timeout` in `@app.function` _(default is A100 40GB and 2 hour timeout)_.\n\n### 5. Training\n- Run the config file in your terminal: `modal run run_modal.py --config-file-list-str=/root/ai-toolkit/config/whatever_you_want.yml`.\n- You can monitor your training in your local terminal, or on [modal.com](https://modal.com/).\n- Models, samples and optimizer will be stored in `Storage > flux-lora-models`.\n\n### 6. Saving the model\n- Check contents of the volume by running `modal volume ls flux-lora-models`. \n- Download the content by running `modal volume get flux-lora-models your-model-name`.\n- Example: `modal volume get flux-lora-models my_first_flux_lora_v1`.\n\n### Screenshot from Modal\n\n<img width=\"1728\" alt=\"Modal Traning Screenshot\" src=\"https://github.com/user-attachments/assets/7497eb38-0090-49d6-8ad9-9c8ea7b5388b\">\n\n---\n\n## Dataset Preparation\n\nDatasets generally need to be a folder containing images and associated text files. Currently, the only supported\nformats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images\nbut with a `.txt` extension. For example `image2.jpg` and `image2.txt`. The text file should contain only the caption.\nYou can add the word `[trigger]` in the caption file and if you have `trigger_word` in your config, it will be automatically\nreplaced. \n\nImages are never upscaled but they are downscaled and placed in buckets for batching. **You do not need to crop/resize your images**.\nThe loader will automatically resize them and can handle varying aspect ratios. \n\n\n## Training Specific Layers\n\nTo train specific layers with LoRA, you can use the `only_if_contains` network kwargs. For instance, if you want to train only the 2 layers\nused by The Last Ben, [mentioned in this post](https://x.com/__TheBen/status/1829554120270987740), you can adjust your\nnetwork kwargs like so:\n\n```yaml\n      network:\n        type: \"lora\"\n        linear: 128\n        linear_alpha: 128\n        network_kwargs:\n          only_if_contains:\n            - \"transformer.single_transformer_blocks.7.proj_out\"\n            - \"transformer.single_transformer_blocks.20.proj_out\"\n```\n\nThe naming conventions of the layers are in diffusers format, so checking the state dict of a model will reveal \nthe suffix of the name of the layers you want to train. You can also use this method to only train specific groups of weights.\nFor instance to only train the `single_transformer` for FLUX.1, you can use the following:\n\n```yaml\n      network:\n        type: \"lora\"\n        linear: 128\n        linear_alpha: 128\n        network_kwargs:\n          only_if_contains:\n            - \"transformer.single_transformer_blocks.\"\n```\n\nYou can also exclude layers by their names by using `ignore_if_contains` network kwarg. So to exclude all the single transformer blocks,\n\n\n```yaml\n      network:\n        type: \"lora\"\n        linear: 128\n        linear_alpha: 128\n        network_kwargs:\n          ignore_if_contains:\n            - \"transformer.single_transformer_blocks.\"\n```\n\n`ignore_if_contains` takes priority over `only_if_contains`. So if a weight is covered by both,\nif will be ignored.\n\n---\n\n## EVERYTHING BELOW THIS LINE IS OUTDATED \n\nIt may still work like that, but I have not tested it in a while.\n\n---\n\n### Batch Image Generation\n\nA image generator that can take frompts from a config file or form a txt file and generate them to a \nfolder. I mainly needed this for an SDXL test I am doing but added some polish to it so it can be used\nfor generat batch image generation.\nIt all runs off a config file, which you can find an example of in  `config/examples/generate.example.yaml`.\nMere info is in the comments in the example\n\n---\n\n### LoRA (lierla), LoCON (LyCORIS) extractor\n\nIt is based on the extractor in the [LyCORIS](https://github.com/KohakuBlueleaf/LyCORIS) tool, but adding some QOL features\nand LoRA (lierla) support. It can do multiple types of extractions in one run.\nIt all runs off a config file, which you can find an example of in  `config/examples/extract.example.yml`.\nJust copy that file, into the `config` folder, and rename it to `whatever_you_want.yml`.\nThen you can edit the file to your liking. and call it like so:\n\n```bash\npython3 run.py config/whatever_you_want.yml\n```\n\nYou can also put a full path to a config file, if you want to keep it somewhere else.\n\n```bash\npython3 run.py \"/home/user/whatever_you_want.yml\"\n```\n\nMore notes on how it works are available in the example config file itself. LoRA and LoCON both support\nextractions of 'fixed', 'threshold', 'ratio', 'quantile'. I'll update what these do and mean later.\nMost people used fixed, which is traditional fixed dimension extraction.\n\n`process` is an array of different processes to run. You can add a few and mix and match. One LoRA, one LyCON, etc.\n\n---\n\n### LoRA Rescale\n\nChange `<lora:my_lora:4.6>` to `<lora:my_lora:1.0>` or whatever you want with the same effect. \nA tool for rescaling a LoRA's weights. Should would with LoCON as well, but I have not tested it.\nIt all runs off a config file, which you can find an example of in  `config/examples/mod_lora_scale.yml`.\nJust copy that file, into the `config` folder, and rename it to `whatever_you_want.yml`.\nThen you can edit the file to your liking. and call it like so:\n\n```bash\npython3 run.py config/whatever_you_want.yml\n```\n\nYou can also put a full path to a config file, if you want to keep it somewhere else.\n\n```bash\npython3 run.py \"/home/user/whatever_you_want.yml\"\n```\n\nMore notes on how it works are available in the example config file itself. This is useful when making \nall LoRAs, as the ideal weight is rarely 1.0, but now you can fix that. For sliders, they can have weird scales form -2 to 2\nor even -15 to 15. This will allow you to dile it in so they all have your desired scale\n\n---\n\n### LoRA Slider Trainer\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/ostris/ai-toolkit/blob/main/notebooks/SliderTraining.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nThis is how I train most of the recent sliders I have on Civitai, you can check them out in my [Civitai profile](https://civitai.com/user/Ostris/models).\nIt is based off the work by [p1atdev/LECO](https://github.com/p1atdev/LECO) and [rohitgandikota/erasing](https://github.com/rohitgandikota/erasing)\nBut has been heavily modified to create sliders rather than erasing concepts. I have a lot more plans on this, but it is\nvery functional as is. It is also very easy to use. Just copy the example config file in `config/examples/train_slider.example.yml`\nto the `config` folder and rename it to `whatever_you_want.yml`. Then you can edit the file to your liking. and call it like so:\n\n```bash\npython3 run.py config/whatever_you_want.yml\n```\n\nThere is a lot more information in that example file. You can even run the example as is without any modifications to see\nhow it works. It will create a slider that turns all animals into dogs(neg) or cats(pos). Just run it like so:\n\n```bash\npython3 run.py config/examples/train_slider.example.yml\n```\n\nAnd you will be able to see how it works without configuring anything. No datasets are required for this method.\nI will post an better tutorial soon. \n\n---\n\n## Extensions!!\n\nYou can now make and share custom extensions. That run within this framework and have all the inbuilt tools\navailable to them. I will probably use this as the primary development method going\nforward so I dont keep adding and adding more and more features to this base repo. I will likely migrate a lot\nof the existing functionality as well to make everything modular. There is an example extension in the `extensions`\nfolder that shows how to make a model merger extension. All of the code is heavily documented which is hopefully\nenough to get you started. To make an extension, just copy that example and replace all the things you need to.\n\n\n### Model Merger - Example Extension\nIt is located in the `extensions` folder. It is a fully finctional model merger that can merge as many models together\nas you want. It is a good example of how to make an extension, but is also a pretty useful feature as well since most\nmergers can only do one model at a time and this one will take as many as you want to feed it. There is an \nexample config file in there, just copy that to your `config` folder and rename it to `whatever_you_want.yml`.\nand use it like any other config file.\n\n## WIP Tools\n\n\n### VAE (Variational Auto Encoder) Trainer\n\nThis works, but is not ready for others to use and therefore does not have an example config. \nI am still working on it. I will update this when it is ready.\nI am adding a lot of features for criteria that I have used in my image enlargement work. A Critic (discriminator),\ncontent loss, style loss, and a few more. If you don't know, the VAE\nfor stable diffusion (yes even the MSE one, and SDXL), are horrible at smaller faces and it holds SD back. I will fix this.\nI'll post more about this later with better examples later, but here is a quick test of a run through with various VAEs.\nJust went in and out. It is much worse on smaller faces than shown here.\n\n<img src=\"https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/VAE_test1.jpg\" width=\"768\" height=\"auto\"> \n\n---\n\n## TODO\n- [X] Add proper regs on sliders\n- [X] Add SDXL support (base model only for now)\n- [ ] Add plain erasing\n- [ ] Make Textual inversion network trainer (network that spits out TI embeddings)\n\n---\n\n## Change Log\n\n#### 2023-08-05\n - Huge memory rework and slider rework. Slider training is better thant ever with no more\nram spikes. I also made it so all 4 parts of the slider algorythm run in one batch so they share gradient\naccumulation. This makes it much faster and more stable. \n - Updated the example config to be something more practical and more updated to current methods. It is now\na detail slide and shows how to train one without a subject. 512x512 slider training for 1.5 should work on \n6GB gpu now. Will test soon to verify. \n\n\n#### 2021-10-20\n - Windows support bug fixes\n - Extensions! Added functionality to make and share custom extensions for training, merging, whatever.\ncheck out the example in the `extensions` folder. Read more about that above.\n - Model Merging, provided via the example extension.\n\n#### 2023-08-03\nAnother big refactor to make SD more modular.\n\nMade batch image generation script\n\n#### 2023-08-01\nMajor changes and update. New LoRA rescale tool, look above for details. Added better metadata so\nAutomatic1111 knows what the base model is. Added some experiments and a ton of updates. This thing is still unstable\nat the moment, so hopefully there are not breaking changes. \n\nUnfortunately, I am too lazy to write a proper changelog with all the changes.\n\nI added SDXL training to sliders... but.. it does not work properly. \nThe slider training relies on a model's ability to understand that an unconditional (negative prompt)\nmeans you do not want that concept in the output. SDXL does not understand this for whatever reason, \nwhich makes separating out\nconcepts within the model hard. I am sure the community will find a way to fix this \nover time, but for now, it is not \ngoing to work properly. And if any of you are thinking \"Could we maybe fix it by adding 1 or 2 more text\nencoders to the model as well as a few more entirely separate diffusion networks?\" No. God no. It just needs a little\ntraining without every experimental new paper added to it. The KISS principal. \n\n\n#### 2023-07-30\nAdded \"anchors\" to the slider trainer. This allows you to set a prompt that will be used as a \nregularizer. You can set the network multiplier to force spread consistency at high weights\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "build_and_push_docker.yaml",
          "type": "blob",
          "size": 0.3076171875,
          "content": "#!/usr/bin/env bash\n\necho \"Docker builds from the repo, not this dir. Make sure changes are pushed to the repo.\"\n# wait 2 seconds\nsleep 2\ndocker build --build-arg CACHEBUST=$(date +%s) -t aitoolkit:latest -f docker/Dockerfile .\ndocker tag aitoolkit:latest ostris/aitoolkit:latest\ndocker push ostris/aitoolkit:latest"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "extensions",
          "type": "tree",
          "content": null
        },
        {
          "name": "extensions_built_in",
          "type": "tree",
          "content": null
        },
        {
          "name": "flux_train_ui.py",
          "type": "blob",
          "size": 16.408203125,
          "content": "import os\nfrom huggingface_hub import whoami    \nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nimport sys\n\n# Add the current working directory to the Python path\nsys.path.insert(0, os.getcwd())\n\nimport gradio as gr\nfrom PIL import Image\nimport torch\nimport uuid\nimport os\nimport shutil\nimport json\nimport yaml\nfrom slugify import slugify\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\nsys.path.insert(0, \"ai-toolkit\")\nfrom toolkit.job import get_job\n\nMAX_IMAGES = 150\n\ndef load_captioning(uploaded_files, concept_sentence):\n    uploaded_images = [file for file in uploaded_files if not file.endswith('.txt')]\n    txt_files = [file for file in uploaded_files if file.endswith('.txt')]\n    txt_files_dict = {os.path.splitext(os.path.basename(txt_file))[0]: txt_file for txt_file in txt_files}\n    updates = []\n    if len(uploaded_images) <= 1:\n        raise gr.Error(\n            \"Please upload at least 2 images to train your model (the ideal number with default settings is between 4-30)\"\n        )\n    elif len(uploaded_images) > MAX_IMAGES:\n        raise gr.Error(f\"For now, only {MAX_IMAGES} or less images are allowed for training\")\n    # Update for the captioning_area\n    # for _ in range(3):\n    updates.append(gr.update(visible=True))\n    # Update visibility and image for each captioning row and image\n    for i in range(1, MAX_IMAGES + 1):\n        # Determine if the current row and image should be visible\n        visible = i <= len(uploaded_images)\n        \n        # Update visibility of the captioning row\n        updates.append(gr.update(visible=visible))\n\n        # Update for image component - display image if available, otherwise hide\n        image_value = uploaded_images[i - 1] if visible else None\n        updates.append(gr.update(value=image_value, visible=visible))\n        \n        corresponding_caption = False\n        if(image_value):\n            base_name = os.path.splitext(os.path.basename(image_value))[0]\n            print(base_name)\n            print(image_value)\n            if base_name in txt_files_dict:\n                print(\"entrou\")\n                with open(txt_files_dict[base_name], 'r') as file:\n                    corresponding_caption = file.read()\n                    \n        # Update value of captioning area\n        text_value = corresponding_caption if visible and corresponding_caption else \"[trigger]\" if visible and concept_sentence else None\n        updates.append(gr.update(value=text_value, visible=visible))\n\n    # Update for the sample caption area\n    updates.append(gr.update(visible=True))\n    # Update prompt samples\n    updates.append(gr.update(placeholder=f'A portrait of person in a bustling cafe {concept_sentence}', value=f'A person in a bustling cafe {concept_sentence}'))\n    updates.append(gr.update(placeholder=f\"A mountainous landscape in the style of {concept_sentence}\"))\n    updates.append(gr.update(placeholder=f\"A {concept_sentence} in a mall\"))\n    updates.append(gr.update(visible=True))\n    return updates\n\ndef hide_captioning():\n    return gr.update(visible=False), gr.update(visible=False), gr.update(visible=False) \n\ndef create_dataset(*inputs):\n    print(\"Creating dataset\")\n    images = inputs[0]\n    destination_folder = str(f\"datasets/{uuid.uuid4()}\")\n    if not os.path.exists(destination_folder):\n        os.makedirs(destination_folder)\n\n    jsonl_file_path = os.path.join(destination_folder, \"metadata.jsonl\")\n    with open(jsonl_file_path, \"a\") as jsonl_file:\n        for index, image in enumerate(images):\n            new_image_path = shutil.copy(image, destination_folder)\n\n            original_caption = inputs[index + 1]\n            file_name = os.path.basename(new_image_path)\n\n            data = {\"file_name\": file_name, \"prompt\": original_caption}\n\n            jsonl_file.write(json.dumps(data) + \"\\n\")\n\n    return destination_folder\n\n\ndef run_captioning(images, concept_sentence, *captions):\n    #Load internally to not consume resources for training\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    torch_dtype = torch.float16\n    model = AutoModelForCausalLM.from_pretrained(\n        \"multimodalart/Florence-2-large-no-flash-attn\", torch_dtype=torch_dtype, trust_remote_code=True\n    ).to(device)\n    processor = AutoProcessor.from_pretrained(\"multimodalart/Florence-2-large-no-flash-attn\", trust_remote_code=True)\n\n    captions = list(captions)\n    for i, image_path in enumerate(images):\n        print(captions[i])\n        if isinstance(image_path, str):  # If image is a file path\n            image = Image.open(image_path).convert(\"RGB\")\n\n        prompt = \"<DETAILED_CAPTION>\"\n        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n\n        generated_ids = model.generate(\n            input_ids=inputs[\"input_ids\"], pixel_values=inputs[\"pixel_values\"], max_new_tokens=1024, num_beams=3\n        )\n\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n        parsed_answer = processor.post_process_generation(\n            generated_text, task=prompt, image_size=(image.width, image.height)\n        )\n        caption_text = parsed_answer[\"<DETAILED_CAPTION>\"].replace(\"The image shows \", \"\")\n        if concept_sentence:\n            caption_text = f\"{caption_text} [trigger]\"\n        captions[i] = caption_text\n\n        yield captions\n    model.to(\"cpu\")\n    del model\n    del processor\n\ndef recursive_update(d, u):\n    for k, v in u.items():\n        if isinstance(v, dict) and v:\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d\n\ndef start_training(\n    lora_name,\n    concept_sentence,\n    steps,\n    lr,\n    rank,\n    model_to_train,\n    low_vram,\n    dataset_folder,\n    sample_1,\n    sample_2,\n    sample_3,\n    use_more_advanced_options,\n    more_advanced_options,\n):\n    push_to_hub = True\n    if not lora_name:\n        raise gr.Error(\"You forgot to insert your LoRA name! This name has to be unique.\")\n    try:\n        if whoami()[\"auth\"][\"accessToken\"][\"role\"] == \"write\" or \"repo.write\" in whoami()[\"auth\"][\"accessToken\"][\"fineGrained\"][\"scoped\"][0][\"permissions\"]:\n            gr.Info(f\"Starting training locally {whoami()['name']}. Your LoRA will be available locally and in Hugging Face after it finishes.\")\n        else:\n            push_to_hub = False\n            gr.Warning(\"Started training locally. Your LoRa will only be available locally because you didn't login with a `write` token to Hugging Face\")\n    except:\n        push_to_hub = False\n        gr.Warning(\"Started training locally. Your LoRa will only be available locally because you didn't login with a `write` token to Hugging Face\")\n            \n    print(\"Started training\")\n    slugged_lora_name = slugify(lora_name)\n\n    # Load the default config\n    with open(\"config/examples/train_lora_flux_24gb.yaml\", \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Update the config with user inputs\n    config[\"config\"][\"name\"] = slugged_lora_name\n    config[\"config\"][\"process\"][0][\"model\"][\"low_vram\"] = low_vram\n    config[\"config\"][\"process\"][0][\"train\"][\"skip_first_sample\"] = True\n    config[\"config\"][\"process\"][0][\"train\"][\"steps\"] = int(steps)\n    config[\"config\"][\"process\"][0][\"train\"][\"lr\"] = float(lr)\n    config[\"config\"][\"process\"][0][\"network\"][\"linear\"] = int(rank)\n    config[\"config\"][\"process\"][0][\"network\"][\"linear_alpha\"] = int(rank)\n    config[\"config\"][\"process\"][0][\"datasets\"][0][\"folder_path\"] = dataset_folder\n    config[\"config\"][\"process\"][0][\"save\"][\"push_to_hub\"] = push_to_hub\n    if(push_to_hub):\n        try:\n            username = whoami()[\"name\"]\n        except:\n            raise gr.Error(\"Error trying to retrieve your username. Are you sure you are logged in with Hugging Face?\")\n        config[\"config\"][\"process\"][0][\"save\"][\"hf_repo_id\"] = f\"{username}/{slugged_lora_name}\"\n        config[\"config\"][\"process\"][0][\"save\"][\"hf_private\"] = True\n    if concept_sentence:\n        config[\"config\"][\"process\"][0][\"trigger_word\"] = concept_sentence\n    \n    if sample_1 or sample_2 or sample_3:\n        config[\"config\"][\"process\"][0][\"train\"][\"disable_sampling\"] = False\n        config[\"config\"][\"process\"][0][\"sample\"][\"sample_every\"] = steps\n        config[\"config\"][\"process\"][0][\"sample\"][\"sample_steps\"] = 28\n        config[\"config\"][\"process\"][0][\"sample\"][\"prompts\"] = []\n        if sample_1:\n            config[\"config\"][\"process\"][0][\"sample\"][\"prompts\"].append(sample_1)\n        if sample_2:\n            config[\"config\"][\"process\"][0][\"sample\"][\"prompts\"].append(sample_2)\n        if sample_3:\n            config[\"config\"][\"process\"][0][\"sample\"][\"prompts\"].append(sample_3)\n    else:\n        config[\"config\"][\"process\"][0][\"train\"][\"disable_sampling\"] = True\n    if(model_to_train == \"schnell\"):\n        config[\"config\"][\"process\"][0][\"model\"][\"name_or_path\"] = \"black-forest-labs/FLUX.1-schnell\"\n        config[\"config\"][\"process\"][0][\"model\"][\"assistant_lora_path\"] = \"ostris/FLUX.1-schnell-training-adapter\"\n        config[\"config\"][\"process\"][0][\"sample\"][\"sample_steps\"] = 4\n    if(use_more_advanced_options):\n        more_advanced_options_dict = yaml.safe_load(more_advanced_options)\n        config[\"config\"][\"process\"][0] = recursive_update(config[\"config\"][\"process\"][0], more_advanced_options_dict)\n        print(config)\n    \n    # Save the updated config\n    # generate a random name for the config\n    random_config_name = str(uuid.uuid4())\n    os.makedirs(\"tmp\", exist_ok=True)\n    config_path = f\"tmp/{random_config_name}-{slugged_lora_name}.yaml\"\n    with open(config_path, \"w\") as f:\n        yaml.dump(config, f)\n    \n    # run the job locally\n    job = get_job(config_path)\n    job.run()\n    job.cleanup()\n\n    return f\"Training completed successfully. Model saved as {slugged_lora_name}\"\n\nconfig_yaml = '''\ndevice: cuda:0\nmodel:\n  is_flux: true\n  quantize: true\nnetwork:\n  linear: 16 #it will overcome the 'rank' parameter\n  linear_alpha: 16 #you can have an alpha different than the ranking if you'd like\n  type: lora\nsample:\n  guidance_scale: 3.5\n  height: 1024\n  neg: '' #doesn't work for FLUX\n  sample_every: 1000\n  sample_steps: 28\n  sampler: flowmatch\n  seed: 42\n  walk_seed: true\n  width: 1024\nsave:\n  dtype: float16\n  hf_private: true\n  max_step_saves_to_keep: 4\n  push_to_hub: true\n  save_every: 10000\ntrain:\n  batch_size: 1\n  dtype: bf16\n  ema_config:\n    ema_decay: 0.99\n    use_ema: true\n  gradient_accumulation_steps: 1\n  gradient_checkpointing: true\n  noise_scheduler: flowmatch \n  optimizer: adamw8bit #options: prodigy, dadaptation, adamw, adamw8bit, lion, lion8bit\n  train_text_encoder: false #probably doesn't work for flux\n  train_unet: true\n'''\n\ntheme = gr.themes.Monochrome(\n    text_size=gr.themes.Size(lg=\"18px\", md=\"15px\", sm=\"13px\", xl=\"22px\", xs=\"12px\", xxl=\"24px\", xxs=\"9px\"),\n    font=[gr.themes.GoogleFont(\"Source Sans Pro\"), \"ui-sans-serif\", \"system-ui\", \"sans-serif\"],\n)\ncss = \"\"\"\nh1{font-size: 2em}\nh3{margin-top: 0}\n#component-1{text-align:center}\n.main_ui_logged_out{opacity: 0.3; pointer-events: none}\n.tabitem{border: 0px}\n.group_padding{padding: .55em}\n\"\"\"\nwith gr.Blocks(theme=theme, css=css) as demo:\n    gr.Markdown(\n        \"\"\"# LoRA Ease for FLUX 🧞‍♂️\n### Train a high quality FLUX LoRA in a breeze ༄ using [Ostris' AI Toolkit](https://github.com/ostris/ai-toolkit)\"\"\"\n    )\n    with gr.Column() as main_ui:\n        with gr.Row():\n            lora_name = gr.Textbox(\n                label=\"The name of your LoRA\",\n                info=\"This has to be a unique name\",\n                placeholder=\"e.g.: Persian Miniature Painting style, Cat Toy\",\n            )\n            concept_sentence = gr.Textbox(\n                label=\"Trigger word/sentence\",\n                info=\"Trigger word or sentence to be used\",\n                placeholder=\"uncommon word like p3rs0n or trtcrd, or sentence like 'in the style of CNSTLL'\",\n                interactive=True,\n            )\n        with gr.Group(visible=True) as image_upload:\n            with gr.Row():\n                images = gr.File(\n                    file_types=[\"image\", \".txt\"],\n                    label=\"Upload your images\",\n                    file_count=\"multiple\",\n                    interactive=True,\n                    visible=True,\n                    scale=1,\n                )\n                with gr.Column(scale=3, visible=False) as captioning_area:\n                    with gr.Column():\n                        gr.Markdown(\n                            \"\"\"# Custom captioning\n<p style=\"margin-top:0\">You can optionally add a custom caption for each image (or use an AI model for this). [trigger] will represent your concept sentence/trigger word.</p>\n\"\"\", elem_classes=\"group_padding\")\n                        do_captioning = gr.Button(\"Add AI captions with Florence-2\")\n                        output_components = [captioning_area]\n                        caption_list = []\n                        for i in range(1, MAX_IMAGES + 1):\n                            locals()[f\"captioning_row_{i}\"] = gr.Row(visible=False)\n                            with locals()[f\"captioning_row_{i}\"]:\n                                locals()[f\"image_{i}\"] = gr.Image(\n                                    type=\"filepath\",\n                                    width=111,\n                                    height=111,\n                                    min_width=111,\n                                    interactive=False,\n                                    scale=2,\n                                    show_label=False,\n                                    show_share_button=False,\n                                    show_download_button=False,\n                                )\n                                locals()[f\"caption_{i}\"] = gr.Textbox(\n                                    label=f\"Caption {i}\", scale=15, interactive=True\n                                )\n\n                            output_components.append(locals()[f\"captioning_row_{i}\"])\n                            output_components.append(locals()[f\"image_{i}\"])\n                            output_components.append(locals()[f\"caption_{i}\"])\n                            caption_list.append(locals()[f\"caption_{i}\"])\n\n        with gr.Accordion(\"Advanced options\", open=False):\n            steps = gr.Number(label=\"Steps\", value=1000, minimum=1, maximum=10000, step=1)\n            lr = gr.Number(label=\"Learning Rate\", value=4e-4, minimum=1e-6, maximum=1e-3, step=1e-6)\n            rank = gr.Number(label=\"LoRA Rank\", value=16, minimum=4, maximum=128, step=4)\n            model_to_train = gr.Radio([\"dev\", \"schnell\"], value=\"dev\", label=\"Model to train\")\n            low_vram = gr.Checkbox(label=\"Low VRAM\", value=True)\n            with gr.Accordion(\"Even more advanced options\", open=False):\n                use_more_advanced_options = gr.Checkbox(label=\"Use more advanced options\", value=False)\n                more_advanced_options = gr.Code(config_yaml, language=\"yaml\")\n\n        with gr.Accordion(\"Sample prompts (optional)\", visible=False) as sample:\n            gr.Markdown(\n                \"Include sample prompts to test out your trained model. Don't forget to include your trigger word/sentence (optional)\"\n            )\n            sample_1 = gr.Textbox(label=\"Test prompt 1\")\n            sample_2 = gr.Textbox(label=\"Test prompt 2\")\n            sample_3 = gr.Textbox(label=\"Test prompt 3\")\n        \n        output_components.append(sample)\n        output_components.append(sample_1)\n        output_components.append(sample_2)\n        output_components.append(sample_3)\n        start = gr.Button(\"Start training\", visible=False)\n        output_components.append(start)\n        progress_area = gr.Markdown(\"\")\n\n    dataset_folder = gr.State()\n\n    images.upload(\n        load_captioning,\n        inputs=[images, concept_sentence],\n        outputs=output_components\n    )\n    \n    images.delete(\n        load_captioning,\n        inputs=[images, concept_sentence],\n        outputs=output_components\n    )\n\n    images.clear(\n        hide_captioning,\n        outputs=[captioning_area, sample, start]\n    )\n    \n    start.click(fn=create_dataset, inputs=[images] + caption_list, outputs=dataset_folder).then(\n        fn=start_training,\n        inputs=[\n            lora_name,\n            concept_sentence,\n            steps,\n            lr,\n            rank,\n            model_to_train,\n            low_vram,\n            dataset_folder,\n            sample_1,\n            sample_2,\n            sample_3,\n            use_more_advanced_options,\n            more_advanced_options\n        ],\n        outputs=progress_area,\n    )\n\n    do_captioning.click(fn=run_captioning, inputs=[images, concept_sentence] + caption_list, outputs=caption_list)\n\nif __name__ == \"__main__\":\n    demo.launch(share=True, show_error=True)"
        },
        {
          "name": "info.py",
          "type": "blob",
          "size": 0.1689453125,
          "content": "from collections import OrderedDict\n\nv = OrderedDict()\nv[\"name\"] = \"ai-toolkit\"\nv[\"repo\"] = \"https://github.com/ostris/ai-toolkit\"\nv[\"version\"] = \"0.1.0\"\n\nsoftware_meta = v\n"
        },
        {
          "name": "jobs",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "output",
          "type": "tree",
          "content": null
        },
        {
          "name": "repositories",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.44921875,
          "content": "torch\ntorchvision\nsafetensors\ngit+https://github.com/huggingface/diffusers.git\ntransformers\nlycoris-lora==1.8.3\nflatten_json\npyyaml\noyaml\ntensorboard\nkornia\ninvisible-watermark\neinops\naccelerate\ntoml\nalbumentations==1.4.15\nalbucore==0.0.16\npydantic\nomegaconf\nk-diffusion\nopen_clip_torch\ntimm\nprodigyopt\ncontrolnet_aux==0.0.7\npython-dotenv\nbitsandbytes\nhf_transfer\nlpips\npytorch_fid\noptimum-quanto==0.2.4\nsentencepiece\nhuggingface_hub\npeft\ngradio\npython-slugify"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 2.6787109375,
          "content": "import os\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nimport sys\nfrom typing import Union, OrderedDict\nfrom dotenv import load_dotenv\n# Load the .env file if it exists\nload_dotenv()\n\nsys.path.insert(0, os.getcwd())\n# must come before ANY torch or fastai imports\n# import toolkit.cuda_malloc\n\n# turn off diffusers telemetry until I can figure out how to make it opt-in\nos.environ['DISABLE_TELEMETRY'] = 'YES'\n\n# check if we have DEBUG_TOOLKIT in env\nif os.environ.get(\"DEBUG_TOOLKIT\", \"0\") == \"1\":\n    # set torch to trace mode\n    import torch\n    torch.autograd.set_detect_anomaly(True)\nimport argparse\nfrom toolkit.job import get_job\n\n\ndef print_end_message(jobs_completed, jobs_failed):\n    failure_string = f\"{jobs_failed} failure{'' if jobs_failed == 1 else 's'}\" if jobs_failed > 0 else \"\"\n    completed_string = f\"{jobs_completed} completed job{'' if jobs_completed == 1 else 's'}\"\n\n    print(\"\")\n    print(\"========================================\")\n    print(\"Result:\")\n    if len(completed_string) > 0:\n        print(f\" - {completed_string}\")\n    if len(failure_string) > 0:\n        print(f\" - {failure_string}\")\n    print(\"========================================\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    # require at lease one config file\n    parser.add_argument(\n        'config_file_list',\n        nargs='+',\n        type=str,\n        help='Name of config file (eg: person_v1 for config/person_v1.json/yaml), or full path if it is not in config folder, you can pass multiple config files and run them all sequentially'\n    )\n\n    # flag to continue if failed job\n    parser.add_argument(\n        '-r', '--recover',\n        action='store_true',\n        help='Continue running additional jobs even if a job fails'\n    )\n\n    # flag to continue if failed job\n    parser.add_argument(\n        '-n', '--name',\n        type=str,\n        default=None,\n        help='Name to replace [name] tag in config file, useful for shared config file'\n    )\n    args = parser.parse_args()\n\n    config_file_list = args.config_file_list\n    if len(config_file_list) == 0:\n        raise Exception(\"You must provide at least one config file\")\n\n    jobs_completed = 0\n    jobs_failed = 0\n\n    print(f\"Running {len(config_file_list)} job{'' if len(config_file_list) == 1 else 's'}\")\n\n    for config_file in config_file_list:\n        try:\n            job = get_job(config_file, args.name)\n            job.run()\n            job.cleanup()\n            jobs_completed += 1\n        except Exception as e:\n            print(f\"Error running job: {e}\")\n            jobs_failed += 1\n            if not args.recover:\n                print_end_message(jobs_completed, jobs_failed)\n                raise e\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "run_modal.py",
          "type": "blob",
          "size": 5.724609375,
          "content": "'''\n\nostris/ai-toolkit on https://modal.com\nRun training with the following command:\nmodal run run_modal.py --config-file-list-str=/root/ai-toolkit/config/whatever_you_want.yml\n\n'''\n\nimport os\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nimport sys\nimport modal\nfrom dotenv import load_dotenv\n# Load the .env file if it exists\nload_dotenv()\n\nsys.path.insert(0, \"/root/ai-toolkit\")\n# must come before ANY torch or fastai imports\n# import toolkit.cuda_malloc\n\n# turn off diffusers telemetry until I can figure out how to make it opt-in\nos.environ['DISABLE_TELEMETRY'] = 'YES'\n\n# define the volume for storing model outputs, using \"creating volumes lazily\": https://modal.com/docs/guide/volumes\n# you will find your model, samples and optimizer stored in: https://modal.com/storage/your-username/main/flux-lora-models\nmodel_volume = modal.Volume.from_name(\"flux-lora-models\", create_if_missing=True)\n\n# modal_output, due to \"cannot mount volume on non-empty path\" requirement\nMOUNT_DIR = \"/root/ai-toolkit/modal_output\"  # modal_output, due to \"cannot mount volume on non-empty path\" requirement\n\n# define modal app\nimage = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    # install required system and pip packages, more about this modal approach: https://modal.com/docs/examples/dreambooth_app\n    .apt_install(\"libgl1\", \"libglib2.0-0\")\n    .pip_install(\n        \"python-dotenv\",\n        \"torch\", \n        \"diffusers[torch]\", \n        \"transformers\", \n        \"ftfy\", \n        \"torchvision\", \n        \"oyaml\", \n        \"opencv-python\", \n        \"albumentations\",\n        \"safetensors\",\n        \"lycoris-lora==1.8.3\",\n        \"flatten_json\",\n        \"pyyaml\",\n        \"tensorboard\", \n        \"kornia\", \n        \"invisible-watermark\", \n        \"einops\", \n        \"accelerate\", \n        \"toml\", \n        \"pydantic\",\n        \"omegaconf\",\n        \"k-diffusion\",\n        \"open_clip_torch\",\n        \"timm\",\n        \"prodigyopt\",\n        \"controlnet_aux==0.0.7\",\n        \"bitsandbytes\",\n        \"hf_transfer\",\n        \"lpips\", \n        \"pytorch_fid\", \n        \"optimum-quanto\", \n        \"sentencepiece\", \n        \"huggingface_hub\", \n        \"peft\"\n    )\n)\n\n# mount for the entire ai-toolkit directory\n# example: \"/Users/username/ai-toolkit\" is the local directory, \"/root/ai-toolkit\" is the remote directory\ncode_mount = modal.Mount.from_local_dir(\"/Users/username/ai-toolkit\", remote_path=\"/root/ai-toolkit\")\n\n# create the Modal app with the necessary mounts and volumes\napp = modal.App(name=\"flux-lora-training\", image=image, mounts=[code_mount], volumes={MOUNT_DIR: model_volume})\n\n# Check if we have DEBUG_TOOLKIT in env\nif os.environ.get(\"DEBUG_TOOLKIT\", \"0\") == \"1\":\n    # Set torch to trace mode\n    import torch\n    torch.autograd.set_detect_anomaly(True)\n\nimport argparse\nfrom toolkit.job import get_job\n\ndef print_end_message(jobs_completed, jobs_failed):\n    failure_string = f\"{jobs_failed} failure{'' if jobs_failed == 1 else 's'}\" if jobs_failed > 0 else \"\"\n    completed_string = f\"{jobs_completed} completed job{'' if jobs_completed == 1 else 's'}\"\n\n    print(\"\")\n    print(\"========================================\")\n    print(\"Result:\")\n    if len(completed_string) > 0:\n        print(f\" - {completed_string}\")\n    if len(failure_string) > 0:\n        print(f\" - {failure_string}\")\n    print(\"========================================\")\n\n\n@app.function(\n    # request a GPU with at least 24GB VRAM\n    # more about modal GPU's: https://modal.com/docs/guide/gpu\n    gpu=\"A100\", # gpu=\"H100\"\n    # more about modal timeouts: https://modal.com/docs/guide/timeouts\n    timeout=7200  # 2 hours, increase or decrease if needed\n)\ndef main(config_file_list_str: str, recover: bool = False, name: str = None):\n    # convert the config file list from a string to a list\n    config_file_list = config_file_list_str.split(\",\")\n\n    jobs_completed = 0\n    jobs_failed = 0\n\n    print(f\"Running {len(config_file_list)} job{'' if len(config_file_list) == 1 else 's'}\")\n\n    for config_file in config_file_list:\n        try:\n            job = get_job(config_file, name)\n            \n            job.config['process'][0]['training_folder'] = MOUNT_DIR\n            os.makedirs(MOUNT_DIR, exist_ok=True)\n            print(f\"Training outputs will be saved to: {MOUNT_DIR}\")\n            \n            # run the job\n            job.run()\n            \n            # commit the volume after training\n            model_volume.commit()\n            \n            job.cleanup()\n            jobs_completed += 1\n        except Exception as e:\n            print(f\"Error running job: {e}\")\n            jobs_failed += 1\n            if not recover:\n                print_end_message(jobs_completed, jobs_failed)\n                raise e\n\n    print_end_message(jobs_completed, jobs_failed)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    # require at least one config file\n    parser.add_argument(\n        'config_file_list',\n        nargs='+',\n        type=str,\n        help='Name of config file (eg: person_v1 for config/person_v1.json/yaml), or full path if it is not in config folder, you can pass multiple config files and run them all sequentially'\n    )\n\n    # flag to continue if a job fails\n    parser.add_argument(\n        '-r', '--recover',\n        action='store_true',\n        help='Continue running additional jobs even if a job fails'\n    )\n\n    # optional name replacement for config file\n    parser.add_argument(\n        '-n', '--name',\n        type=str,\n        default=None,\n        help='Name to replace [name] tag in config file, useful for shared config file'\n    )\n    args = parser.parse_args()\n\n    # convert list of config files to a comma-separated string for Modal compatibility\n    config_file_list_str = \",\".join(args.config_file_list)\n\n    main.call(config_file_list_str=config_file_list_str, recover=args.recover, name=args.name)\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "testing",
          "type": "tree",
          "content": null
        },
        {
          "name": "toolkit",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}