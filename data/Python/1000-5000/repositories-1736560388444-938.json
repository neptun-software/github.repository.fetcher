{
  "metadata": {
    "timestamp": 1736560388444,
    "page": 938,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVlabs/eg3d",
      "stars": 3263,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2138671875,
          "content": "images/\n.DS_Store\n.ipynb_checkpoints\ndata\noutput\ndebug\n*.pyc\ndeep-head-pose\nEvalImages\ncache*\n*.pkl\ngif\narchive\n*.ply\neval\nout\n\n# evaluation:\ntemp/\nshapes/\nimgs/\nvids/\n*.mp4\n\nstylegan3/results\neg3d/results\neg3d_results\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.1796875,
          "content": "[submodule \"dataset_preprocessing/ffhq/Deep3DFaceRecon_pytorch\"]\n\tpath = dataset_preprocessing/ffhq/Deep3DFaceRecon_pytorch\n\turl = https://github.com/sicxu/Deep3DFaceRecon_pytorch.git\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 4.3603515625,
          "content": "Copyright (c) 2021-2022, NVIDIA Corporation & affiliates. All rights\nreserved.\n\n\nNVIDIA Source Code License for EG3D\n\n\n=======================================================================\n\n1. Definitions\n\n\"Licensor\" means any person or entity that distributes its Work.\n\n\"Software\" means the original work of authorship made available under\nthis License.\n\n\"Work\" means the Software and any additions to or derivative works of\nthe Software that are made available under this License.\n\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and\n\"distribution\" have the meaning as provided under U.S. copyright law;\nprovided, however, that for the purposes of this License, derivative\nworks shall not include works that remain separable from, or merely\nlink (or bind by name) to the interfaces of, the Work.\n\nWorks, including the Software, are \"made available\" under this License\nby including in or with the Work either (a) a copyright notice\nreferencing the applicability of this License to the Work, or (b) a\ncopy of this License.\n\n2. License Grants\n\n    2.1 Copyright Grant. Subject to the terms and conditions of this\n    License, each Licensor grants to you a perpetual, worldwide,\n    non-exclusive, royalty-free, copyright license to reproduce,\n    prepare derivative works of, publicly display, publicly perform,\n    sublicense and distribute its Work and any resulting derivative\n    works in any form.\n\n3. Limitations\n\n    3.1 Redistribution. You may reproduce or distribute the Work only\n    if (a) you do so under this License, (b) you include a complete\n    copy of this License with your distribution, and (c) you retain\n    without modification any copyright, patent, trademark, or\n    attribution notices that are present in the Work.\n\n    3.2 Derivative Works. You may specify that additional or different\n    terms apply to the use, reproduction, and distribution of your\n    derivative works of the Work (\"Your Terms\") only if (a) Your Terms\n    provide that the use limitation in Section 3.3 applies to your\n    derivative works, and (b) you identify the specific derivative\n    works that are subject to Your Terms. Notwithstanding Your Terms,\n    this License (including the redistribution requirements in Section\n    3.1) will continue to apply to the Work itself.\n\n    3.3 Use Limitation. The Work and any derivative works thereof only\n    may be used or intended for use non-commercially. The Work or\n    derivative works thereof may be used or intended for use by NVIDIA\n    or it’s affiliates commercially or non-commercially. As used\n    herein, \"non-commercially\" means for research or evaluation\n    purposes only and not for any direct or indirect monetary gain.\n\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim\n    against any Licensor (including any claim, cross-claim or\n    counterclaim in a lawsuit) to enforce any patents that you allege\n    are infringed by any Work, then your rights under this License from\n    such Licensor (including the grants in Sections 2.1) will terminate\n    immediately.\n\n    3.5 Trademarks. This License does not grant any rights to use any\n    Licensor’s or its affiliates’ names, logos, or trademarks, except\n    as necessary to reproduce the notices described in this License.\n\n    3.6 Termination. If you violate any term of this License, then your\n    rights under this License (including the grants in Sections 2.1)\n    will terminate immediately.\n\n4. Disclaimer of Warranty.\n\nTHE WORK IS PROVIDED \"AS IS\" WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR\nNON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER\nTHIS LICENSE.\n\n5. Limitation of Liability.\n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL\nTHEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE\nSHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF\nOR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK\n(INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION,\nLOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER\nCOMMERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF\nTHE POSSIBILITY OF SUCH DAMAGES.\n\n=======================================================================\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.6025390625,
          "content": "## Efficient Geometry-aware 3D Generative Adversarial Networks (EG3D)<br><sub>Official PyTorch implementation of the CVPR 2022 paper</sub>\n\n![Teaser image](./docs/teaser.jpeg)\n\n**Efficient Geometry-aware 3D Generative Adversarial Networks**<br>\nEric R. Chan*, Connor Z. Lin*, Matthew A. Chan*, Koki Nagano*, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein<br>*\\* equal contribution*<br>\n<br>https://nvlabs.github.io/eg3d/<br>\n\nAbstract: *Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.*\n\nFor business inquiries, please visit our website and submit the form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)\n\n## Requirements\n\n* We recommend Linux for performance and compatibility reasons.\n* 1&ndash;8 high-end NVIDIA GPUs. We have done all testing and development using V100, RTX3090, and A100 GPUs.\n* 64-bit Python 3.8 and PyTorch 1.11.0 (or later). See https://pytorch.org for PyTorch install instructions.\n* CUDA toolkit 11.3 or later.  (Why is a separate CUDA toolkit installation required?  We use the custom CUDA extensions from the StyleGAN3 repo. Please see [Troubleshooting](https://github.com/NVlabs/stylegan3/blob/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary)).\n* Python libraries: see [environment.yml](./eg3d/environment.yml) for exact library dependencies.  You can use the following commands with Miniconda3 to create and activate your Python environment:\n  - `cd eg3d`\n  - `conda env create -f environment.yml`\n  - `conda activate eg3d`\n\n## Getting started\n\nPre-trained networks are stored as `*.pkl` files that can be referenced using local filenames. See [Models](./docs/models.md) for download links to pre-trained checkpoints.\n\n\n## Generating media\n\n```.bash\n# Generate videos using pre-trained model\n\npython gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=2x2 \\\n    --network=networks/network_snapshot.pkl\n\n# Generate the same 4 seeds in an interpolation sequence\n\npython gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=1x1 \\\n    --network=networks/network_snapshot.pkl\n```\n\n```.bash\n# Generate images and shapes (as .mrc files) using pre-trained model\n\npython gen_samples.py --outdir=out --trunc=0.7 --shapes=true --seeds=0-3 \\\n    --network=networks/network_snapshot.pkl\n```\n\nWe visualize our .mrc shape files with [UCSF Chimerax](https://www.cgl.ucsf.edu/chimerax/).\n\nTo visualize a shape in ChimeraX do the following:\n1. Import the `.mrc` file with `File > Open`\n1. Find the selected shape in the Volume Viewer tool\n    1. The Volume Viewer tool is located under `Tools > Volume Data > Volume Viewer`\n1. Change volume type to \"Surface\"\n1. Change step size to 1\n1. Change level set to 10\n    1. Note that the optimal level can vary by each object, but is usually between 2 and 20. Individual adjustment may make certain shapes slightly sharper\n1. In the `Lighting` menu in the top bar, change lighting to \"Full\"\n\n\n## Interactive visualization\n\nThis release contains an interactive model visualization tool that can be used to explore various characteristics of a trained model.  To start it, run:\n\n```.bash\npython visualizer.py\n```\n\nSee the [`Visualizer Guide`](./docs/visualizer_guide.md) for a description of important options.\n\n\n## Using networks from Python\n\nYou can use pre-trained networks in your own Python code as follows:\n\n```.python\nwith open('ffhq.pkl', 'rb') as f:\n    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module\nz = torch.randn([1, G.z_dim]).cuda()    # latent codes\nc = torch.cat([cam2world_pose.reshape(-1, 16), intrinsics.reshape(-1, 9)], 1) # camera parameters\nimg = G(z, c)['image']                           # NCHW, float32, dynamic range [-1, +1], no truncation\n```\n\nThe above code requires `torch_utils` and `dnnlib` to be accessible via `PYTHONPATH`. It does not need source code for the networks themselves &mdash; their class definitions are loaded from the pickle via `torch_utils.persistence`.\n\nThe pickle contains three networks. `'G'` and `'D'` are instantaneous snapshots taken during training, and `'G_ema'` represents a moving average of the generator weights over several training steps. The networks are regular instances of `torch.nn.Module`, with all of their parameters and buffers placed on the CPU at import and gradient computation disabled by default.\n\nThe generator consists of two submodules, `G.mapping` and `G.synthesis`, that can be executed separately. They also support various additional options:\n\n```.python\nw = G.mapping(z, conditioning_params, truncation_psi=0.5, truncation_cutoff=8)\nimg = G.synthesis(w, camera_params)['image]\n```\n\nPlease refer to [`gen_samples.py`](eg3d/gen_samples.py) for complete code example.\n\n## Preparing datasets\n\nDatasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels. Each label is a 25-length list of floating point numbers, which is the concatenation of the flattened 4x4 camera extrinsic matrix and flattened 3x3 camera intrinsic matrix. Custom datasets can be created from a folder containing images; see `python dataset_tool.py --help` for more information. Alternatively, the folder can also be used directly as a dataset, without running it through `dataset_tool.py` first, but doing so may lead to suboptimal performance.\n\n**FFHQ**: Download and process the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) using the following commands.\n\n1. Ensure the [Deep3DFaceRecon_pytorch](https://github.com/sicxu/Deep3DFaceRecon_pytorch/tree/6ba3d22f84bf508f0dde002da8fff277196fef21) submodule is properly initialized\n```.bash\ngit submodule update --init --recursive\n```\n\n2. Run the following commands\n```.bash\ncd dataset_preprocessing/ffhq\npython runme.py\n```\n\nOptional: preprocessing in-the-wild portrait images. \nIn case you want to crop in-the-wild face images and extract poses using [Deep3DFaceRecon_pytorch](https://github.com/sicxu/Deep3DFaceRecon_pytorch/tree/6ba3d22f84bf508f0dde002da8fff277196fef21) in a way that align with the FFHQ data above and the checkpoint, run the following commands \n```.bash\ncd dataset_preprocessing/ffhq\npython preprocess_in_the_wild.py --indir=INPUT_IMAGE_FOLDER\n```\n\n\n**AFHQv2**: Download and process the [AFHQv2 dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) with the following.\n\n1. Download the AFHQv2 images zipfile from the [StarGAN V2 repository](https://github.com/clovaai/stargan-v2/)\n2. Run the following commands:\n```.bash\ncd dataset_preprocessing/afhq\npython runme.py \"path/to/downloaded/afhq.zip\"\n```\n\n**ShapeNet Cars**: Download and process renderings of the cars category of [ShapeNet](https://shapenet.org/) using the following commands.\nNOTE: the following commands download renderings of the ShapeNet cars from the [Scene Representation Networks repository](https://www.vincentsitzmann.com/srns/).\n\n```.bash\ncd dataset_preprocessing/shapenet\npython runme.py\n```\n\n## Training\n\nYou can train new networks using `train.py`. For example:\n\n```.bash\n# Train with FFHQ from scratch with raw neural rendering resolution=64, using 8 GPUs.\npython train.py --outdir=~/training-runs --cfg=ffhq --data=~/datasets/FFHQ_512.zip \\\n  --gpus=8 --batch=32 --gamma=1 --gen_pose_cond=True\n\n# Second stage finetuning of FFHQ to 128 neural rendering resolution (optional).\npython train.py --outdir=~/training-runs --cfg=ffhq --data=~/datasets/FFHQ_512.zip \\\n  --resume=~/training-runs/ffhq_experiment_dir/network-snapshot-025000.pkl \\\n  --gpus=8 --batch=32 --gamma=1 --gen_pose_cond=True --neural_rendering_resolution_final=128\n\n# Train with Shapenet from scratch, using 8 GPUs.\npython train.py --outdir=~/training-runs --cfg=shapenet --data=~/datasets/cars_train.zip \\\n  --gpus=8 --batch=32 --gamma=0.3\n\n# Train with AFHQ, finetuning from FFHQ with ADA, using 8 GPUs.\npython train.py --outdir=~/training-runs --cfg=afhq --data=~/datasets/afhq.zip \\\n  --gpus=8 --batch=32 --gamma=5 --aug=ada --neural_rendering_resolution_final=128 --gen_pose_cond=True --gpc_reg_prob=0.8\n```\n\nPlease see the [Training Guide](./docs/training_guide.md) for a guide to setting up a training run on your own data.\n\nPlease see [Models](./docs/models.md) for recommended training configurations and download links for pre-trained checkpoints.\n\n\nThe results of each training run are saved to a newly created directory, for example `~/training-runs/00000-ffhq-ffhq512-gpus8-batch32-gamma1`. The training loop exports network pickles (`network-snapshot-<KIMG>.pkl`) and random image grids (`fakes<KIMG>.png`) at regular intervals (controlled by `--snap`). For each exported pickle, it evaluates FID (controlled by `--metrics`) and logs the result in `metric-fid50k_full.jsonl`. It also records various statistics in `training_stats.jsonl`, as well as `*.tfevents` if TensorBoard is installed.\n\n## Quality metrics\n\nBy default, `train.py` automatically computes FID for each network pickle exported during training. We recommend inspecting `metric-fid50k_full.jsonl` (or TensorBoard) at regular intervals to monitor the training progress. When desired, the automatic computation can be disabled with `--metrics=none` to speed up the training slightly.\n\nAdditional quality metrics can also be computed after the training:\n\n```.bash\n# Previous training run: look up options automatically, save result to JSONL file.\npython calc_metrics.py --metrics=fid50k_full \\\n    --network=~/training-runs/network-snapshot-000000.pkl\n\n# Pre-trained network pickle: specify dataset explicitly, print result to stdout.\npython calc_metrics.py --metrics=fid50k_full --data=~/datasets/ffhq_512.zip \\\n    --network=ffhq-128.pkl\n```\n\nNote that the metrics can be quite expensive to compute (up to 1h), and many of them have an additional one-off cost for each new dataset (up to 30min). Also note that the evaluation is done using a different random seed each time, so the results will vary if the same metric is computed multiple times.\n\nReferences:\n1. [GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500), Heusel et al. 2017\n2. [Demystifying MMD GANs](https://arxiv.org/abs/1801.01401), Bi&nacute;kowski et al. 2018\n\n<!-- ## License\n\nCopyright &copy; 2021, NVIDIA Corporation & affiliates. All rights reserved.\n\nThis work is made available under the [Nvidia Source Code License](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt). -->\n\n## Citation\n\n```\n@inproceedings{Chan2022,\n  author = {Eric R. Chan and Connor Z. Lin and Matthew A. Chan and Koki Nagano and Boxiao Pan and Shalini De Mello and Orazio Gallo and Leonidas Guibas and Jonathan Tremblay and Sameh Khamis and Tero Karras and Gordon Wetzstein},\n  title = {Efficient Geometry-aware {3D} Generative Adversarial Networks},\n  booktitle = {CVPR},\n  year = {2022}\n}\n```\n\n## Development\n\nThis is a research reference implementation and is treated as a one-time code drop. As such, we do not accept outside code contributions in the form of pull requests.\n\n## Acknowledgements\n\nWe thank David Luebke, Jan Kautz, Jaewoo Seo, Jonathan Granskog, Simon Yuen, Alex Evans, Stan Birchfield, Alexander Bergman, and Joy Hsu for feedback on drafts, Alex Chan, Giap Nguyen, and Trevor Chan for help with diagrams, and Colette Kress and Bryan Catanzaro for allowing use of their photographs. This project was in part supported by Stanford HAI and a Samsung GRO. Koki Nagano and Eric Chan were partially supported by DARPA’s Semantic Forensics (SemaFor) contract (HR0011-20-3-0005). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. Distribution Statement \"A\" (Approved for Public Release, Distribution Unlimited).\n"
        },
        {
          "name": "dataset_preprocessing",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "eg3d",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}