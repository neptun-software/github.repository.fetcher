{
  "metadata": {
    "timestamp": 1736560360980,
    "page": 895,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "modelscope/data-juicer",
      "stars": 3313,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.26171875,
          "content": "[run]\nomit =\n    # avoid measuring strange non-existing files\n    /workspace/config.py\n    /workspace/config-3.py\n\n    # avoid measuring third-party dist packages\n    */dist-packages/*\n\n    # avoid measuring code of unittest\n    tests/*\n\n[report]\nignore_errors = True\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.15625,
          "content": "\n# data & resources\noutputs/\nassets/\n\n# setup\ndata_juicer.egg-info/\npy_data_juicer.egg-info/\nbuild/\ndist\n\n# others\n.DS_Store\n.idea/\nwandb/\n__pycache__\n.vscode/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.1572265625,
          "content": "repos:\n  - repo: https://github.com/PyCQA/flake8\n    rev: 4.0.1\n    hooks:\n      - id: flake8\n  - repo: https://github.com/PyCQA/isort.git\n    rev: 5.12.0\n    hooks:\n      - id: isort\n  - repo: https://github.com/pre-commit/mirrors-yapf\n    rev: v0.32.0\n    hooks:\n      - id: yapf\n        exclude: data_juicer/ops/common/special_characters.py\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n      - id: trailing-whitespace\n        exclude: thirdparty/\n      - id: check-yaml\n        exclude: thirdparty/\n      - id: end-of-file-fixer\n        exclude: thirdparty/\n      - id: requirements-txt-fixer\n        exclude: thirdparty/\n      - id: double-quote-string-fixer\n        exclude: ^(thirdparty/|data_juicer/ops/common/special_characters.py)\n      - id: check-merge-conflict\n        exclude: thirdparty/\n      - id: fix-encoding-pragma\n        exclude: thirdparty/\n        args: [ \"--remove\" ]\n      - id: mixed-line-ending\n        exclude: thirdparty/\n        args: [ \"--fix=lf\" ]\n\nexclude: |\n  (?x)^(\n    docs/.*|\n    tests/.*|\n    demos/(?!api_service/).*|\n    tools/mm_eval/inception_metrics/.*|\n    thirdparty/easy_animate/.*|\n    .*\\.md\n  )$\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.3349609375,
          "content": "# The data-juicer image includes all open-source contents of data-juicer,\n# and it will be instaled in editable mode.\n\nFROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04\n\n# install python 3.10\nRUN apt-get update \\\n    && apt-get install -y git curl vim wget python3.10 libpython3.10-dev python3-pip \\\n    && apt-get install -y libgl1-mesa-glx libglib2.0-0 \\\n    && ln -sf /usr/bin/python3.10  /usr/bin/python3 \\\n    && ln -sf /usr/bin/python3.10  /usr/bin/python \\\n    && apt-get autoclean && rm -rf /var/lib/apt/lists/* \\\n    && pip install --upgrade pip\n\n# install 3rd-party system dependencies\nRUN apt-get update \\\n    && apt-get install ffmpeg libsm6 libxext6 software-properties-common build-essential cmake gfortran libopenblas-dev liblapack-dev -y\n\n# prepare the java env\nWORKDIR /opt\n# download jdk\nRUN wget https://aka.ms/download-jdk/microsoft-jdk-17.0.9-linux-x64.tar.gz -O jdk.tar.gz \\\n    && tar -xzf jdk.tar.gz \\\n    && rm -rf jdk.tar.gz \\\n    && mv jdk-17.0.9+8 jdk\n\n# set the environment variable\nENV JAVA_HOME=/opt/jdk\n\nWORKDIR /data-juicer\n\n# install requirements which need to be installed from source\nRUN pip install --upgrade setuptools==69.5.1 setuptools_scm \\\n    && pip install git+https://github.com/xinyu1205/recognize-anything.git --default-timeout 1000\n\n# install data-juicer then\nCOPY . .\nRUN pip install -v -e .[all] --default-timeout 1000\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 20.4150390625,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Alibaba\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n-------------------------------------------------------------------------------\n\nCode in data_juicer/ops/common/helper_func.py, data_juicer/ops/deduplicator/document_deduplicator.py,\ndata_juicer/ops/deduplicator/document_simhash_deduplicator.py, data_juicer/ops/filter/character_repetition_filter.py,\ndata_juicer/ops/filter/flagged_words_filter.py, data_juicer/ops/filter/perplexity_filter.py,\ndata_juicer/ops/filter/special_characters_filter.py, data_juicer/ops/filter/stopwords_filter.py,\ndata_juicer/ops/filter/word_repetition_filter.py, data_juicer/ops/mapper/punctuation_normalization_mapper.py,\ndata_juicer/ops/mapper/remove_long_words_mapper.py, app.py is adapted from\nhttps://huggingface.co/spaces/huggingface/text-data-filtering or\nhttps://github.com/bigscience-workshop/data-preparation\n\n   Copyright [2021] [Bigscience]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n-------------------------------------------------------------------------------\n\nCode in data_juicer/ops/deduplicator/document_minhash_deduplicator.py is\nadapted from\nhttps://github.com/bigcode-project/bigcode-dataset\n\n   Copyright 2022 bigcode authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n-------------------------------------------------------------------------------\n\nCode in data_juicer/ops/mapper/clean_copyright_mapper.py, data_juicer/ops/mapper/clean_html_mapper.py,\ndata_juicer/ops/mapper/expand_macro_mapper.py, data_juicer/ops/mapper/remove_bibliography_mapper.py,\ndata_juicer/ops/mapper/remove_comments_mapper.py, data_juicer/ops/mapper/remove_header_mapper.py,\nis adapted from\nhttps://github.com/togethercomputer/RedPajama-Data/tree/rp_v1/\n\n   Copyright 2023 RedPajama authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n-------------------------------------------------------------------------------\n\nThe implementations of gpt_evaluator in tools/evaluator/gpt_eval/gpt_evaluator.py\nis adapted from https://github.com/lm-sys/FastChat (Apache License)\n\nCopyright (c) 2023 The FastChat Authors\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n-----------------------------------------------------\n\n\nThe implementations of checkpoint converter in tools/converter/\nconvert_gpt_to_transformers.py and tools/converter/modeling_megatron_llama.py\nare adapted from https://github.com/huggingface/transformers (Apache License)\n\nCopyright (c) 2022 EleutherAI and the HuggingFace Inc. team.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n-----------------------------------------------------\n\nCode in thirdparty/Megatron-LM\nis adapted from https://github.com/NVIDIA/Megatron-LM\n\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n-----------------------------------------------------\n\nCode in thirdparty/helm\nis adapted from https://github.com/stanford-crfm/helm (Apache License)\n\nCopyright (c) 2023 The helm Authors\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n-----------------------------------------------------\n\nCode in tests/run.py is adapted from https://github\n.com/alibaba/FederatedScope/blob/master/tests/run.py (Apache License)\n\nCopyright (c) 2023 The FederatedScope Team\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n-----------------------------------------------------\n\nCode in utils/logger_utils.py is adapted from https://github.com/MegEngine/\nYOLOX/blob/main/yolox/utils/logger.py (Apache License)\n\nCopyright 2021 Megvii, Base Detection\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n-----------------------------------------------------\n\nCode in tools/__init__.py is adapted from\nhttps://github.com/Megvii-BaseDetection/YOLOX/blob/main/yolox/tools/__init__.py\n(Apache License)\n\nCopyright (c) 2021-2022 Megvii Inc. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.6845703125,
          "content": "[[ä¸­æ–‡ä¸»é¡µ]](README_ZH.md) | [[Docs]](#documents) | [[API]](https://modelscope.github.io/data-juicer) |  [[DJ-SORA]](docs/DJ_SORA.md) | [[Awesome List]](docs/awesome_llm_data.md)\n\n\n# Data-Juicer: A One-Stop Data Processing System for Large Language Models\n\n <img src=\"https://img.alicdn.com/imgextra/i3/O1CN017Eq5kf27AlA2NUKef_!!6000000007757-0-tps-1280-720.jpg\" width = \"640\" height = \"360\" alt=\"Data-Juicer\"/>\n\n![](https://img.shields.io/badge/language-Python-214870.svg)\n![](https://img.shields.io/badge/license-Apache--2.0-000000.svg)\n[![pypi version](https://img.shields.io/pypi/v/py-data-juicer?logo=pypi&color=026cad)](https://pypi.org/project/py-data-juicer)\n[![Docker version](https://img.shields.io/docker/v/datajuicer/data-juicer?logo=docker&label=Docker&color=498bdf)](https://hub.docker.com/r/datajuicer/data-juicer)\n\n[![DataModality](https://img.shields.io/badge/DataModality-Text,Image,Audio,Video-brightgreen.svg)](docs/DeveloperGuide_ZH.md)\n[![Usage](https://img.shields.io/badge/Usage-Cleaning,Generation,Analysis-FFD21E.svg)](docs/DeveloperGuide_ZH.md)\n[![ModelScope- Demos](https://img.shields.io/badge/ModelScope-Demos-4e29ff.svg?logo=data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjI0IDEyMS4zMyIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxwYXRoIGQ9Im0wIDQ3Ljg0aDI1LjY1djI1LjY1aC0yNS42NXoiIGZpbGw9IiM2MjRhZmYiIC8+Cgk8cGF0aCBkPSJtOTkuMTQgNzMuNDloMjUuNjV2MjUuNjVoLTI1LjY1eiIgZmlsbD0iIzYyNGFmZiIgLz4KCTxwYXRoIGQ9Im0xNzYuMDkgOTkuMTRoLTI1LjY1djIyLjE5aDQ3Ljg0di00Ny44NGgtMjIuMTl6IiBmaWxsPSIjNjI0YWZmIiAvPgoJPHBhdGggZD0ibTEyNC43OSA0Ny44NGgyNS42NXYyNS42NWgtMjUuNjV6IiBmaWxsPSIjMzZjZmQxIiAvPgoJPHBhdGggZD0ibTAgMjIuMTloMjUuNjV2MjUuNjVoLTI1LjY1eiIgZmlsbD0iIzM2Y2ZkMSIgLz4KCTxwYXRoIGQ9Im0xOTguMjggNDcuODRoMjUuNjV2MjUuNjVoLTI1LjY1eiIgZmlsbD0iIzYyNGFmZiIgLz4KCTxwYXRoIGQ9Im0xOTguMjggMjIuMTloMjUuNjV2MjUuNjVoLTI1LjY1eiIgZmlsbD0iIzM2Y2ZkMSIgLz4KCTxwYXRoIGQ9Im0xNTAuNDQgMHYyMi4xOWgyNS42NXYyNS42NWgyMi4xOXYtNDcuODR6IiBmaWxsPSIjNjI0YWZmIiAvPgoJPHBhdGggZD0ibTczLjQ5IDQ3Ljg0aDI1LjY1djI1LjY1aC0yNS42NXoiIGZpbGw9IiMzNmNmZDEiIC8+Cgk8cGF0aCBkPSJtNDcuODQgMjIuMTloMjUuNjV2LTIyLjE5aC00Ny44NHY0Ny44NGgyMi4xOXoiIGZpbGw9IiM2MjRhZmYiIC8+Cgk8cGF0aCBkPSJtNDcuODQgNzMuNDloLTIyLjE5djQ3Ljg0aDQ3Ljg0di0yMi4xOWgtMjUuNjV6IiBmaWxsPSIjNjI0YWZmIiAvPgo8L3N2Zz4K)](https://modelscope.cn/studios?name=Data-Jiucer&page=1&sort=latest&type=1)\n[![HuggingFace- Demos](https://img.shields.io/badge/ðŸ¤—HuggingFace-Demos-4e29ff.svg)](https://huggingface.co/spaces?&search=datajuicer)\n\n\n\n[![Document_List](https://img.shields.io/badge/Docs-English-blue?logo=Markdown)](#documents)\n[![æ–‡æ¡£åˆ—è¡¨](https://img.shields.io/badge/æ–‡æ¡£-ä¸­æ–‡-blue?logo=Markdown)](README_ZH.md#documents)\n[![API Reference](https://img.shields.io/badge/Docs-API_Reference-blue?logo=Markdown)](https://modelscope.github.io/data-juicer/)\n[![Paper](http://img.shields.io/badge/cs.LG-arXiv%3A2309.02033-B31B1B?logo=arxiv&logoColor=red)](https://arxiv.org/abs/2309.02033)\n\n\n\n\nData-Juicer is a one-stop **multimodal** data processing system to make data higher-quality,\njuicier, and more digestible for LLMs.\n\n\nWe provide a [playground](http://8.138.149.181/) with a managed JupyterLab. [Try Data-Juicer](http://8.138.149.181/) straight away in your browser! If you find Data-Juicer useful for your research or development, please kindly cite our [work](#references).\n\n[Platform for AI of Alibaba Cloud (PAI)](https://www.aliyun.com/product/bigdata/learn) has cited our work and integrated Data-Juicer into its data processing products. PAI is an AI Native large model and AIGC engineering platform that provides dataset management, computing power management, model tool chain, model development, model training, model deployment, and AI asset management. For documentation on data processing, please refer to: [PAI-Data Processing for Large Models](https://help.aliyun.com/zh/pai/user-guide/components-related-to-data-processing-for-foundation-models/?spm=a2c4g.11186623.0.0.3e9821a69kWdvX).\n\nData-Juicer is being actively updated and maintained. We will periodically enhance and add more features, data recipes and datasets. \nWe welcome you to join us (via issues, PRs, [Slack](https://join.slack.com/t/data-juicer/shared_invite/zt-23zxltg9d-Z4d3EJuhZbCLGwtnLWWUDg?spm=a2c22.12281976.0.0.7a8253f30mgpjw)  channel, [DingDing](https://qr.dingtalk.com/action/joingroup?code=v1,k1,YFIXM2leDEk7gJP5aMC95AfYT+Oo/EP/ihnaIEhMyJM=&_dt_no_comment=1&origin=11) group, ...), in promoting data-model co-development along with research and applications of (multimodal) LLMs!\n\n----\n\n## News\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png) [2024-08-09] We propose Img-Diff, which enhances the performance of multimodal large language models through *contrastive data synthesis*, achieving a score that is 12 points higher than GPT-4V on the [MMVP benchmark](https://tsb0601.github.io/mmvp_blog/). See more details in our [paper](https://arxiv.org/abs/2408.04594), and download the dataset from [huggingface](https://huggingface.co/datasets/datajuicer/Img-Diff) and [modelscope](https://modelscope.cn/datasets/Data-Juicer/Img-Diff).\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png) [2024-07-24] \"Tianchi Better Synth Data Synthesis Competition for Multimodal Large Models\" â€” Our 4th data-centric LLM competition has kicked off! Please visit the competition's [official website](https://tianchi.aliyun.com/competition/entrance/532251) for more information.\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png) [2024-07-17] We utilized the Data-Juicer [Sandbox Laboratory Suite](https://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md) to systematically optimize data and models through a co-development workflow between data and models, achieving a new top spot on the [VBench](https://huggingface.co/spaces/Vchitect/VBench_Leaderboard) text-to-video leaderboard. The related achievements have been compiled and published in a [paper](http://arxiv.org/abs/2407.11784), and the model has been released on the [ModelScope](https://modelscope.cn/models/Data-Juicer/Data-Juicer-T2V) and [HuggingFace](https://huggingface.co/datajuicer/Data-Juicer-T2V) platforms.\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png) [2024-07-12] Our *awesome list of MLLM-Data* has evolved into a systemic [survey](https://arxiv.org/abs/2407.08583) from model-data co-development perspective. Welcome to [explore](docs/awesome_llm_data.md) and contribute!\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png) [2024-06-01] ModelScope-Sora \"Data Directors\" creative sprintâ€”Our third data-centric LLM competition has kicked off! Please visit the competition's [official website](https://tianchi.aliyun.com/competition/entrance/532219) for more information.\n\n<details>\n<summary> History News:\n</summary>>\n\n- [2024-03-07] We release **Data-Juicer [v0.2.0](https://github.com/alibaba/data-juicer/releases/tag/v0.2.0)** now! \nIn this new version, we support more features for **multimodal data (including video now)**, and introduce **[DJ-SORA](docs/DJ_SORA.md)** to provide open large-scale, high-quality datasets for SORA-like models.\n- [2024-02-20] We have actively maintained an *awesome list of LLM-Data*, welcome to [visit](docs/awesome_llm_data.md) and contribute!\n- [2024-02-05] Our paper has been accepted by SIGMOD'24 industrial track!\n- [2024-01-10] Discover new horizons in \"Data Mixture\"â€”Our second data-centric LLM competition has kicked off! Please visit the competition's [official website](https://tianchi.aliyun.com/competition/entrance/532174) for more information.\n- [2024-01-05] We release **Data-Juicer v0.1.3** now!\nIn this new version, we support **more Python versions** (3.8-3.10), and support **multimodal** dataset [converting](tools/fmt_conversion/multimodal/README.md)/[processing](docs/Operators.md) (Including texts, images, and audios. More modalities will be supported in the future).\nBesides, our paper is also updated to [v3](https://arxiv.org/abs/2309.02033).\n- [2023-10-13] Our first data-centric LLM competition begins! Please\n  visit the competition's official websites, FT-Data Ranker ([1B Track](https://tianchi.aliyun.com/competition/entrance/532157), [7B Track](https://tianchi.aliyun.com/competition/entrance/532158)), for more information.\n</details>\n\n\n<div id=\"table\" align=\"center\"></div>\n\nTable of Contents\n=================\n\n- [Data-Juicer:  A One-Stop Data Processing System for Large Language Models](#data-juicer--a-one-stop-data-processing-system-for-large-language-models)\n  - [News](#news)\n- [Table of Contents](#table-of-contents)\n  - [Features](#features)\n  - [Documentation Index ](#documentation-index-)\n  - [Demos](#demos)\n  - [Prerequisites](#prerequisites)\n  - [Installation](#installation)\n    - [From Source](#from-source)\n    - [Using pip](#using-pip)\n    - [Using Docker](#using-docker)\n    - [Installation check](#installation-check)\n  - [Quick Start](#quick-start)\n    - [Data Processing](#data-processing)\n    - [Distributed Data Processing](#distributed-data-processing)\n    - [Data Analysis](#data-analysis)\n    - [Data Visualization](#data-visualization)\n    - [Build Up Config Files](#build-up-config-files)\n    - [Sandbox](#sandbox)\n    - [Preprocess Raw Data (Optional)](#preprocess-raw-data-optional)\n    - [For Docker Users](#for-docker-users)\n  - [Data Recipes](#data-recipes)\n  - [License](#license)\n  - [Contributing](#contributing)\n  - [Acknowledgement](#acknowledgement)\n  - [References](#references)\n\n\n## Features\n\n![Overview](https://img.alicdn.com/imgextra/i4/O1CN01WYQP3Z1JHsaXaQDK6_!!6000000001004-0-tps-3640-1812.jpg)\n\n- **Systematic & Reusable**:\n  Empowering users with a systematic library of 80+ core [OPs](docs/Operators.md), 20+ reusable [config recipes](configs), and 20+ feature-rich\n  dedicated [toolkits](#documentation), designed to\n  function independently of specific multimodal LLM datasets and processing pipelines.\n\n- **Data-in-the-loop & Sandbox**: Supporting one-stop data-model collaborative development, enabling rapid iteration\n  through the [sandbox laboratory](docs/Sandbox.md), and providing features such as feedback loops based on data and model,\n  visualization, and multidimensional automatic evaluation, so that you can better understand and improve your data and models.\n  ![Data-in-the-loop](https://img.alicdn.com/imgextra/i2/O1CN017U7Zz31Y7XtCJ5GOz_!!6000000003012-0-tps-3640-1567.jpg)\n\n- **Towards production environment**: Providing efficient and parallel data processing pipelines (Aliyun-PAI\\Ray\\Slurm\\CUDA\\OP Fusion)\n  requiring less memory and CPU usage, optimized with automatic fault-toleration.\n  ![sys-perf](https://img.alicdn.com/imgextra/i4/O1CN01Sk0q2U1hdRxbnQXFg_!!6000000004300-0-tps-2438-709.jpg)\n\n- **Comprehensive Data Processing Recipes**: Offering tens of [pre-built data\n  processing recipes](configs/data_juicer_recipes/README.md) for pre-training, fine-tuning, en, zh, and more scenarios. Validated on\n  reference LLaMA and LLaVA models.\n  ![exp_llama](https://img.alicdn.com/imgextra/i2/O1CN019WtUPP1uhebnDlPR8_!!6000000006069-2-tps-2530-1005.png)\n\n- **Flexible & Extensible**: Accommodating most types of data formats (e.g., jsonl, parquet, csv, ...) and allowing flexible combinations of OPs. Feel free to [implement your own OPs](docs/DeveloperGuide.md#build-your-own-ops) for customizable data processing.\n\n- **User-Friendly Experience**: Designed for simplicity, with [comprehensive documentation](#documents), [easy start guides](#quick-start) and [demo configs](configs/README.md), and intuitive configuration with simple adding/removing OPs from [existing configs](configs/config_all.yaml).\n\n\n\n## Documentation Index <a name=\"documents\"/>\n\n- [Overview](README.md)\n- [Operator Zoo](docs/Operators.md)\n- [Configs](configs/README.md)\n- [Developer Guide](docs/DeveloperGuide.md)\n- [API references](https://modelscope.github.io/data-juicer/)\n- [KDD-Tutorial](https://modelscope.github.io/data-juicer/_static/tutorial_kdd24.html)\n- [\"Bad\" Data Exhibition](docs/BadDataExhibition.md)\n- [Awesome LLM-Data](docs/awesome_llm_data.md)\n- Dedicated Toolkits\n  - [Quality Classifier](tools/quality_classifier/README.md)\n  - [Auto Evaluation](tools/evaluator/README.md)\n  - [Preprocess](tools/preprocess/README.md)\n  - [Postprocess](tools/postprocess/README.md)\n- [DJ-SORA](docs/DJ_SORA.md)\n- [Third-parties (LLM Ecosystems)](thirdparty/README.md)\n\n\n## Demos\n- Introduction to Data-Juicer [[ModelScope](https://modelscope.cn/studios/Data-Juicer/overview_scan/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/overview_scan)]\n- Data Visualization:\n  - Basic Statistics [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_visulization_statistics/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_visualization_statistics)]\n  - Lexical Diversity [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_visulization_diversity/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_visualization_diversity)]\n  - Operator Insight (Single OP) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_visualization_op_insight/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_visualization_op_insight)]\n  - Operator Effect (Multiple OPs) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_visulization_op_effect/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_visualization_op_effect)]\n- Data Processing:\n  - Scientific Literature (e.g. [arXiv](https://info.arxiv.org/help/bulk_data_s3.html)) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/process_sci_data/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/process_sci_data)]\n  - Programming Code (e.g. [TheStack](https://huggingface.co/datasets/bigcode/the-stack)) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/process_code_data/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/process_code_data)]\n  - Chinese Instruction Data (e.g. [Alpaca-CoT](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/process_sft_zh_data/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/process_cft_zh_data)]\n- Tool Pool:\n  - Dataset Splitting by Language [[ModelScope](https://modelscope.cn/studios/Data-Juicer/tool_dataset_splitting_by_language/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/tool_dataset_splitting_by_language)]\n  - Quality Classifier for CommonCrawl [[ModelScope](https://modelscope.cn/studios/Data-Juicer/tool_quality_classifier/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/tool_quality_classifier)]\n  - Auto Evaluation on [HELM](https://github.com/stanford-crfm/helm) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/auto_evaluation_helm/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/auto_evaluation_helm)]\n  - Data Sampling and Mixture [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_mixture/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_mixture)]\n- Data Processing Loop [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_process_loop/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_process_loop)]\n\n## Prerequisites\n\n- Recommend Python>=3.9,<=3.10\n- gcc >= 5 (at least C++14 support)\n\n## Installation\n\n### From Source \n\n- Run the following commands to install the latest basic `data_juicer` version in\n  editable mode:\n```shell\ncd <path_to_data_juicer>\npip install -v -e .\n```\n\n- Some OPs rely on some other too large or low-platform-compatibility third-party libraries. You can install optional dependencies as needed:\n\n```shell\ncd <path_to_data_juicer>\npip install -v -e .  # install a minimal dependencies, which support the basic functions\npip install -v -e .[tools] # install a subset of tools dependencies\n```\n\nThe dependency options are listed below:\n\n| Tag              | Description                                                                                  |\n|------------------|----------------------------------------------------------------------------------------------|\n| `.` or `.[mini]` | Install minimal dependencies for basic Data-Juicer.                                          |\n| `.[all]`         | Install all dependencies except sandbox.                                                     |\n| `.[sci]`         | Install all dependencies for all OPs.                                                        |\n| `.[dist]`        | Install dependencies for distributed data processing. (Experimental)                         |\n| `.[dev]`         | Install dependencies for developing the package as contributors.                             |\n| `.[tools]`       | Install dependencies for dedicated tools, such as quality classifiers.                       |\n| `.[sandbox]`     | Install all dependencies for sandbox.                                                        |\n\n- Install dependencies for specific OPs\n\nWith the growth of the number of OPs, the dependencies of all OPs becomes very heavy. Instead of using the command `pip install -v -e .[sci]` to install all dependencies,\nwe provide two alternative, lighter options:\n\n  - Automatic Minimal Dependency Installation: During the execution of Data-Juicer, minimal dependencies will be automatically installed. This allows for immediate execution, but may potentially lead to dependency conflicts.\n\n  - Manual Minimal Dependency Installation: To manually install minimal dependencies tailored to a specific execution configuration, run the following command:\n    ```shell\n    # only for installation from source\n    python tools/dj_install.py --config path_to_your_data-juicer_config_file\n\n    # use command line tool\n    dj-install --config path_to_your_data-juicer_config_file\n    ```\n\n### Using pip\n\n- Run the following command to install the latest released `data_juicer` using `pip`:\n\n```shell\npip install py-data-juicer\n```\n\n- **Note**:\n  - only the basic APIs in `data_juicer` and two basic tools\n    (data [processing](#data-processing) and [analysis](#data-analysis)) are available in this way. If you want customizable\n    and complete functions, we recommend you install `data_juicer` [from source](#from-source).\n  - The release versions from pypi have a certain lag compared to the latest version from source.\n    So if you want to follow the latest functions of `data_juicer`, we recommend you install [from source](#from-source).\n\n### Using Docker\n\n- You can\n  - either pull our pre-built image from DockerHub:\n    ```shell\n    docker pull datajuicer/data-juicer:<version_tag>\n    ```\n\n  - or run the following command to build the docker image including the\n    latest `data-juicer` with provided [Dockerfile](Dockerfile):\n\n    ```shell\n    docker build -t datajuicer/data-juicer:<version_tag> .\n    ```\n\n  - The format of `<version_tag>` is like `v0.2.0`, which is the same as release version tag.\n\n### Installation check\n\n```python\nimport data_juicer as dj\nprint(dj.__version__)\n```\n\n### For Video-related Operators\nBefore using video-related operators, **FFmpeg** should be installed and accessible via the $PATH environment variable.\n\nYou can install FFmpeg using package managers(e.g. sudo apt install ffmpeg on Debian/Ubuntu, brew install ffmpeg on OS X) or visit the [official ffmpeg link](https://ffmpeg.org/download.html).\n\nCheck if your environment path is set correctly by running the ffmpeg command from the terminal.\n\n\n<p align=\"right\"><a href=\"#table\">ðŸ”¼ back to index</a></p>\n\n\n## Quick Start\n\n\n### Data Processing\n\n- Run `process_data.py` tool or `dj-process` command line tool with your config as the argument to process\n  your dataset.\n\n```shell\n# only for installation from source\npython tools/process_data.py --config configs/demo/process.yaml\n\n# use command line tool\ndj-process --config configs/demo/process.yaml\n```\n\n- **Note:** For some operators that involve third-party models or resources which are not stored locally on your computer, it might be slow for the first running because these ops need to download corresponding resources into a directory first.\nThe default download cache directory is `~/.cache/data_juicer`. Change the cache location by setting the shell environment variable, `DATA_JUICER_CACHE_HOME` to another directory, and you can also change `DATA_JUICER_MODELS_CACHE` or `DATA_JUICER_ASSETS_CACHE` in the same way:\n\n- **Note:** When using operators with third-party models, it's necessary to declare the corresponding `mem_required` in the configuration file (you can refer to the settings in the `config_all.yaml` file). During runtime, Data-Juicer will control the number of processes based on memory availability and the memory requirements of the operator models to achieve better data processing efficiency. When running with CUDA environment, if the mem_required for an operator is not declared correctly, it could potentially lead to a CUDA Out of Memory issue.\n\n```shell\n# cache home\nexport DATA_JUICER_CACHE_HOME=\"/path/to/another/directory\"\n# cache models\nexport DATA_JUICER_MODELS_CACHE=\"/path/to/another/directory/models\"\n# cache assets\nexport DATA_JUICER_ASSETS_CACHE=\"/path/to/another/directory/assets\"\n```\n\n#### Flexible Programming Interface\nWe provide various simple interfaces for users to choose from as follows. \n```python\n#... init op & dataset ...\n\n# Chain call style, support single operator or operator list\ndataset = dataset.process(op)\ndataset = dataset.process([op1, op2])\n# Functional programming style for quick integration or script prototype iteration\ndataset = op(dataset)\ndataset = op.run(dataset)\n```\n\n\n### Distributed Data Processing\n\nWe have now implemented multi-machine distributed data processing based on [RAY](https://www.ray.io/). The corresponding demos can be run using the following commands:\n\n```shell\n# Run text data processing\npython tools/process_data.py --config ./demos/process_on_ray/configs/demo.yaml\n# Run video data processing\npython tools/process_data.py --config ./demos/process_video_on_ray/configs/demo.yaml\n```\n\n- To run data processing across multiple machines, it is necessary to ensure that all distributed nodes can access the corresponding data paths (for example, by mounting the respective data paths on a file-sharing system such as NAS).\n- The deduplicator operators for RAY mode are different from the single-machine version, and all those operators are prefixed with `ray`, e.g. `ray_video_deduplicator` and `ray_document_deduplicator`. Those operators also rely on a [Redis](https://redis.io/) instance. So in addition to starting the RAY cluster, you also need to setup your Redis instance in advance and provide `host` and `port` of your Redis instance in configuration.\n\n> Users can also opt not to use RAY and instead split the dataset to run on a cluster with [Slurm](https://slurm.schedmd.com/). In this case, please use the default Data-Juicer without RAY.\n> [Aliyun PAI-DLC](https://www.aliyun.com/activity/bigdata/pai-dlc) supports the RAY framework, Slurm framework, etc. Users can directly create RAY jobs and Slurm jobs on the DLC cluster.\n\n### Data Analysis\n- Run `analyze_data.py` tool or `dj-analyze` command line tool with your config as the argument to analyze your dataset.\n\n```shell\n# only for installation from source\npython tools/analyze_data.py --config configs/demo/analyzer.yaml\n\n# use command line tool\ndj-analyze --config configs/demo/analyzer.yaml\n\n# you can also use auto mode to avoid writing a recipe. It will analyze a small\n# part (e.g. 1000 samples, specified by argument `auto_num`) of your dataset \n# with all Filters that produce stats.\ndj-analyze --auto --dataset_path xx.jsonl [--auto_num 1000]\n```\n\n- **Note:** Analyzer only compute stats for Filters that produce stats or other OPs that produce tags/categories in meta. So other OPs will be ignored in the analysis process. We use the following registries to decorate OPs:\n  - `NON_STATS_FILTERS`: decorate Filters that **DO NOT** produce any stats.\n  - `TAGGING_OPS`: decorate OPs that **DO** produce tags/categories in meta field.\n\n### Data Visualization\n\n- Run `app.py` tool to visualize your dataset in your browser.\n- **Note**: only available for installation from source.\n\n```shell\nstreamlit run app.py\n```\n\n### Build Up Config Files\n\n- Config files specify some global arguments, and an operator list for the\n  data process. You need to set:\n  - Global arguments: input/output dataset path, number of workers, etc.\n  - Operator list: list operators with their arguments used to process the dataset.\n- You can build up your own config files by:\n  - âž–ï¼šModify from our example config file [`config_all.yaml`](configs/config_all.yaml) which includes **all** ops and default\n    arguments. You just need to **remove** ops that you won't use and refine\n    some arguments of ops.\n  - âž•ï¼šBuild up your own config files **from scratch**. You can refer our\n    example config file [`config_all.yaml`](configs/config_all.yaml), [op documents](docs/Operators.md), and advanced [Build-Up Guide for developers](docs/DeveloperGuide.md#build-your-own-configs).\n  - Besides the yaml files, you also have the flexibility to specify just\n    one (of several) parameters on the command line, which will override\n    the values in yaml files.\n\n```shell\npython xxx.py --config configs/demo/process.yaml --language_id_score_filter.lang=en\n```\n\n- The basic config format and definition is shown below.\n\n  ![Basic config example of format and definition](https://img.alicdn.com/imgextra/i1/O1CN01uXgjgj1khWKOigYww_!!6000000004715-0-tps-1745-871.jpg \"Basic config file example\")\n\n### Sandbox\n\nThe data sandbox laboratory (DJ-Sandbox) provides users with the best practices for continuously producing data recipes. It features low overhead, portability, and guidance.\n\n- In the sandbox, users can quickly experiment, iterate, and refine data recipes based on small-scale datasets and models, before scaling up to produce high-quality data to serve large-scale models.\n- In addition to the basic data optimization and recipe refinement features offered by Data-Juicer, users can seamlessly use configurable components such as data probe and analysis, model training and evaluation, and data and model feedback-based recipe refinement to form a complete one-stop data-model research and development pipeline.\n\nThe sandbox is run using the following commands by default, and for more information and details, please refer to the [sandbox documentation](docs/Sandbox.md).\n```shell\npython tools/sandbox_starter.py --config configs/demo/sandbox/sandbox.yaml\n```\n\n### Preprocess Raw Data (Optional)\n- Our formatters support some common input dataset formats for now:\n  - Multi-sample in one file: jsonl/json, parquet, csv/tsv, etc.\n  - Single-sample in one file: txt, code, docx, pdf, etc.\n- However, data from different sources are complicated and diverse. Such as:\n  - [Raw arXiv data downloaded from S3](https://info.arxiv.org/help/bulk_data_s3.html) include thousands of tar files and even more gzip files in them, and expected tex files are embedded in the gzip files so they are hard to obtain directly.\n  - Some crawled data include different kinds of files (pdf, html, docx, etc.). And extra information like tables, charts, and so on is hard to extract.\n- It's impossible to handle all kinds of data in Data-Juicer, issues/PRs are welcome to contribute to process new data types!\n- Thus, we provide some **common preprocessing tools** in [`tools/preprocess`](tools/preprocess/) for you to preprocess these data.\n  - You are welcome to make your contributions to new preprocessing tools for the community.\n  - We **highly recommend** that complicated data can be preprocessed to jsonl or parquet files.\n\n### For Docker Users\n\n- If you build or pull the docker image of `data-juicer`, you can run the commands or tools mentioned above using this docker image.\n- Run directly:\n\n```shell\n# run the data processing directly\ndocker run --rm \\  # remove container after the processing\n  --privileged \\\n  --shm-size 256g \\\n  --network host \\\n  --gpus all \\\n  --name dj \\  # name of the container\n  -v <host_data_path>:<image_data_path> \\  # mount data or config directory into the container\n  -v ~/.cache/:/root/.cache/ \\  # mount the cache directory into the container to reuse caches and models (recommended)\n  datajuicer/data-juicer:<version_tag> \\  # image to run\n  dj-process --config /path/to/config.yaml  # similar data processing commands\n```\n\n- Or enter into the running container and run commands in editable mode:\n\n```shell\n# start the container\ndocker run -dit \\  # run the container in the background\n  --privileged \\\n  --shm-size 256g \\\n  --network host \\\n  --gpus all \\\n  --rm \\\n  --name dj \\\n  -v <host_data_path>:<image_data_path> \\\n  -v ~/.cache/:/root/.cache/ \\\n  datajuicer/data-juicer:latest /bin/bash\n\n# enter into this container and then you can use data-juicer in editable mode\ndocker exec -it <container_id> bash\n```\n\n\n<p align=\"right\"><a href=\"#table\">ðŸ”¼ back to index</a></p>\n\n## Data Recipes\n- [Recipes for data process in BLOOM](configs/reproduced_bloom/README.md)\n- [Recipes for data process in RedPajama](configs/redpajama/README.md)\n- [Refined recipes for pre-training text data](configs/data_juicer_recipes/README.md)\n- [Refined recipes for fine-tuning text data](configs/data_juicer_recipes/README.md#before-and-after-refining-for-alpaca-cot-dataset)\n- [Refined recipes for pre-training multi-modal data](configs/data_juicer_recipes/README.md#before-and-after-refining-for-multimodal-dataset)\n\n\n\n## License\nData-Juicer is released under Apache License 2.0.\n\n## Contributing\nWe are in a rapidly developing field and greatly welcome contributions of new\nfeatures, bug fixes and better documentations. Please refer to\n[How-to Guide for Developers](docs/DeveloperGuide.md).\n\nIf you have any questions, please join our [discussion groups](README.md).\n\n## Acknowledgement\nData-Juicer is used across various LLM products and research initiatives,\nincluding industrial LLMs from Alibaba Cloud's Tongyi, such as Dianjin for\nfinancial analysis, and Zhiwen for reading assistant, as well as the Alibaba\nCloud's platform for AI (PAI).\nWe look forward to more of your experience, suggestions and discussions for collaboration!\n\nData-Juicer thanks and refers to several community projects, such as\n[Huggingface-Datasets](https://github.com/huggingface/datasets), [Bloom](https://huggingface.co/bigscience/bloom), [RedPajama](https://github.com/togethercomputer/RedPajama-Data/tree/rp_v1), [Pile](https://huggingface.co/datasets/EleutherAI/pile), [Alpaca-Cot](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [DeepSpeed](https://www.deepspeed.ai/), [Arrow](https://github.com/apache/arrow), [Ray](https://github.com/ray-project/ray), [Beam](https://github.com/apache/beam),  [LM-Harness](https://github.com/EleutherAI/lm-evaluation-harness), [HELM](https://github.com/stanford-crfm/helm), ....\n\n\n\n## References\nIf you find our work useful for your research or development, please kindly cite the following [paper](https://arxiv.org/abs/2309.02033).\n```\n@inproceedings{chen2024datajuicer,\n  title={Data-Juicer: A One-Stop Data Processing System for Large Language Models},\n  author={Daoyuan Chen and Yilun Huang and Zhijian Ma and Hesen Chen and Xuchen Pan and Ce Ge and Dawei Gao and Yuexiang Xie and Zhaoyang Liu and Jinyang Gao and Yaliang Li and Bolin Ding and Jingren Zhou},\n  booktitle={International Conference on Management of Data},\n  year={2024}\n}\n```\n\n<details>\n<summary> More related papers from Data-Juicer Team:\n</summary>>\n\n- [Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development](https://arxiv.org/abs/2407.11784)\n\n- [The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective](https://arxiv.org/abs/2407.08583)\n\n- [ImgDiff: Contrastive Data Synthesis for Vision Large Language Models](https://arxiv.org/abs/2408.04594)\n\n- [Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining](https://arxiv.org/abs/2405.14908)\n\n</details>\n\n\n\n<p align=\"right\"><a href=\"#table\">ðŸ”¼ back to index</a></p>\n"
        },
        {
          "name": "README_ZH.md",
          "type": "blob",
          "size": 29.9462890625,
          "content": "[[English Page]](README.md) | [[æ–‡æ¡£ç´¢å¼•]](#documents) | [[API]](https://modelscope.github.io/data-juicer) | [[DJ-SORA]](docs/DJ_SORA_ZH.md) | [[Awesome List]](docs/awesome_llm_data.md)\n\n# Data-Juicer: ä¸ºå¤§æ¨¡åž‹æä¾›æ›´é«˜è´¨é‡ã€æ›´ä¸°å¯Œã€æ›´æ˜“â€œæ¶ˆåŒ–â€çš„æ•°æ®\n\n <img src=\"https://img.alicdn.com/imgextra/i3/O1CN017Eq5kf27AlA2NUKef_!!6000000007757-0-tps-1280-720.jpg\" width = \"640\" height = \"360\" alt=\"Data-Juicer\"/>\n\n![](https://img.shields.io/badge/language-Python-214870.svg)\n![](https://img.shields.io/badge/license-Apache--2.0-000000.svg)\n[![pypi version](https://img.shields.io/pypi/v/py-data-juicer?logo=pypi&color=026cad)](https://pypi.org/project/py-data-juicer)\n[![Docker version](https://img.shields.io/docker/v/datajuicer/data-juicer?logo=docker&label=Docker&color=498bdf)](https://hub.docker.com/r/datajuicer/data-juicer)\n\n[![DataModality](https://img.shields.io/badge/DataModality-Text,Image,Audio,Video-brightgreen.svg)](docs/DeveloperGuide_ZH.md)\n[![Usage](https://img.shields.io/badge/Usage-Cleaning,Generation,Analysis-FFD21E.svg)](docs/DeveloperGuide_ZH.md)\n[![ModelScope- Demos](https://img.shields.io/badge/ModelScope-Demos-4e29ff.svg?logo=data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjI0IDEyMS4zMyIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxwYXRoIGQ9Im0wIDQ3Ljg0aDI1LjY1djI1LjY1aC0yNS42NXoiIGZpbGw9IiM2MjRhZmYiIC8+Cgk8cGF0aCBkPSJtOTkuMTQgNzMuNDloMjUuNjV2MjUuNjVoLTI1LjY1eiIgZmlsbD0iIzYyNGFmZiIgLz4KCTxwYXRoIGQ9Im0xNzYuMDkgOTkuMTRoLTI1LjY1djIyLjE5aDQ3Ljg0di00Ny44NGgtMjIuMTl6IiBmaWxsPSIjNjI0YWZmIiAvPgoJPHBhdGggZD0ibTEyNC43OSA0Ny44NGgyNS42NXYyNS42NWgtMjUuNjV6IiBmaWxsPSIjMzZjZmQxIiAvPgoJPHBhdGggZD0ibTAgMjIuMTloMjUuNjV2MjUuNjVoLTI1LjY1eiIgZmlsbD0iIzM2Y2ZkMSIgLz4KCTxwYXRoIGQ9Im0xOTguMjggNDcuODRoMjUuNjV2MjUuNjVoLTI1LjY1eiIgZmlsbD0iIzYyNGFmZiIgLz4KCTxwYXRoIGQ9Im0xOTguMjggMjIuMTloMjUuNjV2MjUuNjVoLTI1LjY1eiIgZmlsbD0iIzM2Y2ZkMSIgLz4KCTxwYXRoIGQ9Im0xNTAuNDQgMHYyMi4xOWgyNS42NXYyNS42NWgyMi4xOXYtNDcuODR6IiBmaWxsPSIjNjI0YWZmIiAvPgoJPHBhdGggZD0ibTczLjQ5IDQ3Ljg0aDI1LjY1djI1LjY1aC0yNS42NXoiIGZpbGw9IiMzNmNmZDEiIC8+Cgk8cGF0aCBkPSJtNDcuODQgMjIuMTloMjUuNjV2LTIyLjE5aC00Ny44NHY0Ny44NGgyMi4xOXoiIGZpbGw9IiM2MjRhZmYiIC8+Cgk8cGF0aCBkPSJtNDcuODQgNzMuNDloLTIyLjE5djQ3Ljg0aDQ3Ljg0di0yMi4xOWgtMjUuNjV6IiBmaWxsPSIjNjI0YWZmIiAvPgo8L3N2Zz4K)](https://modelscope.cn/studios?name=Data-Jiucer&page=1&sort=latest&type=1)\n[![HuggingFace- Demos](https://img.shields.io/badge/ðŸ¤—HuggingFace-Demos-4e29ff.svg)](https://huggingface.co/spaces?&search=datajuicer)\n\n[![Document_List](https://img.shields.io/badge/Docs-English-blue?logo=Markdown)](README.md#documents)\n[![æ–‡æ¡£åˆ—è¡¨](https://img.shields.io/badge/æ–‡æ¡£-ä¸­æ–‡-blue?logo=Markdown)](#documents)\n[![API Reference](https://img.shields.io/badge/Docs-API_Reference-blue?logo=Markdown)](https://modelscope.github.io/data-juicer/)\n[![Paper](http://img.shields.io/badge/cs.LG-arXiv%3A2309.02033-B31B1B?logo=arxiv&logoColor=red)](https://arxiv.org/abs/2309.02033)\n\n\nData-Juicer æ˜¯ä¸€ä¸ªä¸€ç«™å¼**å¤šæ¨¡æ€**æ•°æ®å¤„ç†ç³»ç»Ÿï¼Œæ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡åž‹ (LLM) æä¾›æ›´é«˜è´¨é‡ã€æ›´ä¸°å¯Œã€æ›´æ˜“â€œæ¶ˆåŒ–â€çš„æ•°æ®ã€‚\n\n\næˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäºŽ JupyterLab çš„ [Playground](http://8.138.149.181/)ï¼Œæ‚¨å¯ä»¥ä»Žæµè§ˆå™¨ä¸­åœ¨çº¿è¯•ç”¨ Data-Juicerã€‚ å¦‚æžœData-Juicerå¯¹æ‚¨çš„ç ”å‘æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„[å·¥ä½œ](#å‚è€ƒæ–‡çŒ®) ã€‚\n\n[é˜¿é‡Œäº‘äººå·¥æ™ºèƒ½å¹³å° PAI](https://www.aliyun.com/product/bigdata/learn) å·²å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼Œå°†Data-Juicerçš„èƒ½åŠ›é›†æˆåˆ°PAIçš„æ•°æ®å¤„ç†äº§å“ä¸­ã€‚PAIæä¾›åŒ…å«æ•°æ®é›†ç®¡ç†ã€ç®—åŠ›ç®¡ç†ã€æ¨¡åž‹å·¥å…·é“¾ã€æ¨¡åž‹å¼€å‘ã€æ¨¡åž‹è®­ç»ƒã€æ¨¡åž‹éƒ¨ç½²ã€AIèµ„äº§ç®¡ç†åœ¨å†…çš„åŠŸèƒ½æ¨¡å—ï¼Œä¸ºç”¨æˆ·æä¾›é«˜æ€§èƒ½ã€é«˜ç¨³å®šã€ä¼ä¸šçº§çš„å¤§æ¨¡åž‹å·¥ç¨‹åŒ–èƒ½åŠ›ã€‚æ•°æ®å¤„ç†çš„ä½¿ç”¨æ–‡æ¡£è¯·å‚è€ƒï¼š[PAI-å¤§æ¨¡åž‹æ•°æ®å¤„ç†](https://help.aliyun.com/zh/pai/user-guide/components-related-to-data-processing-for-foundation-models/?spm=a2c4g.11186623.0.0.3e9821a69kWdvX)ã€‚\n\nData-Juiceræ­£åœ¨ç§¯æžæ›´æ–°å’Œç»´æŠ¤ä¸­ï¼Œæˆ‘ä»¬å°†å®šæœŸå¼ºåŒ–å’Œæ–°å¢žæ›´å¤šçš„åŠŸèƒ½å’Œæ•°æ®èœè°±ã€‚çƒ­çƒˆæ¬¢è¿Žæ‚¨åŠ å…¥æˆ‘ä»¬ï¼ˆissues/PRs/[Slacké¢‘é“](https://join.slack.com/t/data-juicer/shared_invite/zt-23zxltg9d-Z4d3EJuhZbCLGwtnLWWUDg?spm=a2c22.12281976.0.0.7a8275bc8g7ypp) /[é’‰é’‰ç¾¤](https://qr.dingtalk.com/action/joingroup?code=v1,k1,YFIXM2leDEk7gJP5aMC95AfYT+Oo/EP/ihnaIEhMyJM=&_dt_no_comment=1&origin=11)/...ï¼‰ï¼Œä¸€èµ·æŽ¨è¿›LLM-æ•°æ®çš„ååŒå¼€å‘å’Œç ”ç©¶ï¼\n\n\n----\n\n## æ–°æ¶ˆæ¯\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png) [2024-08-09] æˆ‘ä»¬æå‡ºäº†Img-Diffï¼Œå®ƒé€šè¿‡*å¯¹æ¯”æ•°æ®åˆæˆ*æ¥å¢žå¼ºå¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹çš„æ€§èƒ½ï¼Œåœ¨[MMVP benchmark](https://tsb0601.github.io/mmvp_blog/)ä¸­æ¯”GPT-4Vé«˜å‡º12ä¸ªç‚¹ã€‚ æ›´å¤šç»†èŠ‚è¯·å‚é˜…æˆ‘ä»¬çš„ [è®ºæ–‡](https://arxiv.org/abs/2408.04594), ä»¥åŠä»Ž [huggingface](https://huggingface.co/datasets/datajuicer/Img-Diff) å’Œ [modelscope](https://modelscope.cn/datasets/Data-Juicer/Img-Diff)ä¸‹è½½è¿™ä»½æ•°æ®é›†ã€‚\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png) [2024-07-24] â€œå¤©æ±  Better Synth å¤šæ¨¡æ€å¤§æ¨¡åž‹æ•°æ®åˆæˆèµ›â€â€”â€”ç¬¬å››å±ŠData-Juicerå¤§æ¨¡åž‹æ•°æ®æŒ‘æˆ˜èµ›å·²ç»æ­£å¼å¯åŠ¨ï¼ç«‹å³è®¿é—®[ç«žèµ›å®˜ç½‘](https://tianchi.aliyun.com/competition/entrance/532251)ï¼Œäº†è§£èµ›äº‹è¯¦æƒ…ã€‚\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png)[2024-07-17] æˆ‘ä»¬åˆ©ç”¨Data-Juicer[æ²™ç›’å®žéªŒå®¤å¥—ä»¶](https://github.com/modelscope/data-juicer/blob/main/docs/Sandbox-ZH.md)ï¼Œé€šè¿‡æ•°æ®ä¸Žæ¨¡åž‹é—´çš„ç³»ç»Ÿæ€§ç ”å‘å·¥ä½œæµï¼Œè°ƒä¼˜æ•°æ®å’Œæ¨¡åž‹ï¼Œåœ¨[VBench](https://huggingface.co/spaces/Vchitect/VBench_Leaderboard)æ–‡ç”Ÿè§†é¢‘æŽ’è¡Œæ¦œå–å¾—äº†æ–°çš„æ¦œé¦–ã€‚ç›¸å…³æˆæžœå·²ç»æ•´ç†å‘è¡¨åœ¨[è®ºæ–‡](http://arxiv.org/abs/2407.11784)ä¸­ï¼Œå¹¶ä¸”æ¨¡åž‹å·²åœ¨[ModelScope](https://modelscope.cn/models/Data-Juicer/Data-Juicer-T2V)å’Œ[HuggingFace](https://huggingface.co/datajuicer/Data-Juicer-T2V)å¹³å°å‘å¸ƒã€‚\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png)[2024-07-12] æˆ‘ä»¬çš„MLLM-Dataç²¾é€‰åˆ—è¡¨å·²ç»æ¼”åŒ–ä¸ºä¸€ä¸ªæ¨¡åž‹-æ•°æ®ååŒå¼€å‘çš„è§’åº¦ç³»ç»Ÿæ€§[ç»¼è¿°](https://arxiv.org/abs/2407.08583)ã€‚æ¬¢è¿Ž[æµè§ˆ](docs/awesome_llm_data.md)æˆ–å‚ä¸Žè´¡çŒ®!\n- ![new](https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png) [2024-06-01] ModelScope-Soraâ€œæ•°æ®å¯¼æ¼”â€åˆ›æ„ç«žé€Ÿâ€”â€”ç¬¬ä¸‰å±ŠData-Juicerå¤§æ¨¡åž‹æ•°æ®æŒ‘æˆ˜èµ›å·²ç»æ­£å¼å¯åŠ¨ï¼ç«‹å³è®¿é—®[ç«žèµ›å®˜ç½‘](https://tianchi.aliyun.com/competition/entrance/532219)ï¼Œäº†è§£èµ›äº‹è¯¦æƒ…ã€‚\n<details>\n<summary> History News:\n</summary>>\n\n- [2024-03-07] æˆ‘ä»¬çŽ°åœ¨å‘å¸ƒäº† **Data-Juicer [v0.2.0](https://github.com/alibaba/data-juicer/releases/tag/v0.2.0)**! åœ¨è¿™ä¸ªæ–°ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬æ”¯æŒäº†æ›´å¤šçš„ **å¤šæ¨¡æ€æ•°æ®(åŒ…æ‹¬è§†é¢‘)** ç›¸å…³ç‰¹æ€§ã€‚æˆ‘ä»¬è¿˜å¯åŠ¨äº† **[DJ-SORA](docs/DJ_SORA_ZH.md)** ï¼Œä¸ºSORA-likeå¤§æ¨¡åž‹æž„å»ºå¼€æ”¾çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼\n- [2024-02-20] æˆ‘ä»¬åœ¨ç§¯æžç»´æŠ¤ä¸€ä»½å…³äºŽLLM-Dataçš„*ç²¾é€‰åˆ—è¡¨*ï¼Œæ¬¢è¿Ž[è®¿é—®](docs/awesome_llm_data.md)å¹¶å‚ä¸Žè´¡çŒ®ï¼\n- [2024-02-05] æˆ‘ä»¬çš„è®ºæ–‡è¢«SIGMOD'24 industrial trackæŽ¥æ”¶ï¼\n- [2024-01-10] å¼€å¯â€œæ•°æ®æ··åˆâ€æ–°è§†ç•Œâ€”â€”ç¬¬äºŒå±ŠData-Juicerå¤§æ¨¡åž‹æ•°æ®æŒ‘æˆ˜èµ›å·²ç»æ­£å¼å¯åŠ¨ï¼ç«‹å³è®¿é—®[ç«žèµ›å®˜ç½‘](https://tianchi.aliyun.com/competition/entrance/532174)ï¼Œäº†è§£èµ›äº‹è¯¦æƒ…ã€‚\n- [2024-01-05] **Data-Juicer v0.1.3** ç‰ˆæœ¬å‘å¸ƒäº†ã€‚ \nåœ¨è¿™ä¸ªæ–°ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬æ”¯æŒäº†**æ›´å¤šPythonç‰ˆæœ¬**ï¼ˆ3.8-3.10ï¼‰ï¼ŒåŒæ—¶æ”¯æŒäº†**å¤šæ¨¡æ€**æ•°æ®é›†çš„[è½¬æ¢](tools/fmt_conversion/multimodal/README_ZH.md)å’Œ[å¤„ç†](docs/Operators_ZH.md)ï¼ˆåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ã€‚æ›´å¤šæ¨¡æ€ä¹Ÿå°†ä¼šåœ¨ä¹‹åŽæ”¯æŒï¼‰ï¼\næ­¤å¤–ï¼Œæˆ‘ä»¬çš„è®ºæ–‡ä¹Ÿæ›´æ–°åˆ°äº†[ç¬¬ä¸‰ç‰ˆ](https://arxiv.org/abs/2309.02033) ã€‚\n- [2023-10-13] æˆ‘ä»¬çš„ç¬¬ä¸€å±Šä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ LLM ç«žèµ›å¼€å§‹äº†ï¼\n  è¯·è®¿é—®å¤§èµ›å®˜ç½‘ï¼ŒFT-Data Rankerï¼ˆ[1Bèµ›é“](https://tianchi.aliyun.com/competition/entrance/532157) ã€[7Bèµ›é“](https://tianchi.aliyun.com/competition/entrance/532158) ) ï¼Œäº†è§£æ›´å¤šä¿¡æ¯ã€‚\n</details>\n\n\n<div id=\"table\" align=\"center\"></div>\n\nç›®å½•\n===\n- [Data-Juicer: ä¸ºå¤§è¯­è¨€æ¨¡åž‹æä¾›æ›´é«˜è´¨é‡ã€æ›´ä¸°å¯Œã€æ›´æ˜“â€œæ¶ˆåŒ–â€çš„æ•°æ®](#data-juicer-ä¸ºå¤§è¯­è¨€æ¨¡åž‹æä¾›æ›´é«˜è´¨é‡æ›´ä¸°å¯Œæ›´æ˜“æ¶ˆåŒ–çš„æ•°æ®)\n  - [æ–°æ¶ˆæ¯](#æ–°æ¶ˆæ¯)\n- [ç›®å½•](#ç›®å½•)\n  - [ç‰¹ç‚¹](#ç‰¹ç‚¹)\n  - [æ–‡æ¡£ç´¢å¼• ](#æ–‡æ¡£ç´¢å¼•-)\n  - [æ¼”ç¤ºæ ·ä¾‹](#æ¼”ç¤ºæ ·ä¾‹)\n  - [å‰ç½®æ¡ä»¶](#å‰ç½®æ¡ä»¶)\n  - [å®‰è£…](#å®‰è£…)\n    - [ä»Žæºç å®‰è£…](#ä»Žæºç å®‰è£…)\n    - [ä½¿ç”¨ pip å®‰è£…](#ä½¿ç”¨-pip-å®‰è£…)\n    - [ä½¿ç”¨ Docker å®‰è£…](#ä½¿ç”¨-docker-å®‰è£…)\n    - [å®‰è£…æ ¡éªŒ](#å®‰è£…æ ¡éªŒ)\n  - [å¿«é€Ÿä¸Šæ‰‹](#å¿«é€Ÿä¸Šæ‰‹)\n    - [æ•°æ®å¤„ç†](#æ•°æ®å¤„ç†)\n    - [åˆ†å¸ƒå¼æ•°æ®å¤„ç†](#åˆ†å¸ƒå¼æ•°æ®å¤„ç†)\n    - [æ•°æ®åˆ†æž](#æ•°æ®åˆ†æž)\n    - [æ•°æ®å¯è§†åŒ–](#æ•°æ®å¯è§†åŒ–)\n    - [æž„å»ºé…ç½®æ–‡ä»¶](#æž„å»ºé…ç½®æ–‡ä»¶)\n    - [æ²™ç›’å®žéªŒå®¤](#æ²™ç›’å®žéªŒå®¤)\n    - [é¢„å¤„ç†åŽŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰](#é¢„å¤„ç†åŽŸå§‹æ•°æ®å¯é€‰)\n    - [å¯¹äºŽ Docker ç”¨æˆ·](#å¯¹äºŽ-docker-ç”¨æˆ·)\n  - [æ•°æ®å¤„ç†èœè°±](#æ•°æ®å¤„ç†èœè°±)\n  - [å¼€æºåè®®](#å¼€æºåè®®)\n  - [è´¡çŒ®](#è´¡çŒ®)\n  - [è‡´è°¢](#è‡´è°¢)\n  - [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)\n\n\n## ç‰¹ç‚¹\n\n![Overview](https://img.alicdn.com/imgextra/i4/O1CN01WYQP3Z1JHsaXaQDK6_!!6000000001004-0-tps-3640-1812.jpg)\n\n* **ç³»ç»ŸåŒ– & å¯å¤ç”¨**ï¼šä¸ºç”¨æˆ·æä¾›ç³»ç»ŸåŒ–ä¸”å¯å¤ç”¨çš„80+æ ¸å¿ƒ[ç®—å­](docs/Operators_ZH.md)ï¼Œ20+[é…ç½®èœè°±](configs/README_ZH.md)å’Œ20+ä¸“ç”¨[å·¥å…·æ± ](#documentation)ï¼Œæ—¨åœ¨è®©å¤šæ¨¡æ€æ•°æ®å¤„ç†ç‹¬ç«‹äºŽç‰¹å®šçš„å¤§è¯­è¨€æ¨¡åž‹æ•°æ®é›†å’Œå¤„ç†æµæ°´çº¿ã€‚\n\n* **æ•°æ®åé¦ˆå›žè·¯ & æ²™ç›’å®žéªŒå®¤**ï¼šæ”¯æŒä¸€ç«™å¼æ•°æ®-æ¨¡åž‹ååŒå¼€å‘ï¼Œé€šè¿‡[æ²™ç›’å®žéªŒå®¤](docs/Sandbox-ZH.md)å¿«é€Ÿè¿­ä»£ï¼ŒåŸºäºŽæ•°æ®å’Œæ¨¡åž‹åé¦ˆå›žè·¯ã€å¯è§†åŒ–å’Œå¤šç»´åº¦è‡ªåŠ¨è¯„ä¼°ç­‰åŠŸèƒ½ï¼Œä½¿æ‚¨æ›´äº†è§£å’Œæ”¹è¿›æ‚¨çš„æ•°æ®å’Œæ¨¡åž‹ã€‚  ![Data-in-the-loop](https://img.alicdn.com/imgextra/i2/O1CN017U7Zz31Y7XtCJ5GOz_!!6000000003012-0-tps-3640-1567.jpg)\n\n* **é¢å‘ç”Ÿäº§çŽ¯å¢ƒ**ï¼šæä¾›é«˜æ•ˆå¹¶è¡ŒåŒ–çš„æ•°æ®å¤„ç†æµæ°´çº¿ï¼ˆAliyun-PAI\\Ray\\Slurm\\CUDA\\ç®—å­èžåˆï¼‰ï¼Œå‡å°‘å†…å­˜å ç”¨å’ŒCPUå¼€é”€ï¼Œæ”¯æŒè‡ªåŠ¨åŒ–å¤„ç†å®¹é”™ã€‚  ![sys-perf](https://img.alicdn.com/imgextra/i4/O1CN01Sk0q2U1hdRxbnQXFg_!!6000000004300-0-tps-2438-709.jpg)\n\n* **å…¨é¢çš„æ•°æ®å¤„ç†èœè°±**ï¼šä¸ºpre-trainingã€fine-tuningã€ä¸­è‹±æ–‡ç­‰åœºæ™¯æä¾›æ•°åç§[é¢„æž„å»ºçš„æ•°æ®å¤„ç†èœè°±](configs/data_juicer_recipes/README_ZH.md)ã€‚ åœ¨LLaMAã€LLaVAç­‰æ¨¡åž‹ä¸Šæœ‰æ•ˆéªŒè¯ã€‚ ![exp_llama](https://img.alicdn.com/imgextra/i2/O1CN019WtUPP1uhebnDlPR8_!!6000000006069-2-tps-2530-1005.png)\n\n* **ç”¨æˆ·å‹å¥½**ï¼šè®¾è®¡ç®€å•æ˜“ç”¨ï¼Œæä¾›å…¨é¢çš„[æ–‡æ¡£](#documents)ã€ç®€æ˜“[å…¥é—¨æŒ‡å—](#å¿«é€Ÿä¸Šæ‰‹)å’Œ[æ¼”ç¤ºé…ç½®](configs/README_ZH.md)ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°æ·»åŠ /åˆ é™¤[çŽ°æœ‰é…ç½®](configs/config_all.yaml)ä¸­çš„ç®—å­ã€‚\n\n* **çµæ´» & æ˜“æ‰©å±•**ï¼šæ”¯æŒå¤§å¤šæ•°æ•°æ®æ ¼å¼ï¼ˆå¦‚jsonlã€parquetã€csvç­‰ï¼‰ï¼Œå¹¶å…è®¸çµæ´»ç»„åˆç®—å­ã€‚æ”¯æŒ[è‡ªå®šä¹‰ç®—å­](docs/DeveloperGuide_ZH.md#æž„å»ºè‡ªå·±çš„ç®—å­)ï¼Œä»¥æ‰§è¡Œå®šåˆ¶åŒ–çš„æ•°æ®å¤„ç†ã€‚\n\n\n## æ–‡æ¡£ç´¢å¼• <a name=\"documents\"/>\n\n* [æ¦‚è§ˆ](README_ZH.md)\n* [ç®—å­åº“](docs/Operators_ZH.md)\n* [é…ç½®ç³»ç»Ÿ](configs/README_ZH.md)\n* [å¼€å‘è€…æŒ‡å—](docs/DeveloperGuide_ZH.md)\n* [API å‚è€ƒ](https://modelscope.github.io/data-juicer/)\n* [KDD'24 ç›¸å…³æ•™ç¨‹](https://modelscope.github.io/data-juicer/_static/tutorial_kdd24.html)\n* [â€œåâ€æ•°æ®å±•è§ˆ](docs/BadDataExhibition_ZH.md)\n* [Awesome LLM-Data](docs/awesome_llm_data.md)\n* ä¸“ç”¨å·¥å…·ç®±\n  * [è´¨é‡åˆ†ç±»å™¨](tools/quality_classifier/README_ZH.md)\n  * [è‡ªåŠ¨è¯„æµ‹](tools/evaluator/README_ZH.md)\n  * [å‰å¤„ç†](tools/preprocess/README_ZH.md)\n  * [åŽå¤„ç†](tools/postprocess/README_ZH.md)\n* [DJ-SORA](docs/DJ_SORA_ZH.md)\n* [ç¬¬ä¸‰æ–¹åº“ï¼ˆå¤§è¯­è¨€æ¨¡åž‹ç”Ÿæ€ï¼‰](thirdparty/README_ZH.md)\n\n\n## æ¼”ç¤ºæ ·ä¾‹\n\n* Data-Juicer ä»‹ç» [[ModelScope](https://modelscope.cn/studios/Data-Juicer/overview_scan/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/overview_scan)]\n* æ•°æ®å¯è§†åŒ–:\n  * åŸºç¡€æŒ‡æ ‡ç»Ÿè®¡ [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_visulization_statistics/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_visualization_statistics)]\n  * è¯æ±‡å¤šæ ·æ€§ [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_visulization_diversity/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_visualization_diversity)]\n  * ç®—å­æ´žå¯Ÿï¼ˆå•OPï¼‰ [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_visualization_op_insight/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_visualization_op_insight)]\n  * ç®—å­æ•ˆæžœï¼ˆå¤šOPï¼‰ [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_visulization_op_effect/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_visualization_op_effect)]\n* æ•°æ®å¤„ç†:\n  * ç§‘å­¦æ–‡çŒ® (ä¾‹å¦‚ [arXiv](https://info.arxiv.org/help/bulk_data_s3.html)) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/process_sci_data/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/process_sci_data)]\n  * ç¼–ç¨‹ä»£ç  (ä¾‹å¦‚ [TheStack](https://huggingface.co/datasets/bigcode/the-stack)) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/process_code_data/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/process_code_data)]\n  * ä¸­æ–‡æŒ‡ä»¤æ•°æ® (ä¾‹å¦‚ [Alpaca-CoT](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)) [[ModelScope](https://modelscope.cn/studios/Data-Juicer/process_sft_zh_data/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/process_cft_zh_data)]\n* å·¥å…·æ± :\n  * æŒ‰è¯­è¨€åˆ†å‰²æ•°æ®é›† [[ModelScope](https://modelscope.cn/studios/Data-Juicer/tool_dataset_splitting_by_language/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/tool_dataset_splitting_by_language)]\n  * CommonCrawl è´¨é‡åˆ†ç±»å™¨ [[ModelScope](https://modelscope.cn/studios/Data-Juicer/tool_quality_classifier/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/tool_quality_classifier)]\n  * åŸºäºŽ [HELM](https://github.com/stanford-crfm/helm) çš„è‡ªåŠ¨è¯„æµ‹ [[ModelScope](https://modelscope.cn/studios/Data-Juicer/auto_evaluation_helm/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/auto_evaluation_helm)]\n  * æ•°æ®é‡‡æ ·åŠæ··åˆ [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_mixture/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_mixture)]\n* æ•°æ®å¤„ç†å›žè·¯ [[ModelScope](https://modelscope.cn/studios/Data-Juicer/data_process_loop/summary)] [[HuggingFace](https://huggingface.co/spaces/datajuicer/data_process_loop)]\n\n\n## å‰ç½®æ¡ä»¶\n\n* æŽ¨è Python>=3.9,<=3.10\n* gcc >= 5 (at least C++14 support)\n\n## å®‰è£…\n\n### ä»Žæºç å®‰è£…\n\n* è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥å®‰è£… `data_juicer` å¯ç¼–è¾‘æ¨¡å¼çš„æœ€æ–°åŸºç¡€ç‰ˆæœ¬\n\n```shell\ncd <path_to_data_juicer>\npip install -v -e .\n```\n\n* éƒ¨åˆ†ç®—å­åŠŸèƒ½ä¾èµ–äºŽè¾ƒå¤§çš„æˆ–è€…å¹³å°å…¼å®¹æ€§ä¸æ˜¯å¾ˆå¥½çš„ç¬¬ä¸‰æ–¹åº“ï¼Œå› æ­¤ç”¨æˆ·å¯æŒ‰éœ€é¢å¤–å®‰è£…å¯é€‰çš„ä¾èµ–é¡¹:\n\n```shell\ncd <path_to_data_juicer>\npip install -v -e .  # å®‰è£…æœ€å°ä¾èµ–ï¼Œæ”¯æŒåŸºç¡€åŠŸèƒ½\npip install -v -e .[tools] # å®‰è£…éƒ¨åˆ†å·¥å…·åº“çš„ä¾èµ–\n```\n\nä¾èµ–é€‰é¡¹å¦‚ä¸‹è¡¨æ‰€ç¤º:\n\n| æ ‡ç­¾               | æè¿°                           |\n|------------------|------------------------------|\n| `.` æˆ–è€… `.[mini]` | å®‰è£…æ”¯æŒ Data-Juicer åŸºç¡€åŠŸèƒ½çš„æœ€å°ä¾èµ–é¡¹  |\n| `.[all]`         | å®‰è£…é™¤äº†æ²™ç›’å®žéªŒä»¥å¤–çš„æ‰€æœ‰ä¾èµ–é¡¹  |\n| `.[sci]`         | å®‰è£…æ‰€æœ‰ç®—å­çš„å…¨é‡ä¾èµ–                  |\n| `.[dist]`        | å®‰è£…ä»¥åˆ†å¸ƒå¼æ–¹å¼è¿›è¡Œæ•°æ®å¤„ç†çš„ä¾èµ–ï¼ˆå®žéªŒæ€§åŠŸèƒ½ï¼‰     |\n| `.[dev]`         | å®‰è£…ä½œä¸ºè´¡çŒ®è€…å¼€å‘ Data-Juicer æ‰€éœ€çš„ä¾èµ–é¡¹ |\n| `.[tools]`       | å®‰è£…ä¸“ç”¨å·¥å…·åº“ï¼ˆå¦‚è´¨é‡åˆ†ç±»å™¨ï¼‰æ‰€éœ€çš„ä¾èµ–é¡¹        |\n| `.[sandbox]`     | å®‰è£…æ²™ç›’å®žéªŒå®¤çš„åŸºç¡€ä¾èµ–                 |\n\n* åªå®‰è£…éƒ¨åˆ†ç®—å­ä¾èµ–\n\néšç€OPæ•°é‡çš„å¢žé•¿ï¼Œæ‰€æœ‰OPçš„ä¾èµ–å˜å¾—å¾ˆé‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æä¾›äº†ä¸¤ä¸ªæ›¿ä»£çš„ã€æ›´è½»é‡çš„é€‰é¡¹ï¼Œä½œä¸ºä½¿ç”¨å‘½ä»¤`pip install -v -e .[sci]`å®‰è£…æ‰€æœ‰ä¾èµ–çš„æ›¿ä»£ï¼š\n\n  * è‡ªåŠ¨æœ€å°ä¾èµ–å®‰è£…ï¼šåœ¨æ‰§è¡ŒData-Juicerçš„è¿‡ç¨‹ä¸­ï¼Œå°†è‡ªåŠ¨å®‰è£…æœ€å°ä¾èµ–ã€‚ä¹Ÿå°±æ˜¯è¯´ä½ å¯ä»¥ç›´æŽ¥æ‰§è¡Œï¼Œä½†è¿™ç§æ–¹å¼å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›ä¾èµ–å†²çªã€‚\n\n  * æ‰‹åŠ¨æœ€å°ä¾èµ–å®‰è£…ï¼šå¯ä»¥é€šè¿‡å¦‚ä¸‹æŒ‡ä»¤æ‰‹åŠ¨å®‰è£…é€‚åˆç‰¹å®šæ‰§è¡Œé…ç½®çš„æœ€å°ä¾èµ–ï¼š\n    ```shell\n    # é€‚ç”¨äºŽä»Žæºç å®‰è£…\n    python tools/dj_install.py --config path_to_your_data-juicer_config_file\n    \n    # ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·\n    dj-install --config path_to_your_data-juicer_config_file\n    ```\n\n### ä½¿ç”¨ pip å®‰è£…\n\n* è¿è¡Œä»¥ä¸‹å‘½ä»¤ç”¨ `pip` å®‰è£… `data_juicer` çš„æœ€æ–°å‘å¸ƒç‰ˆæœ¬ï¼š\n\n```shell\npip install py-data-juicer\n```\n\n* **æ³¨æ„**ï¼š\n  * ä½¿ç”¨è¿™ç§æ–¹æ³•å®‰è£…æ—¶ï¼Œåªæœ‰`data_juicer`ä¸­çš„åŸºç¡€çš„ API å’Œ2ä¸ªåŸºç¡€å·¥å…·\n    ï¼ˆæ•°æ®[å¤„ç†](#æ•°æ®å¤„ç†)ä¸Ž[åˆ†æž](#æ•°æ®åˆ†æž)ï¼‰å¯ä»¥ä½¿ç”¨ã€‚å¦‚éœ€æ›´å®šåˆ¶åŒ–åœ°ä½¿ç”¨å®Œæ•´åŠŸèƒ½ï¼Œå»ºè®®[ä»Žæºç è¿›è¡Œå®‰è£…](#ä»Žæºç å®‰è£…)ã€‚\n  * pypi çš„å‘å¸ƒç‰ˆæœ¬è¾ƒæºç çš„æœ€æ–°ç‰ˆæœ¬æœ‰ä¸€å®šçš„æ»žåŽæ€§ï¼Œå¦‚éœ€è¦éšæ—¶è·Ÿè¿› `data_juicer` çš„æœ€æ–°åŠŸèƒ½æ”¯æŒï¼Œå»ºè®®[ä»Žæºç è¿›è¡Œå®‰è£…](#ä»Žæºç å®‰è£…)ã€‚\n\n### ä½¿ç”¨ Docker å®‰è£…\n\n- æ‚¨å¯ä»¥é€‰æ‹©\n  - ä»ŽDockerHubç›´æŽ¥æ‹‰å–æˆ‘ä»¬çš„é¢„ç½®é•œåƒ:\n    ```shell\n    docker pull datajuicer/data-juicer:<version_tag>\n    ```\n  - æˆ–è€…è¿è¡Œå¦‚ä¸‹å‘½ä»¤ç”¨æˆ‘ä»¬æä¾›çš„ [Dockerfile](Dockerfile) æ¥æž„å»ºåŒ…æ‹¬æœ€æ–°ç‰ˆæœ¬çš„ `data-juicer` çš„ docker é•œåƒï¼š\n\n    ```shell\n    docker build -t datajuicer/data-juicer:<version_tag> .\n    ```\n\n  - `<version_tag>`çš„æ ¼å¼ç±»ä¼¼äºŽ`v0.2.0`ï¼Œä¸Žå‘å¸ƒï¼ˆReleaseï¼‰çš„ç‰ˆæœ¬å·ç›¸åŒã€‚\n\n### å®‰è£…æ ¡éªŒ\n\n```python\nimport data_juicer as dj\nprint(dj.__version__)\n```\n\n### ä½¿ç”¨è§†é¢‘ç›¸å…³ç®—å­\n\nåœ¨ä½¿ç”¨è§†é¢‘ç›¸å…³ç®—å­ä¹‹å‰ï¼Œåº”è¯¥å®‰è£… **FFmpeg** å¹¶ç¡®ä¿å…¶å¯é€šè¿‡ $PATH çŽ¯å¢ƒå˜é‡è®¿é—®ã€‚\n\nä½ å¯ä»¥ä½¿ç”¨åŒ…ç®¡ç†å™¨å®‰è£… FFmpegï¼ˆä¾‹å¦‚ï¼Œåœ¨ Debian/Ubuntu ä¸Šä½¿ç”¨ sudo apt install ffmpegï¼Œåœ¨ OS X ä¸Šä½¿ç”¨ brew install ffmpegï¼‰ï¼Œæˆ–è®¿é—®[å®˜æ–¹FFmpegé“¾æŽ¥](https://ffmpeg.org/download.html)ã€‚\n\néšåŽåœ¨ç»ˆç«¯è¿è¡Œ ffmpeg å‘½ä»¤æ£€æŸ¥çŽ¯å¢ƒæ˜¯å¦è®¾ç½®æ­£ç¡®ã€‚\n\n\n<p align=\"right\"><a href=\"#table\">ðŸ”¼ back to index</a></p>\n\n## å¿«é€Ÿä¸Šæ‰‹\n\n### æ•°æ®å¤„ç†\n\n* ä»¥é…ç½®æ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°æ¥è¿è¡Œ `process_data.py` æˆ–è€… `dj-process` å‘½ä»¤è¡Œå·¥å…·æ¥å¤„ç†æ•°æ®é›†ã€‚\n\n```shell\n# é€‚ç”¨äºŽä»Žæºç å®‰è£…\npython tools/process_data.py --config configs/demo/process.yaml\n\n# ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·\ndj-process --config configs/demo/process.yaml\n```\n\n* **æ³¨æ„**ï¼šä½¿ç”¨æœªä¿å­˜åœ¨æœ¬åœ°çš„ç¬¬ä¸‰æ–¹æ¨¡åž‹æˆ–èµ„æºçš„ç®—å­ç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½ä¼šå¾ˆæ…¢ï¼Œå› ä¸ºè¿™äº›ç®—å­éœ€è¦å°†ç›¸åº”çš„èµ„æºä¸‹è½½åˆ°ç¼“å­˜ç›®å½•ä¸­ã€‚é»˜è®¤çš„ä¸‹è½½ç¼“å­˜ç›®å½•ä¸º`~/.cache/data_juicer`ã€‚æ‚¨å¯é€šè¿‡è®¾ç½® shell çŽ¯å¢ƒå˜é‡ `DATA_JUICER_CACHE_HOME` æ›´æ”¹ç¼“å­˜ç›®å½•ä½ç½®ï¼Œæ‚¨ä¹Ÿå¯ä»¥é€šè¿‡åŒæ ·çš„æ–¹å¼æ›´æ”¹ `DATA_JUICER_MODELS_CACHE` æˆ– `DATA_JUICER_ASSETS_CACHE` æ¥åˆ†åˆ«ä¿®æ”¹æ¨¡åž‹ç¼“å­˜æˆ–èµ„æºç¼“å­˜ç›®å½•:\n\n* **æ³¨æ„**ï¼šå¯¹äºŽä½¿ç”¨äº†ç¬¬ä¸‰æ–¹æ¨¡åž‹çš„ç®—å­ï¼Œåœ¨å¡«å†™configæ–‡ä»¶æ—¶éœ€è¦åŽ»å£°æ˜Žå…¶å¯¹åº”çš„`mem_required`ï¼ˆå¯ä»¥å‚è€ƒ`config_all.yaml`æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰ã€‚Data-Juiceråœ¨è¿è¡Œè¿‡ç¨‹ä¸­ä¼šæ ¹æ®å†…å­˜æƒ…å†µå’Œç®—å­æ¨¡åž‹æ‰€éœ€çš„memoryå¤§å°æ¥æŽ§åˆ¶å¯¹åº”çš„è¿›ç¨‹æ•°ï¼Œä»¥è¾¾æˆæ›´å¥½çš„æ•°æ®å¤„ç†çš„æ€§èƒ½æ•ˆçŽ‡ã€‚è€Œåœ¨ä½¿ç”¨CUDAçŽ¯å¢ƒè¿è¡Œæ—¶ï¼Œå¦‚æžœä¸æ­£ç¡®çš„å£°æ˜Žç®—å­çš„`mem_required`æƒ…å†µï¼Œåˆ™æœ‰å¯èƒ½å¯¼è‡´CUDA Out of Memoryã€‚\n\n```shell\n# ç¼“å­˜ä¸»ç›®å½•\nexport DATA_JUICER_CACHE_HOME=\"/path/to/another/directory\"\n# æ¨¡åž‹ç¼“å­˜ç›®å½•\nexport DATA_JUICER_MODELS_CACHE=\"/path/to/another/directory/models\"\n# èµ„æºç¼“å­˜ç›®å½•\nexport DATA_JUICER_ASSETS_CACHE=\"/path/to/another/directory/assets\"\n```\n\n#### çµæ´»çš„ç¼–ç¨‹æŽ¥å£\næˆ‘ä»¬æä¾›äº†å„ç§å±‚æ¬¡çš„ç®€å•ç¼–ç¨‹æŽ¥å£ï¼Œä»¥ä¾›ç”¨æˆ·é€‰æ‹©ï¼š\n```python\n# ... init op & dataset ...\n\n# é“¾å¼è°ƒç”¨é£Žæ ¼ï¼Œæ”¯æŒå•ç®—å­æˆ–ç®—å­åˆ—è¡¨\ndataset = dataset.process(op)\ndataset = dataset.process([op1, op2])\n# å‡½æ•°å¼ç¼–ç¨‹é£Žæ ¼ï¼Œæ–¹ä¾¿å¿«é€Ÿé›†æˆæˆ–è„šæœ¬åŽŸåž‹è¿­ä»£\ndataset = op(dataset)\ndataset = op.run(dataset)\n```\n\n### åˆ†å¸ƒå¼æ•°æ®å¤„ç†\n\nData-Juicer çŽ°åœ¨åŸºäºŽ[RAY](https://www.ray.io/)å®žçŽ°äº†å¤šæœºåˆ†å¸ƒå¼æ•°æ®å¤„ç†ã€‚\nå¯¹åº”Demoå¯ä»¥é€šè¿‡å¦‚ä¸‹å‘½ä»¤è¿è¡Œï¼š\n\n```shell\n\n# è¿è¡Œæ–‡å­—æ•°æ®å¤„ç†\npython tools/process_data.py --config ./demos/process_on_ray/configs/demo.yaml\n\n# è¿è¡Œè§†é¢‘æ•°æ®å¤„ç†\npython tools/process_data.py --config ./demos/process_video_on_ray/configs/demo.yaml\n\n```\n\n - å¦‚æžœéœ€è¦åœ¨å¤šæœºä¸Šä½¿ç”¨RAYæ‰§è¡Œæ•°æ®å¤„ç†ï¼Œéœ€è¦ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½å¯ä»¥è®¿é—®å¯¹åº”çš„æ•°æ®è·¯å¾„ï¼Œå³å°†å¯¹åº”çš„æ•°æ®è·¯å¾„æŒ‚è½½åœ¨å…±äº«æ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚NASï¼‰ä¸­ã€‚\n - RAY æ¨¡å¼ä¸‹çš„åŽ»é‡ç®—å­ä¸Žå•æœºç‰ˆæœ¬ä¸åŒï¼Œæ‰€æœ‰ RAY æ¨¡å¼ä¸‹çš„åŽ»é‡ç®—å­åç§°éƒ½ä»¥ `ray` ä½œä¸ºå‰ç¼€ï¼Œä¾‹å¦‚ `ray_video_deduplicator` å’Œ `ray_document_deduplicator`ã€‚è¿™äº›åŽ»é‡ç®—å­ä¾èµ–äºŽ [Redis](https://redis.io/) å®žä¾‹.å› æ­¤ä½¿ç”¨å‰é™¤å¯åŠ¨ RAY é›†ç¾¤å¤–è¿˜éœ€è¦å¯åŠ¨ Redis å®žä¾‹ï¼Œå¹¶åœ¨å¯¹åº”çš„é…ç½®æ–‡ä»¶ä¸­å¡«å†™ Redis å®žä¾‹çš„ `host` å’Œ `port`ã€‚\n\n> ç”¨æˆ·ä¹Ÿå¯ä»¥ä¸ä½¿ç”¨ RAYï¼Œæ‹†åˆ†æ•°æ®é›†åŽä½¿ç”¨ [Slurm](https://slurm.schedmd.com/) åœ¨é›†ç¾¤ä¸Šè¿è¡Œï¼Œæ­¤æ—¶ä½¿ç”¨ä¸åŒ…å« RAY çš„åŽŸç‰ˆ Data-Juicer å³å¯ã€‚\n> [é˜¿é‡Œäº‘ PAI-DLC](https://www.aliyun.com/activity/bigdata/pai-dlc) æ”¯æŒ RAY æ¡†æž¶ã€Slurm æ¡†æž¶ç­‰ï¼Œç”¨æˆ·å¯ä»¥ç›´æŽ¥åœ¨DLCé›†ç¾¤ä¸Šåˆ›å»º RAY ä½œä¸š å’Œ Slurm ä½œä¸šã€‚\n\n### æ•°æ®åˆ†æž\n\n- ä»¥é…ç½®æ–‡ä»¶è·¯å¾„ä¸ºå‚æ•°è¿è¡Œ `analyze_data.py` æˆ–è€… `dj-analyze` å‘½ä»¤è¡Œå·¥å…·æ¥åˆ†æžæ•°æ®é›†ã€‚\n\n```shell\n# é€‚ç”¨äºŽä»Žæºç å®‰è£…\npython tools/analyze_data.py --config configs/demo/analyzer.yaml\n\n# ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·\ndj-analyze --config configs/demo/analyzer.yaml\n\n# ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨\"è‡ªåŠ¨\"æ¨¡å¼æ¥é¿å…å†™ä¸€ä¸ªæ–°çš„æ•°æ®èœè°±ã€‚å®ƒä¼šä½¿ç”¨å…¨éƒ¨å¯äº§å‡ºç»Ÿè®¡ä¿¡æ¯çš„ Filter æ¥åˆ†æž\n# ä½ çš„æ•°æ®é›†çš„ä¸€å°éƒ¨åˆ†ï¼ˆå¦‚1000æ¡æ ·æœ¬ï¼Œå¯é€šè¿‡ `auto_num` å‚æ•°æŒ‡å®šï¼‰\ndj-analyze --auto --dataset_path xx.jsonl [--auto_num 1000]\n```\n\n* **æ³¨æ„**ï¼šAnalyzer åªç”¨äºŽèƒ½åœ¨ stats å­—æ®µé‡Œäº§å‡ºç»Ÿè®¡ä¿¡æ¯çš„ Filter ç®—å­å’Œèƒ½åœ¨ meta å­—æ®µé‡Œäº§å‡º tags æˆ–ç±»åˆ«æ ‡ç­¾çš„å…¶ä»–ç®—å­ã€‚é™¤æ­¤ä¹‹å¤–çš„å…¶ä»–çš„ç®—å­ä¼šåœ¨åˆ†æžè¿‡ç¨‹ä¸­è¢«å¿½ç•¥ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä¸¤ç§æ³¨å†Œå™¨æ¥è£…é¥°ç›¸å…³çš„ç®—å­ï¼š\n  * `NON_STATS_FILTERS`ï¼šè£…é¥°é‚£äº›**ä¸èƒ½**äº§å‡ºä»»ä½•ç»Ÿè®¡ä¿¡æ¯çš„ Filter ç®—å­ã€‚\n  * `TAGGING_OPS`ï¼šè£…é¥°é‚£äº›èƒ½åœ¨ meta å­—æ®µä¸­äº§å‡º tags æˆ–ç±»åˆ«æ ‡ç­¾çš„ç®—å­ã€‚\n\n### æ•°æ®å¯è§†åŒ–\n\n* è¿è¡Œ `app.py` æ¥åœ¨æµè§ˆå™¨ä¸­å¯è§†åŒ–æ‚¨çš„æ•°æ®é›†ã€‚\n* **æ³¨æ„**ï¼šåªå¯ç”¨äºŽä»Žæºç å®‰è£…çš„æ–¹æ³•ã€‚\n\n```shell\nstreamlit run app.py\n```\n\n\n\n\n### æž„å»ºé…ç½®æ–‡ä»¶\n\n* é…ç½®æ–‡ä»¶åŒ…å«ä¸€ç³»åˆ—å…¨å±€å‚æ•°å’Œç”¨äºŽæ•°æ®å¤„ç†çš„ç®—å­åˆ—è¡¨ã€‚æ‚¨éœ€è¦è®¾ç½®:\n  * å…¨å±€å‚æ•°ï¼šè¾“å…¥/è¾“å‡º æ•°æ®é›†è·¯å¾„ï¼Œworker è¿›ç¨‹æ•°é‡ç­‰ã€‚\n  * ç®—å­åˆ—è¡¨ï¼šåˆ—å‡ºç”¨äºŽå¤„ç†æ•°æ®é›†çš„ç®—å­åŠå…¶å‚æ•°ã€‚\n* æ‚¨å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼æž„å»ºè‡ªå·±çš„é…ç½®æ–‡ä»¶:\n  * âž–ï¼šä¿®æ”¹æˆ‘ä»¬çš„æ ·ä¾‹é…ç½®æ–‡ä»¶ [`config_all.yaml`](configs/config_all.yaml)ã€‚è¯¥æ–‡ä»¶åŒ…å«äº†**æ‰€æœ‰**ç®—å­ä»¥åŠç®—å­å¯¹åº”çš„é»˜è®¤å‚æ•°ã€‚æ‚¨åªéœ€è¦**ç§»é™¤**ä¸éœ€è¦çš„ç®—å­å¹¶é‡æ–°è®¾ç½®éƒ¨åˆ†ç®—å­çš„å‚æ•°å³å¯ã€‚\n  * âž•ï¼šä»Žå¤´å¼€å§‹æž„å»ºè‡ªå·±çš„é…ç½®æ–‡ä»¶ã€‚æ‚¨å¯ä»¥å‚è€ƒæˆ‘ä»¬æä¾›çš„æ ·ä¾‹é…ç½®æ–‡ä»¶ [`config_all.yaml`](configs/config_all.yaml)ï¼Œ[ç®—å­æ–‡æ¡£](docs/Operators_ZH.md)ï¼Œä»¥åŠ [å¼€å‘è€…æŒ‡å—](docs/DeveloperGuide_ZH.md#æž„å»ºè‡ªå·±çš„ç®—å­).\n  * é™¤äº†ä½¿ç”¨ yaml æ–‡ä»¶å¤–ï¼Œæ‚¨è¿˜å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸ŠæŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†è¦†ç›– yaml æ–‡ä»¶ä¸­çš„å€¼ã€‚\n\n```shell\npython xxx.py --config configs/demo/process.yaml --language_id_score_filter.lang=en\n```\n\n* åŸºç¡€çš„é…ç½®é¡¹æ ¼å¼åŠå®šä¹‰å¦‚ä¸‹å›¾æ‰€ç¤º\n\n  ![åŸºç¡€é…ç½®é¡¹æ ¼å¼åŠå®šä¹‰æ ·ä¾‹](https://img.alicdn.com/imgextra/i4/O1CN01xPtU0t1YOwsZyuqCx_!!6000000003050-0-tps-1692-879.jpg \"åŸºç¡€é…ç½®æ–‡ä»¶æ ·ä¾‹\")\n\n### æ²™ç›’å®žéªŒå®¤\n\næ•°æ®æ²™ç›’å®žéªŒå®¤ (DJ-Sandbox) ä¸ºç”¨æˆ·æä¾›äº†æŒç»­ç”Ÿäº§æ•°æ®èœè°±çš„æœ€ä½³å®žè·µï¼Œå…¶å…·æœ‰ä½Žå¼€é”€ã€å¯è¿ç§»ã€æœ‰æŒ‡å¯¼æ€§ç­‰ç‰¹ç‚¹ã€‚\n- ç”¨æˆ·åœ¨æ²™ç›’ä¸­å¯ä»¥åŸºäºŽä¸€äº›å°è§„æ¨¡æ•°æ®é›†ã€æ¨¡åž‹å¯¹æ•°æ®èœè°±è¿›è¡Œå¿«é€Ÿå®žéªŒã€è¿­ä»£ã€ä¼˜åŒ–ï¼Œå†è¿ç§»åˆ°æ›´å¤§å°ºåº¦ä¸Šï¼Œå¤§è§„æ¨¡ç”Ÿäº§é«˜è´¨é‡æ•°æ®ä»¥æœåŠ¡å¤§æ¨¡åž‹ã€‚\n- ç”¨æˆ·åœ¨æ²™ç›’ä¸­ï¼Œé™¤äº†Data-JuiceråŸºç¡€çš„æ•°æ®ä¼˜åŒ–ä¸Žæ•°æ®èœè°±å¾®è°ƒåŠŸèƒ½å¤–ï¼Œè¿˜å¯ä»¥ä¾¿æ·åœ°ä½¿ç”¨æ•°æ®æ´žå¯Ÿä¸Žåˆ†æžã€æ²™ç›’æ¨¡åž‹è®­ç»ƒä¸Žè¯„æµ‹ã€åŸºäºŽæ•°æ®å’Œæ¨¡åž‹åé¦ˆä¼˜åŒ–æ•°æ®èœè°±ç­‰å¯é…ç½®ç»„ä»¶ï¼Œå…±åŒç»„æˆå®Œæ•´çš„ä¸€ç«™å¼æ•°æ®-æ¨¡åž‹ç ”å‘æµæ°´çº¿ã€‚\n\næ²™ç›’é»˜è®¤é€šè¿‡å¦‚ä¸‹å‘½ä»¤è¿è¡Œï¼Œæ›´å¤šä»‹ç»å’Œç»†èŠ‚è¯·å‚é˜…[æ²™ç›’æ–‡æ¡£](docs/Sandbox-ZH.md).\n```shell\npython tools/sandbox_starter.py --config configs/demo/sandbox/sandbox.yaml\n```\n\n\n\n### é¢„å¤„ç†åŽŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰\n\n* æˆ‘ä»¬çš„ Formatter ç›®å‰æ”¯æŒä¸€äº›å¸¸è§çš„è¾“å…¥æ•°æ®é›†æ ¼å¼ï¼š\n  * å•ä¸ªæ–‡ä»¶ä¸­åŒ…å«å¤šä¸ªæ ·æœ¬ï¼šjsonl/jsonã€parquetã€csv/tsv ç­‰ã€‚\n  * å•ä¸ªæ–‡ä»¶ä¸­åŒ…å«å•ä¸ªæ ·æœ¬ï¼štxtã€codeã€docxã€pdf ç­‰ã€‚\n* ä½†æ¥è‡ªä¸åŒæºçš„æ•°æ®æ˜¯å¤æ‚å’Œå¤šæ ·åŒ–çš„ï¼Œä¾‹å¦‚:\n  * [ä»Ž S3 ä¸‹è½½çš„ arXiv åŽŸå§‹æ•°æ®](https://info.arxiv.org/help/bulk_data_s3.html) åŒ…æ‹¬æ•°åƒä¸ª tar æ–‡ä»¶ä»¥åŠæ›´å¤šçš„ gzip æ–‡ä»¶ï¼Œå¹¶ä¸”æ‰€éœ€çš„ tex æ–‡ä»¶åœ¨ gzip æ–‡ä»¶ä¸­ï¼Œå¾ˆéš¾ç›´æŽ¥èŽ·å–ã€‚\n  * ä¸€äº›çˆ¬å–çš„æ•°æ®åŒ…å«ä¸åŒç±»åž‹çš„æ–‡ä»¶ï¼ˆpdfã€htmlã€docx ç­‰ï¼‰ï¼Œå¹¶ä¸”å¾ˆéš¾æå–é¢å¤–çš„ä¿¡æ¯ï¼Œä¾‹å¦‚è¡¨æ ¼ã€å›¾è¡¨ç­‰ã€‚\n* Data-Juicer ä¸å¯èƒ½å¤„ç†æ‰€æœ‰ç±»åž‹çš„æ•°æ®ï¼Œæ¬¢è¿Žæ Issues/PRsï¼Œè´¡çŒ®å¯¹æ–°æ•°æ®ç±»åž‹çš„å¤„ç†èƒ½åŠ›ï¼\n* å› æ­¤æˆ‘ä»¬åœ¨ [`tools/preprocess`](tools/preprocess) ä¸­æä¾›äº†ä¸€äº›**å¸¸è§çš„é¢„å¤„ç†å·¥å…·**ï¼Œç”¨äºŽé¢„å¤„ç†è¿™äº›ç±»åž‹å„å¼‚çš„æ•°æ®ã€‚\n  * æ¬¢è¿Žæ‚¨ä¸ºç¤¾åŒºè´¡çŒ®æ–°çš„é¢„å¤„ç†å·¥å…·ã€‚\n  * æˆ‘ä»¬**å¼ºçƒˆå»ºè®®**å°†å¤æ‚çš„æ•°æ®é¢„å¤„ç†ä¸º jsonl æˆ– parquet æ–‡ä»¶ã€‚\n\n### å¯¹äºŽ Docker ç”¨æˆ·\n\n- å¦‚æžœæ‚¨æž„å»ºæˆ–è€…æ‹‰å–äº† `data-juicer` çš„ docker é•œåƒï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™ä¸ª docker é•œåƒæ¥è¿è¡Œä¸Šé¢æåˆ°çš„è¿™äº›å‘½ä»¤æˆ–è€…å·¥å…·ã€‚\n- ç›´æŽ¥è¿è¡Œï¼š\n\n```shell\n# ç›´æŽ¥è¿è¡Œæ•°æ®å¤„ç†\ndocker run --rm \\  # åœ¨å¤„ç†ç»“æŸåŽå°†å®¹å™¨ç§»é™¤\n  --privileged \\\n  --shm-size 256g \\\n  --network host \\\n  --gpus all \\\n  --name dj \\  # å®¹å™¨åç§°\n  -v <host_data_path>:<image_data_path> \\  # å°†æœ¬åœ°çš„æ•°æ®æˆ–è€…é…ç½®ç›®å½•æŒ‚è½½åˆ°å®¹å™¨ä¸­\n  -v ~/.cache/:/root/.cache/ \\  # å°† cache ç›®å½•æŒ‚è½½åˆ°å®¹å™¨ä»¥å¤ç”¨ cache å’Œæ¨¡åž‹èµ„æºï¼ˆæŽ¨èï¼‰\n  datajuicer/data-juicer:<version_tag> \\  # è¿è¡Œçš„é•œåƒ\n  dj-process --config /path/to/config.yaml  # ç±»ä¼¼çš„æ•°æ®å¤„ç†å‘½ä»¤\n```\n\n- æˆ–è€…æ‚¨å¯ä»¥è¿›å…¥æ­£åœ¨è¿è¡Œçš„å®¹å™¨ï¼Œç„¶åŽåœ¨å¯ç¼–è¾‘æ¨¡å¼ä¸‹è¿è¡Œå‘½ä»¤ï¼š\n\n```shell\n# å¯åŠ¨å®¹å™¨\ndocker run -dit \\  # åœ¨åŽå°å¯åŠ¨å®¹å™¨\n  --privileged \\\n  --shm-size 256g \\\n  --network host \\\n  --gpus all \\\n  --rm \\\n  --name dj \\\n  -v <host_data_path>:<image_data_path> \\\n  -v ~/.cache/:/root/.cache/ \\\n  datajuicer/data-juicer:latest /bin/bash\n\n# è¿›å…¥è¿™ä¸ªå®¹å™¨ï¼Œç„¶åŽæ‚¨å¯ä»¥åœ¨ç¼–è¾‘æ¨¡å¼ä¸‹ä½¿ç”¨ data-juicer\ndocker exec -it <container_id> bash\n```\n\n\n<p align=\"right\"><a href=\"#table\">ðŸ”¼ back to index</a></p>\n\n## æ•°æ®å¤„ç†èœè°±\n\n* [BLOOM æ•°æ®å¤„ç†èœè°±](configs/reproduced_bloom/README_ZH.md)\n* [RedPajama æ•°æ®å¤„ç†èœè°±](configs/reproduced_redpajama/README_ZH.md)\n* [é¢„è®­ç»ƒæ–‡æœ¬æ•°æ®å¢žå¼ºèœè°±](configs/data_juicer_recipes/README_ZH.md)\n* [Fine-tuningæ–‡æœ¬æ•°æ®å¢žå¼ºèœè°±](configs/data_juicer_recipes/README_ZH.md#å®Œå–„å‰åŽçš„alpaca-cotæ•°æ®é›†)\n* [é¢„è®­ç»ƒå¤šæ¨¡æ€æ•°æ®å¢žå¼ºèœè°±](configs/data_juicer_recipes/README_ZH.md#before-and-after-refining-for-multimodal-dataset)\n\n## å¼€æºåè®®\n\nData-Juicer åœ¨ Apache License 2.0 åè®®ä¸‹å‘å¸ƒã€‚\n\n## è´¡çŒ®\n\nå¤§æ¨¡åž‹æ˜¯ä¸€ä¸ªé«˜é€Ÿå‘å±•çš„é¢†åŸŸï¼Œæˆ‘ä»¬éžå¸¸æ¬¢è¿Žè´¡çŒ®æ–°åŠŸèƒ½ã€ä¿®å¤æ¼æ´žä»¥åŠæ–‡æ¡£æ”¹å–„ã€‚è¯·å‚è€ƒ[å¼€å‘è€…æŒ‡å—](docs/DeveloperGuide_ZH.md)ã€‚\n\nå¦‚æžœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿ŽåŠ å…¥æˆ‘ä»¬çš„[è®¨è®ºç¾¤](README_ZH.md) ã€‚\n\n## è‡´è°¢\n\nData-Juicer è¢«å„ç§ LLMäº§å“å’Œç ”ç©¶å·¥ä½œä½¿ç”¨ï¼ŒåŒ…æ‹¬æ¥è‡ªé˜¿é‡Œäº‘-é€šä¹‰çš„è¡Œä¸šå¤§æ¨¡åž‹ï¼Œä¾‹å¦‚ç‚¹é‡‘\nï¼ˆé‡‘èžåˆ†æžï¼‰ï¼Œæ™ºæ–‡ï¼ˆé˜…è¯»åŠ©æ‰‹ï¼‰ï¼Œè¿˜æœ‰é˜¿é‡Œäº‘äººå·¥æ™ºèƒ½å¹³å° (PAI)ã€‚ æˆ‘ä»¬æœŸå¾…æ›´å¤šæ‚¨çš„ä½“éªŒåé¦ˆã€å»ºè®®å’Œåˆä½œå…±å»ºï¼\n\n\nData-Juicer æ„Ÿè°¢å¹¶å‚è€ƒäº†ç¤¾åŒºå¼€æºé¡¹ç›®ï¼š\n[Huggingface-Datasets](https://github.com/huggingface/datasets), [Bloom](https://huggingface.co/bigscience/bloom), [RedPajama](https://github.com/togethercomputer/RedPajama-Data/tree/rp_v1), [Pile](https://huggingface.co/datasets/EleutherAI/pile), [Alpaca-Cot](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [DeepSpeed](https://www.deepspeed.ai/), [Arrow](https://github.com/apache/arrow), [Ray](https://github.com/ray-project/ray), [Beam](https://github.com/apache/beam),  [LM-Harness](https://github.com/EleutherAI/lm-evaluation-harness), [HELM](https://github.com/stanford-crfm/helm), ....\n\n## å‚è€ƒæ–‡çŒ®\nå¦‚æžœæ‚¨å‘çŽ°æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨çš„ç ”å‘æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹[è®ºæ–‡](https://arxiv.org/abs/2309.02033) ã€‚\n\n```\n@inproceedings{chen2024datajuicer,\n  title={Data-Juicer: A One-Stop Data Processing System for Large Language Models},\n  author={Daoyuan Chen and Yilun Huang and Zhijian Ma and Hesen Chen and Xuchen Pan and Ce Ge and Dawei Gao and Yuexiang Xie and Zhaoyang Liu and Jinyang Gao and Yaliang Li and Bolin Ding and Jingren Zhou},\n  booktitle={International Conference on Management of Data},\n  year={2024}\n}\n```\n<details>\n<summary>æ›´å¤šData-Juicerå›¢é˜Ÿç›¸å…³è®ºæ–‡:\n</summary>>\n\n- [Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development](https://arxiv.org/abs/2407.11784)\n\n- [The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective](https://arxiv.org/abs/2407.08583)\n\n- [ImgDiff: Contrastive Data Synthesis for Vision Large Language Models](https://arxiv.org/abs/2408.04594)\n\n- [Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining](https://arxiv.org/abs/2405.14908)\n\n</details>\n\n\n\n<p align=\"right\"><a href=\"#table\">ðŸ”¼ back to index</a></p>"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 29.318359375,
          "content": "# Some code here has been modified from:\n# https://huggingface.co/spaces/huggingface/text-data-filtering\n# --------------------------------------------------------\n\nimport copy\nimport math\nimport os\nimport sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport streamlit as st\nimport yaml\nfrom loguru import logger\n\nfrom data_juicer.analysis.diversity_analysis import (DiversityAnalysis,\n                                                     get_diversity)\nfrom data_juicer.config import init_configs\nfrom data_juicer.core import Analyzer, Executor\nfrom data_juicer.ops.base_op import OPERATORS\nfrom data_juicer.utils.constant import Fields, StatsKeys\nfrom data_juicer.utils.logger_utils import get_log_file_path\nfrom data_juicer.utils.model_utils import MODEL_ZOO, prepare_model\n\n\n@st.cache_data\ndef convert_to_csv(df):\n    # IMPORTANT: Cache the conversion to prevent computation on every rerun\n    return df.to_csv().encode('utf_8_sig')\n\n\n@st.cache_data\ndef convert_to_jsonl(df):\n    # IMPORTANT: Cache the conversion to prevent computation on every rerun\n    return df.to_json(orient='records', lines=True,\n                      force_ascii=False).encode('utf_8_sig')\n\n\n@st.cache_data\ndef get_diversity_model(lang):\n    model_key = prepare_model('spacy', lang=lang)\n    diversity_model = MODEL_ZOO.get(model_key)\n    return diversity_model\n\n\n@st.cache_data\ndef postproc_diversity(dataframe, **kwargs):\n    df = get_diversity(dataframe, **kwargs)\n    return df\n\n\ndef read_log_file():\n    log_f_path = get_log_file_path()\n    if log_f_path is None or not os.path.exists(log_f_path):\n        return ''\n    sys.stdout.flush()\n    with open(log_f_path, 'r') as f:\n        return f.read()\n\n\ndef pretty_out(d):\n    res = ''\n    process = ''\n    op_names = set(OPERATORS.modules.keys())\n    for key, value in d.items():\n        if key == 'process':\n            process = yaml.dump(value,\n                                allow_unicode=True,\n                                default_flow_style=False)\n        elif key == 'config' or key.split('.')[0] in op_names:\n            continue\n        else:\n            res += f'{key}:\\n \\t {value}\\n'\n    res += 'process:\\n' + \\\n           '\\n'.join(['\\t' + line for line in process.splitlines()])\n\n    return res\n\n\ndef parse_cfg():\n\n    cfg_file = st.session_state.input_cfg_file\n    cfg_cmd = st.session_state.input_cfg_cmd\n\n    cfg_f_name = 'null'\n    del_cfg_file = False\n    if cfg_file is not None:\n        cfg_f_name = cfg_file.name\n        file_contents = cfg_file.getvalue()\n        with open(cfg_f_name, 'wb') as f:\n            f.write(file_contents)\n        cfg_cmd = f'--config {cfg_f_name}'\n        del_cfg_file = True\n\n    args_in_cmd = cfg_cmd.split()\n\n    if len(args_in_cmd) >= 2 and args_in_cmd[0] == '--config':\n        cfg_f_name = args_in_cmd[1]\n    else:\n        st.warning('Please specify a config command or upload a config file.')\n        st.stop()\n\n    if not os.path.exists(cfg_f_name):\n        st.warning('do not parse'\n                   f'config file does not exist with cfg_f_name={cfg_f_name}')\n        st.stop()\n\n    with open(cfg_f_name, 'r') as cfg_f:\n        specified_cfg = yaml.safe_load(cfg_f)\n\n    try:\n        parsed_cfg = init_configs(args=args_in_cmd)\n        st.session_state.cfg = parsed_cfg\n        if isinstance(parsed_cfg.text_keys, list):\n            text_key = parsed_cfg.text_keys[0]\n        else:\n            text_key = parsed_cfg.text_keys\n        st.session_state.text_key = text_key\n        if del_cfg_file:\n            os.remove(cfg_f_name)\n        return pretty_out(parsed_cfg), pretty_out(specified_cfg), parsed_cfg\n    except Exception as e:\n        return str(e), pretty_out(specified_cfg), None\n\n\ndef analyze_and_show_res():\n    images_ori = []\n    cfg = st.session_state.get('cfg', parse_cfg()[2])\n    if cfg is None:\n        raise ValueError('you have not specify valid cfg')\n    # force generating separate figures\n    cfg['save_stats_in_one_file'] = True\n\n    logger.info('=========Stage 1: analyze original data=========')\n    analyzer = Analyzer(cfg)\n    dataset = analyzer.run()\n\n    overall_file = os.path.join(analyzer.analysis_path, 'overall.csv')\n    analysis_res_ori = pd.DataFrame()\n    if os.path.exists(overall_file):\n        analysis_res_ori = pd.read_csv(overall_file)\n\n    if os.path.exists(analyzer.analysis_path):\n        for f_path in os.listdir(analyzer.analysis_path):\n            if '.png' in f_path and 'all-stats' in f_path:\n                images_ori.append(os.path.join(analyzer.analysis_path, f_path))\n\n    st.session_state.dataset = dataset\n    st.session_state.original_overall = analysis_res_ori\n    st.session_state.original_imgs = images_ori\n\n\ndef process_and_show_res():\n    images_processed = []\n    cfg = st.session_state.get('cfg', parse_cfg()[2])\n    if cfg is None:\n        raise ValueError('you have not specify valid cfg')\n    # force generating separate figures\n    cfg['save_stats_in_one_file'] = True\n    logger.info('=========Stage 2: process original data=========')\n    executor = Executor(cfg)\n    dataset = executor.run()\n\n    logger.info('=========Stage 3: analyze the processed data==========')\n    analysis_res_processed = pd.DataFrame()\n    try:\n        cfg_for_processed_data = copy.deepcopy(cfg)\n        cfg_for_processed_data.dataset_path = cfg.export_path\n\n        cfg_for_processed_data.export_path = os.path.dirname(\n            cfg.export_path) + '_processed/data.jsonl'\n        analyzer = Analyzer(cfg_for_processed_data)\n        analyzer.analysis_path = os.path.dirname(\n            cfg_for_processed_data.export_path) + '/analysis'\n        analyzer.run()\n\n        overall_file = os.path.join(analyzer.analysis_path, 'overall.csv')\n        if os.path.exists(overall_file):\n            analysis_res_processed = pd.read_csv(overall_file)\n\n        if os.path.exists(analyzer.analysis_path):\n            for f_path in os.listdir(analyzer.analysis_path):\n                if '.png' in f_path and 'all-stats' in f_path:\n                    images_processed.append(\n                        os.path.join(analyzer.analysis_path, f_path))\n    except Exception as e:\n        st.warning(f'Something error with {str(e)}')\n\n    logger.info('=========Stage 4: Render the analysis results==========')\n    st.session_state.dataset = dataset\n    st.session_state.processed_overall = analysis_res_processed\n    st.session_state.processed_imgs = images_processed\n\n\ndef get_min_max_step(data):\n    max_value = np.max(data)\n    if max_value > 2.0:\n        min_value = 0\n        max_value = int(max_value + 1)\n        step = 1\n    else:\n        min_value = 0.0\n        max_value = max(1.0, max_value)\n        step = 0.01\n    return min_value, max_value, step\n\n\nop_stats_dict = {\n    'alphanumeric_filter':\n    [StatsKeys.alpha_token_ratio, StatsKeys.alnum_ratio],\n    'average_line_length_filter': [StatsKeys.avg_line_length],\n    'character_repetition_filter': [StatsKeys.char_rep_ratio],\n    'flagged_words_filter': [StatsKeys.flagged_words_ratio],\n    'language_id_score_filter': [StatsKeys.lang, StatsKeys.lang_score],\n    'maximum_line_length_filter': [StatsKeys.max_line_length],\n    'perplexity_filter': [StatsKeys.perplexity],\n    'special_characters_filter': [StatsKeys.special_char_ratio],\n    'stopwords_filter': [StatsKeys.stopwords_ratio],\n    'text_length_filter': [StatsKeys.text_len],\n    'token_num_filter': [StatsKeys.num_token],\n    'words_num_filter': [StatsKeys.num_words],\n    'word_repetition_filter': [StatsKeys.word_rep_ratio],\n}\n\n\nclass Visualize:\n\n    @staticmethod\n    def filter_dataset(dataset):\n        if Fields.stats not in dataset.features:\n            return\n        text_key = st.session_state.get('text_key', 'text')\n        text = dataset[text_key]\n        stats = pd.DataFrame(dataset[Fields.stats])\n        stats[text_key] = text\n\n        non_num_list = [StatsKeys.lang]\n        min_cutoff_list = [\n            StatsKeys.lang_score,\n            StatsKeys.stopwords_ratio,\n        ]\n        max_cutoff_list = [\n            StatsKeys.flagged_words_ratio,\n            StatsKeys.perplexity,\n        ]\n        mask_list = [text_key]\n\n        cfg = st.session_state.get('cfg', None)\n        if cfg is None:\n            return\n\n        def set_sliders(total_stats, ordered):\n            stats = copy.deepcopy(total_stats)\n            conds = list()\n            index = 1\n            for op_cfg in cfg.process:\n                op_name = list(op_cfg.keys())[0]\n                op_stats = op_stats_dict.get(op_name, [])\n\n                cutoff_ratio = None\n\n                with st.sidebar.expander(f'{index} {op_name}'):\n\n                    for column_name in op_stats:\n                        if column_name not in stats:\n                            continue\n                        data = stats[column_name]\n\n                        if column_name in non_num_list:\n                            options = ['all'] + list(set(data))\n                            label = f'Which {column_name} would \\\n                                     you like to keep?'\n\n                            selected = st.selectbox(\n                                label=label,\n                                options=options,\n                            )\n                            if selected == 'all':\n                                cond = [True] * len(data)\n                            else:\n                                cond = data == selected\n                            Visualize.display_discarded_ratio(\n                                cond, column_name)\n\n                        elif column_name in min_cutoff_list:\n                            label = f'If the {column_name} of a document  \\\n                                    is lower than this number,  \\\n                                    the document is removed.'\n\n                            low, high, step = get_min_max_step(data)\n\n                            cutoff_ratio = st.slider(label,\n                                                     low,\n                                                     high,\n                                                     low,\n                                                     step=step)\n                            cond = data >= cutoff_ratio\n                            Visualize.display_discarded_ratio(\n                                cond, column_name)\n\n                        elif column_name in max_cutoff_list:\n                            label = f'If the {column_name} of a document  \\\n                                    is higher than this number,  \\\n                                    the document is removed.'\n\n                            low, high, step = get_min_max_step(data)\n                            cutoff_ratio = st.slider(label,\n                                                     low,\n                                                     high,\n                                                     high,\n                                                     step=step)\n                            cond = data <= cutoff_ratio\n\n                            Visualize.display_discarded_ratio(\n                                cond, column_name)\n                        elif column_name not in mask_list:\n                            # lower\n                            label = f'If the {column_name} of a document  \\\n                                    is lower than this number,  \\\n                                    the document is removed.'\n\n                            low, high, step = get_min_max_step(data)\n\n                            cutoff_ratio_l = st.slider(label,\n                                                       low,\n                                                       high,\n                                                       low,\n                                                       step=step)\n                            cond_l = data >= cutoff_ratio_l\n\n                            Visualize.display_discarded_ratio(\n                                cond_l, column_name)\n\n                            # higher\n                            label = f'If the {column_name} of a document  \\\n                                    is higher than this number,  \\\n                                    the document is removed.'\n\n                            cutoff_ratio_h = st.slider(label,\n                                                       low,\n                                                       high,\n                                                       high,\n                                                       step=step)\n\n                            cond_h = data <= cutoff_ratio_h\n                            Visualize.display_discarded_ratio(\n                                cond_h, column_name)\n                            cond = [\n                                low & high\n                                for low, high in zip(cond_l, cond_h)\n                            ]\n\n                            cutoff_ratio = (cutoff_ratio_l, cutoff_ratio_h)\n\n                        if column_name not in mask_list:\n                            Visualize.draw_hist(data, cutoff_ratio)\n                            conds.append({\n                                (' '.join([str(index), op_name]), column_name):\n                                cond\n                            })\n\n                        if ordered:\n                            stats = stats.loc[cond]\n                    index += 1\n            return conds, stats\n\n        st.sidebar.subheader('Parameters of filter ops')\n        ordered = st.sidebar.checkbox('Process by op order')\n        conds, filtered_stats = set_sliders(stats, ordered)\n\n        st.subheader('How many samples do you want to show?')\n        show_num = st.number_input(\n            label='How many samples do you want to show?',\n            value=5,\n            label_visibility='hidden')\n        if ordered:\n            all_conds = [\n                True if i in filtered_stats.index else False\n                for i in range(len(stats))\n            ]\n        else:\n            all_conds = np.all([list(cond.values())[0] for cond in conds],\n                               axis=0)\n        ds = pd.DataFrame(dataset)\n        Visualize.display_dataset(ds, all_conds, show_num, 'Retained sampels',\n                                  'docs')\n        st.download_button('Download Retained data as JSONL',\n                           data=convert_to_jsonl(ds.loc[all_conds]),\n                           file_name='retained.jsonl')\n        Visualize.display_dataset(ds, np.invert(all_conds), show_num,\n                                  'Discarded sampels', 'docs')\n        st.download_button('Download Discarded data as JSONL',\n                           data=convert_to_jsonl(ds.loc[np.invert(all_conds)]),\n                           file_name='discarded.jsonl')\n        display_discarded_details = st.checkbox(\n            'Display discarded documents by filter details')\n\n        show_stats = copy.deepcopy(stats)\n        bar_labels = []\n        bar_sizes = []\n        for item in conds:\n            for op_key, cond in item.items():\n                op_name, column_name = op_key\n                if column_name not in mask_list:\n                    sub_stats = show_stats[[column_name, text_key]]\n                    if display_discarded_details:\n                        Visualize.display_dataset(\n                            sub_stats,\n                            np.invert(cond) if len(cond) > 0 else [],\n                            show_num,\n                            # f'Discarded documents for the filter on \\\n                            f'{op_name} {column_name} filtered ',\n                            'docs',\n                        )\n                    before_filtered_num = len(show_stats.index)\n                    if ordered:\n                        show_stats = show_stats.loc[cond]\n                        retained = np.sum(1 * cond)\n                        filtered = before_filtered_num - len(show_stats.index)\n                    else:\n                        retained = np.sum(1 * cond)\n                        filtered = before_filtered_num - retained\n\n                    bar_sizes.append(retained)\n                    bar_sizes.append(filtered)\n                    bar_labels.append(f'{op_name}\\n{column_name}')\n\n        bar_title = 'Effect of Filter OPs'\n        Visualize.draw_stack_bar(bar_sizes, bar_labels, len(stats.index),\n                                 bar_title)\n\n    @staticmethod\n    def diversity():\n        with st.expander('Diversity for CFT dataset', expanded=False):\n            dataset = st.session_state.get('dataset', None)\n            cfg = st.session_state.get('cfg', parse_cfg()[2])\n            text_key = st.session_state.get('text_key', 'text')\n            if dataset:\n\n                col1, col2, col3, col4 = st.columns(4)\n                with col1:\n                    label = 'Which language of your dataset'\n                    options = ['en', 'zh']\n                    lang_select = st.selectbox(\n                        label=label,\n                        options=options,\n                    )\n                with col2:\n                    top_k_verbs = st.number_input(\n                        'Set the top_k nums of verbs', value=20)\n                with col3:\n                    top_k_nouns = st.number_input(\n                        'Set the top_k nums of nouns', value=4)\n                with col4:\n                    threshold = st.slider('Count threshold',\n                                          min_value=0,\n                                          value=0,\n                                          max_value=100,\n                                          step=1)\n\n                diversity_btn = st.button('Analyze_diversity',\n                                          use_container_width=True)\n                output_path = os.path.join(os.path.dirname(cfg.export_path),\n                                           'analysis')\n                raw_df = None\n                if diversity_btn:\n                    try:\n                        diversity_analysis = DiversityAnalysis(\n                            dataset, output_path)\n                        with st.spinner('Wait for analyze diversity...'):\n                            raw_df = diversity_analysis.compute(\n                                lang_or_model=get_diversity_model(lang_select),\n                                column_name=text_key)\n\n                        st.session_state[f'diversity{lang_select}'] = raw_df\n\n                    except Exception as e:\n                        st.warning(f'Error {str(e)} in {lang_select}')\n                else:\n                    raw_df = st.session_state.get(f'diversity{lang_select}',\n                                                  None)\n\n                if raw_df is not None:\n                    df = postproc_diversity(raw_df,\n                                            top_k_verbs=top_k_verbs,\n                                            top_k_nouns=top_k_nouns)\n                    df = df[df['count'] >= threshold]\n                    Visualize.draw_sunburst(df,\n                                            path=['verb', 'noun'],\n                                            values='count')\n\n                    st.download_button(\n                        label='Download diversity data as CSV',\n                        data=convert_to_csv(df),\n                        file_name='diversity.csv',\n                        mime='text/csv',\n                    )\n            else:\n                st.warning('Please analyze original data first')\n\n    @staticmethod\n    def draw_sunburst(df, path, values):\n\n        fig = px.sunburst(df, path=path, values=values)\n        fig.update_layout(margin=dict(l=0, r=0, t=0, b=0),\n                          font_family='Times New Roman',\n                          font=dict(size=40))\n        st.plotly_chart(fig, use_container_width=True)\n\n    @staticmethod\n    def draw_stack_bar(bar_sizes, bar_labels, total_num, title=''):\n        filtered_size = [\n            k / total_num * 100 for i, k in enumerate(bar_sizes[::-1])\n            if i % 2 == 0\n        ]\n        retain_size = [\n            k / total_num * 100 for i, k in enumerate(bar_sizes[::-1])\n            if i % 2 != 0\n        ]\n        plt.clf()\n        plt.title(title)\n        bar_labels = bar_labels[::-1]\n        # retained\n        r_bars = plt.barh(bar_labels,\n                          retain_size,\n                          label='Retained',\n                          height=0.5,\n                          color='limegreen')\n\n        # filtered\n        f_bars = plt.barh(bar_labels,\n                          filtered_size,\n                          label='Filtered',\n                          left=retain_size,\n                          height=0.5,\n                          color='orangered')\n\n        for idx, bar in enumerate(r_bars):\n            width = bar.get_width()\n            plt.text(bar.get_x() + width / 2,\n                     bar.get_y() + bar.get_height() / 2,\n                     f'{retain_size[idx]:.2f}%',\n                     ha='center',\n                     va='center')\n\n        for idx, bar in enumerate(f_bars):\n            width = bar.get_width()\n            plt.text(bar.get_x() + width / 2,\n                     bar.get_y() + bar.get_height() / 2,\n                     f'{filtered_size[idx]:.2f}%',\n                     ha='center',\n                     va='center')\n\n        plt.legend()\n        plt.gcf()\n        st.pyplot(plt, use_container_width=True)\n\n    @staticmethod\n    def draw_pie(bar_labels, big_sizes, small_labels, bar_sizes):\n        plt.clf()\n\n        # filter op circle\n        plt.pie(big_sizes, labels=bar_labels, startangle=90, frame=True)\n        # retained and filtered circle\n        plt.pie(bar_sizes,\n                labels=small_labels,\n                radius=0.7,\n                rotatelabels=True,\n                startangle=90,\n                labeldistance=0.7)\n        centre_circle = plt.Circle((0, 0), 0.4, color='white', linewidth=0)\n        fig = plt.gcf()\n        fig.gca().add_artist(centre_circle)\n\n        plt.axis('equal')\n        plt.tight_layout()\n        st.pyplot(plt, use_container_width=True)\n\n    @staticmethod\n    def display_discarded_ratio(cond, key):\n        if len(cond) > 0:\n            st.caption(\n                f':red[{(len(cond) - np.sum(1*cond)) / len(cond) * 100:.2f}%] \\\n                of the total (:red[{len(cond)}]) is discarded with {key}.')\n        else:\n            st.caption(f':red[{0:.2f}%] \\\n                of the total (:red[0]) is discarded with {key}.')\n\n    @staticmethod\n    def display_dataset(dataframe, cond, show_num, desp, type, all=True):\n        examples = dataframe.loc[cond]\n        if all or len(examples) > 0:\n            st.subheader(\n                f'{desp}: :red[{len(examples)}] of '\n                f'{len(dataframe.index)} {type} '\n                f'(:red[{len(examples)/len(dataframe.index) * 100:.2f}%])')\n\n            # st.markdown('Click on a column to sort by it, \\\n            #    place the cursor on the text to display it.')\n            st.dataframe(examples[:show_num], use_container_width=True)\n\n    @staticmethod\n    def draw_hist(data, cutoff=None):\n\n        fig, ax = plt.subplots()\n        data_num = len(data)\n        if data_num >= 100:\n            rec_bins = int(math.sqrt(len(data)))\n        else:\n            rec_bins = 50\n\n        if data_num > 0:\n            ax.hist(data, bins=rec_bins, density=True)\n        if hasattr(data, 'name'):\n            ax.set_title(data.name)\n\n        if isinstance(cutoff, (float, int)):\n            ax.axvline(x=cutoff, color='r', linestyle='dashed')\n        elif isinstance(cutoff, tuple) and len(cutoff) == 2:\n            ax.axvline(x=cutoff[0], color='r', linestyle='dashed')\n            ax.axvline(x=cutoff[1], color='r', linestyle='dashed')\n        st.pyplot(fig)\n\n    @staticmethod\n    def setup():\n        st.set_page_config(\n            page_title='Data-Juicer',\n            page_icon=':smile',\n            layout='wide',\n            # initial_sidebar_state=\"expanded\",\n        )\n\n        readme_link = 'https://github.com/alibaba/data-juicer'\n        st.markdown(\n            '<div align = \"center\"> <font size = \"70\"> Data-Juicer \\\n            </font> </div>',\n            unsafe_allow_html=True,\n        )\n        st.markdown(\n            f'<div align = \"center\"> A One-Stop Data Processing System for \\\n                Large Language Models, \\\n                see more details in our <a href={readme_link}>page</a></div>',\n            unsafe_allow_html=True,\n        )\n\n    @staticmethod\n    def parser():\n        with st.expander('Configuration', expanded=True):\n            st.markdown('Please specify the cfg via '\n                        '(i) specifying the cfg file path with commands or '\n                        '(ii) uploading the cfg file.')\n\n            col1, col2 = st.columns(2)\n            with col1:\n                example_cfg_f = os.path.abspath(\n                    os.path.join(os.path.dirname(__file__),\n                                 './configs/demo/process.yaml'))\n                st.text_area(label='(i) Input Cfg Commands',\n                             key='input_cfg_cmd',\n                             value=f'--config {example_cfg_f}')\n                example_my_cmd = '--dataset_path ' \\\n                                 './demos/data/demo-dataset.jsonl ' \\\n                                 '--export_path '\\\n                                 './outputs/demo/demo-processed.jsonl'\n\n                st.text_area(\n                    label='cmd example. (the cmd-args will override '\n                    'yaml-file-args)',\n                    disabled=True,\n                    value=f'--config {example_cfg_f} {example_my_cmd}')\n\n            with col2:\n                st.file_uploader(label='(ii) Input Cfg File',\n                                 key='input_cfg_file',\n                                 type=['yaml'])\n\n            btn_show_cfg = st.button('1. Parse Cfg', use_container_width=True)\n            if btn_show_cfg:\n                text1, text2, cfg = parse_cfg()\n                st.session_state.cfg_text1 = text1\n                st.session_state.cfg_text2 = text2\n\n            else:\n                text1 = st.session_state.get('cfg_text1', '')\n                text2 = st.session_state.get('cfg_text2', '')\n\n            col3, col4 = st.columns(2)\n            with col3:\n                st.text_area(label='Parsed Cfg (in memory)', value=text1)\n            with col4:\n                st.text_area(label='Specified Cfg (in yaml file)', value=text2)\n\n    @staticmethod\n    def analyze_process():\n        start_btn = st.button(\n            '2. Start to analyze original data (per filter op)',\n            use_container_width=True)\n        start_btn_process = st.button('3. Start to process data',\n                                      use_container_width=True)\n\n        # with st.expander('Log', expanded=False):\n        #    logs = st.Textbox(show_label=False)\n        #    demo.load(read_log_file, inputs=None, outputs=logs, every=1)\n\n        with st.expander('Data Analysis Results', expanded=False):\n\n            if start_btn:\n                with st.spinner('Wait for analyze...'):\n                    analyze_and_show_res()\n\n            if start_btn_process:\n                with st.spinner('Wait for process...'):\n                    process_and_show_res()\n\n            original_overall = st.session_state.get('original_overall', None)\n            original_imgs = st.session_state.get('original_imgs', [])\n            processed_overall = st.session_state.get('processed_overall', None)\n            processed_imgs = st.session_state.get('processed_imgs', [])\n\n            col1, col2 = st.columns(2)\n            with col1:\n                st.caption('Original Data')\n                st.dataframe(original_overall, use_container_width=True)\n                for img in original_imgs:\n                    st.image(img, output_format='png')\n\n            with col2:\n                st.caption('Processed Data')\n                st.dataframe(processed_overall, use_container_width=True)\n                for img in processed_imgs:\n                    st.image(img, output_format='png')\n\n    @staticmethod\n    def filter():\n        with st.expander('Effect of Filter OPs', expanded=False):\n            dataset = st.session_state.get('dataset', None)\n            if dataset:\n                Visualize.filter_dataset(dataset)\n            else:\n                st.warning('Please analyze original data first')\n\n    @staticmethod\n    def auxiliary():\n        st.markdown('[WIP] Auxiliary Models on Processed Data')\n        col1, col2 = st.columns(2)\n        with col1:\n            with st.expander('Quality Scorer', expanded=False):\n                wiki_socre_btn = st.button('Run Wiki-score classifier',\n                                           use_container_width=True)\n\n                if wiki_socre_btn:\n                    st.warning('No support for now')\n\n                wikibook_score_btn = st.button('Run WikiBook-score classifier',\n                                               use_container_width=True)\n                if wikibook_score_btn:\n                    st.warning('No support for now')\n\n        with col2:\n            with st.expander('[WIP] Proxy LM Models Training', expanded=False):\n                st.file_uploader(label='LM Training Cfg File', type=['yaml'])\n                st.button('Train proxy model')\n                st.markdown('[Training Monitoring](http://'\n                            '8.130.26.137:8083/dail/'\n                            'llama-re-2nd?workspace=user-dail)')\n\n    @staticmethod\n    def visualize():\n        Visualize.setup()\n        Visualize.parser()\n        Visualize.analyze_process()\n        Visualize.filter()\n        Visualize.diversity()\n        Visualize.auxiliary()\n\n\ndef main():\n    Visualize.visualize()\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data_juicer",
          "type": "tree",
          "content": null
        },
        {
          "name": "demos",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environments",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "service.py",
          "type": "blob",
          "size": 5.755859375,
          "content": "import datetime\nimport importlib\nimport inspect\nimport json\nimport logging\nimport os\nfrom typing import Dict\nfrom urllib.parse import parse_qs\n\nfrom fastapi import FastAPI, HTTPException, Request\nfrom jsonargparse import Namespace\nfrom pydantic import validate_call\n\nfrom data_juicer.core.exporter import Exporter\nfrom data_juicer.format.load import load_formatter\n\nDJ_OUTPUT = 'outputs'\n\nallowed_methods = {\n    'run', 'process', 'compute_stats', 'compute_hash', 'analyze', 'compute'\n}\n\nlogger = logging.getLogger('uvicorn.error')\napp = FastAPI()\n\n\ndef register_objects_from_init(directory: str):\n    \"\"\"\n    Traverse the specified directory for __init__.py files and\n    register objects defined in __all__.\n    \"\"\"\n    for dirpath, _, filenames in os.walk(os.path.normpath(directory)):\n        if '__init__.py' in filenames:\n            module_path = dirpath.replace(os.sep, '.')\n            module = importlib.import_module(module_path)\n\n            if hasattr(module, '__all__'):\n                for name in module.__all__:\n                    obj = getattr(module, name)\n                    if inspect.isclass(obj):\n                        register_class(module, obj)\n                    elif callable(obj):\n                        register_function(module, obj)\n\n\ndef register_class(module, cls):\n    \"\"\"Register class and its methods as endpoints.\"\"\"\n\n    def create_class_call(cls, method_name: str):\n\n        async def class_call(request: Request):\n            try:\n                # wrap init method\n                cls.__init__ = validate_call(\n                    cls.__init__, config=dict(arbitrary_types_allowed=True))\n                # parse json body as cls init args\n                init_args = await request.json() if await request.body(\n                ) else {}\n                # create an instance\n                instance = cls(**_setup_cfg(init_args))\n                # wrap called method\n                method = validate_call(getattr(instance, method_name))\n                result = _invoke(method, request)\n                return {'status': 'success', 'result': result}\n            except Exception as e:\n                raise HTTPException(status_code=500, detail=str(e))\n\n        return class_call\n\n    module_path = module.__name__.replace('.', os.sep)\n    cls_name = cls.__name__\n    for method_name in _get_public_methods(cls, allowed_methods):\n        api_path = f'/{module_path}/{cls_name}/{method_name}'\n        class_call = create_class_call(cls, method_name)\n        app.add_api_route(api_path,\n                          class_call,\n                          methods=['POST'],\n                          tags=['POST'])\n        logger.debug(f'Registered {api_path}')\n\n\ndef register_function(module, func):\n    \"\"\"Register a function as an endpoint.\"\"\"\n\n    def create_func_call(func):\n\n        async def func_call(request: Request):\n            try:\n                nonlocal func\n                func = validate_call(func,\n                                     config=dict(arbitrary_types_allowed=True))\n                result = _invoke(func, request)\n                return {'status': 'success', 'result': result}\n            except Exception as e:\n                raise HTTPException(status_code=500, detail=str(e))\n\n        return func_call\n\n    module_path = module.__name__.replace('.', os.sep)\n    func_name = func.__name__\n    api_path = f'/{module_path}/{func_name}'\n    func_call = create_func_call(func)\n    app.add_api_route(api_path, func_call, methods=['GET'], tags=['GET'])\n    logger.debug(f'Registered {api_path}')\n\n\ndef _invoke(callable, request):\n    # parse query params as cls method args\n    q_params = parse_qs(request.url.query, keep_blank_values=True)\n    # flatten lists with a single element\n    d_params = dict(\n        (k, v if len(v) > 1 else v[0]) for k, v in q_params.items())\n    # pre-processing\n    d_params = _setup_cfg(d_params)\n    exporter = _setup_dataset(d_params)\n    skip_return = d_params.pop('skip_return', False)\n    # invoke callable\n    result = callable(**d_params)\n    # post-processing\n    if exporter is not None:\n        exporter.export(result)\n        result = exporter.export_path\n    if skip_return:\n        result = ''\n    return result\n\n\ndef _setup_cfg(params: Dict):\n    \"\"\"convert string `cfg` to Namespace\"\"\"\n    # TODO: Traverse method's signature and convert any arguments \\\n    #  that should be Namespace but are passed as str\n    if cfg_str := params.get('cfg'):\n        if isinstance(cfg_str, str):\n            cfg = Namespace(**json.loads(cfg_str))\n            params['cfg'] = cfg\n    return params\n\n\ndef _setup_dataset(params: Dict):\n    \"\"\"setup dataset loading and exporting\"\"\"\n    exporter = None\n    if dataset_path := params.get('dataset'):\n        if isinstance(dataset_path, str):\n            dataset = load_formatter(dataset_path).load_dataset()\n            params['dataset'] = dataset\n            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n            export_path = os.path.join(DJ_OUTPUT, timestamp,\n                                       'processed_data.jsonl')\n            exporter = Exporter(export_path,\n                                keep_stats_in_res_ds=True,\n                                keep_hashes_in_res_ds=True,\n                                export_stats=False)\n    return exporter\n\n\ndef _get_public_methods(cls, allowed=None):\n    \"\"\"Get public methods of a class.\"\"\"\n    all_methods = inspect.getmembers(cls, predicate=inspect.isfunction)\n    return [\n        name for name, _ in all_methods\n        if not name.startswith('_') and (allowed is None or name in allowed)\n    ]\n\n\n# Specify the directories to search\ndirectories_to_search = [\n    'data_juicer',\n    # \"tools\",  # Uncomment to add more directories\n]\n\n# Register objects from each specified directory\nfor directory in directories_to_search:\n    register_objects_from_init(directory)\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.05078125,
          "content": "[flake8]\nper-file-ignores =\n    */__init__.py: F401\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.62109375,
          "content": "import logging\nimport os.path\nimport re\n\nimport setuptools\n\n\ndef get_package_dir():\n    pkg_dir = {\n        'data_juicer.tools': 'tools',\n    }\n    return pkg_dir\n\n\ndef get_install_requirements(require_f_paths, env_dir='environments'):\n    reqs = []\n    for path in require_f_paths:\n        target_f = os.path.join(env_dir, path)\n        if not os.path.exists(target_f):\n            logging.warning(f'target file does not exist: {target_f}')\n        else:\n            with open(target_f, 'r', encoding='utf-8') as fin:\n                reqs += [x.strip() for x in fin.read().splitlines()]\n    reqs = [x for x in reqs if not x.startswith('#')]\n    return reqs\n\n\n# allowing selective installment based on users' needs\n# TODO: The specific taxonomy and dependencies will be determined\n#  after implementing some preliminary operators and detailed discussions\nmin_requires = get_install_requirements(['minimal_requires.txt'])\nextra_requires = {\n    'mini':\n    min_requires,\n    'sci':\n    get_install_requirements(['science_requires.txt']),\n    'dist':\n    get_install_requirements(['dist_requires.txt']),\n    'dev':\n    get_install_requirements(['dev_requires.txt']),\n    'tools':\n    get_install_requirements(\n        ['preprocess_requires.txt', 'quality_classifier_requires.txt']),\n}\nextra_requires['all'] = [v for v in extra_requires.values()]\nextra_requires['sandbox'] = get_install_requirements(['sandbox_requires.txt'])\n\nwith open('data_juicer/__init__.py', 'r') as f:\n    version = re.search(r'^__version__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]', f.read(),\n                        re.MULTILINE).group(1)\n\nwith open('README.md', encoding='utf-8') as f:\n    readme_md = f.read()\n\nsetuptools.setup(\n    name='py-data-juicer',\n    version=version,\n    url='https://github.com/alibaba/data-juicer',\n    author='SysML Team of Alibaba Tongyi Lab',\n    description='A One-Stop Data Processing System for Large Language '\n    'Models.',\n    long_description=readme_md,\n    long_description_content_type='text/markdown',\n    license='Apache License 2.0',\n    packages=setuptools.find_packages(exclude=['tests*', 'tools*']) +\n    list(get_package_dir().keys()),\n    package_dir=get_package_dir(),\n    entry_points={\n        'console_scripts': [\n            'dj-process = data_juicer.tools.process_data:main',\n            'dj-analyze = data_juicer.tools.analyze_data:main',\n            'dj-install = data_juicer.tools.dj_install:main',\n        ]\n    },\n    install_requires=min_requires,\n    extras_require=extra_requires,\n    classifiers=[\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python :: 3',\n        'Operating System :: OS Independent'\n    ],\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "thirdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}