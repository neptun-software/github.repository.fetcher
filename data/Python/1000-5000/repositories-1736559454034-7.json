{
  "metadata": {
    "timestamp": 1736559454034,
    "page": 7,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "CSAILVision/semantic-segmentation-pytorch",
      "stars": 4982,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0263671875,
          "content": "*.ipynb filter=clean_ipynb\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.087890625,
          "content": "*.pyc\n\nckpt/\nvis/\nlog/\npretrained/\n\n.ipynb_checkpoints\n\nADE_val*.jpg\nADE_val*.png\n\n.idea/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4892578125,
          "content": "BSD 3-Clause License\n\nCopyright (c) 2019, MIT CSAIL Computer Vision\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.078125,
          "content": "# Semantic Segmentation on MIT ADE20K dataset in PyTorch\n\nThis is a PyTorch implementation of semantic segmentation models on MIT ADE20K scene parsing dataset (http://sceneparsing.csail.mit.edu/).\n\nADE20K is the largest open source dataset for semantic segmentation and scene parsing, released by MIT Computer Vision team. Follow the link below to find the repository for our dataset and implementations on Caffe and Torch7:\nhttps://github.com/CSAILVision/sceneparsing\n\nIf you simply want to play with our demo, please try this link: http://scenesegmentation.csail.mit.edu You can upload your own photo and parse it!\n\n[You can also use this colab notebook playground here](https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb) to tinker with the code for segmenting an image.\n\nAll pretrained models can be found at:\nhttp://sceneparsing.csail.mit.edu/model/pytorch\n\n<img src=\"./teaser/ADE_val_00000278.png\" width=\"900\"/>\n<img src=\"./teaser/ADE_val_00001519.png\" width=\"900\"/>\n[From left to right: Test Image, Ground Truth, Predicted Result]\n\nColor encoding of semantic categories can be found here:\nhttps://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing\n\n## Updates\n- HRNet model is now supported.\n- We use configuration files to store most options which were in argument parser. The definitions of options are detailed in ```config/defaults.py```.\n- We conform to Pytorch practice in data preprocessing (RGB [0, 1], substract mean, divide std).\n\n\n## Highlights\n\n### Syncronized Batch Normalization on PyTorch\nThis module computes the mean and standard-deviation across all devices during training. We empirically find that a reasonable large batch size is important for segmentation. We thank [Jiayuan Mao](http://vccy.xyz/) for his kind contributions, please refer to [Synchronized-BatchNorm-PyTorch](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch) for details.\n\nThe implementation is easy to use as:\n- It is pure-python, no C++ extra extension libs.\n- It is completely compatible with PyTorch's implementation. Specifically, it uses unbiased variance to update the moving average, and use sqrt(max(var, eps)) instead of sqrt(var + eps).\n- It is efficient, only 20% to 30% slower than UnsyncBN.\n\n### Dynamic scales of input for training with multiple GPUs \nFor the task of semantic segmentation, it is good to keep aspect ratio of images during training. So we re-implement the `DataParallel` module, and make it support distributing data to multiple GPUs in python dict, so that each gpu can process images of different sizes. At the same time, the dataloader also operates differently. \n\n<sup>*Now the batch size of a dataloader always equals to the number of GPUs*, each element will be sent to a GPU. It is also compatible with multi-processing. Note that the file index for the multi-processing dataloader is stored on the master process, which is in contradict to our goal that each worker maintains its own file list. So we use a trick that although the master process still gives dataloader an index for `__getitem__` function, we just ignore such request and send a random batch dict. Also, *the multiple workers forked by the dataloader all have the same seed*, you will find that multiple workers will yield exactly the same data, if we use the above-mentioned trick directly. Therefore, we add one line of code which sets the defaut seed for `numpy.random` before activating multiple worker in dataloader.</sup>\n\n### State-of-the-Art models\n- **PSPNet** is scene parsing network that aggregates global representation with Pyramid Pooling Module (PPM). It is the winner model of ILSVRC'16 MIT Scene Parsing Challenge. Please refer to [https://arxiv.org/abs/1612.01105](https://arxiv.org/abs/1612.01105) for details.\n- **UPerNet** is a model based on Feature Pyramid Network (FPN) and Pyramid Pooling Module (PPM). It doesn't need dilated convolution, an operator that is time-and-memory consuming. *Without bells and whistles*, it is comparable or even better compared with PSPNet, while requiring much shorter training time and less GPU memory. Please refer to [https://arxiv.org/abs/1807.10221](https://arxiv.org/abs/1807.10221) for details.\n- **HRNet** is a recently proposed model that retains high resolution representations throughout the model, without the traditional bottleneck design. It achieves the SOTA performance on a series of pixel labeling tasks. Please refer to [https://arxiv.org/abs/1904.04514](https://arxiv.org/abs/1904.04514) for details.\n\n\n## Supported models\nWe split our models into encoder and decoder, where encoders are usually modified directly from classification networks, and decoders consist of final convolutions and upsampling. We have provided some pre-configured models in the ```config``` folder.\n\nEncoder:\n- MobileNetV2dilated\n- ResNet18/ResNet18dilated\n- ResNet50/ResNet50dilated\n- ResNet101/ResNet101dilated\n- HRNetV2 (W48)\n\nDecoder:\n- C1 (one convolution module)\n- C1_deepsup (C1 + deep supervision trick)\n- PPM (Pyramid Pooling Module, see [PSPNet](https://hszhao.github.io/projects/pspnet) paper for details.)\n- PPM_deepsup (PPM + deep supervision trick)\n- UPerNet (Pyramid Pooling + FPN head, see [UperNet](https://arxiv.org/abs/1807.10221) for details.)\n\n## Performance:\nIMPORTANT: The base ResNet in our repository is a customized (different from the one in torchvision). The base models will be automatically downloaded when needed.\n\n<table><tbody>\n    <th valign=\"bottom\">Architecture</th>\n    <th valign=\"bottom\">MultiScale Testing</th>\n    <th valign=\"bottom\">Mean IoU</th>\n    <th valign=\"bottom\">Pixel Accuracy(%)</th>\n    <th valign=\"bottom\">Overall Score</th>\n    <th valign=\"bottom\">Inference Speed(fps)</th>\n    <tr>\n        <td rowspan=\"2\">MobileNetV2dilated + C1_deepsup</td>\n        <td>No</td><td>34.84</td><td>75.75</td><td>54.07</td>\n        <td>17.2</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>33.84</td><td>76.80</td><td>55.32</td>\n        <td>10.3</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">MobileNetV2dilated + PPM_deepsup</td>\n        <td>No</td><td>35.76</td><td>77.77</td><td>56.27</td>\n        <td>14.9</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>36.28</td><td>78.26</td><td>57.27</td>\n        <td>6.7</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">ResNet18dilated + C1_deepsup</td>\n        <td>No</td><td>33.82</td><td>76.05</td><td>54.94</td>\n        <td>13.9</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>35.34</td><td>77.41</td><td>56.38</td>\n        <td>5.8</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">ResNet18dilated + PPM_deepsup</td>\n        <td>No</td><td>38.00</td><td>78.64</td><td>58.32</td>\n        <td>11.7</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>38.81</td><td>79.29</td><td>59.05</td>\n        <td>4.2</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">ResNet50dilated + PPM_deepsup</td>\n        <td>No</td><td>41.26</td><td>79.73</td><td>60.50</td>\n        <td>8.3</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>42.14</td><td>80.13</td><td>61.14</td>\n        <td>2.6</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">ResNet101dilated + PPM_deepsup</td>\n        <td>No</td><td>42.19</td><td>80.59</td><td>61.39</td>\n        <td>6.8</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>42.53</td><td>80.91</td><td>61.72</td>\n        <td>2.0</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">UperNet50</td>\n        <td>No</td><td>40.44</td><td>79.80</td><td>60.12</td>\n        <td>8.4</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>41.55</td><td>80.23</td><td>60.89</td>\n        <td>2.9</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">UperNet101</td>\n        <td>No</td><td>42.00</td><td>80.79</td><td>61.40</td>\n        <td>7.8</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>42.66</td><td>81.01</td><td>61.84</td>\n        <td>2.3</td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">HRNetV2</td>\n        <td>No</td><td>42.03</td><td>80.77</td><td>61.40</td>\n        <td>5.8</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td>43.20</td><td>81.47</td><td>62.34</td>\n        <td>1.9</td>\n    </tr>\n\n</tbody></table>\n\nThe training is benchmarked on a server with 8 NVIDIA Pascal Titan Xp GPUs (12GB GPU memory), the inference speed is benchmarked a single NVIDIA Pascal Titan Xp GPU, without visualization.\n\n## Environment\nThe code is developed under the following configurations.\n- Hardware: >=4 GPUs for training, >=1 GPU for testing (set ```[--gpus GPUS]``` accordingly)\n- Software: Ubuntu 16.04.3 LTS, ***CUDA>=8.0, Python>=3.5, PyTorch>=0.4.0***\n- Dependencies: numpy, scipy, opencv, yacs, tqdm\n\n## Quick start: Test on an image using our trained model \n1. Here is a simple demo to do inference on a single image:\n```bash\nchmod +x demo_test.sh\n./demo_test.sh\n```\nThis script downloads a trained model (ResNet50dilated + PPM_deepsup) and a test image, runs the test script, and saves predicted segmentation (.png) to the working directory.\n\n2. To test on an image or a folder of images (```$PATH_IMG```), you can simply do the following:\n```\npython3 -u test.py --imgs $PATH_IMG --gpu $GPU --cfg $CFG\n```\n\n## Training\n1. Download the ADE20K scene parsing dataset:\n```bash\nchmod +x download_ADE20K.sh\n./download_ADE20K.sh\n```\n2. Train a model by selecting the GPUs (```$GPUS```) and configuration file (```$CFG```) to use. During training, checkpoints by default are saved in folder ```ckpt```.\n```bash\npython3 train.py --gpus $GPUS --cfg $CFG \n```\n- To choose which gpus to use, you can either do ```--gpus 0-7```, or ```--gpus 0,2,4,6```.\n\nFor example, you can start with our provided configurations: \n\n* Train MobileNetV2dilated + C1_deepsup\n```bash\npython3 train.py --gpus GPUS --cfg config/ade20k-mobilenetv2dilated-c1_deepsup.yaml\n```\n\n* Train ResNet50dilated + PPM_deepsup\n```bash\npython3 train.py --gpus GPUS --cfg config/ade20k-resnet50dilated-ppm_deepsup.yaml\n```\n\n* Train UPerNet101\n```bash\npython3 train.py --gpus GPUS --cfg config/ade20k-resnet101-upernet.yaml\n```\n\n3. You can also override options in commandline, for example  ```python3 train.py TRAIN.num_epoch 10 ```.\n\n\n## Evaluation\n1. Evaluate a trained model on the validation set. Add ```VAL.visualize True``` in argument to output visualizations as shown in teaser.\n\nFor example:\n\n* Evaluate MobileNetV2dilated + C1_deepsup\n```bash\npython3 eval_multipro.py --gpus GPUS --cfg config/ade20k-mobilenetv2dilated-c1_deepsup.yaml\n```\n\n* Evaluate ResNet50dilated + PPM_deepsup\n```bash\npython3 eval_multipro.py --gpus GPUS --cfg config/ade20k-resnet50dilated-ppm_deepsup.yaml\n```\n\n* Evaluate UPerNet101\n```bash\npython3 eval_multipro.py --gpus GPUS --cfg config/ade20k-resnet101-upernet.yaml\n```\n\n## Integration with other projects\nThis library can be installed via `pip` to easily integrate with another codebase\n```bash\npip install git+https://github.com/CSAILVision/semantic-segmentation-pytorch.git@master\n```\n\nNow this library can easily be consumed programmatically. For example\n```python\nfrom mit_semseg.config import cfg\nfrom mit_semseg.dataset import TestDataset\nfrom mit_semseg.models import ModelBuilder, SegmentationModule\n```\n\n## Reference\n\nIf you find the code or pre-trained models useful, please cite the following papers:\n\nSemantic Understanding of Scenes through ADE20K Dataset. B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso and A. Torralba. International Journal on Computer Vision (IJCV), 2018. (https://arxiv.org/pdf/1608.05442.pdf)\n\n    @article{zhou2018semantic,\n      title={Semantic understanding of scenes through the ade20k dataset},\n      author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},\n      journal={International Journal on Computer Vision},\n      year={2018}\n    }\n\nScene Parsing through ADE20K Dataset. B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso and A. Torralba. Computer Vision and Pattern Recognition (CVPR), 2017. (http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf)\n\n    @inproceedings{zhou2017scene,\n        title={Scene Parsing through ADE20K Dataset},\n        author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},\n        booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n        year={2017}\n    }\n    \n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo_test.sh",
          "type": "blob",
          "size": 0.888671875,
          "content": "#!/bin/bash\n\n# Image and model names\nTEST_IMG=ADE_val_00001519.jpg\nMODEL_NAME=ade20k-resnet50dilated-ppm_deepsup\nMODEL_PATH=ckpt/$MODEL_NAME\nRESULT_PATH=./\n\nENCODER=$MODEL_NAME/encoder_epoch_20.pth\nDECODER=$MODEL_NAME/decoder_epoch_20.pth\n\n# Download model weights and image\nif [ ! -e $MODEL_PATH ]; then\n  mkdir -p $MODEL_PATH\nfi\nif [ ! -e $ENCODER ]; then\n  wget -P $MODEL_PATH http://sceneparsing.csail.mit.edu/model/pytorch/$ENCODER\nfi\nif [ ! -e $DECODER ]; then\n  wget -P $MODEL_PATH http://sceneparsing.csail.mit.edu/model/pytorch/$DECODER\nfi\nif [ ! -e $TEST_IMG ]; then\n  wget -P $RESULT_PATH http://sceneparsing.csail.mit.edu/data/ADEChallengeData2016/images/validation/$TEST_IMG\nfi\n\nif [ -z \"$DOWNLOAD_ONLY\" ]\nthen\n\n# Inference\npython3 -u test.py \\\n  --imgs $TEST_IMG \\\n  --cfg config/ade20k-resnet50dilated-ppm_deepsup.yaml \\\n  DIR $MODEL_PATH \\\n  TEST.result ./ \\\n  TEST.checkpoint epoch_20.pth\n\nfi\n"
        },
        {
          "name": "download_ADE20K.sh",
          "type": "blob",
          "size": 0.2158203125,
          "content": "wget -O ./data/ADEChallengeData2016.zip http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\nunzip ./data/ADEChallengeData2016.zip -d ./data\nrm ./data/ADEChallengeData2016.zip\necho \"Dataset downloaded.\"\n"
        },
        {
          "name": "eval.py",
          "type": "blob",
          "size": 5.8515625,
          "content": "# System libs\nimport os\nimport time\nimport argparse\nfrom distutils.version import LooseVersion\n# Numerical libs\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom scipy.io import loadmat\n# Our libs\nfrom mit_semseg.config import cfg\nfrom mit_semseg.dataset import ValDataset\nfrom mit_semseg.models import ModelBuilder, SegmentationModule\nfrom mit_semseg.utils import AverageMeter, colorEncode, accuracy, intersectionAndUnion, setup_logger\nfrom mit_semseg.lib.nn import user_scattered_collate, async_copy_to\nfrom mit_semseg.lib.utils import as_numpy\nfrom PIL import Image\nfrom tqdm import tqdm\n\ncolors = loadmat('data/color150.mat')['colors']\n\n\ndef visualize_result(data, pred, dir_result):\n    (img, seg, info) = data\n\n    # segmentation\n    seg_color = colorEncode(seg, colors)\n\n    # prediction\n    pred_color = colorEncode(pred, colors)\n\n    # aggregate images and save\n    im_vis = np.concatenate((img, seg_color, pred_color),\n                            axis=1).astype(np.uint8)\n\n    img_name = info.split('/')[-1]\n    Image.fromarray(im_vis).save(os.path.join(dir_result, img_name.replace('.jpg', '.png')))\n\n\ndef evaluate(segmentation_module, loader, cfg, gpu):\n    acc_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n    time_meter = AverageMeter()\n\n    segmentation_module.eval()\n\n    pbar = tqdm(total=len(loader))\n    for batch_data in loader:\n        # process data\n        batch_data = batch_data[0]\n        seg_label = as_numpy(batch_data['seg_label'][0])\n        img_resized_list = batch_data['img_data']\n\n        torch.cuda.synchronize()\n        tic = time.perf_counter()\n        with torch.no_grad():\n            segSize = (seg_label.shape[0], seg_label.shape[1])\n            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n            scores = async_copy_to(scores, gpu)\n\n            for img in img_resized_list:\n                feed_dict = batch_data.copy()\n                feed_dict['img_data'] = img\n                del feed_dict['img_ori']\n                del feed_dict['info']\n                feed_dict = async_copy_to(feed_dict, gpu)\n\n                # forward pass\n                scores_tmp = segmentation_module(feed_dict, segSize=segSize)\n                scores = scores + scores_tmp / len(cfg.DATASET.imgSizes)\n\n            _, pred = torch.max(scores, dim=1)\n            pred = as_numpy(pred.squeeze(0).cpu())\n\n        torch.cuda.synchronize()\n        time_meter.update(time.perf_counter() - tic)\n\n        # calculate accuracy\n        acc, pix = accuracy(pred, seg_label)\n        intersection, union = intersectionAndUnion(pred, seg_label, cfg.DATASET.num_class)\n        acc_meter.update(acc, pix)\n        intersection_meter.update(intersection)\n        union_meter.update(union)\n\n        # visualization\n        if cfg.VAL.visualize:\n            visualize_result(\n                (batch_data['img_ori'], seg_label, batch_data['info']),\n                pred,\n                os.path.join(cfg.DIR, 'result')\n            )\n\n        pbar.update(1)\n\n    # summary\n    iou = intersection_meter.sum / (union_meter.sum + 1e-10)\n    for i, _iou in enumerate(iou):\n        print('class [{}], IoU: {:.4f}'.format(i, _iou))\n\n    print('[Eval Summary]:')\n    print('Mean IoU: {:.4f}, Accuracy: {:.2f}%, Inference Time: {:.4f}s'\n          .format(iou.mean(), acc_meter.average()*100, time_meter.average()))\n\n\ndef main(cfg, gpu):\n    torch.cuda.set_device(gpu)\n\n    # Network Builders\n    net_encoder = ModelBuilder.build_encoder(\n        arch=cfg.MODEL.arch_encoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        weights=cfg.MODEL.weights_encoder)\n    net_decoder = ModelBuilder.build_decoder(\n        arch=cfg.MODEL.arch_decoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        num_class=cfg.DATASET.num_class,\n        weights=cfg.MODEL.weights_decoder,\n        use_softmax=True)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_val = ValDataset(\n        cfg.DATASET.root_dataset,\n        cfg.DATASET.list_val,\n        cfg.DATASET)\n    loader_val = torch.utils.data.DataLoader(\n        dataset_val,\n        batch_size=cfg.VAL.batch_size,\n        shuffle=False,\n        collate_fn=user_scattered_collate,\n        num_workers=5,\n        drop_last=True)\n\n    segmentation_module.cuda()\n\n    # Main loop\n    evaluate(segmentation_module, loader_val, cfg, gpu)\n\n    print('Evaluation Done!')\n\n\nif __name__ == '__main__':\n    assert LooseVersion(torch.__version__) >= LooseVersion('0.4.0'), \\\n        'PyTorch>=0.4.0 is required'\n\n    parser = argparse.ArgumentParser(\n        description=\"PyTorch Semantic Segmentation Validation\"\n    )\n    parser.add_argument(\n        \"--cfg\",\n        default=\"config/ade20k-resnet50dilated-ppm_deepsup.yaml\",\n        metavar=\"FILE\",\n        help=\"path to config file\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--gpu\",\n        default=0,\n        help=\"gpu to use\"\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"Modify config options using the command-line\",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    # cfg.freeze()\n\n    logger = setup_logger(distributed_rank=0)   # TODO\n    logger.info(\"Loaded configuration file {}\".format(args.cfg))\n    logger.info(\"Running with config:\\n{}\".format(cfg))\n\n    # absolute paths of model weights\n    cfg.MODEL.weights_encoder = os.path.join(\n        cfg.DIR, 'encoder_' + cfg.VAL.checkpoint)\n    cfg.MODEL.weights_decoder = os.path.join(\n        cfg.DIR, 'decoder_' + cfg.VAL.checkpoint)\n    assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n        os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exitst!\"\n\n    if not os.path.isdir(os.path.join(cfg.DIR, \"result\")):\n        os.makedirs(os.path.join(cfg.DIR, \"result\"))\n\n    main(cfg, args.gpu)\n"
        },
        {
          "name": "eval_multipro.py",
          "type": "blob",
          "size": 6.8935546875,
          "content": "# System libs\nimport os\nimport argparse\nfrom distutils.version import LooseVersion\nfrom multiprocessing import Queue, Process\n# Numerical libs\nimport numpy as np\nimport math\nimport torch\nimport torch.nn as nn\nfrom scipy.io import loadmat\n# Our libs\nfrom mit_semseg.config import cfg\nfrom mit_semseg.dataset import ValDataset\nfrom mit_semseg.models import ModelBuilder, SegmentationModule\nfrom mit_semseg.utils import AverageMeter, colorEncode, accuracy, intersectionAndUnion, parse_devices, setup_logger\nfrom mit_semseg.lib.nn import user_scattered_collate, async_copy_to\nfrom mit_semseg.lib.utils import as_numpy\nfrom PIL import Image\nfrom tqdm import tqdm\n\ncolors = loadmat('data/color150.mat')['colors']\n\n\ndef visualize_result(data, pred, dir_result):\n    (img, seg, info) = data\n\n    # segmentation\n    seg_color = colorEncode(seg, colors)\n\n    # prediction\n    pred_color = colorEncode(pred, colors)\n\n    # aggregate images and save\n    im_vis = np.concatenate((img, seg_color, pred_color),\n                            axis=1).astype(np.uint8)\n\n    img_name = info.split('/')[-1]\n    Image.fromarray(im_vis).save(os.path.join(dir_result, img_name.replace('.jpg', '.png')))\n\n\ndef evaluate(segmentation_module, loader, cfg, gpu_id, result_queue):\n    segmentation_module.eval()\n\n    for batch_data in loader:\n        # process data\n        batch_data = batch_data[0]\n        seg_label = as_numpy(batch_data['seg_label'][0])\n        img_resized_list = batch_data['img_data']\n\n        with torch.no_grad():\n            segSize = (seg_label.shape[0], seg_label.shape[1])\n            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n            scores = async_copy_to(scores, gpu_id)\n\n            for img in img_resized_list:\n                feed_dict = batch_data.copy()\n                feed_dict['img_data'] = img\n                del feed_dict['img_ori']\n                del feed_dict['info']\n                feed_dict = async_copy_to(feed_dict, gpu_id)\n\n                # forward pass\n                scores_tmp = segmentation_module(feed_dict, segSize=segSize)\n                scores = scores + scores_tmp / len(cfg.DATASET.imgSizes)\n\n            _, pred = torch.max(scores, dim=1)\n            pred = as_numpy(pred.squeeze(0).cpu())\n\n        # calculate accuracy and SEND THEM TO MASTER\n        acc, pix = accuracy(pred, seg_label)\n        intersection, union = intersectionAndUnion(pred, seg_label, cfg.DATASET.num_class)\n        result_queue.put_nowait((acc, pix, intersection, union))\n\n        # visualization\n        if cfg.VAL.visualize:\n            visualize_result(\n                (batch_data['img_ori'], seg_label, batch_data['info']),\n                pred,\n                os.path.join(cfg.DIR, 'result')\n            )\n\n\ndef worker(cfg, gpu_id, start_idx, end_idx, result_queue):\n    torch.cuda.set_device(gpu_id)\n\n    # Dataset and Loader\n    dataset_val = ValDataset(\n        cfg.DATASET.root_dataset,\n        cfg.DATASET.list_val,\n        cfg.DATASET,\n        start_idx=start_idx, end_idx=end_idx)\n    loader_val = torch.utils.data.DataLoader(\n        dataset_val,\n        batch_size=cfg.VAL.batch_size,\n        shuffle=False,\n        collate_fn=user_scattered_collate,\n        num_workers=2)\n\n    # Network Builders\n    net_encoder = ModelBuilder.build_encoder(\n        arch=cfg.MODEL.arch_encoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        weights=cfg.MODEL.weights_encoder)\n    net_decoder = ModelBuilder.build_decoder(\n        arch=cfg.MODEL.arch_decoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        num_class=cfg.DATASET.num_class,\n        weights=cfg.MODEL.weights_decoder,\n        use_softmax=True)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n\n    segmentation_module.cuda()\n\n    # Main loop\n    evaluate(segmentation_module, loader_val, cfg, gpu_id, result_queue)\n\n\ndef main(cfg, gpus):\n    with open(cfg.DATASET.list_val, 'r') as f:\n        lines = f.readlines()\n        num_files = len(lines)\n\n    num_files_per_gpu = math.ceil(num_files / len(gpus))\n\n    pbar = tqdm(total=num_files)\n\n    acc_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n\n    result_queue = Queue(500)\n    procs = []\n    for idx, gpu_id in enumerate(gpus):\n        start_idx = idx * num_files_per_gpu\n        end_idx = min(start_idx + num_files_per_gpu, num_files)\n        proc = Process(target=worker, args=(cfg, gpu_id, start_idx, end_idx, result_queue))\n        print('gpu:{}, start_idx:{}, end_idx:{}'.format(gpu_id, start_idx, end_idx))\n        proc.start()\n        procs.append(proc)\n\n    # master fetches results\n    processed_counter = 0\n    while processed_counter < num_files:\n        if result_queue.empty():\n            continue\n        (acc, pix, intersection, union) = result_queue.get()\n        acc_meter.update(acc, pix)\n        intersection_meter.update(intersection)\n        union_meter.update(union)\n        processed_counter += 1\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n    # summary\n    iou = intersection_meter.sum / (union_meter.sum + 1e-10)\n    for i, _iou in enumerate(iou):\n        print('class [{}], IoU: {:.4f}'.format(i, _iou))\n\n    print('[Eval Summary]:')\n    print('Mean IoU: {:.4f}, Accuracy: {:.2f}%'\n          .format(iou.mean(), acc_meter.average()*100))\n\n    print('Evaluation Done!')\n\n\nif __name__ == '__main__':\n    assert LooseVersion(torch.__version__) >= LooseVersion('0.4.0'), \\\n        'PyTorch>=0.4.0 is required'\n\n    parser = argparse.ArgumentParser(\n        description=\"PyTorch Semantic Segmentation Validation\"\n    )\n    parser.add_argument(\n        \"--cfg\",\n        default=\"config/ade20k-resnet50dilated-ppm_deepsup.yaml\",\n        metavar=\"FILE\",\n        help=\"path to config file\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--gpus\",\n        default=\"0-3\",\n        help=\"gpus to use, e.g. 0-3 or 0,1,2,3\"\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"Modify config options using the command-line\",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    # cfg.freeze()\n\n    logger = setup_logger(distributed_rank=0)   # TODO\n    logger.info(\"Loaded configuration file {}\".format(args.cfg))\n    logger.info(\"Running with config:\\n{}\".format(cfg))\n\n    # absolute paths of model weights\n    cfg.MODEL.weights_encoder = os.path.join(\n        cfg.DIR, 'encoder_' + cfg.VAL.checkpoint)\n    cfg.MODEL.weights_decoder = os.path.join(\n        cfg.DIR, 'decoder_' + cfg.VAL.checkpoint)\n    assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n        os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exitst!\"\n\n    if not os.path.isdir(os.path.join(cfg.DIR, \"result\")):\n        os.makedirs(os.path.join(cfg.DIR, \"result\"))\n\n    # Parse gpu ids\n    gpus = parse_devices(args.gpus)\n    gpus = [x.replace('gpu', '') for x in gpus]\n    gpus = [int(x) for x in gpus]\n\n    main(cfg, gpus)\n"
        },
        {
          "name": "mit_semseg",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0556640625,
          "content": "numpy\nscipy\npytorch==0.4.1\ntorchvision\nopencv3\nyacs\ntqdm\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.7978515625,
          "content": "import setuptools\n\nwith open('README.md', 'r') as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name='mit_semseg',\n    version='1.0.0',\n    author='MIT CSAIL',\n    description='Pytorch implementation for Semantic Segmentation/Scene Parsing on MIT ADE20K dataset',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://github.com/CSAILVision/semantic-segmentation-pytorch',\n    packages=setuptools.find_packages(),\n    classifiers=(\n        'Programming Language :: Python :: 3',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n    ),\n    install_requires=[\n        'numpy',\n        'torch>=0.4.1',\n        'torchvision',\n        'opencv-python',\n        'yacs',\n        'scipy',\n        'tqdm'\n    ]\n)\n"
        },
        {
          "name": "teaser",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 5.732421875,
          "content": "# System libs\nimport os\nimport argparse\nfrom distutils.version import LooseVersion\n# Numerical libs\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom scipy.io import loadmat\nimport csv\n# Our libs\nfrom mit_semseg.dataset import TestDataset\nfrom mit_semseg.models import ModelBuilder, SegmentationModule\nfrom mit_semseg.utils import colorEncode, find_recursive, setup_logger\nfrom mit_semseg.lib.nn import user_scattered_collate, async_copy_to\nfrom mit_semseg.lib.utils import as_numpy\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom mit_semseg.config import cfg\n\ncolors = loadmat('data/color150.mat')['colors']\nnames = {}\nwith open('data/object150_info.csv') as f:\n    reader = csv.reader(f)\n    next(reader)\n    for row in reader:\n        names[int(row[0])] = row[5].split(\";\")[0]\n\n\ndef visualize_result(data, pred, cfg):\n    (img, info) = data\n\n    # print predictions in descending order\n    pred = np.int32(pred)\n    pixs = pred.size\n    uniques, counts = np.unique(pred, return_counts=True)\n    print(\"Predictions in [{}]:\".format(info))\n    for idx in np.argsort(counts)[::-1]:\n        name = names[uniques[idx] + 1]\n        ratio = counts[idx] / pixs * 100\n        if ratio > 0.1:\n            print(\"  {}: {:.2f}%\".format(name, ratio))\n\n    # colorize prediction\n    pred_color = colorEncode(pred, colors).astype(np.uint8)\n\n    # aggregate images and save\n    im_vis = np.concatenate((img, pred_color), axis=1)\n\n    img_name = info.split('/')[-1]\n    Image.fromarray(im_vis).save(\n        os.path.join(cfg.TEST.result, img_name.replace('.jpg', '.png')))\n\n\ndef test(segmentation_module, loader, gpu):\n    segmentation_module.eval()\n\n    pbar = tqdm(total=len(loader))\n    for batch_data in loader:\n        # process data\n        batch_data = batch_data[0]\n        segSize = (batch_data['img_ori'].shape[0],\n                   batch_data['img_ori'].shape[1])\n        img_resized_list = batch_data['img_data']\n\n        with torch.no_grad():\n            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n            scores = async_copy_to(scores, gpu)\n\n            for img in img_resized_list:\n                feed_dict = batch_data.copy()\n                feed_dict['img_data'] = img\n                del feed_dict['img_ori']\n                del feed_dict['info']\n                feed_dict = async_copy_to(feed_dict, gpu)\n\n                # forward pass\n                pred_tmp = segmentation_module(feed_dict, segSize=segSize)\n                scores = scores + pred_tmp / len(cfg.DATASET.imgSizes)\n\n            _, pred = torch.max(scores, dim=1)\n            pred = as_numpy(pred.squeeze(0).cpu())\n\n        # visualization\n        visualize_result(\n            (batch_data['img_ori'], batch_data['info']),\n            pred,\n            cfg\n        )\n\n        pbar.update(1)\n\n\ndef main(cfg, gpu):\n    torch.cuda.set_device(gpu)\n\n    # Network Builders\n    net_encoder = ModelBuilder.build_encoder(\n        arch=cfg.MODEL.arch_encoder,\n        fc_dim=cfg.MODEL.fc_dim,\n        weights=cfg.MODEL.weights_encoder)\n    net_decoder = ModelBuilder.build_decoder(\n        arch=cfg.MODEL.arch_decoder,\n        fc_dim=cfg.MODEL.fc_dim,\n        num_class=cfg.DATASET.num_class,\n        weights=cfg.MODEL.weights_decoder,\n        use_softmax=True)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_test = TestDataset(\n        cfg.list_test,\n        cfg.DATASET)\n    loader_test = torch.utils.data.DataLoader(\n        dataset_test,\n        batch_size=cfg.TEST.batch_size,\n        shuffle=False,\n        collate_fn=user_scattered_collate,\n        num_workers=5,\n        drop_last=True)\n\n    segmentation_module.cuda()\n\n    # Main loop\n    test(segmentation_module, loader_test, gpu)\n\n    print('Inference done!')\n\n\nif __name__ == '__main__':\n    assert LooseVersion(torch.__version__) >= LooseVersion('0.4.0'), \\\n        'PyTorch>=0.4.0 is required'\n\n    parser = argparse.ArgumentParser(\n        description=\"PyTorch Semantic Segmentation Testing\"\n    )\n    parser.add_argument(\n        \"--imgs\",\n        required=True,\n        type=str,\n        help=\"an image path, or a directory name\"\n    )\n    parser.add_argument(\n        \"--cfg\",\n        default=\"config/ade20k-resnet50dilated-ppm_deepsup.yaml\",\n        metavar=\"FILE\",\n        help=\"path to config file\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--gpu\",\n        default=0,\n        type=int,\n        help=\"gpu id for evaluation\"\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"Modify config options using the command-line\",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    # cfg.freeze()\n\n    logger = setup_logger(distributed_rank=0)   # TODO\n    logger.info(\"Loaded configuration file {}\".format(args.cfg))\n    logger.info(\"Running with config:\\n{}\".format(cfg))\n\n    cfg.MODEL.arch_encoder = cfg.MODEL.arch_encoder.lower()\n    cfg.MODEL.arch_decoder = cfg.MODEL.arch_decoder.lower()\n\n    # absolute paths of model weights\n    cfg.MODEL.weights_encoder = os.path.join(\n        cfg.DIR, 'encoder_' + cfg.TEST.checkpoint)\n    cfg.MODEL.weights_decoder = os.path.join(\n        cfg.DIR, 'decoder_' + cfg.TEST.checkpoint)\n\n    assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n        os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exitst!\"\n\n    # generate testing image list\n    if os.path.isdir(args.imgs):\n        imgs = find_recursive(args.imgs)\n    else:\n        imgs = [args.imgs]\n    assert len(imgs), \"imgs should be a path to image (.jpg) or directory.\"\n    cfg.list_test = [{'fpath_img': x} for x in imgs]\n\n    if not os.path.isdir(cfg.TEST.result):\n        os.makedirs(cfg.TEST.result)\n\n    main(cfg, args.gpu)\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 9.0078125,
          "content": "# System libs\nimport os\nimport time\n# import math\nimport random\nimport argparse\nfrom distutils.version import LooseVersion\n# Numerical libs\nimport torch\nimport torch.nn as nn\n# Our libs\nfrom mit_semseg.config import cfg\nfrom mit_semseg.dataset import TrainDataset\nfrom mit_semseg.models import ModelBuilder, SegmentationModule\nfrom mit_semseg.utils import AverageMeter, parse_devices, setup_logger\nfrom mit_semseg.lib.nn import UserScatteredDataParallel, user_scattered_collate, patch_replication_callback\n\n\n# train one epoch\ndef train(segmentation_module, iterator, optimizers, history, epoch, cfg):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    ave_total_loss = AverageMeter()\n    ave_acc = AverageMeter()\n\n    segmentation_module.train(not cfg.TRAIN.fix_bn)\n\n    # main loop\n    tic = time.time()\n    for i in range(cfg.TRAIN.epoch_iters):\n        # load a batch of data\n        batch_data = next(iterator)\n        data_time.update(time.time() - tic)\n        segmentation_module.zero_grad()\n\n        # adjust learning rate\n        cur_iter = i + (epoch - 1) * cfg.TRAIN.epoch_iters\n        adjust_learning_rate(optimizers, cur_iter, cfg)\n\n        # forward pass\n        loss, acc = segmentation_module(batch_data)\n        loss = loss.mean()\n        acc = acc.mean()\n\n        # Backward\n        loss.backward()\n        for optimizer in optimizers:\n            optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - tic)\n        tic = time.time()\n\n        # update average loss and acc\n        ave_total_loss.update(loss.data.item())\n        ave_acc.update(acc.data.item()*100)\n\n        # calculate accuracy, and display\n        if i % cfg.TRAIN.disp_iter == 0:\n            print('Epoch: [{}][{}/{}], Time: {:.2f}, Data: {:.2f}, '\n                  'lr_encoder: {:.6f}, lr_decoder: {:.6f}, '\n                  'Accuracy: {:4.2f}, Loss: {:.6f}'\n                  .format(epoch, i, cfg.TRAIN.epoch_iters,\n                          batch_time.average(), data_time.average(),\n                          cfg.TRAIN.running_lr_encoder, cfg.TRAIN.running_lr_decoder,\n                          ave_acc.average(), ave_total_loss.average()))\n\n            fractional_epoch = epoch - 1 + 1. * i / cfg.TRAIN.epoch_iters\n            history['train']['epoch'].append(fractional_epoch)\n            history['train']['loss'].append(loss.data.item())\n            history['train']['acc'].append(acc.data.item())\n\n\ndef checkpoint(nets, history, cfg, epoch):\n    print('Saving checkpoints...')\n    (net_encoder, net_decoder, crit) = nets\n\n    dict_encoder = net_encoder.state_dict()\n    dict_decoder = net_decoder.state_dict()\n\n    torch.save(\n        history,\n        '{}/history_epoch_{}.pth'.format(cfg.DIR, epoch))\n    torch.save(\n        dict_encoder,\n        '{}/encoder_epoch_{}.pth'.format(cfg.DIR, epoch))\n    torch.save(\n        dict_decoder,\n        '{}/decoder_epoch_{}.pth'.format(cfg.DIR, epoch))\n\n\ndef group_weight(module):\n    group_decay = []\n    group_no_decay = []\n    for m in module.modules():\n        if isinstance(m, nn.Linear):\n            group_decay.append(m.weight)\n            if m.bias is not None:\n                group_no_decay.append(m.bias)\n        elif isinstance(m, nn.modules.conv._ConvNd):\n            group_decay.append(m.weight)\n            if m.bias is not None:\n                group_no_decay.append(m.bias)\n        elif isinstance(m, nn.modules.batchnorm._BatchNorm):\n            if m.weight is not None:\n                group_no_decay.append(m.weight)\n            if m.bias is not None:\n                group_no_decay.append(m.bias)\n\n    assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n    groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]\n    return groups\n\n\ndef create_optimizers(nets, cfg):\n    (net_encoder, net_decoder, crit) = nets\n    optimizer_encoder = torch.optim.SGD(\n        group_weight(net_encoder),\n        lr=cfg.TRAIN.lr_encoder,\n        momentum=cfg.TRAIN.beta1,\n        weight_decay=cfg.TRAIN.weight_decay)\n    optimizer_decoder = torch.optim.SGD(\n        group_weight(net_decoder),\n        lr=cfg.TRAIN.lr_decoder,\n        momentum=cfg.TRAIN.beta1,\n        weight_decay=cfg.TRAIN.weight_decay)\n    return (optimizer_encoder, optimizer_decoder)\n\n\ndef adjust_learning_rate(optimizers, cur_iter, cfg):\n    scale_running_lr = ((1. - float(cur_iter) / cfg.TRAIN.max_iters) ** cfg.TRAIN.lr_pow)\n    cfg.TRAIN.running_lr_encoder = cfg.TRAIN.lr_encoder * scale_running_lr\n    cfg.TRAIN.running_lr_decoder = cfg.TRAIN.lr_decoder * scale_running_lr\n\n    (optimizer_encoder, optimizer_decoder) = optimizers\n    for param_group in optimizer_encoder.param_groups:\n        param_group['lr'] = cfg.TRAIN.running_lr_encoder\n    for param_group in optimizer_decoder.param_groups:\n        param_group['lr'] = cfg.TRAIN.running_lr_decoder\n\n\ndef main(cfg, gpus):\n    # Network Builders\n    net_encoder = ModelBuilder.build_encoder(\n        arch=cfg.MODEL.arch_encoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        weights=cfg.MODEL.weights_encoder)\n    net_decoder = ModelBuilder.build_decoder(\n        arch=cfg.MODEL.arch_decoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        num_class=cfg.DATASET.num_class,\n        weights=cfg.MODEL.weights_decoder)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    if cfg.MODEL.arch_decoder.endswith('deepsup'):\n        segmentation_module = SegmentationModule(\n            net_encoder, net_decoder, crit, cfg.TRAIN.deep_sup_scale)\n    else:\n        segmentation_module = SegmentationModule(\n            net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_train = TrainDataset(\n        cfg.DATASET.root_dataset,\n        cfg.DATASET.list_train,\n        cfg.DATASET,\n        batch_per_gpu=cfg.TRAIN.batch_size_per_gpu)\n\n    loader_train = torch.utils.data.DataLoader(\n        dataset_train,\n        batch_size=len(gpus),  # we have modified data_parallel\n        shuffle=False,  # we do not use this param\n        collate_fn=user_scattered_collate,\n        num_workers=cfg.TRAIN.workers,\n        drop_last=True,\n        pin_memory=True)\n    print('1 Epoch = {} iters'.format(cfg.TRAIN.epoch_iters))\n\n    # create loader iterator\n    iterator_train = iter(loader_train)\n\n    # load nets into gpu\n    if len(gpus) > 1:\n        segmentation_module = UserScatteredDataParallel(\n            segmentation_module,\n            device_ids=gpus)\n        # For sync bn\n        patch_replication_callback(segmentation_module)\n    segmentation_module.cuda()\n\n    # Set up optimizers\n    nets = (net_encoder, net_decoder, crit)\n    optimizers = create_optimizers(nets, cfg)\n\n    # Main loop\n    history = {'train': {'epoch': [], 'loss': [], 'acc': []}}\n\n    for epoch in range(cfg.TRAIN.start_epoch, cfg.TRAIN.num_epoch):\n        train(segmentation_module, iterator_train, optimizers, history, epoch+1, cfg)\n\n        # checkpointing\n        checkpoint(nets, history, cfg, epoch+1)\n\n    print('Training Done!')\n\n\nif __name__ == '__main__':\n    assert LooseVersion(torch.__version__) >= LooseVersion('0.4.0'), \\\n        'PyTorch>=0.4.0 is required'\n\n    parser = argparse.ArgumentParser(\n        description=\"PyTorch Semantic Segmentation Training\"\n    )\n    parser.add_argument(\n        \"--cfg\",\n        default=\"config/ade20k-resnet50dilated-ppm_deepsup.yaml\",\n        metavar=\"FILE\",\n        help=\"path to config file\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--gpus\",\n        default=\"0-3\",\n        help=\"gpus to use, e.g. 0-3 or 0,1,2,3\"\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"Modify config options using the command-line\",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    # cfg.freeze()\n\n    logger = setup_logger(distributed_rank=0)   # TODO\n    logger.info(\"Loaded configuration file {}\".format(args.cfg))\n    logger.info(\"Running with config:\\n{}\".format(cfg))\n\n    # Output directory\n    if not os.path.isdir(cfg.DIR):\n        os.makedirs(cfg.DIR)\n    logger.info(\"Outputing checkpoints to: {}\".format(cfg.DIR))\n    with open(os.path.join(cfg.DIR, 'config.yaml'), 'w') as f:\n        f.write(\"{}\".format(cfg))\n\n    # Start from checkpoint\n    if cfg.TRAIN.start_epoch > 0:\n        cfg.MODEL.weights_encoder = os.path.join(\n            cfg.DIR, 'encoder_epoch_{}.pth'.format(cfg.TRAIN.start_epoch))\n        cfg.MODEL.weights_decoder = os.path.join(\n            cfg.DIR, 'decoder_epoch_{}.pth'.format(cfg.TRAIN.start_epoch))\n        assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n            os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exitst!\"\n\n    # Parse gpu ids\n    gpus = parse_devices(args.gpus)\n    gpus = [x.replace('gpu', '') for x in gpus]\n    gpus = [int(x) for x in gpus]\n    num_gpus = len(gpus)\n    cfg.TRAIN.batch_size = num_gpus * cfg.TRAIN.batch_size_per_gpu\n\n    cfg.TRAIN.max_iters = cfg.TRAIN.epoch_iters * cfg.TRAIN.num_epoch\n    cfg.TRAIN.running_lr_encoder = cfg.TRAIN.lr_encoder\n    cfg.TRAIN.running_lr_decoder = cfg.TRAIN.lr_decoder\n\n    random.seed(cfg.TRAIN.seed)\n    torch.manual_seed(cfg.TRAIN.seed)\n\n    main(cfg, gpus)\n"
        }
      ]
    }
  ]
}