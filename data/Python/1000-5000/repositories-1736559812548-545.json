{
  "metadata": {
    "timestamp": 1736559812548,
    "page": 545,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "turboderp-org/exllamav2",
      "stars": 3830,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.11328125,
          "content": "*.egg-info/\nbuild/\n\n*.pyc\n__pycache__/\n.idea\nvenv\ndist\n\n# produced by `pip install -e .`\nexllamav2_ext.cpython-*.so\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0107421875,
          "content": "MIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.091796875,
          "content": "recursive-include exllamav2 *\nglobal-include *.typed\nglobal-exclude *.pyc\nglobal-exclude dni_*"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.9384765625,
          "content": "# ExLlamaV2\n\nExLlamaV2 is an inference library for running local LLMs on modern consumer GPUs.\n\nThe official and recommended backend server for ExLlamaV2 is [TabbyAPI](https://github.com/theroyallab/tabbyAPI/),\nwhich provides an OpenAI-compatible API for local or remote inference, with extended features like HF model\ndownloading, embedding model support and support for HF Jinja2 chat templates.\n\nSee the [wiki](https://github.com/theroyallab/tabbyAPI/wiki/1.-Getting-Started) for help getting started.\n\n\n## New in v0.1.0+:\n\n- ExLlamaV2 now supports paged attention via [Flash Attention](https://github.com/Dao-AILab/flash-attention) 2.5.7+\n- New generator with dynamic batching, smart prompt caching, K/V cache deduplication and simplified API\n\n![alt_text](doc/dynamic_gen.gif)\n\n## Dynamic generator\n\nThe dynamic generator supports all inference, sampling and speculative decoding features of the previous two \ngenerators, consolidated into one API (with the exception of FP8 cache, though the Q4 cache mode is supported and\nperforms better anyway, see [here](doc/qcache_eval.md).)\n\nThe generator is explained in detail [here](doc/dynamic.md).\n\n- Single generation:\n  ```python\n  output = generator.generate(prompt = \"Hello, my name is\", max_new_tokens = 200)\n  ```\n- Batched generation:\n    ```python\n    outputs = generator.generate(\n        prompt = [\n            \"Hello, my name is\",\n            \"Once upon a time,\",\n            \"Large language models are\",\n        ], \n        max_new_tokens = 200\n    )\n    ```\n- Streamed generation with `asyncio`:\n    ```python\n    job = ExLlamaV2DynamicJobAsync(\n        generator,\n        input_ids = tokenizer.encode(\"You can lead a horse to water\"),\n        banned_strings = [\"make it drink\"],\n        gen_settings = ExLlamaV2Sampler.Settings.greedy(),\n        max_new_tokens = 200\n    )  \n    async for result in job:\n        text = result.get(\"text\", \"\")\n        print(text, end = \"\")       \n    ``` \nSee the full, updated examples [here](https://github.com/turboderp/exllamav2/tree/master/examples).\n\n\n## Performance\n\nSome quick tests to compare performance with ExLlama V1. There may be more performance optimizations in the future,\nand speeds will vary across GPUs, with slow CPUs still being a potential bottleneck:\n\n| Model      | Mode         | Size  | grpsz | act | 3090Ti  | 4090        |\n|------------|--------------|-------|-------|-----|---------|-------------|\n| Llama      | GPTQ         | 7B    | 128   | no  | 181 t/s | **205** t/s |\n| Llama      | GPTQ         | 13B   | 128   | no  | 110 t/s | **114** t/s |\n| Llama      | GPTQ         | 33B   | 128   | yes | 44 t/s  | **48** t/s  |\n| OpenLlama  | GPTQ         | 3B    | 128   | yes | 259 t/s | **296** t/s |\n| CodeLlama  | EXL2 4.0 bpw | 34B   | -     | -   | 44 t/s  | **50** t/s  |\n| Llama2     | EXL2 3.0 bpw | 7B    | -     | -   | 217 t/s | **257** t/s |\n| Llama2     | EXL2 4.0 bpw | 7B    | -     | -   | 185 t/s | **211** t/s |\n| Llama2     | EXL2 5.0 bpw | 7B    | -     | -   | 164 t/s | **179** t/s |\n| Llama2     | EXL2 2.5 bpw | 70B   | -     | -   | 33 t/s  | **38** t/s  |\n| TinyLlama  | EXL2 3.0 bpw | 1.1B  | -     | -   | 656 t/s | **770** t/s |\n| TinyLlama  | EXL2 4.0 bpw | 1.1B  | -     | -   | 602 t/s | **700** t/s |\n\n\n## How to\n\nTo install from the repo you'll need the CUDA Toolkit and either gcc on Linux or (Build Tools for) Visual Studio\non Windows). Also make sure you have an appropriate version of [PyTorch](https://pytorch.org/get-started/locally/), then run:\n\n```sh\ngit clone https://github.com/turboderp/exllamav2\ncd exllamav2\npip install -r requirements.txt\npip install .\n\npython test_inference.py -m <path_to_model> -p \"Once upon a time,\"\n# Append the '--gpu_split auto' flag for multi-GPU inference\n```\n\nA simple console chatbot is included. Run it with:\n\n```sh\npython examples/chat.py -m <path_to_model> -mode llama -gs auto\n```\n\n\nThe `-mode` argument chooses the prompt format to use. `raw` will produce a simple chatlog-style chat that works with base \nmodels and various other finetunes. Run with `-modes` for a list of all available prompt formats. You can also provide\na custom system prompt with `-sp`. \n\n\n## Integration and APIs\n\n- [TabbyAPI](https://github.com/theroyallab/tabbyAPI/) is a FastAPI-based server that provides an OpenAI-style web API\ncompatible with [SillyTavern](https://sillytavernai.com/) and other frontends.  \n\n- [ExUI](https://github.com/turboderp/exui) is a simple, standalone single-user web UI that serves an ExLlamaV2 instance\ndirectly with chat and notebook modes.\n\n- [text-generation-webui](https://github.com/oobabooga/text-generation-webui) supports ExLlamaV2 through the **exllamav2**\nand **exllamav2_HF** loaders.\n\n- [lollms-webui](https://github.com/ParisNeo/lollms-webui) supports ExLlamaV2 through the exllamav2 binding.\n\n## Installation\n\n### Method 1: Install from source\n\nTo install the current dev version, clone the repo and run the setup script:\n\n```sh\ngit clone https://github.com/turboderp/exllamav2\ncd exllamav2\npip install -r requirements.txt\npip install .\n```\n\nBy default this will also compile and install the Torch C++ extension (`exllamav2_ext`) that the library relies on. \nYou can skip this step by setting the `EXLLAMA_NOCOMPILE` environment variable:\n\n```sh\nEXLLAMA_NOCOMPILE= pip install .\n```\n\nThis will install the \"JIT version\" of the package, i.e. it will install the Python components without building the\nC++ extension in the process. Instead, the extension will be built the first time the library is used, then cached in \n`~/.cache/torch_extensions` for subsequent use.\n\n### Method 2: Install from release (with prebuilt extension)\n\nReleases are available [here](https://github.com/turboderp/exllamav2/releases), with prebuilt wheels that contain the extension binaries. Make sure to grab\nthe right version, matching your platform, Python version (`cp`) and CUDA version. Crucially, you must also match\nthe prebuilt wheel with your PyTorch version, since the Torch C++ extension ABI breaks with every new version of \nPyTorch.\n\nEither download an appropriate wheel or install directly from the appropriate URL:\n\n```sh\npip install https://github.com/turboderp/exllamav2/releases/download/v0.0.12/exllamav2-0.0.12+cu121-cp311-cp311-linux_x86_64.whl\n```\n\nThe `py3-none-any.whl` version is the JIT version which will build the extension on first launch. The `.tar.gz` file\ncan also be installed this way, and it will build the extension while installing.\n\n### Method 3: Install from PyPI\n\nA PyPI package is available as well. This is the same as the JIT version (see above). It can be installed with:\n\n```sh\npip install exllamav2\n```\n\n\n## EXL2 quantization\n\nExLlamaV2 supports the same 4-bit GPTQ models as V1, but also a new \"EXL2\" format. EXL2 is based on the same\noptimization method as GPTQ and supports 2, 3, 4, 5, 6 and 8-bit quantization. The format allows for mixing quantization\nlevels within a model to achieve any average bitrate between 2 and 8 bits per weight.\n\nMoreover, it's possible to apply multiple quantization levels to each linear layer, producing something akin to sparse \nquantization wherein more important weights (columns) are quantized with more bits. The same remapping trick that lets\nExLlama work efficiently with act-order models allows this mixing of formats to happen with little to no impact on\nperformance.\n\nParameter selection is done automatically by quantizing each matrix multiple times, measuring the quantization \nerror (with respect to the chosen calibration data) for each of a number of possible settings, per layer. Finally, a\ncombination is chosen that minimizes the maximum quantization error over the entire model while meeting a target\naverage bitrate.\n\nIn my tests, this scheme allows Llama2 70B to run on a single 24 GB GPU with a 2048-token context, producing coherent \nand mostly stable output with 2.55 bits per weight. 13B models run at 2.65 bits within 8 GB of VRAM, although currently\nnone of them uses GQA which effectively limits the context size to 2048. In either case it's unlikely that the model\nwill fit alongside a desktop environment. For now.\n\n[![chat_screenshot](doc/llama2_70b_chat_thumb.png)](doc/llama2_70b_chat.png)\n[![chat_screenshot](doc/codellama_13b_instruct_thumb.png)](doc/codellama_13b_instruct.png)\n\n### Conversion\n\nA script is provided to quantize models. Converting large models can be somewhat slow, so be warned. The conversion\nscript and its options are explained in [detail here](doc/convert.md)\n\n### Evaluation\n\nA number of evaluaion scripts are provided. See [here](doc/eval.md) for details.\n\n### Community\n\nA test community is provided at https://discord.gg/NSFwVuCjRq \nQuanting service free of charge is provided at #bot test. The computation is generiously provided by the Bloke powered by Lambda labs. \n\n### HuggingFace repos\n\n- I've uploaded a few EXL2-quantized models to Hugging Face to play around with, [here](https://huggingface.co/turboderp).\n\n- [LoneStriker](https://huggingface.co/LoneStriker) provides a large number of EXL2 models on Hugging Face. \n\n- [bartowski](https://huggingface.co/bartowski) has some more EXL2 models on HF.\n"
        },
        {
          "name": "convert.py",
          "type": "blob",
          "size": 0.0400390625,
          "content": "import exllamav2.conversion.convert_exl2\n"
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "exllamav2",
          "type": "tree",
          "content": null
        },
        {
          "name": "experimental",
          "type": "tree",
          "content": null
        },
        {
          "name": "model_diff.py",
          "type": "blob",
          "size": 7.890625,
          "content": "\nfrom exllamav2 import(\n    ExLlamaV2,\n    ExLlamaV2Config,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Tokenizer,\n    model_init,\n)\n\nfrom exllamav2.attn import ExLlamaV2Attention\n\nimport argparse, os, math, time\nimport pandas, fastparquet\nimport torch\nimport torch.nn.functional as F\nfrom exllamav2.conversion.tokenize import get_tokens\nfrom exllamav2.util import list_live_tensors\nimport gc\n\nimport sys\nimport json\n\ntorch.cuda._lazy_init()\ntorch.set_printoptions(precision = 10)\n\nparser = argparse.ArgumentParser(description = \"Test layer-by-layer hidden state difference between two models\")\nparser.add_argument(\"-ed\", \"--eval_dataset\", type = str, help = \"Perplexity evaluation dataset (.parquet file)\")\nparser.add_argument(\"-er\", \"--eval_rows\", type = int, default = 20, help = \"Number of rows to apply from dataset\")\nparser.add_argument(\"-el\", \"--eval_length\", type = int, default = 2048, help = \"Max no. tokens per sample\")\nparser.add_argument(\"-ma\", \"--model_a\", type = str, help = \"Path to model A\")\nparser.add_argument(\"-mb\", \"--model_b\", type = str, help = \"Path to model B\")\nparser.add_argument(\"-k\", \"--keep_layers\", type = int, default = 0, help = \"Maintain state from model A for this many layers\")\nparser.add_argument(\"-tkm\", \"--topk_max\", type = int, default = 5, help = \"Max top-K interval to test\")\n\nargs = parser.parse_args()\n\n# Initialize both models\n\nprint(f\" -- Model A: {args.model_a}\")\nprint(f\" -- Model B: {args.model_b}\")\n\nconfig = (ExLlamaV2Config(), ExLlamaV2Config())\nconfig[0].model_dir = args.model_a\nconfig[1].model_dir = args.model_b\nconfig[0].prepare()\nconfig[1].prepare()\nconfig[0].max_batch_size = 1\nconfig[1].max_batch_size = 1\nconfig[0].arch_compat_overrides()\nconfig[1].arch_compat_overrides()\n\nmodel = (ExLlamaV2(config[0]), ExLlamaV2(config[1]))\nmodel[0].load(lazy = True)\nmodel[1].load(lazy = True)\n\nnum_modules = len(model[0].modules)\nassert len(model[1].modules) == num_modules\n\n# Tokenizer\n\nprint(f\" -- Loading tokenizer\")\ntokenizer = ExLlamaV2Tokenizer(config[0])\n\nwith torch.no_grad():\n\n    # Input\n\n    print(f\" -- Tokenizing eval data\")\n    eval_tokens = get_tokens(args.eval_rows, args.eval_length, args.eval_dataset, tokenizer)\n    num_rows, seq_len = eval_tokens.shape\n\n    eval_tokens = [eval_tokens[i:i+1, :] for i in range(eval_tokens.shape[0])]\n    attn_params = ExLlamaV2Attention.Params(1, seq_len, 0, None, None)\n\n    # Get embeddings\n\n    print(f\" -- Embeddings\")\n    hidden_state = [[], []]\n    for i in [0, 1]:\n        module = model[i].modules[0]\n        module.load()\n        for j in range(num_rows):\n            hidden_state[i].append(module.forward(eval_tokens[j]))\n        module.unload()\n\n    # Forward\n\n    rfn_error = []\n\n    for idx in range(1, num_modules):\n\n        for i in [0, 1]:\n\n            module = model[i].modules[idx]\n            if i == 0:\n                print(f\" -- {module.key + ' (' + module.name + ')':40}\", end = \"\")\n\n            module.load()\n\n            for j in range(num_rows):\n                if i == 1 and idx <= args.keep_layers:\n                    hidden_state[1][j] = hidden_state[0][j].clone()\n                else:\n                    x = hidden_state[i][j].to(\"cuda:0\")\n                    x = module.forward(x, cache = None, attn_params = attn_params, past_len = 0, loras = None)\n                    hidden_state[i][j] = x.to(\"cpu\")\n                    x = None\n\n            module.unload()\n            module = None\n\n        max_error_ = 0\n        rfn_error_sum = 0\n        mse_sum = 0\n\n        for j in range(num_rows):\n\n            x = hidden_state[0][j].to(\"cuda:0\").float()\n            y = hidden_state[1][j].to(\"cuda:0\").float()\n            rfn_error_sum += torch.linalg.norm(y[0] - x[0], 'fro') / torch.linalg.norm(x[0], 'fro').item()\n            x = None\n            y = None\n\n        rfn_error_ = rfn_error_sum / num_rows\n        print(f\" rfn_error: {rfn_error_:8.6f}\")\n        rfn_error.append(rfn_error_)\n\n\n    # Test outputs\n\n    def ppl(input_ids_, logits_):\n\n        logprob_sum_ = 0.0\n        logprob_count_ = 0\n\n        chunksize = logits_.shape[1] * 16000 // logits_.shape[2]\n        b_ = 0\n        while b_ < logits_.shape[1]:\n            a_ = b_\n            b_ = min(b_ + chunksize, logits_.shape[1])\n\n            logits_f = logits_[:, a_:b_, :].float() + 1e-10\n            target_ids = input_ids_[:, a_ + 1:b_ + 1].to(logits_.device)\n\n            log_probs = F.log_softmax(logits_f, dim=-1)\n            token_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n            logprob_sum_ += token_log_probs.sum().item()\n            logprob_count_ += target_ids.numel()\n\n        return logprob_sum_, logprob_count_\n\n    topk_max = args.topk_max\n    logprob_sum = [0, 0]\n    logprob_count = [0, 0]\n    kl_div_sum = 0\n    kl_div_count = 0\n    mse_sum = 0\n    mse_count = 0\n    topk_hits_sum = [[0] * topk_max, [0] * topk_max]\n    topk_hits_count = [[0] * topk_max, [0] * topk_max]\n    topk_agreement_sum = [0] * topk_max\n    topk_agreement_count = [0] * topk_max\n\n    print(f\" -- Testing outputs\")\n\n    b = 0\n    for j in range(num_rows):\n\n        # Perplexity\n\n        x = (hidden_state[0][j].to(\"cuda:0\"), hidden_state[1][j].to(\"cuda:0\"))\n        input_ids = eval_tokens[j]\n\n        top_indices = []\n\n        for i in [0, 1]:\n            logits = x[i][:, :-1, :]\n            logprob_sum__, logprob_count__ = ppl(input_ids, logits)\n            logprob_sum[i] += logprob_sum__\n            logprob_count[i] += logprob_count__\n\n            _, top_index = torch.topk(logits, topk_max, dim = -1)\n            top_index = top_index.cpu().view(-1, topk_max)\n            top_indices.append(top_index)\n            targets = input_ids[:, 1:].view(-1, 1)\n\n            for t in range(topk_max):\n                top_slice = top_index[:, :t + 1]\n                hits = torch.eq(targets, top_slice)\n                row_hits = hits.any(dim = 1)\n                topk_hits_sum[i][t] += row_hits.sum().item()\n                topk_hits_count[i][t] += top_slice.shape[0]\n\n        for t in range(topk_max):\n            top_slice_a = top_indices[0][:, :t + 1]\n            top_slice_b = top_indices[1][:, :t + 1]\n            hits = torch.eq(top_slice_a, top_slice_b)\n            row_hits = hits.all(dim = 1)\n            topk_agreement_sum[t] += row_hits.sum().item()\n            topk_agreement_count[t] += top_slice_a.shape[0]\n\n        epsilon = 1e-10\n        probs_a = torch.softmax(x[0].float(), dim = -1)\n        probs_b = torch.softmax(x[1].float(), dim = -1)\n        kl_div = F.kl_div(torch.log(probs_a + epsilon), probs_b, reduction = 'none')\n        kl_div_sum += kl_div.sum(dim = -1).mean().item()\n\n        mse_sum += F.mse_loss(probs_a, probs_b)\n        mse_count += 1\n\n    perplexity = (math.exp(-logprob_sum[0] / logprob_count[0]), math.exp(-logprob_sum[1] / logprob_count[1]))\n    mse = mse_sum / mse_count\n    kl_div = kl_div_sum / num_rows\n\n    a_acc = []\n    b_acc = []\n    a_acc_str = \"\"\n    b_acc_str = \"\"\n    agree_str = \"\"\n    topk_agree = []\n    for t in range(topk_max):\n        a_acc_ = topk_hits_sum[0][t] / topk_hits_count[0][t]\n        b_acc_ = topk_hits_sum[1][t] / topk_hits_count[1][t]\n        topk_agree_ = topk_agreement_sum[t] / topk_agreement_count[t]\n        a_acc.append(a_acc_)\n        b_acc.append(b_acc_)\n        topk_agree.append(topk_agree_)\n        a_acc_str += f\"{a_acc_:6.4f}   \"\n        b_acc_str += f\"{b_acc_:6.4f}   \"\n        agree_str += f\"{topk_agree_:6.4f}   \"\n\n# CSV output\n\nprint()\nprint(\"-----------------\")\nprint()\nprint(\";\".join([f\"{p:.8f}\" for p in perplexity]))\nprint()\nprint(f\"{kl_div:.8f}\")\nprint(f\"{mse:.8f}\")\nprint()\nfor i in range(topk_max):\n    print(f\"{i+1};{a_acc[i]:.8f};{b_acc[i]:.8f};{topk_agree[i]:.8f}\")\nprint()\nfor idx, err in enumerate(rfn_error):\n    print(f\"{idx};{err:.8f}\")\nprint()\nprint(\"-----------------\")\nprint()\n\n# Results\n\nprint(f\" -- A, ppl: {perplexity[0]:11.8f}   acc: {a_acc_str}\")\nprint(f\" -- B, ppl: {perplexity[1]:11.8f}   acc: {b_acc_str}\")\nprint(f\" -- Top-K agreement: {agree_str}\")\nprint(f\" -- KL divergence: {kl_div:11.8f}\")\nprint(f\" -- MSE: {mse:11.8f}\")\n\n\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1611328125,
          "content": "pandas\nninja\nwheel\nsetuptools\nfastparquet\ntorch>=2.2.0\nsafetensors>=0.4.3\nsentencepiece>=0.1.97\npygments\nwebsockets\nregex\nnumpy~=1.26.4\ntokenizers\nrich\npillow>=9.1.0"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 5.3740234375,
          "content": "from setuptools import setup, Extension\nimport importlib.util\nimport os\n\nif torch := importlib.util.find_spec(\"torch\") is not None:\n    from torch.utils import cpp_extension\n    from torch import version as torch_version\n\nextension_name = \"exllamav2_ext\"\nprecompile = \"EXLLAMA_NOCOMPILE\" not in os.environ\nverbose = \"EXLLAMA_VERBOSE\" in os.environ\next_debug = \"EXLLAMA_EXT_DEBUG\" in os.environ\n\nif precompile and not torch:\n    print(\n        \"cannot precompile unless torch is installed \\\nTo explicitly JIT install run EXLLAMA_NOCOMPILE= pip install <xyz>\"\n    )\n\nwindows = os.name == \"nt\"\n\nextra_cflags = [\"/Ox\"] if windows else [\"-O3\"]\n\nif ext_debug:\n    extra_cflags += [\"-ftime-report\", \"-DTORCH_USE_CUDA_DSA\"]\n\nextra_cuda_cflags = [\"-lineinfo\", \"-O3\"]\n\nif torch and torch_version.hip:\n    extra_cuda_cflags += [\"-DHIPBLAS_USE_HIP_HALF\"]\n\nextra_compile_args = {\n    \"cxx\": extra_cflags,\n    \"nvcc\": extra_cuda_cflags,\n}\n\nsetup_kwargs = (\n    {\n        \"ext_modules\": [\n            cpp_extension.CUDAExtension(\n                extension_name,\n                [\n                    \"exllamav2/exllamav2_ext/ext_bindings.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_cache.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_gemm.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_hadamard.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_norm.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_qattn.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_qmatrix.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_qmlp.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_quant.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_rope.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_stloader.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_sampling.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_element.cpp\",\n                    \"exllamav2/exllamav2_ext/ext_tp.cpp\",\n                    \"exllamav2/exllamav2_ext/cuda/graph.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/h_add.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/h_gemm.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/lora.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/pack_tensor.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/quantize.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/q_matrix.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/q_attn.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/q_mlp.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/q_gemm.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/rms_norm.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/head_norm.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/layer_norm.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/rope.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/cache.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/util.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/softcap.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/tp.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_1.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_2.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_3.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1a.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1b.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2a.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2b.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3a.cu\",\n                    \"exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3b.cu\",\n                    \"exllamav2/exllamav2_ext/cpp/quantize_func.cpp\",\n                    \"exllamav2/exllamav2_ext/cpp/profiling.cpp\",\n                    \"exllamav2/exllamav2_ext/cpp/generator.cpp\",\n                    \"exllamav2/exllamav2_ext/cpp/sampling.cpp\",\n                    \"exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp\",\n                ],\n                extra_compile_args=extra_compile_args,\n                libraries=[\"cublas\"] if windows else [],\n            )\n        ],\n        \"cmdclass\": {\"build_ext\": cpp_extension.BuildExtension},\n    }\n    if precompile and torch\n    else {}\n)\n\nversion_py = {}\nwith open(\"exllamav2/version.py\", encoding=\"utf8\") as fp:\n    exec(fp.read(), version_py)\nversion = version_py[\"__version__\"]\nprint(\"Version:\", version)\n\n# version = \"0.0.5\"\n\nsetup(\n    name=\"exllamav2\",\n    version=version,\n    packages=[\n        \"exllamav2\",\n        \"exllamav2.generator\",\n        # \"exllamav2.generator.filters\",\n        # \"exllamav2.server\",\n        # \"exllamav2.exllamav2_ext\",\n        # \"exllamav2.exllamav2_ext.cpp\",\n        # \"exllamav2.exllamav2_ext.cuda\",\n        # \"exllamav2.exllamav2_ext.cuda.quant\",\n    ],\n    url=\"https://github.com/turboderp/exllamav2\",\n    license=\"MIT\",\n    author=\"turboderp\",\n    install_requires=[\n        \"pandas\",\n        \"ninja\",\n        \"fastparquet\",\n        \"torch>=2.2.0\",\n        \"safetensors>=0.3.2\",\n        \"sentencepiece>=0.1.97\",\n        \"pygments\",\n        \"websockets\",\n        \"regex\",\n        \"numpy\",\n        \"rich\",\n        \"pillow>=9.1.0\"\n    ],\n    include_package_data=True,\n    package_data={\n        \"\": [\"py.typed\"],\n    },\n    verbose=verbose,\n    **setup_kwargs,\n)\n"
        },
        {
          "name": "test_inference.py",
          "type": "blob",
          "size": 23.0771484375,
          "content": "\nfrom exllamav2 import(\n    ExLlamaV2,\n    ExLlamaV2Config,\n    ExLlamaV2Cache,\n    ExLlamaV2Cache_8bit,\n    ExLlamaV2Cache_Q4,\n    ExLlamaV2Cache_Q6,\n    ExLlamaV2Cache_Q8,\n    ExLlamaV2Cache_TP,\n    ExLlamaV2Tokenizer,\n    model_init,\n)\n\nfrom exllamav2.generator import (\n    ExLlamaV2BaseGenerator,\n    ExLlamaV2Sampler\n)\n\nfrom exllamav2.attn import ExLlamaV2Attention\nfrom exllamav2.mlp import ExLlamaV2MLP\nfrom exllamav2.moe_mlp import ExLlamaV2MoEMLP\nfrom exllamav2.parallel_decoder import ExLlamaV2ParallelDecoder\n\nimport argparse, os, math, time\nimport torch\nimport torch.nn.functional as F\nfrom exllamav2.conversion.tokenize import get_tokens\nfrom exllamav2.conversion.quantize import list_live_tensors\nimport gc\n\n# from exllamav2.mlp import set_catch\n\nimport sys\nimport json\n\ntorch.cuda._lazy_init()\ntorch.set_printoptions(precision = 5, sci_mode = False, linewidth = 150)\n\n# torch.backends.cuda.matmul.allow_tf32 = True\n# torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n# torch.set_float32_matmul_precision(\"medium\")\n\n# (!!!) NOTE: These go on top of the engine arguments that can be found in `model_init.py` (!!!)\nparser = argparse.ArgumentParser(description = \"Test inference on ExLlamaV2 model\")\nparser.add_argument(\"-ed\", \"--eval_dataset\", type = str, help = \"Perplexity evaluation dataset (.parquet file)\")\nparser.add_argument(\"-er\", \"--eval_rows\", type = int, default = 128, help = \"Number of rows to apply from dataset\")\nparser.add_argument(\"-el\", \"--eval_length\", type = int, default = 2048, help = \"Max no. tokens per sample\")\nparser.add_argument(\"-et\", \"--eval_token\", action = \"store_true\", help = \"Evaluate perplexity on token-by-token inference using cache\")\nparser.add_argument(\"-e8\", \"--eval_token_8bit\", action = \"store_true\", help = \"Evaluate perplexity on token-by-token inference using 8-bit (FP8) cache\")\nparser.add_argument(\"-eq4\", \"--eval_token_q4\", action = \"store_true\", help = \"Evaluate perplexity on token-by-token inference using Q4 cache\")\nparser.add_argument(\"-eq6\", \"--eval_token_q6\", action = \"store_true\", help = \"Evaluate perplexity on token-by-token inference using Q6 cache\")\nparser.add_argument(\"-eq8\", \"--eval_token_q8\", action = \"store_true\", help = \"Evaluate perplexity on token-by-token inference using Q8 cache\")\nparser.add_argument(\"-ecl\", \"--eval_context_lens\", action = \"store_true\", help = \"Evaluate perplexity at range of context lengths\")\n# parser.add_argument(\"-eb\", \"--eval_bos\", action = \"store_true\", help = \"Add BOS token to every row in perplexity test (required by Gemma and maybe other models.)\")\nparser.add_argument(\"-p\", \"--prompt\", type = str, help = \"Generate from prompt (basic sampling settings)\")\nparser.add_argument(\"-pnb\", \"--prompt_no_bos\", action = \"store_true\", help = \"Don't add BOS token to prompt\")\nparser.add_argument(\"-t\", \"--tokens\", type = int, default = 128, help = \"Max no. tokens\")\nparser.add_argument(\"-ps\", \"--prompt_speed\", action = \"store_true\", help = \"Test prompt processing (batch) speed over context length\")\nparser.add_argument(\"-s\", \"--speed\", action = \"store_true\", help = \"Test raw generation speed over context length\")\nparser.add_argument(\"-mix\", \"--mix_layers\", type = str, help = \"Load replacement layers from secondary model. Example: --mix_layers 1,6-7:/mnt/models/other_model\")\nparser.add_argument(\"-nwu\", \"--no_warmup\", action = \"store_true\", help = \"Skip warmup before testing model\")\nparser.add_argument(\"-sl\", \"--stream_layers\", action = \"store_true\", help = \"Load model layer by layer (perplexity evaluation only)\")\nparser.add_argument(\"-sp\", \"--standard_perplexity\", choices = [\"wiki2\"], help = \"Run standard (HF) perplexity test, stride 512 (experimental)\")\nparser.add_argument(\"-rr\", \"--rank_reduce\", type = str, help = \"Rank-reduction for MLP layers of model, in reverse order (for experimentation)\")\nparser.add_argument(\"-mol\", \"--max_output_len\", type = int, help = \"Set max output chunk size (incompatible with ppl tests)\")\n\n# Initialize model and tokenizer\n\nmodel_init.add_args(parser)\nargs = parser.parse_args()\n\n# Check conflicting settings\n\nif args.stream_layers:\n    if args.eval_token or args.eval_token_8bit or args.eval_token_q4 or args.eval_token_q6 or args.eval_token_q8:\n        print(\" ## Can't test token ppl while streaming layers\")\n        sys.exit()\n    if args.prompt:\n        print(\" ## Can't generate while streaming layers\")\n        sys.exit()\n    if args.speed or args.prompt_speed:\n        print(\" ## Can't test speed while streaming layers\")\n        sys.exit()\n    if args.gpu_split:\n        print(\" ## Can only use one GPU when streaming layers\")\n        sys.exit()\n    if args.eval_context_lens and args.stream_layers:\n        print(\" ## eval_context_lens not compatible with stream_layers\")\n        sys.exit()\n    if args.eval_dataset:\n        if args.length and args.eval_length != args.length:\n            print(\" !! Overriding model context length to match eval row length\")\n        args.length = args.eval_length\n\n# Init\n\nmodel_init.check_args(args)\nmodel_init.print_options(args)\nmodel, tokenizer = model_init.init(\n    args,\n    allow_auto_split = True,\n    skip_load = args.stream_layers,\n    benchmark = True,\n    max_output_len = args.max_output_len,\n    progress = True\n)\ncache = None\n\n# Auto split\n\nif not model.loaded and not args.stream_layers:\n\n    if args.mix_layers:\n        print(\" !! Warning, auto split does not account for VRAM requirement of replacement layers\")\n\n    print(\" -- Loading model...\")\n    cache = ExLlamaV2Cache(model, lazy = True)\n    t = time.time()\n    model.load_autosplit(cache, progress = True)\n    t = time.time() - t\n    print(f\" -- Loaded model in {t:.4f} seconds\")\n\nif args.stream_layers:\n\n    stream_batch_size = 2\n    model.config.max_batch_size = stream_batch_size\n    model.load(lazy = True)\n\n# Rank reduction\n\nif args.rank_reduce:\n\n    if args.stream_layers:\n        print(\" ## --rank_reduce can not be combined with --stream_layers\")\n        sys.exit()\n\n    rr = args.rank_reduce.split(\",\")\n    idx = len(model.modules) - 1\n    for r in rr:\n        k = float(r)\n\n        while True:\n            idx -= 1\n            module = model.modules[idx]\n            if isinstance(module, ExLlamaV2ParallelDecoder): break\n            if isinstance(module, ExLlamaV2MLP): break\n            if isinstance(module, ExLlamaV2MoEMLP): break\n            if idx < 0:\n                print(\" ## Not enough layers\")\n                sys.exit()\n\n        print(f\" -- Reducing {module.key} ({module.name}) to {k * 100:.2f}%\")\n        module.rank_reduce(k)\n\n# Replacement\n\nif args.mix_layers:\n    intervals_, extra_dir = args.mix_layers.split(\":\")\n\n    print(f\" -- Loading replacement layers from: {extra_dir}\")\n\n    extra_config = ExLlamaV2Config()\n    extra_config.model_dir = extra_dir\n    extra_config.prepare()\n    intervals = intervals_.split(\",\")\n    for interval in intervals:\n        ab = interval.split(\"-\")\n        a, b = int(ab[0]), int(ab[-1])\n        for idx in range(a, b + 1):\n            print(f\" --   Layer {idx}...\")\n            layerkey = \"model.layers.\" + str(idx) + \".\"\n            remove = [k for k in model.config.tensor_file_map.keys() if k.startswith(layerkey)]\n            replace = [k for k in extra_config.tensor_file_map.keys() if k.startswith(layerkey)]\n            # reload = [k for k in model.modules_dict.keys() if k.startswith(layerkey)]\n            for k in remove: del model.config.tensor_file_map[k]\n            for k in replace: model.config.tensor_file_map[k] = extra_config.tensor_file_map[k]\n            # for k in reload:\n            #     model.modules_dict[k].unload()\n            #     model.modules_dict[k].load()\n            if not args.stream_layers:\n                model.modules[idx * 2 + 1].reload()\n                model.modules[idx * 2 + 2].reload()\n\n# Test generation\n\nif args.prompt:\n\n    with torch.inference_mode():\n\n        if cache is None:\n            cache = ExLlamaV2Cache(model) if not model.tp_context else ExLlamaV2Cache_TP(model)\n\n        ids = tokenizer.encode(args.prompt)\n        tokens_prompt = ids.shape[-1]\n\n        print(f\" -- Warmup...\")\n\n        generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n        if not args.no_warmup: generator.warmup()\n\n        print(f\" -- Generating...\")\n        print()\n\n        settings = ExLlamaV2Sampler.Settings()\n        settings.temperature = 1.0\n        settings.top_k = 0\n        settings.top_p = 0.8\n        settings.token_repetition_penalty = 1.02\n        settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n\n        time_begin = time.time()\n\n        output = generator.generate_simple(args.prompt, settings, args.tokens, token_healing = True, add_bos = not args.prompt_no_bos)\n\n        torch.cuda.synchronize()\n        time_prompt = time.time()\n\n        time_end = time.time()\n\n    print(output)\n    print()\n\n    total_gen = time_end - time_begin\n    print(f\" -- Response generated in {total_gen:.2f} seconds, {args.tokens} tokens, {args.tokens / total_gen:.2f} tokens/second (includes prompt eval.)\")\n\n\n# Test perplexity\n\nif args.eval_dataset or args.standard_perplexity:\n\n    with torch.inference_mode():\n\n        print(f\" -- Running perplexity test\")\n\n        if args.standard_perplexity:\n\n            eval_length = args.eval_length\n            if args.eval_dataset:\n                print(f\" !! Note, overriding specified --eval_dataset with {args.standard_perplexity}\")\n\n            from datasets import load_dataset\n\n            if args.standard_perplexity == \"wiki2\":\n                ds = \"wikitext\"\n                part = \"wikitext-2-raw-v1\"\n                split = \"test\"\n            # if args.standard_perplexity == \"c4\":\n            #     ds = \"allenai/c4\"\n            #     part = \"allenai--c4\"\n            #     split = \"train\"\n\n            print(f\" -- Loading dataset {ds}, {part}, {split}...\")\n            test = load_dataset(ds, part, split = split)\n\n            print(f\" -- Tokenizing samples...\")\n            text = \"\\n\\n\".join(test[\"text\"])\n            eval_tokens = tokenizer.encode(text)\n\n            stride = 512\n            seqs = []\n            eval_len = []\n            a = 0\n            while True:\n                b = a + model.config.max_seq_len\n                if b > eval_tokens.shape[-1]: break\n                seqs.append(eval_tokens[:, a:b])\n                eval_len.append(b if a == 0 else stride)\n                a += stride\n\n            eval_tokens = torch.cat(seqs, dim = 0)\n\n        else:\n\n            eval_dataset = args.eval_dataset\n            eval_rows = args.eval_rows\n            eval_length = args.eval_length\n\n            print(f\" -- Dataset: {eval_dataset}\")\n            print(f\" -- Tokenizing eval data, {eval_rows} rows x {eval_length} tokens...\")\n\n            eval_tokens = get_tokens(eval_rows, eval_length, eval_dataset, tokenizer)\n            eval_len = [eval_tokens.shape[1]] * eval_tokens.shape[0]\n\n            # if args.eval_bos:\n            if model.config.arch.lm.requires_bos:\n                boss = torch.full((eval_tokens.shape[0], 1), tokenizer.bos_token_id, dtype = torch.long)\n                eval_tokens = torch.cat((boss, eval_tokens[:, :-1]), dim = 1)\n\n        if args.eval_context_lens:\n            logprob_sum = []\n            logprob_count = []\n        else:\n            logprob_sum = 0.0\n            logprob_count = 0\n\n        def ppl(input_ids__, logits__, lengths__, bins = False):\n\n            logits_device = model.modules[-1].device() if not model.tp_context else \\\n                            torch.device(model.tp_context.device)\n\n            if bins:\n                num_bins = (max(lengths__) + 255) // 256\n                logprob_sum_ = [0.0] * num_bins\n                logprob_count_ = [0] * num_bins\n            else:\n                logprob_sum_ = 0.0\n                logprob_count_ = 0\n\n            assert logits__.shape[0] == input_ids__.shape[0]\n            ll = logits__.shape[1]\n\n            for bi in range(logits__.shape[0]):\n                cl = max(ll - lengths__[bi], 0)\n                logits_ = logits__[bi:bi+1, cl:, :]\n                input_ids_ = input_ids__[bi:bi+1, cl:]\n\n                if bins:\n                    chunksize = 256\n                else:\n                    chunksize = logits_.shape[1] * 4000 // logits_.shape[2] + 1\n                b_ = 0\n                while b_ < logits_.shape[1]:\n                    a_ = b_\n                    b_ = min(b_ + chunksize, logits_.shape[1])\n\n                    logits_f = logits_[:, a_:b_, :].to(logits_device).float() + 1e-10\n                    target_ids = input_ids_[:, a_ + 1:b_ + 1].to(logits_f.device)\n\n                    log_probs = F.log_softmax(logits_f, dim=-1)\n                    token_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n                    if bins:\n                        # for cbin in range(a_ // 256 + 1):\n                        cbin = a_ // 256\n                        logprob_sum_[cbin] += token_log_probs.sum().item()\n                        logprob_count_[cbin] += target_ids.numel()\n                    else:\n                        logprob_sum_ += token_log_probs.sum().item()\n                        logprob_count_ += target_ids.numel()\n\n            return logprob_sum_, logprob_count_\n\n        if args.stream_layers:\n\n            print(f\" -- Inference (streamed)\", end = \"\")\n            sys.stdout.flush()\n\n            batch_size, seq_len = eval_tokens.shape\n            attn_params = ExLlamaV2Attention.Params(stream_batch_size, seq_len, 0, None, None)\n            # attn_mask = model.build_attn_mask(stream_batch_size, seq_len, 0, None, \"cuda:0\")\n\n            for idx, module in enumerate(model.modules):\n                module.set_device_idx(-1 if idx == 0 else 0)\n\n            model.modules[0].load()\n            hidden_state = model.modules[0].forward(eval_tokens)\n            model.modules[0].unload()\n\n            for idx, module in enumerate(model.modules):\n                if idx == 0: continue\n\n                print(\".\", end = \"\")\n                sys.stdout.flush()\n                module.load()\n\n                b = 0\n                while b < eval_tokens.shape[0]:\n                    a = b\n                    b = min(b + stream_batch_size, eval_tokens.shape[0])\n                    x = hidden_state[a:b, :, :].to(\"cuda:0\")\n                    x = module.forward(x, cache = None, attn_params = attn_params, past_len = 0, loras = None)\n\n                    if idx < len(model.modules) - 1:\n                        hidden_state[a:b, :, :] = x.to(\"cpu\")\n\n                    else:\n                        input_ids = eval_tokens[a:b, :]\n                        logits = x[:, :-1, :]\n\n                        # if model.config.logit_scale != 1:\n                        #     logits.mul_(model.config.logit_scale)\n\n                        logprob_sum__, logprob_count__ = ppl(input_ids, logits, eval_len[a:b])\n                        logprob_sum += logprob_sum__\n                        logprob_count += logprob_count__\n\n                module.unload()\n\n            print()\n\n        else:\n\n            print(f\" -- Inference\", end = \"\")\n            sys.stdout.flush()\n\n            if cache is None:\n                if eval_length > model.config.max_input_len:\n                    cache = ExLlamaV2Cache(model, max_seq_len = eval_length) if not model.tp_context else ExLlamaV2Cache_TP(model, max_seq_len = eval_length)\n                else:\n                    cache = None\n\n            for i in range(eval_tokens.shape[0]):\n\n                if i % 10 == 0: print(\".\", end = \"\")\n                sys.stdout.flush()\n\n                input_ids = eval_tokens[i:i+1, :]\n\n                input_ids = input_ids[:, :]\n                if cache is not None: cache.current_seq_len = 0\n                logits = model.forward(input_ids, cache, cpu_logits = input_ids.numel() > 2048)\n                logits = logits[:, :-1, :]\n\n                logprob_sum__, logprob_count__ = ppl(input_ids, logits, eval_len[i:i+1], args.eval_context_lens)\n                if args.eval_context_lens:\n                    while len(logprob_sum) < len(logprob_sum__):\n                        logprob_sum.append(0.0)\n                        logprob_count.append(0)\n                    for j in range(len(logprob_sum__)):\n                        logprob_sum[j] += logprob_sum__[j]\n                        logprob_count[j] += logprob_count__[j]\n                else:\n                    logprob_sum += logprob_sum__\n                    logprob_count += logprob_count__\n\n        if not args.eval_context_lens:\n            print()\n            mean_log_prob = logprob_sum / logprob_count\n            perplexity = math.exp(-mean_log_prob)\n            print(f\" -- Evaluation perplexity: {perplexity:.4f}\")\n        else:\n            print()\n            for j in range(len(logprob_sum__)):\n                mean_log_prob = logprob_sum[j] / logprob_count[j]\n                perplexity = math.exp(-mean_log_prob)\n                dl = min((j + 1) * 256, eval_length)\n                print(f\" -- Evaluation perplexity: {dl} {perplexity:.4f}\")\n\n        def test_ppl_token():\n            global logprob_sum, logprob_count, i, input_ids\n            global logits, target_ids, log_probs, token_log_probs\n            global mean_log_prob, perplexity\n\n            # set_catch(\"model.layers.3\")\n\n            logprob_sum = 0\n            logprob_count = 0\n\n            for i in range(eval_tokens.shape[0]):\n\n                cache.current_seq_len = 0\n\n                for j in range(eval_tokens.shape[1] - 1):\n                    if j % 256 == 0: print(\".\", end = \"\")\n                    sys.stdout.flush()\n\n                    input_ids = eval_tokens[i:i + 1, j:j + 1]\n                    logits = model.forward(input_ids, cache)\n                    logits = logits.float() + 1e-10\n\n                    log_probs = F.log_softmax(logits, dim = -1)\n                    logprob_sum += log_probs[0, 0, eval_tokens[i, j+1]]\n                    logprob_count += 1\n\n                    # mean_log_prob = logprob_sum / logprob_count\n                    # perplexity = math.exp(-mean_log_prob)\n                    # print(f\" -- Token {j}: {perplexity:.4f}\")\n\n            print()\n\n            mean_log_prob = logprob_sum / logprob_count\n            perplexity = math.exp(-mean_log_prob)\n            print(f\" -- Evaluation perplexity: {perplexity:.4f}\")\n\n        if args.eval_token:\n            if args.standard_perplexity:\n                print(f\" !! Note, can't evalutate token perplexity on standard test\")\n            else:\n                print(f\" -- Inference (token)\", end = \"\")\n                sys.stdout.flush()\n                cache = ExLlamaV2Cache(model, max_seq_len = eval_length) if not model.tp_context else \\\n                        ExLlamaV2Cache_TP(model, max_seq_len = eval_length)\n                test_ppl_token()\n\n        if args.eval_token_8bit:\n            if args.standard_perplexity:\n                print(f\" !! Note, can't evalutate token perplexity on standard test\")\n            else:\n                print(f\" -- Inference (token, 8-bit cache)\", end = \"\")\n                sys.stdout.flush()\n                cache = ExLlamaV2Cache_8bit(model, max_seq_len = eval_length) if not model.tp_context else \\\n                        ExLlamaV2Cache_TP(model, max_seq_len = eval_length, base = ExLlamaV2Cache_8bit)\n                test_ppl_token()\n\n        if args.eval_token_q4:\n            if args.standard_perplexity:\n                print(f\" !! Note, can't evalutate token perplexity on standard test\")\n            else:\n                print(f\" -- Inference (token, Q4 cache)\", end = \"\")\n                sys.stdout.flush()\n                cache = ExLlamaV2Cache_Q4(model, max_seq_len = eval_length) if not model.tp_context else \\\n                        ExLlamaV2Cache_TP(model, max_seq_len = eval_length, base = ExLlamaV2Cache_Q4)\n                # cache.calibrate(tokenizer)\n                test_ppl_token()\n\n        if args.eval_token_q6:\n            if args.standard_perplexity:\n                print(f\" !! Note, can't evalutate token perplexity on standard test\")\n            else:\n                print(f\" -- Inference (token, Q6 cache)\", end = \"\")\n                sys.stdout.flush()\n                cache = ExLlamaV2Cache_Q6(model, max_seq_len = eval_length) if not model.tp_context else \\\n                        ExLlamaV2Cache_TP(model, max_seq_len = eval_length, base = ExLlamaV2Cache_Q6)\n                # cache.calibrate(tokenizer)\n                test_ppl_token()\n\n        if args.eval_token_q8:\n            if args.standard_perplexity:\n                print(f\" !! Note, can't evalutate token perplexity on standard test\")\n            else:\n                print(f\" -- Inference (token, Q8 cache)\", end = \"\")\n                sys.stdout.flush()\n                cache = ExLlamaV2Cache_Q8(model, max_seq_len = eval_length) if not model.tp_context else \\\n                        ExLlamaV2Cache_TP(model, max_seq_len = eval_length, base = ExLlamaV2Cache_Q8)\n                # cache.calibrate(tokenizer)\n                test_ppl_token()\n\n\n# Test prompt speed\n\nif args.prompt_speed:\n\n    with torch.inference_mode():\n\n        if cache is None:\n            cache = ExLlamaV2Cache(model) if not model.tp_context else ExLlamaV2Cache_TP(model)\n\n        ids = torch.randint(0, model.config.vocab_size - 1, (1, model.config.max_seq_len))\n\n        print(f\" -- Warmup...\")\n\n        if not args.no_warmup:\n            model.forward(ids[:, -1:])\n\n        print(f\" -- Measuring prompt speed...\")\n\n        torch.cuda.synchronize()\n\n        current_len = 128\n        step = 128\n        prompt_iters = 3\n        while True:\n\n            total_time = 0\n            for i in range(prompt_iters):\n\n                torch.cuda.synchronize()\n                time_begin = time.time()\n\n                cache.current_seq_len = 0\n                model.forward(ids[:, :current_len], cache, preprocess_only = True)\n\n                torch.cuda.synchronize()\n                time_end = time.time()\n                total_time += time_end - time_begin\n\n            tps = current_len / (total_time / prompt_iters)\n\n            print(f\" ** Length {current_len:>5} tokens: {tps:>11.4f} t/s\")\n\n            if current_len >= 1024: step = 1024\n            if current_len >= 4096: step = 4096\n            if current_len >= 16384: step = 8192\n\n            current_len_ = current_len\n            current_len = min(current_len + step, model.config.max_seq_len)\n            if current_len == current_len_: break\n\n\n# Test token speed\n\nif args.speed:\n\n    with torch.inference_mode():\n\n        if cache is None:\n            cache = ExLlamaV2Cache(model) if not model.tp_context else ExLlamaV2Cache_TP(model)\n        cache.current_seq_len = 0\n\n        print(f\" -- Measuring token speed...\")\n        ids = tokenizer.encode(\"X\")\n        model.forward(ids[:, :])\n\n        current_idx = ids.shape[-1]\n        next_stop = 128\n\n        while True:\n\n            time_begin = time.time()\n\n            tokens = next_stop - current_idx\n            for i in range(tokens):\n\n                logits = model.forward(ids[:, -1:], cache)\n                sample = torch.argmax(logits[0, -1]).cpu().unsqueeze(0).unsqueeze(0)\n                sample.clamp_(0, tokenizer.get_vocab_size() - 1)\n                ids = torch.cat((ids, sample), dim=-1)\n\n            time_end = time.time()\n            tps = tokens / (time_end - time_begin)\n\n            print(f\" ** Position {current_idx:>5} + {tokens:>3} tokens: {tps:>9.4f} t/s\")\n\n            current_idx = next_stop\n            next_stop = min(next_stop + 128, model.config.max_seq_len)\n            if next_stop == current_idx: break\n\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}