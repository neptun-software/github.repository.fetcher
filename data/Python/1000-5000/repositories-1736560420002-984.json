{
  "metadata": {
    "timestamp": 1736560420002,
    "page": 984,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lucidrains/musiclm-pytorch",
      "stars": 3208,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.7568359375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2023 Phil Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.3466796875,
          "content": "<img src=\"./musiclm.png\" width=\"450px\"></img>\n\n## MusicLM - Pytorch\n\nImplementation of <a href=\"https://google-research.github.io/seanet/musiclm/examples/\">MusicLM</a>, Google's new SOTA model for music generation using attention networks, in Pytorch.\n\nThey are basically using text-conditioned <a href=\"https://github.com/lucidrains/audiolm-pytorch\">AudioLM</a>, but surprisingly with the embeddings from a text-audio contrastive learned model named <a href=\"https://arxiv.org/abs/2208.12415\">MuLan</a>. MuLan is what will be built out in this repository, with AudioLM modified from the other repository to support the music generation needs here.\n\nPlease join <a href=\"https://discord.gg/xBPBXfcFHd\"><img alt=\"Join us on Discord\" src=\"https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white\"></a> if you are interested in helping out with the replication with the <a href=\"https://laion.ai/\">LAION</a> community\n\n<a href=\"https://www.youtube.com/watch?v=jTrYIGxOuKQ\">What's AI by Louis Bouchard</a>\n\n## Appreciation\n\n- <a href=\"https://stability.ai/\">Stability.ai</a> for the generous sponsorship to work and open source cutting edge artificial intelligence research\n\n- <a href=\"https://huggingface.co/\">ðŸ¤— Huggingface</a> for their <a href=\"https://huggingface.co/docs/accelerate/index\">accelerate</a> training library\n\n## Usage\n\n```install\n$ pip install musiclm-pytorch\n```\n\n## Usage\n\n`MuLaN` first needs to be trained\n\n```python\nimport torch\nfrom musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer\n\naudio_transformer = AudioSpectrogramTransformer(\n    dim = 512,\n    depth = 6,\n    heads = 8,\n    dim_head = 64,\n    spec_n_fft = 128,\n    spec_win_length = 24,\n    spec_aug_stretch_factor = 0.8\n)\n\ntext_transformer = TextTransformer(\n    dim = 512,\n    depth = 6,\n    heads = 8,\n    dim_head = 64\n)\n\nmulan = MuLaN(\n    audio_transformer = audio_transformer,\n    text_transformer = text_transformer\n)\n\n# get a ton of <sound, text> pairs and train\n\nwavs = torch.randn(2, 1024)\ntexts = torch.randint(0, 20000, (2, 256))\n\nloss = mulan(wavs, texts)\nloss.backward()\n\n# after much training, you can embed sounds and text into a joint embedding space\n# for conditioning the audio LM\n\nembeds = mulan.get_audio_latents(wavs)  # during training\n\nembeds = mulan.get_text_latents(texts)  # during inference\n```\n\nTo obtain the conditioning embeddings for the three transformers that are a part of `AudioLM`, you must use the `MuLaNEmbedQuantizer` as so\n\n```python\nfrom musiclm_pytorch import MuLaNEmbedQuantizer\n\n# setup the quantizer with the namespaced conditioning embeddings, unique per quantizer as well as namespace (per transformer)\n\nquantizer = MuLaNEmbedQuantizer(\n    mulan = mulan,                          # pass in trained mulan from above\n    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n    namespaces = ('semantic', 'coarse', 'fine')\n)\n\n# now say you want the conditioning embeddings for semantic transformer\n\nwavs = torch.randn(2, 1024)\nconds = quantizer(wavs = wavs, namespace = 'semantic') # (2, 8, 1024) - 8 is number of quantizers\n```\n\nTo train (or finetune) the three transformers that are a part of `AudioLM`, you simply follow the instructions over at `audiolm-pytorch` for training, but pass in the `MulanEmbedQuantizer` instance to the training classes under the keyword `audio_conditioner`\n\nex. `SemanticTransformerTrainer`\n\n```python\nimport torch\nfrom audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer\n\nwav2vec = HubertWithKmeans(\n    checkpoint_path = './hubert/hubert_base_ls960.pt',\n    kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\n)\n\nsemantic_transformer = SemanticTransformer(\n    num_semantic_tokens = wav2vec.codebook_size,\n    dim = 1024,\n    depth = 6,\n    audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n).cuda()\n\ntrainer = SemanticTransformerTrainer(\n    transformer = semantic_transformer,\n    wav2vec = wav2vec,\n    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n    folder ='/path/to/audio/files',\n    batch_size = 1,\n    data_max_length = 320 * 32,\n    num_train_steps = 1\n)\n\ntrainer.train()\n```\n\nAfter much training on all three transformers (semantic, coarse, fine), you will pass your finetuned or trained-from-scratch `AudioLM` and `MuLaN` wrapped in `MuLaNEmbedQuantizer` to the `MusicLM`\n\n```python\n# you need the trained AudioLM (audio_lm) from above\n# with the MulanEmbedQuantizer (mulan_embed_quantizer)\n\nfrom musiclm_pytorch import MusicLM\n\nmusiclm = MusicLM(\n    audio_lm = audio_lm,                 # `AudioLM` from https://github.com/lucidrains/audiolm-pytorch\n    mulan_embed_quantizer = quantizer    # the `MuLaNEmbedQuantizer` from above\n)\n\nmusic = musiclm('the crystalline sounds of the piano in a ballroom', num_samples = 4) # sample 4 and pick the top match with mulan\n```\n\n## Todo\n\n- [x] mulan seems to be using decoupled contrastive learning, offer that as an option\n- [x] wrap mulan with mulan wrapper and quantize the output, project to audiolm dimensions\n- [x] modify audiolm to accept conditioning embeddings, optionally take care of different dimensions through a separate projection\n- [x] audiolm and mulan goes into musiclm and generate, filter with mulan\n- [x] give dynamic positional bias to self attention in AST\n- [x] implement MusicLM generating multiple samples and selecting top match with MuLaN\n\n- [ ] support variable lengthed audio with masking in audio transformer\n- [ ] add a version of mulan to <a href=\"https://github.com/mlfoundations/open_clip\">open clip</a>\n- [ ] set all the proper spectrogram hyperparameters\n\n## Citations\n\n```bibtex\n@inproceedings{Agostinelli2023MusicLMGM,\n    title     = {MusicLM: Generating Music From Text},\n    author    = {Andrea Agostinelli and Timo I. Denk and Zal{\\'a}n Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and Aren Jansen and Adam Roberts and Marco Tagliasacchi and Matthew Sharifi and Neil Zeghidour and C. Frank},\n    year      = {2023}\n}\n```\n\n```bibtex\n@article{Huang2022MuLanAJ,\n    title   = {MuLan: A Joint Embedding of Music Audio and Natural Language},\n    author  = {Qingqing Huang and Aren Jansen and Joonseok Lee and Ravi Ganti and Judith Yue Li and Daniel P. W. Ellis},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2208.12415}\n}\n```\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2302.01327,\n    doi     = {10.48550/ARXIV.2302.01327},\n    url     = {https://arxiv.org/abs/2302.01327},\n    author  = {Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil},\n    title   = {Dual PatchNorm},\n    publisher = {arXiv},\n    year    = {2023},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n\n```bibtex\n@article{Liu2022PatchDropoutEV,\n    title   = {PatchDropout: Economizing Vision Transformers Using Patch Dropout},\n    author  = {Yue Liu and Christos Matsoukas and Fredrik Strand and Hossein Azizpour and Kevin Smith},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2208.07220}\n}\n```\n\n```bibtex\n@misc{liu2021swin,\n    title   = {Swin Transformer V2: Scaling Up Capacity and Resolution},\n    author  = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},\n    year    = {2021},\n    eprint  = {2111.09883},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{gilmer2023intriguing\n    title  = {Intriguing Properties of Transformer Training Instabilities},\n    author = {Justin Gilmer, Andrea Schioppa, and Jeremy Cohen},\n    year   = {2023},\n    status = {to be published - one attention stabilization technique is circulating within Google Brain, being used by multiple teams}\n}\n```\n\n```bibtex\n@inproceedings{Shukor2022EfficientVP,\n    title   = {Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment},\n    author  = {Mustafa Shukor and Guillaume Couairon and Matthieu Cord},\n    booktitle = {British Machine Vision Conference},\n    year    = {2022}\n}\n```\n\n```bibtex\n@inproceedings{Zhai2023SigmoidLF,\n    title   = {Sigmoid Loss for Language Image Pre-Training},\n    author  = {Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n    year    = {2023}\n}\n```\n\n*The only truth is music.* - Jack Kerouac\n\n*Music is the universal language of mankind.* - Henry Wadsworth Longfellow\n"
        },
        {
          "name": "musiclm.png",
          "type": "blob",
          "size": 137.44921875,
          "content": null
        },
        {
          "name": "musiclm_pytorch",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.025390625,
          "content": "from setuptools import setup, find_packages\n\nsetup(\n  name = 'musiclm-pytorch',\n  packages = find_packages(exclude=[]),\n  version = '0.2.8',\n  license='MIT',\n  description = 'MusicLM - AudioLM + Audio CLIP to text to music synthesis',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/musiclm-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'attention mechanism',\n    'text to music',\n    'contrastive learning'\n  ],\n  install_requires=[\n    'accelerate',\n    'audiolm-pytorch>=0.17.0',\n    'beartype',\n    'einops>=0.6',\n    'lion-pytorch',\n    'vector-quantize-pytorch>=1.0.0',\n    'x-clip',\n    'torch>=1.12',\n    'torchaudio'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)\n"
        }
      ]
    }
  ]
}