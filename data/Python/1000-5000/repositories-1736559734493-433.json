{
  "metadata": {
    "timestamp": 1736559734493,
    "page": 433,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "FunAudioLLM/SenseVoice",
      "stars": 4005,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3232421875,
          "content": ".idea\n./__pycache__/\n*/__pycache__/\n*/*/__pycache__/\n*/*/*/__pycache__/\n.DS_Store\ninit_model/\n*.tar.gz\ntest_local/\nRapidASR\nexport/*\n*.pyc\n.eggs\nMaaS-lib\n.gitignore\n.egg*\ndist\nbuild\nfunasr.egg-info\ndocs/_build\nmodelscope\nsamples\n.ipynb_checkpoints\noutputs*\nemotion2vec*\nGPT-SoVITS*\nmodelscope_models\nexamples/aishell/llm_asr_nar/*\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.0693359375,
          "content": "Ref to https://github.com/modelscope/FunASR?tab=readme-ov-file#license\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.375,
          "content": "([ç®€ä½“ä¸­æ–‡](./README_zh.md)|English|[æ—¥æœ¬èª](./README_ja.md))\n\n\n# Introduction\n\nSenseVoice is a speech foundation model with multiple speech understanding capabilities, including automatic speech recognition (ASR),  spoken language identification (LID), speech emotion recognition (SER), and audio event detection (AED). \n\n<div align=\"center\">  \n<img src=\"image/sensevoice2.png\">\n</div>\n\n[//]: # (<div align=\"center\"><img src=\"image/sensevoice.png\" width=\"700\"/> </div>)\n\n<div align=\"center\">  \n<h4>\n<a href=\"https://funaudiollm.github.io/\"> Homepage </a>\nï½œ<a href=\"#What's News\"> What's News </a>\nï½œ<a href=\"#Benchmarks\"> Benchmarks </a>\nï½œ<a href=\"#Install\"> Install </a>\nï½œ<a href=\"#Usage\"> Usage </a>\nï½œ<a href=\"#Community\"> Community </a>\n</h4>\n\nModel Zoo:\n[modelscope](https://www.modelscope.cn/models/iic/SenseVoiceSmall), [huggingface](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)\n\nOnline Demo:\n[modelscope demo](https://www.modelscope.cn/studios/iic/SenseVoice), [huggingface space](https://huggingface.co/spaces/FunAudioLLM/SenseVoice)\n\n\n</div>\n\n\n<a name=\"Highligts\"></a>\n# Highlights ğŸ¯\n**SenseVoice** focuses on high-accuracy multilingual speech recognition, speech emotion recognition, and audio event detection.\n- **Multilingual Speech Recognition:** Trained with over 400,000 hours of data, supporting more than 50 languages, the recognition performance surpasses that of the Whisper model.\n- **Rich transcribe:** \n  - Possess excellent emotion recognition capabilities, achieving and surpassing the effectiveness of the current best emotion recognition models on test data.\n  - Offer sound event detection capabilities, supporting the detection of various common human-computer interaction events such as bgm, applause, laughter, crying, coughing, and sneezing.\n- **Efficient Inference:** The SenseVoice-Small model utilizes a non-autoregressive end-to-end framework, leading to exceptionally low inference latency. It requires only 70ms to process 10 seconds of audio, which is 15 times faster than Whisper-Large.\n- **Convenient Finetuning:** Provide convenient finetuning scripts and strategies, allowing users to easily address long-tail sample issues according to their business scenarios.\n- **Service Deployment:** Offer service deployment pipeline,  supporting multi-concurrent requests, with client-side languages including Python, C++, HTML, Java, and C#, among others.\n\n<a name=\"What's News\"></a>\n# What's New ğŸ”¥\n- 2024/11: Add support for timestamp based on the CTC alignment.\n- 2024/7: Added Export Features for [ONNX](./demo_onnx.py) and [libtorch](./demo_libtorch.py), as well as Python Version Runtimes: [funasr-onnx-0.4.0](https://pypi.org/project/funasr-onnx/), [funasr-torch-0.1.1](https://pypi.org/project/funasr-torch/)\n- 2024/7: The [SenseVoice-Small](https://www.modelscope.cn/models/iic/SenseVoiceSmall) voice understanding model is open-sourced, which offers high-precision multilingual speech recognition, emotion recognition, and audio event detection capabilities for Mandarin, Cantonese, English, Japanese, and Korean and leads to exceptionally low inference latency.  \n- 2024/7: The CosyVoice for natural speech generation with multi-language, timbre, and emotion control. CosyVoice excels in multi-lingual voice generation, zero-shot voice generation, cross-lingual voice cloning, and instruction-following capabilities. [CosyVoice repo](https://github.com/FunAudioLLM/CosyVoice) and [CosyVoice space](https://www.modelscope.cn/studios/iic/CosyVoice-300M).\n- 2024/7: [FunASR](https://github.com/modelscope/FunASR) is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR.\n\n<a name=\"Benchmarks\"></a>\n# Benchmarks ğŸ“\n\n## Multilingual Speech Recognition\nWe compared the performance of multilingual speech recognition between SenseVoice and Whisper on open-source benchmark datasets, including AISHELL-1, AISHELL-2, Wenetspeech, LibriSpeech, and Common Voice. In terms of Chinese and Cantonese recognition, the SenseVoice-Small model has advantages.\n\n<div align=\"center\">  \n<img src=\"image/asr_results1.png\" width=\"400\" /><img src=\"image/asr_results2.png\" width=\"400\" />\n</div>\n\n## Speech Emotion Recognition\n\nDue to the current lack of widely-used benchmarks and methods for speech emotion recognition, we conducted evaluations across various metrics on multiple test sets and performed a comprehensive comparison with numerous results from recent benchmarks. The selected test sets encompass data in both Chinese and English, and include multiple styles such as performances, films, and natural conversations. Without finetuning on the target data, SenseVoice was able to achieve and exceed the performance of the current best speech emotion recognition models.\n\n<div align=\"center\">  \n<img src=\"image/ser_table.png\" width=\"1000\" />\n</div>\n\nFurthermore, we compared multiple open-source speech emotion recognition models on the test sets, and the results indicate that the SenseVoice-Large model achieved the best performance on nearly all datasets, while the SenseVoice-Small model also surpassed other open-source models on the majority of the datasets.\n\n<div align=\"center\">  \n<img src=\"image/ser_figure.png\" width=\"500\" />\n</div>\n\n## Audio Event Detection\n\nAlthough trained exclusively on speech data, SenseVoice can still function as a standalone event detection model. We compared its performance on the environmental sound classification ESC-50 dataset against the widely used industry models BEATS and PANN. The SenseVoice model achieved commendable results on these tasks. However, due to limitations in training data and methodology, its event classification performance has some gaps compared to specialized AED models.\n\n<div align=\"center\">  \n<img src=\"image/aed_figure.png\" width=\"500\" />\n</div>\n\n## Computational  Efficiency\n\nThe SenseVoice-Small model deploys a non-autoregressive end-to-end architecture, resulting in extremely low inference latency. With a similar number of parameters to the Whisper-Small model, it infers more than 5 times faster than Whisper-Small and 15 times faster than Whisper-Large. \n\n<div align=\"center\">  \n<img src=\"image/inference.png\" width=\"1000\" />\n</div>\n\n\n# Requirements\n\n```shell\npip install -r requirements.txt\n```\n\n<a name=\"Usage\"></a>\n# Usage\n\n## Inference\n\nSupports input of audio in any format and of any duration.\n\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",    \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n<details><summary>Parameter Description (Click to Expand)</summary>\n\n- `model_dir`: The name of the model, or the path to the model on the local disk.\n- `trust_remote_code`:\n  - When `True`, it means that the model's code implementation is loaded from `remote_code`, which specifies the exact location of the `model` code (for example, `model.py` in the current directory). It supports absolute paths, relative paths, and network URLs.\n  - When `False`, it indicates that the model's code implementation is the integrated version within [FunASR](https://github.com/modelscope/FunASR). At this time, modifications made to `model.py` in the current directory will not be effective, as the version loaded is the internal one from FunASR. For the model code, [click here to view](https://github.com/modelscope/FunASR/tree/main/funasr/models/sense_voice).\n- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.\n- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).\n- `use_itn`: Whether the output result includes punctuation and inverse text normalization.\n- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).\n- `merge_vad`: Whether to merge short audio fragments segmented by the VAD model, with the merged length being `merge_length_s`, in seconds (s).\n- `ban_emo_unk`: Whether to ban the output of the `emo_unk` token.\n</details>\n\nIf all inputs are short audios (<30s), and batch inference is needed to speed up inference efficiency, the VAD model can be removed, and `batch_size` can be set accordingly.\n```python\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"zh\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    batch_size=64, \n)\n```\n\nFor more usage, please refer to [docs](https://github.com/modelscope/FunASR/blob/main/docs/tutorial/README.md)\n\n### Inference directly\n\nSupports input of audio in any format, with an input duration limit of 30 seconds or less.\n\n```python\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\n```\n\n### Export and Test\n<details><summary>ONNX and Libtorch Export</summary>\n\n#### ONNX\n```python\n# pip3 install -U funasr funasr-onnx\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\nNote: ONNX model is exported to the original model directory.\n\n#### Libtorch\n```python\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\nNote: Libtorch model is exported to the original model directory.\n</details>\n\n## Service\n\n### Deployment with FastAPI\n```shell\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n## Finetune\n\n### Requirements\n\n```shell\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n### Data prepare\n\nData examples\n\n```text\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\n\nFull ref to `data/train_example.jsonl`\n\n<details><summary>Data Prepare Details</summary>\n\nDescriptionï¼š\n- `key`: audio file unique ID\n- `source`ï¼špath to the audio file\n- `source_len`ï¼šnumber of fbank frames of the audio file\n- `target`ï¼štranscription\n- `target_len`ï¼šlength of target\n- `text_language`ï¼šlanguage id of the audio file\n- `emo_target`ï¼šemotion label of the audio file\n- `event_target`ï¼ševent label of the audio file\n- `with_or_wo_itn`ï¼šwhether includes punctuation and inverse text normalization\n\n\n`train_text.txt`\n\n\n```bash\nBAC009S0764W0121 ç”šè‡³å‡ºç°äº¤æ˜“å‡ ä¹åœæ»çš„æƒ…å†µ\nBAC009S0916W0489 æ¹–åŒ—ä¸€å…¬å¸ä»¥å‘˜å·¥åä¹‰è´·æ¬¾æ•°åå‘˜å·¥è´Ÿå€ºåƒä¸‡\nasr_example_cn_en æ‰€æœ‰åªè¦å¤„ç† data ä¸ç®¡ä½ æ˜¯åš machine learning åš deep learning åš data analytics åš data science ä¹Ÿå¥½ scientist ä¹Ÿå¥½é€šé€šéƒ½è¦éƒ½åšçš„åŸºæœ¬åŠŸå•Šé‚£ again å…ˆå…ˆå¯¹æœ‰ä¸€äº›>ä¹Ÿè®¸å¯¹\nID0012W0014 he tried to think how it could be\n```\n\n`train_wav.scp`\n\n\n\n```bash\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n\n`train_text_language.txt`\n\nThe language ids include `<|zh|>`ã€`<|en|>`ã€`<|yue|>`ã€`<|ja|>` and `<|ko|>`.\n\n```bash\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n\n`train_emo.txt`\n\nThe emotion labels include`<|HAPPY|>`ã€`<|SAD|>`ã€`<|ANGRY|>`ã€`<|NEUTRAL|>`ã€`<|FEARFUL|>`ã€`<|DISGUSTED|>` and `<|SURPRISED|>`.\n\n```bash\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n\n`train_event.txt`\n\nThe event labels include`<|BGM|>`ã€`<|Speech|>`ã€`<|Applause|>`ã€`<|Laughter|>`ã€`<|Cry|>`ã€`<|Sneeze|>`ã€`<|Breath|>` and `<|Cough|>`.\n\n```bash\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```\n\n`Command`\n```shell\n# generate train.jsonl and val.jsonl from wav.scp, text.txt, text_language.txt, emo_target.txt, event_target.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\nIf there is no `train_text_language.txt`, `train_emo_target.txt` and `train_event_target.txt`, the language, emotion and event label will be predicted automatically by using the `SenseVoice` model.\n```shell\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\" \\\n++model_dir='iic/SenseVoiceSmall'\n```\n</details>\n\n### Finetune\n\nEnsure to modify the train_tool in finetune.sh to the absolute path of `funasr/bin/train_ds.py` from the FunASR installation directory you have set up earlier.\n\n```shell\nbash finetune.sh\n```\n\n## WebUI\n\n```shell\npython webui.py\n```\n\n<div align=\"center\"><img src=\"image/webui.png\" width=\"700\"/> </div>\n\n\n## Remarkable Third-Party Work\n- Triton (GPU) Deployment Best Practices: Using Triton + TensorRT, tested with FP32, achieving an acceleration ratio of 526 on V100 GPU. FP16 support is in progress. [Repository](https://github.com/modelscope/FunASR/blob/main/runtime/triton_gpu/README.md)\n- Sherpa-onnx Deployment Best Practices: Supports using SenseVoice in 10 programming languages: C++, C, Python, C#, Go, Swift, Kotlin, Java, JavaScript, and Dart. Also supports deploying SenseVoice on platforms like iOS, Android, and Raspberry Pi. [Repository](https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html)\n- [SenseVoice.cpp](https://github.com/lovemefan/SenseVoice.cpp). Inference of SenseVoice in pure C/C++ based on GGML, supporting 3-bit, 4-bit, 5-bit, 8-bit quantization, etc. with no third-party dependencies.\n- [streaming-sensevoice](https://github.com/pengzhendong/streaming-sensevoice) processes inference in chunks. To achieve pseudo-streaming, it employs a truncated attention mechanism, sacrificing some accuracy. Additionally, this technology supports CTC prefix beam search and hot-word boosting features.\n- [OmniSenseVoice](https://github.com/lifeiteng/OmniSenseVoice) is optimized for lightning-fast inference and batching process. \n\n<a name=\"Community\"></a>\n# Community\nIf you encounter problems in use, you can directly raise Issues on the github page.\n\nYou can also scan the following DingTalk group QR code to join the community group for communication and discussion.\n\n|                          FunASR                          |\n|:--------------------------------------------------------:|\n| <img src=\"image/dingding_funasr.png\" width=\"250\"/></div> |\n\n\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 19.517578125,
          "content": "# SenseVoice\n\nã€Œ[ç®€ä½“ä¸­æ–‡](./README_zh.md)ã€|ã€Œ[English](./README.md)ã€|ã€Œæ—¥æœ¬èªã€\n\nSenseVoiceã¯ã€éŸ³å£°èªè­˜ï¼ˆASRï¼‰ã€è¨€èªè­˜åˆ¥ï¼ˆLIDï¼‰ã€éŸ³å£°æ„Ÿæƒ…èªè­˜ï¼ˆSERï¼‰ã€ãŠã‚ˆã³éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆåˆ†é¡ï¼ˆAECï¼‰ã¾ãŸã¯éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºï¼ˆAEDï¼‰ã‚’å«ã‚€éŸ³å£°ç†è§£èƒ½åŠ›ã‚’å‚™ãˆãŸéŸ³å£°åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€SenseVoiceãƒ¢ãƒ‡ãƒ«ã®ç´¹ä»‹ã¨ã€è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«ã®ä½“é¨“ã«å¿…è¦ãªç’°å¢ƒã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨æ¨è«–æ–¹æ³•ã‚’æä¾›ã—ã¾ã™ã€‚\n\n<div align=\"center\">  \n<img src=\"image/sensevoice2.png\">\n</div>\n[//]: # (<div align=\"center\"><img src=\"image/sensevoice2.png\" width=\"700\"/> </div>)\n \n<div align=\"center\">  \n<h4>\n<a href=\"https://funaudiollm.github.io/\"> ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ </a>\nï½œ<a href=\"#æœ€æ–°åŠ¨æ€\"> æœ€æ–°æƒ…å ± </a>\nï½œ<a href=\"#æ€§èƒ½è¯„æµ‹\"> æ€§èƒ½è©•ä¾¡ </a>\nï½œ<a href=\"#ç¯å¢ƒå®‰è£…\"> ç’°å¢ƒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« </a>\nï½œ<a href=\"#ç”¨æ³•æ•™ç¨‹\"> ä½¿ç”¨æ–¹æ³•ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« </a>\nï½œ<a href=\"#è”ç³»æˆ‘ä»¬\"> ãŠå•ã„åˆã‚ã› </a>\n</h4>\n\nãƒ¢ãƒ‡ãƒ«ãƒªãƒã‚¸ãƒˆãƒªï¼š[modelscope](https://www.modelscope.cn/models/iic/SenseVoiceSmall)ï¼Œ[huggingface](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)\n\nã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä½“é¨“ï¼š\n[modelscope demo](https://www.modelscope.cn/studios/iic/SenseVoice), [huggingface space](https://huggingface.co/spaces/FunAudioLLM/SenseVoice)\n\n</div>\n\n<a name=\"æ ¸å¿ƒåŠŸèƒ½\"></a>\n# ã‚³ã‚¢æ©Ÿèƒ½ ğŸ¯\n**SenseVoice**ã¯ã€é«˜ç²¾åº¦ãªå¤šè¨€èªéŸ³å£°èªè­˜ã€æ„Ÿæƒ…èªè­˜ã€ãŠã‚ˆã³éŸ³å£°ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚\n- **å¤šè¨€èªèªè­˜ï¼š** 40ä¸‡æ™‚é–“ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€50ä»¥ä¸Šã®è¨€èªã‚’ã‚µãƒãƒ¼ãƒˆã—ã€èªè­˜æ€§èƒ½ã¯Whisperãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã¾ã™ã€‚\n- **ãƒªãƒƒãƒãƒ†ã‚­ã‚¹ãƒˆèªè­˜ï¼š** \n  - å„ªã‚ŒãŸæ„Ÿæƒ…èªè­˜èƒ½åŠ›ã‚’æŒã¡ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ç¾åœ¨ã®æœ€è‰¯ã®æ„Ÿæƒ…èªè­˜ãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœã‚’é”æˆãŠã‚ˆã³ä¸Šå›ã‚Šã¾ã™ã€‚\n  - éŸ³å£°ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºèƒ½åŠ›ã‚’æä¾›ã—ã€éŸ³æ¥½ã€æ‹æ‰‹ã€ç¬‘ã„å£°ã€æ³£ãå£°ã€å’³ã€ãã—ã‚ƒã¿ãªã©ã®ã•ã¾ã–ã¾ãªä¸€èˆ¬çš„ãªäººé–“ã¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œå‡ºã—ã¾ã™ã€‚\n- **åŠ¹ç‡çš„ãªæ¨è«–ï¼š** SenseVoice-Smallãƒ¢ãƒ‡ãƒ«ã¯éè‡ªå·±å›å¸°ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€æ¨è«–é…å»¶ãŒéå¸¸ã«ä½ãã€10ç§’ã®éŸ³å£°ã®æ¨è«–ã«70msã—ã‹ã‹ã‹ã‚Šã¾ã›ã‚“ã€‚Whisper-Largeã‚ˆã‚Š15å€é«˜é€Ÿã§ã™ã€‚\n- **ç°¡å˜ãªå¾®èª¿æ•´ï¼š** ä¾¿åˆ©ãªå¾®èª¿æ•´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨æˆ¦ç•¥ã‚’æä¾›ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ“ã‚¸ãƒã‚¹ã‚·ãƒŠãƒªã‚ªã«å¿œã˜ã¦ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ã‚µãƒ³ãƒ—ãƒ«ã®å•é¡Œã‚’ç°¡å˜ã«è§£æ±ºã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n- **ã‚µãƒ¼ãƒ“ã‚¹å±•é–‹ï¼š** ãƒãƒ«ãƒã‚³ãƒ³ã‚«ãƒ¬ãƒ³ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å®Œå…¨ãªã‚µãƒ¼ãƒ“ã‚¹å±•é–‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã®è¨€èªã«ã¯Pythonã€C++ã€HTMLã€Javaã€C#ãªã©ãŒã‚ã‚Šã¾ã™ã€‚\n\n<a name=\"æœ€æ–°åŠ¨æ€\"></a>\n# æœ€æ–°æƒ…å ± ğŸ”¥\n- 2024/7ï¼šæ–°ã—ã[ONNX](./demo_onnx.py)ã¨[libtorch](./demo_libtorch.py)ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã‚’è¿½åŠ ã—ã€Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ï¼š[funasr-onnx-0.4.0](https://pypi.org/project/funasr-onnx/)ã€[funasr-torch-0.1.1](https://pypi.org/project/funasr-torch/)ã‚‚æä¾›é–‹å§‹ã€‚\n- 2024/7: [SenseVoice-Small](https://www.modelscope.cn/models/iic/SenseVoiceSmall) å¤šè¨€èªéŸ³å£°ç†è§£ãƒ¢ãƒ‡ãƒ«ãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¾ã—ãŸã€‚ä¸­å›½èªã€åºƒæ±èªã€è‹±èªã€æ—¥æœ¬èªã€éŸ“å›½èªã®å¤šè¨€èªéŸ³å£°èªè­˜ã€æ„Ÿæƒ…èªè­˜ã€ãŠã‚ˆã³ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºèƒ½åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€éå¸¸ã«ä½ã„æ¨è«–é…å»¶ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚\n- 2024/7: CosyVoiceã¯è‡ªç„¶ãªéŸ³å£°ç”Ÿæˆã«å–ã‚Šçµ„ã‚“ã§ãŠã‚Šã€å¤šè¨€èªã€éŸ³è‰²ã€æ„Ÿæƒ…åˆ¶å¾¡ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚å¤šè¨€èªéŸ³å£°ç”Ÿæˆã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆéŸ³å£°ç”Ÿæˆã€ã‚¯ãƒ­ã‚¹ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ³ã€ãŠã‚ˆã³æŒ‡ç¤ºã«å¾“ã†èƒ½åŠ›ã«å„ªã‚Œã¦ã„ã¾ã™ã€‚[CosyVoice repo](https://github.com/FunAudioLLM/CosyVoice) and [CosyVoice ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä½“é¨“](https://www.modelscope.cn/studios/iic/CosyVoice-300M).\n- 2024/7: [FunASR](https://github.com/modelscope/FunASR) ã¯ã€éŸ³å£°èªè­˜ï¼ˆASRï¼‰ã€éŸ³å£°æ´»å‹•æ¤œå‡ºï¼ˆVADï¼‰ã€å¥èª­ç‚¹å¾©å…ƒã€è¨€èªãƒ¢ãƒ‡ãƒ«ã€è©±è€…æ¤œè¨¼ã€è©±è€…åˆ†é›¢ã€ãŠã‚ˆã³ãƒãƒ«ãƒãƒˆãƒ¼ã‚«ãƒ¼ASRãªã©ã®æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹åŸºæœ¬çš„ãªéŸ³å£°èªè­˜ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã§ã™ã€‚\n\n<a name=\"Benchmarks\"></a>\n# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ğŸ“\n\n## å¤šè¨€èªéŸ³å£°èªè­˜\n\nã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆAISHELL-1ã€AISHELL-2ã€Wenetspeechã€Librispeechã€Common Voiceã‚’å«ã‚€ï¼‰ã§SenseVoiceã¨Whisperã®å¤šè¨€èªéŸ³å£°èªè­˜æ€§èƒ½ã¨æ¨è«–åŠ¹ç‡ã‚’æ¯”è¼ƒã—ã¾ã—ãŸã€‚ä¸­å›½èªã¨åºƒæ±èªã®èªè­˜åŠ¹æœã«ãŠã„ã¦ã€SenseVoice-Smallãƒ¢ãƒ‡ãƒ«ã¯æ˜ã‚‰ã‹ãªåŠ¹æœã®å„ªä½æ€§ã‚’æŒã£ã¦ã„ã¾ã™ã€‚\n\n<div align=\"center\">  \n<img src=\"image/asr_results1.png\" width=\"400\" /><img src=\"image/asr_results2.png\" width=\"400\" />\n</div>\n\n## æ„Ÿæƒ…èªè­˜\n\nç¾åœ¨ã€åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ„Ÿæƒ…èªè­˜ã®ãƒ†ã‚¹ãƒˆæŒ‡æ¨™ã¨æ–¹æ³•ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã•ã¾ã–ã¾ãªæŒ‡æ¨™ã‚’ãƒ†ã‚¹ãƒˆã—ã€æœ€è¿‘ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®è¤‡æ•°ã®çµæœã¨åŒ…æ‹¬çš„ã«æ¯”è¼ƒã—ã¾ã—ãŸã€‚é¸æŠã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«ã¯ã€ä¸­å›½èª/è‹±èªã®ä¸¡æ–¹ã®è¨€èªã¨ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€æ˜ ç”»ã€è‡ªç„¶ãªä¼šè©±ãªã©ã®ã•ã¾ã–ã¾ãªã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®å¾®èª¿æ•´ã‚’è¡Œã‚ãªã„å‰æã§ã€SenseVoiceã¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ç¾åœ¨ã®æœ€è‰¯ã®æ„Ÿæƒ…èªè­˜ãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœã‚’é”æˆãŠã‚ˆã³ä¸Šå›ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚\n\n<div align=\"center\">  \n<img src=\"image/ser_table.png\" width=\"1000\" />\n</div>\n\nã•ã‚‰ã«ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è¤‡æ•°ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®æ„Ÿæƒ…èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã€çµæœã¯SenseVoice-Largeãƒ¢ãƒ‡ãƒ«ãŒã»ã¼ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã§æœ€è‰¯ã®åŠ¹æœã‚’é”æˆã—ã€SenseVoice-Smallãƒ¢ãƒ‡ãƒ«ã‚‚å¤šæ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä»–ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹åŠ¹æœã‚’é”æˆã—ãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n\n<div align=\"center\">  \n<img src=\"image/ser_figure.png\" width=\"500\" />\n</div>\n\n## ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡º\n\nSenseVoiceã¯éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦å˜ç‹¬ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ç’°å¢ƒéŸ³åˆ†é¡ESC-50ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€ç¾åœ¨æ¥­ç•Œã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹BEATSãŠã‚ˆã³PANNãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœã¨æ¯”è¼ƒã—ã¾ã—ãŸã€‚SenseVoiceãƒ¢ãƒ‡ãƒ«ã¯ã“ã‚Œã‚‰ã®ã‚¿ã‚¹ã‚¯ã§è‰¯å¥½ãªåŠ¹æœã‚’é”æˆã—ã¾ã—ãŸãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã®åˆ¶ç´„ã«ã‚ˆã‚Šã€ã‚¤ãƒ™ãƒ³ãƒˆåˆ†é¡ã®åŠ¹æœã¯å°‚é–€ã®ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã¾ã ä¸€å®šã®å·®ãŒã‚ã‚Šã¾ã™ã€‚\n\n<div align=\"center\">  \n<img src=\"image/aed_figure.png\" width=\"500\" />\n</div>\n\n## æ¨è«–åŠ¹ç‡\n\nSenseVoice-smallãƒ¢ãƒ‡ãƒ«ã¯éè‡ªå·±å›å¸°ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€æ¨è«–é…å»¶ãŒéå¸¸ã«ä½ã„ã§ã™ã€‚Whisper-Smallãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡ã§ã€Whisper-Smallãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Š5å€é«˜é€Ÿã§ã€Whisper-Largeãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Š15å€é«˜é€Ÿã§ã™ã€‚åŒæ™‚ã«ã€SenseVoice-smallãƒ¢ãƒ‡ãƒ«ã¯éŸ³å£°ã®é•·ã•ãŒå¢—åŠ ã—ã¦ã‚‚ã€æ¨è«–æ™‚é–“ã«æ˜ã‚‰ã‹ãªå¢—åŠ ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n\n<div align=\"center\">  \n<img src=\"image/inference.png\" width=\"1000\" />\n</div>\n\n<a name=\"ç¯å¢ƒå®‰è£…\"></a>\n# ç’°å¢ƒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ğŸ\n\n```shell\npip install -r requirements.txt\n```\n\n<a name=\"ç”¨æ³•æ•™ç¨‹\"></a>\n# ä½¿ç”¨æ–¹æ³• ğŸ› ï¸\n\n## æ¨è«–\n\nä»»æ„ã®å½¢å¼ã®éŸ³å£°å…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€ä»»æ„ã®é•·ã•ã®å…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",  \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n<details><summary>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¬æ˜ï¼ˆã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹ï¼‰</summary>\n\n- `model_dir`ï¼šãƒ¢ãƒ‡ãƒ«åã€ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ä¸Šã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã€‚\n- `trust_remote_code`ï¼š\n  - `True`ã¯ã€modelã‚³ãƒ¼ãƒ‰ã®å®Ÿè£…ãŒ`remote_code`ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€`remote_code`ã¯`model`ã‚³ãƒ¼ãƒ‰ã®æ­£ç¢ºãªä½ç½®ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆä¾‹ï¼šç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®`model.py`ï¼‰ã€‚çµ¶å¯¾ãƒ‘ã‚¹ã€ç›¸å¯¾ãƒ‘ã‚¹ã€ãŠã‚ˆã³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯URLã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n  - `False`ã¯ã€modelã‚³ãƒ¼ãƒ‰ã®å®Ÿè£…ãŒ[FunASR](https://github.com/modelscope/FunASR)å†…éƒ¨ã«çµ±åˆã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€ã“ã®å ´åˆã€ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®`model.py`ã‚’å¤‰æ›´ã—ã¦ã‚‚åŠ¹æœãŒã‚ã‚Šã¾ã›ã‚“ã€‚FunASRå†…éƒ¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãŸã‚ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ¼ãƒ‰[ã“ã¡ã‚‰ã‚’å‚ç…§](https://github.com/modelscope/FunASR/tree/main/funasr/models/sense_voice)ã€‚\n- `vad_model`ï¼šVADï¼ˆéŸ³å£°æ´»å‹•æ¤œå‡ºï¼‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚VADã®ç›®çš„ã¯ã€é•·ã„éŸ³å£°ã‚’çŸ­ã„ã‚¯ãƒªãƒƒãƒ—ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã®å ´åˆã€æ¨è«–æ™‚é–“ã«ã¯VADã¨SenseVoiceã®åˆè¨ˆæ¶ˆè²»ãŒå«ã¾ã‚Œã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®é…å»¶ã‚’è¡¨ã—ã¾ã™ã€‚SenseVoiceãƒ¢ãƒ‡ãƒ«ã®æ¨è«–æ™‚é–“ã‚’å€‹åˆ¥ã«ãƒ†ã‚¹ãƒˆã™ã‚‹å ´åˆã¯ã€VADãƒ¢ãƒ‡ãƒ«ã‚’ç„¡åŠ¹ã«ã§ãã¾ã™ã€‚\n- `vad_kwargs`ï¼šVADãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’æŒ‡å®šã—ã¾ã™ã€‚`max_single_segment_time`ï¼š`vad_model`ã«ã‚ˆã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€å¤§é•·ã‚’ç¤ºã—ã€å˜ä½ã¯ãƒŸãƒªç§’ï¼ˆmsï¼‰ã§ã™ã€‚\n- `use_itn`ï¼šå‡ºåŠ›çµæœã«å¥èª­ç‚¹ã¨é€†ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ãŒå«ã¾ã‚Œã‚‹ã‹ã©ã†ã‹ã€‚\n- `batch_size_s`ï¼šå‹•çš„ãƒãƒƒãƒã®ä½¿ç”¨ã‚’ç¤ºã—ã€ãƒãƒƒãƒå†…ã®éŸ³å£°ã®åˆè¨ˆé•·ã‚’ç§’ï¼ˆsï¼‰ã§æ¸¬å®šã—ã¾ã™ã€‚\n- `merge_vad`ï¼šVADãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦åˆ†å‰²ã•ã‚ŒãŸçŸ­ã„éŸ³å£°ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã‹ã©ã†ã‹ã€‚ãƒãƒ¼ã‚¸å¾Œã®é•·ã•ã¯`merge_length_s`ã§ã€å˜ä½ã¯ç§’ï¼ˆsï¼‰ã§ã™ã€‚\n- `ban_emo_unk`ï¼šemo_unkãƒ©ãƒ™ãƒ«ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã€‚\n</details>\n\nã™ã¹ã¦ã®å…¥åŠ›ãŒçŸ­ã„éŸ³å£°ï¼ˆ30ç§’æœªæº€ï¼‰ã§ã‚ã‚Šã€ãƒãƒƒãƒæ¨è«–ãŒå¿…è¦ãªå ´åˆã€æ¨è«–åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«VADãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤ã—ã€`batch_size`ã‚’è¨­å®šã§ãã¾ã™ã€‚\n\n```python\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size=64, \n)\n```\n\nè©³ç´°ãªä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/modelscope/FunASR/blob/main/docs/tutorial/README.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n### ç›´æ¥æ¨è«–\n\nä»»æ„ã®å½¢å¼ã®éŸ³å£°å…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€å…¥åŠ›éŸ³å£°ã®é•·ã•ã¯30ç§’ä»¥ä¸‹ã«åˆ¶é™ã•ã‚Œã¾ã™ã€‚\n\n```python\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\n```\n\n## ã‚µãƒ¼ãƒ“ã‚¹å±•é–‹\n\næœªå®Œäº†\n\n### ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨ãƒ†ã‚¹ãƒˆ\n<details><summary>ONNXã¨Libtorchã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ</summary>\n\n#### ONNX\n```python\n# pip3 install -U funasr funasr-onnx\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\nå‚™è€ƒï¼šONNXãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚\n\n#### Libtorch\n```python\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\nå‚™è€ƒï¼šLibtorchãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚\n\n</details>\n\n### å±•é–‹\n\n### FastAPIã‚’ä½¿ã£ãŸå±•é–‹\n```shell\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n## å¾®èª¿æ•´\n\n### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç’°å¢ƒã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n```shell\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n### ãƒ‡ãƒ¼ã‚¿æº–å‚™\n\nãƒ‡ãƒ¼ã‚¿ä¾‹\n```text\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\nè©³ç´°ã¯ `data/train_example.jsonl` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n<details><summary>ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®è©³ç´°</summary>\n\nèª¬æ˜ï¼š\n- `key`ï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ID\n- `source`ï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n- `source_len`ï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®fbankãƒ•ãƒ¬ãƒ¼ãƒ æ•°\n- `target`ï¼šæ–‡å­—èµ·ã“ã—çµæœ\n- `target_len`ï¼štargetï¼ˆæ–‡å­—èµ·ã“ã—ï¼‰ã®é•·ã•\n- `text_language`ï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨€èªID\n- `emo_target`ï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«\n- `event_target`ï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ©ãƒ™ãƒ«\n- `with_or_wo_itn`ï¼šå¥èª­ç‚¹ã¨é€†ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ã‚’å«ã‚€ã‹ã©ã†ã‹\n\n`train_text.txt`\n```bash\nBAC009S0764W0121 ç”šè‡³å‡ºç°äº¤æ˜“å‡ ä¹åœæ»çš„æƒ…å†µ\nBAC009S0916W0489 æ¹–åŒ—ä¸€å…¬å¸ä»¥å‘˜å·¥åä¹‰è´·æ¬¾æ•°åå‘˜å·¥è´Ÿå€ºåƒä¸‡\nasr_example_cn_en æ‰€æœ‰åªè¦å¤„ç† data ä¸ç®¡ä½ æ˜¯åš machine learning åš deep learning åš data analytics åš data science ä¹Ÿå¥½ scientist ä¹Ÿå¥½é€šé€šéƒ½è¦éƒ½åšçš„åŸºæœ¬åŠŸå•Šé‚£ again å…ˆå…ˆå¯¹æœ‰ä¸€äº›>ä¹Ÿè®¸å¯¹\nID0012W0014 he tried to think how it could be\n```\n`train_wav.scp`\n```bash\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n`train_text_language.txt`\nè¨€èªIDã¯ `<|zh|>`ã€`<|en|>`ã€`<|yue|>`ã€`<|ja|>`ã€ãŠã‚ˆã³ `<|ko|>`ã‚’å«ã¿ã¾ã™ã€‚\n```bash\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n`train_emo.txt`\næ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã¯ã€`<|HAPPY|>`ã€`<|SAD|>`ã€`<|ANGRY|>`ã€`<|NEUTRAL|>`ã€`<|FEARFUL|>`ã€`<|DISGUSTED|>` ãŠã‚ˆã³ `<|SURPRISED|>`ã‚’å«ã¿ã¾ã™ã€‚\n```bash\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n`train_event.txt`\nã‚¤ãƒ™ãƒ³ãƒˆãƒ©ãƒ™ãƒ«ã¯ã€ `<|BGM|>`ã€`<|Speech|>`ã€`<|Applause|>`ã€`<|Laughter|>`ã€`<|Cry|>`ã€`<|Sneeze|>`ã€`<|Breath|>` ãŠã‚ˆã³ `<|Cough|>`ã‚’å«ã¿ã¾ã™ã€‚\n```bash\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```\n`ã‚³ãƒãƒ³ãƒ‰`\n```shell\n# wav.scpã€text.txtã€text_language.txtã€emo_target.txtã€event_target.txt ã‹ã‚‰ train.jsonl ã¨ val.jsonl ã‚’ç”Ÿæˆã—ã¾ã™\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n`train_text_language.txt`ã€`train_emo_target.txt`ã€`train_event_target.txt` ãŒãªã„å ´åˆã€`SenseVoice` ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è¨€èªã€æ„Ÿæƒ…ã€ãŠã‚ˆã³ã‚¤ãƒ™ãƒ³ãƒˆãƒ©ãƒ™ãƒ«ãŒè‡ªå‹•çš„ã«äºˆæ¸¬ã•ã‚Œã¾ã™ã€‚\n```shell\n# wav.scp ã¨ text.txt ã‹ã‚‰ train.jsonl ã¨ val.jsonl ã‚’ç”Ÿæˆã—ã¾ã™\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n</details>\n\n### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é–‹å§‹\n\n`finetune.sh`ã®`train_tool`ã‚’ã€å‰è¿°ã®FunASRãƒ‘ã‚¹å†…ã®`funasr/bin/train_ds.py`ã®çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚\n\n```shell\nbash finetune.sh\n```\n\n## WebUI\n\n```shell\npython webui.py\n```\n\n<div align=\"center\"><img src=\"image/webui.png\" width=\"700\"/> </div>\n\n## æ³¨ç›®ã™ã¹ãã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®å–ã‚Šçµ„ã¿\n- Triton (GPU) ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼šTriton + TensorRT ã‚’ä½¿ç”¨ã—ã€FP32 ã§ãƒ†ã‚¹ãƒˆã€‚V100 GPU ã§åŠ é€Ÿæ¯” 526 ã‚’é”æˆã€‚FP16 ã®ã‚µãƒãƒ¼ãƒˆã¯é€²è¡Œä¸­ã§ã™ã€‚[ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/modelscope/FunASR/blob/main/runtime/triton_gpu/README.md)\n- Sherpa-onnx ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼šSenseVoice ã‚’10ç¨®é¡ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªï¼ˆC++, C, Python, C#, Go, Swift, Kotlin, Java, JavaScript, Dartï¼‰ã§ä½¿ç”¨å¯èƒ½ã€‚ã¾ãŸã€iOS, Android, Raspberry Pi ãªã©ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚‚ SenseVoice ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã¾ã™ã€‚[ãƒªãƒã‚¸ãƒˆãƒª](https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html)\n- [SenseVoice.cpp](https://github.com/lovemefan/SenseVoice.cpp) GGMLã«åŸºã¥ã„ã¦ç´”ç²‹ãªC/C++ã§SenseVoiceã‚’æ¨æ¸¬ã—ã€3ãƒ“ãƒƒãƒˆã€4ãƒ“ãƒƒãƒˆã€5ãƒ“ãƒƒãƒˆã€8ãƒ“ãƒƒãƒˆé‡å­åŒ–ãªã©ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®ä¾å­˜é–¢ä¿‚ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n- [streaming-sensevoice](https://github.com/pengzhendong/streaming-sensevoice) ã‚¹ãƒˆãƒªãƒ¼ãƒ å‹SenseVoiceã¯ã€ãƒãƒ£ãƒ³ã‚¯ï¼ˆchunkï¼‰æ–¹å¼ã§æ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚æ“¬ä¼¼ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€ä¸€éƒ¨ã®ç²¾åº¦ã‚’çŠ ç‰²ã«ã—ã¦åˆ‡ã‚Šæ¨ã¦æ³¨æ„æ©Ÿæ§‹ï¼ˆtruncated attentionï¼‰ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€ã“ã®æŠ€è¡“ã¯CTCãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒï¼ˆCTC prefix beam searchï¼‰ã¨ãƒ›ãƒƒãƒˆãƒ¯ãƒ¼ãƒ‰å¼·åŒ–æ©Ÿèƒ½ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n- [OmniSenseVoice](https://github.com/lifeiteng/OmniSenseVoice) ã¯ã€è¶…é«˜é€Ÿæ¨è«–ã¨ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n# ãŠå•ã„åˆã‚ã›\n\nä½¿ç”¨ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€githubãƒšãƒ¼ã‚¸ã§ç›´æ¥Issuesã‚’æèµ·ã§ãã¾ã™ã€‚éŸ³å£°ã«èˆˆå‘³ã®ã‚ã‚‹æ–¹ã¯ã€ä»¥ä¸‹ã®DingTalkã‚°ãƒ«ãƒ¼ãƒ—QRã‚³ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ã«å‚åŠ ã—ã€äº¤æµã¨è­°è«–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n\n|                          FunASR                          |\n|:--------------------------------------------------------:|\n| <img src=\"image/dingding_funasr.png\" width=\"250\"/></div> |\n\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 16.888671875,
          "content": "# SenseVoice\n\nã€Œç®€ä½“ä¸­æ–‡ã€|ã€Œ[English](./README.md)ã€|ã€Œ[æ—¥æœ¬èª](./README_ja.md)ã€\n\nSenseVoice æ˜¯å…·æœ‰éŸ³é¢‘ç†è§£èƒ½åŠ›çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯­ç§è¯†åˆ«ï¼ˆLIDï¼‰ã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å’Œå£°å­¦äº‹ä»¶åˆ†ç±»ï¼ˆAECï¼‰æˆ–å£°å­¦äº‹ä»¶æ£€æµ‹ï¼ˆAEDï¼‰ã€‚æœ¬é¡¹ç›®æä¾› SenseVoice æ¨¡å‹çš„ä»‹ç»ä»¥åŠåœ¨å¤šä¸ªä»»åŠ¡æµ‹è¯•é›†ä¸Šçš„ benchmarkï¼Œä»¥åŠä½“éªŒæ¨¡å‹æ‰€éœ€çš„ç¯å¢ƒå®‰è£…çš„ä¸æ¨ç†æ–¹å¼ã€‚\n\n<div align=\"center\">  \n<img src=\"image/sensevoice2.png\">\n</div>\n\n<div align=\"center\">  \n<h4>\n<a href=\"https://funaudiollm.github.io/\"> Homepage </a>\nï½œ<a href=\"#æœ€æ–°åŠ¨æ€\"> æœ€æ–°åŠ¨æ€ </a>\nï½œ<a href=\"#æ€§èƒ½è¯„æµ‹\"> æ€§èƒ½è¯„æµ‹ </a>\nï½œ<a href=\"#ç¯å¢ƒå®‰è£…\"> ç¯å¢ƒå®‰è£… </a>\nï½œ<a href=\"#ç”¨æ³•æ•™ç¨‹\"> ç”¨æ³•æ•™ç¨‹ </a>\nï½œ<a href=\"#è”ç³»æˆ‘ä»¬\"> è”ç³»æˆ‘ä»¬ </a>\n\n</h4>\n\næ¨¡å‹ä»“åº“ï¼š[modelscope](https://www.modelscope.cn/models/iic/SenseVoiceSmall)ï¼Œ[huggingface](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)\n\nåœ¨çº¿ä½“éªŒï¼š\n[modelscope demo](https://www.modelscope.cn/studios/iic/SenseVoice), [huggingface space](https://huggingface.co/spaces/FunAudioLLM/SenseVoice)\n\n</div>\n\n<a name=\"æ ¸å¿ƒåŠŸèƒ½\"></a>\n\n# æ ¸å¿ƒåŠŸèƒ½ ğŸ¯\n\n**SenseVoice** ä¸“æ³¨äºé«˜ç²¾åº¦å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿè¾¨è¯†å’ŒéŸ³é¢‘äº‹ä»¶æ£€æµ‹\n\n- **å¤šè¯­è¨€è¯†åˆ«ï¼š** é‡‡ç”¨è¶…è¿‡ 40 ä¸‡å°æ—¶æ•°æ®è®­ç»ƒï¼Œæ”¯æŒè¶…è¿‡ 50 ç§è¯­è¨€ï¼Œè¯†åˆ«æ•ˆæœä¸Šä¼˜äº Whisper æ¨¡å‹ã€‚\n- **å¯Œæ–‡æœ¬è¯†åˆ«ï¼š**\n  - å…·å¤‡ä¼˜ç§€çš„æƒ…æ„Ÿè¯†åˆ«ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•æ•°æ®ä¸Šè¾¾åˆ°å’Œè¶…è¿‡ç›®å‰æœ€ä½³æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„æ•ˆæœã€‚\n  - æ”¯æŒå£°éŸ³äº‹ä»¶æ£€æµ‹èƒ½åŠ›ï¼Œæ”¯æŒéŸ³ä¹ã€æŒå£°ã€ç¬‘å£°ã€å“­å£°ã€å’³å—½ã€å–·åšç­‰å¤šç§å¸¸è§äººæœºäº¤äº’äº‹ä»¶è¿›è¡Œæ£€æµ‹ã€‚\n- **é«˜æ•ˆæ¨ç†ï¼š** SenseVoice-Small æ¨¡å‹é‡‡ç”¨éè‡ªå›å½’ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ¨ç†å»¶è¿Ÿæä½ï¼Œ10s éŸ³é¢‘æ¨ç†ä»…è€—æ—¶ 70msï¼Œ15 å€ä¼˜äº Whisper-Largeã€‚\n- **å¾®è°ƒå®šåˆ¶ï¼š** å…·å¤‡ä¾¿æ·çš„å¾®è°ƒè„šæœ¬ä¸ç­–ç•¥ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®ä¸šåŠ¡åœºæ™¯ä¿®å¤é•¿å°¾æ ·æœ¬é—®é¢˜ã€‚\n- **æœåŠ¡éƒ¨ç½²ï¼š** å…·æœ‰å®Œæ•´çš„æœåŠ¡éƒ¨ç½²é“¾è·¯ï¼Œæ”¯æŒå¤šå¹¶å‘è¯·æ±‚ï¼Œæ”¯æŒå®¢æˆ·ç«¯è¯­è¨€æœ‰ï¼Œpythonã€c++ã€htmlã€java ä¸ c# ç­‰ã€‚\n\n<a name=\"æœ€æ–°åŠ¨æ€\"></a>\n\n# æœ€æ–°åŠ¨æ€ ğŸ”¥\n\n- 2024/7ï¼šæ–°å¢åŠ å¯¼å‡º [ONNX](./demo_onnx.py) ä¸ [libtorch](./demo_libtorch.py) åŠŸèƒ½ï¼Œä»¥åŠ python ç‰ˆæœ¬ runtimeï¼š[funasr-onnx-0.4.0](https://pypi.org/project/funasr-onnx/)ï¼Œ[funasr-torch-0.1.1](https://pypi.org/project/funasr-torch/)\n- 2024/7: [SenseVoice-Small](https://www.modelscope.cn/models/iic/SenseVoiceSmall) å¤šè¯­è¨€éŸ³é¢‘ç†è§£æ¨¡å‹å¼€æºï¼Œæ”¯æŒä¸­ã€ç²¤ã€è‹±ã€æ—¥ã€éŸ©è¯­çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ï¼Œæƒ…æ„Ÿè¯†åˆ«å’Œäº‹ä»¶æ£€æµ‹èƒ½åŠ›ï¼Œå…·æœ‰æä½çš„æ¨ç†å»¶è¿Ÿã€‚ã€‚\n- 2024/7: CosyVoice è‡´åŠ›äºè‡ªç„¶è¯­éŸ³ç”Ÿæˆï¼Œæ”¯æŒå¤šè¯­è¨€ã€éŸ³è‰²å’Œæƒ…æ„Ÿæ§åˆ¶ï¼Œæ“…é•¿å¤šè¯­è¨€è¯­éŸ³ç”Ÿæˆã€é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆã€è·¨è¯­è¨€è¯­éŸ³å…‹éš†ä»¥åŠéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚[CosyVoice repo](https://github.com/FunAudioLLM/CosyVoice) and [CosyVoice åœ¨çº¿ä½“éªŒ](https://www.modelscope.cn/studios/iic/CosyVoice-300M).\n- 2024/7: [FunASR](https://github.com/modelscope/FunASR) æ˜¯ä¸€ä¸ªåŸºç¡€è¯­éŸ³è¯†åˆ«å·¥å…·åŒ…ï¼Œæä¾›å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆVADï¼‰ã€æ ‡ç‚¹æ¢å¤ã€è¯­è¨€æ¨¡å‹ã€è¯´è¯äººéªŒè¯ã€è¯´è¯äººåˆ†ç¦»å’Œå¤šäººå¯¹è¯è¯­éŸ³è¯†åˆ«ç­‰ã€‚\n\n<a name=\"Benchmarks\"></a>\n\n# æ€§èƒ½è¯„æµ‹ ğŸ“\n\n## å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«\n\næˆ‘ä»¬åœ¨å¼€æºåŸºå‡†æ•°æ®é›†ï¼ˆåŒ…æ‹¬ AISHELL-1ã€AISHELL-2ã€Wenetspeechã€Librispeech å’Œ Common Voiceï¼‰ä¸Šæ¯”è¾ƒäº† SenseVoice ä¸ Whisper çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æ€§èƒ½å’Œæ¨ç†æ•ˆç‡ã€‚åœ¨ä¸­æ–‡å’Œç²¤è¯­è¯†åˆ«æ•ˆæœä¸Šï¼ŒSenseVoice-Small æ¨¡å‹å…·æœ‰æ˜æ˜¾çš„æ•ˆæœä¼˜åŠ¿ã€‚\n\n<div align=\"center\">  \n<img src=\"image/asr_results1.png\" width=\"400\" /><img src=\"image/asr_results2.png\" width=\"400\" />\n</div>\n\n## æƒ…æ„Ÿè¯†åˆ«\n\nç”±äºç›®å‰ç¼ºä¹è¢«å¹¿æ³›ä½¿ç”¨çš„æƒ…æ„Ÿè¯†åˆ«æµ‹è¯•æŒ‡æ ‡å’Œæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæµ‹è¯•é›†çš„å¤šç§æŒ‡æ ‡è¿›è¡Œæµ‹è¯•ï¼Œå¹¶ä¸è¿‘å¹´æ¥ Benchmark ä¸Šçš„å¤šä¸ªç»“æœè¿›è¡Œäº†å…¨é¢çš„å¯¹æ¯”ã€‚æ‰€é€‰å–çš„æµ‹è¯•é›†åŒæ—¶åŒ…å«ä¸­æ–‡ / è‹±æ–‡ä¸¤ç§è¯­è¨€ä»¥åŠè¡¨æ¼”ã€å½±è§†å‰§ã€è‡ªç„¶å¯¹è¯ç­‰å¤šç§é£æ ¼çš„æ•°æ®ï¼Œåœ¨ä¸è¿›è¡Œç›®æ ‡æ•°æ®å¾®è°ƒçš„å‰æä¸‹ï¼ŒSenseVoice èƒ½å¤Ÿåœ¨æµ‹è¯•æ•°æ®ä¸Šè¾¾åˆ°å’Œè¶…è¿‡ç›®å‰æœ€ä½³æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„æ•ˆæœã€‚\n\n<div align=\"center\">  \n<img src=\"image/ser_table.png\" width=\"1000\" />\n</div>\n\nåŒæ—¶ï¼Œæˆ‘ä»¬è¿˜åœ¨æµ‹è¯•é›†ä¸Šå¯¹å¤šä¸ªå¼€æºæƒ…æ„Ÿè¯†åˆ«æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œç»“æœè¡¨æ˜ï¼ŒSenseVoice-Large æ¨¡å‹å¯ä»¥åœ¨å‡ ä¹æ‰€æœ‰æ•°æ®ä¸Šéƒ½è¾¾åˆ°äº†æœ€ä½³æ•ˆæœï¼Œè€Œ SenseVoice-Small æ¨¡å‹åŒæ ·å¯ä»¥åœ¨å¤šæ•°æ•°æ®é›†ä¸Šå–å¾—è¶…è¶Šå…¶ä»–å¼€æºæ¨¡å‹çš„æ•ˆæœã€‚\n\n<div align=\"center\">  \n<img src=\"image/ser_figure.png\" width=\"500\" />\n</div>\n\n## äº‹ä»¶æ£€æµ‹\n\nå°½ç®¡ SenseVoice åªåœ¨è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ƒä»ç„¶å¯ä»¥ä½œä¸ºäº‹ä»¶æ£€æµ‹æ¨¡å‹è¿›è¡Œå•ç‹¬ä½¿ç”¨ã€‚æˆ‘ä»¬åœ¨ç¯å¢ƒéŸ³åˆ†ç±» ESC-50 æ•°æ®é›†ä¸Šä¸ç›®å‰ä¸šå†…å¹¿æ³›ä½¿ç”¨çš„ BEATS ä¸ PANN æ¨¡å‹çš„æ•ˆæœè¿›è¡Œäº†å¯¹æ¯”ã€‚SenseVoice æ¨¡å‹èƒ½å¤Ÿåœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—è¾ƒå¥½çš„æ•ˆæœï¼Œä½†å—é™äºè®­ç»ƒæ•°æ®ä¸è®­ç»ƒæ–¹å¼ï¼Œå…¶äº‹ä»¶åˆ†ç±»æ•ˆæœä¸“ä¸šçš„äº‹ä»¶æ£€æµ‹æ¨¡å‹ç›¸æ¯”ä»ç„¶æœ‰ä¸€å®šçš„å·®è·ã€‚\n\n<div align=\"center\">  \n<img src=\"image/aed_figure.png\" width=\"500\" />\n</div>\n\n## æ¨ç†æ•ˆç‡\n\nSenseVoice-small æ¨¡å‹é‡‡ç”¨éè‡ªå›å½’ç«¯åˆ°ç«¯æ¶æ„ï¼Œæ¨ç†å»¶è¿Ÿæä½ã€‚åœ¨å‚æ•°é‡ä¸ Whisper-Small æ¨¡å‹ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œæ¯” Whisper-Small æ¨¡å‹æ¨ç†é€Ÿåº¦å¿« 5 å€ï¼Œæ¯” Whisper-Large æ¨¡å‹å¿« 15 å€ã€‚åŒæ—¶ SenseVoice-small æ¨¡å‹åœ¨éŸ³é¢‘æ—¶é•¿å¢åŠ çš„æƒ…å†µä¸‹ï¼Œæ¨ç†è€—æ—¶ä¹Ÿæ— æ˜æ˜¾å¢åŠ ã€‚\n\n<div align=\"center\">  \n<img src=\"image/inference.png\" width=\"1000\" />\n</div>\n\n<a name=\"ç¯å¢ƒå®‰è£…\"></a>\n\n# å®‰è£…ä¾èµ–ç¯å¢ƒ ğŸ\n\n```shell\npip install -r requirements.txt\n```\n\n<a name=\"ç”¨æ³•æ•™ç¨‹\"></a>\n\n# ç”¨æ³• ğŸ› ï¸\n\n## æ¨ç†\n\n### ä½¿ç”¨ funasr æ¨ç†\n\næ”¯æŒä»»æ„æ ¼å¼éŸ³é¢‘è¾“å…¥ï¼Œæ”¯æŒä»»æ„æ—¶é•¿è¾“å…¥\n\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",  \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n<details><summary> å‚æ•°è¯´æ˜ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n- `model_dir`ï¼šæ¨¡å‹åç§°ï¼Œæˆ–æœ¬åœ°ç£ç›˜ä¸­çš„æ¨¡å‹è·¯å¾„ã€‚\n- `trust_remote_code`ï¼š\n  - `True` è¡¨ç¤º model ä»£ç å®ç°ä» `remote_code` å¤„åŠ è½½ï¼Œ`remote_code` æŒ‡å®š `model` å…·ä½“ä»£ç çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œå½“å‰ç›®å½•ä¸‹çš„ `model.py`ï¼‰ï¼Œæ”¯æŒç»å¯¹è·¯å¾„ä¸ç›¸å¯¹è·¯å¾„ï¼Œä»¥åŠç½‘ç»œ urlã€‚\n  - `False` è¡¨ç¤ºï¼Œmodel ä»£ç å®ç°ä¸º [FunASR](https://github.com/modelscope/FunASR) å†…éƒ¨é›†æˆç‰ˆæœ¬ï¼Œæ­¤æ—¶ä¿®æ”¹å½“å‰ç›®å½•ä¸‹çš„ `model.py` ä¸ä¼šç”Ÿæ•ˆï¼Œå› ä¸ºåŠ è½½çš„æ˜¯ funasr å†…éƒ¨ç‰ˆæœ¬ï¼Œæ¨¡å‹ä»£ç  [ç‚¹å‡»æŸ¥çœ‹](https://github.com/modelscope/FunASR/tree/main/funasr/models/sense_voice)ã€‚\n- `vad_model`ï¼šè¡¨ç¤ºå¼€å¯ VADï¼ŒVAD çš„ä½œç”¨æ˜¯å°†é•¿éŸ³é¢‘åˆ‡å‰²æˆçŸ­éŸ³é¢‘ï¼Œæ­¤æ—¶æ¨ç†è€—æ—¶åŒ…æ‹¬äº† VAD ä¸ SenseVoice æ€»è€—æ—¶ï¼Œä¸ºé“¾è·¯è€—æ—¶ï¼Œå¦‚æœéœ€è¦å•ç‹¬æµ‹è¯• SenseVoice æ¨¡å‹è€—æ—¶ï¼Œå¯ä»¥å…³é—­ VAD æ¨¡å‹ã€‚\n- `vad_kwargs`ï¼šè¡¨ç¤º VAD æ¨¡å‹é…ç½®ï¼Œ`max_single_segment_time`: è¡¨ç¤º `vad_model` æœ€å¤§åˆ‡å‰²éŸ³é¢‘æ—¶é•¿ï¼Œå•ä½æ˜¯æ¯«ç§’ msã€‚\n- `use_itn`ï¼šè¾“å‡ºç»“æœä¸­æ˜¯å¦åŒ…å«æ ‡ç‚¹ä¸é€†æ–‡æœ¬æ­£åˆ™åŒ–ã€‚\n- `batch_size_s` è¡¨ç¤ºé‡‡ç”¨åŠ¨æ€ batchï¼Œbatch ä¸­æ€»éŸ³é¢‘æ—¶é•¿ï¼Œå•ä½ä¸ºç§’ sã€‚\n- `merge_vad`ï¼šæ˜¯å¦å°† vad æ¨¡å‹åˆ‡å‰²çš„çŸ­éŸ³é¢‘ç¢ç‰‡åˆæˆï¼Œåˆå¹¶åé•¿åº¦ä¸º `merge_length_s`ï¼Œå•ä½ä¸ºç§’ sã€‚\n- `ban_emo_unk`ï¼šç¦ç”¨ emo_unk æ ‡ç­¾ï¼Œç¦ç”¨åæ‰€æœ‰çš„å¥å­éƒ½ä¼šè¢«èµ‹ä¸æƒ…æ„Ÿæ ‡ç­¾ã€‚é»˜è®¤ `False`\n\n</details>\n\nå¦‚æœè¾“å…¥å‡ä¸ºçŸ­éŸ³é¢‘ï¼ˆå°äº 30sï¼‰ï¼Œå¹¶ä¸”éœ€è¦æ‰¹é‡åŒ–æ¨ç†ï¼Œä¸ºäº†åŠ å¿«æ¨ç†æ•ˆç‡ï¼Œå¯ä»¥ç§»é™¤ vad æ¨¡å‹ï¼Œå¹¶è®¾ç½® `batch_size`\n\n```python\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size=64, \n)\n```\n\næ›´å¤šè¯¦ç»†ç”¨æ³•ï¼Œè¯·å‚è€ƒ [æ–‡æ¡£](https://github.com/modelscope/FunASR/blob/main/docs/tutorial/README.md)\n\n### ç›´æ¥æ¨ç†\n\næ”¯æŒä»»æ„æ ¼å¼éŸ³é¢‘è¾“å…¥ï¼Œè¾“å…¥éŸ³é¢‘æ—¶é•¿é™åˆ¶åœ¨ 30s ä»¥ä¸‹\n\n```python\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs ['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res [0][0][\"text\"])\nprint(text)\n```\n\n## æœåŠ¡éƒ¨ç½²\n\nUndo\n\n### å¯¼å‡ºä¸æµ‹è¯•\n\n<details><summary>ONNX ä¸ Libtorch å¯¼å‡º </summary>\n\n#### ONNX\n\n```python\n# pip3 install -U funasr funasr-onnx\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\n\nå¤‡æ³¨ï¼šONNX æ¨¡å‹å¯¼å‡ºåˆ°åŸæ¨¡å‹ç›®å½•ä¸­\n\n#### Libtorch\n\n```python\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess (i) for i in res])\n```\n\nå¤‡æ³¨ï¼šLibtorch æ¨¡å‹å¯¼å‡ºåˆ°åŸæ¨¡å‹ç›®å½•ä¸­\n\n</details>\n\n### éƒ¨ç½²\n\n### ä½¿ç”¨ FastAPI éƒ¨ç½²\n\n```shell\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n## å¾®è°ƒ\n\n### å®‰è£…è®­ç»ƒç¯å¢ƒ\n\n```shell\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n### æ•°æ®å‡†å¤‡\n\næ•°æ®æ ¼å¼éœ€è¦åŒ…æ‹¬å¦‚ä¸‹å‡ ä¸ªå­—æ®µï¼š\n\n```text\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\n\nè¯¦ç»†å¯ä»¥å‚è€ƒï¼š`data/train_example.jsonl`\n\n<details><summary > æ•°æ®å‡†å¤‡ç»†èŠ‚ä»‹ç» </summary>\n\n- `key`: æ•°æ®å”¯ä¸€ ID\n- `source`ï¼šéŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„\n- `source_len`ï¼šéŸ³é¢‘æ–‡ä»¶çš„ fbank å¸§æ•°\n- `target`ï¼šéŸ³é¢‘æ–‡ä»¶æ ‡æ³¨æ–‡æœ¬\n- `target_len`ï¼šéŸ³é¢‘æ–‡ä»¶æ ‡æ³¨æ–‡æœ¬é•¿åº¦\n- `text_language`ï¼šéŸ³é¢‘æ–‡ä»¶çš„è¯­ç§æ ‡ç­¾\n- `emo_target`ï¼šéŸ³é¢‘æ–‡ä»¶çš„æƒ…æ„Ÿæ ‡ç­¾\n- `event_target`ï¼šéŸ³é¢‘æ–‡ä»¶çš„äº‹ä»¶æ ‡ç­¾\n- `with_or_wo_itn`ï¼šæ ‡æ³¨æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æ ‡ç‚¹ä¸é€†æ–‡æœ¬æ­£åˆ™åŒ–\n\nå¯ä»¥ç”¨æŒ‡ä»¤ `sensevoice2jsonl` ä» train_wav.scpã€train_text.txtã€train_text_language.txtã€train_emo_target.txt å’Œ train_event_target.txt ç”Ÿæˆï¼Œå‡†å¤‡è¿‡ç¨‹å¦‚ä¸‹ï¼š\n\n`train_text.txt`\n\nå·¦è¾¹ä¸ºæ•°æ®å”¯ä¸€ IDï¼Œéœ€ä¸ `train_wav.scp` ä¸­çš„ `ID` ä¸€ä¸€å¯¹åº”\nå³è¾¹ä¸ºéŸ³é¢‘æ–‡ä»¶æ ‡æ³¨æ–‡æœ¬ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n\n```bash\nBAC009S0764W0121 ç”šè‡³å‡ºç°äº¤æ˜“å‡ ä¹åœæ»çš„æƒ…å†µ\nBAC009S0916W0489 æ¹–åŒ—ä¸€å…¬å¸ä»¥å‘˜å·¥åä¹‰è´·æ¬¾æ•°åå‘˜å·¥è´Ÿå€ºåƒä¸‡\nasr_example_cn_en æ‰€æœ‰åªè¦å¤„ç† data ä¸ç®¡ä½ æ˜¯åš machine learning åš deep learning åš data analytics åš data science ä¹Ÿå¥½ scientist ä¹Ÿå¥½é€šé€šéƒ½è¦éƒ½åšçš„åŸºæœ¬åŠŸå•Šé‚£ again å…ˆå…ˆå¯¹æœ‰ä¸€äº› > ä¹Ÿè®¸å¯¹\nID0012W0014 he tried to think how it could be\n```\n\n`train_wav.scp`\n\nå·¦è¾¹ä¸ºæ•°æ®å”¯ä¸€ IDï¼Œéœ€ä¸ `train_text.txt` ä¸­çš„ `ID` ä¸€ä¸€å¯¹åº”\nå³è¾¹ä¸ºéŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ï¼Œæ ¼å¼å¦‚ä¸‹\n\n```bash\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n\n`train_text_language.txt`\n\nå·¦è¾¹ä¸ºæ•°æ®å”¯ä¸€ IDï¼Œéœ€ä¸ `train_text_language.txt` ä¸­çš„ `ID` ä¸€ä¸€å¯¹åº”\nå³è¾¹ä¸ºéŸ³é¢‘æ–‡ä»¶çš„è¯­ç§æ ‡ç­¾ï¼Œæ”¯æŒ `<|zh|>`ã€`<|en|>`ã€`<|yue|>`ã€`<|ja|>` å’Œ `<|ko|>`ï¼Œæ ¼å¼å¦‚ä¸‹\n\n```bash\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n\n`train_emo.txt`\n\nå·¦è¾¹ä¸ºæ•°æ®å”¯ä¸€ IDï¼Œéœ€ä¸ `train_emo.txt` ä¸­çš„ `ID` ä¸€ä¸€å¯¹åº”\nå³è¾¹ä¸ºéŸ³é¢‘æ–‡ä»¶çš„æƒ…æ„Ÿæ ‡ç­¾ï¼Œæ”¯æŒ `<|HAPPY|>`ã€`<|SAD|>`ã€`<|ANGRY|>`ã€`<|NEUTRAL|>`ã€`<|FEARFUL|>`ã€`<|DISGUSTED|>` å’Œ `<|SURPRISED|>`ï¼Œæ ¼å¼å¦‚ä¸‹\n\n```bash\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n\n`train_event.txt`\n\nå·¦è¾¹ä¸ºæ•°æ®å”¯ä¸€ IDï¼Œéœ€ä¸ `train_event.txt` ä¸­çš„ `ID` ä¸€ä¸€å¯¹åº”\nå³è¾¹ä¸ºéŸ³é¢‘æ–‡ä»¶çš„äº‹ä»¶æ ‡ç­¾ï¼Œæ”¯æŒ `<|BGM|>`ã€`<|Speech|>`ã€`<|Applause|>`ã€`<|Laughter|>`ã€`<|Cry|>`ã€`<|Sneeze|>`ã€`<|Breath|>` å’Œ `<|Cough|>`ï¼Œæ ¼å¼å¦‚ä¸‹\n\n```bash\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```\n\n`ç”ŸæˆæŒ‡ä»¤`\n\n```shell\n# generate train.jsonl and val.jsonl from wav.scp, text.txt, text_language.txt, emo_target.txt, event_target.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\nè‹¥æ—  train_text_language.txtã€train_emo_target.txt å’Œ train_event_target.txtï¼Œåˆ™è‡ªåŠ¨é€šè¿‡ä½¿ç”¨ `SenseVoice` æ¨¡å‹å¯¹è¯­ç§ã€æƒ…æ„Ÿå’Œäº‹ä»¶æ‰“æ ‡ã€‚\n\n```shell\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\" \\\n++model_dir='iic/SenseVoiceSmall'\n```\n\n</details>\n\n### å¯åŠ¨è®­ç»ƒ\n\næ³¨æ„ä¿®æ”¹ `finetune.sh` ä¸­ `train_tool` ä¸ºä½ å‰é¢å®‰è£… FunASR è·¯å¾„ä¸­ `funasr/bin/train_ds.py` ç»å¯¹è·¯å¾„\n\n```shell\nbash finetune.sh\n```\n\n## WebUI\n\n```shell\npython webui.py\n```\n\n<div align=\"center\"><img src=\"image/webui.png\" width=\"700\"/> </div>\n\n## ä¼˜ç§€ä¸‰æ–¹å·¥ä½œ\n\n- Tritonï¼ˆGPUï¼‰éƒ¨ç½²æœ€ä½³å®è·µï¼Œtriton + tensorrtï¼Œfp32 æµ‹è¯•ï¼ŒV100 GPU ä¸ŠåŠ é€Ÿæ¯” 526ï¼Œfp16 æ”¯æŒä¸­ï¼Œ[repo](https://github.com/modelscope/FunASR/blob/main/runtime/triton_gpu/README.md)\n- sherpa-onnx éƒ¨ç½²æœ€ä½³å®è·µï¼Œæ”¯æŒåœ¨ 10 ç§ç¼–ç¨‹è¯­è¨€é‡Œé¢ä½¿ç”¨ SenseVoice, å³ C++, C, Python, C#, Go, Swift, Kotlin, Java, JavaScript, Dart. æ”¯æŒåœ¨ iOS, Android, Raspberry Pi ç­‰å¹³å°ä½¿ç”¨ SenseVoiceï¼Œ[repo](https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html)\n- [SenseVoice.cpp](https://github.com/lovemefan/SenseVoice.cpp) åŸºäºGGMLï¼Œåœ¨çº¯C/C++ä¸­æ¨æ–­SenseVoiceï¼Œæ”¯æŒ3ä½ã€4ä½ã€5ä½ã€8ä½é‡åŒ–ç­‰ï¼Œæ— éœ€ç¬¬ä¸‰æ–¹ä¾èµ–ã€‚\n- [æµå¼SenseVoice](https://github.com/pengzhendong/streaming-sensevoice)ï¼Œé€šè¿‡åˆ†å—ï¼ˆchunkï¼‰çš„æ–¹å¼è¿›è¡Œæ¨ç†ï¼Œä¸ºäº†å®ç°ä¼ªæµå¼å¤„ç†ï¼Œé‡‡ç”¨äº†æˆªæ–­æ³¨æ„åŠ›æœºåˆ¶ï¼ˆtruncated attentionï¼‰ï¼Œç‰ºç‰²äº†éƒ¨åˆ†ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜æ”¯æŒCTCå‰ç¼€æŸæœç´¢ï¼ˆCTC prefix beam searchï¼‰ä»¥åŠçƒ­è¯å¢å¼ºåŠŸèƒ½ã€‚\n- [OmniSenseVoice](https://github.com/lifeiteng/OmniSenseVoice) è½»é‡åŒ–æ¨ç†åº“ï¼Œæ”¯æŒbatchæ¨ç†ã€‚\n\n# è”ç³»æˆ‘ä»¬\n\nå¦‚æœæ‚¨åœ¨ä½¿ç”¨ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥åœ¨ github é¡µé¢æ Issuesã€‚æ¬¢è¿è¯­éŸ³å…´è¶£çˆ±å¥½è€…æ‰«æä»¥ä¸‹çš„é’‰é’‰ç¾¤äºŒç»´ç åŠ å…¥ç¤¾åŒºç¾¤ï¼Œè¿›è¡Œäº¤æµå’Œè®¨è®ºã€‚\n\n|                          FunASR                          |\n|:--------------------------------------------------------:|\n| <img src=\"image/dingding_funasr.png\" width=\"250\"/></div> |\n"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 2.240234375,
          "content": "# Set the device with environment, default is cuda:0\n# export SENSEVOICE_DEVICE=cuda:1\n\nimport os, re\nfrom fastapi import FastAPI, File, Form\nfrom fastapi.responses import HTMLResponse\nfrom typing_extensions import Annotated\nfrom typing import List\nfrom enum import Enum\nimport torchaudio\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\nfrom io import BytesIO\n\n\nclass Language(str, Enum):\n    auto = \"auto\"\n    zh = \"zh\"\n    en = \"en\"\n    yue = \"yue\"\n    ja = \"ja\"\n    ko = \"ko\"\n    nospeech = \"nospeech\"\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=os.getenv(\"SENSEVOICE_DEVICE\", \"cuda:0\"))\nm.eval()\n\nregex = r\"<\\|.*\\|>\"\n\napp = FastAPI()\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def root():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n        <head>\n            <meta charset=utf-8>\n            <title>Api information</title>\n        </head>\n        <body>\n            <a href='./docs'>Documents of API</a>\n        </body>\n    </html>\n    \"\"\"\n\n@app.post(\"/api/v1/asr\")\nasync def turn_audio_to_text(files: Annotated[List[bytes], File(description=\"wav or mp3 audios in 16KHz\")], keys: Annotated[str, Form(description=\"name of each audio joined with comma\")], lang: Annotated[Language, Form(description=\"language of audio content\")] = \"auto\"):\n    audios = []\n    audio_fs = 0\n    for file in files:\n        file_io = BytesIO(file)\n        data_or_path_or_list, audio_fs = torchaudio.load(file_io)\n        data_or_path_or_list = data_or_path_or_list.mean(0)\n        audios.append(data_or_path_or_list)\n        file_io.close()\n    if lang == \"\":\n        lang = \"auto\"\n    if keys == \"\":\n        key = [\"wav_file_tmp_name\"]\n    else:\n        key = keys.split(\",\")\n    res = m.inference(\n        data_in=audios,\n        language=lang, # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n        ban_emo_unk=False,\n        key=key,\n        fs=audio_fs,\n        **kwargs,\n    )\n    if len(res) == 0:\n        return {\"result\": []}\n    for it in res[0]:\n        it[\"raw_text\"] = it[\"text\"]\n        it[\"clean_text\"] = re.sub(regex, \"\", it[\"text\"], 0, re.MULTILINE)\n        it[\"text\"] = rich_transcription_postprocess(it[\"text\"])\n    return {\"result\": res[0]}\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "deepspeed_conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo1.py",
          "type": "blob",
          "size": 2.0263671875,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n# zh\nres = model.generate(\n    input=f\"{model.model_path}/example/zh.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n# yue\nres = model.generate(\n    input=f\"{model.model_path}/example/yue.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n# ja\nres = model.generate(\n    input=f\"{model.model_path}/example/ja.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n\n# ko\nres = model.generate(\n    input=f\"{model.model_path}/example/ko.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n"
        },
        {
          "name": "demo2.py",
          "type": "blob",
          "size": 1.0087890625,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    output_timestamp=True,\n    **kwargs,\n)\n\ntimestamp = res[0][0][\"timestamp\"]\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\nprint(timestamp)\n"
        },
        {
          "name": "demo_libtorch.py",
          "type": "blob",
          "size": 0.6279296875,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n"
        },
        {
          "name": "demo_onnx.py",
          "type": "blob",
          "size": 0.6416015625,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", textnorm=\"withitn\")\nprint([rich_transcription_postprocess(i) for i in res])\n"
        },
        {
          "name": "export.py",
          "type": "blob",
          "size": 1.576171875,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nimport os\nimport torch\nfrom model import SenseVoiceSmall\nfrom utils import export_utils\nfrom utils.model_bin import SenseVoiceSmallONNX\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nquantize = False\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nmodel, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\n\nrebuilt_model = model.export(type=\"onnx\", quantize=False)\nmodel_path = kwargs.get(\"output_dir\", os.path.dirname(kwargs.get(\"init_param\")))\n\nmodel_file = os.path.join(model_path, \"model.onnx\")\nif quantize:\n    model_file = os.path.join(model_path, \"model_quant.onnx\")\n\n# export model\nif not os.path.exists(model_file):\n    with torch.no_grad():\n        del kwargs['model']\n        export_dir = export_utils.export(model=rebuilt_model, **kwargs)\n        print(\"Export model onnx to {}\".format(model_file))\n        \n# export model init\nmodel_bin = SenseVoiceSmallONNX(model_path)\n\n# build tokenizer\ntry:\n    from funasr.tokenizer.sentencepiece_tokenizer import SentencepiecesTokenizer\n    tokenizer = SentencepiecesTokenizer(bpemodel=os.path.join(model_path, \"chn_jpn_yue_eng_ko_spectok.bpe.model\"))\nexcept:\n    tokenizer = None\n\n# inference\nwav_or_scp = \"/Users/shixian/Downloads/asr_example_hotword.wav\"\nlanguage_list = [0]\ntextnorm_list = [15]\nres = model_bin(wav_or_scp, language_list, textnorm_list, tokenizer=tokenizer)\nprint([rich_transcription_postprocess(i) for i in res])\n"
        },
        {
          "name": "export_meta.py",
          "type": "blob",
          "size": 2.724609375,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nimport types\nimport torch\nfrom funasr.utils.torch_function import sequence_mask\n\n\ndef export_rebuild_model(model, **kwargs):\n    model.device = kwargs.get(\"device\")\n    model.make_pad_mask = sequence_mask(kwargs[\"max_seq_len\"], flip=False)\n    model.forward = types.MethodType(export_forward, model)\n    model.export_dummy_inputs = types.MethodType(export_dummy_inputs, model)\n    model.export_input_names = types.MethodType(export_input_names, model)\n    model.export_output_names = types.MethodType(export_output_names, model)\n    model.export_dynamic_axes = types.MethodType(export_dynamic_axes, model)\n    model.export_name = types.MethodType(export_name, model)\n    return model\n\ndef export_forward(\n    self,\n    speech: torch.Tensor,\n    speech_lengths: torch.Tensor,\n    language: torch.Tensor,\n    textnorm: torch.Tensor,\n    **kwargs,\n):\n    # speech = speech.to(device=\"cuda\")\n    # speech_lengths = speech_lengths.to(device=\"cuda\")\n    language_query = self.embed(language.to(speech.device)).unsqueeze(1)\n    textnorm_query = self.embed(textnorm.to(speech.device)).unsqueeze(1)\n    print(textnorm_query.shape, speech.shape)\n    speech = torch.cat((textnorm_query, speech), dim=1)\n    speech_lengths += 1\n    \n    event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(\n        speech.size(0), 1, 1\n    )\n    input_query = torch.cat((language_query, event_emo_query), dim=1)\n    speech = torch.cat((input_query, speech), dim=1)\n    speech_lengths += 3\n    \n    encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n    if isinstance(encoder_out, tuple):\n        encoder_out = encoder_out[0]\n\n    ctc_logits = self.ctc.ctc_lo(encoder_out)\n    \n    return ctc_logits, encoder_out_lens\n\ndef export_dummy_inputs(self):\n    speech = torch.randn(2, 30, 560)\n    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n    language = torch.tensor([0, 0], dtype=torch.int32)\n    textnorm = torch.tensor([15, 15], dtype=torch.int32)\n    return (speech, speech_lengths, language, textnorm)\n\ndef export_input_names(self):\n    return [\"speech\", \"speech_lengths\", \"language\", \"textnorm\"]\n\ndef export_output_names(self):\n    return [\"ctc_logits\", \"encoder_out_lens\"]\n\ndef export_dynamic_axes(self):\n    return {\n        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n        \"speech_lengths\": {0: \"batch_size\"},\n        \"language\": {0: \"batch_size\"},\n        \"textnorm\": {0: \"batch_size\"},\n        \"ctc_logits\": {0: \"batch_size\", 1: \"logits_length\"},\n        \"encoder_out_lens\":  {0: \"batch_size\"},\n    }\n\ndef export_name(self):\n    return \"model.onnx\"\n\n"
        },
        {
          "name": "finetune.sh",
          "type": "blob",
          "size": 2.138671875,
          "content": "# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nworkspace=`pwd`\n\n# which gpu to train or finetune\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\n# model_name from model_hub, or model_dir in local path\n\n## option 1, download model automatically\nmodel_name_or_model_dir=\"iic/SenseVoiceSmall\"\n\n## option 2, download model by git\n#local_path_root=${workspace}/modelscope_models\n#mkdir -p ${local_path_root}/${model_name_or_model_dir}\n#git clone https://www.modelscope.cn/${model_name_or_model_dir}.git ${local_path_root}/${model_name_or_model_dir}\n#model_name_or_model_dir=${local_path_root}/${model_name_or_model_dir}\n\n\n# data dir, which contains: train.json, val.json\ntrain_data=${workspace}/data/train_example.jsonl\nval_data=${workspace}/data/val_example.jsonl\n\n# exp output dir\noutput_dir=\"./outputs\"\nlog_file=\"${output_dir}/log.txt\"\n\ndeepspeed_config=${workspace}/deepspeed_conf/ds_stage1.json\n\nmkdir -p ${output_dir}\necho \"log_file: ${log_file}\"\n\nDISTRIBUTED_ARGS=\"\n    --nnodes ${WORLD_SIZE:-1} \\\n    --nproc_per_node $gpu_num \\\n    --node_rank ${RANK:-0} \\\n    --master_addr ${MASTER_ADDR:-127.0.0.1} \\\n    --master_port ${MASTER_PORT:-26669}\n\"\n\necho $DISTRIBUTED_ARGS\n\n# funasr trainer path\ntrain_tool=`dirname $(which funasr)`/train_ds.py\n\ntorchrun $DISTRIBUTED_ARGS \\\n${train_tool} \\\n++model=\"${model_name_or_model_dir}\" \\\n++trust_remote_code=true \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.data_split_num=1 \\\n++dataset_conf.batch_sampler=\"BatchSampler\" \\\n++dataset_conf.batch_size=6000  \\\n++dataset_conf.sort_size=1024 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=true \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++train_conf.use_deepspeed=false \\\n++train_conf.deepspeed_config=${deepspeed_config} \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}"
        },
        {
          "name": "image",
          "type": "tree",
          "content": null
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 32.451171875,
          "content": "\nimport time\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom typing import Iterable, Optional\n\nfrom funasr.register import tables\nfrom funasr.models.ctc.ctc import CTC\nfrom funasr.utils.datadir_writer import DatadirWriter\nfrom funasr.models.paraformer.search import Hypothesis\nfrom funasr.train_utils.device_funcs import force_gatherable\nfrom funasr.losses.label_smoothing_loss import LabelSmoothingLoss\nfrom funasr.metrics.compute_acc import compute_accuracy, th_accuracy\nfrom funasr.utils.load_utils import load_audio_text_image_video, extract_fbank\nfrom utils.ctc_alignment import ctc_forced_align\n\nclass SinusoidalPositionEncoder(torch.nn.Module):\n    \"\"\" \"\"\"\n\n    def __int__(self, d_model=80, dropout_rate=0.1):\n        pass\n\n    def encode(\n        self, positions: torch.Tensor = None, depth: int = None, dtype: torch.dtype = torch.float32\n    ):\n        batch_size = positions.size(0)\n        positions = positions.type(dtype)\n        device = positions.device\n        log_timescale_increment = torch.log(torch.tensor([10000], dtype=dtype, device=device)) / (\n            depth / 2 - 1\n        )\n        inv_timescales = torch.exp(\n            torch.arange(depth / 2, device=device).type(dtype) * (-log_timescale_increment)\n        )\n        inv_timescales = torch.reshape(inv_timescales, [batch_size, -1])\n        scaled_time = torch.reshape(positions, [1, -1, 1]) * torch.reshape(\n            inv_timescales, [1, 1, -1]\n        )\n        encoding = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=2)\n        return encoding.type(dtype)\n\n    def forward(self, x):\n        batch_size, timesteps, input_dim = x.size()\n        positions = torch.arange(1, timesteps + 1, device=x.device)[None, :]\n        position_encoding = self.encode(positions, input_dim, x.dtype).to(x.device)\n\n        return x + position_encoding\n\n\nclass PositionwiseFeedForward(torch.nn.Module):\n    \"\"\"Positionwise feed forward layer.\n\n    Args:\n        idim (int): Input dimenstion.\n        hidden_units (int): The number of hidden units.\n        dropout_rate (float): Dropout rate.\n\n    \"\"\"\n\n    def __init__(self, idim, hidden_units, dropout_rate, activation=torch.nn.ReLU()):\n        \"\"\"Construct an PositionwiseFeedForward object.\"\"\"\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = torch.nn.Linear(idim, hidden_units)\n        self.w_2 = torch.nn.Linear(hidden_units, idim)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        self.activation = activation\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n\n\nclass MultiHeadedAttentionSANM(nn.Module):\n    \"\"\"Multi-Head Attention layer.\n\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        n_head,\n        in_feat,\n        n_feat,\n        dropout_rate,\n        kernel_size,\n        sanm_shfit=0,\n        lora_list=None,\n        lora_rank=8,\n        lora_alpha=16,\n        lora_dropout=0.1,\n    ):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super().__init__()\n        assert n_feat % n_head == 0\n        # We assume d_v always equals d_k\n        self.d_k = n_feat // n_head\n        self.h = n_head\n        # self.linear_q = nn.Linear(n_feat, n_feat)\n        # self.linear_k = nn.Linear(n_feat, n_feat)\n        # self.linear_v = nn.Linear(n_feat, n_feat)\n\n        self.linear_out = nn.Linear(n_feat, n_feat)\n        self.linear_q_k_v = nn.Linear(in_feat, n_feat * 3)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        self.fsmn_block = nn.Conv1d(\n            n_feat, n_feat, kernel_size, stride=1, padding=0, groups=n_feat, bias=False\n        )\n        # padding\n        left_padding = (kernel_size - 1) // 2\n        if sanm_shfit > 0:\n            left_padding = left_padding + sanm_shfit\n        right_padding = kernel_size - 1 - left_padding\n        self.pad_fn = nn.ConstantPad1d((left_padding, right_padding), 0.0)\n\n    def forward_fsmn(self, inputs, mask, mask_shfit_chunk=None):\n        b, t, d = inputs.size()\n        if mask is not None:\n            mask = torch.reshape(mask, (b, -1, 1))\n            if mask_shfit_chunk is not None:\n                mask = mask * mask_shfit_chunk\n            inputs = inputs * mask\n\n        x = inputs.transpose(1, 2)\n        x = self.pad_fn(x)\n        x = self.fsmn_block(x)\n        x = x.transpose(1, 2)\n        x += inputs\n        x = self.dropout(x)\n        if mask is not None:\n            x = x * mask\n        return x\n\n    def forward_qkv(self, x):\n        \"\"\"Transform query, key and value.\n\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n\n        Returns:\n            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).\n            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).\n            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\n\n        \"\"\"\n        b, t, d = x.size()\n        q_k_v = self.linear_q_k_v(x)\n        q, k, v = torch.split(q_k_v, int(self.h * self.d_k), dim=-1)\n        q_h = torch.reshape(q, (b, t, self.h, self.d_k)).transpose(\n            1, 2\n        )  # (batch, head, time1, d_k)\n        k_h = torch.reshape(k, (b, t, self.h, self.d_k)).transpose(\n            1, 2\n        )  # (batch, head, time2, d_k)\n        v_h = torch.reshape(v, (b, t, self.h, self.d_k)).transpose(\n            1, 2\n        )  # (batch, head, time2, d_k)\n\n        return q_h, k_h, v_h, v\n\n    def forward_attention(self, value, scores, mask, mask_att_chunk_encoder=None):\n        \"\"\"Compute attention context vector.\n\n        Args:\n            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).\n            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).\n            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).\n\n        Returns:\n            torch.Tensor: Transformed value (#batch, time1, d_model)\n                weighted by the attention score (#batch, time1, time2).\n\n        \"\"\"\n        n_batch = value.size(0)\n        if mask is not None:\n            if mask_att_chunk_encoder is not None:\n                mask = mask * mask_att_chunk_encoder\n\n            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)\n\n            min_value = -float(\n                \"inf\"\n            )  # float(numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min)\n            scores = scores.masked_fill(mask, min_value)\n            attn = torch.softmax(scores, dim=-1).masked_fill(\n                mask, 0.0\n            )  # (batch, head, time1, time2)\n        else:\n            attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\n        p_attn = self.dropout(attn)\n        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n        x = (\n            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)\n        )  # (batch, time1, d_model)\n\n        return self.linear_out(x)  # (batch, time1, d_model)\n\n    def forward(self, x, mask, mask_shfit_chunk=None, mask_att_chunk_encoder=None):\n        \"\"\"Compute scaled dot product attention.\n\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n\n        \"\"\"\n        q_h, k_h, v_h, v = self.forward_qkv(x)\n        fsmn_memory = self.forward_fsmn(v, mask, mask_shfit_chunk)\n        q_h = q_h * self.d_k ** (-0.5)\n        scores = torch.matmul(q_h, k_h.transpose(-2, -1))\n        att_outs = self.forward_attention(v_h, scores, mask, mask_att_chunk_encoder)\n        return att_outs + fsmn_memory\n\n    def forward_chunk(self, x, cache=None, chunk_size=None, look_back=0):\n        \"\"\"Compute scaled dot product attention.\n\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n\n        \"\"\"\n        q_h, k_h, v_h, v = self.forward_qkv(x)\n        if chunk_size is not None and look_back > 0 or look_back == -1:\n            if cache is not None:\n                k_h_stride = k_h[:, :, : -(chunk_size[2]), :]\n                v_h_stride = v_h[:, :, : -(chunk_size[2]), :]\n                k_h = torch.cat((cache[\"k\"], k_h), dim=2)\n                v_h = torch.cat((cache[\"v\"], v_h), dim=2)\n\n                cache[\"k\"] = torch.cat((cache[\"k\"], k_h_stride), dim=2)\n                cache[\"v\"] = torch.cat((cache[\"v\"], v_h_stride), dim=2)\n                if look_back != -1:\n                    cache[\"k\"] = cache[\"k\"][:, :, -(look_back * chunk_size[1]) :, :]\n                    cache[\"v\"] = cache[\"v\"][:, :, -(look_back * chunk_size[1]) :, :]\n            else:\n                cache_tmp = {\n                    \"k\": k_h[:, :, : -(chunk_size[2]), :],\n                    \"v\": v_h[:, :, : -(chunk_size[2]), :],\n                }\n                cache = cache_tmp\n        fsmn_memory = self.forward_fsmn(v, None)\n        q_h = q_h * self.d_k ** (-0.5)\n        scores = torch.matmul(q_h, k_h.transpose(-2, -1))\n        att_outs = self.forward_attention(v_h, scores, None)\n        return att_outs + fsmn_memory, cache\n\n\nclass LayerNorm(nn.LayerNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = F.layer_norm(\n            input.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(input)\n\n\ndef sequence_mask(lengths, maxlen=None, dtype=torch.float32, device=None):\n    if maxlen is None:\n        maxlen = lengths.max()\n    row_vector = torch.arange(0, maxlen, 1).to(lengths.device)\n    matrix = torch.unsqueeze(lengths, dim=-1)\n    mask = row_vector < matrix\n    mask = mask.detach()\n\n    return mask.type(dtype).to(device) if device is not None else mask.type(dtype)\n\n\nclass EncoderLayerSANM(nn.Module):\n    def __init__(\n        self,\n        in_size,\n        size,\n        self_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n        stochastic_depth_rate=0.0,\n    ):\n        \"\"\"Construct an EncoderLayer object.\"\"\"\n        super(EncoderLayerSANM, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.norm1 = LayerNorm(in_size)\n        self.norm2 = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.in_size = in_size\n        self.size = size\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear = nn.Linear(size + size, size)\n        self.stochastic_depth_rate = stochastic_depth_rate\n        self.dropout_rate = dropout_rate\n\n    def forward(self, x, mask, cache=None, mask_shfit_chunk=None, mask_att_chunk_encoder=None):\n        \"\"\"Compute encoded features.\n\n        Args:\n            x_input (torch.Tensor): Input tensor (#batch, time, size).\n            mask (torch.Tensor): Mask tensor for the input (#batch, time).\n            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time, size).\n            torch.Tensor: Mask tensor (#batch, time).\n\n        \"\"\"\n        skip_layer = False\n        # with stochastic depth, residual connection `x + f(x)` becomes\n        # `x <- x + 1 / (1 - p) * f(x)` at training time.\n        stoch_layer_coeff = 1.0\n        if self.training and self.stochastic_depth_rate > 0:\n            skip_layer = torch.rand(1).item() < self.stochastic_depth_rate\n            stoch_layer_coeff = 1.0 / (1 - self.stochastic_depth_rate)\n\n        if skip_layer:\n            if cache is not None:\n                x = torch.cat([cache, x], dim=1)\n            return x, mask\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm1(x)\n\n        if self.concat_after:\n            x_concat = torch.cat(\n                (\n                    x,\n                    self.self_attn(\n                        x,\n                        mask,\n                        mask_shfit_chunk=mask_shfit_chunk,\n                        mask_att_chunk_encoder=mask_att_chunk_encoder,\n                    ),\n                ),\n                dim=-1,\n            )\n            if self.in_size == self.size:\n                x = residual + stoch_layer_coeff * self.concat_linear(x_concat)\n            else:\n                x = stoch_layer_coeff * self.concat_linear(x_concat)\n        else:\n            if self.in_size == self.size:\n                x = residual + stoch_layer_coeff * self.dropout(\n                    self.self_attn(\n                        x,\n                        mask,\n                        mask_shfit_chunk=mask_shfit_chunk,\n                        mask_att_chunk_encoder=mask_att_chunk_encoder,\n                    )\n                )\n            else:\n                x = stoch_layer_coeff * self.dropout(\n                    self.self_attn(\n                        x,\n                        mask,\n                        mask_shfit_chunk=mask_shfit_chunk,\n                        mask_att_chunk_encoder=mask_att_chunk_encoder,\n                    )\n                )\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        x = residual + stoch_layer_coeff * self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm2(x)\n\n        return x, mask, cache, mask_shfit_chunk, mask_att_chunk_encoder\n\n    def forward_chunk(self, x, cache=None, chunk_size=None, look_back=0):\n        \"\"\"Compute encoded features.\n\n        Args:\n            x_input (torch.Tensor): Input tensor (#batch, time, size).\n            mask (torch.Tensor): Mask tensor for the input (#batch, time).\n            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time, size).\n            torch.Tensor: Mask tensor (#batch, time).\n\n        \"\"\"\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm1(x)\n\n        if self.in_size == self.size:\n            attn, cache = self.self_attn.forward_chunk(x, cache, chunk_size, look_back)\n            x = residual + attn\n        else:\n            x, cache = self.self_attn.forward_chunk(x, cache, chunk_size, look_back)\n\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        x = residual + self.feed_forward(x)\n        if not self.normalize_before:\n            x = self.norm2(x)\n\n        return x, cache\n\n\n@tables.register(\"encoder_classes\", \"SenseVoiceEncoderSmall\")\nclass SenseVoiceEncoderSmall(nn.Module):\n    \"\"\"\n    Author: Speech Lab of DAMO Academy, Alibaba Group\n    SCAMA: Streaming chunk-aware multihead attention for online end-to-end speech recognition\n    https://arxiv.org/abs/2006.01713\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int = 256,\n        attention_heads: int = 4,\n        linear_units: int = 2048,\n        num_blocks: int = 6,\n        tp_blocks: int = 0,\n        dropout_rate: float = 0.1,\n        positional_dropout_rate: float = 0.1,\n        attention_dropout_rate: float = 0.0,\n        stochastic_depth_rate: float = 0.0,\n        input_layer: Optional[str] = \"conv2d\",\n        pos_enc_class=SinusoidalPositionEncoder,\n        normalize_before: bool = True,\n        concat_after: bool = False,\n        positionwise_layer_type: str = \"linear\",\n        positionwise_conv_kernel_size: int = 1,\n        padding_idx: int = -1,\n        kernel_size: int = 11,\n        sanm_shfit: int = 0,\n        selfattention_layer_type: str = \"sanm\",\n        **kwargs,\n    ):\n        super().__init__()\n        self._output_size = output_size\n\n        self.embed = SinusoidalPositionEncoder()\n\n        self.normalize_before = normalize_before\n\n        positionwise_layer = PositionwiseFeedForward\n        positionwise_layer_args = (\n            output_size,\n            linear_units,\n            dropout_rate,\n        )\n\n        encoder_selfattn_layer = MultiHeadedAttentionSANM\n        encoder_selfattn_layer_args0 = (\n            attention_heads,\n            input_size,\n            output_size,\n            attention_dropout_rate,\n            kernel_size,\n            sanm_shfit,\n        )\n        encoder_selfattn_layer_args = (\n            attention_heads,\n            output_size,\n            output_size,\n            attention_dropout_rate,\n            kernel_size,\n            sanm_shfit,\n        )\n\n        self.encoders0 = nn.ModuleList(\n            [\n                EncoderLayerSANM(\n                    input_size,\n                    output_size,\n                    encoder_selfattn_layer(*encoder_selfattn_layer_args0),\n                    positionwise_layer(*positionwise_layer_args),\n                    dropout_rate,\n                )\n                for i in range(1)\n            ]\n        )\n        self.encoders = nn.ModuleList(\n            [\n                EncoderLayerSANM(\n                    output_size,\n                    output_size,\n                    encoder_selfattn_layer(*encoder_selfattn_layer_args),\n                    positionwise_layer(*positionwise_layer_args),\n                    dropout_rate,\n                )\n                for i in range(num_blocks - 1)\n            ]\n        )\n\n        self.tp_encoders = nn.ModuleList(\n            [\n                EncoderLayerSANM(\n                    output_size,\n                    output_size,\n                    encoder_selfattn_layer(*encoder_selfattn_layer_args),\n                    positionwise_layer(*positionwise_layer_args),\n                    dropout_rate,\n                )\n                for i in range(tp_blocks)\n            ]\n        )\n\n        self.after_norm = LayerNorm(output_size)\n\n        self.tp_norm = LayerNorm(output_size)\n\n    def output_size(self) -> int:\n        return self._output_size\n\n    def forward(\n        self,\n        xs_pad: torch.Tensor,\n        ilens: torch.Tensor,\n    ):\n        \"\"\"Embed positions in tensor.\"\"\"\n        masks = sequence_mask(ilens, device=ilens.device)[:, None, :]\n\n        xs_pad *= self.output_size() ** 0.5\n\n        xs_pad = self.embed(xs_pad)\n\n        # forward encoder1\n        for layer_idx, encoder_layer in enumerate(self.encoders0):\n            encoder_outs = encoder_layer(xs_pad, masks)\n            xs_pad, masks = encoder_outs[0], encoder_outs[1]\n\n        for layer_idx, encoder_layer in enumerate(self.encoders):\n            encoder_outs = encoder_layer(xs_pad, masks)\n            xs_pad, masks = encoder_outs[0], encoder_outs[1]\n\n        xs_pad = self.after_norm(xs_pad)\n\n        # forward encoder2\n        olens = masks.squeeze(1).sum(1).int()\n\n        for layer_idx, encoder_layer in enumerate(self.tp_encoders):\n            encoder_outs = encoder_layer(xs_pad, masks)\n            xs_pad, masks = encoder_outs[0], encoder_outs[1]\n\n        xs_pad = self.tp_norm(xs_pad)\n        return xs_pad, olens\n\n\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n    \"\"\"CTC-attention hybrid Encoder-Decoder model\"\"\"\n\n    def __init__(\n        self,\n        specaug: str = None,\n        specaug_conf: dict = None,\n        normalize: str = None,\n        normalize_conf: dict = None,\n        encoder: str = None,\n        encoder_conf: dict = None,\n        ctc_conf: dict = None,\n        input_size: int = 80,\n        vocab_size: int = -1,\n        ignore_id: int = -1,\n        blank_id: int = 0,\n        sos: int = 1,\n        eos: int = 2,\n        length_normalized_loss: bool = False,\n        **kwargs,\n    ):\n\n        super().__init__()\n\n        if specaug is not None:\n            specaug_class = tables.specaug_classes.get(specaug)\n            specaug = specaug_class(**specaug_conf)\n        if normalize is not None:\n            normalize_class = tables.normalize_classes.get(normalize)\n            normalize = normalize_class(**normalize_conf)\n        encoder_class = tables.encoder_classes.get(encoder)\n        encoder = encoder_class(input_size=input_size, **encoder_conf)\n        encoder_output_size = encoder.output_size()\n\n        if ctc_conf is None:\n            ctc_conf = {}\n        ctc = CTC(odim=vocab_size, encoder_output_size=encoder_output_size, **ctc_conf)\n\n        self.blank_id = blank_id\n        self.sos = sos if sos is not None else vocab_size - 1\n        self.eos = eos if eos is not None else vocab_size - 1\n        self.vocab_size = vocab_size\n        self.ignore_id = ignore_id\n        self.specaug = specaug\n        self.normalize = normalize\n        self.encoder = encoder\n        self.error_calculator = None\n\n        self.ctc = ctc\n\n        self.length_normalized_loss = length_normalized_loss\n        self.encoder_output_size = encoder_output_size\n\n        self.lid_dict = {\"auto\": 0, \"zh\": 3, \"en\": 4, \"yue\": 7, \"ja\": 11, \"ko\": 12, \"nospeech\": 13}\n        self.lid_int_dict = {24884: 3, 24885: 4, 24888: 7, 24892: 11, 24896: 12, 24992: 13}\n        self.textnorm_dict = {\"withitn\": 14, \"woitn\": 15}\n        self.textnorm_int_dict = {25016: 14, 25017: 15}\n        self.embed = torch.nn.Embedding(7 + len(self.lid_dict) + len(self.textnorm_dict), input_size)\n        self.emo_dict = {\"unk\": 25009, \"happy\": 25001, \"sad\": 25002, \"angry\": 25003, \"neutral\": 25004}\n        \n        self.criterion_att = LabelSmoothingLoss(\n            size=self.vocab_size,\n            padding_idx=self.ignore_id,\n            smoothing=kwargs.get(\"lsm_weight\", 0.0),\n            normalize_length=self.length_normalized_loss,\n        )\n    \n    @staticmethod\n    def from_pretrained(model:str=None, **kwargs):\n        from funasr import AutoModel\n        model, kwargs = AutoModel.build_model(model=model, trust_remote_code=True, **kwargs)\n        \n        return model, kwargs\n\n    def forward(\n        self,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        **kwargs,\n    ):\n        \"\"\"Encoder + Decoder + Calc loss\n        Args:\n                speech: (Batch, Length, ...)\n                speech_lengths: (Batch, )\n                text: (Batch, Length)\n                text_lengths: (Batch,)\n        \"\"\"\n        # import pdb;\n        # pdb.set_trace()\n        if len(text_lengths.size()) > 1:\n            text_lengths = text_lengths[:, 0]\n        if len(speech_lengths.size()) > 1:\n            speech_lengths = speech_lengths[:, 0]\n\n        batch_size = speech.shape[0]\n\n        # 1. Encoder\n        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, text)\n\n        loss_ctc, cer_ctc = None, None\n        loss_rich, acc_rich = None, None\n        stats = dict()\n\n        loss_ctc, cer_ctc = self._calc_ctc_loss(\n            encoder_out[:, 4:, :], encoder_out_lens - 4, text[:, 4:], text_lengths - 4\n        )\n\n        loss_rich, acc_rich = self._calc_rich_ce_loss(\n            encoder_out[:, :4, :], text[:, :4]\n        )\n\n        loss = loss_ctc + loss_rich\n        # Collect total loss stats\n        stats[\"loss_ctc\"] = torch.clone(loss_ctc.detach()) if loss_ctc is not None else None\n        stats[\"loss_rich\"] = torch.clone(loss_rich.detach()) if loss_rich is not None else None\n        stats[\"loss\"] = torch.clone(loss.detach()) if loss is not None else None\n        stats[\"acc_rich\"] = acc_rich\n\n        # force_gatherable: to-device and to-tensor if scalar for DataParallel\n        if self.length_normalized_loss:\n            batch_size = int((text_lengths + 1).sum())\n        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)\n        return loss, stats, weight\n\n    def encode(\n        self,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        text: torch.Tensor,\n        **kwargs,\n    ):\n        \"\"\"Frontend + Encoder. Note that this method is used by asr_inference.py\n        Args:\n                speech: (Batch, Length, ...)\n                speech_lengths: (Batch, )\n                ind: int\n        \"\"\"\n\n        # Data augmentation\n        if self.specaug is not None and self.training:\n            speech, speech_lengths = self.specaug(speech, speech_lengths)\n\n        # Normalization for feature: e.g. Global-CMVN, Utterance-CMVN\n        if self.normalize is not None:\n            speech, speech_lengths = self.normalize(speech, speech_lengths)\n\n\n        lids = torch.LongTensor([[self.lid_int_dict[int(lid)] if torch.rand(1) > 0.2 and int(lid) in self.lid_int_dict else 0 ] for lid in text[:, 0]]).to(speech.device)\n        language_query = self.embed(lids)\n        \n        styles = torch.LongTensor([[self.textnorm_int_dict[int(style)]] for style in text[:, 3]]).to(speech.device)\n        style_query = self.embed(styles)\n        speech = torch.cat((style_query, speech), dim=1)\n        speech_lengths += 1\n\n        event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(speech.size(0), 1, 1)\n        input_query = torch.cat((language_query, event_emo_query), dim=1)\n        speech = torch.cat((input_query, speech), dim=1)\n        speech_lengths += 3\n\n        encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n\n        return encoder_out, encoder_out_lens\n\n    def _calc_ctc_loss(\n        self,\n        encoder_out: torch.Tensor,\n        encoder_out_lens: torch.Tensor,\n        ys_pad: torch.Tensor,\n        ys_pad_lens: torch.Tensor,\n    ):\n        # Calc CTC loss\n        loss_ctc = self.ctc(encoder_out, encoder_out_lens, ys_pad, ys_pad_lens)\n\n        # Calc CER using CTC\n        cer_ctc = None\n        if not self.training and self.error_calculator is not None:\n            ys_hat = self.ctc.argmax(encoder_out).data\n            cer_ctc = self.error_calculator(ys_hat.cpu(), ys_pad.cpu(), is_ctc=True)\n        return loss_ctc, cer_ctc\n\n    def _calc_rich_ce_loss(\n        self,\n        encoder_out: torch.Tensor,\n        ys_pad: torch.Tensor,\n    ):\n        decoder_out = self.ctc.ctc_lo(encoder_out)\n        # 2. Compute attention loss\n        loss_rich = self.criterion_att(decoder_out, ys_pad.contiguous())\n        acc_rich = th_accuracy(\n            decoder_out.view(-1, self.vocab_size),\n            ys_pad.contiguous(),\n            ignore_label=self.ignore_id,\n        )\n\n        return loss_rich, acc_rich\n\n\n    def inference(\n        self,\n        data_in,\n        data_lengths=None,\n        key: list = [\"wav_file_tmp_name\"],\n        tokenizer=None,\n        frontend=None,\n        **kwargs,\n    ):\n\n\n        meta_data = {}\n        if (\n            isinstance(data_in, torch.Tensor) and kwargs.get(\"data_type\", \"sound\") == \"fbank\"\n        ):  # fbank\n            speech, speech_lengths = data_in, data_lengths\n            if len(speech.shape) < 3:\n                speech = speech[None, :, :]\n            if speech_lengths is None:\n                speech_lengths = speech.shape[1]\n        else:\n            # extract fbank feats\n            time1 = time.perf_counter()\n            audio_sample_list = load_audio_text_image_video(\n                data_in,\n                fs=frontend.fs,\n                audio_fs=kwargs.get(\"fs\", 16000),\n                data_type=kwargs.get(\"data_type\", \"sound\"),\n                tokenizer=tokenizer,\n            )\n            time2 = time.perf_counter()\n            meta_data[\"load_data\"] = f\"{time2 - time1:0.3f}\"\n            speech, speech_lengths = extract_fbank(\n                audio_sample_list, data_type=kwargs.get(\"data_type\", \"sound\"), frontend=frontend\n            )\n            time3 = time.perf_counter()\n            meta_data[\"extract_feat\"] = f\"{time3 - time2:0.3f}\"\n            meta_data[\"batch_data_time\"] = (\n                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n            )\n\n        speech = speech.to(device=kwargs[\"device\"])\n        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n\n        language = kwargs.get(\"language\", \"auto\")\n        language_query = self.embed(\n            torch.LongTensor(\n                [[self.lid_dict[language] if language in self.lid_dict else 0]]\n            ).to(speech.device)\n        ).repeat(speech.size(0), 1, 1)\n        \n        use_itn = kwargs.get(\"use_itn\", False)\n        output_timestamp = kwargs.get(\"output_timestamp\", False)\n\n        textnorm = kwargs.get(\"text_norm\", None)\n        if textnorm is None:\n            textnorm = \"withitn\" if use_itn else \"woitn\"\n        textnorm_query = self.embed(\n            torch.LongTensor([[self.textnorm_dict[textnorm]]]).to(speech.device)\n        ).repeat(speech.size(0), 1, 1)\n        speech = torch.cat((textnorm_query, speech), dim=1)\n        speech_lengths += 1\n\n        event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(\n            speech.size(0), 1, 1\n        )\n        input_query = torch.cat((language_query, event_emo_query), dim=1)\n        speech = torch.cat((input_query, speech), dim=1)\n        speech_lengths += 3\n\n        # Encoder\n        encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n        if isinstance(encoder_out, tuple):\n            encoder_out = encoder_out[0]\n\n        # c. Passed the encoder result and the beam search\n        ctc_logits = self.ctc.log_softmax(encoder_out)\n        if kwargs.get(\"ban_emo_unk\", False):\n            ctc_logits[:, :, self.emo_dict[\"unk\"]] = -float(\"inf\")\n\n        results = []\n        b, n, d = encoder_out.size()\n        if isinstance(key[0], (list, tuple)):\n            key = key[0]\n        if len(key) < b:\n            key = key * b\n        for i in range(b):\n            x = ctc_logits[i, : encoder_out_lens[i].item(), :]\n            yseq = x.argmax(dim=-1)\n            yseq = torch.unique_consecutive(yseq, dim=-1)\n\n            ibest_writer = None\n            if kwargs.get(\"output_dir\") is not None:\n                if not hasattr(self, \"writer\"):\n                    self.writer = DatadirWriter(kwargs.get(\"output_dir\"))\n                ibest_writer = self.writer[f\"1best_recog\"]\n\n            mask = yseq != self.blank_id\n            token_int = yseq[mask].tolist()\n\n            # Change integer-ids to tokens\n            text = tokenizer.decode(token_int)\n            if ibest_writer is not None:\n                ibest_writer[\"text\"][key[i]] = text\n\n            if output_timestamp:\n                from itertools import groupby\n                timestamp = []\n                tokens = tokenizer.text2tokens(text)[4:]\n\n                logits_speech = self.ctc.softmax(encoder_out)[i, 4:encoder_out_lens[i].item(), :]\n\n                pred = logits_speech.argmax(-1).cpu()\n                logits_speech[pred==self.blank_id, self.blank_id] = 0\n\n                align = ctc_forced_align(\n                    logits_speech.unsqueeze(0).float(),\n                    torch.Tensor(token_int[4:]).unsqueeze(0).long().to(logits_speech.device),\n                    (encoder_out_lens-4).long(),\n                    torch.tensor(len(token_int)-4).unsqueeze(0).long().to(logits_speech.device),\n                    ignore_id=self.ignore_id,\n                )\n\n                pred = groupby(align[0, :encoder_out_lens[0]])\n                _start = 0\n                token_id = 0\n                ts_max = encoder_out_lens[i] - 4\n                for pred_token, pred_frame in pred:\n                    _end = _start + len(list(pred_frame))\n                    if pred_token != 0:\n                        ts_left = max((_start*60-30)/1000, 0)\n                        ts_right = min((_end*60-30)/1000, (ts_max*60-30)/1000)\n                        timestamp.append([tokens[token_id], ts_left, ts_right])\n                        token_id += 1\n                    _start = _end\n\n                result_i = {\"key\": key[i], \"text\": text, \"timestamp\": timestamp}\n                results.append(result_i)\n            else:\n                result_i = {\"key\": key[i], \"text\": text}\n                results.append(result_i)\n        return results, meta_data\n\n    def export(self, **kwargs):\n        from export_meta import export_rebuild_model\n\n        if \"max_seq_len\" not in kwargs:\n            kwargs[\"max_seq_len\"] = 512\n        models = export_rebuild_model(model=self, **kwargs)\n        return models\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1103515625,
          "content": "torch<=2.3\ntorchaudio\nmodelscope\nhuggingface\nhuggingface_hub\nfunasr>=1.1.3\nnumpy<=1.26.4\ngradio\nfastapi>=0.111.1\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "webui.py",
          "type": "blob",
          "size": 7.697265625,
          "content": "# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel = AutoModel(model=model,\n\t\t\t\t  vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n\t\t\t\t  vad_kwargs={\"max_single_segment_time\": 30000},\n\t\t\t\t  trust_remote_code=True,\n\t\t\t\t  )\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"ğŸ˜Š\",\n\t\"<|SAD|>\": \"ğŸ˜”\",\n\t\"<|ANGRY|>\": \"ğŸ˜¡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"ğŸ˜°\",\n\t\"<|DISGUSTED|>\": \"ğŸ¤¢\",\n\t\"<|SURPRISED|>\": \"ğŸ˜®\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"ğŸ¼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"ğŸ‘\",\n\t\"<|Laughter|>\": \"ğŸ˜€\",\n\t\"<|Cry|>\": \"ğŸ˜­\",\n\t\"<|Sneeze|>\": \"ğŸ¤§\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"ğŸ¤§\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"â“\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"ğŸ˜Š\",\n\t\"<|SAD|>\": \"ğŸ˜”\",\n\t\"<|ANGRY|>\": \"ğŸ˜¡\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"ğŸ¼\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"ğŸ‘\",\n\t\"<|Laughter|>\": \"ğŸ˜€\",\n\t\"<|FEARFUL|>\": \"ğŸ˜°\",\n\t\"<|DISGUSTED|>\": \"ğŸ¤¢\",\n\t\"<|SURPRISED|>\": \"ğŸ˜®\",\n\t\"<|Cry|>\": \"ğŸ˜­\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"ğŸ¤§\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"ğŸ˜·\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"ğŸ˜Š\", \"ğŸ˜”\", \"ğŸ˜¡\", \"ğŸ˜°\", \"ğŸ¤¢\", \"ğŸ˜®\"}\nevent_set = {\"ğŸ¼\", \"ğŸ‘\", \"ğŸ˜€\", \"ğŸ˜­\", \"ğŸ¤§\", \"ğŸ˜·\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"â“\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (ğŸ˜Š happy, ğŸ˜¡ angry/exicting, ğŸ˜” sad) and types of sound events (ğŸ˜€ laughter, ğŸ¼ music, ğŸ‘ applause, ğŸ¤§ cough&sneeze, ğŸ˜­ cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n"
        }
      ]
    }
  ]
}