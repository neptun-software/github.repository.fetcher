{
  "metadata": {
    "timestamp": 1736559734493,
    "page": 433,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "FunAudioLLM/SenseVoice",
      "stars": 4005,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3232421875,
          "content": ".idea\n./__pycache__/\n*/__pycache__/\n*/*/__pycache__/\n*/*/*/__pycache__/\n.DS_Store\ninit_model/\n*.tar.gz\ntest_local/\nRapidASR\nexport/*\n*.pyc\n.eggs\nMaaS-lib\n.gitignore\n.egg*\ndist\nbuild\nfunasr.egg-info\ndocs/_build\nmodelscope\nsamples\n.ipynb_checkpoints\noutputs*\nemotion2vec*\nGPT-SoVITS*\nmodelscope_models\nexamples/aishell/llm_asr_nar/*\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.0693359375,
          "content": "Ref to https://github.com/modelscope/FunASR?tab=readme-ov-file#license\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.375,
          "content": "([ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh.md)|English|[Êó•Êú¨Ë™û](./README_ja.md))\n\n\n# Introduction\n\nSenseVoice is a speech foundation model with multiple speech understanding capabilities, including automatic speech recognition (ASR),  spoken language identification (LID), speech emotion recognition (SER), and audio event detection (AED). \n\n<div align=\"center\">  \n<img src=\"image/sensevoice2.png\">\n</div>\n\n[//]: # (<div align=\"center\"><img src=\"image/sensevoice.png\" width=\"700\"/> </div>)\n\n<div align=\"center\">  \n<h4>\n<a href=\"https://funaudiollm.github.io/\"> Homepage </a>\nÔΩú<a href=\"#What's News\"> What's News </a>\nÔΩú<a href=\"#Benchmarks\"> Benchmarks </a>\nÔΩú<a href=\"#Install\"> Install </a>\nÔΩú<a href=\"#Usage\"> Usage </a>\nÔΩú<a href=\"#Community\"> Community </a>\n</h4>\n\nModel Zoo:\n[modelscope](https://www.modelscope.cn/models/iic/SenseVoiceSmall), [huggingface](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)\n\nOnline Demo:\n[modelscope demo](https://www.modelscope.cn/studios/iic/SenseVoice), [huggingface space](https://huggingface.co/spaces/FunAudioLLM/SenseVoice)\n\n\n</div>\n\n\n<a name=\"Highligts\"></a>\n# Highlights üéØ\n**SenseVoice** focuses on high-accuracy multilingual speech recognition, speech emotion recognition, and audio event detection.\n- **Multilingual Speech Recognition:** Trained with over 400,000 hours of data, supporting more than 50 languages, the recognition performance surpasses that of the Whisper model.\n- **Rich transcribe:** \n  - Possess excellent emotion recognition capabilities, achieving and surpassing the effectiveness of the current best emotion recognition models on test data.\n  - Offer sound event detection capabilities, supporting the detection of various common human-computer interaction events such as bgm, applause, laughter, crying, coughing, and sneezing.\n- **Efficient Inference:** The SenseVoice-Small model utilizes a non-autoregressive end-to-end framework, leading to exceptionally low inference latency. It requires only 70ms to process 10 seconds of audio, which is 15 times faster than Whisper-Large.\n- **Convenient Finetuning:** Provide convenient finetuning scripts and strategies, allowing users to easily address long-tail sample issues according to their business scenarios.\n- **Service Deployment:** Offer service deployment pipeline,  supporting multi-concurrent requests, with client-side languages including Python, C++, HTML, Java, and C#, among others.\n\n<a name=\"What's News\"></a>\n# What's New üî•\n- 2024/11: Add support for timestamp based on the CTC alignment.\n- 2024/7: Added Export Features for [ONNX](./demo_onnx.py) and [libtorch](./demo_libtorch.py), as well as Python Version Runtimes: [funasr-onnx-0.4.0](https://pypi.org/project/funasr-onnx/), [funasr-torch-0.1.1](https://pypi.org/project/funasr-torch/)\n- 2024/7: The [SenseVoice-Small](https://www.modelscope.cn/models/iic/SenseVoiceSmall) voice understanding model is open-sourced, which offers high-precision multilingual speech recognition, emotion recognition, and audio event detection capabilities for Mandarin, Cantonese, English, Japanese, and Korean and leads to exceptionally low inference latency.  \n- 2024/7: The CosyVoice for natural speech generation with multi-language, timbre, and emotion control. CosyVoice excels in multi-lingual voice generation, zero-shot voice generation, cross-lingual voice cloning, and instruction-following capabilities. [CosyVoice repo](https://github.com/FunAudioLLM/CosyVoice) and [CosyVoice space](https://www.modelscope.cn/studios/iic/CosyVoice-300M).\n- 2024/7: [FunASR](https://github.com/modelscope/FunASR) is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR.\n\n<a name=\"Benchmarks\"></a>\n# Benchmarks üìù\n\n## Multilingual Speech Recognition\nWe compared the performance of multilingual speech recognition between SenseVoice and Whisper on open-source benchmark datasets, including AISHELL-1, AISHELL-2, Wenetspeech, LibriSpeech, and Common Voice. In terms of Chinese and Cantonese recognition, the SenseVoice-Small model has advantages.\n\n<div align=\"center\">  \n<img src=\"image/asr_results1.png\" width=\"400\" /><img src=\"image/asr_results2.png\" width=\"400\" />\n</div>\n\n## Speech Emotion Recognition\n\nDue to the current lack of widely-used benchmarks and methods for speech emotion recognition, we conducted evaluations across various metrics on multiple test sets and performed a comprehensive comparison with numerous results from recent benchmarks. The selected test sets encompass data in both Chinese and English, and include multiple styles such as performances, films, and natural conversations. Without finetuning on the target data, SenseVoice was able to achieve and exceed the performance of the current best speech emotion recognition models.\n\n<div align=\"center\">  \n<img src=\"image/ser_table.png\" width=\"1000\" />\n</div>\n\nFurthermore, we compared multiple open-source speech emotion recognition models on the test sets, and the results indicate that the SenseVoice-Large model achieved the best performance on nearly all datasets, while the SenseVoice-Small model also surpassed other open-source models on the majority of the datasets.\n\n<div align=\"center\">  \n<img src=\"image/ser_figure.png\" width=\"500\" />\n</div>\n\n## Audio Event Detection\n\nAlthough trained exclusively on speech data, SenseVoice can still function as a standalone event detection model. We compared its performance on the environmental sound classification ESC-50 dataset against the widely used industry models BEATS and PANN. The SenseVoice model achieved commendable results on these tasks. However, due to limitations in training data and methodology, its event classification performance has some gaps compared to specialized AED models.\n\n<div align=\"center\">  \n<img src=\"image/aed_figure.png\" width=\"500\" />\n</div>\n\n## Computational  Efficiency\n\nThe SenseVoice-Small model deploys a non-autoregressive end-to-end architecture, resulting in extremely low inference latency. With a similar number of parameters to the Whisper-Small model, it infers more than 5 times faster than Whisper-Small and 15 times faster than Whisper-Large. \n\n<div align=\"center\">  \n<img src=\"image/inference.png\" width=\"1000\" />\n</div>\n\n\n# Requirements\n\n```shell\npip install -r requirements.txt\n```\n\n<a name=\"Usage\"></a>\n# Usage\n\n## Inference\n\nSupports input of audio in any format and of any duration.\n\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",    \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n<details><summary>Parameter Description (Click to Expand)</summary>\n\n- `model_dir`: The name of the model, or the path to the model on the local disk.\n- `trust_remote_code`:\n  - When `True`, it means that the model's code implementation is loaded from `remote_code`, which specifies the exact location of the `model` code (for example, `model.py` in the current directory). It supports absolute paths, relative paths, and network URLs.\n  - When `False`, it indicates that the model's code implementation is the integrated version within [FunASR](https://github.com/modelscope/FunASR). At this time, modifications made to `model.py` in the current directory will not be effective, as the version loaded is the internal one from FunASR. For the model code, [click here to view](https://github.com/modelscope/FunASR/tree/main/funasr/models/sense_voice).\n- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.\n- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).\n- `use_itn`: Whether the output result includes punctuation and inverse text normalization.\n- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).\n- `merge_vad`: Whether to merge short audio fragments segmented by the VAD model, with the merged length being `merge_length_s`, in seconds (s).\n- `ban_emo_unk`: Whether to ban the output of the `emo_unk` token.\n</details>\n\nIf all inputs are short audios (<30s), and batch inference is needed to speed up inference efficiency, the VAD model can be removed, and `batch_size` can be set accordingly.\n```python\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"zh\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    batch_size=64, \n)\n```\n\nFor more usage, please refer to [docs](https://github.com/modelscope/FunASR/blob/main/docs/tutorial/README.md)\n\n### Inference directly\n\nSupports input of audio in any format, with an input duration limit of 30 seconds or less.\n\n```python\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\n```\n\n### Export and Test\n<details><summary>ONNX and Libtorch Export</summary>\n\n#### ONNX\n```python\n# pip3 install -U funasr funasr-onnx\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\nNote: ONNX model is exported to the original model directory.\n\n#### Libtorch\n```python\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\nNote: Libtorch model is exported to the original model directory.\n</details>\n\n## Service\n\n### Deployment with FastAPI\n```shell\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n## Finetune\n\n### Requirements\n\n```shell\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n### Data prepare\n\nData examples\n\n```text\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\n\nFull ref to `data/train_example.jsonl`\n\n<details><summary>Data Prepare Details</summary>\n\nDescriptionÔºö\n- `key`: audio file unique ID\n- `source`Ôºöpath to the audio file\n- `source_len`Ôºönumber of fbank frames of the audio file\n- `target`Ôºötranscription\n- `target_len`Ôºölength of target\n- `text_language`Ôºölanguage id of the audio file\n- `emo_target`Ôºöemotion label of the audio file\n- `event_target`Ôºöevent label of the audio file\n- `with_or_wo_itn`Ôºöwhether includes punctuation and inverse text normalization\n\n\n`train_text.txt`\n\n\n```bash\nBAC009S0764W0121 ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ\nBAC009S0916W0489 ÊπñÂåó‰∏ÄÂÖ¨Âè∏‰ª•ÂëòÂ∑•Âêç‰πâË¥∑Ê¨æÊï∞ÂçÅÂëòÂ∑•Ë¥üÂÄ∫ÂçÉ‰∏á\nasr_example_cn_en ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning ÂÅö data analytics ÂÅö data science ‰πüÂ•Ω scientist ‰πüÂ•ΩÈÄöÈÄöÈÉΩË¶ÅÈÉΩÂÅöÁöÑÂü∫Êú¨ÂäüÂïäÈÇ£ again ÂÖàÂÖàÂØπÊúâ‰∏Ä‰∫õ>‰πüËÆ∏ÂØπ\nID0012W0014 he tried to think how it could be\n```\n\n`train_wav.scp`\n\n\n\n```bash\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n\n`train_text_language.txt`\n\nThe language ids include `<|zh|>`„ÄÅ`<|en|>`„ÄÅ`<|yue|>`„ÄÅ`<|ja|>` and `<|ko|>`.\n\n```bash\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n\n`train_emo.txt`\n\nThe emotion labels include`<|HAPPY|>`„ÄÅ`<|SAD|>`„ÄÅ`<|ANGRY|>`„ÄÅ`<|NEUTRAL|>`„ÄÅ`<|FEARFUL|>`„ÄÅ`<|DISGUSTED|>` and `<|SURPRISED|>`.\n\n```bash\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n\n`train_event.txt`\n\nThe event labels include`<|BGM|>`„ÄÅ`<|Speech|>`„ÄÅ`<|Applause|>`„ÄÅ`<|Laughter|>`„ÄÅ`<|Cry|>`„ÄÅ`<|Sneeze|>`„ÄÅ`<|Breath|>` and `<|Cough|>`.\n\n```bash\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```\n\n`Command`\n```shell\n# generate train.jsonl and val.jsonl from wav.scp, text.txt, text_language.txt, emo_target.txt, event_target.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\nIf there is no `train_text_language.txt`, `train_emo_target.txt` and `train_event_target.txt`, the language, emotion and event label will be predicted automatically by using the `SenseVoice` model.\n```shell\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\" \\\n++model_dir='iic/SenseVoiceSmall'\n```\n</details>\n\n### Finetune\n\nEnsure to modify the train_tool in finetune.sh to the absolute path of `funasr/bin/train_ds.py` from the FunASR installation directory you have set up earlier.\n\n```shell\nbash finetune.sh\n```\n\n## WebUI\n\n```shell\npython webui.py\n```\n\n<div align=\"center\"><img src=\"image/webui.png\" width=\"700\"/> </div>\n\n\n## Remarkable Third-Party Work\n- Triton (GPU) Deployment Best Practices: Using Triton + TensorRT, tested with FP32, achieving an acceleration ratio of 526 on V100 GPU. FP16 support is in progress. [Repository](https://github.com/modelscope/FunASR/blob/main/runtime/triton_gpu/README.md)\n- Sherpa-onnx Deployment Best Practices: Supports using SenseVoice in 10 programming languages: C++, C, Python, C#, Go, Swift, Kotlin, Java, JavaScript, and Dart. Also supports deploying SenseVoice on platforms like iOS, Android, and Raspberry Pi. [Repository](https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html)\n- [SenseVoice.cpp](https://github.com/lovemefan/SenseVoice.cpp). Inference of SenseVoice in pure C/C++ based on GGML, supporting 3-bit, 4-bit, 5-bit, 8-bit quantization, etc. with no third-party dependencies.\n- [streaming-sensevoice](https://github.com/pengzhendong/streaming-sensevoice) processes inference in chunks. To achieve pseudo-streaming, it employs a truncated attention mechanism, sacrificing some accuracy. Additionally, this technology supports CTC prefix beam search and hot-word boosting features.\n- [OmniSenseVoice](https://github.com/lifeiteng/OmniSenseVoice) is optimized for lightning-fast inference and batching process. \n\n<a name=\"Community\"></a>\n# Community\nIf you encounter problems in use, you can directly raise Issues on the github page.\n\nYou can also scan the following DingTalk group QR code to join the community group for communication and discussion.\n\n|                          FunASR                          |\n|:--------------------------------------------------------:|\n| <img src=\"image/dingding_funasr.png\" width=\"250\"/></div> |\n\n\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 19.517578125,
          "content": "# SenseVoice\n\n„Äå[ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh.md)„Äç|„Äå[English](./README.md)„Äç|„ÄåÊó•Êú¨Ë™û„Äç\n\nSenseVoice„ÅØ„ÄÅÈü≥Â£∞Ë™çË≠òÔºàASRÔºâ„ÄÅË®ÄË™ûË≠òÂà•ÔºàLIDÔºâ„ÄÅÈü≥Â£∞ÊÑüÊÉÖË™çË≠òÔºàSERÔºâ„ÄÅ„Åä„Çà„Å≥Èü≥Èüø„Ç§„Éô„É≥„ÉàÂàÜÈ°ûÔºàAECÔºâ„Åæ„Åü„ÅØÈü≥Èüø„Ç§„Éô„É≥„ÉàÊ§úÂá∫ÔºàAEDÔºâ„ÇíÂê´„ÇÄÈü≥Â£∞ÁêÜËß£ËÉΩÂäõ„ÇíÂÇô„Åà„ÅüÈü≥Â£∞Âü∫Áõ§„É¢„Éá„É´„Åß„Åô„ÄÇÊú¨„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Åß„ÅØ„ÄÅSenseVoice„É¢„Éá„É´„ÅÆÁ¥π‰ªã„Å®„ÄÅË§áÊï∞„ÅÆ„Çø„Çπ„ÇØ„ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÄÅ„Åä„Çà„Å≥„É¢„Éá„É´„ÅÆ‰ΩìÈ®ì„Å´ÂøÖË¶Å„Å™Áí∞Â¢É„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´„Å®Êé®Ë´ñÊñπÊ≥ï„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/sensevoice2.png\">\n</div>\n[//]: # (<div align=\"center\"><img src=\"image/sensevoice2.png\" width=\"700\"/> </div>)\n \n<div align=\"center\">  \n<h4>\n<a href=\"https://funaudiollm.github.io/\"> „Éõ„Éº„É†„Éö„Éº„Ç∏ </a>\nÔΩú<a href=\"#ÊúÄÊñ∞Âä®ÊÄÅ\"> ÊúÄÊñ∞ÊÉÖÂ†± </a>\nÔΩú<a href=\"#ÊÄßËÉΩËØÑÊµã\"> ÊÄßËÉΩË©ï‰æ° </a>\nÔΩú<a href=\"#ÁéØÂ¢ÉÂÆâË£Ö\"> Áí∞Â¢É„Ç§„É≥„Çπ„Éà„Éº„É´ </a>\nÔΩú<a href=\"#Áî®Ê≥ïÊïôÁ®ã\"> ‰ΩøÁî®ÊñπÊ≥ï„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´ </a>\nÔΩú<a href=\"#ËÅîÁ≥ªÊàë‰ª¨\"> „ÅäÂïè„ÅÑÂêà„Çè„Åõ </a>\n</h4>\n\n„É¢„Éá„É´„É™„Éù„Ç∏„Éà„É™Ôºö[modelscope](https://www.modelscope.cn/models/iic/SenseVoiceSmall)Ôºå[huggingface](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)\n\n„Ç™„É≥„É©„Ç§„É≥‰ΩìÈ®ìÔºö\n[modelscope demo](https://www.modelscope.cn/studios/iic/SenseVoice), [huggingface space](https://huggingface.co/spaces/FunAudioLLM/SenseVoice)\n\n</div>\n\n<a name=\"Ê†∏ÂøÉÂäüËÉΩ\"></a>\n# „Ç≥„Ç¢Ê©üËÉΩ üéØ\n**SenseVoice**„ÅØ„ÄÅÈ´òÁ≤æÂ∫¶„Å™Â§öË®ÄË™ûÈü≥Â£∞Ë™çË≠ò„ÄÅÊÑüÊÉÖË™çË≠ò„ÄÅ„Åä„Çà„Å≥Èü≥Â£∞„Ç§„Éô„É≥„ÉàÊ§úÂá∫„Å´ÁÑ¶ÁÇπ„ÇíÂΩì„Å¶„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n- **Â§öË®ÄË™ûË™çË≠òÔºö** 40‰∏áÊôÇÈñì‰ª•‰∏ä„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®„Åó„Å¶„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÄÅ50‰ª•‰∏ä„ÅÆË®ÄË™û„Çí„Çµ„Éù„Éº„Éà„Åó„ÄÅË™çË≠òÊÄßËÉΩ„ÅØWhisper„É¢„Éá„É´„Çí‰∏äÂõû„Çä„Åæ„Åô„ÄÇ\n- **„É™„ÉÉ„ÉÅ„ÉÜ„Ç≠„Çπ„ÉàË™çË≠òÔºö** \n  - ÂÑ™„Çå„ÅüÊÑüÊÉÖË™çË≠òËÉΩÂäõ„ÇíÊåÅ„Å°„ÄÅ„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅßÁèæÂú®„ÅÆÊúÄËâØ„ÅÆÊÑüÊÉÖË™çË≠ò„É¢„Éá„É´„ÅÆÂäπÊûú„ÇíÈÅîÊàê„Åä„Çà„Å≥‰∏äÂõû„Çä„Åæ„Åô„ÄÇ\n  - Èü≥Â£∞„Ç§„Éô„É≥„ÉàÊ§úÂá∫ËÉΩÂäõ„ÇíÊèê‰æõ„Åó„ÄÅÈü≥Ê•Ω„ÄÅÊãçÊâã„ÄÅÁ¨ë„ÅÑÂ£∞„ÄÅÊ≥£„ÅçÂ£∞„ÄÅÂí≥„ÄÅ„Åè„Åó„ÇÉ„Åø„Å™„Å©„ÅÆ„Åï„Åæ„Åñ„Åæ„Å™‰∏ÄËà¨ÁöÑ„Å™‰∫∫Èñì„Å®„Ç≥„É≥„Éî„É•„Éº„Çø„ÅÆ„Ç§„É≥„Çø„É©„ÇØ„Ç∑„Éß„É≥„Ç§„Éô„É≥„Éà„ÇíÊ§úÂá∫„Åó„Åæ„Åô„ÄÇ\n- **ÂäπÁéáÁöÑ„Å™Êé®Ë´ñÔºö** SenseVoice-Small„É¢„Éá„É´„ÅØÈùûËá™Â∑±ÂõûÂ∏∞„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÇíÊé°Áî®„Åó„Å¶„Åä„Çä„ÄÅÊé®Ë´ñÈÅÖÂª∂„ÅåÈùûÂ∏∏„Å´‰Ωé„Åè„ÄÅ10Áßí„ÅÆÈü≥Â£∞„ÅÆÊé®Ë´ñ„Å´70ms„Åó„Åã„Åã„Åã„Çä„Åæ„Åõ„Çì„ÄÇWhisper-Large„Çà„Çä15ÂÄçÈ´òÈÄü„Åß„Åô„ÄÇ\n- **Á∞°Âçò„Å™ÂæÆË™øÊï¥Ôºö** ‰æøÂà©„Å™ÂæÆË™øÊï¥„Çπ„ÇØ„É™„Éó„Éà„Å®Êà¶Áï•„ÇíÊèê‰æõ„Åó„ÄÅ„É¶„Éº„Ç∂„Éº„Åå„Éì„Ç∏„Éç„Çπ„Ç∑„Éä„É™„Ç™„Å´Âøú„Åò„Å¶„É≠„É≥„Ç∞„ÉÜ„Éº„É´„Çµ„É≥„Éó„É´„ÅÆÂïèÈ°å„ÇíÁ∞°Âçò„Å´Ëß£Ê±∫„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åæ„Åô„ÄÇ\n- **„Çµ„Éº„Éì„ÇπÂ±ïÈñãÔºö** „Éû„É´„ÉÅ„Ç≥„É≥„Ç´„É¨„É≥„Éà„É™„ÇØ„Ç®„Çπ„Éà„Çí„Çµ„Éù„Éº„Éà„Åô„ÇãÂÆåÂÖ®„Å™„Çµ„Éº„Éì„ÇπÂ±ïÈñã„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÊèê‰æõ„Åó„ÄÅ„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Çµ„Ç§„Éâ„ÅÆË®ÄË™û„Å´„ÅØPython„ÄÅC++„ÄÅHTML„ÄÅJava„ÄÅC#„Å™„Å©„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n\n<a name=\"ÊúÄÊñ∞Âä®ÊÄÅ\"></a>\n# ÊúÄÊñ∞ÊÉÖÂ†± üî•\n- 2024/7ÔºöÊñ∞„Åó„Åè[ONNX](./demo_onnx.py)„Å®[libtorch](./demo_libtorch.py)„ÅÆ„Ç®„ÇØ„Çπ„Éù„Éº„ÉàÊ©üËÉΩ„ÇíËøΩÂä†„Åó„ÄÅPython„Éê„Éº„Ç∏„Éß„É≥„ÅÆ„É©„É≥„Çø„Ç§„É†Ôºö[funasr-onnx-0.4.0](https://pypi.org/project/funasr-onnx/)„ÄÅ[funasr-torch-0.1.1](https://pypi.org/project/funasr-torch/)„ÇÇÊèê‰æõÈñãÂßã„ÄÇ\n- 2024/7: [SenseVoice-Small](https://www.modelscope.cn/models/iic/SenseVoiceSmall) Â§öË®ÄË™ûÈü≥Â£∞ÁêÜËß£„É¢„Éá„É´„Åå„Ç™„Éº„Éó„É≥„ÇΩ„Éº„ÇπÂåñ„Åï„Çå„Åæ„Åó„Åü„ÄÇ‰∏≠ÂõΩË™û„ÄÅÂ∫ÉÊù±Ë™û„ÄÅËã±Ë™û„ÄÅÊó•Êú¨Ë™û„ÄÅÈüìÂõΩË™û„ÅÆÂ§öË®ÄË™ûÈü≥Â£∞Ë™çË≠ò„ÄÅÊÑüÊÉÖË™çË≠ò„ÄÅ„Åä„Çà„Å≥„Ç§„Éô„É≥„ÉàÊ§úÂá∫ËÉΩÂäõ„Çí„Çµ„Éù„Éº„Éà„Åó„ÄÅÈùûÂ∏∏„Å´‰Ωé„ÅÑÊé®Ë´ñÈÅÖÂª∂„ÇíÂÆüÁèæ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n- 2024/7: CosyVoice„ÅØËá™ÁÑ∂„Å™Èü≥Â£∞ÁîüÊàê„Å´Âèñ„ÇäÁµÑ„Çì„Åß„Åä„Çä„ÄÅÂ§öË®ÄË™û„ÄÅÈü≥Ëâ≤„ÄÅÊÑüÊÉÖÂà∂Âæ°„Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇÂ§öË®ÄË™ûÈü≥Â£∞ÁîüÊàê„ÄÅ„Çº„É≠„Ç∑„Éß„ÉÉ„ÉàÈü≥Â£∞ÁîüÊàê„ÄÅ„ÇØ„É≠„Çπ„É©„É≥„Ç≤„Éº„Ç∏Èü≥Â£∞„ÇØ„É≠„Éº„É≥„ÄÅ„Åä„Çà„Å≥ÊåáÁ§∫„Å´Âæì„ÅÜËÉΩÂäõ„Å´ÂÑ™„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ[CosyVoice repo](https://github.com/FunAudioLLM/CosyVoice) and [CosyVoice „Ç™„É≥„É©„Ç§„É≥‰ΩìÈ®ì](https://www.modelscope.cn/studios/iic/CosyVoice-300M).\n- 2024/7: [FunASR](https://github.com/modelscope/FunASR) „ÅØ„ÄÅÈü≥Â£∞Ë™çË≠òÔºàASRÔºâ„ÄÅÈü≥Â£∞Ê¥ªÂãïÊ§úÂá∫ÔºàVADÔºâ„ÄÅÂè•Ë™≠ÁÇπÂæ©ÂÖÉ„ÄÅË®ÄË™û„É¢„Éá„É´„ÄÅË©±ËÄÖÊ§úË®º„ÄÅË©±ËÄÖÂàÜÈõ¢„ÄÅ„Åä„Çà„Å≥„Éû„É´„ÉÅ„Éà„Éº„Ç´„ÉºASR„Å™„Å©„ÅÆÊ©üËÉΩ„ÇíÊèê‰æõ„Åô„ÇãÂü∫Êú¨ÁöÑ„Å™Èü≥Â£∞Ë™çË≠ò„ÉÑ„Éº„É´„Ç≠„ÉÉ„Éà„Åß„Åô„ÄÇ\n\n<a name=\"Benchmarks\"></a>\n# „Éô„É≥„ÉÅ„Éû„Éº„ÇØ üìù\n\n## Â§öË®ÄË™ûÈü≥Â£∞Ë™çË≠ò\n\n„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàAISHELL-1„ÄÅAISHELL-2„ÄÅWenetspeech„ÄÅLibrispeech„ÄÅCommon Voice„ÇíÂê´„ÇÄÔºâ„ÅßSenseVoice„Å®Whisper„ÅÆÂ§öË®ÄË™ûÈü≥Â£∞Ë™çË≠òÊÄßËÉΩ„Å®Êé®Ë´ñÂäπÁéá„ÇíÊØîËºÉ„Åó„Åæ„Åó„Åü„ÄÇ‰∏≠ÂõΩË™û„Å®Â∫ÉÊù±Ë™û„ÅÆË™çË≠òÂäπÊûú„Å´„Åä„ÅÑ„Å¶„ÄÅSenseVoice-Small„É¢„Éá„É´„ÅØÊòé„Çâ„Åã„Å™ÂäπÊûú„ÅÆÂÑ™‰ΩçÊÄß„ÇíÊåÅ„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/asr_results1.png\" width=\"400\" /><img src=\"image/asr_results2.png\" width=\"400\" />\n</div>\n\n## ÊÑüÊÉÖË™çË≠ò\n\nÁèæÂú®„ÄÅÂ∫É„Åè‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„ÇãÊÑüÊÉÖË™çË≠ò„ÅÆ„ÉÜ„Çπ„ÉàÊåáÊ®ô„Å®ÊñπÊ≥ï„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅË§áÊï∞„ÅÆ„ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß„Åï„Åæ„Åñ„Åæ„Å™ÊåáÊ®ô„Çí„ÉÜ„Çπ„Éà„Åó„ÄÅÊúÄËøë„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆË§áÊï∞„ÅÆÁµêÊûú„Å®ÂåÖÊã¨ÁöÑ„Å´ÊØîËºÉ„Åó„Åæ„Åó„Åü„ÄÇÈÅ∏Êäû„Åï„Çå„Åü„ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Å´„ÅØ„ÄÅ‰∏≠ÂõΩË™û/Ëã±Ë™û„ÅÆ‰∏°Êñπ„ÅÆË®ÄË™û„Å®„ÄÅ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÄÅÊò†Áîª„ÄÅËá™ÁÑ∂„Å™‰ºöË©±„Å™„Å©„ÅÆ„Åï„Åæ„Åñ„Åæ„Å™„Çπ„Çø„Ç§„É´„ÅÆ„Éá„Éº„Çø„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Çø„Éº„Ç≤„ÉÉ„Éà„Éá„Éº„Çø„ÅÆÂæÆË™øÊï¥„ÇíË°å„Çè„Å™„ÅÑÂâçÊèê„Åß„ÄÅSenseVoice„ÅØ„ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅßÁèæÂú®„ÅÆÊúÄËâØ„ÅÆÊÑüÊÉÖË™çË≠ò„É¢„Éá„É´„ÅÆÂäπÊûú„ÇíÈÅîÊàê„Åä„Çà„Å≥‰∏äÂõû„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åó„Åü„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/ser_table.png\" width=\"1000\" />\n</div>\n\n„Åï„Çâ„Å´„ÄÅ„ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„ÅßË§áÊï∞„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„ÅÆÊÑüÊÉÖË™çË≠ò„É¢„Éá„É´„ÇíÊØîËºÉ„Åó„ÄÅÁµêÊûú„ÅØSenseVoice-Large„É¢„Éá„É´„Åå„Åª„Åº„Åô„Åπ„Å¶„ÅÆ„Éá„Éº„Çø„ÅßÊúÄËâØ„ÅÆÂäπÊûú„ÇíÈÅîÊàê„Åó„ÄÅSenseVoice-Small„É¢„Éá„É´„ÇÇÂ§öÊï∞„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß‰ªñ„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„É¢„Éá„É´„Çí‰∏äÂõû„ÇãÂäπÊûú„ÇíÈÅîÊàê„Åó„Åü„Åì„Å®„ÇíÁ§∫„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/ser_figure.png\" width=\"500\" />\n</div>\n\n## „Ç§„Éô„É≥„ÉàÊ§úÂá∫\n\nSenseVoice„ÅØÈü≥Â£∞„Éá„Éº„Çø„ÅÆ„Åø„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ„Ç§„Éô„É≥„ÉàÊ§úÂá∫„É¢„Éá„É´„Å®„Åó„Å¶ÂçòÁã¨„Åß‰ΩøÁî®„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇÁí∞Â¢ÉÈü≥ÂàÜÈ°ûESC-50„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÄÅÁèæÂú®Ê•≠Áïå„ÅßÂ∫É„Åè‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„ÇãBEATS„Åä„Çà„Å≥PANN„É¢„Éá„É´„ÅÆÂäπÊûú„Å®ÊØîËºÉ„Åó„Åæ„Åó„Åü„ÄÇSenseVoice„É¢„Éá„É´„ÅØ„Åì„Çå„Çâ„ÅÆ„Çø„Çπ„ÇØ„ÅßËâØÂ•Ω„Å™ÂäπÊûú„ÇíÈÅîÊàê„Åó„Åæ„Åó„Åü„Åå„ÄÅ„Éà„É¨„Éº„Éã„É≥„Ç∞„Éá„Éº„Çø„Å®„Éà„É¨„Éº„Éã„É≥„Ç∞ÊñπÊ≥ï„ÅÆÂà∂Á¥Ñ„Å´„Çà„Çä„ÄÅ„Ç§„Éô„É≥„ÉàÂàÜÈ°û„ÅÆÂäπÊûú„ÅØÂ∞ÇÈñÄ„ÅÆ„Ç§„Éô„É≥„ÉàÊ§úÂá∫„É¢„Éá„É´„Å®ÊØîËºÉ„Åó„Å¶„Åæ„Å†‰∏ÄÂÆö„ÅÆÂ∑Æ„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/aed_figure.png\" width=\"500\" />\n</div>\n\n## Êé®Ë´ñÂäπÁéá\n\nSenseVoice-small„É¢„Éá„É´„ÅØÈùûËá™Â∑±ÂõûÂ∏∞„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÊé°Áî®„Åó„Å¶„Åä„Çä„ÄÅÊé®Ë´ñÈÅÖÂª∂„ÅåÈùûÂ∏∏„Å´‰Ωé„ÅÑ„Åß„Åô„ÄÇWhisper-Small„É¢„Éá„É´„Å®ÂêåÁ≠â„ÅÆ„Éë„É©„É°„Éº„ÇøÈáè„Åß„ÄÅWhisper-Small„É¢„Éá„É´„Çà„Çä5ÂÄçÈ´òÈÄü„Åß„ÄÅWhisper-Large„É¢„Éá„É´„Çà„Çä15ÂÄçÈ´òÈÄü„Åß„Åô„ÄÇÂêåÊôÇ„Å´„ÄÅSenseVoice-small„É¢„Éá„É´„ÅØÈü≥Â£∞„ÅÆÈï∑„Åï„ÅåÂ¢óÂä†„Åó„Å¶„ÇÇ„ÄÅÊé®Ë´ñÊôÇÈñì„Å´Êòé„Çâ„Åã„Å™Â¢óÂä†„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/inference.png\" width=\"1000\" />\n</div>\n\n<a name=\"ÁéØÂ¢ÉÂÆâË£Ö\"></a>\n# Áí∞Â¢É„Ç§„É≥„Çπ„Éà„Éº„É´ üêç\n\n```shell\npip install -r requirements.txt\n```\n\n<a name=\"Áî®Ê≥ïÊïôÁ®ã\"></a>\n# ‰ΩøÁî®ÊñπÊ≥ï üõ†Ô∏è\n\n## Êé®Ë´ñ\n\n‰ªªÊÑè„ÅÆÂΩ¢Âºè„ÅÆÈü≥Â£∞ÂÖ•Âäõ„Çí„Çµ„Éù„Éº„Éà„Åó„ÄÅ‰ªªÊÑè„ÅÆÈï∑„Åï„ÅÆÂÖ•Âäõ„Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ\n\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",  \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n<details><summary>„Éë„É©„É°„Éº„Çø„ÅÆË™¨ÊòéÔºà„ÇØ„É™„ÉÉ„ÇØ„Åó„Å¶Â±ïÈñãÔºâ</summary>\n\n- `model_dir`Ôºö„É¢„Éá„É´Âêç„ÄÅ„Åæ„Åü„ÅØ„É≠„Éº„Ç´„É´„Éá„Ç£„Çπ„ÇØ‰∏ä„ÅÆ„É¢„Éá„É´„Éë„Çπ„ÄÇ\n- `trust_remote_code`Ôºö\n  - `True`„ÅØ„ÄÅmodel„Ç≥„Éº„Éâ„ÅÆÂÆüË£Ö„Åå`remote_code`„Åã„Çâ„É≠„Éº„Éâ„Åï„Çå„Çã„Åì„Å®„ÇíÊÑèÂë≥„Åó„ÄÅ`remote_code`„ÅØ`model`„Ç≥„Éº„Éâ„ÅÆÊ≠£Á¢∫„Å™‰ΩçÁΩÆ„ÇíÊåáÂÆö„Åó„Åæ„ÅôÔºà‰æãÔºöÁèæÂú®„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ`model.py`Ôºâ„ÄÇÁµ∂ÂØæ„Éë„Çπ„ÄÅÁõ∏ÂØæ„Éë„Çπ„ÄÅ„Åä„Çà„Å≥„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØURL„Çí„Çµ„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ\n  - `False`„ÅØ„ÄÅmodel„Ç≥„Éº„Éâ„ÅÆÂÆüË£Ö„Åå[FunASR](https://github.com/modelscope/FunASR)ÂÜÖÈÉ®„Å´Áµ±Âêà„Åï„Çå„Åü„Éê„Éº„Ç∏„Éß„É≥„Åß„ÅÇ„Çã„Åì„Å®„ÇíÊÑèÂë≥„Åó„ÄÅ„Åì„ÅÆÂ†¥Âêà„ÄÅÁèæÂú®„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ`model.py`„ÇíÂ§âÊõ¥„Åó„Å¶„ÇÇÂäπÊûú„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇFunASRÂÜÖÈÉ®„Éê„Éº„Ç∏„Éß„É≥„Åå„É≠„Éº„Éâ„Åï„Çå„Çã„Åü„ÇÅ„Åß„Åô„ÄÇ„É¢„Éá„É´„Ç≥„Éº„Éâ[„Åì„Å°„Çâ„ÇíÂèÇÁÖß](https://github.com/modelscope/FunASR/tree/main/funasr/models/sense_voice)„ÄÇ\n- `vad_model`ÔºöVADÔºàÈü≥Â£∞Ê¥ªÂãïÊ§úÂá∫Ôºâ„ÇíÊúâÂäπ„Å´„Åô„Çã„Åì„Å®„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇVAD„ÅÆÁõÆÁöÑ„ÅØ„ÄÅÈï∑„ÅÑÈü≥Â£∞„ÇíÁü≠„ÅÑ„ÇØ„É™„ÉÉ„Éó„Å´ÂàÜÂâ≤„Åô„Çã„Åì„Å®„Åß„Åô„ÄÇ„Åì„ÅÆÂ†¥Âêà„ÄÅÊé®Ë´ñÊôÇÈñì„Å´„ÅØVAD„Å®SenseVoice„ÅÆÂêàË®àÊ∂àË≤ª„ÅåÂê´„Åæ„Çå„ÄÅ„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„ÅÆÈÅÖÂª∂„ÇíË°®„Åó„Åæ„Åô„ÄÇSenseVoice„É¢„Éá„É´„ÅÆÊé®Ë´ñÊôÇÈñì„ÇíÂÄãÂà•„Å´„ÉÜ„Çπ„Éà„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅVAD„É¢„Éá„É´„ÇíÁÑ°Âäπ„Å´„Åß„Åç„Åæ„Åô„ÄÇ\n- `vad_kwargs`ÔºöVAD„É¢„Éá„É´„ÅÆË®≠ÂÆö„ÇíÊåáÂÆö„Åó„Åæ„Åô„ÄÇ`max_single_segment_time`Ôºö`vad_model`„Å´„Çà„ÇãÈü≥Â£∞„Çª„Ç∞„É°„É≥„Éà„ÅÆÊúÄÂ§ßÈï∑„ÇíÁ§∫„Åó„ÄÅÂçò‰Ωç„ÅØ„Éü„É™ÁßíÔºàmsÔºâ„Åß„Åô„ÄÇ\n- `use_itn`ÔºöÂá∫ÂäõÁµêÊûú„Å´Âè•Ë™≠ÁÇπ„Å®ÈÄÜ„ÉÜ„Ç≠„Çπ„ÉàÊ≠£Ë¶èÂåñ„ÅåÂê´„Åæ„Çå„Çã„Åã„Å©„ÅÜ„Åã„ÄÇ\n- `batch_size_s`ÔºöÂãïÁöÑ„Éê„ÉÉ„ÉÅ„ÅÆ‰ΩøÁî®„ÇíÁ§∫„Åó„ÄÅ„Éê„ÉÉ„ÉÅÂÜÖ„ÅÆÈü≥Â£∞„ÅÆÂêàË®àÈï∑„ÇíÁßíÔºàsÔºâ„ÅßÊ∏¨ÂÆö„Åó„Åæ„Åô„ÄÇ\n- `merge_vad`ÔºöVAD„É¢„Éá„É´„Å´„Çà„Å£„Å¶ÂàÜÂâ≤„Åï„Çå„ÅüÁü≠„ÅÑÈü≥Â£∞„Éï„É©„Ç∞„É°„É≥„Éà„Çí„Éû„Éº„Ç∏„Åô„Çã„Åã„Å©„ÅÜ„Åã„ÄÇ„Éû„Éº„Ç∏Âæå„ÅÆÈï∑„Åï„ÅØ`merge_length_s`„Åß„ÄÅÂçò‰Ωç„ÅØÁßíÔºàsÔºâ„Åß„Åô„ÄÇ\n- `ban_emo_unk`Ôºöemo_unk„É©„Éô„É´„ÇíÁÑ°Âäπ„Å´„Åô„Çã„ÄÇ\n</details>\n\n„Åô„Åπ„Å¶„ÅÆÂÖ•Âäõ„ÅåÁü≠„ÅÑÈü≥Â£∞Ôºà30ÁßíÊú™Ê∫ÄÔºâ„Åß„ÅÇ„Çä„ÄÅ„Éê„ÉÉ„ÉÅÊé®Ë´ñ„ÅåÂøÖË¶Å„Å™Â†¥Âêà„ÄÅÊé®Ë´ñÂäπÁéá„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´VAD„É¢„Éá„É´„ÇíÂâäÈô§„Åó„ÄÅ`batch_size`„ÇíË®≠ÂÆö„Åß„Åç„Åæ„Åô„ÄÇ\n\n```python\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size=64, \n)\n```\n\nË©≥Á¥∞„Å™‰ΩøÁî®ÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ[„Éâ„Ç≠„É•„É°„É≥„Éà](https://github.com/modelscope/FunASR/blob/main/docs/tutorial/README.md)„ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n### Áõ¥Êé•Êé®Ë´ñ\n\n‰ªªÊÑè„ÅÆÂΩ¢Âºè„ÅÆÈü≥Â£∞ÂÖ•Âäõ„Çí„Çµ„Éù„Éº„Éà„Åó„ÄÅÂÖ•ÂäõÈü≥Â£∞„ÅÆÈï∑„Åï„ÅØ30Áßí‰ª•‰∏ã„Å´Âà∂Èôê„Åï„Çå„Åæ„Åô„ÄÇ\n\n```python\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\n```\n\n## „Çµ„Éº„Éì„ÇπÂ±ïÈñã\n\nÊú™ÂÆå‰∫Ü\n\n### „Ç®„ÇØ„Çπ„Éù„Éº„Éà„Å®„ÉÜ„Çπ„Éà\n<details><summary>ONNX„Å®Libtorch„ÅÆ„Ç®„ÇØ„Çπ„Éù„Éº„Éà</summary>\n\n#### ONNX\n```python\n# pip3 install -U funasr funasr-onnx\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\nÂÇôËÄÉÔºöONNX„É¢„Éá„É´„ÅØÂÖÉ„ÅÆ„É¢„Éá„É´„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åï„Çå„Åæ„Åô„ÄÇ\n\n#### Libtorch\n```python\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\nÂÇôËÄÉÔºöLibtorch„É¢„Éá„É´„ÅØÂÖÉ„ÅÆ„É¢„Éá„É´„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åï„Çå„Åæ„Åô„ÄÇ\n\n</details>\n\n### Â±ïÈñã\n\n### FastAPI„Çí‰Ωø„Å£„ÅüÂ±ïÈñã\n```shell\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n## ÂæÆË™øÊï¥\n\n### „Éà„É¨„Éº„Éã„É≥„Ç∞Áí∞Â¢É„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´\n\n```shell\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n### „Éá„Éº„ÇøÊ∫ñÂÇô\n\n„Éá„Éº„Çø‰æã\n```text\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\nË©≥Á¥∞„ÅØ `data/train_example.jsonl` „ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n<details><summary>„Éá„Éº„ÇøÊ∫ñÂÇô„ÅÆË©≥Á¥∞</summary>\n\nË™¨ÊòéÔºö\n- `key`ÔºöÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÅÆ„É¶„Éã„Éº„ÇØID\n- `source`ÔºöÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ\n- `source_len`ÔºöÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÅÆfbank„Éï„É¨„Éº„É†Êï∞\n- `target`ÔºöÊñáÂ≠óËµ∑„Åì„ÅóÁµêÊûú\n- `target_len`ÔºötargetÔºàÊñáÂ≠óËµ∑„Åì„ÅóÔºâ„ÅÆÈï∑„Åï\n- `text_language`ÔºöÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÅÆË®ÄË™ûID\n- `emo_target`ÔºöÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÅÆÊÑüÊÉÖ„É©„Éô„É´\n- `event_target`ÔºöÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÅÆ„Ç§„Éô„É≥„Éà„É©„Éô„É´\n- `with_or_wo_itn`ÔºöÂè•Ë™≠ÁÇπ„Å®ÈÄÜ„ÉÜ„Ç≠„Çπ„ÉàÊ≠£Ë¶èÂåñ„ÇíÂê´„ÇÄ„Åã„Å©„ÅÜ„Åã\n\n`train_text.txt`\n```bash\nBAC009S0764W0121 ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ\nBAC009S0916W0489 ÊπñÂåó‰∏ÄÂÖ¨Âè∏‰ª•ÂëòÂ∑•Âêç‰πâË¥∑Ê¨æÊï∞ÂçÅÂëòÂ∑•Ë¥üÂÄ∫ÂçÉ‰∏á\nasr_example_cn_en ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning ÂÅö data analytics ÂÅö data science ‰πüÂ•Ω scientist ‰πüÂ•ΩÈÄöÈÄöÈÉΩË¶ÅÈÉΩÂÅöÁöÑÂü∫Êú¨ÂäüÂïäÈÇ£ again ÂÖàÂÖàÂØπÊúâ‰∏Ä‰∫õ>‰πüËÆ∏ÂØπ\nID0012W0014 he tried to think how it could be\n```\n`train_wav.scp`\n```bash\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n`train_text_language.txt`\nË®ÄË™ûID„ÅØ `<|zh|>`„ÄÅ`<|en|>`„ÄÅ`<|yue|>`„ÄÅ`<|ja|>`„ÄÅ„Åä„Çà„Å≥ `<|ko|>`„ÇíÂê´„Åø„Åæ„Åô„ÄÇ\n```bash\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n`train_emo.txt`\nÊÑüÊÉÖ„É©„Éô„É´„ÅØ„ÄÅ`<|HAPPY|>`„ÄÅ`<|SAD|>`„ÄÅ`<|ANGRY|>`„ÄÅ`<|NEUTRAL|>`„ÄÅ`<|FEARFUL|>`„ÄÅ`<|DISGUSTED|>` „Åä„Çà„Å≥ `<|SURPRISED|>`„ÇíÂê´„Åø„Åæ„Åô„ÄÇ\n```bash\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n`train_event.txt`\n„Ç§„Éô„É≥„Éà„É©„Éô„É´„ÅØ„ÄÅ `<|BGM|>`„ÄÅ`<|Speech|>`„ÄÅ`<|Applause|>`„ÄÅ`<|Laughter|>`„ÄÅ`<|Cry|>`„ÄÅ`<|Sneeze|>`„ÄÅ`<|Breath|>` „Åä„Çà„Å≥ `<|Cough|>`„ÇíÂê´„Åø„Åæ„Åô„ÄÇ\n```bash\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```\n`„Ç≥„Éû„É≥„Éâ`\n```shell\n# wav.scp„ÄÅtext.txt„ÄÅtext_language.txt„ÄÅemo_target.txt„ÄÅevent_target.txt „Åã„Çâ train.jsonl „Å® val.jsonl „ÇíÁîüÊàê„Åó„Åæ„Åô\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n`train_text_language.txt`„ÄÅ`train_emo_target.txt`„ÄÅ`train_event_target.txt` „Åå„Å™„ÅÑÂ†¥Âêà„ÄÅ`SenseVoice` „É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Å¶Ë®ÄË™û„ÄÅÊÑüÊÉÖ„ÄÅ„Åä„Çà„Å≥„Ç§„Éô„É≥„Éà„É©„Éô„É´„ÅåËá™ÂãïÁöÑ„Å´‰∫àÊ∏¨„Åï„Çå„Åæ„Åô„ÄÇ\n```shell\n# wav.scp „Å® text.txt „Åã„Çâ train.jsonl „Å® val.jsonl „ÇíÁîüÊàê„Åó„Åæ„Åô\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n</details>\n\n### „Éà„É¨„Éº„Éã„É≥„Ç∞„ÅÆÈñãÂßã\n\n`finetune.sh`„ÅÆ`train_tool`„Çí„ÄÅÂâçËø∞„ÅÆFunASR„Éë„ÇπÂÜÖ„ÅÆ`funasr/bin/train_ds.py`„ÅÆÁµ∂ÂØæ„Éë„Çπ„Å´Â§âÊõ¥„Åô„Çã„Åì„Å®„ÇíÂøò„Çå„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n```shell\nbash finetune.sh\n```\n\n## WebUI\n\n```shell\npython webui.py\n```\n\n<div align=\"center\"><img src=\"image/webui.png\" width=\"700\"/> </div>\n\n## Ê≥®ÁõÆ„Åô„Åπ„Åç„Çµ„Éº„Éâ„Éë„Éº„ÉÜ„Ç£„ÅÆÂèñ„ÇäÁµÑ„Åø\n- Triton (GPU) „Éá„Éó„É≠„Ç§„É°„É≥„Éà„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„ÇπÔºöTriton + TensorRT „Çí‰ΩøÁî®„Åó„ÄÅFP32 „Åß„ÉÜ„Çπ„Éà„ÄÇV100 GPU „ÅßÂä†ÈÄüÊØî 526 „ÇíÈÅîÊàê„ÄÇFP16 „ÅÆ„Çµ„Éù„Éº„Éà„ÅØÈÄ≤Ë°å‰∏≠„Åß„Åô„ÄÇ[„É™„Éù„Ç∏„Éà„É™](https://github.com/modelscope/FunASR/blob/main/runtime/triton_gpu/README.md)\n- Sherpa-onnx „Éá„Éó„É≠„Ç§„É°„É≥„Éà„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„ÇπÔºöSenseVoice „Çí10Á®ÆÈ°û„ÅÆ„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™ûÔºàC++, C, Python, C#, Go, Swift, Kotlin, Java, JavaScript, DartÔºâ„Åß‰ΩøÁî®ÂèØËÉΩ„ÄÇ„Åæ„Åü„ÄÅiOS, Android, Raspberry Pi „Å™„Å©„ÅÆ„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†„Åß„ÇÇ SenseVoice „Çí„Éá„Éó„É≠„Ç§„Åß„Åç„Åæ„Åô„ÄÇ[„É™„Éù„Ç∏„Éà„É™](https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html)\n- [SenseVoice.cpp](https://github.com/lovemefan/SenseVoice.cpp) GGML„Å´Âü∫„Å•„ÅÑ„Å¶Á¥îÁ≤ã„Å™C/C++„ÅßSenseVoice„ÇíÊé®Ê∏¨„Åó„ÄÅ3„Éì„ÉÉ„Éà„ÄÅ4„Éì„ÉÉ„Éà„ÄÅ5„Éì„ÉÉ„Éà„ÄÅ8„Éì„ÉÉ„ÉàÈáèÂ≠êÂåñ„Å™„Å©„Çí„Çµ„Éù„Éº„Éà„Åó„ÄÅ„Çµ„Éº„Éâ„Éë„Éº„ÉÜ„Ç£„ÅÆ‰æùÂ≠òÈñ¢‰øÇ„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n- [streaming-sensevoice](https://github.com/pengzhendong/streaming-sensevoice) „Çπ„Éà„É™„Éº„É†ÂûãSenseVoice„ÅØ„ÄÅ„ÉÅ„É£„É≥„ÇØÔºàchunkÔºâÊñπÂºè„ÅßÊé®Ë´ñ„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇÊì¨‰ºº„Çπ„Éà„É™„Éº„Éü„É≥„Ç∞Âá¶ÁêÜ„ÇíÂÆüÁèæ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ‰∏ÄÈÉ®„ÅÆÁ≤æÂ∫¶„ÇíÁä†Áâ≤„Å´„Åó„Å¶Âàá„ÇäÊç®„Å¶Ê≥®ÊÑèÊ©üÊßãÔºàtruncated attentionÔºâ„ÇíÊé°Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åï„Çâ„Å´„ÄÅ„Åì„ÅÆÊäÄË°ì„ÅØCTC„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ„Éì„Éº„É†„Çµ„Éº„ÉÅÔºàCTC prefix beam searchÔºâ„Å®„Éõ„ÉÉ„Éà„ÉØ„Éº„ÉâÂº∑ÂåñÊ©üËÉΩ„ÇÇ„Çµ„Éù„Éº„Éà„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n- [OmniSenseVoice](https://github.com/lifeiteng/OmniSenseVoice) „ÅØ„ÄÅË∂ÖÈ´òÈÄüÊé®Ë´ñ„Å®„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÅÆ„Åü„ÇÅ„Å´ÊúÄÈÅ©Âåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n# „ÅäÂïè„ÅÑÂêà„Çè„Åõ\n\n‰ΩøÁî®‰∏≠„Å´ÂïèÈ°å„ÅåÁô∫Áîü„Åó„ÅüÂ†¥Âêà„ÅØ„ÄÅgithub„Éö„Éº„Ç∏„ÅßÁõ¥Êé•Issues„ÇíÊèêËµ∑„Åß„Åç„Åæ„Åô„ÄÇÈü≥Â£∞„Å´ËààÂë≥„ÅÆ„ÅÇ„ÇãÊñπ„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆDingTalk„Ç∞„É´„Éº„ÉóQR„Ç≥„Éº„Éâ„Çí„Çπ„Ç≠„É£„É≥„Åó„Å¶„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„Ç∞„É´„Éº„Éó„Å´ÂèÇÂä†„Åó„ÄÅ‰∫§ÊµÅ„Å®Ë≠∞Ë´ñ„ÇíË°å„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n|                          FunASR                          |\n|:--------------------------------------------------------:|\n| <img src=\"image/dingding_funasr.png\" width=\"250\"/></div> |\n\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 16.888671875,
          "content": "# SenseVoice\n\n„ÄåÁÆÄ‰Ωì‰∏≠Êñá„Äç|„Äå[English](./README.md)„Äç|„Äå[Êó•Êú¨Ë™û](./README_ja.md)„Äç\n\nSenseVoice ÊòØÂÖ∑ÊúâÈü≥È¢ëÁêÜËß£ËÉΩÂäõÁöÑÈü≥È¢ëÂü∫Á°ÄÊ®°ÂûãÔºåÂåÖÊã¨ËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâ„ÄÅËØ≠ÁßçËØÜÂà´ÔºàLIDÔºâ„ÄÅËØ≠Èü≥ÊÉÖÊÑüËØÜÂà´ÔºàSERÔºâÂíåÂ£∞Â≠¶‰∫ã‰ª∂ÂàÜÁ±ªÔºàAECÔºâÊàñÂ£∞Â≠¶‰∫ã‰ª∂Ê£ÄÊµãÔºàAEDÔºâ„ÄÇÊú¨È°πÁõÆÊèê‰æõ SenseVoice Ê®°ÂûãÁöÑ‰ªãÁªç‰ª•ÂèäÂú®Â§ö‰∏™‰ªªÂä°ÊµãËØïÈõÜ‰∏äÁöÑ benchmarkÔºå‰ª•Âèä‰ΩìÈ™åÊ®°ÂûãÊâÄÈúÄÁöÑÁéØÂ¢ÉÂÆâË£ÖÁöÑ‰∏éÊé®ÁêÜÊñπÂºè„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/sensevoice2.png\">\n</div>\n\n<div align=\"center\">  \n<h4>\n<a href=\"https://funaudiollm.github.io/\"> Homepage </a>\nÔΩú<a href=\"#ÊúÄÊñ∞Âä®ÊÄÅ\"> ÊúÄÊñ∞Âä®ÊÄÅ </a>\nÔΩú<a href=\"#ÊÄßËÉΩËØÑÊµã\"> ÊÄßËÉΩËØÑÊµã </a>\nÔΩú<a href=\"#ÁéØÂ¢ÉÂÆâË£Ö\"> ÁéØÂ¢ÉÂÆâË£Ö </a>\nÔΩú<a href=\"#Áî®Ê≥ïÊïôÁ®ã\"> Áî®Ê≥ïÊïôÁ®ã </a>\nÔΩú<a href=\"#ËÅîÁ≥ªÊàë‰ª¨\"> ËÅîÁ≥ªÊàë‰ª¨ </a>\n\n</h4>\n\nÊ®°Âûã‰ªìÂ∫ìÔºö[modelscope](https://www.modelscope.cn/models/iic/SenseVoiceSmall)Ôºå[huggingface](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)\n\nÂú®Á∫ø‰ΩìÈ™åÔºö\n[modelscope demo](https://www.modelscope.cn/studios/iic/SenseVoice), [huggingface space](https://huggingface.co/spaces/FunAudioLLM/SenseVoice)\n\n</div>\n\n<a name=\"Ê†∏ÂøÉÂäüËÉΩ\"></a>\n\n# Ê†∏ÂøÉÂäüËÉΩ üéØ\n\n**SenseVoice** ‰∏ìÊ≥®‰∫éÈ´òÁ≤æÂ∫¶Â§öËØ≠Ë®ÄËØ≠Èü≥ËØÜÂà´„ÄÅÊÉÖÊÑüËæ®ËØÜÂíåÈü≥È¢ë‰∫ã‰ª∂Ê£ÄÊµã\n\n- **Â§öËØ≠Ë®ÄËØÜÂà´Ôºö** ÈááÁî®Ë∂ÖËøá 40 ‰∏áÂ∞èÊó∂Êï∞ÊçÆËÆ≠ÁªÉÔºåÊîØÊåÅË∂ÖËøá 50 ÁßçËØ≠Ë®ÄÔºåËØÜÂà´ÊïàÊûú‰∏ä‰ºò‰∫é Whisper Ê®°Âûã„ÄÇ\n- **ÂØåÊñáÊú¨ËØÜÂà´Ôºö**\n  - ÂÖ∑Â§á‰ºòÁßÄÁöÑÊÉÖÊÑüËØÜÂà´ÔºåËÉΩÂ§üÂú®ÊµãËØïÊï∞ÊçÆ‰∏äËææÂà∞ÂíåË∂ÖËøáÁõÆÂâçÊúÄ‰Ω≥ÊÉÖÊÑüËØÜÂà´Ê®°ÂûãÁöÑÊïàÊûú„ÄÇ\n  - ÊîØÊåÅÂ£∞Èü≥‰∫ã‰ª∂Ê£ÄÊµãËÉΩÂäõÔºåÊîØÊåÅÈü≥‰πê„ÄÅÊéåÂ£∞„ÄÅÁ¨ëÂ£∞„ÄÅÂì≠Â£∞„ÄÅÂí≥ÂóΩ„ÄÅÂñ∑ÂöèÁ≠âÂ§öÁßçÂ∏∏ËßÅ‰∫∫Êú∫‰∫§‰∫í‰∫ã‰ª∂ËøõË°åÊ£ÄÊµã„ÄÇ\n- **È´òÊïàÊé®ÁêÜÔºö** SenseVoice-Small Ê®°ÂûãÈááÁî®ÈùûËá™ÂõûÂΩíÁ´ØÂà∞Á´ØÊ°ÜÊû∂ÔºåÊé®ÁêÜÂª∂ËøüÊûÅ‰ΩéÔºå10s Èü≥È¢ëÊé®ÁêÜ‰ªÖËÄóÊó∂ 70msÔºå15 ÂÄç‰ºò‰∫é Whisper-Large„ÄÇ\n- **ÂæÆË∞ÉÂÆöÂà∂Ôºö** ÂÖ∑Â§á‰æøÊç∑ÁöÑÂæÆË∞ÉËÑöÊú¨‰∏éÁ≠ñÁï•ÔºåÊñπ‰æøÁî®Êà∑Ê†πÊçÆ‰∏öÂä°Âú∫ÊôØ‰øÆÂ§çÈïøÂ∞æÊ†∑Êú¨ÈóÆÈ¢ò„ÄÇ\n- **ÊúçÂä°ÈÉ®ÁΩ≤Ôºö** ÂÖ∑ÊúâÂÆåÊï¥ÁöÑÊúçÂä°ÈÉ®ÁΩ≤ÈìæË∑ØÔºåÊîØÊåÅÂ§öÂπ∂ÂèëËØ∑Ê±ÇÔºåÊîØÊåÅÂÆ¢Êà∑Á´ØËØ≠Ë®ÄÊúâÔºåpython„ÄÅc++„ÄÅhtml„ÄÅjava ‰∏é c# Á≠â„ÄÇ\n\n<a name=\"ÊúÄÊñ∞Âä®ÊÄÅ\"></a>\n\n# ÊúÄÊñ∞Âä®ÊÄÅ üî•\n\n- 2024/7ÔºöÊñ∞Â¢ûÂä†ÂØºÂá∫ [ONNX](./demo_onnx.py) ‰∏é [libtorch](./demo_libtorch.py) ÂäüËÉΩÔºå‰ª•Âèä python ÁâàÊú¨ runtimeÔºö[funasr-onnx-0.4.0](https://pypi.org/project/funasr-onnx/)Ôºå[funasr-torch-0.1.1](https://pypi.org/project/funasr-torch/)\n- 2024/7: [SenseVoice-Small](https://www.modelscope.cn/models/iic/SenseVoiceSmall) Â§öËØ≠Ë®ÄÈü≥È¢ëÁêÜËß£Ê®°ÂûãÂºÄÊ∫êÔºåÊîØÊåÅ‰∏≠„ÄÅÁ≤§„ÄÅËã±„ÄÅÊó•„ÄÅÈü©ËØ≠ÁöÑÂ§öËØ≠Ë®ÄËØ≠Èü≥ËØÜÂà´ÔºåÊÉÖÊÑüËØÜÂà´Âíå‰∫ã‰ª∂Ê£ÄÊµãËÉΩÂäõÔºåÂÖ∑ÊúâÊûÅ‰ΩéÁöÑÊé®ÁêÜÂª∂Ëøü„ÄÇ„ÄÇ\n- 2024/7: CosyVoice Ëá¥Âäõ‰∫éËá™ÁÑ∂ËØ≠Èü≥ÁîüÊàêÔºåÊîØÊåÅÂ§öËØ≠Ë®Ä„ÄÅÈü≥Ëâ≤ÂíåÊÉÖÊÑüÊéßÂà∂ÔºåÊìÖÈïøÂ§öËØ≠Ë®ÄËØ≠Èü≥ÁîüÊàê„ÄÅÈõ∂Ê†∑Êú¨ËØ≠Èü≥ÁîüÊàê„ÄÅË∑®ËØ≠Ë®ÄËØ≠Èü≥ÂÖãÈöÜ‰ª•ÂèäÈÅµÂæ™Êåá‰ª§ÁöÑËÉΩÂäõ„ÄÇ[CosyVoice repo](https://github.com/FunAudioLLM/CosyVoice) and [CosyVoice Âú®Á∫ø‰ΩìÈ™å](https://www.modelscope.cn/studios/iic/CosyVoice-300M).\n- 2024/7: [FunASR](https://github.com/modelscope/FunASR) ÊòØ‰∏Ä‰∏™Âü∫Á°ÄËØ≠Èü≥ËØÜÂà´Â∑•ÂÖ∑ÂåÖÔºåÊèê‰æõÂ§öÁßçÂäüËÉΩÔºåÂåÖÊã¨ËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâ„ÄÅËØ≠Èü≥Á´ØÁÇπÊ£ÄÊµãÔºàVADÔºâ„ÄÅÊ†áÁÇπÊÅ¢Â§ç„ÄÅËØ≠Ë®ÄÊ®°Âûã„ÄÅËØ¥ËØù‰∫∫È™åËØÅ„ÄÅËØ¥ËØù‰∫∫ÂàÜÁ¶ªÂíåÂ§ö‰∫∫ÂØπËØùËØ≠Èü≥ËØÜÂà´Á≠â„ÄÇ\n\n<a name=\"Benchmarks\"></a>\n\n# ÊÄßËÉΩËØÑÊµã üìù\n\n## Â§öËØ≠Ë®ÄËØ≠Èü≥ËØÜÂà´\n\nÊàë‰ª¨Âú®ÂºÄÊ∫êÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºàÂåÖÊã¨ AISHELL-1„ÄÅAISHELL-2„ÄÅWenetspeech„ÄÅLibrispeech Âíå Common VoiceÔºâ‰∏äÊØîËæÉ‰∫Ü SenseVoice ‰∏é Whisper ÁöÑÂ§öËØ≠Ë®ÄËØ≠Èü≥ËØÜÂà´ÊÄßËÉΩÂíåÊé®ÁêÜÊïàÁéá„ÄÇÂú®‰∏≠ÊñáÂíåÁ≤§ËØ≠ËØÜÂà´ÊïàÊûú‰∏äÔºåSenseVoice-Small Ê®°ÂûãÂÖ∑ÊúâÊòéÊòæÁöÑÊïàÊûú‰ºòÂäø„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/asr_results1.png\" width=\"400\" /><img src=\"image/asr_results2.png\" width=\"400\" />\n</div>\n\n## ÊÉÖÊÑüËØÜÂà´\n\nÁî±‰∫éÁõÆÂâçÁº∫‰πèË¢´ÂπøÊ≥õ‰ΩøÁî®ÁöÑÊÉÖÊÑüËØÜÂà´ÊµãËØïÊåáÊ†áÂíåÊñπÊ≥ïÔºåÊàë‰ª¨Âú®Â§ö‰∏™ÊµãËØïÈõÜÁöÑÂ§öÁßçÊåáÊ†áËøõË°åÊµãËØïÔºåÂπ∂‰∏éËøëÂπ¥Êù• Benchmark ‰∏äÁöÑÂ§ö‰∏™ÁªìÊûúËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂØπÊØî„ÄÇÊâÄÈÄâÂèñÁöÑÊµãËØïÈõÜÂêåÊó∂ÂåÖÂê´‰∏≠Êñá / Ëã±Êñá‰∏§ÁßçËØ≠Ë®Ä‰ª•ÂèäË°®Êºî„ÄÅÂΩ±ËßÜÂâß„ÄÅËá™ÁÑ∂ÂØπËØùÁ≠âÂ§öÁßçÈ£éÊ†ºÁöÑÊï∞ÊçÆÔºåÂú®‰∏çËøõË°åÁõÆÊ†áÊï∞ÊçÆÂæÆË∞ÉÁöÑÂâçÊèê‰∏ãÔºåSenseVoice ËÉΩÂ§üÂú®ÊµãËØïÊï∞ÊçÆ‰∏äËææÂà∞ÂíåË∂ÖËøáÁõÆÂâçÊúÄ‰Ω≥ÊÉÖÊÑüËØÜÂà´Ê®°ÂûãÁöÑÊïàÊûú„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/ser_table.png\" width=\"1000\" />\n</div>\n\nÂêåÊó∂ÔºåÊàë‰ª¨ËøòÂú®ÊµãËØïÈõÜ‰∏äÂØπÂ§ö‰∏™ÂºÄÊ∫êÊÉÖÊÑüËØÜÂà´Ê®°ÂûãËøõË°åÂØπÊØîÔºåÁªìÊûúË°®ÊòéÔºåSenseVoice-Large Ê®°ÂûãÂèØ‰ª•Âú®Âá†‰πéÊâÄÊúâÊï∞ÊçÆ‰∏äÈÉΩËææÂà∞‰∫ÜÊúÄ‰Ω≥ÊïàÊûúÔºåËÄå SenseVoice-Small Ê®°ÂûãÂêåÊ†∑ÂèØ‰ª•Âú®Â§öÊï∞Êï∞ÊçÆÈõÜ‰∏äÂèñÂæóË∂ÖË∂äÂÖ∂‰ªñÂºÄÊ∫êÊ®°ÂûãÁöÑÊïàÊûú„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/ser_figure.png\" width=\"500\" />\n</div>\n\n## ‰∫ã‰ª∂Ê£ÄÊµã\n\nÂ∞ΩÁÆ° SenseVoice Âè™Âú®ËØ≠Èü≥Êï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºåÂÆÉ‰ªçÁÑ∂ÂèØ‰ª•‰Ωú‰∏∫‰∫ã‰ª∂Ê£ÄÊµãÊ®°ÂûãËøõË°åÂçïÁã¨‰ΩøÁî®„ÄÇÊàë‰ª¨Âú®ÁéØÂ¢ÉÈü≥ÂàÜÁ±ª ESC-50 Êï∞ÊçÆÈõÜ‰∏ä‰∏éÁõÆÂâç‰∏öÂÜÖÂπøÊ≥õ‰ΩøÁî®ÁöÑ BEATS ‰∏é PANN Ê®°ÂûãÁöÑÊïàÊûúËøõË°å‰∫ÜÂØπÊØî„ÄÇSenseVoice Ê®°ÂûãËÉΩÂ§üÂú®Ëøô‰∫õ‰ªªÂä°‰∏äÂèñÂæóËæÉÂ•ΩÁöÑÊïàÊûúÔºå‰ΩÜÂèóÈôê‰∫éËÆ≠ÁªÉÊï∞ÊçÆ‰∏éËÆ≠ÁªÉÊñπÂºèÔºåÂÖ∂‰∫ã‰ª∂ÂàÜÁ±ªÊïàÊûú‰∏ì‰∏öÁöÑ‰∫ã‰ª∂Ê£ÄÊµãÊ®°ÂûãÁõ∏ÊØî‰ªçÁÑ∂Êúâ‰∏ÄÂÆöÁöÑÂ∑ÆË∑ù„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/aed_figure.png\" width=\"500\" />\n</div>\n\n## Êé®ÁêÜÊïàÁéá\n\nSenseVoice-small Ê®°ÂûãÈááÁî®ÈùûËá™ÂõûÂΩíÁ´ØÂà∞Á´ØÊû∂ÊûÑÔºåÊé®ÁêÜÂª∂ËøüÊûÅ‰Ωé„ÄÇÂú®ÂèÇÊï∞Èáè‰∏é Whisper-Small Ê®°ÂûãÁõ∏ÂΩìÁöÑÊÉÖÂÜµ‰∏ãÔºåÊØî Whisper-Small Ê®°ÂûãÊé®ÁêÜÈÄüÂ∫¶Âø´ 5 ÂÄçÔºåÊØî Whisper-Large Ê®°ÂûãÂø´ 15 ÂÄç„ÄÇÂêåÊó∂ SenseVoice-small Ê®°ÂûãÂú®Èü≥È¢ëÊó∂ÈïøÂ¢ûÂä†ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊé®ÁêÜËÄóÊó∂‰πüÊó†ÊòéÊòæÂ¢ûÂä†„ÄÇ\n\n<div align=\"center\">  \n<img src=\"image/inference.png\" width=\"1000\" />\n</div>\n\n<a name=\"ÁéØÂ¢ÉÂÆâË£Ö\"></a>\n\n# ÂÆâË£Ö‰æùËµñÁéØÂ¢É üêç\n\n```shell\npip install -r requirements.txt\n```\n\n<a name=\"Áî®Ê≥ïÊïôÁ®ã\"></a>\n\n# Áî®Ê≥ï üõ†Ô∏è\n\n## Êé®ÁêÜ\n\n### ‰ΩøÁî® funasr Êé®ÁêÜ\n\nÊîØÊåÅ‰ªªÊÑèÊ†ºÂºèÈü≥È¢ëËæìÂÖ•ÔºåÊîØÊåÅ‰ªªÊÑèÊó∂ÈïøËæìÂÖ•\n\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",  \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n<details><summary> ÂèÇÊï∞ËØ¥ÊòéÔºàÁÇπÂáªÂ±ïÂºÄÔºâ</summary>\n\n- `model_dir`ÔºöÊ®°ÂûãÂêçÁß∞ÔºåÊàñÊú¨Âú∞Á£ÅÁõò‰∏≠ÁöÑÊ®°ÂûãË∑ØÂæÑ„ÄÇ\n- `trust_remote_code`Ôºö\n  - `True` Ë°®Á§∫ model ‰ª£Á†ÅÂÆûÁé∞‰ªé `remote_code` Â§ÑÂä†ËΩΩÔºå`remote_code` ÊåáÂÆö `model` ÂÖ∑‰Ωì‰ª£Á†ÅÁöÑ‰ΩçÁΩÆÔºà‰æãÂ¶ÇÔºåÂΩìÂâçÁõÆÂΩï‰∏ãÁöÑ `model.py`ÔºâÔºåÊîØÊåÅÁªùÂØπË∑ØÂæÑ‰∏éÁõ∏ÂØπË∑ØÂæÑÔºå‰ª•ÂèäÁΩëÁªú url„ÄÇ\n  - `False` Ë°®Á§∫Ôºåmodel ‰ª£Á†ÅÂÆûÁé∞‰∏∫ [FunASR](https://github.com/modelscope/FunASR) ÂÜÖÈÉ®ÈõÜÊàêÁâàÊú¨ÔºåÊ≠§Êó∂‰øÆÊîπÂΩìÂâçÁõÆÂΩï‰∏ãÁöÑ `model.py` ‰∏ç‰ºöÁîüÊïàÔºåÂõ†‰∏∫Âä†ËΩΩÁöÑÊòØ funasr ÂÜÖÈÉ®ÁâàÊú¨ÔºåÊ®°Âûã‰ª£Á†Å [ÁÇπÂáªÊü•Áúã](https://github.com/modelscope/FunASR/tree/main/funasr/models/sense_voice)„ÄÇ\n- `vad_model`ÔºöË°®Á§∫ÂºÄÂêØ VADÔºåVAD ÁöÑ‰ΩúÁî®ÊòØÂ∞ÜÈïøÈü≥È¢ëÂàáÂâ≤ÊàêÁü≠Èü≥È¢ëÔºåÊ≠§Êó∂Êé®ÁêÜËÄóÊó∂ÂåÖÊã¨‰∫Ü VAD ‰∏é SenseVoice ÊÄªËÄóÊó∂Ôºå‰∏∫ÈìæË∑ØËÄóÊó∂ÔºåÂ¶ÇÊûúÈúÄË¶ÅÂçïÁã¨ÊµãËØï SenseVoice Ê®°ÂûãËÄóÊó∂ÔºåÂèØ‰ª•ÂÖ≥Èó≠ VAD Ê®°Âûã„ÄÇ\n- `vad_kwargs`ÔºöË°®Á§∫ VAD Ê®°ÂûãÈÖçÁΩÆÔºå`max_single_segment_time`: Ë°®Á§∫ `vad_model` ÊúÄÂ§ßÂàáÂâ≤Èü≥È¢ëÊó∂ÈïøÔºåÂçï‰ΩçÊòØÊØ´Áßí ms„ÄÇ\n- `use_itn`ÔºöËæìÂá∫ÁªìÊûú‰∏≠ÊòØÂê¶ÂåÖÂê´Ê†áÁÇπ‰∏éÈÄÜÊñáÊú¨Ê≠£ÂàôÂåñ„ÄÇ\n- `batch_size_s` Ë°®Á§∫ÈááÁî®Âä®ÊÄÅ batchÔºåbatch ‰∏≠ÊÄªÈü≥È¢ëÊó∂ÈïøÔºåÂçï‰Ωç‰∏∫Áßí s„ÄÇ\n- `merge_vad`ÔºöÊòØÂê¶Â∞Ü vad Ê®°ÂûãÂàáÂâ≤ÁöÑÁü≠Èü≥È¢ëÁ¢éÁâáÂêàÊàêÔºåÂêàÂπ∂ÂêéÈïøÂ∫¶‰∏∫ `merge_length_s`ÔºåÂçï‰Ωç‰∏∫Áßí s„ÄÇ\n- `ban_emo_unk`ÔºöÁ¶ÅÁî® emo_unk Ê†áÁ≠æÔºåÁ¶ÅÁî®ÂêéÊâÄÊúâÁöÑÂè•Â≠êÈÉΩ‰ºöË¢´Ëµã‰∏éÊÉÖÊÑüÊ†áÁ≠æ„ÄÇÈªòËÆ§ `False`\n\n</details>\n\nÂ¶ÇÊûúËæìÂÖ•Âùá‰∏∫Áü≠Èü≥È¢ëÔºàÂ∞è‰∫é 30sÔºâÔºåÂπ∂‰∏îÈúÄË¶ÅÊâπÈáèÂåñÊé®ÁêÜÔºå‰∏∫‰∫ÜÂä†Âø´Êé®ÁêÜÊïàÁéáÔºåÂèØ‰ª•ÁßªÈô§ vad Ê®°ÂûãÔºåÂπ∂ËÆæÁΩÆ `batch_size`\n\n```python\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size=64, \n)\n```\n\nÊõ¥Â§öËØ¶ÁªÜÁî®Ê≥ïÔºåËØ∑ÂèÇËÄÉ [ÊñáÊ°£](https://github.com/modelscope/FunASR/blob/main/docs/tutorial/README.md)\n\n### Áõ¥Êé•Êé®ÁêÜ\n\nÊîØÊåÅ‰ªªÊÑèÊ†ºÂºèÈü≥È¢ëËæìÂÖ•ÔºåËæìÂÖ•Èü≥È¢ëÊó∂ÈïøÈôêÂà∂Âú® 30s ‰ª•‰∏ã\n\n```python\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs ['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res [0][0][\"text\"])\nprint(text)\n```\n\n## ÊúçÂä°ÈÉ®ÁΩ≤\n\nUndo\n\n### ÂØºÂá∫‰∏éÊµãËØï\n\n<details><summary>ONNX ‰∏é Libtorch ÂØºÂá∫ </summary>\n\n#### ONNX\n\n```python\n# pip3 install -U funasr funasr-onnx\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\n\nÂ§áÊ≥®ÔºöONNX Ê®°ÂûãÂØºÂá∫Âà∞ÂéüÊ®°ÂûãÁõÆÂΩï‰∏≠\n\n#### Libtorch\n\n```python\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess (i) for i in res])\n```\n\nÂ§áÊ≥®ÔºöLibtorch Ê®°ÂûãÂØºÂá∫Âà∞ÂéüÊ®°ÂûãÁõÆÂΩï‰∏≠\n\n</details>\n\n### ÈÉ®ÁΩ≤\n\n### ‰ΩøÁî® FastAPI ÈÉ®ÁΩ≤\n\n```shell\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n## ÂæÆË∞É\n\n### ÂÆâË£ÖËÆ≠ÁªÉÁéØÂ¢É\n\n```shell\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n### Êï∞ÊçÆÂáÜÂ§á\n\nÊï∞ÊçÆÊ†ºÂºèÈúÄË¶ÅÂåÖÊã¨Â¶Ç‰∏ãÂá†‰∏™Â≠óÊÆµÔºö\n\n```text\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\n\nËØ¶ÁªÜÂèØ‰ª•ÂèÇËÄÉÔºö`data/train_example.jsonl`\n\n<details><summary > Êï∞ÊçÆÂáÜÂ§áÁªÜËäÇ‰ªãÁªç </summary>\n\n- `key`: Êï∞ÊçÆÂîØ‰∏Ä ID\n- `source`ÔºöÈü≥È¢ëÊñá‰ª∂ÁöÑË∑ØÂæÑ\n- `source_len`ÔºöÈü≥È¢ëÊñá‰ª∂ÁöÑ fbank Â∏ßÊï∞\n- `target`ÔºöÈü≥È¢ëÊñá‰ª∂Ê†áÊ≥®ÊñáÊú¨\n- `target_len`ÔºöÈü≥È¢ëÊñá‰ª∂Ê†áÊ≥®ÊñáÊú¨ÈïøÂ∫¶\n- `text_language`ÔºöÈü≥È¢ëÊñá‰ª∂ÁöÑËØ≠ÁßçÊ†áÁ≠æ\n- `emo_target`ÔºöÈü≥È¢ëÊñá‰ª∂ÁöÑÊÉÖÊÑüÊ†áÁ≠æ\n- `event_target`ÔºöÈü≥È¢ëÊñá‰ª∂ÁöÑ‰∫ã‰ª∂Ê†áÁ≠æ\n- `with_or_wo_itn`ÔºöÊ†áÊ≥®ÊñáÊú¨‰∏≠ÊòØÂê¶ÂåÖÂê´Ê†áÁÇπ‰∏éÈÄÜÊñáÊú¨Ê≠£ÂàôÂåñ\n\nÂèØ‰ª•Áî®Êåá‰ª§ `sensevoice2jsonl` ‰ªé train_wav.scp„ÄÅtrain_text.txt„ÄÅtrain_text_language.txt„ÄÅtrain_emo_target.txt Âíå train_event_target.txt ÁîüÊàêÔºåÂáÜÂ§áËøáÁ®ãÂ¶Ç‰∏ãÔºö\n\n`train_text.txt`\n\nÂ∑¶Ëæπ‰∏∫Êï∞ÊçÆÂîØ‰∏Ä IDÔºåÈúÄ‰∏é `train_wav.scp` ‰∏≠ÁöÑ `ID` ‰∏Ä‰∏ÄÂØπÂ∫î\nÂè≥Ëæπ‰∏∫Èü≥È¢ëÊñá‰ª∂Ê†áÊ≥®ÊñáÊú¨ÔºåÊ†ºÂºèÂ¶Ç‰∏ãÔºö\n\n```bash\nBAC009S0764W0121 ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ\nBAC009S0916W0489 ÊπñÂåó‰∏ÄÂÖ¨Âè∏‰ª•ÂëòÂ∑•Âêç‰πâË¥∑Ê¨æÊï∞ÂçÅÂëòÂ∑•Ë¥üÂÄ∫ÂçÉ‰∏á\nasr_example_cn_en ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning ÂÅö data analytics ÂÅö data science ‰πüÂ•Ω scientist ‰πüÂ•ΩÈÄöÈÄöÈÉΩË¶ÅÈÉΩÂÅöÁöÑÂü∫Êú¨ÂäüÂïäÈÇ£ again ÂÖàÂÖàÂØπÊúâ‰∏Ä‰∫õ > ‰πüËÆ∏ÂØπ\nID0012W0014 he tried to think how it could be\n```\n\n`train_wav.scp`\n\nÂ∑¶Ëæπ‰∏∫Êï∞ÊçÆÂîØ‰∏Ä IDÔºåÈúÄ‰∏é `train_text.txt` ‰∏≠ÁöÑ `ID` ‰∏Ä‰∏ÄÂØπÂ∫î\nÂè≥Ëæπ‰∏∫Èü≥È¢ëÊñá‰ª∂ÁöÑË∑ØÂæÑÔºåÊ†ºÂºèÂ¶Ç‰∏ã\n\n```bash\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n\n`train_text_language.txt`\n\nÂ∑¶Ëæπ‰∏∫Êï∞ÊçÆÂîØ‰∏Ä IDÔºåÈúÄ‰∏é `train_text_language.txt` ‰∏≠ÁöÑ `ID` ‰∏Ä‰∏ÄÂØπÂ∫î\nÂè≥Ëæπ‰∏∫Èü≥È¢ëÊñá‰ª∂ÁöÑËØ≠ÁßçÊ†áÁ≠æÔºåÊîØÊåÅ `<|zh|>`„ÄÅ`<|en|>`„ÄÅ`<|yue|>`„ÄÅ`<|ja|>` Âíå `<|ko|>`ÔºåÊ†ºÂºèÂ¶Ç‰∏ã\n\n```bash\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n\n`train_emo.txt`\n\nÂ∑¶Ëæπ‰∏∫Êï∞ÊçÆÂîØ‰∏Ä IDÔºåÈúÄ‰∏é `train_emo.txt` ‰∏≠ÁöÑ `ID` ‰∏Ä‰∏ÄÂØπÂ∫î\nÂè≥Ëæπ‰∏∫Èü≥È¢ëÊñá‰ª∂ÁöÑÊÉÖÊÑüÊ†áÁ≠æÔºåÊîØÊåÅ `<|HAPPY|>`„ÄÅ`<|SAD|>`„ÄÅ`<|ANGRY|>`„ÄÅ`<|NEUTRAL|>`„ÄÅ`<|FEARFUL|>`„ÄÅ`<|DISGUSTED|>` Âíå `<|SURPRISED|>`ÔºåÊ†ºÂºèÂ¶Ç‰∏ã\n\n```bash\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n\n`train_event.txt`\n\nÂ∑¶Ëæπ‰∏∫Êï∞ÊçÆÂîØ‰∏Ä IDÔºåÈúÄ‰∏é `train_event.txt` ‰∏≠ÁöÑ `ID` ‰∏Ä‰∏ÄÂØπÂ∫î\nÂè≥Ëæπ‰∏∫Èü≥È¢ëÊñá‰ª∂ÁöÑ‰∫ã‰ª∂Ê†áÁ≠æÔºåÊîØÊåÅ `<|BGM|>`„ÄÅ`<|Speech|>`„ÄÅ`<|Applause|>`„ÄÅ`<|Laughter|>`„ÄÅ`<|Cry|>`„ÄÅ`<|Sneeze|>`„ÄÅ`<|Breath|>` Âíå `<|Cough|>`ÔºåÊ†ºÂºèÂ¶Ç‰∏ã\n\n```bash\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```\n\n`ÁîüÊàêÊåá‰ª§`\n\n```shell\n# generate train.jsonl and val.jsonl from wav.scp, text.txt, text_language.txt, emo_target.txt, event_target.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\nËã•Êó† train_text_language.txt„ÄÅtrain_emo_target.txt Âíå train_event_target.txtÔºåÂàôËá™Âä®ÈÄöËøá‰ΩøÁî® `SenseVoice` Ê®°ÂûãÂØπËØ≠Áßç„ÄÅÊÉÖÊÑüÂíå‰∫ã‰ª∂ÊâìÊ†á„ÄÇ\n\n```shell\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\" \\\n++model_dir='iic/SenseVoiceSmall'\n```\n\n</details>\n\n### ÂêØÂä®ËÆ≠ÁªÉ\n\nÊ≥®ÊÑè‰øÆÊîπ `finetune.sh` ‰∏≠ `train_tool` ‰∏∫‰Ω†ÂâçÈù¢ÂÆâË£Ö FunASR Ë∑ØÂæÑ‰∏≠ `funasr/bin/train_ds.py` ÁªùÂØπË∑ØÂæÑ\n\n```shell\nbash finetune.sh\n```\n\n## WebUI\n\n```shell\npython webui.py\n```\n\n<div align=\"center\"><img src=\"image/webui.png\" width=\"700\"/> </div>\n\n## ‰ºòÁßÄ‰∏âÊñπÂ∑•‰Ωú\n\n- TritonÔºàGPUÔºâÈÉ®ÁΩ≤ÊúÄ‰Ω≥ÂÆûË∑µÔºåtriton + tensorrtÔºåfp32 ÊµãËØïÔºåV100 GPU ‰∏äÂä†ÈÄüÊØî 526Ôºåfp16 ÊîØÊåÅ‰∏≠Ôºå[repo](https://github.com/modelscope/FunASR/blob/main/runtime/triton_gpu/README.md)\n- sherpa-onnx ÈÉ®ÁΩ≤ÊúÄ‰Ω≥ÂÆûË∑µÔºåÊîØÊåÅÂú® 10 ÁßçÁºñÁ®ãËØ≠Ë®ÄÈáåÈù¢‰ΩøÁî® SenseVoice, Âç≥ C++, C, Python, C#, Go, Swift, Kotlin, Java, JavaScript, Dart. ÊîØÊåÅÂú® iOS, Android, Raspberry Pi Á≠âÂπ≥Âè∞‰ΩøÁî® SenseVoiceÔºå[repo](https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html)\n- [SenseVoice.cpp](https://github.com/lovemefan/SenseVoice.cpp) Âü∫‰∫éGGMLÔºåÂú®Á∫ØC/C++‰∏≠Êé®Êñ≠SenseVoiceÔºåÊîØÊåÅ3‰Ωç„ÄÅ4‰Ωç„ÄÅ5‰Ωç„ÄÅ8‰ΩçÈáèÂåñÁ≠âÔºåÊó†ÈúÄÁ¨¨‰∏âÊñπ‰æùËµñ„ÄÇ\n- [ÊµÅÂºèSenseVoice](https://github.com/pengzhendong/streaming-sensevoice)ÔºåÈÄöËøáÂàÜÂùóÔºàchunkÔºâÁöÑÊñπÂºèËøõË°åÊé®ÁêÜÔºå‰∏∫‰∫ÜÂÆûÁé∞‰º™ÊµÅÂºèÂ§ÑÁêÜÔºåÈááÁî®‰∫ÜÊà™Êñ≠Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºàtruncated attentionÔºâÔºåÁâ∫Áâ≤‰∫ÜÈÉ®ÂàÜÁ≤æÂ∫¶„ÄÇÊ≠§Â§ñÔºåËØ•ÊäÄÊúØËøòÊîØÊåÅCTCÂâçÁºÄÊùüÊêúÁ¥¢ÔºàCTC prefix beam searchÔºâ‰ª•ÂèäÁÉ≠ËØçÂ¢ûÂº∫ÂäüËÉΩ„ÄÇ\n- [OmniSenseVoice](https://github.com/lifeiteng/OmniSenseVoice) ËΩªÈáèÂåñÊé®ÁêÜÂ∫ìÔºåÊîØÊåÅbatchÊé®ÁêÜ„ÄÇ\n\n# ËÅîÁ≥ªÊàë‰ª¨\n\nÂ¶ÇÊûúÊÇ®Âú®‰ΩøÁî®‰∏≠ÈÅáÂà∞ÈóÆÈ¢òÔºåÂèØ‰ª•Áõ¥Êé•Âú® github È°µÈù¢Êèê Issues„ÄÇÊ¨¢ËøéËØ≠Èü≥ÂÖ¥Ë∂£Áà±Â•ΩËÄÖÊâ´Êèè‰ª•‰∏ãÁöÑÈíâÈíâÁæ§‰∫åÁª¥Á†ÅÂä†ÂÖ•Á§æÂå∫Áæ§ÔºåËøõË°å‰∫§ÊµÅÂíåËÆ®ËÆ∫„ÄÇ\n\n|                          FunASR                          |\n|:--------------------------------------------------------:|\n| <img src=\"image/dingding_funasr.png\" width=\"250\"/></div> |\n"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 2.240234375,
          "content": "# Set the device with environment, default is cuda:0\n# export SENSEVOICE_DEVICE=cuda:1\n\nimport os, re\nfrom fastapi import FastAPI, File, Form\nfrom fastapi.responses import HTMLResponse\nfrom typing_extensions import Annotated\nfrom typing import List\nfrom enum import Enum\nimport torchaudio\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\nfrom io import BytesIO\n\n\nclass Language(str, Enum):\n    auto = \"auto\"\n    zh = \"zh\"\n    en = \"en\"\n    yue = \"yue\"\n    ja = \"ja\"\n    ko = \"ko\"\n    nospeech = \"nospeech\"\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=os.getenv(\"SENSEVOICE_DEVICE\", \"cuda:0\"))\nm.eval()\n\nregex = r\"<\\|.*\\|>\"\n\napp = FastAPI()\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def root():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n        <head>\n            <meta charset=utf-8>\n            <title>Api information</title>\n        </head>\n        <body>\n            <a href='./docs'>Documents of API</a>\n        </body>\n    </html>\n    \"\"\"\n\n@app.post(\"/api/v1/asr\")\nasync def turn_audio_to_text(files: Annotated[List[bytes], File(description=\"wav or mp3 audios in 16KHz\")], keys: Annotated[str, Form(description=\"name of each audio joined with comma\")], lang: Annotated[Language, Form(description=\"language of audio content\")] = \"auto\"):\n    audios = []\n    audio_fs = 0\n    for file in files:\n        file_io = BytesIO(file)\n        data_or_path_or_list, audio_fs = torchaudio.load(file_io)\n        data_or_path_or_list = data_or_path_or_list.mean(0)\n        audios.append(data_or_path_or_list)\n        file_io.close()\n    if lang == \"\":\n        lang = \"auto\"\n    if keys == \"\":\n        key = [\"wav_file_tmp_name\"]\n    else:\n        key = keys.split(\",\")\n    res = m.inference(\n        data_in=audios,\n        language=lang, # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n        ban_emo_unk=False,\n        key=key,\n        fs=audio_fs,\n        **kwargs,\n    )\n    if len(res) == 0:\n        return {\"result\": []}\n    for it in res[0]:\n        it[\"raw_text\"] = it[\"text\"]\n        it[\"clean_text\"] = re.sub(regex, \"\", it[\"text\"], 0, re.MULTILINE)\n        it[\"text\"] = rich_transcription_postprocess(it[\"text\"])\n    return {\"result\": res[0]}\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "deepspeed_conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo1.py",
          "type": "blob",
          "size": 2.0263671875,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n# zh\nres = model.generate(\n    input=f\"{model.model_path}/example/zh.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n# yue\nres = model.generate(\n    input=f\"{model.model_path}/example/yue.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n# ja\nres = model.generate(\n    input=f\"{model.model_path}/example/ja.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n\n# ko\nres = model.generate(\n    input=f\"{model.model_path}/example/ko.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n"
        },
        {
          "name": "demo2.py",
          "type": "blob",
          "size": 1.0087890625,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    output_timestamp=True,\n    **kwargs,\n)\n\ntimestamp = res[0][0][\"timestamp\"]\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\nprint(timestamp)\n"
        },
        {
          "name": "demo_libtorch.py",
          "type": "blob",
          "size": 0.6279296875,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n"
        },
        {
          "name": "demo_onnx.py",
          "type": "blob",
          "size": 0.6416015625,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", textnorm=\"withitn\")\nprint([rich_transcription_postprocess(i) for i in res])\n"
        },
        {
          "name": "export.py",
          "type": "blob",
          "size": 1.576171875,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/FunAudioLLM/SenseVoice). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nimport os\nimport torch\nfrom model import SenseVoiceSmall\nfrom utils import export_utils\nfrom utils.model_bin import SenseVoiceSmallONNX\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nquantize = False\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nmodel, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\n\nrebuilt_model = model.export(type=\"onnx\", quantize=False)\nmodel_path = kwargs.get(\"output_dir\", os.path.dirname(kwargs.get(\"init_param\")))\n\nmodel_file = os.path.join(model_path, \"model.onnx\")\nif quantize:\n    model_file = os.path.join(model_path, \"model_quant.onnx\")\n\n# export model\nif not os.path.exists(model_file):\n    with torch.no_grad():\n        del kwargs['model']\n        export_dir = export_utils.export(model=rebuilt_model, **kwargs)\n        print(\"Export model onnx to {}\".format(model_file))\n        \n# export model init\nmodel_bin = SenseVoiceSmallONNX(model_path)\n\n# build tokenizer\ntry:\n    from funasr.tokenizer.sentencepiece_tokenizer import SentencepiecesTokenizer\n    tokenizer = SentencepiecesTokenizer(bpemodel=os.path.join(model_path, \"chn_jpn_yue_eng_ko_spectok.bpe.model\"))\nexcept:\n    tokenizer = None\n\n# inference\nwav_or_scp = \"/Users/shixian/Downloads/asr_example_hotword.wav\"\nlanguage_list = [0]\ntextnorm_list = [15]\nres = model_bin(wav_or_scp, language_list, textnorm_list, tokenizer=tokenizer)\nprint([rich_transcription_postprocess(i) for i in res])\n"
        },
        {
          "name": "export_meta.py",
          "type": "blob",
          "size": 2.724609375,
          "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nimport types\nimport torch\nfrom funasr.utils.torch_function import sequence_mask\n\n\ndef export_rebuild_model(model, **kwargs):\n    model.device = kwargs.get(\"device\")\n    model.make_pad_mask = sequence_mask(kwargs[\"max_seq_len\"], flip=False)\n    model.forward = types.MethodType(export_forward, model)\n    model.export_dummy_inputs = types.MethodType(export_dummy_inputs, model)\n    model.export_input_names = types.MethodType(export_input_names, model)\n    model.export_output_names = types.MethodType(export_output_names, model)\n    model.export_dynamic_axes = types.MethodType(export_dynamic_axes, model)\n    model.export_name = types.MethodType(export_name, model)\n    return model\n\ndef export_forward(\n    self,\n    speech: torch.Tensor,\n    speech_lengths: torch.Tensor,\n    language: torch.Tensor,\n    textnorm: torch.Tensor,\n    **kwargs,\n):\n    # speech = speech.to(device=\"cuda\")\n    # speech_lengths = speech_lengths.to(device=\"cuda\")\n    language_query = self.embed(language.to(speech.device)).unsqueeze(1)\n    textnorm_query = self.embed(textnorm.to(speech.device)).unsqueeze(1)\n    print(textnorm_query.shape, speech.shape)\n    speech = torch.cat((textnorm_query, speech), dim=1)\n    speech_lengths += 1\n    \n    event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(\n        speech.size(0), 1, 1\n    )\n    input_query = torch.cat((language_query, event_emo_query), dim=1)\n    speech = torch.cat((input_query, speech), dim=1)\n    speech_lengths += 3\n    \n    encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n    if isinstance(encoder_out, tuple):\n        encoder_out = encoder_out[0]\n\n    ctc_logits = self.ctc.ctc_lo(encoder_out)\n    \n    return ctc_logits, encoder_out_lens\n\ndef export_dummy_inputs(self):\n    speech = torch.randn(2, 30, 560)\n    speech_lengths = torch.tensor([6, 30], dtype=torch.int32)\n    language = torch.tensor([0, 0], dtype=torch.int32)\n    textnorm = torch.tensor([15, 15], dtype=torch.int32)\n    return (speech, speech_lengths, language, textnorm)\n\ndef export_input_names(self):\n    return [\"speech\", \"speech_lengths\", \"language\", \"textnorm\"]\n\ndef export_output_names(self):\n    return [\"ctc_logits\", \"encoder_out_lens\"]\n\ndef export_dynamic_axes(self):\n    return {\n        \"speech\": {0: \"batch_size\", 1: \"feats_length\"},\n        \"speech_lengths\": {0: \"batch_size\"},\n        \"language\": {0: \"batch_size\"},\n        \"textnorm\": {0: \"batch_size\"},\n        \"ctc_logits\": {0: \"batch_size\", 1: \"logits_length\"},\n        \"encoder_out_lens\":  {0: \"batch_size\"},\n    }\n\ndef export_name(self):\n    return \"model.onnx\"\n\n"
        },
        {
          "name": "finetune.sh",
          "type": "blob",
          "size": 2.138671875,
          "content": "# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nworkspace=`pwd`\n\n# which gpu to train or finetune\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\n# model_name from model_hub, or model_dir in local path\n\n## option 1, download model automatically\nmodel_name_or_model_dir=\"iic/SenseVoiceSmall\"\n\n## option 2, download model by git\n#local_path_root=${workspace}/modelscope_models\n#mkdir -p ${local_path_root}/${model_name_or_model_dir}\n#git clone https://www.modelscope.cn/${model_name_or_model_dir}.git ${local_path_root}/${model_name_or_model_dir}\n#model_name_or_model_dir=${local_path_root}/${model_name_or_model_dir}\n\n\n# data dir, which contains: train.json, val.json\ntrain_data=${workspace}/data/train_example.jsonl\nval_data=${workspace}/data/val_example.jsonl\n\n# exp output dir\noutput_dir=\"./outputs\"\nlog_file=\"${output_dir}/log.txt\"\n\ndeepspeed_config=${workspace}/deepspeed_conf/ds_stage1.json\n\nmkdir -p ${output_dir}\necho \"log_file: ${log_file}\"\n\nDISTRIBUTED_ARGS=\"\n    --nnodes ${WORLD_SIZE:-1} \\\n    --nproc_per_node $gpu_num \\\n    --node_rank ${RANK:-0} \\\n    --master_addr ${MASTER_ADDR:-127.0.0.1} \\\n    --master_port ${MASTER_PORT:-26669}\n\"\n\necho $DISTRIBUTED_ARGS\n\n# funasr trainer path\ntrain_tool=`dirname $(which funasr)`/train_ds.py\n\ntorchrun $DISTRIBUTED_ARGS \\\n${train_tool} \\\n++model=\"${model_name_or_model_dir}\" \\\n++trust_remote_code=true \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.data_split_num=1 \\\n++dataset_conf.batch_sampler=\"BatchSampler\" \\\n++dataset_conf.batch_size=6000  \\\n++dataset_conf.sort_size=1024 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=true \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++train_conf.use_deepspeed=false \\\n++train_conf.deepspeed_config=${deepspeed_config} \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}"
        },
        {
          "name": "image",
          "type": "tree",
          "content": null
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 32.451171875,
          "content": "\nimport time\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom typing import Iterable, Optional\n\nfrom funasr.register import tables\nfrom funasr.models.ctc.ctc import CTC\nfrom funasr.utils.datadir_writer import DatadirWriter\nfrom funasr.models.paraformer.search import Hypothesis\nfrom funasr.train_utils.device_funcs import force_gatherable\nfrom funasr.losses.label_smoothing_loss import LabelSmoothingLoss\nfrom funasr.metrics.compute_acc import compute_accuracy, th_accuracy\nfrom funasr.utils.load_utils import load_audio_text_image_video, extract_fbank\nfrom utils.ctc_alignment import ctc_forced_align\n\nclass SinusoidalPositionEncoder(torch.nn.Module):\n    \"\"\" \"\"\"\n\n    def __int__(self, d_model=80, dropout_rate=0.1):\n        pass\n\n    def encode(\n        self, positions: torch.Tensor = None, depth: int = None, dtype: torch.dtype = torch.float32\n    ):\n        batch_size = positions.size(0)\n        positions = positions.type(dtype)\n        device = positions.device\n        log_timescale_increment = torch.log(torch.tensor([10000], dtype=dtype, device=device)) / (\n            depth / 2 - 1\n        )\n        inv_timescales = torch.exp(\n            torch.arange(depth / 2, device=device).type(dtype) * (-log_timescale_increment)\n        )\n        inv_timescales = torch.reshape(inv_timescales, [batch_size, -1])\n        scaled_time = torch.reshape(positions, [1, -1, 1]) * torch.reshape(\n            inv_timescales, [1, 1, -1]\n        )\n        encoding = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=2)\n        return encoding.type(dtype)\n\n    def forward(self, x):\n        batch_size, timesteps, input_dim = x.size()\n        positions = torch.arange(1, timesteps + 1, device=x.device)[None, :]\n        position_encoding = self.encode(positions, input_dim, x.dtype).to(x.device)\n\n        return x + position_encoding\n\n\nclass PositionwiseFeedForward(torch.nn.Module):\n    \"\"\"Positionwise feed forward layer.\n\n    Args:\n        idim (int): Input dimenstion.\n        hidden_units (int): The number of hidden units.\n        dropout_rate (float): Dropout rate.\n\n    \"\"\"\n\n    def __init__(self, idim, hidden_units, dropout_rate, activation=torch.nn.ReLU()):\n        \"\"\"Construct an PositionwiseFeedForward object.\"\"\"\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = torch.nn.Linear(idim, hidden_units)\n        self.w_2 = torch.nn.Linear(hidden_units, idim)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        self.activation = activation\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n\n\nclass MultiHeadedAttentionSANM(nn.Module):\n    \"\"\"Multi-Head Attention layer.\n\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        n_head,\n        in_feat,\n        n_feat,\n        dropout_rate,\n        kernel_size,\n        sanm_shfit=0,\n        lora_list=None,\n        lora_rank=8,\n        lora_alpha=16,\n        lora_dropout=0.1,\n    ):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super().__init__()\n        assert n_feat % n_head == 0\n        # We assume d_v always equals d_k\n        self.d_k = n_feat // n_head\n        self.h = n_head\n        # self.linear_q = nn.Linear(n_feat, n_feat)\n        # self.linear_k = nn.Linear(n_feat, n_feat)\n        # self.linear_v = nn.Linear(n_feat, n_feat)\n\n        self.linear_out = nn.Linear(n_feat, n_feat)\n        self.linear_q_k_v = nn.Linear(in_feat, n_feat * 3)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        self.fsmn_block = nn.Conv1d(\n            n_feat, n_feat, kernel_size, stride=1, padding=0, groups=n_feat, bias=False\n        )\n        # padding\n        left_padding = (kernel_size - 1) // 2\n        if sanm_shfit > 0:\n            left_padding = left_padding + sanm_shfit\n        right_padding = kernel_size - 1 - left_padding\n        self.pad_fn = nn.ConstantPad1d((left_padding, right_padding), 0.0)\n\n    def forward_fsmn(self, inputs, mask, mask_shfit_chunk=None):\n        b, t, d = inputs.size()\n        if mask is not None:\n            mask = torch.reshape(mask, (b, -1, 1))\n            if mask_shfit_chunk is not None:\n                mask = mask * mask_shfit_chunk\n            inputs = inputs * mask\n\n        x = inputs.transpose(1, 2)\n        x = self.pad_fn(x)\n        x = self.fsmn_block(x)\n        x = x.transpose(1, 2)\n        x += inputs\n        x = self.dropout(x)\n        if mask is not None:\n            x = x * mask\n        return x\n\n    def forward_qkv(self, x):\n        \"\"\"Transform query, key and value.\n\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n\n        Returns:\n            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).\n            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).\n            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\n\n        \"\"\"\n        b, t, d = x.size()\n        q_k_v = self.linear_q_k_v(x)\n        q, k, v = torch.split(q_k_v, int(self.h * self.d_k), dim=-1)\n        q_h = torch.reshape(q, (b, t, self.h, self.d_k)).transpose(\n            1, 2\n        )  # (batch, head, time1, d_k)\n        k_h = torch.reshape(k, (b, t, self.h, self.d_k)).transpose(\n            1, 2\n        )  # (batch, head, time2, d_k)\n        v_h = torch.reshape(v, (b, t, self.h, self.d_k)).transpose(\n            1, 2\n        )  # (batch, head, time2, d_k)\n\n        return q_h, k_h, v_h, v\n\n    def forward_attention(self, value, scores, mask, mask_att_chunk_encoder=None):\n        \"\"\"Compute attention context vector.\n\n        Args:\n            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).\n            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).\n            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).\n\n        Returns:\n            torch.Tensor: Transformed value (#batch, time1, d_model)\n                weighted by the attention score (#batch, time1, time2).\n\n        \"\"\"\n        n_batch = value.size(0)\n        if mask is not None:\n            if mask_att_chunk_encoder is not None:\n                mask = mask * mask_att_chunk_encoder\n\n            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)\n\n            min_value = -float(\n                \"inf\"\n            )  # float(numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min)\n            scores = scores.masked_fill(mask, min_value)\n            attn = torch.softmax(scores, dim=-1).masked_fill(\n                mask, 0.0\n            )  # (batch, head, time1, time2)\n        else:\n            attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\n        p_attn = self.dropout(attn)\n        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n        x = (\n            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)\n        )  # (batch, time1, d_model)\n\n        return self.linear_out(x)  # (batch, time1, d_model)\n\n    def forward(self, x, mask, mask_shfit_chunk=None, mask_att_chunk_encoder=None):\n        \"\"\"Compute scaled dot product attention.\n\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n\n        \"\"\"\n        q_h, k_h, v_h, v = self.forward_qkv(x)\n        fsmn_memory = self.forward_fsmn(v, mask, mask_shfit_chunk)\n        q_h = q_h * self.d_k ** (-0.5)\n        scores = torch.matmul(q_h, k_h.transpose(-2, -1))\n        att_outs = self.forward_attention(v_h, scores, mask, mask_att_chunk_encoder)\n        return att_outs + fsmn_memory\n\n    def forward_chunk(self, x, cache=None, chunk_size=None, look_back=0):\n        \"\"\"Compute scaled dot product attention.\n\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n\n        \"\"\"\n        q_h, k_h, v_h, v = self.forward_qkv(x)\n        if chunk_size is not None and look_back > 0 or look_back == -1:\n            if cache is not None:\n                k_h_stride = k_h[:, :, : -(chunk_size[2]), :]\n                v_h_stride = v_h[:, :, : -(chunk_size[2]), :]\n                k_h = torch.cat((cache[\"k\"], k_h), dim=2)\n                v_h = torch.cat((cache[\"v\"], v_h), dim=2)\n\n                cache[\"k\"] = torch.cat((cache[\"k\"], k_h_stride), dim=2)\n                cache[\"v\"] = torch.cat((cache[\"v\"], v_h_stride), dim=2)\n                if look_back != -1:\n                    cache[\"k\"] = cache[\"k\"][:, :, -(look_back * chunk_size[1]) :, :]\n                    cache[\"v\"] = cache[\"v\"][:, :, -(look_back * chunk_size[1]) :, :]\n            else:\n                cache_tmp = {\n                    \"k\": k_h[:, :, : -(chunk_size[2]), :],\n                    \"v\": v_h[:, :, : -(chunk_size[2]), :],\n                }\n                cache = cache_tmp\n        fsmn_memory = self.forward_fsmn(v, None)\n        q_h = q_h * self.d_k ** (-0.5)\n        scores = torch.matmul(q_h, k_h.transpose(-2, -1))\n        att_outs = self.forward_attention(v_h, scores, None)\n        return att_outs + fsmn_memory, cache\n\n\nclass LayerNorm(nn.LayerNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = F.layer_norm(\n            input.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(input)\n\n\ndef sequence_mask(lengths, maxlen=None, dtype=torch.float32, device=None):\n    if maxlen is None:\n        maxlen = lengths.max()\n    row_vector = torch.arange(0, maxlen, 1).to(lengths.device)\n    matrix = torch.unsqueeze(lengths, dim=-1)\n    mask = row_vector < matrix\n    mask = mask.detach()\n\n    return mask.type(dtype).to(device) if device is not None else mask.type(dtype)\n\n\nclass EncoderLayerSANM(nn.Module):\n    def __init__(\n        self,\n        in_size,\n        size,\n        self_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n        stochastic_depth_rate=0.0,\n    ):\n        \"\"\"Construct an EncoderLayer object.\"\"\"\n        super(EncoderLayerSANM, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.norm1 = LayerNorm(in_size)\n        self.norm2 = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.in_size = in_size\n        self.size = size\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear = nn.Linear(size + size, size)\n        self.stochastic_depth_rate = stochastic_depth_rate\n        self.dropout_rate = dropout_rate\n\n    def forward(self, x, mask, cache=None, mask_shfit_chunk=None, mask_att_chunk_encoder=None):\n        \"\"\"Compute encoded features.\n\n        Args:\n            x_input (torch.Tensor): Input tensor (#batch, time, size).\n            mask (torch.Tensor): Mask tensor for the input (#batch, time).\n            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time, size).\n            torch.Tensor: Mask tensor (#batch, time).\n\n        \"\"\"\n        skip_layer = False\n        # with stochastic depth, residual connection `x + f(x)` becomes\n        # `x <- x + 1 / (1 - p) * f(x)` at training time.\n        stoch_layer_coeff = 1.0\n        if self.training and self.stochastic_depth_rate > 0:\n            skip_layer = torch.rand(1).item() < self.stochastic_depth_rate\n            stoch_layer_coeff = 1.0 / (1 - self.stochastic_depth_rate)\n\n        if skip_layer:\n            if cache is not None:\n                x = torch.cat([cache, x], dim=1)\n            return x, mask\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm1(x)\n\n        if self.concat_after:\n            x_concat = torch.cat(\n                (\n                    x,\n                    self.self_attn(\n                        x,\n                        mask,\n                        mask_shfit_chunk=mask_shfit_chunk,\n                        mask_att_chunk_encoder=mask_att_chunk_encoder,\n                    ),\n                ),\n                dim=-1,\n            )\n            if self.in_size == self.size:\n                x = residual + stoch_layer_coeff * self.concat_linear(x_concat)\n            else:\n                x = stoch_layer_coeff * self.concat_linear(x_concat)\n        else:\n            if self.in_size == self.size:\n                x = residual + stoch_layer_coeff * self.dropout(\n                    self.self_attn(\n                        x,\n                        mask,\n                        mask_shfit_chunk=mask_shfit_chunk,\n                        mask_att_chunk_encoder=mask_att_chunk_encoder,\n                    )\n                )\n            else:\n                x = stoch_layer_coeff * self.dropout(\n                    self.self_attn(\n                        x,\n                        mask,\n                        mask_shfit_chunk=mask_shfit_chunk,\n                        mask_att_chunk_encoder=mask_att_chunk_encoder,\n                    )\n                )\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        x = residual + stoch_layer_coeff * self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm2(x)\n\n        return x, mask, cache, mask_shfit_chunk, mask_att_chunk_encoder\n\n    def forward_chunk(self, x, cache=None, chunk_size=None, look_back=0):\n        \"\"\"Compute encoded features.\n\n        Args:\n            x_input (torch.Tensor): Input tensor (#batch, time, size).\n            mask (torch.Tensor): Mask tensor for the input (#batch, time).\n            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time, size).\n            torch.Tensor: Mask tensor (#batch, time).\n\n        \"\"\"\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm1(x)\n\n        if self.in_size == self.size:\n            attn, cache = self.self_attn.forward_chunk(x, cache, chunk_size, look_back)\n            x = residual + attn\n        else:\n            x, cache = self.self_attn.forward_chunk(x, cache, chunk_size, look_back)\n\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        x = residual + self.feed_forward(x)\n        if not self.normalize_before:\n            x = self.norm2(x)\n\n        return x, cache\n\n\n@tables.register(\"encoder_classes\", \"SenseVoiceEncoderSmall\")\nclass SenseVoiceEncoderSmall(nn.Module):\n    \"\"\"\n    Author: Speech Lab of DAMO Academy, Alibaba Group\n    SCAMA: Streaming chunk-aware multihead attention for online end-to-end speech recognition\n    https://arxiv.org/abs/2006.01713\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int = 256,\n        attention_heads: int = 4,\n        linear_units: int = 2048,\n        num_blocks: int = 6,\n        tp_blocks: int = 0,\n        dropout_rate: float = 0.1,\n        positional_dropout_rate: float = 0.1,\n        attention_dropout_rate: float = 0.0,\n        stochastic_depth_rate: float = 0.0,\n        input_layer: Optional[str] = \"conv2d\",\n        pos_enc_class=SinusoidalPositionEncoder,\n        normalize_before: bool = True,\n        concat_after: bool = False,\n        positionwise_layer_type: str = \"linear\",\n        positionwise_conv_kernel_size: int = 1,\n        padding_idx: int = -1,\n        kernel_size: int = 11,\n        sanm_shfit: int = 0,\n        selfattention_layer_type: str = \"sanm\",\n        **kwargs,\n    ):\n        super().__init__()\n        self._output_size = output_size\n\n        self.embed = SinusoidalPositionEncoder()\n\n        self.normalize_before = normalize_before\n\n        positionwise_layer = PositionwiseFeedForward\n        positionwise_layer_args = (\n            output_size,\n            linear_units,\n            dropout_rate,\n        )\n\n        encoder_selfattn_layer = MultiHeadedAttentionSANM\n        encoder_selfattn_layer_args0 = (\n            attention_heads,\n            input_size,\n            output_size,\n            attention_dropout_rate,\n            kernel_size,\n            sanm_shfit,\n        )\n        encoder_selfattn_layer_args = (\n            attention_heads,\n            output_size,\n            output_size,\n            attention_dropout_rate,\n            kernel_size,\n            sanm_shfit,\n        )\n\n        self.encoders0 = nn.ModuleList(\n            [\n                EncoderLayerSANM(\n                    input_size,\n                    output_size,\n                    encoder_selfattn_layer(*encoder_selfattn_layer_args0),\n                    positionwise_layer(*positionwise_layer_args),\n                    dropout_rate,\n                )\n                for i in range(1)\n            ]\n        )\n        self.encoders = nn.ModuleList(\n            [\n                EncoderLayerSANM(\n                    output_size,\n                    output_size,\n                    encoder_selfattn_layer(*encoder_selfattn_layer_args),\n                    positionwise_layer(*positionwise_layer_args),\n                    dropout_rate,\n                )\n                for i in range(num_blocks - 1)\n            ]\n        )\n\n        self.tp_encoders = nn.ModuleList(\n            [\n                EncoderLayerSANM(\n                    output_size,\n                    output_size,\n                    encoder_selfattn_layer(*encoder_selfattn_layer_args),\n                    positionwise_layer(*positionwise_layer_args),\n                    dropout_rate,\n                )\n                for i in range(tp_blocks)\n            ]\n        )\n\n        self.after_norm = LayerNorm(output_size)\n\n        self.tp_norm = LayerNorm(output_size)\n\n    def output_size(self) -> int:\n        return self._output_size\n\n    def forward(\n        self,\n        xs_pad: torch.Tensor,\n        ilens: torch.Tensor,\n    ):\n        \"\"\"Embed positions in tensor.\"\"\"\n        masks = sequence_mask(ilens, device=ilens.device)[:, None, :]\n\n        xs_pad *= self.output_size() ** 0.5\n\n        xs_pad = self.embed(xs_pad)\n\n        # forward encoder1\n        for layer_idx, encoder_layer in enumerate(self.encoders0):\n            encoder_outs = encoder_layer(xs_pad, masks)\n            xs_pad, masks = encoder_outs[0], encoder_outs[1]\n\n        for layer_idx, encoder_layer in enumerate(self.encoders):\n            encoder_outs = encoder_layer(xs_pad, masks)\n            xs_pad, masks = encoder_outs[0], encoder_outs[1]\n\n        xs_pad = self.after_norm(xs_pad)\n\n        # forward encoder2\n        olens = masks.squeeze(1).sum(1).int()\n\n        for layer_idx, encoder_layer in enumerate(self.tp_encoders):\n            encoder_outs = encoder_layer(xs_pad, masks)\n            xs_pad, masks = encoder_outs[0], encoder_outs[1]\n\n        xs_pad = self.tp_norm(xs_pad)\n        return xs_pad, olens\n\n\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n    \"\"\"CTC-attention hybrid Encoder-Decoder model\"\"\"\n\n    def __init__(\n        self,\n        specaug: str = None,\n        specaug_conf: dict = None,\n        normalize: str = None,\n        normalize_conf: dict = None,\n        encoder: str = None,\n        encoder_conf: dict = None,\n        ctc_conf: dict = None,\n        input_size: int = 80,\n        vocab_size: int = -1,\n        ignore_id: int = -1,\n        blank_id: int = 0,\n        sos: int = 1,\n        eos: int = 2,\n        length_normalized_loss: bool = False,\n        **kwargs,\n    ):\n\n        super().__init__()\n\n        if specaug is not None:\n            specaug_class = tables.specaug_classes.get(specaug)\n            specaug = specaug_class(**specaug_conf)\n        if normalize is not None:\n            normalize_class = tables.normalize_classes.get(normalize)\n            normalize = normalize_class(**normalize_conf)\n        encoder_class = tables.encoder_classes.get(encoder)\n        encoder = encoder_class(input_size=input_size, **encoder_conf)\n        encoder_output_size = encoder.output_size()\n\n        if ctc_conf is None:\n            ctc_conf = {}\n        ctc = CTC(odim=vocab_size, encoder_output_size=encoder_output_size, **ctc_conf)\n\n        self.blank_id = blank_id\n        self.sos = sos if sos is not None else vocab_size - 1\n        self.eos = eos if eos is not None else vocab_size - 1\n        self.vocab_size = vocab_size\n        self.ignore_id = ignore_id\n        self.specaug = specaug\n        self.normalize = normalize\n        self.encoder = encoder\n        self.error_calculator = None\n\n        self.ctc = ctc\n\n        self.length_normalized_loss = length_normalized_loss\n        self.encoder_output_size = encoder_output_size\n\n        self.lid_dict = {\"auto\": 0, \"zh\": 3, \"en\": 4, \"yue\": 7, \"ja\": 11, \"ko\": 12, \"nospeech\": 13}\n        self.lid_int_dict = {24884: 3, 24885: 4, 24888: 7, 24892: 11, 24896: 12, 24992: 13}\n        self.textnorm_dict = {\"withitn\": 14, \"woitn\": 15}\n        self.textnorm_int_dict = {25016: 14, 25017: 15}\n        self.embed = torch.nn.Embedding(7 + len(self.lid_dict) + len(self.textnorm_dict), input_size)\n        self.emo_dict = {\"unk\": 25009, \"happy\": 25001, \"sad\": 25002, \"angry\": 25003, \"neutral\": 25004}\n        \n        self.criterion_att = LabelSmoothingLoss(\n            size=self.vocab_size,\n            padding_idx=self.ignore_id,\n            smoothing=kwargs.get(\"lsm_weight\", 0.0),\n            normalize_length=self.length_normalized_loss,\n        )\n    \n    @staticmethod\n    def from_pretrained(model:str=None, **kwargs):\n        from funasr import AutoModel\n        model, kwargs = AutoModel.build_model(model=model, trust_remote_code=True, **kwargs)\n        \n        return model, kwargs\n\n    def forward(\n        self,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        **kwargs,\n    ):\n        \"\"\"Encoder + Decoder + Calc loss\n        Args:\n                speech: (Batch, Length, ...)\n                speech_lengths: (Batch, )\n                text: (Batch, Length)\n                text_lengths: (Batch,)\n        \"\"\"\n        # import pdb;\n        # pdb.set_trace()\n        if len(text_lengths.size()) > 1:\n            text_lengths = text_lengths[:, 0]\n        if len(speech_lengths.size()) > 1:\n            speech_lengths = speech_lengths[:, 0]\n\n        batch_size = speech.shape[0]\n\n        # 1. Encoder\n        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths, text)\n\n        loss_ctc, cer_ctc = None, None\n        loss_rich, acc_rich = None, None\n        stats = dict()\n\n        loss_ctc, cer_ctc = self._calc_ctc_loss(\n            encoder_out[:, 4:, :], encoder_out_lens - 4, text[:, 4:], text_lengths - 4\n        )\n\n        loss_rich, acc_rich = self._calc_rich_ce_loss(\n            encoder_out[:, :4, :], text[:, :4]\n        )\n\n        loss = loss_ctc + loss_rich\n        # Collect total loss stats\n        stats[\"loss_ctc\"] = torch.clone(loss_ctc.detach()) if loss_ctc is not None else None\n        stats[\"loss_rich\"] = torch.clone(loss_rich.detach()) if loss_rich is not None else None\n        stats[\"loss\"] = torch.clone(loss.detach()) if loss is not None else None\n        stats[\"acc_rich\"] = acc_rich\n\n        # force_gatherable: to-device and to-tensor if scalar for DataParallel\n        if self.length_normalized_loss:\n            batch_size = int((text_lengths + 1).sum())\n        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)\n        return loss, stats, weight\n\n    def encode(\n        self,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        text: torch.Tensor,\n        **kwargs,\n    ):\n        \"\"\"Frontend + Encoder. Note that this method is used by asr_inference.py\n        Args:\n                speech: (Batch, Length, ...)\n                speech_lengths: (Batch, )\n                ind: int\n        \"\"\"\n\n        # Data augmentation\n        if self.specaug is not None and self.training:\n            speech, speech_lengths = self.specaug(speech, speech_lengths)\n\n        # Normalization for feature: e.g. Global-CMVN, Utterance-CMVN\n        if self.normalize is not None:\n            speech, speech_lengths = self.normalize(speech, speech_lengths)\n\n\n        lids = torch.LongTensor([[self.lid_int_dict[int(lid)] if torch.rand(1) > 0.2 and int(lid) in self.lid_int_dict else 0 ] for lid in text[:, 0]]).to(speech.device)\n        language_query = self.embed(lids)\n        \n        styles = torch.LongTensor([[self.textnorm_int_dict[int(style)]] for style in text[:, 3]]).to(speech.device)\n        style_query = self.embed(styles)\n        speech = torch.cat((style_query, speech), dim=1)\n        speech_lengths += 1\n\n        event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(speech.size(0), 1, 1)\n        input_query = torch.cat((language_query, event_emo_query), dim=1)\n        speech = torch.cat((input_query, speech), dim=1)\n        speech_lengths += 3\n\n        encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n\n        return encoder_out, encoder_out_lens\n\n    def _calc_ctc_loss(\n        self,\n        encoder_out: torch.Tensor,\n        encoder_out_lens: torch.Tensor,\n        ys_pad: torch.Tensor,\n        ys_pad_lens: torch.Tensor,\n    ):\n        # Calc CTC loss\n        loss_ctc = self.ctc(encoder_out, encoder_out_lens, ys_pad, ys_pad_lens)\n\n        # Calc CER using CTC\n        cer_ctc = None\n        if not self.training and self.error_calculator is not None:\n            ys_hat = self.ctc.argmax(encoder_out).data\n            cer_ctc = self.error_calculator(ys_hat.cpu(), ys_pad.cpu(), is_ctc=True)\n        return loss_ctc, cer_ctc\n\n    def _calc_rich_ce_loss(\n        self,\n        encoder_out: torch.Tensor,\n        ys_pad: torch.Tensor,\n    ):\n        decoder_out = self.ctc.ctc_lo(encoder_out)\n        # 2. Compute attention loss\n        loss_rich = self.criterion_att(decoder_out, ys_pad.contiguous())\n        acc_rich = th_accuracy(\n            decoder_out.view(-1, self.vocab_size),\n            ys_pad.contiguous(),\n            ignore_label=self.ignore_id,\n        )\n\n        return loss_rich, acc_rich\n\n\n    def inference(\n        self,\n        data_in,\n        data_lengths=None,\n        key: list = [\"wav_file_tmp_name\"],\n        tokenizer=None,\n        frontend=None,\n        **kwargs,\n    ):\n\n\n        meta_data = {}\n        if (\n            isinstance(data_in, torch.Tensor) and kwargs.get(\"data_type\", \"sound\") == \"fbank\"\n        ):  # fbank\n            speech, speech_lengths = data_in, data_lengths\n            if len(speech.shape) < 3:\n                speech = speech[None, :, :]\n            if speech_lengths is None:\n                speech_lengths = speech.shape[1]\n        else:\n            # extract fbank feats\n            time1 = time.perf_counter()\n            audio_sample_list = load_audio_text_image_video(\n                data_in,\n                fs=frontend.fs,\n                audio_fs=kwargs.get(\"fs\", 16000),\n                data_type=kwargs.get(\"data_type\", \"sound\"),\n                tokenizer=tokenizer,\n            )\n            time2 = time.perf_counter()\n            meta_data[\"load_data\"] = f\"{time2 - time1:0.3f}\"\n            speech, speech_lengths = extract_fbank(\n                audio_sample_list, data_type=kwargs.get(\"data_type\", \"sound\"), frontend=frontend\n            )\n            time3 = time.perf_counter()\n            meta_data[\"extract_feat\"] = f\"{time3 - time2:0.3f}\"\n            meta_data[\"batch_data_time\"] = (\n                speech_lengths.sum().item() * frontend.frame_shift * frontend.lfr_n / 1000\n            )\n\n        speech = speech.to(device=kwargs[\"device\"])\n        speech_lengths = speech_lengths.to(device=kwargs[\"device\"])\n\n        language = kwargs.get(\"language\", \"auto\")\n        language_query = self.embed(\n            torch.LongTensor(\n                [[self.lid_dict[language] if language in self.lid_dict else 0]]\n            ).to(speech.device)\n        ).repeat(speech.size(0), 1, 1)\n        \n        use_itn = kwargs.get(\"use_itn\", False)\n        output_timestamp = kwargs.get(\"output_timestamp\", False)\n\n        textnorm = kwargs.get(\"text_norm\", None)\n        if textnorm is None:\n            textnorm = \"withitn\" if use_itn else \"woitn\"\n        textnorm_query = self.embed(\n            torch.LongTensor([[self.textnorm_dict[textnorm]]]).to(speech.device)\n        ).repeat(speech.size(0), 1, 1)\n        speech = torch.cat((textnorm_query, speech), dim=1)\n        speech_lengths += 1\n\n        event_emo_query = self.embed(torch.LongTensor([[1, 2]]).to(speech.device)).repeat(\n            speech.size(0), 1, 1\n        )\n        input_query = torch.cat((language_query, event_emo_query), dim=1)\n        speech = torch.cat((input_query, speech), dim=1)\n        speech_lengths += 3\n\n        # Encoder\n        encoder_out, encoder_out_lens = self.encoder(speech, speech_lengths)\n        if isinstance(encoder_out, tuple):\n            encoder_out = encoder_out[0]\n\n        # c. Passed the encoder result and the beam search\n        ctc_logits = self.ctc.log_softmax(encoder_out)\n        if kwargs.get(\"ban_emo_unk\", False):\n            ctc_logits[:, :, self.emo_dict[\"unk\"]] = -float(\"inf\")\n\n        results = []\n        b, n, d = encoder_out.size()\n        if isinstance(key[0], (list, tuple)):\n            key = key[0]\n        if len(key) < b:\n            key = key * b\n        for i in range(b):\n            x = ctc_logits[i, : encoder_out_lens[i].item(), :]\n            yseq = x.argmax(dim=-1)\n            yseq = torch.unique_consecutive(yseq, dim=-1)\n\n            ibest_writer = None\n            if kwargs.get(\"output_dir\") is not None:\n                if not hasattr(self, \"writer\"):\n                    self.writer = DatadirWriter(kwargs.get(\"output_dir\"))\n                ibest_writer = self.writer[f\"1best_recog\"]\n\n            mask = yseq != self.blank_id\n            token_int = yseq[mask].tolist()\n\n            # Change integer-ids to tokens\n            text = tokenizer.decode(token_int)\n            if ibest_writer is not None:\n                ibest_writer[\"text\"][key[i]] = text\n\n            if output_timestamp:\n                from itertools import groupby\n                timestamp = []\n                tokens = tokenizer.text2tokens(text)[4:]\n\n                logits_speech = self.ctc.softmax(encoder_out)[i, 4:encoder_out_lens[i].item(), :]\n\n                pred = logits_speech.argmax(-1).cpu()\n                logits_speech[pred==self.blank_id, self.blank_id] = 0\n\n                align = ctc_forced_align(\n                    logits_speech.unsqueeze(0).float(),\n                    torch.Tensor(token_int[4:]).unsqueeze(0).long().to(logits_speech.device),\n                    (encoder_out_lens-4).long(),\n                    torch.tensor(len(token_int)-4).unsqueeze(0).long().to(logits_speech.device),\n                    ignore_id=self.ignore_id,\n                )\n\n                pred = groupby(align[0, :encoder_out_lens[0]])\n                _start = 0\n                token_id = 0\n                ts_max = encoder_out_lens[i] - 4\n                for pred_token, pred_frame in pred:\n                    _end = _start + len(list(pred_frame))\n                    if pred_token != 0:\n                        ts_left = max((_start*60-30)/1000, 0)\n                        ts_right = min((_end*60-30)/1000, (ts_max*60-30)/1000)\n                        timestamp.append([tokens[token_id], ts_left, ts_right])\n                        token_id += 1\n                    _start = _end\n\n                result_i = {\"key\": key[i], \"text\": text, \"timestamp\": timestamp}\n                results.append(result_i)\n            else:\n                result_i = {\"key\": key[i], \"text\": text}\n                results.append(result_i)\n        return results, meta_data\n\n    def export(self, **kwargs):\n        from export_meta import export_rebuild_model\n\n        if \"max_seq_len\" not in kwargs:\n            kwargs[\"max_seq_len\"] = 512\n        models = export_rebuild_model(model=self, **kwargs)\n        return models\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1103515625,
          "content": "torch<=2.3\ntorchaudio\nmodelscope\nhuggingface\nhuggingface_hub\nfunasr>=1.1.3\nnumpy<=1.26.4\ngradio\nfastapi>=0.111.1\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "webui.py",
          "type": "blob",
          "size": 7.697265625,
          "content": "# coding=utf-8\n\nimport os\nimport librosa\nimport base64\nimport io\nimport gradio as gr\nimport re\n\nimport numpy as np\nimport torch\nimport torchaudio\n\n\nfrom funasr import AutoModel\n\nmodel = \"iic/SenseVoiceSmall\"\nmodel = AutoModel(model=model,\n\t\t\t\t  vad_model=\"iic/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n\t\t\t\t  vad_kwargs={\"max_single_segment_time\": 30000},\n\t\t\t\t  trust_remote_code=True,\n\t\t\t\t  )\n\nimport re\n\nemo_dict = {\n\t\"<|HAPPY|>\": \"üòä\",\n\t\"<|SAD|>\": \"üòî\",\n\t\"<|ANGRY|>\": \"üò°\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|FEARFUL|>\": \"üò∞\",\n\t\"<|DISGUSTED|>\": \"ü§¢\",\n\t\"<|SURPRISED|>\": \"üòÆ\",\n}\n\nevent_dict = {\n\t\"<|BGM|>\": \"üéº\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"üëè\",\n\t\"<|Laughter|>\": \"üòÄ\",\n\t\"<|Cry|>\": \"üò≠\",\n\t\"<|Sneeze|>\": \"ü§ß\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"ü§ß\",\n}\n\nemoji_dict = {\n\t\"<|nospeech|><|Event_UNK|>\": \"‚ùì\",\n\t\"<|zh|>\": \"\",\n\t\"<|en|>\": \"\",\n\t\"<|yue|>\": \"\",\n\t\"<|ja|>\": \"\",\n\t\"<|ko|>\": \"\",\n\t\"<|nospeech|>\": \"\",\n\t\"<|HAPPY|>\": \"üòä\",\n\t\"<|SAD|>\": \"üòî\",\n\t\"<|ANGRY|>\": \"üò°\",\n\t\"<|NEUTRAL|>\": \"\",\n\t\"<|BGM|>\": \"üéº\",\n\t\"<|Speech|>\": \"\",\n\t\"<|Applause|>\": \"üëè\",\n\t\"<|Laughter|>\": \"üòÄ\",\n\t\"<|FEARFUL|>\": \"üò∞\",\n\t\"<|DISGUSTED|>\": \"ü§¢\",\n\t\"<|SURPRISED|>\": \"üòÆ\",\n\t\"<|Cry|>\": \"üò≠\",\n\t\"<|EMO_UNKNOWN|>\": \"\",\n\t\"<|Sneeze|>\": \"ü§ß\",\n\t\"<|Breath|>\": \"\",\n\t\"<|Cough|>\": \"üò∑\",\n\t\"<|Sing|>\": \"\",\n\t\"<|Speech_Noise|>\": \"\",\n\t\"<|withitn|>\": \"\",\n\t\"<|woitn|>\": \"\",\n\t\"<|GBG|>\": \"\",\n\t\"<|Event_UNK|>\": \"\",\n}\n\nlang_dict =  {\n    \"<|zh|>\": \"<|lang|>\",\n    \"<|en|>\": \"<|lang|>\",\n    \"<|yue|>\": \"<|lang|>\",\n    \"<|ja|>\": \"<|lang|>\",\n    \"<|ko|>\": \"<|lang|>\",\n    \"<|nospeech|>\": \"<|lang|>\",\n}\n\nemo_set = {\"üòä\", \"üòî\", \"üò°\", \"üò∞\", \"ü§¢\", \"üòÆ\"}\nevent_set = {\"üéº\", \"üëè\", \"üòÄ\", \"üò≠\", \"ü§ß\", \"üò∑\",}\n\ndef format_str(s):\n\tfor sptk in emoji_dict:\n\t\ts = s.replace(sptk, emoji_dict[sptk])\n\treturn s\n\n\ndef format_str_v2(s):\n\tsptk_dict = {}\n\tfor sptk in emoji_dict:\n\t\tsptk_dict[sptk] = s.count(sptk)\n\t\ts = s.replace(sptk, \"\")\n\temo = \"<|NEUTRAL|>\"\n\tfor e in emo_dict:\n\t\tif sptk_dict[e] > sptk_dict[emo]:\n\t\t\temo = e\n\tfor e in event_dict:\n\t\tif sptk_dict[e] > 0:\n\t\t\ts = event_dict[e] + s\n\ts = s + emo_dict[emo]\n\n\tfor emoji in emo_set.union(event_set):\n\t\ts = s.replace(\" \" + emoji, emoji)\n\t\ts = s.replace(emoji + \" \", emoji)\n\treturn s.strip()\n\ndef format_str_v3(s):\n\tdef get_emo(s):\n\t\treturn s[-1] if s[-1] in emo_set else None\n\tdef get_event(s):\n\t\treturn s[0] if s[0] in event_set else None\n\n\ts = s.replace(\"<|nospeech|><|Event_UNK|>\", \"‚ùì\")\n\tfor lang in lang_dict:\n\t\ts = s.replace(lang, \"<|lang|>\")\n\ts_list = [format_str_v2(s_i).strip(\" \") for s_i in s.split(\"<|lang|>\")]\n\tnew_s = \" \" + s_list[0]\n\tcur_ent_event = get_event(new_s)\n\tfor i in range(1, len(s_list)):\n\t\tif len(s_list[i]) == 0:\n\t\t\tcontinue\n\t\tif get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:\n\t\t\ts_list[i] = s_list[i][1:]\n\t\t#else:\n\t\tcur_ent_event = get_event(s_list[i])\n\t\tif get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):\n\t\t\tnew_s = new_s[:-1]\n\t\tnew_s += s_list[i].strip().lstrip()\n\tnew_s = new_s.replace(\"The.\", \" \")\n\treturn new_s.strip()\n\ndef model_inference(input_wav, language, fs=16000):\n\t# task_abbr = {\"Speech Recognition\": \"ASR\", \"Rich Text Transcription\": (\"ASR\", \"AED\", \"SER\")}\n\tlanguage_abbr = {\"auto\": \"auto\", \"zh\": \"zh\", \"en\": \"en\", \"yue\": \"yue\", \"ja\": \"ja\", \"ko\": \"ko\",\n\t\t\t\t\t \"nospeech\": \"nospeech\"}\n\t\n\t# task = \"Speech Recognition\" if task is None else task\n\tlanguage = \"auto\" if len(language) < 1 else language\n\tselected_language = language_abbr[language]\n\t# selected_task = task_abbr.get(task)\n\t\n\t# print(f\"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}\")\n\t\n\tif isinstance(input_wav, tuple):\n\t\tfs, input_wav = input_wav\n\t\tinput_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max\n\t\tif len(input_wav.shape) > 1:\n\t\t\tinput_wav = input_wav.mean(-1)\n\t\tif fs != 16000:\n\t\t\tprint(f\"audio_fs: {fs}\")\n\t\t\tresampler = torchaudio.transforms.Resample(fs, 16000)\n\t\t\tinput_wav_t = torch.from_numpy(input_wav).to(torch.float32)\n\t\t\tinput_wav = resampler(input_wav_t[None, :])[0, :].numpy()\n\t\n\t\n\tmerge_vad = True #False if selected_task == \"ASR\" else True\n\tprint(f\"language: {language}, merge_vad: {merge_vad}\")\n\ttext = model.generate(input=input_wav,\n\t\t\t\t\t\t  cache={},\n\t\t\t\t\t\t  language=language,\n\t\t\t\t\t\t  use_itn=True,\n\t\t\t\t\t\t  batch_size_s=60, merge_vad=merge_vad)\n\t\n\tprint(text)\n\ttext = text[0][\"text\"]\n\ttext = format_str_v3(text)\n\t\n\tprint(text)\n\t\n\treturn text\n\n\naudio_examples = [\n    [\"example/zh.mp3\", \"zh\"],\n    [\"example/yue.mp3\", \"yue\"],\n    [\"example/en.mp3\", \"en\"],\n    [\"example/ja.mp3\", \"ja\"],\n    [\"example/ko.mp3\", \"ko\"],\n    [\"example/emo_1.wav\", \"auto\"],\n    [\"example/emo_2.wav\", \"auto\"],\n    [\"example/emo_3.wav\", \"auto\"],\n    #[\"example/emo_4.wav\", \"auto\"],\n    #[\"example/event_1.wav\", \"auto\"],\n    #[\"example/event_2.wav\", \"auto\"],\n    #[\"example/event_3.wav\", \"auto\"],\n    [\"example/rich_1.wav\", \"auto\"],\n    [\"example/rich_2.wav\", \"auto\"],\n    #[\"example/rich_3.wav\", \"auto\"],\n    [\"example/longwav_1.wav\", \"auto\"],\n    [\"example/longwav_2.wav\", \"auto\"],\n    [\"example/longwav_3.wav\", \"auto\"],\n    #[\"example/longwav_4.wav\", \"auto\"],\n]\n\n\n\nhtml_content = \"\"\"\n<div>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Voice Understanding Model: SenseVoice-Small</h2>\n    <p style=\"font-size: 18px;margin-left: 20px;\">SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.</p>\n    <h2 style=\"font-size: 22px;margin-left: 0px;\">Usage</h2> <p style=\"font-size: 18px;margin-left: 20px;\">Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (üòä happy, üò° angry/exicting, üòî sad) and types of sound events (üòÄ laughter, üéº music, üëè applause, ü§ß cough&sneeze, üò≠ cry). The event labels are placed in the front of the text and the emotion are in the back of the text.</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\">Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.</p>\n\t<h2 style=\"font-size: 22px;margin-left: 0px;\">Repo</h2>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/SenseVoice\" target=\"_blank\">SenseVoice</a>: multilingual speech understanding model</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/modelscope/FunASR\" target=\"_blank\">FunASR</a>: fundamental speech recognition toolkit</p>\n\t<p style=\"font-size: 18px;margin-left: 20px;\"><a href=\"https://github.com/FunAudioLLM/CosyVoice\" target=\"_blank\">CosyVoice</a>: high-quality multilingual TTS model</p>\n</div>\n\"\"\"\n\n\ndef launch():\n\twith gr.Blocks(theme=gr.themes.Soft()) as demo:\n\t\t# gr.Markdown(description)\n\t\tgr.HTML(html_content)\n\t\twith gr.Row():\n\t\t\twith gr.Column():\n\t\t\t\taudio_inputs = gr.Audio(label=\"Upload audio or use the microphone\")\n\t\t\t\t\n\t\t\t\twith gr.Accordion(\"Configuration\"):\n\t\t\t\t\tlanguage_inputs = gr.Dropdown(choices=[\"auto\", \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"],\n\t\t\t\t\t\t\t\t\t\t\t\t  value=\"auto\",\n\t\t\t\t\t\t\t\t\t\t\t\t  label=\"Language\")\n\t\t\t\tfn_button = gr.Button(\"Start\", variant=\"primary\")\n\t\t\t\ttext_outputs = gr.Textbox(label=\"Results\")\n\t\t\tgr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)\n\t\t\n\t\tfn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)\n\n\tdemo.launch()\n\n\nif __name__ == \"__main__\":\n\t# iface.launch()\n\tlaunch()\n\n\n"
        }
      ]
    }
  ]
}