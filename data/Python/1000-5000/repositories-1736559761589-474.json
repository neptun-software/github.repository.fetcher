{
  "metadata": {
    "timestamp": 1736559761589,
    "page": 474,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "kenshohara/3D-ResNets-PyTorch",
      "stars": 3932,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1484375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n.DS_Store\n\n.vscode"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2017 Kensho Hara\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.4482421875,
          "content": "# 3D ResNets for Action Recognition\n\n## Update (2020/4/13)\n\nWe published a paper on arXiv.\n\n[\nHirokatsu Kataoka, Tenga Wakamiya, Kensho Hara, and Yutaka Satoh,  \n\"Would Mega-scale Datasets Further Enhance Spatiotemporal 3D CNNs\",  \narXiv preprint, arXiv:2004.04968, 2020.\n](https://arxiv.org/abs/2004.04968)\n\nWe uploaded the pretrained models described in this paper including ResNet-50 pretrained on the combined dataset with Kinetics-700 and Moments in Time.\n\n## Update (2020/4/10)\n\nWe significantly updated our scripts. If you want to use older versions to reproduce our CVPR2018 paper, you should use the scripts in the CVPR2018 branch.\n\nThis update includes as follows:\n* Refactoring whole project\n* Supporting the newer PyTorch versions\n* Supporting distributed training\n* Supporting training and testing on the Moments in Time dataset.\n* Adding R(2+1)D models\n* Uploading 3D ResNet models trained on the Kinetics-700, Moments in Time, and STAIR-Actions datasets\n\n## Summary\n\nThis is the PyTorch code for the following papers:\n\n[\nHirokatsu Kataoka, Tenga Wakamiya, Kensho Hara, and Yutaka Satoh,  \n\"Would Mega-scale Datasets Further Enhance Spatiotemporal 3D CNNs\",  \narXiv preprint, arXiv:2004.04968, 2020.\n](https://arxiv.org/abs/2004.04968)\n\n[\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  \n\"Towards Good Practice for Action Recognition with Spatiotemporal 3D Convolutions\",  \nProceedings of the International Conference on Pattern Recognition, pp. 2516-2521, 2018.\n](https://ieeexplore.ieee.org/document/8546325)\n\n[\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  \n\"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\",  \nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6546-6555, 2018.\n](http://openaccess.thecvf.com/content_cvpr_2018/html/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.html)\n\n[\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  \n\"Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition\",  \nProceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition, 2017.\n](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w44/Hara_Learning_Spatio-Temporal_Features_ICCV_2017_paper.pdf)\n\nThis code includes training, fine-tuning and testing on Kinetics, Moments in Time, ActivityNet, UCF-101, and HMDB-51.\n\n## Citation\n\nIf you use this code or pre-trained models, please cite the following:\n\n```bibtex\n@inproceedings{hara3dcnns,\n  author={Kensho Hara and Hirokatsu Kataoka and Yutaka Satoh},\n  title={Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  pages={6546--6555},\n  year={2018},\n}\n```\n\n## Pre-trained models\n\nPre-trained models are available [here](https://drive.google.com/open?id=1xbYbZ7rpyjftI_KCk6YuL-XrfQDz7Yd4).  \nAll models are trained on Kinetics-700 (_K_), Moments in Time (_M_), STAIR-Actions (_S_), or merged datasets of them (_KM_, _KS_, _MS_, _KMS_).  \nIf you want to finetune the models on your dataset, you should specify the following options.\n\n```misc\nr3d18_K_200ep.pth: --model resnet --model_depth 18 --n_pretrain_classes 700\nr3d18_KM_200ep.pth: --model resnet --model_depth 18 --n_pretrain_classes 1039\nr3d34_K_200ep.pth: --model resnet --model_depth 34 --n_pretrain_classes 700\nr3d34_KM_200ep.pth: --model resnet --model_depth 34 --n_pretrain_classes 1039\nr3d50_K_200ep.pth: --model resnet --model_depth 50 --n_pretrain_classes 700\nr3d50_KM_200ep.pth: --model resnet --model_depth 50 --n_pretrain_classes 1039\nr3d50_KMS_200ep.pth: --model resnet --model_depth 50 --n_pretrain_classes 1139\nr3d50_KS_200ep.pth: --model resnet --model_depth 50 --n_pretrain_classes 800\nr3d50_M_200ep.pth: --model resnet --model_depth 50 --n_pretrain_classes 339\nr3d50_MS_200ep.pth: --model resnet --model_depth 50 --n_pretrain_classes 439\nr3d50_S_200ep.pth: --model resnet --model_depth 50 --n_pretrain_classes 100\nr3d101_K_200ep.pth: --model resnet --model_depth 101 --n_pretrain_classes 700\nr3d101_KM_200ep.pth: --model resnet --model_depth 101 --n_pretrain_classes 1039\nr3d152_K_200ep.pth: --model resnet --model_depth 152 --n_pretrain_classes 700\nr3d152_KM_200ep.pth: --model resnet --model_depth 152 --n_pretrain_classes 1039\nr3d200_K_200ep.pth: --model resnet --model_depth 200 --n_pretrain_classes 700\nr3d200_KM_200ep.pth: --model resnet --model_depth 200 --n_pretrain_classes 1039\n```\n\nOld pretrained models are still available [here](https://drive.google.com/drive/folders/1zvl89AgFAApbH0At-gMuZSeQB_LpNP-M?usp=sharing).  \nHowever, some modifications are required to use the old pretrained models in the current scripts.\n\n## Requirements\n\n* [PyTorch](http://pytorch.org/) (ver. 0.4+ required)\n\n```bash\nconda install pytorch torchvision cudatoolkit=10.1 -c soumith\n```\n\n* FFmpeg, FFprobe\n\n* Python 3\n\n## Preparation\n\n### ActivityNet\n\n* Download videos using [the official crawler](https://github.com/activitynet/ActivityNet/tree/master/Crawler).\n* Convert from avi to jpg files using ```util_scripts/generate_video_jpgs.py```\n\n```bash\npython -m util_scripts.generate_video_jpgs mp4_video_dir_path jpg_video_dir_path activitynet\n```\n\n* Add fps infomartion into the json file ```util_scripts/add_fps_into_activitynet_json.py```\n\n```bash\npython -m util_scripts.add_fps_into_activitynet_json mp4_video_dir_path json_file_path\n```\n\n### Kinetics\n\n* Download videos using [the official crawler](https://github.com/activitynet/ActivityNet/tree/master/Crawler/Kinetics).\n  * Locate test set in ```video_directory/test```.\n* Convert from avi to jpg files using ```util_scripts/generate_video_jpgs.py```\n\n```bash\npython -m util_scripts.generate_video_jpgs mp4_video_dir_path jpg_video_dir_path kinetics\n```\n\n* Generate annotation file in json format similar to ActivityNet using ```util_scripts/kinetics_json.py```\n  * The CSV files (kinetics_{train, val, test}.csv) are included in the crawler.\n\n```bash\npython -m util_scripts.kinetics_json csv_dir_path 700 jpg_video_dir_path jpg dst_json_path\n```\n\n### UCF-101\n\n* Download videos and train/test splits [here](http://crcv.ucf.edu/data/UCF101.php).\n* Convert from avi to jpg files using ```util_scripts/generate_video_jpgs.py```\n\n```bash\npython -m util_scripts.generate_video_jpgs avi_video_dir_path jpg_video_dir_path ucf101\n```\n\n* Generate annotation file in json format similar to ActivityNet using ```util_scripts/ucf101_json.py```\n  * ```annotation_dir_path``` includes classInd.txt, trainlist0{1, 2, 3}.txt, testlist0{1, 2, 3}.txt\n\n```bash\npython -m util_scripts.ucf101_json annotation_dir_path jpg_video_dir_path dst_json_path\n```\n\n### HMDB-51\n\n* Download videos and train/test splits [here](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/).\n* Convert from avi to jpg files using ```util_scripts/generate_video_jpgs.py```\n\n```bash\npython -m util_scripts.generate_video_jpgs avi_video_dir_path jpg_video_dir_path hmdb51\n```\n\n* Generate annotation file in json format similar to ActivityNet using ```util_scripts/hmdb51_json.py```\n  * ```annotation_dir_path``` includes brush_hair_test_split1.txt, ...\n\n```bash\npython -m util_scripts.hmdb51_json annotation_dir_path jpg_video_dir_path dst_json_path\n```\n\n## Running the code\n\nAssume the structure of data directories is the following:\n\n```misc\n~/\n  data/\n    kinetics_videos/\n      jpg/\n        .../ (directories of class names)\n          .../ (directories of video names)\n            ... (jpg files)\n    results/\n      save_100.pth\n    kinetics.json\n```\n\nConfirm all options.\n\n```bash\npython main.py -h\n```\n\nTrain ResNets-50 on the Kinetics-700 dataset (700 classes) with 4 CPU threads (for data loading).  \nBatch size is 128.  \nSave models at every 5 epochs.\nAll GPUs is used for the training.\nIf you want a part of GPUs, use ```CUDA_VISIBLE_DEVICES=...```.\n\n```bash\npython main.py --root_path ~/data --video_path kinetics_videos/jpg --annotation_path kinetics.json \\\n--result_path results --dataset kinetics --model resnet \\\n--model_depth 50 --n_classes 700 --batch_size 128 --n_threads 4 --checkpoint 5\n```\n\nContinue Training from epoch 101. (~/data/results/save_100.pth is loaded.)\n\n```bash\npython main.py --root_path ~/data --video_path kinetics_videos/jpg --annotation_path kinetics.json \\\n--result_path results --dataset kinetics --resume_path results/save_100.pth \\\n--model_depth 50 --n_classes 700 --batch_size 128 --n_threads 4 --checkpoint 5\n```\n\nCalculate top-5 class probabilities of each video using a trained model (~/data/results/save_200.pth.)  \nNote that ```inference_batch_size``` should be small because actual batch size is calculated by ```inference_batch_size * (n_video_frames / inference_stride)```.\n\n```bash\npython main.py --root_path ~/data --video_path kinetics_videos/jpg --annotation_path kinetics.json \\\n--result_path results --dataset kinetics --resume_path results/save_200.pth \\\n--model_depth 50 --n_classes 700 --n_threads 4 --no_train --no_val --inference --output_topk 5 --inference_batch_size 1\n```\n\nEvaluate top-1 video accuracy of a recognition result (~/data/results/val.json).\n\n```bash\npython -m util_scripts.eval_accuracy ~/data/kinetics.json ~/data/results/val.json --subset val -k 1 --ignore\n```\n\nFine-tune fc layers of a pretrained model (~/data/models/resnet-50-kinetics.pth) on UCF-101.\n\n```bash\npython main.py --root_path ~/data --video_path ucf101_videos/jpg --annotation_path ucf101_01.json \\\n--result_path results --dataset ucf101 --n_classes 101 --n_pretrain_classes 700 \\\n--pretrain_path models/resnet-50-kinetics.pth --ft_begin_module fc \\\n--model resnet --model_depth 50 --batch_size 128 --n_threads 4 --checkpoint 5\n```\n"
        },
        {
          "name": "dataset.py",
          "type": "blob",
          "size": 7.439453125,
          "content": "from torchvision import get_image_backend\n\nfrom datasets.videodataset import VideoDataset\nfrom datasets.videodataset_multiclips import (VideoDatasetMultiClips,\n                                              collate_fn)\nfrom datasets.activitynet import ActivityNet\nfrom datasets.loader import VideoLoader, VideoLoaderHDF5, VideoLoaderFlowHDF5\n\n\ndef image_name_formatter(x):\n    return f'image_{x:05d}.jpg'\n\n\ndef get_training_data(video_path,\n                      annotation_path,\n                      dataset_name,\n                      input_type,\n                      file_type,\n                      spatial_transform=None,\n                      temporal_transform=None,\n                      target_transform=None):\n    assert dataset_name in [\n        'kinetics', 'activitynet', 'ucf101', 'hmdb51', 'mit'\n    ]\n    assert input_type in ['rgb', 'flow']\n    assert file_type in ['jpg', 'hdf5']\n\n    if file_type == 'jpg':\n        assert input_type == 'rgb', 'flow input is supported only when input type is hdf5.'\n\n        if get_image_backend() == 'accimage':\n            from datasets.loader import ImageLoaderAccImage\n            loader = VideoLoader(image_name_formatter, ImageLoaderAccImage())\n        else:\n            loader = VideoLoader(image_name_formatter)\n\n        video_path_formatter = (\n            lambda root_path, label, video_id: root_path / label / video_id)\n    else:\n        if input_type == 'rgb':\n            loader = VideoLoaderHDF5()\n        else:\n            loader = VideoLoaderFlowHDF5()\n        video_path_formatter = (lambda root_path, label, video_id: root_path /\n                                label / f'{video_id}.hdf5')\n\n    if dataset_name == 'activitynet':\n        training_data = ActivityNet(video_path,\n                                    annotation_path,\n                                    'training',\n                                    spatial_transform=spatial_transform,\n                                    temporal_transform=temporal_transform,\n                                    target_transform=target_transform,\n                                    video_loader=loader,\n                                    video_path_formatter=video_path_formatter)\n    else:\n        training_data = VideoDataset(video_path,\n                                     annotation_path,\n                                     'training',\n                                     spatial_transform=spatial_transform,\n                                     temporal_transform=temporal_transform,\n                                     target_transform=target_transform,\n                                     video_loader=loader,\n                                     video_path_formatter=video_path_formatter)\n\n    return training_data\n\n\ndef get_validation_data(video_path,\n                        annotation_path,\n                        dataset_name,\n                        input_type,\n                        file_type,\n                        spatial_transform=None,\n                        temporal_transform=None,\n                        target_transform=None):\n    assert dataset_name in [\n        'kinetics', 'activitynet', 'ucf101', 'hmdb51', 'mit'\n    ]\n    assert input_type in ['rgb', 'flow']\n    assert file_type in ['jpg', 'hdf5']\n\n    if file_type == 'jpg':\n        assert input_type == 'rgb', 'flow input is supported only when input type is hdf5.'\n\n        if get_image_backend() == 'accimage':\n            from datasets.loader import ImageLoaderAccImage\n            loader = VideoLoader(image_name_formatter, ImageLoaderAccImage())\n        else:\n            loader = VideoLoader(image_name_formatter)\n\n        video_path_formatter = (\n            lambda root_path, label, video_id: root_path / label / video_id)\n    else:\n        if input_type == 'rgb':\n            loader = VideoLoaderHDF5()\n        else:\n            loader = VideoLoaderFlowHDF5()\n        video_path_formatter = (lambda root_path, label, video_id: root_path /\n                                label / f'{video_id}.hdf5')\n\n    if dataset_name == 'activitynet':\n        validation_data = ActivityNet(video_path,\n                                      annotation_path,\n                                      'validation',\n                                      spatial_transform=spatial_transform,\n                                      temporal_transform=temporal_transform,\n                                      target_transform=target_transform,\n                                      video_loader=loader,\n                                      video_path_formatter=video_path_formatter)\n    else:\n        validation_data = VideoDatasetMultiClips(\n            video_path,\n            annotation_path,\n            'validation',\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            video_loader=loader,\n            video_path_formatter=video_path_formatter)\n\n    return validation_data, collate_fn\n\n\ndef get_inference_data(video_path,\n                       annotation_path,\n                       dataset_name,\n                       input_type,\n                       file_type,\n                       inference_subset,\n                       spatial_transform=None,\n                       temporal_transform=None,\n                       target_transform=None):\n    assert dataset_name in [\n        'kinetics', 'activitynet', 'ucf101', 'hmdb51', 'mit'\n    ]\n    assert input_type in ['rgb', 'flow']\n    assert file_type in ['jpg', 'hdf5']\n    assert inference_subset in ['train', 'val', 'test']\n\n    if file_type == 'jpg':\n        assert input_type == 'rgb', 'flow input is supported only when input type is hdf5.'\n\n        if get_image_backend() == 'accimage':\n            from datasets.loader import ImageLoaderAccImage\n            loader = VideoLoader(image_name_formatter, ImageLoaderAccImage())\n        else:\n            loader = VideoLoader(image_name_formatter)\n\n        video_path_formatter = (\n            lambda root_path, label, video_id: root_path / label / video_id)\n    else:\n        if input_type == 'rgb':\n            loader = VideoLoaderHDF5()\n        else:\n            loader = VideoLoaderFlowHDF5()\n        video_path_formatter = (lambda root_path, label, video_id: root_path /\n                                label / f'{video_id}.hdf5')\n\n    if inference_subset == 'train':\n        subset = 'training'\n    elif inference_subset == 'val':\n        subset = 'validation'\n    elif inference_subset == 'test':\n        subset = 'testing'\n    if dataset_name == 'activitynet':\n        inference_data = ActivityNet(video_path,\n                                     annotation_path,\n                                     subset,\n                                     spatial_transform=spatial_transform,\n                                     temporal_transform=temporal_transform,\n                                     target_transform=target_transform,\n                                     video_loader=loader,\n                                     video_path_formatter=video_path_formatter,\n                                     is_untrimmed_setting=True)\n    else:\n        inference_data = VideoDatasetMultiClips(\n            video_path,\n            annotation_path,\n            subset,\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            video_loader=loader,\n            video_path_formatter=video_path_formatter,\n            target_type=['video_id', 'segment'])\n\n    return inference_data, collate_fn"
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 2.78515625,
          "content": "import time\nimport json\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn.functional as F\n\nfrom utils import AverageMeter\n\n\ndef get_video_results(outputs, class_names, output_topk):\n    sorted_scores, locs = torch.topk(outputs,\n                                     k=min(output_topk, len(class_names)))\n\n    video_results = []\n    for i in range(sorted_scores.size(0)):\n        video_results.append({\n            'label': class_names[locs[i].item()],\n            'score': sorted_scores[i].item()\n        })\n\n    return video_results\n\n\ndef inference(data_loader, model, result_path, class_names, no_average,\n              output_topk):\n    print('inference')\n\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    results = {'results': defaultdict(list)}\n\n    end_time = time.time()\n\n    with torch.no_grad():\n        for i, (inputs, targets) in enumerate(data_loader):\n            data_time.update(time.time() - end_time)\n\n            video_ids, segments = zip(*targets)\n            outputs = model(inputs)\n            outputs = F.softmax(outputs, dim=1).cpu()\n\n            for j in range(outputs.size(0)):\n                results['results'][video_ids[j]].append({\n                    'segment': segments[j],\n                    'output': outputs[j]\n                })\n\n            batch_time.update(time.time() - end_time)\n            end_time = time.time()\n\n            print('[{}/{}]\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'.format(\n                      i + 1,\n                      len(data_loader),\n                      batch_time=batch_time,\n                      data_time=data_time))\n\n    inference_results = {'results': {}}\n    if not no_average:\n        for video_id, video_results in results['results'].items():\n            video_outputs = [\n                segment_result['output'] for segment_result in video_results\n            ]\n            video_outputs = torch.stack(video_outputs)\n            average_scores = torch.mean(video_outputs, dim=0)\n            inference_results['results'][video_id] = get_video_results(\n                average_scores, class_names, output_topk)\n    else:\n        for video_id, video_results in results['results'].items():\n            inference_results['results'][video_id] = []\n            for segment_result in video_results:\n                segment = segment_result['segment']\n                result = get_video_results(segment_result['output'],\n                                           class_names, output_topk)\n                inference_results['results'][video_id].append({\n                    'segment': segment,\n                    'result': result\n                })\n\n    with result_path.open('w') as f:\n        json.dump(inference_results, f)\n"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 16.2880859375,
          "content": "from pathlib import Path\nimport json\nimport random\nimport os\n\nimport numpy as np\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import SGD, lr_scheduler\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.backends import cudnn\nimport torchvision\n\nfrom opts import parse_opts\nfrom model import (generate_model, load_pretrained_model, make_data_parallel,\n                   get_fine_tuning_parameters)\nfrom mean import get_mean_std\nfrom spatial_transforms import (Compose, Normalize, Resize, CenterCrop,\n                                CornerCrop, MultiScaleCornerCrop,\n                                RandomResizedCrop, RandomHorizontalFlip,\n                                ToTensor, ScaleValue, ColorJitter,\n                                PickFirstChannels)\nfrom temporal_transforms import (LoopPadding, TemporalRandomCrop,\n                                 TemporalCenterCrop, TemporalEvenCrop,\n                                 SlidingWindow, TemporalSubsampling)\nfrom temporal_transforms import Compose as TemporalCompose\nfrom dataset import get_training_data, get_validation_data, get_inference_data\nfrom utils import Logger, worker_init_fn, get_lr\nfrom training import train_epoch\nfrom validation import val_epoch\nimport inference\n\n\ndef json_serial(obj):\n    if isinstance(obj, Path):\n        return str(obj)\n\n\ndef get_opt():\n    opt = parse_opts()\n\n    if opt.root_path is not None:\n        opt.video_path = opt.root_path / opt.video_path\n        opt.annotation_path = opt.root_path / opt.annotation_path\n        opt.result_path = opt.root_path / opt.result_path\n        if opt.resume_path is not None:\n            opt.resume_path = opt.root_path / opt.resume_path\n        if opt.pretrain_path is not None:\n            opt.pretrain_path = opt.root_path / opt.pretrain_path\n\n    if opt.pretrain_path is not None:\n        opt.n_finetune_classes = opt.n_classes\n        opt.n_classes = opt.n_pretrain_classes\n\n    if opt.output_topk <= 0:\n        opt.output_topk = opt.n_classes\n\n    if opt.inference_batch_size == 0:\n        opt.inference_batch_size = opt.batch_size\n\n    opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n    opt.begin_epoch = 1\n    opt.mean, opt.std = get_mean_std(opt.value_scale, dataset=opt.mean_dataset)\n    opt.n_input_channels = 3\n    if opt.input_type == 'flow':\n        opt.n_input_channels = 2\n        opt.mean = opt.mean[:2]\n        opt.std = opt.std[:2]\n\n    if opt.distributed:\n        opt.dist_rank = int(os.environ[\"OMPI_COMM_WORLD_RANK\"])\n\n        if opt.dist_rank == 0:\n            print(opt)\n            with (opt.result_path / 'opts.json').open('w') as opt_file:\n                json.dump(vars(opt), opt_file, default=json_serial)\n    else:\n        print(opt)\n        with (opt.result_path / 'opts.json').open('w') as opt_file:\n            json.dump(vars(opt), opt_file, default=json_serial)\n\n    return opt\n\n\ndef resume_model(resume_path, arch, model):\n    print('loading checkpoint {} model'.format(resume_path))\n    checkpoint = torch.load(resume_path, map_location='cpu')\n    assert arch == checkpoint['arch']\n\n    if hasattr(model, 'module'):\n        model.module.load_state_dict(checkpoint['state_dict'])\n    else:\n        model.load_state_dict(checkpoint['state_dict'])\n\n    return model\n\n\ndef resume_train_utils(resume_path, begin_epoch, optimizer, scheduler):\n    print('loading checkpoint {} train utils'.format(resume_path))\n    checkpoint = torch.load(resume_path, map_location='cpu')\n\n    begin_epoch = checkpoint['epoch'] + 1\n    if optimizer is not None and 'optimizer' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n    if scheduler is not None and 'scheduler' in checkpoint:\n        scheduler.load_state_dict(checkpoint['scheduler'])\n\n    return begin_epoch, optimizer, scheduler\n\n\ndef get_normalize_method(mean, std, no_mean_norm, no_std_norm):\n    if no_mean_norm:\n        if no_std_norm:\n            return Normalize([0, 0, 0], [1, 1, 1])\n        else:\n            return Normalize([0, 0, 0], std)\n    else:\n        if no_std_norm:\n            return Normalize(mean, [1, 1, 1])\n        else:\n            return Normalize(mean, std)\n\n\ndef get_train_utils(opt, model_parameters):\n    assert opt.train_crop in ['random', 'corner', 'center']\n    spatial_transform = []\n    if opt.train_crop == 'random':\n        spatial_transform.append(\n            RandomResizedCrop(\n                opt.sample_size, (opt.train_crop_min_scale, 1.0),\n                (opt.train_crop_min_ratio, 1.0 / opt.train_crop_min_ratio)))\n    elif opt.train_crop == 'corner':\n        scales = [1.0]\n        scale_step = 1 / (2**(1 / 4))\n        for _ in range(1, 5):\n            scales.append(scales[-1] * scale_step)\n        spatial_transform.append(MultiScaleCornerCrop(opt.sample_size, scales))\n    elif opt.train_crop == 'center':\n        spatial_transform.append(Resize(opt.sample_size))\n        spatial_transform.append(CenterCrop(opt.sample_size))\n    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n                                     opt.no_std_norm)\n    if not opt.no_hflip:\n        spatial_transform.append(RandomHorizontalFlip())\n    if opt.colorjitter:\n        spatial_transform.append(ColorJitter())\n    spatial_transform.append(ToTensor())\n    if opt.input_type == 'flow':\n        spatial_transform.append(PickFirstChannels(n=2))\n    spatial_transform.append(ScaleValue(opt.value_scale))\n    spatial_transform.append(normalize)\n    spatial_transform = Compose(spatial_transform)\n\n    assert opt.train_t_crop in ['random', 'center']\n    temporal_transform = []\n    if opt.sample_t_stride > 1:\n        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n    if opt.train_t_crop == 'random':\n        temporal_transform.append(TemporalRandomCrop(opt.sample_duration))\n    elif opt.train_t_crop == 'center':\n        temporal_transform.append(TemporalCenterCrop(opt.sample_duration))\n    temporal_transform = TemporalCompose(temporal_transform)\n\n    train_data = get_training_data(opt.video_path, opt.annotation_path,\n                                   opt.dataset, opt.input_type, opt.file_type,\n                                   spatial_transform, temporal_transform)\n    if opt.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            train_data)\n    else:\n        train_sampler = None\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=opt.batch_size,\n                                               shuffle=(train_sampler is None),\n                                               num_workers=opt.n_threads,\n                                               pin_memory=True,\n                                               sampler=train_sampler,\n                                               worker_init_fn=worker_init_fn)\n\n    if opt.is_master_node:\n        train_logger = Logger(opt.result_path / 'train.log',\n                              ['epoch', 'loss', 'acc', 'lr'])\n        train_batch_logger = Logger(\n            opt.result_path / 'train_batch.log',\n            ['epoch', 'batch', 'iter', 'loss', 'acc', 'lr'])\n    else:\n        train_logger = None\n        train_batch_logger = None\n\n    if opt.nesterov:\n        dampening = 0\n    else:\n        dampening = opt.dampening\n    optimizer = SGD(model_parameters,\n                    lr=opt.learning_rate,\n                    momentum=opt.momentum,\n                    dampening=dampening,\n                    weight_decay=opt.weight_decay,\n                    nesterov=opt.nesterov)\n\n    assert opt.lr_scheduler in ['plateau', 'multistep']\n    assert not (opt.lr_scheduler == 'plateau' and opt.no_val)\n    if opt.lr_scheduler == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(\n            optimizer, 'min', patience=opt.plateau_patience)\n    else:\n        scheduler = lr_scheduler.MultiStepLR(optimizer,\n                                             opt.multistep_milestones)\n\n    return (train_loader, train_sampler, train_logger, train_batch_logger,\n            optimizer, scheduler)\n\n\ndef get_val_utils(opt):\n    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n                                     opt.no_std_norm)\n    spatial_transform = [\n        Resize(opt.sample_size),\n        CenterCrop(opt.sample_size),\n        ToTensor()\n    ]\n    if opt.input_type == 'flow':\n        spatial_transform.append(PickFirstChannels(n=2))\n    spatial_transform.extend([ScaleValue(opt.value_scale), normalize])\n    spatial_transform = Compose(spatial_transform)\n\n    temporal_transform = []\n    if opt.sample_t_stride > 1:\n        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n    temporal_transform.append(\n        TemporalEvenCrop(opt.sample_duration, opt.n_val_samples))\n    temporal_transform = TemporalCompose(temporal_transform)\n\n    val_data, collate_fn = get_validation_data(opt.video_path,\n                                               opt.annotation_path, opt.dataset,\n                                               opt.input_type, opt.file_type,\n                                               spatial_transform,\n                                               temporal_transform)\n    if opt.distributed:\n        val_sampler = torch.utils.data.distributed.DistributedSampler(\n            val_data, shuffle=False)\n    else:\n        val_sampler = None\n    val_loader = torch.utils.data.DataLoader(val_data,\n                                             batch_size=(opt.batch_size //\n                                                         opt.n_val_samples),\n                                             shuffle=False,\n                                             num_workers=opt.n_threads,\n                                             pin_memory=True,\n                                             sampler=val_sampler,\n                                             worker_init_fn=worker_init_fn,\n                                             collate_fn=collate_fn)\n\n    if opt.is_master_node:\n        val_logger = Logger(opt.result_path / 'val.log',\n                            ['epoch', 'loss', 'acc'])\n    else:\n        val_logger = None\n\n    return val_loader, val_logger\n\n\ndef get_inference_utils(opt):\n    assert opt.inference_crop in ['center', 'nocrop']\n\n    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n                                     opt.no_std_norm)\n\n    spatial_transform = [Resize(opt.sample_size)]\n    if opt.inference_crop == 'center':\n        spatial_transform.append(CenterCrop(opt.sample_size))\n    spatial_transform.append(ToTensor())\n    if opt.input_type == 'flow':\n        spatial_transform.append(PickFirstChannels(n=2))\n    spatial_transform.extend([ScaleValue(opt.value_scale), normalize])\n    spatial_transform = Compose(spatial_transform)\n\n    temporal_transform = []\n    if opt.sample_t_stride > 1:\n        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n    temporal_transform.append(\n        SlidingWindow(opt.sample_duration, opt.inference_stride))\n    temporal_transform = TemporalCompose(temporal_transform)\n\n    inference_data, collate_fn = get_inference_data(\n        opt.video_path, opt.annotation_path, opt.dataset, opt.input_type,\n        opt.file_type, opt.inference_subset, spatial_transform,\n        temporal_transform)\n\n    inference_loader = torch.utils.data.DataLoader(\n        inference_data,\n        batch_size=opt.inference_batch_size,\n        shuffle=False,\n        num_workers=opt.n_threads,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn,\n        collate_fn=collate_fn)\n\n    return inference_loader, inference_data.class_names\n\n\ndef save_checkpoint(save_file_path, epoch, arch, model, optimizer, scheduler):\n    if hasattr(model, 'module'):\n        model_state_dict = model.module.state_dict()\n    else:\n        model_state_dict = model.state_dict()\n    save_states = {\n        'epoch': epoch,\n        'arch': arch,\n        'state_dict': model_state_dict,\n        'optimizer': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict()\n    }\n    torch.save(save_states, save_file_path)\n\n\ndef main_worker(index, opt):\n    random.seed(opt.manual_seed)\n    np.random.seed(opt.manual_seed)\n    torch.manual_seed(opt.manual_seed)\n\n    if index >= 0 and opt.device.type == 'cuda':\n        opt.device = torch.device(f'cuda:{index}')\n\n    if opt.distributed:\n        opt.dist_rank = opt.dist_rank * opt.ngpus_per_node + index\n        dist.init_process_group(backend='nccl',\n                                init_method=opt.dist_url,\n                                world_size=opt.world_size,\n                                rank=opt.dist_rank)\n        opt.batch_size = int(opt.batch_size / opt.ngpus_per_node)\n        opt.n_threads = int(\n            (opt.n_threads + opt.ngpus_per_node - 1) / opt.ngpus_per_node)\n    opt.is_master_node = not opt.distributed or opt.dist_rank == 0\n\n    model = generate_model(opt)\n    if opt.batchnorm_sync:\n        assert opt.distributed, 'SyncBatchNorm only supports DistributedDataParallel.'\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n    if opt.pretrain_path:\n        model = load_pretrained_model(model, opt.pretrain_path, opt.model,\n                                      opt.n_finetune_classes)\n    if opt.resume_path is not None:\n        model = resume_model(opt.resume_path, opt.arch, model)\n    model = make_data_parallel(model, opt.distributed, opt.device)\n\n    if opt.pretrain_path:\n        parameters = get_fine_tuning_parameters(model, opt.ft_begin_module)\n    else:\n        parameters = model.parameters()\n\n    if opt.is_master_node:\n        print(model)\n\n    criterion = CrossEntropyLoss().to(opt.device)\n\n    if not opt.no_train:\n        (train_loader, train_sampler, train_logger, train_batch_logger,\n         optimizer, scheduler) = get_train_utils(opt, parameters)\n        if opt.resume_path is not None:\n            opt.begin_epoch, optimizer, scheduler = resume_train_utils(\n                opt.resume_path, opt.begin_epoch, optimizer, scheduler)\n            if opt.overwrite_milestones:\n                scheduler.milestones = opt.multistep_milestones\n    if not opt.no_val:\n        val_loader, val_logger = get_val_utils(opt)\n\n    if opt.tensorboard and opt.is_master_node:\n        from torch.utils.tensorboard import SummaryWriter\n        if opt.begin_epoch == 1:\n            tb_writer = SummaryWriter(log_dir=opt.result_path)\n        else:\n            tb_writer = SummaryWriter(log_dir=opt.result_path,\n                                      purge_step=opt.begin_epoch)\n    else:\n        tb_writer = None\n\n    prev_val_loss = None\n    for i in range(opt.begin_epoch, opt.n_epochs + 1):\n        if not opt.no_train:\n            if opt.distributed:\n                train_sampler.set_epoch(i)\n            current_lr = get_lr(optimizer)\n            train_epoch(i, train_loader, model, criterion, optimizer,\n                        opt.device, current_lr, train_logger,\n                        train_batch_logger, tb_writer, opt.distributed)\n\n            if i % opt.checkpoint == 0 and opt.is_master_node:\n                save_file_path = opt.result_path / 'save_{}.pth'.format(i)\n                save_checkpoint(save_file_path, i, opt.arch, model, optimizer,\n                                scheduler)\n\n        if not opt.no_val:\n            prev_val_loss = val_epoch(i, val_loader, model, criterion,\n                                      opt.device, val_logger, tb_writer,\n                                      opt.distributed)\n\n        if not opt.no_train and opt.lr_scheduler == 'multistep':\n            scheduler.step()\n        elif not opt.no_train and opt.lr_scheduler == 'plateau':\n            scheduler.step(prev_val_loss)\n\n    if opt.inference:\n        inference_loader, inference_class_names = get_inference_utils(opt)\n        inference_result_path = opt.result_path / '{}.json'.format(\n            opt.inference_subset)\n\n        inference.inference(inference_loader, model, inference_result_path,\n                            inference_class_names, opt.inference_no_average,\n                            opt.output_topk)\n\n\nif __name__ == '__main__':\n    opt = get_opt()\n\n    opt.device = torch.device('cpu' if opt.no_cuda else 'cuda')\n    if not opt.no_cuda:\n        cudnn.benchmark = True\n    if opt.accimage:\n        torchvision.set_image_backend('accimage')\n\n    opt.ngpus_per_node = torch.cuda.device_count()\n    if opt.distributed:\n        opt.world_size = opt.ngpus_per_node * opt.world_size\n        mp.spawn(main_worker, nprocs=opt.ngpus_per_node, args=(opt,))\n    else:\n        main_worker(-1, opt)"
        },
        {
          "name": "mean.py",
          "type": "blob",
          "size": 0.5029296875,
          "content": "def get_mean_std(value_scale, dataset):\n    assert dataset in ['activitynet', 'kinetics', '0.5']\n\n    if dataset == 'activitynet':\n        mean = [0.4477, 0.4209, 0.3906]\n        std = [0.2767, 0.2695, 0.2714]\n    elif dataset == 'kinetics':\n        mean = [0.4345, 0.4051, 0.3775]\n        std = [0.2768, 0.2713, 0.2737]\n    elif dataset == '0.5':\n        mean = [0.5, 0.5, 0.5]\n        std = [0.5, 0.5, 0.5]\n\n    mean = [x * value_scale for x in mean]\n    std = [x * value_scale for x in std]\n\n    return mean, std"
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 5.046875,
          "content": "import torch\nfrom torch import nn\n\nfrom models import resnet, resnet2p1d, pre_act_resnet, wide_resnet, resnext, densenet\n\n\ndef get_module_name(name):\n    name = name.split('.')\n    if name[0] == 'module':\n        i = 1\n    else:\n        i = 0\n    if name[i] == 'features':\n        i += 1\n\n    return name[i]\n\n\ndef get_fine_tuning_parameters(model, ft_begin_module):\n    if not ft_begin_module:\n        return model.parameters()\n\n    parameters = []\n    add_flag = False\n    for k, v in model.named_parameters():\n        if ft_begin_module == get_module_name(k):\n            add_flag = True\n\n        if add_flag:\n            parameters.append({'params': v})\n\n    return parameters\n\n\ndef generate_model(opt):\n    assert opt.model in [\n        'resnet', 'resnet2p1d', 'preresnet', 'wideresnet', 'resnext', 'densenet'\n    ]\n\n    if opt.model == 'resnet':\n        model = resnet.generate_model(model_depth=opt.model_depth,\n                                      n_classes=opt.n_classes,\n                                      n_input_channels=opt.n_input_channels,\n                                      shortcut_type=opt.resnet_shortcut,\n                                      conv1_t_size=opt.conv1_t_size,\n                                      conv1_t_stride=opt.conv1_t_stride,\n                                      no_max_pool=opt.no_max_pool,\n                                      widen_factor=opt.resnet_widen_factor)\n    elif opt.model == 'resnet2p1d':\n        model = resnet2p1d.generate_model(model_depth=opt.model_depth,\n                                          n_classes=opt.n_classes,\n                                          n_input_channels=opt.n_input_channels,\n                                          shortcut_type=opt.resnet_shortcut,\n                                          conv1_t_size=opt.conv1_t_size,\n                                          conv1_t_stride=opt.conv1_t_stride,\n                                          no_max_pool=opt.no_max_pool,\n                                          widen_factor=opt.resnet_widen_factor)\n    elif opt.model == 'wideresnet':\n        model = wide_resnet.generate_model(\n            model_depth=opt.model_depth,\n            k=opt.wide_resnet_k,\n            n_classes=opt.n_classes,\n            n_input_channels=opt.n_input_channels,\n            shortcut_type=opt.resnet_shortcut,\n            conv1_t_size=opt.conv1_t_size,\n            conv1_t_stride=opt.conv1_t_stride,\n            no_max_pool=opt.no_max_pool)\n    elif opt.model == 'resnext':\n        model = resnext.generate_model(model_depth=opt.model_depth,\n                                       cardinality=opt.resnext_cardinality,\n                                       n_classes=opt.n_classes,\n                                       n_input_channels=opt.n_input_channels,\n                                       shortcut_type=opt.resnet_shortcut,\n                                       conv1_t_size=opt.conv1_t_size,\n                                       conv1_t_stride=opt.conv1_t_stride,\n                                       no_max_pool=opt.no_max_pool)\n    elif opt.model == 'preresnet':\n        model = pre_act_resnet.generate_model(\n            model_depth=opt.model_depth,\n            n_classes=opt.n_classes,\n            n_input_channels=opt.n_input_channels,\n            shortcut_type=opt.resnet_shortcut,\n            conv1_t_size=opt.conv1_t_size,\n            conv1_t_stride=opt.conv1_t_stride,\n            no_max_pool=opt.no_max_pool)\n    elif opt.model == 'densenet':\n        model = densenet.generate_model(model_depth=opt.model_depth,\n                                        n_classes=opt.n_classes,\n                                        n_input_channels=opt.n_input_channels,\n                                        conv1_t_size=opt.conv1_t_size,\n                                        conv1_t_stride=opt.conv1_t_stride,\n                                        no_max_pool=opt.no_max_pool)\n\n    return model\n\n\ndef load_pretrained_model(model, pretrain_path, model_name, n_finetune_classes):\n    if pretrain_path:\n        print('loading pretrained model {}'.format(pretrain_path))\n        pretrain = torch.load(pretrain_path, map_location='cpu')\n\n        model.load_state_dict(pretrain['state_dict'])\n        tmp_model = model\n        if model_name == 'densenet':\n            tmp_model.classifier = nn.Linear(tmp_model.classifier.in_features,\n                                             n_finetune_classes)\n        else:\n            tmp_model.fc = nn.Linear(tmp_model.fc.in_features,\n                                     n_finetune_classes)\n\n    return model\n\n\ndef make_data_parallel(model, is_distributed, device):\n    if is_distributed:\n        if device.type == 'cuda' and device.index is not None:\n            torch.cuda.set_device(device)\n            model.to(device)\n\n            model = nn.parallel.DistributedDataParallel(model,\n                                                        device_ids=[device])\n        else:\n            model.to(device)\n            model = nn.parallel.DistributedDataParallel(model)\n    elif device.type == 'cuda':\n        model = nn.DataParallel(model, device_ids=None).cuda()\n\n    return model\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "opts.py",
          "type": "blob",
          "size": 11.8828125,
          "content": "import argparse\nfrom pathlib import Path\n\n\ndef parse_opts():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--root_path',\n                        default=None,\n                        type=Path,\n                        help='Root directory path')\n    parser.add_argument('--video_path',\n                        default=None,\n                        type=Path,\n                        help='Directory path of videos')\n    parser.add_argument('--annotation_path',\n                        default=None,\n                        type=Path,\n                        help='Annotation file path')\n    parser.add_argument('--result_path',\n                        default=None,\n                        type=Path,\n                        help='Result directory path')\n    parser.add_argument(\n        '--dataset',\n        default='kinetics',\n        type=str,\n        help='Used dataset (activitynet | kinetics | ucf101 | hmdb51)')\n    parser.add_argument(\n        '--n_classes',\n        default=400,\n        type=int,\n        help=\n        'Number of classes (activitynet: 200, kinetics: 400 or 600, ucf101: 101, hmdb51: 51)'\n    )\n    parser.add_argument('--n_pretrain_classes',\n                        default=0,\n                        type=int,\n                        help=('Number of classes of pretraining task.'\n                              'When using --pretrain_path, this must be set.'))\n    parser.add_argument('--pretrain_path',\n                        default=None,\n                        type=Path,\n                        help='Pretrained model path (.pth).')\n    parser.add_argument(\n        '--ft_begin_module',\n        default='',\n        type=str,\n        help=('Module name of beginning of fine-tuning'\n              '(conv1, layer1, fc, denseblock1, classifier, ...).'\n              'The default means all layers are fine-tuned.'))\n    parser.add_argument('--sample_size',\n                        default=112,\n                        type=int,\n                        help='Height and width of inputs')\n    parser.add_argument('--sample_duration',\n                        default=16,\n                        type=int,\n                        help='Temporal duration of inputs')\n    parser.add_argument(\n        '--sample_t_stride',\n        default=1,\n        type=int,\n        help='If larger than 1, input frames are subsampled with the stride.')\n    parser.add_argument(\n        '--train_crop',\n        default='random',\n        type=str,\n        help=('Spatial cropping method in training. '\n              'random is uniform. '\n              'corner is selection from 4 corners and 1 center. '\n              '(random | corner | center)'))\n    parser.add_argument('--train_crop_min_scale',\n                        default=0.25,\n                        type=float,\n                        help='Min scale for random cropping in training')\n    parser.add_argument('--train_crop_min_ratio',\n                        default=0.75,\n                        type=float,\n                        help='Min aspect ratio for random cropping in training')\n    parser.add_argument('--no_hflip',\n                        action='store_true',\n                        help='If true holizontal flipping is not performed.')\n    parser.add_argument('--colorjitter',\n                        action='store_true',\n                        help='If true colorjitter is performed.')\n    parser.add_argument('--train_t_crop',\n                        default='random',\n                        type=str,\n                        help=('Temporal cropping method in training. '\n                              'random is uniform. '\n                              '(random | center)'))\n    parser.add_argument('--learning_rate',\n                        default=0.1,\n                        type=float,\n                        help=('Initial learning rate'\n                              '(divided by 10 while training by lr scheduler)'))\n    parser.add_argument('--momentum', default=0.9, type=float, help='Momentum')\n    parser.add_argument('--dampening',\n                        default=0.0,\n                        type=float,\n                        help='dampening of SGD')\n    parser.add_argument('--weight_decay',\n                        default=1e-3,\n                        type=float,\n                        help='Weight Decay')\n    parser.add_argument('--mean_dataset',\n                        default='kinetics',\n                        type=str,\n                        help=('dataset for mean values of mean subtraction'\n                              '(activitynet | kinetics | 0.5)'))\n    parser.add_argument('--no_mean_norm',\n                        action='store_true',\n                        help='If true, inputs are not normalized by mean.')\n    parser.add_argument(\n        '--no_std_norm',\n        action='store_true',\n        help='If true, inputs are not normalized by standard deviation.')\n    parser.add_argument(\n        '--value_scale',\n        default=1,\n        type=int,\n        help=\n        'If 1, range of inputs is [0-1]. If 255, range of inputs is [0-255].')\n    parser.add_argument('--nesterov',\n                        action='store_true',\n                        help='Nesterov momentum')\n    parser.add_argument('--optimizer',\n                        default='sgd',\n                        type=str,\n                        help='Currently only support SGD')\n    parser.add_argument('--lr_scheduler',\n                        default='multistep',\n                        type=str,\n                        help='Type of LR scheduler (multistep | plateau)')\n    parser.add_argument(\n        '--multistep_milestones',\n        default=[50, 100, 150],\n        type=int,\n        nargs='+',\n        help='Milestones of LR scheduler. See documentation of MultistepLR.')\n    parser.add_argument(\n        '--overwrite_milestones',\n        action='store_true',\n        help='If true, overwriting multistep_milestones when resuming training.'\n    )\n    parser.add_argument(\n        '--plateau_patience',\n        default=10,\n        type=int,\n        help='Patience of LR scheduler. See documentation of ReduceLROnPlateau.'\n    )\n    parser.add_argument('--batch_size',\n                        default=128,\n                        type=int,\n                        help='Batch Size')\n    parser.add_argument(\n        '--inference_batch_size',\n        default=0,\n        type=int,\n        help='Batch Size for inference. 0 means this is the same as batch_size.'\n    )\n    parser.add_argument(\n        '--batchnorm_sync',\n        action='store_true',\n        help='If true, SyncBatchNorm is used instead of BatchNorm.')\n    parser.add_argument('--n_epochs',\n                        default=200,\n                        type=int,\n                        help='Number of total epochs to run')\n    parser.add_argument('--n_val_samples',\n                        default=3,\n                        type=int,\n                        help='Number of validation samples for each activity')\n    parser.add_argument('--resume_path',\n                        default=None,\n                        type=Path,\n                        help='Save data (.pth) of previous training')\n    parser.add_argument('--no_train',\n                        action='store_true',\n                        help='If true, training is not performed.')\n    parser.add_argument('--no_val',\n                        action='store_true',\n                        help='If true, validation is not performed.')\n    parser.add_argument('--inference',\n                        action='store_true',\n                        help='If true, inference is performed.')\n    parser.add_argument('--inference_subset',\n                        default='val',\n                        type=str,\n                        help='Used subset in inference (train | val | test)')\n    parser.add_argument('--inference_stride',\n                        default=16,\n                        type=int,\n                        help='Stride of sliding window in inference.')\n    parser.add_argument(\n        '--inference_crop',\n        default='center',\n        type=str,\n        help=('Cropping method in inference. (center | nocrop)'\n              'When nocrop, fully convolutional inference is performed,'\n              'and mini-batch consists of clips of one video.'))\n    parser.add_argument(\n        '--inference_no_average',\n        action='store_true',\n        help='If true, outputs for segments in a video are not averaged.')\n    parser.add_argument('--no_cuda',\n                        action='store_true',\n                        help='If true, cuda is not used.')\n    parser.add_argument('--n_threads',\n                        default=4,\n                        type=int,\n                        help='Number of threads for multi-thread loading')\n    parser.add_argument('--checkpoint',\n                        default=10,\n                        type=int,\n                        help='Trained model is saved at every this epochs.')\n    parser.add_argument(\n        '--model',\n        default='resnet',\n        type=str,\n        help=\n        '(resnet | resnet2p1d | preresnet | wideresnet | resnext | densenet | ')\n    parser.add_argument('--model_depth',\n                        default=18,\n                        type=int,\n                        help='Depth of resnet (10 | 18 | 34 | 50 | 101)')\n    parser.add_argument('--conv1_t_size',\n                        default=7,\n                        type=int,\n                        help='Kernel size in t dim of conv1.')\n    parser.add_argument('--conv1_t_stride',\n                        default=1,\n                        type=int,\n                        help='Stride in t dim of conv1.')\n    parser.add_argument('--no_max_pool',\n                        action='store_true',\n                        help='If true, the max pooling after conv1 is removed.')\n    parser.add_argument('--resnet_shortcut',\n                        default='B',\n                        type=str,\n                        help='Shortcut type of resnet (A | B)')\n    parser.add_argument(\n        '--resnet_widen_factor',\n        default=1.0,\n        type=float,\n        help='The number of feature maps of resnet is multiplied by this value')\n    parser.add_argument('--wide_resnet_k',\n                        default=2,\n                        type=int,\n                        help='Wide resnet k')\n    parser.add_argument('--resnext_cardinality',\n                        default=32,\n                        type=int,\n                        help='ResNeXt cardinality')\n    parser.add_argument('--input_type',\n                        default='rgb',\n                        type=str,\n                        help='(rgb | flow)')\n    parser.add_argument('--manual_seed',\n                        default=1,\n                        type=int,\n                        help='Manually set random seed')\n    parser.add_argument('--accimage',\n                        action='store_true',\n                        help='If true, accimage is used to load images.')\n    parser.add_argument('--output_topk',\n                        default=5,\n                        type=int,\n                        help='Top-k scores are saved in json file.')\n    parser.add_argument('--file_type',\n                        default='jpg',\n                        type=str,\n                        help='(jpg | hdf5)')\n    parser.add_argument('--tensorboard',\n                        action='store_true',\n                        help='If true, output tensorboard log file.')\n    parser.add_argument(\n        '--distributed',\n        action='store_true',\n        help='Use multi-processing distributed training to launch '\n        'N processes per node, which has N GPUs.')\n    parser.add_argument('--dist_url',\n                        default='tcp://127.0.0.1:23456',\n                        type=str,\n                        help='url used to set up distributed training')\n    parser.add_argument('--world_size',\n                        default=-1,\n                        type=int,\n                        help='number of nodes for distributed training')\n\n    args = parser.parse_args()\n\n    return args\n"
        },
        {
          "name": "spatial_transforms.py",
          "type": "blob",
          "size": 5.439453125,
          "content": "import random\n\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms import functional as F\nfrom PIL import Image\n\n\nclass Compose(transforms.Compose):\n\n    def randomize_parameters(self):\n        for t in self.transforms:\n            t.randomize_parameters()\n\n\nclass ToTensor(transforms.ToTensor):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass Normalize(transforms.Normalize):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass ScaleValue(object):\n\n    def __init__(self, s):\n        self.s = s\n\n    def __call__(self, tensor):\n        tensor *= self.s\n        return tensor\n\n    def randomize_parameters(self):\n        pass\n\n\nclass Resize(transforms.Resize):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass Scale(transforms.Scale):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass CenterCrop(transforms.CenterCrop):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass CornerCrop(object):\n\n    def __init__(self,\n                 size,\n                 crop_position=None,\n                 crop_positions=['c', 'tl', 'tr', 'bl', 'br']):\n        self.size = size\n        self.crop_position = crop_position\n        self.crop_positions = crop_positions\n\n        if crop_position is None:\n            self.randomize = True\n        else:\n            self.randomize = False\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        image_width = img.size[0]\n        image_height = img.size[1]\n\n        h, w = (self.size, self.size)\n        if self.crop_position == 'c':\n            i = int(round((image_height - h) / 2.))\n            j = int(round((image_width - w) / 2.))\n        elif self.crop_position == 'tl':\n            i = 0\n            j = 0\n        elif self.crop_position == 'tr':\n            i = 0\n            j = image_width - self.size\n        elif self.crop_position == 'bl':\n            i = image_height - self.size\n            j = 0\n        elif self.crop_position == 'br':\n            i = image_height - self.size\n            j = image_width - self.size\n\n        img = F.crop(img, i, j, h, w)\n\n        return img\n\n    def randomize_parameters(self):\n        if self.randomize:\n            self.crop_position = self.crop_positions[random.randint(\n                0,\n                len(self.crop_positions) - 1)]\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(size={0}, crop_position={1}, randomize={2})'.format(\n            self.size, self.crop_position, self.randomize)\n\n\nclass RandomHorizontalFlip(transforms.RandomHorizontalFlip):\n\n    def __init__(self, p=0.5):\n        super().__init__(p)\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL.Image): Image to be flipped.\n        Returns:\n            PIL.Image: Randomly flipped image.\n        \"\"\"\n        if self.random_p < self.p:\n            return F.hflip(img)\n        return img\n\n    def randomize_parameters(self):\n        self.random_p = random.random()\n\n\nclass MultiScaleCornerCrop(object):\n\n    def __init__(self,\n                 size,\n                 scales,\n                 crop_positions=['c', 'tl', 'tr', 'bl', 'br'],\n                 interpolation=Image.BILINEAR):\n        self.size = size\n        self.scales = scales\n        self.interpolation = interpolation\n        self.crop_positions = crop_positions\n\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        short_side = min(img.size[0], img.size[1])\n        crop_size = int(short_side * self.scale)\n        self.corner_crop.size = crop_size\n\n        img = self.corner_crop(img)\n        return img.resize((self.size, self.size), self.interpolation)\n\n    def randomize_parameters(self):\n        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n        crop_position = self.crop_positions[random.randint(\n            0,\n            len(self.crop_positions) - 1)]\n\n        self.corner_crop = CornerCrop(None, crop_position)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(size={0}, scales={1}, interpolation={2})'.format(\n            self.size, self.scales, self.interpolation)\n\n\nclass RandomResizedCrop(transforms.RandomResizedCrop):\n\n    def __init__(self,\n                 size,\n                 scale=(0.08, 1.0),\n                 ratio=(3. / 4., 4. / 3.),\n                 interpolation=Image.BILINEAR):\n        super().__init__(size, scale, ratio, interpolation)\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        if self.randomize:\n            self.random_crop = self.get_params(img, self.scale, self.ratio)\n            self.randomize = False\n\n        i, j, h, w = self.random_crop\n        return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n\n    def randomize_parameters(self):\n        self.randomize = True\n\n\nclass ColorJitter(transforms.ColorJitter):\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        if self.randomize:\n            self.transform = self.get_params(self.brightness, self.contrast,\n                                             self.saturation, self.hue)\n            self.randomize = False\n\n        return self.transform(img)\n\n    def randomize_parameters(self):\n        self.randomize = True\n\n\nclass PickFirstChannels(object):\n\n    def __init__(self, n):\n        self.n = n\n\n    def __call__(self, tensor):\n        return tensor[:self.n, :, :]\n\n    def randomize_parameters(self):\n        pass"
        },
        {
          "name": "temporal_transforms.py",
          "type": "blob",
          "size": 4.2109375,
          "content": "import random\nimport math\n\n\nclass Compose(object):\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, frame_indices):\n        for i, t in enumerate(self.transforms):\n            if isinstance(frame_indices[0], list):\n                next_transforms = Compose(self.transforms[i:])\n                dst_frame_indices = [\n                    next_transforms(clip_frame_indices)\n                    for clip_frame_indices in frame_indices\n                ]\n\n                return dst_frame_indices\n            else:\n                frame_indices = t(frame_indices)\n        return frame_indices\n\n\nclass LoopPadding(object):\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        out = frame_indices\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n\nclass TemporalBeginCrop(object):\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        out = frame_indices[:self.size]\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n\nclass TemporalCenterCrop(object):\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n\n        center_index = len(frame_indices) // 2\n        begin_index = max(0, center_index - (self.size // 2))\n        end_index = min(begin_index + self.size, len(frame_indices))\n\n        out = frame_indices[begin_index:end_index]\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n\nclass TemporalRandomCrop(object):\n\n    def __init__(self, size):\n        self.size = size\n        self.loop = LoopPadding(size)\n\n    def __call__(self, frame_indices):\n\n        rand_end = max(0, len(frame_indices) - self.size - 1)\n        begin_index = random.randint(0, rand_end)\n        end_index = min(begin_index + self.size, len(frame_indices))\n\n        out = frame_indices[begin_index:end_index]\n\n        if len(out) < self.size:\n            out = self.loop(out)\n\n        return out\n\n\nclass TemporalEvenCrop(object):\n\n    def __init__(self, size, n_samples=1):\n        self.size = size\n        self.n_samples = n_samples\n        self.loop = LoopPadding(size)\n\n    def __call__(self, frame_indices):\n        n_frames = len(frame_indices)\n        stride = max(\n            1, math.ceil((n_frames - 1 - self.size) / (self.n_samples - 1)))\n\n        out = []\n        for begin_index in frame_indices[::stride]:\n            if len(out) >= self.n_samples:\n                break\n            end_index = min(frame_indices[-1] + 1, begin_index + self.size)\n            sample = list(range(begin_index, end_index))\n\n            if len(sample) < self.size:\n                out.append(self.loop(sample))\n                break\n            else:\n                out.append(sample)\n\n        return out\n\n\nclass SlidingWindow(object):\n\n    def __init__(self, size, stride=0):\n        self.size = size\n        if stride == 0:\n            self.stride = self.size\n        else:\n            self.stride = stride\n        self.loop = LoopPadding(size)\n\n    def __call__(self, frame_indices):\n        out = []\n        for begin_index in frame_indices[::self.stride]:\n            end_index = min(frame_indices[-1] + 1, begin_index + self.size)\n            sample = list(range(begin_index, end_index))\n\n            if len(sample) < self.size:\n                out.append(self.loop(sample))\n                break\n            else:\n                out.append(sample)\n\n        return out\n\n\nclass TemporalSubsampling(object):\n\n    def __init__(self, stride):\n        self.stride = stride\n\n    def __call__(self, frame_indices):\n        return frame_indices[::self.stride]\n\n\nclass Shuffle(object):\n\n    def __init__(self, block_size):\n        self.block_size = block_size\n\n    def __call__(self, frame_indices):\n        frame_indices = [\n            frame_indices[i:(i + self.block_size)]\n            for i in range(0, len(frame_indices), self.block_size)\n        ]\n        random.shuffle(frame_indices)\n        frame_indices = [t for block in frame_indices for t in block]\n        return frame_indices"
        },
        {
          "name": "training.py",
          "type": "blob",
          "size": 3.5751953125,
          "content": "import torch\nimport time\nimport os\nimport sys\n\nimport torch\nimport torch.distributed as dist\n\nfrom utils import AverageMeter, calculate_accuracy\n\n\ndef train_epoch(epoch,\n                data_loader,\n                model,\n                criterion,\n                optimizer,\n                device,\n                current_lr,\n                epoch_logger,\n                batch_logger,\n                tb_writer=None,\n                distributed=False):\n    print('train at epoch {}'.format(epoch))\n\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n\n    end_time = time.time()\n    for i, (inputs, targets) in enumerate(data_loader):\n        data_time.update(time.time() - end_time)\n\n        targets = targets.to(device, non_blocking=True)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        acc = calculate_accuracy(outputs, targets)\n\n        losses.update(loss.item(), inputs.size(0))\n        accuracies.update(acc, inputs.size(0))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        if batch_logger is not None:\n            batch_logger.log({\n                'epoch': epoch,\n                'batch': i + 1,\n                'iter': (epoch - 1) * len(data_loader) + (i + 1),\n                'loss': losses.val,\n                'acc': accuracies.val,\n                'lr': current_lr\n            })\n\n        print('Epoch: [{0}][{1}/{2}]\\t'\n              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n              'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(epoch,\n                                                         i + 1,\n                                                         len(data_loader),\n                                                         batch_time=batch_time,\n                                                         data_time=data_time,\n                                                         loss=losses,\n                                                         acc=accuracies))\n\n    if distributed:\n        loss_sum = torch.tensor([losses.sum],\n                                dtype=torch.float32,\n                                device=device)\n        loss_count = torch.tensor([losses.count],\n                                  dtype=torch.float32,\n                                  device=device)\n        acc_sum = torch.tensor([accuracies.sum],\n                               dtype=torch.float32,\n                               device=device)\n        acc_count = torch.tensor([accuracies.count],\n                                 dtype=torch.float32,\n                                 device=device)\n\n        dist.all_reduce(loss_sum, op=dist.ReduceOp.SUM)\n        dist.all_reduce(loss_count, op=dist.ReduceOp.SUM)\n        dist.all_reduce(acc_sum, op=dist.ReduceOp.SUM)\n        dist.all_reduce(acc_count, op=dist.ReduceOp.SUM)\n\n        losses.avg = loss_sum.item() / loss_count.item()\n        accuracies.avg = acc_sum.item() / acc_count.item()\n\n    if epoch_logger is not None:\n        epoch_logger.log({\n            'epoch': epoch,\n            'loss': losses.avg,\n            'acc': accuracies.avg,\n            'lr': current_lr\n        })\n\n    if tb_writer is not None:\n        tb_writer.add_scalar('train/loss', losses.avg, epoch)\n        tb_writer.add_scalar('train/acc', accuracies.avg, epoch)\n        tb_writer.add_scalar('train/lr', accuracies.avg, epoch)\n"
        },
        {
          "name": "util_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 2.2705078125,
          "content": "import csv\nimport random\nfrom functools import partialmethod\n\nimport torch\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass Logger(object):\n\n    def __init__(self, path, header):\n        self.log_file = path.open('w')\n        self.logger = csv.writer(self.log_file, delimiter='\\t')\n\n        self.logger.writerow(header)\n        self.header = header\n\n    def __del(self):\n        self.log_file.close()\n\n    def log(self, values):\n        write_values = []\n        for col in self.header:\n            assert col in values\n            write_values.append(values[col])\n\n        self.logger.writerow(write_values)\n        self.log_file.flush()\n\n\ndef calculate_accuracy(outputs, targets):\n    with torch.no_grad():\n        batch_size = targets.size(0)\n\n        _, pred = outputs.topk(1, 1, largest=True, sorted=True)\n        pred = pred.t()\n        correct = pred.eq(targets.view(1, -1))\n        n_correct_elems = correct.float().sum().item()\n\n        return n_correct_elems / batch_size\n\n\ndef calculate_precision_and_recall(outputs, targets, pos_label=1):\n    with torch.no_grad():\n        _, pred = outputs.topk(1, 1, largest=True, sorted=True)\n        precision, recall, _, _ = precision_recall_fscore_support(\n            targets.view(-1, 1).cpu().numpy(),\n            pred.cpu().numpy())\n\n        return precision[pos_label], recall[pos_label]\n\n\ndef worker_init_fn(worker_id):\n    torch_seed = torch.initial_seed()\n\n    random.seed(torch_seed + worker_id)\n\n    if torch_seed >= 2**32:\n        torch_seed = torch_seed % 2**32\n    np.random.seed(torch_seed + worker_id)\n\n\ndef get_lr(optimizer):\n    lrs = []\n    for param_group in optimizer.param_groups:\n        lr = float(param_group['lr'])\n        lrs.append(lr)\n\n    return max(lrs)\n\n\ndef partialclass(cls, *args, **kwargs):\n\n    class PartialClass(cls):\n        __init__ = partialmethod(cls.__init__, *args, **kwargs)\n\n    return PartialClass"
        },
        {
          "name": "validation.py",
          "type": "blob",
          "size": 2.8447265625,
          "content": "import torch\nimport time\nimport sys\n\nimport torch\nimport torch.distributed as dist\n\nfrom utils import AverageMeter, calculate_accuracy\n\n\ndef val_epoch(epoch,\n              data_loader,\n              model,\n              criterion,\n              device,\n              logger,\n              tb_writer=None,\n              distributed=False):\n    print('validation at epoch {}'.format(epoch))\n\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n\n    end_time = time.time()\n\n    with torch.no_grad():\n        for i, (inputs, targets) in enumerate(data_loader):\n            data_time.update(time.time() - end_time)\n\n            targets = targets.to(device, non_blocking=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            acc = calculate_accuracy(outputs, targets)\n\n            losses.update(loss.item(), inputs.size(0))\n            accuracies.update(acc, inputs.size(0))\n\n            batch_time.update(time.time() - end_time)\n            end_time = time.time()\n\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n                      epoch,\n                      i + 1,\n                      len(data_loader),\n                      batch_time=batch_time,\n                      data_time=data_time,\n                      loss=losses,\n                      acc=accuracies))\n\n    if distributed:\n        loss_sum = torch.tensor([losses.sum],\n                                dtype=torch.float32,\n                                device=device)\n        loss_count = torch.tensor([losses.count],\n                                  dtype=torch.float32,\n                                  device=device)\n        acc_sum = torch.tensor([accuracies.sum],\n                               dtype=torch.float32,\n                               device=device)\n        acc_count = torch.tensor([accuracies.count],\n                                 dtype=torch.float32,\n                                 device=device)\n\n        dist.all_reduce(loss_sum, op=dist.ReduceOp.SUM)\n        dist.all_reduce(loss_count, op=dist.ReduceOp.SUM)\n        dist.all_reduce(acc_sum, op=dist.ReduceOp.SUM)\n        dist.all_reduce(acc_count, op=dist.ReduceOp.SUM)\n\n        losses.avg = loss_sum.item() / loss_count.item()\n        accuracies.avg = acc_sum.item() / acc_count.item()\n\n    if logger is not None:\n        logger.log({'epoch': epoch, 'loss': losses.avg, 'acc': accuracies.avg})\n\n    if tb_writer is not None:\n        tb_writer.add_scalar('val/loss', losses.avg, epoch)\n        tb_writer.add_scalar('val/acc', accuracies.avg, epoch)\n\n    return losses.avg\n"
        }
      ]
    }
  ]
}