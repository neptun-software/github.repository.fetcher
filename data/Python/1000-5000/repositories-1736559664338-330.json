{
  "metadata": {
    "timestamp": 1736559664338,
    "page": 330,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "qiyeboy/IPProxyPool",
      "stars": 4191,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.03125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n.idea/\n*.db"
        },
        {
          "name": "IPProxy.py",
          "type": "blob",
          "size": 0.74609375,
          "content": "# coding:utf-8\n\nfrom multiprocessing import Value, Queue, Process\nfrom api.apiServer import start_api_server\nfrom db.DataStore import store_data\n\nfrom validator.Validator import validator, getMyIP\nfrom spider.ProxyCrawl import startProxyCrawl\n\nfrom config import TASK_QUEUE_SIZE\n\nif __name__ == \"__main__\":\n    myip = getMyIP()\n    DB_PROXY_NUM = Value('i', 0)\n    q1 = Queue(maxsize=TASK_QUEUE_SIZE)\n    q2 = Queue()\n    p0 = Process(target=start_api_server)\n    p1 = Process(target=startProxyCrawl, args=(q1, DB_PROXY_NUM,myip))\n    p2 = Process(target=validator, args=(q1, q2, myip))\n    p3 = Process(target=store_data, args=(q2, DB_PROXY_NUM))\n    p0.start()\n    p1.start()\n    p2.start()\n    p3.start()\n    p0.join()\n    p1.join()\n    p2.join()\n    p3.join()\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.728515625,
          "content": "﻿# IPProxyPool\nIPProxyPool代理池项目，提供代理ip。支持py2和py3两个版本。\n### 我的新书[《Python爬虫开发与项目实战》](https://item.jd.com/12206762.html)出版了,喜欢的话可以看一下[样章](http://pan.baidu.com/s/1hrWEOYg)\n<br/>\n详细使用方式，请看我的博客:\nhttp://www.cnblogs.com/qiyeboy/p/5693128.html\n<br/>\n最近正在为IPProxyPool添加二级代理，方便调度。大家可以关注我的公众号，更新我会及时通知。\n<br/>\n\n#### 我的微信公众号:\n\n![](qiye2.jpg)\n<br/>\n希望大家提供更多的代理网站，现在爬取的好用的代理ip还是太少。\n<br/>\n同时感谢[super1-chen](https://github.com/super1-chen),[fancoo](https://github.com/fancoo),[Leibnizhu](https://github.com/Leibnizhu)对项目的贡献。\n<br/>\n\n## 项目依赖\n\n#### Ubuntu,debian\n\n1.安装sqlite数据库(一般系统内置):\napt-get install sqlite3\n<br/>\n2.安装requests,chardet,web.py,gevent psutil:\npip install requests chardet web.py sqlalchemy gevent psutil\n<br/>\n3.安装lxml:\napt-get install python-lxml\n<br/>\n注意：\n\n* python3下的是pip3\n* 有时候使用的gevent版本过低会出现自动退出情况，请使用pip install gevent --upgrade更新)\n* 在python3中安装web.py，不能使用pip，直接下载py3版本的[源码](https://codeload.github.com/webpy/webpy/zip/py3)进行安装\n\n#### Windows\n\n1.下载[sqlite](http://www.sqlite.org/download.html),路径添加到环境变量\n<br/>\n2.安装requests,chardet,web.py,gevent:\npip install requests chardet web.py sqlalchemy gevent\n<br/>\n3.安装lxml:\npip install lxml或者下载[lxml windows版](https://pypi.python.org/pypi/lxml/)\n<br/>\n注意：\n\n* python3下的是pip3\n* 有时候使用的gevent版本过低会出现自动退出情况，请使用pip install gevent --upgrade更新)\n* 在python3中安装web.py，不能使用pip，直接下载py3版本的[源码](https://codeload.github.com/webpy/webpy/zip/py3)进行安装\n\n#### 扩展说明\n\n本项目默认数据库是sqlite，但是采用sqlalchemy的ORM模型，通过预留接口可以拓展使用MySQL，MongoDB等数据库。\n配置方法：\n<br/>\n1.MySQL配置\n```\n第一步：首先安装MySQL数据库并启动\n第二步：安装MySQLdb或者pymysql(推荐)\n第三步：在config.py文件中配置DB_CONFIG。如果安装的是MySQLdb模块，配置如下：\n        DB_CONFIG={\n            'DB_CONNECT_TYPE':'sqlalchemy',\n            'DB_CONNECT_STRING':'mysql+mysqldb://root:root@localhost/proxy?charset=utf8'\n        }\n        如果安装的是pymysql模块，配置如下：\n         DB_CONFIG={\n            'DB_CONNECT_TYPE':'sqlalchemy',\n            'DB_CONNECT_STRING':'mysql+pymysql://root:root@localhost/proxy?charset=utf8'\n        }\n```\nsqlalchemy下的DB_CONNECT_STRING参考[支持数据库](http://docs.sqlalchemy.org/en/latest/core/engines.html#supported-databases)，理论上使用这种配置方式不只是适配MySQL，sqlalchemy支持的数据库都可以，但是仅仅测试过MySQL。\n<br/>\n2.MongoDB配置\n```\n第一步：首先安装MongoDB数据库并启动\n第二步：安装pymongo模块\n第三步：在config.py文件中配置DB_CONFIG。配置类似如下：\n        DB_CONFIG={\n            'DB_CONNECT_TYPE':'pymongo',\n            'DB_CONNECT_STRING':'mongodb://localhost:27017/'\n        }\n```\n由于sqlalchemy并不支持MongoDB,因此额外添加了pymongo模式，DB_CONNECT_STRING参考pymongo的连接字符串。\n\n##### 注意\n\n如果大家想拓展其他数据库，可以直接继承db下ISqlHelper类，实现其中的方法，具体实现参考我的代码，然后在DataStore中导入类即可。\n```\ntry:\n    if DB_CONFIG['DB_CONNECT_TYPE'] == 'pymongo':\n        from db.MongoHelper import MongoHelper as SqlHelper\n    else:\n        from db.SqlHelper import SqlHelper as SqlHelper\n    sqlhelper = SqlHelper()\n    sqlhelper.init_db()\nexcept Exception,e:\n    raise Con_DB_Fail\n```\n有感兴趣的朋友，可以将Redis的实现方式添加进来。\n\n\n## 如何使用\n\n将项目目录clone到当前文件夹\n\n$ git clone \n\n切换工程目录\n\n```\n$ cd IPProxyPool\n```\n\n运行脚本\n\n```\npython IPProxy.py\n```\n成功运行后，打印信息\n```\nIPProxyPool----->>>>>>>>beginning\nhttp://0.0.0.0:8000/\nIPProxyPool----->>>>>>>>db exists ip:0\nIPProxyPool----->>>>>>>>now ip num < MINNUM,start crawling...\nIPProxyPool----->>>>>>>>Success ip num :134,Fail ip num:7882\n```\n\n## API 使用方法\n\n#### 第一种模式\n```\nGET /\n```\n这种模式用于查询代理ip数据，同时加入评分机制，返回数据的顺序是按照评分由高到低，速度由快到慢制定的。\n\n#### 参数 \n\n| Name | Type | Description |\n| ----| ---- | ---- |\n| types | int | 0: 高匿,1:匿名,2 透明 |\n| protocol | int | 0: http, 1 https, 2 http/https |\n| count | int | 数量 |\n| country | str | 取值为 国内, 国外 |\n| area | str | 地区 |\n\n\n\n#### 例子\n\n##### IPProxys默认端口为8000,端口可以在config.py中配置。\n\n##### 如果是在本机上测试：\n\n1.获取5个ip地址在中国的高匿代理：http://127.0.0.1:8000/?types=0&count=5&country=国内\n<br/>\n2.响应为JSON格式，按照评分由高到低，响应速度由高到低的顺序，返回数据：\n<br/>\n```\n[[\"122.226.189.55\", 138, 10], [\"183.61.236.54\", 3128, 10], [\"61.132.241.109\", 808, 10], [\"183.61.236.53\", 3128, 10], [\"122.227.246.102\", 808, 10]]\n```\n<br/>\n以[\"122.226.189.55\", 138, 10]为例，第一个元素是ip,第二个元素是port，第三个元素是分值score。\n\n```\nimport requests\nimport json\nr = requests.get('http://127.0.0.1:8000/?types=0&count=5&country=国内')\nip_ports = json.loads(r.text)\nprint ip_ports\nip = ip_ports[0][0]\nport = ip_ports[0][1]\nproxies={\n    'http':'http://%s:%s'%(ip,port),\n    'https':'http://%s:%s'%(ip,port)\n}\nr = requests.get('http://ip.chinaz.com/',proxies=proxies)\nr.encoding='utf-8'\nprint r.text\n```\n#### 第二种模式\n```\nGET /delete\n```\n这种模式用于方便用户根据自己的需求删除代理ip数据\n\n#### 参数 \n\n| Name | Type | Description |\n| ----| ---- | ---- |\n| ip | str | 类似192.168.1.1 |\n| port | int | 类似 80 |\n| types | int |  0: 高匿,1:匿名,2 透明 |\n| protocol | int | 0: http, 1 https, 2 http/https |\n| count | int | 数量 |\n| country | str | 取值为 国内, 国外 |\n| area | str | 地区 |\n\n大家可以根据指定以上一种或几种方式删除数据。\n\n#### 例子\n\n##### 如果是在本机上测试：\n\n1.删除ip为120.92.3.127的代理：http://127.0.0.1:8000/delete?ip=120.92.3.127\n<br/>\n2.响应为JSON格式，返回删除的结果为成功,失败或者返回删除的个数,类似如下的效果：\n[\"deleteNum\", \"ok\"]或者[\"deleteNum\", 1]\n```\nimport requests\nr = requests.get('http://127.0.0.1:8000/delete?ip=120.92.3.127')\nprint r.text\n```\n## config.py参数配置\n```\n#parserList是网址解析规则表,大家可以将发现的代理网址,将提取规则添加到其中,方便爬虫的爬取。\nparserList = [\n    {\n        'urls': ['http://www.66ip.cn/%s.html' % n for n in ['index'] + list(range(2, 12))],\n        'type': 'xpath',\n        'pattern': \".//*[@id='main']/div/div[1]/table/tr[position()>1]\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': './td[4]', 'protocol': ''}\n    },\n    \n   ......\n \n   \n    {\n        'urls': ['http://www.cnproxy.com/proxy%s.html' % i for i in range(1, 11)],\n        'type': 'module',\n        'moduleName': 'CnproxyPraser',\n        'pattern': r'<tr><td>(\\d+\\.\\d+\\.\\d+\\.\\d+)<SCRIPT type=text/javascript>document.write\\(\\\"\\:\\\"(.+)\\)</SCRIPT></td><td>(HTTP|SOCKS4)\\s*',\n        'position': {'ip': 0, 'port': 1, 'type': -1, 'protocol': 2}\n    }\n]\n\n#数据库的配置\n\nDB_CONFIG = {\n\n    'DB_CONNECT_TYPE': 'sqlalchemy',  # 'pymongo'sqlalchemy;redis\n    # 'DB_CONNECT_STRING':'mongodb://localhost:27017/'\n    'DB_CONNECT_STRING': 'sqlite:///' + os.path.dirname(__file__) + '/data/proxy.db'\n    # DB_CONNECT_STRING : 'mysql+mysqldb://root:root@localhost/proxy?charset=utf8'\n\n    # 'DB_CONNECT_TYPE': 'redis',  # 'pymongo'sqlalchemy;redis\n    # 'DB_CONNECT_STRING': 'redis://localhost:6379/8',\n\n}\n#THREADNUM为gevent pool的协程数目\nTHREADNUM = 5\n\n#API_PORT为API web服务器的端口\nAPI_PORT = 8000\n\n#爬虫爬取和检测ip的设置条件\n#不需要检测ip是否已经存在，因为会定时清理\n# UPDATE_TIME:每半个小时检测一次是否有代理ip失效\nUPDATE_TIME = 30 * 60 \n\n# 当有效的ip值小于MINNUM时 需要启动爬虫进行爬取\nMINNUM = 50  \n\n# socket超时\nTIMEOUT = 5 \n\n\n\n\n#爬虫下载网页的重试次数\nRETRY_TIME = 3\n\n\n#USER_AGENTS 随机头信息,用来突破爬取网站的反爬虫\n\nUSER_AGENTS = [\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n   ]\n#默认给抓取的ip分配20分,每次连接失败,减一分,直到分数全部扣完从数据库中删除\nDEFAULT_SCORE=10\n\n#CHECK_PROXY变量是为了用户自定义检测代理的函数,，默认是CHECK_PROXY={'function':'checkProxy'}。\n#现在使用检测的网址是httpbin.org,但是即使ip通过了验证和检测\n#也只能说明通过此代理ip可以到达httpbin.org,但是不一定能到达用户爬取的网址\n#因此在这个地方用户可以自己添加检测函数,我以百度为访问网址尝试一下\n#大家可以看一下Validator.py文件中的baidu_check函数和detect_proxy函数就会明白\n\nCHECK_PROXY={'function':'checkProxy'}#{'function':'baidu_check'}\n```\n## TODO\n1.添加squid代理，简化爬虫配置\n<br/>\n\n\n## 更新进度\n-----------------------------2017-4-6----------------------------\n<br/>\n1.更新评分机制。\n<br/>\n* 之前的评分机制是刚添加进来每个代理ip为0分，每隔半个小时检测一次，检测之后依然有效则加分，无效则删除。\n* 现在的评分机制是每个新的代理ip分配10分,每隔半个小时检测一次，检测之后依然有效则分数不变，无效则分数减一,直至为0删除,可以避免由于检测网站不稳定导致的误删。\n\n2.用户可以自定义检测函数,在config.py的CHECK_PROXY变量中可以配置。\n```\nCHECK_PROXY变量是为了用户自定义检测代理的函数，默认是CHECK_PROXY={'function':'checkProxy'}\n现在使用检测的网址是httpbin.org,但是即使ip通过了验证和检测\n也只能说明通过此代理ip可以到达httpbin.org,但是不一定能到达用户爬取的网址\n因此在这个地方用户可以自己添加检测函数,我以百度为访问网址尝试一下\n大家可以看一下Validator.py文件中的baidu_check函数和detect_proxy函数就会明白。\n\nCHECK_PROXY={'function':'baidu_check'}\n```\n3.经过大家的共同努力,彻底解决了僵死进程的问题。\n\n-----------------------------2017-1-16----------------------------\n<br/>\n1.将py2和py3版本合并，并且兼容\n<br/>\n2.修复pymongo查询bug\n<br/>\n-----------------------------2017-1-11----------------------------\n<br/>\n1.使用httpbin.org检测代理ip的高匿性\n<br/>\n2.使用 国内 和 国外 作为country的查询条件\n<br/>\n3.修改types和protocol参数，一定要注意protocol的使用，试试访问http://www.baidu.com和https://www.baidu.com\n<br/>\n4.美化代码风格\n<br/>\n-----------------------------2016-12-11----------------------------\n####大规模重构，主要包括以下几个方面：\n1.使用多进程+协程的方式，将爬取和验证的效率提高了50倍以上，可以在几分钟之内获取所有的有效IP\n<br/>\n2.使用web.py作为API服务器，重构HTTP接口\n<br/>\n3.增加Mysql,MongoDB等数据库的适配\n<br/>\n4.增加了三个代理网站\n<br/>\n5.增加评分机制，评比稳定的ip\n<br/>\n6.支持python3\n<br/>\n-----------------------------2016-11-24----------------------------\n<br/>\n1.增加chardet识别网页编码\n<br/>\n2.突破66ip.cn反爬限制\n<br/>\n-----------------------------2016-10-27----------------------------\n<br/>\n1.增加对代理的检测，测试是否能真正访问到网址，实现代理\n<br/>\n2.添加通过正则表达式和加载插件解析网页的方式\n<br/>\n3.又增加一个新的代理网站\n<br/>\n\n-----------------------------2016-7-20----------------------------\n<br/>\n1.修复bug ,将数据库进行压缩\n<br/>\n"
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 11.021484375,
          "content": "# coding:utf-8\n'''\n定义规则 urls:url列表\n         type：解析方式,取值 regular(正则表达式),xpath(xpath解析),module(自定义第三方模块解析)\n         patten：可以是正则表达式,可以是xpath语句不过要和上面的相对应\n'''\nimport os\nimport random\n\n'''\nip，端口，类型(0高匿名，1透明)，protocol(0 http,1 https),country(国家),area(省市),updatetime(更新时间)\n speed(连接速度)\n'''\nparserList = [\n    {\n        'urls': ['http://www.66ip.cn/%s.html' % n for n in ['index'] + list(range(2, 12))],\n        'type': 'xpath',\n        'pattern': \".//*[@id='main']/div/div[1]/table/tr[position()>1]\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': './td[4]', 'protocol': ''}\n    },\n    {\n        'urls': ['http://www.66ip.cn/areaindex_%s/%s.html' % (m, n) for m in range(1, 35) for n in range(1, 10)],\n        'type': 'xpath',\n        'pattern': \".//*[@id='footer']/div/table/tr[position()>1]\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': './td[4]', 'protocol': ''}\n    },\n    {\n        'urls': ['http://cn-proxy.com/', 'http://cn-proxy.com/archives/218'],\n        'type': 'xpath',\n        'pattern': \".//table[@class='sortable']/tbody/tr\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': '', 'protocol': ''}\n\n    },\n    {\n        'urls': ['http://www.mimiip.com/gngao/%s' % n for n in range(1, 10)],\n        'type': 'xpath',\n        'pattern': \".//table[@class='list']/tr\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': '', 'protocol': ''}\n\n    },\n    {\n        'urls': ['https://proxy-list.org/english/index.php?p=%s' % n for n in range(1, 10)],\n        'type': 'module',\n        'moduleName': 'proxy_listPraser',\n        'pattern': 'Proxy\\(.+\\)',\n        'position': {'ip': 0, 'port': -1, 'type': -1, 'protocol': 2}\n\n    },\n    {\n        'urls': ['http://incloak.com/proxy-list/%s#list' % n for n in\n                 ([''] + ['?start=%s' % (64 * m) for m in range(1, 10)])],\n        'type': 'xpath',\n        'pattern': \".//table[@class='proxy__t']/tbody/tr\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': '', 'protocol': ''}\n\n    },\n    {\n        'urls': ['http://www.kuaidaili.com/proxylist/%s/' % n for n in range(1, 11)],\n        'type': 'xpath',\n        'pattern': \".//*[@id='index_free_list']/table/tbody/tr[position()>0]\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': './td[3]', 'protocol': './td[4]'}\n    },\n    {\n        'urls': ['http://www.kuaidaili.com/free/%s/%s/' % (m, n) for m in ['inha', 'intr', 'outha', 'outtr'] for n in\n                 range(1, 11)],\n        'type': 'xpath',\n        'pattern': \".//*[@id='list']/table/tbody/tr[position()>0]\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': './td[3]', 'protocol': './td[4]'}\n    },\n    {\n        'urls': ['http://www.cz88.net/proxy/%s' % m for m in\n                 ['index.shtml'] + ['http_%s.shtml' % n for n in range(2, 11)]],\n        'type': 'xpath',\n        'pattern': \".//*[@id='boxright']/div/ul/li[position()>1]\",\n        'position': {'ip': './div[1]', 'port': './div[2]', 'type': './div[3]', 'protocol': ''}\n\n    },\n    {\n        'urls': ['http://www.ip181.com/daili/%s.html' % n for n in range(1, 11)],\n        'type': 'xpath',\n        'pattern': \".//div[@class='row']/div[3]/table/tbody/tr[position()>1]\",\n        'position': {'ip': './td[1]', 'port': './td[2]', 'type': './td[3]', 'protocol': './td[4]'}\n\n    },\n    {\n        'urls': ['http://www.xicidaili.com/%s/%s' % (m, n) for m in ['nn', 'nt', 'wn', 'wt'] for n in range(1, 8)],\n        'type': 'xpath',\n        'pattern': \".//*[@id='ip_list']/tr[position()>1]\",\n        'position': {'ip': './td[2]', 'port': './td[3]', 'type': './td[5]', 'protocol': './td[6]'}\n    },\n    {\n        'urls': ['http://www.cnproxy.com/proxy%s.html' % i for i in range(1, 11)],\n        'type': 'module',\n        'moduleName': 'CnproxyPraser',\n        'pattern': r'<tr><td>(\\d+\\.\\d+\\.\\d+\\.\\d+)<SCRIPT type=text/javascript>document.write\\(\\\"\\:\\\"(.+)\\)</SCRIPT></td><td>(HTTP|SOCKS4)\\s*',\n        'position': {'ip': 0, 'port': 1, 'type': -1, 'protocol': 2}\n    }\n]\n'''\n数据库的配置\n'''\nDB_CONFIG = {\n\n    'DB_CONNECT_TYPE': 'sqlalchemy',  # 'pymongo'sqlalchemy;redis\n    # 'DB_CONNECT_STRING':'mongodb://localhost:27017/'\n    'DB_CONNECT_STRING': 'sqlite:///' + os.path.dirname(__file__) + '/data/proxy.db'\n    # DB_CONNECT_STRING : 'mysql+mysqldb://root:root@localhost/proxy?charset=utf8'\n\n    # 'DB_CONNECT_TYPE': 'redis',  # 'pymongo'sqlalchemy;redis\n    # 'DB_CONNECT_STRING': 'redis://localhost:6379/8',\n\n}\nCHINA_AREA = ['河北', '山东', '辽宁', '黑龙江', '吉林'\n    , '甘肃', '青海', '河南', '江苏', '湖北', '湖南',\n              '江西', '浙江', '广东', '云南', '福建',\n              '台湾', '海南', '山西', '四川', '陕西',\n              '贵州', '安徽', '重庆', '北京', '上海', '天津', '广西', '内蒙', '西藏', '新疆', '宁夏', '香港', '澳门']\nQQWRY_PATH = os.path.dirname(__file__) + \"/data/qqwry.dat\"\nTHREADNUM = 5\nAPI_PORT = 8000\n'''\n爬虫爬取和检测ip的设置条件\n不需要检测ip是否已经存在，因为会定时清理\n'''\nUPDATE_TIME = 30 * 60  # 每半个小时检测一次是否有代理ip失效\nMINNUM = 50  # 当有效的ip值小于一个时 需要启动爬虫进行爬取\n\nTIMEOUT = 5  # socket延时\n'''\n反爬虫的设置\n'''\n'''\n重试次数\n'''\nRETRY_TIME = 3\n\n'''\nUSER_AGENTS 随机头信息\n'''\nUSER_AGENTS = [\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER\",\n    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)\",\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)\",\n    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)\",\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)\",\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)\",\n    \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1\",\n    \"Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5\",\n    \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/4.0b13pre\",\n    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n    \"Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10\"\n]\n\n\ndef get_header():\n    return {\n        'User-Agent': random.choice(USER_AGENTS),\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.5',\n        'Connection': 'keep-alive',\n        'Accept-Encoding': 'gzip, deflate',\n    }\n#默认给抓取的ip分配20分,每次连接失败,减一分,直到分数全部扣完从数据库中删除\nDEFAULT_SCORE=10\n\nTEST_URL = 'http://ip.chinaz.com/getip.aspx'\nTEST_IP = 'http://httpbin.org/ip'\nTEST_HTTP_HEADER = 'http://httpbin.org/get'\nTEST_HTTPS_HEADER = 'https://httpbin.org/get'\n#CHECK_PROXY变量是为了用户自定义检测代理的函数\n#现在使用检测的网址是httpbin.org,但是即使ip通过了验证和检测\n#也只能说明通过此代理ip可以到达httpbin.org,但是不一定能到达用户爬取的网址\n#因此在这个地方用户可以自己添加检测函数,我以百度为访问网址尝试一下\n#大家可以看一下Validator.py文件中的baidu_check函数和detect_proxy函数就会明白\n\nCHECK_PROXY={'function':'checkProxy'}#{'function':'baidu_check'}\n\n#下面配置squid,现在还没实现\n#SQUID={'path':None,'confpath':'C:/squid/etc/squid.conf'}\n\nMAX_CHECK_PROCESS = 2 # CHECK_PROXY最大进程数\nMAX_CHECK_CONCURRENT_PER_PROCESS = 30 # CHECK_PROXY时每个进程的最大并发\nTASK_QUEUE_SIZE = 50 # 任务队列SIZE\nMAX_DOWNLOAD_CONCURRENT = 3 # 从免费代理网站下载时的最大并发 \nCHECK_WATI_TIME = 1#进程数达到上限时的等待时间"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "db",
          "type": "tree",
          "content": null
        },
        {
          "name": "qiye2.jpg",
          "type": "blob",
          "size": 27.56640625,
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1240234375,
          "content": "chardet==2.3.0\ngevent==1.2.0\ngreenlet==0.4.11\nlxml==3.7.1\nrequests==2.12.4\nSQLAlchemy==1.1.4\nweb.py==0.38\nredis==2.10.5\npsutil\n"
        },
        {
          "name": "spider",
          "type": "tree",
          "content": null
        },
        {
          "name": "start.bat",
          "type": "blob",
          "size": 0.015625,
          "content": "py -2 IPProxy.py"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        },
        {
          "name": "validator",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}