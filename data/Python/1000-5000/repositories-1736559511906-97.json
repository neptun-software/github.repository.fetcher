{
  "metadata": {
    "timestamp": 1736559511906,
    "page": 97,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Netflix/vmaf",
      "stars": 4742,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.056640625,
          "content": "matlab\nworkspace\npython/.tox\n.git\n.venv\n*.mp4\n*.yuv\n*.mov\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.1689453125,
          "content": "# Set the default behavior, in case people don't have core.autocrlf set.\n* text=auto\n\n# The model file parser requires LF line endings\n*.pkl text eol=lf\n*.model text eol=lf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1689453125,
          "content": "# Caches, build artifacts\n.DS_Store\n.cache/\n*.egg*\n.gradle/\n.idea/\n*.py[cod]\n.pypirc\n.tox/\n.*version\n.venv*/\n__pycache__/\nbuild/\ndist/\nvmaf_output.xml\ncompile_commands.json\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.2001953125,
          "content": "os: linux\ndist: jammy\nlanguage: c++\ncache: ccache\n\naddons:\n  apt:\n    update: true\n    sources:\n      - sourceline: \"ppa:ubuntu-toolchain-r/test\"\n      - sourceline: \"deb http://apt.llvm.org/jammy/ llvm-toolchain-jammy-11 main\"\n        key_url: \"https://apt.llvm.org/llvm-snapshot.gpg.key\"\n    packages:\n      - python3-pip\n      - ninja-build\n      - python3-setuptools\n      - ccache\n      - nasm\n      - clang\n  homebrew:\n    packages:\n      - ninja\n      - ccache\n      - nasm\n    update: true\n\nbefore_install: \"sudo chown -R travis: $HOME/.ccache\"\n\ninstall:\n  - ccache -s\n  - pip3 install meson\n  - |\n    if type apt-get &>/dev/null; then\n      case \"$CC\" in\n      gcc-7) sudo apt-get install -qq gcc-7 g++-7 ;;\n      gcc-8) sudo apt-get install -qq gcc-8 g++-8 ;;\n      gcc-9) sudo apt-get install -qq gcc-9 g++-9 ;;\n      esac\n    fi\n  - export CC=\"ccache $CC\" CXX=\"ccache $CXX\"\n\nafter_script:\n  - ccache -s\n\nmatrix:\n  fast_finish: true\n  include:\n    - name: libvmaf arm\n      arch: arm64\n      script: &base-gcc-script\n        - mkdir libvmaf/build && cd libvmaf/build\n        - meson .. --buildtype release -Denable_float=true\n        - sudo ninja -v install\n        - sudo ninja -v test\n        - cd $TRAVIS_BUILD_DIR\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 17.603515625,
          "content": "# Change Log\n\n## (2022-04-11) [v2.3.1]\n\nThis is a minor release with some CAMBI extensions and speed-ups and adding it to AOM CTC v3, as well as a few minor fixes/cleanups.\n- CAMBI extensions: full reference, PQ eotf, up to 16 bit-depth support, max_log_contrast parameter.\n- CAMBI: option to output heatmaps.\n\n## (2021-10-16) [v2.3.0]\n\nNew release to add CAMBI (Contrast Aware Multiscale Banding Index).\n\n- Python library: add encode width and height to Asset.\n- libvmaf: add pixel format VMAF_PIX_FMT_YUV400P.\n- Add cambi; add tests. \n- Improve documentation. (#912)\n\n## (2021-09-20) [v2.2.1]\n\nThis is another minor release to address a few last minute items for the AOM CTC v2, as well as a few minor fixes/cleanups.\n\n- Fix a race condition in vmaf_thread_pool_wait(). (#894)\n- Avoid chroma resampling for 420mpeg2 y4m input (#906)\n\n## (2021-07-02) [v2.2.0]\n\nThis is a minor release to address a few items for the AOM CTC v2, as well as a few minor fixes/cleanups.\n\n- Fixes a CIEDE-2000 precision issue, where cross-platform mismatches were seen. (#878)\n- Adds libvmaf API function vmaf_feature_dictionary_free(). (#879)\n\n## (2021-01-13) [v2.1.1]\n\nThis is a minor release to address a few last minute items for the initial AOM CTC.\n\n**New features:**\n- Fixes a SSIM/MS-SSIM precision bug where a lossless comparison did not always result in a perfect 1.0 score. (#796).\n- Adds feature extractor options to clip the dB scores for both PSNR/SSIM. --aom_ctc v1.0 has been updated to use these clipping options according to the AOM CTC. (#802).\n\n## (2020-12-30) [v2.1.0]\nThis is a minor release for the initial AOM CTC. Support has been added for templated feature names. While this is a general purpose software feature, templated feature names are immediately useful for simultaneous computation of VMAF and VMAF NEG since the two metrics rely on slightly different VIF/ADM variations. Global feature overrides via the `--feature` flag are no longer supported, instead individual models can have their features overloaded individually, the syntax for which is as follows:\n\n ```sh\n--model version=vmaf_v0.6.1:vif.vif_enhn_gain_limit=1.0:adm.adm_enhn_gain_limit=1.0\n```\n\n**New features:**\n- Per-model feature overloading via new API `vmaf_model_feature_overload()`.\n- Multiple unique configurations of the same feature extractor may be registered run at the same time.\n- `--aom_ctc v1.0` preset, encompassing all metrics specified by the AOM CTC.\n\n## (2020-12-4) [2.0.0]\n\n**New features:**\n- Add PSNR-HVS and CIEDE2000 metrics.\n- ci/actions: upload linux/macos artifacts (#738)\n- libvmaf/feature: deprecate daala_ssim (#735)\n- libvmaf: remove support for pkl models\n- libvmaf/psnr: rewrite using integer types, 2x speedup\n- vmaf: if no model is specified, enable v0.6.1 by default (#730)\n- libvmaf/x86: add AVX2/AVX-512 optimizations for adm, vif and motion\n- ci/actions: add xxd to build dependencies for Windows\n- libvmaf: add support for built-in models\n- libvmaf/integer_vif: use symmetrical mirroring on edges\n- Fix log2 by replacing log2f_approx with log2f\n- libvmaf_rc: provide a backwards compatible compute_vmaf(), link vmafossexec with libvmaf\n- libvmaf: add framework support for json models\n- libvmaf/libsvm: update libsvm to version 324\n- libvmaf/motion: add motion_force_zero to motion fex\n- return sha1 if Asset string is longer than 255\n- Add CID/iCID Matlab source code\n- build: unbreak x86 builds (Fixes: #374)\n- Add 12bit and 16bit support for python YUV reader; add tests.\n- Add PypsnrFeatureExtractor\n- Add processes to FeatureAssembler. (#662)\n\n**Fixed bugs:**\n- fix motion flush for single frame input\n- Fixing the perf_metric for a single entry list input\n\n## (2020-8-24) [1.5.3]\n\n(Updates since 1.5.1)\n\n**Fixed bugs:**\n- Fix inverted height and width in integer_motion in vmaf_rc (#650).\n\n**New features:**\n- libvmaf: add support for CSV and JSON logging\n- Python: Add an (optional) step in Executor class to do python-based processing to ref/dis files (#523).\n- Restructure python project and documentation (#544).\n- Move test resource to Netflix/vmaf_resource repo (#552).\n- Add Github CI (#558).\n- Add vmaf_float_v0.6.1neg model; add vif_enhn_gain_limit and adm_enhn_gain_limit options to vmaf_rc.\n- Update documentation for FFmpeg+libvmaf.\n- Improvements to AucPerfMetric (#643).\n- Add motion_force_zero option to vmaf_rc.\n\n## (2020-6-30) [1.5.2]\n\n**Fixed bugs:**\n- Fix pkgconfig version sync issue (#572)\n\n**New features:**\n- libvmaf_rc general improvements\n\n## (2020-2-27) [1.5.1]\n    \n**New features:**\n- `libvmaf` has been relocated, and now has its own self-enclosed source tree (`./libvmaf/`) and build system (`meson`).\n- Update license to BSD+Patent.\n- Migrate the build system from makefile to meson.\n- Introduce a new release candidate API with the associated library `libvmaf_rc` and executable `vmaf_rc` under `./libvmaf/build`. \n- Add SI and TI feature extractor python classes.\n- Add fixed-point SSIM implementation.\n- Migrate to python3.\n\n## (2019-9-8) [1.3.15]\n\n**Fixed bugs:**\n- Fix a case when CPU cores > 128(MAX_NUM_THREADS) / 3 (#319).\n- Avoid dis-filtering ref when not needed, fix return type (#325).\n- Update name of file for failed dis_path fopen (#334).\n- A few compilation fixes (warnings and errors) (#326).\n- Bump up g++ version to 9 for travis (#352).\n- Use stat struct instead of ftell to retrieve the file size (#350).\n\n**New features:**\n- Write aggregate scores, exec FPS to json output.\n- Add support for python3 (#332).\n- Print progress in vmafossexec (#337).\n- Add VMAF logo.\n- Add link to report VMAF bad cases.\n\n## (2019-3-1) [1.3.14]\n\n**Fixed bugs:**\n- Fix VMAF value mismatch on 160x90 videos after optimization (#315).\n- Fix w10 error with using uninitialized offset_flag variable (#302).\n\n**New features:**\n- Add automated Windows builds with AddVeyor (#313).\n- Report aggregate CI scores and fix empty model name in log (#304).\n\n## (2019-1-31) [1.3.13]\n\n**New features:**\n- Optimized C code for speed. Running in multithreading mode, `vmafossexec` achieves ~40% run time reduction compared to the previous version.\n- Printed out individual vmaf bootstrap scores in text file from `vmafossexec`.\n- refactored windows solution (#283) (#284) (#285) (#291) (#298).\n\n## (2018-12-17) [1.3.11]\n\n**New features:**\n- Revise number of bootstrap models definition: model/vmaf_rb_v0.6.3/vmaf_rb_v0.6.3.pkl has 21 models (20 bootstrap models and one using the full data). From these 21 models, the 20 of them are same as v0.6.2, only added an additional bootstrap model.\n- Output the per bootstrap model predictions from wrapper/vmafossexec.\n- Print bootstrap individual scores in xml and json.\n- Add BD-rate calculator and update documentation.\n- Report aggregate PSNR, SSIM, and MS-SSIM scores.\n- Add sklearn linear regression class to TrainTestModel.\n- Enable BRISQUE feature in VMAF training with bootstrapping.\n- Add --save-plot option to command line tools.\n- Add ST-RREDOpt (time optimized), ST-MAD feature extractors, quality runners and unittestts. Refactor ST-RRED feature extractor. (#216)\n\n**Fixed bugs:**\n- Bug fixed. When start vmaf in multi-thread at the same time. (#239)\n- Fix name of min function in vmaf.h and vmaf.cpp. (#227)\n- Fix implicit declaration of functions (#225)\n\n## (2018-9-13) [1.3.10]\n\n**New features:**\n- Remove sureal as a submodule to vmaf. sureal is now available through pip install.\n\n## (2018-8-7) [1.3.9]\n\n**Fixed bugs:**\n- libvmaf: fix case where user defined read_frame() callback was being ignored.\n\n## (2018-6-21) [1.3.8]\n\n**Fixed bugs:**\n- Fix compute_vmaf boolean type issue (#178).\n\n## (2018-6-12) [1.3.7]\n\n**New features:**\n- Add the --ci option to calculate confidence intervals to predicted VMAF scores (run_vmaf, run_vmaf_in_batch, ffmpeg2vmaf, vmafossexec).\n- Update libvmaf version to 1.3.7 after compute_vmaf() interface change (added enable_conf_interval option).\n- Add new models: 1) model/vmaf_4k_v0.6.1.pkl for 4KTV viewing at distance 1.5H, 2) model/vmaf_rb_v0.6.2/vmaf_rb_v0.6.2.pkl for VMAF prediction with a confidence interval, 3) model/vmaf_4k_rb_v0.6.2/vmaf_4k_rb_v0.6.2.pkl for 4KTV viewing at distance 1.5H, with a confidence interval.\n\n## (2018-6-4) [1.3.6]\n\n**New features:**\n- Update libvmaf version to 1.3.6 (to make consistent with VDK version from now on) after compute_vmaf() interface change (added thread and subsample options).\n- Add the option to set the number of threads to use in vmafossexec.\n- Add the option to subsample frames to save computation in vmafossexec.\n\n## (2018-5-23) [1.3.5]\n\n**New features:**\n- Add multi-threading to vmafossexec.\n\n## (2018-5-8) [1.3.4]\n\n**Refactoring:**\n- Refactor mos out of vmaf repo; rename to sureal as submodule.\n- Refactor TrainTestModel to make predict() to output dictionary.\n- Refactor TrainTestModel.\n- Rename KFLK metric to AUC (Area Under the Curve) for better interpretability.\n\n**New features:**\n- Add bootstrapping to VMAF. Add two new classes BootstrapVmafQualityRunner and BaggingVmafQualityRunner\n- Add Resolving Power Performance Metric.\n- Add BRISQUE and NIQE feature extractors. Added two new classes BrisqueNorefFeatureExtractor and NiqeNorefFeatureExtractor. Add NiqeQualityRunner.\n\n**Fixed bugs:**\n- Add .gitattributes (#127). Force .pkl and .model files to retain LF line-ending. Required for use on Windows where model files would otherwise be checked out as CRLF which VMAF's parser doesn't handle.\n- Allow MinGW compilation of ptools (#133). ptools doesn't build on MinGW as *nix socket headers are included. This patch selects Windows headers for MinGW builds.\n- Update compute vmaf interface (#138). Update VMAF version in libvmaf.pc and etc. Catch logic error (resulted from wrong model file format) in compute_vmaf(). Use custom error code.\n\n## (2017-12-3) [1.3.3]\n\n**Fixed bugs:**\n- Update VMAF version to 0.6.2 after compute_vmaf() interface change (#124).\n\n## (2017-12-3) [1.3.2]\n\n**Refactoring:**\n- Lift check for exec existence during program load.\n- Refactor psnr, ssim, ms_ssim and vmaf_feature to call ExternalProgramCaller.\n- Refactor feature/Makefile to make executables depend on libvmaf.a.\n- Refactor wrapper/Makefile to include additional objs in libvmaf.a but exclude main.o.\n- Remove ar -d command after removing main.o from libvmaf.a.\n\n**New features:**\n- Generalize read_dataset.\n- Update default Asset resampling method to bicubic (#116).\n- Extend ffmpeg2vmaf script to allow ref/dis input to be YUV (#118).\n- Improve README.md (#121).\n\n**Fixed bugs:**\n- Temporary fix Visual Studio builds (#112).\n- Avoid unnecessary dependency on matplotlib in run_vmaf (#114).\n- Remove unneeded dependencies in Dockerfile, fixes #115 (#117).\n- MinGW support (#123).\n- Change compute_vmaf() interface to return an error code instead of throw an error #124 (#126).\n\n## (2017-8-12) [1.3.1]\n\n**Refactoring:**\n- Refactor NorefExecutorMixin to eliminate repeated codes.\n- Refactor C code: get rid of unused double functions; uniformly use read_frame callback function to void repeated code;\n- Add strip option to Makefile.\n\n**New features:**\n- Update Asset class: add copy functions to Asset; add ref/dis_yuv_type; deprecate yuv_type; add ref/dis_start_sec;\n- Update subjective models: add confidence interval to subjective model parameters; refactor MLE model and make subclasses; add run_subj command line.\n- Recommend pip, add ffmpeg2vmaf info and reorganize prerequisite installation (#88).\n- Reduce sleep time in parallel_map.\n- Add library interface for VMAF (#90).\n- Add VisualStudio2015 support (#92).\n- Add example of image dataset notyuv.\n- Add pkgconfig file and changed Makefile.\n- Add VmafPhoneQualityRunner class.\n- Add DMOS_MLE_CO subjective model.\n\n**Fixed bugs:**\n- Update RegressionMixin to handle AUC exception for dicitonary-style dataset.\n- Fix Makefile fedora libptools issue. (#98)\n\n## (2017-4-13) [1.2.4]\n\n**Refactoring:**\n- Deprecate run_executors_in_parallel.\n- Refactor NorefFeatureExtractor into NorefExecutorMixin so that it can be used for all executors.\n- Add abstract methods to some base classes.\n\n**New features:**\n- Add ST-RRED runner (StrredQualityRunner), based on \"Video Quality Assessment by Reduced Reference Spatio-Temporal Entropic Differencing\", by R. Soundararaajan, A. Bovik.\n- Add start/end frame support for Executor.\n\n## (2017-3-8) [1.2.3]\n\n**New features:**\n- Refactor to replace config.ROOT with config.VmafConfig.\n\n## (2017-3-1) [1.2.2]\n\n**New features:**\n- Generalize Result and FileSystemResultStore to allow None values.\n\n## (2017-2-27) [1.2.1]\n\n**Tasks:**\n- Refactor to prepare for pypi packaging.\n\n## (2017-2-20) [1.2.0]\n\n**New features:**\n- Updated VMAF model to version v0.6.1. Changes include: 1) added a custom model for cellular phone screen viewing; 2) trained using new dataset, covering more difficult content; 3) elementary metric fixes: ADM behavior at near-black frames, motion behavior at scene boundaries; 4) compressed quality score range by 20% to accommodate higher dynamic range; 5) Use MLE instead of DMOS as subjective model.\n\n## (2017-1-24) [1.1.23]\n\n**Fixed bugs:**\n- Replace subprocess.call with run_process (checking return value).\n\n## (2017-1-22) [1.1.22]\n\n**New features:**\n- Add command line ffmpeg2vmaf, which takes encoded videos as input.\n\n## (2017-1-18) [1.1.21]\n\n**New features:**\n- Allow processing non-YUV input videos.\n\n## (2016-12-20) [1.1.20]\n\n**New features:**\n- Add STRRED runner.\n\n## (2016-12-19) [1.1.19]\n\n**New features:**\n- Allow specifying crop and pad parameter in dataset files.\n\n## (2016-12-8) [1.1.18]\n\n**Fixed bugs:**\n- Replace pathos with custom function for parallel executor running.\n\n## (2016-12-8) [1.1.17]\n\n**Fixed bugs:**\n- Fix command line run_testing issue. Add command line test cases.\n\n## (2016-12-5) [1.1.16]\n\n**New features:**\n- Speed up VMAF convolution operation by AVX.\n\n## (2016-11-30) [1.1.15]\n\n**Fixed bugs:**\n- Fix vmafossexec memory leakage.\n\n## (2016-11-28) [1.1.14]\n\n**New features:**\n- Add enable_transform_score option to VmafQualityRunner, VmafossExecQualityRunner.\n\n## (2016-11-18) [1.1.13]\n\n**Fixed bugs:**\n- Fix a bug in DatasetReader.to_aggregated_dataset_file.\n\n## (2016-11-15) [1.1.12]\n\n**New features:**\n- Add Travis continuous integration.\n\n## (2016-11-11) [1.1.11]\n\n**New features:**\n- Add implementation of AUC (Area Under the Curve) - quality metric evaluation method based on AUC. Refer to: L. Krasula, K. Fliegel, P. Le Callet, M.Klima, \"On the accuracy of objective image and video quality models: New methodology for performance evaluation\", QoMEX 2016.\n\n## (2016-11-07) [1.1.10]\n\n**New features:**\n- Add options to use custom subjective models in run_vmaf_training and run_testing commands.\n\n## (2016-11-02) [1.1.9]\n\n**New features:**\n- Add DatasetReader and subclasses; add SubjectiveModel and subclasses.\n\n## (2016-10-19) [1.1.8]\n\n**New features:**\n- Add quality runners for each individual VMAF elementary metrics.\n\n## (2016-10-14) [1.1.7]\n\n**Fixed bugs:**\n- Issue #36: SSIM and MS-SSIM sometimes get negative values.\n\n## (2016-10-10) [1.1.6]\n\n**New features:**\n- Add Xcode project support.\n- Add more pooling options (median, percx) to CLIs.\n\n## (2016-10-8) [1.1.5]\n\n**New features:**\n- Add support for docker usage (#30).\n\n## (2016-10-7) [1.1.4]\n\n**Fixed bugs:**\n- Issue #29: Make ptools build under Fedora.\n\n## (2016-10-6) [1.1.3]\n\n**New features:**\n- Generalize dataset format to allow per-content YUV format.\n\n## (2016-10-5) [1.1.2]\n\n**Fixed bugs:**\n- Make ptools work under Mac OS.\n- Update SklearnRandomForestTrainTestModel test with sklearn 0.18.\n\n## (2016-09-29) [1.1.1]\n\n**New features:**\n- Update command lines run_vmaf, run_psnr, run_vmaf_in_batch, run_cleaning_cache, run_vmaf_training and run_testing.\n\n## (2016-09-28) [1.1.0]\n\n**New features:**\n- Update wrapper/vmafossexec: 1) it now takes pkl model file as input, so that slopes/intercepts are no longer hard-coded; 2) it now takes multiple YUV input formats; 3) add flag to enable/disable VMAF score clipping at 0/100; 4) allow customly running PSNR/SSIM/MS-SSIM; 5) allow customly outputing XML/JSON\n- Add SSIM/MS-SSIM option in run_testing.\n\n## (2016-09-09) [1.0.9]\n\n**Fixed bugs:**\n- Move VmafQualityRunnerWithLocalExplainer to quality_runner_adhoc to resolve multiple instances of VMAF found when calling QualityRunner.find_subclass.\n\n**New features:**\n- Add custom_clip_0to1 to TrainTestModel.\n\n## (2016-09-07) [1.0.8]\n\n**New features:**\n- Generalize read_dataset to allow specifying width, height and resampling method on which to calculate quality.\n- Add bicubic to SUPPORTED_RESAMPLING_TYPES for Asset.\n- Update Asset rule with resampling_type in __str__ to avoid duplicates in data store.\n\n## (2016-08-20) [1.0.7]\n\n**New features:**\n- Update VmafFeatureExtractor to 0.2.2b with scaled ADM features exposed (adm_scale0-3).\n\n## (2016-08-20) [1.0.6]\n\n**New features:**\n- Add DisYUVRawVideoExtractor and related classes.\n- Add NeuralNetworkTrainTestModel base class that integrates TensorFlow.\n- Add example class ToddNoiseClassifierTrainTestModel.\n\n## (2016-08-20) [1.0.5]\n\n**New features:**\n- Add LocalExplainer class.\n- Add show_local_explanation option to run_vmaf script.\n\n## (2016-07-21) [1.0.4]\n\n**Fixed bugs:**\n- Fix a series of numerical issues in VMAF features, increment VmafFeatureExtractor version number.\n- Retrain VmafQualityRunner after feature update, increment version number.\n\n## (2016-07-20) [1.0.3]\n\n**New features:**\n- Add base class NorefFeatureExtractor for any feature extractor that do not use a reference video.\n- Add MomentNorefFeatureExtractor subclassing NorefFeatureExtractor as an example implementation.\n\n## (2016-06-16) [1.0.2]\n\n**New features:**\n- Refactor feature code to expose ssim/ms-ssim, speed up ssim/ms-ssim.\n\n## (2016-06-10) [1.0.1]\n\n**Fixed bugs:**\n- Fix feature while looping by moving feof to after read_image.\n- Fix issue #2 use hashed string for log filename and result filename to avoid file names getting too long.\n\n**New features:**\n- Add SsimFeatureExtractor and MsSsimFeatureExtractor with intermediate features (luminence, contrast, structure).\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 13.0283203125,
          "content": "# Contributing to VMAF\n\nPlease refer to this [slide deck](https://docs.google.com/presentation/d/1Gr4-MvOXu9HUiH4nnqLGWupJYMeh6nl2MNz6Qy9153c/edit#slide=id.p) for an overview contribution guide.\n\nIf you would like to contribute code to the VMAF repository, you can do so through GitHub by forking the repository and sending a pull request. When submitting code, please make every effort to follow existing conventions and style in order to keep the code as readable as possible.\n\n## License\n\nBy contributing your code, you agree to license your contribution under the terms of the [BSD+Patent](https://opensource.org/licenses/BSDplusPatent). Your contributions should also include the following header:\n\n```\n/**\n * Copyright 2016-2020 [the original author or authors].\n * \n * Licensed under the BSD+Patent License (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n * \n * https://opensource.org/licenses/BSDplusPatent\n * \n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n```\n\n## Ways to Contribute\n\nThere are many ways you can contribute, and no contribution is too small. To name a few:\n- Submitting a bugfix\n- Improving documentation\n- Making the code run on a new platform\n- Robustifying the build system\n- Improving the CI loop\n- Improving code coverage by adding tests\n- Optimizing the speed\n- Implementing a well-known quality metric\n- Implementing a custom VMAF model for a specific use case\n\n## Algorithmic Contribution\n\nThis section focuses on *algorithmic contribution*, which cover two main use cases: \n  - Implementing a *well-known quality metric* that can be found in the literature\n  - Implementing a *custom VMAF model*, using new elementary features and trained on a specific dataset\n\nFor both cases, one can follow the procedure below:\n  - First, implement the feature extractor(s), by subclassing the `FeatureExtractor` Python class. A new `FeatureExtractor` class created could be either 1) native Python implementation, or 2) calling a subprocess implemented in a different language (in C or in Matlab, for example).\n  - Second, implement the quality runner, by: \n    - creating a new `QualityRunner` class as a thin wrapper around the new `FeatureExtractor` created, or\n    - using the established `VmafQualityRunner` class but training a custom VMAF model.\n\nFor the concepts of `FeatureExtractor`, `QualityRunner` and `VmafQualityRunner`, please refer to the [Core Classes](resource/doc/python.md#core-classes) section of the VMAF Python library documentation.\n\nFor algorithmic contribution, for a clean organization of the repo, it is advised to submit new files under directory prefixed with `third_party/[orginization]`. For example, for a new model trained, it should go under `model/third_party/[organization]/`. As another example, the [PSNR-HVS feature extractor](https://github.com/Netflix/vmaf/commit/ce2ad1af0b1ba8dd1fbae3e03da0329f078e6bc6) code sits under `libvmaf/src/feature/third_party/xiph/`.\n\n### Creating a New `FeatureExtractor`\n\n#### Native Python\nTo create a subclass of `FeatureExtractor` in native Python code, a minimalist example to follow is the `PypsnrFeatureExtractor` class (\"Py-PSNR\", see the [code diff](https://github.com/Netflix/vmaf/commit/e698b4d788fb3dcabdc4df2fd1bffe88dc0d3ecd)). The following steps discuss the implementation strategy.\n  - Create a subclass of the `FeatureExtractor`. Make sure to specify the `TYPE`, `VERSION` and `ATOM_FEATURES`, which play a role in caching the features extracted in `ResultStore`. Optionally, one can specify a `DERIVED_ATOM_FEATURES` field (refer to the `_post_process_result()` method section for more details).\n  - Implement the `_generate_result()` method, which is responsible for the heavy-lifting feature calculation. This method is called by the `_run_on_asset()` method in the parent `Executor` class, which preprocesses the reference and distorted videos to prepare them in a proper YUV format. It calls two `YuvReader` to read the video frame-by-frame, and run computations on them. The result is written to the `log_file_path` file path.\n  - Implement the `_get_feature_scores()` method, which parses the result from the `log_file_path` and put it in a dictionary to prepare for a new `Result` object.\n  - Optionally, implement the `_post_process_result()` method to compute the `DERIVED_ATOM_FEATURES` from the `ATOM_FEATURES`. Refer to the [Python Calling Matlab](#python-calling-matlab) section for a specific example.\n  - Create [test cases](https://github.com/Netflix/vmaf/commit/e698b4d788fb3dcabdc4df2fd1bffe88dc0d3ecd#diff-5b58c2457df7e9b30b0a678d6f79b1caaad6c3f036edfadb6ca9fb0955bede33R630) to lock the numerical results.  \n  - Notice that `FeatureExtractor` allows one to pass in optional parameters that has an effect on the numerical result. This is demonstrated by the `max_db` parameter in `PypsnrFeatureExtractor` (see this [test case](https://github.com/Netflix/vmaf/commit/e698b4d788fb3dcabdc4df2fd1bffe88dc0d3ecd#diff-5b58c2457df7e9b30b0a678d6f79b1caaad6c3f036edfadb6ca9fb0955bede33R730) for an example use case). By default, there is a bit depth-dependent maximum PSNR value (see [here](FAQ.md#q-why-are-the-psnr-values-capped-at-60-db-for-8-bit-inputs-and-72-db-for-12-bit-inputs-in-the-packages-implementation) for the motivation behind), but the `max_db` parameter specified in the `optional_dict` input allows one to specify a maximum value.\n\n#### Python Calling `libvmaf` in C\nVery often the feature extractor implementation is in the C library `libvmaf`. In this case we simply create a thin Python `FeatureExtractor` subclass to call the `vmaf` command line executable. For more information on implementing a new feature extractor in `libvmaf`, refer to [this section](libvmaf/README.md#contributing-a-new-vmaffeatureextractor). An example to follow is the PSNR-HVS feature extractor (see the [code diff](https://github.com/Netflix/vmaf/commit/ce2ad1af0b1ba8dd1fbae3e03da0329f078e6bc6)). The following steps discuss the implementation strategy.\n  - Add a new feature extractor implementation `vmaf_fex_psnr_hvs` in file `libvmaf/src/feature/third_party/xiph/psnr_hvs.c`. It is recommended to put the code under directory `third_party/[org]`.\n  - In `libvmaf/src/feature/feature_extractor.c`:\n    - Declare the new feature extractor as `extern`:\n        ```c\n        extern VmafFeatureExtractor vmaf_fex_psnr_hvs;\n        ```\n    - Add the new feature extractor to the `feature_extractor_list`:\n        ```c\n        static VmafFeatureExtractor *feature_extractor_list[] = {\n        ...\n        &vmaf_fex_psnr_hvs,\n        ...\n        };\n        ```\n  - In `libvmaf/src/meson.build`, add the new `psnr_hvs.c` file to the `libvmaf_feature_sources` list:\n      ```c\n        libvmaf_feature_sources = [\n        ...\n        feature_src_dir + 'third_party/xiph/psnr_hvs.c',\n        ...\n        ]    \n      ```\n  - Create a Python wrapper class `PsnrhvsFeatureExtractor` in `python/vmaf/third_party/xiph/vmafexec_feature_extractor.py` (Note: you also need to make `vmaf.third_party.xiph` a Python package by adding the `__init__.py` files in corresponding directories.)\n  - Add a test case for `PsnrhvsFeatureExtractor` in `python/test/third_party/xiph/vmafexec_feature_extractor_test.py` to lock the numerical values.\n      \n#### Python Calling Matlab\nOftentimes for a well-known quality metric, its Matlab implementation already exists. The VMAF Python library allows directly plugging in the Matlab code by creating a thin Python `MatlabFeatureExtractor` subclass to call the Matlab script. An example to follow is the STRRED feature extractor (see [implementation](https://github.com/Netflix/vmaf/blob/master/python/vmaf/core/matlab_feature_extractor.py#L24-L113) and [test case](https://github.com/Netflix/vmaf/blob/e698b4d788fb3dcabdc4df2fd1bffe88dc0d3ecd/python/test/extra/feature_extractor_extratest.py#L25-L44)). The following steps discuss the implementation strategy.\n  - First, Matlab must be pre-installed and its path specified in the `MATLAB_PATH` field in the `python/vmaf/externals.py` file. If not, a user will be prompt with the installation instructions.\n  - Create a subclass of the `MatlabFeatureExtractor`. Make sure to specify the `TYPE`, `VERSION` and `ATOM_FEATURES`, which play a role in caching the features extracted in `ResultStore`. Optionally, one can specify a `DERIVED_ATOM_FEATURES` field (refer to the `_post_process_result()` method section for more details).\n  - Implement the `_generate_result()` method, which is responsible for calling the Matlab command line to output the result to the file at `log_file_path`.\n  - Implement the `_get_feature_scores()` method, which parses the result from the `log_file_path` and put it in a dictionary to prepare for a new `Result` object. In the case of the `StrredFeatureExtractor`, the default method provided by the `FeatureExtractor` superclass can be directly used as the Matlab script's data format is compatible with it, hence the implementation is skipped. But in general, this methods needs to be implemented.\n  - Optionally, implement the `_post_process_result()` method to compute the `DERIVED_ATOM_FEATURES` from the `ATOM_FEATURES`. In the case of STRRED, the `strred` feature can be derived from the `srred` and `trred` features via simple computation:\n    ```python\n    strred = srred * trred\n    ```\n    Therefore, we define the `strred` feature as \"derived\" and skip the caching process.\n  - Create [test cases]((https://github.com/Netflix/vmaf/blob/e698b4d788fb3dcabdc4df2fd1bffe88dc0d3ecd/python/test/extra/feature_extractor_extratest.py#L25-L44)) to lock the numerical results.\n\n### Creating a Thin `QualityRunner` Wrapper\nFor the use case of implementing a *well-known quality metric*, after the feature extractor is created, the job is almost done. But to run tests and scripts uniformly, we need to create a thin wrapper of the `QualityRunner` subclass around the new `FeatureExtractor` already created. A good example of this is the `SsimQualityRunner` class (see [code](https://github.com/Netflix/vmaf/blob/master/python/vmaf/core/quality_runner.py#L815)). One simply needs to create a subclass from `QualityRunnerFromFeatureExtractor`, and override the `_get_feature_extractor_class()` and `_get_feature_key_for_score()` methods.\n\n### Implementing a Custom VMAF Model\nFor the use case of implementing a *custom VMAF model*, one basically follows the two-step approach of first extracting quality-inducing features, and then using a machine-learning regressor to fuse the features and align the final result with subjective scores. The first step is covered by the `FeatureExtractor` subclasses; the second step is done through the `TrainTestModel` subclasses.\n\n#### Creating a New `TrainTestModel` Class\nThe default VMAF model has been using the `LibsvmNusvrTrainTestModel` class for training (via specifying the `model_type` field in the model parameter file, see the next section for more details). To use a different regressor, one needs to first create a new `TrainTestModel` class. One minimum example to follow is the `Logistic5PLRegressionTrainTestModel` (see [code diff](https://github.com/Netflix/vmaf/commit/bf607883f6bf37b9bdf6382ca191f3b1a4eb9102)). The following steps discuss the implementation strategy.\n  - Create a new class by subclassing `TrainTestModel` and `RegressorMixin`. Specify the `TYPE` and `VERSION` fields. The `TYPE` is to be specified by the `model_type` field in the model parameter file.\n  - Implement the `_train()` method, which fits the model with the input training data (preprocessed to be a 2D-array) and model parameters.\n  - Implement the `_predict()` method, which takes the fitted model and the input data (preprocess to be a 2D-array) and generate the predicted score.\n  - Optionally override housekeeping functions such as `_to_file()`, `_delete()`, `_from_info_loaded()` when needed.\n\n#### Calling the `run_vmaf_training` Script\nOnce the `FeatureExtractor` and `TrainTestModel` classes are ready, the actually training of a VMAF model against a dataset of subjective scores can be initiated by calling the `run_vmaf_training` script. Detailed description of how to use the script can be found in the [Train a New Model](resource/doc/python.md#train-a-new-model) section of VMAF Python Library documentation.\n\nNotice that the current `run_vmaf_training` implementation does not work with `FeatureExtractors` with a custom input parameter (e.g. the `max_db` of `PypsnrFeatureExtractor`). A workaround of this limitation is to create a subclass of the feature extractor with the hard-coded parameter (by overriding the `_custom_init()` method). Refer to [this code](https://github.com/Netflix/vmaf/commit/e698b4d788fb3dcabdc4df2fd1bffe88dc0d3ecd#diff-c5c651eeebd2949a31e059575be35f2018ec57ebdc09c0706cfc1187b9b32dbcR536) and [this test](https://github.com/Netflix/vmaf/commit/e698b4d788fb3dcabdc4df2fd1bffe88dc0d3ecd#diff-5b58c2457df7e9b30b0a678d6f79b1caaad6c3f036edfadb6ca9fb0955bede33R751) for an example.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.515625,
          "content": "FROM ubuntu:22.04\n\n# get and install building tools\nRUN apt-get update && \\\n    apt-get install -y \\\n    build-essential \\\n    ninja-build \\\n    nasm \\\n    doxygen \\\n    python3 \\\n    python3-pip \\\n    python3-venv \\\n    xxd\n\n# retrieve source code\nCOPY . /vmaf\n\n# setup environment\nENV PATH=/vmaf:/vmaf/libvmaf/build/tools:$PATH\n\n# make vmaf\nRUN cd /vmaf && make clean && make\n\n# install python tools\nRUN pip3 install --no-cache-dir -r /vmaf/python/requirements.txt\n\nWORKDIR /vmaf\n\nENV PYTHONPATH=python\n\nENTRYPOINT [ \"vmaf\" ]\n"
        },
        {
          "name": "Dockerfile.cuda",
          "type": "blob",
          "size": 1.8359375,
          "content": "ARG CUDA_VERSION=12.3.1\n# By copying the installation from a devel to a runtime container one could likely save a lot container size\nFROM nvidia/cuda:$CUDA_VERSION-devel-ubuntu22.04 \n\nARG VMAF_TAG=master\nARG FFMPEG_TAG=master\n\nRUN DEBIAN_FRONTEND=noninteractive apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y libopenjp2-7-dev \\\n    ninja-build cmake git python3 python3-pip nasm xxd pkg-config curl unzip\n\nRUN git clone https://github.com/Netflix/vmaf.git && cd vmaf && git checkout $VMAF_TAG\n\nRUN git clone https://github.com/FFmpeg/FFmpeg.git && cd FFmpeg && git checkout $FFMPEG_TAG\n\nRUN git clone https://github.com/FFmpeg/nv-codec-headers.git && cd nv-codec-headers && make && make install\n\n# install vmaf\nRUN python3 -m pip install meson\nRUN cd vmaf && meson libvmaf/build libvmaf -Denable_cuda=true -Denable_avx512=true --buildtype release && \\\n    ninja -vC libvmaf/build  && \\\n    ninja -vC libvmaf/build  install\n\n# install ffmpeg\nRUN cd FFmpeg && ./configure \\\n    --enable-libnpp \\\n    --enable-nonfree \\\n    --enable-nvdec \\\n    --enable-nvenc \\\n    --enable-cuvid \\\n    --enable-cuda \\\n    --enable-cuda-nvcc \\\n    --enable-libvmaf \\\n    --enable-ffnvcodec \\\n    --disable-stripping \\\n    --extra-cflags=\"-I/usr/local/cuda/include\" \\\n    --extra-ldflags=\"-L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs/\" \n\nRUN cd FFmpeg && make -j && make install\n\nRUN mkdir /data\n# VMAF+decode GPU (only works for NVDec supported formats https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new)\nENTRYPOINT [\"ffmpeg\" ,\"-hwaccel\", \"cuda\", \"-hwaccel_output_format\" ,\"cuda\", \\\n    \"-i\" ,\"/data/ref.mp4\", \\\n    \"-hwaccel\", \"cuda\", \"-hwaccel_output_format\", \"cuda\", \\\n    \"-i\", \"/data/dis.mp4\"  ,\\\n    \"-filter_complex\", \"[0:v]scale_cuda=format=yuv420p[ref];[1:v]scale_cuda=format=yuv420p[dist];[dist][ref]libvmaf_cuda\" ,\"-f\" ,\"null\", \"-\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 2.7666015625,
          "content": "LICENSE - BSD+Patent\nSPDX short identifier: BSD-2-Clause-Patent\n\nNote: This license is designed to provide: a) a simple permissive license; b) that is compatible with the GNU General\nPublic License (GPL), version 2; and c) which also has an express patent grant included.\n\nCopyright (c) 2020 Netflix, Inc.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the\nfollowing conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following\ndisclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following\ndisclaimer in the documentation and/or other materials provided with the distribution.\n\nSubject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those\nreceiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except\nfor failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell,\nimport, and otherwise transfer this software, where such license applies only to those patent claims, already acquired\nor hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by:\n\n(a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors,\nin source or binary form) alone; or\n\n(b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such\ncopyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be\nnecessarily infringed. The patent license shall not apply to any other combinations which include the Contribution.\n\nExcept as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this\nlicense, whether expressly, by implication, estoppel or otherwise.\n\nDISCLAIMER\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.9384765625,
          "content": "# Use bash for shell\nSHELL := /bin/bash\n\n# Path and environment setup\nVENV := .venv\nVIRTUAL_ENV_PATH := $(VENV)/bin\n\n# Build tools configured in the virtual environment\nPYTHON_INTERPRETER := python3.10\nVENV_PIP := $(VIRTUAL_ENV_PATH)/pip\nVENV_PYTHON := $(VIRTUAL_ENV_PATH)/python\nMESON_SETUP := $(VIRTUAL_ENV_PATH)/meson setup\nNINJA := $(VIRTUAL_ENV_PATH)/ninja\n\n# Build types and options\nBUILDTYPE_RELEASE := --buildtype release\nBUILDTYPE_DEBUG := --buildtype debug\nENABLE_FLOAT := -Denable_float=true\n\n# Directories\nLIBVMAF_DIR := libvmaf\nBUILD_DIR := $(LIBVMAF_DIR)/build\nDEBUG_DIR := $(LIBVMAF_DIR)/debug\n\n.PHONY: default all debug build install cythonize clean distclean\n\ndefault: build\n\nall: build debug install test cythonize\n\n$(BUILD_DIR): $(VENV)\n\tPATH=\"$(VENV)/bin:$$PATH\" $(MESON_SETUP) $(BUILD_DIR) $(LIBVMAF_DIR) $(BUILDTYPE_RELEASE) $(ENABLE_FLOAT)\n\n$(DEBUG_DIR): $(VENV)\n\tPATH=\"$(VENV)/bin:$$PATH\" $(MESON_SETUP) $(DEBUG_DIR) $(LIBVMAF_DIR) $(BUILDTYPE_DEBUG) $(ENABLE_FLOAT)\n\ncythonize: $(VENV)\n\tpushd python && ../$(VENV_PYTHON) setup.py build_ext --build-lib . && popd || exit 1\n\nbuild: $(BUILD_DIR) $(VENV)\n\tPATH=\"$(VENV)/bin:$$PATH\" $(NINJA) -vC $(BUILD_DIR)\n\ntest: build $(VENV)\n\tPATH=\"$(VENV)/bin:$$PATH\" $(NINJA) -vC $(BUILD_DIR) test\n\ndebug: $(DEBUG_DIR) $(VENV)\n\tPATH=\"$(VENV)/bin:$$PATH\" $(NINJA) -vC $(DEBUG_DIR)\n\ninstall: $(BUILD_DIR) $(VENV)\n\tPATH=\"$(VENV)/bin:$$PATH\" $(NINJA) -vC $(BUILD_DIR) install\n\nclean:\n\trm -rf $(BUILD_DIR) $(DEBUG_DIR)\n\trm -f python/vmaf/core/adm_dwt2_cy.c*\n\ndistclean: clean\n\trm -rf $(VENV)\n\n# Set up or rebuild virtual environment\n$(VENV):\n\t@echo \"Setting up the virtual environment...\"\n\t@set -e; \\\n\t$(PYTHON_INTERPRETER) -m venv $(VENV) || { echo \"Failed to create virtual environment\"; exit 1; }; \\\n\t$(VENV_PIP) install --upgrade pip || { echo \"Failed to upgrade pip\"; exit 1; }; \\\n\t$(VENV_PIP) install meson ninja cython numpy || { echo \"Failed to install dependencies\"; exit 1; }\n\t@echo \"Virtual environment setup complete.\""
        },
        {
          "name": "OSSMETADATA",
          "type": "blob",
          "size": 0.0185546875,
          "content": "osslifecycle=active"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.95703125,
          "content": "# VMAF - Video Multi-Method Assessment Fusion\n\n[![Build Status](https://app.travis-ci.com/Netflix/vmaf.svg?token=KLMprpotiqp3mZnrjtA6&branch=master)](https://app.travis-ci.com/Netflix/vmaf)\n[![libvmaf](https://github.com/Netflix/vmaf/actions/workflows/libvmaf.yml/badge.svg)](https://github.com/Netflix/vmaf/actions/workflows/libvmaf.yml)\n[![Windows](https://github.com/Netflix/vmaf/actions/workflows/windows.yml/badge.svg)](https://github.com/Netflix/vmaf/actions/workflows/windows.yml)\n[![ffmpeg](https://github.com/Netflix/vmaf/actions/workflows/ffmpeg.yml/badge.svg)](https://github.com/Netflix/vmaf/actions/workflows/ffmpeg.yml)\n[![Docker](https://github.com/Netflix/vmaf/actions/workflows/docker.yml/badge.svg)](https://github.com/Netflix/vmaf/actions/workflows/docker.yml)\n\nVMAF is an [Emmy-winning](https://theemmys.tv/) perceptual video quality assessment algorithm developed by Netflix. This software package includes a stand-alone C library `libvmaf` and its wrapping Python library. The Python library also provides a set of tools that allows a user to train and test a custom VMAF model.\n\nRead [this](https://medium.com/netflix-techblog/toward-a-practical-perceptual-video-quality-metric-653f208b9652) tech blog post for an overview, [this](https://medium.com/netflix-techblog/vmaf-the-journey-continues-44b51ee9ed12) post for the tips of best practices, and [this](https://netflixtechblog.com/toward-a-better-quality-metric-for-the-video-community-7ed94e752a30) post for our latest efforts on speed optimization, new API design and the introduction of a codec evaluation-friendly [NEG mode](resource/doc/models.md#disabling-enhancement-gain-neg-mode).\n\nAlso included in `libvmaf` are implementations of several other metrics: PSNR, PSNR-HVS, SSIM, MS-SSIM and CIEDE2000.\n\n![vmaf logo](resource/images/vmaf_logo.jpg)\n\n## News\n\n- (2023-12-07) We are releasing `libvmaf v3.0.0`. It contains several optimizations and bug fixes, and a full removal of the APIs which were deprecated in `v2.0.0`.\n- (2021-12-15) We have added to CAMBI the `full_ref` input parameter to allow running CAMBI as a full-reference metric, taking into account the banding that was already present on the source. Check out the [usage](resource/doc/cambi.md) page.\n- (2021-12-1) We have added to CAMBI the `max_log_contrast` input parameter to allow to capture banding with higher contrasts than the default. We have also sped up CAMBI (e.g., around 4.5x for 4k). Check out the [usage](resource/doc/cambi.md) page.\n- (2021-10-7) We are open-sourcing CAMBI (Contrast Aware Multiscale Banding Index) - Netflix's detector for banding (aka contouring) artifacts. Check out the [tech blog](https://netflixtechblog.medium.com/cambi-a-banding-artifact-detector-96777ae12fe2) for an overview and the [technical paper](resource/doc/papers/CAMBI_PCS2021.pdf) published in PCS 2021 (note that the paper describes an initial version of CAMBI that no longer matches the code exactly, but it is still a good introduction). Also check out the [usage](resource/doc/cambi.md) page.\n- (2020-12-7) Check out our [latest tech blog](https://netflixtechblog.com/toward-a-better-quality-metric-for-the-video-community-7ed94e752a30) on speed optimization, new API design and the introduction of a codec evaluation-friendly NEG mode.\n- (2020-12-3) We are releasing `libvmaf v2.0.0`. It has a new fixed-point and x86 SIMD-optimized (AVX2, AVX-512) implementation that achieves 2x speed up compared to the previous floating-point version. It also has a [new API](libvmaf/README.md) that is more flexible and extensible.\n- (2020-7-13) We have created a [memo](https://docs.google.com/document/d/1dJczEhXO0MZjBSNyKmd3ARiCTdFVMNPBykH4_HMPoyY/edit?usp=sharing) to share our thoughts on VMAF's property in the presence of image enhancement operations, its impact on codec evaluation, and our solutions. Accordingly, we have added a new mode called [No Enhancement Gain (NEG)](resource/doc/models.md#disabling-enhancement-gain-neg-mode).\n- (2020-2-27) We have changed VMAF's license from Apache 2.0 to [BSD+Patent](https://opensource.org/licenses/BSDplusPatent), a more permissive license compared to Apache that also includes an express patent grant.\n\n## Documentation\n\nThere is an [overview of the documentation](resource/doc/index.md) with links to specific pages, covering FAQs, available models and features, software usage guides, and a list of resources.\n\n## Usage\n\nThe software package offers a number of ways to interact with the VMAF implementation.\n\n  - The command-line tool [`vmaf`](libvmaf/tools/README.md) provides a complete algorithm implementation, such that one can easily deploy VMAF in a production environment. Additionally, the `vmaf` tool provides a number of auxillary features such as PSNR, SSIM and MS-SSIM.\n  - The [C library `libvmaf`](libvmaf/README.md) provides an interface to incorporate VMAF into your code, and tools to integrate other feature extractors into the library.\n  - The [Python library](resource/doc/python.md) offers a full array of wrapper classes and scripts for software testing, VMAF model training and validation, dataset processing, data visualization, etc.\n  - VMAF is now included as a filter in FFmpeg, and can be configured using: `./configure --enable-libvmaf`. Refer to the [Using VMAF with FFmpeg](resource/doc/ffmpeg.md) page.\n  - [VMAF Dockerfile](Dockerfile) generates a docker image from the [Python library](resource/doc/python.md). Refer to [this](resource/doc/docker.md) document for detailed usage.\n  - To build VMAF on Windows, follow [these](resource/doc/windows.md) instructions.\n  - AOM CTC: [AOM]((http://aomedia.org/)) has specified vmaf to be the standard implementation metrics tool according to the AOM common test conditions (CTC). Refer to [this page](resource/doc/aom_ctc.md) for usage compliant with AOM CTC.\n\n## Contribution Guide\n\nRefer to the [contribution](CONTRIBUTING.md) page. Also refer to this [slide deck](https://docs.google.com/presentation/d/1Gr4-MvOXu9HUiH4nnqLGWupJYMeh6nl2MNz6Qy9153c/edit#slide=id.gc20398b4b7_0_132) for an overview contribution guide.\n"
        },
        {
          "name": "libvmaf",
          "type": "tree",
          "content": null
        },
        {
          "name": "matlab",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "resource",
          "type": "tree",
          "content": null
        },
        {
          "name": "unittest",
          "type": "blob",
          "size": 0.1640625,
          "content": "#!/usr/bin/env sh\n\nif [ -z \"$1\" ]; then\n    pattern='*_test.py'\nelse\n    pattern=\"$1\"\nfi\n\nPYTHONPATH=python python3 -m unittest discover -v -s python/test/ -p $pattern\n"
        },
        {
          "name": "workspace",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}