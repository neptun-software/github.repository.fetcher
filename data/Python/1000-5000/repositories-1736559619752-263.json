{
  "metadata": {
    "timestamp": 1736559619752,
    "page": 263,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "thunlp/OpenNRE",
      "stars": 4367,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.3857421875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# test\ntest.py\n\n# ckpt\nckpt\n\n# vscode\n.vscode\n\n# tacred\nbenchmark/tacred\n*.swp\n\n<<<<<<< HEAD\n# data and pretrain\npretrain\nbenchmark\n!benchmark/*.sh\n!pretrain/*.sh\n\n# test env\n.test\n\n# package\nopennre-egg.info\n\n*.sh\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2019 Tianyu Gao\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.533203125,
          "content": "# OpenNRE (sub-project of OpenSKL)\n \nOpenNRE is a sub-project of OpenSKL, providing an **Open**-source **N**eural **R**elation **E**xtraction toolkit for extracting structured knowledge from plain text, with [ATT](https://aclanthology.org/P16-1200.pdf) as key features to consider relation-associated text information.\n\n## Overview\n\nOpenNRE is an open-source and extensible toolkit that provides a unified framework to implement relation extraction models. We unify the input and output interfaces of different relation extraction models and provide scalable options for each model. The toolkit covers both supervised and distant supervised settings, and is compatible with both conventional neural networks and pre-trained language models.\n\nRelation extraction is a natural language processing (NLP) task aiming at extracting relations (e.g., *founder of*) between entities (e.g., **Bill Gates** and **Microsoft**). For example, from the sentence *Bill Gates founded Microsoft*, we can extract the relation triple (**Bill Gates**, *founder of*, **Microsoft**). \n\nRelation extraction is a crucial technique in automatic knowledge graph construction. By using relation extraction, we can accumulatively extract new relation facts and expand the knowledge graph, which, as a way for machines to understand the human world, has many downstream applications like question answering, recommender system and search engine. If you want to learn more about neural relation extraction, visit another project of ours ([NREPapers](https://github.com/thunlp/NREPapers)).\n\nIt's our honor to help you better explore relation extraction with our OpenNRE toolkit! You can refer to our [document](https://opennre-docs.readthedocs.io/en/latest/) for more details about this project.\n\n<!--### What is Relation Extraction\n### Target Group\n\nThis package is designed for the following groups:\n\n* **Beginner**: We have hand-by-hand tutorials and detailed documents that can not only enable you to use relation extraction tools, but also help you better understand the research progress in this field.\n* **Developers**: Our easy-to-use interface and high-performance implementation can acclerate your deployment in the real-world applications. Besides, we provide several pretrained models which can be put into production without any training.\n* **Researchers**: With our modular design, various task settings and metric tools, you can easily carry out experiments on your own models with only minor modification. We have also provided several most-used benchmarks for different settings of relation extraction.\n* **Anyone who need to submit an NLP homework to impress their professors**: With state-of-the-art models, our package can definitely help you stand out among your classmates!-->\n\n## Models\nIn this toolkit, we support CNN-based relation extraction models including standard CNN and our proposed [CNN+ATT](https://aclanthology.org/P16-1200v2.pdf). We also implement methods based on pre-trained language models (BERT).\n\n## Evaluation\n\nTo validate the effectiveness of this toolkit, we employ the Bag-Level Relation Extraction task for evaluation.\n\n### Settings\n\nWe utilize the NYT10 dataset, which is a distantly supervised collection derived from the New York Times corpus and FreeBase. We mainly experiment on CNN-ATT model, which employs instance-level attention and shows superior performance compared with vanilla CNN.\n\n### Results\n\nWe report AUC and F1 scores of two models. The right two columns marked with (\\*) indicates the results sourced from [Gao et al.(2021)](https://aclanthology.org/2021.findings-acl.112.pdf) and [Lin et al.(2016)](https://aclanthology.org/P16-1200v2.pdf). The results show that our implementation of CNN-ATT model is slighly better than the original paper, and also confirm the better performance of CNN-ATT over standard CNN model.\n\n| Model | AUC | F1 | AUC(Paper \\*) | F1(Paper \\*) |\n| :-: | :-: | :-: | :-: | :-: |\n| CNN | - | - | 0.212 | 0.318 | \n| CNN-ATT | 0.333 | 0.397 | 0.318 | 0.380 |\n\n## Usage\n\n### Installation\n\n#### Install as A Python Package\n\nWe are now working on deploy OpenNRE as a Python package. Coming soon!\n\n#### Using Git Repository\n\nClone the repository from our github page (don't forget to star us!)\n\n```bash\ngit clone https://github.com/thunlp/OpenNRE.git\n```\n\nIf it is too slow, you can try\n```\ngit clone https://github.com/thunlp/OpenNRE.git --depth 1\n```\n\nThen install all the requirements:\n\n```\npip install -r requirements.txt\n```\n\n**Note**: Please choose appropriate PyTorch version based on your machine (related to your CUDA version). For details, refer to https://pytorch.org/. \n\nThen install the package with \n```\npython setup.py install \n```\n\nIf you also want to modify the code, run this:\n```\npython setup.py develop\n```\n\nNote that we have excluded all data and pretrain files for fast deployment. You can manually download them by running scripts in the ``benchmark`` and ``pretrain`` folders. For example, if you want to download FewRel dataset, you can run\n\n```bash\nbash benchmark/download_fewrel.sh\n```\n### Data\n\nYou can go into the `benchmark` folder and download datasets using our scripts. We also list some of the information about the datasets in [this document](https://opennre-docs.readthedocs.io/en/latest/get_started/benchmark.html#bag-level-relation-extraction). We provide two distantly-supervised datasets with human-annotated test sets, **NYT10m** and **Wiki20m**. Check the [datasets](#datasets) section for details.\n\n### Easy Start\n\nMake sure you have installed OpenNRE as instructed above. Then import our package and load pre-trained models.\n\n```python\n>>> import opennre\n>>> model = opennre.get_model('wiki80_cnn_softmax')\n```\n\nNote that it may take a few minutes to download checkpoint and data for the first time. Then use `infer` to do sentence-level relation extraction\n\n```python\n>>> model.infer({'text': 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})\n('father', 0.5108704566955566)\n```\n\nYou will get the relation result and its confidence score.\n\nIf you want to use the model on your GPU, just run \n```python\n>>> model = model.cuda()\n```\nbefore calling the inference function.\n\nFor now, we have the following available models:\n\n* `wiki80_cnn_softmax`: trained on `wiki80` dataset with a CNN encoder.\n* `wiki80_bert_softmax`: trained on `wiki80` dataset with a BERT encoder.\n* `wiki80_bertentity_softmax`: trained on `wiki80` dataset with a BERT encoder (using entity representation concatenation).\n* `tacred_bert_softmax`: trained on `TACRED` dataset with a BERT encoder.\n* `tacred_bertentity_softmax`: trained on `TACRED` dataset with a BERT encoder (using entity representation concatenation).\n\n### Training\n\nYou can train your own models on your own data with OpenNRE. In `example` folder we give example training codes for supervised RE models and bag-level RE models. You can either use our provided datasets or your own datasets. For example, you can use the following script to train a PCNN-ATT bag-level model on the NYT10 dataset with manual test set. The ATT algorithm is a typical method to combine a bag of sentences for extracting relations between entities.\n\n```bash\npython example/train_bag_cnn.py \\\n    --metric auc \\\n    --dataset nyt10m \\\n    --batch_size 160 \\\n    --lr 0.1 \\\n    --weight_decay 1e-5 \\\n    --max_epoch 100 \\\n    --max_length 128 \\\n    --seed 42 \\\n    --encoder pcnn \\\n    --aggr att\n```\n\nOr use the following script to train a BERT model on the Wiki80 dataset:\n```bash\npython example/train_supervised_bert.py \\\n    --pretrain_path bert-base-uncased \\\n    --dataset wiki80\n```\n\nWe provide many options in the example training code and you can check them out for detailed instructions.\n\n## Citation\n\nIf you find OpenNRE is useful for your research, please consider citing the following papers:\n\n```\n@inproceedings{han-etal-2019-opennre,\n    title = \"{O}pen{NRE}: An Open and Extensible Toolkit for Neural Relation Extraction\",\n    author = \"Han, Xu and Gao, Tianyu and Yao, Yuan and Ye, Deming and Liu, Zhiyuan and Sun, Maosong\",\n    booktitle = \"Proceedings of EMNLP-IJCNLP: System Demonstrations\",\n    year = \"2019\",\n    url = \"https://www.aclweb.org/anthology/D19-3029\",\n    doi = \"10.18653/v1/D19-3029\",\n    pages = \"169--174\"\n}\n```\n\nThis package is mainly contributed by [Tianyu Gao](https://github.com/gaotianyu1350), [Xu Han](https://github.com/THUCSTHanxu13), [Shulian Cao](https://github.com/ShulinCao), [Lumin Tang](https://github.com/Tsingularity), [Yankai Lin](https://github.com/Mrlyk423), [Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/)\n\n\n******************\n## About OpenSKL\nOpenSKL project aims to harness the power of both structured knowledge and natural languages via representation learning. All sub-projects of OpenSKL, under the categories of **Algorithm**, **Resource** and **Application**, are as follows.\n\n- **Algorithm**: \n  - [OpenKE](https://www.github.com/thunlp/OpenKE)\n    - An effective and efficient toolkit for representing structured knowledge in large-scale knowledge graphs as embeddings, with <a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/9491/9350\"> TransR</a> and  <a href=\"https://aclanthology.org/D15-1082.pdf\">PTransE</a> as key features to handle complex relations and relational paths.\n    - This toolkit also includes three repositories:\n       - [KB2E](https://www.github.com/thunlp/KB2E)\n       - [TensorFlow-Transx](https://www.github.com/thunlp/TensorFlow-Transx)\n       - [Fast-TransX](https://www.github.com/thunlp/Fast-TransX)\n  - [ERNIE](https://github.com/thunlp/ERNIE)\n    - An effective and efficient toolkit for augmenting pre-trained language models with knowledge graph representations.\n  - [OpenNE](https://www.github.com/thunlp/OpenNE)\n    - An effective and efficient toolkit for representing nodes in large-scale graphs as embeddings, with [TADW](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) as key features to incorporate text attributes of nodes.\n  - [OpenNRE](https://www.github.com/thunlp/OpenNRE)\n    - An effective and efficient toolkit for implementing neural networks for extracting structured knowledge from text, with [ATT](https://aclanthology.org/P16-1200.pdf) as key features to consider relation-associated text information.\n    - This toolkit also includes two repositories:\n      - [JointNRE](https://www.github.com/thunlp/JointNRE)\n      - [NRE](https://github.com/thunlp/NRE)\n- **Resource**:\n  - The embeddings of large-scale knowledge graphs pre-trained by OpenKE, covering three typical large-scale knowledge graphs: Wikidata, Freebase, and XLORE. The embeddings are free to use under the [MIT license](https://opensource.org/license/mit/), and please click the following link to submit [download requests](http://139.129.163.161/download/wikidata).\n  - OpenKE-Wikidata\n    - Wikidata is a free and collaborative database, collecting structured data to provide support for Wikipedia. The original Wikidata contains 20,982,733 entities, 594 relations and 68,904,773 triplets. In particular, Wikidata-5M is the core subgraph of Wikidata, containing  5,040,986 high-frequency entities from Wikidata with their corresponding 927 relations and 24,267,796 triplets.\n    - [TransE version](http://139.129.163.161/download/wikidata): Knowledge embeddings of Wikidata pre-trained by OpenKE. \n    - [TransR version](http://139.129.163.161/download/wikidata) of Wikidata-5M: Knowledge embeddings of Wikidata-5M pre-trained by OpenKE.\n  - OpenKE-Freebase\n    - Freebase was a large collaborative knowledge base consisting of data composed mainly by its community members. It was an online collection of structured data harvested from many sources. Freebase contains 86,054,151 entities, 14,824 relations and 338,586,276 triplets.\n    - [TransE version](http://139.129.163.161/download/wikidata): Knowledge embeddings of Freebase pre-trained by OpenKE. \n  - OpenKE-XLORE\n    - XLORE is one of the most popular Chinese knowledge graphs developed by THUKEG. XLORE contains 10,572,209 entities, 138,581 relations and 35,954,249 triplets.\n    - [TransE version](http://139.129.163.161/download/wikidata): Knowledge embeddings of XLORE pre-trained by OpenKE.\n- **Application**:   \n    - [Knowledge-Plugin](https://github.com/THUNLP/Knowledge-Plugin)\n      - An effective and efficient toolkit of plug-and-play knowledge injection for pre-trained language models. Knowledge-Plugin is general for all kinds of knowledge graph embeddings mentioned above. In the toolkit, we plug the TransR version of Wikidata-5M into BERT as an example of applications. With the TransR embedding, we enhance the knowledge ability of BERT without fine-tuning the original model, e.g., up to 8% improvement on question answering.\n"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "example",
          "type": "tree",
          "content": null
        },
        {
          "name": "opennre",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretrain",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.091796875,
          "content": "torch==1.6.0\ntransformers==3.4.0\npytest==5.3.2\nscikit-learn==0.22.1\nscipy==1.4.1\nnltk>=3.6.4\n\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.6455078125,
          "content": "import setuptools\nwith open(\"README.md\", \"r\") as fh:\n    setuptools.setup(\n        name='open-nre',  \n        version='0.1.1',\n        author=\"Tianyu Gao\",\n        author_email=\"gaotianyu1350@126.com\",\n        description=\"An open source toolkit for relation extraction\",\n        url=\"https://github.com/thunlp/opennre\",\n        packages=setuptools.find_packages(),\n        classifiers=[\n            \"Programming Language :: Python :: 3\",\n            \"License :: OSI Approved :: MIT License\",\n            \"Operating System :: POSIX :: Linux\",\n        ],\n        install_requires=['nltk', 'transformers', 'scikit-learn'],\n        setup_requires=['wheel']\n     )\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}