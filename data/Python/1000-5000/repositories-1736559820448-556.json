{
  "metadata": {
    "timestamp": 1736559820448,
    "page": 556,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "zju3dv/EasyMocap",
      "stars": 3805,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.3046875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.vscode\n.tensorboard\nexp/coco*\n*.pth\n*.weights\n.idea\noutput\ndata/**\n.DS*\ncode_deprecate\ncode\n# neuralbody\nlightning_logs\nmodels\nyolov5m.pt\n"
        },
        {
          "name": "3rdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.953125,
          "content": "//////////////////////////////////////////////////////////////////////////// \n// Copyright 2020-2021 3D Vision Group at the State Key Lab of CAD&CG,  \n// Zhejiang University. All Rights Reserved. \n// \n// For more information see <https://github.com/zju3dv/EasyMocap> \n// If you use this software, please cite the corresponding publications   \n// listed on the above website. \n// \n// Permission to use, copy, modify and distribute this software and its \n// documentation for educational, research and non-profit purposes only. \n// Any modification based on this work must be open-source and prohibited \n// for commercial use. \n// You must retain, in the source form of any derivative works that you  \n// distribute, all copyright, patent, trademark, and attribution notices  \n// from the source form of this work. \n//  \n// For commercial uses of this software, please send email to xwzhou@zju.edu.cn\n////////////////////////////////////////////////////////////////////////////\n"
        },
        {
          "name": "Readme.md",
          "type": "blob",
          "size": 13.4033203125,
          "content": "<!--\n * @Date: 2021-01-13 20:32:12\n * @Author: Qing Shuai\n * @LastEditors: Qing Shuai\n * @LastEditTime: 2022-11-03 13:09:58\n * @FilePath: /EasyMocapRelease/Readme.md\n-->\n\n<div align=\"center\">\n    <img src=\"logo.png\" width=\"40%\">\n</div>\n\n**EasyMocap** is an open-source toolbox for **markerless human motion capture** and **novel view synthesis** from RGB videos. In this project, we provide a lot of motion capture demos in different settings.\n\n![python](https://img.shields.io/github/languages/top/zju3dv/EasyMocap)\n![star](https://img.shields.io/github/stars/zju3dv/EasyMocap?style=social)\n\n## News\n\n- :tada: Our SIGGRAPH 2022 [**Novel View Synthesis of Human Interactions From Sparse Multi-view Videos**](https://chingswy.github.io/easymocap-public-doc/works/multinb.html) is released! Check the [documentation](https://chingswy.github.io/easymocap-public-doc/works/multinb.html).\n- :tada: EasyMocap v0.2 is released! We support motion capture from Internet videos. Please check the [Quick Start](https://chingswy.github.io/easymocap-public-doc/quickstart/quickstart.html) for more details.\n\n\n---\n\n## Core features\n\n### Multiple views of a single person\n\n[![report](https://img.shields.io/badge/quickstart-green)](./doc/quickstart.md) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Cyvu_lPFUajr2RKt6yJIfS3HQIIYl6QU?usp=sharing)\n\nThis is the basic code for fitting SMPL[^loper2015]/SMPL+H[^romero2017]/SMPL-X[^pavlakos2019]/MANO[^romero2017] model to capture body+hand+face poses from multiple views.\n\n<div align=\"center\">\n    <img src=\"doc/feng/mv1pmf-smplx.gif\" width=\"80%\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/mv1p-dance-smpl.gif\" width=\"80%\">\n    <br>\n    <sup>Videos are from ZJU-MoCap, with 23 calibrated and synchronized cameras.</sup>\n</div>\n\n<div align=\"center\">\n    <img src=\"doc/feng/mano.gif\" width=\"80%\">\n    <br>\n    <sup>Captured with 8 cameras.</sup>\n</div>\n\n### Internet video\n\nThis part is the basic code for fitting SMPL[^loper2015] with 2D keypoints estimation[^cao2018][^hrnet] and CNN initialization[^kolotouros2019].\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/23EfsN7vEOA%2B003170%2B003670.gif\" width=\"80%\">\n    <br>\n    <sup>The raw video is from <a href=\"https://www.youtube.com/watch?v=23EfsN7vEOA\">Youtube</a>.</sup>\n</div>\n\n### Internet video with a mirror\n\n[![report](https://img.shields.io/badge/CVPR21-mirror-red)](https://arxiv.org/pdf/2104.00340.pdf) [![quickstart](https://img.shields.io/badge/quickstart-green)](https://github.com/zju3dv/Mirrored-Human)\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/zju3dv/Mirrored-Human/main/doc/assets/smpl-avatar.gif\" width=\"80%\">\n    <br>\n    <sup>The raw video is from <a href=\"https://www.youtube.com/watch?v=KOCJJ27hhIE\">Youtube</a>.</sup>\n</div>\n\n\n### Multiple Internet videos with a specific action (Coming soon)\n\n[![report](https://img.shields.io/badge/ECCV20-imocap-red)](https://arxiv.org/pdf/2008.07931.pdf) [![quickstart](https://img.shields.io/badge/quickstart-green)](./doc/todo.md)\n\n<div align=\"center\">\n    <img src=\"doc/imocap/imocap.gif\" width=\"80%\"><br/>\n    <sup>Internet videos of Roger Federer's serving</sup>\n</div>\n\n### Multiple views of multiple people\n\n[![report](https://img.shields.io/badge/CVPR19-mvpose-red)](https://arxiv.org/pdf/1901.04111.pdf) [![quickstart](https://img.shields.io/badge/quickstart-green)](./doc/mvmp.md)\n\n<div align=\"center\">\n    <img src=\"doc/assets/mvmp1f.gif\" width=\"80%\"><br/>\n    <sup>Captured with 8 consumer cameras</sup>\n</div>\n\n### Novel view synthesis from sparse views\n[![report](https://img.shields.io/badge/CVPR21-neuralbody-red)](https://arxiv.org/pdf/2012.15838.pdf) [![quickstart](https://img.shields.io/badge/quickstart-green)](https://github.com/zju3dv/neuralbody)\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/female-ballet.gif\" width=\"80%\"><br/>\n    <sup>Novel view synthesis for chanllenge motion(coming soon)</sup>\n</div>\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/nvs_mp_soccer1_6_rgb.gif\" width=\"80%\"><br/>\n    <sup>Novel view synthesis for human interaction</sup>\n</div>\n\n    \n## ZJU-MoCap\n\nWith our proposed method, we release two large dataset of human motion: LightStage and Mirrored-Human. See the [website](https://chingswy.github.io/Dataset-Demo/) for more details.\n\nIf you would like to download the ZJU-Mocap dataset, please sign the [agreement](https://pengsida.net/project_page_assets/files/ZJU-MoCap_Agreement.pdf), and email it to Qing Shuai (s_q@zju.edu.cn) and cc Xiaowei Zhou (xwzhou@zju.edu.cn) to request the download link.\n\n<div align=\"center\">\n<div align=\"center\" width=\"40%\">\n    <img src=\"doc/assets/ZJU-MoCap-lightstage.jpg\" width=\"40%\"><br/>\n    <sup>LightStage: captured with LightStage system</sup>\n</div>\n<div align=\"center\" width=\"40%\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/mirrored-human.jpg\" width=\"40%\"><br/>\n    <sup>Mirrored-Human: collected from the Internet</sup>\n</div>\n</div>\n\nMany works have achieved wonderful results based on our dataset:\n\n- [Real-time volumetric rendering of dynamic humans](https://real-time-humans.github.io/)\n- [CVPR2022: HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video](https://grail.cs.washington.edu/projects/humannerf/)\n- [ECCV2022: KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints](https://markomih.github.io/KeypointNeRF/)\n- [SIGGRAPH 2022 paper Drivable Volumetric Avatars using Texel-Aligned Features](https://github.com/facebookresearch/dva)\n\n## Other features\n\n### 3D Realtime visualization\n[![quickstart](https://img.shields.io/badge/quickstart-green)](./doc/realtime_visualization.md)\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/assets/vis3d/skel-body25.gif\" width=\"26%\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/assets/vis3d/skel-total.gif\" width=\"26%\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/assets/vis3d/skel-multi.gif\" width=\"26%\">\n</div>\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/assets/vis3d/mesh-smpl.gif\" width=\"26%\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/assets/vis3d/mesh-smplx.gif\" width=\"26%\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/assets/vis3d/mesh-manol.gif\" width=\"26%\">\n</div>\n\n### [Camera calibration](apps/calibration/Readme.md)\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/calib_intri.jpg\" width=\"40%\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/calib_extri.jpg\" width=\"40%\">\n    <br>\n    <sup>Calibration for intrinsic and extrinsic parameters</sup>\n</div>\n\n### [Annotator](apps/annotation/Readme.md)\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/annot_keypoints.jpg\" width=\"40%\">\n    <img src=\"https://raw.githubusercontent.com/chingswy/Dataset-Demo/main/EasyMocap/annot_mask.jpg\" width=\"40%\">\n    <br>\n    <sup>Annotator for bounding box, keypoints and mask</sup>\n</div>\n\n\n## Updates\n- 11/03/2022: Support MultiNeuralBody.\n- 12/25/2021: Support mediapipe keypoints detector.\n- 08/09/2021: Add a colab demo [here](https://colab.research.google.com/drive/1Cyvu_lPFUajr2RKt6yJIfS3HQIIYl6QU?usp=sharing).\n- 06/28/2021: The **Multi-view Multi-person** part is released!\n- 06/10/2021: The **real-time 3D visualization** part is released!\n- 04/11/2021: The calibration tool and the annotator are released.\n- 04/11/2021: **Mirrored-Human** part is released.\n\n## Installation\n\nSee [documentation](https://chingswy.github.io/easymocap-public-doc/install/install.html) for more instructions.\n\n## Acknowledgements\n\nHere are the great works this project is built upon:\n\n- SMPL models and layer are from MPII [SMPL-X model](https://github.com/vchoutas/smplx).\n- Some functions are borrowed from [SPIN](https://github.com/nkolot/SPIN), [VIBE](https://github.com/mkocabas/VIBE), [SMPLify-X](https://github.com/vchoutas/smplify-x)\n- The method for fitting 3D skeleton and SMPL model is similar to [SMPLify-X](https://github.com/vchoutas/smplify-x)(with 3D keypoints loss), [TotalCapture](http://www.cs.cmu.edu/~hanbyulj/totalcapture/)(without using point clouds).\n- We integrate some easy-to-use functions for previous great work:\n  - `easymocap/estimator/mediapipe_wrapper.py`: [MediaPipe](https://github.com/google/mediapipe)\n  - `easymocap/estimator/SPIN`  : an SMPL estimator[^cao2018]\n  - `easymocap/estimator/YOLOv4`: an object detector[^kolotouros2019]\n  - `easymocap/estimator/HRNet` : a 2D human pose estimator[^bochkovskiy2020]\n\n## Contact\n\nPlease open an issue if you have any questions. We appreciate all contributions to improve our project.\n    \n\n## Contributor\n\nEasyMocap is **built by** researchers from the 3D vision group of Zhejiang University: [**Qing Shuai**](https://chingswy.github.io/), [**Qi Fang**](https://raypine.github.io/), [**Junting Dong**](https://jtdong.com/), [**Sida Peng**](https://pengsida.net/), **Di Huang**, [**Hujun Bao**](http://www.cad.zju.edu.cn/home/bao/), **and** [**Xiaowei Zhou**](https://xzhou.me/). \n\nWe would like to thank Wenduo Feng, Di Huang, Yuji Chen, Hao Xu, Qing Shuai, Qi Fang, Ting Xie, Junting Dong, Sida Peng and Xiaopeng Ji who are the performers in the sample data. We would also like to thank all the people who has helped EasyMocap [in any way](https://github.com/zju3dv/EasyMocap/graphs/contributors).\n\n## Citation\n\nThis project is a part of our work [iMocap](https://zju3dv.github.io/iMoCap/), [Mirrored-Human](https://zju3dv.github.io/Mirrored-Human/), [mvpose](https://zju3dv.github.io/mvpose/), [Neural Body](https://zju3dv.github.io/neuralbody/), [MultiNeuralBody](https://chingswy.github.io/easymocap-public-doc/works/multinb.html), [enerf]().\n\nPlease consider citing these works if you find this repo is useful for your projects.\n\n```bibtex\n@Misc{easymocap,  \n    title = {EasyMoCap - Make human motion capture easier.},\n    howpublished = {Github},  \n    year = {2021},\n    url = {https://github.com/zju3dv/EasyMocap}\n}\n\n@inproceedings{shuai2022multinb,\n  title={Novel View Synthesis of Human Interactions from Sparse\nMulti-view Videos},\n  author={Shuai, Qing and Geng, Chen and Fang, Qi and Peng, Sida and Shen, Wenhao and Zhou, Xiaowei and Bao, Hujun},\n  booktitle={SIGGRAPH Conference Proceedings},\n  year={2022}\n}\n\n@inproceedings{lin2022efficient,\n  title={Efficient Neural Radiance Fields for Interactive Free-viewpoint Video},\n  author={Lin, Haotong and Peng, Sida and Xu, Zhen and Yan, Yunzhi and Shuai, Qing and Bao, Hujun and Zhou, Xiaowei},\n  booktitle={SIGGRAPH Asia Conference Proceedings},\n  year={2022}\n}\n\n@inproceedings{dong2021fast,\n  title={Fast and Robust Multi-Person 3D Pose Estimation and Tracking from Multiple Views},\n  author={Dong, Junting and Fang, Qi and Jiang, Wen and Yang, Yurou and Bao, Hujun and Zhou, Xiaowei},\n  booktitle={T-PAMI},\n  year={2021}\n}\n    \n@inproceedings{dong2020motion,\n  title={Motion capture from internet videos},\n  author={Dong, Junting and Shuai, Qing and Zhang, Yuanqing and Liu, Xian and Zhou, Xiaowei and Bao, Hujun},\n  booktitle={European Conference on Computer Vision},\n  pages={210--227},\n  year={2020},\n  organization={Springer}\n}\n\n@inproceedings{peng2021neural,\n  title={Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans},\n  author={Peng, Sida and Zhang, Yuanqing and Xu, Yinghao and Wang, Qianqian and Shuai, Qing and Bao, Hujun and Zhou, Xiaowei},\n  booktitle={CVPR},\n  year={2021}\n}\n\n@inproceedings{fang2021mirrored,\n  title={Reconstructing 3D Human Pose by Watching Humans in the Mirror},\n  author={Fang, Qi and Shuai, Qing and Dong, Junting and Bao, Hujun and Zhou, Xiaowei},\n  booktitle={CVPR},\n  year={2021}\n}\n\n```\n\n[^loper2015]: Loper, Matthew, et al. \"SMPL: A skinned multi-person linear model.\" ACM transactions on graphics (TOG) 34.6 (2015): 1-16.\n\n[^romero2017]: Romero, Javier, Dimitrios Tzionas, and Michael J. Black. \"Embodied hands: Modeling and capturing hands and bodies together.\" ACM Transactions on Graphics (ToG) 36.6 (2017): 1-17.\n\n[^pavlakos2019]: Pavlakos, Georgios, et al. \"Expressive body capture: 3d hands, face, and body from a single image.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n<!-- [4] Bogo, Federica, et al. \"Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image.\" European conference on computer vision. Springer, Cham, 2016. -->\n\n[^cao2018]: Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: Openpose: real-time multi-person 2d pose estimation using part affinity fields. arXiv preprint arXiv:1812.08008 (2018)\n\n[^kolotouros2019]: Kolotouros, Nikos, et al. \"Learning to reconstruct 3D human pose and shape via model-fitting in the loop.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019\n\n[^bochkovskiy2020]: Bochkovskiy, Alexey, Chien-Yao Wang, and Hong-Yuan Mark Liao. \"Yolov4: Optimal speed and accuracy of object detection.\" arXiv preprint arXiv:2004.10934 (2020).\n\n[^hrnet]: Sun, Ke, et al. \"Deep high-resolution representation learning for human pose estimation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n"
        },
        {
          "name": "apps",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "easymocap",
          "type": "tree",
          "content": null
        },
        {
          "name": "library",
          "type": "tree",
          "content": null
        },
        {
          "name": "logo.png",
          "type": "blob",
          "size": 144.3193359375,
          "content": null
        },
        {
          "name": "myeasymocap",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.205078125,
          "content": "ipdb\njoblib\ntqdm\nopencv-python\nyacs\ntabulate\ntermcolor\ngit+https://github.com/mattloper/chumpy.git\nmediapipe==0.10.0\nfunc_timeout\nultralytics\ngdown\nsetuptools==59.5.0\ntensorboard==2.8.0\npytorch-lightning==1.5.0"
        },
        {
          "name": "requirements_neuralbody.txt",
          "type": "blob",
          "size": 0.109375,
          "content": "setuptools==59.5.0\ntensorboard==2.8.0\npytorch-lightning==1.5.0\nspconv-cu111\nlpips\nscikit-image\ntorch_tb_profiler"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.068359375,
          "content": "'''\n  @ Date: 2021-03-02 16:53:55\n  @ Author: Qing Shuai\n  @ LastEditors: Qing Shuai\n  @ LastEditTime: 2022-11-03 13:10:59\n  @ FilePath: /EasyMocapRelease/setup.py\n'''\nfrom setuptools import setup\n\nsetup(\n    name='easymocap',     \n    version='0.2.1',   #\n    description='Easy Human Motion Capture Toolbox',\n    author='Qing Shuai', \n    author_email='s_q@zju.edu.cn',\n    # test_suite='setup.test_all',\n    packages=[\n        'easymocap',\n        'easymocap.config',\n        'easymocap.dataset',\n        'easymocap.smplmodel',\n        'easymocap.pyfitting',\n        'easymocap.mytools', \n        'easymocap.annotator',\n        'easymocap.estimator',\n        'myeasymocap'\n    ],\n    entry_points={\n        'console_scripts': [\n            'emc=apps.mocap.run:main_entrypoint',\n            # 'easymocap_calib=easymocap.mytools.entry:calib',\n            # 'easymocap_tools=easymocap.mytools.entry:main',\n            # 'extract_keypoints=easymocap.mytools.cmdtools.extract_keypoints:main'\n        ],\n    },\n    install_requires=[],\n    data_files = []\n)\n\nemc = \"apps.mocap.run:main_entrypoint\"\n"
        }
      ]
    }
  ]
}