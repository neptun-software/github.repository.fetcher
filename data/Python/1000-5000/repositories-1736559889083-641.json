{
  "metadata": {
    "timestamp": 1736559889083,
    "page": 641,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "MaartenGr/KeyBERT",
      "stars": 3647,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 0.0400390625,
          "content": "d8c24872c6aee5c8e10496291def5c97098e017b\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0302734375,
          "content": "*.ipynb linguist-documentation\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.939453125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Sphinx documentation\ndocs/_build/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n.idea\n.idea/\n.vscode\n\n# MacOS\n.DS_Store\n\n# ruff\n.ruff_cache\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.4853515625,
          "content": "repos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n    -   id: trailing-whitespace\n        exclude: |\n            (?x)^(\n                README.md|\n                docs/\n            )$\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n-   repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.5.1\n    hooks:\n    -   id: ruff\n        args: [--fix, --show-fixes, --exit-non-zero-on-fix]\n    -   id: ruff-format\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0556640625,
          "content": "MIT License\n\nCopyright (c) 2020, Maarten P. Grootendorst\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.3154296875,
          "content": "test:\n\tpytest\n\ninstall:\n\tpython -m pip install -e .\n\ninstall-test:\n\tpython -m pip install -e \".[test]\"\n\tpython -m pip install -e \".[all]\"\n\npypi:\n\tpython -m build\n\ttwine upload dist/*\n\nclean:\n\trm -rf **/.ipynb_checkpoints **/.pytest_cache **/__pycache__ **/**/__pycache__ .ipynb_checkpoints .pytest_cache\n\ncheck: test clean\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.51953125,
          "content": "[![PyPI - Python](https://img.shields.io/badge/python-3.6%20|%203.7%20|%203.8-blue.svg)](https://pypi.org/project/keybert/)\n[![PyPI - License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/MaartenGr/keybert/blob/master/LICENSE)\n[![PyPI - PyPi](https://img.shields.io/pypi/v/keyBERT)](https://pypi.org/project/keybert/)\n[![Build](https://img.shields.io/github/actions/workflow/status/MaartenGr/keyBERT/testing.yml?branch=master)](https://pypi.org/keybert/)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1OxpgwKqSzODtO3vS7Xe1nEmZMCAIMckX?usp=sharing)\n\n<img src=\"images/logo.png\" width=\"35%\" height=\"35%\" align=\"right\" />\n\n# KeyBERT\n\nKeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to\ncreate keywords and keyphrases that are most similar to a document.\n\nCorresponding medium post can be found [here](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea).\n\n<a name=\"toc\"/></a>\n## Table of Contents  \n<!--ts-->  \n   1. [About the Project](#about)  \n   2. [Getting Started](#gettingstarted)  \n        2.1. [Installation](#installation)  \n        2.2. [Basic Usage](#usage)  \n        2.3. [Max Sum Distance](#maxsum)  \n        2.4. [Maximal Marginal Relevance](#maximal)  \n        2.5. [Embedding Models](#embeddings)  \n   3. [Large Language Models](#llms)  \n<!--te-->  \n\n\n<a name=\"about\"/></a>\n## 1. About the Project\n[Back to ToC](#toc)\n\nAlthough there are already many methods available for keyword generation\n(e.g.,\n[Rake](https://github.com/aneesha/RAKE),\n[YAKE!](https://github.com/LIAAD/yake), TF-IDF, etc.)\nI wanted to create a very basic, but powerful method for extracting keywords and keyphrases.\nThis is where **KeyBERT** comes in! Which uses BERT-embeddings and simple cosine similarity\nto find the sub-phrases in a document that are the most similar to the document itself.\n\nFirst, document embeddings are extracted with BERT to get a document-level representation.\nThen, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity\nto find the words/phrases that are the most similar to the document. The most similar words could\nthen be identified as the words that best describe the entire document.\n\nKeyBERT is by no means unique and is created as a quick and easy method\nfor creating keywords and keyphrases. Although there are many great\npapers and solutions out there that use BERT-embeddings\n(e.g.,\n[1](https://github.com/pranav-ust/BERT-keyphrase-extraction),\n[2](https://github.com/ibatra/BERT-Keyword-Extractor),\n[3](https://www.preprints.org/manuscript/201908.0073/download/final_file),\n), I could not find a BERT-based solution that did not have to be trained from scratch and\ncould be used for beginners (**correct me if I'm wrong!**).\nThus, the goal was a `pip install keybert` and at most 3 lines of code in usage.\n\n<a name=\"gettingstarted\"/></a>\n## 2. Getting Started\n[Back to ToC](#toc)\n\n<a name=\"installation\"/></a>\n###  2.1. Installation\nInstallation can be done using [pypi](https://pypi.org/project/keybert/):\n\n```\npip install keybert\n```\n\nYou may want to install more depending on the transformers and language backends that you will be using. The possible installations are:\n\n```\npip install keybert[flair]\npip install keybert[gensim]\npip install keybert[spacy]\npip install keybert[use]\n```\n\n<a name=\"usage\"/></a>\n###  2.2. Usage\n\nThe most minimal example can be seen below for the extraction of keywords:\n```python\nfrom keybert import KeyBERT\n\ndoc = \"\"\"\n         Supervised learning is the machine learning task of learning a function that\n         maps an input to an output based on example input-output pairs. It infers a\n         function from labeled training data consisting of a set of training examples.\n         In supervised learning, each example is a pair consisting of an input object\n         (typically a vector) and a desired output value (also called the supervisory signal).\n         A supervised learning algorithm analyzes the training data and produces an inferred function,\n         which can be used for mapping new examples. An optimal scenario will allow for the\n         algorithm to correctly determine the class labels for unseen instances. This requires\n         the learning algorithm to generalize from the training data to unseen situations in a\n         'reasonable' way (see inductive bias).\n      \"\"\"\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc)\n```\n\nYou can set `keyphrase_ngram_range` to set the length of the resulting keywords/keyphrases:\n\n```python\n>>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None)\n[('learning', 0.4604),\n ('algorithm', 0.4556),\n ('training', 0.4487),\n ('class', 0.4086),\n ('mapping', 0.3700)]\n```\n\nTo extract keyphrases, simply set `keyphrase_ngram_range` to (1, 2) or higher depending on the number\nof words you would like in the resulting keyphrases:\n\n```python\n>>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words=None)\n[('learning algorithm', 0.6978),\n ('machine learning', 0.6305),\n ('supervised learning', 0.5985),\n ('algorithm analyzes', 0.5860),\n ('learning function', 0.5850)]\n```\n\nWe can highlight the keywords in the document by simply setting `highlight`:\n\n```python\nkeywords = kw_model.extract_keywords(doc, highlight=True)\n```\n<img src=\"images/highlight.png\" width=\"75%\" height=\"75%\" />\n\n\n**NOTE**: For a full overview of all possible transformer models see [sentence-transformer](https://www.sbert.net/docs/pretrained_models.html).\nI would advise either `\"all-MiniLM-L6-v2\"` for English documents or `\"paraphrase-multilingual-MiniLM-L12-v2\"`\nfor multi-lingual documents or any other language.\n\n<a name=\"maxsum\"/></a>\n###  2.3. Max Sum Distance\n\nTo diversify the results, we take the 2 x top_n most similar words/phrases to the document.\nThen, we take all top_n combinations from the 2 x top_n words and extract the combination\nthat are the least similar to each other by cosine similarity.\n\n```python\n>>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n                              use_maxsum=True, nr_candidates=20, top_n=5)\n[('set training examples', 0.7504),\n ('generalize training data', 0.7727),\n ('requires learning algorithm', 0.5050),\n ('supervised learning algorithm', 0.3779),\n ('learning machine learning', 0.2891)]\n```\n\n\n<a name=\"maximal\"/></a>\n###  2.4. Maximal Marginal Relevance\n\nTo diversify the results, we can use Maximal Margin Relevance (MMR) to create\nkeywords / keyphrases which is also based on cosine similarity. The results\nwith **high diversity**:\n\n```python\n>>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n                              use_mmr=True, diversity=0.7)\n[('algorithm generalize training', 0.7727),\n ('labels unseen instances', 0.1649),\n ('new examples optimal', 0.4185),\n ('determine class labels', 0.4774),\n ('supervised learning algorithm', 0.7502)]\n```\n\nThe results with **low diversity**:\n\n```python\n>>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n                              use_mmr=True, diversity=0.2)\n[('algorithm generalize training', 0.7727),\n ('supervised learning algorithm', 0.7502),\n ('learning machine learning', 0.7577),\n ('learning algorithm analyzes', 0.7587),\n ('learning algorithm generalize', 0.7514)]\n```\n\n\n<a name=\"embeddings\"/></a>\n###  2.5. Embedding Models\nKeyBERT supports many embedding models that can be used to embed the documents and words:\n\n* Sentence-Transformers\n* Flair\n* Spacy\n* Gensim\n* USE\n\nClick [here](https://maartengr.github.io/KeyBERT/guides/embeddings.html) for a full overview of all supported embedding models.\n\n**Sentence-Transformers**  \nYou can select any model from `sentence-transformers` [here](https://www.sbert.net/docs/pretrained_models.html)\nand pass it through KeyBERT with `model`:\n\n```python\nfrom keybert import KeyBERT\nkw_model = KeyBERT(model='all-MiniLM-L6-v2')\n```\n\nOr select a SentenceTransformer model with your own parameters:\n\n```python\nfrom keybert import KeyBERT\nfrom sentence_transformers import SentenceTransformer\n\nsentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nkw_model = KeyBERT(model=sentence_model)\n```\n\n**Flair**  \n[Flair](https://github.com/flairNLP/flair) allows you to choose almost any embedding model that\nis publicly available. Flair can be used as follows:\n\n```python\nfrom keybert import KeyBERT\nfrom flair.embeddings import TransformerDocumentEmbeddings\n\nroberta = TransformerDocumentEmbeddings('roberta-base')\nkw_model = KeyBERT(model=roberta)\n```\n\nYou can select any 🤗 transformers model [here](https://huggingface.co/models).\n\n<a name=\"llms\"/></a>\n## 3. Large Language Models\n[Back to ToC](#toc)\n\nWith `KeyLLM` you can new perform keyword extraction with Large Language Models (LLM). You can find the full documentation [here](https://maartengr.github.io/KeyBERT/guides/keyllm.html) but there are two examples that are common with this new method. Make sure to install the OpenAI package through `pip install openai` before you start.\n\nFirst, we can ask OpenAI directly to extract keywords:\n\n```python\nimport openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n```\n\nThis will query any ChatGPT model and ask it to extract keywords from text.\n\nSecond, we can find documents that are likely to have the same keywords and only extract keywords for those. \nThis is much more efficient then asking the keywords for every single documents. There are likely documents that \nhave the exact same keywords. Doing so is straightforward:\n\n```python\nimport openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\nfrom sentence_transformers import SentenceTransformer\n\n# Extract embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(MY_DOCUMENTS, convert_to_tensor=True)\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\nkeywords = kw_model.extract_keywords(MY_DOCUMENTS, embeddings=embeddings, threshold=.75)\n```\n\nYou can use the `threshold` parameter to decide how similar documents need to be in order to receive the same keywords.\n\n## Citation\nTo cite KeyBERT in your work, please use the following bibtex reference:\n\n```bibtex\n@misc{grootendorst2020keybert,\n  author       = {Maarten Grootendorst},\n  title        = {KeyBERT: Minimal keyword extraction with BERT.},\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {v0.3.0},\n  doi          = {10.5281/zenodo.4461265},\n  url          = {https://doi.org/10.5281/zenodo.4461265}\n}\n```\n\n## References\nBelow, you can find several resources that were used for the creation of KeyBERT\nbut most importantly, these are amazing resources for creating impressive keyword extraction models:\n\n**Papers**:\n* Sharma, P., & Li, Y. (2019). [Self-Supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling.](https://www.preprints.org/manuscript/201908.0073/download/final_file)\n\n**Github Repos**:\n* https://github.com/thunlp/BERT-KPE\n* https://github.com/ibatra/BERT-Keyword-Extractor\n* https://github.com/pranav-ust/BERT-keyphrase-extraction\n* https://github.com/swisscom/ai-research-keyphrase-extraction\n\n**MMR**:\nThe selection of keywords/keyphrases was modeled after:\n* https://github.com/swisscom/ai-research-keyphrase-extraction\n\n**NOTE**: If you find a paper or github repo that has an easy-to-use implementation\nof BERT-embeddings for keyword/keyphrase extraction, let me know! I'll make sure to\nadd a reference to this repo.\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "keybert",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 1.8046875,
          "content": "site_name: KeyBERT\nrepo_url: https://github.com/MaartenGr/keyBERT\nsite_url: https://maartengr.github.io/keyBERT/\nsite_description: Leveraging BERT to extract important keywords\nsite_author: Maarten P. Grootendorst\nuse_directory_urls: false\nextra_css:\n  - stylesheets/extra.css\n\n\nnav:\n  - Home: index.md\n  - Guides:\n      - Quickstart: guides/quickstart.md\n      - Embedding Models: guides/embeddings.md\n      - CountVectorizer: guides/countvectorizer.md\n      - KeyLLM: guides/keyllm.md\n      - LLMs: guides/llms.md\n  - API:\n      - KeyBERT: api/keybert.md\n      - MMR: api/mmr.md\n      - MaxSum: api/maxsum.md\n      - KeyLLM: api/keyllm.md\n      - LLM:\n        - OpenAI: api/openai.md\n        - Cohere: api/cohere.md\n        - LangChain: api/langchain.md\n        - TextGeneration: api/textgeneration.md\n        - LiteLLM: api/litellm.md\n  - FAQ: faq.md\n  - Changelog: changelog.md\n\nplugins:\n  - mkdocstrings:\n      watch:\n        - keybert\n  - search\ncopyright: Copyright &copy; 2024 Maintained by <a href=\"https://github.com/MaartenGr\">Maarten Grootendorst</a>.\n\ntheme:\n  custom_dir: images/\n  name: material\n  icon:\n    logo: material/library\n  font:\n    text: Ubuntu\n    code: Ubuntu Mono\n  favicon: icon.png\n  logo: icon.png\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.instant\n    - navigation.top\n    - navigation.tracking\n    - toc.follow\n  palette:\n  - media: \"(prefers-color-scheme: light)\"\n    scheme: black\n    toggle:\n      icon: material/weather-sunny\n      name: Switch to dark mode\n  - media: \"(prefers-color-scheme: dark)\"\n    scheme: slate\n    primary: black\n    toggle:\n      icon: material/weather-night\n      name: Switch to light mode\n\nmarkdown_extensions:\n  - admonition\n  - pymdownx.details\n  - pymdownx.highlight\n  - pymdownx.superfences\n  - pymdownx.snippets\n  - toc:\n      permalink: true\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.611328125,
          "content": "[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"keybert\"\nversion = \"0.8.5\"\ndescription = \"KeyBERT performs keyword extraction with state-of-the-art transformer models.\"\nreadme = \"README.md\"\nlicense = {file = \"LICENSE\"}\nrequires-python = \">=3.8\"\nauthors = [\n    { name = \"Maarten Grootendorst\", email = \"maartengrootendorst@gmail.com\" },\n]\nkeywords = [\n    \"nlp\",\n    \"bert\",\n    \"keyword\",\n    \"extraction\",\n    \"embeddings\",\n]\nclassifiers = [\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: MacOS\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Operating System :: POSIX\",\n    \"Operating System :: Unix\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n]\ndependencies = [\n    \"numpy>=1.18.5\",\n    \"rich>=10.4.0\",\n    \"scikit-learn>=0.22.2\",\n    \"sentence-transformers>=0.3.8\"\n]\n\n[project.optional-dependencies]\ndev = [\n    \"keybert[docs,test]\",\n]\ndocs = [\n    \"mkdocs-material>=4.6.3\",\n    \"mkdocs>=1.1\",\n    \"mkdocstrings>=0.8.0\",\n]\nflair = [\n    \"flair>=0.7\",\n    \"torch>=1.4.0\",\n    \"transformers>=3.5.1\",\n]\ngensim = [\n    \"gensim>=3.6.0\",\n]\nspacy = [\n    \"spacy>=3.0.1\",\n]\ntest = [\n    \"black>=19.3b0\",\n    \"flake8>=3.6.0\",\n    \"pre-commit>=2.2.0\",\n    \"pytest-cov>=2.6.1\",\n    \"pytest>=5.4.3\",\n]\ntgi = [\n    \"huggingface-hub>=0.23.3\",\n    \"pydantic>=2.7.4\"\n]\nuse = [\n    \"tensorflow\",\n    \"tensorflow_hub\",\n    \"tensorflow_text\",\n]\n\n[project.urls]\nDocumentation = \"https://maartengr.github.io/KeyBERT/\"\nHomepage = \"https://github.com/MaartenGr/KeyBERT\"\nIssues = \"https://github.com/MaartenGr/KeyBERT/issues\"\nRepository = \"https://github.com/MaartenGr/KeyBERT.git\"\n\n[tool.setuptools.packages.find]\ninclude = [\"keybert*\"]\nexclude = [\"tests\"]\n\n[tool.ruff]\nline-length = 120\n\n[tool.ruff.lint]\nselect = [\n    \"E4\",  # Ruff Defaults\n    \"E7\",\n    \"E9\",\n    \"F\",   # End Ruff Defaults,\n    \"D\"\n]\nignore = [\n    \"D100\", # Missing docstring in public module\n    \"D104\", # Missing docstring in public package\n    \"D205\", # 1 blank line required between summary line and description\n    \"E731\", # Do not assign a lambda expression, use a def\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"**/tests/*\" = [\"D\"] # Ignore all docstring errors in tests\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "theme",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}