{
  "metadata": {
    "timestamp": 1736559956824,
    "page": 736,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "XPixelGroup/DiffBIR",
      "stars": 3506,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.16796875,
          "content": "__pycache__\n*.ckpt\n*.pth\n/data\n/exps\n*.sh\n!install_env.sh\n/weights\n/temp\n/results\n.ipynb_checkpoints/\n/TODO.txt\n/deprecated\n/temp_scripts\n/.vscode\n/runs\n/tests\n/meta_files\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0751953125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 XPixelGroup\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.630859375,
          "content": "<p align=\"center\">\n    <img src=\"assets/logo.png\" width=\"400\">\n</p>\n\n## DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior\n\n[Paper](https://arxiv.org/abs/2308.15070) | [Project Page](https://0x3f3f3f3fun.github.io/projects/diffbir/)\n\n![visitors](https://visitor-badge.laobi.icu/badge?page_id=XPixelGroup/DiffBIR) [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/linxinqi/DiffBIR-official) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/DiffBIR-colab/blob/main/DiffBIR_colab.ipynb) [![Try a demo on Replicate](https://replicate.com/zsxkib/diffbir/badge)](https://replicate.com/zsxkib/diffbir)\n\n[Xinqi Lin](https://0x3f3f3f3fun.github.io/)<sup>1,\\*</sup>, [Jingwen He](https://github.com/hejingwenhejingwen)<sup>2,3,\\*</sup>, [Ziyan Chen](https://orcid.org/0000-0001-6277-5635)<sup>1</sup>, [Zhaoyang Lyu](https://scholar.google.com.tw/citations?user=gkXFhbwAAAAJ&hl=en)<sup>2</sup>, [Bo Dai](http://daibo.info/)<sup>2</sup>, [Fanghua Yu](https://github.com/Fanghua-Yu)<sup>1</sup>, [Wanli Ouyang](https://wlouyang.github.io/)<sup>2</sup>, [Yu Qiao](http://mmlab.siat.ac.cn/yuqiao)<sup>2</sup>, [Chao Dong](http://xpixel.group/2010/01/20/chaodong.html)<sup>1,2</sup>\n\n<sup>1</sup>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences<br><sup>2</sup>Shanghai AI Laboratory<br><sup>3</sup>The Chinese University of Hong Kong\n\n<p align=\"center\">\n    <img src=\"assets/teaser.png\">\n</p>\n\n---\n\n<p align=\"center\">\n    <img src=\"assets/pipeline.png\">\n</p>\n\n:star:If DiffBIR is helpful for you, please help star this repo. Thanks!:hugs:\n\n## :book:Table Of Contents\n\n- [Update](#update)\n- [Visual Results On Real-world Images](#visual_results)\n- [TODO](#todo)\n- [Installation](#installation)\n- [Quick Start](#quick_start)\n- [Pretrained Models](#pretrained_models)\n- [Inference](#inference)\n- [Train](#train)\n\n## <a name=\"update\"></a>:new:Update\n\n- **2024.11.27**: ✅ Release DiffBIR v2.1, including a **new model** trained on unsplash dataset with [LLaVA]()-generated captions, more samplers, better tiled-sampling support and so on. Check [release note](https://github.com/XPixelGroup/DiffBIR/releases/tag/v2.1.0) for details.\n- **2024.04.08**: ✅ Release everything about our [updated manuscript](https://arxiv.org/abs/2308.15070), including (1) a **new model** trained on subset of laion2b-en and (2) a **more readable code base**, etc. DiffBIR is now a general restoration pipeline that could handle different blind image restoration tasks with a unified generation module.\n- **2023.09.19**: ✅ Add support for Apple Silicon! Check [installation_xOS.md](assets/docs/installation_xOS.md) to work with **CPU/CUDA/MPS** device!\n- **2023.09.14**: ✅ Integrate a patch-based sampling strategy ([mixture-of-diffusers](https://github.com/albarji/mixture-of-diffusers)). [**Try it!**](#tiled-sampling) Here is an [example](https://imgsli.com/MjA2MDA1) with a resolution of 2396 x 1596. GPU memory usage will continue to be optimized in the future and we are looking forward to your pull requests!\n- **2023.09.14**: ✅ Add support for background upsampler (DiffBIR/[RealESRGAN](https://github.com/xinntao/Real-ESRGAN)) in face enhancement! :rocket: [**Try it!**](#inference_fr)\n- **2023.09.13**: :rocket: Provide online demo (DiffBIR-official) in [OpenXLab](https://openxlab.org.cn/apps/detail/linxinqi/DiffBIR-official), which integrates both general model and face model. Please have a try! [camenduru](https://github.com/camenduru) also implements an online demo, thanks for his work.:hugs:\n- **2023.09.12**: ✅ Upload inference code of latent image guidance and release [real47](inputs/real47) testset.\n- **2023.09.08**: ✅ Add support for restoring unaligned faces.\n- **2023.09.06**: :rocket: Update [colab demo](https://colab.research.google.com/github/camenduru/DiffBIR-colab/blob/main/DiffBIR_colab.ipynb). Thanks to [camenduru](https://github.com/camenduru)!:hugs:\n- **2023.08.30**: This repo is released.\n\n## <a name=\"visual_results\"></a>:eyes:Visual Results On Real-world Images\n\n### Blind Image Super-Resolution\n\n[<img src=\"assets/visual_results/bsr6.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODI3) [<img src=\"assets/visual_results/bsr7.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODI4) [<img src=\"assets/visual_results/bsr4.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODI1)\n\n<!-- [<img src=\"assets/visual_results/bsr1.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODIy) [<img src=\"assets/visual_results/bsr2.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODIz)\n\n[<img src=\"assets/visual_results/bsr3.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODI0) [<img src=\"assets/visual_results/bsr5.png\" height=\"223px\"/>](https://imgsli.com/MjAxMjM0) -->\n\n<!-- [<img src=\"assets/visual_results/bsr1.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODIy) [<img src=\"assets/visual_results/bsr5.png\" height=\"223px\"/>](https://imgsli.com/MjAxMjM0) -->\n\n### Blind Face Restoration\n\n<!-- [<img src=\"assets/visual_results/bfr1.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODI5) [<img src=\"assets/visual_results/bfr2.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODMw) [<img src=\"assets/visual_results/bfr4.png\" height=\"223px\"/>](https://imgsli.com/MTk5ODM0) -->\n\n[<img src=\"assets/visual_results/whole_image1.png\" height=\"370\"/>](https://imgsli.com/MjA2MTU0) \n[<img src=\"assets/visual_results/whole_image2.png\" height=\"370\"/>](https://imgsli.com/MjA2MTQ4)\n\n:star: Face and the background enhanced by DiffBIR.\n\n### Blind Image Denoising\n\n[<img src=\"assets/visual_results/bid1.png\" height=\"215px\"/>](https://imgsli.com/MjUzNzkz) [<img src=\"assets/visual_results/bid3.png\" height=\"215px\"/>](https://imgsli.com/MjUzNzky)\n[<img src=\"assets/visual_results/bid2.png\" height=\"215px\"/>](https://imgsli.com/MjUzNzkx)\n\n### 8x Blind Super-Resolution With Tiled Sampling\n\n> I often think of Bag End. I miss my books and my arm chair, and my garden. See, that's where I belong. That's home. --- Bilbo Baggins\n\n[<img src=\"assets/visual_results/tiled_sampling.png\" height=\"480px\"/>](https://imgsli.com/MjUzODE4)\n\n## <a name=\"todo\"></a>:climbing:TODO\n\n- [x] Release code and pretrained models :computer:.\n- [x] Update links to paper and project page :link:.\n- [x] Release real47 testset :minidisc:.\n- [ ] Provide webui.\n- [x] Reduce the vram usage of DiffBIR :fire::fire::fire:.\n- [ ] Provide HuggingFace demo :notebook:.\n- [x] Add a patch-based sampling schedule :mag:.\n- [x] Upload inference code of latent image guidance :page_facing_up:.\n- [x] Improve the performance :superhero:.\n- [x] Support MPS acceleration for MacOS users.\n- [ ] DiffBIR-turbo :fire::fire::fire:.\n- [x] Speed up inference, such as using fp16/bf16, torch.compile :fire::fire::fire:.\n\n## <a name=\"installation\"></a>:gear:Installation\n\n```shell\n# clone this repo\ngit clone https://github.com/XPixelGroup/DiffBIR.git\ncd DiffBIR\n\n# create environment\nconda create -n diffbir python=3.10\nconda activate diffbir\npip install -r requirements.txt\n```\n\nOur new code is based on pytorch 2.2.2 for the built-in support of memory-efficient attention. If you are working on a GPU that is not compatible with the latest pytorch, just downgrade pytorch to 1.13.1+cu116 and install xformers 0.0.16 as an alternative.\n<!-- Note the installation is only compatible with **Linux** users. If you are working on different platforms, please check [xOS Installation](assets/docs/installation_xOS.md). -->\n\n## <a name=\"quick_start\"></a>:flight_departure:Quick Start\n\nRun the following command to interact with the gradio website.\n\n```shell\n# For low-VRAM users, set captioner to ram or none\npython run_gradio.py --captioner llava\n```\n\n<div align=\"center\">\n    <kbd><img src=\"assets/gradio.png\"></img></kbd>\n</div>\n\n## <a name=\"pretrained_models\"></a>:dna:Pretrained Models\n\nHere we list pretrained weight of stage 2 model (IRControlNet) and our trained SwinIR, which was used for degradation removal during the training of stage 2 model.\n\n| Model Name | Description | HuggingFace | BaiduNetdisk | OpenXLab |\n| :---------: | :----------: | :----------: | :----------: | :----------: |\n| v2.1.pt | IRControlNet trained on filtered unsplash | [download](https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/DiffBIR_v2.1.pt) | N/A | N/A |\n| v2.pth | IRControlNet trained on filtered laion2b-en  | [download](https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/v2.pth) | [download](https://pan.baidu.com/s/1uTAFl13xgGAzrnznAApyng?pwd=xiu3)<br>(pwd: xiu3) | [download](https://openxlab.org.cn/models/detail/linxinqi/DiffBIR/tree/main) |\n| v1_general.pth | IRControlNet trained on ImageNet-1k | [download](https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/v1_general.pth) | [download](https://pan.baidu.com/s/1PhXHAQSTOUX4Gy3MOc2t2Q?pwd=79n9)<br>(pwd: 79n9) | [download](https://openxlab.org.cn/models/detail/linxinqi/DiffBIR/tree/main) |\n| v1_face.pth | IRControlNet trained on FFHQ | [download](https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/v1_face.pth) | [download](https://pan.baidu.com/s/1kvM_SB1VbXjbipLxdzlI3Q?pwd=n7dx)<br>(pwd: n7dx) | [download](https://openxlab.org.cn/models/detail/linxinqi/DiffBIR/tree/main) |\n| codeformer_swinir.ckpt | SwinIR trained on ImageNet-1k with CodeFormer degradation | [download](https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/codeformer_swinir.ckpt) | [download](https://pan.baidu.com/s/176fARg2ySYtDgX2vQOeRbA?pwd=vfif)<br>(pwd: vfif) | [download](https://openxlab.org.cn/models/detail/linxinqi/DiffBIR/tree/main) |\n| realesrgan_s4_swinir_100k.pth | SwinIR trained on ImageNet-1k with Real-ESRGAN degradation | [download](https://huggingface.co/lxq007/DiffBIR-v2/resolve/main/realesrgan_s4_swinir_100k.pth) | N/A | N/A |\n\nDuring inference, we use off-the-shelf models from other papers as the stage 1 model: [BSRNet](https://github.com/cszn/BSRGAN) for BSR, [SwinIR-Face](https://github.com/zsyOAOA/DifFace) used in DifFace for BFR, and [SCUNet-PSNR](https://github.com/cszn/SCUNet) for BID, while the trained IRControlNet remains **unchanged** for all tasks. Please check [code](diffbir/inference/pretrained_models.py) for more details. Thanks for their work!\n\n## <a name=\"inference\"></a>:crossed_swords:Inference\n\nWe provide some examples for inference, check [inference.py](inference.py) for more arguments. Pretrained weights will be **automatically downloaded**. For users with limited VRAM, please run the following scripts with [tiled sampling](#tiled-sampling).\n\n### Blind Image Super-Resolution\n\n```shell\n# DiffBIR v2 (ECCV paper version)\npython -u inference.py \\\n--task sr \\\n--upscale 4 \\\n--version v2 \\\n--sampler spaced \\\n--steps 50 \\\n--captioner none \\\n--pos_prompt '' \\\n--neg_prompt 'low quality, blurry, low-resolution, noisy, unsharp, weird textures' \\\n--cfg_scale 4 \\\n--input inputs/demo/bsr \\\n--output results/v2_demo_bsr \\\n--device cuda --precision fp32\n\n# DiffBIR v2.1\npython -u inference.py \\\n--task sr \\\n--upscale 4 \\\n--version v2.1 \\\n--captioner llava \\\n--cfg_scale 8 \\\n--noise_aug 0 \\\n--input inputs/demo/bsr \\\n--output results/v2.1_demo_bsr\n```\n\n### Blind Aligned-Face Restoration\n<a name=\"inference_fr\"></a>\n\n```shell\n# DiffBIR v2 (ECCV paper version)\npython -u inference.py \\\n--task face \\\n--upscale 1 \\\n--version v2 \\\n--sampler spaced \\\n--steps 50 \\\n--captioner none \\\n--pos_prompt '' \\\n--neg_prompt 'low quality, blurry, low-resolution, noisy, unsharp, weird textures' \\\n--cfg_scale 4.0 \\\n--input inputs/demo/bfr/aligned \\\n--output results/v2_demo_bfr_aligned \\\n--device cuda --precision fp32\n\n# DiffBIR v2.1\npython -u inference.py \\\n--task face \\\n--upscale 1 \\\n--version v2.1 \\\n--captioner llava \\\n--cfg_scale 8 \\\n--noise_aug 0 \\\n--input inputs/demo/bfr/aligned \\\n--output results/v2.1_demo_bfr_aligned\n```\n\n### Blind Unaligned-Face Restoration\n\n```shell\n# DiffBIR v2 (ECCV paper version)\npython -u inference.py \\\n--task face_background \\\n--upscale 2 \\\n--version v2 \\\n--sampler spaced \\\n--steps 50 \\\n--captioner none \\\n--pos_prompt '' \\\n--neg_prompt 'low quality, blurry, low-resolution, noisy, unsharp, weird textures' \\\n--cfg_scale 4.0 \\\n--input inputs/demo/bfr/whole_img \\\n--output results/v2_demo_bfr_unaligned \\\n--device cuda --precision fp32\n\n# DiffBIR v2.1\npython -u inference.py \\\n--task face_background \\\n--upscale 2 \\\n--version v2.1 \\\n--captioner llava \\\n--cfg_scale 8 \\\n--noise_aug 0 \\\n--input inputs/demo/bfr/whole_img \\\n--output results/v2.1_demo_bfr_unaligned\n```\n\n### Blind Image Denoising\n\n```shell\n# DiffBIR v2 (ECCV paper version)\npython -u inference.py \\\n--task denoise \\\n--upscale 1 \\\n--version v2 \\\n--sampler spaced \\\n--steps 50 \\\n--captioner none \\\n--pos_prompt '' \\\n--neg_prompt 'low quality, blurry, low-resolution, noisy, unsharp, weird textures' \\\n--cfg_scale 4.0 \\\n--input inputs/demo/bid \\\n--output results/v2_demo_bid \\\n--device cuda --precision fp32\n\n# DiffBIR v2.1\npython -u inference.py \\\n--task denoise \\\n--upscale 1 \\\n--version v2.1 \\\n--captioner llava \\\n--cfg_scale 8 \\\n--noise_aug 0 \\\n--input inputs/demo/bid \\\n--output results/v2.1_demo_bid\n```\n\n### Custom-model Inference\n\n```shell\npython -u inference.py \\\n--upscale 4 \\\n--version custom \\\n--train_cfg [path/to/training/config] \\\n--ckpt [path/to/saved/checkpoint] \\\n--captioner llava \\\n--cfg_scale 8 \\\n--noise_aug 0 \\\n--input inputs/demo/bsr \\\n--output results/custom_demo_bsr\n```\n\n### Other options\n\n#### Tiled sampling\n<a name=\"patch_based_sampling\"></a>\n\nAdd the following arguments to enable tiled sampling:\n\n```shell\n[command...] \\\n# tiled inference for stage-1 model\n--cleaner_tiled \\\n--cleaner_tile_size 256 \\\n--cleaner_tile_stride 128 \\\n# tiled inference for VAE encoding\n--vae_encoder_tiled \\\n--vae_encoder_tile_size 256 \\\n# tiled inference for VAE decoding\n--vae_decoder_tiled \\\n--vae_decoder_tile_size 256 \\\n# tiled inference for diffusion process\n--cldm_tiled \\\n--cldm_tile_size 512 \\\n--cldm_tile_stride 256\n```\n\nTiled sampling supports super-resolution with a large scale factor on low-VRAM graphics cards. Our tiled sampling is built upon [mixture-of-diffusers](https://github.com/albarji/mixture-of-diffusers) and [Tiled-VAE](https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111). Thanks for their work!\n<!-- \n#### Restoration Guidance\n\nRestoration guidance is used to achieve a trade-off bwtween quality and fidelity. We default to closing it since we prefer quality rather than fidelity. Here is an example:\n\n```shell\npython -u inference.py \\\n--version v2 \\\n--task sr \\\n--upscale 4 \\\n--cfg_scale 4.0 \\\n--input inputs/demo/bsr \\\n--guidance --g_loss w_mse --g_scale 0.5 --g_space rgb \\\n--output results/demo_bsr_wg \\\n--device cuda\n```\n\nYou will see that the results become more smooth. -->\n\n#### Condition as Start Point of Sampling\n\n**This option only works with DiffBIR v1 and v2.** As proposed in [SeeSR](https://arxiv.org/abs/2311.16518), the LR embedding (LRE) strategy provides a more faithful\nstart point for sampling and consequently suppresses the artifacts in flat region:\n\n```shell\n[command...] --start_point_type cond\n```\n\nFor our model, we use the diffused condition as start point. This option makes the results more stable and ensures that the outcomes from ODE samplers like DDIM and DPMS are normal. However, it may lead to a decrease in sample quality.\n\n## <a name=\"train\"></a>:stars:Train\n\n### Stage 1\n\nFirst, we train a SwinIR, which will be used for degradation removal during the training of stage 2.\n\n<a name=\"gen_file_list\"></a>\n1. Generate file list of training set and validation set, a file list looks like:\n\n    ```txt\n    /path/to/image_1\n    /path/to/image_2\n    /path/to/image_3\n    ...\n    ```\n\n    You can write a simple python script or directly use shell command to produce file lists. Here is an example:\n    \n    ```shell\n    # collect all iamge files in img_dir\n    find [img_dir] -type f > files.list\n    # shuffle collected files\n    shuf files.list > files_shuf.list\n    # pick train_size files in the front as training set\n    head -n [train_size] files_shuf.list > files_shuf_train.list\n    # pick remaining files as validation set\n    tail -n +[train_size + 1] files_shuf.list > files_shuf_val.list\n    ```\n\n2. Fill in the [training configuration file](configs/train/train_stage1.yaml) with appropriate values.\n\n3. Start training!\n\n    ```shell\n    accelerate launch train_stage1.py --config configs/train/train_stage1.yaml\n    ```\n\n### Stage 2\n\n1. Download pretrained [Stable Diffusion v2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1-base) to provide generative capabilities. :bulb:: If you have ran the [inference script](inference.py), the SD v2.1 checkpoint can be found in [weights](weights).\n\n    ```shell\n    wget https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt --no-check-certificate\n    ```\n\n2. Generate file list as mentioned [above](#gen_file_list). Currently, the training script of stage 2 doesn't support validation set, so you only need to create training file list.\n\n3. Fill in the [training configuration file](configs/train/train_stage2.yaml) with appropriate values.\n\n4. Start training!\n\n    ```shell\n    accelerate launch train_stage2.py --config configs/train/train_stage2.yaml\n    ```\n\n## Citation\n\nPlease cite us if our work is useful for your research.\n\n```\n@misc{lin2024diffbir,\n      title={DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior}, \n      author={Xinqi Lin and Jingwen He and Ziyan Chen and Zhaoyang Lyu and Bo Dai and Fanghua Yu and Wanli Ouyang and Yu Qiao and Chao Dong},\n      year={2024},\n      eprint={2308.15070},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## License\n\nThis project is released under the [Apache 2.0 license](LICENSE).\n\n## Acknowledgement\n\nThis project is based on [ControlNet](https://github.com/lllyasviel/ControlNet) and [BasicSR](https://github.com/XPixelGroup/BasicSR). Thanks for their awesome work.\n\n## Contact\n\nIf you have any questions, please feel free to contact with me at linxinqi23@mails.ucas.ac.cn.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "diffbir",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 9.4736328125,
          "content": "from argparse import ArgumentParser, Namespace\n\nimport torch\n\nfrom accelerate.utils import set_seed\nfrom diffbir.inference import (\n    BSRInferenceLoop,\n    BFRInferenceLoop,\n    BIDInferenceLoop,\n    UnAlignedBFRInferenceLoop,\n    CustomInferenceLoop,\n)\n\n\ndef check_device(device: str) -> str:\n    if device == \"cuda\":\n        if not torch.cuda.is_available():\n            print(\n                \"CUDA not available because the current PyTorch install was not \"\n                \"built with CUDA enabled.\"\n            )\n            device = \"cpu\"\n    else:\n        if device == \"mps\":\n            if not torch.backends.mps.is_available():\n                if not torch.backends.mps.is_built():\n                    print(\n                        \"MPS not available because the current PyTorch install was not \"\n                        \"built with MPS enabled.\"\n                    )\n                    device = \"cpu\"\n                else:\n                    print(\n                        \"MPS not available because the current MacOS version is not 12.3+ \"\n                        \"and/or you do not have an MPS-enabled device on this machine.\"\n                    )\n                    device = \"cpu\"\n    print(f\"using device {device}\")\n    return device\n\n\nDEFAULT_POS_PROMPT = (\n    \"Cinematic, High Contrast, highly detailed, taken using a Canon EOS R camera, \"\n    \"hyper detailed photo - realistic maximum detail, 32k, Color Grading, ultra HD, extreme meticulous detailing, \"\n    \"skin pore detailing, hyper sharpness, perfect without deformations.\"\n)\n\nDEFAULT_NEG_PROMPT = (\n    \"painting, oil painting, illustration, drawing, art, sketch, oil painting, cartoon, \"\n    \"CG Style, 3D render, unreal engine, blurring, dirty, messy, worst quality, low quality, frames, watermark, \"\n    \"signature, jpeg artifacts, deformed, lowres, over-smooth.\"\n)\n\n\ndef parse_args() -> Namespace:\n    parser = ArgumentParser()\n    # model parameters\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        default=\"sr\",\n        choices=[\"sr\", \"face\", \"denoise\", \"unaligned_face\"],\n        help=\"Task you want to do. Ignore this option if you are using self-trained model.\",\n    )\n    parser.add_argument(\n        \"--upscale\", type=float, default=4, help=\"Upscale factor of output.\"\n    )\n    parser.add_argument(\n        \"--version\",\n        type=str,\n        default=\"v2.1\",\n        choices=[\"v1\", \"v2\", \"v2.1\", \"custom\"],\n        help=\"DiffBIR model version.\",\n    )\n    parser.add_argument(\n        \"--train_cfg\",\n        type=str,\n        default=\"\",\n        help=\"Path to training config. Only works when version is custom.\",\n    )\n    parser.add_argument(\n        \"--ckpt\",\n        type=str,\n        default=\"\",\n        help=\"Path to saved checkpoint. Only works when version is custom.\",\n    )\n    # sampling parameters\n    parser.add_argument(\n        \"--sampler\",\n        type=str,\n        default=\"edm_dpm++_3m_sde\",\n        choices=[\n            \"dpm++_m2\",\n            \"spaced\",\n            \"ddim\",\n            \"edm_euler\",\n            \"edm_euler_a\",\n            \"edm_heun\",\n            \"edm_dpm_2\",\n            \"edm_dpm_2_a\",\n            \"edm_lms\",\n            \"edm_dpm++_2s_a\",\n            \"edm_dpm++_sde\",\n            \"edm_dpm++_2m\",\n            \"edm_dpm++_2m_sde\",\n            \"edm_dpm++_3m_sde\",\n        ],\n        help=\"Sampler type. Different samplers may produce very different samples.\",\n    )\n    parser.add_argument(\n        \"--steps\",\n        type=int,\n        default=10,\n        help=\"Sampling steps. More steps, more details.\",\n    )\n    parser.add_argument(\n        \"--start_point_type\",\n        type=str,\n        choices=[\"noise\", \"cond\"],\n        default=\"noise\",\n        help=(\n            \"For DiffBIR v1 and v2, setting the start point types to 'cond' can make the results much more stable \"\n            \"and ensure that the outcomes from ODE samplers like DDIM and DPMS are normal. \"\n            \"However, this adjustment may lead to a decrease in sample quality.\"\n        ),\n    )\n    parser.add_argument(\n        \"--cleaner_tiled\",\n        action=\"store_true\",\n        help=\"Enable tiled inference for stage-1 model, which reduces the GPU memory usage.\",\n    )\n    parser.add_argument(\n        \"--cleaner_tile_size\", type=int, default=512, help=\"Size of each tile.\"\n    )\n    parser.add_argument(\n        \"--cleaner_tile_stride\", type=int, default=256, help=\"Stride between tiles.\"\n    )\n    parser.add_argument(\n        \"--vae_encoder_tiled\",\n        action=\"store_true\",\n        help=\"Enable tiled inference for AE encoder, which reduces the GPU memory usage.\",\n    )\n    parser.add_argument(\n        \"--vae_encoder_tile_size\", type=int, default=256, help=\"Size of each tile.\"\n    )\n    parser.add_argument(\n        \"--vae_decoder_tiled\",\n        action=\"store_true\",\n        help=\"Enable tiled inference for AE decoder, which reduces the GPU memory usage.\",\n    )\n    parser.add_argument(\n        \"--vae_decoder_tile_size\", type=int, default=256, help=\"Size of each tile.\"\n    )\n    parser.add_argument(\n        \"--cldm_tiled\",\n        action=\"store_true\",\n        help=\"Enable tiled sampling, which reduces the GPU memory usage.\",\n    )\n    parser.add_argument(\n        \"--cldm_tile_size\", type=int, default=512, help=\"Size of each tile.\"\n    )\n    parser.add_argument(\n        \"--cldm_tile_stride\", type=int, default=256, help=\"Stride between tiles.\"\n    )\n    parser.add_argument(\n        \"--captioner\",\n        type=str,\n        choices=[\"none\", \"llava\", \"ram\"],\n        default=\"llava\",\n        help=\"Select a model to describe the content of your input image.\",\n    )\n    parser.add_argument(\n        \"--pos_prompt\",\n        type=str,\n        default=DEFAULT_POS_PROMPT,\n        help=(\n            \"Descriptive words for 'good image quality'. \"\n            \"It can also describe the things you WANT to appear in the image.\"\n        ),\n    )\n    parser.add_argument(\n        \"--neg_prompt\",\n        type=str,\n        default=DEFAULT_NEG_PROMPT,\n        help=(\n            \"Descriptive words for 'bad image quality'. \"\n            \"It can also describe the things you DON'T WANT to appear in the image.\"\n        ),\n    )\n    parser.add_argument(\n        \"--cfg_scale\", type=float, default=6.0, help=\"Classifier-free guidance scale.\"\n    )\n    parser.add_argument(\n        \"--rescale_cfg\",\n        action=\"store_true\",\n        help=\"Gradually increase cfg scale from 1 to ('cfg_scale' + 1)\",\n    )\n    parser.add_argument(\n        \"--noise_aug\",\n        type=int,\n        default=0,\n        help=\"Level of noise augmentation. More noise, more creative.\",\n    )\n    parser.add_argument(\n        \"--s_churn\",\n        type=float,\n        default=0,\n        help=\"Randomness in sampling. Only works with some edm samplers.\",\n    )\n    parser.add_argument(\n        \"--s_tmin\",\n        type=float,\n        default=0,\n        help=\"Minimum sigma for adding ramdomness to sampling. Only works with some edm samplers.\",\n    )\n    parser.add_argument(\n        \"--s_tmax\",\n        type=float,\n        default=300,\n        help=\"Maximum  sigma for adding ramdomness to sampling. Only works with some edm samplers.\",\n    )\n    parser.add_argument(\n        \"--s_noise\",\n        type=float,\n        default=1,\n        help=\"Randomness in sampling. Only works with some edm samplers.\",\n    )\n    parser.add_argument(\n        \"--eta\",\n        type=float,\n        default=1,\n        help=\"I don't understand this parameter. Leave it as default.\",\n    )\n    parser.add_argument(\n        \"--order\",\n        type=int,\n        default=1,\n        help=\"Order of solver. Only works with edm_lms sampler.\",\n    )\n    parser.add_argument(\n        \"--strength\",\n        type=float,\n        default=1,\n        help=\"Control strength from ControlNet. Less strength, more creative.\",\n    )\n    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"Nothing to say.\")\n    # guidance parameters\n    parser.add_argument(\n        \"--guidance\", action=\"store_true\", help=\"Enable restoration guidance.\"\n    )\n    parser.add_argument(\n        \"--g_loss\",\n        type=str,\n        default=\"w_mse\",\n        choices=[\"mse\", \"w_mse\"],\n        help=\"Loss function of restoration guidance.\",\n    )\n    parser.add_argument(\n        \"--g_scale\",\n        type=float,\n        default=0.0,\n        help=\"Learning rate of optimizing the guidance loss function.\",\n    )\n    # common parameters\n    parser.add_argument(\n        \"--input\",\n        type=str,\n        required=True,\n        help=\"Path to folder that contains your low-quality images.\",\n    )\n    parser.add_argument(\n        \"--n_samples\", type=int, default=1, help=\"Number of samples for each image.\"\n    )\n    parser.add_argument(\n        \"--output\", type=str, required=True, help=\"Path to save restored results.\"\n    )\n    parser.add_argument(\"--seed\", type=int, default=231)\n    # mps has not been tested\n    parser.add_argument(\n        \"--device\", type=str, default=\"cuda\", choices=[\"cpu\", \"cuda\", \"mps\"]\n    )\n    parser.add_argument(\n        \"--precision\", type=str, default=\"fp16\", choices=[\"fp32\", \"fp16\", \"bf16\"]\n    )\n    parser.add_argument(\"--llava_bit\", type=str, default=\"4\", choices=[\"16\", \"8\", \"4\"])\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    args.device = check_device(args.device)\n    set_seed(args.seed)\n\n    if args.version != \"custom\":\n        loops = {\n            \"sr\": BSRInferenceLoop,\n            \"denoise\": BIDInferenceLoop,\n            \"face\": BFRInferenceLoop,\n            \"unaligned_face\": UnAlignedBFRInferenceLoop,\n        }\n        loops[args.task](args).run()\n    else:\n        CustomInferenceLoop(args).run()\n    print(\"done!\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "inputs",
          "type": "tree",
          "content": null
        },
        {
          "name": "llava",
          "type": "tree",
          "content": null
        },
        {
          "name": "ram",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.681640625,
          "content": "--extra-index-url https://download.pytorch.org/whl/cu118\ntorch==2.2.2+cu118\ntorchvision==0.17.2+cu118\ntorchaudio==2.2.2+cu118\nxformers==0.0.25.post1+cu118\nomegaconf==2.3.0\naccelerate==0.28.0\neinops==0.7.0\nopencv_python==4.9.0.80\nscipy==1.12.0\nftfy==6.2.0\nregex==2023.12.25\npython-dateutil==2.9.0.post0\ntimm==0.9.16\npytorch-lightning==2.2.1 # only for loading pretrained sd weight\ntensorboard==2.16.2 # for tensorboard event visualization\nprotobuf==4.25.3 # for tensorboard\nlpips==0.1.4\nfacexlib==0.3.0\ngradio==4.43.0\npolars==1.12.0\ntorchsde==0.2.6\nbitsandbytes==0.44.1\n\n# requirements for llava\ntransformers==4.37.2\ntokenizers==0.15.1\nsentencepiece==0.1.99\n\n# requirements for ram\nfairscale==0.4.4\n"
        },
        {
          "name": "run_gradio.py",
          "type": "blob",
          "size": 13.10546875,
          "content": "from typing import List\nimport math\nfrom argparse import ArgumentParser\nimport random\n\nimport numpy as np\nimport torch\nimport gradio as gr\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom tqdm import tqdm\nfrom accelerate.utils import set_seed\n\nfrom diffbir.model.cldm import ControlLDM\nfrom diffbir.model.swinir import SwinIR\nfrom diffbir.inference.pretrained_models import MODELS\nfrom diffbir.utils.common import instantiate_from_config, load_model_from_url\nfrom diffbir.model.gaussian_diffusion import Diffusion\nfrom diffbir.pipeline import SwinIRPipeline\nfrom diffbir.utils.caption import (\n    EmptyCaptioner,\n    LLaVACaptioner,\n    RAMCaptioner,\n    LLAVA_AVAILABLE,\n    RAM_AVAILABLE,\n)\n\ntorch.set_grad_enabled(False)\n\n# This gradio script only support DiffBIR v2.1\nparser = ArgumentParser()\nparser.add_argument(\"--captioner\", type=str, choices=[\"none\", \"ram\", \"llava\"], required=True)\nparser.add_argument(\"--llava_bit\", type=str, choices=[\"4\", \"8\", \"16\"], default=\"4\")\nargs = parser.parse_args()\n\n# Set max height and width to constraint inference time for online demo\nmax_height = 2048\nmax_width = 2048\n\ntasks = [\"sr\", \"face\"]\ndevice = \"cuda\"\nprecision = \"fp16\"\nllava_bit = args.llava_bit\n# Set captioner to llava or ram to enable auto-caption\ncaptioner = args.captioner\n\nif captioner == \"llava\":\n    assert LLAVA_AVAILABLE\nelif captioner == \"ram\":\n    assert RAM_AVAILABLE\n\n# 1. load stage-1 models\nswinir: SwinIR = instantiate_from_config(\n    OmegaConf.load(\"configs/inference/swinir.yaml\")\n)\nswinir.load_state_dict(load_model_from_url(MODELS[\"swinir_realesrgan\"]))\nswinir.eval().to(device)\n\nface_swinir: SwinIR = instantiate_from_config(\n    OmegaConf.load(\"configs/inference/swinir.yaml\")\n)\nface_swinir.load_state_dict(load_model_from_url(MODELS[\"swinir_face\"]))\nface_swinir.eval().to(device)\n\n# 2. load stage-2 model\ncldm: ControlLDM = instantiate_from_config(\n    OmegaConf.load(\"configs/inference/cldm.yaml\")\n)\n# 2.1 load pre-trained SD\nsd_weight = load_model_from_url(MODELS[\"sd_v2.1_zsnr\"])\nunused, missing = cldm.load_pretrained_sd(sd_weight)\nprint(\n    f\"load pretrained stable diffusion, \"\n    f\"unused weights: {unused}, missing weights: {missing}\"\n)\n# 2.2 load ControlNet\ncontrol_weight = load_model_from_url(MODELS[\"v2.1\"])\ncldm.load_controlnet_from_ckpt(control_weight)\nprint(\"load controlnet weight\")\ncldm.eval().to(device)\ncast_type = {\n    \"fp32\": torch.float32,\n    \"fp16\": torch.float16,\n    \"bf16\": torch.bfloat16,\n}[precision]\ncldm.cast_dtype(cast_type)\n\n# 3. load noise schedule\ndiffusion: Diffusion = instantiate_from_config(\n    OmegaConf.load(\"configs/inference/diffusion_v2.1.yaml\")\n)\ndiffusion.to(device)\n\n# 4. load captioner\nif captioner == \"none\":\n    captioner = EmptyCaptioner(device)\nelif captioner == \"llava\":\n    captioner = LLaVACaptioner(device, llava_bit)\nelse:\n    captioner = RAMCaptioner(device)\n\nerror_image = np.array(Image.open(\"assets/gradio_error_img.png\"))\n\n\n@torch.no_grad()\ndef process(\n    input_image,\n    task,\n    upscale,\n    cleaner_tiled,\n    cleaner_tile_size,\n    vae_encoder_tiled,\n    vae_encoder_tile_size,\n    vae_decoder_tiled,\n    vae_decoder_tile_size,\n    cldm_tiled,\n    cldm_tile_size,\n    positive_prompt,\n    negative_prompt,\n    cfg_scale,\n    rescale_cfg,\n    strength,\n    noise_aug,\n    steps,\n    sampler_type,\n    s_churn,\n    s_tmin,\n    s_tmax,\n    s_noise,\n    order,\n    seed,\n    progress=gr.Progress(track_tqdm=True),\n) -> List[np.ndarray]:\n    if seed == -1:\n        seed = random.randint(0, 2147483647)\n    set_seed(seed)\n    lq = input_image\n    # Prepare prompt\n    caption = captioner(lq)\n    pos_prompt = \", \".join([text for text in [caption, positive_prompt] if text])\n    neg_prompt = negative_prompt\n    # Upscale and convert to numpy array\n    out_w, out_h = tuple(int(x * upscale) for x in lq.size)\n    if out_w > max_width or out_h > max_height:\n        return [error_image], (\n            \"Failed :( The requested resolution exceeds the maximum limit. \"\n            f\"Your requested resolution is ({out_h}, {out_w}). \"\n            f\"The maximum allowed resolution is ({max_height}, {max_width}).\"\n        )\n    lq = lq.resize((out_w, out_h), Image.BICUBIC)\n    lq = np.array(lq)\n    # Select cleaner\n    if task == \"sr\":\n        cleaner = swinir\n    else:\n        cleaner = face_swinir\n    # Create pipeline\n    pipeline = SwinIRPipeline(cleaner, cldm, diffusion, None, device)\n    # Run pipeline to restore this image\n    try:\n        sample = pipeline.run(\n            lq[None],\n            steps,\n            strength,\n            cleaner_tiled,\n            cleaner_tile_size,\n            cleaner_tile_size // 2,\n            vae_encoder_tiled,\n            vae_encoder_tile_size,\n            vae_decoder_tiled,\n            vae_decoder_tile_size,\n            cldm_tiled,\n            cldm_tile_size,\n            cldm_tile_size // 2,\n            pos_prompt,\n            neg_prompt,\n            cfg_scale,\n            \"noise\",\n            sampler_type,\n            noise_aug,\n            rescale_cfg,\n            s_churn,\n            s_tmin,\n            s_tmax,\n            s_noise,\n            1,\n            order,\n        )[0]\n        return [sample], \"Success :)\"\n    except Exception as e:\n        return [error_image], f\"Failed :( {e}\"\n\n\n# TODO: add help information for each option\nMARKDOWN = \"\"\"\n## DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior\n\n[GitHub](https://github.com/XPixelGroup/DiffBIR) | [Paper](https://arxiv.org/abs/2308.15070) | [Project Page](https://0x3f3f3f3fun.github.io/projects/diffbir/)\n\nIf DiffBIR is helpful for you, please help star the GitHub Repo. Thanks!\n\"\"\"\n\nDEFAULT_POS_PROMPT = (\n    \"Cinematic, High Contrast, highly detailed, taken using a Canon EOS R camera, \"\n    \"hyper detailed photo - realistic maximum detail, 32k, Color Grading, ultra HD, extreme meticulous detailing, \"\n    \"skin pore detailing, hyper sharpness, perfect without deformations.\"\n)\n\nDEFAULT_NEG_PROMPT = (\n    \"painting, oil painting, illustration, drawing, art, sketch, oil painting, cartoon, \"\n    \"CG Style, 3D render, unreal engine, blurring, dirty, messy, worst quality, low quality, frames, watermark, \"\n    \"signature, jpeg artifacts, deformed, lowres, over-smooth.\"\n)\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(MARKDOWN)\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(sources=\"upload\", type=\"pil\")\n            run_button = gr.Button(value=\"Run\")\n            with gr.Accordion(\"Basic Options\", open=True):\n                with gr.Row():\n                    task = gr.Dropdown(\n                        label=\"Task\",\n                        choices=tasks,\n                        value=\"sr\",\n                    )\n                    upscale = gr.Slider(\n                        label=\"Upsample factor\",\n                        minimum=1,\n                        maximum=8,\n                        value=4,\n                        step=1,\n                    )\n                with gr.Row():\n                    with gr.Column():\n                        cleaner_tiled = gr.Checkbox(\n                            label=\"Tiled cleaner\",\n                            value=False,\n                        )\n                        cleaner_tile_size = gr.Slider(\n                            label=\"Cleaner tile size\",\n                            minimum=256,\n                            maximum=1024,\n                            value=256,\n                            step=64,\n                        )\n                    with gr.Column():\n                        vae_encoder_tiled = gr.Checkbox(\n                            label=\"Tiled VAE encoder\",\n                            value=False,\n                        )\n                        vae_encoder_tile_size = gr.Slider(\n                            label=\"VAE encoder tile size\",\n                            minimum=256,\n                            maximum=1024,\n                            value=256,\n                            step=64,\n                        )\n                with gr.Row():\n                    with gr.Column():\n                        vae_decoder_tiled = gr.Checkbox(\n                            label=\"Tiled VAE decoder\",\n                            value=False,\n                        )\n                        vae_decoder_tile_size = gr.Slider(\n                            label=\"VAE decoder tile size\",\n                            minimum=256,\n                            maximum=1024,\n                            value=256,\n                            step=64,\n                        )\n                    with gr.Column():\n                        cldm_tiled = gr.Checkbox(\n                            label=\"Tiled diffusion\",\n                            value=True,\n                        )\n                        cldm_tile_size = gr.Slider(\n                            label=\"Diffusion tile size\",\n                            minimum=512,\n                            maximum=1024,\n                            value=512,\n                            step=64,\n                        )\n                seed = gr.Slider(\n                    label=\"Seed\", minimum=-1, maximum=2147483647, step=1, value=231\n                )\n            with gr.Accordion(\"Condition Options\", open=True):\n                pos_prompt = gr.Textbox(\n                    label=\"Positive prompt\",\n                    value=DEFAULT_POS_PROMPT,\n                )\n                neg_prompt = gr.Textbox(\n                    label=\"Negative prompt\",\n                    value=DEFAULT_NEG_PROMPT,\n                )\n                cfg_scale = gr.Slider(\n                    label=\"Classifier-free guidance (cfg) scale\",\n                    minimum=1,\n                    maximum=10,\n                    value=8,\n                    step=1,\n                )\n                rescale_cfg = gr.Checkbox(value=False, label=\"Gradually increase cfg scale\")\n                with gr.Row():\n                    strength = gr.Slider(\n                        label=\"Control strength\",\n                        minimum=0.0,\n                        maximum=1.5,\n                        value=1.0,\n                        step=0.1,\n                    )\n                    noise_aug = gr.Slider(\n                        label=\"Noise level of condition\",\n                        minimum=0,\n                        maximum=199,\n                        value=0,\n                        step=10,\n                    )\n            with gr.Accordion(\"Sampler Options\", open=True):\n                steps = gr.Slider(\n                    label=\"Steps\", minimum=5, maximum=50, value=10, step=5\n                )\n                sampler_type = gr.Dropdown(\n                    label=\"Select a sampler\",\n                    choices=[\n                        \"dpm++_m2\",\n                        \"spaced\",\n                        \"ddim\",\n                        \"edm_euler\",\n                        \"edm_euler_a\",\n                        \"edm_heun\",\n                        \"edm_dpm_2\",\n                        \"edm_dpm_2_a\",\n                        \"edm_lms\",\n                        \"edm_dpm++_2s_a\",\n                        \"edm_dpm++_sde\",\n                        \"edm_dpm++_2m\",\n                        \"edm_dpm++_2m_sde\",\n                        \"edm_dpm++_3m_sde\",\n                    ],\n                    value=\"edm_dpm++_3m_sde\",\n                )\n                s_churn = gr.Slider(\n                    label=\"s_churn\",\n                    minimum=0,\n                    maximum=40,\n                    value=0,\n                    step=1,\n                )\n                s_tmin = gr.Slider(\n                    label=\"s_tmin\",\n                    minimum=0,\n                    maximum=300,\n                    value=0,\n                    step=10,\n                )\n                s_tmax = gr.Slider(\n                    label=\"s_tmax\",\n                    minimum=0,\n                    maximum=300,\n                    value=300,\n                    step=10,\n                )\n                s_noise = gr.Slider(\n                    label=\"s_noise\",\n                    minimum=1,\n                    maximum=1.1,\n                    value=1,\n                    step=0.001,\n                )\n                order = gr.Slider(\n                    label=\"order\",\n                    minimum=1,\n                    maximum=8,\n                    value=1,\n                    step=1,\n                )\n        with gr.Column():\n            result_gallery = gr.Gallery(\n                label=\"Output\", show_label=False, columns=2, format=\"png\"\n            )\n            status = gr.Textbox(label=\"Status\")\n    run_button.click(\n        fn=process,\n        inputs=[\n            input_image,\n            task,\n            upscale,\n            cleaner_tiled,\n            cleaner_tile_size,\n            vae_encoder_tiled,\n            vae_encoder_tile_size,\n            vae_decoder_tiled,\n            vae_decoder_tile_size,\n            cldm_tiled,\n            cldm_tile_size,\n            pos_prompt,\n            neg_prompt,\n            cfg_scale,\n            rescale_cfg,\n            strength,\n            noise_aug,\n            steps,\n            sampler_type,\n            s_churn,\n            s_tmin,\n            s_tmax,\n            s_noise,\n            order,\n            seed,\n        ],\n        outputs=[result_gallery, status],\n    )\n\nblock.launch()\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train_stage1.py",
          "type": "blob",
          "size": 8.9677734375,
          "content": "import os\nfrom argparse import ArgumentParser\nimport warnings\n\nfrom omegaconf import OmegaConf\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.utils import make_grid\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom einops import rearrange\nfrom tqdm import tqdm\nimport lpips\n\nfrom diffbir.model import SwinIR\nfrom diffbir.utils.common import instantiate_from_config, calculate_psnr_pt, to\n\n\ndef main(args) -> None:\n    # Setup accelerator:\n    accelerator = Accelerator(split_batches=True)\n    set_seed(231)\n    device = accelerator.device\n    cfg = OmegaConf.load(args.config)\n\n    # Setup an experiment folder:\n    if accelerator.is_local_main_process:\n        exp_dir = cfg.train.exp_dir\n        os.makedirs(exp_dir, exist_ok=True)\n        ckpt_dir = os.path.join(exp_dir, \"checkpoints\")\n        os.makedirs(ckpt_dir, exist_ok=True)\n        print(f\"Experiment directory created at {exp_dir}\")\n\n    # Create model:\n    swinir: SwinIR = instantiate_from_config(cfg.model.swinir)\n    if cfg.train.resume:\n        swinir.load_state_dict(\n            torch.load(cfg.train.resume, map_location=\"cpu\"), strict=True\n        )\n        if accelerator.is_local_main_process:\n            print(f\"strictly load weight from checkpoint: {cfg.train.resume}\")\n    else:\n        if accelerator.is_local_main_process:\n            print(\"initialize from scratch\")\n\n    # Setup optimizer:\n    opt = torch.optim.AdamW(\n        swinir.parameters(), lr=cfg.train.learning_rate, weight_decay=0\n    )\n\n    # Setup data:\n    dataset = instantiate_from_config(cfg.dataset.train)\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=cfg.train.batch_size,\n        num_workers=cfg.train.num_workers,\n        shuffle=True,\n        drop_last=True,\n    )\n    val_dataset = instantiate_from_config(cfg.dataset.val)\n    val_loader = DataLoader(\n        dataset=val_dataset,\n        batch_size=cfg.train.batch_size,\n        num_workers=cfg.train.num_workers,\n        shuffle=False,\n        drop_last=False,\n    )\n    if accelerator.is_local_main_process:\n        print(f\"Dataset contains {len(dataset):,} images from {dataset.file_list}\")\n\n    batch_transform = instantiate_from_config(cfg.batch_transform)\n\n    # Prepare models for training:\n    swinir.train().to(device)\n    swinir, opt, loader, val_loader = accelerator.prepare(\n        swinir, opt, loader, val_loader\n    )\n    pure_swinir = accelerator.unwrap_model(swinir)\n\n    # Variables for monitoring/logging purposes:\n    global_step = 0\n    max_steps = cfg.train.train_steps\n    step_loss = []\n    epoch = 0\n    epoch_loss = []\n    with warnings.catch_warnings():\n        # avoid warnings from lpips internal\n        warnings.simplefilter(\"ignore\")\n        lpips_model = (\n            lpips.LPIPS(net=\"alex\", verbose=accelerator.is_local_main_process)\n            .eval()\n            .to(device)\n        )\n    if accelerator.is_local_main_process:\n        writer = SummaryWriter(exp_dir)\n        print(f\"Training for {max_steps} steps...\")\n\n    while global_step < max_steps:\n        pbar = tqdm(\n            iterable=None,\n            disable=not accelerator.is_local_main_process,\n            unit=\"batch\",\n            total=len(loader),\n        )\n        for batch in loader:\n            to(batch, device)\n            batch = batch_transform(batch)\n            gt, lq, _ = batch\n            gt = rearrange((gt + 1) / 2, \"b h w c -> b c h w\").contiguous().float()\n            lq = rearrange(lq, \"b h w c -> b c h w\").contiguous().float()\n            pred = swinir(lq)\n            loss = F.mse_loss(pred, gt, reduction=\"sum\")\n\n            opt.zero_grad()\n            accelerator.backward(loss)\n            opt.step()\n            accelerator.wait_for_everyone()\n\n            global_step += 1\n            step_loss.append(loss.item())\n            epoch_loss.append(loss.item())\n            pbar.update(1)\n            pbar.set_description(\n                f\"Epoch: {epoch:04d}, Global Step: {global_step:07d}, Loss: {loss.item():.6f}\"\n            )\n\n            # Log loss values:\n            if global_step % cfg.train.log_every == 0:\n                # Gather values from all processes\n                avg_loss = (\n                    accelerator.gather(\n                        torch.tensor(step_loss, device=device).unsqueeze(0)\n                    )\n                    .mean()\n                    .item()\n                )\n                step_loss.clear()\n                if accelerator.is_local_main_process:\n                    writer.add_scalar(\"train/loss_step\", avg_loss, global_step)\n\n            # Save checkpoint:\n            if global_step % cfg.train.ckpt_every == 0:\n                if accelerator.is_local_main_process:\n                    checkpoint = pure_swinir.state_dict()\n                    ckpt_path = f\"{ckpt_dir}/{global_step:07d}.pt\"\n                    torch.save(checkpoint, ckpt_path)\n\n            if global_step % cfg.train.image_every == 0 or global_step == 1:\n                swinir.eval()\n                N = 12\n                log_gt, log_lq = gt[:N], lq[:N]\n                with torch.no_grad():\n                    log_pred = swinir(log_lq)\n                if accelerator.is_local_main_process:\n                    for tag, image in [\n                        (\"image/pred\", log_pred),\n                        (\"image/gt\", log_gt),\n                        (\"image/lq\", log_lq),\n                    ]:\n                        writer.add_image(tag, make_grid(image, nrow=4), global_step)\n                swinir.train()\n\n            # Evaluate model:\n            if global_step % cfg.train.val_every == 0:\n                swinir.eval()\n                val_loss = []\n                val_lpips = []\n                val_psnr = []\n                val_pbar = tqdm(\n                    iterable=None,\n                    disable=not accelerator.is_local_main_process,\n                    unit=\"batch\",\n                    total=len(val_loader),\n                    leave=False,\n                    desc=\"Validation\",\n                )\n                for val_batch in val_loader:\n                    to(val_batch, device)\n                    val_batch = batch_transform(val_batch)\n                    val_gt, val_lq, _ = val_batch\n                    val_gt = (\n                        rearrange((val_gt + 1) / 2, \"b h w c -> b c h w\")\n                        .contiguous()\n                        .float()\n                    )\n                    val_lq = (\n                        rearrange(val_lq, \"b h w c -> b c h w\").contiguous().float()\n                    )\n                    with torch.no_grad():\n                        val_pred = swinir(val_lq)\n                        val_loss.append(\n                            F.mse_loss(val_pred, val_gt, reduction=\"sum\").item()\n                        )\n                        val_lpips.append(\n                            lpips_model(val_pred, val_gt, normalize=True).mean().item()\n                        )\n                        val_psnr.append(\n                            calculate_psnr_pt(val_pred, val_gt, crop_border=0)\n                            .mean()\n                            .item()\n                        )\n                    val_pbar.update(1)\n                val_pbar.close()\n                avg_val_loss = (\n                    accelerator.gather(\n                        torch.tensor(val_loss, device=device).unsqueeze(0)\n                    )\n                    .mean()\n                    .item()\n                )\n                avg_val_lpips = (\n                    accelerator.gather(\n                        torch.tensor(val_lpips, device=device).unsqueeze(0)\n                    )\n                    .mean()\n                    .item()\n                )\n                avg_val_psnr = (\n                    accelerator.gather(\n                        torch.tensor(val_psnr, device=device).unsqueeze(0)\n                    )\n                    .mean()\n                    .item()\n                )\n                if accelerator.is_local_main_process:\n                    for tag, val in [\n                        (\"val/loss\", avg_val_loss),\n                        (\"val/lpips\", avg_val_lpips),\n                        (\"val/psnr\", avg_val_psnr),\n                    ]:\n                        writer.add_scalar(tag, val, global_step)\n                swinir.train()\n\n            accelerator.wait_for_everyone()\n\n            if global_step == max_steps:\n                break\n\n        pbar.close()\n        epoch += 1\n        avg_epoch_loss = (\n            accelerator.gather(torch.tensor(epoch_loss, device=device).unsqueeze(0))\n            .mean()\n            .item()\n        )\n        epoch_loss.clear()\n        if accelerator.is_local_main_process:\n            writer.add_scalar(\"train/loss_epoch\", avg_epoch_loss, global_step)\n\n    if accelerator.is_local_main_process:\n        print(\"done!\")\n        writer.close()\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--config\", type=str, required=True)\n    args = parser.parse_args()\n    main(args)\n"
        },
        {
          "name": "train_stage2.py",
          "type": "blob",
          "size": 9.072265625,
          "content": "import os\nfrom argparse import ArgumentParser\nimport copy\n\nfrom omegaconf import OmegaConf\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom einops import rearrange\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom diffbir.model import ControlLDM, SwinIR, Diffusion\nfrom diffbir.utils.common import instantiate_from_config, to, log_txt_as_img\nfrom diffbir.sampler import SpacedSampler\n\n\ndef main(args) -> None:\n    # Setup accelerator:\n    accelerator = Accelerator(split_batches=True)\n    set_seed(231, device_specific=True)\n    device = accelerator.device\n    cfg = OmegaConf.load(args.config)\n\n    # Setup an experiment folder:\n    if accelerator.is_main_process:\n        exp_dir = cfg.train.exp_dir\n        os.makedirs(exp_dir, exist_ok=True)\n        ckpt_dir = os.path.join(exp_dir, \"checkpoints\")\n        os.makedirs(ckpt_dir, exist_ok=True)\n        print(f\"Experiment directory created at {exp_dir}\")\n\n    # Create model:\n    cldm: ControlLDM = instantiate_from_config(cfg.model.cldm)\n    sd = torch.load(cfg.train.sd_path, map_location=\"cpu\")[\"state_dict\"]\n    unused, missing = cldm.load_pretrained_sd(sd)\n    if accelerator.is_main_process:\n        print(\n            f\"strictly load pretrained SD weight from {cfg.train.sd_path}\\n\"\n            f\"unused weights: {unused}\\n\"\n            f\"missing weights: {missing}\"\n        )\n\n    if cfg.train.resume:\n        cldm.load_controlnet_from_ckpt(torch.load(cfg.train.resume, map_location=\"cpu\"))\n        if accelerator.is_main_process:\n            print(\n                f\"strictly load controlnet weight from checkpoint: {cfg.train.resume}\"\n            )\n    else:\n        init_with_new_zero, init_with_scratch = cldm.load_controlnet_from_unet()\n        if accelerator.is_main_process:\n            print(\n                f\"strictly load controlnet weight from pretrained SD\\n\"\n                f\"weights initialized with newly added zeros: {init_with_new_zero}\\n\"\n                f\"weights initialized from scratch: {init_with_scratch}\"\n            )\n\n    swinir: SwinIR = instantiate_from_config(cfg.model.swinir)\n    sd = torch.load(cfg.train.swinir_path, map_location=\"cpu\")\n    if \"state_dict\" in sd:\n        sd = sd[\"state_dict\"]\n    sd = {\n        (k[len(\"module.\") :] if k.startswith(\"module.\") else k): v\n        for k, v in sd.items()\n    }\n    swinir.load_state_dict(sd, strict=True)\n    for p in swinir.parameters():\n        p.requires_grad = False\n    if accelerator.is_main_process:\n        print(f\"load SwinIR from {cfg.train.swinir_path}\")\n\n    diffusion: Diffusion = instantiate_from_config(cfg.model.diffusion)\n\n    # Setup optimizer:\n    opt = torch.optim.AdamW(cldm.controlnet.parameters(), lr=cfg.train.learning_rate)\n\n    # Setup data:\n    dataset = instantiate_from_config(cfg.dataset.train)\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=cfg.train.batch_size,\n        num_workers=cfg.train.num_workers,\n        shuffle=True,\n        drop_last=True,\n        pin_memory=True,\n    )\n    if accelerator.is_main_process:\n        print(f\"Dataset contains {len(dataset):,} images\")\n\n    batch_transform = instantiate_from_config(cfg.batch_transform)\n\n    # Prepare models for training:\n    cldm.train().to(device)\n    swinir.eval().to(device)\n    diffusion.to(device)\n    cldm, opt, loader = accelerator.prepare(cldm, opt, loader)\n    pure_cldm: ControlLDM = accelerator.unwrap_model(cldm)\n    noise_aug_timestep = cfg.train.noise_aug_timestep\n\n    # Variables for monitoring/logging purposes:\n    global_step = 0\n    max_steps = cfg.train.train_steps\n    step_loss = []\n    epoch = 0\n    epoch_loss = []\n    sampler = SpacedSampler(\n        diffusion.betas, diffusion.parameterization, rescale_cfg=False\n    )\n    if accelerator.is_main_process:\n        writer = SummaryWriter(exp_dir)\n        print(f\"Training for {max_steps} steps...\")\n\n    while global_step < max_steps:\n        pbar = tqdm(\n            iterable=None,\n            disable=not accelerator.is_main_process,\n            unit=\"batch\",\n            total=len(loader),\n        )\n        for batch in loader:\n            to(batch, device)\n            batch = batch_transform(batch)\n            gt, lq, prompt = batch\n            gt = rearrange(gt, \"b h w c -> b c h w\").contiguous().float()\n            lq = rearrange(lq, \"b h w c -> b c h w\").contiguous().float()\n\n            with torch.no_grad():\n                z_0 = pure_cldm.vae_encode(gt)\n                clean = swinir(lq)\n                cond = pure_cldm.prepare_condition(clean, prompt)\n                # noise augmentation\n                cond_aug = copy.deepcopy(cond)\n                if noise_aug_timestep > 0:\n                    cond_aug[\"c_img\"] = diffusion.q_sample(\n                        x_start=cond_aug[\"c_img\"],\n                        t=torch.randint(\n                            0, noise_aug_timestep, (z_0.shape[0],), device=device\n                        ),\n                        noise=torch.randn_like(cond_aug[\"c_img\"]),\n                    )\n            t = torch.randint(\n                0, diffusion.num_timesteps, (z_0.shape[0],), device=device\n            )\n\n            loss = diffusion.p_losses(cldm, z_0, t, cond_aug)\n            opt.zero_grad()\n            accelerator.backward(loss)\n            opt.step()\n\n            accelerator.wait_for_everyone()\n\n            global_step += 1\n            step_loss.append(loss.item())\n            epoch_loss.append(loss.item())\n            pbar.update(1)\n            pbar.set_description(\n                f\"Epoch: {epoch:04d}, Global Step: {global_step:07d}, Loss: {loss.item():.6f}\"\n            )\n\n            # Log loss values:\n            if global_step % cfg.train.log_every == 0 and global_step > 0:\n                # Gather values from all processes\n                avg_loss = (\n                    accelerator.gather(\n                        torch.tensor(step_loss, device=device).unsqueeze(0)\n                    )\n                    .mean()\n                    .item()\n                )\n                step_loss.clear()\n                if accelerator.is_main_process:\n                    writer.add_scalar(\"loss/loss_simple_step\", avg_loss, global_step)\n\n            # Save checkpoint:\n            if global_step % cfg.train.ckpt_every == 0 and global_step > 0:\n                if accelerator.is_main_process:\n                    checkpoint = pure_cldm.controlnet.state_dict()\n                    ckpt_path = f\"{ckpt_dir}/{global_step:07d}.pt\"\n                    torch.save(checkpoint, ckpt_path)\n\n            if global_step % cfg.train.image_every == 0 or global_step == 1:\n                N = 8\n                log_clean = clean[:N]\n                log_cond = {k: v[:N] for k, v in cond.items()}\n                log_cond_aug = {k: v[:N] for k, v in cond_aug.items()}\n                log_gt, log_lq = gt[:N], lq[:N]\n                log_prompt = prompt[:N]\n                cldm.eval()\n                with torch.no_grad():\n                    z = sampler.sample(\n                        model=cldm,\n                        device=device,\n                        steps=50,\n                        x_size=(len(log_gt), *z_0.shape[1:]),\n                        cond=log_cond,\n                        uncond=None,\n                        cfg_scale=1.0,\n                        progress=accelerator.is_main_process,\n                    )\n                    if accelerator.is_main_process:\n                        for tag, image in [\n                            (\"image/samples\", (pure_cldm.vae_decode(z) + 1) / 2),\n                            (\"image/gt\", (log_gt + 1) / 2),\n                            (\"image/lq\", log_lq),\n                            (\"image/condition\", log_clean),\n                            (\n                                \"image/condition_decoded\",\n                                (pure_cldm.vae_decode(log_cond[\"c_img\"]) + 1) / 2,\n                            ),\n                            (\n                                \"image/condition_aug_decoded\",\n                                (pure_cldm.vae_decode(log_cond_aug[\"c_img\"]) + 1) / 2,\n                            ),\n                            (\n                                \"image/prompt\",\n                                (log_txt_as_img((512, 512), log_prompt) + 1) / 2,\n                            ),\n                        ]:\n                            writer.add_image(tag, make_grid(image, nrow=4), global_step)\n                cldm.train()\n            accelerator.wait_for_everyone()\n            if global_step == max_steps:\n                break\n\n        pbar.close()\n        epoch += 1\n        avg_epoch_loss = (\n            accelerator.gather(torch.tensor(epoch_loss, device=device).unsqueeze(0))\n            .mean()\n            .item()\n        )\n        epoch_loss.clear()\n        if accelerator.is_main_process:\n            writer.add_scalar(\"loss/loss_simple_epoch\", avg_epoch_loss, global_step)\n\n    if accelerator.is_main_process:\n        print(\"done!\")\n        writer.close()\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--config\", type=str, required=True)\n    args = parser.parse_args()\n    main(args)\n"
        }
      ]
    }
  ]
}