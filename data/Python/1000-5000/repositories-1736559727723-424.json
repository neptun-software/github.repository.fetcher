{
  "metadata": {
    "timestamp": 1736559727723,
    "page": 424,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dreamgaussian/dreamgaussian",
      "stars": 4026,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1328125,
          "content": "__pycache__/\nbuild/\n*.egg-info/\n*.so\nvenv_*/\n.vs/\n.vscode/\n.idea/\n\ntmp_*\ndata?\ndata??\nscripts2\n\nmodel_cache\n\nlogs*\nvideos*\nimages*\n*.mp4"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.044921875,
          "content": "MIT License\n\nCopyright (c) 2023 dreamgaussian\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "LICENSE_GAUSSIAN_SPLATTING.md",
          "type": "blob",
          "size": 4.1748046875,
          "content": "Gaussian-Splatting License  \n===========================  \n\n**Inria** and **the Max Planck Institut for Informatik (MPII)** hold all the ownership rights on the *Software* named **gaussian-splatting**.  \nThe *Software* is in the process of being registered with the Agence pour la Protection des  \nProgrammes (APP).  \n\nThe *Software* is still being developed by the *Licensor*.  \n\n*Licensor*'s goal is to allow the research community to use, test and evaluate  \nthe *Software*.  \n\n## 1.  Definitions  \n\n*Licensee* means any person or entity that uses the *Software* and distributes  \nits *Work*.  \n\n*Licensor* means the owners of the *Software*, i.e Inria and MPII  \n\n*Software* means the original work of authorship made available under this  \nLicense ie gaussian-splatting.  \n\n*Work* means the *Software* and any additions to or derivative works of the  \n*Software* that are made available under this License.  \n\n\n## 2.  Purpose  \nThis license is intended to define the rights granted to the *Licensee* by  \nLicensors under the *Software*.  \n\n## 3.  Rights granted  \n\nFor the above reasons Licensors have decided to distribute the *Software*.  \nLicensors grant non-exclusive rights to use the *Software* for research purposes  \nto research users (both academic and industrial), free of charge, without right  \nto sublicense.. The *Software* may be used \"non-commercially\", i.e., for research  \nand/or evaluation purposes only.  \n\nSubject to the terms and conditions of this License, you are granted a  \nnon-exclusive, royalty-free, license to reproduce, prepare derivative works of,  \npublicly display, publicly perform and distribute its *Work* and any resulting  \nderivative works in any form.  \n\n## 4.  Limitations  \n\n**4.1 Redistribution.** You may reproduce or distribute the *Work* only if (a) you do  \nso under this License, (b) you include a complete copy of this License with  \nyour distribution, and (c) you retain without modification any copyright,  \npatent, trademark, or attribution notices that are present in the *Work*.  \n\n**4.2 Derivative Works.** You may specify that additional or different terms apply  \nto the use, reproduction, and distribution of your derivative works of the *Work*  \n(\"Your Terms\") only if (a) Your Terms provide that the use limitation in  \nSection 2 applies to your derivative works, and (b) you identify the specific  \nderivative works that are subject to Your Terms. Notwithstanding Your Terms,  \nthis License (including the redistribution requirements in Section 3.1) will  \ncontinue to apply to the *Work* itself.  \n\n**4.3** Any other use without of prior consent of Licensors is prohibited. Research  \nusers explicitly acknowledge having received from Licensors all information  \nallowing to appreciate the adequacy between of the *Software* and their needs and  \nto undertake all necessary precautions for its execution and use.  \n\n**4.4** The *Software* is provided both as a compiled library file and as source  \ncode. In case of using the *Software* for a publication or other results obtained  \nthrough the use of the *Software*, users are strongly encouraged to cite the  \ncorresponding publications as explained in the documentation of the *Software*.  \n\n## 5.  Disclaimer  \n\nTHE USER CANNOT USE, EXPLOIT OR DISTRIBUTE THE *SOFTWARE* FOR COMMERCIAL PURPOSES  \nWITHOUT PRIOR AND EXPLICIT CONSENT OF LICENSORS. YOU MUST CONTACT INRIA FOR ANY  \nUNAUTHORIZED USE: stip-sophia.transfert@inria.fr . ANY SUCH ACTION WILL  \nCONSTITUTE A FORGERY. THIS *SOFTWARE* IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTIES  \nOF ANY NATURE AND ANY EXPRESS OR IMPLIED WARRANTIES, WITH REGARDS TO COMMERCIAL  \nUSE, PROFESSIONNAL USE, LEGAL OR NOT, OR OTHER, OR COMMERCIALISATION OR  \nADAPTATION. UNLESS EXPLICITLY PROVIDED BY LAW, IN NO EVENT, SHALL INRIA OR THE  \nAUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR  \nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE  \nGOODS OR SERVICES, LOSS OF USE, DATA, OR PROFITS OR BUSINESS INTERRUPTION)  \nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT  \nLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING FROM, OUT OF OR  \nIN CONNECTION WITH THE *SOFTWARE* OR THE USE OR OTHER DEALINGS IN THE *SOFTWARE*.  \n"
        },
        {
          "name": "cam_utils.py",
          "type": "blob",
          "size": 4.671875,
          "content": "import numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nimport torch\n\ndef dot(x, y):\n    if isinstance(x, np.ndarray):\n        return np.sum(x * y, -1, keepdims=True)\n    else:\n        return torch.sum(x * y, -1, keepdim=True)\n\n\ndef length(x, eps=1e-20):\n    if isinstance(x, np.ndarray):\n        return np.sqrt(np.maximum(np.sum(x * x, axis=-1, keepdims=True), eps))\n    else:\n        return torch.sqrt(torch.clamp(dot(x, x), min=eps))\n\n\ndef safe_normalize(x, eps=1e-20):\n    return x / length(x, eps)\n\n\ndef look_at(campos, target, opengl=True):\n    # campos: [N, 3], camera/eye position\n    # target: [N, 3], object to look at\n    # return: [N, 3, 3], rotation matrix\n    if not opengl:\n        # camera forward aligns with -z\n        forward_vector = safe_normalize(target - campos)\n        up_vector = np.array([0, 1, 0], dtype=np.float32)\n        right_vector = safe_normalize(np.cross(forward_vector, up_vector))\n        up_vector = safe_normalize(np.cross(right_vector, forward_vector))\n    else:\n        # camera forward aligns with +z\n        forward_vector = safe_normalize(campos - target)\n        up_vector = np.array([0, 1, 0], dtype=np.float32)\n        right_vector = safe_normalize(np.cross(up_vector, forward_vector))\n        up_vector = safe_normalize(np.cross(forward_vector, right_vector))\n    R = np.stack([right_vector, up_vector, forward_vector], axis=1)\n    return R\n\n\n# elevation & azimuth to pose (cam2world) matrix\ndef orbit_camera(elevation, azimuth, radius=1, is_degree=True, target=None, opengl=True):\n    # radius: scalar\n    # elevation: scalar, in (-90, 90), from +y to -y is (-90, 90)\n    # azimuth: scalar, in (-180, 180), from +z to +x is (0, 90)\n    # return: [4, 4], camera pose matrix\n    if is_degree:\n        elevation = np.deg2rad(elevation)\n        azimuth = np.deg2rad(azimuth)\n    x = radius * np.cos(elevation) * np.sin(azimuth)\n    y = - radius * np.sin(elevation)\n    z = radius * np.cos(elevation) * np.cos(azimuth)\n    if target is None:\n        target = np.zeros([3], dtype=np.float32)\n    campos = np.array([x, y, z]) + target  # [3]\n    T = np.eye(4, dtype=np.float32)\n    T[:3, :3] = look_at(campos, target, opengl)\n    T[:3, 3] = campos\n    return T\n\n\nclass OrbitCamera:\n    def __init__(self, W, H, r=2, fovy=60, near=0.01, far=100):\n        self.W = W\n        self.H = H\n        self.radius = r  # camera distance from center\n        self.fovy = np.deg2rad(fovy)  # deg 2 rad\n        self.near = near\n        self.far = far\n        self.center = np.array([0, 0, 0], dtype=np.float32)  # look at this point\n        self.rot = R.from_matrix(np.eye(3))\n        self.up = np.array([0, 1, 0], dtype=np.float32)  # need to be normalized!\n\n    @property\n    def fovx(self):\n        return 2 * np.arctan(np.tan(self.fovy / 2) * self.W / self.H)\n\n    @property\n    def campos(self):\n        return self.pose[:3, 3]\n\n    # pose (c2w)\n    @property\n    def pose(self):\n        # first move camera to radius\n        res = np.eye(4, dtype=np.float32)\n        res[2, 3] = self.radius  # opengl convention...\n        # rotate\n        rot = np.eye(4, dtype=np.float32)\n        rot[:3, :3] = self.rot.as_matrix()\n        res = rot @ res\n        # translate\n        res[:3, 3] -= self.center\n        return res\n\n    # view (w2c)\n    @property\n    def view(self):\n        return np.linalg.inv(self.pose)\n\n    # projection (perspective)\n    @property\n    def perspective(self):\n        y = np.tan(self.fovy / 2)\n        aspect = self.W / self.H\n        return np.array(\n            [\n                [1 / (y * aspect), 0, 0, 0],\n                [0, -1 / y, 0, 0],\n                [\n                    0,\n                    0,\n                    -(self.far + self.near) / (self.far - self.near),\n                    -(2 * self.far * self.near) / (self.far - self.near),\n                ],\n                [0, 0, -1, 0],\n            ],\n            dtype=np.float32,\n        )\n\n    # intrinsics\n    @property\n    def intrinsics(self):\n        focal = self.H / (2 * np.tan(self.fovy / 2))\n        return np.array([focal, focal, self.W // 2, self.H // 2], dtype=np.float32)\n\n    @property\n    def mvp(self):\n        return self.perspective @ np.linalg.inv(self.pose)  # [4, 4]\n\n    def orbit(self, dx, dy):\n        # rotate along camera up/side axis!\n        side = self.rot.as_matrix()[:3, 0]\n        rotvec_x = self.up * np.radians(-0.05 * dx)\n        rotvec_y = side * np.radians(-0.05 * dy)\n        self.rot = R.from_rotvec(rotvec_x) * R.from_rotvec(rotvec_y) * self.rot\n\n    def scale(self, delta):\n        self.radius *= 1.1 ** (-delta)\n\n    def pan(self, dx, dy, dz=0):\n        # pan in camera coordinate system (careful on the sensitivity!)\n        self.center += 0.0005 * self.rot.as_matrix()[:3, :3] @ np.array([-dx, -dy, dz])"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "gradio_app.py",
          "type": "blob",
          "size": 7.521484375,
          "content": "import gradio as gr\nimport os\nfrom PIL import Image\nimport subprocess\n\n\n# check if there is a picture uploaded or selected\ndef check_img_input(control_image):\n    if control_image is None:\n        raise gr.Error(\"Please select or upload an input image\")\n\n\ndef optimize_stage_1(image_block: Image.Image, preprocess_chk: bool, elevation_slider: float):\n    if not os.path.exists('tmp_data'):\n        os.makedirs('tmp_data')\n    if preprocess_chk:\n        # save image to a designated path\n        image_block.save(os.path.join('tmp_data', 'tmp.png'))\n\n        # preprocess image\n        print(f'python process.py {os.path.join(\"tmp_data\", \"tmp.png\")}')\n        subprocess.run(f'python process.py {os.path.join(\"tmp_data\", \"tmp.png\")}', shell=True)\n    else:\n        image_block.save(os.path.join('tmp_data', 'tmp_rgba.png'))\n\n    # stage 1\n    subprocess.run(f'python main.py --config {os.path.join(\"configs\", \"image.yaml\")} input={os.path.join(\"tmp_data\", \"tmp_rgba.png\")} save_path=tmp mesh_format=glb elevation={elevation_slider} force_cuda_rast=True', shell=True)\n\n    return os.path.join('logs', 'tmp_mesh.glb')\n\n\ndef optimize_stage_2(elevation_slider: float):\n    # stage 2\n    subprocess.run(f'python main2.py --config {os.path.join(\"configs\", \"image.yaml\")} input={os.path.join(\"tmp_data\", \"tmp_rgba.png\")} save_path=tmp mesh_format=glb elevation={elevation_slider} force_cuda_rast=True', shell=True)\n\n    return os.path.join('logs', 'tmp.glb')\n\n\nif __name__ == \"__main__\":\n    _TITLE = '''DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation'''\n\n    _DESCRIPTION = '''\n    <div>\n    <a style=\"display:inline-block\" href=\"https://dreamgaussian.github.io\"><img src='https://img.shields.io/badge/public_website-8A2BE2'></a>\n    <a style=\"display:inline-block; margin-left: .5em\" href=\"https://arxiv.org/abs/2309.16653\"><img src=\"https://img.shields.io/badge/2309.16653-f9f7f7?logo=data:image/png;base64,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\"></a>\n    <a style=\"display:inline-block; margin-left: .5em\" href='https://github.com/dreamgaussian/dreamgaussian'><img src='https://img.shields.io/github/stars/dreamgaussian/dreamgaussian?style=social'/></a>\n    </div>\n    We present DreamGausssion, a 3D content generation framework that significantly improves the efficiency of 3D content creation. \n    '''\n    _IMG_USER_GUIDE = \"Please upload an image in the block above (or choose an example above) and click **Generate 3D**.\"\n\n    # load images in 'data' folder as examples\n    example_folder = os.path.join(os.path.dirname(__file__), 'data')\n    example_fns = os.listdir(example_folder)\n    example_fns.sort()\n    examples_full = [os.path.join(example_folder, x) for x in example_fns if x.endswith('.png')]\n\n    # Compose demo layout & data flow\n    with gr.Blocks(title=_TITLE, theme=gr.themes.Soft()) as demo:\n        with gr.Row():\n            with gr.Column(scale=1):\n                gr.Markdown('# ' + _TITLE)\n        gr.Markdown(_DESCRIPTION)\n\n        # Image-to-3D\n        with gr.Row(variant='panel'):\n            with gr.Column(scale=5):\n                image_block = gr.Image(type='pil', image_mode='RGBA', height=290, label='Input image', tool=None)\n\n                elevation_slider = gr.Slider(-90, 90, value=0, step=1, label='Estimated elevation angle')\n                gr.Markdown(\n                    \"default to 0 (horizontal), range from [-90, 90]. If you upload a look-down image, try a value like -30\")\n\n                preprocess_chk = gr.Checkbox(True,\n                                             label='Preprocess image automatically (remove background and recenter object)')\n\n                gr.Examples(\n                    examples=examples_full,  # NOTE: elements must match inputs list!\n                    inputs=[image_block],\n                    outputs=[image_block],\n                    cache_examples=False,\n                    label='Examples (click one of the images below to start)',\n                    examples_per_page=40\n                )\n                img_run_btn = gr.Button(\"Generate 3D\")\n                img_guide_text = gr.Markdown(_IMG_USER_GUIDE, visible=True)\n\n            with gr.Column(scale=5):\n                obj3d_stage1 = gr.Model3D(clear_color=[0.0, 0.0, 0.0, 0.0], label=\"3D Model (Stage 1)\")\n                obj3d = gr.Model3D(clear_color=[0.0, 0.0, 0.0, 0.0], label=\"3D Model (Final)\")\n\n            # if there is an input image, continue with inference\n            # else display an error message\n            img_run_btn.click(check_img_input, inputs=[image_block], queue=False).success(optimize_stage_1,\n                                                                                          inputs=[image_block,\n                                                                                                  preprocess_chk,\n                                                                                                  elevation_slider],\n                                                                                          outputs=[\n                                                                                              obj3d_stage1]).success(\n                optimize_stage_2, inputs=[elevation_slider], outputs=[obj3d])\n\n    demo.queue().launch(share=True)"
        },
        {
          "name": "grid_put.py",
          "type": "blob",
          "size": 11.3388671875,
          "content": "import torch\nimport torch.nn.functional as F\n\ndef stride_from_shape(shape):\n    stride = [1]\n    for x in reversed(shape[1:]):\n        stride.append(stride[-1] * x) \n    return list(reversed(stride))\n\n\ndef scatter_add_nd(input, indices, values):\n    # input: [..., C], D dimension + C channel\n    # indices: [N, D], long\n    # values: [N, C]\n\n    D = indices.shape[-1]\n    C = input.shape[-1]\n    size = input.shape[:-1]\n    stride = stride_from_shape(size)\n\n    assert len(size) == D\n\n    input = input.view(-1, C)  # [HW, C]\n    flatten_indices = (indices * torch.tensor(stride, dtype=torch.long, device=indices.device)).sum(-1)  # [N]\n\n    input.scatter_add_(0, flatten_indices.unsqueeze(1).repeat(1, C), values)\n\n    return input.view(*size, C)\n\n\ndef scatter_add_nd_with_count(input, count, indices, values, weights=None):\n    # input: [..., C], D dimension + C channel\n    # count: [..., 1], D dimension\n    # indices: [N, D], long\n    # values: [N, C]\n\n    D = indices.shape[-1]\n    C = input.shape[-1]\n    size = input.shape[:-1]\n    stride = stride_from_shape(size)\n\n    assert len(size) == D\n\n    input = input.view(-1, C)  # [HW, C]\n    count = count.view(-1, 1)\n\n    flatten_indices = (indices * torch.tensor(stride, dtype=torch.long, device=indices.device)).sum(-1)  # [N]\n\n    if weights is None:\n        weights = torch.ones_like(values[..., :1]) \n\n    input.scatter_add_(0, flatten_indices.unsqueeze(1).repeat(1, C), values)\n    count.scatter_add_(0, flatten_indices.unsqueeze(1), weights)\n\n    return input.view(*size, C), count.view(*size, 1)\n\ndef nearest_grid_put_2d(H, W, coords, values, return_count=False):\n    # coords: [N, 2], float in [-1, 1]\n    # values: [N, C]\n\n    C = values.shape[-1]\n\n    indices = (coords * 0.5 + 0.5) * torch.tensor(\n        [H - 1, W - 1], dtype=torch.float32, device=coords.device\n    )\n    indices = indices.round().long()  # [N, 2]\n\n    result = torch.zeros(H, W, C, device=values.device, dtype=values.dtype)  # [H, W, C]\n    count = torch.zeros(H, W, 1, device=values.device, dtype=values.dtype)  # [H, W, 1]\n    weights = torch.ones_like(values[..., :1])  # [N, 1]\n    \n    result, count = scatter_add_nd_with_count(result, count, indices, values, weights)\n\n    if return_count:\n        return result, count\n\n    mask = (count.squeeze(-1) > 0)\n    result[mask] = result[mask] / count[mask].repeat(1, C)\n\n    return result\n\n\ndef linear_grid_put_2d(H, W, coords, values, return_count=False):\n    # coords: [N, 2], float in [-1, 1]\n    # values: [N, C]\n\n    C = values.shape[-1]\n\n    indices = (coords * 0.5 + 0.5) * torch.tensor(\n        [H - 1, W - 1], dtype=torch.float32, device=coords.device\n    )\n    indices_00 = indices.floor().long()  # [N, 2]\n    indices_00[:, 0].clamp_(0, H - 2)\n    indices_00[:, 1].clamp_(0, W - 2)\n    indices_01 = indices_00 + torch.tensor(\n        [0, 1], dtype=torch.long, device=indices.device\n    )\n    indices_10 = indices_00 + torch.tensor(\n        [1, 0], dtype=torch.long, device=indices.device\n    )\n    indices_11 = indices_00 + torch.tensor(\n        [1, 1], dtype=torch.long, device=indices.device\n    )\n\n    h = indices[..., 0] - indices_00[..., 0].float()\n    w = indices[..., 1] - indices_00[..., 1].float()\n    w_00 = (1 - h) * (1 - w)\n    w_01 = (1 - h) * w\n    w_10 = h * (1 - w)\n    w_11 = h * w\n\n    result = torch.zeros(H, W, C, device=values.device, dtype=values.dtype)  # [H, W, C]\n    count = torch.zeros(H, W, 1, device=values.device, dtype=values.dtype)  # [H, W, 1]\n    weights = torch.ones_like(values[..., :1])  # [N, 1]\n    \n    result, count = scatter_add_nd_with_count(result, count, indices_00, values * w_00.unsqueeze(1), weights* w_00.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_01, values * w_01.unsqueeze(1), weights* w_01.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_10, values * w_10.unsqueeze(1), weights* w_10.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_11, values * w_11.unsqueeze(1), weights* w_11.unsqueeze(1))\n\n    if return_count:\n        return result, count\n\n    mask = (count.squeeze(-1) > 0)\n    result[mask] = result[mask] / count[mask].repeat(1, C)\n\n    return result\n\ndef mipmap_linear_grid_put_2d(H, W, coords, values, min_resolution=32, return_count=False):\n    # coords: [N, 2], float in [-1, 1]\n    # values: [N, C]\n\n    C = values.shape[-1]\n\n    result = torch.zeros(H, W, C, device=values.device, dtype=values.dtype)  # [H, W, C]\n    count = torch.zeros(H, W, 1, device=values.device, dtype=values.dtype)  # [H, W, 1]\n\n    cur_H, cur_W = H, W\n    \n    while min(cur_H, cur_W) > min_resolution:\n\n        # try to fill the holes\n        mask = (count.squeeze(-1) == 0)\n        if not mask.any():\n            break\n\n        cur_result, cur_count = linear_grid_put_2d(cur_H, cur_W, coords, values, return_count=True)\n        result[mask] = result[mask] + F.interpolate(cur_result.permute(2,0,1).unsqueeze(0).contiguous(), (H, W), mode='bilinear', align_corners=False).squeeze(0).permute(1,2,0).contiguous()[mask]\n        count[mask] = count[mask] + F.interpolate(cur_count.view(1, 1, cur_H, cur_W), (H, W), mode='bilinear', align_corners=False).view(H, W, 1)[mask]\n        cur_H //= 2\n        cur_W //= 2\n    \n    if return_count:\n        return result, count\n\n    mask = (count.squeeze(-1) > 0)\n    result[mask] = result[mask] / count[mask].repeat(1, C)\n\n    return result\n\ndef nearest_grid_put_3d(H, W, D, coords, values, return_count=False):\n    # coords: [N, 3], float in [-1, 1]\n    # values: [N, C]\n\n    C = values.shape[-1]\n\n    indices = (coords * 0.5 + 0.5) * torch.tensor(\n        [H - 1, W - 1, D - 1], dtype=torch.float32, device=coords.device\n    )\n    indices = indices.round().long()  # [N, 2]\n\n    result = torch.zeros(H, W, D, C, device=values.device, dtype=values.dtype)  # [H, W, C]\n    count = torch.zeros(H, W, D, 1, device=values.device, dtype=values.dtype)  # [H, W, 1]\n    weights = torch.ones_like(values[..., :1])  # [N, 1]\n\n    result, count = scatter_add_nd_with_count(result, count, indices, values, weights)\n    \n    if return_count:\n        return result, count\n\n    mask = (count.squeeze(-1) > 0)\n    result[mask] = result[mask] / count[mask].repeat(1, C)\n\n    return result\n\n\ndef linear_grid_put_3d(H, W, D, coords, values, return_count=False):\n    # coords: [N, 3], float in [-1, 1]\n    # values: [N, C]\n\n    C = values.shape[-1]\n\n    indices = (coords * 0.5 + 0.5) * torch.tensor(\n        [H - 1, W - 1, D - 1], dtype=torch.float32, device=coords.device\n    )\n    indices_000 = indices.floor().long()  # [N, 3]\n    indices_000[:, 0].clamp_(0, H - 2)\n    indices_000[:, 1].clamp_(0, W - 2)\n    indices_000[:, 2].clamp_(0, D - 2)\n\n    indices_001 = indices_000 + torch.tensor([0, 0, 1], dtype=torch.long, device=indices.device)\n    indices_010 = indices_000 + torch.tensor([0, 1, 0], dtype=torch.long, device=indices.device)\n    indices_011 = indices_000 + torch.tensor([0, 1, 1], dtype=torch.long, device=indices.device)\n    indices_100 = indices_000 + torch.tensor([1, 0, 0], dtype=torch.long, device=indices.device)\n    indices_101 = indices_000 + torch.tensor([1, 0, 1], dtype=torch.long, device=indices.device)\n    indices_110 = indices_000 + torch.tensor([1, 1, 0], dtype=torch.long, device=indices.device)\n    indices_111 = indices_000 + torch.tensor([1, 1, 1], dtype=torch.long, device=indices.device)\n\n    h = indices[..., 0] - indices_000[..., 0].float()\n    w = indices[..., 1] - indices_000[..., 1].float()\n    d = indices[..., 2] - indices_000[..., 2].float()\n    \n    w_000 = (1 - h) * (1 - w) * (1 - d)\n    w_001 = (1 - h) * w * (1 - d)\n    w_010 = h * (1 - w) * (1 - d)\n    w_011 = h * w * (1 - d)\n    w_100 = (1 - h) * (1 - w) * d\n    w_101 = (1 - h) * w * d\n    w_110 = h * (1 - w) * d\n    w_111 = h * w * d\n\n    result = torch.zeros(H, W, D, C, device=values.device, dtype=values.dtype)  # [H, W, D, C]\n    count = torch.zeros(H, W, D, 1, device=values.device, dtype=values.dtype)  # [H, W, D, 1]\n    weights = torch.ones_like(values[..., :1])  # [N, 1]\n    \n    result, count = scatter_add_nd_with_count(result, count, indices_000, values * w_000.unsqueeze(1), weights * w_000.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_001, values * w_001.unsqueeze(1), weights * w_001.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_010, values * w_010.unsqueeze(1), weights * w_010.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_011, values * w_011.unsqueeze(1), weights * w_011.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_100, values * w_100.unsqueeze(1), weights * w_100.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_101, values * w_101.unsqueeze(1), weights * w_101.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_110, values * w_110.unsqueeze(1), weights * w_110.unsqueeze(1))\n    result, count = scatter_add_nd_with_count(result, count, indices_111, values * w_111.unsqueeze(1), weights * w_111.unsqueeze(1))\n\n    if return_count:\n        return result, count\n\n    mask = (count.squeeze(-1) > 0)\n    result[mask] = result[mask] / count[mask].repeat(1, C)\n\n    return result\n\ndef mipmap_linear_grid_put_3d(H, W, D, coords, values, min_resolution=32, return_count=False):\n    # coords: [N, 3], float in [-1, 1]\n    # values: [N, C]\n\n    C = values.shape[-1]\n\n    result = torch.zeros(H, W, D, C, device=values.device, dtype=values.dtype)  # [H, W, D, C]\n    count = torch.zeros(H, W, D, 1, device=values.device, dtype=values.dtype)  # [H, W, D, 1]\n    cur_H, cur_W, cur_D = H, W, D\n    \n    while min(min(cur_H, cur_W), cur_D) > min_resolution:\n\n        # try to fill the holes\n        mask = (count.squeeze(-1) == 0)\n        if not mask.any():\n            break\n\n        cur_result, cur_count = linear_grid_put_3d(cur_H, cur_W, cur_D, coords, values, return_count=True)\n        result[mask] = result[mask] + F.interpolate(cur_result.permute(3,0,1,2).unsqueeze(0).contiguous(), (H, W, D), mode='trilinear', align_corners=False).squeeze(0).permute(1,2,3,0).contiguous()[mask]\n        count[mask] = count[mask] + F.interpolate(cur_count.view(1, 1, cur_H, cur_W, cur_D), (H, W, D), mode='trilinear', align_corners=False).view(H, W, D, 1)[mask]\n        cur_H //= 2\n        cur_W //= 2\n        cur_D //= 2\n    \n    if return_count:\n        return result, count\n\n    mask = (count.squeeze(-1) > 0)\n    result[mask] = result[mask] / count[mask].repeat(1, C)\n\n    return result\n\n\ndef grid_put(shape, coords, values, mode='linear-mipmap', min_resolution=32, return_raw=False):\n    # shape: [D], list/tuple\n    # coords: [N, D], float in [-1, 1]\n    # values: [N, C]\n\n    D = len(shape)\n    assert D in [2, 3], f'only support D == 2 or 3, but got D == {D}'\n\n    if mode == 'nearest':\n        if D == 2:\n            return nearest_grid_put_2d(*shape, coords, values, return_raw)\n        else:\n            return nearest_grid_put_3d(*shape, coords, values, return_raw)\n    elif mode == 'linear':\n        if D == 2:\n            return linear_grid_put_2d(*shape, coords, values, return_raw)\n        else:\n            return linear_grid_put_3d(*shape, coords, values, return_raw)\n    elif mode == 'linear-mipmap':\n        if D == 2:\n            return mipmap_linear_grid_put_2d(*shape, coords, values, min_resolution, return_raw)\n        else:\n            return mipmap_linear_grid_put_3d(*shape, coords, values, min_resolution, return_raw)\n    else:\n        raise NotImplementedError(f\"got mode {mode}\")    "
        },
        {
          "name": "gs_renderer.py",
          "type": "blob",
          "size": 33.111328125,
          "content": "import os\nimport math\nimport numpy as np\nfrom typing import NamedTuple\nfrom plyfile import PlyData, PlyElement\n\nimport torch\nfrom torch import nn\n\nfrom diff_gaussian_rasterization import (\n    GaussianRasterizationSettings,\n    GaussianRasterizer,\n)\nfrom simple_knn._C import distCUDA2\n\nfrom sh_utils import eval_sh, SH2RGB, RGB2SH\nfrom mesh import Mesh\nfrom mesh_utils import decimate_mesh, clean_mesh\n\nimport kiui\n\ndef inverse_sigmoid(x):\n    return torch.log(x/(1-x))\n\ndef get_expon_lr_func(\n    lr_init, lr_final, lr_delay_steps=0, lr_delay_mult=1.0, max_steps=1000000\n):\n    \n    def helper(step):\n        if lr_init == lr_final:\n            # constant lr, ignore other params\n            return lr_init\n        if step < 0 or (lr_init == 0.0 and lr_final == 0.0):\n            # Disable this parameter\n            return 0.0\n        if lr_delay_steps > 0:\n            # A kind of reverse cosine decay.\n            delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n                0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)\n            )\n        else:\n            delay_rate = 1.0\n        t = np.clip(step / max_steps, 0, 1)\n        log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n        return delay_rate * log_lerp\n\n    return helper\n\n\ndef strip_lowerdiag(L):\n    uncertainty = torch.zeros((L.shape[0], 6), dtype=torch.float, device=\"cuda\")\n\n    uncertainty[:, 0] = L[:, 0, 0]\n    uncertainty[:, 1] = L[:, 0, 1]\n    uncertainty[:, 2] = L[:, 0, 2]\n    uncertainty[:, 3] = L[:, 1, 1]\n    uncertainty[:, 4] = L[:, 1, 2]\n    uncertainty[:, 5] = L[:, 2, 2]\n    return uncertainty\n\ndef strip_symmetric(sym):\n    return strip_lowerdiag(sym)\n\ndef gaussian_3d_coeff(xyzs, covs):\n    # xyzs: [N, 3]\n    # covs: [N, 6]\n    x, y, z = xyzs[:, 0], xyzs[:, 1], xyzs[:, 2]\n    a, b, c, d, e, f = covs[:, 0], covs[:, 1], covs[:, 2], covs[:, 3], covs[:, 4], covs[:, 5]\n\n    # eps must be small enough !!!\n    inv_det = 1 / (a * d * f + 2 * e * c * b - e**2 * a - c**2 * d - b**2 * f + 1e-24)\n    inv_a = (d * f - e**2) * inv_det\n    inv_b = (e * c - b * f) * inv_det\n    inv_c = (e * b - c * d) * inv_det\n    inv_d = (a * f - c**2) * inv_det\n    inv_e = (b * c - e * a) * inv_det\n    inv_f = (a * d - b**2) * inv_det\n\n    power = -0.5 * (x**2 * inv_a + y**2 * inv_d + z**2 * inv_f) - x * y * inv_b - x * z * inv_c - y * z * inv_e\n\n    power[power > 0] = -1e10 # abnormal values... make weights 0\n        \n    return torch.exp(power)\n\ndef build_rotation(r):\n    norm = torch.sqrt(r[:,0]*r[:,0] + r[:,1]*r[:,1] + r[:,2]*r[:,2] + r[:,3]*r[:,3])\n\n    q = r / norm[:, None]\n\n    R = torch.zeros((q.size(0), 3, 3), device='cuda')\n\n    r = q[:, 0]\n    x = q[:, 1]\n    y = q[:, 2]\n    z = q[:, 3]\n\n    R[:, 0, 0] = 1 - 2 * (y*y + z*z)\n    R[:, 0, 1] = 2 * (x*y - r*z)\n    R[:, 0, 2] = 2 * (x*z + r*y)\n    R[:, 1, 0] = 2 * (x*y + r*z)\n    R[:, 1, 1] = 1 - 2 * (x*x + z*z)\n    R[:, 1, 2] = 2 * (y*z - r*x)\n    R[:, 2, 0] = 2 * (x*z - r*y)\n    R[:, 2, 1] = 2 * (y*z + r*x)\n    R[:, 2, 2] = 1 - 2 * (x*x + y*y)\n    return R\n\ndef build_scaling_rotation(s, r):\n    L = torch.zeros((s.shape[0], 3, 3), dtype=torch.float, device=\"cuda\")\n    R = build_rotation(r)\n\n    L[:,0,0] = s[:,0]\n    L[:,1,1] = s[:,1]\n    L[:,2,2] = s[:,2]\n\n    L = R @ L\n    return L\n\nclass BasicPointCloud(NamedTuple):\n    points: np.array\n    colors: np.array\n    normals: np.array\n\n\nclass GaussianModel:\n\n    def setup_functions(self):\n        def build_covariance_from_scaling_rotation(scaling, scaling_modifier, rotation):\n            L = build_scaling_rotation(scaling_modifier * scaling, rotation)\n            actual_covariance = L @ L.transpose(1, 2)\n            symm = strip_symmetric(actual_covariance)\n            return symm\n        \n        self.scaling_activation = torch.exp\n        self.scaling_inverse_activation = torch.log\n\n        self.covariance_activation = build_covariance_from_scaling_rotation\n\n        self.opacity_activation = torch.sigmoid\n        self.inverse_opacity_activation = inverse_sigmoid\n\n        self.rotation_activation = torch.nn.functional.normalize\n\n\n    def __init__(self, sh_degree : int):\n        self.active_sh_degree = 0\n        self.max_sh_degree = sh_degree  \n        self._xyz = torch.empty(0)\n        self._features_dc = torch.empty(0)\n        self._features_rest = torch.empty(0)\n        self._scaling = torch.empty(0)\n        self._rotation = torch.empty(0)\n        self._opacity = torch.empty(0)\n        self.max_radii2D = torch.empty(0)\n        self.xyz_gradient_accum = torch.empty(0)\n        self.denom = torch.empty(0)\n        self.optimizer = None\n        self.percent_dense = 0\n        self.spatial_lr_scale = 0\n        self.setup_functions()\n\n    def capture(self):\n        return (\n            self.active_sh_degree,\n            self._xyz,\n            self._features_dc,\n            self._features_rest,\n            self._scaling,\n            self._rotation,\n            self._opacity,\n            self.max_radii2D,\n            self.xyz_gradient_accum,\n            self.denom,\n            self.optimizer.state_dict(),\n            self.spatial_lr_scale,\n        )\n    \n    def restore(self, model_args, training_args):\n        (self.active_sh_degree, \n        self._xyz, \n        self._features_dc, \n        self._features_rest,\n        self._scaling, \n        self._rotation, \n        self._opacity,\n        self.max_radii2D, \n        xyz_gradient_accum, \n        denom,\n        opt_dict, \n        self.spatial_lr_scale) = model_args\n        self.training_setup(training_args)\n        self.xyz_gradient_accum = xyz_gradient_accum\n        self.denom = denom\n        self.optimizer.load_state_dict(opt_dict)\n\n    @property\n    def get_scaling(self):\n        return self.scaling_activation(self._scaling)\n    \n    @property\n    def get_rotation(self):\n        return self.rotation_activation(self._rotation)\n    \n    @property\n    def get_xyz(self):\n        return self._xyz\n    \n    @property\n    def get_features(self):\n        features_dc = self._features_dc\n        features_rest = self._features_rest\n        return torch.cat((features_dc, features_rest), dim=1)\n    \n    @property\n    def get_opacity(self):\n        return self.opacity_activation(self._opacity)\n\n    @torch.no_grad()\n    def extract_fields(self, resolution=128, num_blocks=16, relax_ratio=1.5):\n        # resolution: resolution of field\n        \n        block_size = 2 / num_blocks\n\n        assert resolution % block_size == 0\n        split_size = resolution // num_blocks\n\n        opacities = self.get_opacity\n\n        # pre-filter low opacity gaussians to save computation\n        mask = (opacities > 0.005).squeeze(1)\n\n        opacities = opacities[mask]\n        xyzs = self.get_xyz[mask]\n        stds = self.get_scaling[mask]\n        \n        # normalize to ~ [-1, 1]\n        mn, mx = xyzs.amin(0), xyzs.amax(0)\n        self.center = (mn + mx) / 2\n        self.scale = 1.8 / (mx - mn).amax().item()\n\n        xyzs = (xyzs - self.center) * self.scale\n        stds = stds * self.scale\n\n        covs = self.covariance_activation(stds, 1, self._rotation[mask])\n\n        # tile\n        device = opacities.device\n        occ = torch.zeros([resolution] * 3, dtype=torch.float32, device=device)\n\n        X = torch.linspace(-1, 1, resolution).split(split_size)\n        Y = torch.linspace(-1, 1, resolution).split(split_size)\n        Z = torch.linspace(-1, 1, resolution).split(split_size)\n\n\n        # loop blocks (assume max size of gaussian is small than relax_ratio * block_size !!!)\n        for xi, xs in enumerate(X):\n            for yi, ys in enumerate(Y):\n                for zi, zs in enumerate(Z):\n                    xx, yy, zz = torch.meshgrid(xs, ys, zs)\n                    # sample points [M, 3]\n                    pts = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1), zz.reshape(-1, 1)], dim=-1).to(device)\n                    # in-tile gaussians mask\n                    vmin, vmax = pts.amin(0), pts.amax(0)\n                    vmin -= block_size * relax_ratio\n                    vmax += block_size * relax_ratio\n                    mask = (xyzs < vmax).all(-1) & (xyzs > vmin).all(-1)\n                    # if hit no gaussian, continue to next block\n                    if not mask.any():\n                        continue\n                    mask_xyzs = xyzs[mask] # [L, 3]\n                    mask_covs = covs[mask] # [L, 6]\n                    mask_opas = opacities[mask].view(1, -1) # [L, 1] --> [1, L]\n\n                    # query per point-gaussian pair.\n                    g_pts = pts.unsqueeze(1).repeat(1, mask_covs.shape[0], 1) - mask_xyzs.unsqueeze(0) # [M, L, 3]\n                    g_covs = mask_covs.unsqueeze(0).repeat(pts.shape[0], 1, 1) # [M, L, 6]\n\n                    # batch on gaussian to avoid OOM\n                    batch_g = 1024\n                    val = 0\n                    for start in range(0, g_covs.shape[1], batch_g):\n                        end = min(start + batch_g, g_covs.shape[1])\n                        w = gaussian_3d_coeff(g_pts[:, start:end].reshape(-1, 3), g_covs[:, start:end].reshape(-1, 6)).reshape(pts.shape[0], -1) # [M, l]\n                        val += (mask_opas[:, start:end] * w).sum(-1)\n                    \n                    # kiui.lo(val, mask_opas, w)\n                \n                    occ[xi * split_size: xi * split_size + len(xs), \n                        yi * split_size: yi * split_size + len(ys), \n                        zi * split_size: zi * split_size + len(zs)] = val.reshape(len(xs), len(ys), len(zs)) \n        \n        kiui.lo(occ, verbose=1)\n\n        return occ\n    \n    def extract_mesh(self, path, density_thresh=1, resolution=128, decimate_target=1e5):\n\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n\n        occ = self.extract_fields(resolution).detach().cpu().numpy()\n\n        import mcubes\n        vertices, triangles = mcubes.marching_cubes(occ, density_thresh)\n        vertices = vertices / (resolution - 1.0) * 2 - 1\n\n        # transform back to the original space\n        vertices = vertices / self.scale + self.center.detach().cpu().numpy()\n\n        vertices, triangles = clean_mesh(vertices, triangles, remesh=True, remesh_size=0.015)\n        if decimate_target > 0 and triangles.shape[0] > decimate_target:\n            vertices, triangles = decimate_mesh(vertices, triangles, decimate_target)\n\n        v = torch.from_numpy(vertices.astype(np.float32)).contiguous().cuda()\n        f = torch.from_numpy(triangles.astype(np.int32)).contiguous().cuda()\n\n        print(\n            f\"[INFO] marching cubes result: {v.shape} ({v.min().item()}-{v.max().item()}), {f.shape}\"\n        )\n\n        mesh = Mesh(v=v, f=f, device='cuda')\n\n        return mesh\n    \n    def get_covariance(self, scaling_modifier = 1):\n        return self.covariance_activation(self.get_scaling, scaling_modifier, self._rotation)\n\n    def oneupSHdegree(self):\n        if self.active_sh_degree < self.max_sh_degree:\n            self.active_sh_degree += 1\n\n    def create_from_pcd(self, pcd : BasicPointCloud, spatial_lr_scale : float = 1):\n        self.spatial_lr_scale = spatial_lr_scale\n        fused_point_cloud = torch.tensor(np.asarray(pcd.points)).float().cuda()\n        fused_color = RGB2SH(torch.tensor(np.asarray(pcd.colors)).float().cuda())\n        features = torch.zeros((fused_color.shape[0], 3, (self.max_sh_degree + 1) ** 2)).float().cuda()\n        features[:, :3, 0 ] = fused_color\n        features[:, 3:, 1:] = 0.0\n\n        print(\"Number of points at initialisation : \", fused_point_cloud.shape[0])\n\n        dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)\n        scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)\n        rots = torch.zeros((fused_point_cloud.shape[0], 4), device=\"cuda\")\n        rots[:, 0] = 1\n\n        opacities = inverse_sigmoid(0.1 * torch.ones((fused_point_cloud.shape[0], 1), dtype=torch.float, device=\"cuda\"))\n\n        self._xyz = nn.Parameter(fused_point_cloud.requires_grad_(True))\n        self._features_dc = nn.Parameter(features[:,:,0:1].transpose(1, 2).contiguous().requires_grad_(True))\n        self._features_rest = nn.Parameter(features[:,:,1:].transpose(1, 2).contiguous().requires_grad_(True))\n        self._scaling = nn.Parameter(scales.requires_grad_(True))\n        self._rotation = nn.Parameter(rots.requires_grad_(True))\n        self._opacity = nn.Parameter(opacities.requires_grad_(True))\n        self.max_radii2D = torch.zeros((self.get_xyz.shape[0]), device=\"cuda\")\n\n    def training_setup(self, training_args):\n        self.percent_dense = training_args.percent_dense\n        self.xyz_gradient_accum = torch.zeros((self.get_xyz.shape[0], 1), device=\"cuda\")\n        self.denom = torch.zeros((self.get_xyz.shape[0], 1), device=\"cuda\")\n\n        l = [\n            {'params': [self._xyz], 'lr': training_args.position_lr_init * self.spatial_lr_scale, \"name\": \"xyz\"},\n            {'params': [self._features_dc], 'lr': training_args.feature_lr, \"name\": \"f_dc\"},\n            {'params': [self._features_rest], 'lr': training_args.feature_lr / 20.0, \"name\": \"f_rest\"},\n            {'params': [self._opacity], 'lr': training_args.opacity_lr, \"name\": \"opacity\"},\n            {'params': [self._scaling], 'lr': training_args.scaling_lr, \"name\": \"scaling\"},\n            {'params': [self._rotation], 'lr': training_args.rotation_lr, \"name\": \"rotation\"}\n        ]\n\n        self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)\n        self.xyz_scheduler_args = get_expon_lr_func(lr_init=training_args.position_lr_init*self.spatial_lr_scale,\n                                                    lr_final=training_args.position_lr_final*self.spatial_lr_scale,\n                                                    lr_delay_mult=training_args.position_lr_delay_mult,\n                                                    max_steps=training_args.position_lr_max_steps)\n\n    def update_learning_rate(self, iteration):\n        ''' Learning rate scheduling per step '''\n        for param_group in self.optimizer.param_groups:\n            if param_group[\"name\"] == \"xyz\":\n                lr = self.xyz_scheduler_args(iteration)\n                param_group['lr'] = lr\n                return lr\n\n    def construct_list_of_attributes(self):\n        l = ['x', 'y', 'z', 'nx', 'ny', 'nz']\n        # All channels except the 3 DC\n        for i in range(self._features_dc.shape[1]*self._features_dc.shape[2]):\n            l.append('f_dc_{}'.format(i))\n        for i in range(self._features_rest.shape[1]*self._features_rest.shape[2]):\n            l.append('f_rest_{}'.format(i))\n        l.append('opacity')\n        for i in range(self._scaling.shape[1]):\n            l.append('scale_{}'.format(i))\n        for i in range(self._rotation.shape[1]):\n            l.append('rot_{}'.format(i))\n        return l\n\n    def save_ply(self, path):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n\n        xyz = self._xyz.detach().cpu().numpy()\n        normals = np.zeros_like(xyz)\n        f_dc = self._features_dc.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()\n        f_rest = self._features_rest.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()\n        opacities = self._opacity.detach().cpu().numpy()\n        scale = self._scaling.detach().cpu().numpy()\n        rotation = self._rotation.detach().cpu().numpy()\n\n        dtype_full = [(attribute, 'f4') for attribute in self.construct_list_of_attributes()]\n\n        elements = np.empty(xyz.shape[0], dtype=dtype_full)\n        attributes = np.concatenate((xyz, normals, f_dc, f_rest, opacities, scale, rotation), axis=1)\n        elements[:] = list(map(tuple, attributes))\n        el = PlyElement.describe(elements, 'vertex')\n        PlyData([el]).write(path)\n\n    def reset_opacity(self):\n        opacities_new = inverse_sigmoid(torch.min(self.get_opacity, torch.ones_like(self.get_opacity)*0.01))\n        optimizable_tensors = self.replace_tensor_to_optimizer(opacities_new, \"opacity\")\n        self._opacity = optimizable_tensors[\"opacity\"]\n\n    def load_ply(self, path):\n        plydata = PlyData.read(path)\n\n        xyz = np.stack((np.asarray(plydata.elements[0][\"x\"]),\n                        np.asarray(plydata.elements[0][\"y\"]),\n                        np.asarray(plydata.elements[0][\"z\"])),  axis=1)\n        opacities = np.asarray(plydata.elements[0][\"opacity\"])[..., np.newaxis]\n\n        print(\"Number of points at loading : \", xyz.shape[0])\n\n        features_dc = np.zeros((xyz.shape[0], 3, 1))\n        features_dc[:, 0, 0] = np.asarray(plydata.elements[0][\"f_dc_0\"])\n        features_dc[:, 1, 0] = np.asarray(plydata.elements[0][\"f_dc_1\"])\n        features_dc[:, 2, 0] = np.asarray(plydata.elements[0][\"f_dc_2\"])\n\n        extra_f_names = [p.name for p in plydata.elements[0].properties if p.name.startswith(\"f_rest_\")]\n        assert len(extra_f_names)==3*(self.max_sh_degree + 1) ** 2 - 3\n        features_extra = np.zeros((xyz.shape[0], len(extra_f_names)))\n        for idx, attr_name in enumerate(extra_f_names):\n            features_extra[:, idx] = np.asarray(plydata.elements[0][attr_name])\n        # Reshape (P,F*SH_coeffs) to (P, F, SH_coeffs except DC)\n        features_extra = features_extra.reshape((features_extra.shape[0], 3, (self.max_sh_degree + 1) ** 2 - 1))\n\n        scale_names = [p.name for p in plydata.elements[0].properties if p.name.startswith(\"scale_\")]\n        scales = np.zeros((xyz.shape[0], len(scale_names)))\n        for idx, attr_name in enumerate(scale_names):\n            scales[:, idx] = np.asarray(plydata.elements[0][attr_name])\n\n        rot_names = [p.name for p in plydata.elements[0].properties if p.name.startswith(\"rot\")]\n        rots = np.zeros((xyz.shape[0], len(rot_names)))\n        for idx, attr_name in enumerate(rot_names):\n            rots[:, idx] = np.asarray(plydata.elements[0][attr_name])\n\n        self._xyz = nn.Parameter(torch.tensor(xyz, dtype=torch.float, device=\"cuda\").requires_grad_(True))\n        self._features_dc = nn.Parameter(torch.tensor(features_dc, dtype=torch.float, device=\"cuda\").transpose(1, 2).contiguous().requires_grad_(True))\n        self._features_rest = nn.Parameter(torch.tensor(features_extra, dtype=torch.float, device=\"cuda\").transpose(1, 2).contiguous().requires_grad_(True))\n        self._opacity = nn.Parameter(torch.tensor(opacities, dtype=torch.float, device=\"cuda\").requires_grad_(True))\n        self._scaling = nn.Parameter(torch.tensor(scales, dtype=torch.float, device=\"cuda\").requires_grad_(True))\n        self._rotation = nn.Parameter(torch.tensor(rots, dtype=torch.float, device=\"cuda\").requires_grad_(True))\n\n        self.active_sh_degree = self.max_sh_degree\n\n    def replace_tensor_to_optimizer(self, tensor, name):\n        optimizable_tensors = {}\n        for group in self.optimizer.param_groups:\n            if group[\"name\"] == name:\n                stored_state = self.optimizer.state.get(group['params'][0], None)\n                stored_state[\"exp_avg\"] = torch.zeros_like(tensor)\n                stored_state[\"exp_avg_sq\"] = torch.zeros_like(tensor)\n\n                del self.optimizer.state[group['params'][0]]\n                group[\"params\"][0] = nn.Parameter(tensor.requires_grad_(True))\n                self.optimizer.state[group['params'][0]] = stored_state\n\n                optimizable_tensors[group[\"name\"]] = group[\"params\"][0]\n        return optimizable_tensors\n\n    def _prune_optimizer(self, mask):\n        optimizable_tensors = {}\n        for group in self.optimizer.param_groups:\n            stored_state = self.optimizer.state.get(group['params'][0], None)\n            if stored_state is not None:\n                stored_state[\"exp_avg\"] = stored_state[\"exp_avg\"][mask]\n                stored_state[\"exp_avg_sq\"] = stored_state[\"exp_avg_sq\"][mask]\n\n                del self.optimizer.state[group['params'][0]]\n                group[\"params\"][0] = nn.Parameter((group[\"params\"][0][mask].requires_grad_(True)))\n                self.optimizer.state[group['params'][0]] = stored_state\n\n                optimizable_tensors[group[\"name\"]] = group[\"params\"][0]\n            else:\n                group[\"params\"][0] = nn.Parameter(group[\"params\"][0][mask].requires_grad_(True))\n                optimizable_tensors[group[\"name\"]] = group[\"params\"][0]\n        return optimizable_tensors\n\n    def prune_points(self, mask):\n        valid_points_mask = ~mask\n        optimizable_tensors = self._prune_optimizer(valid_points_mask)\n\n        self._xyz = optimizable_tensors[\"xyz\"]\n        self._features_dc = optimizable_tensors[\"f_dc\"]\n        self._features_rest = optimizable_tensors[\"f_rest\"]\n        self._opacity = optimizable_tensors[\"opacity\"]\n        self._scaling = optimizable_tensors[\"scaling\"]\n        self._rotation = optimizable_tensors[\"rotation\"]\n\n        self.xyz_gradient_accum = self.xyz_gradient_accum[valid_points_mask]\n\n        self.denom = self.denom[valid_points_mask]\n        self.max_radii2D = self.max_radii2D[valid_points_mask]\n\n    def cat_tensors_to_optimizer(self, tensors_dict):\n        optimizable_tensors = {}\n        for group in self.optimizer.param_groups:\n            assert len(group[\"params\"]) == 1\n            extension_tensor = tensors_dict[group[\"name\"]]\n            stored_state = self.optimizer.state.get(group['params'][0], None)\n            if stored_state is not None:\n\n                stored_state[\"exp_avg\"] = torch.cat((stored_state[\"exp_avg\"], torch.zeros_like(extension_tensor)), dim=0)\n                stored_state[\"exp_avg_sq\"] = torch.cat((stored_state[\"exp_avg_sq\"], torch.zeros_like(extension_tensor)), dim=0)\n\n                del self.optimizer.state[group['params'][0]]\n                group[\"params\"][0] = nn.Parameter(torch.cat((group[\"params\"][0], extension_tensor), dim=0).requires_grad_(True))\n                self.optimizer.state[group['params'][0]] = stored_state\n\n                optimizable_tensors[group[\"name\"]] = group[\"params\"][0]\n            else:\n                group[\"params\"][0] = nn.Parameter(torch.cat((group[\"params\"][0], extension_tensor), dim=0).requires_grad_(True))\n                optimizable_tensors[group[\"name\"]] = group[\"params\"][0]\n\n        return optimizable_tensors\n\n    def densification_postfix(self, new_xyz, new_features_dc, new_features_rest, new_opacities, new_scaling, new_rotation):\n        d = {\"xyz\": new_xyz,\n        \"f_dc\": new_features_dc,\n        \"f_rest\": new_features_rest,\n        \"opacity\": new_opacities,\n        \"scaling\" : new_scaling,\n        \"rotation\" : new_rotation}\n\n        optimizable_tensors = self.cat_tensors_to_optimizer(d)\n        self._xyz = optimizable_tensors[\"xyz\"]\n        self._features_dc = optimizable_tensors[\"f_dc\"]\n        self._features_rest = optimizable_tensors[\"f_rest\"]\n        self._opacity = optimizable_tensors[\"opacity\"]\n        self._scaling = optimizable_tensors[\"scaling\"]\n        self._rotation = optimizable_tensors[\"rotation\"]\n\n        self.xyz_gradient_accum = torch.zeros((self.get_xyz.shape[0], 1), device=\"cuda\")\n        self.denom = torch.zeros((self.get_xyz.shape[0], 1), device=\"cuda\")\n        self.max_radii2D = torch.zeros((self.get_xyz.shape[0]), device=\"cuda\")\n\n    def densify_and_split(self, grads, grad_threshold, scene_extent, N=2):\n        n_init_points = self.get_xyz.shape[0]\n        # Extract points that satisfy the gradient condition\n        padded_grad = torch.zeros((n_init_points), device=\"cuda\")\n        padded_grad[:grads.shape[0]] = grads.squeeze()\n        selected_pts_mask = torch.where(padded_grad >= grad_threshold, True, False)\n        selected_pts_mask = torch.logical_and(selected_pts_mask,\n            torch.max(self.get_scaling, dim=1).values > self.percent_dense*scene_extent\n        )\n\n        stds = self.get_scaling[selected_pts_mask].repeat(N,1)\n        means =torch.zeros((stds.size(0), 3),device=\"cuda\")\n        samples = torch.normal(mean=means, std=stds)\n        rots = build_rotation(self._rotation[selected_pts_mask]).repeat(N,1,1)\n        new_xyz = torch.bmm(rots, samples.unsqueeze(-1)).squeeze(-1) + self.get_xyz[selected_pts_mask].repeat(N, 1)\n        new_scaling = self.scaling_inverse_activation(self.get_scaling[selected_pts_mask].repeat(N,1) / (0.8*N))\n        new_rotation = self._rotation[selected_pts_mask].repeat(N,1)\n        new_features_dc = self._features_dc[selected_pts_mask].repeat(N,1,1)\n        new_features_rest = self._features_rest[selected_pts_mask].repeat(N,1,1)\n        new_opacity = self._opacity[selected_pts_mask].repeat(N,1)\n\n        self.densification_postfix(new_xyz, new_features_dc, new_features_rest, new_opacity, new_scaling, new_rotation)\n\n        prune_filter = torch.cat((selected_pts_mask, torch.zeros(N * selected_pts_mask.sum(), device=\"cuda\", dtype=bool)))\n        self.prune_points(prune_filter)\n\n    def densify_and_clone(self, grads, grad_threshold, scene_extent):\n        # Extract points that satisfy the gradient condition\n        selected_pts_mask = torch.where(torch.norm(grads, dim=-1) >= grad_threshold, True, False)\n        selected_pts_mask = torch.logical_and(selected_pts_mask,\n            torch.max(self.get_scaling, dim=1).values <= self.percent_dense*scene_extent\n        )\n        \n        new_xyz = self._xyz[selected_pts_mask]\n        new_features_dc = self._features_dc[selected_pts_mask]\n        new_features_rest = self._features_rest[selected_pts_mask]\n        new_opacities = self._opacity[selected_pts_mask]\n        new_scaling = self._scaling[selected_pts_mask]\n        new_rotation = self._rotation[selected_pts_mask]\n\n        self.densification_postfix(new_xyz, new_features_dc, new_features_rest, new_opacities, new_scaling, new_rotation)\n\n    def densify_and_prune(self, max_grad, min_opacity, extent, max_screen_size):\n        grads = self.xyz_gradient_accum / self.denom\n        grads[grads.isnan()] = 0.0\n\n        self.densify_and_clone(grads, max_grad, extent)\n        self.densify_and_split(grads, max_grad, extent)\n\n        prune_mask = (self.get_opacity < min_opacity).squeeze()\n        if max_screen_size:\n            big_points_vs = self.max_radii2D > max_screen_size\n            big_points_ws = self.get_scaling.max(dim=1).values > 0.1 * extent\n            prune_mask = torch.logical_or(torch.logical_or(prune_mask, big_points_vs), big_points_ws)\n        self.prune_points(prune_mask)\n\n        torch.cuda.empty_cache()\n\n    def prune(self, min_opacity, extent, max_screen_size):\n\n        prune_mask = (self.get_opacity < min_opacity).squeeze()\n        if max_screen_size:\n            big_points_vs = self.max_radii2D > max_screen_size\n            big_points_ws = self.get_scaling.max(dim=1).values > 0.1 * extent\n            prune_mask = torch.logical_or(torch.logical_or(prune_mask, big_points_vs), big_points_ws)\n        self.prune_points(prune_mask)\n\n        torch.cuda.empty_cache()\n\n\n    def add_densification_stats(self, viewspace_point_tensor, update_filter):\n        self.xyz_gradient_accum[update_filter] += torch.norm(viewspace_point_tensor.grad[update_filter,:2], dim=-1, keepdim=True)\n        self.denom[update_filter] += 1\n\ndef getProjectionMatrix(znear, zfar, fovX, fovY):\n    tanHalfFovY = math.tan((fovY / 2))\n    tanHalfFovX = math.tan((fovX / 2))\n\n    P = torch.zeros(4, 4)\n\n    z_sign = 1.0\n\n    P[0, 0] = 1 / tanHalfFovX\n    P[1, 1] = 1 / tanHalfFovY\n    P[3, 2] = z_sign\n    P[2, 2] = z_sign * zfar / (zfar - znear)\n    P[2, 3] = -(zfar * znear) / (zfar - znear)\n    return P\n\n\nclass MiniCam:\n    def __init__(self, c2w, width, height, fovy, fovx, znear, zfar):\n        # c2w (pose) should be in NeRF convention.\n\n        self.image_width = width\n        self.image_height = height\n        self.FoVy = fovy\n        self.FoVx = fovx\n        self.znear = znear\n        self.zfar = zfar\n\n        w2c = np.linalg.inv(c2w)\n\n        # rectify...\n        w2c[1:3, :3] *= -1\n        w2c[:3, 3] *= -1\n\n        self.world_view_transform = torch.tensor(w2c).transpose(0, 1).cuda()\n        self.projection_matrix = (\n            getProjectionMatrix(\n                znear=self.znear, zfar=self.zfar, fovX=self.FoVx, fovY=self.FoVy\n            )\n            .transpose(0, 1)\n            .cuda()\n        )\n        self.full_proj_transform = self.world_view_transform @ self.projection_matrix\n        self.camera_center = -torch.tensor(c2w[:3, 3]).cuda()\n\n\nclass Renderer:\n    def __init__(self, sh_degree=3, white_background=True, radius=1):\n        \n        self.sh_degree = sh_degree\n        self.white_background = white_background\n        self.radius = radius\n\n        self.gaussians = GaussianModel(sh_degree)\n\n        self.bg_color = torch.tensor(\n            [1, 1, 1] if white_background else [0, 0, 0],\n            dtype=torch.float32,\n            device=\"cuda\",\n        )\n    \n    def initialize(self, input=None, num_pts=5000, radius=0.5):\n        # load checkpoint\n        if input is None:\n            # init from random point cloud\n            \n            phis = np.random.random((num_pts,)) * 2 * np.pi\n            costheta = np.random.random((num_pts,)) * 2 - 1\n            thetas = np.arccos(costheta)\n            mu = np.random.random((num_pts,))\n            radius = radius * np.cbrt(mu)\n            x = radius * np.sin(thetas) * np.cos(phis)\n            y = radius * np.sin(thetas) * np.sin(phis)\n            z = radius * np.cos(thetas)\n            xyz = np.stack((x, y, z), axis=1)\n            # xyz = np.random.random((num_pts, 3)) * 2.6 - 1.3\n\n            shs = np.random.random((num_pts, 3)) / 255.0\n            pcd = BasicPointCloud(\n                points=xyz, colors=SH2RGB(shs), normals=np.zeros((num_pts, 3))\n            )\n            self.gaussians.create_from_pcd(pcd, 10)\n        elif isinstance(input, BasicPointCloud):\n            # load from a provided pcd\n            self.gaussians.create_from_pcd(input, 1)\n        else:\n            # load from saved ply\n            self.gaussians.load_ply(input)\n\n    def render(\n        self,\n        viewpoint_camera,\n        scaling_modifier=1.0,\n        bg_color=None,\n        override_color=None,\n        compute_cov3D_python=False,\n        convert_SHs_python=False,\n    ):\n        # Create zero tensor. We will use it to make pytorch return gradients of the 2D (screen-space) means\n        screenspace_points = (\n            torch.zeros_like(\n                self.gaussians.get_xyz,\n                dtype=self.gaussians.get_xyz.dtype,\n                requires_grad=True,\n                device=\"cuda\",\n            )\n            + 0\n        )\n        try:\n            screenspace_points.retain_grad()\n        except:\n            pass\n\n        # Set up rasterization configuration\n        tanfovx = math.tan(viewpoint_camera.FoVx * 0.5)\n        tanfovy = math.tan(viewpoint_camera.FoVy * 0.5)\n\n        raster_settings = GaussianRasterizationSettings(\n            image_height=int(viewpoint_camera.image_height),\n            image_width=int(viewpoint_camera.image_width),\n            tanfovx=tanfovx,\n            tanfovy=tanfovy,\n            bg=self.bg_color if bg_color is None else bg_color,\n            scale_modifier=scaling_modifier,\n            viewmatrix=viewpoint_camera.world_view_transform,\n            projmatrix=viewpoint_camera.full_proj_transform,\n            sh_degree=self.gaussians.active_sh_degree,\n            campos=viewpoint_camera.camera_center,\n            prefiltered=False,\n            debug=False,\n        )\n\n        rasterizer = GaussianRasterizer(raster_settings=raster_settings)\n\n        means3D = self.gaussians.get_xyz\n        means2D = screenspace_points\n        opacity = self.gaussians.get_opacity\n\n        # If precomputed 3d covariance is provided, use it. If not, then it will be computed from\n        # scaling / rotation by the rasterizer.\n        scales = None\n        rotations = None\n        cov3D_precomp = None\n        if compute_cov3D_python:\n            cov3D_precomp = self.gaussians.get_covariance(scaling_modifier)\n        else:\n            scales = self.gaussians.get_scaling\n            rotations = self.gaussians.get_rotation\n\n        # If precomputed colors are provided, use them. Otherwise, if it is desired to precompute colors\n        # from SHs in Python, do it. If not, then SH -> RGB conversion will be done by rasterizer.\n        shs = None\n        colors_precomp = None\n        if colors_precomp is None:\n            if convert_SHs_python:\n                shs_view = self.gaussians.get_features.transpose(1, 2).view(\n                    -1, 3, (self.gaussians.max_sh_degree + 1) ** 2\n                )\n                dir_pp = self.gaussians.get_xyz - viewpoint_camera.camera_center.repeat(\n                    self.gaussians.get_features.shape[0], 1\n                )\n                dir_pp_normalized = dir_pp / dir_pp.norm(dim=1, keepdim=True)\n                sh2rgb = eval_sh(\n                    self.gaussians.active_sh_degree, shs_view, dir_pp_normalized\n                )\n                colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)\n            else:\n                shs = self.gaussians.get_features\n        else:\n            colors_precomp = override_color\n\n        # Rasterize visible Gaussians to image, obtain their radii (on screen).\n        rendered_image, radii, rendered_depth, rendered_alpha = rasterizer(\n            means3D=means3D,\n            means2D=means2D,\n            shs=shs,\n            colors_precomp=colors_precomp,\n            opacities=opacity,\n            scales=scales,\n            rotations=rotations,\n            cov3D_precomp=cov3D_precomp,\n        )\n\n        rendered_image = rendered_image.clamp(0, 1)\n\n        # Those Gaussians that were frustum culled or had a radius of 0 were not visible.\n        # They will be excluded from value updates used in the splitting criteria.\n        return {\n            \"image\": rendered_image,\n            \"depth\": rendered_depth,\n            \"alpha\": rendered_alpha,\n            \"viewspace_points\": screenspace_points,\n            \"visibility_filter\": radii > 0,\n            \"radii\": radii,\n        }\n"
        },
        {
          "name": "guidance",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 33.61328125,
          "content": "import os\nimport cv2\nimport time\nimport tqdm\nimport numpy as np\nimport dearpygui.dearpygui as dpg\n\nimport torch\nimport torch.nn.functional as F\n\nimport rembg\n\nfrom cam_utils import orbit_camera, OrbitCamera\nfrom gs_renderer import Renderer, MiniCam\n\nfrom grid_put import mipmap_linear_grid_put_2d\nfrom mesh import Mesh, safe_normalize\n\nclass GUI:\n    def __init__(self, opt):\n        self.opt = opt  # shared with the trainer's opt to support in-place modification of rendering parameters.\n        self.gui = opt.gui # enable gui\n        self.W = opt.W\n        self.H = opt.H\n        self.cam = OrbitCamera(opt.W, opt.H, r=opt.radius, fovy=opt.fovy)\n\n        self.mode = \"image\"\n        self.seed = \"random\"\n\n        self.buffer_image = np.ones((self.W, self.H, 3), dtype=np.float32)\n        self.need_update = True  # update buffer_image\n\n        # models\n        self.device = torch.device(\"cuda\")\n        self.bg_remover = None\n\n        self.guidance_sd = None\n        self.guidance_zero123 = None\n\n        self.enable_sd = False\n        self.enable_zero123 = False\n\n        # renderer\n        self.renderer = Renderer(sh_degree=self.opt.sh_degree)\n        self.gaussain_scale_factor = 1\n\n        # input image\n        self.input_img = None\n        self.input_mask = None\n        self.input_img_torch = None\n        self.input_mask_torch = None\n        self.overlay_input_img = False\n        self.overlay_input_img_ratio = 0.5\n\n        # input text\n        self.prompt = \"\"\n        self.negative_prompt = \"\"\n\n        # training stuff\n        self.training = False\n        self.optimizer = None\n        self.step = 0\n        self.train_steps = 1  # steps per rendering loop\n        \n        # load input data from cmdline\n        if self.opt.input is not None:\n            self.load_input(self.opt.input)\n        \n        # override prompt from cmdline\n        if self.opt.prompt is not None:\n            self.prompt = self.opt.prompt\n        if self.opt.negative_prompt is not None:\n            self.negative_prompt = self.opt.negative_prompt\n\n        # override if provide a checkpoint\n        if self.opt.load is not None:\n            self.renderer.initialize(self.opt.load)            \n        else:\n            # initialize gaussians to a blob\n            self.renderer.initialize(num_pts=self.opt.num_pts)\n\n        if self.gui:\n            dpg.create_context()\n            self.register_dpg()\n            self.test_step()\n\n    def __del__(self):\n        if self.gui:\n            dpg.destroy_context()\n\n    def seed_everything(self):\n        try:\n            seed = int(self.seed)\n        except:\n            seed = np.random.randint(0, 1000000)\n\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\n        self.last_seed = seed\n\n    def prepare_train(self):\n\n        self.step = 0\n\n        # setup training\n        self.renderer.gaussians.training_setup(self.opt)\n        # do not do progressive sh-level\n        self.renderer.gaussians.active_sh_degree = self.renderer.gaussians.max_sh_degree\n        self.optimizer = self.renderer.gaussians.optimizer\n\n        # default camera\n        if self.opt.mvdream or self.opt.imagedream:\n            # the second view is the front view for mvdream/imagedream.\n            pose = orbit_camera(self.opt.elevation, 90, self.opt.radius)\n        else:\n            pose = orbit_camera(self.opt.elevation, 0, self.opt.radius)\n        self.fixed_cam = MiniCam(\n            pose,\n            self.opt.ref_size,\n            self.opt.ref_size,\n            self.cam.fovy,\n            self.cam.fovx,\n            self.cam.near,\n            self.cam.far,\n        )\n\n        self.enable_sd = self.opt.lambda_sd > 0 and self.prompt != \"\"\n        self.enable_zero123 = self.opt.lambda_zero123 > 0 and self.input_img is not None\n\n        # lazy load guidance model\n        if self.guidance_sd is None and self.enable_sd:\n            if self.opt.mvdream:\n                print(f\"[INFO] loading MVDream...\")\n                from guidance.mvdream_utils import MVDream\n                self.guidance_sd = MVDream(self.device)\n                print(f\"[INFO] loaded MVDream!\")\n            elif self.opt.imagedream:\n                print(f\"[INFO] loading ImageDream...\")\n                from guidance.imagedream_utils import ImageDream\n                self.guidance_sd = ImageDream(self.device)\n                print(f\"[INFO] loaded ImageDream!\")\n            else:\n                print(f\"[INFO] loading SD...\")\n                from guidance.sd_utils import StableDiffusion\n                self.guidance_sd = StableDiffusion(self.device)\n                print(f\"[INFO] loaded SD!\")\n\n        if self.guidance_zero123 is None and self.enable_zero123:\n            print(f\"[INFO] loading zero123...\")\n            from guidance.zero123_utils import Zero123\n            if self.opt.stable_zero123:\n                self.guidance_zero123 = Zero123(self.device, model_key='ashawkey/stable-zero123-diffusers')\n            else:\n                self.guidance_zero123 = Zero123(self.device, model_key='ashawkey/zero123-xl-diffusers')\n            print(f\"[INFO] loaded zero123!\")\n\n        # input image\n        if self.input_img is not None:\n            self.input_img_torch = torch.from_numpy(self.input_img).permute(2, 0, 1).unsqueeze(0).to(self.device)\n            self.input_img_torch = F.interpolate(self.input_img_torch, (self.opt.ref_size, self.opt.ref_size), mode=\"bilinear\", align_corners=False)\n\n            self.input_mask_torch = torch.from_numpy(self.input_mask).permute(2, 0, 1).unsqueeze(0).to(self.device)\n            self.input_mask_torch = F.interpolate(self.input_mask_torch, (self.opt.ref_size, self.opt.ref_size), mode=\"bilinear\", align_corners=False)\n\n        # prepare embeddings\n        with torch.no_grad():\n\n            if self.enable_sd:\n                if self.opt.imagedream:\n                    self.guidance_sd.get_image_text_embeds(self.input_img_torch, [self.prompt], [self.negative_prompt])\n                else:\n                    self.guidance_sd.get_text_embeds([self.prompt], [self.negative_prompt])\n\n            if self.enable_zero123:\n                self.guidance_zero123.get_img_embeds(self.input_img_torch)\n\n    def train_step(self):\n        starter = torch.cuda.Event(enable_timing=True)\n        ender = torch.cuda.Event(enable_timing=True)\n        starter.record()\n\n        for _ in range(self.train_steps):\n\n            self.step += 1\n            step_ratio = min(1, self.step / self.opt.iters)\n\n            # update lr\n            self.renderer.gaussians.update_learning_rate(self.step)\n\n            loss = 0\n\n            ### known view\n            if self.input_img_torch is not None and not self.opt.imagedream:\n                cur_cam = self.fixed_cam\n                out = self.renderer.render(cur_cam)\n\n                # rgb loss\n                image = out[\"image\"].unsqueeze(0) # [1, 3, H, W] in [0, 1]\n                loss = loss + 10000 * (step_ratio if self.opt.warmup_rgb_loss else 1) * F.mse_loss(image, self.input_img_torch)\n\n                # mask loss\n                mask = out[\"alpha\"].unsqueeze(0) # [1, 1, H, W] in [0, 1]\n                loss = loss + 1000 * (step_ratio if self.opt.warmup_rgb_loss else 1) * F.mse_loss(mask, self.input_mask_torch)\n\n            ### novel view (manual batch)\n            render_resolution = 128 if step_ratio < 0.3 else (256 if step_ratio < 0.6 else 512)\n            images = []\n            poses = []\n            vers, hors, radii = [], [], []\n            # avoid too large elevation (> 80 or < -80), and make sure it always cover [min_ver, max_ver]\n            min_ver = max(min(self.opt.min_ver, self.opt.min_ver - self.opt.elevation), -80 - self.opt.elevation)\n            max_ver = min(max(self.opt.max_ver, self.opt.max_ver - self.opt.elevation), 80 - self.opt.elevation)\n\n            for _ in range(self.opt.batch_size):\n\n                # render random view\n                ver = np.random.randint(min_ver, max_ver)\n                hor = np.random.randint(-180, 180)\n                radius = 0\n\n                vers.append(ver)\n                hors.append(hor)\n                radii.append(radius)\n\n                pose = orbit_camera(self.opt.elevation + ver, hor, self.opt.radius + radius)\n                poses.append(pose)\n\n                cur_cam = MiniCam(pose, render_resolution, render_resolution, self.cam.fovy, self.cam.fovx, self.cam.near, self.cam.far)\n\n                bg_color = torch.tensor([1, 1, 1] if np.random.rand() > self.opt.invert_bg_prob else [0, 0, 0], dtype=torch.float32, device=\"cuda\")\n                out = self.renderer.render(cur_cam, bg_color=bg_color)\n\n                image = out[\"image\"].unsqueeze(0) # [1, 3, H, W] in [0, 1]\n                images.append(image)\n\n                # enable mvdream training\n                if self.opt.mvdream or self.opt.imagedream:\n                    for view_i in range(1, 4):\n                        pose_i = orbit_camera(self.opt.elevation + ver, hor + 90 * view_i, self.opt.radius + radius)\n                        poses.append(pose_i)\n\n                        cur_cam_i = MiniCam(pose_i, render_resolution, render_resolution, self.cam.fovy, self.cam.fovx, self.cam.near, self.cam.far)\n\n                        # bg_color = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32, device=\"cuda\")\n                        out_i = self.renderer.render(cur_cam_i, bg_color=bg_color)\n\n                        image = out_i[\"image\"].unsqueeze(0) # [1, 3, H, W] in [0, 1]\n                        images.append(image)\n                    \n            images = torch.cat(images, dim=0)\n            poses = torch.from_numpy(np.stack(poses, axis=0)).to(self.device)\n\n            # import kiui\n            # print(hor, ver)\n            # kiui.vis.plot_image(images)\n\n            # guidance loss\n            if self.enable_sd:\n                if self.opt.mvdream or self.opt.imagedream:\n                    loss = loss + self.opt.lambda_sd * self.guidance_sd.train_step(images, poses, step_ratio=step_ratio if self.opt.anneal_timestep else None)\n                else:\n                    loss = loss + self.opt.lambda_sd * self.guidance_sd.train_step(images, step_ratio=step_ratio if self.opt.anneal_timestep else None)\n\n            if self.enable_zero123:\n                loss = loss + self.opt.lambda_zero123 * self.guidance_zero123.train_step(images, vers, hors, radii, step_ratio=step_ratio if self.opt.anneal_timestep else None, default_elevation=self.opt.elevation)\n            \n            # optimize step\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            # densify and prune\n            if self.step >= self.opt.density_start_iter and self.step <= self.opt.density_end_iter:\n                viewspace_point_tensor, visibility_filter, radii = out[\"viewspace_points\"], out[\"visibility_filter\"], out[\"radii\"]\n                self.renderer.gaussians.max_radii2D[visibility_filter] = torch.max(self.renderer.gaussians.max_radii2D[visibility_filter], radii[visibility_filter])\n                self.renderer.gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)\n\n                if self.step % self.opt.densification_interval == 0:\n                    self.renderer.gaussians.densify_and_prune(self.opt.densify_grad_threshold, min_opacity=0.01, extent=4, max_screen_size=1)\n                \n                if self.step % self.opt.opacity_reset_interval == 0:\n                    self.renderer.gaussians.reset_opacity()\n\n        ender.record()\n        torch.cuda.synchronize()\n        t = starter.elapsed_time(ender)\n\n        self.need_update = True\n\n        if self.gui:\n            dpg.set_value(\"_log_train_time\", f\"{t:.4f}ms\")\n            dpg.set_value(\n                \"_log_train_log\",\n                f\"step = {self.step: 5d} (+{self.train_steps: 2d}) loss = {loss.item():.4f}\",\n            )\n\n        # dynamic train steps (no need for now)\n        # max allowed train time per-frame is 500 ms\n        # full_t = t / self.train_steps * 16\n        # train_steps = min(16, max(4, int(16 * 500 / full_t)))\n        # if train_steps > self.train_steps * 1.2 or train_steps < self.train_steps * 0.8:\n        #     self.train_steps = train_steps\n\n    @torch.no_grad()\n    def test_step(self):\n        # ignore if no need to update\n        if not self.need_update:\n            return\n\n        starter = torch.cuda.Event(enable_timing=True)\n        ender = torch.cuda.Event(enable_timing=True)\n        starter.record()\n\n        # should update image\n        if self.need_update:\n            # render image\n\n            cur_cam = MiniCam(\n                self.cam.pose,\n                self.W,\n                self.H,\n                self.cam.fovy,\n                self.cam.fovx,\n                self.cam.near,\n                self.cam.far,\n            )\n\n            out = self.renderer.render(cur_cam, self.gaussain_scale_factor)\n\n            buffer_image = out[self.mode]  # [3, H, W]\n\n            if self.mode in ['depth', 'alpha']:\n                buffer_image = buffer_image.repeat(3, 1, 1)\n                if self.mode == 'depth':\n                    buffer_image = (buffer_image - buffer_image.min()) / (buffer_image.max() - buffer_image.min() + 1e-20)\n\n            buffer_image = F.interpolate(\n                buffer_image.unsqueeze(0),\n                size=(self.H, self.W),\n                mode=\"bilinear\",\n                align_corners=False,\n            ).squeeze(0)\n\n            self.buffer_image = (\n                buffer_image.permute(1, 2, 0)\n                .contiguous()\n                .clamp(0, 1)\n                .contiguous()\n                .detach()\n                .cpu()\n                .numpy()\n            )\n\n            # display input_image\n            if self.overlay_input_img and self.input_img is not None:\n                self.buffer_image = (\n                    self.buffer_image * (1 - self.overlay_input_img_ratio)\n                    + self.input_img * self.overlay_input_img_ratio\n                )\n\n            self.need_update = False\n\n        ender.record()\n        torch.cuda.synchronize()\n        t = starter.elapsed_time(ender)\n\n        if self.gui:\n            dpg.set_value(\"_log_infer_time\", f\"{t:.4f}ms ({int(1000/t)} FPS)\")\n            dpg.set_value(\n                \"_texture\", self.buffer_image\n            )  # buffer must be contiguous, else seg fault!\n\n    \n    def load_input(self, file):\n        # load image\n        print(f'[INFO] load image from {file}...')\n        img = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n        if img.shape[-1] == 3:\n            if self.bg_remover is None:\n                self.bg_remover = rembg.new_session()\n            img = rembg.remove(img, session=self.bg_remover)\n\n        img = cv2.resize(img, (self.W, self.H), interpolation=cv2.INTER_AREA)\n        img = img.astype(np.float32) / 255.0\n\n        self.input_mask = img[..., 3:]\n        # white bg\n        self.input_img = img[..., :3] * self.input_mask + (1 - self.input_mask)\n        # bgr to rgb\n        self.input_img = self.input_img[..., ::-1].copy()\n\n        # load prompt\n        file_prompt = file.replace(\"_rgba.png\", \"_caption.txt\")\n        if os.path.exists(file_prompt):\n            print(f'[INFO] load prompt from {file_prompt}...')\n            with open(file_prompt, \"r\") as f:\n                self.prompt = f.read().strip()\n\n    @torch.no_grad()\n    def save_model(self, mode='geo', texture_size=1024):\n        os.makedirs(self.opt.outdir, exist_ok=True)\n        if mode == 'geo':\n            path = os.path.join(self.opt.outdir, self.opt.save_path + '_mesh.ply')\n            mesh = self.renderer.gaussians.extract_mesh(path, self.opt.density_thresh)\n            mesh.write_ply(path)\n\n        elif mode == 'geo+tex':\n            path = os.path.join(self.opt.outdir, self.opt.save_path + '_mesh.' + self.opt.mesh_format)\n            mesh = self.renderer.gaussians.extract_mesh(path, self.opt.density_thresh)\n\n            # perform texture extraction\n            print(f\"[INFO] unwrap uv...\")\n            h = w = texture_size\n            mesh.auto_uv()\n            mesh.auto_normal()\n\n            albedo = torch.zeros((h, w, 3), device=self.device, dtype=torch.float32)\n            cnt = torch.zeros((h, w, 1), device=self.device, dtype=torch.float32)\n\n            # self.prepare_train() # tmp fix for not loading 0123\n            # vers = [0]\n            # hors = [0]\n            vers = [0] * 8 + [-45] * 8 + [45] * 8 + [-89.9, 89.9]\n            hors = [0, 45, -45, 90, -90, 135, -135, 180] * 3 + [0, 0]\n\n            render_resolution = 512\n\n            import nvdiffrast.torch as dr\n\n            if not self.opt.force_cuda_rast and (not self.opt.gui or os.name == 'nt'):\n                glctx = dr.RasterizeGLContext()\n            else:\n                glctx = dr.RasterizeCudaContext()\n\n            for ver, hor in zip(vers, hors):\n                # render image\n                pose = orbit_camera(ver, hor, self.cam.radius)\n\n                cur_cam = MiniCam(\n                    pose,\n                    render_resolution,\n                    render_resolution,\n                    self.cam.fovy,\n                    self.cam.fovx,\n                    self.cam.near,\n                    self.cam.far,\n                )\n                \n                cur_out = self.renderer.render(cur_cam)\n\n                rgbs = cur_out[\"image\"].unsqueeze(0) # [1, 3, H, W] in [0, 1]\n\n                # enhance texture quality with zero123 [not working well]\n                # if self.opt.guidance_model == 'zero123':\n                #     rgbs = self.guidance.refine(rgbs, [ver], [hor], [0])\n                    # import kiui\n                    # kiui.vis.plot_image(rgbs)\n                    \n                # get coordinate in texture image\n                pose = torch.from_numpy(pose.astype(np.float32)).to(self.device)\n                proj = torch.from_numpy(self.cam.perspective.astype(np.float32)).to(self.device)\n\n                v_cam = torch.matmul(F.pad(mesh.v, pad=(0, 1), mode='constant', value=1.0), torch.inverse(pose).T).float().unsqueeze(0)\n                v_clip = v_cam @ proj.T\n                rast, rast_db = dr.rasterize(glctx, v_clip, mesh.f, (render_resolution, render_resolution))\n\n                depth, _ = dr.interpolate(-v_cam[..., [2]], rast, mesh.f) # [1, H, W, 1]\n                depth = depth.squeeze(0) # [H, W, 1]\n\n                alpha = (rast[0, ..., 3:] > 0).float()\n\n                uvs, _ = dr.interpolate(mesh.vt.unsqueeze(0), rast, mesh.ft)  # [1, 512, 512, 2] in [0, 1]\n\n                # use normal to produce a back-project mask\n                normal, _ = dr.interpolate(mesh.vn.unsqueeze(0).contiguous(), rast, mesh.fn)\n                normal = safe_normalize(normal[0])\n\n                # rotated normal (where [0, 0, 1] always faces camera)\n                rot_normal = normal @ pose[:3, :3]\n                viewcos = rot_normal[..., [2]]\n\n                mask = (alpha > 0) & (viewcos > 0.5)  # [H, W, 1]\n                mask = mask.view(-1)\n\n                uvs = uvs.view(-1, 2).clamp(0, 1)[mask]\n                rgbs = rgbs.view(3, -1).permute(1, 0)[mask].contiguous()\n                \n                # update texture image\n                cur_albedo, cur_cnt = mipmap_linear_grid_put_2d(\n                    h, w,\n                    uvs[..., [1, 0]] * 2 - 1,\n                    rgbs,\n                    min_resolution=256,\n                    return_count=True,\n                )\n                \n                # albedo += cur_albedo\n                # cnt += cur_cnt\n                mask = cnt.squeeze(-1) < 0.1\n                albedo[mask] += cur_albedo[mask]\n                cnt[mask] += cur_cnt[mask]\n\n            mask = cnt.squeeze(-1) > 0\n            albedo[mask] = albedo[mask] / cnt[mask].repeat(1, 3)\n\n            mask = mask.view(h, w)\n\n            albedo = albedo.detach().cpu().numpy()\n            mask = mask.detach().cpu().numpy()\n\n            # dilate texture\n            from sklearn.neighbors import NearestNeighbors\n            from scipy.ndimage import binary_dilation, binary_erosion\n\n            inpaint_region = binary_dilation(mask, iterations=32)\n            inpaint_region[mask] = 0\n\n            search_region = mask.copy()\n            not_search_region = binary_erosion(search_region, iterations=3)\n            search_region[not_search_region] = 0\n\n            search_coords = np.stack(np.nonzero(search_region), axis=-1)\n            inpaint_coords = np.stack(np.nonzero(inpaint_region), axis=-1)\n\n            knn = NearestNeighbors(n_neighbors=1, algorithm=\"kd_tree\").fit(\n                search_coords\n            )\n            _, indices = knn.kneighbors(inpaint_coords)\n\n            albedo[tuple(inpaint_coords.T)] = albedo[tuple(search_coords[indices[:, 0]].T)]\n\n            mesh.albedo = torch.from_numpy(albedo).to(self.device)\n            mesh.write(path)\n\n        else:\n            path = os.path.join(self.opt.outdir, self.opt.save_path + '_model.ply')\n            self.renderer.gaussians.save_ply(path)\n\n        print(f\"[INFO] save model to {path}.\")\n\n    def register_dpg(self):\n        ### register texture\n\n        with dpg.texture_registry(show=False):\n            dpg.add_raw_texture(\n                self.W,\n                self.H,\n                self.buffer_image,\n                format=dpg.mvFormat_Float_rgb,\n                tag=\"_texture\",\n            )\n\n        ### register window\n\n        # the rendered image, as the primary window\n        with dpg.window(\n            tag=\"_primary_window\",\n            width=self.W,\n            height=self.H,\n            pos=[0, 0],\n            no_move=True,\n            no_title_bar=True,\n            no_scrollbar=True,\n        ):\n            # add the texture\n            dpg.add_image(\"_texture\")\n\n        # dpg.set_primary_window(\"_primary_window\", True)\n\n        # control window\n        with dpg.window(\n            label=\"Control\",\n            tag=\"_control_window\",\n            width=600,\n            height=self.H,\n            pos=[self.W, 0],\n            no_move=True,\n            no_title_bar=True,\n        ):\n            # button theme\n            with dpg.theme() as theme_button:\n                with dpg.theme_component(dpg.mvButton):\n                    dpg.add_theme_color(dpg.mvThemeCol_Button, (23, 3, 18))\n                    dpg.add_theme_color(dpg.mvThemeCol_ButtonHovered, (51, 3, 47))\n                    dpg.add_theme_color(dpg.mvThemeCol_ButtonActive, (83, 18, 83))\n                    dpg.add_theme_style(dpg.mvStyleVar_FrameRounding, 5)\n                    dpg.add_theme_style(dpg.mvStyleVar_FramePadding, 3, 3)\n\n            # timer stuff\n            with dpg.group(horizontal=True):\n                dpg.add_text(\"Infer time: \")\n                dpg.add_text(\"no data\", tag=\"_log_infer_time\")\n\n            def callback_setattr(sender, app_data, user_data):\n                setattr(self, user_data, app_data)\n\n            # init stuff\n            with dpg.collapsing_header(label=\"Initialize\", default_open=True):\n\n                # seed stuff\n                def callback_set_seed(sender, app_data):\n                    self.seed = app_data\n                    self.seed_everything()\n\n                dpg.add_input_text(\n                    label=\"seed\",\n                    default_value=self.seed,\n                    on_enter=True,\n                    callback=callback_set_seed,\n                )\n\n                # input stuff\n                def callback_select_input(sender, app_data):\n                    # only one item\n                    for k, v in app_data[\"selections\"].items():\n                        dpg.set_value(\"_log_input\", k)\n                        self.load_input(v)\n\n                    self.need_update = True\n\n                with dpg.file_dialog(\n                    directory_selector=False,\n                    show=False,\n                    callback=callback_select_input,\n                    file_count=1,\n                    tag=\"file_dialog_tag\",\n                    width=700,\n                    height=400,\n                ):\n                    dpg.add_file_extension(\"Images{.jpg,.jpeg,.png}\")\n\n                with dpg.group(horizontal=True):\n                    dpg.add_button(\n                        label=\"input\",\n                        callback=lambda: dpg.show_item(\"file_dialog_tag\"),\n                    )\n                    dpg.add_text(\"\", tag=\"_log_input\")\n                \n                # overlay stuff\n                with dpg.group(horizontal=True):\n\n                    def callback_toggle_overlay_input_img(sender, app_data):\n                        self.overlay_input_img = not self.overlay_input_img\n                        self.need_update = True\n\n                    dpg.add_checkbox(\n                        label=\"overlay image\",\n                        default_value=self.overlay_input_img,\n                        callback=callback_toggle_overlay_input_img,\n                    )\n\n                    def callback_set_overlay_input_img_ratio(sender, app_data):\n                        self.overlay_input_img_ratio = app_data\n                        self.need_update = True\n\n                    dpg.add_slider_float(\n                        label=\"ratio\",\n                        min_value=0,\n                        max_value=1,\n                        format=\"%.1f\",\n                        default_value=self.overlay_input_img_ratio,\n                        callback=callback_set_overlay_input_img_ratio,\n                    )\n\n                # prompt stuff\n            \n                dpg.add_input_text(\n                    label=\"prompt\",\n                    default_value=self.prompt,\n                    callback=callback_setattr,\n                    user_data=\"prompt\",\n                )\n\n                dpg.add_input_text(\n                    label=\"negative\",\n                    default_value=self.negative_prompt,\n                    callback=callback_setattr,\n                    user_data=\"negative_prompt\",\n                )\n\n                # save current model\n                with dpg.group(horizontal=True):\n                    dpg.add_text(\"Save: \")\n\n                    def callback_save(sender, app_data, user_data):\n                        self.save_model(mode=user_data)\n\n                    dpg.add_button(\n                        label=\"model\",\n                        tag=\"_button_save_model\",\n                        callback=callback_save,\n                        user_data='model',\n                    )\n                    dpg.bind_item_theme(\"_button_save_model\", theme_button)\n\n                    dpg.add_button(\n                        label=\"geo\",\n                        tag=\"_button_save_mesh\",\n                        callback=callback_save,\n                        user_data='geo',\n                    )\n                    dpg.bind_item_theme(\"_button_save_mesh\", theme_button)\n\n                    dpg.add_button(\n                        label=\"geo+tex\",\n                        tag=\"_button_save_mesh_with_tex\",\n                        callback=callback_save,\n                        user_data='geo+tex',\n                    )\n                    dpg.bind_item_theme(\"_button_save_mesh_with_tex\", theme_button)\n\n                    dpg.add_input_text(\n                        label=\"\",\n                        default_value=self.opt.save_path,\n                        callback=callback_setattr,\n                        user_data=\"save_path\",\n                    )\n\n            # training stuff\n            with dpg.collapsing_header(label=\"Train\", default_open=True):\n                # lr and train button\n                with dpg.group(horizontal=True):\n                    dpg.add_text(\"Train: \")\n\n                    def callback_train(sender, app_data):\n                        if self.training:\n                            self.training = False\n                            dpg.configure_item(\"_button_train\", label=\"start\")\n                        else:\n                            self.prepare_train()\n                            self.training = True\n                            dpg.configure_item(\"_button_train\", label=\"stop\")\n\n                    # dpg.add_button(\n                    #     label=\"init\", tag=\"_button_init\", callback=self.prepare_train\n                    # )\n                    # dpg.bind_item_theme(\"_button_init\", theme_button)\n\n                    dpg.add_button(\n                        label=\"start\", tag=\"_button_train\", callback=callback_train\n                    )\n                    dpg.bind_item_theme(\"_button_train\", theme_button)\n\n                with dpg.group(horizontal=True):\n                    dpg.add_text(\"\", tag=\"_log_train_time\")\n                    dpg.add_text(\"\", tag=\"_log_train_log\")\n\n            # rendering options\n            with dpg.collapsing_header(label=\"Rendering\", default_open=True):\n                # mode combo\n                def callback_change_mode(sender, app_data):\n                    self.mode = app_data\n                    self.need_update = True\n\n                dpg.add_combo(\n                    (\"image\", \"depth\", \"alpha\"),\n                    label=\"mode\",\n                    default_value=self.mode,\n                    callback=callback_change_mode,\n                )\n\n                # fov slider\n                def callback_set_fovy(sender, app_data):\n                    self.cam.fovy = np.deg2rad(app_data)\n                    self.need_update = True\n\n                dpg.add_slider_int(\n                    label=\"FoV (vertical)\",\n                    min_value=1,\n                    max_value=120,\n                    format=\"%d deg\",\n                    default_value=np.rad2deg(self.cam.fovy),\n                    callback=callback_set_fovy,\n                )\n\n                def callback_set_gaussain_scale(sender, app_data):\n                    self.gaussain_scale_factor = app_data\n                    self.need_update = True\n\n                dpg.add_slider_float(\n                    label=\"gaussain scale\",\n                    min_value=0,\n                    max_value=1,\n                    format=\"%.2f\",\n                    default_value=self.gaussain_scale_factor,\n                    callback=callback_set_gaussain_scale,\n                )\n\n        ### register camera handler\n\n        def callback_camera_drag_rotate_or_draw_mask(sender, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n\n            dx = app_data[1]\n            dy = app_data[2]\n\n            self.cam.orbit(dx, dy)\n            self.need_update = True\n\n        def callback_camera_wheel_scale(sender, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n\n            delta = app_data\n\n            self.cam.scale(delta)\n            self.need_update = True\n\n        def callback_camera_drag_pan(sender, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n\n            dx = app_data[1]\n            dy = app_data[2]\n\n            self.cam.pan(dx, dy)\n            self.need_update = True\n\n        def callback_set_mouse_loc(sender, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n\n            # just the pixel coordinate in image\n            self.mouse_loc = np.array(app_data)\n\n        with dpg.handler_registry():\n            # for camera moving\n            dpg.add_mouse_drag_handler(\n                button=dpg.mvMouseButton_Left,\n                callback=callback_camera_drag_rotate_or_draw_mask,\n            )\n            dpg.add_mouse_wheel_handler(callback=callback_camera_wheel_scale)\n            dpg.add_mouse_drag_handler(\n                button=dpg.mvMouseButton_Middle, callback=callback_camera_drag_pan\n            )\n\n        dpg.create_viewport(\n            title=\"Gaussian3D\",\n            width=self.W + 600,\n            height=self.H + (45 if os.name == \"nt\" else 0),\n            resizable=False,\n        )\n\n        ### global theme\n        with dpg.theme() as theme_no_padding:\n            with dpg.theme_component(dpg.mvAll):\n                # set all padding to 0 to avoid scroll bar\n                dpg.add_theme_style(\n                    dpg.mvStyleVar_WindowPadding, 0, 0, category=dpg.mvThemeCat_Core\n                )\n                dpg.add_theme_style(\n                    dpg.mvStyleVar_FramePadding, 0, 0, category=dpg.mvThemeCat_Core\n                )\n                dpg.add_theme_style(\n                    dpg.mvStyleVar_CellPadding, 0, 0, category=dpg.mvThemeCat_Core\n                )\n\n        dpg.bind_item_theme(\"_primary_window\", theme_no_padding)\n\n        dpg.setup_dearpygui()\n\n        ### register a larger font\n        # get it from: https://github.com/lxgw/LxgwWenKai/releases/download/v1.300/LXGWWenKai-Regular.ttf\n        if os.path.exists(\"LXGWWenKai-Regular.ttf\"):\n            with dpg.font_registry():\n                with dpg.font(\"LXGWWenKai-Regular.ttf\", 18) as default_font:\n                    dpg.bind_font(default_font)\n\n        # dpg.show_metrics()\n\n        dpg.show_viewport()\n\n    def render(self):\n        assert self.gui\n        while dpg.is_dearpygui_running():\n            # update texture every frame\n            if self.training:\n                self.train_step()\n            self.test_step()\n            dpg.render_dearpygui_frame()\n    \n    # no gui mode\n    def train(self, iters=500):\n        if iters > 0:\n            self.prepare_train()\n            for i in tqdm.trange(iters):\n                self.train_step()\n            # do a last prune\n            self.renderer.gaussians.prune(min_opacity=0.01, extent=1, max_screen_size=1)\n        # save\n        self.save_model(mode='model')\n        self.save_model(mode='geo+tex')\n        \n\nif __name__ == \"__main__\":\n    import argparse\n    from omegaconf import OmegaConf\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", required=True, help=\"path to the yaml config file\")\n    args, extras = parser.parse_known_args()\n\n    # override default config from cli\n    opt = OmegaConf.merge(OmegaConf.load(args.config), OmegaConf.from_cli(extras))\n\n    gui = GUI(opt)\n\n    if opt.gui:\n        gui.render()\n    else:\n        gui.train(opt.iters)\n"
        },
        {
          "name": "main2.py",
          "type": "blob",
          "size": 25.779296875,
          "content": "import os\nimport cv2\nimport time\nimport tqdm\nimport numpy as np\nimport dearpygui.dearpygui as dpg\n\nimport torch\nimport torch.nn.functional as F\n\nimport trimesh\nimport rembg\n\nfrom cam_utils import orbit_camera, OrbitCamera\nfrom mesh_renderer import Renderer\n\n# from kiui.lpips import LPIPS\n\nclass GUI:\n    def __init__(self, opt):\n        self.opt = opt  # shared with the trainer's opt to support in-place modification of rendering parameters.\n        self.gui = opt.gui # enable gui\n        self.W = opt.W\n        self.H = opt.H\n        self.cam = OrbitCamera(opt.W, opt.H, r=opt.radius, fovy=opt.fovy)\n\n        self.mode = \"image\"\n        self.seed = \"random\"\n\n        self.buffer_image = np.ones((self.W, self.H, 3), dtype=np.float32)\n        self.need_update = True  # update buffer_image\n\n        # models\n        self.device = torch.device(\"cuda\")\n        self.bg_remover = None\n\n        self.guidance_sd = None\n        self.guidance_zero123 = None\n\n        self.enable_sd = False\n        self.enable_zero123 = False\n\n        # renderer\n        self.renderer = Renderer(opt).to(self.device)\n\n        # input image\n        self.input_img = None\n        self.input_mask = None\n        self.input_img_torch = None\n        self.input_mask_torch = None\n        self.overlay_input_img = False\n        self.overlay_input_img_ratio = 0.5\n\n        # input text\n        self.prompt = \"\"\n        self.negative_prompt = \"\"\n\n        # training stuff\n        self.training = False\n        self.optimizer = None\n        self.step = 0\n        self.train_steps = 1  # steps per rendering loop\n        # self.lpips_loss = LPIPS(net='vgg').to(self.device)\n        \n        # load input data from cmdline\n        if self.opt.input is not None:\n            self.load_input(self.opt.input)\n        \n        # override prompt from cmdline\n        if self.opt.prompt is not None:\n            self.prompt = self.opt.prompt\n        if self.opt.negative_prompt is not None:\n            self.negative_prompt = self.opt.negative_prompt\n        \n        if self.gui:\n            dpg.create_context()\n            self.register_dpg()\n            self.test_step()\n\n    def __del__(self):\n        if self.gui:\n            dpg.destroy_context()\n\n    def seed_everything(self):\n        try:\n            seed = int(self.seed)\n        except:\n            seed = np.random.randint(0, 1000000)\n\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\n        self.last_seed = seed\n\n    def prepare_train(self):\n\n        self.step = 0\n\n        # setup training\n        self.optimizer = torch.optim.Adam(self.renderer.get_params())\n\n        # default camera\n        if self.opt.mvdream or self.opt.imagedream:\n            # the second view is the front view for mvdream/imagedream.\n            pose = orbit_camera(self.opt.elevation, 90, self.opt.radius)\n        else:\n            pose = orbit_camera(self.opt.elevation, 0, self.opt.radius)\n\n        self.fixed_cam = (pose, self.cam.perspective)\n        \n\n        self.enable_sd = self.opt.lambda_sd > 0 and self.prompt != \"\"\n        self.enable_zero123 = self.opt.lambda_zero123 > 0 and self.input_img is not None\n\n        # lazy load guidance model\n        if self.guidance_sd is None and self.enable_sd:\n            if self.opt.mvdream:\n                print(f\"[INFO] loading MVDream...\")\n                from guidance.mvdream_utils import MVDream\n                self.guidance_sd = MVDream(self.device)\n                print(f\"[INFO] loaded MVDream!\")\n            elif self.opt.imagedream:\n                print(f\"[INFO] loading ImageDream...\")\n                from guidance.imagedream_utils import ImageDream\n                self.guidance_sd = ImageDream(self.device)\n                print(f\"[INFO] loaded ImageDream!\")\n            else:\n                print(f\"[INFO] loading SD...\")\n                from guidance.sd_utils import StableDiffusion\n                self.guidance_sd = StableDiffusion(self.device)\n                print(f\"[INFO] loaded SD!\")\n\n        if self.guidance_zero123 is None and self.enable_zero123:\n            print(f\"[INFO] loading zero123...\")\n            from guidance.zero123_utils import Zero123\n            if self.opt.stable_zero123:\n                self.guidance_zero123 = Zero123(self.device, model_key='ashawkey/stable-zero123-diffusers')\n            else:\n                self.guidance_zero123 = Zero123(self.device, model_key='ashawkey/zero123-xl-diffusers')\n            print(f\"[INFO] loaded zero123!\")\n        # input image\n        if self.input_img is not None:\n            self.input_img_torch = torch.from_numpy(self.input_img).permute(2, 0, 1).unsqueeze(0).to(self.device)\n            self.input_img_torch = F.interpolate(self.input_img_torch, (self.opt.ref_size, self.opt.ref_size), mode=\"bilinear\", align_corners=False)\n\n            self.input_mask_torch = torch.from_numpy(self.input_mask).permute(2, 0, 1).unsqueeze(0).to(self.device)\n            self.input_mask_torch = F.interpolate(self.input_mask_torch, (self.opt.ref_size, self.opt.ref_size), mode=\"bilinear\", align_corners=False)\n            self.input_img_torch_channel_last = self.input_img_torch[0].permute(1,2,0).contiguous()\n\n        # prepare embeddings\n        with torch.no_grad():\n\n            if self.enable_sd:\n                if self.opt.imagedream:\n                    self.guidance_sd.get_image_text_embeds(self.input_img_torch, [self.prompt], [self.negative_prompt])\n                else:\n                    self.guidance_sd.get_text_embeds([self.prompt], [self.negative_prompt])\n\n            if self.enable_zero123:\n                self.guidance_zero123.get_img_embeds(self.input_img_torch)\n\n    def train_step(self):\n        starter = torch.cuda.Event(enable_timing=True)\n        ender = torch.cuda.Event(enable_timing=True)\n        starter.record()\n\n\n        for _ in range(self.train_steps):\n\n            self.step += 1\n            step_ratio = min(1, self.step / self.opt.iters_refine)\n\n            loss = 0\n\n            ### known view\n            if self.input_img_torch is not None and not self.opt.imagedream:\n\n                ssaa = min(2.0, max(0.125, 2 * np.random.random()))\n                out = self.renderer.render(*self.fixed_cam, self.opt.ref_size, self.opt.ref_size, ssaa=ssaa)\n\n                # rgb loss\n                image = out[\"image\"] # [H, W, 3] in [0, 1]\n                valid_mask = ((out[\"alpha\"] > 0) & (out[\"viewcos\"] > 0.5)).detach()\n                loss = loss + F.mse_loss(image * valid_mask, self.input_img_torch_channel_last * valid_mask)\n\n            ### novel view (manual batch)\n            render_resolution = 512\n            images = []\n            poses = []\n            vers, hors, radii = [], [], []\n            # avoid too large elevation (> 80 or < -80), and make sure it always cover [min_ver, max_ver]\n            min_ver = max(min(self.opt.min_ver, self.opt.min_ver - self.opt.elevation), -80 - self.opt.elevation)\n            max_ver = min(max(self.opt.max_ver, self.opt.max_ver - self.opt.elevation), 80 - self.opt.elevation)\n            for _ in range(self.opt.batch_size):\n\n                # render random view\n                ver = np.random.randint(min_ver, max_ver)\n                hor = np.random.randint(-180, 180)\n                radius = 0\n\n                vers.append(ver)\n                hors.append(hor)\n                radii.append(radius)\n\n                pose = orbit_camera(self.opt.elevation + ver, hor, self.opt.radius + radius)\n                poses.append(pose)\n\n                # random render resolution\n                ssaa = min(2.0, max(0.125, 2 * np.random.random()))\n                out = self.renderer.render(pose, self.cam.perspective, render_resolution, render_resolution, ssaa=ssaa)\n\n                image = out[\"image\"] # [H, W, 3] in [0, 1]\n                image = image.permute(2,0,1).contiguous().unsqueeze(0) # [1, 3, H, W] in [0, 1]\n\n                images.append(image)\n\n                # enable mvdream training\n                if self.opt.mvdream or self.opt.imagedream:\n                    for view_i in range(1, 4):\n                        pose_i = orbit_camera(self.opt.elevation + ver, hor + 90 * view_i, self.opt.radius + radius)\n                        poses.append(pose_i)\n\n                        out_i = self.renderer.render(pose_i, self.cam.perspective, render_resolution, render_resolution, ssaa=ssaa)\n\n                        image = out_i[\"image\"].permute(2,0,1).contiguous().unsqueeze(0) # [1, 3, H, W] in [0, 1]\n                        images.append(image)\n\n            images = torch.cat(images, dim=0)\n            poses = torch.from_numpy(np.stack(poses, axis=0)).to(self.device)\n\n            # import kiui\n            # kiui.lo(hor, ver)\n            # kiui.vis.plot_image(image)\n\n            # guidance loss\n            strength = step_ratio * 0.15 + 0.8\n            if self.enable_sd:\n                if self.opt.mvdream or self.opt.imagedream:\n                    # loss = loss + self.opt.lambda_sd * self.guidance_sd.train_step(images, poses, step_ratio)\n                    refined_images = self.guidance_sd.refine(images, poses, strength=strength).float()\n                    refined_images = F.interpolate(refined_images, (render_resolution, render_resolution), mode=\"bilinear\", align_corners=False)\n                    loss = loss + self.opt.lambda_sd * F.mse_loss(images, refined_images)\n                else:\n                    # loss = loss + self.opt.lambda_sd * self.guidance_sd.train_step(images, step_ratio)\n                    refined_images = self.guidance_sd.refine(images, strength=strength).float()\n                    refined_images = F.interpolate(refined_images, (render_resolution, render_resolution), mode=\"bilinear\", align_corners=False)\n                    loss = loss + self.opt.lambda_sd * F.mse_loss(images, refined_images)\n\n            if self.enable_zero123:\n                # loss = loss + self.opt.lambda_zero123 * self.guidance_zero123.train_step(images, vers, hors, radii, step_ratio)\n                refined_images = self.guidance_zero123.refine(images, vers, hors, radii, strength=strength, default_elevation=self.opt.elevation).float()\n                refined_images = F.interpolate(refined_images, (render_resolution, render_resolution), mode=\"bilinear\", align_corners=False)\n                loss = loss + self.opt.lambda_zero123 * F.mse_loss(images, refined_images)\n                # loss = loss + self.opt.lambda_zero123 * self.lpips_loss(images, refined_images)\n\n            # optimize step\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n        ender.record()\n        torch.cuda.synchronize()\n        t = starter.elapsed_time(ender)\n\n        self.need_update = True\n\n        if self.gui:\n            dpg.set_value(\"_log_train_time\", f\"{t:.4f}ms\")\n            dpg.set_value(\n                \"_log_train_log\",\n                f\"step = {self.step: 5d} (+{self.train_steps: 2d}) loss = {loss.item():.4f}\",\n            )\n\n        # dynamic train steps (no need for now)\n        # max allowed train time per-frame is 500 ms\n        # full_t = t / self.train_steps * 16\n        # train_steps = min(16, max(4, int(16 * 500 / full_t)))\n        # if train_steps > self.train_steps * 1.2 or train_steps < self.train_steps * 0.8:\n        #     self.train_steps = train_steps\n\n    @torch.no_grad()\n    def test_step(self):\n        # ignore if no need to update\n        if not self.need_update:\n            return\n\n        starter = torch.cuda.Event(enable_timing=True)\n        ender = torch.cuda.Event(enable_timing=True)\n        starter.record()\n\n        # should update image\n        if self.need_update:\n            # render image\n\n            out = self.renderer.render(self.cam.pose, self.cam.perspective, self.H, self.W)\n\n            buffer_image = out[self.mode]  # [H, W, 3]\n\n            if self.mode in ['depth', 'alpha']:\n                buffer_image = buffer_image.repeat(1, 1, 3)\n                if self.mode == 'depth':\n                    buffer_image = (buffer_image - buffer_image.min()) / (buffer_image.max() - buffer_image.min() + 1e-20)\n\n            self.buffer_image = buffer_image.contiguous().clamp(0, 1).detach().cpu().numpy()\n            \n            # display input_image\n            if self.overlay_input_img and self.input_img is not None:\n                self.buffer_image = (\n                    self.buffer_image * (1 - self.overlay_input_img_ratio)\n                    + self.input_img * self.overlay_input_img_ratio\n                )\n\n            self.need_update = False\n\n        ender.record()\n        torch.cuda.synchronize()\n        t = starter.elapsed_time(ender)\n\n        if self.gui:\n            dpg.set_value(\"_log_infer_time\", f\"{t:.4f}ms ({int(1000/t)} FPS)\")\n            dpg.set_value(\n                \"_texture\", self.buffer_image\n            )  # buffer must be contiguous, else seg fault!\n\n    \n    def load_input(self, file):\n        # load image\n        print(f'[INFO] load image from {file}...')\n        img = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n        if img.shape[-1] == 3:\n            if self.bg_remover is None:\n                self.bg_remover = rembg.new_session()\n            img = rembg.remove(img, session=self.bg_remover)\n\n        img = cv2.resize(\n            img, (self.W, self.H), interpolation=cv2.INTER_AREA\n        )\n        img = img.astype(np.float32) / 255.0\n\n        self.input_mask = img[..., 3:]\n        # white bg\n        self.input_img = img[..., :3] * self.input_mask + (\n            1 - self.input_mask\n        )\n        # bgr to rgb\n        self.input_img = self.input_img[..., ::-1].copy()\n\n        # load prompt\n        file_prompt = file.replace(\"_rgba.png\", \"_caption.txt\")\n        if os.path.exists(file_prompt):\n            print(f'[INFO] load prompt from {file_prompt}...')\n            with open(file_prompt, \"r\") as f:\n                self.prompt = f.read().strip()\n    \n    def save_model(self):\n        os.makedirs(self.opt.outdir, exist_ok=True)\n    \n        path = os.path.join(self.opt.outdir, self.opt.save_path + '.' + self.opt.mesh_format)\n        self.renderer.export_mesh(path)\n\n        print(f\"[INFO] save model to {path}.\")\n\n    def register_dpg(self):\n        ### register texture\n\n        with dpg.texture_registry(show=False):\n            dpg.add_raw_texture(\n                self.W,\n                self.H,\n                self.buffer_image,\n                format=dpg.mvFormat_Float_rgb,\n                tag=\"_texture\",\n            )\n\n        ### register window\n\n        # the rendered image, as the primary window\n        with dpg.window(\n            tag=\"_primary_window\",\n            width=self.W,\n            height=self.H,\n            pos=[0, 0],\n            no_move=True,\n            no_title_bar=True,\n            no_scrollbar=True,\n        ):\n            # add the texture\n            dpg.add_image(\"_texture\")\n\n        # dpg.set_primary_window(\"_primary_window\", True)\n\n        # control window\n        with dpg.window(\n            label=\"Control\",\n            tag=\"_control_window\",\n            width=600,\n            height=self.H,\n            pos=[self.W, 0],\n            no_move=True,\n            no_title_bar=True,\n        ):\n            # button theme\n            with dpg.theme() as theme_button:\n                with dpg.theme_component(dpg.mvButton):\n                    dpg.add_theme_color(dpg.mvThemeCol_Button, (23, 3, 18))\n                    dpg.add_theme_color(dpg.mvThemeCol_ButtonHovered, (51, 3, 47))\n                    dpg.add_theme_color(dpg.mvThemeCol_ButtonActive, (83, 18, 83))\n                    dpg.add_theme_style(dpg.mvStyleVar_FrameRounding, 5)\n                    dpg.add_theme_style(dpg.mvStyleVar_FramePadding, 3, 3)\n\n            # timer stuff\n            with dpg.group(horizontal=True):\n                dpg.add_text(\"Infer time: \")\n                dpg.add_text(\"no data\", tag=\"_log_infer_time\")\n\n            def callback_setattr(sender, app_data, user_data):\n                setattr(self, user_data, app_data)\n\n            # init stuff\n            with dpg.collapsing_header(label=\"Initialize\", default_open=True):\n\n                # seed stuff\n                def callback_set_seed(sender, app_data):\n                    self.seed = app_data\n                    self.seed_everything()\n\n                dpg.add_input_text(\n                    label=\"seed\",\n                    default_value=self.seed,\n                    on_enter=True,\n                    callback=callback_set_seed,\n                )\n\n                # input stuff\n                def callback_select_input(sender, app_data):\n                    # only one item\n                    for k, v in app_data[\"selections\"].items():\n                        dpg.set_value(\"_log_input\", k)\n                        self.load_input(v)\n\n                    self.need_update = True\n\n                with dpg.file_dialog(\n                    directory_selector=False,\n                    show=False,\n                    callback=callback_select_input,\n                    file_count=1,\n                    tag=\"file_dialog_tag\",\n                    width=700,\n                    height=400,\n                ):\n                    dpg.add_file_extension(\"Images{.jpg,.jpeg,.png}\")\n\n                with dpg.group(horizontal=True):\n                    dpg.add_button(\n                        label=\"input\",\n                        callback=lambda: dpg.show_item(\"file_dialog_tag\"),\n                    )\n                    dpg.add_text(\"\", tag=\"_log_input\")\n                \n                # overlay stuff\n                with dpg.group(horizontal=True):\n\n                    def callback_toggle_overlay_input_img(sender, app_data):\n                        self.overlay_input_img = not self.overlay_input_img\n                        self.need_update = True\n\n                    dpg.add_checkbox(\n                        label=\"overlay image\",\n                        default_value=self.overlay_input_img,\n                        callback=callback_toggle_overlay_input_img,\n                    )\n\n                    def callback_set_overlay_input_img_ratio(sender, app_data):\n                        self.overlay_input_img_ratio = app_data\n                        self.need_update = True\n\n                    dpg.add_slider_float(\n                        label=\"ratio\",\n                        min_value=0,\n                        max_value=1,\n                        format=\"%.1f\",\n                        default_value=self.overlay_input_img_ratio,\n                        callback=callback_set_overlay_input_img_ratio,\n                    )\n\n                # prompt stuff\n            \n                dpg.add_input_text(\n                    label=\"prompt\",\n                    default_value=self.prompt,\n                    callback=callback_setattr,\n                    user_data=\"prompt\",\n                )\n\n                dpg.add_input_text(\n                    label=\"negative\",\n                    default_value=self.negative_prompt,\n                    callback=callback_setattr,\n                    user_data=\"negative_prompt\",\n                )\n\n                # save current model\n                with dpg.group(horizontal=True):\n                    dpg.add_text(\"Save: \")\n\n                    dpg.add_button(\n                        label=\"model\",\n                        tag=\"_button_save_model\",\n                        callback=self.save_model,\n                    )\n                    dpg.bind_item_theme(\"_button_save_model\", theme_button)\n\n                    dpg.add_input_text(\n                        label=\"\",\n                        default_value=self.opt.save_path,\n                        callback=callback_setattr,\n                        user_data=\"save_path\",\n                    )\n\n            # training stuff\n            with dpg.collapsing_header(label=\"Train\", default_open=True):\n                # lr and train button\n                with dpg.group(horizontal=True):\n                    dpg.add_text(\"Train: \")\n\n                    def callback_train(sender, app_data):\n                        if self.training:\n                            self.training = False\n                            dpg.configure_item(\"_button_train\", label=\"start\")\n                        else:\n                            self.prepare_train()\n                            self.training = True\n                            dpg.configure_item(\"_button_train\", label=\"stop\")\n\n                    # dpg.add_button(\n                    #     label=\"init\", tag=\"_button_init\", callback=self.prepare_train\n                    # )\n                    # dpg.bind_item_theme(\"_button_init\", theme_button)\n\n                    dpg.add_button(\n                        label=\"start\", tag=\"_button_train\", callback=callback_train\n                    )\n                    dpg.bind_item_theme(\"_button_train\", theme_button)\n\n                with dpg.group(horizontal=True):\n                    dpg.add_text(\"\", tag=\"_log_train_time\")\n                    dpg.add_text(\"\", tag=\"_log_train_log\")\n\n            # rendering options\n            with dpg.collapsing_header(label=\"Rendering\", default_open=True):\n                # mode combo\n                def callback_change_mode(sender, app_data):\n                    self.mode = app_data\n                    self.need_update = True\n\n                dpg.add_combo(\n                    (\"image\", \"depth\", \"alpha\", \"normal\"),\n                    label=\"mode\",\n                    default_value=self.mode,\n                    callback=callback_change_mode,\n                )\n\n                # fov slider\n                def callback_set_fovy(sender, app_data):\n                    self.cam.fovy = np.deg2rad(app_data)\n                    self.need_update = True\n\n                dpg.add_slider_int(\n                    label=\"FoV (vertical)\",\n                    min_value=1,\n                    max_value=120,\n                    format=\"%d deg\",\n                    default_value=np.rad2deg(self.cam.fovy),\n                    callback=callback_set_fovy,\n                )\n\n        ### register camera handler\n\n        def callback_camera_drag_rotate_or_draw_mask(sender, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n\n            dx = app_data[1]\n            dy = app_data[2]\n\n            self.cam.orbit(dx, dy)\n            self.need_update = True\n\n        def callback_camera_wheel_scale(sender, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n\n            delta = app_data\n\n            self.cam.scale(delta)\n            self.need_update = True\n\n        def callback_camera_drag_pan(sender, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n\n            dx = app_data[1]\n            dy = app_data[2]\n\n            self.cam.pan(dx, dy)\n            self.need_update = True\n\n        def callback_set_mouse_loc(sender, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n\n            # just the pixel coordinate in image\n            self.mouse_loc = np.array(app_data)\n\n        with dpg.handler_registry():\n            # for camera moving\n            dpg.add_mouse_drag_handler(\n                button=dpg.mvMouseButton_Left,\n                callback=callback_camera_drag_rotate_or_draw_mask,\n            )\n            dpg.add_mouse_wheel_handler(callback=callback_camera_wheel_scale)\n            dpg.add_mouse_drag_handler(\n                button=dpg.mvMouseButton_Middle, callback=callback_camera_drag_pan\n            )\n\n        dpg.create_viewport(\n            title=\"Gaussian3D\",\n            width=self.W + 600,\n            height=self.H + (45 if os.name == \"nt\" else 0),\n            resizable=False,\n        )\n\n        ### global theme\n        with dpg.theme() as theme_no_padding:\n            with dpg.theme_component(dpg.mvAll):\n                # set all padding to 0 to avoid scroll bar\n                dpg.add_theme_style(\n                    dpg.mvStyleVar_WindowPadding, 0, 0, category=dpg.mvThemeCat_Core\n                )\n                dpg.add_theme_style(\n                    dpg.mvStyleVar_FramePadding, 0, 0, category=dpg.mvThemeCat_Core\n                )\n                dpg.add_theme_style(\n                    dpg.mvStyleVar_CellPadding, 0, 0, category=dpg.mvThemeCat_Core\n                )\n\n        dpg.bind_item_theme(\"_primary_window\", theme_no_padding)\n\n        dpg.setup_dearpygui()\n\n        ### register a larger font\n        # get it from: https://github.com/lxgw/LxgwWenKai/releases/download/v1.300/LXGWWenKai-Regular.ttf\n        if os.path.exists(\"LXGWWenKai-Regular.ttf\"):\n            with dpg.font_registry():\n                with dpg.font(\"LXGWWenKai-Regular.ttf\", 18) as default_font:\n                    dpg.bind_font(default_font)\n\n        # dpg.show_metrics()\n\n        dpg.show_viewport()\n\n    def render(self):\n        assert self.gui\n        while dpg.is_dearpygui_running():\n            # update texture every frame\n            if self.training:\n                self.train_step()\n            self.test_step()\n            dpg.render_dearpygui_frame()\n    \n    # no gui mode\n    def train(self, iters=500):\n        if iters > 0:\n            self.prepare_train()\n            for i in tqdm.trange(iters):\n                self.train_step()\n        # save\n        self.save_model()\n        \n\nif __name__ == \"__main__\":\n    import argparse\n    from omegaconf import OmegaConf\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", required=True, help=\"path to the yaml config file\")\n    args, extras = parser.parse_known_args()\n\n    # override default config from cli\n    opt = OmegaConf.merge(OmegaConf.load(args.config), OmegaConf.from_cli(extras))\n\n    # auto find mesh from stage 1\n    if opt.mesh is None:\n        default_path = os.path.join(opt.outdir, opt.save_path + '_mesh.' + opt.mesh_format)\n        if os.path.exists(default_path):\n            opt.mesh = default_path\n        else:\n            raise ValueError(f\"Cannot find mesh from {default_path}, must specify --mesh explicitly!\")\n\n    gui = GUI(opt)\n\n    if opt.gui:\n        gui.render()\n    else:\n        gui.train(opt.iters_refine)\n"
        },
        {
          "name": "mesh.py",
          "type": "blob",
          "size": 23.142578125,
          "content": "import os\nimport cv2\nimport torch\nimport trimesh\nimport numpy as np\n\ndef dot(x, y):\n    return torch.sum(x * y, -1, keepdim=True)\n\n\ndef length(x, eps=1e-20):\n    return torch.sqrt(torch.clamp(dot(x, x), min=eps))\n\n\ndef safe_normalize(x, eps=1e-20):\n    return x / length(x, eps)\n\nclass Mesh:\n    def __init__(\n        self,\n        v=None,\n        f=None,\n        vn=None,\n        fn=None,\n        vt=None,\n        ft=None,\n        albedo=None,\n        vc=None, # vertex color\n        device=None,\n    ):\n        self.device = device\n        self.v = v\n        self.vn = vn\n        self.vt = vt\n        self.f = f\n        self.fn = fn\n        self.ft = ft\n        # only support a single albedo\n        self.albedo = albedo\n        # support vertex color is no albedo\n        self.vc = vc\n\n        self.ori_center = 0\n        self.ori_scale = 1\n\n    @classmethod\n    def load(cls, path=None, resize=True, renormal=True, retex=False, front_dir='+z', **kwargs):\n        # assume init with kwargs\n        if path is None:\n            mesh = cls(**kwargs)\n        # obj supports face uv\n        elif path.endswith(\".obj\"):\n            mesh = cls.load_obj(path, **kwargs)\n        # trimesh only supports vertex uv, but can load more formats\n        else:\n            mesh = cls.load_trimesh(path, **kwargs)\n\n        print(f\"[Mesh loading] v: {mesh.v.shape}, f: {mesh.f.shape}\")\n        # auto-normalize\n        if resize:\n            mesh.auto_size()\n        # auto-fix normal\n        if renormal or mesh.vn is None:\n            mesh.auto_normal()\n            print(f\"[Mesh loading] vn: {mesh.vn.shape}, fn: {mesh.fn.shape}\")\n        # auto-fix texcoords\n        if retex or (mesh.albedo is not None and mesh.vt is None):\n            mesh.auto_uv(cache_path=path)\n            print(f\"[Mesh loading] vt: {mesh.vt.shape}, ft: {mesh.ft.shape}\")\n\n        # rotate front dir to +z\n        if front_dir != \"+z\":\n            # axis switch\n            if \"-z\" in front_dir:\n                T = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, -1]], device=mesh.device, dtype=torch.float32)\n            elif \"+x\" in front_dir:\n                T = torch.tensor([[0, 0, 1], [0, 1, 0], [1, 0, 0]], device=mesh.device, dtype=torch.float32)\n            elif \"-x\" in front_dir:\n                T = torch.tensor([[0, 0, -1], [0, 1, 0], [1, 0, 0]], device=mesh.device, dtype=torch.float32)\n            elif \"+y\" in front_dir:\n                T = torch.tensor([[1, 0, 0], [0, 0, 1], [0, 1, 0]], device=mesh.device, dtype=torch.float32)\n            elif \"-y\" in front_dir:\n                T = torch.tensor([[1, 0, 0], [0, 0, -1], [0, 1, 0]], device=mesh.device, dtype=torch.float32)\n            else:\n                T = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], device=mesh.device, dtype=torch.float32)\n            # rotation (how many 90 degrees)\n            if '1' in front_dir:\n                T @= torch.tensor([[0, -1, 0], [1, 0, 0], [0, 0, 1]], device=mesh.device, dtype=torch.float32) \n            elif '2' in front_dir:\n                T @= torch.tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]], device=mesh.device, dtype=torch.float32) \n            elif '3' in front_dir:\n                T @= torch.tensor([[0, 1, 0], [-1, 0, 0], [0, 0, 1]], device=mesh.device, dtype=torch.float32) \n            mesh.v @= T\n            mesh.vn @= T\n\n        return mesh\n\n    # load from obj file\n    @classmethod\n    def load_obj(cls, path, albedo_path=None, device=None):\n        assert os.path.splitext(path)[-1] == \".obj\"\n\n        mesh = cls()\n\n        # device\n        if device is None:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        mesh.device = device\n\n        # load obj\n        with open(path, \"r\") as f:\n            lines = f.readlines()\n\n        def parse_f_v(fv):\n            # pass in a vertex term of a face, return {v, vt, vn} (-1 if not provided)\n            # supported forms:\n            # f v1 v2 v3\n            # f v1/vt1 v2/vt2 v3/vt3\n            # f v1/vt1/vn1 v2/vt2/vn2 v3/vt3/vn3\n            # f v1//vn1 v2//vn2 v3//vn3\n            xs = [int(x) - 1 if x != \"\" else -1 for x in fv.split(\"/\")]\n            xs.extend([-1] * (3 - len(xs)))\n            return xs[0], xs[1], xs[2]\n\n        # NOTE: we ignore usemtl, and assume the mesh ONLY uses one material (first in mtl)\n        vertices, texcoords, normals = [], [], []\n        faces, tfaces, nfaces = [], [], []\n        mtl_path = None\n\n        for line in lines:\n            split_line = line.split()\n            # empty line\n            if len(split_line) == 0:\n                continue\n            prefix = split_line[0].lower()\n            # mtllib\n            if prefix == \"mtllib\":\n                mtl_path = split_line[1]\n            # usemtl\n            elif prefix == \"usemtl\":\n                pass # ignored\n            # v/vn/vt\n            elif prefix == \"v\":\n                vertices.append([float(v) for v in split_line[1:]])\n            elif prefix == \"vn\":\n                normals.append([float(v) for v in split_line[1:]])\n            elif prefix == \"vt\":\n                val = [float(v) for v in split_line[1:]]\n                texcoords.append([val[0], 1.0 - val[1]])\n            elif prefix == \"f\":\n                vs = split_line[1:]\n                nv = len(vs)\n                v0, t0, n0 = parse_f_v(vs[0])\n                for i in range(nv - 2):  # triangulate (assume vertices are ordered)\n                    v1, t1, n1 = parse_f_v(vs[i + 1])\n                    v2, t2, n2 = parse_f_v(vs[i + 2])\n                    faces.append([v0, v1, v2])\n                    tfaces.append([t0, t1, t2])\n                    nfaces.append([n0, n1, n2])\n\n        mesh.v = torch.tensor(vertices, dtype=torch.float32, device=device)\n        mesh.vt = (\n            torch.tensor(texcoords, dtype=torch.float32, device=device)\n            if len(texcoords) > 0\n            else None\n        )\n        mesh.vn = (\n            torch.tensor(normals, dtype=torch.float32, device=device)\n            if len(normals) > 0\n            else None\n        )\n\n        mesh.f = torch.tensor(faces, dtype=torch.int32, device=device)\n        mesh.ft = (\n            torch.tensor(tfaces, dtype=torch.int32, device=device)\n            if len(texcoords) > 0\n            else None\n        )\n        mesh.fn = (\n            torch.tensor(nfaces, dtype=torch.int32, device=device)\n            if len(normals) > 0\n            else None\n        )\n\n        # see if there is vertex color\n        use_vertex_color = False\n        if mesh.v.shape[1] == 6:\n            use_vertex_color = True\n            mesh.vc = mesh.v[:, 3:]\n            mesh.v = mesh.v[:, :3]\n            print(f\"[load_obj] use vertex color: {mesh.vc.shape}\")\n\n        # try to load texture image\n        if not use_vertex_color:\n            # try to retrieve mtl file\n            mtl_path_candidates = []\n            if mtl_path is not None:\n                mtl_path_candidates.append(mtl_path)\n                mtl_path_candidates.append(os.path.join(os.path.dirname(path), mtl_path))\n            mtl_path_candidates.append(path.replace(\".obj\", \".mtl\"))\n\n            mtl_path = None\n            for candidate in mtl_path_candidates:\n                if os.path.exists(candidate):\n                    mtl_path = candidate\n                    break\n            \n            # if albedo_path is not provided, try retrieve it from mtl\n            if mtl_path is not None and albedo_path is None:\n                with open(mtl_path, \"r\") as f:\n                    lines = f.readlines()\n                for line in lines:\n                    split_line = line.split()\n                    # empty line\n                    if len(split_line) == 0:\n                        continue\n                    prefix = split_line[0]\n                    # NOTE: simply use the first map_Kd as albedo!\n                    if \"map_Kd\" in prefix:\n                        albedo_path = os.path.join(os.path.dirname(path), split_line[1])\n                        print(f\"[load_obj] use texture from: {albedo_path}\")\n                        break\n            \n            # still not found albedo_path, or the path doesn't exist\n            if albedo_path is None or not os.path.exists(albedo_path):\n                # init an empty texture\n                print(f\"[load_obj] init empty albedo!\")\n                # albedo = np.random.rand(1024, 1024, 3).astype(np.float32)\n                albedo = np.ones((1024, 1024, 3), dtype=np.float32) * np.array([0.5, 0.5, 0.5])  # default color\n            else:\n                albedo = cv2.imread(albedo_path, cv2.IMREAD_UNCHANGED)\n                albedo = cv2.cvtColor(albedo, cv2.COLOR_BGR2RGB)\n                albedo = albedo.astype(np.float32) / 255\n                print(f\"[load_obj] load texture: {albedo.shape}\")\n\n                # import matplotlib.pyplot as plt\n                # plt.imshow(albedo)\n                # plt.show()\n\n            mesh.albedo = torch.tensor(albedo, dtype=torch.float32, device=device)\n\n        return mesh\n\n    @classmethod\n    def load_trimesh(cls, path, device=None):\n        mesh = cls()\n\n        # device\n        if device is None:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        mesh.device = device\n\n        # use trimesh to load ply/glb, assume only has one single RootMesh...\n        _data = trimesh.load(path)\n        if isinstance(_data, trimesh.Scene):\n            if len(_data.geometry) == 1:\n                _mesh = list(_data.geometry.values())[0]\n            else:\n                # manual concat, will lose texture\n                _concat = []\n                for g in _data.geometry.values():\n                    if isinstance(g, trimesh.Trimesh):\n                        _concat.append(g)\n                _mesh = trimesh.util.concatenate(_concat)\n        else:\n            _mesh = _data\n        \n        if _mesh.visual.kind == 'vertex':\n            vertex_colors = _mesh.visual.vertex_colors\n            vertex_colors = np.array(vertex_colors[..., :3]).astype(np.float32) / 255\n            mesh.vc = torch.tensor(vertex_colors, dtype=torch.float32, device=device)\n            print(f\"[load_trimesh] use vertex color: {mesh.vc.shape}\")\n        elif _mesh.visual.kind == 'texture':\n            _material = _mesh.visual.material\n            if isinstance(_material, trimesh.visual.material.PBRMaterial):\n                texture = np.array(_material.baseColorTexture).astype(np.float32) / 255\n            elif isinstance(_material, trimesh.visual.material.SimpleMaterial):\n                texture = np.array(_material.to_pbr().baseColorTexture).astype(np.float32) / 255\n            else:\n                raise NotImplementedError(f\"material type {type(_material)} not supported!\")\n            mesh.albedo = torch.tensor(texture, dtype=torch.float32, device=device)\n            print(f\"[load_trimesh] load texture: {texture.shape}\")\n        else:\n            texture = np.ones((1024, 1024, 3), dtype=np.float32) * np.array([0.5, 0.5, 0.5])\n            mesh.albedo = torch.tensor(texture, dtype=torch.float32, device=device)\n            print(f\"[load_trimesh] failed to load texture.\")\n\n        vertices = _mesh.vertices\n\n        try:\n            texcoords = _mesh.visual.uv\n            texcoords[:, 1] = 1 - texcoords[:, 1]\n        except Exception as e:\n            texcoords = None\n\n        try:\n            normals = _mesh.vertex_normals\n        except Exception as e:\n            normals = None\n\n        # trimesh only support vertex uv...\n        faces = tfaces = nfaces = _mesh.faces\n\n        mesh.v = torch.tensor(vertices, dtype=torch.float32, device=device)\n        mesh.vt = (\n            torch.tensor(texcoords, dtype=torch.float32, device=device)\n            if texcoords is not None\n            else None\n        )\n        mesh.vn = (\n            torch.tensor(normals, dtype=torch.float32, device=device)\n            if normals is not None\n            else None\n        )\n\n        mesh.f = torch.tensor(faces, dtype=torch.int32, device=device)\n        mesh.ft = (\n            torch.tensor(tfaces, dtype=torch.int32, device=device)\n            if texcoords is not None\n            else None\n        )\n        mesh.fn = (\n            torch.tensor(nfaces, dtype=torch.int32, device=device)\n            if normals is not None\n            else None\n        )\n\n        return mesh\n\n    # aabb\n    def aabb(self):\n        return torch.min(self.v, dim=0).values, torch.max(self.v, dim=0).values\n\n    # unit size\n    @torch.no_grad()\n    def auto_size(self):\n        vmin, vmax = self.aabb()\n        self.ori_center = (vmax + vmin) / 2\n        self.ori_scale = 1.2 / torch.max(vmax - vmin).item()\n        self.v = (self.v - self.ori_center) * self.ori_scale\n\n    def auto_normal(self):\n        i0, i1, i2 = self.f[:, 0].long(), self.f[:, 1].long(), self.f[:, 2].long()\n        v0, v1, v2 = self.v[i0, :], self.v[i1, :], self.v[i2, :]\n\n        face_normals = torch.cross(v1 - v0, v2 - v0)\n\n        # Splat face normals to vertices\n        vn = torch.zeros_like(self.v)\n        vn.scatter_add_(0, i0[:, None].repeat(1, 3), face_normals)\n        vn.scatter_add_(0, i1[:, None].repeat(1, 3), face_normals)\n        vn.scatter_add_(0, i2[:, None].repeat(1, 3), face_normals)\n\n        # Normalize, replace zero (degenerated) normals with some default value\n        vn = torch.where(\n            dot(vn, vn) > 1e-20,\n            vn,\n            torch.tensor([0.0, 0.0, 1.0], dtype=torch.float32, device=vn.device),\n        )\n        vn = safe_normalize(vn)\n\n        self.vn = vn\n        self.fn = self.f\n\n    def auto_uv(self, cache_path=None, vmap=True):\n        # try to load cache\n        if cache_path is not None:\n            cache_path = os.path.splitext(cache_path)[0] + \"_uv.npz\"\n        if cache_path is not None and os.path.exists(cache_path):\n            data = np.load(cache_path)\n            vt_np, ft_np, vmapping = data[\"vt\"], data[\"ft\"], data[\"vmapping\"]\n        else:\n            import xatlas\n\n            v_np = self.v.detach().cpu().numpy()\n            f_np = self.f.detach().int().cpu().numpy()\n            atlas = xatlas.Atlas()\n            atlas.add_mesh(v_np, f_np)\n            chart_options = xatlas.ChartOptions()\n            # chart_options.max_iterations = 4\n            atlas.generate(chart_options=chart_options)\n            vmapping, ft_np, vt_np = atlas[0]  # [N], [M, 3], [N, 2]\n\n            # save to cache\n            if cache_path is not None:\n                np.savez(cache_path, vt=vt_np, ft=ft_np, vmapping=vmapping)\n        \n        vt = torch.from_numpy(vt_np.astype(np.float32)).to(self.device)\n        ft = torch.from_numpy(ft_np.astype(np.int32)).to(self.device)\n        self.vt = vt\n        self.ft = ft\n\n        if vmap:\n            # remap v/f to vt/ft, so each v correspond to a unique vt. (necessary for gltf)\n            vmapping = torch.from_numpy(vmapping.astype(np.int64)).long().to(self.device)\n            self.align_v_to_vt(vmapping)\n    \n    def align_v_to_vt(self, vmapping=None):\n        # remap v/f and vn/vn to vt/ft.\n        if vmapping is None:\n            ft = self.ft.view(-1).long()\n            f = self.f.view(-1).long()\n            vmapping = torch.zeros(self.vt.shape[0], dtype=torch.long, device=self.device)\n            vmapping[ft] = f # scatter, randomly choose one if index is not unique\n\n        self.v = self.v[vmapping]\n        self.f = self.ft\n        # assume fn == f\n        if self.vn is not None:\n            self.vn = self.vn[vmapping]\n            self.fn = self.ft\n\n    def to(self, device):\n        self.device = device\n        for name in [\"v\", \"f\", \"vn\", \"fn\", \"vt\", \"ft\", \"albedo\"]:\n            tensor = getattr(self, name)\n            if tensor is not None:\n                setattr(self, name, tensor.to(device))\n        return self\n    \n    def write(self, path):\n        if path.endswith(\".ply\"):\n            self.write_ply(path)\n        elif path.endswith(\".obj\"):\n            self.write_obj(path)\n        elif path.endswith(\".glb\") or path.endswith(\".gltf\"):\n            self.write_glb(path)\n        else:\n            raise NotImplementedError(f\"format {path} not supported!\")\n    \n    # write to ply file (only geom)\n    def write_ply(self, path):\n\n        v_np = self.v.detach().cpu().numpy()\n        f_np = self.f.detach().cpu().numpy()\n\n        _mesh = trimesh.Trimesh(vertices=v_np, faces=f_np)\n        _mesh.export(path)\n\n    # write to gltf/glb file (geom + texture)\n    def write_glb(self, path):\n\n        assert self.vn is not None and self.vt is not None # should be improved to support export without texture...\n\n        # assert self.v.shape[0] == self.vn.shape[0] and self.v.shape[0] == self.vt.shape[0]\n        if self.v.shape[0] != self.vt.shape[0]:\n            self.align_v_to_vt()\n\n        # assume f == fn == ft\n\n        import pygltflib\n\n        f_np = self.f.detach().cpu().numpy().astype(np.uint32)\n        v_np = self.v.detach().cpu().numpy().astype(np.float32)\n        # vn_np = self.vn.detach().cpu().numpy().astype(np.float32)\n        vt_np = self.vt.detach().cpu().numpy().astype(np.float32)\n\n        albedo = self.albedo.detach().cpu().numpy()\n        albedo = (albedo * 255).astype(np.uint8)\n        albedo = cv2.cvtColor(albedo, cv2.COLOR_RGB2BGR)\n\n        f_np_blob = f_np.flatten().tobytes()\n        v_np_blob = v_np.tobytes()\n        # vn_np_blob = vn_np.tobytes()\n        vt_np_blob = vt_np.tobytes()\n        albedo_blob = cv2.imencode('.png', albedo)[1].tobytes()\n\n        gltf = pygltflib.GLTF2(\n            scene=0,\n            scenes=[pygltflib.Scene(nodes=[0])],\n            nodes=[pygltflib.Node(mesh=0)],\n            meshes=[pygltflib.Mesh(primitives=[\n                pygltflib.Primitive(\n                    # indices to accessors (0 is triangles)\n                    attributes=pygltflib.Attributes(\n                        POSITION=1, TEXCOORD_0=2, \n                    ),\n                    indices=0, material=0,\n                )\n            ])],\n            materials=[\n                pygltflib.Material(\n                    pbrMetallicRoughness=pygltflib.PbrMetallicRoughness(\n                        baseColorTexture=pygltflib.TextureInfo(index=0, texCoord=0),\n                        metallicFactor=0.0,\n                        roughnessFactor=1.0,\n                    ),\n                    alphaCutoff=0,\n                    doubleSided=True,\n                )\n            ],\n            textures=[\n                pygltflib.Texture(sampler=0, source=0),\n            ],\n            samplers=[\n                pygltflib.Sampler(magFilter=pygltflib.LINEAR, minFilter=pygltflib.LINEAR_MIPMAP_LINEAR, wrapS=pygltflib.REPEAT, wrapT=pygltflib.REPEAT),\n            ],\n            images=[\n                # use embedded (buffer) image\n                pygltflib.Image(bufferView=3, mimeType=\"image/png\"),\n            ],\n            buffers=[\n                pygltflib.Buffer(byteLength=len(f_np_blob) + len(v_np_blob) + len(vt_np_blob) + len(albedo_blob))\n            ],\n            # buffer view (based on dtype)\n            bufferViews=[\n                # triangles; as flatten (element) array\n                pygltflib.BufferView(\n                    buffer=0,\n                    byteLength=len(f_np_blob),\n                    target=pygltflib.ELEMENT_ARRAY_BUFFER, # GL_ELEMENT_ARRAY_BUFFER (34963)\n                ),\n                # positions; as vec3 array\n                pygltflib.BufferView(\n                    buffer=0,\n                    byteOffset=len(f_np_blob),\n                    byteLength=len(v_np_blob),\n                    byteStride=12, # vec3\n                    target=pygltflib.ARRAY_BUFFER, # GL_ARRAY_BUFFER (34962)\n                ),\n                # texcoords; as vec2 array\n                pygltflib.BufferView(\n                    buffer=0,\n                    byteOffset=len(f_np_blob) + len(v_np_blob),\n                    byteLength=len(vt_np_blob),\n                    byteStride=8, # vec2\n                    target=pygltflib.ARRAY_BUFFER,\n                ),\n                # texture; as none target\n                pygltflib.BufferView(\n                    buffer=0,\n                    byteOffset=len(f_np_blob) + len(v_np_blob) + len(vt_np_blob),\n                    byteLength=len(albedo_blob),\n                ),\n            ],\n            accessors=[\n                # 0 = triangles\n                pygltflib.Accessor(\n                    bufferView=0,\n                    componentType=pygltflib.UNSIGNED_INT, # GL_UNSIGNED_INT (5125)\n                    count=f_np.size,\n                    type=pygltflib.SCALAR,\n                    max=[int(f_np.max())],\n                    min=[int(f_np.min())],\n                ),\n                # 1 = positions\n                pygltflib.Accessor(\n                    bufferView=1,\n                    componentType=pygltflib.FLOAT, # GL_FLOAT (5126)\n                    count=len(v_np),\n                    type=pygltflib.VEC3,\n                    max=v_np.max(axis=0).tolist(),\n                    min=v_np.min(axis=0).tolist(),\n                ),\n                # 2 = texcoords\n                pygltflib.Accessor(\n                    bufferView=2,\n                    componentType=pygltflib.FLOAT,\n                    count=len(vt_np),\n                    type=pygltflib.VEC2,\n                    max=vt_np.max(axis=0).tolist(),\n                    min=vt_np.min(axis=0).tolist(),\n                ),\n            ],\n        )\n\n        # set actual data\n        gltf.set_binary_blob(f_np_blob + v_np_blob + vt_np_blob + albedo_blob)\n\n        # glb = b\"\".join(gltf.save_to_bytes())\n        gltf.save(path)\n\n    # write to obj file (geom + texture)\n    def write_obj(self, path):\n\n        mtl_path = path.replace(\".obj\", \".mtl\")\n        albedo_path = path.replace(\".obj\", \"_albedo.png\")\n\n        v_np = self.v.detach().cpu().numpy()\n        vt_np = self.vt.detach().cpu().numpy() if self.vt is not None else None\n        vn_np = self.vn.detach().cpu().numpy() if self.vn is not None else None\n        f_np = self.f.detach().cpu().numpy()\n        ft_np = self.ft.detach().cpu().numpy() if self.ft is not None else None\n        fn_np = self.fn.detach().cpu().numpy() if self.fn is not None else None\n\n        with open(path, \"w\") as fp:\n            fp.write(f\"mtllib {os.path.basename(mtl_path)} \\n\")\n\n            for v in v_np:\n                fp.write(f\"v {v[0]} {v[1]} {v[2]} \\n\")\n\n            if vt_np is not None:\n                for v in vt_np:\n                    fp.write(f\"vt {v[0]} {1 - v[1]} \\n\")\n\n            if vn_np is not None:\n                for v in vn_np:\n                    fp.write(f\"vn {v[0]} {v[1]} {v[2]} \\n\")\n\n            fp.write(f\"usemtl defaultMat \\n\")\n            for i in range(len(f_np)):\n                fp.write(\n                    f'f {f_np[i, 0] + 1}/{ft_np[i, 0] + 1 if ft_np is not None else \"\"}/{fn_np[i, 0] + 1 if fn_np is not None else \"\"} \\\n                             {f_np[i, 1] + 1}/{ft_np[i, 1] + 1 if ft_np is not None else \"\"}/{fn_np[i, 1] + 1 if fn_np is not None else \"\"} \\\n                             {f_np[i, 2] + 1}/{ft_np[i, 2] + 1 if ft_np is not None else \"\"}/{fn_np[i, 2] + 1 if fn_np is not None else \"\"} \\n'\n                )\n\n        with open(mtl_path, \"w\") as fp:\n            fp.write(f\"newmtl defaultMat \\n\")\n            fp.write(f\"Ka 1 1 1 \\n\")\n            fp.write(f\"Kd 1 1 1 \\n\")\n            fp.write(f\"Ks 0 0 0 \\n\")\n            fp.write(f\"Tr 1 \\n\")\n            fp.write(f\"illum 1 \\n\")\n            fp.write(f\"Ns 0 \\n\")\n            fp.write(f\"map_Kd {os.path.basename(albedo_path)} \\n\")\n\n        albedo = self.albedo.detach().cpu().numpy()\n        albedo = (albedo * 255).astype(np.uint8)\n        cv2.imwrite(albedo_path, cv2.cvtColor(albedo, cv2.COLOR_RGB2BGR))"
        },
        {
          "name": "mesh_renderer.py",
          "type": "blob",
          "size": 5.5322265625,
          "content": "import os\nimport math\nimport cv2\nimport trimesh\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport nvdiffrast.torch as dr\nfrom mesh import Mesh, safe_normalize\n\ndef scale_img_nhwc(x, size, mag='bilinear', min='bilinear'):\n    assert (x.shape[1] >= size[0] and x.shape[2] >= size[1]) or (x.shape[1] < size[0] and x.shape[2] < size[1]), \"Trying to magnify image in one dimension and minify in the other\"\n    y = x.permute(0, 3, 1, 2) # NHWC -> NCHW\n    if x.shape[1] > size[0] and x.shape[2] > size[1]: # Minification, previous size was bigger\n        y = torch.nn.functional.interpolate(y, size, mode=min)\n    else: # Magnification\n        if mag == 'bilinear' or mag == 'bicubic':\n            y = torch.nn.functional.interpolate(y, size, mode=mag, align_corners=True)\n        else:\n            y = torch.nn.functional.interpolate(y, size, mode=mag)\n    return y.permute(0, 2, 3, 1).contiguous() # NCHW -> NHWC\n\ndef scale_img_hwc(x, size, mag='bilinear', min='bilinear'):\n    return scale_img_nhwc(x[None, ...], size, mag, min)[0]\n\ndef scale_img_nhw(x, size, mag='bilinear', min='bilinear'):\n    return scale_img_nhwc(x[..., None], size, mag, min)[..., 0]\n\ndef scale_img_hw(x, size, mag='bilinear', min='bilinear'):\n    return scale_img_nhwc(x[None, ..., None], size, mag, min)[0, ..., 0]\n\ndef trunc_rev_sigmoid(x, eps=1e-6):\n    x = x.clamp(eps, 1 - eps)\n    return torch.log(x / (1 - x))\n\ndef make_divisible(x, m=8):\n    return int(math.ceil(x / m) * m)\n\nclass Renderer(nn.Module):\n    def __init__(self, opt):\n        \n        super().__init__()\n\n        self.opt = opt\n\n        self.mesh = Mesh.load(self.opt.mesh, resize=False)\n\n        if not self.opt.force_cuda_rast and (not self.opt.gui or os.name == 'nt'):\n            self.glctx = dr.RasterizeGLContext()\n        else:\n            self.glctx = dr.RasterizeCudaContext()\n        \n        # extract trainable parameters\n        self.v_offsets = nn.Parameter(torch.zeros_like(self.mesh.v))\n        self.raw_albedo = nn.Parameter(trunc_rev_sigmoid(self.mesh.albedo))\n\n\n    def get_params(self):\n\n        params = [\n            {'params': self.raw_albedo, 'lr': self.opt.texture_lr},\n        ]\n\n        if self.opt.train_geo:\n            params.append({'params': self.v_offsets, 'lr': self.opt.geom_lr})\n\n        return params\n\n    @torch.no_grad()\n    def export_mesh(self, save_path):\n        self.mesh.v = (self.mesh.v + self.v_offsets).detach()\n        self.mesh.albedo = torch.sigmoid(self.raw_albedo.detach())\n        self.mesh.write(save_path)\n\n    \n    def render(self, pose, proj, h0, w0, ssaa=1, bg_color=1, texture_filter='linear-mipmap-linear'):\n        \n        # do super-sampling\n        if ssaa != 1:\n            h = make_divisible(h0 * ssaa, 8)\n            w = make_divisible(w0 * ssaa, 8)\n        else:\n            h, w = h0, w0\n        \n        results = {}\n\n        # get v\n        if self.opt.train_geo:\n            v = self.mesh.v + self.v_offsets # [N, 3]\n        else:\n            v = self.mesh.v\n\n        pose = torch.from_numpy(pose.astype(np.float32)).to(v.device)\n        proj = torch.from_numpy(proj.astype(np.float32)).to(v.device)\n\n        # get v_clip and render rgb\n        v_cam = torch.matmul(F.pad(v, pad=(0, 1), mode='constant', value=1.0), torch.inverse(pose).T).float().unsqueeze(0)\n        v_clip = v_cam @ proj.T\n\n        rast, rast_db = dr.rasterize(self.glctx, v_clip, self.mesh.f, (h, w))\n\n        alpha = (rast[0, ..., 3:] > 0).float()\n        depth, _ = dr.interpolate(-v_cam[..., [2]], rast, self.mesh.f) # [1, H, W, 1]\n        depth = depth.squeeze(0) # [H, W, 1]\n\n        texc, texc_db = dr.interpolate(self.mesh.vt.unsqueeze(0).contiguous(), rast, self.mesh.ft, rast_db=rast_db, diff_attrs='all')\n        albedo = dr.texture(self.raw_albedo.unsqueeze(0), texc, uv_da=texc_db, filter_mode=texture_filter) # [1, H, W, 3]\n        albedo = torch.sigmoid(albedo)\n        # get vn and render normal\n        if self.opt.train_geo:\n            i0, i1, i2 = self.mesh.f[:, 0].long(), self.mesh.f[:, 1].long(), self.mesh.f[:, 2].long()\n            v0, v1, v2 = v[i0, :], v[i1, :], v[i2, :]\n\n            face_normals = torch.cross(v1 - v0, v2 - v0)\n            face_normals = safe_normalize(face_normals)\n            \n            vn = torch.zeros_like(v)\n            vn.scatter_add_(0, i0[:, None].repeat(1,3), face_normals)\n            vn.scatter_add_(0, i1[:, None].repeat(1,3), face_normals)\n            vn.scatter_add_(0, i2[:, None].repeat(1,3), face_normals)\n\n            vn = torch.where(torch.sum(vn * vn, -1, keepdim=True) > 1e-20, vn, torch.tensor([0.0, 0.0, 1.0], dtype=torch.float32, device=vn.device))\n        else:\n            vn = self.mesh.vn\n        \n        normal, _ = dr.interpolate(vn.unsqueeze(0).contiguous(), rast, self.mesh.fn)\n        normal = safe_normalize(normal[0])\n\n        # rotated normal (where [0, 0, 1] always faces camera)\n        rot_normal = normal @ pose[:3, :3]\n        viewcos = rot_normal[..., [2]]\n\n        # antialias\n        albedo = dr.antialias(albedo, rast, v_clip, self.mesh.f).squeeze(0) # [H, W, 3]\n        albedo = alpha * albedo + (1 - alpha) * bg_color\n\n        # ssaa\n        if ssaa != 1:\n            albedo = scale_img_hwc(albedo, (h0, w0))\n            alpha = scale_img_hwc(alpha, (h0, w0))\n            depth = scale_img_hwc(depth, (h0, w0))\n            normal = scale_img_hwc(normal, (h0, w0))\n            viewcos = scale_img_hwc(viewcos, (h0, w0))\n\n        results['image'] = albedo.clamp(0, 1)\n        results['alpha'] = alpha\n        results['depth'] = depth\n        results['normal'] = (normal + 1) / 2\n        results['viewcos'] = viewcos\n\n        return results"
        },
        {
          "name": "mesh_utils.py",
          "type": "blob",
          "size": 3.9970703125,
          "content": "import numpy as np\nimport pymeshlab as pml\n\n\ndef poisson_mesh_reconstruction(points, normals=None):\n    # points/normals: [N, 3] np.ndarray\n\n    import open3d as o3d\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n\n    # outlier removal\n    pcd, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=10)\n\n    # normals\n    if normals is None:\n        pcd.estimate_normals()\n    else:\n        pcd.normals = o3d.utility.Vector3dVector(normals[ind])\n\n    # visualize\n    o3d.visualization.draw_geometries([pcd], point_show_normal=False)\n\n    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n        pcd, depth=9\n    )\n    vertices_to_remove = densities < np.quantile(densities, 0.1)\n    mesh.remove_vertices_by_mask(vertices_to_remove)\n\n    # visualize\n    o3d.visualization.draw_geometries([mesh])\n\n    vertices = np.asarray(mesh.vertices)\n    triangles = np.asarray(mesh.triangles)\n\n    print(\n        f\"[INFO] poisson mesh reconstruction: {points.shape} --> {vertices.shape} / {triangles.shape}\"\n    )\n\n    return vertices, triangles\n\n\ndef decimate_mesh(\n    verts, faces, target, backend=\"pymeshlab\", remesh=False, optimalplacement=True\n):\n    # optimalplacement: default is True, but for flat mesh must turn False to prevent spike artifect.\n\n    _ori_vert_shape = verts.shape\n    _ori_face_shape = faces.shape\n\n    if backend == \"pyfqmr\":\n        import pyfqmr\n\n        solver = pyfqmr.Simplify()\n        solver.setMesh(verts, faces)\n        solver.simplify_mesh(target_count=target, preserve_border=False, verbose=False)\n        verts, faces, normals = solver.getMesh()\n    else:\n        m = pml.Mesh(verts, faces)\n        ms = pml.MeshSet()\n        ms.add_mesh(m, \"mesh\")  # will copy!\n\n        # filters\n        # ms.meshing_decimation_clustering(threshold=pml.PercentageValue(1))\n        ms.meshing_decimation_quadric_edge_collapse(\n            targetfacenum=int(target), optimalplacement=optimalplacement\n        )\n\n        if remesh:\n            # ms.apply_coord_taubin_smoothing()\n            ms.meshing_isotropic_explicit_remeshing(\n                iterations=3, targetlen=pml.PercentageValue(1)\n            )\n\n        # extract mesh\n        m = ms.current_mesh()\n        verts = m.vertex_matrix()\n        faces = m.face_matrix()\n\n    print(\n        f\"[INFO] mesh decimation: {_ori_vert_shape} --> {verts.shape}, {_ori_face_shape} --> {faces.shape}\"\n    )\n\n    return verts, faces\n\n\ndef clean_mesh(\n    verts,\n    faces,\n    v_pct=1,\n    min_f=64,\n    min_d=20,\n    repair=True,\n    remesh=True,\n    remesh_size=0.01,\n):\n    # verts: [N, 3]\n    # faces: [N, 3]\n\n    _ori_vert_shape = verts.shape\n    _ori_face_shape = faces.shape\n\n    m = pml.Mesh(verts, faces)\n    ms = pml.MeshSet()\n    ms.add_mesh(m, \"mesh\")  # will copy!\n\n    # filters\n    ms.meshing_remove_unreferenced_vertices()  # verts not refed by any faces\n\n    if v_pct > 0:\n        ms.meshing_merge_close_vertices(\n            threshold=pml.PercentageValue(v_pct)\n        )  # 1/10000 of bounding box diagonal\n\n    ms.meshing_remove_duplicate_faces()  # faces defined by the same verts\n    ms.meshing_remove_null_faces()  # faces with area == 0\n\n    if min_d > 0:\n        ms.meshing_remove_connected_component_by_diameter(\n            mincomponentdiag=pml.PercentageValue(min_d)\n        )\n\n    if min_f > 0:\n        ms.meshing_remove_connected_component_by_face_number(mincomponentsize=min_f)\n\n    if repair:\n        # ms.meshing_remove_t_vertices(method=0, threshold=40, repeat=True)\n        ms.meshing_repair_non_manifold_edges(method=0)\n        ms.meshing_repair_non_manifold_vertices(vertdispratio=0)\n\n    if remesh:\n        # ms.apply_coord_taubin_smoothing()\n        ms.meshing_isotropic_explicit_remeshing(\n            iterations=3, targetlen=pml.PureValue(remesh_size)\n        )\n\n    # extract mesh\n    m = ms.current_mesh()\n    verts = m.vertex_matrix()\n    faces = m.face_matrix()\n\n    print(\n        f\"[INFO] mesh cleaning: {_ori_vert_shape} --> {verts.shape}, {_ori_face_shape} --> {faces.shape}\"\n    )\n\n    return verts, faces\n"
        },
        {
          "name": "process.py",
          "type": "blob",
          "size": 3.3427734375,
          "content": "import os\nimport glob\nimport sys\nimport cv2\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nimport rembg\n\nclass BLIP2():\n    def __init__(self, device='cuda'):\n        self.device = device\n        from transformers import AutoProcessor, Blip2ForConditionalGeneration\n        self.processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n        self.model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16).to(device)\n\n    @torch.no_grad()\n    def __call__(self, image):\n        image = Image.fromarray(image)\n        inputs = self.processor(image, return_tensors=\"pt\").to(self.device, torch.float16)\n\n        generated_ids = self.model.generate(**inputs, max_new_tokens=20)\n        generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n\n        return generated_text\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('path', type=str, help=\"path to image (png, jpeg, etc.)\")\n    parser.add_argument('--model', default='u2net', type=str, help=\"rembg model, see https://github.com/danielgatis/rembg#models\")\n    parser.add_argument('--size', default=256, type=int, help=\"output resolution\")\n    parser.add_argument('--border_ratio', default=0.2, type=float, help=\"output border ratio\")\n    parser.add_argument('--recenter', type=bool, default=True, help=\"recenter, potentially not helpful for multiview zero123\")    \n    opt = parser.parse_args()\n\n    session = rembg.new_session(model_name=opt.model)\n\n    if os.path.isdir(opt.path):\n        print(f'[INFO] processing directory {opt.path}...')\n        files = glob.glob(f'{opt.path}/*')\n        out_dir = opt.path\n    else: # isfile\n        files = [opt.path]\n        out_dir = os.path.dirname(opt.path)\n    \n    for file in files:\n\n        out_base = os.path.basename(file).split('.')[0]\n        out_rgba = os.path.join(out_dir, out_base + '_rgba.png')\n\n        # load image\n        print(f'[INFO] loading image {file}...')\n        image = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n        \n        # carve background\n        print(f'[INFO] background removal...')\n        carved_image = rembg.remove(image, session=session) # [H, W, 4]\n        mask = carved_image[..., -1] > 0\n\n        # recenter\n        if opt.recenter:\n            print(f'[INFO] recenter...')\n            final_rgba = np.zeros((opt.size, opt.size, 4), dtype=np.uint8)\n            \n            coords = np.nonzero(mask)\n            x_min, x_max = coords[0].min(), coords[0].max()\n            y_min, y_max = coords[1].min(), coords[1].max()\n            h = x_max - x_min\n            w = y_max - y_min\n            desired_size = int(opt.size * (1 - opt.border_ratio))\n            scale = desired_size / max(h, w)\n            h2 = int(h * scale)\n            w2 = int(w * scale)\n            x2_min = (opt.size - h2) // 2\n            x2_max = x2_min + h2\n            y2_min = (opt.size - w2) // 2\n            y2_max = y2_min + w2\n            final_rgba[x2_min:x2_max, y2_min:y2_max] = cv2.resize(carved_image[x_min:x_max, y_min:y_max], (w2, h2), interpolation=cv2.INTER_AREA)\n            \n        else:\n            final_rgba = carved_image\n        \n        # write image\n        cv2.imwrite(out_rgba, final_rgba)"
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 7.6396484375,
          "content": "# DreamGaussian\n\nThis repository contains the official implementation for [DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation](https://arxiv.org/abs/2309.16653).\n\n### [Project Page](https://dreamgaussian.github.io) | [Arxiv](https://arxiv.org/abs/2309.16653)\n\nhttps://github.com/dreamgaussian/dreamgaussian/assets/25863658/db860801-7b9c-4b30-9eb9-87330175f5c8\n\n### News\n\n- 2023.12.22: add experimental support for [ImageDream](https://github.com/bytedance/ImageDream), check [imagedream.yaml](./configs/image_sai.yaml).\n- 2023.12.14: add support for [Stable-Zero123](https://stability.ai/news/stable-zero123-3d-generation), check [image_sai.yaml](./configs/image_sai.yaml).\n- 2023.10.21: add support for [MVDream](https://github.com/bytedance/MVDream), check [text_mv.yaml](./configs/text_mv.yaml).\n\n### [Colab demo](https://github.com/camenduru/dreamgaussian-colab)\n\n- Image-to-3D: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1sLpYmmLS209-e5eHgcuqdryFRRO6ZhFS?usp=sharing)\n- Text-to-3D: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/dreamgaussian-colab/blob/main/dreamgaussian_colab.ipynb)\n\n### [Gradio demo](https://huggingface.co/spaces/jiawei011/dreamgaussian)\n\n- Image-to-3D: <a href=\"https://huggingface.co/spaces/jiawei011/dreamgaussian\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Gradio%20Demo-Huggingface-orange\"></a>\n- Run Gradio demo on Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1owXJthskHoVXBNvxUB0Bg0JP2Rc7QsTe?usp=sharing)\n\n## Install\n\n```bash\npip install -r requirements.txt\n\n# a modified gaussian splatting (+ depth, alpha rendering)\ngit clone --recursive https://github.com/ashawkey/diff-gaussian-rasterization\npip install ./diff-gaussian-rasterization\n\n# simple-knn\npip install ./simple-knn\n\n# nvdiffrast\npip install git+https://github.com/NVlabs/nvdiffrast/\n\n# kiuikit\npip install git+https://github.com/ashawkey/kiuikit\n\n# To use MVdream, also install:\npip install git+https://github.com/bytedance/MVDream\n\n# To use ImageDream, also install:\npip install git+https://github.com/bytedance/ImageDream/#subdirectory=extern/ImageDream\n```\n\nTested on:\n\n- Ubuntu 22 with torch 1.12 & CUDA 11.6 on a V100.\n- Windows 10 with torch 2.1 & CUDA 12.1 on a 3070.\n\n## Usage\n\nImage-to-3D:\n\n```bash\n### preprocess\n# background removal and recentering, save rgba at 256x256\npython process.py data/name.jpg\n\n# save at a larger resolution\npython process.py data/name.jpg --size 512\n\n# process all jpg images under a dir\npython process.py data\n\n### training gaussian stage\n# train 500 iters (~1min) and export ckpt & coarse_mesh to logs\npython main.py --config configs/image.yaml input=data/name_rgba.png save_path=name\n\n# gui mode (supports visualizing training)\npython main.py --config configs/image.yaml input=data/name_rgba.png save_path=name gui=True\n\n# load and visualize a saved ckpt\npython main.py --config configs/image.yaml load=logs/name_model.ply gui=True\n\n# use an estimated elevation angle if image is not front-view (e.g., common looking-down image can use -30)\npython main.py --config configs/image.yaml input=data/name_rgba.png save_path=name elevation=-30\n\n### training mesh stage\n# auto load coarse_mesh and refine 50 iters (~1min), export fine_mesh to logs\npython main2.py --config configs/image.yaml input=data/name_rgba.png save_path=name\n\n# specify coarse mesh path explicity\npython main2.py --config configs/image.yaml input=data/name_rgba.png save_path=name mesh=logs/name_mesh.obj\n\n# gui mode\npython main2.py --config configs/image.yaml input=data/name_rgba.png save_path=name gui=True\n\n# export glb instead of obj\npython main2.py --config configs/image.yaml input=data/name_rgba.png save_path=name mesh_format=glb\n\n### visualization\n# gui for visualizing mesh\n# `kire` is short for `python -m kiui.render`\nkire logs/name.obj\n\n# save 360 degree video of mesh (can run without gui)\nkire logs/name.obj --save_video name.mp4 --wogui\n\n# save 8 view images of mesh (can run without gui)\nkire logs/name.obj --save images/name/ --wogui\n\n### evaluation of CLIP-similarity\npython -m kiui.cli.clip_sim data/name_rgba.png logs/name.obj\n```\n\nPlease check `./configs/image.yaml` for more options.\n\nImage-to-3D (stable-zero123):\n\n```bash\n### training gaussian stage\npython main.py --config configs/image_sai.yaml input=data/name_rgba.png save_path=name\n\n### training mesh stage\npython main2.py --config configs/image_sai.yaml input=data/name_rgba.png save_path=name\n```\n\nText-to-3D:\n\n```bash\n### training gaussian stage\npython main.py --config configs/text.yaml prompt=\"a photo of an icecream\" save_path=icecream\n\n### training mesh stage\npython main2.py --config configs/text.yaml prompt=\"a photo of an icecream\" save_path=icecream\n```\n\nPlease check `./configs/text.yaml` for more options.\n\nText-to-3D (MVDream):\n\n```bash\n### training gaussian stage\npython main.py --config configs/text_mv.yaml prompt=\"a plush toy of a corgi nurse\" save_path=corgi_nurse\n\n### training mesh stage\npython main2.py --config configs/text_mv.yaml prompt=\"a plush toy of a corgi nurse\" save_path=corgi_nurse\n```\n\nPlease check `./configs/text_mv.yaml` for more options.\n\nImage+Text-to-3D (ImageDream):\n\n```bash\n### training gaussian stage\npython main.py --config configs/imagedream.yaml input=data/ghost_rgba.png prompt=\"a ghost eating hamburger\" save_path=ghost\n\n### training mesh stage\npython main2.py --config configs/imagedream.yaml input=data/ghost_rgba.png prompt=\"a ghost eating hamburger\" save_path=ghost\n```\n\nHelper scripts:\n\n```bash\n# run all image samples (*_rgba.png) in ./data\npython scripts/runall.py --dir ./data --gpu 0\n\n# run all text samples (hardcoded in runall_sd.py)\npython scripts/runall_sd.py --gpu 0\n\n# export all ./logs/*.obj to mp4 in ./videos\npython scripts/convert_obj_to_video.py --dir ./logs\n```\n\nGradio Demo:\n\n```bash\npython gradio_app.py\n```\n\n## Tips\n* The world & camera coordinate system is the same as OpenGL:\n```\n    World            Camera        \n  \n     +y              up  target                                              \n     |               |  /                                            \n     |               | /                                                \n     |______+x       |/______right                                      \n    /                /         \n   /                /          \n  /                /           \n +z               forward           \n\nelevation: in (-90, 90), from +y to -y is (-90, 90)\nazimuth: in (-180, 180), from +z to +x is (0, 90)\n```\n\n* Trouble shooting OpenGL errors (e.g., `[F glutil.cpp:338] eglInitialize() failed`): \n```bash\n# either try to install OpenGL correctly (usually installed with the Nvidia driver), or use force_cuda_rast:\npython main.py --config configs/image_sai.yaml input=data/name_rgba.png save_path=name force_cuda_rast=True\n\nkire mesh.obj --force_cuda_rast\n```\n\n## Acknowledgement\n\nThis work is built on many amazing research works and open-source projects, thanks a lot to all the authors for sharing!\n\n- [gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting) and [diff-gaussian-rasterization](https://github.com/graphdeco-inria/diff-gaussian-rasterization)\n- [threestudio](https://github.com/threestudio-project/threestudio)\n- [nvdiffrast](https://github.com/NVlabs/nvdiffrast)\n- [dearpygui](https://github.com/hoffstadt/DearPyGui)\n\n## Citation\n\n```\n@article{tang2023dreamgaussian,\n  title={DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation},\n  author={Tang, Jiaxiang and Ren, Jiawei and Zhou, Hang and Liu, Ziwei and Zeng, Gang},\n  journal={arXiv preprint arXiv:2309.16653},\n  year={2023}\n}\n```\n"
        },
        {
          "name": "requirements.lock.txt",
          "type": "blob",
          "size": 0.5888671875,
          "content": "tqdm == 4.65.2\nrich == 13.4.2\nninja == 1.10.2.3\nnumpy == 1.22.4\npandas == 1.4.2\nscipy == 1.11.4\nscikit-learn == 1.0.2\nmatplotlib == 3.8.0\nopencv-python == 4.8.0.76\nimageio == 2.21.1\nimageio-ffmpeg == 0.4.8\nomegaconf == 2.3.0\n\ntorch == 2.1.0+cu121\neinops == 0.4.1\nplyfile == 0.7.4\npygltflib == 1.16.0\n\n# for gui\ndearpygui == 1.10.1\n\n# for stable-diffusion\nhuggingface_hub == 0.19.4\ndiffusers == 0.24.0\naccelerate == 0.24.1\ntransformers == 4.36.2\n\n# for dmtet and mesh export\nxatlas == 0.0.6\ntrimesh == 4.0.5\nPyMCubes == 0.1.2\npymeshlab == 2023.12\n\nrembg[gpu,cli] == 2.0.49\n\n# gradio demo\ngradio == 4.8.0\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3251953125,
          "content": "tqdm\nrich\nninja\nnumpy\npandas\nscipy\nscikit-learn\nmatplotlib\nopencv-python\nimageio\nimageio-ffmpeg\nomegaconf\n\ntorch\neinops\nplyfile\npygltflib\n\n# for gui\ndearpygui\n\n# for stable-diffusion\nhuggingface_hub\ndiffusers\naccelerate\ntransformers\n\n# for dmtet and mesh export\nxatlas\ntrimesh\nPyMCubes\npymeshlab\n\nrembg[gpu,cli]\n\n# gradio demo\ngradio"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "sh_utils.py",
          "type": "blob",
          "size": 4.2685546875,
          "content": "#  Copyright 2021 The PlenOctree Authors.\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n#  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n#  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n#  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n#  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n#  POSSIBILITY OF SUCH DAMAGE.\n\nimport torch\n\nC0 = 0.28209479177387814\nC1 = 0.4886025119029199\nC2 = [\n    1.0925484305920792,\n    -1.0925484305920792,\n    0.31539156525252005,\n    -1.0925484305920792,\n    0.5462742152960396\n]\nC3 = [\n    -0.5900435899266435,\n    2.890611442640554,\n    -0.4570457994644658,\n    0.3731763325901154,\n    -0.4570457994644658,\n    1.445305721320277,\n    -0.5900435899266435\n]\nC4 = [\n    2.5033429417967046,\n    -1.7701307697799304,\n    0.9461746957575601,\n    -0.6690465435572892,\n    0.10578554691520431,\n    -0.6690465435572892,\n    0.47308734787878004,\n    -1.7701307697799304,\n    0.6258357354491761,\n]   \n\n\ndef eval_sh(deg, sh, dirs):\n    \"\"\"\n    Evaluate spherical harmonics at unit directions\n    using hardcoded SH polynomials.\n    Works with torch/np/jnp.\n    ... Can be 0 or more batch dimensions.\n    Args:\n        deg: int SH deg. Currently, 0-3 supported\n        sh: jnp.ndarray SH coeffs [..., C, (deg + 1) ** 2]\n        dirs: jnp.ndarray unit directions [..., 3]\n    Returns:\n        [..., C]\n    \"\"\"\n    assert deg <= 4 and deg >= 0\n    coeff = (deg + 1) ** 2\n    assert sh.shape[-1] >= coeff\n\n    result = C0 * sh[..., 0]\n    if deg > 0:\n        x, y, z = dirs[..., 0:1], dirs[..., 1:2], dirs[..., 2:3]\n        result = (result -\n                C1 * y * sh[..., 1] +\n                C1 * z * sh[..., 2] -\n                C1 * x * sh[..., 3])\n\n        if deg > 1:\n            xx, yy, zz = x * x, y * y, z * z\n            xy, yz, xz = x * y, y * z, x * z\n            result = (result +\n                    C2[0] * xy * sh[..., 4] +\n                    C2[1] * yz * sh[..., 5] +\n                    C2[2] * (2.0 * zz - xx - yy) * sh[..., 6] +\n                    C2[3] * xz * sh[..., 7] +\n                    C2[4] * (xx - yy) * sh[..., 8])\n\n            if deg > 2:\n                result = (result +\n                C3[0] * y * (3 * xx - yy) * sh[..., 9] +\n                C3[1] * xy * z * sh[..., 10] +\n                C3[2] * y * (4 * zz - xx - yy)* sh[..., 11] +\n                C3[3] * z * (2 * zz - 3 * xx - 3 * yy) * sh[..., 12] +\n                C3[4] * x * (4 * zz - xx - yy) * sh[..., 13] +\n                C3[5] * z * (xx - yy) * sh[..., 14] +\n                C3[6] * x * (xx - 3 * yy) * sh[..., 15])\n\n                if deg > 3:\n                    result = (result + C4[0] * xy * (xx - yy) * sh[..., 16] +\n                            C4[1] * yz * (3 * xx - yy) * sh[..., 17] +\n                            C4[2] * xy * (7 * zz - 1) * sh[..., 18] +\n                            C4[3] * yz * (7 * zz - 3) * sh[..., 19] +\n                            C4[4] * (zz * (35 * zz - 30) + 3) * sh[..., 20] +\n                            C4[5] * xz * (7 * zz - 3) * sh[..., 21] +\n                            C4[6] * (xx - yy) * (7 * zz - 1) * sh[..., 22] +\n                            C4[7] * xz * (xx - 3 * yy) * sh[..., 23] +\n                            C4[8] * (xx * (xx - 3 * yy) - yy * (3 * xx - yy)) * sh[..., 24])\n    return result\n\ndef RGB2SH(rgb):\n    return (rgb - 0.5) / C0\n\ndef SH2RGB(sh):\n    return sh * C0 + 0.5"
        },
        {
          "name": "simple-knn",
          "type": "tree",
          "content": null
        },
        {
          "name": "zero123.py",
          "type": "blob",
          "size": 29.91015625,
          "content": "# Copyright 2023 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nimport math\nimport warnings\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport PIL\nimport torch\nimport torchvision.transforms.functional as TF\nfrom diffusers.configuration_utils import ConfigMixin, FrozenDict, register_to_config\nfrom diffusers.image_processor import VaeImageProcessor\nfrom diffusers.models import AutoencoderKL, UNet2DConditionModel\nfrom diffusers.models.modeling_utils import ModelMixin\nfrom diffusers.pipelines.pipeline_utils import DiffusionPipeline\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\nfrom diffusers.pipelines.stable_diffusion.safety_checker import (\n    StableDiffusionSafetyChecker,\n)\nfrom diffusers.schedulers import KarrasDiffusionSchedulers\nfrom diffusers.utils import deprecate, is_accelerate_available, logging\nfrom diffusers.utils.torch_utils import randn_tensor\nfrom packaging import version\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\nclass CLIPCameraProjection(ModelMixin, ConfigMixin):\n    \"\"\"\n    A Projection layer for CLIP embedding and camera embedding.\n\n    Parameters:\n        embedding_dim (`int`, *optional*, defaults to 768): The dimension of the model input `clip_embed`\n        additional_embeddings (`int`, *optional*, defaults to 4): The number of additional tokens appended to the\n            projected `hidden_states`. The actual length of the used `hidden_states` is `num_embeddings +\n            additional_embeddings`.\n    \"\"\"\n\n    @register_to_config\n    def __init__(self, embedding_dim: int = 768, additional_embeddings: int = 4):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.additional_embeddings = additional_embeddings\n\n        self.input_dim = self.embedding_dim + self.additional_embeddings\n        self.output_dim = self.embedding_dim\n\n        self.proj = torch.nn.Linear(self.input_dim, self.output_dim)\n\n    def forward(\n        self,\n        embedding: torch.FloatTensor,\n    ):\n        \"\"\"\n        The [`PriorTransformer`] forward method.\n\n        Args:\n            hidden_states (`torch.FloatTensor` of shape `(batch_size, input_dim)`):\n                The currently input embeddings.\n\n        Returns:\n            The output embedding projection (`torch.FloatTensor` of shape `(batch_size, output_dim)`).\n        \"\"\"\n        proj_embedding = self.proj(embedding)\n        return proj_embedding\n\n\nclass Zero123Pipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline to generate variations from an input image using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        image_encoder ([`CLIPVisionModelWithProjection`]):\n            Frozen CLIP image-encoder. Stable Diffusion Image Variation uses the vision portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPVisionModelWithProjection),\n            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n        feature_extractor ([`CLIPImageProcessor`]):\n            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n    \"\"\"\n    # TODO: feature_extractor is required to encode images (if they are in PIL format),\n    # we should give a descriptive message if the pipeline doesn't have one.\n    _optional_components = [\"safety_checker\"]\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        image_encoder: CLIPVisionModelWithProjection,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPImageProcessor,\n        clip_camera_projection: CLIPCameraProjection,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warn(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        is_unet_version_less_0_9_0 = hasattr(\n            unet.config, \"_diffusers_version\"\n        ) and version.parse(\n            version.parse(unet.config._diffusers_version).base_version\n        ) < version.parse(\n            \"0.9.0.dev0\"\n        )\n        is_unet_sample_size_less_64 = (\n            hasattr(unet.config, \"sample_size\") and unet.config.sample_size < 64\n        )\n        if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:\n            deprecation_message = (\n                \"The configuration file of the unet has set the default `sample_size` to smaller than\"\n                \" 64 which seems highly unlikely .If you're checkpoint is a fine-tuned version of any of the\"\n                \" following: \\n- CompVis/stable-diffusion-v1-4 \\n- CompVis/stable-diffusion-v1-3 \\n-\"\n                \" CompVis/stable-diffusion-v1-2 \\n- CompVis/stable-diffusion-v1-1 \\n- runwayml/stable-diffusion-v1-5\"\n                \" \\n- runwayml/stable-diffusion-inpainting \\n you should change 'sample_size' to 64 in the\"\n                \" configuration file. Please make sure to update the config accordingly as leaving `sample_size=32`\"\n                \" in the config might lead to incorrect results in future versions. If you have downloaded this\"\n                \" checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for\"\n                \" the `unet/config.json` file\"\n            )\n            deprecate(\n                \"sample_size<64\", \"1.0.0\", deprecation_message, standard_warn=False\n            )\n            new_config = dict(unet.config)\n            new_config[\"sample_size\"] = 64\n            unet._internal_dict = FrozenDict(new_config)\n\n        self.register_modules(\n            vae=vae,\n            image_encoder=image_encoder,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n            clip_camera_projection=clip_camera_projection,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n        self.register_to_config(requires_safety_checker=requires_safety_checker)\n\n    def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [\n            self.unet,\n            self.image_encoder,\n            self.vae,\n            self.safety_checker,\n        ]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)\n\n    @property\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._execution_device\n    def _execution_device(self):\n        r\"\"\"\n        Returns the device on which the pipeline's models will be executed. After calling\n        `pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module\n        hooks.\n        \"\"\"\n        if not hasattr(self.unet, \"_hf_hook\"):\n            return self.device\n        for module in self.unet.modules():\n            if (\n                hasattr(module, \"_hf_hook\")\n                and hasattr(module._hf_hook, \"execution_device\")\n                and module._hf_hook.execution_device is not None\n            ):\n                return torch.device(module._hf_hook.execution_device)\n        return self.device\n\n    def _encode_image(\n        self,\n        image,\n        elevation,\n        azimuth,\n        distance,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        clip_image_embeddings=None,\n        image_camera_embeddings=None,\n    ):\n        dtype = next(self.image_encoder.parameters()).dtype\n\n        if image_camera_embeddings is None:\n            if image is None:\n                assert clip_image_embeddings is not None\n                image_embeddings = clip_image_embeddings.to(device=device, dtype=dtype)\n            else:\n                if not isinstance(image, torch.Tensor):\n                    image = self.feature_extractor(\n                        images=image, return_tensors=\"pt\"\n                    ).pixel_values\n\n                image = image.to(device=device, dtype=dtype)\n                image_embeddings = self.image_encoder(image).image_embeds\n                image_embeddings = image_embeddings.unsqueeze(1)\n\n            bs_embed, seq_len, _ = image_embeddings.shape\n\n            if isinstance(elevation, float):\n                elevation = torch.as_tensor(\n                    [elevation] * bs_embed, dtype=dtype, device=device\n                )\n            if isinstance(azimuth, float):\n                azimuth = torch.as_tensor(\n                    [azimuth] * bs_embed, dtype=dtype, device=device\n                )\n            if isinstance(distance, float):\n                distance = torch.as_tensor(\n                    [distance] * bs_embed, dtype=dtype, device=device\n                )\n\n            camera_embeddings = torch.stack(\n                [\n                    torch.deg2rad(elevation),\n                    torch.sin(torch.deg2rad(azimuth)),\n                    torch.cos(torch.deg2rad(azimuth)),\n                    distance,\n                ],\n                dim=-1,\n            )[:, None, :]\n\n            image_embeddings = torch.cat([image_embeddings, camera_embeddings], dim=-1)\n\n            # project (image, camera) embeddings to the same dimension as clip embeddings\n            image_embeddings = self.clip_camera_projection(image_embeddings)\n        else:\n            image_embeddings = image_camera_embeddings.to(device=device, dtype=dtype)\n            bs_embed, seq_len, _ = image_embeddings.shape\n\n        # duplicate image embeddings for each generation per prompt, using mps friendly method\n        image_embeddings = image_embeddings.repeat(1, num_images_per_prompt, 1)\n        image_embeddings = image_embeddings.view(\n            bs_embed * num_images_per_prompt, seq_len, -1\n        )\n\n        if do_classifier_free_guidance:\n            negative_prompt_embeds = torch.zeros_like(image_embeddings)\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])\n\n        return image_embeddings\n\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.run_safety_checker\n    def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is None:\n            has_nsfw_concept = None\n        else:\n            if torch.is_tensor(image):\n                feature_extractor_input = self.image_processor.postprocess(\n                    image, output_type=\"pil\"\n                )\n            else:\n                feature_extractor_input = self.image_processor.numpy_to_pil(image)\n            safety_checker_input = self.feature_extractor(\n                feature_extractor_input, return_tensors=\"pt\"\n            ).to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        return image, has_nsfw_concept\n\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.decode_latents\n    def decode_latents(self, latents):\n        warnings.warn(\n            \"The decode_latents method is deprecated and will be removed in a future version. Please\"\n            \" use VaeImageProcessor instead\",\n            FutureWarning,\n        )\n        latents = 1 / self.vae.config.scaling_factor * latents\n        image = self.vae.decode(latents, return_dict=False)[0]\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image\n\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n    def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(\n            inspect.signature(self.scheduler.step).parameters.keys()\n        )\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(\n            inspect.signature(self.scheduler.step).parameters.keys()\n        )\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs\n\n    def check_inputs(self, image, height, width, callback_steps):\n        # TODO: check image size or adjust image size to (height, width)\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(\n                f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\"\n            )\n\n        if (callback_steps is None) or (\n            callback_steps is not None\n            and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_latents\n    def prepare_latents(\n        self,\n        batch_size,\n        num_channels_latents,\n        height,\n        width,\n        dtype,\n        device,\n        generator,\n        latents=None,\n    ):\n        shape = (\n            batch_size,\n            num_channels_latents,\n            height // self.vae_scale_factor,\n            width // self.vae_scale_factor,\n        )\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(\n                shape, generator=generator, device=device, dtype=dtype\n            )\n        else:\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents\n\n    def _get_latent_model_input(\n        self,\n        latents: torch.FloatTensor,\n        image: Optional[\n            Union[PIL.Image.Image, List[PIL.Image.Image], torch.FloatTensor]\n        ],\n        num_images_per_prompt: int,\n        do_classifier_free_guidance: bool,\n        image_latents: Optional[torch.FloatTensor] = None,\n    ):\n        if isinstance(image, PIL.Image.Image):\n            image_pt = TF.to_tensor(image).unsqueeze(0).to(latents)\n        elif isinstance(image, list):\n            image_pt = torch.stack([TF.to_tensor(img) for img in image], dim=0).to(\n                latents\n            )\n        elif isinstance(image, torch.Tensor):\n            image_pt = image\n        else:\n            image_pt = None\n\n        if image_pt is None:\n            assert image_latents is not None\n            image_pt = image_latents.repeat_interleave(num_images_per_prompt, dim=0)\n        else:\n            image_pt = image_pt * 2.0 - 1.0  # scale to [-1, 1]\n            # FIXME: encoded latents should be multiplied with self.vae.config.scaling_factor\n            # but zero123 was not trained this way\n            image_pt = self.vae.encode(image_pt).latent_dist.mode()\n            image_pt = image_pt.repeat_interleave(num_images_per_prompt, dim=0)\n        if do_classifier_free_guidance:\n            latent_model_input = torch.cat(\n                [\n                    torch.cat([latents, latents], dim=0),\n                    torch.cat([torch.zeros_like(image_pt), image_pt], dim=0),\n                ],\n                dim=1,\n            )\n        else:\n            latent_model_input = torch.cat([latents, image_pt], dim=1)\n\n        return latent_model_input\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        image: Optional[\n            Union[PIL.Image.Image, List[PIL.Image.Image], torch.FloatTensor]\n        ] = None,\n        elevation: Optional[Union[float, torch.FloatTensor]] = None,\n        azimuth: Optional[Union[float, torch.FloatTensor]] = None,\n        distance: Optional[Union[float, torch.FloatTensor]] = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 3.0,\n        num_images_per_prompt: int = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        clip_image_embeddings: Optional[torch.FloatTensor] = None,\n        image_camera_embeddings: Optional[torch.FloatTensor] = None,\n        image_latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            image (`PIL.Image.Image` or `List[PIL.Image.Image]` or `torch.FloatTensor`):\n                The image or images to guide the image generation. If you provide a tensor, it needs to comply with the\n                configuration of\n                [this](https://huggingface.co/lambdalabs/sd-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json)\n                `CLIPImageProcessor`\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta () in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        # 1. Check inputs. Raise error if not correct\n        # TODO: check input elevation, azimuth, and distance\n        # TODO: check image, clip_image_embeddings, image_latents\n        self.check_inputs(image, height, width, callback_steps)\n\n        # 2. Define call parameters\n        if isinstance(image, PIL.Image.Image):\n            batch_size = 1\n        elif isinstance(image, list):\n            batch_size = len(image)\n        elif isinstance(image, torch.Tensor):\n            batch_size = image.shape[0]\n        else:\n            assert image_latents is not None\n            assert (\n                clip_image_embeddings is not None or image_camera_embeddings is not None\n            )\n            batch_size = image_latents.shape[0]\n\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # 3. Encode input image\n        if isinstance(image, PIL.Image.Image) or isinstance(image, list):\n            pil_image = image\n        elif isinstance(image, torch.Tensor):\n            pil_image = [TF.to_pil_image(image[i]) for i in range(image.shape[0])]\n        else:\n            pil_image = None\n        image_embeddings = self._encode_image(\n            pil_image,\n            elevation,\n            azimuth,\n            distance,\n            device,\n            num_images_per_prompt,\n            do_classifier_free_guidance,\n            clip_image_embeddings,\n            image_camera_embeddings,\n        )\n\n        # 4. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n        # 5. Prepare latent variables\n        # num_channels_latents = self.unet.config.in_channels\n        num_channels_latents = 4  # FIXME: hard-coded\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            image_embeddings.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 7. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = self._get_latent_model_input(\n                    latents,\n                    image,\n                    num_images_per_prompt,\n                    do_classifier_free_guidance,\n                    image_latents,\n                )\n                latent_model_input = self.scheduler.scale_model_input(\n                    latent_model_input, t\n                )\n\n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=image_embeddings,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                ).sample\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (\n                        noise_pred_text - noise_pred_uncond\n                    )\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(\n                    noise_pred, t, latents, **extra_step_kwargs\n                ).prev_sample\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or (\n                    (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n                ):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n\n        if not output_type == \"latent\":\n            image = self.vae.decode(\n                latents / self.vae.config.scaling_factor, return_dict=False\n            )[0]\n            image, has_nsfw_concept = self.run_safety_checker(\n                image, device, image_embeddings.dtype\n            )\n        else:\n            image = latents\n            has_nsfw_concept = None\n\n        if has_nsfw_concept is None:\n            do_denormalize = [True] * image.shape[0]\n        else:\n            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n\n        image = self.image_processor.postprocess(\n            image, output_type=output_type, do_denormalize=do_denormalize\n        )\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(\n            images=image, nsfw_content_detected=has_nsfw_concept\n        )"
        }
      ]
    }
  ]
}