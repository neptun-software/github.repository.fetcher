{
  "metadata": {
    "timestamp": 1736559843766,
    "page": 590,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "TDAmeritrade/stumpy",
      "stars": 3723,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.3193359375,
          "content": "[flake8]\nignore = \n    E741,\n    W503,\n    E203,\n    D100,\n    D401,\n    D200,\n    D205,\n    D400,\n    D301\nmax-line-length = 88\ndocstring-convention=numpy\nexclude = \n    docs,\n    setup.py\nper-file-ignores =\n    stumpy/__init__.py:D104\n    build/lib/stumpy/__init__.py:D104\n    tests/*:D101,D102,D103\n    stumpy/cache.py:D414\n"
        },
        {
          "name": ".fossa.yml",
          "type": "blob",
          "size": 0.28515625,
          "content": "# Generated by FOSSA CLI (https://github.com/fossas/fossa-cli)\n# Visit https://fossa.com to learn more\n\nversion: 2\ncli:\n  server: https://app.fossa.com\n  fetcher: custom\n  project: https://github.com/TDAmeritrade/stumpy\nanalyze:\n  modules:\n  - name: .\n    type: pip\n    target: .\n    path: .\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0595703125,
          "content": "*.py linguist-language=Python\n*.ipynb linguist-documentation\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.244140625,
          "content": ".idea\n*csv\n*checkout\n*pyc\n.cache\n*ipynb_checkpoints\n.pytest_cache\nLOG*\nPID\n.coverage*\ncoverage.xml\nstumpy.coverage.xml\ndask-worker-space\nstumpy.egg-info\nbuild\ndist\n.vscode\ndocs/_build\n*DS_Store*\n.venv\n.mypy_cache\n.directory\ntest.py\n*.nbconvert.ipynb\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.267578125,
          "content": "version: 2\nformats: []\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\npython:\n    install:\n        - requirements: docs/requirements.txt\n        - requirements: requirements.txt\n        - method: pip\n          path: .\nsubmodules:\n    include: all\n    recursive: true\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 27.17578125,
          "content": "# 2024-07-08    [ 1.13.0 ]:\n---------------------------\n* bugfixes\n  - Fixed Ostinato overwriting original time series\n* features\n  - Added Ray support\n  - Added `numpy` 2.0 support\n  - Added named attributes to matrix profile array\n  - Added Python 3.12 support\n  - Migrated setup.py, setuptools to pyproject.toml\n* tasks\n  - Added version mismatch checker\n  - Added `copy` param to `preprocess`-related functions\n  - Disabled bokeh dashboard in dask\n  - Added `numba` channel to environment.yml\n  - Replace `np.INF` with `-np.infg`\n  - Fixed inability to import packages in tutorials\n  - Added matplotlib to RTD requirements\n  - Removed unnecessary comments in code\n  - Added `tests/__pycache__` to `clean_up` process\n  - Added option to display current dev environment in `test.sh`\n  - Added keyword `test.sh` to only execute `gpu` containing tests\n  - Added \"upgrade pip\" to Github Actions workflow\n  - Removed Twitter and Zenodo badges, added NumFOCUS badge\n  - Added Github Discussions link\n  - Updated codecove version for Github Actions\n  - Removed `pkg_resources` as a dependency\n  - Added codeowners file\n* documentation\n  - Improved syntax highlighting\n  - Updated class docstrings\n  - Removed napoleon extension\n  - Switched to Myst\n  - Relocated notebooks/tutorials\n  - Fixed named attributes being displayed incorrectly\n  - Fixed typos\n\n\n# 2023-08-20    [ 1.12.0 ]:\n---------------------------\n* bugfixes\n  - Fixed prescrump, scrump, and scraamp miscalculation of AB-joins\n  - Fixed bug in `is False` by converting to `bool` type\n  - Fixed bug in `snippet` that caused loss in precision, add unit tests\n  - Fixed RTD incompatibility with urllib3\n  - Fixed loss of precision in distances computed for self-matches\n  - Fixed libiomp5.dylib Github Actions location\n  - Fixed incorrect post-processing in naive.mass_PI\n  - Fixed incorrect number of contiguous windows in snippets\n* features\n  - Improved matrix profile performance (15-20%) with uint64 indexing\n  - Added top-k nearest neighbor feature\n  - Added rolling_isconstant function\n  - Added `subseq_isconstant` to API for transparent handling of constant time series subsequences\n  - Added parallelized rolling_nanstd\n  - Added abstraction layer for distributed client functions (e.g., Dask, Ray, etc)\n  - Added initial support for `numba` function caching\n  - Added Python 3.11 support\n  - Added `query_idx` to improve distances computed for self-matches in `motifs` function\n  - Added `mmotifs` for multi-dimensional motif discovery\n  - Added function `process_isconstant`\n* tasks\n  - Refactored mpdist\n  - Added MERLIN notebook reproducer\n  - Added negative index checks to mmotifs\n  - Fixed mybinder badge and links\n  - Renamed nave.mass to naive.mass_PI\n  - Added `row_wise` parameter to naive.stump\n  - Added input type (list) check to ostinato\n  - Added ability to return fully filled bfs indices\n  - Updated setup.py to enable Github dependency graph tracking\n  - Improved stability of prescrimp\n  - Split coverage tests for more verbosity\n  - Added more features to custom test function\n  - Removed explicit cancellation of dask futures\n  - Removed max `dask`/`distributed` version requirement\n  - Optimized stumpi and aampi class init\n  - Improve aampi update behavior with constant sequences returning nan\n  - Added pytest notebook link checking\n  - Refactored `match` function\n  - Added warnings to motifs and aamp_motifs\n  - Added `numba -s` step in Github Actions workflow\n  - Added explicit link to OpenMP for MacOS Github Actions workflow\n  - Moved to Actions/Checkout V3 in Github Actions\n  - Improved numba function signatures\n  - Added '--editable' install mode to setup.sh\n  - Updated Github Action codecov/codecov-action@v1 to v3\n  - Maintained 100% code coverage\n  - Used unittest.mock.patch to prevent overwriting of config variables during testing\n  - Added Python 3.11 to test matrix\n  - Added ability to detect missing parameter definitions in docstrings (docstring.py)\n  - Updated pytest flags to report skips and added additional summary\n  - Replaced logging.warning with warnings.warn\n  - Improved multi-line warnings\n  - Ensured `bfs_indices` are sent to correct GPU device\n  - Refactored window size check in mass/mass_absolute functions\n  - Added animated GIF to README\n  - Updated minimum black version\n  - Removed `_parallel_rolling_func` as it conflicted with `numba` caching\n  - Removed mamba timeout\n  - Improved warnings\n  - Added missing p-norm param to idx_to_mp and floss functions\n  - Added test failure when coverage is below 100%\n  - Added boolean array test for rolling_isfinite\n  - Added isort, resolved circular dependencies, updated examples\n  - Only build HTML for RTD\n  - Added `mp` param to stumpi to allow pre-computed matrix profile as input\n  - Added various unit tests\n  - Removed codecov as dependency\n  - Added ability to test the execution of tutorial notebooks\n  - Added check for negative matrix profile indices\n  - Added `test_precision.py` for all issues related to loss-of-precision\n  - Fixed tls deprecation warning\n  - Replaced np.int with np.int64\n  - Specified fastmath flags to include nan/inf values in inputs/outputs\n  - Replaced bool dtype with np.bool_\n  - Replaced np.newaxis with np.expand_dims\n  - Added check for docstring and parameter mismatch\n  - Update URLs for minimum version references\n  - Added explicit shell declaration in Github Actions workflow\n  - Show OpenMP libraries in Github Actions workflow\n  - Added `pip.sh` script for setting up dev environment using `pip`\n  - Removed parallel=True in `core._compute_multi_PI`\n  - Updated coverage testing to include all modules\n  - Improved code consistency for `T_A` and `T_A` definitions\n  - Refactored `test.sh` and include missing test files in unit tests\n  - Added minimum dependency compatibility script (min.py)\n  - Updated minimum dependency bumping instructions\n  - Bumped minimum Python version to 3.8\n  - Update PyPI downloads badge\n* documentation\n  - Improved/updated various docstrings\n  - Added shapelet discovery tutorial\n  - Clarified unanchored chain description\n  - Fixed typos\n  - Improved `core._get_QT docstring`\n  - Fixed imbalanced tree representation in docstring\n  - Improved scrump documentation\n  - Made light mode default and remove theme switcher from header nav bar\n  - Improved dataframe layout display in tutorials\n  - Added multi-dimensional motif and match tutorial\n  = Added T_subseq_isfinite to docstring\n  - Added tutorial for \"Discovering motifs under uniform scaling\"\n  - Updated docs for using a dask client\n  - Fixed malformed link in floss docstring\n  - Added missing parameter section in various docstrings\n  - Added Minkowski docstring for Euclidean distance\n  - Added missing parameters for GPU functions in docstrings\n  - Improved documentation for `P` in motifs function\n\n\n# 2022-03-31    [ 1.11.1 ]:\n---------------------------\n* bugfixes\n  - Fixed #582 Allow 1D mean/stddev inputs for `stumpy.match`\n* features\n  - N/A\n* tasks\n  - Added mmotifs and aamp_mmotifs to __init__.py\n* documentation\n  - Added mmotifs docstring in RTD API\n\n\n# 2022-03-21    [ 1.11.0 ]:\n---------------------------\n* bugfixes\n  - Fixed #576 Incorrect stimp, stimped, gpu_stimp normalize rerouting\n  - Fixed bad index in `naive.stimp` normalization method\n  - Fixed unit tests\n  - Fixed `cutoff=np.inf` edge case in `_motifs` function\n* features\n  - Added `mmotifs` and `aamp_mmotifs` functions for multi-dimensional motif discovery and unit tests\n  - Added all AAMP/p-norm `stimp` implementations\n  - Added `atol` parameter to `motifs` function\n  - Added parallelized `prescraamp`\n  - Added parallelized `prescrump`\n  - Added `_get_ranges` function\n  - Added `shoelace` formula for computing total diagonal ndists\n  - Added `p_norm` support\n  - Added `_preprocess` function\n  - Added minimum description length function, `mdl`\n  - Added Python 3.10 support\n  - Added AAMP/p-norm support for pan matrix profiles\n* tasks\n  - Fixed typos\n  - Added `gpu_aamp_stimp` driver error\n  - Increased minimum dependencies\n  - Updated coverage testings to include all unit tests and additional modules\n  - Added `pip.sh` script for setting dev environment\n  - Replaced `argsort` with `argmin`/`argmax`\n  - Refactored redundant preprocessing steps\n  - Replaced deprecated `scipy.ndimage.filters` module\n  - Updated conda environment installation steps in `conda.sh` script\n  - Updated `subspace` vs `subspaces` definition\n  - Replaced `py.test` with `pytest`\n  - Updated minimum `black` version to 22.1.0\n  - Replaced elbow method with `mdl` in multi-dimensional motif tutorial \n  - Converted `float`/`int` type to `np.float64`/`np.int64`\n  - Added `dtype` check and fill value to `apply_excl_zone` function\n  - Added note on anti-correlated subsequences\n* documentation\n  - Updated README and various docstrings\n  - Added MPdist tutorial draft\n  - Added annotation vector tutorial\n  - Added geometric time series chains tutorial draft\n  - Added top-K motif section to motif discovery tutorial\n\n\n# 2021-12-15    [ 1.10.2 ]:\n---------------------------\n* bugfixes\n  - Fixed #501 Allow `max_distance = np.inf` in match function\n* features\n  - Added `atol=1e-8` parameter to match and motifs functions \n* tasks\n  - Fixed typos\n  - Removed conda download badge, updated PyPI download badge\n  - Added removal __pycache__ in test clean up phase\n* documentation\n  - Updated pan matrix profile tutorial\n  - Updated docstring for match and motifs functions\n\n\n# 2021-12-15    [ 1.10.1 ]:\n---------------------------\n* bugfixes\n  - Fixed #495 Reduce import time by removing Numba NJIT signatures \n* features\n  - Added multi_distance_profile function\n* tasks\n  - Refactored _query_mstump_profile (see multi_distance_profile)\n  - Added SVG STUMPY logo\n* documentation\n  - Refactored tutorial\n\n\n# 2021-11-02    [ 1.10.0 ]:\n---------------------------\n* bugfixes\n  - Raise TypeError for non np.float64 input arrays\n  - Fixed NumbaPerformanceWarning backwards compatibility\n* features\n  - Converted all dtypes to np.float64/np.int64\n  - Added explicit NJIT function signatures\n  - Added Python 3.9 support\n  - Added core._idx_to_mp convenience function\n  - Added Nan/Inf support for motifs and matches functions\n* tasks\n  - Removed \"in-tree\" build flag for pip\n  - Added pypi stats query for bigquery\n  - Set explicit sphinx version for RTD\n  - Fixed typos\n  - Replaced direct installation via setup.py\n  - Added numpydoc\n  - Removed ipywidgets from requirements\n  - Removed Azure Pipelines from CI\n  - Used official OSI name in the license metadata, added license name in classifiers section\n  - Added Github Citation BIB format\n  - Fixed broken tutorial links\n* documentation\n  - Fixed multi-dimensional matrix profile description\n  - Added clear warning in discord section of MSTUMP tutorial\n  - Added API examples to all docstrings\n  - Updated docstrings to use numpy.ndarray\n  - Added interactive threshold example for STIMP\n  - Updated matplotlib style sheet to use URL for all tutorials\n\n\n# 2021-07-28    [ 1.9.2 ]:\n--------------------------\n* bugfixes\n  - Fixed cutoff parameter not being used in motifs.py\n* features\n  - Unified motif discovery and pattern matching tools\n* tasks\n  - Added binder link to tutorial\n* documentation\n  - Updated pattern matching tutorial\n  - Added Pearson correlation notebook\n  - Fixed missing sphinx docstring for Python class\n\n\n# 2021-07-20    [ 1.9.1 ]:\n--------------------------\n* tasks\n  Bumped version\n\n# 2021-07-20    [ 1.9.0 ]:\n--------------------------\n* bugfixes\n  - Fixed scenarios where n_chunks == 0 in _get_array_ranges and fixed empty input array edge case\n* features\n  - Added `normalize` to `core.mass`\n  - Added motif discovery (`stumpy.motifs` and `stumpy.match`)\n  - Added snippets for identifying regimes (`stumpy.snippets`)\n  - Added pan matrix profile (`stumpy.stimp`, `stumpy.stimped`, `stumpy.gpu_stimp`)\n  - Added `excl_zone` parameter to config.py (`config.STUMPY_EXCL_ZONE_DENOM`)\n* tasks\n  - Aggregate or Refactor Dask Unit Tests\n  - Added script for testing latest Numba release candidate\n  - Converted bash scripts to [[ ... ]] construct\n  - Updated Python class declaration\n  - Updated to RTD to PyData Sphinx Theme\n  - Updated conda installation environment\n  - Refactored test files and added check to ensure that naive implementations always come ahead of tests\n* documentation\n  - Corrected binder badges to point to \"main\"\n  - Updated tutorial to discover motif/discord indices\n  - Added missing docstrings and fixed minor typos\n  - Added missing logo file and favicon to _static directory\n  - Updated source installation instructions\n  - Added instructions for Apple M1 chip\n  - Updated Contributor guide\n  - Added include/discords tutorial example to subspace\n  - Added bonus content on interpreting the columns of a matrix profile\n  - Added syntax highlighting to tutorials\n  - Replaced matplotlib params with style file\n  - Added Annotation Vectors Tutorial\n  - Added Binder links to top of tutorials\n  - Added introduction to Snippets Tutorial\n\n\n# 2021-02-04    [ 1.8.0 ]:\n--------------------------\n* bugfixes\n  - Fixed chunk size for `scrump` and `scraamp` when time series are short\n* features\n  - Added `maamp` and `maamped` functions\n  - Added `scraamp` function\n  - Added a new `core.non_normalized` decorator that re-routes normalized functions to non-normalized functions\n  - All z-normalized functions now accept a `normalize` parameter\n* tasks\n  - Renamed `main` branch\n  - Removed Azure pipelines badge\n  - Refactored `subspace`\n  - Refactored non-normalized functions\n  - Added non-normalized support to `floss`\n* documentation\n  - Updated README with `if __name__ == \"__main__\"` for Dask and Jupyter notebooks\n  - Removed all `aamp` references as `normalize=False` should be used instead\n  - Fixed function docstrings and typos in API docs\n\n\n# 2021-01-20    [ 1.7.2 ]:\n--------------------------\n* bugfixes\n  - None\n* features\n  - Added the NEP 29 policy\n* tasks\n  - Added CI for minimum version dependencies\n* documentation\n  - Updated README to conver NEP 29\n\n\n# 2021-01-19    [ 1.7.1 ]:\n--------------------------\n* bugfixes\n  - None\n* features\n  - None\n* tasks\n  - Bumped minimum NumPy version to use `np.array_equal`\n* documentation\n  - None\n\n\n# 2021-01-17    [ 1.7.0 ]:\n--------------------------\n* bugfixes\n  - None\n* features\n  - Added maximum window size check\n  - Added window size checking to preprocessing\n* tasks\n  - Replaced array comparison in `core.are_arrays_equal` with `np.array_equal`\n  - Added Github Actions and badge\n  - Improved stability for integration with other packages\n* documentation\n  - Fixed typo\n  - Replaced NABDConf motivation video with PyData Global video\n\n\n# 2020-12-30    [ 1.6.1 ]:\n--------------------------\n* bugfixes\n  - Fixed inconsistent `mstump`/`mstumped` output to match description in published work\n* features\n  - Added `subspace` function for compute multi-dimensional matrix profile subspace\n  - Added `include` and `discords` handling to `subspace`\n  - Added `mdl` building blocks\n  - Added `ostinato` Dask distributed, GPU, and `aamp` variants\n  - Added fast rolling min/max functions\n* tasks\n  - Updated Azure Pipelines CI coverage tests to use latest images\n  - Added `mstump` tutorial to Binder\n  - Fixed bad reference to `aamp_gpu_ostinato`\n  - Fixed `aamp_ostinato` unit test coverage\n  - Converted `conda.sh` to `bash`\n  - Moved all private functions out of `__init__.py`\n  - Added commands to remove `build/`, `dist/`, and `stumpy.egg/` after PyPI upload\n  - Added twine to environment setup script\n  - Fixed incorrect date in `CHANGELOG.md`\n* documentation\n  - Added `mstump` tutorial to RTD\n  - Updated various function docstrings\n  - Added Github Discussions to README and RTD\n  - Updated API list on RTD\n\n\n# 2020-12-10    [ 1.6.0 ]:\n--------------------------\n* bugfixes\n  - Fixed incorrect cancelling of Dask data that was being scattered with `hash=False`\n  - Fixed floating point imprecision in computing distances with `mass_absolute`\n* features\n  - Added `ostinato` function for computing consensus motifs\n  - Added new approach for retrieving the most central consensus motif from `ostinato`\n  - Added `mpdist` function for computing the MPdist distance measure\n  - Added `mpdisted` function for computing the MPdist distance measure\n  - Added `gpu_mpdist` function for computing the MPdist distance measure\n  - Added `aampdist` function for computing the MPdist distance measure\n  - Added `aampdisted` function for computing the MPdist distance measure\n  - Added `gpu_aampdist` function for computing the MPdist distance measure\n  - Changed `np.convolve` to the faster `scipy.signal.convolve` for FFT\n  - Added matrix profile subspace for multi-dimensional motif discovery\n  - Replaced existing rolling functions with fast Welford nanstd and nanvar functions\n* tasks\n  - Updated Azure Pipelines CI to use latest images\n  - Fixed tutorial typos\n  - Added ostinato paper to README References\n  - Added `mamba` to speed up Python environment installation\n  - Added `ostinato` tutorial\n  - Added a series of new unit tests (maintained at 100% test coverage)\n  - Updated all tutorials to use Zenodo links for data retrieval\n  - Removed tutorial plotting function that set default plotting conditions\n  - Added AB-join tutorial\n  - Replaced rolling window isfinite with a much faster function\n  - Updated Github PR template to use conda-forge channel\n  - Updated Welford corrected-sum-of-squares derivation\n  - Updated Binder environment to use STUMPY release in Binder postBuild\n* documentation\n  - Fixed GPU function signatures that were being displayed on RTD\n  - Fixed incorrect docstring indentation\n  - Added STUMPY docs and Github code repo to Resources section of tutorials\n  - Added default values to docstrings\n\n\n# 2020-10-19    [ 1.5.1 ]:\n--------------------------\n* bugfixes\n  - Fixed AB-join so that it now matches the published definition (previously, BA-join)\n* features\n  - Added nan/inf support to FLOSS/FLUSS\n* tasks\n  - Removed Pandas series in GPU tests to improve unit test speed in CI\n  - Identify operating system prior to installing cuda toolkit\n  - Changed `left`/`right` keywords in all unit tests to `ref`/`comp`\n  - Split up unit tests and coverage testing \n  - Updated `displayNames` in Azure Pipelines\n  - `test.sh` now accepts `unit`, `custom`, and `coverage` keywords \n  - Fixed typos\n  - Added pattern searching (MASS) tutorial\n  - Added `Contribute` notebook to RTD table of contents for first time contributors\n  - Refactored `_compute_diagonal` for speed improvements\n  - Replaced `np.roll` with slice indexing in `stumpy.floss`\n  - Refactored and improved `aampi` and `stumpi` update performance\n  - Added `lxml` to environment.yml  \n* documentation\n  - Added `plt.show()` to code figures in tutorials\n  - Updated `stumpi` tutorial with `egress=False`\n\n\n# 2020-08-31    [ 1.5.0 ]:\n--------------------------\n* bugfixes\n  - Fixed warning and check when time series has inappropriate dtype\n  - Fixed scenarios where identical subsequences produce non-zero distances\n* features\n  - For interactive data science work, matrix profile calcs are 10-15x faster\n  - Added `aamp` with non-normalized Euclidean distance (i.e., no z-normalization)\n  - Added `aamped`\n  - Added `aampi`\n  - Added `gpu_aamp`\n  - Added egress for `stumpi` and egress is now the default behavior\n  - Added a `mass_absolute` function for non-normalized distance calculation with FFT convolution\n  - Added diagonal pre-processing function to `core.py`\n  - Added centered-sum-of-products and Pearson correlation in place of sliding dot products\n  - Added left and right matrix profile indices to `scrump` and converted to Pearson correlation\n* tasks\n  - Removed Pandas series in GPU tests to improve unit test speed in CI\n  - Updated to latest version of black for better formatting\n  - Refactored redundant test section\n  - Added unit test for inappropriate dtype inputs\n  - Corrected absolute stumpy import to be relative import\n  - Replaced `._illegal` attribute with a more obvious `._T_isfinite` attribute\n  - Moved common diagonal functions to `core.py`\n  - Replaced `order` variable with the more obvious `diag` name\n  - Added environment.yml for easier installation of dependencies\n  - Removed random print statement in code\n  - Moved STUMPY thresholds to global parameters in `config.py`\n  - Refactored left/right matrix profile indices\n  - Refactored NaN checking\n  - Check for Linux OS and add TBB dynamically especially for CI\n* documentation\n  - Added `aamp` reference to README\n  - Update docstrings to be less verbose for API documentation\n  - Fixed some typos\n  - Replaced `sep=\"\\s+\"` with `sep=\"\\\\s+\"` in tutorials\n  - Added notes and derivations for Pearson correlation and centered-sum-of-products\n  - Renamed tutorials with underscores for consistency\n  - Added all `aamp`-like functions to API reference\n  - Replaced MS Word docs with LaTeX notebooks\n\n\n# 2020-06-15    [ 1.4.0 ]:\n--------------------------\n* bugfixes\n  - Fixed bad chunking in `compute_mean_std`\n* features\n  - Added parallelized `scrump`\n  - Added NaN/inf support to `scrump`\n  - Added `prescrump` (and, consequently, scrump++)\n  - Added AB-join to `scrump`\n  - Added unit test for `scrump`\n  - Added constrained inclusion motif search for `mstump`/`mstumped`\n  - Added discord support for `mstump`/`mstumped`\n  - Changed sorting to pure numpy in `mstump`/`mstumped` for better performance\n  - Added NaN/inf support to `gpu_stump`\n  - Added new `core.preprocess` function\n  - Added NaN/inf support to `core.mass`\n  - Added `core.apply_exclusion_zone` for consistent exclusion zone across functions\n  - Added `stumpi` for incrementally updating matrix profiles with streaming data\n  - Added `stumpi` unit tests\n  - Added NaN/inf support for `stumpi`\n  - Converted `floss.floss` generator to `class`\n* tasks\n  - Added Python 3.8 to Azure Pipelines for unit testing\n  - Moved `stomp` to `_stomp` to prevent public usage\n  - Fixed numerous typos\n  - Added several `np.asarray` calls to input arrays\n  - Split some unit tests out into separate files\n  - Updated distributed teststo use context manager\n  - Refactored `_calculate_squared_distance`\n  - Added `core.py` to JIT-compiled unit test section\n  - Remove mypy config file\n  - Corrected cuda.jit signature for `gpu_stump`\n  - Shortened time series length for faster GPU tests\n  - Removed link to discourse from documentation\n  - Added global variables for controlling chunking in `compute_mean_std`\n  - Replaced name of `naive_mass` with `naive_stamp`\n  - Removed redundant \"caption\" in RTD ToC\n  - Renamed `stamp.mass` to private `stamp._mass_PI`\n  - Added flake8-docstrings checking\n  - Renamed `utils.py` to `naive.py` and updated corresponding function calls\n  - Added `stumpi` and `scrump` to STUMPY API (RTD) \n* documentation\n  - Initialized shapelet discovery tutorial (WIP)\n  - Updated `check_window_size` docstring\n  - Added `gpu_stump` to tutorial\n  - Added `scrump` tutorial for Fast Approximate Matrix Profiles\n  - Updated string formatting to conform to flake8\n  - Added `stumpi` tutorial\n  - Improved `mstump` tutorial (WIP)\n  - Added additional references to original matrix profile papers\n\n# 2020-03-27    [ 1.3.1 ]:\n--------------------------\n* bugfixes\n  - Fixed MSTUMP/MSTUMPED input dimensions check\n  - Fixed inconsistent MSTUMP/MSTUMPED output\n* features\n  - Added support for constant subsequences and added unit tests\n  - Improved GPU memory consumption for self-join\n  - Added ability to handle NaN and inf values in all matrix profile algorithms (except gpu_stump)\n* tasks\n  - Updated performance table with new performance results, better color scheme, intuitive hardware grouping\n  - Re-organized ndarray input verification steps\n  - Added more unit tests and rearranged test order\n  - Removed Python type hints or type annotations\n  - Split failing dask unit tests into multiple test files\n  - Added PR template\n  - Updated Mac OS X image for Azure Pipelines\n  - Replaced stddev computation with a memory efficient rolling chunked stddev \n  - Modified exclusion zone to be symmetrical\n  - Refactored multi-dimensional mass\n  - Fixed scenarios where subsequence contains zero mean\n  - Added explicit PR trigger to Azure Pipelines\n  - Updated installation instructions to use conda-forge channel\n  - Fixed time series chains all_c test to handle differences in Python set order\n* documentation\n  - Fixed various typos\n  - Refactored tutorials for clarity\n\n# 2019-12-30    [ 1.3.0 ]:\n--------------------------\n* bugfixes\n  - Fixed MSTUMP/MSTUMPED input dimensions check\n  - Fixed inconsistent MSTUMP/MSTUMPED output\n* features\n  - Added parallel GPU-STUMP (i.e., multi-GPU support) using Python multiprocessing and file I/O\n  - Added Python type hints/type annotations\n* tasks\n  - Updated performance table and plots with STUMPY.2, GPU-STUMP.1, GPU-STUMP.2, GPU-STUMP.DGX1, and GPU-STUMP.DGX2 results\n  - Fixed test function names\n  - Added Python script for easier performance timing conversion\n* documentation\n  - Added window size (m = 50) for performance calculations\n  - Fixed various typos\n  - Added missing and improved docstrings\n  - Replaced Bokeh with Matplotlib\n  - Updated GPU-STUMP example with multi-GPU support\n\n# 2019-12-03    [ 1.2.4 ]:\n--------------------------\n* features\n  - Added ability to select GPU device in gpu_stump\n* tasks\n  - Changed all README hyperlinks to double underscores\n  - Added API links to README\n  - Added STUMPY circle image (logo)\n  - Suppressed pytest junit_family deprecation warning\n  - Replaced `python install` with `python -m pip install .`\n\n# 2019-11-26    [ 1.2.3 ]:\n--------------------------\n* bugfixes\n  - Fixed incorrect GPU output for self joins\n* features\n  - Added array check for NaNs\n  - Improved test script for CI\n* tasks\n  - Added discourse group at stumpy.discourse.group\n  - Updated formatting and added newline before logo\n  - Made logo a hyperlink to Github repo\n  - Added custom CSS to sphinx theme\n\n# 2019-11-03    [ 1.2.2 ]:\n--------------------------\n* bugfixes\n  - Fixed Python AST utf8 file reading bug on Windows\n\n# 2019-11-03    [ 1.2.1 ]:\n--------------------------\n* bugfixes\n  - Fixed driver not found function when no GPU present\n  - Fixed gpu_stump docstring in RTD\n\n# 2019-11-02    [ 1.2.0 ]:\n--------------------------\n* bugfixes\n  - Fixed transposed Pandas DataFrame issue #66\n* features\n  - Added GPU support (NVIDIA Only)\n* tasks\n  - Added CHANGELOG\n  - Added GPU tests for GPU-STUMP\n  - Added CPU tests (via CUDA simulator) for GPU-STUMP\n  - Added additional STUMPY logos\n  - Disabled bidirectional and left indices for FLOSS\n  - Added STUMPY to Python 3 Statement\n* documentation\n  - Added reference to semantic segmentation\n  - Added gpu_stump API to RTF\n  - Added gpu_stump example to README\n  - Improved docstring for mstump(ed)\n  - Improved time series chains tutorial\n  - Added FLUSS to README\n  - Added missing docstring for FLOSS\n  - Added FLOSS and FLUSS to RTD API\n  - Updated docstring DOIs with URLs to primary references\n  - Added motif discovery to exisiting tutorial\n\n# 2019-08-03    [ 1.1.0 ]:\n--------------------------\n* bugfixes\n  - Removed incorrect compatibility with Py35\n  - Split Numba JIT tests tests to ensure proper Dask PyTest teardown \n* features\n  - Added FLUSS and FLOSS\n  - Added Pandas Series/DataFrame support\n* tasks\n  - Added conda-forge support\n  - Set up CI with Azure Pipelines\n  - Added black and flake8 checks\n  - 100% test coverage\n  - Added badges\n  - Added __version__ attr\n  - Added performance graph\n  - Published in JOSS\n  - Added STUMPY logo suite\n  - Added CHANGELOG\n* documentation\n  - Added RTD documentation\n  - Added Tutorials\n  - Added Binder support\n  - Added NABDConf presentation\n\n# 2019-05-12    [ 1.0.0 ]:\n--------------------------\n* Initial release\n"
        },
        {
          "name": "CITATION.bib",
          "type": "blob",
          "size": 0.2666015625,
          "content": "@article{law2019stumpy,\n  author  = {Law, Sean M.},\n  title   = {{STUMPY: A Powerful and Scalable Python Library for Time Series Data Mining}},\n  journal = {{The Journal of Open Source Software}},\n  volume  = {4},\n  number  = {39},\n  pages   = {1504},\n  year    = {2019}\n}\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.4208984375,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at seanmylaw@gmail.com. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\nIn addition to this Contributor Code of Conduct, TD Ameritrade Associates remain\nsubject to all company policy including our internal Code of Conduct.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.0126953125,
          "content": "# Contributing\n\nContributions of all kinds are welcome. In particular pull requests are appreciated. The authors will endeavour to help walk you through any issues in the pull request discussion, so please feel free to open a pull request even if you are new to such things.\n\n## Issues\n\nThe easiest contribution to make is to [file an issue](https://github.com/TDAmeritrade/stumpy/issues/new). It is beneficial if you perform a cursory search of [existing issues](https://github.com/TDAmeritrade/stumpy/issues?q=is%3Aissue) and it is also helpful, but not necessary, if you can provide clear instruction for how to reproduce a problem. If you have resolved an issue yourself please consider contributing to this repository so others can benefit from your work.\n\n## Documentation\n\nContributing to documentation is the easiest way to get started. Providing simple clear or helpful documentation for new users is critical. Anything that *you* as a new user found hard to understand, or difficult to work out, are excellent places to begin. Contributions to more detailed and descriptive error messages is especially appreciated. To contribute to the documentation please \n[fork the project](https://github.com/TDAmeritrade/stumpy/fork) into your own repository, make changes there, and then submit a pull request.\n\n## Code\n\nCode contributions are always welcome, from simple bug fixes, to new features. To contribute code please [fork the project](https://github.com/TDAmeritrade/stumpy/fork) into your own repository, make changes there, run [black](https://github.com/python/black) and [flake8](http://flake8.pycqa.org/en/latest/) on your code, add tests for bugs/new features, and then submit a pull request. If you are fixing a known issue please add the issue number to the PR message. If you are fixing a new issue feel free to file an issue and then reference it in the PR. You can [browse open issues](https://github.com/TDAmeritrade/stumpy/issues) for potential code contributions. Fixes for issues tagged with 'help wanted' are especially appreciated.\n\n\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.56640625,
          "content": "STUMPY\nCopyright 2019 TD Ameritrade. Released under the terms of the 3-Clause BSD license.\nSTUMPY is a trademark of TD Ameritrade IP Company, Inc. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0458984375,
          "content": "include LICENSE.txt\nprune docs\nprune notebooks\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 26.3193359375,
          "content": "|PyPI Version| |Conda Forge Version| |PyPI Downloads| |License| |Test Status| |Code Coverage|\n\n|RTD Status| |Binder| |JOSS| |NumFOCUS| |FOSSA|\n\n.. |PyPI Version| image:: https://img.shields.io/pypi/v/stumpy.svg\n    :target: https://pypi.org/project/stumpy/\n    :alt: PyPI Version\n.. |Conda Forge Version| image:: https://anaconda.org/conda-forge/stumpy/badges/version.svg\n    :target: https://anaconda.org/conda-forge/stumpy\n    :alt: Conda-Forge Version\n.. |PyPI Downloads| image:: https://static.pepy.tech/badge/stumpy/month\n    :target: https://pepy.tech/project/stumpy\n    :alt: PyPI Downloads\n.. |License| image:: https://img.shields.io/pypi/l/stumpy.svg\n    :target: https://github.com/TDAmeritrade/stumpy/blob/master/LICENSE.txt\n    :alt: License\n.. |Test Status| image:: https://github.com/TDAmeritrade/stumpy/workflows/Tests/badge.svg\n    :target: https://github.com/TDAmeritrade/stumpy/actions?query=workflow%3ATests+branch%3Amain\n    :alt: Test Status\n.. |Code Coverage| image:: https://codecov.io/gh/TDAmeritrade/stumpy/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/TDAmeritrade/stumpy\n    :alt: Code Coverage\n.. |RTD Status| image:: https://readthedocs.org/projects/stumpy/badge/?version=latest\n    :target: https://stumpy.readthedocs.io/\n    :alt: ReadTheDocs Status\n.. |Binder| image:: https://mybinder.org/badge_logo.svg\n    :target: https://mybinder.org/v2/gh/TDAmeritrade/stumpy/main?filepath=notebooks\n    :alt: Binder\n.. |JOSS| image:: http://joss.theoj.org/papers/10.21105/joss.01504/status.svg\n    :target: https://doi.org/10.21105/joss.01504\n    :alt: JOSS\n.. |DOI| image:: https://zenodo.org/badge/184809315.svg\n    :target: https://zenodo.org/badge/latestdoi/184809315\n    :alt: DOI\n.. |NumFOCUS| image:: https://img.shields.io/badge/NumFOCUS-Affiliated%20Project-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n    :target: https://numfocus.org/sponsored-projects/affiliated-projects\n    :alt: NumFOCUS Affiliated Project\n.. |FOSSA| image:: https://app.fossa.com/api/projects/custom%2B9056%2Fgithub.com%2FTDAmeritrade%2Fstumpy.svg?type=shield\n    :target: https://app.fossa.io/projects/custom%2B9056%2Fgithub.com%2FTDAmeritrade%2Fstumpy?ref=badge_shield\n    :alt: FOSSA\n.. |Twitter| image:: https://img.shields.io/twitter/follow/stumpy_dev.svg?style=social\n    :target: https://twitter.com/stumpy_dev\n    :alt: Twitter\n\n|\n\n.. image:: https://raw.githubusercontent.com/TDAmeritrade/stumpy/master/docs/images/stumpy_logo_small.png\n    :target: https://github.com/TDAmeritrade/stumpy\n    :alt: STUMPY Logo\n\n======\nSTUMPY\n======\n\nSTUMPY is a powerful and scalable Python library that efficiently computes something called the `matrix profile <https://stumpy.readthedocs.io/en/latest/Tutorial_The_Matrix_Profile.html>`__, which is just an academic way of saying \"for every (green) subsequence within your time series, automatically identify its corresponding nearest-neighbor (grey)\":\n\n.. image:: https://github.com/TDAmeritrade/stumpy/blob/main/docs/images/stumpy_demo.gif?raw=true\n    :alt: STUMPY Animated GIF\n\nWhat's important is that once you've computed your matrix profile (middle panel above) it can then be used for a variety of time series data mining tasks such as:\n\n* pattern/motif (approximately repeated subsequences within a longer time series) discovery\n* anomaly/novelty (discord) discovery\n* shapelet discovery\n* semantic segmentation \n* streaming (on-line) data\n* fast approximate matrix profiles\n* time series chains (temporally ordered set of subsequence patterns)\n* snippets for summarizing long time series\n* pan matrix profiles for selecting the best subsequence window size(s)\n* `and more ... <https://www.cs.ucr.edu/~eamonn/100_Time_Series_Data_Mining_Questions__with_Answers.pdf>`__\n\nWhether you are an academic, data scientist, software developer, or time series enthusiast, STUMPY is straightforward to install and our goal is to allow you to get to your time series insights faster. See `documentation <https://stumpy.readthedocs.io/en/latest/>`__ for more information.\n\n-------------------------\nHow to use STUMPY\n-------------------------\n\nPlease see our `API documentation <https://stumpy.readthedocs.io/en/latest/api.html>`__ for a complete list of available functions and see our informative `tutorials <https://stumpy.readthedocs.io/en/latest/tutorials.html>`__ for more comprehensive example use cases. Below, you will find code snippets that quickly demonstrate how to use STUMPY.\n\nTypical usage (1-dimensional time series data) with `STUMP <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.stump>`__:\n\n.. code:: python\n\n    import stumpy\n    import numpy as np\n    \n    if __name__ == \"__main__\":\n        your_time_series = np.random.rand(10000)\n        window_size = 50  # Approximately, how many data points might be found in a pattern \n    \n        matrix_profile = stumpy.stump(your_time_series, m=window_size)\n\nDistributed usage for 1-dimensional time series data with Dask Distributed via `STUMPED <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.stumped>`__:\n\n.. code:: python\n\n    import stumpy\n    import numpy as np\n    from dask.distributed import Client\n\n    if __name__ == \"__main__\":\n        with Client() as dask_client:\n            your_time_series = np.random.rand(10000)\n            window_size = 50  # Approximately, how many data points might be found in a pattern \n    \n            matrix_profile = stumpy.stumped(dask_client, your_time_series, m=window_size)\n\nGPU usage for 1-dimensional time series data with `GPU-STUMP <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.gpu_stump>`__:\n\n.. code:: python\n\n    import stumpy\n    import numpy as np\n    from numba import cuda\n\n    if __name__ == \"__main__\":\n        your_time_series = np.random.rand(10000)\n        window_size = 50  # Approximately, how many data points might be found in a pattern\n        all_gpu_devices = [device.id for device in cuda.list_devices()]  # Get a list of all available GPU devices\n\n        matrix_profile = stumpy.gpu_stump(your_time_series, m=window_size, device_id=all_gpu_devices)\n\nMulti-dimensional time series data with `MSTUMP <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.mstump>`__:\n\n.. code:: python\n\n    import stumpy\n    import numpy as np\n\n    if __name__ == \"__main__\":\n        your_time_series = np.random.rand(3, 1000)  # Each row represents data from a different dimension while each column represents data from the same dimension\n        window_size = 50  # Approximately, how many data points might be found in a pattern\n\n        matrix_profile, matrix_profile_indices = stumpy.mstump(your_time_series, m=window_size)\n\nDistributed multi-dimensional time series data analysis with Dask Distributed `MSTUMPED <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.mstumped>`__:\n\n.. code:: python\n\n    import stumpy\n    import numpy as np\n    from dask.distributed import Client\n\n    if __name__ == \"__main__\":\n        with Client() as dask_client:\n            your_time_series = np.random.rand(3, 1000)   # Each row represents data from a different dimension while each column represents data from the same dimension\n            window_size = 50  # Approximately, how many data points might be found in a pattern\n\n            matrix_profile, matrix_profile_indices = stumpy.mstumped(dask_client, your_time_series, m=window_size)\n\nTime Series Chains with `Anchored Time Series Chains (ATSC) <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.atsc>`__:\n\n.. code:: python\n\n    import stumpy\n    import numpy as np\n    \n    if __name__ == \"__main__\":\n        your_time_series = np.random.rand(10000)\n        window_size = 50  # Approximately, how many data points might be found in a pattern \n        \n        matrix_profile = stumpy.stump(your_time_series, m=window_size)\n\n        left_matrix_profile_index = matrix_profile[:, 2]\n        right_matrix_profile_index = matrix_profile[:, 3]\n        idx = 10  # Subsequence index for which to retrieve the anchored time series chain for\n\n        anchored_chain = stumpy.atsc(left_matrix_profile_index, right_matrix_profile_index, idx)\n\n        all_chain_set, longest_unanchored_chain = stumpy.allc(left_matrix_profile_index, right_matrix_profile_index)\n\nSemantic Segmentation with `Fast Low-cost Unipotent Semantic Segmentation (FLUSS) <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.fluss>`__:\n\n.. code:: python\n\n    import stumpy\n    import numpy as np\n\n    if __name__ == \"__main__\":\n        your_time_series = np.random.rand(10000)\n        window_size = 50  # Approximately, how many data points might be found in a pattern\n\n        matrix_profile = stumpy.stump(your_time_series, m=window_size)\n\n        subseq_len = 50\n        correct_arc_curve, regime_locations = stumpy.fluss(matrix_profile[:, 1], \n                                                        L=subseq_len, \n                                                        n_regimes=2, \n                                                        excl_factor=1\n                                                        )\n\n------------\nDependencies\n------------\n\nSupported Python and NumPy versions are determined according to the `NEP 29 deprecation policy <https://numpy.org/neps/nep-0029-deprecation_policy.html>`__.\n\n* `NumPy <http://www.numpy.org/>`__\n* `Numba <http://numba.pydata.org/>`__\n* `SciPy <https://www.scipy.org/>`__\n\n---------------\nWhere to get it\n---------------\n\nConda install (preferred):\n\n.. code:: bash\n    \n    conda install -c conda-forge stumpy\n\nPyPI install, presuming you have numpy, scipy, and numba installed: \n\n.. code:: bash\n\n    python -m pip install stumpy\n\nTo install stumpy from source, see the instructions in the `documentation <https://stumpy.readthedocs.io/en/latest/install.html>`__.\n\n-------------\nDocumentation\n-------------\n\nIn order to fully understand and appreciate the underlying algorithms and applications, it is imperative that you read the original publications_. For a more detailed example of how to use STUMPY please consult the latest `documentation <https://stumpy.readthedocs.io/en/latest/>`__ or explore our `hands-on tutorials <https://stumpy.readthedocs.io/en/latest/tutorials.html>`__.\n\n-----------\nPerformance\n-----------\n\nWe tested the performance of computing the exact matrix profile using the Numba JIT compiled version of the code on randomly generated time series data with various lengths (i.e., ``np.random.rand(n)``) along with different `CPU and GPU hardware resources <hardware_>`_. \n\n.. image:: https://raw.githubusercontent.com/TDAmeritrade/stumpy/master/docs/images/performance.png\n    :alt: STUMPY Performance Plot\n\nThe raw results are displayed in the table below as Hours:Minutes:Seconds.Milliseconds and with a constant window size of `m = 50`. Note that these reported runtimes include the time that it takes to move the data from the host to all of the GPU device(s). You may need to scroll to the right side of the table in order to see all of the runtimes.\n\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n|    i     |  n = 2\\ :sup:`i`  | GPU-STOMP    | STUMP.2     | STUMP.16    | STUMPED.128 | STUMPED.256 | GPU-STUMP.1 | GPU-STUMP.2 | GPU-STUMP.DGX1 | GPU-STUMP.DGX2 |\n+==========+===================+==============+=============+=============+=============+=============+=============+=============+================+================+\n| 6        | 64                | 00:00:10.00  | 00:00:00.00 | 00:00:00.00 | 00:00:05.77 | 00:00:06.08 | 00:00:00.03 | 00:00:01.63 | NaN            | NaN            |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 7        | 128               | 00:00:10.00  | 00:00:00.00 | 00:00:00.00 | 00:00:05.93 | 00:00:07.29 | 00:00:00.04 | 00:00:01.66 | NaN            | NaN            |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 8        | 256               | 00:00:10.00  | 00:00:00.00 | 00:00:00.01 | 00:00:05.95 | 00:00:07.59 | 00:00:00.08 | 00:00:01.69 | 00:00:06.68    | 00:00:25.68    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 9        | 512               | 00:00:10.00  | 00:00:00.00 | 00:00:00.02 | 00:00:05.97 | 00:00:07.47 | 00:00:00.13 | 00:00:01.66 | 00:00:06.59    | 00:00:27.66    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 10       | 1024              | 00:00:10.00  | 00:00:00.02 | 00:00:00.04 | 00:00:05.69 | 00:00:07.64 | 00:00:00.24 | 00:00:01.72 | 00:00:06.70    | 00:00:30.49    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 11       | 2048              | NaN          | 00:00:00.05 | 00:00:00.09 | 00:00:05.60 | 00:00:07.83 | 00:00:00.53 | 00:00:01.88 | 00:00:06.87    | 00:00:31.09    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 12       | 4096              | NaN          | 00:00:00.22 | 00:00:00.19 | 00:00:06.26 | 00:00:07.90 | 00:00:01.04 | 00:00:02.19 | 00:00:06.91    | 00:00:33.93    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 13       | 8192              | NaN          | 00:00:00.50 | 00:00:00.41 | 00:00:06.29 | 00:00:07.73 | 00:00:01.97 | 00:00:02.49 | 00:00:06.61    | 00:00:33.81    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 14       | 16384             | NaN          | 00:00:01.79 | 00:00:00.99 | 00:00:06.24 | 00:00:08.18 | 00:00:03.69 | 00:00:03.29 | 00:00:07.36    | 00:00:35.23    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 15       | 32768             | NaN          | 00:00:06.17 | 00:00:02.39 | 00:00:06.48 | 00:00:08.29 | 00:00:07.45 | 00:00:04.93 | 00:00:07.02    | 00:00:36.09    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 16       | 65536             | NaN          | 00:00:22.94 | 00:00:06.42 | 00:00:07.33 | 00:00:09.01 | 00:00:14.89 | 00:00:08.12 | 00:00:08.10    | 00:00:36.54    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 17       | 131072            | 00:00:10.00  | 00:01:29.27 | 00:00:19.52 | 00:00:09.75 | 00:00:10.53 | 00:00:29.97 | 00:00:15.42 | 00:00:09.45    | 00:00:37.33    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 18       | 262144            | 00:00:18.00  | 00:05:56.50 | 00:01:08.44 | 00:00:33.38 | 00:00:24.07 | 00:00:59.62 | 00:00:27.41 | 00:00:13.18    | 00:00:39.30    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 19       | 524288            | 00:00:46.00  | 00:25:34.58 | 00:03:56.82 | 00:01:35.27 | 00:03:43.66 | 00:01:56.67 | 00:00:54.05 | 00:00:19.65    | 00:00:41.45    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 20       | 1048576           | 00:02:30.00  | 01:51:13.43 | 00:19:54.75 | 00:04:37.15 | 00:03:01.16 | 00:05:06.48 | 00:02:24.73 | 00:00:32.95    | 00:00:46.14    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 21       | 2097152           | 00:09:15.00  | 09:25:47.64 | 03:05:07.64 | 00:13:36.51 | 00:08:47.47 | 00:20:27.94 | 00:09:41.43 | 00:01:06.51    | 00:01:02.67    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 22       | 4194304           | NaN          | 36:12:23.74 | 10:37:51.21 | 00:55:44.43 | 00:32:06.70 | 01:21:12.33 | 00:38:30.86 | 00:04:03.26    | 00:02:23.47    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 23       | 8388608           | NaN          | 143:16:09.94| 38:42:51.42 | 03:33:30.53 | 02:00:49.37 | 05:11:44.45 | 02:33:14.60 | 00:15:46.26    | 00:08:03.76    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 24       | 16777216          | NaN          | NaN         | NaN         | 14:39:11.99 | 07:13:47.12 | 20:43:03.80 | 09:48:43.42 | 01:00:24.06    | 00:29:07.84    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| NaN      | 17729800          | 09:16:12.00  | NaN         | NaN         | 15:31:31.75 | 07:18:42.54 | 23:09:22.43 | 10:54:08.64 | 01:07:35.39    | 00:32:51.55    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 25       | 33554432          | NaN          | NaN         | NaN         | 56:03:46.81 | 26:27:41.29 | 83:29:21.06 | 39:17:43.82 | 03:59:32.79    | 01:54:56.52    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 26       | 67108864          | NaN          | NaN         | NaN         | 211:17:37.60| 106:40:17.17| 328:58:04.68| 157:18:30.50| 15:42:15.94    | 07:18:52.91    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| NaN      | 100000000         | 291:07:12.00 | NaN         | NaN         | NaN         | 234:51:35.39| NaN         | NaN         | 35:03:44.61    | 16:22:40.81    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n| 27       | 134217728         | NaN          | NaN         | NaN         | NaN         | NaN         | NaN         | NaN         | 64:41:55.09    | 29:13:48.12    |\n+----------+-------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+----------------+----------------+\n\n^^^^^^^^^^^^^^^^^^\nHardware Resources\n^^^^^^^^^^^^^^^^^^\n\n.. _hardware:\n\nGPU-STOMP: These results are reproduced from the original `Matrix Profile II <https://ieeexplore.ieee.org/abstract/document/7837898>`__ paper - NVIDIA Tesla K80 (contains 2 GPUs) and serves as the performance benchmark to compare against.\n    \nSTUMP.2: `stumpy.stump <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.stump>`__ executed with 2 CPUs in Total - 2x Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz processors parallelized with Numba on a single server without Dask.\n\nSTUMP.16: `stumpy.stump <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.stump>`__ executed with 16 CPUs in Total - 16x Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz processors parallelized with Numba on a single server without Dask.\n\nSTUMPED.128: `stumpy.stumped <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.stumped>`__ executed with 128 CPUs in Total - 8x Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz processors x 16 servers, parallelized with Numba, and distributed with Dask Distributed.\n\nSTUMPED.256: `stumpy.stumped <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.stumped>`__ executed with 256 CPUs in Total - 8x Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz processors x 32 servers, parallelized with Numba, and distributed with Dask Distributed.\n\nGPU-STUMP.1: `stumpy.gpu_stump <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.gpu_stump>`__ executed with 1x NVIDIA GeForce GTX 1080 Ti GPU, 512 threads per block, 200W power limit, compiled to CUDA with Numba, and parallelized with Python multiprocessing\n\nGPU-STUMP.2: `stumpy.gpu_stump <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.gpu_stump>`__ executed with 2x NVIDIA GeForce GTX 1080 Ti GPU, 512 threads per block, 200W power limit, compiled to CUDA with Numba, and parallelized with Python multiprocessing\n\nGPU-STUMP.DGX1: `stumpy.gpu_stump <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.gpu_stump>`__ executed with 8x NVIDIA Tesla V100, 512 threads per block, compiled to CUDA with Numba, and parallelized with Python multiprocessing\n\nGPU-STUMP.DGX2: `stumpy.gpu_stump <https://stumpy.readthedocs.io/en/latest/api.html#stumpy.gpu_stump>`__ executed with 16x NVIDIA Tesla V100, 512 threads per block, compiled to CUDA with Numba, and parallelized with Python multiprocessing\n\n-------------\nRunning Tests\n-------------\n\nTests are written in the ``tests`` directory and processed using `PyTest <https://docs.pytest.org/en/latest/>`__ and requires ``coverage.py`` for code coverage analysis. Tests can be executed with:\n\n.. code:: bash\n\n    ./test.sh\n\n--------------\nPython Version\n--------------\n\nSTUMPY supports `Python 3.9+ <https://python3statement.org/>`__ and, due to the use of unicode variable names/identifiers, is not compatible with Python 2.x. Given the small dependencies, STUMPY may work on older versions of Python but this is beyond the scope of our support and we strongly recommend that you upgrade to the most recent version of Python.\n\n------------\nGetting Help\n------------\n\nFirst, please check the `discussions <https://github.com/TDAmeritrade/stumpy/discussions>`__ and `issues <https://github.com/TDAmeritrade/stumpy/issues?utf8=%E2%9C%93&q=>`__ on Github to see if your question has already been answered there. If no solution is available there feel free to open a new discussion or issue and the authors will attempt to respond in a reasonably timely fashion.\n\n------------\nContributing\n------------\n\nWe welcome `contributions <https://github.com/TDAmeritrade/stumpy/blob/master/CONTRIBUTING.md>`__ in any form! Assistance with documentation, particularly expanding tutorials, is always welcome. To contribute please `fork the project <https://github.com/TDAmeritrade/stumpy/fork>`__, make your changes, and submit a pull request. We will do our best to work through any issues with you and get your code merged into the main branch.\n\n------\nCiting\n------\n\nIf you have used this codebase in a scientific publication and wish to cite it, please use the `Journal of Open Source Software article <http://joss.theoj.org/papers/10.21105/joss.01504>`__.\n\n    S.M. Law, (2019). *STUMPY: A Powerful and Scalable Python Library for Time Series Data Mining*. Journal of Open Source Software, 4(39), 1504.\n\n.. code:: bibtex\n\n    @article{law2019stumpy,\n      author  = {Law, Sean M.},\n      title   = {{STUMPY: A Powerful and Scalable Python Library for Time Series Data Mining}},\n      journal = {{The Journal of Open Source Software}},\n      volume  = {4},\n      number  = {39},\n      pages   = {1504},\n      year    = {2019}\n    }\n\n----------\nReferences\n----------\n\n.. _publications:\n\nYeh, Chin-Chia Michael, et al. (2016) Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View that Includes Motifs, Discords, and Shapelets. ICDM:1317-1322. `Link <https://ieeexplore.ieee.org/abstract/document/7837992>`__\n\nZhu, Yan, et al. (2016) Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins. ICDM:739-748. `Link <https://ieeexplore.ieee.org/abstract/document/7837898>`__\n\nYeh, Chin-Chia Michael, et al. (2017) Matrix Profile VI: Meaningful Multidimensional Motif Discovery. ICDM:565-574. `Link <https://ieeexplore.ieee.org/abstract/document/8215529>`__ \n\nZhu, Yan, et al. (2017) Matrix Profile VII: Time Series Chains: A New Primitive for Time Series Data Mining. ICDM:695-704. `Link <https://ieeexplore.ieee.org/abstract/document/8215542>`__\n\nGharghabi, Shaghayegh, et al. (2017) Matrix Profile VIII: Domain Agnostic Online Semantic Segmentation at Superhuman Performance Levels. ICDM:117-126. `Link <https://ieeexplore.ieee.org/abstract/document/8215484>`__\n\nZhu, Yan, et al. (2017) Exploiting a Novel Algorithm and GPUs to Break the Ten Quadrillion Pairwise Comparisons Barrier for Time Series Motifs and Joins. KAIS:203-236. `Link <https://link.springer.com/article/10.1007%2Fs10115-017-1138-x>`__\n\nZhu, Yan, et al. (2018) Matrix Profile XI: SCRIMP++: Time Series Motif Discovery at Interactive Speeds. ICDM:837-846. `Link <https://ieeexplore.ieee.org/abstract/document/8594908>`__\n\nYeh, Chin-Chia Michael, et al. (2018) Time Series Joins, Motifs, Discords and Shapelets: a Unifying View that Exploits the Matrix Profile. Data Min Knowl Disc:83-123. `Link <https://link.springer.com/article/10.1007/s10618-017-0519-9>`__\n\nGharghabi, Shaghayegh, et al. (2018) \"Matrix Profile XII: MPdist: A Novel Time Series Distance Measure to Allow Data Mining in More Challenging Scenarios.\" ICDM:965-970. `Link <https://ieeexplore.ieee.org/abstract/document/8594928>`__\n\nZimmerman, Zachary, et al. (2019) Matrix Profile XIV: Scaling Time Series Motif Discovery with GPUs to Break a Quintillion Pairwise Comparisons a Day and Beyond. SoCC '19:74-86. `Link <https://dl.acm.org/doi/10.1145/3357223.3362721>`__\n\nAkbarinia, Reza, and Betrand Cloez. (2019) Efficient Matrix Profile Computation Using Different Distance Functions. arXiv:1901.05708. `Link <https://arxiv.org/abs/1901.05708>`__\n\nKamgar, Kaveh, et al. (2019) Matrix Profile XV: Exploiting Time Series Consensus Motifs to Find Structure in Time Series Sets. ICDM:1156-1161. `Link <https://ieeexplore.ieee.org/abstract/document/8970797>`__\n\n-------------------\nLicense & Trademark\n-------------------\n\n| STUMPY\n| Copyright 2019 TD Ameritrade. Released under the terms of the 3-Clause BSD license.\n| STUMPY is a trademark of TD Ameritrade IP Company, Inc. All rights reserved.\n"
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.05078125,
          "content": "coverage:\n  status:\n    project: off\n    patch: off\n"
        },
        {
          "name": "conda.sh",
          "type": "blob",
          "size": 4.634765625,
          "content": "#!/bin/bash\n\nMAMBA_NO_LOW_SPEED_LIMIT=0\nconda_env=\"$(conda info --envs | grep '*' | awk '{print $1}')\"\narch_name=\"$(uname -m)\"\nif [[ $1 == \"numba\" ]] && [[ $arch_name == \"arm64\" ]]; then\n    echo \"Sorry, cannot install numba release candidate envrionment for ARM64 architecture\"\nfi\ninstall_mode=\"normal\"\n\n# Parse first command line argument\nif [[ $# -gt 0 ]]; then\n    if [ $1 == \"min\" ]; then\n        install_mode=\"min\"\n        echo \"Installing minimum dependencies with install_mode=\\\"min\\\"\"\n    elif [[ $1 == \"ray\" ]]; then\n        install_mode=\"ray\"\n        echo \"Installing ray dependencies with install_mode=\\\"ray\\\"\"\n    elif [[ $1 == \"numba\" ]] && [[ \"${arch_name}\" != \"arm64\" ]]; then\n        install_mode=\"numba\"\n        echo \"Installing numba release candidate dependencies with install_mode=\\\"numba\\\"\"\n        if [[ -z $2 ]]; then\n            numba_version=`conda search --override-channels -c numba numba | tail -n 1 | awk '{print $2}'`\n        else\n            numba_version=$2\n        fi\n        # Set Python version\n        if [[ -z $3 ]]; then\n            python_version=`conda search --override-channels -c conda-forge python | tail -n 1 | awk '{print $2}'`\n            # Strip away patch version\n            # python_version=\"${python_version%.*}\"\n        else\n            python_version=$3\n        fi\n    else\n        echo \"Using default install_mode=\\\"normal\\\"\"\n    fi\nfi\n\n###############\n#  Functions  #\n###############\n\ngenerate_min_environment_yaml()\n{\n    echo \"Generating \\\"environment.min.yml\\\" File\"\n    numpy=\"$(grep -E \"numpy\" environment.yml)\"\n    scipy=\"$(grep -E \"scipy\" environment.yml)\"\n    numba=\"$(grep -E \"numba\" environment.yml)\"\n    min_numpy=\"$(grep -E \"numpy\" environment.yml | sed 's/>//')\"\n    min_scipy=\"$(grep -E \"scipy\" environment.yml | sed 's/>//')\"\n    min_numba=\"$(grep -E \"numba\" environment.yml | sed 's/>//')\"\n\n    sed \"s/${numpy}/${min_numpy}/\" environment.yml | sed \"s/${scipy}/${min_scipy}/\" | sed \"s/${numba}/${min_numba}/\" > environment.min.yml\n}\n\ngenerate_numba_environment_yaml()\n{\n    echo \"Generating \\\"environment.numba.yml\\\" File\"\n    grep -Ev \"numba|python\" environment.yml > environment.numba.yml\n}\n\ngenerate_ray_environment_yaml()\n{\n    # Limit max Python version and append pip install ray\n    echo \"Generating \\\"environment.ray.yml\\\" File\"\n    ray_python=`./ray_python_version.py`\n    sed \"/  - python/ s/$/,<=$ray_python/\" environment.yml | cat - <(echo $'  - pip\\n  - pip:\\n    - ray>=2.23.0') > environment.ray.yml\n}\n\nfix_libopenblas()\n{\n    if [ ! -f $CONDA_PREFIX/lib/libopenblas.dylib ]; then\n        if [ -f $CONDA_PREFIX/lib/libopenblas.0.dylib ]; then\n            ln -s $CONDA_PREFIX/lib/libopenblas.0.dylib $CONDA_PREFIX/lib/libopenblas.dylib\n        fi\n    fi\n}\n\nclean_up()\n{\n    echo \"Cleaning Up\"\n    rm -rf \"environment.min.yml\"\n    rm -rf \"environment.numba.yml\"\n    rm -rf \"environment.ray.yml\"\n}\n\n###########\n#   Main  #\n###########\n\nconda update -c conda-forge -y conda\nconda update -c conda-forge -y --all\nconda install -y -c conda-forge mamba\n\nif [[ `uname` == \"Linux\" && `which nvcc | wc -l` -lt \"1\" ]]; then\n    rm -rf /tmp/cuda-installer.log\n    # conda install -y -c conda-forge cudatoolkit-dev'<11.4'\n    conda install -y -c conda-forge cudatoolkit-dev\n    # mamba install -y -c conda-forge cudatoolkit-dev\n    echo \"Please reboot the server to resolve any CUDA driver/library version mismatches\"\nfi\n\nif [[ $install_mode == \"min\" ]]; then\n    generate_min_environment_yaml\n    mamba env update --name $conda_env --file environment.min.yml || conda env update --name $conda_env --file environment.min.yml\nelif [[ $install_mode == \"ray\" ]]; then\n    generate_ray_environment_yaml\n    mamba env update --name $conda_env --file environment.ray.yml || conda env update --name $conda_env --file environment.ray.yml\nelif [[ $install_mode == \"numba\" ]]; then\n    echo \"\"\n    echo \"Installing python=$python_version\"\n    echo \"\"\n    mamba install -y -c conda-forge python=$python_version || conda install -y -c conda-forge python=$python_version\n\n    echo \"\"\n    echo \"Installing numba=$numba_version\"\n    echo \"\"\n    mamba install -y -c numba numba=$numba_version || conda install -y -c numba numba=$numba_version\n\n    generate_numba_environment_yaml\n    mamba env update --name $conda_env --file environment.numba.yml || conda env update --name $conda_env --file environment.numba.yml\nelse\n    mamba env update --name $conda_env --file environment.yml || conda env update --name $conda_env --file environment.yml\n    conda update -c conda-forge -y numpy scipy numba black twine\nfi\n\nfix_libopenblas\n\n#conda install -y -c conda-forge numpy scipy numba pandas flake8 flake8-docstrings black pytest-cov\n#conda install -y -c conda-forge dask distributed\n\nclean_up\n"
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 0.26171875,
          "content": "# This file acts as an entry point for pytest.\n\n# Its root directory is added to `sys.path` when pytest is executed\n# to fix eventual module import errors that can arise, for example when\n# running tests from inside VS code.\n# See https://stackoverflow.com/a/34520971\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "docstring.py",
          "type": "blob",
          "size": 3.685546875,
          "content": "#!/usr/bin/env python\n\nimport ast\nimport pathlib\nimport re\n\n\ndef get_docstring_args(fd, file_name, func_name, class_name=None):\n    \"\"\"\n    Extract docstring parameters from function definition\n    \"\"\"\n    docstring = ast.get_docstring(fd)\n    if len(re.findall(r\"Parameters\", docstring)) != 1:\n        msg = \"Missing required 'Parameters' section in docstring in \\n\"\n        msg += f\"file: {file_name}\\n\"\n        if class_name is not None:\n            msg += f\"class: {class_name}\\n\"\n        msg += f\"function/method: {func_name}\\n\"\n        raise RuntimeError(msg)\n    if class_name is None and len(re.findall(r\"Returns\", docstring)) != 1:\n        msg = \"Missing required 'Returns' section in docstring in \\n\"\n        msg += f\"file: {file_name}\\n\"\n        msg += f\"function/method: {func_name}\\n\"\n        raise RuntimeError(msg)\n\n    if class_name is None:\n        params_section = re.findall(\n            r\"(?<=Parameters)(.*)(?=Returns)\", docstring, re.DOTALL\n        )[0]\n    else:\n        params_section = re.findall(r\"(?<=Parameters)(.*)\", docstring, re.DOTALL)[0]\n\n    args = re.findall(r\"(\\w+)\\s+\\:\", params_section)\n    args = set([a for a in args if a != \"i\"])  # `i` should never be a parameter\n\n    return args\n\n\ndef get_signature_args(fd):\n    \"\"\"\n    Extract signature arguments from function definition\n    \"\"\"\n    return set([a.arg for a in fd.args.args if a.arg != \"self\"])\n\n\ndef check_args(doc_args, sig_args, file_name, func_name, class_name=None):\n    \"\"\"\n    Compare docstring arguments and signature argments\n    \"\"\"\n    diff_args = signature_args.difference(docstring_args)\n    if len(diff_args) > 0:\n        msg = \"Found one or more arguments/parameters with missing docstring in \\n\"\n        msg += f\"file: {file_name}\\n\"\n        if class_name is not None:\n            msg += f\"class: {class_name}\\n\"\n        msg += f\"function/method: {func_name}\\n\"\n        msg += f\"parameter(s): {diff_args}\\n\"\n        raise RuntimeError(msg)\n\n    diff_args = docstring_args.difference(signature_args)\n    if len(diff_args) > 0:\n        msg = \"Found one or more unsupported arguments/parameters with docstring in \\n\"\n        msg += f\"file: {file_name}\\n\"\n        if class_name is not None:\n            msg += f\"class: {class_name}\\n\"\n        msg += f\"function/method: {func_name}\\n\"\n        msg += f\"parameter(s): {diff_args}\\n\"\n        raise RuntimeError(msg)\n\n\nignore = [\"__init__.py\", \"__pycache__\"]\n\nstumpy_path = pathlib.Path(__file__).parent / \"stumpy\"\nfilepaths = sorted(f for f in pathlib.Path(stumpy_path).iterdir() if f.is_file())\nfor filepath in filepaths:\n    if filepath.name not in ignore and str(filepath).endswith(\".py\"):\n        file_contents = \"\"\n        with open(filepath, encoding=\"utf8\") as f:\n            file_contents = f.read()\n        module = ast.parse(file_contents)\n\n        # Check Functions\n        function_definitions = [\n            node for node in module.body if isinstance(node, ast.FunctionDef)\n        ]\n        for fd in function_definitions:\n            docstring_args = get_docstring_args(fd, filepath.name, fd.name)\n            signature_args = get_signature_args(fd)\n            check_args(docstring_args, signature_args, filepath.name, fd.name)\n\n        # Check Class Methods\n        class_definitions = [\n            node for node in module.body if isinstance(node, ast.ClassDef)\n        ]\n        for cd in class_definitions:\n            methods = [node for node in cd.body if isinstance(node, ast.FunctionDef)]\n            for fd in methods:\n                docstring_args = get_docstring_args(fd, filepath.name, fd.name, cd.name)\n                signature_args = get_signature_args(fd)\n                check_args(\n                    docstring_args, signature_args, filepath.name, fd.name, cd.name\n                )\n"
        },
        {
          "name": "doctest.sh",
          "type": "blob",
          "size": 0.181640625,
          "content": "#!/bin/bash\n\n#py.test --doctest-modules stumpy/core.py\n#py.test --doctest-modules stumpy/stump.py\n#py.test --doctest-modules stumpy/stumped.py\npy.test --doctest-modules stumpy/scrump.py\n"
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.5556640625,
          "content": "channels:\n  - numba\n  - conda-forge\ndependencies:\n  - python>=3.9\n  - numpy>=1.22\n  - scipy>=1.10\n  - numba>=0.59.1\n  - pandas>=0.20.0\n  - flake8>=3.7.7\n  - flake8-docstrings>=1.5.0\n  - black>=22.1.0\n  - pytest-cov>=2.10.0\n  - dask>=1.2.2\n  - distributed>=1.28.1\n  - jupyterlab>=3.0\n  - matplotlib>=3.3.0\n  - lxml>=4.5.2\n  - twine>=3.2.0\n  - sphinx>=3.5.3\n  - pydata-sphinx-theme>=0.5.2\n  - scikit-learn>=0.21.3\n  - numpydoc>=1.1.0\n  - python-build>=0.7.0\n  - pytest-check-links>=0.7.1\n  - isort>=5.11.0\n  - jupyterlab-myst>=2.0.0\n  - myst-nb>=1.0.0\n  - polars>=1.14.0\n"
        },
        {
          "name": "min_versions.py",
          "type": "blob",
          "size": 7.6376953125,
          "content": "#!/usr/bin/env python\n\nimport argparse\nimport re\n\nimport pandas as pd\nfrom packaging.specifiers import SpecifierSet\nfrom packaging.version import Version\n\n\ndef get_min_python_version():\n    \"\"\"\n    Find the minimum version of Python supported (i.e., not end-of-life)\n    \"\"\"\n    min_python = (\n        pd.read_html(\"https://devguide.python.org/versions/\")[0].iloc[-1].Branch\n    )\n    return min_python\n\n\ndef get_min_numba_numpy_version(min_python):\n    \"\"\"\n    Find the minimum versions of Numba and NumPy that supports the specified\n    `min_python` version\n    \"\"\"\n    df = (\n        pd.read_html(\n            \"https://numba.readthedocs.io/en/stable/user/installing.html#version-support-information\"  # noqa\n        )[0]\n        .dropna()\n        .drop(columns=[\"Numba.1\", \"llvmlite\", \"LLVM\", \"TBB\"])\n        .query('`Python`.str.contains(\"2.7\") == False')\n        .query('`Numba`.str.contains(\".x\") == False')\n        .query('`Numba`.str.contains(\"{\") == False')\n        .pipe(\n            lambda df: df.assign(\n                MIN_PYTHON_SPEC=(\n                    df.Python.str.split().str[1].replace({\"<\": \"=\"}, regex=True)\n                    + df.Python.str.split().str[0].replace({\".x\": \"\"}, regex=True)\n                ).apply(SpecifierSet)\n            )\n        )\n        .pipe(\n            lambda df: df.assign(\n                MIN_NUMPY=(df.NumPy.str.split().str[0].replace({\".x\": \"\"}, regex=True))\n            )\n        )\n        .assign(\n            COMPATIBLE=lambda row: row.apply(\n                check_python_compatibility, axis=1, args=(Version(min_python),)\n            )\n        )\n        .query(\"COMPATIBLE == True\")\n        .pipe(lambda df: df.assign(MINOR=df.Numba.str.split(\".\").str[1]))\n        .pipe(lambda df: df.assign(PATCH=df.Numba.str.split(\".\").str[2]))\n        .sort_values([\"MINOR\", \"PATCH\"], ascending=[False, True])\n        .iloc[-1]\n    )\n    return df.Numba, df.MIN_NUMPY\n\n\ndef check_python_compatibility(row, min_python):\n    \"\"\"\n    Determine the Python version compatibility\n    \"\"\"\n    python_compatible = min_python in (row.MIN_PYTHON_SPEC)\n    return python_compatible\n\n\ndef check_scipy_compatibility(row, min_python, min_numpy):\n    \"\"\"\n    Determine the Python and NumPy version compatibility\n    \"\"\"\n    python_compatible = min_python in (row.MIN_PYTHON_SPEC & row.MAX_PYTHON_SPEC)\n    numpy_compatible = min_numpy in (row.MIN_NUMPY_SPEC & row.MAX_NUMPY_SPEC)\n    return python_compatible & numpy_compatible\n\n\ndef get_min_scipy_version(min_python, min_numpy):\n    \"\"\"\n    Determine the SciPy version compatibility\n    \"\"\"\n    colnames = pd.read_html(\n        \"https://docs.scipy.org/doc/scipy/dev/toolchain.html#numpy\"\n    )[1].columns\n    converter = {colname: str for colname in colnames}\n    df = (\n        pd.read_html(\n            \"https://docs.scipy.org/doc/scipy/dev/toolchain.html#numpy\",\n            converters=converter,\n        )[1]\n        .rename(columns=lambda x: x.replace(\" \", \"_\"))\n        .replace({\".x\": \"\"}, regex=True)\n        .pipe(\n            lambda df: df.assign(\n                SciPy_version=df.SciPy_version.str.replace(\n                    r\"\\d\\/\", \"\", regex=True  # noqa\n                )\n            )\n        )\n        .query('`Python_versions`.str.contains(\"2.7\") == False')\n        .pipe(\n            lambda df: df.assign(\n                MIN_PYTHON_SPEC=df.Python_versions.str.split(\",\")\n                .str[0]\n                .apply(SpecifierSet)\n            )\n        )\n        .pipe(\n            lambda df: df.assign(\n                MAX_PYTHON_SPEC=df.Python_versions.str.split(\",\")\n                .str[1]\n                .apply(SpecifierSet)\n            )\n        )\n        .pipe(\n            lambda df: df.assign(\n                MIN_NUMPY_SPEC=df.NumPy_versions.str.split(\",\")\n                .str[0]\n                .apply(SpecifierSet)\n            )\n        )\n        .pipe(\n            lambda df: df.assign(\n                MAX_NUMPY_SPEC=df.NumPy_versions.str.split(\",\")\n                .str[1]\n                .apply(SpecifierSet)\n            )\n        )\n        .assign(\n            COMPATIBLE=lambda row: row.apply(\n                check_scipy_compatibility,\n                axis=1,\n                args=(Version(min_python), Version(min_numpy)),\n            )\n        )\n        .query(\"COMPATIBLE == True\")\n        .pipe(lambda df: df.assign(MINOR=df.SciPy_version.str.split(\".\").str[1]))\n        .pipe(lambda df: df.assign(PATCH=df.SciPy_version.str.split(\".\").str[2]))\n        .sort_values([\"MINOR\", \"PATCH\"], ascending=[False, True])\n        .iloc[-1]\n    )\n    return df.SciPy_version\n\n\ndef match_pkg_version(line, pkg_name):\n    \"\"\"\n    Regular expression to match package versions\n    \"\"\"\n    matches = re.search(\n        rf\"\"\"\n                        {pkg_name}  # Package name\n                        [\\s=><:\"\\'\\[\\]]*  # Zero or more spaces or special characters\n                        (\\d+\\.\\d+[\\.0-9]*)  # Capture \"version\" in `matches`\n                        \"\"\",\n        line,\n        re.VERBOSE | re.IGNORECASE,  # Ignores all whitespace and case in pattern\n    )\n\n    return matches\n\n\ndef find_pkg_mismatches(pkg_name, pkg_version, fnames):\n    \"\"\"\n    Determine if any package version has mismatches\n    \"\"\"\n    pkg_mismatches = []\n\n    for fname in fnames:\n        with open(fname, \"r\") as file:\n            for line_num, line in enumerate(file, start=1):\n                l = line.strip().replace(\" \", \"\").lower()\n                matches = match_pkg_version(l, pkg_name)\n                if matches is not None:\n                    version = matches.groups()[0]\n                    if version != pkg_version:\n                        pkg_mismatches.append((pkg_name, version, fname, line_num))\n\n    return pkg_mismatches\n\n\ndef test_pkg_mismatch_regex():\n    \"\"\"\n    Validation function for the package mismatch regex\n    \"\"\"\n    pkgs = {\n        \"numpy\": \"0.0\",\n        \"scipy\": \"0.0\",\n        \"python\": \"2.7\",\n        \"python-version\": \"2.7\",\n        \"numba\": \"0.0\",\n    }\n\n    lines = [\n        \"Programming Language :: Python :: 3.8\",\n        \"STUMPY supports Python 3.8\",\n        \"python-version: ['3.8']\",\n        'requires-python = \">=3.8\"',\n        \"numba>=0.55.2\",\n    ]\n\n    for line in lines:\n        match_found = False\n        for pkg_name, pkg_version in pkgs.items():\n            matches = match_pkg_version(line, pkg_name)\n\n            if matches:\n                match_found = True\n                break\n\n        if not match_found:\n            raise ValueError(f'Package mismatch regex fails to cover/match \"{line}\"')\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"min_python\", nargs=\"?\", default=None)\n    args = parser.parse_args()\n\n    if args.min_python is not None:\n        MIN_PYTHON = str(args.min_python)\n    else:\n        MIN_PYTHON = get_min_python_version()\n    MIN_NUMBA, MIN_NUMPY = get_min_numba_numpy_version(MIN_PYTHON)\n    MIN_SCIPY = get_min_scipy_version(MIN_PYTHON, MIN_NUMPY)\n\n    print(\n        f\"python: {MIN_PYTHON}\\n\"\n        f\"numba: {MIN_NUMBA}\\n\"\n        f\"numpy: {MIN_NUMPY}\\n\"\n        f\"scipy: {MIN_SCIPY}\"\n    )\n\n    pkgs = {\n        \"numpy\": MIN_NUMPY,\n        \"scipy\": MIN_SCIPY,\n        \"numba\": MIN_NUMBA,\n        \"python\": MIN_PYTHON,\n        \"python-version\": MIN_PYTHON,\n    }\n\n    fnames = [\n        \"pyproject.toml\",\n        \"requirements.txt\",\n        \"environment.yml\",\n        \".github/workflows/github-actions.yml\",\n        \"README.rst\",\n    ]\n\n    test_pkg_mismatch_regex()\n\n    for pkg_name, pkg_version in pkgs.items():\n        for name, version, fname, line_num in find_pkg_mismatches(\n            pkg_name, pkg_version, fnames\n        ):\n            print(\n                f\"{pkg_name} {pkg_version} Mismatch: Version {version} \"\n                f\"found in {fname}:{line_num}\"\n            )\n"
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "numba.sh",
          "type": "blob",
          "size": 0.029296875,
          "content": "#!/bin/bash\n\n./conda.sh numba\n"
        },
        {
          "name": "paper",
          "type": "tree",
          "content": null
        },
        {
          "name": "pip.sh",
          "type": "blob",
          "size": 0.2197265625,
          "content": "#!/bin/bash\n\n#Convert environment.yml to requirements.dev.txt\nsed -n '/python/,$p' environment.yml | sed '1d' | sed 's/  - //g' > requirements.dev.txt\npython -m pip install -r requirements.dev.txt\nrm -rf requirements.dev.txt\n"
        },
        {
          "name": "postBuild",
          "type": "blob",
          "size": 0.076171875,
          "content": "#!/bin/bash\n\nconda install -c conda-forge stumpy ipywidgets matplotlib pandas\n"
        },
        {
          "name": "pypi.sh",
          "type": "blob",
          "size": 3.564453125,
          "content": "#!/bin/sh\n\n# 1. Update version number in pyproject.toml\n# 2. Update CHANGELOG\n# 3. Update README with new features/functions/tutorials\n# 4. Determine minimum versions and dependencies with ./min.py\n# 5. Bump minimum versions and dependencies\n#    a) pyproject.toml\n#    b) requirements.txt\n#    c) environment.yml\n#    d) .github/worflows/github-actions.yml\n#    e) recipes/meta.yaml in conda-feedstock\n#    f) README.rst\n# 6. Commit all above changes as the latest version number and push\n#\n# For conda-forge\n# 1. Fork the stumpy-feedstock: https://github.com/conda-forge/stumpy-feedstock\n# 2. Create a new branch for the new version:\n#    git checkout -b v1.0.0\n# 3. In the recipe/meta.yaml file\n#    a) Update version number on line 2\n#    b) Update the sha256 on line 10 according to what is found on PyPI\n#       in the \"Download files\" section of the left navigation pane for\n#       the tar.gz file: https://pypi.org/project/stumpy/#files\n#    c) Reset the build number (to zero) on line 14 since this is a new version\n# 4. Commit the changes and push upstream for a PR\n# 5. Check the checkboxes in the PR\n# 6. Add a comment with \"@conda-forge-admin, please rerender\"\n#\n# For readthedocs\n# 1. Update the docs/api.rst to include new features/functions\n#\n# For socializing\n# 1. Post on Twitter\n# 2. Post on LinkedIn\n# 3. Post on Reddit\n# 4. Post new tutorials on Medium\n#\n# To check that the distribution is valid, execute:\n# twine check dist/* \n#\n# Github Release\n# 1. Navigate to the Github release page: https://github.com/TDAmeritrade/stumpy/releases\n# 2. Click \"Draft a new release\": https://github.com/TDAmeritrade/stumpy/releases/new\n# 3. In the \"Tag version\" box, add the version number i.e., \"v1.0.0\"\n# 4. In the Release title\" box, add the version number i.e., \"v1.0.0\"\n# 5. In the \"Describe this release\" box, add the description i.e., \"Version 1.1.0 Release\"\n# 6. Finally, click the \"Publish release\" button\n#\n# PyPI Stats - https://packaging.python.org/guides/analyzing-pypi-package-downloads/\n# SELECT date, count, SUM(count) OVER (ORDER BY date) AS cumsum\n# FROM (\n#     SELECT DATE(timestamp) AS date, COUNT(*) as count\n#     FROM `bigquery-public-data.pypi.file_downloads`\n#     WHERE file.project = 'stumpy'\n#         AND DATE(timestamp)\n#             BETWEEN DATE('2019-01-01')\n#             AND DATE_ADD(CURRENT_DATE(), INTERVAL -1 DAY)\n#     GROUP BY DATE(timestamp)\n# )\n# ORDER BY date\n\n###############\n#  Functions  #\n###############\n\nupload_test_pypi()\n{\n    # Upload to Test PyPi\n    if ! [ -f $HOME/.pypirc ]; then\n        # .pypirc file does not exist, prompt for API token\n        twine upload --verbose --repository-url https://test.pypi.org/legacy/ dist/*\n    else\n        # Get API token from .pypirc file\n        twine upload --verbose -r testpypi dist/*\n    fi\n}\n\nupload_pypi()\n{\n    # Upload to PyPi\n    if ! [ -f $HOME/.pypirc ]; then\n        # .pypirc file does not exist, prompt for API token\n        twine upload dist/*\n    else\n        # Get API token from .pypirc file\n        twine upload -r pypi dist/*\n    fi\n}\n\n# Use API Token instead of username+password\n# https://pypi.org/help/#apitoken\n# Place the API Token(s) in your $HOME/.pypirc\n#\n# # Example .pypirc file\n#\n# [distutils]\n# index-servers =\n#     pypi\n#     testpypi\n#\n# [pypi]\n# repository = https://upload.pypi.org/legacy/\n# username = __token__\n# password = <PyPI API Token>\n#\n# [testpypi]\n# repository = https://test.pypi.org/legacy/\n# username = __token__\n# password = <Test PyPI API Token>\n\n###########\n#   Main  #\n###########\n\nrm -rf dist\npython3 -m build --sdist --wheel\nupload_test_pypi\n# upload_pypi\nrm -rf build dist stumpy.egg-info\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.75390625,
          "content": "[build-system]\nrequires = [\"setuptools\", \"setuptools-scm\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"stumpy\"\nversion = \"1.13.0\"\nrequires-python = \">=3.9\"\nauthors = [\n    {name = \"Sean M. Law\", email = \"seanmylaw@gmail.com\"}\n]\ndescription = \"A powerful and scalable library that can be used for a variety of time series data mining tasks\"\nreadme = \"README.rst\"\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Science/Research\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Programming Language :: Python\",\n    \"Topic :: Software Development\",\n    \"Topic :: Scientific/Engineering\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Operating System :: POSIX\",\n    \"Operating System :: Unix\",\n    \"Operating System :: MacOS\",\n    \"Programming Language :: Python :: 3.9\",\n]\nkeywords = [\"time series\", \"matrix profile\", \"motif\", \"discord\"]\nmaintainers = [\n    {name = \"Sean M. Law\", email = \"seanmylaw@gmail.com\"},\n    {name = \"Nima Sarajpoor\", email = \"nimasarajpoor@gmail.com\"}\n]\nlicense = {text = \"3-clause BSD License\"}\ndependencies = [\n    \"numpy >= 1.22\",\n    \"scipy >= 1.10\",\n    \"numba >= 0.59.1\"\n]\n\n[tool.setuptools]\npackages = [\"stumpy\"]\nlicense-files = [\"LICENSE.txt\"]\n\n[project.optional-dependencies]\nci = [\n    \"pandas >= 0.20.0\",\n    \"dask >= 1.2.2\",\n    \"distributed >= 1.28.1\",\n    \"coverage >= 4.5.3\",\n    \"flake8 >= 3.7.7\",\n    \"flake8-docstrings >= 1.5.0\",\n    \"black >= 22.1.0\",\n    \"pytest >= 4.4.1\",\n    \"isort >= 5.11.0\",\n    'tbb >= 2019.5 ; platform_system == \"Linux\"',\n    \"polars >= 1.14.0\"\n]\n\n[project.urls]\nHomepage = \"https://github.com/TDAmeritrade/stumpy\"\nDocumentation = \"https://stumpy.readthedocs.io/en/latest/\"\nRepository = \"https://github.com/TDAmeritrade/stumpy\"\n\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.0283203125,
          "content": "[pytest]\njunit_family=legacy\n"
        },
        {
          "name": "ray_python_version.py",
          "type": "blob",
          "size": 0.3330078125,
          "content": "#!/usr/bin/env python\n\nimport requests\nfrom packaging.version import Version\n\nclassifiers = (\n    requests.get(\"https://pypi.org/pypi/ray/json\").json().get(\"info\").get(\"classifiers\")\n)\n\nversions = []\nfor c in classifiers:\n    x = c.split()\n    if \"Python\" in x:\n        versions.append(x[-1])\n\nversions.sort(key=Version)\nprint(versions[-1])\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.037109375,
          "content": "numpy>=1.22\nscipy>=1.10\nnumba>=0.59.1\n"
        },
        {
          "name": "setup.sh",
          "type": "blob",
          "size": 0.8330078125,
          "content": "#!/bin/sh\n\nmode=\"\"\nextra=\"\"\n\necho \"y\" |  python -m pip uninstall stumpy\n\n# Parse command line arguments\nfor var in \"$@\"\ndo\n    if [[ $var == \"dev\" ]] || [[ $var == \"ci\" ]]; then\n        echo 'Installing stumpy locally with extra \"ci\" requirement'\n        mode=\"\"\n        extra=\"[ci]\"\n    elif [[ $var == \"edit\" ]]; then\n        echo 'Installing stumpy locally in \"--editable\" mode'\n        mode=\"--editable\"\n    elif [[ $var == \"-e\" ]]; then\n        echo 'Installing stumpy locally in \"--editable\" mode'\n        mode=\"--editable\"\n    else\n        echo \"Installing stumpy in site-packages\"\n    fi\ndone\n\npython -m pip install $mode .$extra\nrm -rf build dist stumpy.egg-info __pycache__\n\nsite_pkgs=$(python -c 'import site; print(site.getsitepackages()[0])')\nif [ -d \"$site_pkgs/stumpy/__pycache__\" ]; then\n    rm -rf $site_pkgs/stumpy/__pycache__/*nb*\nfi\n"
        },
        {
          "name": "stumpy",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.sh",
          "type": "blob",
          "size": 11.3193359375,
          "content": "#!/bin/bash\n\ntest_mode=\"all\"\nprint_mode=\"verbose\"\ncustom_testfiles=()\nmax_iter=10\nsite_pkgs=$(python -c 'import site; print(site.getsitepackages()[0])')\nfcoveragexml=\"coverage.stumpy.xml\"\n# Parse command line arguments\nfor var in \"$@\"\ndo\n    if [[ $var == \"unit\" ]]; then\n        test_mode=\"unit\"\n    elif [[ $var == \"coverage\" ]]; then\n        test_mode=\"coverage\"\n    elif [[ $var == \"notebooks\" ]]; then\n        test_mode=\"notebooks\"\n    elif [[ $var == \"gpu\" ]] || [[ $var == \"gpus\" ]]; then\n        test_mode=\"gpu\"\n    elif [[ $var == \"show\" ]]; then\n        test_mode=\"show\"\n    elif [[ $var == \"count\" ]]; then\n        test_mode=\"count\"\n    elif [[ $var == \"custom\" ]]; then\n        test_mode=\"custom\"\n    elif [[ $var == \"report\" ]]; then\n        test_mode=\"report\"\n    elif [[ $var == \"silent\" || $var == \"print\" ]]; then\n        print_mode=\"silent\"\n    elif [[ \"$var\" == *\"test_\"*\".py\"* ]]; then\n        custom_testfiles+=(\"$var\")\n    elif [[ $var =~ ^[\\-0-9]+$ ]]; then\n        max_iter=$var\n    elif [[ \"$var\" == *\".xml\" ]]; then\n        fcoveragexml=$var\n    elif [[ \"$var\" == \"links\" ]]; then\n        test_mode=\"links\"\n    else\n        echo \"Using default test_mode=\\\"all\\\"\"\n    fi\ndone\n\n###############\n#  Functions  #\n###############\n\ncheck_errs()\n{\n  # Function. Parameter 1 is the return code\n  if [[ $1 -ne \"0\" && $1 -ne \"5\" ]]; then\n    echo \"Error: Test execution encountered exit code $1\"\n    # as a bonus, make our script exit with the right error code.\n    exit $1\n  fi\n}\n\ncheck_black()\n{\n    echo \"Checking Black Code Formatting\"\n    black --check --exclude=\".*\\.ipynb\" --extend-exclude=\".venv\" --diff ./\n    check_errs $?\n}\n\ncheck_isort()\n{\n    echo \"Checking iSort Import Formatting\"\n    isort --profile black --skip .venv --check-only ./\n    check_errs $?\n}\n\ncheck_docstrings()\n{\n    echo \"Checking Missing Docstrings\"\n    ./docstring.py\n    check_errs $?\n}\n\ncheck_flake()\n{\n    echo \"Checking Flake8 Style Guide Enforcement\"\n    flake8 --extend-exclude=.venv ./\n    check_errs $?\n}\n\ncheck_print()\n{\n    if [[ $print_mode == \"verbose\" ]]; then\n        if [[ `grep print */*.py | wc -l` -gt \"0\" ]]; then\n            echo \"Error: print statement found in code\"\n            grep print */*.py\n            exit 1\n        fi\n    fi\n}\n\ncheck_naive()\n{\n    # Check if there are any naive implementations not at start of test file\n    for testfile in tests/test_*.py\n    do\n        last_naive=\"$(grep -n 'def naive_' $testfile | tail -n 1 | awk -F: '{print $1}')\"\n        first_test=\"$(grep -n 'def test_' $testfile | head -n 1 | awk -F: '{print $1}')\"\n        if [[ ! -z $last_naive && ! -z $first_test && $last_naive -gt $first_test ]]; then\n            echo \"Error: naive implementation found in the middle of $testfile line $last_naive\"\n            exit 1\n        fi\n    done\n}\n\ncheck_ray()\n{\n    if ! command -v ray &> /dev/null\n    then\n        echo \"Ray Not Installed\"\n    else\n        echo \"Ray Installed\"\n    fi\n}\n\ngen_ray_coveragerc()\n{\n    # Generate a .coveragerc_ray file that excludes Ray functions and tests\n    echo \"[report]\" > .coveragerc_ray\n    echo \"; Regexes for lines to exclude from consideration\" >> .coveragerc_ray\n    echo \"exclude_also =\" >> .coveragerc_ray\n    echo \"    def .*_ray_*\" >> .coveragerc_ray\n    echo \"    def ,*_ray\\(*\" >> .coveragerc_ray\n    echo \"    def ray_.*\" >> .coveragerc_ray\n    echo \"    def test_.*_ray*\" >> .coveragerc_ray\n}\n\nset_ray_coveragerc()\n{\n    # If `ray` command is not found then generate a .coveragerc_ray file\n    if ! command -v ray &> /dev/null\n    then\n        echo \"Ray Not Installed\"\n        gen_ray_coveragerc\n        fcoveragerc=\"--rcfile=.coveragerc_ray\"\n    else\n        echo \"Ray Installed\"\n        fcoveragerc=\"\"\n    fi\n}\n\nshow_coverage_report()\n{\n    set_ray_coveragerc\n    coverage report -m --fail-under=100 --skip-covered --omit=docstring.py,min_versions.py,ray_python_version.py,stumpy/cache.py $fcoveragerc\n}\n\ngen_coverage_xml_report()\n{\n    # This function saves the coverage report in Cobertura XML format, which is compatible with codecov\n    set_ray_coveragerc\n    coverage xml -o $fcoveragexml --fail-under=100 --omit=docstring.py,min_versions.py,ray_python_version.py,stumpy/cache.py $fcoveragerc\n}\n\ntest_custom()\n{\n    # export NUMBA_DISABLE_JIT=1\n    # export NUMBA_ENABLE_CUDASIM=1\n    # Test one or more user-defined functions repeatedly\n    # \n    # ./test.sh custom tests/test_stump.py\n    # ./test.sh custom 5 tests/test_stump.py\n    # ./test.sh custom 5 tests/test_stump.py::test_stump_self_join\n    #\n    # You may mimic coverage testing conditions by disabling `numba` JIT\n    # and enabling the `cuda` simulator by setting two environment\n    # variables prior to calling `test.sh`:\n    #\n    # NUMBA_DISABLE_JIT=1 NUMBA_ENABLE_CUDASIM=1 ./test.sh custom 5 tests/test_gpu_stump.py\n\n    if [[ ${#custom_testfiles[@]}  -eq \"0\" ]]; then\n        echo \"\"\n        echo \"Error: Missing custom test file(s)\"\n        echo \"Please specify one or more custom test files\"\n        echo \"Example: ./test.sh custom tests/test_stump.py\"\n        exit 1\n    else\n        for i in $(seq $max_iter)\n        do\n            echo \"Custom Test: $i / $max_iter\"\n            for testfile in \"${custom_testfiles[@]}\";\n            do\n                pytest -rsx -W ignore::RuntimeWarning -W ignore::DeprecationWarning -W ignore::UserWarning $testfile\n                check_errs $?\n            done\n        done\n        clean_up\n        exit 0\n    fi\n}\n\ntest_unit()\n{\n    echo \"Testing Numba JIT Compiled Functions\"\n    SECONDS=0\n    if [[ ${#custom_testfiles[@]}  -eq \"0\" ]]; then\n        for testfile in tests/test_*.py\n        do\n            pytest -rsx -W ignore::RuntimeWarning -W ignore::DeprecationWarning -W ignore::UserWarning $testfile\n            check_errs $?\n        done\n    else\n        for testfile in \"${custom_testfiles[@]}\";\n        do\n            pytest -rsx -W ignore::RuntimeWarning -W ignore::DeprecationWarning -W ignore::UserWarning $testfile\n            check_errs $?\n        done\n    fi\n    duration=$SECONDS\n    echo \"Elapsed Time: $((duration / 60)) minutes and $((duration % 60)) seconds\" \n}\n\ntest_coverage()\n{\n    echo \"Disabling Numba JIT and CUDA Compiled Functions\"\n    export NUMBA_DISABLE_JIT=1\n    export NUMBA_ENABLE_CUDASIM=1\n\n    # echo \"Testing Python Functions\"\n    # pytest -rsx -W ignore::RuntimeWarning -W ignore::DeprecationWarning -W ignore::UserWarning tests\n    # check_errs $?\n\n    echo \"Testing Code Coverage\"\n    coverage erase\n\n    # We always attempt to test everything but we may ignore things (ray, helper scripts) when we generate the coverage report\n\n    SECONDS=0\n    if [[ ${#custom_testfiles[@]}  -eq \"0\" ]]; then\n        # Execute all tests\n        for testfile in tests/test_*.py;\n        do\n            coverage run --append --source=. -m pytest -rsx -W ignore::RuntimeWarning -W ignore::DeprecationWarning -W ignore::UserWarning $testfile\n            check_errs $?\n        done\n    else\n        # Execute custom tests\n        for testfile in \"${custom_testfiles[@]}\";\n        do\n            coverage run --append --source=. -m pytest -rsx -W ignore::RuntimeWarning -W ignore::DeprecationWarning -W ignore::UserWarning $testfile\n            check_errs $?\n        done\n    fi\n    duration=$SECONDS\n    echo \"Elapsed Time: $((duration / 60)) minutes and $((duration % 60)) seconds\"\n    show_coverage_report\n}\n\ntest_gpu()\n{\n    echo \"Testing Numba JIT CUDA GPU Compiled Functions\"\n    #for testfile in tests/test_*gpu*.py tests/test_core.py tests/test_precision.py tests/test_non_normalized_decorator.py\n    for testfile in $(grep gpu tests/* | awk -v FS=':' '{print $1}' | uniq);\n    do\n        pytest -rsx -W ignore::RuntimeWarning -W ignore::DeprecationWarning -W ignore::UserWarning $testfile\n        check_errs $?\n    done\n}\n\nshow()\n{\n    echo \"Current working directory: \" `pwd`\n    echo \"Black version: \" `python -c 'exec(\"try:\\n\\timport black;\\n\\tprint(black.__version__);\\nexcept ModuleNotFoundError:\\n\\tprint(\\\"Module Not Found\\\");\")'`\n    echo \"Flake8 version: \" `python -c 'exec(\"try:\\n\\timport flake8;\\n\\tprint(flake8.__version__);\\nexcept ModuleNotFoundError:\\n\\tprint(\\\"Module Not Found\\\");\")'`\n    echo \"Python version: \" `python -c \"import platform; print(platform.python_version())\"`\n    echo \"NumPy version: \" `python -c 'exec(\"try:\\n\\timport numpy;\\n\\tprint(numpy.__version__);\\nexcept ModuleNotFoundError:\\n\\tprint(\\\"Module Not Found\\\");\")'`\n    echo \"SciPy version: \" `python -c 'exec(\"try:\\n\\timport scipy;\\n\\tprint(scipy.__version__);\\nexcept ModuleNotFoundError:\\n\\tprint(\\\"Module Not Found\\\");\")'`\n    echo \"Numba version: \" `python -c 'exec(\"try:\\n\\timport numba;\\n\\tprint(numba.__version__);\\nexcept ModuleNotFoundError:\\n\\tprint(\\\"Module Not Found\\\");\")'`\n    echo \"Dask version: \" `python -c 'exec(\"try:\\n\\timport dask;\\n\\tprint(dask.__version__);\\nexcept ModuleNotFoundError:\\n\\tprint(\\\"Module Not Found\\\");\")'`\n    echo \"Distributed version: \" `python -c 'exec(\"try:\\n\\timport distributed;\\n\\tprint(distributed.__version__);\\nexcept ModuleNotFoundError:\\n\\tprint(\\\"Module Not Found\\\");\")'`\n    echo \"PyTest version: \" `python -c 'exec(\"try:\\n\\timport pytest;\\n\\tprint(pytest.__version__);\\nexcept ModuleNotFoundError:\\n\\tprint(\\\"Module Not Found\\\");\")'`\n    exit 0\n}\n\ncheck_links()\n{\n    echo \"Checking notebook links\"\n    export JUPYTER_PLATFORM_DIRS=1\n    jupyter --paths\n    pytest --check-links docs/Tutorial_*.ipynb notebooks/Tutorial_*.ipynb docs/*.md docs/*.rst  ./*.md ./*.rst\n}\n\ncount()\n{\n    test_count=$(pytest --collect-only -q | sed '$d' | sed '$d' | wc -l | sed 's/ //g')\n    echo \"Found $test_count Unit Tests\"\n}\n\nclean_up()\n{\n    echo \"Cleaning Up\"\n    rm -rf \"dask-worker-space\"\n    rm -rf \"stumpy/__pycache__/\"\n    rm -rf \"tests/__pycache__/\"\n    rm -rf build dist stumpy.egg-info __pycache__\n    rm -f docs/*.nbconvert.ipynb\n    rm -rf \".coveragerc_ray\"\n    if [ -d \"$site_pkgs/stumpy/__pycache__\" ]; then\n        rm -rf $site_pkgs/stumpy/__pycache__/*nb*\n    fi\n\n}\n\nconvert_notebooks()\n{\n    echo \"testing notebooks\"\n    for notebook in `grep ipynb docs/tutorials.rst | sed -e 's/^[ \\t]*//'`\n    do\n        jupyter nbconvert --to notebook --execute \"docs/$notebook\"\n        check_errs $?\n    done\n}\n\n###########\n#   Main  #\n###########\n\nif [[ $test_mode == \"show\" ]]; then\n    echo \"Show development/test environment\"\n    show\nfi\n\nclean_up\ncheck_black\ncheck_isort\ncheck_flake\ncheck_docstrings\ncheck_print\ncheck_naive\ncheck_ray\n\nif [[ $test_mode == \"notebooks\" ]]; then\n    echo \"Executing Tutorial Notebooks Only\"\n    convert_notebooks\nelif [[ $test_mode == \"unit\" ]]; then\n    echo \"Executing Unit Tests Only\"\n    test_unit\nelif [[ $test_mode == \"coverage\" ]]; then\n    echo \"Executing Code Coverage Only\"\n    test_coverage\nelif [[ $test_mode == \"custom\" ]]; then\n    echo \"Executing Custom User-Defined Tests Only\"\n    # Define tests in `test_custom` function above\n    # echo \"Disabling Numba JIT and CUDA Compiled Functions\"\n    # export NUMBA_DISABLE_JIT=1\n    # export NUMBA_ENABLE_CUDASIM=1\n    test_custom\nelif [[ $test_mode == \"report\" ]]; then\n    echo \"Generate Coverage Report Only\"\n    # Assume coverage tests have already been executed\n    # and a coverage file exists\n    gen_coverage_xml_report\nelif [[ $test_mode == \"gpu\" ]]; then\n    echo \"Executing GPU Unit Tests Only\"\n    test_gpu\nelif [[ $test_mode == \"count\" ]]; then\n    echo \"Counting Unit Tests\"\n    count\nelif [[ $test_mode == \"links\" ]]; then\n    echo \"Check Notebook Links  Only\"\n    check_links\nelse\n    echo \"Executing Unit Tests And Code Coverage\"\n    test_unit\n    clean_up\n    test_coverage\nfi\n\nclean_up\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "whitelist.txt",
          "type": "blob",
          "size": 1.2236328125,
          "content": "1d\n1e\n2d\naamp\naampdist\naampdisted\naamped\naampi\nabba\naj\nallc\nameritrade\narange\nargmax\nargmin\nargsort\nargwhere\nasarray\nastype\natol\natsc\nb1\nbfs\nbinarize\nbincount\nbj\nblocky\nbsf\ncac\ncdist\nconvolve\ncov\ncuda\ncudadrv\ncumsum\ncurr\ndask\ndataframe\ndeallocations\ndenom\ndeque\ndf\ndiag\ndiags\ndiscretize\ndoi\ndtype\nf8\nfastmath\nfilepath\nfilepaths\nfilter1d\nfilterwarnings\nfinfo\nflatnonzero\nflipud\nfloc\nfluss\nfname\nfnames\nfromlist\nfscale\nfunc\nfuncs\ngetsitepackages\nglobals\ngpus\ngridsize\nhstack\ni8\niac\nib\nicdm\nidx\nin1d\nincrementing\nintersect1d\nisclose\nisconstant\nisfinite\nisin\nisinf\nismodule\nisnan\nisscalar\nissubdtype\niterdir\njit\nkeepdims\nknn\nlinalg\nlinspace\nloc\nlocs\nlog2\nlru\nlstsq\nm1\nm2\nmaamp\nmaamped\nmaxmatch\nmdl\nmdls\nminlength\nmmotifs\nmpdist\nmpdisted\nmps\nmstump\nmstumped\nmueen\nnanmax\nnanmean\nnanmin\nnanstd\nnanvar\nncols\nncores\nndim\nndimage\nndist\nndists\nnewaxis\nnidx\nnindices\nninf\nnjit\nnlevel\nnn\nnnmark\nnns\nnocover\nnormcase\nnpt\nnrepeat\nnrows\nnum\nnumba\nnumba's\nnumpy\nnworkers\nord\nostinato\nostinatoed\nparam\nparams\npdist\nperformant\npos\nppf\nprange\npreprocess\nprescraamp\nprescrump\nptp\nrcond\nrea\nrst\nrtol\nscipy\nscraamp\nsearchsorted\nsig\nsqrt\nstddev\nstimp\nstimped\nstrided\nstumpi\nsubseq\nsubseqs\nsubsequence\nsubsequences\nsubspaces\ntmp\ntolist\ntopk\nunittest\nunlink\nvect\nvstack\nwelford\n"
        }
      ]
    }
  ]
}