{
  "metadata": {
    "timestamp": 1736559686753,
    "page": 362,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/VisualGLM-6B",
      "stars": 4121,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.04296875,
          "content": "checkpoints/\nruns/\nmodel/__pycache__/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 11.0703125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright Zhengxiao Du\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "MODEL_LICENSE.txt",
          "type": "blob",
          "size": 2.2763671875,
          "content": "The VisualGLM-6B License\n\n1. Definitions\n\n“Licensor” means the VisualGLM-6B Model Team that distributes its Software.\n\n“Software” means the VisualGLM-6B model parameters made available under this license.\n\n2. License Grant\n\nSubject to the terms and conditions of this License, the Licensor hereby grants to you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to use the Software solely for your non-commercial research purposes.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any commercial, military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of People’s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.5625,
          "content": "# VisualGLM-6B\n\n<p align=\"center\">\n🤗 <a href=\"https://huggingface.co/THUDM/visualglm-6b\" target=\"_blank\">HF Repo</a> • ⚒️ <a href=\"https://github.com/THUDM/SwissArmyTransformer\" target=\"_blank\">SwissArmyTransformer (sat)</a> • 🐦 <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> \n</p>\n<p align=\"center\">\n•  📃 <a href=\"https://arxiv.org/abs/2105.13290\" target=\"_blank\">[CogView@NeurIPS 21]</a>  <a href=\"https://github.com/THUDM/CogView\" target=\"_blank\">[GitHub]</a> • 📃 <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n<p align=\"center\">\n    👋 加入我们的 <a href=\"https://join.slack.com/t/chatglm/shared_invite/zt-1y7pqoloy-9b1g6T6JjA8J0KxvUjbwJw\" target=\"_blank\">Slack</a> 和 <a href=\"examples/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<!-- <p align=\"center\">\n🤖<a href=\"https://huggingface.co/spaces/THUDM/visualglm-6b\" target=\"_blank\">VisualGLM-6B在线演示网站</a>\n</p> -->\n\n## News\n[2023.10] 欢迎关注智谱AI新一代多模态对话模型CogVLM（ https://github.com/THUDM/CogVLM ），采用视觉专家新架构，在10项权威经典多模态任务上取得第一名。目前开源CogVLM-17B英文模型，即将基于GLM开源中文模型。\n\n## 介绍\n\nVisualGLM-6B is an open-source, multi-modal dialog language model that supports **images, Chinese, and English**. The language model is based on [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) with 6.2 billion parameters; the image part builds a bridge between the visual model and the language model through the training of [BLIP2-Qformer](https://arxiv.org/abs/2301.12597), with the total model comprising 7.8 billion parameters. **[Click here for English version.](README_en.md)**\n\nVisualGLM-6B 是一个开源的，支持**图像、中文和英文**的多模态对话语言模型，语言模型基于 [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)，具有 62 亿参数；图像部分通过训练 [BLIP2-Qformer](https://arxiv.org/abs/2301.12597) 构建起视觉模型与语言模型的桥梁，整体模型共78亿参数。\n\nVisualGLM-6B 依靠来自于 [CogView](https://arxiv.org/abs/2105.13290) 数据集的30M高质量中文图文对，与300M经过筛选的英文图文对进行预训练，中英文权重相同。该训练方式较好地将视觉信息对齐到ChatGLM的语义空间；之后的微调阶段，模型在长视觉问答数据上训练，以生成符合人类偏好的答案。\n\nVisualGLM-6B 由 [SwissArmyTransformer](https://github.com/THUDM/SwissArmyTransformer)(简称`sat`) 库训练，这是一个支持Transformer灵活修改、训练的工具库，支持Lora、P-tuning等参数高效微调方法。本项目提供了符合用户习惯的huggingface接口，也提供了基于sat的接口。\n\n结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4量化级别下最低只需6.3G显存）。\n\n-----\n\nVisualGLM-6B 开源模型旨在与开源社区一起推动大模型技术发展，恳请开发者和大家遵守开源协议，勿将该开源模型和代码及基于该开源项目产生的衍生物用于任何可能给国家和社会带来危害的用途以及用于任何未经过安全评估和备案的服务。目前，本项目官方未基于 VisualGLM-6B 开发任何应用，包括网站、安卓App、苹果 iOS应用及 Windows App 等。\n\n由于 VisualGLM-6B 仍处于v1版本，目前已知其具有相当多的[**局限性**](README.md#局限性)，如图像描述事实性/模型幻觉问题，图像细节信息捕捉不足，以及一些来自语言模型的局限性。尽管模型在训练的各个阶段都尽力确保数据的合规性和准确性，但由于 VisualGLM-6B 模型规模较小，且模型受概率随机性因素影响，无法保证输出内容的准确性，且模型易被误导（详见局限性部分）。在VisualGLM之后的版本中，将会着力对此类问题进行优化。本项目不承担开源模型和代码导致的数据安全、舆情风险或发生任何模型被误导、滥用、传播、不当利用而产生的风险和责任。\n\n## 样例\nVisualGLM-6B 可以进行图像的描述的相关知识的问答。\n![泰坦尼克号样例](examples/chat_example1.png)\n\n<details>\n<summary>也能结合常识或提出有趣的观点，点击展开/折叠更多样例</summary>\n\n![出租车熨衣服样例](examples/chat_example2.png)\n![蒙娜丽莎狗样例](examples/chat_example3.png)\n\n</details>\n\n## 友情链接\n\n* [XrayGLM](https://github.com/WangRongsheng/XrayGLM) 是基于visualGLM-6B在X光诊断数据集上微调的X光诊断问答的项目，能根据X光片回答医学相关询问。\n<details>\n<summary>点击查看样例</summary>\n\n![样例](https://github.com/WangRongsheng/XrayGLM/raw/main/assets/images/xrayglm.png)\n</details>\n\n* [StarGLM](https://github.com/WangRongsheng/XrayGLM) 是基于Chat/visualGLM-6B在天文数据集上微调的项目，能回答变星光变曲线相关的信息。\n<details>\n<summary>点击查看样例</summary>\n\n![样例](https://github.com/Yu-Yang-Li/StarGLM/raw/main/example/example_4.png)\n\n</details>\n\n\n## 使用\n\n### 模型推理\n\n使用pip安装依赖\n```\npip install -i https://pypi.org/simple -r requirements.txt\n# 国内请使用aliyun镜像，TUNA等镜像同步最近出现问题，命令如下\npip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements.txt\n```\n此时默认会安装`deepspeed`库（支持`sat`库训练），此库对于模型推理并非必要，同时部分Windows环境安装此库时会遇到问题。\n如果想绕过`deepspeed`安装，我们可以将命令改为\n```\npip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements_wo_ds.txt\npip install -i https://mirrors.aliyun.com/pypi/simple/ --no-deps \"SwissArmyTransformer>=0.4.4\"\n```\n\n如果使用Huggingface transformers库调用模型（**也需要安装上述依赖包！**），可以通过如下代码（其中图像路径为本地路径）：\n```python\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\nimage_path = \"your image path\"\nresponse, history = model.chat(tokenizer, image_path, \"描述这张图片。\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, image_path, \"这张图片可能是在什么场所拍摄的？\", history=history)\nprint(response)\n```\n以上代码会由 `transformers` 自动下载模型实现和参数。完整的模型实现可以在 [Hugging Face Hub](https://huggingface.co/THUDM/visualglm-6b)。如果你从 Hugging Face Hub 上下载模型参数的速度较慢，可以从[这里](https://cloud.tsinghua.edu.cn/d/43ffb021ca5f4897b56a/)手动下载模型参数文件，并从本地加载模型。具体做法请参考[从本地加载模型](https://github.com/THUDM/ChatGLM-6B#%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B)。关于基于 transformers 库模型的量化、CPU推理、Mac MPS 后端加速等内容，请参考 [ChatGLM-6B 的低成本部署](https://github.com/THUDM/ChatGLM-6B#%E4%BD%8E%E6%88%90%E6%9C%AC%E9%83%A8%E7%BD%B2)。\n\n如果使用SwissArmyTransformer库调用模型，方法类似，可以使用环境变量`SAT_HOME`决定模型下载位置。在本仓库目录下：\n```python\nimport argparse\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nfrom model import chat, VisualGLMModel\nmodel, model_args = VisualGLMModel.from_pretrained('visualglm-6b', args=argparse.Namespace(fp16=True, skip_init=True))\nfrom sat.model.mixins import CachedAutoregressiveMixin\nmodel.add_mixin('auto-regressive', CachedAutoregressiveMixin())\nimage_path = \"your image path or URL\"\nresponse, history, cache_image = chat(image_path, model, tokenizer, \"描述这张图片。\", history=[])\nprint(response)\nresponse, history, cache_image = chat(None, model, tokenizer, \"这张图片可能是在什么场所拍摄的？\", history=history, image=cache_image)\nprint(response)\n```\n使用`sat`库也可以轻松进行进行参数高效微调。<!-- TODO 具体代码 -->\n\n## 模型微调\n\n多模态任务分布广、种类多，预训练往往不能面面俱到。\n这里我们提供了一个小样本微调的例子，使用20张标注图增强模型回答“背景”问题的能力。\n\n解压`fewshot-data.zip`以后运行如下命令：\n```\nbash finetune/finetune_visualglm.sh\n```\n\n目前支持三种方式的微调：\n\n* LoRA：样例中为ChatGLM模型的第0层和第14层加入了rank=10的LoRA微调，可以根据具体情景和数据量调整`--layer_range`和`--lora_rank`参数。\n* QLoRA：如果资源有限，可以考虑使用`bash finetune/finetune_visualglm_qlora.sh`，QLoRA将ChatGLM的线性层进行了4-bit量化，只需要9.8GB显存即可微调。\n* P-tuning：可以将`--use_lora`替换为`--use_ptuning`，不过不推荐使用，除非模型应用场景非常固定。\n\n训练好以后可以使用如下命令推理：\n\n```\npython cli_demo.py --from_pretrained your_checkpoint_path --prompt_zh 这张图片的背景里有什么内容？\n```\n\n<details>\n<summary>微调前后的效果对比</summary>\n\n微调前：\n\n```\n欢迎使用 VisualGLM-6B 模型，输入图像URL或本地路径读图，继续输入内容对话，clear 重新开始，stop 终止程序\n请输入图像路径或URL（回车进入纯文本对话）： fewshot-data/2p.png\n用户：这张图片的背景里有什么内容？\nVisualGLM-6B：人们总是被生活琐事所困扰，无法摆脱束缚.\n用户：clear\n请输入图像路径或URL（回车进入纯文本对话）： fewshot-data/meme.png\n用户：这张图片的背景里有什么内容？\nVisualGLM-6B：这张图片的背景是一张狗的图片，它躺在木地板上。\n用户：clear\n请输入图像路径或URL（回车进入纯文本对话）： fewshot-data/passport.png\n用户：这张图片的背景里有什么内容？\nVisualGLM-6B：这张图片的背景是一台电脑键盘和两张护照。\n用户：clear\n请输入图像路径或URL（回车进入纯文本对话）：\n```\n\n微调后：\n\n```\n欢迎使用 VisualGLM-6B 模型，输入图像URL或本地路径读图，继续输入内容对话，clear 重新开始，stop 终止程序\n请输入图像路径或URL（回车进入纯文本对话）： fewshot-data/2p.png\n用户：这张图片的背景里有什么内容？\nVisualGLM-6B：这张图片的背景是雨天，\n用户：描述一下这张图片。\nVisualGLM-6B：这张图片描绘了雨天的情景，雨水淅淅沥沥地落在地面上。人们穿着雨衣或雨伞，走在雨中，似乎在寻找着什么.\n用户：clear\n请输入图像路径或URL（回车进入纯文本对话）： fewshot-data/meme.png\n用户：这张图片的背景里有什么内容？\nVisualGLM-6B：这张图片的背景是木质地板。\n用户：什么颜色的地板\nVisualGLM-6B：这张图片的背景是蓝色的。\n用户：描述一下这张图片。\nVisualGLM-6B：这张图片描绘了一只棕色的小狗躺在蓝色的木质地板上。\n用户：clear\n请输入图像路径或URL（回车进入纯文本对话）： fewshot-data/passport.png\n用户：这张图片的背景里有什么内容？\nVisualGLM-6B：这张图片的背景是电脑键盘。\n用户：描述一下这张图片。\nVisualGLM-6B：这张图片的特点是电脑键盘和护照。\n用户：有几张护照    \nVisualGLM-6B：两张护照。\n用户：clear\n请输入图像路径或URL（回车进入纯文本对话）：\n```\n\n</details>\n\n如果希望把LoRA部分的参数合并到原始的权重，可以调用`merge_lora()`，例如：\n\n```python\nfrom finetune_visualglm import FineTuneVisualGLMModel\nimport argparse\n\nmodel, args = FineTuneVisualGLMModel.from_pretrained('checkpoints/finetune-visualglm-6b-05-19-07-36',\n        args=argparse.Namespace(\n        fp16=True,\n        skip_init=True,\n        use_gpu_initialization=True,\n        device='cuda',\n    ))\nmodel.get_mixin('lora').merge_lora()\nargs.layer_range = []\nargs.save = 'merge_lora'\nargs.mode = 'inference'\nfrom sat.training.model_io import save_checkpoint\nsave_checkpoint(1, model, None, None, args)\n```\n\n微调需要安装`deepspeed`库，目前本流程仅支持linux系统，更多的样例说明和Windows系统的流程说明将在近期完成。\n\n## 部署工具\n\n### 命令行 Demo\n\n```shell\npython cli_demo.py \n```\n程序会自动下载sat模型，并在命令行中进行交互式的对话，输入指示并回车即可生成回复，输入 clear 可以清空对话历史，输入 stop 终止程序。\n\n![cli_demo](examples/thu.png)\n程序提供如下超参数控制生成过程与量化精度：\n```\nusage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english] [--quant {8,4}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --max_length MAX_LENGTH\n                        max length of the total sequence\n  --top_p TOP_P         top p for nucleus sampling\n  --top_k TOP_K         top k for top k sampling\n  --temperature TEMPERATURE\n                        temperature for sampling\n  --english             only output English\n  --quant {8,4}         quantization bits\n```\n需要注意的是，在训练时英文问答对的提示词为`Q: A:`，而中文为`问：答：`，在网页demo中采取了中文的提示，因此英文回复会差一些且夹杂中文；如果需要英文回复，请使用`cli_demo.py`中的`--english`选项。\n\n我们也提供了继承自`ChatGLM-6B`的打字机效果命令行工具，此工具使用Huggingface模型：\n```shell\npython cli_demo_hf.py\n```\n\n我们也支持模型并行多卡部署：（需要更新最新版本的sat，如果之前下载了checkpoint，也需要手动删除后重新下载）\n```\ntorchrun --nnode 1 --nproc-per-node 2 cli_demo_mp.py\n```\n\n### 网页版 Demo\n![web_demo](examples/web_demo.png)\n\n我们提供了一个基于 [Gradio](https://gradio.app) 的网页版 Demo，首先安装 Gradio：`pip install gradio`。\n然后下载并进入本仓库运行`web_demo.py`：\n\n```\ngit clone https://github.com/THUDM/VisualGLM-6B\ncd VisualGLM-6B\npython web_demo.py\n```\n程序会自动下载 sat 模型，并运行一个 Web Server，并输出地址。在浏览器中打开输出的地址即可使用。\n\n\n我们也提供了继承自`ChatGLM-6B`的打字机效果网页版工具，此工具使用 Huggingface 模型，启动后将运行在`:8080`端口上：\n```shell\npython web_demo_hf.py\n```\n\n两种网页版 demo 均接受命令行参数`--share`以生成 gradio 公开链接，接受`--quant 4`和`--quant 8`以分别使用4比特量化/8比特量化减少显存占用。\n\n### API部署\n首先需要安装额外的依赖 `pip install fastapi uvicorn`，然后运行仓库中的 [api.py](api.py)：\n```shell\npython api.py\n```\n程序会自动下载 sat 模型，默认部署在本地的 8080 端口，通过 POST 方法进行调用。下面是用`curl`请求的例子，一般而言可以也可以使用代码方法进行POST。\n```shell\necho \"{\\\"image\\\":\\\"$(base64 path/to/example.jpg)\\\",\\\"text\\\":\\\"描述这张图片\\\",\\\"history\\\":[]}\" > temp.json\ncurl -X POST -H \"Content-Type: application/json\" -d @temp.json http://127.0.0.1:8080\n```\n得到的返回值为\n```\n  {\n    \"response\":\"这张图片展现了一只可爱的卡通羊驼，它站在一个透明的背景上。这只羊驼长着一张毛茸茸的耳朵和一双大大的眼睛，它的身体是白色的，带有棕色斑点。\",\n    \"history\":[('描述这张图片', '这张图片展现了一只可爱的卡通羊驼，它站在一个透明的背景上。这只羊驼长着一张毛茸茸的耳朵和一双大大的眼睛，它的身体是白色的，带有棕色斑点。')],\n    \"status\":200,\n    \"time\":\"2023-05-16 20:20:10\"\n  }\n```\n\n我们也提供了使用Huggingface模型的 [api_hf.py](api_hf.py)，用法和sat模型的api一致：\n```shell\npython api_hf.py\n```\n\n\n## 模型量化\n在Huggingface实现中，模型默认以 FP16 精度加载，运行上述代码需要大概 15GB 显存。如果你的 GPU 显存有限，可以尝试以量化方式加载模型。\n使用方法如下：\n```python\n# 按需修改，目前只支持 4/8 bit 量化。下面将只量化ChatGLM，ViT 量化时误差较大\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).quantize(8).half().cuda()\n```\n\n在sat实现中，需先传参将加载位置改为`cpu`，再进行量化。方法如下，详见`cli_demo.py`：\n```python\nfrom sat.quantization.kernels import quantize\nquantize(model, args.quant).cuda()\n# 只需要 7GB 显存即可推理\n```\n\n## 局限性\n本项目正处于V1版本视觉和语言模型的参数、计算量都较小，我们总结了如下主要存在的改进方向：\n- 图像描述事实性/模型幻觉问题。在生成图像长描述的时候，距离图像较远时，语言模型的将占主导，有一定可能根据上下文生成并不存在于图像的内容。\n- 属性错配问题。在多物体的场景中，部分物体的某些属性，经常被错误安插到其他物体上。\n- 分辨率问题。本项目使用了224*224的分辨率，也是视觉模型中最为常用的尺寸；然而为了进行更细粒度的理解，更大的分辨率和计算量是必要的。\n- 由于数据等方面原因，模型暂时不具有中文ocr的能力（英文ocr能力有一些），我们会在后续版本中增加这个能力。\n## 协议\n\n本仓库的代码依照 [Apache-2.0](LICENSE.txt) 协议开源，VisualGLM-6B 模型的权重的使用则需要遵循 [Model License](MODEL_LICENSE.txt)。\n\n## 引用与致谢\n如果你觉得我们的工作有帮助的话，请考虑引用下列论文\n```\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n@article{ding2021cogview,\n  title={Cogview: Mastering text-to-image generation via transformers},\n  author={Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others},\n  journal={Advances in Neural Information Processing Systems},\n  volume={34},\n  pages={19822--19835},\n  year={2021}\n}\n```\n在VisualGLM-6B的指令微调阶段的数据集中，包含了来自[MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)和[LLAVA](https://github.com/haotian-liu/LLaVA)项目的一部分英文图文数据，以及许多经典的跨模态工作数据集，衷心感谢他们的贡献。\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 14.0234375,
          "content": "# VisualGLM-6B\n<p align=\"center\">\n🤗 <a href=\"https://huggingface.co/THUDM/visualglm-6b\" target=\"_blank\">HF Repo</a> • ⚒️ <a href=\"https://github.com/THUDM/SwissArmyTransformer\" target=\"_blank\">SwissArmyTransformer (sat)</a> • 🐦 <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> \n</p>\n<p align=\"center\">\n•  📃 <a href=\"https://arxiv.org/abs/2105.13290\" target=\"_blank\">[CogView@NeurIPS 21]</a>  <a href=\"https://github.com/THUDM/CogView\" target=\"_blank\">[GitHub]</a> • 📃 <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n<p align=\"center\">\n    👋 Join us on <a href=\"https://join.slack.com/t/chatglm/shared_invite/zt-1th2q5u69-7tURzFuOPanmuHy9hsZnKA\" target=\"_blank\">Slack</a> and <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<!-- <p align=\"center\">\n🤖<a href=\"https://huggingface.co/spaces/THUDM/visualglm-6b\" target=\"_blank\">VisualGLM-6B Online Demo Website</a>\n</p> -->\n\n## Introduction\nVisualGLM-6B is an open-source, multi-modal dialog language model that supports **images, Chinese, and English**. The language model is based on [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) with 6.2 billion parameters; the image part builds a bridge between the visual model and the language model through the training of [BLIP2-Qformer](https://arxiv.org/abs/2301.12597), with the total model comprising 7.8 billion parameters.\n\nVisualGLM-6B relies on 30M high-quality Chinese image-text pairs from the [CogView](https://arxiv.org/abs/2105.13290) dataset and 300M filtered English image-text pairs for pre-training, with equal weight for Chinese and English. This training method aligns visual information well to the semantic space of ChatGLM. In the subsequent fine-tuning phase, the model is trained on long visual question answering data to generate answers that align with human preferences.\n\nVisualGLM-6B is trained using the [SwissArmyTransformer](https://github.com/THUDM/SwissArmyTransformer) (abbreviated as sat) library, a utility library for flexible modification and training of Transformer, supporting efficient fine-tuning methods like Lora and P-tuning. This project provides a user-friendly huggingface interface, as well as an interface based on sat.\n\nHowever, as VisualGLM-6B is still at the v1 stage, it is known to have quite a few [**limitations**](#Limitations), such as factual inaccuracy/model hallucination in image description, lack of capturing image detail information, and some limitations from the language model. Please be aware of these issues and evaluate the potential risks before using. In future versions of VisualGLM, we will strive to optimize these issues.\n\nWith model quantization technology, users can deploy locally on consumer-grade graphics cards (requiring as little as 6.3G memory under INT4 quantization level).\n\n## Examples\nVisualGLM-6B can answer questions related to image description.\n![Titanic example](examples/chat_example1.png)\n\n<details>\n<summary>It can also combine common sense or propose interesting views. Click to expand/collapse more examples</summary>\n\n![Ironing shirt taxi example](examples/chat_example2.png)\n![Mona Lisa dog example](examples/chat_example3.png)\n\n</details>\n\n\n## Usage\n\n### Model Inference\n\nInstall dependencies with pip\n```\npip install -i https://pypi.org/simple -r requirements.txt\npip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements.txt\n```\nThis will default to installing the deepspeed library (which supports the sat library training). This library is not necessary for model inference and can cause problems when installed in some Windows environments.\nIf you want to bypass deepspeed installation, you can change the command to:\n```\npip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements_wo_ds.txt\npip install -i https://mirrors.aliyun.com/pypi/simple/ --no-deps \"SwissArmyTransformer>=0.3.6\"\n```\n\nIf you are calling the model using the Huggingface transformers library (you also need to install the above dependency packages!), you can use the following code (where the image path is the local path):\n```python\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\nimage_path = \"your image path\"\nresponse, history = model.chat(tokenizer, image_path, \"描述这张图片。\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, image_path, \"这张图片可能是在什么场所拍摄的？\", history=history)\nprint(response)\n```\n\nIf you use the SwissArmyTransformer library to call the model, the method is similar, and you can use the environment variable SAT_HOME to determine the model download location. In the directory of this repository:\n```python\nimport argparse\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nfrom model import chat, VisualGLMModel\nmodel, model_args = VisualGLMModel.from_pretrained('visualglm-6b', args=argparse.Namespace(fp16=True, skip_init=True))\nfrom sat.model.mixins import CachedAutoregressiveMixin\nmodel.add_mixin('auto-regressive', CachedAutoregressiveMixin())\nimage_path = \"your image path or URL\"\nresponse, history, cache_image = chat(image_path, model, tokenizer, \"Describe this picture.\", history=[])\nprint(response)\nresponse, history, cache_image = chat(None, model, tokenizer, \"Where could this picture possibly have been taken?\", history=history, image=cache_image)\nprint(response)\n```\n\nUsing the `sat` library can also easily carry out efficient parameter fine-tuning. <!-- TODO specific code -->\n\nPlease note that the Huggingface model implementation is located in the [Huggingface repository](https://huggingface.co/THUDM/visualglm-6b), and the `sat` model implementation is included in this repository.\n\n## Model Fine-tuning\n\nMultimodal tasks are wide-ranging and diverse, and pre-training often cannot cover all bases.\nHere we provide an example of small sample fine-tuning, using 20 labeled images to enhance the model's ability to answer \"background\" questions.\n\nAfter unzipping fewshot-data.zip, run the following command:\n```\nbash finetune/finetune_visualglm.sh\n```\n\nCurrently we support three types of (parameter-efficient) fine-tuning:\n\n* LoRA: In the given example, we add rank=10 LoRA for layer 0 and layer 14 in ChatGLM. You can adjust `--layer_range` and `--lora_rank` to fit your application and data amount.\n* QLoRA: If your resource is limited, consider using `bash finetune/finetune_visualglm_qlora.sh`, which do 4-bit quantization for ChatGLM Linear layers, reducing the required GPU memory to 9.8 GB.\n* P-tuning: You can replace `--use_lora` to `--use_ptuning`, but not recommended, unless your application has a relatively fixed input and output template.\n\nAfter training, you can use the following command for inference:\n\n```\npython cli_demo.py --from_pretrained your_checkpoint_path --prompt_zh 这张图片的背景里有什么内容？\n```\n\nFine-tuning requires the installation of the deepspeed library, and currently this process only supports the Linux system. More examples and instructions for the Windows system will be completed in the near future.\n\nIf you want to merge LoRA weights into original weights, just call `merge_lora()`:\n\n```python\nfrom finetune_visualglm import FineTuneVisualGLMModel\nimport argparse\n\nmodel, args = FineTuneVisualGLMModel.from_pretrained('checkpoints/finetune-visualglm-6b-05-19-07-36',\n        args=argparse.Namespace(\n        fp16=True,\n        skip_init=True,\n        use_gpu_initialization=True,\n        device='cuda',\n    ))\nmodel.get_mixin('lora').merge_lora()\nargs.layer_range = []\nargs.save = 'merge_lora'\nargs.mode = 'inference'\nfrom sat.training.model_io import save_checkpoint\nsave_checkpoint(1, model, None, None, args)\n```\n\n## Deployment Tools\n\n### Command Line Demo\n\n```shell\npython cli_demo.py \n```\nThe program will automatically download the sat model and interact in the command line. You can generate replies by entering instructions and pressing enter. Enter 'clear' to clear the conversation history and 'stop' to stop the program.\n\n![cli_demo](examples/thu.png)\nThe program provides the following hyperparameters to control the generation process and quantization accuracy:\n```\nusage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english] [--quant {8,4}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --max_length MAX_LENGTH\n                        max length of the total sequence\n  --top_p TOP_P         top p for nucleus sampling\n  --top_k TOP_K         top k for top k sampling\n  --temperature TEMPERATURE\n                        temperature for sampling\n  --english             only output English\n  --quant {8,4}         quantization bits\n```\nNote that during training, the prompt words for English Q&A pairs are 'Q: A:', while in Chinese they are '问：答：'. The web demo uses Chinese prompts, so the English replies will be worse and interspersed with Chinese; if you need English replies, please use the --english option in cli_demo.py.\n\nWe also provide a typewriter effect command line tool inherited from ChatGLM-6B, which uses the Huggingface model:\n```shell\npython cli_demo_hf.py\n```\n\n### Web Demo\n![web_demo](examples/web_demo.png)\n\nWe provide a web demo based on [Gradio](https://gradio.app). First, install Gradio: `pip install gradio`.\nThen download and enter this repository and run `web_demo.py`:\n\n```\ngit clone https://github.com/THUDM/VisualGLM-6B\ncd VisualGLM-6B\npython web_demo.py\n```\nThe program will automatically download the sat model and run a Web Server, outputting the address. Open the output address in your browser to use it.\n\nWe also provide a web tool with a typewriter effect inherited from ChatGLM-6B, which uses the Huggingface model and will run on port :8080 after starting:\n```shell\npython web_demo_hf.py\n```\n\nBoth web demos accept the command line parameter --share to generate a public link for gradio, and accept --quant 4 and --quant 8 to use 4-bit quantization/8-bit quantization to reduce GPU memory usage.\n\n### API Deployment\nFirst, you need to install additional dependencies pip install fastapi uvicorn, then run the api.py in the repository:\n```shell\npython api.py\n```\nThe program will automatically download the sat model, and by default it will be deployed on local port 8080 and called through the POST method. Below is an example of a request with curl, but in general you can also use a code method to POST.\n```shell\necho \"{\\\"image\\\":\\\"$(base64 path/to/example.jpg)\\\",\\\"text\\\":\\\"Describe this picture\\\",\\\"history\\\":[]}\" > temp.json\ncurl -X POST -H \"Content-Type: application/json\" -d @temp.json http://127.0.0.1:8080\n```\n\nWe also provide an api_hf.py that uses the Huggingface model, which works the same way as the sat model's api:\n```shell\npython api_hf.py\n```\n\n\n## Model Quantization\nIn the Huggingface implementation, the model is loaded with FP16 precision by default, and running the above code requires about 15GB of GPU memory. If your GPU memory is limited, you can try loading the model in a quantized manner.\nHere's how:\n```python\n# Modify as needed, currently only 4/8 bit quantization is supported. The following will only quantize ChatGLM, as the error is larger when quantizing ViT\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).quantize(8).half().cuda()\n```\n\nIn the sat implementation, you need to change the loading location to 'cpu' first, and then perform quantization. Here's how, see cli_demo.py for details:\n```python\nfrom sat.quantization.kernels import quantize\nmodel = quantize(model, args.quant).cuda()\n# only need 7GB GPU memory to inference\n```\n\n## Limitations\nThis project is currently at V1 version of the visual and language model parameters, the amount of calculation is small, we have summarized the following main improvements:\n\n- Image description factuality/model hallucination problem. When generating long descriptions of images, as the distance from the image increases, the language model will dominate, and there is a certain possibility of generating content that does not exist in the image based on the context.\n- Attribute mismatch problem. In scenes with multiple objects, some attributes of some objects are often incorrectly inserted onto other objects.\n- Resolution issue. This project uses a resolution of 224*224, which is the most commonly used size in visual models; however, for more fine-grained understanding, larger resolution and computation are necessary.\n- Due to data and other reasons, the model currently does not have the ability to perform Chinese OCR (some ability for English OCR), we will add this ability in future versions.\n## License\n\nThe code in this repository is open source under the Apache-2.0 license, while the use of the VisualGLM-6B model weights must comply with the Model License.\n\n## Citation & Acknowledgements\nIf you find our work helpful, please consider citing the following papers\n```\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n@article{ding2021cogview,\n  title={Cogview: Mastering text-to-image generation via transformers},\n  author={Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others},\n  journal={Advances in Neural Information Processing Systems},\n  volume={34},\n  pages={19822--19835},\n  year={2021}\n}\n```\nIn the instruction fine-tuning phase of the VisualGLM-6B dataset, there are some English image-text data from the [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) and [LLAVA](https://github.com/haotian-liu/LLaVA) projects, as well as many classic cross-modal work datasets. We sincerely thank them for their contributions.\n"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 1.6455078125,
          "content": "import os\nimport json\nimport uvicorn\nfrom fastapi import FastAPI, Request\nfrom model import is_chinese, get_infer_setting, generate_input, chat\nimport datetime\nimport torch\n\ngpu_number = 0\nmodel, tokenizer = get_infer_setting(gpu_device=gpu_number)\n\napp = FastAPI()\n@app.post('/')\nasync def visual_glm(request: Request):\n    json_post_raw = await request.json()\n    print(\"Start to process request\")\n\n    json_post = json.dumps(json_post_raw)\n    request_data = json.loads(json_post)\n    input_text, input_image_encoded, history = request_data['text'], request_data['image'], request_data['history']\n    input_para = {\n        \"max_length\": 2048,\n        \"min_length\": 50,\n        \"temperature\": 0.8,\n        \"top_p\": 0.4,\n        \"top_k\": 100,\n        \"repetition_penalty\": 1.2\n    }\n    input_para.update(request_data)\n\n    is_zh = is_chinese(input_text)\n    input_data = generate_input(input_text, input_image_encoded, history, input_para)\n    input_image, gen_kwargs =  input_data['input_image'], input_data['gen_kwargs']\n    with torch.no_grad():\n        answer, history, _ = chat(None, model, tokenizer, input_text, history=history, image=input_image, \\\n                            max_length=gen_kwargs['max_length'], top_p=gen_kwargs['top_p'], \\\n                            top_k = gen_kwargs['top_k'], temperature=gen_kwargs['temperature'], english=not is_zh)\n        \n    now = datetime.datetime.now()\n    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    response = {\n        \"result\": answer,\n        \"history\": history,\n        \"status\": 200,\n        \"time\": time\n    }\n    return response\n\n\nif __name__ == '__main__':\n    uvicorn.run(app, host='0.0.0.0', port=8080, workers=1)"
        },
        {
          "name": "api_hf.py",
          "type": "blob",
          "size": 1.3330078125,
          "content": "import os\nimport json\nfrom transformers import AutoTokenizer, AutoModel\nimport uvicorn\nfrom fastapi import FastAPI, Request\nimport datetime\nfrom model import process_image\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\n\n\napp = FastAPI()\n@app.post('/')\nasync def visual_glm(request: Request):\n    json_post_raw = await request.json()\n    print(\"Start to process request\")\n\n    json_post = json.dumps(json_post_raw)\n    request_data = json.loads(json_post)\n\n    history = request_data.get(\"history\")\n    image_encoded = request_data.get(\"image\")\n    query = request_data.get(\"text\")\n    image_path = process_image(image_encoded)\n\n    with torch.no_grad():    \n        result = model.stream_chat(tokenizer, image_path, query, history=history)\n    last_result = None\n    for value in result:\n        last_result = value\n    answer = last_result[0]\n\n    if os.path.isfile(image_path):\n        os.remove(image_path)\n    now = datetime.datetime.now()\n    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    response = {\n        \"result\": answer,\n        \"history\": history,\n        \"status\": 200,\n        \"time\": time\n    }\n    return response\n\n\nif __name__ == \"__main__\":\n   uvicorn.run(app, host='0.0.0.0', port=8080, workers=1)"
        },
        {
          "name": "cli_demo.py",
          "type": "blob",
          "size": 4.3095703125,
          "content": "# -*- encoding: utf-8 -*-\n\nimport os\nimport sys\nimport torch\nimport argparse\nfrom transformers import AutoTokenizer\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.quantization.kernels import quantize\n\nfrom model import VisualGLMModel, chat\nfrom finetune_visualglm import FineTuneVisualGLMModel\nfrom sat.model import AutoModel\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max_length\", type=int, default=2048, help='max length of the total sequence')\n    parser.add_argument(\"--top_p\", type=float, default=0.4, help='top p for nucleus sampling')\n    parser.add_argument(\"--top_k\", type=int, default=100, help='top k for top k sampling')\n    parser.add_argument(\"--temperature\", type=float, default=.8, help='temperature for sampling')\n    parser.add_argument(\"--english\", action='store_true', help='only output English')\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None, help='quantization bits')\n    parser.add_argument(\"--from_pretrained\", type=str, default=\"visualglm-6b\", help='pretrained ckpt')\n    parser.add_argument(\"--prompt_zh\", type=str, default=\"描述这张图片。\", help='Chinese prompt for the first round')\n    parser.add_argument(\"--prompt_en\", type=str, default=\"Describe the image.\", help='English prompt for the first round')\n    args = parser.parse_args()\n\n    # load model\n    model, model_args = AutoModel.from_pretrained(\n        args.from_pretrained,\n        args=argparse.Namespace(\n        fp16=True,\n        skip_init=True,\n        use_gpu_initialization=True if (torch.cuda.is_available() and args.quant is None) else False,\n        device='cuda' if (torch.cuda.is_available() and args.quant is None) else 'cpu',\n    ))\n    model = model.eval()\n\n    if args.quant:\n        quantize(model, args.quant)\n        if torch.cuda.is_available():\n            model = model.cuda()\n\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n    if not args.english:\n        print('欢迎使用 VisualGLM-6B 模型，输入图像URL或本地路径读图，继续输入内容对话，clear 重新开始，stop 终止程序')\n    else:\n        print('Welcome to VisualGLM-6B model. Enter an image URL or local file path to load an image. Continue inputting text to engage in a conversation. Type \"clear\" to start over, or \"stop\" to end the program.')\n    with torch.no_grad():\n        while True:\n            history = None\n            cache_image = None\n            if not args.english:\n                image_path = input(\"请输入图像路径或URL（回车进入纯文本对话）： \")\n            else:\n                image_path = input(\"Please enter the image path or URL (press Enter for plain text conversation): \")\n\n            if image_path == 'stop':\n                break\n            if len(image_path) > 0:\n                query = args.prompt_en if args.english else args.prompt_zh\n            else:\n                if not args.english:\n                    query = input(\"用户：\")\n                else:\n                    query = input(\"User: \")\n            while True:\n                if query == \"clear\":\n                    break\n                if query == \"stop\":\n                    sys.exit(0)\n                try:\n                    response, history, cache_image = chat(\n                        image_path, \n                        model, \n                        tokenizer,\n                        query, \n                        history=history, \n                        image=cache_image, \n                        max_length=args.max_length, \n                        top_p=args.top_p, \n                        temperature=args.temperature,\n                        top_k=args.top_k,\n                        english=args.english,\n                        invalid_slices=[slice(63823, 130000)] if args.english else []\n                        )\n                except Exception as e:\n                    print(e)\n                    break\n                sep = 'A:' if args.english else '答：'\n                print(\"VisualGLM-6B：\"+response.split(sep)[-1].strip())\n                image_path = None\n                if not args.english:\n                    query = input(\"用户：\")\n                else:\n                    query = input(\"User: \")\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        {
          "name": "cli_demo_hf.py",
          "type": "blob",
          "size": 2.2001953125,
          "content": "import os\nimport platform\nimport signal\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\nmodel = model.eval()\n\nos_name = platform.system()\nclear_command = 'cls' if os_name == 'Windows' else 'clear'\nstop_stream = False\n\n\ndef build_prompt(history, prefix):\n    prompt = prefix\n    for query, response in history:\n        prompt += f\"\\n\\n用户：{query}\"\n        prompt += f\"\\n\\nVisualGLM-6B：{response}\"\n    return prompt\n\n\ndef signal_handler(signal, frame):\n    global stop_stream\n    stop_stream = True\n\n\ndef main():\n    global stop_stream\n    while True:\n        history = []\n        prefix = \"欢迎使用 VisualGLM-6B 模型，输入图片路径和内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n        print(prefix)\n        image_path = input(\"\\n请输入图片路径：\")\n        if image_path == \"stop\":\n            break\n        prefix = prefix + \"\\n\" + image_path\n        query = \"描述这张图片。\"\n        while True:\n            count = 0\n            with torch.no_grad():\n                for response, history in model.stream_chat(tokenizer, image_path, query, history=history):\n                    if stop_stream:\n                        stop_stream = False\n                        break\n                    else:\n                        count += 1\n                        if count % 8 == 0:\n                            os.system(clear_command)\n                            print(build_prompt(history, prefix), flush=True)\n                            signal.signal(signal.SIGINT, signal_handler)\n            os.system(clear_command)\n            print(build_prompt(history, prefix), flush=True)\n            query = input(\"\\n用户：\")\n            if query.strip() == \"clear\":\n                break\n            if query.strip() == \"stop\":\n                stop_stream = True\n                exit(0)\n            # if query.strip() == \"clear\":\n            #     history = []\n            #     os.system(clear_command)\n            #     print(prefix)\n            #     continue\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        {
          "name": "cli_demo_mp.py",
          "type": "blob",
          "size": 2.7080078125,
          "content": "# -*- encoding: utf-8 -*-\n\nimport os\nimport sys\nimport torch\nimport argparse\nfrom transformers import AutoTokenizer\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.quantization.kernels import quantize\n\nfrom model import VisualGLMModel, chat\nfrom finetune_visualglm import FineTuneVisualGLMModel\nfrom sat.model import AutoModel\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max_length\", type=int, default=2048, help='max length of the total sequence')\n    parser.add_argument(\"--top_p\", type=float, default=0.4, help='top p for nucleus sampling')\n    parser.add_argument(\"--top_k\", type=int, default=100, help='top k for top k sampling')\n    parser.add_argument(\"--temperature\", type=float, default=.8, help='temperature for sampling')\n    parser.add_argument(\"--english\", action='store_true', help='only output English')\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None, help='quantization bits')\n    parser.add_argument(\"--from_pretrained\", type=str, default=\"visualglm-6b\", help='pretrained ckpt')\n    parser.add_argument(\"--prompt_zh\", type=str, default=\"描述这张图片。\", help='Chinese prompt for the first round')\n    parser.add_argument(\"--prompt_en\", type=str, default=\"Describe the image.\", help='English prompt for the first round')\n    args = parser.parse_args()\n\n    # load model\n    model, model_args = AutoModel.from_pretrained(\n        args.from_pretrained,\n        args=argparse.Namespace(\n        fp16=True,\n        skip_init=True,\n        use_gpu_initialization=True if (torch.cuda.is_available() and args.quant is None) else False,\n        device='cuda' if (torch.cuda.is_available() and args.quant is None) else 'cpu',\n    ), overwrite_args={'model_parallel_size': 2})\n    model = model.eval()\n\n    if args.quant:\n        quantize(model.transformer, args.quant)\n        if torch.cuda.is_available():\n            model = model.cuda()\n\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n    image_path = 'fewshot-data/meme.png'\n    query = args.prompt_en if args.english else args.prompt_zh\n    history = None\n    cache_image = None\n    response, history, cache_image = chat(\n        image_path, \n        model, \n        tokenizer,\n        query, \n        history=history, \n        image=cache_image, \n        max_length=args.max_length, \n        top_p=args.top_p, \n        temperature=args.temperature,\n        top_k=args.top_k,\n        english=args.english,\n        invalid_slices=[slice(63823, 130000)] if args.english else []\n        )\n    sep = 'A:' if args.english else '答：'\n    print(response.split(sep)[-1].strip())\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "fewshot-data.zip",
          "type": "blob",
          "size": 6538.33984375,
          "content": ""
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune_visualglm.py",
          "type": "blob",
          "size": 7.7880859375,
          "content": "import os\nimport torch\nimport argparse\n\nfrom sat import mpu, get_args, get_tokenizer\nfrom sat.training.deepspeed_training import training_main\nfrom model import VisualGLMModel\nfrom sat.model.finetune import PTuningV2Mixin\nfrom sat.model.finetune.lora2 import LoraMixin\n\nclass FineTuneVisualGLMModel(VisualGLMModel):\n    def __init__(self, args, transformer=None, **kw_args):\n        super().__init__(args, transformer=transformer, **kw_args)\n        if args.use_ptuning:\n            self.add_mixin(\"ptuning\", PTuningV2Mixin(args.num_layers, args.hidden_size // args.num_attention_heads, args.num_attention_heads, args.pre_seq_len))\n        if args.use_lora:\n            self.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range), reinit=True)\n            # self.get_mixin(\"eva\").model.glm_proj = replace_linear_with_lora(self.get_mixin(\"eva\").model.glm_proj, LoraLinear, args.lora_rank)\n        elif args.use_qlora:\n            self.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range, qlora=True), reinit=True)\n        self.args = args\n        \n    @classmethod\n    def add_model_specific_args(cls, parser):\n        group = parser.add_argument_group('VisualGLM-finetune', 'VisualGLM finetune Configurations')\n        group.add_argument('--pre_seq_len', type=int, default=8)\n        group.add_argument('--lora_rank', type=int, default=10)\n        group.add_argument('--use_ptuning', action=\"store_true\")\n        group.add_argument('--use_lora', action=\"store_true\")\n        group.add_argument('--use_qlora', action=\"store_true\")\n        group.add_argument('--layer_range', nargs='+', type=int, default=None)\n        return super().add_model_specific_args(parser)\n\n    def disable_untrainable_params(self):\n        enable = []\n        if self.args.use_ptuning:\n            enable.extend(['ptuning'])\n        if self.args.use_lora or self.args.use_qlora:\n            enable.extend(['matrix_A', 'matrix_B'])\n        for n, p in self.named_parameters():\n            flag = False\n            for e in enable:\n                if e.lower() in n.lower():\n                    flag = True\n                    break\n            if not flag:\n                p.requires_grad_(False)\n            else:\n                print(n)\n\n\ndef get_batch(data_iterator, args, timers):\n    # Items and their type.\n    keys = ['input_ids', 'labels']\n    datatype = torch.int64\n\n    # Broadcast data.\n    timers('data loader').start()\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    timers('data loader').stop()\n    data_b = mpu.broadcast_data(keys, data, datatype)\n    data_i = mpu.broadcast_data(['image'], data, torch.float32)\n    # Unpack.\n    tokens = data_b['input_ids'].long()\n    labels = data_b['labels'].long()\n    img = data_i['image']\n    if args.fp16:\n        img = img.half()\n    \n    return tokens, labels, img, data['pre_image']\n\n\nfrom torch.nn import CrossEntropyLoss\n\ndef forward_step(data_iterator, model, args, timers):\n    \"\"\"Forward step.\"\"\"\n\n    # Get the batch.\n    timers('batch generator').start()\n    tokens, labels, image, pre_image = get_batch(\n        data_iterator, args, timers)\n    timers('batch generator').stop()\n\n    logits = model(input_ids=tokens, image=image, pre_image=pre_image)[0]\n    dtype = logits.dtype\n    lm_logits = logits.to(torch.float32)\n\n    # Shift so that tokens < n predict n\n    shift_logits = lm_logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n    # Flatten the tokens\n    loss_fct = CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n    lm_logits = lm_logits.to(dtype)\n    loss = loss.to(dtype)\n    return loss, {'loss': loss}\n\n\nfrom model.blip2 import BlipImageEvalProcessor\nfrom torch.utils.data import Dataset\nimport json\nfrom PIL import Image\n\nclass FewShotDataset(Dataset):\n    def __init__(self, path, processor, tokenizer, args):\n        max_seq_length = args.max_source_length + args.max_target_length\n        with open(path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        self.images = []\n        self.input_ids = []\n        self.labels = []\n        for item in data:\n            image = processor(Image.open(item['img']).convert('RGB'))\n            input0 = tokenizer.encode(\"<img>\", add_special_tokens=False)\n            input1 = [tokenizer.pad_token_id] * args.image_length\n            input2 = tokenizer.encode(\"</img>问：\"+item['prompt']+\"\\n答：\", add_special_tokens=False)\n            a_ids = sum([input0, input1, input2], [])\n            b_ids = tokenizer.encode(text=item['label'], add_special_tokens=False)\n            if len(a_ids) > args.max_source_length - 1:\n                a_ids = a_ids[: args.max_source_length - 1]\n            if len(b_ids) > args.max_target_length - 2:\n                b_ids = b_ids[: args.max_target_length - 2]\n            pre_image = len(input0)\n            input_ids = tokenizer.build_inputs_with_special_tokens(a_ids, b_ids)\n\n            context_length = input_ids.index(tokenizer.bos_token_id)\n            mask_position = context_length - 1\n            labels = [-100] * context_length + input_ids[mask_position+1:]\n            \n            pad_len = max_seq_length - len(input_ids)\n            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n            labels = labels + [tokenizer.pad_token_id] * pad_len\n            if args.ignore_pad_token_for_loss:\n                labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n            self.images.append(image)\n            self.input_ids.append(input_ids)\n            self.labels.append(labels)\n        self.pre_image = pre_image\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        return {\n            \"image\": self.images[idx],\n            \"input_ids\": self.input_ids[idx],\n            \"labels\": self.labels[idx],\n            \"pre_image\": self.pre_image\n        }\n\n\ndef create_dataset_function(path, args):\n    tokenizer = get_tokenizer(args)\n    image_processor = BlipImageEvalProcessor(224)\n\n    dataset = FewShotDataset(path, image_processor, tokenizer, args)\n    return dataset\n\n\nif __name__ == '__main__':\n    py_parser = argparse.ArgumentParser(add_help=False)\n    py_parser.add_argument('--max_source_length', type=int)\n    py_parser.add_argument('--max_target_length', type=int)\n    py_parser.add_argument('--ignore_pad_token_for_loss', type=bool, default=True)\n    # py_parser.add_argument('--old_checkpoint', action=\"store_true\")\n    py_parser.add_argument('--source_prefix', type=str, default=\"\")\n    py_parser = FineTuneVisualGLMModel.add_model_specific_args(py_parser)\n    known, args_list = py_parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    args.device = 'cpu'\n\n    model_type = 'visualglm-6b'\n    model, args = FineTuneVisualGLMModel.from_pretrained(model_type, args)\n    if torch.cuda.is_available():\n        model = model.to('cuda')\n        args.device = 'cuda'\n    tokenizer = get_tokenizer(args)\n    label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    def data_collator(examples):\n        for example in examples:\n            example['input_ids'] = torch.tensor(example['input_ids'], dtype=torch.long)\n            example['labels'] = torch.tensor(example['labels'], dtype=torch.long)\n        ret = {\n            'input_ids': torch.stack([example['input_ids'] for example in examples]),\n            'labels': torch.stack([example['labels'] for example in examples]),\n            'image': torch.stack([example['image'] for example in examples]),\n            'pre_image': example['pre_image']\n        }\n        return ret\n    training_main(args, model_cls=model, forward_step_function=forward_step, create_dataset_function=create_dataset_function, collate_fn=data_collator)\n"
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.08984375,
          "content": "SwissArmyTransformer>=0.4.4\ntorch>1.10.0\ntorchvision\ntransformers>=4.27.1\nmdtex2html\ngradio\n"
        },
        {
          "name": "requirements_wo_ds.txt",
          "type": "blob",
          "size": 0.115234375,
          "content": "torch>1.10.0\ntorchvision\ntransformers>=4.27.1\nmdtex2html\ngradio\nsentencepiece\ntensorboardX\ndatasets\ncpm_kernels\neinops"
        },
        {
          "name": "web_demo.py",
          "type": "blob",
          "size": 5.6708984375,
          "content": "#!/usr/bin/env python\n\nimport gradio as gr\nfrom PIL import Image\nimport os\nimport json\nfrom model import is_chinese, get_infer_setting, generate_input, chat\nimport torch\n\ndef generate_text_with_image(input_text, image, history=[], request_data=dict(), is_zh=True):\n    input_para = {\n        \"max_length\": 2048,\n        \"min_length\": 50,\n        \"temperature\": 0.8,\n        \"top_p\": 0.4,\n        \"top_k\": 100,\n        \"repetition_penalty\": 1.2\n    }\n    input_para.update(request_data)\n\n    input_data = generate_input(input_text, image, history, input_para, image_is_encoded=False)\n    input_image, gen_kwargs =  input_data['input_image'], input_data['gen_kwargs']\n    with torch.no_grad():\n        answer, history, _ = chat(None, model, tokenizer, input_text, history=history, image=input_image, \\\n                            max_length=gen_kwargs['max_length'], top_p=gen_kwargs['top_p'], \\\n                            top_k = gen_kwargs['top_k'], temperature=gen_kwargs['temperature'], english=not is_zh)\n    return answer\n\n\ndef request_model(input_text, temperature, top_p, image_prompt, result_previous):\n    result_text = [(ele[0], ele[1]) for ele in result_previous]\n    for i in range(len(result_text)-1, -1, -1):\n        if result_text[i][0] == \"\" or result_text[i][1] == \"\":\n            del result_text[i]\n    print(f\"history {result_text}\")\n\n    is_zh = is_chinese(input_text)\n    if image_prompt is None:\n        if is_zh:\n            result_text.append((input_text, '图片为空！请上传图片并重试。'))\n        else:\n            result_text.append((input_text, 'Image empty! Please upload a image and retry.'))\n        return input_text, result_text\n    elif input_text == \"\":\n        result_text.append((input_text, 'Text empty! Please enter text and retry.'))\n        return \"\", result_text                \n\n    request_para = {\"temperature\": temperature, \"top_p\": top_p}\n    image = Image.open(image_prompt)\n    try:\n        answer = generate_text_with_image(input_text, image, result_text.copy(), request_para, is_zh)\n    except Exception as e:\n        print(f\"error: {e}\")\n        if is_zh:\n            result_text.append((input_text, '超时！请稍等几分钟再重试。'))\n        else:\n            result_text.append((input_text, 'Timeout! Please wait a few minutes and retry.'))\n        return \"\", result_text\n\n    result_text.append((input_text, answer))\n    print(result_text)\n    return \"\", result_text\n\n\nDESCRIPTION = '''# <a href=\"https://github.com/THUDM/VisualGLM-6B\">VisualGLM</a>'''\n\nMAINTENANCE_NOTICE1 = 'Hint 1: If the app report \"Something went wrong, connection error out\", please turn off your proxy and retry.\\nHint 2: If you upload a large size of image like 10MB, it may take some time to upload and process. Please be patient and wait.'\nMAINTENANCE_NOTICE2 = '提示1: 如果应用报了“Something went wrong, connection error out”的错误，请关闭代理并重试。\\n提示2: 如果你上传了很大的图片，比如10MB大小，那将需要一些时间来上传和处理，请耐心等待。'\n\nNOTES = 'This app is adapted from <a href=\"https://github.com/THUDM/VisualGLM-6B\">https://github.com/THUDM/VisualGLM-6B</a>. It would be recommended to check out the repo if you want to see the detail of our model and training process.'\n\n\ndef clear_fn(value):\n    return \"\", [(\"\", \"Hi, What do you want to know about this image?\")], None\n\ndef clear_fn2(value):\n    return [(\"\", \"Hi, What do you want to know about this image?\")]\n\n\ndef main(args):\n    gr.close_all()\n    global model, tokenizer\n    model, tokenizer = get_infer_setting(gpu_device=0, quant=args.quant)\n    \n    with gr.Blocks(css='style.css') as demo:\n        gr.Markdown(DESCRIPTION)\n        with gr.Row():\n            with gr.Column(scale=4.5):\n                with gr.Group():\n                    input_text = gr.Textbox(label='Input Text', placeholder='Please enter text prompt below and press ENTER.')\n                    with gr.Row():\n                        run_button = gr.Button('Generate')\n                        clear_button = gr.Button('Clear')\n\n                    image_prompt = gr.Image(type=\"filepath\", label=\"Image Prompt\", value=None)\n                with gr.Row():\n                    temperature = gr.Slider(maximum=1, value=0.8, minimum=0, label='Temperature')\n                    top_p = gr.Slider(maximum=1, value=0.4, minimum=0, label='Top P')\n                with gr.Group():\n                    with gr.Row():\n                        maintenance_notice = gr.Markdown(MAINTENANCE_NOTICE1)\n            with gr.Column(scale=5.5):\n                result_text = gr.components.Chatbot(label='Multi-round conversation History', value=[(\"\", \"Hi, What do you want to know about this image?\")]).style(height=550)\n\n        gr.Markdown(NOTES)\n\n        print(gr.__version__)\n        run_button.click(fn=request_model,inputs=[input_text, temperature, top_p, image_prompt, result_text],\n                         outputs=[input_text, result_text])\n        input_text.submit(fn=request_model,inputs=[input_text, temperature, top_p, image_prompt, result_text],\n                         outputs=[input_text, result_text])\n        clear_button.click(fn=clear_fn, inputs=clear_button, outputs=[input_text, result_text, image_prompt])\n        image_prompt.upload(fn=clear_fn2, inputs=clear_button, outputs=[result_text])\n        image_prompt.clear(fn=clear_fn2, inputs=clear_button, outputs=[result_text])\n\n        print(gr.__version__)\n\n    demo.queue(concurrency_count=10)\n    demo.launch(share=args.share)\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    args = parser.parse_args()\n\n    main(args)"
        },
        {
          "name": "web_demo_hf.py",
          "type": "blob",
          "size": 6.19921875,
          "content": "from transformers import AutoModel, AutoTokenizer\nimport gradio as gr\nimport mdtex2html\nimport torch\n\n\"\"\"Override Chatbot.postprocess\"\"\"\n\ndef postprocess(self, y):\n    if y is None:\n        return []\n    for i, (message, response) in enumerate(y):\n        y[i] = (\n            None if message is None else mdtex2html.convert((message)),\n            None if response is None else mdtex2html.convert(response),\n        )\n    return y\n\ngr.Chatbot.postprocess = postprocess\n\n\ndef parse_text(text):\n    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n    lines = text.split(\"\\n\")\n    lines = [line for line in lines if line != \"\"]\n    count = 0\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            count += 1\n            items = line.split('`')\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = f'<br></code></pre>'\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace(\"`\", \"\\`\")\n                    line = line.replace(\"<\", \"&lt;\")\n                    line = line.replace(\">\", \"&gt;\")\n                    line = line.replace(\" \", \"&nbsp;\")\n                    line = line.replace(\"*\", \"&ast;\")\n                    line = line.replace(\"_\", \"&lowbar;\")\n                    line = line.replace(\"-\", \"&#45;\")\n                    line = line.replace(\".\", \"&#46;\")\n                    line = line.replace(\"!\", \"&#33;\")\n                    line = line.replace(\"(\", \"&#40;\")\n                    line = line.replace(\")\", \"&#41;\")\n                    line = line.replace(\"$\", \"&#36;\")\n                lines[i] = \"<br>\"+line\n    text = \"\".join(lines)\n    return text\n\n\ndef predict(input, image_path, chatbot, max_length, top_p, temperature, history):\n    if image_path is None:\n        return [(input, \"图片不能为空。请重新上传图片并重试。\")], []\n    chatbot.append((parse_text(input), \"\"))\n    with torch.no_grad():\n        for response, history in model.stream_chat(tokenizer, image_path, input, history, max_length=max_length, top_p=top_p,\n                                               temperature=temperature):\n            chatbot[-1] = (parse_text(input), parse_text(response))\n\n            yield chatbot, history\n\n\ndef predict_new_image(image_path, chatbot, max_length, top_p, temperature):\n    input, history = \"描述这张图片。\", []\n    chatbot.append((parse_text(input), \"\"))\n    with torch.no_grad():\n        for response, history in model.stream_chat(tokenizer, image_path, input, history, max_length=max_length,\n                                               top_p=top_p,\n                                               temperature=temperature):\n            chatbot[-1] = (parse_text(input), parse_text(response))\n\n            yield chatbot, history\n\n\ndef reset_user_input():\n    return gr.update(value='')\n\n\ndef reset_state():\n    return None, [], []\n\n\nDESCRIPTION = '''<h1 align=\"center\"><a href=\"https://github.com/THUDM/VisualGLM-6B\">VisualGLM</a></h1>'''\nMAINTENANCE_NOTICE = 'Hint 1: If the app report \"Something went wrong, connection error out\", please turn off your proxy and retry.\\nHint 2: If you upload a large size of image like 10MB, it may take some time to upload and process. Please be patient and wait.'\nNOTES = 'This app is adapted from <a href=\"https://github.com/THUDM/VisualGLM-6B\">https://github.com/THUDM/VisualGLM-6B</a>. It would be recommended to check out the repo if you want to see the detail of our model and training process.'\n\ndef main(args):\n    global model, tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\n    if args.quant in [4, 8]:\n        model = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).quantize(args.quant).half().cuda()\n    else:\n        model = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\n    model = model.eval()\n\n    with gr.Blocks(css='style.css') as demo:\n        gr.HTML(DESCRIPTION)\n        \n        with gr.Row():\n            with gr.Column(scale=2):\n                image_path = gr.Image(type=\"filepath\", label=\"Image Prompt\", value=None).style(height=504)\n            with gr.Column(scale=4):\n                chatbot = gr.Chatbot().style(height=480)\n        with gr.Row():\n            with gr.Column(scale=2, min_width=100):\n                max_length = gr.Slider(0, 4096, value=2048, step=1.0, label=\"Maximum length\", interactive=True)\n                top_p = gr.Slider(0, 1, value=0.4, step=0.01, label=\"Top P\", interactive=True)\n                temperature = gr.Slider(0, 1, value=0.8, step=0.01, label=\"Temperature\", interactive=True)\n            with gr.Column(scale=4):\n                with gr.Box():\n                    with gr.Row():\n                        with gr.Column(scale=2):\n                            user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=4).style(\n                                container=False)\n                        with gr.Column(scale=1, min_width=64):\n                            submitBtn = gr.Button(\"Submit\", variant=\"primary\")\n                            emptyBtn = gr.Button(\"Clear History\")\n                    gr.Markdown(MAINTENANCE_NOTICE + '\\n' + NOTES)\n        history = gr.State([])\n        \n\n        submitBtn.click(predict, [user_input, image_path, chatbot, max_length, top_p, temperature, history], [chatbot, history],\n                        show_progress=True)\n        image_path.upload(predict_new_image, [image_path, chatbot, max_length, top_p, temperature], [chatbot, history],\n                        show_progress=True)\n        image_path.clear(reset_state, outputs=[image_path, chatbot, history], show_progress=True)\n        submitBtn.click(reset_user_input, [], [user_input])\n        emptyBtn.click(reset_state, outputs=[image_path, chatbot, history], show_progress=True)\n\n        print(gr.__version__)\n\n        demo.queue().launch(share=args.share, inbrowser=True, server_name='0.0.0.0', server_port=8080)\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    args = parser.parse_args()\n    main(args)\n"
        }
      ]
    }
  ]
}