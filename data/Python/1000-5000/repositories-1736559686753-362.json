{
  "metadata": {
    "timestamp": 1736559686753,
    "page": 362,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/VisualGLM-6B",
      "stars": 4121,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.04296875,
          "content": "checkpoints/\nruns/\nmodel/__pycache__/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 11.0703125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright Zhengxiao Du\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "MODEL_LICENSE.txt",
          "type": "blob",
          "size": 2.2763671875,
          "content": "The VisualGLM-6B License\n\n1. Definitions\n\nâ€œLicensorâ€ means the VisualGLM-6B Model Team that distributes its Software.\n\nâ€œSoftwareâ€ means the VisualGLM-6B model parameters made available under this license.\n\n2. License Grant\n\nSubject to the terms and conditions of this License, the Licensor hereby grants to you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to use the Software solely for your non-commercial research purposes.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any commercial, military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of Peopleâ€™s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.5625,
          "content": "# VisualGLM-6B\n\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/THUDM/visualglm-6b\" target=\"_blank\">HF Repo</a> â€¢ âš’ï¸ <a href=\"https://github.com/THUDM/SwissArmyTransformer\" target=\"_blank\">SwissArmyTransformer (sat)</a> â€¢ ğŸ¦ <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> \n</p>\n<p align=\"center\">\nâ€¢  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2105.13290\" target=\"_blank\">[CogView@NeurIPS 21]</a>  <a href=\"https://github.com/THUDM/CogView\" target=\"_blank\">[GitHub]</a> â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n<p align=\"center\">\n    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href=\"https://join.slack.com/t/chatglm/shared_invite/zt-1y7pqoloy-9b1g6T6JjA8J0KxvUjbwJw\" target=\"_blank\">Slack</a> å’Œ <a href=\"examples/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<!-- <p align=\"center\">\nğŸ¤–<a href=\"https://huggingface.co/spaces/THUDM/visualglm-6b\" target=\"_blank\">VisualGLM-6Båœ¨çº¿æ¼”ç¤ºç½‘ç«™</a>\n</p> -->\n\n## News\n[2023.10] æ¬¢è¿å…³æ³¨æ™ºè°±AIæ–°ä¸€ä»£å¤šæ¨¡æ€å¯¹è¯æ¨¡å‹CogVLMï¼ˆ https://github.com/THUDM/CogVLM ï¼‰ï¼Œé‡‡ç”¨è§†è§‰ä¸“å®¶æ–°æ¶æ„ï¼Œåœ¨10é¡¹æƒå¨ç»å…¸å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—ç¬¬ä¸€åã€‚ç›®å‰å¼€æºCogVLM-17Bè‹±æ–‡æ¨¡å‹ï¼Œå³å°†åŸºäºGLMå¼€æºä¸­æ–‡æ¨¡å‹ã€‚\n\n## ä»‹ç»\n\nVisualGLM-6B is an open-source, multi-modal dialog language model that supports **images, Chinese, and English**. The language model is based on [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) with 6.2 billion parameters; the image part builds a bridge between the visual model and the language model through the training of [BLIP2-Qformer](https://arxiv.org/abs/2301.12597), with the total model comprising 7.8 billion parameters. **[Click here for English version.](README_en.md)**\n\nVisualGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ï¼Œæ”¯æŒ**å›¾åƒã€ä¸­æ–‡å’Œè‹±æ–‡**çš„å¤šæ¨¡æ€å¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œè¯­è¨€æ¨¡å‹åŸºäº [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ï¼›å›¾åƒéƒ¨åˆ†é€šè¿‡è®­ç»ƒ [BLIP2-Qformer](https://arxiv.org/abs/2301.12597) æ„å»ºèµ·è§†è§‰æ¨¡å‹ä¸è¯­è¨€æ¨¡å‹çš„æ¡¥æ¢ï¼Œæ•´ä½“æ¨¡å‹å…±78äº¿å‚æ•°ã€‚\n\nVisualGLM-6B ä¾é æ¥è‡ªäº [CogView](https://arxiv.org/abs/2105.13290) æ•°æ®é›†çš„30Mé«˜è´¨é‡ä¸­æ–‡å›¾æ–‡å¯¹ï¼Œä¸300Mç»è¿‡ç­›é€‰çš„è‹±æ–‡å›¾æ–‡å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸­è‹±æ–‡æƒé‡ç›¸åŒã€‚è¯¥è®­ç»ƒæ–¹å¼è¾ƒå¥½åœ°å°†è§†è§‰ä¿¡æ¯å¯¹é½åˆ°ChatGLMçš„è¯­ä¹‰ç©ºé—´ï¼›ä¹‹åçš„å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨é•¿è§†è§‰é—®ç­”æ•°æ®ä¸Šè®­ç»ƒï¼Œä»¥ç”Ÿæˆç¬¦åˆäººç±»åå¥½çš„ç­”æ¡ˆã€‚\n\nVisualGLM-6B ç”± [SwissArmyTransformer](https://github.com/THUDM/SwissArmyTransformer)(ç®€ç§°`sat`) åº“è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒTransformerçµæ´»ä¿®æ”¹ã€è®­ç»ƒçš„å·¥å…·åº“ï¼Œæ”¯æŒLoraã€P-tuningç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚æœ¬é¡¹ç›®æä¾›äº†ç¬¦åˆç”¨æˆ·ä¹ æƒ¯çš„huggingfaceæ¥å£ï¼Œä¹Ÿæä¾›äº†åŸºäºsatçš„æ¥å£ã€‚\n\nç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€6.3Gæ˜¾å­˜ï¼‰ã€‚\n\n-----\n\nVisualGLM-6B å¼€æºæ¨¡å‹æ—¨åœ¨ä¸å¼€æºç¤¾åŒºä¸€èµ·æ¨åŠ¨å¤§æ¨¡å‹æŠ€æœ¯å‘å±•ï¼Œæ³è¯·å¼€å‘è€…å’Œå¤§å®¶éµå®ˆå¼€æºåè®®ï¼Œå‹¿å°†è¯¥å¼€æºæ¨¡å‹å’Œä»£ç åŠåŸºäºè¯¥å¼€æºé¡¹ç›®äº§ç”Ÿçš„è¡ç”Ÿç‰©ç”¨äºä»»ä½•å¯èƒ½ç»™å›½å®¶å’Œç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ä»¥åŠç”¨äºä»»ä½•æœªç»è¿‡å®‰å…¨è¯„ä¼°å’Œå¤‡æ¡ˆçš„æœåŠ¡ã€‚ç›®å‰ï¼Œæœ¬é¡¹ç›®å®˜æ–¹æœªåŸºäº VisualGLM-6B å¼€å‘ä»»ä½•åº”ç”¨ï¼ŒåŒ…æ‹¬ç½‘ç«™ã€å®‰å“Appã€è‹¹æœ iOSåº”ç”¨åŠ Windows App ç­‰ã€‚\n\nç”±äº VisualGLM-6B ä»å¤„äºv1ç‰ˆæœ¬ï¼Œç›®å‰å·²çŸ¥å…¶å…·æœ‰ç›¸å½“å¤šçš„[**å±€é™æ€§**](README.md#å±€é™æ€§)ï¼Œå¦‚å›¾åƒæè¿°äº‹å®æ€§/æ¨¡å‹å¹»è§‰é—®é¢˜ï¼Œå›¾åƒç»†èŠ‚ä¿¡æ¯æ•æ‰ä¸è¶³ï¼Œä»¥åŠä¸€äº›æ¥è‡ªè¯­è¨€æ¨¡å‹çš„å±€é™æ€§ã€‚å°½ç®¡æ¨¡å‹åœ¨è®­ç»ƒçš„å„ä¸ªé˜¶æ®µéƒ½å°½åŠ›ç¡®ä¿æ•°æ®çš„åˆè§„æ€§å’Œå‡†ç¡®æ€§ï¼Œä½†ç”±äº VisualGLM-6B æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä¸”æ¨¡å‹å—æ¦‚ç‡éšæœºæ€§å› ç´ å½±å“ï¼Œæ— æ³•ä¿è¯è¾“å‡ºå†…å®¹çš„å‡†ç¡®æ€§ï¼Œä¸”æ¨¡å‹æ˜“è¢«è¯¯å¯¼ï¼ˆè¯¦è§å±€é™æ€§éƒ¨åˆ†ï¼‰ã€‚åœ¨VisualGLMä¹‹åçš„ç‰ˆæœ¬ä¸­ï¼Œå°†ä¼šç€åŠ›å¯¹æ­¤ç±»é—®é¢˜è¿›è¡Œä¼˜åŒ–ã€‚æœ¬é¡¹ç›®ä¸æ‰¿æ‹…å¼€æºæ¨¡å‹å’Œä»£ç å¯¼è‡´çš„æ•°æ®å®‰å…¨ã€èˆ†æƒ…é£é™©æˆ–å‘ç”Ÿä»»ä½•æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­ã€ä¸å½“åˆ©ç”¨è€Œäº§ç”Ÿçš„é£é™©å’Œè´£ä»»ã€‚\n\n## æ ·ä¾‹\nVisualGLM-6B å¯ä»¥è¿›è¡Œå›¾åƒçš„æè¿°çš„ç›¸å…³çŸ¥è¯†çš„é—®ç­”ã€‚\n![æ³°å¦å°¼å…‹å·æ ·ä¾‹](examples/chat_example1.png)\n\n<details>\n<summary>ä¹Ÿèƒ½ç»“åˆå¸¸è¯†æˆ–æå‡ºæœ‰è¶£çš„è§‚ç‚¹ï¼Œç‚¹å‡»å±•å¼€/æŠ˜å æ›´å¤šæ ·ä¾‹</summary>\n\n![å‡ºç§Ÿè½¦ç†¨è¡£æœæ ·ä¾‹](examples/chat_example2.png)\n![è’™å¨œä¸½èç‹—æ ·ä¾‹](examples/chat_example3.png)\n\n</details>\n\n## å‹æƒ…é“¾æ¥\n\n* [XrayGLM](https://github.com/WangRongsheng/XrayGLM) æ˜¯åŸºäºvisualGLM-6Båœ¨Xå…‰è¯Šæ–­æ•°æ®é›†ä¸Šå¾®è°ƒçš„Xå…‰è¯Šæ–­é—®ç­”çš„é¡¹ç›®ï¼Œèƒ½æ ¹æ®Xå…‰ç‰‡å›ç­”åŒ»å­¦ç›¸å…³è¯¢é—®ã€‚\n<details>\n<summary>ç‚¹å‡»æŸ¥çœ‹æ ·ä¾‹</summary>\n\n![æ ·ä¾‹](https://github.com/WangRongsheng/XrayGLM/raw/main/assets/images/xrayglm.png)\n</details>\n\n* [StarGLM](https://github.com/WangRongsheng/XrayGLM) æ˜¯åŸºäºChat/visualGLM-6Båœ¨å¤©æ–‡æ•°æ®é›†ä¸Šå¾®è°ƒçš„é¡¹ç›®ï¼Œèƒ½å›ç­”å˜æ˜Ÿå…‰å˜æ›²çº¿ç›¸å…³çš„ä¿¡æ¯ã€‚\n<details>\n<summary>ç‚¹å‡»æŸ¥çœ‹æ ·ä¾‹</summary>\n\n![æ ·ä¾‹](https://github.com/Yu-Yang-Li/StarGLM/raw/main/example/example_4.png)\n\n</details>\n\n\n## ä½¿ç”¨\n\n### æ¨¡å‹æ¨ç†\n\nä½¿ç”¨pipå®‰è£…ä¾èµ–\n```\npip install -i https://pypi.org/simple -r requirements.txt\n# å›½å†…è¯·ä½¿ç”¨aliyuné•œåƒï¼ŒTUNAç­‰é•œåƒåŒæ­¥æœ€è¿‘å‡ºç°é—®é¢˜ï¼Œå‘½ä»¤å¦‚ä¸‹\npip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements.txt\n```\næ­¤æ—¶é»˜è®¤ä¼šå®‰è£…`deepspeed`åº“ï¼ˆæ”¯æŒ`sat`åº“è®­ç»ƒï¼‰ï¼Œæ­¤åº“å¯¹äºæ¨¡å‹æ¨ç†å¹¶éå¿…è¦ï¼ŒåŒæ—¶éƒ¨åˆ†Windowsç¯å¢ƒå®‰è£…æ­¤åº“æ—¶ä¼šé‡åˆ°é—®é¢˜ã€‚\nå¦‚æœæƒ³ç»•è¿‡`deepspeed`å®‰è£…ï¼Œæˆ‘ä»¬å¯ä»¥å°†å‘½ä»¤æ”¹ä¸º\n```\npip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements_wo_ds.txt\npip install -i https://mirrors.aliyun.com/pypi/simple/ --no-deps \"SwissArmyTransformer>=0.4.4\"\n```\n\nå¦‚æœä½¿ç”¨Huggingface transformersåº“è°ƒç”¨æ¨¡å‹ï¼ˆ**ä¹Ÿéœ€è¦å®‰è£…ä¸Šè¿°ä¾èµ–åŒ…ï¼**ï¼‰ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç ï¼ˆå…¶ä¸­å›¾åƒè·¯å¾„ä¸ºæœ¬åœ°è·¯å¾„ï¼‰ï¼š\n```python\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\nimage_path = \"your image path\"\nresponse, history = model.chat(tokenizer, image_path, \"æè¿°è¿™å¼ å›¾ç‰‡ã€‚\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, image_path, \"è¿™å¼ å›¾ç‰‡å¯èƒ½æ˜¯åœ¨ä»€ä¹ˆåœºæ‰€æ‹æ‘„çš„ï¼Ÿ\", history=history)\nprint(response)\n```\nä»¥ä¸Šä»£ç ä¼šç”± `transformers` è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å®ç°å’Œå‚æ•°ã€‚å®Œæ•´çš„æ¨¡å‹å®ç°å¯ä»¥åœ¨ [Hugging Face Hub](https://huggingface.co/THUDM/visualglm-6b)ã€‚å¦‚æœä½ ä» Hugging Face Hub ä¸Šä¸‹è½½æ¨¡å‹å‚æ•°çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯ä»¥ä»[è¿™é‡Œ](https://cloud.tsinghua.edu.cn/d/43ffb021ca5f4897b56a/)æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å‚æ•°æ–‡ä»¶ï¼Œå¹¶ä»æœ¬åœ°åŠ è½½æ¨¡å‹ã€‚å…·ä½“åšæ³•è¯·å‚è€ƒ[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](https://github.com/THUDM/ChatGLM-6B#%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B)ã€‚å…³äºåŸºäº transformers åº“æ¨¡å‹çš„é‡åŒ–ã€CPUæ¨ç†ã€Mac MPS åç«¯åŠ é€Ÿç­‰å†…å®¹ï¼Œè¯·å‚è€ƒ [ChatGLM-6B çš„ä½æˆæœ¬éƒ¨ç½²](https://github.com/THUDM/ChatGLM-6B#%E4%BD%8E%E6%88%90%E6%9C%AC%E9%83%A8%E7%BD%B2)ã€‚\n\nå¦‚æœä½¿ç”¨SwissArmyTransformeråº“è°ƒç”¨æ¨¡å‹ï¼Œæ–¹æ³•ç±»ä¼¼ï¼Œå¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡`SAT_HOME`å†³å®šæ¨¡å‹ä¸‹è½½ä½ç½®ã€‚åœ¨æœ¬ä»“åº“ç›®å½•ä¸‹ï¼š\n```python\nimport argparse\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nfrom model import chat, VisualGLMModel\nmodel, model_args = VisualGLMModel.from_pretrained('visualglm-6b', args=argparse.Namespace(fp16=True, skip_init=True))\nfrom sat.model.mixins import CachedAutoregressiveMixin\nmodel.add_mixin('auto-regressive', CachedAutoregressiveMixin())\nimage_path = \"your image path or URL\"\nresponse, history, cache_image = chat(image_path, model, tokenizer, \"æè¿°è¿™å¼ å›¾ç‰‡ã€‚\", history=[])\nprint(response)\nresponse, history, cache_image = chat(None, model, tokenizer, \"è¿™å¼ å›¾ç‰‡å¯èƒ½æ˜¯åœ¨ä»€ä¹ˆåœºæ‰€æ‹æ‘„çš„ï¼Ÿ\", history=history, image=cache_image)\nprint(response)\n```\nä½¿ç”¨`sat`åº“ä¹Ÿå¯ä»¥è½»æ¾è¿›è¡Œè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚<!-- TODO å…·ä½“ä»£ç  -->\n\n## æ¨¡å‹å¾®è°ƒ\n\nå¤šæ¨¡æ€ä»»åŠ¡åˆ†å¸ƒå¹¿ã€ç§ç±»å¤šï¼Œé¢„è®­ç»ƒå¾€å¾€ä¸èƒ½é¢é¢ä¿±åˆ°ã€‚\nè¿™é‡Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå°æ ·æœ¬å¾®è°ƒçš„ä¾‹å­ï¼Œä½¿ç”¨20å¼ æ ‡æ³¨å›¾å¢å¼ºæ¨¡å‹å›ç­”â€œèƒŒæ™¯â€é—®é¢˜çš„èƒ½åŠ›ã€‚\n\nè§£å‹`fewshot-data.zip`ä»¥åè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š\n```\nbash finetune/finetune_visualglm.sh\n```\n\nç›®å‰æ”¯æŒä¸‰ç§æ–¹å¼çš„å¾®è°ƒï¼š\n\n* LoRAï¼šæ ·ä¾‹ä¸­ä¸ºChatGLMæ¨¡å‹çš„ç¬¬0å±‚å’Œç¬¬14å±‚åŠ å…¥äº†rank=10çš„LoRAå¾®è°ƒï¼Œå¯ä»¥æ ¹æ®å…·ä½“æƒ…æ™¯å’Œæ•°æ®é‡è°ƒæ•´`--layer_range`å’Œ`--lora_rank`å‚æ•°ã€‚\n* QLoRAï¼šå¦‚æœèµ„æºæœ‰é™ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨`bash finetune/finetune_visualglm_qlora.sh`ï¼ŒQLoRAå°†ChatGLMçš„çº¿æ€§å±‚è¿›è¡Œäº†4-bité‡åŒ–ï¼Œåªéœ€è¦9.8GBæ˜¾å­˜å³å¯å¾®è°ƒã€‚\n* P-tuningï¼šå¯ä»¥å°†`--use_lora`æ›¿æ¢ä¸º`--use_ptuning`ï¼Œä¸è¿‡ä¸æ¨èä½¿ç”¨ï¼Œé™¤éæ¨¡å‹åº”ç”¨åœºæ™¯éå¸¸å›ºå®šã€‚\n\nè®­ç»ƒå¥½ä»¥åå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æ¨ç†ï¼š\n\n```\npython cli_demo.py --from_pretrained your_checkpoint_path --prompt_zh è¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ\n```\n\n<details>\n<summary>å¾®è°ƒå‰åçš„æ•ˆæœå¯¹æ¯”</summary>\n\nå¾®è°ƒå‰ï¼š\n\n```\næ¬¢è¿ä½¿ç”¨ VisualGLM-6B æ¨¡å‹ï¼Œè¾“å…¥å›¾åƒURLæˆ–æœ¬åœ°è·¯å¾„è¯»å›¾ï¼Œç»§ç»­è¾“å…¥å†…å®¹å¯¹è¯ï¼Œclear é‡æ–°å¼€å§‹ï¼Œstop ç»ˆæ­¢ç¨‹åº\nè¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š fewshot-data/2p.png\nç”¨æˆ·ï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ\nVisualGLM-6Bï¼šäººä»¬æ€»æ˜¯è¢«ç”Ÿæ´»çäº‹æ‰€å›°æ‰°ï¼Œæ— æ³•æ‘†è„±æŸç¼š.\nç”¨æˆ·ï¼šclear\nè¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š fewshot-data/meme.png\nç”¨æˆ·ï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯æ˜¯ä¸€å¼ ç‹—çš„å›¾ç‰‡ï¼Œå®ƒèººåœ¨æœ¨åœ°æ¿ä¸Šã€‚\nç”¨æˆ·ï¼šclear\nè¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š fewshot-data/passport.png\nç”¨æˆ·ï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯æ˜¯ä¸€å°ç”µè„‘é”®ç›˜å’Œä¸¤å¼ æŠ¤ç…§ã€‚\nç”¨æˆ·ï¼šclear\nè¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š\n```\n\nå¾®è°ƒåï¼š\n\n```\næ¬¢è¿ä½¿ç”¨ VisualGLM-6B æ¨¡å‹ï¼Œè¾“å…¥å›¾åƒURLæˆ–æœ¬åœ°è·¯å¾„è¯»å›¾ï¼Œç»§ç»­è¾“å…¥å†…å®¹å¯¹è¯ï¼Œclear é‡æ–°å¼€å§‹ï¼Œstop ç»ˆæ­¢ç¨‹åº\nè¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š fewshot-data/2p.png\nç”¨æˆ·ï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯æ˜¯é›¨å¤©ï¼Œ\nç”¨æˆ·ï¼šæè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡ã€‚\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡æç»˜äº†é›¨å¤©çš„æƒ…æ™¯ï¼Œé›¨æ°´æ·…æ·…æ²¥æ²¥åœ°è½åœ¨åœ°é¢ä¸Šã€‚äººä»¬ç©¿ç€é›¨è¡£æˆ–é›¨ä¼ï¼Œèµ°åœ¨é›¨ä¸­ï¼Œä¼¼ä¹åœ¨å¯»æ‰¾ç€ä»€ä¹ˆ.\nç”¨æˆ·ï¼šclear\nè¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š fewshot-data/meme.png\nç”¨æˆ·ï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯æ˜¯æœ¨è´¨åœ°æ¿ã€‚\nç”¨æˆ·ï¼šä»€ä¹ˆé¢œè‰²çš„åœ°æ¿\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯æ˜¯è“è‰²çš„ã€‚\nç”¨æˆ·ï¼šæè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡ã€‚\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡æç»˜äº†ä¸€åªæ£•è‰²çš„å°ç‹—èººåœ¨è“è‰²çš„æœ¨è´¨åœ°æ¿ä¸Šã€‚\nç”¨æˆ·ï¼šclear\nè¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š fewshot-data/passport.png\nç”¨æˆ·ï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯æ˜¯ç”µè„‘é”®ç›˜ã€‚\nç”¨æˆ·ï¼šæè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡ã€‚\nVisualGLM-6Bï¼šè¿™å¼ å›¾ç‰‡çš„ç‰¹ç‚¹æ˜¯ç”µè„‘é”®ç›˜å’ŒæŠ¤ç…§ã€‚\nç”¨æˆ·ï¼šæœ‰å‡ å¼ æŠ¤ç…§    \nVisualGLM-6Bï¼šä¸¤å¼ æŠ¤ç…§ã€‚\nç”¨æˆ·ï¼šclear\nè¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š\n```\n\n</details>\n\nå¦‚æœå¸Œæœ›æŠŠLoRAéƒ¨åˆ†çš„å‚æ•°åˆå¹¶åˆ°åŸå§‹çš„æƒé‡ï¼Œå¯ä»¥è°ƒç”¨`merge_lora()`ï¼Œä¾‹å¦‚ï¼š\n\n```python\nfrom finetune_visualglm import FineTuneVisualGLMModel\nimport argparse\n\nmodel, args = FineTuneVisualGLMModel.from_pretrained('checkpoints/finetune-visualglm-6b-05-19-07-36',\n        args=argparse.Namespace(\n        fp16=True,\n        skip_init=True,\n        use_gpu_initialization=True,\n        device='cuda',\n    ))\nmodel.get_mixin('lora').merge_lora()\nargs.layer_range = []\nargs.save = 'merge_lora'\nargs.mode = 'inference'\nfrom sat.training.model_io import save_checkpoint\nsave_checkpoint(1, model, None, None, args)\n```\n\nå¾®è°ƒéœ€è¦å®‰è£…`deepspeed`åº“ï¼Œç›®å‰æœ¬æµç¨‹ä»…æ”¯æŒlinuxç³»ç»Ÿï¼Œæ›´å¤šçš„æ ·ä¾‹è¯´æ˜å’ŒWindowsç³»ç»Ÿçš„æµç¨‹è¯´æ˜å°†åœ¨è¿‘æœŸå®Œæˆã€‚\n\n## éƒ¨ç½²å·¥å…·\n\n### å‘½ä»¤è¡Œ Demo\n\n```shell\npython cli_demo.py \n```\nç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½satæ¨¡å‹ï¼Œå¹¶åœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œè¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ clear å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥ stop ç»ˆæ­¢ç¨‹åºã€‚\n\n![cli_demo](examples/thu.png)\nç¨‹åºæä¾›å¦‚ä¸‹è¶…å‚æ•°æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä¸é‡åŒ–ç²¾åº¦ï¼š\n```\nusage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english] [--quant {8,4}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --max_length MAX_LENGTH\n                        max length of the total sequence\n  --top_p TOP_P         top p for nucleus sampling\n  --top_k TOP_K         top k for top k sampling\n  --temperature TEMPERATURE\n                        temperature for sampling\n  --english             only output English\n  --quant {8,4}         quantization bits\n```\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®­ç»ƒæ—¶è‹±æ–‡é—®ç­”å¯¹çš„æç¤ºè¯ä¸º`Q: A:`ï¼Œè€Œä¸­æ–‡ä¸º`é—®ï¼šç­”ï¼š`ï¼Œåœ¨ç½‘é¡µdemoä¸­é‡‡å–äº†ä¸­æ–‡çš„æç¤ºï¼Œå› æ­¤è‹±æ–‡å›å¤ä¼šå·®ä¸€äº›ä¸”å¤¹æ‚ä¸­æ–‡ï¼›å¦‚æœéœ€è¦è‹±æ–‡å›å¤ï¼Œè¯·ä½¿ç”¨`cli_demo.py`ä¸­çš„`--english`é€‰é¡¹ã€‚\n\næˆ‘ä»¬ä¹Ÿæä¾›äº†ç»§æ‰¿è‡ª`ChatGLM-6B`çš„æ‰“å­—æœºæ•ˆæœå‘½ä»¤è¡Œå·¥å…·ï¼Œæ­¤å·¥å…·ä½¿ç”¨Huggingfaceæ¨¡å‹ï¼š\n```shell\npython cli_demo_hf.py\n```\n\næˆ‘ä»¬ä¹Ÿæ”¯æŒæ¨¡å‹å¹¶è¡Œå¤šå¡éƒ¨ç½²ï¼šï¼ˆéœ€è¦æ›´æ–°æœ€æ–°ç‰ˆæœ¬çš„satï¼Œå¦‚æœä¹‹å‰ä¸‹è½½äº†checkpointï¼Œä¹Ÿéœ€è¦æ‰‹åŠ¨åˆ é™¤åé‡æ–°ä¸‹è½½ï¼‰\n```\ntorchrun --nnode 1 --nproc-per-node 2 cli_demo_mp.py\n```\n\n### ç½‘é¡µç‰ˆ Demo\n![web_demo](examples/web_demo.png)\n\næˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäº [Gradio](https://gradio.app) çš„ç½‘é¡µç‰ˆ Demoï¼Œé¦–å…ˆå®‰è£… Gradioï¼š`pip install gradio`ã€‚\nç„¶åä¸‹è½½å¹¶è¿›å…¥æœ¬ä»“åº“è¿è¡Œ`web_demo.py`ï¼š\n\n```\ngit clone https://github.com/THUDM/VisualGLM-6B\ncd VisualGLM-6B\npython web_demo.py\n```\nç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½ sat æ¨¡å‹ï¼Œå¹¶è¿è¡Œä¸€ä¸ª Web Serverï¼Œå¹¶è¾“å‡ºåœ°å€ã€‚åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡ºçš„åœ°å€å³å¯ä½¿ç”¨ã€‚\n\n\næˆ‘ä»¬ä¹Ÿæä¾›äº†ç»§æ‰¿è‡ª`ChatGLM-6B`çš„æ‰“å­—æœºæ•ˆæœç½‘é¡µç‰ˆå·¥å…·ï¼Œæ­¤å·¥å…·ä½¿ç”¨ Huggingface æ¨¡å‹ï¼Œå¯åŠ¨åå°†è¿è¡Œåœ¨`:8080`ç«¯å£ä¸Šï¼š\n```shell\npython web_demo_hf.py\n```\n\nä¸¤ç§ç½‘é¡µç‰ˆ demo å‡æ¥å—å‘½ä»¤è¡Œå‚æ•°`--share`ä»¥ç”Ÿæˆ gradio å…¬å¼€é“¾æ¥ï¼Œæ¥å—`--quant 4`å’Œ`--quant 8`ä»¥åˆ†åˆ«ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–/8æ¯”ç‰¹é‡åŒ–å‡å°‘æ˜¾å­˜å ç”¨ã€‚\n\n### APIéƒ¨ç½²\né¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ– `pip install fastapi uvicorn`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ [api.py](api.py)ï¼š\n```shell\npython api.py\n```\nç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½ sat æ¨¡å‹ï¼Œé»˜è®¤éƒ¨ç½²åœ¨æœ¬åœ°çš„ 8080 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ã€‚ä¸‹é¢æ˜¯ç”¨`curl`è¯·æ±‚çš„ä¾‹å­ï¼Œä¸€èˆ¬è€Œè¨€å¯ä»¥ä¹Ÿå¯ä»¥ä½¿ç”¨ä»£ç æ–¹æ³•è¿›è¡ŒPOSTã€‚\n```shell\necho \"{\\\"image\\\":\\\"$(base64 path/to/example.jpg)\\\",\\\"text\\\":\\\"æè¿°è¿™å¼ å›¾ç‰‡\\\",\\\"history\\\":[]}\" > temp.json\ncurl -X POST -H \"Content-Type: application/json\" -d @temp.json http://127.0.0.1:8080\n```\nå¾—åˆ°çš„è¿”å›å€¼ä¸º\n```\n  {\n    \"response\":\"è¿™å¼ å›¾ç‰‡å±•ç°äº†ä¸€åªå¯çˆ±çš„å¡é€šç¾Šé©¼ï¼Œå®ƒç«™åœ¨ä¸€ä¸ªé€æ˜çš„èƒŒæ™¯ä¸Šã€‚è¿™åªç¾Šé©¼é•¿ç€ä¸€å¼ æ¯›èŒ¸èŒ¸çš„è€³æœµå’Œä¸€åŒå¤§å¤§çš„çœ¼ç›ï¼Œå®ƒçš„èº«ä½“æ˜¯ç™½è‰²çš„ï¼Œå¸¦æœ‰æ£•è‰²æ–‘ç‚¹ã€‚\",\n    \"history\":[('æè¿°è¿™å¼ å›¾ç‰‡', 'è¿™å¼ å›¾ç‰‡å±•ç°äº†ä¸€åªå¯çˆ±çš„å¡é€šç¾Šé©¼ï¼Œå®ƒç«™åœ¨ä¸€ä¸ªé€æ˜çš„èƒŒæ™¯ä¸Šã€‚è¿™åªç¾Šé©¼é•¿ç€ä¸€å¼ æ¯›èŒ¸èŒ¸çš„è€³æœµå’Œä¸€åŒå¤§å¤§çš„çœ¼ç›ï¼Œå®ƒçš„èº«ä½“æ˜¯ç™½è‰²çš„ï¼Œå¸¦æœ‰æ£•è‰²æ–‘ç‚¹ã€‚')],\n    \"status\":200,\n    \"time\":\"2023-05-16 20:20:10\"\n  }\n```\n\næˆ‘ä»¬ä¹Ÿæä¾›äº†ä½¿ç”¨Huggingfaceæ¨¡å‹çš„ [api_hf.py](api_hf.py)ï¼Œç”¨æ³•å’Œsatæ¨¡å‹çš„apiä¸€è‡´ï¼š\n```shell\npython api_hf.py\n```\n\n\n## æ¨¡å‹é‡åŒ–\nåœ¨Huggingfaceå®ç°ä¸­ï¼Œæ¨¡å‹é»˜è®¤ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 15GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ã€‚\nä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n```python\n# æŒ‰éœ€ä¿®æ”¹ï¼Œç›®å‰åªæ”¯æŒ 4/8 bit é‡åŒ–ã€‚ä¸‹é¢å°†åªé‡åŒ–ChatGLMï¼ŒViT é‡åŒ–æ—¶è¯¯å·®è¾ƒå¤§\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).quantize(8).half().cuda()\n```\n\nåœ¨satå®ç°ä¸­ï¼Œéœ€å…ˆä¼ å‚å°†åŠ è½½ä½ç½®æ”¹ä¸º`cpu`ï¼Œå†è¿›è¡Œé‡åŒ–ã€‚æ–¹æ³•å¦‚ä¸‹ï¼Œè¯¦è§`cli_demo.py`ï¼š\n```python\nfrom sat.quantization.kernels import quantize\nquantize(model, args.quant).cuda()\n# åªéœ€è¦ 7GB æ˜¾å­˜å³å¯æ¨ç†\n```\n\n## å±€é™æ€§\næœ¬é¡¹ç›®æ­£å¤„äºV1ç‰ˆæœ¬è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„å‚æ•°ã€è®¡ç®—é‡éƒ½è¾ƒå°ï¼Œæˆ‘ä»¬æ€»ç»“äº†å¦‚ä¸‹ä¸»è¦å­˜åœ¨çš„æ”¹è¿›æ–¹å‘ï¼š\n- å›¾åƒæè¿°äº‹å®æ€§/æ¨¡å‹å¹»è§‰é—®é¢˜ã€‚åœ¨ç”Ÿæˆå›¾åƒé•¿æè¿°çš„æ—¶å€™ï¼Œè·ç¦»å›¾åƒè¾ƒè¿œæ—¶ï¼Œè¯­è¨€æ¨¡å‹çš„å°†å ä¸»å¯¼ï¼Œæœ‰ä¸€å®šå¯èƒ½æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆå¹¶ä¸å­˜åœ¨äºå›¾åƒçš„å†…å®¹ã€‚\n- å±æ€§é”™é…é—®é¢˜ã€‚åœ¨å¤šç‰©ä½“çš„åœºæ™¯ä¸­ï¼Œéƒ¨åˆ†ç‰©ä½“çš„æŸäº›å±æ€§ï¼Œç»å¸¸è¢«é”™è¯¯å®‰æ’åˆ°å…¶ä»–ç‰©ä½“ä¸Šã€‚\n- åˆ†è¾¨ç‡é—®é¢˜ã€‚æœ¬é¡¹ç›®ä½¿ç”¨äº†224*224çš„åˆ†è¾¨ç‡ï¼Œä¹Ÿæ˜¯è§†è§‰æ¨¡å‹ä¸­æœ€ä¸ºå¸¸ç”¨çš„å°ºå¯¸ï¼›ç„¶è€Œä¸ºäº†è¿›è¡Œæ›´ç»†ç²’åº¦çš„ç†è§£ï¼Œæ›´å¤§çš„åˆ†è¾¨ç‡å’Œè®¡ç®—é‡æ˜¯å¿…è¦çš„ã€‚\n- ç”±äºæ•°æ®ç­‰æ–¹é¢åŸå› ï¼Œæ¨¡å‹æš‚æ—¶ä¸å…·æœ‰ä¸­æ–‡ocrçš„èƒ½åŠ›ï¼ˆè‹±æ–‡ocrèƒ½åŠ›æœ‰ä¸€äº›ï¼‰ï¼Œæˆ‘ä»¬ä¼šåœ¨åç»­ç‰ˆæœ¬ä¸­å¢åŠ è¿™ä¸ªèƒ½åŠ›ã€‚\n## åè®®\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE.txt) åè®®å¼€æºï¼ŒVisualGLM-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE.txt)ã€‚\n\n## å¼•ç”¨ä¸è‡´è°¢\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡\n```\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n@article{ding2021cogview,\n  title={Cogview: Mastering text-to-image generation via transformers},\n  author={Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others},\n  journal={Advances in Neural Information Processing Systems},\n  volume={34},\n  pages={19822--19835},\n  year={2021}\n}\n```\nåœ¨VisualGLM-6Bçš„æŒ‡ä»¤å¾®è°ƒé˜¶æ®µçš„æ•°æ®é›†ä¸­ï¼ŒåŒ…å«äº†æ¥è‡ª[MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)å’Œ[LLAVA](https://github.com/haotian-liu/LLaVA)é¡¹ç›®çš„ä¸€éƒ¨åˆ†è‹±æ–‡å›¾æ–‡æ•°æ®ï¼Œä»¥åŠè®¸å¤šç»å…¸çš„è·¨æ¨¡æ€å·¥ä½œæ•°æ®é›†ï¼Œè¡·å¿ƒæ„Ÿè°¢ä»–ä»¬çš„è´¡çŒ®ã€‚\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 14.0234375,
          "content": "# VisualGLM-6B\n<p align=\"center\">\nğŸ¤— <a href=\"https://huggingface.co/THUDM/visualglm-6b\" target=\"_blank\">HF Repo</a> â€¢ âš’ï¸ <a href=\"https://github.com/THUDM/SwissArmyTransformer\" target=\"_blank\">SwissArmyTransformer (sat)</a> â€¢ ğŸ¦ <a href=\"https://twitter.com/thukeg\" target=\"_blank\">Twitter</a> \n</p>\n<p align=\"center\">\nâ€¢  ğŸ“ƒ <a href=\"https://arxiv.org/abs/2105.13290\" target=\"_blank\">[CogView@NeurIPS 21]</a>  <a href=\"https://github.com/THUDM/CogView\" target=\"_blank\">[GitHub]</a> â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> <br>\n</p>\n<p align=\"center\">\n    ğŸ‘‹ Join us on <a href=\"https://join.slack.com/t/chatglm/shared_invite/zt-1th2q5u69-7tURzFuOPanmuHy9hsZnKA\" target=\"_blank\">Slack</a> and <a href=\"resources/WECHAT.md\" target=\"_blank\">WeChat</a>\n</p>\n<!-- <p align=\"center\">\nğŸ¤–<a href=\"https://huggingface.co/spaces/THUDM/visualglm-6b\" target=\"_blank\">VisualGLM-6B Online Demo Website</a>\n</p> -->\n\n## Introduction\nVisualGLM-6B is an open-source, multi-modal dialog language model that supports **images, Chinese, and English**. The language model is based on [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) with 6.2 billion parameters; the image part builds a bridge between the visual model and the language model through the training of [BLIP2-Qformer](https://arxiv.org/abs/2301.12597), with the total model comprising 7.8 billion parameters.\n\nVisualGLM-6B relies on 30M high-quality Chinese image-text pairs from the [CogView](https://arxiv.org/abs/2105.13290) dataset and 300M filtered English image-text pairs for pre-training, with equal weight for Chinese and English. This training method aligns visual information well to the semantic space of ChatGLM. In the subsequent fine-tuning phase, the model is trained on long visual question answering data to generate answers that align with human preferences.\n\nVisualGLM-6B is trained using the [SwissArmyTransformer](https://github.com/THUDM/SwissArmyTransformer) (abbreviated as sat) library, a utility library for flexible modification and training of Transformer, supporting efficient fine-tuning methods like Lora and P-tuning. This project provides a user-friendly huggingface interface, as well as an interface based on sat.\n\nHowever, as VisualGLM-6B is still at the v1 stage, it is known to have quite a few [**limitations**](#Limitations), such as factual inaccuracy/model hallucination in image description, lack of capturing image detail information, and some limitations from the language model. Please be aware of these issues and evaluate the potential risks before using. In future versions of VisualGLM, we will strive to optimize these issues.\n\nWith model quantization technology, users can deploy locally on consumer-grade graphics cards (requiring as little as 6.3G memory under INT4 quantization level).\n\n## Examples\nVisualGLM-6B can answer questions related to image description.\n![Titanic example](examples/chat_example1.png)\n\n<details>\n<summary>It can also combine common sense or propose interesting views. Click to expand/collapse more examples</summary>\n\n![Ironing shirt taxi example](examples/chat_example2.png)\n![Mona Lisa dog example](examples/chat_example3.png)\n\n</details>\n\n\n## Usage\n\n### Model Inference\n\nInstall dependencies with pip\n```\npip install -i https://pypi.org/simple -r requirements.txt\npip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements.txt\n```\nThis will default to installing the deepspeed library (which supports the sat library training). This library is not necessary for model inference and can cause problems when installed in some Windows environments.\nIf you want to bypass deepspeed installation, you can change the command to:\n```\npip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements_wo_ds.txt\npip install -i https://mirrors.aliyun.com/pypi/simple/ --no-deps \"SwissArmyTransformer>=0.3.6\"\n```\n\nIf you are calling the model using the Huggingface transformers library (you also need to install the above dependency packages!), you can use the following code (where the image path is the local path):\n```python\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\nimage_path = \"your image path\"\nresponse, history = model.chat(tokenizer, image_path, \"æè¿°è¿™å¼ å›¾ç‰‡ã€‚\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, image_path, \"è¿™å¼ å›¾ç‰‡å¯èƒ½æ˜¯åœ¨ä»€ä¹ˆåœºæ‰€æ‹æ‘„çš„ï¼Ÿ\", history=history)\nprint(response)\n```\n\nIf you use the SwissArmyTransformer library to call the model, the method is similar, and you can use the environment variable SAT_HOME to determine the model download location. In the directory of this repository:\n```python\nimport argparse\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nfrom model import chat, VisualGLMModel\nmodel, model_args = VisualGLMModel.from_pretrained('visualglm-6b', args=argparse.Namespace(fp16=True, skip_init=True))\nfrom sat.model.mixins import CachedAutoregressiveMixin\nmodel.add_mixin('auto-regressive', CachedAutoregressiveMixin())\nimage_path = \"your image path or URL\"\nresponse, history, cache_image = chat(image_path, model, tokenizer, \"Describe this picture.\", history=[])\nprint(response)\nresponse, history, cache_image = chat(None, model, tokenizer, \"Where could this picture possibly have been taken?\", history=history, image=cache_image)\nprint(response)\n```\n\nUsing the `sat` library can also easily carry out efficient parameter fine-tuning. <!-- TODO specific code -->\n\nPlease note that the Huggingface model implementation is located in the [Huggingface repository](https://huggingface.co/THUDM/visualglm-6b), and the `sat` model implementation is included in this repository.\n\n## Model Fine-tuning\n\nMultimodal tasks are wide-ranging and diverse, and pre-training often cannot cover all bases.\nHere we provide an example of small sample fine-tuning, using 20 labeled images to enhance the model's ability to answer \"background\" questions.\n\nAfter unzipping fewshot-data.zip, run the following command:\n```\nbash finetune/finetune_visualglm.sh\n```\n\nCurrently we support three types of (parameter-efficient) fine-tuning:\n\n* LoRA: In the given example, we add rank=10 LoRA for layer 0 and layer 14 in ChatGLM. You can adjust `--layer_range` and `--lora_rank` to fit your application and data amount.\n* QLoRA: If your resource is limited, consider using `bash finetune/finetune_visualglm_qlora.sh`, which do 4-bit quantization for ChatGLM Linear layers, reducing the required GPU memory to 9.8 GB.\n* P-tuning: You can replace `--use_lora` to `--use_ptuning`, but not recommended, unless your application has a relatively fixed input and output template.\n\nAfter training, you can use the following command for inference:\n\n```\npython cli_demo.py --from_pretrained your_checkpoint_path --prompt_zh è¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ\n```\n\nFine-tuning requires the installation of the deepspeed library, and currently this process only supports the Linux system. More examples and instructions for the Windows system will be completed in the near future.\n\nIf you want to merge LoRA weights into original weights, just call `merge_lora()`:\n\n```python\nfrom finetune_visualglm import FineTuneVisualGLMModel\nimport argparse\n\nmodel, args = FineTuneVisualGLMModel.from_pretrained('checkpoints/finetune-visualglm-6b-05-19-07-36',\n        args=argparse.Namespace(\n        fp16=True,\n        skip_init=True,\n        use_gpu_initialization=True,\n        device='cuda',\n    ))\nmodel.get_mixin('lora').merge_lora()\nargs.layer_range = []\nargs.save = 'merge_lora'\nargs.mode = 'inference'\nfrom sat.training.model_io import save_checkpoint\nsave_checkpoint(1, model, None, None, args)\n```\n\n## Deployment Tools\n\n### Command Line Demo\n\n```shell\npython cli_demo.py \n```\nThe program will automatically download the sat model and interact in the command line. You can generate replies by entering instructions and pressing enter. Enter 'clear' to clear the conversation history and 'stop' to stop the program.\n\n![cli_demo](examples/thu.png)\nThe program provides the following hyperparameters to control the generation process and quantization accuracy:\n```\nusage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english] [--quant {8,4}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --max_length MAX_LENGTH\n                        max length of the total sequence\n  --top_p TOP_P         top p for nucleus sampling\n  --top_k TOP_K         top k for top k sampling\n  --temperature TEMPERATURE\n                        temperature for sampling\n  --english             only output English\n  --quant {8,4}         quantization bits\n```\nNote that during training, the prompt words for English Q&A pairs are 'Q: A:', while in Chinese they are 'é—®ï¼šç­”ï¼š'. The web demo uses Chinese prompts, so the English replies will be worse and interspersed with Chinese; if you need English replies, please use the --english option in cli_demo.py.\n\nWe also provide a typewriter effect command line tool inherited from ChatGLM-6B, which uses the Huggingface model:\n```shell\npython cli_demo_hf.py\n```\n\n### Web Demo\n![web_demo](examples/web_demo.png)\n\nWe provide a web demo based on [Gradio](https://gradio.app). First, install Gradio: `pip install gradio`.\nThen download and enter this repository and run `web_demo.py`:\n\n```\ngit clone https://github.com/THUDM/VisualGLM-6B\ncd VisualGLM-6B\npython web_demo.py\n```\nThe program will automatically download the sat model and run a Web Server, outputting the address. Open the output address in your browser to use it.\n\nWe also provide a web tool with a typewriter effect inherited from ChatGLM-6B, which uses the Huggingface model and will run on port :8080 after starting:\n```shell\npython web_demo_hf.py\n```\n\nBoth web demos accept the command line parameter --share to generate a public link for gradio, and accept --quant 4 and --quant 8 to use 4-bit quantization/8-bit quantization to reduce GPU memory usage.\n\n### API Deployment\nFirst, you need to install additional dependencies pip install fastapi uvicorn, then run the api.py in the repository:\n```shell\npython api.py\n```\nThe program will automatically download the sat model, and by default it will be deployed on local port 8080 and called through the POST method. Below is an example of a request with curl, but in general you can also use a code method to POST.\n```shell\necho \"{\\\"image\\\":\\\"$(base64 path/to/example.jpg)\\\",\\\"text\\\":\\\"Describe this picture\\\",\\\"history\\\":[]}\" > temp.json\ncurl -X POST -H \"Content-Type: application/json\" -d @temp.json http://127.0.0.1:8080\n```\n\nWe also provide an api_hf.py that uses the Huggingface model, which works the same way as the sat model's api:\n```shell\npython api_hf.py\n```\n\n\n## Model Quantization\nIn the Huggingface implementation, the model is loaded with FP16 precision by default, and running the above code requires about 15GB of GPU memory. If your GPU memory is limited, you can try loading the model in a quantized manner.\nHere's how:\n```python\n# Modify as needed, currently only 4/8 bit quantization is supported. The following will only quantize ChatGLM, as the error is larger when quantizing ViT\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).quantize(8).half().cuda()\n```\n\nIn the sat implementation, you need to change the loading location to 'cpu' first, and then perform quantization. Here's how, see cli_demo.py for details:\n```python\nfrom sat.quantization.kernels import quantize\nmodel = quantize(model, args.quant).cuda()\n# only need 7GB GPU memory to inference\n```\n\n## Limitations\nThis project is currently at V1 version of the visual and language model parameters, the amount of calculation is small, we have summarized the following main improvements:\n\n- Image description factuality/model hallucination problem. When generating long descriptions of images, as the distance from the image increases, the language model will dominate, and there is a certain possibility of generating content that does not exist in the image based on the context.\n- Attribute mismatch problem. In scenes with multiple objects, some attributes of some objects are often incorrectly inserted onto other objects.\n- Resolution issue. This project uses a resolution of 224*224, which is the most commonly used size in visual models; however, for more fine-grained understanding, larger resolution and computation are necessary.\n- Due to data and other reasons, the model currently does not have the ability to perform Chinese OCR (some ability for English OCR), we will add this ability in future versions.\n## License\n\nThe code in this repository is open source under the Apache-2.0 license, while the use of the VisualGLM-6B model weights must comply with the Model License.\n\n## Citation & Acknowledgements\nIf you find our work helpful, please consider citing the following papers\n```\n@inproceedings{du2022glm,\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages={320--335},\n  year={2022}\n}\n@article{ding2021cogview,\n  title={Cogview: Mastering text-to-image generation via transformers},\n  author={Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others},\n  journal={Advances in Neural Information Processing Systems},\n  volume={34},\n  pages={19822--19835},\n  year={2021}\n}\n```\nIn the instruction fine-tuning phase of the VisualGLM-6B dataset, there are some English image-text data from the [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) and [LLAVA](https://github.com/haotian-liu/LLaVA) projects, as well as many classic cross-modal work datasets. We sincerely thank them for their contributions.\n"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 1.6455078125,
          "content": "import os\nimport json\nimport uvicorn\nfrom fastapi import FastAPI, Request\nfrom model import is_chinese, get_infer_setting, generate_input, chat\nimport datetime\nimport torch\n\ngpu_number = 0\nmodel, tokenizer = get_infer_setting(gpu_device=gpu_number)\n\napp = FastAPI()\n@app.post('/')\nasync def visual_glm(request: Request):\n    json_post_raw = await request.json()\n    print(\"Start to process request\")\n\n    json_post = json.dumps(json_post_raw)\n    request_data = json.loads(json_post)\n    input_text, input_image_encoded, history = request_data['text'], request_data['image'], request_data['history']\n    input_para = {\n        \"max_length\": 2048,\n        \"min_length\": 50,\n        \"temperature\": 0.8,\n        \"top_p\": 0.4,\n        \"top_k\": 100,\n        \"repetition_penalty\": 1.2\n    }\n    input_para.update(request_data)\n\n    is_zh = is_chinese(input_text)\n    input_data = generate_input(input_text, input_image_encoded, history, input_para)\n    input_image, gen_kwargs =  input_data['input_image'], input_data['gen_kwargs']\n    with torch.no_grad():\n        answer, history, _ = chat(None, model, tokenizer, input_text, history=history, image=input_image, \\\n                            max_length=gen_kwargs['max_length'], top_p=gen_kwargs['top_p'], \\\n                            top_k = gen_kwargs['top_k'], temperature=gen_kwargs['temperature'], english=not is_zh)\n        \n    now = datetime.datetime.now()\n    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    response = {\n        \"result\": answer,\n        \"history\": history,\n        \"status\": 200,\n        \"time\": time\n    }\n    return response\n\n\nif __name__ == '__main__':\n    uvicorn.run(app, host='0.0.0.0', port=8080, workers=1)"
        },
        {
          "name": "api_hf.py",
          "type": "blob",
          "size": 1.3330078125,
          "content": "import os\nimport json\nfrom transformers import AutoTokenizer, AutoModel\nimport uvicorn\nfrom fastapi import FastAPI, Request\nimport datetime\nfrom model import process_image\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\n\n\napp = FastAPI()\n@app.post('/')\nasync def visual_glm(request: Request):\n    json_post_raw = await request.json()\n    print(\"Start to process request\")\n\n    json_post = json.dumps(json_post_raw)\n    request_data = json.loads(json_post)\n\n    history = request_data.get(\"history\")\n    image_encoded = request_data.get(\"image\")\n    query = request_data.get(\"text\")\n    image_path = process_image(image_encoded)\n\n    with torch.no_grad():    \n        result = model.stream_chat(tokenizer, image_path, query, history=history)\n    last_result = None\n    for value in result:\n        last_result = value\n    answer = last_result[0]\n\n    if os.path.isfile(image_path):\n        os.remove(image_path)\n    now = datetime.datetime.now()\n    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    response = {\n        \"result\": answer,\n        \"history\": history,\n        \"status\": 200,\n        \"time\": time\n    }\n    return response\n\n\nif __name__ == \"__main__\":\n   uvicorn.run(app, host='0.0.0.0', port=8080, workers=1)"
        },
        {
          "name": "cli_demo.py",
          "type": "blob",
          "size": 4.3095703125,
          "content": "# -*- encoding: utf-8 -*-\n\nimport os\nimport sys\nimport torch\nimport argparse\nfrom transformers import AutoTokenizer\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.quantization.kernels import quantize\n\nfrom model import VisualGLMModel, chat\nfrom finetune_visualglm import FineTuneVisualGLMModel\nfrom sat.model import AutoModel\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max_length\", type=int, default=2048, help='max length of the total sequence')\n    parser.add_argument(\"--top_p\", type=float, default=0.4, help='top p for nucleus sampling')\n    parser.add_argument(\"--top_k\", type=int, default=100, help='top k for top k sampling')\n    parser.add_argument(\"--temperature\", type=float, default=.8, help='temperature for sampling')\n    parser.add_argument(\"--english\", action='store_true', help='only output English')\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None, help='quantization bits')\n    parser.add_argument(\"--from_pretrained\", type=str, default=\"visualglm-6b\", help='pretrained ckpt')\n    parser.add_argument(\"--prompt_zh\", type=str, default=\"æè¿°è¿™å¼ å›¾ç‰‡ã€‚\", help='Chinese prompt for the first round')\n    parser.add_argument(\"--prompt_en\", type=str, default=\"Describe the image.\", help='English prompt for the first round')\n    args = parser.parse_args()\n\n    # load model\n    model, model_args = AutoModel.from_pretrained(\n        args.from_pretrained,\n        args=argparse.Namespace(\n        fp16=True,\n        skip_init=True,\n        use_gpu_initialization=True if (torch.cuda.is_available() and args.quant is None) else False,\n        device='cuda' if (torch.cuda.is_available() and args.quant is None) else 'cpu',\n    ))\n    model = model.eval()\n\n    if args.quant:\n        quantize(model, args.quant)\n        if torch.cuda.is_available():\n            model = model.cuda()\n\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n    if not args.english:\n        print('æ¬¢è¿ä½¿ç”¨ VisualGLM-6B æ¨¡å‹ï¼Œè¾“å…¥å›¾åƒURLæˆ–æœ¬åœ°è·¯å¾„è¯»å›¾ï¼Œç»§ç»­è¾“å…¥å†…å®¹å¯¹è¯ï¼Œclear é‡æ–°å¼€å§‹ï¼Œstop ç»ˆæ­¢ç¨‹åº')\n    else:\n        print('Welcome to VisualGLM-6B model. Enter an image URL or local file path to load an image. Continue inputting text to engage in a conversation. Type \"clear\" to start over, or \"stop\" to end the program.')\n    with torch.no_grad():\n        while True:\n            history = None\n            cache_image = None\n            if not args.english:\n                image_path = input(\"è¯·è¾“å…¥å›¾åƒè·¯å¾„æˆ–URLï¼ˆå›è½¦è¿›å…¥çº¯æ–‡æœ¬å¯¹è¯ï¼‰ï¼š \")\n            else:\n                image_path = input(\"Please enter the image path or URL (press Enter for plain text conversation): \")\n\n            if image_path == 'stop':\n                break\n            if len(image_path) > 0:\n                query = args.prompt_en if args.english else args.prompt_zh\n            else:\n                if not args.english:\n                    query = input(\"ç”¨æˆ·ï¼š\")\n                else:\n                    query = input(\"User: \")\n            while True:\n                if query == \"clear\":\n                    break\n                if query == \"stop\":\n                    sys.exit(0)\n                try:\n                    response, history, cache_image = chat(\n                        image_path, \n                        model, \n                        tokenizer,\n                        query, \n                        history=history, \n                        image=cache_image, \n                        max_length=args.max_length, \n                        top_p=args.top_p, \n                        temperature=args.temperature,\n                        top_k=args.top_k,\n                        english=args.english,\n                        invalid_slices=[slice(63823, 130000)] if args.english else []\n                        )\n                except Exception as e:\n                    print(e)\n                    break\n                sep = 'A:' if args.english else 'ç­”ï¼š'\n                print(\"VisualGLM-6Bï¼š\"+response.split(sep)[-1].strip())\n                image_path = None\n                if not args.english:\n                    query = input(\"ç”¨æˆ·ï¼š\")\n                else:\n                    query = input(\"User: \")\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        {
          "name": "cli_demo_hf.py",
          "type": "blob",
          "size": 2.2001953125,
          "content": "import os\nimport platform\nimport signal\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\nmodel = model.eval()\n\nos_name = platform.system()\nclear_command = 'cls' if os_name == 'Windows' else 'clear'\nstop_stream = False\n\n\ndef build_prompt(history, prefix):\n    prompt = prefix\n    for query, response in history:\n        prompt += f\"\\n\\nç”¨æˆ·ï¼š{query}\"\n        prompt += f\"\\n\\nVisualGLM-6Bï¼š{response}\"\n    return prompt\n\n\ndef signal_handler(signal, frame):\n    global stop_stream\n    stop_stream = True\n\n\ndef main():\n    global stop_stream\n    while True:\n        history = []\n        prefix = \"æ¬¢è¿ä½¿ç”¨ VisualGLM-6B æ¨¡å‹ï¼Œè¾“å…¥å›¾ç‰‡è·¯å¾„å’Œå†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\"\n        print(prefix)\n        image_path = input(\"\\nè¯·è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼š\")\n        if image_path == \"stop\":\n            break\n        prefix = prefix + \"\\n\" + image_path\n        query = \"æè¿°è¿™å¼ å›¾ç‰‡ã€‚\"\n        while True:\n            count = 0\n            with torch.no_grad():\n                for response, history in model.stream_chat(tokenizer, image_path, query, history=history):\n                    if stop_stream:\n                        stop_stream = False\n                        break\n                    else:\n                        count += 1\n                        if count % 8 == 0:\n                            os.system(clear_command)\n                            print(build_prompt(history, prefix), flush=True)\n                            signal.signal(signal.SIGINT, signal_handler)\n            os.system(clear_command)\n            print(build_prompt(history, prefix), flush=True)\n            query = input(\"\\nç”¨æˆ·ï¼š\")\n            if query.strip() == \"clear\":\n                break\n            if query.strip() == \"stop\":\n                stop_stream = True\n                exit(0)\n            # if query.strip() == \"clear\":\n            #     history = []\n            #     os.system(clear_command)\n            #     print(prefix)\n            #     continue\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        {
          "name": "cli_demo_mp.py",
          "type": "blob",
          "size": 2.7080078125,
          "content": "# -*- encoding: utf-8 -*-\n\nimport os\nimport sys\nimport torch\nimport argparse\nfrom transformers import AutoTokenizer\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.quantization.kernels import quantize\n\nfrom model import VisualGLMModel, chat\nfrom finetune_visualglm import FineTuneVisualGLMModel\nfrom sat.model import AutoModel\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max_length\", type=int, default=2048, help='max length of the total sequence')\n    parser.add_argument(\"--top_p\", type=float, default=0.4, help='top p for nucleus sampling')\n    parser.add_argument(\"--top_k\", type=int, default=100, help='top k for top k sampling')\n    parser.add_argument(\"--temperature\", type=float, default=.8, help='temperature for sampling')\n    parser.add_argument(\"--english\", action='store_true', help='only output English')\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None, help='quantization bits')\n    parser.add_argument(\"--from_pretrained\", type=str, default=\"visualglm-6b\", help='pretrained ckpt')\n    parser.add_argument(\"--prompt_zh\", type=str, default=\"æè¿°è¿™å¼ å›¾ç‰‡ã€‚\", help='Chinese prompt for the first round')\n    parser.add_argument(\"--prompt_en\", type=str, default=\"Describe the image.\", help='English prompt for the first round')\n    args = parser.parse_args()\n\n    # load model\n    model, model_args = AutoModel.from_pretrained(\n        args.from_pretrained,\n        args=argparse.Namespace(\n        fp16=True,\n        skip_init=True,\n        use_gpu_initialization=True if (torch.cuda.is_available() and args.quant is None) else False,\n        device='cuda' if (torch.cuda.is_available() and args.quant is None) else 'cpu',\n    ), overwrite_args={'model_parallel_size': 2})\n    model = model.eval()\n\n    if args.quant:\n        quantize(model.transformer, args.quant)\n        if torch.cuda.is_available():\n            model = model.cuda()\n\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n    image_path = 'fewshot-data/meme.png'\n    query = args.prompt_en if args.english else args.prompt_zh\n    history = None\n    cache_image = None\n    response, history, cache_image = chat(\n        image_path, \n        model, \n        tokenizer,\n        query, \n        history=history, \n        image=cache_image, \n        max_length=args.max_length, \n        top_p=args.top_p, \n        temperature=args.temperature,\n        top_k=args.top_k,\n        english=args.english,\n        invalid_slices=[slice(63823, 130000)] if args.english else []\n        )\n    sep = 'A:' if args.english else 'ç­”ï¼š'\n    print(response.split(sep)[-1].strip())\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "fewshot-data.zip",
          "type": "blob",
          "size": 6538.33984375,
          "content": ""
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune_visualglm.py",
          "type": "blob",
          "size": 7.7880859375,
          "content": "import os\nimport torch\nimport argparse\n\nfrom sat import mpu, get_args, get_tokenizer\nfrom sat.training.deepspeed_training import training_main\nfrom model import VisualGLMModel\nfrom sat.model.finetune import PTuningV2Mixin\nfrom sat.model.finetune.lora2 import LoraMixin\n\nclass FineTuneVisualGLMModel(VisualGLMModel):\n    def __init__(self, args, transformer=None, **kw_args):\n        super().__init__(args, transformer=transformer, **kw_args)\n        if args.use_ptuning:\n            self.add_mixin(\"ptuning\", PTuningV2Mixin(args.num_layers, args.hidden_size // args.num_attention_heads, args.num_attention_heads, args.pre_seq_len))\n        if args.use_lora:\n            self.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range), reinit=True)\n            # self.get_mixin(\"eva\").model.glm_proj = replace_linear_with_lora(self.get_mixin(\"eva\").model.glm_proj, LoraLinear, args.lora_rank)\n        elif args.use_qlora:\n            self.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range, qlora=True), reinit=True)\n        self.args = args\n        \n    @classmethod\n    def add_model_specific_args(cls, parser):\n        group = parser.add_argument_group('VisualGLM-finetune', 'VisualGLM finetune Configurations')\n        group.add_argument('--pre_seq_len', type=int, default=8)\n        group.add_argument('--lora_rank', type=int, default=10)\n        group.add_argument('--use_ptuning', action=\"store_true\")\n        group.add_argument('--use_lora', action=\"store_true\")\n        group.add_argument('--use_qlora', action=\"store_true\")\n        group.add_argument('--layer_range', nargs='+', type=int, default=None)\n        return super().add_model_specific_args(parser)\n\n    def disable_untrainable_params(self):\n        enable = []\n        if self.args.use_ptuning:\n            enable.extend(['ptuning'])\n        if self.args.use_lora or self.args.use_qlora:\n            enable.extend(['matrix_A', 'matrix_B'])\n        for n, p in self.named_parameters():\n            flag = False\n            for e in enable:\n                if e.lower() in n.lower():\n                    flag = True\n                    break\n            if not flag:\n                p.requires_grad_(False)\n            else:\n                print(n)\n\n\ndef get_batch(data_iterator, args, timers):\n    # Items and their type.\n    keys = ['input_ids', 'labels']\n    datatype = torch.int64\n\n    # Broadcast data.\n    timers('data loader').start()\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    timers('data loader').stop()\n    data_b = mpu.broadcast_data(keys, data, datatype)\n    data_i = mpu.broadcast_data(['image'], data, torch.float32)\n    # Unpack.\n    tokens = data_b['input_ids'].long()\n    labels = data_b['labels'].long()\n    img = data_i['image']\n    if args.fp16:\n        img = img.half()\n    \n    return tokens, labels, img, data['pre_image']\n\n\nfrom torch.nn import CrossEntropyLoss\n\ndef forward_step(data_iterator, model, args, timers):\n    \"\"\"Forward step.\"\"\"\n\n    # Get the batch.\n    timers('batch generator').start()\n    tokens, labels, image, pre_image = get_batch(\n        data_iterator, args, timers)\n    timers('batch generator').stop()\n\n    logits = model(input_ids=tokens, image=image, pre_image=pre_image)[0]\n    dtype = logits.dtype\n    lm_logits = logits.to(torch.float32)\n\n    # Shift so that tokens < n predict n\n    shift_logits = lm_logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n    # Flatten the tokens\n    loss_fct = CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n    lm_logits = lm_logits.to(dtype)\n    loss = loss.to(dtype)\n    return loss, {'loss': loss}\n\n\nfrom model.blip2 import BlipImageEvalProcessor\nfrom torch.utils.data import Dataset\nimport json\nfrom PIL import Image\n\nclass FewShotDataset(Dataset):\n    def __init__(self, path, processor, tokenizer, args):\n        max_seq_length = args.max_source_length + args.max_target_length\n        with open(path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        self.images = []\n        self.input_ids = []\n        self.labels = []\n        for item in data:\n            image = processor(Image.open(item['img']).convert('RGB'))\n            input0 = tokenizer.encode(\"<img>\", add_special_tokens=False)\n            input1 = [tokenizer.pad_token_id] * args.image_length\n            input2 = tokenizer.encode(\"</img>é—®ï¼š\"+item['prompt']+\"\\nç­”ï¼š\", add_special_tokens=False)\n            a_ids = sum([input0, input1, input2], [])\n            b_ids = tokenizer.encode(text=item['label'], add_special_tokens=False)\n            if len(a_ids) > args.max_source_length - 1:\n                a_ids = a_ids[: args.max_source_length - 1]\n            if len(b_ids) > args.max_target_length - 2:\n                b_ids = b_ids[: args.max_target_length - 2]\n            pre_image = len(input0)\n            input_ids = tokenizer.build_inputs_with_special_tokens(a_ids, b_ids)\n\n            context_length = input_ids.index(tokenizer.bos_token_id)\n            mask_position = context_length - 1\n            labels = [-100] * context_length + input_ids[mask_position+1:]\n            \n            pad_len = max_seq_length - len(input_ids)\n            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n            labels = labels + [tokenizer.pad_token_id] * pad_len\n            if args.ignore_pad_token_for_loss:\n                labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n            self.images.append(image)\n            self.input_ids.append(input_ids)\n            self.labels.append(labels)\n        self.pre_image = pre_image\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        return {\n            \"image\": self.images[idx],\n            \"input_ids\": self.input_ids[idx],\n            \"labels\": self.labels[idx],\n            \"pre_image\": self.pre_image\n        }\n\n\ndef create_dataset_function(path, args):\n    tokenizer = get_tokenizer(args)\n    image_processor = BlipImageEvalProcessor(224)\n\n    dataset = FewShotDataset(path, image_processor, tokenizer, args)\n    return dataset\n\n\nif __name__ == '__main__':\n    py_parser = argparse.ArgumentParser(add_help=False)\n    py_parser.add_argument('--max_source_length', type=int)\n    py_parser.add_argument('--max_target_length', type=int)\n    py_parser.add_argument('--ignore_pad_token_for_loss', type=bool, default=True)\n    # py_parser.add_argument('--old_checkpoint', action=\"store_true\")\n    py_parser.add_argument('--source_prefix', type=str, default=\"\")\n    py_parser = FineTuneVisualGLMModel.add_model_specific_args(py_parser)\n    known, args_list = py_parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    args.device = 'cpu'\n\n    model_type = 'visualglm-6b'\n    model, args = FineTuneVisualGLMModel.from_pretrained(model_type, args)\n    if torch.cuda.is_available():\n        model = model.to('cuda')\n        args.device = 'cuda'\n    tokenizer = get_tokenizer(args)\n    label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    def data_collator(examples):\n        for example in examples:\n            example['input_ids'] = torch.tensor(example['input_ids'], dtype=torch.long)\n            example['labels'] = torch.tensor(example['labels'], dtype=torch.long)\n        ret = {\n            'input_ids': torch.stack([example['input_ids'] for example in examples]),\n            'labels': torch.stack([example['labels'] for example in examples]),\n            'image': torch.stack([example['image'] for example in examples]),\n            'pre_image': example['pre_image']\n        }\n        return ret\n    training_main(args, model_cls=model, forward_step_function=forward_step, create_dataset_function=create_dataset_function, collate_fn=data_collator)\n"
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.08984375,
          "content": "SwissArmyTransformer>=0.4.4\ntorch>1.10.0\ntorchvision\ntransformers>=4.27.1\nmdtex2html\ngradio\n"
        },
        {
          "name": "requirements_wo_ds.txt",
          "type": "blob",
          "size": 0.115234375,
          "content": "torch>1.10.0\ntorchvision\ntransformers>=4.27.1\nmdtex2html\ngradio\nsentencepiece\ntensorboardX\ndatasets\ncpm_kernels\neinops"
        },
        {
          "name": "web_demo.py",
          "type": "blob",
          "size": 5.6708984375,
          "content": "#!/usr/bin/env python\n\nimport gradio as gr\nfrom PIL import Image\nimport os\nimport json\nfrom model import is_chinese, get_infer_setting, generate_input, chat\nimport torch\n\ndef generate_text_with_image(input_text, image, history=[], request_data=dict(), is_zh=True):\n    input_para = {\n        \"max_length\": 2048,\n        \"min_length\": 50,\n        \"temperature\": 0.8,\n        \"top_p\": 0.4,\n        \"top_k\": 100,\n        \"repetition_penalty\": 1.2\n    }\n    input_para.update(request_data)\n\n    input_data = generate_input(input_text, image, history, input_para, image_is_encoded=False)\n    input_image, gen_kwargs =  input_data['input_image'], input_data['gen_kwargs']\n    with torch.no_grad():\n        answer, history, _ = chat(None, model, tokenizer, input_text, history=history, image=input_image, \\\n                            max_length=gen_kwargs['max_length'], top_p=gen_kwargs['top_p'], \\\n                            top_k = gen_kwargs['top_k'], temperature=gen_kwargs['temperature'], english=not is_zh)\n    return answer\n\n\ndef request_model(input_text, temperature, top_p, image_prompt, result_previous):\n    result_text = [(ele[0], ele[1]) for ele in result_previous]\n    for i in range(len(result_text)-1, -1, -1):\n        if result_text[i][0] == \"\" or result_text[i][1] == \"\":\n            del result_text[i]\n    print(f\"history {result_text}\")\n\n    is_zh = is_chinese(input_text)\n    if image_prompt is None:\n        if is_zh:\n            result_text.append((input_text, 'å›¾ç‰‡ä¸ºç©ºï¼è¯·ä¸Šä¼ å›¾ç‰‡å¹¶é‡è¯•ã€‚'))\n        else:\n            result_text.append((input_text, 'Image empty! Please upload a image and retry.'))\n        return input_text, result_text\n    elif input_text == \"\":\n        result_text.append((input_text, 'Text empty! Please enter text and retry.'))\n        return \"\", result_text                \n\n    request_para = {\"temperature\": temperature, \"top_p\": top_p}\n    image = Image.open(image_prompt)\n    try:\n        answer = generate_text_with_image(input_text, image, result_text.copy(), request_para, is_zh)\n    except Exception as e:\n        print(f\"error: {e}\")\n        if is_zh:\n            result_text.append((input_text, 'è¶…æ—¶ï¼è¯·ç¨ç­‰å‡ åˆ†é’Ÿå†é‡è¯•ã€‚'))\n        else:\n            result_text.append((input_text, 'Timeout! Please wait a few minutes and retry.'))\n        return \"\", result_text\n\n    result_text.append((input_text, answer))\n    print(result_text)\n    return \"\", result_text\n\n\nDESCRIPTION = '''# <a href=\"https://github.com/THUDM/VisualGLM-6B\">VisualGLM</a>'''\n\nMAINTENANCE_NOTICE1 = 'Hint 1: If the app report \"Something went wrong, connection error out\", please turn off your proxy and retry.\\nHint 2: If you upload a large size of image like 10MB, it may take some time to upload and process. Please be patient and wait.'\nMAINTENANCE_NOTICE2 = 'æç¤º1: å¦‚æœåº”ç”¨æŠ¥äº†â€œSomething went wrong, connection error outâ€çš„é”™è¯¯ï¼Œè¯·å…³é—­ä»£ç†å¹¶é‡è¯•ã€‚\\næç¤º2: å¦‚æœä½ ä¸Šä¼ äº†å¾ˆå¤§çš„å›¾ç‰‡ï¼Œæ¯”å¦‚10MBå¤§å°ï¼Œé‚£å°†éœ€è¦ä¸€äº›æ—¶é—´æ¥ä¸Šä¼ å’Œå¤„ç†ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚'\n\nNOTES = 'This app is adapted from <a href=\"https://github.com/THUDM/VisualGLM-6B\">https://github.com/THUDM/VisualGLM-6B</a>. It would be recommended to check out the repo if you want to see the detail of our model and training process.'\n\n\ndef clear_fn(value):\n    return \"\", [(\"\", \"Hi, What do you want to know about this image?\")], None\n\ndef clear_fn2(value):\n    return [(\"\", \"Hi, What do you want to know about this image?\")]\n\n\ndef main(args):\n    gr.close_all()\n    global model, tokenizer\n    model, tokenizer = get_infer_setting(gpu_device=0, quant=args.quant)\n    \n    with gr.Blocks(css='style.css') as demo:\n        gr.Markdown(DESCRIPTION)\n        with gr.Row():\n            with gr.Column(scale=4.5):\n                with gr.Group():\n                    input_text = gr.Textbox(label='Input Text', placeholder='Please enter text prompt below and press ENTER.')\n                    with gr.Row():\n                        run_button = gr.Button('Generate')\n                        clear_button = gr.Button('Clear')\n\n                    image_prompt = gr.Image(type=\"filepath\", label=\"Image Prompt\", value=None)\n                with gr.Row():\n                    temperature = gr.Slider(maximum=1, value=0.8, minimum=0, label='Temperature')\n                    top_p = gr.Slider(maximum=1, value=0.4, minimum=0, label='Top P')\n                with gr.Group():\n                    with gr.Row():\n                        maintenance_notice = gr.Markdown(MAINTENANCE_NOTICE1)\n            with gr.Column(scale=5.5):\n                result_text = gr.components.Chatbot(label='Multi-round conversation History', value=[(\"\", \"Hi, What do you want to know about this image?\")]).style(height=550)\n\n        gr.Markdown(NOTES)\n\n        print(gr.__version__)\n        run_button.click(fn=request_model,inputs=[input_text, temperature, top_p, image_prompt, result_text],\n                         outputs=[input_text, result_text])\n        input_text.submit(fn=request_model,inputs=[input_text, temperature, top_p, image_prompt, result_text],\n                         outputs=[input_text, result_text])\n        clear_button.click(fn=clear_fn, inputs=clear_button, outputs=[input_text, result_text, image_prompt])\n        image_prompt.upload(fn=clear_fn2, inputs=clear_button, outputs=[result_text])\n        image_prompt.clear(fn=clear_fn2, inputs=clear_button, outputs=[result_text])\n\n        print(gr.__version__)\n\n    demo.queue(concurrency_count=10)\n    demo.launch(share=args.share)\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    args = parser.parse_args()\n\n    main(args)"
        },
        {
          "name": "web_demo_hf.py",
          "type": "blob",
          "size": 6.19921875,
          "content": "from transformers import AutoModel, AutoTokenizer\nimport gradio as gr\nimport mdtex2html\nimport torch\n\n\"\"\"Override Chatbot.postprocess\"\"\"\n\ndef postprocess(self, y):\n    if y is None:\n        return []\n    for i, (message, response) in enumerate(y):\n        y[i] = (\n            None if message is None else mdtex2html.convert((message)),\n            None if response is None else mdtex2html.convert(response),\n        )\n    return y\n\ngr.Chatbot.postprocess = postprocess\n\n\ndef parse_text(text):\n    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n    lines = text.split(\"\\n\")\n    lines = [line for line in lines if line != \"\"]\n    count = 0\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            count += 1\n            items = line.split('`')\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = f'<br></code></pre>'\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace(\"`\", \"\\`\")\n                    line = line.replace(\"<\", \"&lt;\")\n                    line = line.replace(\">\", \"&gt;\")\n                    line = line.replace(\" \", \"&nbsp;\")\n                    line = line.replace(\"*\", \"&ast;\")\n                    line = line.replace(\"_\", \"&lowbar;\")\n                    line = line.replace(\"-\", \"&#45;\")\n                    line = line.replace(\".\", \"&#46;\")\n                    line = line.replace(\"!\", \"&#33;\")\n                    line = line.replace(\"(\", \"&#40;\")\n                    line = line.replace(\")\", \"&#41;\")\n                    line = line.replace(\"$\", \"&#36;\")\n                lines[i] = \"<br>\"+line\n    text = \"\".join(lines)\n    return text\n\n\ndef predict(input, image_path, chatbot, max_length, top_p, temperature, history):\n    if image_path is None:\n        return [(input, \"å›¾ç‰‡ä¸èƒ½ä¸ºç©ºã€‚è¯·é‡æ–°ä¸Šä¼ å›¾ç‰‡å¹¶é‡è¯•ã€‚\")], []\n    chatbot.append((parse_text(input), \"\"))\n    with torch.no_grad():\n        for response, history in model.stream_chat(tokenizer, image_path, input, history, max_length=max_length, top_p=top_p,\n                                               temperature=temperature):\n            chatbot[-1] = (parse_text(input), parse_text(response))\n\n            yield chatbot, history\n\n\ndef predict_new_image(image_path, chatbot, max_length, top_p, temperature):\n    input, history = \"æè¿°è¿™å¼ å›¾ç‰‡ã€‚\", []\n    chatbot.append((parse_text(input), \"\"))\n    with torch.no_grad():\n        for response, history in model.stream_chat(tokenizer, image_path, input, history, max_length=max_length,\n                                               top_p=top_p,\n                                               temperature=temperature):\n            chatbot[-1] = (parse_text(input), parse_text(response))\n\n            yield chatbot, history\n\n\ndef reset_user_input():\n    return gr.update(value='')\n\n\ndef reset_state():\n    return None, [], []\n\n\nDESCRIPTION = '''<h1 align=\"center\"><a href=\"https://github.com/THUDM/VisualGLM-6B\">VisualGLM</a></h1>'''\nMAINTENANCE_NOTICE = 'Hint 1: If the app report \"Something went wrong, connection error out\", please turn off your proxy and retry.\\nHint 2: If you upload a large size of image like 10MB, it may take some time to upload and process. Please be patient and wait.'\nNOTES = 'This app is adapted from <a href=\"https://github.com/THUDM/VisualGLM-6B\">https://github.com/THUDM/VisualGLM-6B</a>. It would be recommended to check out the repo if you want to see the detail of our model and training process.'\n\ndef main(args):\n    global model, tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True)\n    if args.quant in [4, 8]:\n        model = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).quantize(args.quant).half().cuda()\n    else:\n        model = AutoModel.from_pretrained(\"THUDM/visualglm-6b\", trust_remote_code=True).half().cuda()\n    model = model.eval()\n\n    with gr.Blocks(css='style.css') as demo:\n        gr.HTML(DESCRIPTION)\n        \n        with gr.Row():\n            with gr.Column(scale=2):\n                image_path = gr.Image(type=\"filepath\", label=\"Image Prompt\", value=None).style(height=504)\n            with gr.Column(scale=4):\n                chatbot = gr.Chatbot().style(height=480)\n        with gr.Row():\n            with gr.Column(scale=2, min_width=100):\n                max_length = gr.Slider(0, 4096, value=2048, step=1.0, label=\"Maximum length\", interactive=True)\n                top_p = gr.Slider(0, 1, value=0.4, step=0.01, label=\"Top P\", interactive=True)\n                temperature = gr.Slider(0, 1, value=0.8, step=0.01, label=\"Temperature\", interactive=True)\n            with gr.Column(scale=4):\n                with gr.Box():\n                    with gr.Row():\n                        with gr.Column(scale=2):\n                            user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=4).style(\n                                container=False)\n                        with gr.Column(scale=1, min_width=64):\n                            submitBtn = gr.Button(\"Submit\", variant=\"primary\")\n                            emptyBtn = gr.Button(\"Clear History\")\n                    gr.Markdown(MAINTENANCE_NOTICE + '\\n' + NOTES)\n        history = gr.State([])\n        \n\n        submitBtn.click(predict, [user_input, image_path, chatbot, max_length, top_p, temperature, history], [chatbot, history],\n                        show_progress=True)\n        image_path.upload(predict_new_image, [image_path, chatbot, max_length, top_p, temperature], [chatbot, history],\n                        show_progress=True)\n        image_path.clear(reset_state, outputs=[image_path, chatbot, history], show_progress=True)\n        submitBtn.click(reset_user_input, [], [user_input])\n        emptyBtn.click(reset_state, outputs=[image_path, chatbot, history], show_progress=True)\n\n        print(gr.__version__)\n\n        demo.queue().launch(share=args.share, inbrowser=True, server_name='0.0.0.0', server_port=8080)\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    args = parser.parse_args()\n    main(args)\n"
        }
      ]
    }
  ]
}