{
  "metadata": {
    "timestamp": 1736559617238,
    "page": 260,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lucidrains/deep-daze",
      "stars": 4373,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.802734375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# IDEs\n.vscode/\n.idea/\n\noutput/\nrun.py\nrun.sh\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0546875,
          "content": "MIT License\n\nCopyright (c) 2021 Ryan Murdock, Phil Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.033203125,
          "content": "recursive-include deep_daze *.txt\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.91796875,
          "content": "## Deep Daze\n\n<img src=\"./samples/Mist_over_green_hills.jpg\" width=\"256px\"></img>\n\n*mist over green hills*\n\n<img src=\"./samples/Shattered_plates_on_the_grass.jpg\" width=\"256px\"></img>\n\n*shattered plates on the grass*\n\n<img src=\"./samples/Cosmic_love_and_attention.jpg\" width=\"256px\"></img>\n\n*cosmic love and attention*\n\n<img src=\"./samples/A_time_traveler_in_the_crowd.jpg\" width=\"256px\"></img>\n\n*a time traveler in the crowd*\n\n<img src=\"./samples/Life_during_the_plague.jpg\" width=\"256px\"></img>\n\n*life during the plague*\n\n<img src=\"./samples/Meditative_peace_in_a_sunlit_forest.jpg\" width=\"256px\"></img>\n\n*meditative peace in a sunlit forest*\n\n<img src=\"./samples/A_man_painting_a_completely_red_image.png\" width=\"256px\"></img>\n\n*a man painting a completely red image*\n\n<img src=\"./samples/A_psychedelic_experience_on_LSD.png\" width=\"256px\"></img>\n\n*a psychedelic experience on LSD*\n\n## What is this?\n\nSimple command line tool for text to image generation using OpenAI's <a href=\"https://github.com/openai/CLIP\">CLIP</a> and <a href=\"https://arxiv.org/abs/2006.09661\">Siren</a>. Credit goes to <a href=\"https://twitter.com/advadnoun\">Ryan Murdock</a> for the discovery of this technique (and for coming up with the great name)!\n\nOriginal notebook [![Open In Colab][colab-badge]][colab-notebook]\n\nNew simplified notebook [![Open In Colab][colab-badge]][colab-notebook-2]\n\n[colab-notebook]: <https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP>\n[colab-notebook-2]: <https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj?usp=sharing>\n[colab-badge]: <https://colab.research.google.com/assets/colab-badge.svg>\n\nThis will require that you have an Nvidia GPU or AMD GPU\n- Recommended: 16GB VRAM\n- Minimum Requirements: 4GB VRAM (Using VERY LOW settings, see usage instructions below) \n\n## Install\n\n```bash\n$ pip install deep-daze\n```  \n\n### Windows Install\n\n<img src=\"./instruction_images/Windows/Step_1_DD_Win.png\" width=\"480px\"></img>\n\nPresuming Python is installed: \n- Open command prompt and navigate to the directory of your current version of Python\n```bash\n  pip install deep-daze\n```\n\n## Examples\n\n```bash\n$ imagine \"a house in the forest\"\n```\nFor Windows:\n\n<img src=\"./instruction_images/Windows/Step_2_DD_Win.png\" width=\"480px\"></img>\n\n- Open command prompt as administrator\n```bash\n  imagine \"a house in the forest\"\n```\n\nThat's it.\n\n\nIf you have enough memory, you can get better quality by adding a `--deeper` flag\n\n```bash\n$ imagine \"shattered plates on the ground\" --deeper\n```\n\n### Advanced\n\nIn true deep learning fashion, more layers will yield better results. Default is at `16`, but can be increased to `32` depending on your resources.\n\n```bash\n$ imagine \"stranger in strange lands\" --num-layers 32\n```\n\n## Usage\n\n### CLI\n```bash\nNAME\n    imagine\n\nSYNOPSIS\n    imagine TEXT <flags>\n\nPOSITIONAL ARGUMENTS\n    TEXT\n        (required) A phrase less than 77 tokens which you would like to visualize.\n\nFLAGS\n    --img=IMAGE_PATH\n        Default: None\n        Path to png/jpg image or PIL image to optimize on\n    --encoding=ENCODING\n        Default: None\n        User-created custom CLIP encoding. If used, replaces any text or image that was used.\n    --create_story=CREATE_STORY\n        Default: False\n        Creates a story by optimizing each epoch on a new sliding-window of the input words. If this is enabled, much longer texts than 77 tokens can be used. Requires save_progress to visualize the transitions of the story.\n    --story_start_words=STORY_START_WORDS\n        Default: 5\n        Only used if create_story is True. How many words to optimize on for the first epoch.\n    --story_words_per_epoch=STORY_WORDS_PER_EPOCH\n        Default: 5\n        Only used if create_story is True. How many words to add to the optimization goal per epoch after the first one.\n    --story_separator:\n        Default: None\n        Only used if create_story is True. Defines a separator like '.' that splits the text into groups for each epoch. Separator needs to be in the text otherwise it will be ignored\n    --lower_bound_cutout=LOWER_BOUND_CUTOUT\n        Default: 0.1\n        Lower bound of the sampling of the size of the random cut-out of the SIREN image per batch. Should be smaller than 0.8.\n    --upper_bound_cutout=UPPER_BOUND_CUTOUT\n        Default: 1.0\n        Upper bound of the sampling of the size of the random cut-out of the SIREN image per batch. Should probably stay at 1.0.\n    --saturate_bound=SATURATE_BOUND\n        Default: False\n        If True, the LOWER_BOUND_CUTOUT is linearly increased to 0.75 during training.\n    --learning_rate=LEARNING_RATE\n        Default: 1e-05\n        The learning rate of the neural net.\n    --num_layers=NUM_LAYERS\n        Default: 16\n        The number of hidden layers to use in the Siren neural net.\n    --batch_size=BATCH_SIZE\n        Default: 4\n        The number of generated images to pass into Siren before calculating loss. Decreasing this can lower memory and accuracy.\n    --gradient_accumulate_every=GRADIENT_ACCUMULATE_EVERY\n        Default: 4\n        Calculate a weighted loss of n samples for each iteration. Increasing this can help increase accuracy with lower batch sizes.\n    --epochs=EPOCHS\n        Default: 20\n        The number of epochs to run.\n    --iterations=ITERATIONS\n        Default: 1050\n        The number of times to calculate and backpropagate loss in a given epoch.\n    --save_every=SAVE_EVERY\n        Default: 100\n        Generate an image every time iterations is a multiple of this number.\n    --image_width=IMAGE_WIDTH\n        Default: 512\n        The desired resolution of the image.\n    --deeper=DEEPER\n        Default: False\n        Uses a Siren neural net with 32 hidden layers.\n    --overwrite=OVERWRITE\n        Default: False\n        Whether or not to overwrite existing generated images of the same name.\n    --save_progress=SAVE_PROGRESS\n        Default: False\n        Whether or not to save images generated before training Siren is complete.\n    --seed=SEED\n        Type: Optional[]\n        Default: None\n        A seed to be used for deterministic runs.\n    --open_folder=OPEN_FOLDER\n        Default: True\n        Whether or not to open a folder showing your generated images.\n    --save_date_time=SAVE_DATE_TIME\n        Default: False\n        Save files with a timestamp prepended e.g. `%y%m%d-%H%M%S-my_phrase_here`\n    --start_image_path=START_IMAGE_PATH\n        Default: None\n        The generator is trained first on a starting image before steered towards the textual input\n    --start_image_train_iters=START_IMAGE_TRAIN_ITERS\n        Default: 50\n        The number of steps for the initial training on the starting image\n    --theta_initial=THETA_INITIAL\n        Default: 30.0\n        Hyperparameter describing the frequency of the color space. Only applies to the first layer of the network.\n    --theta_hidden=THETA_INITIAL\n        Default: 30.0\n        Hyperparameter describing the frequency of the color space. Only applies to the hidden layers of the network.\n    --save_gif=SAVE_GIF\n        Default: False\n        Whether or not to save a GIF animation of the generation procedure. Only works if save_progress is set to True.\n```\n\n### Priming\n\nTechnique first devised and shared by <a href=\"https://twitter.com/quasimondo\">Mario Klingemann</a>, it allows you to prime the generator network with a starting image, before being steered towards the text.\n\nSimply specify the path to the image you wish to use, and optionally the number of initial training steps.\n\n```bash\n$ imagine 'a clear night sky filled with stars' --start_image_path ./cloudy-night-sky.jpg\n```\n\nPrimed starting image\n\n<img src=\"./samples/prime-orig.jpg\" width=\"256px\"></img>\n\nThen trained with the prompt `A pizza with green pepper.`\n\n<img src=\"./samples/prime-trained.png\" width=\"256px\"></img>\n\n\n### Optimize for the interpretation of an image\n\nWe can also feed in an image as an optimization goal, instead of only priming the generator network. Deepdaze will then render its own interpretation of that image:\n```bash\n$ imagine --img samples/Autumn_1875_Frederic_Edwin_Church.jpg\n```\nOriginal image:\n\n<img src=\"./samples/Autumn_1875_Frederic_Edwin_Church_original.jpg\" width=\"256px\"></img>\n\nThe network's interpretation:  \n\n<img src=\"./samples/Autumn_1875_Frederic_Edwin_Church.jpg\" width=\"256px\"></img>\n\nOriginal image:\n\n<img src=\"./samples/hot-dog.jpg\" width=\"256px\"></img>\n\nThe network's interpretation:  \n\n<img src=\"./samples/hot-dog_imagined.png\" width=\"256px\"></img>\n\n#### Optimize for text and image combined\n\n```bash\n$ imagine \"A psychedelic experience.\" --img samples/hot-dog.jpg\n```\nThe network's interpretation:  \n<img src=\"./samples/psychedelic_hot_dog.png\" width=\"256px\"></img>\n\n\n### New: Create a story\nThe regular mode for texts only allows 77 tokens. If you want to visualize a full story/paragraph/song/poem, set `create_story` to `True`.\n\nGiven the poem “Stopping by Woods On a Snowy Evening” by Robert Frost - \n\"Whose woods these are I think I know. His house is in the village though; He will not see me stopping here To watch his woods fill up with snow. My little horse must think it queer To stop without a farmhouse near Between the woods and frozen lake The darkest evening of the year. He gives his harness bells a shake To ask if there is some mistake. The only other sound’s the sweep Of easy wind and downy flake. The woods are lovely, dark and deep, But I have promises to keep, And miles to go before I sleep, And miles to go before I sleep.\".\n\nWe get:\n\nhttps://user-images.githubusercontent.com/19983153/109539633-d671ef80-7ac1-11eb-8d8c-380332d7c868.mp4\n\n\n\n### Python\n#### Invoke `deep_daze.Imagine` in Python\n```python\nfrom deep_daze import Imagine\n\nimagine = Imagine(\n    text = 'cosmic love and attention',\n    num_layers = 24,\n)\nimagine()\n```\n\n#### Save progress every fourth iteration\nSave images in the format insert_text_here.00001.png, insert_text_here.00002.png, ...up to `(total_iterations % save_every)`\n```python\nimagine = Imagine(\n    text=text,\n    save_every=4,\n    save_progress=True\n)\n```\n\n#### Prepend current timestamp on each image.\nCreates files with both the timestamp and the sequence number.\n\ne.g. 210129-043928_328751_insert_text_here.00001.png, 210129-043928_512351_insert_text_here.00002.png, ...\n```python\nimagine = Imagine(\n    text=text,\n    save_every=4,\n    save_progress=True,\n    save_date_time=True,\n)\n```\n\n#### High GPU memory usage\nIf you have at least 16 GiB of vram available, you should be able to run these settings with some wiggle room.\n```python\nimagine = Imagine(\n    text=text,\n    num_layers=42,\n    batch_size=64,\n    gradient_accumulate_every=1,\n)\n```\n\n#### Average GPU memory usage\n```python\nimagine = Imagine(\n    text=text,\n    num_layers=24,\n    batch_size=16,\n    gradient_accumulate_every=2\n)\n```\n\n#### Very low GPU memory usage (less than 4 GiB)\nIf you are desperate to run this on a card with less than 8 GiB vram, you can lower the image_width.\n```python\nimagine = Imagine(\n    text=text,\n    image_width=256,\n    num_layers=16,\n    batch_size=1,\n    gradient_accumulate_every=16 # Increase gradient_accumulate_every to correct for loss in low batch sizes\n)\n```\n\n### VRAM and speed benchmarks:\nThese experiments were conducted with a 2060 Super RTX and a 3700X Ryzen 5. We first mention the parameters (bs = batch size), then the memory usage and in some cases the training iterations per second:\n\nFor an image resolution of 512: \n* bs 1,  num_layers 22: 7.96 GB\n* bs 2,  num_layers 20: 7.5 GB\n* bs 16, num_layers 16: 6.5 GB\n\nFor an image resolution of 256:\n* bs 8, num_layers 48: 5.3 GB\n* bs 16, num_layers 48: 5.46 GB - 2.0 it/s\n* bs 32, num_layers 48: 5.92 GB - 1.67 it/s\n* bs 8, num_layers 44: 5 GB - 2.39 it/s\n* bs 32, num_layers 44, grad_acc 1: 5.62 GB - 4.83 it/s\n* bs 96, num_layers 44, grad_acc 1: 7.51 GB - 2.77 it/s\n* bs 32, num_layers 66, grad_acc 1: 7.09 GB - 3.7 it/s\n    \n@NotNANtoN recommends a batch size of 32 with 44 layers and training 1-8 epochs.\n\n\n## Where is this going?\n\nThis is just a teaser. We will be able to generate images, sound, anything at will, with natural language. The holodeck is about to become real in our lifetimes.\n\nPlease join replication efforts for DALL-E for <a href=\"https://github.com/lucidrains/dalle-pytorch\">Pytorch</a> or <a href=\"https://github.com/EleutherAI/DALLE-mtf\">Mesh Tensorflow</a> if you are interested in furthering this technology.\n\n## Alternatives\n\n<a href=\"https://github.com/lucidrains/big-sleep\">Big Sleep</a> - CLIP and the generator from Big GAN\n\n## Citations\n\n```bibtex\n@misc{unpublished2021clip,\n    title  = {CLIP: Connecting Text and Images},\n    author = {Alec Radford, Ilya Sutskever, Jong Wook Kim, Gretchen Krueger, Sandhini Agarwal},\n    year   = {2021}\n}\n```\n\n```bibtex\n@misc{sitzmann2020implicit,\n    title   = {Implicit Neural Representations with Periodic Activation Functions},\n    author  = {Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},\n    year    = {2020},\n    eprint  = {2006.09661},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n[colab-notebook]: <https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP>\n"
        },
        {
          "name": "deep_daze",
          "type": "tree",
          "content": null
        },
        {
          "name": "instruction_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.0947265625,
          "content": "import sys\nfrom setuptools import setup, find_packages\n\nsys.path[0:0] = ['deep_daze']\nfrom version import __version__\n\nsetup(\n  name = 'deep-daze',\n  packages = find_packages(),\n  include_package_data = True,\n  entry_points={\n    'console_scripts': [\n      'imagine = deep_daze.cli:main',\n    ],\n  },\n  version = __version__,\n  license='MIT',\n  description = 'Deep Daze',\n  author = 'Ryan Murdock, Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  url = 'https://github.com/lucidrains/deep-daze',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'implicit neural representations',\n    'text to image'\n  ],\n  install_requires=[\n    'einops>=0.3',\n    'fire',\n    'ftfy',\n    'imageio>=2.9.0',\n    'siren-pytorch>=0.0.8',\n    'torch>=1.10',\n    'torch_optimizer',\n    'torchvision>=0.8.2',\n    'tqdm',\n    'regex'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)\n"
        }
      ]
    }
  ]
}