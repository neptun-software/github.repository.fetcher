{
  "metadata": {
    "timestamp": 1736559741211,
    "page": 444,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "junyanz/iGAN",
      "stars": 3982,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1015625,
          "content": "# Byte-compiled / optimized / DLL files\nunused/\ndatasets/\nmodels/\n__pycache__/\n*.py[cod]\n*$py.class\n*.hdf5\n# C extensions\n*.so\ncache/\n# added\n*.dcgan_theano\n*.pkl\n.idea/\nweb/\n\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\n.venv/\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0517578125,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2016 Jun-Yan Zhu\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.9111328125,
          "content": "## iGAN: Interactive Image Generation via Generative Adversarial Networks\n[Project](http://efrosgans.eecs.berkeley.edu/iGAN/) |  [Youtube](https://youtu.be/9c4z6YsBGQ0) |  [Paper](https://arxiv.org/abs/1609.03552)  \n\nRecent projects:  \n[[pix2pix]](https://github.com/phillipi/pix2pix): Torch implementation for learning a mapping from input images to output images.  \n[[CycleGAN]](https://github.com/junyanz/CycleGAN): Torch implementation for learning an image-to-image translation (i.e., pix2pix) without input-output pairs.  \n[[pytorch-CycleGAN-and-pix2pix]](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix): PyTorch implementation for both unpaired and paired image-to-image translation.\n\n\n<img src='pics/demo.gif' width=320>\n\n## Overview\niGAN (aka. interactive GAN) is the author's implementation of interactive image generation interface described in:  \n\"Generative Visual Manipulation on the Natural Image Manifold\"   \n[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/), [Philipp Krähenbühl](http://www.philkr.net/), [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros/)    \nIn European Conference on Computer Vision (ECCV) 2016\n\n<img src='pics/demo_teaser.jpg' width=800>\n\n\nGiven a few user strokes, our system could produce photo-realistic samples that best satisfy the user edits in real-time. Our system is based on deep generative models such as Generative Adversarial Networks ([GAN](https://arxiv.org/abs/1406.2661)) and [DCGAN](https://github.com/Newmu/dcgan_code). The system serves the following two purposes:\n* An intelligent drawing interface for automatically generating images inspired by the color and shape of the brush strokes.\n* An interactive visual debugging tool for understanding and visualizing deep generative models. By interacting with the generative model, a developer can understand what visual content the model can produce, as well as the limitation of the model.\n\nPlease cite our paper if you find this code useful in your research. (Contact: Jun-Yan Zhu, junyanz at mit dot edu)\n\n## Getting started\n* Install the python libraries. (See [Requirements](https://github.com/junyanz/iGAN#requirements)).\n* Download the code from GitHub:\n```bash\ngit clone https://github.com/junyanz/iGAN\ncd iGAN\n```\n* Download the model. (See `Model Zoo` for details):\n``` bash\nbash ./models/scripts/download_dcgan_model.sh outdoor_64\n```\n\n* Run the python script:\n``` bash\nTHEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python iGAN_main.py --model_name outdoor_64\n```\n\n## Requirements\nThe code is written in Python2 and requires the following 3rd party libraries:\n* numpy\n* [OpenCV](http://opencv.org/)\n```bash\nsudo apt-get install python-opencv\n```\n* [Theano](https://github.com/Theano/Theano)\n```bash\nsudo pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n```\n* [PyQt4](https://wiki.python.org/moin/PyQt4): more details on Qt installation can be found [here](http://www.saltycrane.com/blog/2008/01/how-to-install-pyqt4-on-ubuntu-linux/)\n```bash\nsudo apt-get install python-qt4\n```\n* [Qdarkstyle](https://github.com/ColinDuquesnoy/QDarkStyleSheet)\n```bash\nsudo pip install qdarkstyle\n```\n* [dominate](https://github.com/Knio/dominate)\n```bash\nsudo pip install dominate\n```\n* GPU + CUDA + cuDNN:\nThe code is tested on GTX Titan X + CUDA 7.5 + cuDNN 5.  Here are the tutorials on how to install [CUDA](http://www.r-tutor.com/gpu-computing/cuda-installation/cuda7.5-ubuntu) and [cuDNN](http://askubuntu.com/questions/767269/how-can-i-install-cudnn-on-ubuntu-16-04). A decent GPU is required to run the system in real-time. [**Warning**] If you run the program on a GPU server, you need to use remote desktop software (e.g., VNC), which may introduce display artifacts and latency problem.\n\n## Python3\nFor `Python3` users, you need to replace `pip` with `pip3`:\n* PyQt4 with Python3:\n``` bash\nsudo apt-get install python3-pyqt4\n```\n* OpenCV3 with Python3: see the installation [instruction](http://www.pyimagesearch.com/2015/07/20/install-opencv-3-0-and-python-3-4-on-ubuntu/).\n\n\n## Interface:\nSee [[Youtube]](https://youtu.be/9c4z6YsBGQ0?t=2m18s) at 2:18s for the interactive image generation demos.  \n\n<img src='pics/ui_intro.jpg' width=800>  \n\n#### Layout\n* Drawing Pad: This is the main window of our interface. A user can apply different edits via our brush tools, and the system will display the generated image. Check/Uncheck `Edits` button to display/hide user edits.  \n* Candidate Results: a display showing thumbnails of all the candidate results (e.g., different modes) that fits the user edits. A user can click a mode (highlighted by a green rectangle), and the drawing pad will show this result.\n* Brush Tools:  `Coloring Brush` for changing the color of a specific region; `Sketching brush` for outlining the shape. `Warping brush` for modifying the shape more explicitly.\n* Slider Bar: drag the slider bar to explore the interpolation sequence between the initial result (i.e., randomly generated image) and the current result (e.g., image that satisfies the user edits).\n* Control Panel: `Play`: play the interpolation sequence; `Fix`: use the current result as additional constraints for further editing  `Restart`: restart the system; `Save`: save the result to a webpage. `Edits`: Check the box if you would like to show the edits on top of the generated image.\n\n\n#### User interaction\n* `Coloring Brush`:  right-click to select a color; hold left click to paint; scroll the mouse wheel to adjust the width of the brush.\n* `Sketching Brush`: hold left-click to sketch the shape.\n* `Warping Brush`: We recommend you first use coloring and sketching before the warping brush. Right-click to select a square region; hold left click to drag the region; scroll the mouse wheel to adjust the size of the square region.\n* Shortcuts: P for `Play`, F for `Fix`, R for `Restart`; S for `Save`; E for `Edits`; Q for quitting the program.\n* Tooltips: when you move the cursor over a button, the system will display the tooltip of the button.\n\n\n## Model Zoo:\nDownload the Theano DCGAN model (e.g., outdoor_64). Before using our system, please check out the random real images vs. DCGAN generated samples to see which kind of images that a model can produce.\n\n``` bash\nbash ./models/scripts/download_dcgan_model.sh outdoor_64\n```\n* [ourdoor_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/outdoor_64.dcgan_theano) (64x64): trained on 150K landscape images from MIT [Places](http://places.csail.mit.edu/) dataset [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/outdoor_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/outdoor_64_dcgan.png)].\n* [church_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/church_64.dcgan_theano) (64x64): trained on 126k church images from the [LSUN](http://lsun.cs.princeton.edu/2016/) challenge [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/church_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/church_64_dcgan.png)].\n* [handbag_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/handbag_64.dcgan_theano) (64x64): trained on 137K handbag images downloaded from Amazon [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/handbag_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/handbag_64_dcgan.png)].\n* [shoes_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/shoes_64.dcgan_theano) (64x64): trained on 50K shoes images collected by [Yu and Grauman](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/) [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/shoes_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/shoes_64_dcgan.png)].\n* [hed_shoes_64.dcgan_theano](http://efrosgans.eecs.berkeley.edu/iGAN/models/theano_dcgan/hed_shoes_64.dcgan_theano) (64x64): trained on 50K shoes sketches (computed by [HED](https://github.com/s9xie/hed)) [[Real](http://efrosgans.eecs.berkeley.edu/iGAN/samples/hed_shoes_64_real.png) vs. [DCGAN](http://efrosgans.eecs.berkeley.edu/iGAN/samples/hed_shoes_64_dcgan.png)]. (Use this model with `--shadow` flag)\n\nWe provide a simple script to generate samples from a pre-trained DCGAN model. You can run this script to test if Theano, CUDA, cuDNN are configured properly before running our interface.\n```bash\nTHEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python generate_samples.py --model_name outdoor_64 --output_image outdoor_64_dcgan.png\n```\n\n\n## Command line arguments:\nType `python iGAN_main.py --help` for a complete list of the arguments. Here we discuss some important arguments:\n* `--model_name`: the name of the model (e.g., outdoor_64, shoes_64, etc.)\n* `--model_type`: currently only supports dcgan_theano.\n* `--model_file`: the file that stores the generative model; If not specified, `model_file='./models/%s.%s' % (model_name, model_type)`\n* `--top_k`: the number of the candidate results being displayed\n* `--average`: show an average image in the main window. Inspired by [AverageExplorer](https://www.cs.cmu.edu/~junyanz/projects/averageExplorer/), average image is a weighted average of multiple generated results, with the weights reflecting user-indicated importance. You can switch between average mode and normal mode by press `A`.\n* `--shadow`: We build a sketching assistance system for guiding the freeform drawing of objects inspired by [ShadowDraw](http://vision.cs.utexas.edu/projects/shadowdraw/shadowdraw.html)\nTo use the interface, download the model `hed_shoes_64` and run the following script\n```bash\nTHEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python iGAN_main.py --model_name hed_shoes_64 --shadow --average\n```\n\n## Dataset and Training\nSee more details [here](./train_dcgan/README.md)\n\n## Projecting an Image onto Latent Space\n<img src='pics/predict.jpg' width=800>\n\nWe provide a script to project an image into latent space (i.e., `x->z`):\n* Download the pre-trained AlexNet model (`conv4`):\n```bash\nbash models/scripts/download_alexnet.sh conv4\n```\n* Run the following script with a model and an input image. (e.g., model: `shoes_64.dcgan_theano`, and input image `./pics/shoes_test.png`)\n```bash\nTHEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python iGAN_predict.py --model_name shoes_64 --input_image ./pics/shoes_test.png --solver cnn_opt\n```\n* Check the result saved in `./pics/shoes_test_cnn_opt.png`\n* We provide three methods: `opt` for optimization method; `cnn` for feed-forward network method (fastest); `cnn_opt` hybrid of the previous methods (default and best). Type `python iGAN_predict.py --help` for a complete list of the arguments.\n\n## Script without UI\n<img src='pics/script_result.png' width=1000>\n\nWe also provide a standalone script that should work without UI. Given user constraints (i.e., a color map, a color mask, and an edge map), the script generates multiple images that mostly satisfy the user constraints. See `python iGAN_script.py --help` for more details.\n```bash\nTHEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python iGAN_script.py --model_name outdoor_64\n```\n\n\n## Citation\n```\n@inproceedings{zhu2016generative,\n  title={Generative Visual Manipulation on the Natural Image Manifold},\n  author={Zhu, Jun-Yan and Kr{\\\"a}henb{\\\"u}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},\n  booktitle={Proceedings of European Conference on Computer Vision (ECCV)},\n  year={2016}\n}\n```\n\n## Cat Paper Collection\nIf you love cats, and love reading cool graphics, vision, and learning papers, please check out our Cat Paper Collection:  \n[[Github]](https://github.com/junyanz/CatPapers) [[Webpage]](https://www.cs.cmu.edu/~junyanz/cat/cat_papers.html)\n\n## Acknowledgement\n* We modified the DCGAN [code](https://github.com/Newmu/dcgan_code) in our package. Please cite the original [DCGAN](https://arxiv.org/abs/1511.06434) paper if you use their models.\n* This work was supported, in part, by funding from Adobe, eBay, and Intel, as well as a hardware grant from NVIDIA. J.-Y. Zhu is supported by Facebook Graduate Fellowship.\n"
        },
        {
          "name": "constrained_opt.py",
          "type": "blob",
          "size": 9.337890625,
          "content": "from __future__ import print_function\nfrom time import time\nfrom lib.rng import np_rng\nimport numpy as np\nimport sys\nfrom lib import utils\nfrom PyQt4.QtCore import *\n\n\nclass Constrained_OPT(QThread):\n    def __init__(self, opt_solver, batch_size=32, n_iters=25, topK=16, morph_steps=16, interp='linear'):\n        QThread.__init__(self)\n        self.nz = 100\n        self.opt_solver = opt_solver\n        self.topK = topK\n        self.max_iters = n_iters\n        self.fixed_iters = 150  # [hack] after 150 iterations, do not change the order of the results\n        self.batch_size = batch_size\n        self.morph_steps = morph_steps  # number of intermediate frames\n        self.interp = interp  # interpolation method\n        # data\n        self.z_seq = None     # sequence of latent vector\n        self.img_seq = None   # sequence of images\n        self.im0 = None       # initial image\n        self.z0 = None        # initial latent vector\n        self.prev_z = self.z0  # previous latent vector\n        # constraints\n        self.constraints = None\n        # current frames\n        self.current_ims = None   # the images being displayed now\n        self.iter_count = 0\n        self.iter_total = 0\n        self.to_update = False\n        self.to_set_constraints = False\n        self.order = None\n        self.init_constraints()  # initialize\n        self.init_z()            # initialize latent vectors\n        self.just_fixed = True\n        self.weights = None\n\n    def is_fixed(self):\n        return self.just_fixed\n\n    def update_fix(self):\n        self.just_fixed = False\n\n    def init_z(self, frame_id=-1, image_id=-1):\n        nz = self.nz\n        n_sigma = 0.5\n        self.iter_total = 0\n        # set prev_z\n        if self.z_seq is not None and image_id >= 0:\n            image_id = image_id % self.z_seq.shape[0]\n            frame_id = frame_id % self.z_seq.shape[1]\n            print('set z as image %d, frame %d' % (image_id, frame_id))\n            self.prev_z = self.z_seq[image_id, frame_id]\n\n        if self.prev_z is None:  # random initialization\n            self.z_init = np_rng.uniform(-1.0, 1.0, size=(self.batch_size, nz))\n            self.opt_solver.set_smoothness(0.0)\n            self.z_const = self.z_init\n            self.prev_zs = self.z_init\n        else:  # add small noise to initial latent vector, so that we can get different results\n            z0_r = np.tile(self.prev_z, [self.batch_size, 1])\n            z0_n = np_rng.uniform(-1.0, 1.0, size=(self.batch_size, nz)) * n_sigma\n            self.z_init = np.clip(z0_r + z0_n, -0.99, 0.99)\n            self.opt_solver.set_smoothness(5.0)\n            self.z_const = np.tile(self.prev_z, [self.batch_size, 1])\n            self.prev_zs = z0_r\n\n        self.opt_solver.initialize(self.z_init)\n        self.just_fixed = True\n\n    def update(self):   # update ui\n        self.to_update = True\n        self.to_set_constraints = True\n        self.iter_count = 0\n        self.img_seq = None\n\n    def save_constraints(self):\n        [im_c, mask_c, im_e, mask_e] = self.combine_constraints(self.constraints)\n        self.prev_im_c = im_c.copy()\n        self.prev_mask_c = mask_c.copy()\n        self.prev_im_e = im_e.copy()\n        self.prev_mask_e = mask_e.copy()\n\n    def init_constraints(self):\n        self.prev_im_c = None\n        self.prev_mask_c = None\n        self.prev_im_e = None\n        self.prev_mask_e = None\n\n    def combine_constraints(self, constraints):\n        if constraints is not None:  # [hack]\n            # print('combine strokes')\n            [im_c, mask_c, im_e, mask_e] = constraints\n            if self.prev_im_c is None:\n                mask_c_f = mask_c\n            else:\n                mask_c_f = np.maximum(self.prev_mask_c, mask_c)\n\n            if self.prev_im_e is None:\n                mask_e_f = mask_e\n            else:\n                mask_e_f = np.maximum(self.prev_mask_e, mask_e)\n\n            if self.prev_im_c is None:\n                im_c_f = im_c\n            else:\n                im_c_f = self.prev_im_c.copy()\n                mask_c3 = np.tile(mask_c, [1, 1, im_c.shape[2]])\n                np.copyto(im_c_f, im_c, where=mask_c3.astype(np.bool))  # [hack]\n\n            if self.prev_im_e is None:\n                im_e_f = im_e\n            else:\n                im_e_f = self.prev_im_e.copy()\n                mask_e3 = np.tile(mask_e, [1, 1, im_e.shape[2]])\n                np.copyto(im_e_f, im_e, where=mask_e3.astype(np.bool))\n\n            return [im_c_f, mask_c_f, im_e_f, mask_e_f]\n        else:\n            return [self.prev_im_c, self.prev_mask_c, self.prev_im_e, self.prev_mask_e]\n\n    def set_constraints(self, constraints):\n        self.constraints = constraints\n\n    def get_z(self, image_id, frame_id):\n        if self.z_seq is not None:\n            image_id = image_id % self.z_seq.shape[0]\n            frame_id = frame_id % self.z_seq.shape[1]\n            return self.z_seq[image_id, frame_id]\n        else:\n            return None\n\n    def get_image(self, image_id, frame_id, useAverage=False):\n        if self.to_update:\n            if self.current_ims is None or self.current_ims.size == 0:\n                return None\n            else:\n                image_id = image_id % self.current_ims.shape[0]\n                if useAverage and self.weights is not None:\n                    return utils.average_image(self.current_ims, self.weights)  # get averages\n                else:\n                    return self.current_ims[image_id]\n        else:\n            if self.img_seq is None:\n                return None\n            else:\n                frame_id = frame_id % self.img_seq.shape[1]\n                image_id = image_id % self.img_seq.shape[0]\n                if useAverage and self.weights is not None:\n                    return utils.average_image(self.img_seq[:, frame_id, ...], self.weights)\n                else:\n                    return self.img_seq[image_id, frame_id]\n\n    def get_images(self, frame_id):\n        if self.to_update:\n            return self.current_ims\n        else:\n            if self.img_seq is None:\n                return None\n            else:\n                frame_id = frame_id % self.img_seq.shape[1]\n                return self.img_seq[:, frame_id]\n\n    def get_num_images(self):\n        if self.img_seq is None:\n            return 0\n        else:\n            return self.img_seq.shape[0]\n\n    def get_num_frames(self):\n        if self.img_seq is None:\n            return 0\n        else:\n            return self.img_seq.shape[1]\n\n    def get_current_results(self):\n        return self.current_ims\n\n    def run(self):  # main function\n        time_to_wait = 33  # 33 millisecond\n        while (1):\n            t1 = time()\n            if self.to_set_constraints:  # update constraints\n                self.to_set_constraints = False\n\n            if self.constraints is not None and self.iter_count < self.max_iters:\n                self.update_invert(constraints=self.constraints)\n                self.iter_count += 1\n                self.iter_total += 1\n\n            if self.iter_count == self.max_iters:\n                self.gen_morphing(self.interp, self.morph_steps)\n                self.to_update = False\n                self.iter_count += 1\n\n            t_c = int(1000 * (time() - t1))\n            print('update one iteration: %03d ms' % t_c, end='\\r')\n            sys.stdout.flush()\n            if t_c < time_to_wait:\n                self.msleep(time_to_wait - t_c)\n\n    def update_invert(self, constraints):\n        constraints_c = self.combine_constraints(constraints)\n        gx_t, z_t, cost_all = self.opt_solver.invert(constraints_c, self.z_const)\n\n        order = np.argsort(cost_all)\n\n        if self.topK > 1:\n            cost_sort = cost_all[order]\n            thres_top = 2 * np.mean(cost_sort[0:min(int(self.topK / 2.0), len(cost_sort))])\n            ids = cost_sort - thres_top < 1e-10\n            topK = np.min([self.topK, sum(ids)])\n        else:\n            topK = self.topK\n\n        order = order[0:topK]\n\n        if self.iter_total < self.fixed_iters:\n            self.order = order\n        else:\n            order = self.order\n        self.current_ims = gx_t[order]\n        # compute weights\n        cost_weights = cost_all[order]\n        self.weights = np.exp(-(cost_weights - np.mean(cost_weights)) / (np.std(cost_weights) + 1e-10))\n        self.current_zs = z_t[order]\n        self.emit(SIGNAL('update_image'))\n\n    def gen_morphing(self, interp='linear', n_steps=8):\n        if self.current_ims is None:\n            return\n\n        z1 = self.prev_zs[self.order]\n        z2 = self.current_zs\n        t = time()\n        img_seq = []\n        z_seq = []\n\n        for n in range(n_steps):\n            ratio = n / float(n_steps - 1)\n            z_t = utils.interp_z(z1, z2, ratio, interp=interp)\n            seq = self.opt_solver.gen_samples(z0=z_t)\n            img_seq.append(seq[:, np.newaxis, ...])\n            z_seq.append(z_t[:, np.newaxis, ...])\n        self.img_seq = np.concatenate(img_seq, axis=1)\n        self.z_seq = np.concatenate(z_seq, axis=1)\n        print('generate morphing sequence (%.3f seconds)' % (time() - t))\n\n    def reset(self):\n        self.prev_z = self.z0\n        self.init_z()\n        self.init_constraints()\n        self.just_fixed = True\n        self.z_seq = None\n        self.img_seq = None\n        self.constraints = None\n        self.current_ims = None\n        self.to_update = False\n        self.order = None\n        self.to_set_constraints = False\n        self.iter_total = 0\n        self.iter_count = 0\n        self.weights = None\n"
        },
        {
          "name": "constrained_opt_theano.py",
          "type": "blob",
          "size": 4.6630859375,
          "content": "import theano\nimport theano.tensor as T\nfrom time import time\nfrom lib import updates, HOGNet\nfrom lib.rng import np_rng\nfrom lib.theano_utils import floatX, sharedX\nimport numpy as np\n\n\nclass OPT_Solver():\n    def __init__(self, model, batch_size=32, d_weight=0.0):\n        self.model = model\n        self.npx = model.npx\n        self.nc = model.nc\n        self.nz = model.nz\n        self.model_name = model.model_name\n        self.transform = model.transform\n        self.transform_mask = model.transform_mask\n        self.inverse_transform = model.inverse_transform\n        BS = 4 if self.nc == 1 else 8  # [hack]\n        self.hog = HOGNet.HOGNet(use_bin=True, NO=8, BS=BS, nc=self.nc)\n        self.opt_model = self.def_invert(model, batch_size=batch_size, d_weight=d_weight, nc=self.nc)\n        self.batch_size = batch_size\n\n    def get_image_size(self):\n        return self.npx\n\n    def invert(self, constraints, z_i):\n        [_invert, z_updates, z, beta_r, z_const] = self.opt_model\n        constraints_t = self.preprocess_constraints(constraints)\n        [im_c_t, mask_c_t, im_e_t, mask_e_t] = constraints_t  # [im_c_t, mask_c_t, im_e_t, mask_e_t]\n\n        results = _invert(im_c_t, mask_c_t, im_e_t, mask_e_t, z_i.astype(np.float32))\n\n        [gx, cost, cost_all, rec_all, real_all, init_all, sum_e, sum_x_edge] = results\n        gx_t = (255 * self.inverse_transform(gx, npx=self.npx, nc=self.nc)).astype(np.uint8)\n        if self.nc == 1:\n            gx_t = np.tile(gx_t, (1, 1, 1, 3))\n        z_t = np.tanh(z.get_value()).copy()\n        return gx_t, z_t, cost_all\n\n    def preprocess_constraints(self, constraints):\n        [im_c_o, mask_c_o, im_e_o, mask_e_o] = constraints\n        im_c = self.transform(im_c_o[np.newaxis, :], self.nc)\n        mask_c = self.transform_mask(mask_c_o[np.newaxis, :])\n        im_e = self.transform(im_e_o[np.newaxis, :], self.nc)\n        mask_t = self.transform_mask(mask_e_o[np.newaxis, :])\n        mask_e = self.hog.comp_mask(mask_t)\n        shp = [self.batch_size, 1, 1, 1]\n        im_c_t = np.tile(im_c, shp)\n        mask_c_t = np.tile(mask_c, shp)\n        im_e_t = np.tile(im_e, shp)\n        mask_e_t = np.tile(mask_e, shp)\n        return [im_c_t, mask_c_t, im_e_t, mask_e_t]\n\n    def initialize(self, z0):\n        z = self.opt_model[2]\n        z.set_value(floatX(np.arctanh(z0)))\n\n    def set_smoothness(self, l):\n        print('set z const = 0')\n        z_const = self.opt_model[-1]\n        z_const.set_value(floatX(l))\n\n    def gen_samples(self, z0):\n        samples = self.model.gen_samples(z0=z0)\n        if self.nc == 1:\n            samples = np.tile(samples, [1, 1, 1, 3])\n        return samples\n\n    def def_invert(self, model, batch_size=1, d_weight=0.5, nc=1, lr=0.1, b1=0.9, nz=100, use_bin=True):\n        d_weight_r = sharedX(d_weight)\n        x_c = T.tensor4()\n        m_c = T.tensor4()\n        x_e = T.tensor4()\n        m_e = T.tensor4()\n        z0 = T.matrix()\n        z = sharedX(floatX(np_rng.uniform(-1., 1., size=(batch_size, nz))))\n        gx = model.model_G(z)\n        # input: im_c: 255: no edge; 0: edge; transform=> 1: no edge, 0: edge\n\n        if nc == 1:  # gx, range [0, 1] => edge, 1\n            gx3 = 1.0 - gx  # T.tile(gx, (1, 3, 1, 1))\n        else:\n            gx3 = gx\n        mm_c = T.tile(m_c, (1, gx3.shape[1], 1, 1))\n        color_all = T.mean(T.sqr(gx3 - x_c) * mm_c, axis=(1, 2, 3)) / (T.mean(m_c, axis=(1, 2, 3)) + sharedX(1e-5))\n        gx_edge = self.hog.get_hog(gx3)\n        x_edge = self.hog.get_hog(x_e)\n        mm_e = T.tile(m_e, (1, gx_edge.shape[1], 1, 1))\n        sum_e = T.sum(T.abs_(mm_e))\n        sum_x_edge = T.sum(T.abs_(x_edge))\n        edge_all = T.mean(T.sqr(x_edge - gx_edge) * mm_e, axis=(1, 2, 3)) / (T.mean(m_e, axis=(1, 2, 3)) + sharedX(1e-5))\n        rec_all = color_all + edge_all * sharedX(0.2)\n        z_const = sharedX(5.0)\n        init_all = T.mean(T.sqr(z0 - z)) * z_const\n\n        if d_weight > 0:\n            print('using D')\n            p_gen = model.model_D(gx)\n            real_all = T.nnet.binary_crossentropy(p_gen, T.ones(p_gen.shape)).T\n            cost_all = rec_all + d_weight_r * real_all[0] + init_all\n        else:\n            print('without D')\n            cost_all = rec_all + init_all\n            real_all = T.zeros(cost_all.shape)\n\n        cost = T.sum(cost_all)\n        d_updater = updates.Adam(lr=sharedX(lr), b1=sharedX(b1))\n        output = [gx, cost, cost_all, rec_all, real_all, init_all, sum_e, sum_x_edge]\n\n        print('COMPILING...')\n        t = time()\n\n        z_updates = d_updater([z], cost)\n        _invert = theano.function(inputs=[x_c, m_c, x_e, m_e, z0], outputs=output, updates=z_updates)\n        print('%.2f seconds to compile _invert function' % (time() - t))\n        return [_invert, z_updates, z, d_weight_r, z_const]\n"
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "generate_samples.py",
          "type": "blob",
          "size": 1.853515625,
          "content": "from __future__ import print_function\nimport argparse\nfrom pydoc import locate\nfrom lib import utils\nimport cv2\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='generated random samples (dcgan_theano)')\n    parser.add_argument('--model_name', dest='model_name', help='the model name', default='outdoor_64', type=str)\n    parser.add_argument('--model_type', dest='model_type', help='the generative models and its deep learning framework', default='dcgan_theano', type=str)\n    parser.add_argument('--framework', dest='framework', help='deep learning framework', default='theano')\n    parser.add_argument('--model_file', dest='model_file', help='the file that stores the generative model', type=str, default=None)\n    parser.add_argument('--output_image', dest='output_image', help='the name of output image', type=str, default=None)\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    if not args.model_file:  # if model directory is not specified\n        args.model_file = './models/%s.%s' % (args.model_name, args.model_type)\n\n    if not args.output_image:\n        args.output_image = '%s_%s_samples.png' % (args.model_name, args.model_type)\n\n    for arg in vars(args):\n        print('[%s] =' % arg, getattr(args, arg))\n\n    # initialize model and constrained optimization problem\n    model_class = locate('model_def.%s' % args.model_type)\n    model = model_class.Model(model_name=args.model_name, model_file=args.model_file)\n    # generate samples\n    samples = model.gen_samples(z0=None, n=196, batch_size=49, use_transform=True)\n    # generate grid visualization\n    im_vis = utils.grid_vis(samples, 14, 14)\n    # write to the disk\n    im_vis = cv2.cvtColor(im_vis, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(args.output_image, im_vis)\n    print('samples_shape', samples.shape)\n    print('save image to %s' % args.output_image)\n"
        },
        {
          "name": "iGAN_main.py",
          "type": "blob",
          "size": 3.611328125,
          "content": "from __future__ import print_function\nimport sys\nimport argparse\nimport qdarkstyle\nfrom PyQt4.QtGui import QApplication, QIcon\nfrom PyQt4.QtCore import Qt\nfrom ui import gui_design\nfrom pydoc import locate\nimport constrained_opt\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='iGAN: Interactive Visual Synthesis Powered by GAN')\n    parser.add_argument('--model_name', dest='model_name', help='the model name', default='outdoor_64', type=str)\n    parser.add_argument('--model_type', dest='model_type', help='the generative models and its deep learning framework', default='dcgan_theano', type=str)\n    parser.add_argument('--framework', dest='framework', help='deep learning framework', default='theano')\n    parser.add_argument('--win_size', dest='win_size', help='the size of the main window', type=int, default=384)\n    parser.add_argument('--batch_size', dest='batch_size', help='the number of random initializations', type=int, default=64)\n    parser.add_argument('--n_iters', dest='n_iters', help='the number of total optimization iterations', type=int, default=40)\n    parser.add_argument('--top_k', dest='top_k', help='the number of the thumbnail results being displayed', type=int, default=16)\n    parser.add_argument('--morph_steps', dest='morph_steps', help='the number of intermediate frames of morphing sequence', type=int, default=16)\n    parser.add_argument('--model_file', dest='model_file', help='the file that stores the generative model', type=str, default=None)\n    parser.add_argument('--d_weight', dest='d_weight', help='captures the visual realism based on GAN discriminator', type=float, default=0.0)\n    parser.add_argument('--interp', dest='interp', help='the interpolation method (linear or slerp)', type=str, default='linear')\n    parser.add_argument('--average', dest='average', help='averageExplorer mode', action=\"store_true\", default=False)\n    parser.add_argument('--shadow', dest='shadow', help='shadowDraw mode', action=\"store_true\", default=False)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    if not args.model_file:  # if the model_file is not specified\n        args.model_file = './models/%s.%s' % (args.model_name, args.model_type)\n\n    for arg in vars(args):\n        print('[%s] =' % arg, getattr(args, arg))\n\n    args.win_size = int(args.win_size / 4.0) * 4  # make sure the width of the image can be divided by 4\n\n    # initialize model and constrained optimization problem\n    model_class = locate('model_def.%s' % args.model_type)\n    model = model_class.Model(model_name=args.model_name, model_file=args.model_file)\n    opt_class = locate('constrained_opt_%s' % args.framework)\n    opt_solver = opt_class.OPT_Solver(model, batch_size=args.batch_size, d_weight=args.d_weight)\n    img_size = opt_solver.get_image_size()\n    opt_engine = constrained_opt.Constrained_OPT(opt_solver, batch_size=args.batch_size, n_iters=args.n_iters, topK=args.top_k,\n                                                 morph_steps=args.morph_steps, interp=args.interp)\n\n    # initialize application\n    app = QApplication(sys.argv)\n    window = gui_design.GUIDesign(opt_engine, win_size=args.win_size, img_size=img_size, topK=args.top_k,\n                                  model_name=args.model_name, useAverage=args.average, shadow=args.shadow)\n    app.setStyleSheet(qdarkstyle.load_stylesheet(pyside=False))  # comment this if you do not like dark stylesheet\n    app.setWindowIcon(QIcon('pics/logo.png'))  # load logo\n    window.setWindowTitle('Interactive GAN')\n    window.setWindowFlags(window.windowFlags() & ~Qt.WindowMaximizeButtonHint)   # fix window siz\n    window.show()\n    app.exec_()\n"
        },
        {
          "name": "iGAN_predict.py",
          "type": "blob",
          "size": 7.4970703125,
          "content": "from __future__ import print_function\n\nimport theano\nimport theano.tensor as T\nfrom time import time\nfrom lib import HOGNet\nfrom lib.rng import np_rng\nfrom lib.theano_utils import floatX, sharedX\nimport numpy as np\n\nfrom lib import AlexNet\nimport lasagne\nfrom scipy import optimize\nimport argparse\nfrom PIL import Image\nfrom pydoc import locate\nfrom lib import activations\n\n\ndef def_feature(layer='conv4', up_scale=4):\n    print('COMPILING...')\n    t = time()\n    x = T.tensor4()\n    x_t = AlexNet.transform_im(x)\n    x_net = AlexNet.build_model(x_t, layer=layer, shape=(None, 3, 64, 64), up_scale=up_scale)\n    AlexNet.load_model(x_net, layer=layer)\n    x_f = lasagne.layers.get_output(x_net[layer], deterministic=True)\n    _ftr = theano.function(inputs=[x], outputs=x_f)\n    print('%.2f seconds to compile _feature function' % (time() - t))\n    return _ftr\n\n\ndef def_bfgs(model_G, layer='conv4', npx=64, alpha=0.002):\n    print('COMPILING...')\n    t = time()\n\n    x_f = T.tensor4()\n    x = T.tensor4()\n    z = T.matrix()\n    tanh = activations.Tanh()\n    gx = model_G(tanh(z))\n\n    if layer is 'hog':\n        gx_f = HOGNet.get_hog(gx, use_bin=True, BS=4)\n    else:\n        gx_t = AlexNet.transform_im(gx)\n        gx_net = AlexNet.build_model(gx_t, layer=layer, shape=(None, 3, npx, npx))\n        AlexNet.load_model(gx_net, layer=layer)\n        gx_f = lasagne.layers.get_output(gx_net[layer], deterministic=True)\n\n    f_rec = T.mean(T.sqr(x_f - gx_f), axis=(1, 2, 3)) * sharedX(alpha)\n    x_rec = T.mean(T.sqr(x - gx), axis=(1, 2, 3))\n    cost = T.sum(f_rec) + T.sum(x_rec)\n    grad = T.grad(cost, z)\n    output = [cost, grad, gx]\n    _invert = theano.function(inputs=[z, x, x_f], outputs=output)\n\n    print('%.2f seconds to compile _bfgs function' % (time() - t))\n    return _invert, z\n\n\ndef def_predict(model_P):\n    print('COMPILING...')\n    t = time()\n    x = T.tensor4()\n    z = model_P(x)\n    _predict = theano.function([x], [z])\n    print('%.2f seconds to compile _predict function' % (time() - t))\n    return _predict\n\n\ndef def_invert_models(gen_model, layer='conv4', alpha=0.002):\n    bfgs_model = def_bfgs(gen_model.model_G, layer=layer, npx=gen_model.npx, alpha=alpha)\n    ftr_model = def_feature(layer=layer)\n    predict_model = def_predict(gen_model.model_P)\n    return gen_model, bfgs_model, ftr_model, predict_model\n\n\ndef predict_z(gen_model, _predict, ims, batch_size=32):\n    n = ims.shape[0]\n    n_gen = 0\n    zs = []\n    n_batch = int(np.ceil(n / float(batch_size)))\n    for i in range(n_batch):\n        imb = gen_model.transform(ims[batch_size * i:min(n, batch_size * (i + 1)), :, :, :])\n        zmb = _predict(imb)\n        zs.append(zmb)\n        n_gen += len(imb)\n    zs = np.squeeze(np.concatenate(zs, axis=0))\n    if np.ndim(zs) == 1:\n        zs = zs[np.newaxis, :]\n\n    return zs\n\n\ndef invert_bfgs_batch(gen_model, invert_model, ftr_model, ims, z_predict=None, npx=64):\n    zs = []\n    recs = []\n    fs = []\n    n_imgs = ims.shape[0]\n    print('reconstruct %d images using bfgs' % n_imgs)\n\n    for n in range(n_imgs):\n        im_n = ims[[n], :, :, :]\n        if z_predict is not None:\n            z0_n = z_predict[[n], ...]\n        else:\n            z0_n = None\n        gx, z_value, f_value = invert_bfgs(gen_model, invert_model, ftr_model, im=im_n, z_predict=z0_n, npx=npx)\n        rec_im = (gx * 255).astype(np.uint8)\n        fs.append(f_value[np.newaxis, ...])\n        zs.append(z_value[np.newaxis, ...])\n        recs.append(rec_im)\n    recs = np.concatenate(recs, axis=0)\n    zs = np.concatenate(zs, axis=0)\n    fs = np.concatenate(fs, axis=0)\n    return recs, zs, fs\n\n\ndef invert_bfgs(gen_model, invert_model, ftr_model, im, z_predict=None, npx=64):\n    _f, z = invert_model\n    nz = gen_model.nz\n    if z_predict is None:\n        z_predict = np_rng.uniform(-1., 1., size=(1, nz))\n    else:\n        z_predict = floatX(z_predict)\n    z_predict = np.arctanh(z_predict)\n    im_t = gen_model.transform(im)\n    ftr = ftr_model(im_t)\n\n    prob = optimize.minimize(f_bfgs, z_predict, args=(_f, im_t, ftr),\n                             tol=1e-6, jac=True, method='L-BFGS-B', options={'maxiter': 200})\n    print('n_iters = %3d, f = %.3f' % (prob.nit, prob.fun))\n    z_opt = prob.x\n    z_opt_n = floatX(z_opt[np.newaxis, :])\n    [f_opt, g, gx] = _f(z_opt_n, im_t, ftr)\n    gx = gen_model.inverse_transform(gx, npx=npx)\n    z_opt = np.tanh(z_opt)\n    return gx, z_opt, f_opt\n\n\ndef f_bfgs(z0, _f, x, x_f):\n    z0_n = floatX(z0[np.newaxis, :])\n    [f, g, gx] = _f(z0_n, x, x_f)\n    f = f.astype(np.float64)\n    g = g[0].astype(np.float64)\n    return f, g\n\n\ndef invert_images_CNN_opt(invert_models, ims, solver='cnn'):\n    gen_model, invert_model, ftr_model, predict_model = invert_models\n    n_imgs = len(ims)\n    print('process %d images' % n_imgs)\n    # gen_samples(self, z0=None, n=32, batch_size=32, use_transform=True)\n    if solver == 'cnn' or solver == 'cnn_opt':\n        z_predict = predict_z(gen_model, predict_model, ims, batch_size=n_imgs)\n    else:\n        z_predict = None\n\n    if solver == 'cnn':\n        recs = gen_model.gen_samples(z0=z_predict, n=n_imgs, batch_size=n_imgs)\n        zs = None\n\n    if solver == 'cnn_opt' or solver == 'opt':\n        recs, zs, loss = invert_bfgs_batch(gen_model, invert_model, ftr_model, ims, z_predict=z_predict, npx=npx)\n\n    return recs, zs, z_predict\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='iGAN: Interactive Visual Synthesis Powered by GAN')\n    parser.add_argument('--model_name', dest='model_name', help='the model name', default='shoes_64', type=str)\n    parser.add_argument('--model_type', dest='model_type', help='the generative models and its deep learning framework', default='dcgan_theano', type=str)\n    parser.add_argument('--input_image', dest='input_image', help='input image', default='./pics/shoes_test.png', type=str)\n    parser.add_argument('--output_image', dest='output_image', help='output reconstruction image', default=None, type=str)\n    parser.add_argument('--model_file', dest='model_file', help='the file that stores the generative model', type=str, default=None)\n    # cnn: feed-forward network; opt: optimization based; cnn_opt: hybrid of the two methods\n    parser.add_argument('--solver', dest='solver', help='solver (cnn, opt, or cnn_opt)', type=str, default='cnn_opt')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    if not args.model_file:  # if the model file is not specified\n        args.model_file = './models/%s.%s' % (args.model_name, args.model_type)\n    if not args.output_image:  # if the output image path is not specified\n        args.output_image = args.input_image.replace('.png', '_%s.png' % args.solver)\n\n    for arg in vars(args):\n        print('[%s] =' % arg, getattr(args, arg))\n\n    # read a single image\n    im = Image.open(args.input_image)\n    [h, w] = im.size\n    print('read image: %s (%dx%d)' % (args.input_image, h, w))\n    # define the theano models\n    model_class = locate('model_def.%s' % args.model_type)\n    gen_model = model_class.Model(model_name=args.model_name, model_file=args.model_file, use_predict=True)\n    invert_models = def_invert_models(gen_model, layer='conv4', alpha=0.002)\n    # pre-processing steps\n    npx = gen_model.npx\n    im = im.resize((npx, npx))\n    im = np.array(im)\n    im_pre = im[np.newaxis, :, :, :]\n    # run the model\n    rec, _, _ = invert_images_CNN_opt(invert_models, im_pre, solver=args.solver)\n    rec = np.squeeze(rec)\n    rec_im = Image.fromarray(rec)\n    # resize the image (input aspect ratio)\n    rec_im = rec_im.resize((h, w))\n    print('write result to %s' % args.output_image)\n    rec_im.save(args.output_image)\n"
        },
        {
          "name": "iGAN_script.py",
          "type": "blob",
          "size": 3.6416015625,
          "content": "from __future__ import print_function\nimport argparse\nfrom pydoc import locate\nimport constrained_opt\nimport cv2\nimport numpy as np\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='iGAN: Interactive Visual Synthesis Powered by GAN')\n    parser.add_argument('--model_name', dest='model_name', help='the model name', default='outdoor_64', type=str)\n    parser.add_argument('--model_type', dest='model_type', help='the generative models and its deep learning framework', default='dcgan_theano', type=str)\n    parser.add_argument('--framework', dest='framework', help='deep learning framework', default='theano')\n    parser.add_argument('--input_color', dest='input_color', help='input color image', default='./pics/input_color.png')\n    parser.add_argument('--input_color_mask', dest='input_color_mask', help='input color mask', default='./pics/input_color_mask.png')\n    parser.add_argument('--input_edge', dest='input_edge', help='input edge image', default='./pics/input_edge.png')\n    parser.add_argument('--output_result', dest='output_result', help='output_result', default='./pics/script_result.png')\n    parser.add_argument('--batch_size', dest='batch_size', help='the number of random initializations', type=int, default=64)\n    parser.add_argument('--n_iters', dest='n_iters', help='the number of total optimization iterations', type=int, default=100)\n    parser.add_argument('--top_k', dest='top_k', help='the number of the thumbnail results being displayed', type=int, default=16)\n    parser.add_argument('--model_file', dest='model_file', help='the file that stores the generative model', type=str, default=None)\n    parser.add_argument('--d_weight', dest='d_weight', help='captures the visual realism based on GAN discriminator', type=float, default=0.0)\n    args = parser.parse_args()\n    return args\n\n\ndef preprocess_image(img_path, npx):\n    im = cv2.imread(img_path, 1)\n    if im.shape[0] != npx or im.shape[1] != npx:\n        out = cv2.resize(im, (npx, npx))\n    else:\n        out = np.copy(im)\n\n    out = cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n    return out\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    if not args.model_file:  # if the model_file is not specified\n        args.model_file = './models/%s.%s' % (args.model_name, args.model_type)\n\n    for arg in vars(args):\n        print('[%s] =' % arg, getattr(args, arg))\n\n    # initialize model and constrained optimization problem\n    model_class = locate('model_def.%s' % args.model_type)\n    model = model_class.Model(model_name=args.model_name, model_file=args.model_file)\n    opt_class = locate('constrained_opt_%s' % args.framework)\n    opt_solver = opt_class.OPT_Solver(model, batch_size=args.batch_size, d_weight=args.d_weight)\n    img_size = opt_solver.get_image_size()\n    opt_engine = constrained_opt.Constrained_OPT(opt_solver, batch_size=args.batch_size, n_iters=args.n_iters, topK=args.top_k)\n    # load user inputs\n    npx = model.npx\n    im_color = preprocess_image(args.input_color, npx)\n    im_color_mask = preprocess_image(args.input_color_mask, npx)\n    im_edge = preprocess_image(args.input_edge, npx)\n    # run the optimization\n    opt_engine.init_z()\n    constraints = [im_color, im_color_mask[..., [0]], im_edge, im_edge[..., [0]]]\n    for n in range(args.n_iters):\n        opt_engine.update_invert(constraints=constraints)\n    results = opt_engine.get_current_results()\n    final_result = np.concatenate(results, 1)\n    # combine input and output\n    final_vis = np. hstack([im_color, im_color_mask, im_edge, final_result])\n    final_vis = cv2.cvtColor(final_vis, cv2.COLOR_RGB2BGR)\n    final_vis = cv2.resize(final_vis, (0, 0), fx=2.0, fy=2.0)\n    # save\n    cv2.imwrite(args.output_result, final_vis)\n"
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "model_def",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        },
        {
          "name": "train_dcgan",
          "type": "tree",
          "content": null
        },
        {
          "name": "ui",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}