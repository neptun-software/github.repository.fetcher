{
  "metadata": {
    "timestamp": 1736559930700,
    "page": 699,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "KoboldAI/KoboldAI-Client",
      "stars": 3557,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.1025390625,
          "content": "*.min.lua linguist-vendored\n*documentation.html linguist-vendored\n/static/swagger-ui/* linguist-vendored\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.5859375,
          "content": "# Ignore client settings file\nsettings\n\n# Ignore stories file except for test_story\nstories\n!stories/sample_story.json\n\n# Ignore stuff that would polute our Git\n/.project\n*.bak\nminiconda3\nruntime\nbin\n*.settings\n__pycache__\n*.log\ncache\naccelerate-disk-cache\nuserscripts\n!userscripts/examples\n!userscripts/kaipreset_*.lua\n!userscripts/Readme.*\n!userscripts/api_documentation.*\nsoftprompts\nmodels\n!models/models go here.txt\nUninstall\nflask_session\naccelerate-disk-cache\n.ipynb_checkpoints\n\n# Ignore PyCharm project files.\n.idea\n\n# Ignore compiled Python files.\n*.pyc\n\n# Don't ignore defaults\n!defaults/*"
        },
        {
          "name": "Jupyter.bat",
          "type": "blob",
          "size": 0.6953125,
          "content": "@echo off\ncd %~dp0\nTITLE Jupyter for KoboldAI Runtime\nSET /P M=<loader.settings\nIF %M%==1 GOTO drivemap\nIF %M%==2 GOTO subfolder\nIF %M%==3 GOTO drivemap_B\n\n:subfolder\numamba.exe install --no-shortcuts -r miniconda3 -n base -c conda-forge jupyterlab jupyterlab-git\ncall miniconda3\\condabin\\activate\njupyter-lab\ncmd /k\n\n:drivemap\nsubst K: miniconda3 >nul\numamba.exe install --no-shortcuts -r K:\\python\\ -n base -c conda-forge jupyterlab jupyterlab-git\ncall K:\\python\\condabin\\activate\njupyter-lab\nsubst K: /D\ncmd /k\n\n:drivemap_B\nsubst B: miniconda3 >nul\numamba.exe install --no-shortcuts -r B:\\python\\ -n base -c conda-forge jupyterlab jupyterlab-git\ncall B:\\python\\condabin\\activate\njupyter-lab\nsubst B: /D\ncmd /k"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 33.7138671875,
          "content": "                    GNU AFFERO GENERAL PUBLIC LICENSE\n                       Version 3, 19 November 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU Affero General Public License is a free, copyleft license for\nsoftware and other kinds of works, specifically designed to ensure\ncooperation with the community in the case of network server software.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nour General Public Licenses are intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  Developers that use our General Public Licenses protect your rights\nwith two steps: (1) assert copyright on the software, and (2) offer\nyou this License which gives you legal permission to copy, distribute\nand/or modify the software.\n\n  A secondary benefit of defending all users' freedom is that\nimprovements made in alternate versions of the program, if they\nreceive widespread use, become available for other developers to\nincorporate.  Many developers of free software are heartened and\nencouraged by the resulting cooperation.  However, in the case of\nsoftware used on network servers, this result may fail to come about.\nThe GNU General Public License permits making a modified version and\nletting the public access it on a server without ever releasing its\nsource code to the public.\n\n  The GNU Affero General Public License is designed specifically to\nensure that, in such cases, the modified source code becomes available\nto the community.  It requires the operator of a network server to\nprovide the source code of the modified version running there to the\nusers of that server.  Therefore, public use of a modified version, on\na publicly accessible server, gives the public access to the source\ncode of the modified version.\n\n  An older license, called the Affero General Public License and\npublished by Affero, was designed to accomplish similar goals.  This is\na different license, not a version of the Affero GPL, but Affero has\nreleased a new version of the Affero GPL which permits relicensing under\nthis license.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU Affero General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Remote Network Interaction; Use with the GNU General Public License.\n\n  Notwithstanding any other provision of this License, if you modify the\nProgram, your modified version must prominently offer all users\ninteracting with it remotely through a computer network (if your version\nsupports such interaction) an opportunity to receive the Corresponding\nSource of your version by providing access to the Corresponding Source\nfrom a network server at no charge, through some standard or customary\nmeans of facilitating copying of software.  This Corresponding Source\nshall include the Corresponding Source for any work covered by version 3\nof the GNU General Public License that is incorporated pursuant to the\nfollowing paragraph.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the work with which it is combined will remain governed by version\n3 of the GNU General Public License.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU Affero General Public License from time to time.  Such new versions\nwill be similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU Affero General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU Affero General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU Affero General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the program's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as published\n    by the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If your software can interact with users remotely through a computer\nnetwork, you should also make sure that it provides a way for users to\nget its source.  For example, if your program is a web application, its\ninterface could display a \"Source\" link that leads users to an archive\nof the code.  There are many ways you could offer source, and different\nsolutions will be better for different programs; see section 13 for the\nspecific requirements.\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU AGPL, see\n<https://www.gnu.org/licenses/>.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 36.5576171875,
          "content": "## Looking for KoboldCpp?\nKoboldAI is named after the KoboldAI software, currently our newer most popular program is KoboldCpp.\n\nIf you are reading this message you are on the page of the original KoboldAI sofware.\n\nPrefer using KoboldCpp with GGUF models and the latest API features? You can visit https://koboldai.org/cpp\n\nNeed support for newer models such as Llama based models using the Huggingface / Exllama (safetensors/pytorch) platforms? Check out KoboldAI's development version KoboldAI United at https://koboldai.org/united\n\n## KoboldAI - Your gateway to GPT writing\n\nThis is a browser-based front-end for AI-assisted writing with multiple local & remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save & Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed.\n\n## Multiple ways to play\n\nStories can be played like a Novel, a text adventure game or used as a chatbot with an easy toggles to change between the multiple gameplay styles. This makes KoboldAI both a writing assistant, a game and a platform for so much more. The way you play and how good the AI will be depends on the model or service you decide to use. No matter if you want to use the free, fast power of Google Colab, your own high end graphics card, an online service you have an API key for (Like OpenAI or Inferkit) or if you rather just run it slower on your CPU you will be able to find a way to use KoboldAI that works for you.\n\n### Adventure mode\n\nBy default KoboldAI will run in a generic mode optimized for writing, but with the right model you can play this like AI Dungeon without any issues. You can enable this in the settings and bring your own prompt, try generating a random prompt or download one of the prompts available at [/aids/ Prompts](https://aetherroom.club/).\n\nThe gameplay will be slightly different than the gameplay in AI Dungeon because we adopted the Type of the Unleashed fork, giving you full control over all the characters because we do not automatically adapt your sentences behind the scenes. This means you can more reliably control characters that are not you.\n\nAs a result of this what you need to type is slightly different, in AI Dungeon you would type _**take the sword**_ while in KoboldAI you would type it like a sentence such as _**You take the sword**_ and this is best done with the word You instead of I.\n\nTo speak simply type : _You say \"We should probably gather some supplies first\"_  \nJust typing the quote might work, but the AI is at its best when you specify who does what in your commands.\n\nIf you want to do this with your friends we advise using the main character as You and using the other characters by their name if you are playing on a model trained for Adventures. These models assume there is a You in the story. This mode does usually not perform well on Novel models because they do not know how to handle the input those are best used with regular story writing where you take turns with the AI.\n\n### Writing assistant\n\nIf you want to use KoboldAI as a writing assistant this is best done in the regular mode with a model optimized for Novels. These models do not make the assumption that there is a You character and focus on Novel like writing. For writing these will often give you better results than Adventure or Generic models. That said, if you give it a good introduction to the story large generic models like 13B can be used if a more specific model is not available for what you wish to write. You can also try to use models that are not specific to what you wish to do, for example a NSFW Novel model for a SFW story if a SFW model is unavailable. This will mean you will have to correct the model more often because of its bias, but can still produce good enough results if it is familiar enough with your topic.\n\n### Chatbot Mode\n\nIn chatbot mode you can use a suitable model as a chatbot, this mode automatically adds your name to the beginning of the sentences and prevents the AI from talking as you. To use it properly you must write your story opening as both characters in the following format (You can use your own text) :\n\n```plaintext\nBot : Hey!\nYou : Hey Boyname, how have you been?\nBot : Been good! How about you?\nYou : Been great to, excited to try out KoboldAI\nBot : KoboldAI is really fun!\nYou : For sure! What is your favorite game?\n```\n\nIts recommended to have your own input be the last input, especially in the beginning its possible that the AI mixes up the names. In that case either retry or manually correct the name. This behavior improves as the chat progresses. Some models may swap names if they are more familiar with a different name that is similar to the name you defined for the bot. In that case you can either do the occasional manual correction or choose a name for your chatbot that the AI likes better.\n\nThis mode works the best on either a Generic model or a chatbot model specifically designed for it, some models like the AvrilAI model are instead designed to be used in Adventure mode and do not conform to the format above. These models typically ship with adventure mode enabled by default and should not be switched over to chatbot mode.\n\nNovel or Adventure models are not recommended for this feature but might still work but can derail away from the conversation format quickly.\n\n## Play KoboldAI online for free on Google Colab (The easiest way to play)\n\nIf you would like to play KoboldAI online for free on a powerful computer you can use Google Colaboraty. We provide two editions, a TPU and a GPU edition with a variety of models available. These run entirely on Google's Servers and will automatically upload saves to your Google Drive if you choose to save a story (Alternatively, you can choose to download your save instead so that it never gets stored on Google Drive). Detailed instructions on how to use them are at the bottom of the Colab's.\n\nEach edition features different models and requires different hardware to run, this means that if you are unable to obtain a TPU or a GPU you might still be able to use the other version. The models you can use are listed underneath the edition. To open a Colab click the big link featuring the editions name.\n\n## [Models the TPU can run:](https://colab.research.google.com/github/KoboldAI/KoboldAI-Client/blob/main/colab/TPU.ipynb)\n\n| Model | Style | Description |\n| --- | --- | --- |\n| [Nerys](https://huggingface.co/KoboldAI/fairseq-dense-13B-Nerys) by Mr Seeker | Novel/Adventure | Nerys is a hybrid model based on Pike (A newer Janeway), on top of the Pike dataset you also get some Light Novels, Adventure mode support and a little bit of Shinen thrown in the mix. The end result is a very diverse model that is heavily biased towards SFW novel writing, but one that can go beyond its novel training and make for an excellent adventure model to. Adventure mode is best played from a second person perspective, but can be played in first or third person as well. Novel writing can be done best from the first or third person. |\n| [Erebus](https://huggingface.co/KoboldAI/OPT-13B-Erebus) by Mr Seeker | NSFW | Erebus is our community's flagship NSFW model, being a combination of multiple large datasets that include Literotica, Shinen and erotic novels from Nerys and featuring thourough tagging support it covers the vast majority of erotic writing styles. This model is capable of replacing both the Lit and Shinen models in terms of content and style and has been well received as (one of) the best NSFW models out there. If you wish to use this model for commercial or non research usage we recommend choosing the 20B version as that one is not subject to the restrictive OPT license. |\n| [Janeway](https://huggingface.co/KoboldAI/fairseq-dense-13B-Janeway) by Mr Seeker | Novel | Janeway is a model created from Picard's dataset combined with a brand new collection of ebooks. This model is trained on 20% more content than Picard and has been trained on literature from various genres. Although the model is mainly focussed on SFW, romantic scenes might involve a degree of nudity. |\n| [Shinen](https://huggingface.co/KoboldAI/fairseq-dense-13B-Shinen) by Mr Seeker | NSFW | Shinen is an NSFW model trained on a variety of stories from the website Sexstories it contains many different kinks. It has been merged into the larger (and better) Erebus model. |\n| [Skein](https://huggingface.co/KoboldAI/GPT-J-6B-Skein) by VE\\_FORBRYDERNE | Adventure | Skein is best used with Adventure mode enabled, it consists of a 4 times larger adventure dataset than the Adventure model making it excellent for text adventure gaming. On top of that it also consists of light novel training further expanding its knowledge and writing capabilities. It can be used with the You filter bias if you wish to write Novels with it, but dedicated Novel models can perform better for this task. |\n| [Adventure](https://huggingface.co/KoboldAI/GPT-J-6B-Adventure) by VE\\_FORBRYDERNE | Adventure | Adventure is a 6B model designed to mimick the behavior of AI Dungeon. It is exclusively for Adventure Mode and can take you on the epic and wackey adventures that AI Dungeon players love. It also features the many tropes of AI Dungeon as it has been trained on very similar data. It must be used in second person (You). |\n| [Lit](https://huggingface.co/hakurei/lit-6B) ([V2](https://huggingface.co/hakurei/litv2-6B-rev3)) by Haru | NSFW | Lit is a great NSFW model trained by Haru on both a large set of Literotica stories and high quality novels along with tagging support. Creating a high quality model for your NSFW stories. This model is exclusively a novel model and is best used in third person. |\n| [OPT](https://huggingface.co/facebook/opt-13b) by Metaseq | Generic | OPT is considered one of the best base models as far as content goes, its behavior has the strengths of both GPT-Neo and Fairseq Dense. Compared to Neo duplicate and unnecessary content has been left out, while additional literature was added in similar to the Fairseq Dense model. The Fairseq Dense model however lacks the broader data that OPT does have. The biggest downfall of OPT is its license, which prohibits any commercial usage, or usage beyond research purposes. |\n| [Neo(X)](https://huggingface.co/EleutherAI/gpt-neox-20b) by EleutherAI | Generic | NeoX is the largest EleutherAI model currently available, being a generic model it is not particularly trained towards anything and can do a variety of writing, Q&A and coding tasks. 20B's performance is closely compared to the 13B models and it is worth trying both especially if you have a task that does not involve english writing. Its behavior will be similar to the GPT-J-6B model since they are trained on the same dataset but with more sensitivity towards repetition penalty and with more knowledge. |\n| [Fairseq Dense](https://huggingface.co/KoboldAI/fairseq-dense-13B) | Generic | Trained by Facebook Researchers this model stems from the MOE research project within Fairseq. This particular version has been converted by us for use in KoboldAI. It is known to be on par with the larger 20B model from EleutherAI and considered as better for pop culture and language tasks. Because the model has never seen a new line (enter) it may perform worse on formatting and paragraphing. Compared to other models the dataset focuses primarily on literature and contains little else. |\n| [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) by EleutherAI | Generic | This model serves as the basis for most other 6B models (Some being based on Fairseq Dense instead). Being trained on the Pile and not biased towards anything in particular it is suitable for a variety of tasks such as writing, Q&A and coding tasks. You will likely get better result with larger generic models or finetuned models. |\n\n## [Models the Colab GPU can run:](https://colab.research.google.com/github/KoboldAI/KoboldAI-Client/blob/main/colab/GPU.ipynb)\n\n| Model | Style | Description |\n| --- | --- | --- |\n| [Nerys](https://huggingface.co/KoboldAI/fairseq-dense-2.7B-Nerys) by Mr Seeker | Novel/Adventure | Nerys is a hybrid model based on Pike (A newer Janeway), on top of the Pike dataset you also get some Light Novels, Adventure mode support and a little bit of Shinen thrown in the mix. The end result is a very diverse model that is heavily biased towards SFW novel writing, but one that can go beyond its novel training and make for an excellent adventure model to. Adventure mode is best played from a second person perspective, but can be played in first or third person as well. Novel writing can be done best from the first or third person. |\n| [Tiefighter 13B by KoboldAI](https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter) | Hybrid | Tiefighter 13B is a very versitile fiction Hybrid, it can write, chat and play adventure games and can also answer regular instructions (Although we do not recommend this model for factual use due to its fictional nature). This is an excellent starting model, for the best results avoid using Second person writing in your chats unless you are wanting it to become a text adventure.|\n| [Janeway](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Janeway) by Mr Seeker | Novel | Janeway is a model created from Picard's dataset combined with a brand new collection of ebooks. This model is trained on 20% more content than Picard and has been trained on literature from various genres. Although the model is mainly focussed on SFW, romantic scenes might involve a degree of nudity. |\n| [Picard](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Picard) by Mr Seeker | Novel | Picard is a model trained for SFW Novels based on Neo 2.7B. It is focused on Novel style writing without the NSFW bias. While the name suggests a sci-fi model this model is designed for Novels of a variety of genre's. It is meant to be used in KoboldAI's regular mode. |\n| [AID](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-AID) by melastacho | Adventure | Also know as Adventure 2.7B this is a clone of the AI Dungeon Classic model and is best known for the epic wackey adventures that AI Dungeon Classic players love. |\n| [OPT](https://huggingface.co/facebook/opt-2.7b) by Metaseq | Generic | OPT is considered one of the best base models as far as content goes, its behavior has the strengths of both GPT-Neo and Fairseq Dense. Compared to Neo duplicate and unnecessary content has been left out, while additional literature was added in similar to the Fairseq Dense model. The Fairseq Dense model however lacks the broader data that OPT does have. The biggest downfall of OPT is its license, which prohibits any commercial usage, or usage beyond research purposes. |\n| [Fairseq Dense](https://huggingface.co/KoboldAI/fairseq-dense-2.7B) | Generic | Trained by Facebook Researchers this model stems from the MOE research project within Fairseq. This particular version has been converted by us for use in KoboldAI. It is known to be on par with the larger models from EleutherAI and considered as better for pop culture and language tasks. Because the model has never seen a new line (enter) it may perform worse on formatting and paragraphing. Compared to other models the dataset focuses primarily on literature and contains little else. |\n| [MythoMax 13B](https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ) by Gryphe | Roleplay | An improved, potentially even perfected variant of MythoMix, my MythoLogic-L2 and Huginn merge using a highly experimental tensor type merge technique¹. |\n| [Holomax 13B by KoboldAI](https://huggingface.co/KoboldAI/LLaMA2-13B-Holomax) | Adventure | This is an expansion merge to the well-praised MythoMax model from Gryphe (60%) using MrSeeker's KoboldAI Holodeck model (40%). The goal of this model is to enhance story-writing capabilities while preserving the desirable traits of the MythoMax model as much as possible (It does limit chat reply length). |\n| [Airoboros 13B](https://huggingface.co/jondurbin/airoboros-13b) by Jon Durbin | Generic | This is an instruction fine-tuned llama-2 model, using synthetic instructions generated by airoboros⁵. |\n| [Emerhyst 13B](https://huggingface.co/Undi95/Emerhyst-13B) by Undi | Roleplay | An attempt using BlockMerge_Gradient to get better result. In addition, LimaRP v3 was used⁷. |\n| [Chronos 13B](https://huggingface.co/elinas/chronos-13b) by Elinas | Generic | This model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding. Chronos generates very long outputs with coherent text, largely due to the human inputs it was trained on. |\n| [Spring Dragon by Henk717](https://huggingface.co/Henk717/spring-dragon) | Adventure | This model is a recreation attempt of the AI Dungeon 2 Dragon model. To achieve this, the \"text_adventures.txt\" dataset was used, which was bundled with the original AI Dungeon 2 GitHub release prior to the online service. It is worth noting that the same dataset file was used to create the Dragon model, where Dragon is a GPT-3 175B Davinci model from 2020. |\n| [Holodeck By KoboldAI](https://huggingface.co/KoboldAI/LLAMA2-13B-Holodeck-1) | Adventure |LLAMA2 13B-Holodeck is a finetune created using Meta's llama 2 model.The training data contains around 3000 ebooks in various genres. Most parts of the dataset have been prepended using the following text: [Genre: <genre1>, <genre2>|\n| [Neo](https://huggingface.co/EleutherAI/gpt-neo-2.7B) by EleutherAI | Generic | This is the base model for all the other 2.7B models, it is best used when you have a use case that we have no other models available for, such as writing blog articles or programming. It can also be a good basis for the experience of some of the softprompts if your softprompt is not about a subject the other models cover. |\n| [Various 2.7b models]() by various | Various smaller models are also possible to load in GPU colab. | |\n### Styles\n\n| Type | Description |\n| --- | --- |\n| Novel | For regular story writing, not compatible with Adventure mode or other specialty modes. |\n| NSFW | Indicates that the model is strongly biased towards NSFW content and is not suitable for children, work environments or livestreaming. Most NSFW models are also Novel models in nature. |\n| Adventure | These models are excellent for people willing to play KoboldAI like a Text Adventure game and are meant to be used with Adventure mode enabled. Even if you wish to use it as a Novel Type model you should always have Adventure mode on and set it to story. These models typically have a strong bias towards the use of the word You and without Adventure mode enabled break the story flow and write actions on your behalf. |\n| Hybrid | Hybrid models are a blend between different Types, for example they are trained on both Novel stories and Adventure stories. These models are great variety models that you can use for multiple different playTypes and modes, but depending on your usage you may need to enable Adventure Mode or the You bias (in userscripts). |\n| Generic | Generic models are not trained towards anything specific, typically used as a basis for other tasks and models. They can do everything the other models can do, but require much more handholding to work properly. Generic models are an ideal basis for tasks that we have no specific model for, or for experiencing a softprompt in its raw form. |\n\n## Tips to get the most out of Google Colab\n\n*   Google will occationally show a Captcha, typically after it has been open for 30 minutes but it can be more frequent if you often use Colab. Make sure to do these properly, or you risk getting your instance shut down and getting a lower priority towards the TPU's.\n*   KoboldAI uses Google Drive to store your files and settings, if you wish to upload a softprompt or userscript this can be done directly on the Google Drive website. You can also use this to download backups of your KoboldAI related files or upload models of your own.\n*   Don't want to save your stories on Google Drive for privacy reasons? Do not use KoboldAI's save function and instead click Download as .json, this will automatically download the story to your own computer without ever touching Google's harddrives. You can load this back trough the Load from file option.\n*   Google shut your instance down unexpectedly? You can still make use of the Download as .json button to recover your story as long as you did not close the KoboldAI window. You can then load this back up in your next session.\n*   Done with KoboldAI? Go to the Runtime menu, click on Manage Sessions and terminate your open sessions that you no longer need. This trick can help you maintain higher priority towards getting a TPU.\n*   Models stored on Google Drive typically load faster than models we need to download from the internet.\n\n## Install KoboldAI on your own computer\n\nKoboldAI has a large number of dependencies you will need to install on your computer, unfortunately Python does not make it easy for us to provide instructions that work for everyone. The instructions below will work on most computers, but if you have multiple versions of Python installed conflicts can occur.\n\n### Downloading the latest version of KoboldAI\n\nKoboldAI is a rolling release on our github, the code you see is also the game. You can download the software by clicking on the green Code button at the top of the page and clicking Download ZIP, or use the `git clone` command instead. Then, on Windows you need to you run install_requirements.bat (using admin mode is recommanded to avoid errors), and once it's done, or if you're on Linux, either play.bat/sh or remote-play.bat/sh to run it.\n\nThe easiest way for Windows users is to use the [offline installer](https://sourceforge.net/projects/koboldai/files/latest/download) below.\n\n### Installing KoboldAI offline bundle on Windows 7 or higher using the KoboldAI Offline Installer (Easiest)\n\n1.  [Download the latest offline installer from here](https://sourceforge.net/projects/koboldai/files/latest/download)\n2.  Run the installer to place KoboldAI on a location of choice, KoboldAI is portable software and is not bound to a specific harddrive. (Because of long paths inside our dependencies you may not be able to extract it many folders deep).\n3.  Update KoboldAI to the latest version with update-koboldai.bat if desired.\n4.  Use KoboldAI offline using play.bat or remotely with remote-play.bat\n\n### Installing KoboldAI Github release on Windows 10 or higher using the KoboldAI Runtime Installer\n\n1.  Extract the .zip to a location you wish to install KoboldAI, you will need roughly 20GB of free space for the installation (this does not include the models).\n2.  Open install\\_requirements.bat as **administrator**.\n3.  Choose the regular version of Transformers (Option 1), finetuneanon is depreciated and no longer recommended.\n4.  You will now be asked to choose the installation mode, we **strongly** recommend the Temporary B: drive option. This option eliminates most installation issues and also makes KoboldAI portable. The B: drive will be gone after a reboot and will automatically be recreated each time you play KoboldAI.\n5.  The installation will now automatically install its requirements, some stages may appear to freeze do not close the installer until it asks you to press a key. Before pressing a key to exit the installer please check if errors occurred. Most problems with the game crashing are related to installation/download errors. Disabling your antivirus can help if you get errors.\n6.  Use play.bat to start KoboldAI.\n\n### Installing KoboldAI on Linux using the KoboldAI Runtime (Easiest)\n\n1.  Clone the URL of this Github repository (For example git clone [https://github.com/koboldai/koboldai-client](https://github.com/koboldai/koboldai-client) )\n2.  AMD user? Make sure ROCm is installed if you want GPU support. Is yours not compatible with ROCm? Follow the usual instructions.\n3.  Run play.sh or if your AMD GPU supports ROCm use play-rocm.sh\n\nKoboldAI will now automatically configure its dependencies and start up, everything is contained in its own conda runtime so we will not clutter your system. The files will be located in the runtime subfolder. If at any point you wish to force a reinstallation of the runtime you can do so with the install\\_requirements.sh file. While you can run this manually it is not neccesary.\n\n### Manual installation / Mac\n\nWe can not provide a step by step guide for manual installation due to the vast differences between the existing software configuration and the systems of our users.\n\nIf you would like to manually install KoboldAI you will need some python/conda package management knowledge to manually do one of the following steps :\n\n1.  Use our bundled environments files to install your own conda environment, this should also automatically install CUDA (Recommended, you can get Miniconda from https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links). The recommended configuration is huggingface.yml for CUDA users and rocm.yml for ROCm users.\n2.  If conda is proving difficult you could also look inside requirements.txt for the required dependencies and try to install them yourself. This will likely be a mixture of pip and your native package manager, just installing our requirements.txt is not recommended since we assume local users will run conda to get all dependencies. For local installations definitely prioritize conda as that is a better way for us to enforce that you have the compatible versions.\n3.  Clone our Github or download the zip file.\n4.  Now start KoboldAI with aiserver.py and not with our play.bat or play.sh files.\n\n### AMD GPU's (Linux only)\n\nAMD GPU's have terrible compute support, this will currently not work on Windows and will only work for a select few Linux GPU's. [You can find a list of the compatible GPU's here](https://github.com/RadeonOpenCompute/ROCm#Hardware-and-Software-Support). Any GPU that is not listed is guaranteed not to work with KoboldAI and we will not be able to provide proper support on GPU's that are not compatible with the versions of ROCm we require. Make sure to first install ROCm on your Linux system using a guide for your distribution, after that you can follow the usual linux instructions above.\n\n### Troubleshooting\n\nThere are multiple things that can go wrong with the way Python handles its dependencies, unfortunately we do not have direct step by step solutions for every scenario but there are a few common solutions you can try.\n\n#### ModuleNotFoundError\n\nThis is ALWAYS either a download/installation failure or a conflict with other versions of Python. This is very common if users chose the subfolder option during the installation while putting KoboldAI in a location that has spaces in the path. When an antivirus sandboxes the installation or otherwise interferes with the downloads, systems with low disk space or when your operating system was not configured for Long FIle Paths (The installer will do this on Windows 10 and higher if you run it as administrator, anything other than Windows 10 is not supported by our installers).\n\nAnother reason the installation may have failed is if you have conflicting installations of Python on your machine, if you press the Windows Key + R and enter %appdata% in the Run Dialog it will open the folder Python installs dependencies on some systems. If you have a Python folder in this location rename this folder and try to run the installer again. It should now no longer get stuck on existing dependencies. Try the game and see if it works well. If it does you can try renaming the folder back to see if it remains functional.\n\nThe third reason the installation may have failed is if you have conda/mamba on your system for other reasons, in that case we recommend either removing your existing installations of python/conda if you do not need them and testing our installer again. Or using conda itself with our bundled environment files to let it create its runtime manually. **Keep in mind that if you go the manual route you should NEVER use play.bat but should instead run aiserver.py directly**.\n\nIn general, the less versions of Python you have on your system the higher your chances of it installing correctly. We are consistently trying to mitigate these installation conflicts in our installers but for some users we can not yet avoid all conflicts.\n\n#### GPU not found errors\n\nGPU not found errors can be caused by one of two things, either you do not have a suitable Nvidia GPU (It needs Compute Capability 5.0 or higher to be able to play KoboldAI). Your Nvidia GPU is supported by KoboldAI but is not supported by the latest version of CUDA. Your Nvidia GPU is not yet supported by the latest version of CUDA or you have a dependency conflict like the ones mentioned above.\n\nLike with Python version conflicts we recommend uninstalling CUDA from your system if you have manually installed it and do not need it for anything else and trying again. If your GPU needs CUDA10 to function open environments\\\\finetuneanon.yml and add a line that says - cudatoolkit=10.2 underneath dependencies: . After this you can run the installer again (Pick the option to delete the existing files) and it will download a CUDA10 compatible version.\n\nIf you do not have a suitable Nvidia GPU that can run on CUDA10 or Higher and that supports Compute Capabilities 5.0 or higher we can not help you get the game detected on the GPU. Unless you are following our ROCm guide with a compatible AMD GPU.\n\n#### vocab.json / config.json is not found error\n\nIf you get these errors you either did not select the correct folder for your custom model or the model you have downloaded is not (yet) compatible with KoboldAI. There exist a few models out there that are compatible and provide a pytorch\\_model.bin file but do not ship all the required files. In this case try downloading a compatible model of the same kind (For example another GPT-Neo if you downloaded a GPT-Neo model) and replace the pytorch\\_model.bin file with the one you are trying to run. Chances are this will work fine.\n\n## Softprompts\n\nSoftprompts (also known as Modules in other products) are addons that can change the output of existing models. For example you may load a softprompt that biases the AI towards a certain subject and style like transcripts from your favorite TV show.\n\nSince these softprompts are often based on existing franchises we currently do not bundle any of them with KoboldAI due to copyright concerns (We do not want to put the entire project at risk). Instead look at community resources like #softprompts on the [KoboldAI Discord](https://discord.gg/XuQWadgU9k) or the [community hosted mirror](https://storage.henk.tech/KoboldAI/softprompts/).\n\nThat way we are better protected from any DMCA claims as things can be taken down easier than directly on Github. If you have a copyright free softprompt that you made from scratch and is not based on existing IP that you would like to see officially bundled with KoboldAI issue a pull request with your softprompt.\n\nTraining softprompts can be done for free with the [Easy Softprompt Tuner](https://colab.research.google.com/gist/henk717/281fd57ebd2e88d852ef9dcc3f29bebf/easy-softprompt-tuner.ipynb#sandboxMode=true), in that case you can leave most of the settings default. Your source data needs to be a folder with text files that are UTF-8 formatted and contain Unix line endings.\n\n## Userscripts\n\nUserscripts are scripts that can automate tasks in KoboldAI, or modify the AI behavior / input / output.  \nScripting is done in LUA5.4 (Lua does not need to be separately installed as long as you got all the python requirements) and has sandboxing to help protect you from malicious behavior. Even with these measures in place we strongly advise you only run userscripts from places you trust and/or understand, otherwise consult the community for advice on how safe the script might be.\n\nInside the userscripts folder you will find our kaipreset scripts, these are default scripts that we think will be useful for our users. These scripts are automatically overwritten when you update KoboldAI, if you wish to modify these scripts make sure to first rename them to something else that does not contain kaipreset so your changes are not lost. These scripts range from a You Bias filter that prevents the AI from addressing characters as you. Ways to be able to prevent the AI from using words, word replacements and more.\n\nAlong with our preset scripts we also ship examples in the examples folder that merely serve as a demonstration and do not enhance your usage of KoboldAI. To use these scripts make sure to move them out of the examples folder before either using or modifying the script.\n\nLastly the all the features of our userscript API are documented inside the API Documentation files inside the userscripts folder.\n\nFor our TPU versions keep in mind that scripts modifying AI behavior relies on a different way of processing that is slower than if you leave these userscripts disabled even if your script only sporadically uses this modifier. If you want to partially use a script at its full speed than you can enable \"No Gen Modifiers\" to ensure that the parts that would make the TPU slow are not active.\n\n## API\n\nKoboldAI has a REST API that can be accessed by adding /api to the URL that Kobold provides you (For example http://127.0.0.1:5000/api).  \nWhen accessing this link in a browser you will be taken to the interactive documentation.\n\n## Contributors\n\nThis project contains work from the following contributors :\n\n*   The Gantian - Creator of KoboldAI, has created most features such as the interface, the different AI model / API integrations and in general the largest part of the project.\n*   VE FORBRYDERNE - Contributed many features such as the Editing overhaul, Adventure Mode, expansions to the world info section, breakmodel integration, scripting support, API, softpromtps and much more. As well as vastly improving the TPU compatibility and integrating external code into KoboldAI so we could use official versions of Transformers with virtually no downsides.\n*   Henk717 - Contributed the installation scripts, this readme, random story generator, the docker scripts, the foundation for the commandline interface and other smaller changes as well as integrating multiple parts of the code of different forks to unite it all. He also optimized the model loading so that downloaded models get converted to efficient offline models and that in future models are more likely to work out of the box. Not all code Github attributes to Henk717 is by Henk717 as some of it has been integrations of other people's work. We try to clarify this in the contributors list as much as we can.\n*   Ebolam - Automatic Saving, back/redo, pinning, web loading of models\n*   one-some, Logits Viewer and Token Streaming\n*   db0, KoboldAI Horde\n*   Frogging101 - top\\_k / tfs support (Part of this support was later redone by VE to integrate what was originally inside of finetuneanon's transformers)\n*   UWUplus (Ralf) - Contributed storage systems for community colabs, as well as cleaning up and integrating the website dependencies/code better. He is also the maintainer of flask-cloudflared which we use to generate the cloudflare links.\n*   Javalar - Initial Performance increases on the story\\_refresh\n*   LexSong - Initial environment file adaptation for conda that served as a basis for the install\\_requirements.bat overhaul.\n*   Arrmansa - Breakmodel support for other projects that served as a basis for VE FORBRYDERNE's integration.\n*   Jojorne - Small improvements to the response selection for gens per action.\n*   OccultSage (GooseAI) - Improved support for GooseAI/OpenAI\n\nAs well as various Model creators who will be listed near their models, and all the testers who helped make this possible!\n\nDid we miss your contribution? Feel free to issue a commit adding your name to this list.\n\n## License\n\nKoboldAI is licensed with a AGPL license, in short this means that it can be used by anyone for any purpose. However, if you decide to make a publicly available instance your users are entitled to a copy of the source code including all modifications that you have made (which needs to be available trough an interface such as a button on your website), you may also not distribute this project in a form that does not contain the source code (Such as compiling / encrypting the code and distributing this version without also distributing the source code that includes the changes that you made. You are allowed to distribute this in a closed form if you also provide a separate archive with the source code.).\n\numamba.exe is bundled for convenience because we observed that many of our users had trouble with command line download methods, it is not part of our project and does not fall under the AGPL license. It is licensed under the BSD-3-Clause license. Other files with differing licenses will have a reference or embedded version of this license within the file. It has been sourced from https://anaconda.org/conda-forge/micromamba/files and its source code can be found here : https://github.com/mamba-org/mamba/tree/master/micromamba\n"
        },
        {
          "name": "Uninstall.bat",
          "type": "blob",
          "size": 1.2529296875,
          "content": "@echo off \ncd /D %~dp0\nTITLE KoboldAI Uninstall Helper\nSET /P M=<loader.settings\nIF %M%==3 subst /D B: >nul\nIF %M%==1 subst /D K: >nul\n\nIF \"%1\" == \"FORCE\" GOTO UNINSTALL\n\nIF EXIST \"Uninstall\\unins000.exe\" (\n   start Uninstall\\unins000.exe\n   exit\n) ELSE (\n   echo This will remove all KoboldAI folders that do not contain user data.\n   echo DO NOT CONTINUE IF KOBOLDAI IS NOT IN ITS OWN FOLDER! OTHERWISE YOUR OTHER DATA IN THIS FOLDER WILL BE DELETED AS WELL!\n   pause\n   set /P D=Type DELETE if you wish to continue the uninstallation: \n)\n\nIF %D%==DELETE GOTO UNINSTALL\nexit\n\t\n:UNINSTALL\necho Uninstallation in progress, please wait...\nset DM=Y\nattrib -h .git >nul\nfor /d %%D in (*) do if not \"%%~nxD\"==\"stories\" if not \"%%~nxD\"==\"userscripts\" if not \"%%~nxD\"==\"settings\" if not \"%%~nxD\"==\"softprompts\" if not \"%%~nxD\"==\"models\" if not \"%%~nxD\"==\"Uninstall\" rmdir /S /Q %%~nxD\nfor %%i in (*) do if not \"%%i\"==\"Uninstall.bat\" del /q \"%%i\"\nset /P DM=Would you like to delete the models folder? (Y/n) :\nIF %DM%==Y rmdir models /s /q\nIF %DM%==y rmdir models /s /q\nset DM=N\nset /P DM=Would you like to delete all other user folders? (y/N) :\nIF %DM%==Y rmdir stories userscripts settings softprompts /s /q\nIF %DM%==y rmdir stories userscripts settings softprompts /s /q\ndel Uninstall.bat"
        },
        {
          "name": "aiserver.py",
          "type": "blob",
          "size": 448.8212890625,
          "content": "#!/usr/bin/python3\n#==================================================================#\n# KoboldAI\n# Version: 1.19.2\n# By: The KoboldAI Community\n#==================================================================#\n\n# External packages\nimport eventlet\neventlet.monkey_patch(all=True, thread=False, os=False)\nimport os\nos.system(\"\")\n__file__ = os.path.dirname(os.path.realpath(__file__))\nos.chdir(__file__)\nos.environ['EVENTLET_THREADPOOL_SIZE'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nfrom eventlet import tpool\n\nimport logging\nfrom logger import logger, set_logger_verbosity, quiesce_logger\n\nlogging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n\nfrom os import path, getcwd\nimport time\nimport re\nimport json\nimport collections\nimport zipfile\nimport packaging\nimport packaging.version\nimport contextlib\nimport traceback\nimport threading\nimport markdown\nimport bleach\nimport itertools\nimport bisect\nimport functools\nimport traceback\nimport inspect\nimport warnings\nfrom collections.abc import Iterable\nfrom typing import Any, Callable, TypeVar, Tuple, Union, Dict, Set, List, Optional, Type\n\nimport requests\nimport html\nimport argparse\nimport sys\nimport gc\n\nimport lupa\nimport importlib\n\n# KoboldAI\nimport fileops\nimport gensettings\nfrom utils import debounce\nimport utils\nimport structures\nimport torch\nfrom transformers import StoppingCriteria, GPT2Tokenizer, GPT2LMHeadModel, GPTNeoForCausalLM, GPTNeoModel, AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, modeling_utils\nfrom transformers import __version__ as transformers_version\nimport transformers\ntry:\n    from transformers.models.opt.modeling_opt import OPTDecoder\nexcept:\n    pass\nimport transformers.generation_utils\n\nglobal tpu_mtj_backend\n\n\nif lupa.LUA_VERSION[:2] != (5, 4):\n    logger.error(f\"Please install lupa==1.10. You have lupa {lupa.__version__}.\")\n\npatch_causallm_patched = False\n\n# Make sure tqdm progress bars display properly in Colab\nfrom tqdm.auto import tqdm\nold_init = tqdm.__init__\ndef new_init(self, *args, **kwargs):\n    old_init(self, *args, **kwargs)\n    if(self.ncols == 0 and kwargs.get(\"ncols\") != 0):\n        self.ncols = 99\ntqdm.__init__ = new_init\n\n# Fix some issues with the OPT tokenizer\nfrom transformers import PreTrainedTokenizerBase\nold_pretrainedtokenizerbase_from_pretrained = PreTrainedTokenizerBase.from_pretrained.__func__\n@classmethod\ndef new_pretrainedtokenizerbase_from_pretrained(cls, *args, **kwargs):\n    tokenizer = old_pretrainedtokenizerbase_from_pretrained(cls, *args, **kwargs)\n    tokenizer._koboldai_header = tokenizer.encode(\"\")\n    tokenizer.add_bos_token = False\n    tokenizer.add_prefix_space = False\n    return tokenizer\nPreTrainedTokenizerBase.from_pretrained = new_pretrainedtokenizerbase_from_pretrained\n\n#==================================================================#\n# Variables & Storage\n#==================================================================#\n\n# Terminal tags for colored text\nclass colors:\n    PURPLE    = '\\033[95m'\n    BLUE      = '\\033[94m'\n    CYAN      = '\\033[96m'\n    GREEN     = '\\033[92m'\n    YELLOW    = '\\033[93m'\n    RED       = '\\033[91m'\n    END       = '\\033[0m'\n    UNDERLINE = '\\033[4m'\n\n# AI models Menu\n# This is a dict of lists where they key is the menu name, and the list is the menu items.\n# Each item takes the 4 elements, 1: Text to display, 2: Model Name (var.model) or menu name (Key name for another menu),\n# 3: the memory requirement for the model, 4: if the item is a menu or not (True/False)\nmodel_menu = {\n    'mainmenu': [\n        [\"Load a model from its directory\", \"NeoCustom\", \"\", False],\n        [\"Load an old GPT-2 model (eg CloverEdition)\", \"GPT2Custom\", \"\", False],\n        [\"Adventure Models\", \"adventurelist\", \"\", True],\n        [\"Novel Models\", \"novellist\", \"\", True],\n        [\"NSFW Models\", \"nsfwlist\", \"\", True],\n        [\"Untuned OPT\", \"optlist\", \"\", True],\n        [\"Untuned GPT-Neo/J\", \"gptneolist\", \"\", True],\n        [\"Untuned Pythia\", \"pythialist\", \"\", True],\n        [\"Untuned Fairseq Dense\", \"fsdlist\", \"\", True],\n        [\"Untuned Bloom\", \"bloomlist\", \"\", True],\n        [\"Untuned XGLM\", \"xglmlist\", \"\", True],\n        [\"Untuned GPT2\", \"gpt2list\", \"\", True],\n        [\"Online Services\", \"apilist\", \"\", True],\n        [\"Read Only (No AI)\", \"ReadOnly\", \"\", False]\n        ],\n    'adventurelist': [\n        [\"Skein 20B\", \"KoboldAI/GPT-NeoX-20B-Skein\", \"64GB\", False],\n        [\"Nerys OPT 13B V2 (Hybrid)\", \"KoboldAI/OPT-13B-Nerys-v2\", \"32GB\", False],\n        [\"Nerys FSD 13B V2 (Hybrid)\", \"KoboldAI/fairseq-dense-13B-Nerys-v2\", \"32GB\", False],\n        [\"Nerys FSD 13B (Hybrid)\", \"KoboldAI/fairseq-dense-13B-Nerys\", \"32GB\", False],\n        [\"Skein 6B\", \"KoboldAI/GPT-J-6B-Skein\", \"16GB\", False],\n        [\"OPT Nerys 6B V2 (Hybrid)\", \"KoboldAI/OPT-6B-nerys-v2\", \"16GB\", False],\n        [\"Adventure 6B\", \"KoboldAI/GPT-J-6B-Adventure\", \"16GB\", False],\n        [\"Nerys FSD 2.7B (Hybrid)\", \"KoboldAI/fairseq-dense-2.7B-Nerys\", \"8GB\", False],\n        [\"Adventure 2.7B\", \"KoboldAI/GPT-Neo-2.7B-AID\", \"8GB\", False],\n        [\"Adventure 1.3B\", \"KoboldAI/GPT-Neo-1.3B-Adventure\", \"6GB\", False],\n        [\"Adventure 125M (Mia)\", \"Merry/AID-Neo-125M\", \"2GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'novellist': [\n        [\"Nerys OPT 13B V2 (Hybrid)\", \"KoboldAI/OPT-13B-Nerys-v2\", \"32GB\", False],\n        [\"Nerys FSD 13B V2 (Hybrid)\", \"KoboldAI/fairseq-dense-13B-Nerys-v2\", \"32GB\", False],\n        [\"Janeway FSD 13B\", \"KoboldAI/fairseq-dense-13B-Janeway\", \"32GB\", False],\n        [\"Nerys FSD 13B (Hybrid)\", \"KoboldAI/fairseq-dense-13B-Nerys\", \"32GB\", False],\n        [\"OPT Nerys 6B V2 (Hybrid)\", \"KoboldAI/OPT-6B-nerys-v2\", \"16GB\", False],\n        [\"Janeway FSD 6.7B\", \"KoboldAI/fairseq-dense-6.7B-Janeway\", \"16GB\", False],\n        [\"Janeway Neo 6B\", \"KoboldAI/GPT-J-6B-Janeway\", \"16GB\", False],\n        [\"Qilin Lit 6B (SFW)\", \"rexwang8/qilin-lit-6b\", \"16GB\", False],       \n        [\"Janeway Neo 2.7B\", \"KoboldAI/GPT-Neo-2.7B-Janeway\", \"8GB\", False],\n        [\"Janeway FSD 2.7B\", \"KoboldAI/fairseq-dense-2.7B-Janeway\", \"8GB\", False],\n        [\"Nerys FSD 2.7B (Hybrid)\", \"KoboldAI/fairseq-dense-2.7B-Nerys\", \"8GB\", False],\n        [\"Horni-LN 2.7B\", \"KoboldAI/GPT-Neo-2.7B-Horni-LN\", \"8GB\", False],\n        [\"Picard 2.7B (Older Janeway)\", \"KoboldAI/GPT-Neo-2.7B-Picard\", \"8GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'nsfwlist': [\n        [\"Erebus 20B (NSFW)\", \"KoboldAI/GPT-NeoX-20B-Erebus\", \"64GB\", False],\n        [\"Nerybus 13B (NSFW)\", \"KoboldAI/OPT-13B-Nerybus-Mix\", \"32GB\", False],\n        [\"Erebus 13B (NSFW)\", \"KoboldAI/OPT-13B-Erebus\", \"32GB\", False],\n        [\"Shinen FSD 13B (NSFW)\", \"KoboldAI/fairseq-dense-13B-Shinen\", \"32GB\", False],\n        [\"Nerybus 6.7B (NSFW)\", \"KoboldAI/OPT-6.7B-Nerybus-Mix\", \"16GB\", False],\n        [\"Erebus 6.7B (NSFW)\", \"KoboldAI/OPT-6.7B-Erebus\", \"16GB\", False],\n        [\"Shinen FSD 6.7B (NSFW)\", \"KoboldAI/fairseq-dense-6.7B-Shinen\", \"16GB\", False],\n        [\"Lit V2 6B (NSFW)\", \"hakurei/litv2-6B-rev3\", \"16GB\", False],\n        [\"Lit 6B (NSFW)\", \"hakurei/lit-6B\", \"16GB\", False],\n        [\"Shinen 6B (NSFW)\", \"KoboldAI/GPT-J-6B-Shinen\", \"16GB\", False],\n        [\"Nerybus 2.7B (NSFW)\", \"KoboldAI/OPT-2.7B-Nerybus-Mix\", \"8GB\", False],\n        [\"Erebus 2.7B (NSFW)\", \"KoboldAI/OPT-2.7B-Erebus\", \"8GB\", False],\n        [\"Horni 2.7B (NSFW)\", \"KoboldAI/GPT-Neo-2.7B-Horni\", \"8GB\", False],\n        [\"Shinen 2.7B (NSFW)\", \"KoboldAI/GPT-Neo-2.7B-Shinen\", \"8GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'chatlist': [\n        [\"Convo 6B (Chatbot)\", \"hitomi-team/convo-6B\", \"16GB\", False],\n        [\"C1 6B (Chatbot)\", \"hakurei/c1-6B\", \"16GB\", False],\n        [\"C1 1.3B (Chatbot)\", \"iokru/c1-1.3B\", \"6GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'gptneolist': [\n        [\"GPT-NeoX 20B\", \"EleutherAI/gpt-neox-20b\", \"64GB\", False],\n        [\"Pythia 13B (NeoX, Same dataset)\", \"EleutherAI/pythia-13b\", \"32GB\", False],\n        [\"GPT-J 6B\", \"EleutherAI/gpt-j-6B\", \"16GB\", False],\n        [\"GPT-Neo 2.7B\", \"EleutherAI/gpt-neo-2.7B\", \"8GB\", False],\n        [\"GPT-Neo 1.3B\", \"EleutherAI/gpt-neo-1.3B\", \"6GB\", False],\n        [\"Pythia 800M (NeoX, Same dataset)\", \"EleutherAI/pythia-800m\", \"4GB\", False],\n        [\"Pythia 350M (NeoX, Same dataset)\", \"EleutherAI/pythia-350m\", \"2GB\", False],\n        [\"GPT-Neo 125M\", \"EleutherAI/gpt-neo-125M\", \"2GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'pythialist': [\n        [\"Pythia 13B Deduped\", \"EleutherAI/pythia-13b-deduped\", \"32GB\", False],\n        [\"Pythia 13B\", \"EleutherAI/pythia-13b\", \"32GB\", False],\n        [\"Pythia 6.7B Deduped\", \"EleutherAI/pythia-6.7b-deduped\", \"16GB\", False],\n        [\"Pythia 6.7B\", \"EleutherAI/pythia-6.7b\", \"16GB\", False],\n        [\"Pythia 1.3B Deduped\", \"EleutherAI/pythia-1.3b-deduped\", \"6GB\", False],\n        [\"Pythia 1.3B\", \"EleutherAI/pythia-1.3b\", \"6GB\", False],\n        [\"Pythia 800M\", \"EleutherAI/pythia-800m\", \"4GB\", False],\n        [\"Pythia 350M Deduped\", \"EleutherAI/pythia-350m-deduped\", \"2GB\", False],\n        [\"Pythia 350M\", \"EleutherAI/pythia-350m\", \"2GB\", False],        \n        [\"Pythia 125M Deduped\", \"EleutherAI/pythia-125m-deduped\", \"2GB\", False],\n        [\"Pythia 125M\", \"EleutherAI/pythia-125m\", \"2GB\", False],\n        [\"Pythia 19M Deduped\", \"EleutherAI/pythia-19m-deduped\", \"1GB\", False],\n        [\"Pythia 19M\", \"EleutherAI/pythia-19m\", \"1GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'gpt2list': [\n        [\"GPT-2 XL\", \"gpt2-xl\", \"6GB\", False],\n        [\"GPT-2 Large\", \"gpt2-large\", \"4GB\", False],\n        [\"GPT-2 Med\", \"gpt2-medium\", \"2GB\", False],\n        [\"GPT-2\", \"gpt2\", \"2GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'bloomlist': [\n        [\"Bloom 176B\", \"bigscience/bloom\", \"\", False],\n        [\"Bloom 7.1B\", \"bigscience/bloom-7b1\", \"\", False],   \n        [\"Bloom 3B\", \"bigscience/bloom-3b\", \"\", False], \n        [\"Bloom 1.7B\", \"bigscience/bloom-1b7\", \"\", False], \n        [\"Bloom 560M\", \"bigscience/bloom-560m\", \"\", False], \n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'optlist': [\n        [\"OPT 66B\", \"facebook/opt-66b\", \"128GB\", False],\n        [\"OPT 30B\", \"facebook/opt-30b\", \"64GB\", False],\n        [\"OPT 13B\", \"facebook/opt-13b\", \"32GB\", False],\n        [\"OPT 6.7B\", \"facebook/opt-6.7b\", \"16GB\", False],\n        [\"OPT 2.7B\", \"facebook/opt-2.7b\", \"8GB\", False],\n        [\"OPT 1.3B\", \"facebook/opt-1.3b\", \"4GB\", False],\n        [\"OPT 350M\", \"facebook/opt-350m\", \"2GB\", False],\n        [\"OPT 125M\", \"facebook/opt-125m\", \"1GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'fsdlist': [\n        [\"Fairseq Dense 13B\", \"KoboldAI/fairseq-dense-13B\", \"32GB\", False],\n        [\"Fairseq Dense 6.7B\", \"KoboldAI/fairseq-dense-6.7B\", \"16GB\", False],\n        [\"Fairseq Dense 2.7B\", \"KoboldAI/fairseq-dense-2.7B\", \"8GB\", False],\n        [\"Fairseq Dense 1.3B\", \"KoboldAI/fairseq-dense-1.3B\", \"4GB\", False],\n        [\"Fairseq Dense 355M\", \"KoboldAI/fairseq-dense-355M\", \"2GB\", False],\n        [\"Fairseq Dense 125M\", \"KoboldAI/fairseq-dense-125M\", \"1GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'xglmlist': [\n        [\"XGLM 4.5B (Larger Dataset)\", \"facebook/xglm-4.5B\", \"12GB\", False],\n        [\"XGLM 7.5B\", \"facebook/xglm-7.5B\", \"18GB\", False],\n        [\"XGLM 2.9B\", \"facebook/xglm-2.9B\", \"10GB\", False],\n        [\"XGLM 1.7B\", \"facebook/xglm-1.7B\", \"6GB\", False],\n        [\"XGLM 564M\", \"facebook/xglm-564M\", \"4GB\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n        ],\n    'apilist': [\n        [\"GooseAI API (requires API key)\", \"GooseAI\", \"\", False],\n        [\"OpenAI API (requires API key)\", \"OAI\", \"\", False],\n        [\"InferKit API (requires API key)\", \"InferKit\", \"\", False],\n        # [\"KoboldAI Server API (Old Google Colab)\", \"Colab\", \"\", False],\n        [\"KoboldAI API\", \"API\", \"\", False],\n        [\"KoboldAI Horde\", \"CLUSTER\", \"\", False],\n        [\"Return to Main Menu\", \"mainmenu\", \"\", True],\n    ]\n    }\n\nclass TokenStreamQueue:\n    def __init__(self):\n        self.probability_buffer = None\n        self.queue = []\n\n    def add_text(self, text):\n        self.queue.append({\n            \"decoded\": text,\n            \"probabilities\": self.probability_buffer\n        })\n        self.probability_buffer = None\n\n# Variables\nclass vars:\n    lastact     = \"\"     # The last action received from the user\n    submission  = \"\"     # Same as above, but after applying input formatting\n    lastctx     = \"\"     # The last context submitted to the generator\n    model       = \"ReadOnly\"     # Model ID string chosen at startup\n    online_model = \"\"     # Used when Model ID is an online service, and there is a secondary option for the actual model name\n    model_selected = \"\"  #selected model in UI\n    model_type  = \"\"     # Model Type (Automatically taken from the model config)\n    noai        = False  # Runs the script without starting up the transformers pipeline\n    aibusy      = False  # Stops submissions while the AI is working\n    max_length  = 1024    # Maximum number of tokens to submit per action\n    ikmax       = 3000   # Maximum number of characters to submit to InferKit\n    genamt      = 80     # Amount of text for each action to generate\n    ikgen       = 200    # Number of characters for InferKit to generate\n    rep_pen     = 1.1    # Default generator repetition_penalty\n    rep_pen_slope = 0.7  # Default generator repetition penalty slope\n    rep_pen_range = 1024 # Default generator repetition penalty range\n    temp        = 0.5    # Default generator temperature\n    top_p       = 0.9    # Default generator top_p\n    top_k       = 0      # Default generator top_k\n    top_a       = 0.0    # Default generator top-a\n    tfs         = 1.0    # Default generator tfs (tail-free sampling)\n    typical     = 1.0    # Default generator typical sampling threshold\n    numseqs     = 1     # Number of sequences to ask the generator to create\n    full_determinism = False  # Whether or not full determinism is enabled\n    seed_specified = False  # Whether or not the current RNG seed was specified by the user (in their settings file)\n    seed        = None   # The current RNG seed (as an int), or None if unknown\n    gamestarted = False  # Whether the game has started (disables UI elements)\n    gamesaved   = True   # Whether or not current game is saved\n    serverstarted = False  # Whether or not the Flask server has started\n    prompt      = \"\"     # Prompt\n    memory      = \"\"     # Text submitted to memory field\n    authornote  = \"\"     # Text submitted to Author's Note field\n    authornotetemplate = \"[Author's note: <|>]\"  # Author's note template\n    setauthornotetemplate = authornotetemplate  # Saved author's note template in settings\n    andepth     = 3      # How far back in history to append author's note\n    actions     = structures.KoboldStoryRegister()  # Actions submitted by user and AI\n    actions_metadata = {} # List of dictonaries, one dictonary for every action that contains information about the action like alternative options.\n                          # Contains at least the same number of items as actions. Back action will remove an item from actions, but not actions_metadata\n                          # Dictonary keys are:\n                          # Selected Text: (text the user had selected. None when this is a newly generated action)\n                          # Alternative Generated Text: {Text, Pinned, Previous Selection, Edited}\n                          # \n    worldinfo   = []     # List of World Info key/value objects\n    worldinfo_i = []     # List of World Info key/value objects sans uninitialized entries\n    worldinfo_u = {}     # Dictionary of World Info UID - key/value pairs\n    wifolders_d = {}     # Dictionary of World Info folder UID-info pairs\n    wifolders_l = []     # List of World Info folder UIDs\n    wifolders_u = {}     # Dictionary of pairs of folder UID - list of WI UID\n    modelconfig = {}     # Raw contents of the model's config.json, or empty dictionary if none found\n    lua_state   = None   # Lua state of the Lua scripting system\n    lua_koboldbridge = None  # `koboldbridge` from bridge.lua\n    lua_kobold  = None   # `kobold` from` bridge.lua\n    lua_koboldcore = None  # `koboldcore` from bridge.lua\n    lua_logname = ...    # Name of previous userscript that logged to terminal\n    lua_running = False  # Whether or not Lua is running (i.e. wasn't stopped due to an error)\n    lua_edited  = set()  # Set of chunk numbers that were edited from a Lua generation modifier\n    lua_deleted = set()  # Set of chunk numbers that were deleted from a Lua generation modifier\n    generated_tkns = 0   # If using a backend that supports Lua generation modifiers, how many tokens have already been generated, otherwise 0\n    abort       = False  # Whether or not generation was aborted by clicking on the submit button during generation\n    compiling   = False  # If using a TPU Colab, this will be set to True when the TPU backend starts compiling and then set to False again\n    checking    = False  # Whether or not we are actively checking to see if TPU backend is compiling or not\n    sp_changed  = False  # This gets set to True whenever a userscript changes the soft prompt so that check_for_sp_change() can alert the browser that the soft prompt has changed\n    spfilename  = \"\"     # Filename of soft prompt to load, or an empty string if not using a soft prompt\n    userscripts = []     # List of userscripts to load\n    last_userscripts = []  # List of previous userscript filenames from the previous time userscripts were send via usstatitems\n    corescript  = \"default.lua\"  # Filename of corescript to load\n    # badwords    = []     # Array of str/chr values that should be removed from output\n    badwordsids = []\n    badwordsids_default = [[13460], [6880], [50256], [42496], [4613], [17414], [22039], [16410], [27], [29], [38430], [37922], [15913], [24618], [28725], [58], [47175], [36937], [26700], [12878], [16471], [37981], [5218], [29795], [13412], [45160], [3693], [49778], [4211], [20598], [36475], [33409], [44167], [32406], [29847], [29342], [42669], [685], [25787], [7359], [3784], [5320], [33994], [33490], [34516], [43734], [17635], [24293], [9959], [23785], [21737], [28401], [18161], [26358], [32509], [1279], [38155], [18189], [26894], [6927], [14610], [23834], [11037], [14631], [26933], [46904], [22330], [25915], [47934], [38214], [1875], [14692], [41832], [13163], [25970], [29565], [44926], [19841], [37250], [49029], [9609], [44438], [16791], [17816], [30109], [41888], [47527], [42924], [23984], [49074], [33717], [31161], [49082], [30138], [31175], [12240], [14804], [7131], [26076], [33250], [3556], [38381], [36338], [32756], [46581], [17912], [49146]] # Tokenized array of badwords used to prevent AI artifacting\n    badwordsids_neox = [[0], [1], [44162], [9502], [12520], [31841], [36320], [49824], [34417], [6038], [34494], [24815], [26635], [24345], [3455], [28905], [44270], [17278], [32666], [46880], [7086], [43189], [37322], [17778], [20879], [49821], [3138], [14490], [4681], [21391], [26786], [43134], [9336], [683], [48074], [41256], [19181], [29650], [28532], [36487], [45114], [46275], [16445], [15104], [11337], [1168], [5647], [29], [27482], [44965], [43782], [31011], [42944], [47389], [6334], [17548], [38329], [32044], [35487], [2239], [34761], [7444], [1084], [12399], [18990], [17636], [39083], [1184], [35830], [28365], [16731], [43467], [47744], [1138], [16079], [40116], [45564], [18297], [42368], [5456], [18022], [42696], [34476], [23505], [23741], [39334], [37944], [45382], [38709], [33440], [26077], [43600], [34418], [36033], [6660], [48167], [48471], [15775], [19884], [41533], [1008], [31053], [36692], [46576], [20095], [20629], [31759], [46410], [41000], [13488], [30952], [39258], [16160], [27655], [22367], [42767], [43736], [49694], [13811], [12004], [46768], [6257], [37471], [5264], [44153], [33805], [20977], [21083], [25416], [14277], [31096], [42041], [18331], [33376], [22372], [46294], [28379], [38475], [1656], [5204], [27075], [50001], [16616], [11396], [7748], [48744], [35402], [28120], [41512], [4207], [43144], [14767], [15640], [16595], [41305], [44479], [38958], [18474], [22734], [30522], [46267], [60], [13976], [31830], [48701], [39822], [9014], [21966], [31422], [28052], [34607], [2479], [3851], [32214], [44082], [45507], [3001], [34368], [34758], [13380], [38363], [4299], [46802], [30996], [12630], [49236], [7082], [8795], [5218], [44740], [9686], [9983], [45301], [27114], [40125], [1570], [26997], [544], [5290], [49193], [23781], [14193], [40000], [2947], [43781], [9102], [48064], [42274], [18772], [49384], [9884], [45635], [43521], [31258], [32056], [47686], [21760], [13143], [10148], [26119], [44308], [31379], [36399], [23983], [46694], [36134], [8562], [12977], [35117], [28591], [49021], [47093], [28653], [29013], [46468], [8605], [7254], [25896], [5032], [8168], [36893], [38270], [20499], [27501], [34419], [29547], [28571], [36586], [20871], [30537], [26842], [21375], [31148], [27618], [33094], [3291], [31789], [28391], [870], [9793], [41361], [47916], [27468], [43856], [8850], [35237], [15707], [47552], [2730], [41449], [45488], [3073], [49806], [21938], [24430], [22747], [20924], [46145], [20481], [20197], [8239], [28231], [17987], [42804], [47269], [29972], [49884], [21382], [46295], [36676], [34616], [3921], [26991], [27720], [46265], [654], [9855], [40354], [5291], [34904], [44342], [2470], [14598], [880], [19282], [2498], [24237], [21431], [16369], [8994], [44524], [45662], [13663], [37077], [1447], [37786], [30863], [42854], [1019], [20322], [4398], [12159], [44072], [48664], [31547], [18736], [9259], [31], [16354], [21810], [4357], [37982], [5064], [2033], [32871], [47446], [62], [22158], [37387], [8743], [47007], [17981], [11049], [4622], [37916], [36786], [35138], [29925], [14157], [18095], [27829], [1181], [22226], [5709], [4725], [30189], [37014], [1254], [11380], [42989], [696], [24576], [39487], [30119], [1092], [8088], [2194], [9899], [14412], [21828], [3725], [13544], [5180], [44679], [34398], [3891], [28739], [14219], [37594], [49550], [11326], [6904], [17266], [5749], [10174], [23405], [9955], [38271], [41018], [13011], [48392], [36784], [24254], [21687], [23734], [5413], [41447], [45472], [10122], [17555], [15830], [47384], [12084], [31350], [47940], [11661], [27988], [45443], [905], [49651], [16614], [34993], [6781], [30803], [35869], [8001], [41604], [28118], [46462], [46762], [16262], [17281], [5774], [10943], [5013], [18257], [6750], [4713], [3951], [11899], [38791], [16943], [37596], [9318], [18413], [40473], [13208], [16375]]\n    badwordsids_opt = [[44717], [46613], [48513], [49923], [50185], [48755], [8488], [43303], [49659], [48601], [49817], [45405], [48742], [49925], [47720], [11227], [48937], [48784], [50017], [42248], [49310], [48082], [49895], [50025], [49092], [49007], [8061], [44226], [0], [742], [28578], [15698], [49784], [46679], [39365], [49281], [49609], [48081], [48906], [46161], [48554], [49670], [48677], [49721], [49632], [48610], [48462], [47457], [10975], [46077], [28696], [48709], [43839], [49798], [49154], [48203], [49625], [48395], [50155], [47161], [49095], [48833], [49420], [49666], [48443], [22176], [49242], [48651], [49138], [49750], [40389], [48021], [21838], [49070], [45333], [40862], [1], [49915], [33525], [49858], [50254], [44403], [48992], [48872], [46117], [49853], [47567], [50206], [41552], [50068], [48999], [49703], [49940], [49329], [47620], [49868], [49962], [2], [44082], [50236], [31274], [50260], [47052], [42645], [49177], [17523], [48691], [49900], [49069], [49358], [48794], [47529], [46479], [48457], [646], [49910], [48077], [48935], [46386], [48902], [49151], [48759], [49803], [45587], [48392], [47789], [48654], [49836], [49230], [48188], [50264], [46844], [44690], [48505], [50161], [27779], [49995], [41833], [50154], [49097], [48520], [50018], [8174], [50084], [49366], [49526], [50193], [7479], [49982], [3]]\n    fp32_model  = False  # Whether or not the most recently loaded HF model was in fp32 format\n    deletewi    = None   # Temporary storage for UID to delete\n    wirmvwhtsp  = True  # Whether to remove leading whitespace from WI entries\n    widepth     = 3      # How many historical actions to scan for WI hits\n    mode        = \"play\" # Whether the interface is in play, memory, or edit mode\n    editln      = 0      # Which line was last selected in Edit Mode\n    gpu_device  = 0      # Which PyTorch device to use when using pure GPU generation\n    url         = \"https://api.inferkit.com/v1/models/standard/generate\" # InferKit API URL\n    oaiurl      = \"\" # OpenAI API URL\n    oaiengines  = \"https://api.openai.com/v1/engines\"\n    colaburl    = \"\"     # Ngrok url for Google Colab mode\n    apikey      = \"\"     # API key to use for InferKit API calls\n    oaiapikey   = \"\"     # API key to use for OpenAI API calls\n    cluster_requested_models = [] # The models which we allow to generate during cluster mode\n    savedir     = getcwd()+\"\\\\stories\"\n    hascuda     = False  # Whether torch has detected CUDA on the system\n    usegpu      = False  # Whether to launch pipeline with GPU support\n    custmodpth  = \"\"     # Filesystem location of custom model to run\n    formatoptns = {'frmttriminc': True, 'frmtrmblln': False, 'frmtrmspch': False, 'frmtadsnsp': True, 'singleline': False}     # Container for state of formatting options\n    importnum   = -1     # Selection on import popup list\n    importjs    = {}     # Temporary storage for import data\n    loadselect  = \"\"     # Temporary storage for story filename to load\n    spselect    = \"\"     # Temporary storage for soft prompt filename to load\n    spmeta      = None   # Metadata of current soft prompt, or None if not using a soft prompt\n    sp          = None   # Current soft prompt tensor (as a NumPy array)\n    sp_length   = 0      # Length of current soft prompt in tokens, or 0 if not using a soft prompt\n    has_genmod  = False  # Whether or not at least one loaded Lua userscript has a generation modifier\n    svowname    = \"\"     # Filename that was flagged for overwrite confirm\n    saveow      = False  # Whether or not overwrite confirm has been displayed\n    autosave    = False  # Whether or not to automatically save after each action\n    genseqs     = []     # Temporary storage for generated sequences\n    recentback  = False  # Whether Back button was recently used without Submitting or Retrying after\n    recentrng   = None   # If a new random game was recently generated without Submitting after, this is the topic used (as a string), otherwise this is None\n    recentrngm  = None   # If a new random game was recently generated without Submitting after, this is the memory used (as a string), otherwise this is None\n    useprompt   = False   # Whether to send the full prompt with every submit action\n    breakmodel  = False  # For GPU users, whether to use both system RAM and VRAM to conserve VRAM while offering speedup compared to CPU-only\n    bmsupported = False  # Whether the breakmodel option is supported (GPT-Neo/GPT-J/XGLM/OPT only, currently)\n    nobreakmodel = False  # Something specifically requested Breakmodel to be disabled (For example a models config)\n    smandelete  = False  # Whether stories can be deleted from inside the browser\n    smanrename  = False  # Whether stories can be renamed from inside the browser\n    allowsp     = False  # Whether we are allowed to use soft prompts (by default enabled if we're using GPT-2, GPT-Neo or GPT-J)\n    modeldim    = -1     # Embedding dimension of your model (e.g. it's 4096 for GPT-J-6B and 2560 for GPT-Neo-2.7B)\n    laststory   = None   # Filename (without extension) of most recent story JSON file we loaded\n    regex_sl    = re.compile(r'\\n*(?<=.) *\\n(.|\\n)*')  # Pattern for limiting the output to a single line\n    acregex_ai  = re.compile(r'\\n* *>(.|\\n)*')  # Pattern for matching adventure actions from the AI so we can remove them\n    acregex_ui  = re.compile(r'^ *(&gt;.*)$', re.MULTILINE)    # Pattern for matching actions in the HTML-escaped story so we can apply colouring, etc (make sure to encase part to format in parentheses)\n    comregex_ai = re.compile(r'(?:\\n<\\|(?:.|\\n)*?\\|>(?=\\n|$))|(?:<\\|(?:.|\\n)*?\\|>\\n?)')  # Pattern for matching comments to remove them before sending them to the AI\n    comregex_ui = re.compile(r'(&lt;\\|(?:.|\\n)*?\\|&gt;)')  # Pattern for matching comments in the editor\n    sampler_order = utils.default_sampler_order.copy()\n    rng_states  = {}  # Used by the POST /generate endpoint to store sampler RNG states\n    chatmode    = False\n    chatname    = \"You\"\n    adventure   = False\n    actionmode  = 1\n    dynamicscan = False\n    host        = False\n    nopromptgen = False\n    rngpersist  = False\n    nogenmod    = False\n    welcome     = False  # Custom Welcome Text (False is default)\n    newlinemode = \"ns\"\n    quiet       = False # If set will suppress any story text from being printed to the console (will only be seen on the client web page)\n    debug       = False # If set to true, will send debug information to the client for display\n    lazy_load   = True  # Whether or not to use torch_lazy_loader.py for transformers models in order to reduce CPU memory usage\n    use_colab_tpu = os.environ.get(\"COLAB_TPU_ADDR\", \"\") != \"\" or os.environ.get(\"TPU_NAME\", \"\") != \"\"  # Whether or not we're in a Colab TPU instance or Kaggle TPU instance and are going to use the TPU rather than the CPU\n    revision    = None\n    standalone = False\n    api_tokenizer_id = None\n    disable_set_aibusy = False\n    disable_input_formatting = False\n    disable_output_formatting = False\n    output_streaming = True\n    token_stream_queue = TokenStreamQueue() # Queue for the token streaming\n    show_probs = False # Whether or not to show token probabilities\n    show_budget = False # Whether or not to show token probabilities\n    configname = None\n\nutils.vars = vars\n\nclass Send_to_socketio(object):\n    def write(self, bar):\n        print(bar, end=\"\")\n        time.sleep(0.01)\n        try:\n            gui_msg = bar.replace(f\"{colors.PURPLE}INIT{colors.END}       | \",\"\").replace(\" \", \"&nbsp;\")\n            emit('from_server', {'cmd': 'model_load_status', 'data': gui_msg}, broadcast=True)\n        except:\n            pass\n                                \n# Set logging level to reduce chatter from Flask\nimport logging\nlog = logging.getLogger('werkzeug')\nlog.setLevel(logging.ERROR)\n\nfrom flask import Flask, render_template, Response, request, copy_current_request_context, send_from_directory, session, jsonify, abort, redirect\nfrom flask_socketio import SocketIO\nfrom flask_socketio import emit as _emit\nfrom flask_session import Session\nfrom werkzeug.exceptions import HTTPException, NotFound, InternalServerError\nimport secrets\napp = Flask(__name__, root_path=os.getcwd())\napp.secret_key = secrets.token_hex()\napp.config['SESSION_TYPE'] = 'filesystem'\napp.config['TEMPLATES_AUTO_RELOAD'] = True\nsocketio = SocketIO(app, async_method=\"eventlet\")\n\nold_socketio_on = socketio.on\ndef new_socketio_on(*a, **k):\n    decorator = old_socketio_on(*a, **k)\n    def new_decorator(f):\n        @functools.wraps(f)\n        def g(*a, **k):\n            if args.no_ui:\n                return\n            return f(*a, **k)\n        return decorator(g)\n    return new_decorator\nsocketio.on = new_socketio_on\n\ndef emit(*args, **kwargs):\n    try:\n        return _emit(*args, **kwargs)\n    except AttributeError:\n        return socketio.emit(*args, **kwargs)\nutils.emit = emit\n\n# marshmallow/apispec setup\nfrom apispec import APISpec\nfrom apispec.ext.marshmallow import MarshmallowPlugin\nfrom apispec.ext.marshmallow.field_converter import make_min_max_attributes\nfrom apispec_webframeworks.flask import FlaskPlugin\nfrom marshmallow import Schema, fields, validate, EXCLUDE\nfrom marshmallow.exceptions import ValidationError\n\nclass KoboldSchema(Schema):\n    pass\n\ndef new_make_min_max_attributes(validators, min_attr, max_attr) -> dict:\n    # Patched apispec function that creates \"exclusiveMinimum\"/\"exclusiveMaximum\" OpenAPI attributes insteaed of \"minimum\"/\"maximum\" when using validators.Range or validators.Length with min_inclusive=False or max_inclusive=False\n    attributes = {}\n    min_list = [validator.min for validator in validators if validator.min is not None]\n    max_list = [validator.max for validator in validators if validator.max is not None]\n    min_inclusive_list = [getattr(validator, \"min_inclusive\", True) for validator in validators if validator.min is not None]\n    max_inclusive_list = [getattr(validator, \"max_inclusive\", True) for validator in validators if validator.max is not None]\n    if min_list:\n        if min_attr == \"minimum\" and not min_inclusive_list[max(range(len(min_list)), key=min_list.__getitem__)]:\n            min_attr = \"exclusiveMinimum\"\n        attributes[min_attr] = max(min_list)\n    if max_list:\n        if min_attr == \"maximum\" and not max_inclusive_list[min(range(len(max_list)), key=max_list.__getitem__)]:\n            min_attr = \"exclusiveMaximum\"\n        attributes[max_attr] = min(max_list)\n    return attributes\nmake_min_max_attributes.__code__ = new_make_min_max_attributes.__code__\n\ndef api_format_docstring(f):\n    f.__doc__ = eval('f\"\"\"{}\"\"\"'.format(f.__doc__.replace(\"\\\\\", \"\\\\\\\\\")))\n    return f\n\ndef api_catch_out_of_memory_errors(f):\n    @functools.wraps(f)\n    def decorated(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except Exception as e:\n            if any (s in traceback.format_exc().lower() for s in (\"out of memory\", \"not enough memory\")):\n                for line in reversed(traceback.format_exc().split(\"\\n\")):\n                    if any(s in line.lower() for s in (\"out of memory\", \"not enough memory\")) and line.count(\":\"):\n                        line = line.split(\":\", 1)[1]\n                        line = re.sub(r\"\\[.+?\\] +data\\.\", \"\", line).strip()\n                        raise KoboldOutOfMemoryError(\"KoboldAI ran out of memory: \" + line, type=\"out_of_memory.gpu.cuda\" if \"cuda out of memory\" in line.lower() else \"out_of_memory.gpu.hip\" if \"hip out of memory\" in line.lower() else \"out_of_memory.tpu.hbm\" if \"memory space hbm\" in line.lower() else \"out_of_memory.cpu.default_memory_allocator\" if \"defaultmemoryallocator\" in line.lower() else \"out_of_memory.unknown.unknown\")\n                raise KoboldOutOfMemoryError(type=\"out_of_memory.unknown.unknown\")\n            raise e\n    return decorated\n\ndef api_schema_wrap(f):\n    try:\n        input_schema: Type[Schema] = next(iter(inspect.signature(f).parameters.values())).annotation\n    except:\n        HAS_SCHEMA = False\n    else:\n        HAS_SCHEMA = inspect.isclass(input_schema) and issubclass(input_schema, Schema)\n    f = api_format_docstring(f)\n    f = api_catch_out_of_memory_errors(f)\n    @functools.wraps(f)\n    def decorated(*args, **kwargs):\n        if HAS_SCHEMA:\n            body = request.get_json()\n            schema = input_schema.from_dict(input_schema().load(body))\n            response = f(schema, *args, **kwargs)\n        else:\n            response = f(*args, **kwargs)\n        if not isinstance(response, Response):\n            response = jsonify(response)\n        return response\n    return decorated\n\n@app.errorhandler(HTTPException)\ndef handler(e):\n    if request.path != \"/api\" and not request.path.startswith(\"/api/\"):\n        return e\n    resp = jsonify(detail={\"msg\": str(e), \"type\": \"generic.error_\" + str(e.code)})\n    if e.code == 405 and e.valid_methods is not None:\n        resp.headers[\"Allow\"] = \", \".join(e.valid_methods)\n    return resp, e.code\n\nclass KoboldOutOfMemoryError(HTTPException):\n    code = 507\n    description = \"KoboldAI ran out of memory.\"\n    type = \"out_of_memory.unknown.unknown\"\n    def __init__(self, *args, type=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        if type is not None:\n            self.type = type\n@app.errorhandler(KoboldOutOfMemoryError)\ndef handler(e):\n    if request.path != \"/api\" and not request.path.startswith(\"/api/\"):\n        return InternalServerError()\n    return jsonify(detail={\"type\": e.type, \"msg\": e.description}), e.code\n\n@app.errorhandler(ValidationError)\ndef handler(e):\n    if request.path != \"/api\" and not request.path.startswith(\"/api/\"):\n        return InternalServerError()\n    return jsonify(detail=e.messages), 422\n\n@app.errorhandler(NotImplementedError)\ndef handler(e):\n    if request.path != \"/api\" and not request.path.startswith(\"/api/\"):\n        return InternalServerError()\n    return jsonify(detail={\"type\": \"not_implemented\", \"msg\": str(e).strip()}), 501\n\napi_versions: List[str] = []\n\nclass KoboldAPISpec(APISpec):\n    class KoboldFlaskPlugin(FlaskPlugin):\n        def __init__(self, api: \"KoboldAPISpec\", *args, **kwargs):\n            self._kobold_api_spec = api\n            super().__init__(*args, **kwargs)\n\n        def path_helper(self, *args, **kwargs):\n            return super().path_helper(*args, **kwargs)[len(self._kobold_api_spec._prefixes[0]):]\n\n    def __init__(self, *args, title: str = \"KoboldAI API\", openapi_version: str = \"3.0.3\", version: str = \"1.0.0\", prefixes: List[str] = None, **kwargs):\n        plugins = [KoboldAPISpec.KoboldFlaskPlugin(self), MarshmallowPlugin()]\n        self._prefixes = prefixes if prefixes is not None else [\"\"]\n        self._kobold_api_spec_version = version\n        api_versions.append(version)\n        api_versions.sort(key=lambda x: [int(e) for e in x.split(\".\")])\n        super().__init__(*args, title=title, openapi_version=openapi_version, version=version, plugins=plugins, servers=[{\"url\": self._prefixes[0]}], **kwargs)\n        for prefix in self._prefixes:\n            app.route(prefix, endpoint=\"~KoboldAPISpec~\" + prefix)(lambda: redirect(request.path + \"/docs/\"))\n            app.route(prefix + \"/\", endpoint=\"~KoboldAPISpec~\" + prefix + \"/\")(lambda: redirect(\"docs/\"))\n            app.route(prefix + \"/docs\", endpoint=\"~KoboldAPISpec~\" + prefix + \"/docs\")(lambda: redirect(\"docs/\"))\n            app.route(prefix + \"/docs/\", endpoint=\"~KoboldAPISpec~\" + prefix + \"/docs/\")(lambda: render_template(\"swagger-ui.html\", url=self._prefixes[0] + \"/openapi.json\"))\n            app.route(prefix + \"/openapi.json\", endpoint=\"~KoboldAPISpec~\" + prefix + \"/openapi.json\")(lambda: jsonify(self.to_dict()))\n\n    def route(self, rule: str, methods=[\"GET\"], **kwargs):\n        __F = TypeVar(\"__F\", bound=Callable[..., Any])\n        if \"strict_slashes\" not in kwargs:\n            kwargs[\"strict_slashes\"] = False\n        def new_decorator(f: __F) -> __F:\n            @functools.wraps(f)\n            def g(*args, **kwargs):\n                global api_version\n                api_version = self._kobold_api_spec_version\n                try:\n                    return f(*args, **kwargs)\n                finally:\n                    api_version = None\n            for prefix in self._prefixes:\n                g = app.route(prefix + rule, methods=methods, **kwargs)(g)\n            with app.test_request_context():\n                self.path(view=g, **kwargs)\n            return g\n        return new_decorator\n\n    def get(self, rule: str, **kwargs):\n        return self.route(rule, methods=[\"GET\"], **kwargs)\n    \n    def post(self, rule: str, **kwargs):\n        return self.route(rule, methods=[\"POST\"], **kwargs)\n    \n    def put(self, rule: str, **kwargs):\n        return self.route(rule, methods=[\"PUT\"], **kwargs)\n    \n    def patch(self, rule: str, **kwargs):\n        return self.route(rule, methods=[\"PATCH\"], **kwargs)\n    \n    def delete(self, rule: str, **kwargs):\n        return self.route(rule, methods=[\"DELETE\"], **kwargs)\n\ntags = [\n    {\"name\": \"info\", \"description\": \"Metadata about this API\"},\n    {\"name\": \"generate\", \"description\": \"Text generation endpoints\"},\n    {\"name\": \"model\", \"description\": \"Information about the current text generation model\"},\n    {\"name\": \"story\", \"description\": \"Endpoints for managing the story in the KoboldAI GUI\"},\n    {\"name\": \"world_info\", \"description\": \"Endpoints for managing the world info in the KoboldAI GUI\"},\n    {\"name\": \"config\", \"description\": \"Allows you to get/set various setting values\"},\n]\n\napi_version = None  # This gets set automatically so don't change this value\n\napi_v1 = KoboldAPISpec(\n    version=\"1.2.1\",\n    prefixes=[\"/api/v1\", \"/api/latest\"],\n    tags=tags,\n)\n\n# Returns the expected config filename for the current setup.\n# If the model_name is specified, it returns what the settings file would be for that model\ndef get_config_filename(model_name = None):\n    if model_name:\n        return(f\"settings/{model_name.replace('/', '_')}.settings\")\n    elif args.configname:\n        return(f\"settings/{args.configname.replace('/', '_')}.settings\")\n    elif vars.configname != '':\n        return(f\"settings/{vars.configname.replace('/', '_')}.settings\")\n    else:\n        logger.warning(f\"Empty configfile name sent back. Defaulting to ReadOnly\")\n        return(f\"settings/ReadOnly.settings\")\n#==================================================================#\n# Function to get model selection at startup\n#==================================================================#\ndef sendModelSelection(menu=\"mainmenu\", folder=\"./models\"):\n    #If we send one of the manual load options, send back the list of model directories, otherwise send the menu\n    if menu in ('NeoCustom', 'GPT2Custom'):\n        (paths, breadcrumbs) = get_folder_path_info(folder)\n        if vars.host:\n            breadcrumbs = []\n        menu_list = [[folder, menu, \"\", False] for folder in paths]\n        menu_list.append([\"Return to Main Menu\", \"mainmenu\", \"\", True])\n        if os.path.abspath(\"{}/models\".format(os.getcwd())) == os.path.abspath(folder):\n            showdelete=True\n        else:\n            showdelete=False\n        emit('from_server', {'cmd': 'show_model_menu', 'data': menu_list, 'menu': menu, 'breadcrumbs': breadcrumbs, \"showdelete\": showdelete}, broadcast=True)\n    else:\n        emit('from_server', {'cmd': 'show_model_menu', 'data': model_menu[menu], 'menu': menu, 'breadcrumbs': [], \"showdelete\": False}, broadcast=True)\n\ndef get_folder_path_info(base):\n    if base == 'This PC':\n        breadcrumbs = [['This PC', 'This PC']]\n        paths = [[\"{}:\\\\\".format(chr(i)), \"{}:\\\\\".format(chr(i))] for i in range(65, 91) if os.path.exists(\"{}:\".format(chr(i)))]\n    else:\n        path = os.path.abspath(base)\n        if path[-1] == \"\\\\\":\n            path = path[:-1]\n        breadcrumbs = []\n        for i in range(len(path.replace(\"/\", \"\\\\\").split(\"\\\\\"))):\n            breadcrumbs.append([\"\\\\\".join(path.replace(\"/\", \"\\\\\").split(\"\\\\\")[:i+1]),\n                                 path.replace(\"/\", \"\\\\\").split(\"\\\\\")[i]])\n        if len(breadcrumbs) == 1:\n            breadcrumbs = [[\"{}:\\\\\".format(chr(i)), \"{}:\\\\\".format(chr(i))] for i in range(65, 91) if os.path.exists(\"{}:\".format(chr(i)))]\n        else:\n            if len([[\"{}:\\\\\".format(chr(i)), \"{}:\\\\\".format(chr(i))] for i in range(65, 91) if os.path.exists(\"{}:\".format(chr(i)))]) > 0:\n                breadcrumbs.insert(0, ['This PC', 'This PC'])\n        paths = []\n        base_path = os.path.abspath(base)\n        for item in os.listdir(base_path):\n            if os.path.isdir(os.path.join(base_path, item)):\n                paths.append([os.path.join(base_path, item), item])\n    # Paths/breadcrumbs is a list of lists, where the first element in the sublist is the full path and the second is the folder name\n    return (paths, breadcrumbs)\n\n\ndef getModelSelection(modellist):\n    print(\"    #    Model\\t\\t\\t\\t\\t\\tVRAM\\n    ========================================================\")\n    i = 1\n    for m in modellist:\n        print(\"    {0} - {1}\\t\\t\\t{2}\".format(\"{:<2}\".format(i), m[0].ljust(25), m[2]))\n        i += 1\n    print(\" \");\n    modelsel = 0\n    vars.model = ''\n    while(vars.model == ''):\n        modelsel = input(\"Model #> \")\n        if(modelsel.isnumeric() and int(modelsel) > 0 and int(modelsel) <= len(modellist)):\n            vars.model = modellist[int(modelsel)-1][1]\n        else:\n            print(\"{0}Please enter a valid selection.{1}\".format(colors.RED, colors.END))\n    \n    # Model Lists\n    try:\n        getModelSelection(eval(vars.model))\n    except Exception as e:\n        if(vars.model == \"Return\"):\n            getModelSelection(mainmenu)\n                \n        # If custom model was selected, get the filesystem location and store it\n        if(vars.model == \"NeoCustom\" or vars.model == \"GPT2Custom\"):\n            print(\"{0}Please choose the folder where pytorch_model.bin is located:{1}\\n\".format(colors.CYAN, colors.END))\n            modpath = fileops.getdirpath(getcwd() + \"/models\", \"Select Model Folder\")\n        \n            if(modpath):\n                # Save directory to vars\n                vars.custmodpth = modpath\n            else:\n                # Print error and retry model selection\n                print(\"{0}Model select cancelled!{1}\".format(colors.RED, colors.END))\n                print(\"{0}Select an AI model to continue:{1}\\n\".format(colors.CYAN, colors.END))\n                getModelSelection(mainmenu)\n\ndef check_if_dir_is_model(path):\n    return os.path.exists(os.path.join(path, 'config.json'))\n    \n#==================================================================#\n# Return all keys in tokenizer dictionary containing char\n#==================================================================#\n#def gettokenids(char):\n#    keys = []\n#    for key in vocab_keys:\n#        if(key.find(char) != -1):\n#            keys.append(key)\n#    return keys\n\n#==================================================================#\n# Return Model Name\n#==================================================================#\ndef getmodelname():\n    if(vars.online_model != ''):\n       return(f\"{vars.model}/{vars.online_model}\")\n    if(vars.model in (\"NeoCustom\", \"GPT2Custom\", \"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")):\n        modelname = os.path.basename(os.path.normpath(vars.custmodpth))\n        return modelname\n    else:\n        modelname = vars.model\n        return modelname\n\n#==================================================================#\n# Get hidden size from model\n#==================================================================#\ndef get_hidden_size_from_model(model):\n    return model.get_input_embeddings().embedding_dim\n\n#==================================================================#\n# Breakmodel configuration functions\n#==================================================================#\ndef device_list(n_layers, primary=None, selected=None):\n    device_count = torch.cuda.device_count()\n    if(device_count < 2):\n        primary = None\n    gpu_blocks = breakmodel.gpu_blocks + (device_count - len(breakmodel.gpu_blocks))*[0]\n    print(f\"{colors.YELLOW}       DEVICE ID  |  LAYERS  |  DEVICE NAME{colors.END}\")\n    for i in range(device_count):\n        name = torch.cuda.get_device_name(i)\n        if(len(name) > 47):\n            name = \"...\" + name[-44:]\n        row_color = colors.END\n        sep_color = colors.YELLOW\n        print(f\"{row_color}{colors.YELLOW + '->' + row_color if i == selected else '  '} {'(primary)' if i == primary else ' '*9} {i:3}  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}\")\n    row_color = colors.END\n    sep_color = colors.YELLOW\n    if(utils.HAS_ACCELERATE):\n        print(f\"{row_color}{colors.YELLOW + '->' + row_color if -1 == selected else '  '} {' '*9} N/A  {sep_color}|{row_color}     {breakmodel.disk_blocks:3}  {sep_color}|{row_color}  (Disk cache){colors.END}\")\n    print(f\"{row_color}   {' '*9} N/A  {sep_color}|{row_color}     {n_layers:3}  {sep_color}|{row_color}  (CPU){colors.END}\")\n\ndef device_config(config):\n    global breakmodel, generator\n    import breakmodel\n    n_layers = utils.num_layers(config)\n    if args.cpu:\n        breakmodel.gpu_blocks = [0]*n_layers\n        return\n    elif(args.breakmodel_gpulayers is not None or (utils.HAS_ACCELERATE and args.breakmodel_disklayers is not None)):\n        try:\n            if(not args.breakmodel_gpulayers):\n                breakmodel.gpu_blocks = []\n            else:\n                breakmodel.gpu_blocks = list(map(int, args.breakmodel_gpulayers.split(',')))\n            assert len(breakmodel.gpu_blocks) <= torch.cuda.device_count()\n            s = n_layers\n            for i in range(len(breakmodel.gpu_blocks)):\n                if(breakmodel.gpu_blocks[i] <= -1):\n                    breakmodel.gpu_blocks[i] = s\n                    break\n                else:\n                    s -= breakmodel.gpu_blocks[i]\n            assert sum(breakmodel.gpu_blocks) <= n_layers\n            n_layers -= sum(breakmodel.gpu_blocks)\n            if(args.breakmodel_disklayers is not None):\n                assert args.breakmodel_disklayers <= n_layers\n                breakmodel.disk_blocks = args.breakmodel_disklayers\n                n_layers -= args.breakmodel_disklayers\n        except:\n            logger.warning(\"--breakmodel_gpulayers is malformatted. Please use the --help option to see correct usage of --breakmodel_gpulayers. Defaulting to all layers on device 0.\")\n            breakmodel.gpu_blocks = [n_layers]\n            n_layers = 0\n    elif(args.breakmodel_layers is not None):\n        breakmodel.gpu_blocks = [n_layers - max(0, min(n_layers, args.breakmodel_layers))]\n        n_layers -= sum(breakmodel.gpu_blocks)\n    elif(args.model is not None):\n        logger.info(\"Breakmodel not specified, assuming GPU 0\")\n        breakmodel.gpu_blocks = [n_layers]\n        n_layers = 0\n    else:\n        device_count = torch.cuda.device_count()\n        if(device_count > 1):\n            print(colors.CYAN + \"\\nPlease select one of your GPUs to be your primary GPU.\")\n            print(\"VRAM usage in your primary GPU will be higher than for your other ones.\")\n            print(\"It is recommended you make your fastest GPU your primary GPU.\")\n            device_list(n_layers)\n            while(True):\n                primaryselect = input(\"device ID> \")\n                if(primaryselect.isnumeric() and 0 <= int(primaryselect) < device_count):\n                    breakmodel.primary_device = int(primaryselect)\n                    break\n                else:\n                    print(f\"{colors.RED}Please enter an integer between 0 and {device_count-1}.{colors.END}\")\n        else:\n            breakmodel.primary_device = 0\n\n        print(colors.PURPLE + \"\\nIf you don't have enough VRAM to run the model on a single GPU\")\n        print(\"you can split the model between your CPU and your GPU(s), or between\")\n        print(\"multiple GPUs if you have more than one.\")\n        print(\"By putting more 'layers' on a GPU or CPU, more computations will be\")\n        print(\"done on that device and more VRAM or RAM will be required on that device\")\n        print(\"(roughly proportional to number of layers).\")\n        print(\"It should be noted that GPUs are orders of magnitude faster than the CPU.\")\n        print(f\"This model has{colors.YELLOW} {n_layers} {colors.PURPLE}layers.{colors.END}\\n\")\n\n        for i in range(device_count):\n            device_list(n_layers, primary=breakmodel.primary_device, selected=i)\n            print(f\"{colors.CYAN}\\nHow many of the remaining{colors.YELLOW} {n_layers} {colors.CYAN}layers would you like to put into device {i}?\\nYou can also enter -1 to allocate all remaining layers to this device.{colors.END}\\n\")\n            while(True):\n                layerselect = input(\"# of layers> \")\n                if((layerselect.isnumeric() or layerselect.strip() == '-1') and -1 <= int(layerselect) <= n_layers):\n                    layerselect = int(layerselect)\n                    layerselect = n_layers if layerselect == -1 else layerselect\n                    breakmodel.gpu_blocks.append(layerselect)\n                    n_layers -= layerselect\n                    break\n                else:\n                    print(f\"{colors.RED}Please enter an integer between -1 and {n_layers}.{colors.END}\")\n            if(n_layers == 0):\n                break\n\n        if(utils.HAS_ACCELERATE and n_layers > 0):\n            device_list(n_layers, primary=breakmodel.primary_device, selected=-1)\n            print(f\"{colors.CYAN}\\nHow many of the remaining{colors.YELLOW} {n_layers} {colors.CYAN}layers would you like to put into the disk cache?\\nYou can also enter -1 to allocate all remaining layers to this device.{colors.END}\\n\")\n            while(True):\n                layerselect = input(\"# of layers> \")\n                if((layerselect.isnumeric() or layerselect.strip() == '-1') and -1 <= int(layerselect) <= n_layers):\n                    layerselect = int(layerselect)\n                    layerselect = n_layers if layerselect == -1 else layerselect\n                    breakmodel.disk_blocks = layerselect\n                    n_layers -= layerselect\n                    break\n                else:\n                    print(f\"{colors.RED}Please enter an integer between -1 and {n_layers}.{colors.END}\")\n\n    logger.init_ok(\"Final device configuration:\", status=\"Info\")\n    device_list(n_layers, primary=breakmodel.primary_device)\n\n    # If all layers are on the same device, use the old GPU generation mode\n    while(len(breakmodel.gpu_blocks) and breakmodel.gpu_blocks[-1] == 0):\n        breakmodel.gpu_blocks.pop()\n    if(len(breakmodel.gpu_blocks) and breakmodel.gpu_blocks[-1] in (-1, utils.num_layers(config))):\n        vars.breakmodel = False\n        vars.usegpu = True\n        vars.gpu_device = len(breakmodel.gpu_blocks)-1\n        return\n\n    if(not breakmodel.gpu_blocks):\n        logger.warning(\"Nothing assigned to a GPU, reverting to CPU only mode\")\n        import breakmodel\n        breakmodel.primary_device = \"cpu\"\n        vars.breakmodel = False\n        vars.usegpu = False\n        return\n\ndef move_model_to_devices(model):\n    global generator\n\n    if(not utils.HAS_ACCELERATE and not vars.breakmodel):\n        if(vars.usegpu):\n            model = model.half().to(vars.gpu_device)\n        else:\n            model = model.to('cpu').float()\n        generator = model.generate\n        return\n\n    import breakmodel\n\n    if(utils.HAS_ACCELERATE):\n        import accelerate.utils\n        for key, value in model.state_dict().items():\n            target_dtype = torch.float32 if breakmodel.primary_device == \"cpu\" else torch.float16\n            if(value.dtype is not target_dtype):\n                accelerate.utils.set_module_tensor_to_device(model, key, target_dtype)\n        disk_blocks = breakmodel.disk_blocks\n        gpu_blocks = breakmodel.gpu_blocks\n        ram_blocks = len(utils.layers_module_names) - sum(gpu_blocks)\n        cumulative_gpu_blocks = tuple(itertools.accumulate(gpu_blocks))\n        device_map = {}\n        for name in utils.layers_module_names:\n            layer = int(name.rsplit(\".\", 1)[1])\n            device = (\"disk\" if layer < disk_blocks else \"cpu\") if layer < ram_blocks else bisect.bisect_right(cumulative_gpu_blocks, layer - ram_blocks)\n            device_map[name] = device\n        for name in utils.get_missing_module_names(model, list(device_map.keys())):\n            device_map[name] = breakmodel.primary_device\n        breakmodel.dispatch_model_ex(model, device_map, main_device=breakmodel.primary_device, offload_buffers=True, offload_dir=\"accelerate-disk-cache\")\n        gc.collect()\n        generator = model.generate\n        return\n\n    model.half()\n    gc.collect()\n\n    if(hasattr(model, \"transformer\")):\n        model.transformer.wte.to(breakmodel.primary_device)\n        model.transformer.ln_f.to(breakmodel.primary_device)\n        if(hasattr(model, 'lm_head')):\n            model.lm_head.to(breakmodel.primary_device)\n        if(hasattr(model.transformer, 'wpe')):\n            model.transformer.wpe.to(breakmodel.primary_device)\n    elif(not hasattr(model.model, \"decoder\")):\n        model.model.embed_tokens.to(breakmodel.primary_device)\n        model.model.layer_norm.to(breakmodel.primary_device)\n        model.lm_head.to(breakmodel.primary_device)\n        model.model.embed_positions.to(breakmodel.primary_device)\n    else:\n        model.model.decoder.embed_tokens.to(breakmodel.primary_device)\n        if(model.model.decoder.project_in is not None):\n            model.model.decoder.project_in.to(breakmodel.primary_device)\n        if(model.model.decoder.project_out is not None):\n            model.model.decoder.project_out.to(breakmodel.primary_device)\n        model.model.decoder.embed_positions.to(breakmodel.primary_device)\n    gc.collect()\n    GPTNeoModel.forward = breakmodel.new_forward_neo\n    if(\"GPTJModel\" in globals()):\n        GPTJModel.forward = breakmodel.new_forward_neo # type: ignore\n    if(\"XGLMModel\" in globals()):\n        XGLMModel.forward = breakmodel.new_forward_xglm # type: ignore\n    if(\"OPTDecoder\" in globals()):\n        OPTDecoder.forward = breakmodel.new_forward_opt # type: ignore\n    generator = model.generate\n    if(hasattr(model, \"transformer\")):\n        breakmodel.move_hidden_layers(model.transformer)\n    elif(not hasattr(model.model, \"decoder\")):\n        breakmodel.move_hidden_layers(model.model, model.model.layers)\n    else:\n        breakmodel.move_hidden_layers(model.model.decoder, model.model.decoder.layers)\n\n#==================================================================#\n#  Allow the models to override some settings\n#==================================================================#\ndef loadmodelsettings():\n    try:\n        js   = json.loads(str(model_config).partition(' ')[2])\n    except Exception as e:\n        try:\n            try:\n                js   = json.load(open(vars.custmodpth + \"/config.json\", \"r\"))\n            except Exception as e:\n                js   = json.load(open(vars.custmodpth.replace('/', '_') + \"/config.json\", \"r\"))            \n        except Exception as e:\n            js   = {}\n    if vars.model_type == \"xglm\" or js.get(\"compat\", \"j\") == \"fairseq_lm\":\n        vars.newlinemode = \"s\"  # Default to </s> newline mode if using XGLM\n    if vars.model_type == \"opt\" or vars.model_type == \"bloom\":\n        vars.newlinemode = \"ns\"  # Handle </s> but don't convert newlines if using Fairseq models that have newlines trained in them\n    vars.modelconfig = js\n    if(\"badwordsids\" in js):\n        vars.badwordsids = js[\"badwordsids\"]\n    if(\"nobreakmodel\" in js):\n        vars.nobreakmodel = js[\"nobreakmodel\"]\n    if(\"sampler_order\" in js):\n        sampler_order = js[\"sampler_order\"]\n        if(len(sampler_order) < 7):\n            sampler_order = [6] + sampler_order\n        vars.sampler_order = sampler_order\n    if(\"temp\" in js):\n        vars.temp       = js[\"temp\"]\n    if(\"top_p\" in js):\n        vars.top_p      = js[\"top_p\"]\n    if(\"top_k\" in js):\n        vars.top_k      = js[\"top_k\"]\n    if(\"tfs\" in js):\n        vars.tfs        = js[\"tfs\"]\n    if(\"typical\" in js):\n        vars.typical    = js[\"typical\"]\n    if(\"top_a\" in js):\n        vars.top_a      = js[\"top_a\"]\n    if(\"rep_pen\" in js):\n        vars.rep_pen    = js[\"rep_pen\"]\n    if(\"rep_pen_slope\" in js):\n        vars.rep_pen_slope = js[\"rep_pen_slope\"]\n    if(\"rep_pen_range\" in js):\n        vars.rep_pen_range = js[\"rep_pen_range\"]\n    if(\"adventure\" in js):\n        vars.adventure = js[\"adventure\"]\n    if(\"chatmode\" in js):\n        vars.chatmode = js[\"chatmode\"]\n    if(\"dynamicscan\" in js):\n        vars.dynamicscan = js[\"dynamicscan\"]\n    if(\"formatoptns\" in js):\n        vars.formatoptns = js[\"formatoptns\"]\n    if(\"welcome\" in js):\n        vars.welcome = js[\"welcome\"]\n    if(\"newlinemode\" in js):\n        vars.newlinemode = js[\"newlinemode\"]\n    if(\"antemplate\" in js):\n        vars.setauthornotetemplate = js[\"antemplate\"]\n        if(not vars.gamestarted):\n            vars.authornotetemplate = vars.setauthornotetemplate\n\n#==================================================================#\n#  Take settings from vars and write them to client settings file\n#==================================================================#\ndef savesettings():\n     # Build json to write\n    js = {}\n    js[\"apikey\"]      = vars.apikey\n    js[\"andepth\"]     = vars.andepth\n    js[\"sampler_order\"] = vars.sampler_order\n    js[\"temp\"]        = vars.temp\n    js[\"top_p\"]       = vars.top_p\n    js[\"top_k\"]       = vars.top_k\n    js[\"tfs\"]         = vars.tfs\n    js[\"typical\"]     = vars.typical\n    js[\"top_a\"]       = vars.top_a\n    js[\"rep_pen\"]     = vars.rep_pen\n    js[\"rep_pen_slope\"] = vars.rep_pen_slope\n    js[\"rep_pen_range\"] = vars.rep_pen_range\n    js[\"genamt\"]      = vars.genamt\n    js[\"max_length\"]  = vars.max_length\n    js[\"ikgen\"]       = vars.ikgen\n    js[\"formatoptns\"] = vars.formatoptns\n    js[\"numseqs\"]     = vars.numseqs\n    js[\"widepth\"]     = vars.widepth\n    js[\"useprompt\"]   = vars.useprompt\n    js[\"adventure\"]   = vars.adventure\n    js[\"chatmode\"]    = vars.chatmode\n    js[\"chatname\"]    = vars.chatname\n    js[\"dynamicscan\"] = vars.dynamicscan\n    js[\"nopromptgen\"] = vars.nopromptgen\n    js[\"rngpersist\"]  = vars.rngpersist\n    js[\"nogenmod\"]    = vars.nogenmod\n    js[\"fulldeterminism\"] = vars.full_determinism\n    js[\"autosave\"]    = vars.autosave\n    js[\"welcome\"]     = vars.welcome\n    js[\"output_streaming\"] = vars.output_streaming\n    js[\"show_probs\"] = vars.show_probs\n    js[\"show_budget\"] = vars.show_budget\n\n    if(vars.seed_specified):\n        js[\"seed\"]    = vars.seed\n    else:\n        js[\"seed\"]    = None\n\n    js[\"newlinemode\"] = vars.newlinemode\n\n    js[\"antemplate\"]  = vars.setauthornotetemplate\n\n    js[\"userscripts\"] = vars.userscripts\n    js[\"corescript\"]  = vars.corescript\n    js[\"softprompt\"]  = vars.spfilename\n\n    # Write it\n    if not os.path.exists('settings'):\n        os.mkdir('settings')\n    file = open(get_config_filename(), \"w\")\n    try:\n        file.write(json.dumps(js, indent=3))\n    finally:\n        file.close()\n\n#==================================================================#\n#  Don't save settings unless 2 seconds have passed without modification\n#==================================================================#\n@debounce(2)\ndef settingschanged():\n    logger.info(\"Saving settings.\")\n    savesettings()\n\n#==================================================================#\n#  Read settings from client file JSON and send to vars\n#==================================================================#\n\ndef loadsettings():\n    if(path.exists(\"defaults/\" + getmodelname().replace('/', '_') + \".settings\")):\n        # Read file contents into JSON object\n        file = open(\"defaults/\" + getmodelname().replace('/', '_') + \".settings\", \"r\")\n        js   = json.load(file)\n        \n        processsettings(js)\n        file.close()\n    if(path.exists(get_config_filename())):\n        # Read file contents into JSON object\n        file = open(get_config_filename(), \"r\")\n        js   = json.load(file)\n        \n        processsettings(js)\n        file.close()\n        \ndef processsettings(js):\n# Copy file contents to vars\n    if(\"apikey\" in js):\n        # If the model is the HORDE, then previously saved API key in settings\n        # Will always override a new key set.\n        if vars.model != \"CLUSTER\" or vars.apikey == '':\n            vars.apikey = js[\"apikey\"]\n    if(\"andepth\" in js):\n        vars.andepth = js[\"andepth\"]\n    if(\"sampler_order\" in js):\n        sampler_order = js[\"sampler_order\"]\n        if(len(sampler_order) < 7):\n            sampler_order = [6] + sampler_order\n        vars.sampler_order = sampler_order\n    if(\"temp\" in js):\n        vars.temp = js[\"temp\"]\n    if(\"top_p\" in js):\n        vars.top_p = js[\"top_p\"]\n    if(\"top_k\" in js):\n        vars.top_k = js[\"top_k\"]\n    if(\"tfs\" in js):\n        vars.tfs = js[\"tfs\"]\n    if(\"typical\" in js):\n        vars.typical = js[\"typical\"]\n    if(\"top_a\" in js):\n        vars.top_a = js[\"top_a\"]\n    if(\"rep_pen\" in js):\n        vars.rep_pen = js[\"rep_pen\"]\n    if(\"rep_pen_slope\" in js):\n        vars.rep_pen_slope = js[\"rep_pen_slope\"]\n    if(\"rep_pen_range\" in js):\n        vars.rep_pen_range = js[\"rep_pen_range\"]\n    if(\"genamt\" in js):\n        vars.genamt = js[\"genamt\"]\n    if(\"max_length\" in js):\n        vars.max_length = js[\"max_length\"]\n    if(\"ikgen\" in js):\n        vars.ikgen = js[\"ikgen\"]\n    if(\"formatoptns\" in js):\n        vars.formatoptns = js[\"formatoptns\"]\n    if(\"numseqs\" in js):\n        vars.numseqs = js[\"numseqs\"]\n    if(\"widepth\" in js):\n        vars.widepth = js[\"widepth\"]\n    if(\"useprompt\" in js):\n        vars.useprompt = js[\"useprompt\"]\n    if(\"adventure\" in js):\n        vars.adventure = js[\"adventure\"]\n    if(\"chatmode\" in js):\n        vars.chatmode = js[\"chatmode\"]\n    if(\"chatname\" in js):\n        vars.chatname = js[\"chatname\"]\n    if(\"dynamicscan\" in js):\n        vars.dynamicscan = js[\"dynamicscan\"]\n    if(\"nopromptgen\" in js):\n        vars.nopromptgen = js[\"nopromptgen\"]\n    if(\"rngpersist\" in js):\n        vars.rngpersist = js[\"rngpersist\"]\n    if(\"nogenmod\" in js):\n        vars.nogenmod = js[\"nogenmod\"]\n    if(\"fulldeterminism\" in js):\n        vars.full_determinism = js[\"fulldeterminism\"]\n    if(\"autosave\" in js):\n        vars.autosave = js[\"autosave\"]\n    if(\"newlinemode\" in js):\n        vars.newlinemode = js[\"newlinemode\"]\n    if(\"welcome\" in js):\n        vars.welcome = js[\"welcome\"]\n    if(\"output_streaming\" in js):\n        vars.output_streaming = js[\"output_streaming\"]\n    if(\"show_probs\" in js):\n        vars.show_probs = js[\"show_probs\"]\n    if(\"show_budget\" in js):\n        vars.show_budget = js[\"show_budget\"]\n    \n    if(\"seed\" in js):\n        vars.seed = js[\"seed\"]\n        if(vars.seed is not None):\n            vars.seed_specified = True\n        else:\n            vars.seed_specified = False\n    else:\n        vars.seed_specified = False\n\n    if(\"antemplate\" in js):\n        vars.setauthornotetemplate = js[\"antemplate\"]\n        if(not vars.gamestarted):\n            vars.authornotetemplate = vars.setauthornotetemplate\n    \n    if(\"userscripts\" in js):\n        vars.userscripts = []\n        for userscript in js[\"userscripts\"]:\n            if type(userscript) is not str:\n                continue\n            userscript = userscript.strip()\n            if len(userscript) != 0 and all(q not in userscript for q in (\"..\", \":\")) and all(userscript[0] not in q for q in (\"/\", \"\\\\\")) and os.path.exists(fileops.uspath(userscript)):\n                vars.userscripts.append(userscript)\n\n    if(\"corescript\" in js and type(js[\"corescript\"]) is str and all(q not in js[\"corescript\"] for q in (\"..\", \":\")) and all(js[\"corescript\"][0] not in q for q in (\"/\", \"\\\\\"))):\n        vars.corescript = js[\"corescript\"]\n    else:\n        vars.corescript = \"default.lua\"\n\n#==================================================================#\n#  Load a soft prompt from a file\n#==================================================================#\n\ndef check_for_sp_change():\n    while(True):\n        time.sleep(0.05)\n\n        if(vars.sp_changed):\n            with app.app_context():\n                emit('from_server', {'cmd': 'spstatitems', 'data': {vars.spfilename: vars.spmeta} if vars.allowsp and len(vars.spfilename) else {}}, namespace=None, broadcast=True)\n            vars.sp_changed = False\n\n        if(vars.token_stream_queue.queue):\n            # If emit blocks, waiting for it to complete before clearing could\n            # introduce a race condition that drops tokens.\n            queued_tokens = list(vars.token_stream_queue.queue)\n            vars.token_stream_queue.queue.clear()\n            socketio.emit(\"from_server\", {\"cmd\": \"streamtoken\", \"data\": queued_tokens}, namespace=None, broadcast=True)\n\nsocketio.start_background_task(check_for_sp_change)\n\ndef spRequest(filename):\n    if(not vars.allowsp):\n        raise RuntimeError(\"Soft prompts are not supported by your current model/backend\")\n    \n    old_filename = vars.spfilename\n\n    vars.spfilename = \"\"\n    settingschanged()\n\n    if(len(filename) == 0):\n        vars.sp = None\n        vars.sp_length = 0\n        if(old_filename != filename):\n            vars.sp_changed = True\n        return\n\n    global np\n    if 'np' not in globals():\n        import numpy as np\n\n    z, version, shape, fortran_order, dtype = fileops.checksp(filename, vars.modeldim)\n    if not isinstance(z, zipfile.ZipFile):\n        raise RuntimeError(f\"{repr(filename)} is not a valid soft prompt file\")\n    with z.open('meta.json') as f:\n        vars.spmeta = json.load(f)\n    z.close()\n\n    with np.load(fileops.sppath(filename), allow_pickle=False) as f:\n        tensor = f['tensor.npy']\n\n    # If the tensor is in bfloat16 format, convert it to float32\n    if(tensor.dtype == 'V2'):\n        tensor.dtype = np.uint16\n        tensor = np.uint32(tensor) << 16\n        tensor.dtype = np.float32\n\n    if(tensor.dtype != np.float16):\n        tensor = np.float32(tensor)\n    assert not np.isinf(tensor).any() and not np.isnan(tensor).any()\n\n    vars.sp_length = tensor.shape[-2]\n    vars.spmeta[\"n_tokens\"] = vars.sp_length\n\n    if(vars.use_colab_tpu or vars.model in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")):\n        rows = tensor.shape[0]\n        padding_amount = tpu_mtj_backend.params[\"seq\"] - (tpu_mtj_backend.params[\"seq\"] % -tpu_mtj_backend.params[\"cores_per_replica\"]) - rows\n        tensor = np.pad(tensor, ((0, padding_amount), (0, 0)))\n        tensor = tensor.reshape(\n            tpu_mtj_backend.params[\"cores_per_replica\"],\n            -1,\n            tpu_mtj_backend.params.get(\"d_embed\", tpu_mtj_backend.params[\"d_model\"]),\n        )\n        vars.sp = tpu_mtj_backend.shard_xmap(np.float32(tensor))\n    else:\n        vars.sp = torch.from_numpy(tensor)\n\n    vars.spfilename = filename\n    settingschanged()\n    if(old_filename != filename):\n            vars.sp_changed = True\n\n#==================================================================#\n# Startup\n#==================================================================#\ndef general_startup(override_args=None):\n    global args\n    # Parsing Parameters\n    parser = argparse.ArgumentParser(description=\"KoboldAI Server\")\n    parser.add_argument(\"--remote\", action='store_true', help=\"Optimizes KoboldAI for Remote Play\")\n    parser.add_argument(\"--noaimenu\", action='store_true', help=\"Disables the ability to select the AI\")\n    parser.add_argument(\"--ngrok\", action='store_true', help=\"Optimizes KoboldAI for Remote Play using Ngrok\")\n    parser.add_argument(\"--localtunnel\", action='store_true', help=\"Optimizes KoboldAI for Remote Play using Localtunnel\")\n    parser.add_argument(\"--host\", action='store_true', help=\"Optimizes KoboldAI for Remote Play without using a proxy service\")\n    parser.add_argument(\"--port\", type=int, help=\"Specify the port on which the application will be joinable\")\n    parser.add_argument(\"--aria2_port\", type=int, help=\"Specify the port on which aria2's RPC interface will be open if aria2 is installed (defaults to 6799)\")\n    parser.add_argument(\"--model\", help=\"Specify the Model Type to skip the Menu\")\n    parser.add_argument(\"--path\", help=\"Specify the Path for local models (For model NeoCustom or GPT2Custom)\")\n    parser.add_argument(\"--apikey\", help=\"Specify the API key to use for online services\")\n    parser.add_argument(\"--req_model\", type=str, action='append', required=False, help=\"Which models which we allow to generate for us during cluster mode. Can be specified multiple times.\")\n    parser.add_argument(\"--revision\", help=\"Specify the model revision for huggingface models (can be a git branch/tag name or a git commit hash)\")\n    parser.add_argument(\"--cpu\", action='store_true', help=\"By default unattended launches are on the GPU use this option to force CPU usage.\")\n    parser.add_argument(\"--breakmodel\", action='store_true', help=argparse.SUPPRESS)\n    parser.add_argument(\"--breakmodel_layers\", type=int, help=argparse.SUPPRESS)\n    parser.add_argument(\"--breakmodel_gpulayers\", type=str, help=\"If using a model that supports hybrid generation, this is a comma-separated list that specifies how many layers to put on each GPU device. For example to put 8 layers on device 0, 9 layers on device 1 and 11 layers on device 2, use --breakmodel_gpulayers 8,9,11\")\n    parser.add_argument(\"--breakmodel_disklayers\", type=int, help=\"If using a model that supports hybrid generation, this is the number of layers to put in disk cache.\")\n    parser.add_argument(\"--override_delete\", action='store_true', help=\"Deleting stories from inside the browser is disabled if you are using --remote and enabled otherwise. Using this option will instead allow deleting stories if using --remote and prevent deleting stories otherwise.\")\n    parser.add_argument(\"--override_rename\", action='store_true', help=\"Renaming stories from inside the browser is disabled if you are using --remote and enabled otherwise. Using this option will instead allow renaming stories if using --remote and prevent renaming stories otherwise.\")\n    parser.add_argument(\"--configname\", help=\"Force a fixed configuration name to aid with config management.\")\n    parser.add_argument(\"--colab\", action='store_true', help=\"Optimize for Google Colab.\")\n    parser.add_argument(\"--nobreakmodel\", action='store_true', help=\"Disables Breakmodel support completely.\")\n    parser.add_argument(\"--unblock\", action='store_true', default=False, help=\"Unblocks the KoboldAI port to be accessible from other machines without optimizing for remote play (It is recommended to use --host instead)\")\n    parser.add_argument(\"--quiet\", action='store_true', default=False, help=\"If present will suppress any story related text from showing on the console\")\n    parser.add_argument(\"--no_aria2\", action='store_true', default=False, help=\"Prevents KoboldAI from using aria2 to download huggingface models more efficiently, in case aria2 is causing you issues\")\n    parser.add_argument(\"--lowmem\", action='store_true', help=\"Extra Low Memory loading for the GPU, slower but memory does not peak to twice the usage\")\n    parser.add_argument(\"--savemodel\", action='store_true', help=\"Saves the model to the models folder even if --colab is used (Allows you to save models to Google Drive)\")\n    parser.add_argument(\"--customsettings\", help=\"Preloads arguements from json file. You only need to provide the location of the json file. Use customsettings.json template file. It can be renamed if you wish so that you can store multiple configurations. Leave any settings you want as default as null. Any values you wish to set need to be in double quotation marks\")\n    parser.add_argument(\"--no_ui\", action='store_true', default=False, help=\"Disables the GUI and Socket.IO server while leaving the API server running.\")\n    parser.add_argument('-v', '--verbosity', action='count', default=0, help=\"The default logging level is ERROR or higher. This value increases the amount of logging seen in your screen\")\n    parser.add_argument('-q', '--quiesce', action='count', default=0, help=\"The default logging level is ERROR or higher. This value decreases the amount of logging seen in your screen\")\n\n    #args: argparse.Namespace = None\n    if \"pytest\" in sys.modules and override_args is None:\n        args = parser.parse_args([])\n        return\n    if override_args is not None:\n        import shlex\n        args = parser.parse_args(shlex.split(override_args))\n    elif(os.environ.get(\"KOBOLDAI_ARGS\") is not None):\n        import shlex\n        args = parser.parse_args(shlex.split(os.environ[\"KOBOLDAI_ARGS\"]))\n    else:\n        args = parser.parse_args()\n    \n    utils.args = args\n\n    set_logger_verbosity(args.verbosity)\n    quiesce_logger(args.quiesce)\n    if args.customsettings:\n        f = open (args.customsettings)\n        importedsettings = json.load(f)\n        for items in importedsettings:\n            if importedsettings[items] is not None:\n                setattr(args, items, importedsettings[items])            \n        f.close()\n    \n    if args.no_ui:\n        def new_emit(*args, **kwargs):\n            return\n        old_emit = socketio.emit\n        socketio.emit = new_emit\n\n    vars.model = args.model;\n    vars.revision = args.revision\n\n    if args.apikey:\n        vars.apikey = args.apikey\n    if args.req_model:\n        vars.cluster_requested_models = args.req_model\n\n    if args.colab:\n        args.remote = True;\n        args.override_rename = True;\n        args.override_delete = True;\n        args.nobreakmodel = True;\n        args.quiet = True;\n        args.lowmem = True;\n        args.noaimenu = True;\n\n    if args.quiet:\n        vars.quiet = True\n\n    if args.nobreakmodel:\n        vars.nobreakmodel = True;\n\n    if args.remote:\n        vars.host = True;\n\n    if args.ngrok:\n        vars.host = True;\n\n    if args.localtunnel:\n        vars.host = True;\n\n    if args.host:\n        vars.host = True;\n\n    if args.cpu:\n        vars.use_colab_tpu = False\n\n    vars.smandelete = vars.host == args.override_delete\n    vars.smanrename = vars.host == args.override_rename\n\n    vars.aria2_port = args.aria2_port or 6799\n    \n    #Now let's look to see if we are going to force a load of a model from a user selected folder\n    if(vars.model == \"selectfolder\"):\n        print(\"{0}Please choose the folder where pytorch_model.bin is located:{1}\\n\".format(colors.CYAN, colors.END))\n        modpath = fileops.getdirpath(getcwd() + \"/models\", \"Select Model Folder\")\n    \n        if(modpath):\n            # Save directory to vars\n            vars.model = \"NeoCustom\"\n            vars.custmodpth = modpath\n    elif args.model:\n        logger.message(f\"Welcome to KoboldAI!\")\n        logger.message(f\"You have selected the following Model: {vars.model}\")\n        if args.path:\n            logger.message(f\"You have selected the following path for your Model: {args.path}\")\n            vars.custmodpth = args.path;\n            vars.colaburl = args.path + \"/request\"; # Lets just use the same parameter to keep it simple\n#==================================================================#\n# Load Model\n#==================================================================# \n\ndef tpumtjgetsofttokens():\n    soft_tokens = None\n    if(vars.sp is None):\n        global np\n        if 'np' not in globals():\n            import numpy as np\n        tensor = np.zeros((1, tpu_mtj_backend.params.get(\"d_embed\", tpu_mtj_backend.params[\"d_model\"])), dtype=np.float32)\n        rows = tensor.shape[0]\n        padding_amount = tpu_mtj_backend.params[\"seq\"] - (tpu_mtj_backend.params[\"seq\"] % -tpu_mtj_backend.params[\"cores_per_replica\"]) - rows\n        tensor = np.pad(tensor, ((0, padding_amount), (0, 0)))\n        tensor = tensor.reshape(\n            tpu_mtj_backend.params[\"cores_per_replica\"],\n            -1,\n            tpu_mtj_backend.params.get(\"d_embed\", tpu_mtj_backend.params[\"d_model\"]),\n        )\n        vars.sp = tpu_mtj_backend.shard_xmap(tensor)\n    soft_tokens = np.arange(\n        tpu_mtj_backend.params[\"n_vocab\"] + tpu_mtj_backend.params[\"n_vocab_padding\"],\n        tpu_mtj_backend.params[\"n_vocab\"] + tpu_mtj_backend.params[\"n_vocab_padding\"] + vars.sp_length,\n        dtype=np.uint32\n    )\n    return soft_tokens\n \ndef get_model_info(model, directory=\"\"):\n    # if the model is in the api list\n    disk_blocks = 0\n    key = False\n    breakmodel = False\n    gpu = False\n    layer_count = None\n    key_value = \"\"\n    break_values = []\n    url = False\n    default_url = None\n    models_on_url = False\n    multi_online_models = False\n    gpu_count = torch.cuda.device_count()\n    gpu_names = []\n    send_horde_models = False\n    for i in range(gpu_count):\n        gpu_names.append(torch.cuda.get_device_name(i))\n    if model in ['Colab', 'API']:\n        url = True\n    elif model == 'CLUSTER':\n        models_on_url = True\n        url = True\n        key = True\n        default_url = 'https://horde.koboldai.net'\n        multi_online_models = True\n        if path.exists(get_config_filename(model)):\n            with open(get_config_filename(model), \"r\") as file:\n                # Check if API key exists\n                js = json.load(file)\n                if(\"apikey\" in js and js[\"apikey\"] != \"\"):\n                    # API key exists, grab it and close the file\n                    key_value = js[\"apikey\"]\n                elif 'oaiapikey' in js and js['oaiapikey'] != \"\":\n                    key_value = js[\"oaiapikey\"]\n                if 'url' in js and js['url'] != \"\":\n                    url = js['url']\n            if key_value != \"\":\n                send_horde_models = True\n    elif model in [x[1] for x in model_menu['apilist']]:\n        if path.exists(get_config_filename(model)):\n            with open(get_config_filename(model), \"r\") as file:\n                # Check if API key exists\n                js = json.load(file)\n                if(\"apikey\" in js and js[\"apikey\"] != \"\"):\n                    # API key exists, grab it and close the file\n                    key_value = js[\"apikey\"]\n                elif 'oaiapikey' in js and js['oaiapikey'] != \"\":\n                    key_value = js[\"oaiapikey\"]\n        key = True\n    elif model == 'ReadOnly':\n        pass\n    elif not utils.HAS_ACCELERATE and not torch.cuda.is_available():\n        pass\n    elif args.cpu:\n        pass\n    else:\n        layer_count = get_layer_count(model, directory=directory)\n        if layer_count is None:\n            breakmodel = False\n            gpu = True\n        else:\n            breakmodel = True\n            if model in [\"NeoCustom\", \"GPT2Custom\"]:\n                filename = \"settings/{}.breakmodel\".format(os.path.basename(os.path.normpath(directory)))\n            else:\n                filename = \"settings/{}.breakmodel\".format(model.replace(\"/\", \"_\"))\n            if path.exists(filename):\n                with open(filename, \"r\") as file:\n                    data = file.read().split(\"\\n\")[:2]\n                    if len(data) < 2:\n                        data.append(\"0\")\n                    break_values, disk_blocks = data\n                    break_values = break_values.split(\",\")\n            else:\n                break_values = [layer_count]\n            break_values += [0] * (gpu_count - len(break_values))\n    #print(\"Model_info: {}\".format({'cmd': 'selected_model_info', 'key_value': key_value, 'key':key, \n    #                     'gpu':gpu, 'layer_count':layer_count, 'breakmodel':breakmodel, \n    #                     'break_values': break_values, 'gpu_count': gpu_count,\n    #                     'url': url, 'gpu_names': gpu_names}))\n    emit('from_server', {'cmd': 'selected_model_info', 'key_value': key_value, 'key':key, \n                         'gpu':gpu, 'layer_count':layer_count, 'breakmodel':breakmodel, \n                         'disk_break_value': disk_blocks, 'accelerate': utils.HAS_ACCELERATE,\n                         'break_values': break_values, 'gpu_count': gpu_count, 'multi_online_models': multi_online_models,\n                         'url': url, 'default_url': default_url, 'gpu_names': gpu_names, 'models_on_url': models_on_url}, broadcast=True)\n    if send_horde_models:\n        get_cluster_models({'key': key_value, 'url': default_url})\n    elif key_value != \"\" and model in [x[1] for x in model_menu['apilist']] and model != 'CLUSTER':\n        get_oai_models(key_value)\n    \n\ndef get_layer_count(model, directory=\"\"):\n    if(model not in [\"InferKit\", \"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"GooseAI\" , \"ReadOnly\", \"TPUMeshTransformerGPTJ\"]):\n        if(model == \"GPT2Custom\"):\n            with open(os.path.join(directory, \"config.json\"), \"r\") as f:\n                model_config = json.load(f)\n        # Get the model_type from the config or assume a model type if it isn't present\n        else:\n            if(directory):\n                model = directory\n            from transformers import AutoConfig\n            if(os.path.isdir(model.replace('/', '_'))):\n                model_config = AutoConfig.from_pretrained(model.replace('/', '_'), revision=args.revision, cache_dir=\"cache\")\n            elif(os.path.isdir(\"models/{}\".format(model.replace('/', '_')))):\n                model_config = AutoConfig.from_pretrained(\"models/{}\".format(model.replace('/', '_')), revision=args.revision, cache_dir=\"cache\")\n            elif(os.path.isdir(directory)):\n                model_config = AutoConfig.from_pretrained(directory, revision=args.revision, cache_dir=\"cache\")\n            else:\n                model_config = AutoConfig.from_pretrained(model, revision=args.revision, cache_dir=\"cache\")\n        try:\n            if ((utils.HAS_ACCELERATE and model_config.model_type != 'gpt2') or model_config.model_type in (\"gpt_neo\", \"gptj\", \"xglm\", \"opt\")) and not vars.nobreakmodel:\n                return utils.num_layers(model_config)\n            else:\n                return None\n        except:\n            return None\n    else:\n        return None\n\ndef get_oai_models(key):\n    vars.oaiapikey = key\n    if vars.model_selected == 'OAI':\n        url = \"https://api.openai.com/v1/engines\"\n    elif vars.model_selected == 'GooseAI':\n        url = \"https://api.goose.ai/v1/engines\"\n    else:\n        return\n        \n    # Get list of models from OAI\n    logger.init(\"OAI Engines\", status=\"Retrieving\")\n    req = requests.get(\n        url, \n        headers = {\n            'Authorization': 'Bearer '+key\n            }\n        )\n    if(req.status_code == 200):\n        engines = req.json()[\"data\"]\n        try:\n            engines = [[en[\"id\"], \"{} ({})\".format(en['id'], \"Ready\" if en[\"ready\"] == True else \"Not Ready\")] for en in engines]\n        except:\n            logger.error(engines)\n            raise\n        \n        online_model = \"\"\n        changed=False\n        \n        #Save the key\n        if not path.exists(\"settings\"):\n            # If the client settings file doesn't exist, create it\n            # Write API key to file\n            os.makedirs('settings', exist_ok=True)\n        if path.exists(get_config_filename(vars.model_selected)):\n            with open(get_config_filename(vars.model_selected), \"r\") as file:\n                js = json.load(file)\n                if 'online_model' in js:\n                    online_model = js['online_model']\n                if \"apikey\" in js:\n                    if js['apikey'] != key:\n                        changed=True\n        else:\n            changed=True\n        if changed:\n            js={}\n            with open(get_config_filename(vars.model_selected), \"w\") as file:\n                js[\"apikey\"] = key\n                file.write(json.dumps(js, indent=3))\n            \n        logger.init_ok(\"OAI Engines\", status=\"OK\")\n        emit('from_server', {'cmd': 'oai_engines', 'data': engines, 'online_model': online_model}, broadcast=True)\n    else:\n        # Something went wrong, print the message and quit since we can't initialize an engine\n        logger.init_err(\"OAI Engines\", status=\"Failed\")\n        logger.error(req.json())\n        emit('from_server', {'cmd': 'errmsg', 'data': req.json()})\n\ndef get_cluster_models(msg):\n    vars.oaiapikey = msg['key']\n    vars.apikey = vars.oaiapikey\n    url = msg['url']\n    # Get list of models from public cluster\n    logger.init(\"KAI Horde Models\", status=\"Retrieving\")\n    try:\n        req = requests.get(f\"{url}/api/v2/status/models?type=text\")\n    except requests.exceptions.ConnectionError:\n        logger.init_err(\"KAI Horde Models\", status=\"Failed\")\n        logger.error(\"Provided KoboldAI Horde URL unreachable\")\n        emit('from_server', {'cmd': 'errmsg', 'data': \"Provided KoboldAI Horde URL unreachable\"})\n        return\n    if(not req.ok):\n        # Something went wrong, print the message and quit since we can't initialize an engine\n        logger.init_err(\"KAI Horde Models\", status=\"Failed\")\n        logger.error(req.json())\n        emit('from_server', {'cmd': 'errmsg', 'data': req.json()})\n        return\n\n    engines = req.json()\n    logger.debug(engines)\n    try:\n        engines = [[en[\"name\"], en[\"name\"]] for en in engines]\n    except:\n        logger.error(engines)\n        raise\n    logger.debug(engines)\n    \n    online_model = \"\"\n    changed=False\n    \n    #Save the key\n    if not path.exists(\"settings\"):\n        # If the client settings file doesn't exist, create it\n        # Write API key to file\n        os.makedirs('settings', exist_ok=True)\n    if path.exists(get_config_filename(vars.model_selected)):\n        with open(get_config_filename(vars.model_selected), \"r\") as file:\n            js = json.load(file)\n            if 'online_model' in js:\n                online_model = js['online_model']\n            if \"apikey\" in js:\n                if js['apikey'] != vars.oaiapikey:\n                    changed=True\n    else:\n        changed=True\n    if changed:\n        js={}\n        with open(get_config_filename(vars.model_selected), \"w\") as file:\n            js[\"apikey\"] = vars.oaiapikey\n            js[\"url\"] = url\n            file.write(json.dumps(js, indent=3))\n        \n    logger.init_ok(\"KAI Horde Models\", status=\"OK\")\n    emit('from_server', {'cmd': 'oai_engines', 'data': engines, 'online_model': online_model}, broadcast=True)\n\n\n# Function to patch transformers to use our soft prompt\ndef patch_causallm(model):\n    from torch.nn import Embedding\n    if(getattr(Embedding, \"_koboldai_patch_causallm_model\", None)):\n        Embedding._koboldai_patch_causallm_model = model\n        return model\n    old_embedding_call = Embedding.__call__\n    def new_embedding_call(self, input_ids, *args, **kwargs):\n        if(Embedding._koboldai_patch_causallm_model.get_input_embeddings() is not self):\n            return old_embedding_call(self, input_ids, *args, **kwargs)\n        assert input_ids is not None\n        if(vars.sp is not None):\n            shifted_input_ids = input_ids - model.config.vocab_size\n        input_ids.clamp_(max=model.config.vocab_size-1)\n        inputs_embeds = old_embedding_call(self, input_ids, *args, **kwargs)\n        if(vars.sp is not None):\n            vars.sp = vars.sp.to(inputs_embeds.dtype).to(inputs_embeds.device)\n            inputs_embeds = torch.where(\n                (shifted_input_ids >= 0)[..., None],\n                vars.sp[shifted_input_ids.clamp(min=0)],\n                inputs_embeds,\n            )\n        return inputs_embeds\n    Embedding.__call__ = new_embedding_call\n    Embedding._koboldai_patch_causallm_model = model\n    return model\n\ndef patch_transformers_download():\n    global transformers\n    import copy, requests, tqdm, time\n    class Send_to_socketio(object):\n        def write(self, bar):\n            bar = bar.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n            if bar != \"\":\n                try:\n                    print(bar, end=\"\\r\")\n                    emit('from_server', {'cmd': 'model_load_status', 'data': bar.replace(\" \", \"&nbsp;\")}, broadcast=True)\n                    eventlet.sleep(seconds=0)\n                except:\n                    pass\n    def http_get(\n        url: str,\n        temp_file,\n        proxies=None,\n        resume_size=0,\n        headers=None,\n        file_name=None,\n    ):\n        \"\"\"\n        Download remote file. Do not gobble up errors.\n        \"\"\"\n        headers = copy.deepcopy(headers)\n        if resume_size > 0:\n            headers[\"Range\"] = f\"bytes={resume_size}-\"\n        r = requests.get(url, stream=True, proxies=proxies, headers=headers)\n        transformers.utils.hub._raise_for_status(r)\n        content_length = r.headers.get(\"Content-Length\")\n        total = resume_size + int(content_length) if content_length is not None else None\n        # `tqdm` behavior is determined by `utils.logging.is_progress_bar_enabled()`\n        # and can be set using `utils.logging.enable/disable_progress_bar()`\n        if url[-11:] != 'config.json':\n            progress = tqdm.tqdm(\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n                total=total,\n                initial=resume_size,\n                desc=f\"Downloading {file_name}\" if file_name is not None else \"Downloading\",\n                file=Send_to_socketio(),\n            )\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                if url[-11:] != 'config.json':\n                    progress.update(len(chunk))\n                temp_file.write(chunk)\n        if url[-11:] != 'config.json':\n            progress.close()\n\n    transformers.utils.hub.http_get = http_get\n    \n\ndef patch_transformers():\n    global transformers\n    \n    patch_transformers_download()\n    \n    old_from_pretrained = PreTrainedModel.from_pretrained.__func__\n    @classmethod\n    def new_from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        vars.fp32_model = False\n        utils.num_shards = None\n        utils.current_shard = 0\n        utils.from_pretrained_model_name = pretrained_model_name_or_path\n        utils.from_pretrained_index_filename = None\n        utils.from_pretrained_kwargs = kwargs\n        utils.bar = None\n        if not args.no_aria2:\n            utils.aria2_hook(pretrained_model_name_or_path, **kwargs)\n        return old_from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n    if(not hasattr(PreTrainedModel, \"_kai_patched\")):\n        PreTrainedModel.from_pretrained = new_from_pretrained\n        PreTrainedModel._kai_patched = True\n    if(hasattr(modeling_utils, \"get_checkpoint_shard_files\")):\n        old_get_checkpoint_shard_files = modeling_utils.get_checkpoint_shard_files\n        def new_get_checkpoint_shard_files(pretrained_model_name_or_path, index_filename, *args, **kwargs):\n            utils.num_shards = utils.get_num_shards(index_filename)\n            utils.from_pretrained_index_filename = index_filename\n            return old_get_checkpoint_shard_files(pretrained_model_name_or_path, index_filename, *args, **kwargs)\n        modeling_utils.get_checkpoint_shard_files = new_get_checkpoint_shard_files\n        \n    # Some versions of transformers 4.17.0.dev0 are affected by\n    # https://github.com/huggingface/transformers/issues/15736\n    # This is a workaround for those versions of transformers.\n    if(transformers_version == \"4.17.0.dev0\"):\n        try:\n            from transformers.models.xglm.modeling_xglm import XGLMSinusoidalPositionalEmbedding\n        except ImportError:\n            pass\n        else:\n            @torch.no_grad()\n            def new_forward(self, input_ids: torch.Tensor = None, inputs_embeds: torch.Tensor = None, past_key_values_length: int = 0):\n                bsz, seq_len = inputs_embeds.size()[:-1]\n                input_shape = inputs_embeds.size()[:-1]\n                sequence_length = input_shape[1]\n                position_ids = torch.arange(\n                    past_key_values_length + self.padding_idx + 1, past_key_values_length + sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n                ).unsqueeze(0).expand(input_shape).contiguous()\n                max_pos = self.padding_idx + 1 + seq_len + past_key_values_length\n                if max_pos > self.weights.size(0):\n                    self.make_weights(max_pos + self.offset, self.embedding_dim, self.padding_idx)\n                return self.weights.index_select(0, position_ids.view(-1)).view(bsz, seq_len, -1).detach()\n            XGLMSinusoidalPositionalEmbedding.forward = new_forward\n\n\n    # Fix a bug in OPTForCausalLM where self.lm_head is the wrong size\n    if(packaging.version.parse(\"4.19.0.dev0\") <= packaging.version.parse(transformers_version) < packaging.version.parse(\"4.20.0\")):\n        try:\n            from transformers import OPTForCausalLM, OPTModel\n        except ImportError:\n            pass\n        else:\n            # This is the same as the original __init__ but with\n            # config.hidden_size\n            # replaced with\n            # config.word_embed_proj_dim\n            def new_init(self, config):\n                super(OPTForCausalLM, self).__init__(config)\n                self.model = OPTModel(config)\n                self.lm_head = torch.nn.Linear(config.word_embed_proj_dim, config.vocab_size, bias=False)\n                self.post_init()\n            OPTForCausalLM.__init__ = new_init\n\n\n    # Patch transformers to use our custom logit warpers\n    from transformers import LogitsProcessorList, LogitsWarper, LogitsProcessor, TopKLogitsWarper, TopPLogitsWarper, TemperatureLogitsWarper, RepetitionPenaltyLogitsProcessor\n    from warpers import AdvancedRepetitionPenaltyLogitsProcessor, TailFreeLogitsWarper, TypicalLogitsWarper, TopALogitsWarper\n\n    def dynamic_processor_wrap(cls, field_name, var_name, cond=None):\n        old_call = cls.__call__\n        def new_call(self, *args, **kwargs):\n            if(not isinstance(field_name, str) and isinstance(field_name, Iterable)):\n                conds = []\n                for f, v in zip(field_name, var_name):\n                    conds.append(getattr(vars, v))\n                    setattr(self, f, conds[-1])\n            else:\n                conds = getattr(vars, var_name)\n                setattr(self, field_name, conds)\n            assert len(args) == 2\n            if(cond is None or cond(conds)):\n                return old_call(self, *args, **kwargs)\n            return args[1]\n        cls.__call__ = new_call\n    dynamic_processor_wrap(AdvancedRepetitionPenaltyLogitsProcessor, (\"penalty\", \"penalty_slope\", \"penalty_range\"), (\"rep_pen\", \"rep_pen_slope\", \"rep_pen_range\"), cond=lambda x: x[0] != 1.0)\n    dynamic_processor_wrap(TopKLogitsWarper, \"top_k\", \"top_k\", cond=lambda x: x > 0)\n    dynamic_processor_wrap(TopALogitsWarper, \"top_a\", \"top_a\", cond=lambda x: x > 0.0)\n    dynamic_processor_wrap(TopPLogitsWarper, \"top_p\", \"top_p\", cond=lambda x: x < 1.0)\n    dynamic_processor_wrap(TailFreeLogitsWarper, \"tfs\", \"tfs\", cond=lambda x: x < 1.0)\n    dynamic_processor_wrap(TypicalLogitsWarper, \"typical\", \"typical\", cond=lambda x: x < 1.0)\n    dynamic_processor_wrap(TemperatureLogitsWarper, \"temperature\", \"temp\", cond=lambda x: x != 1.0)\n\n    class LuaLogitsProcessor(LogitsProcessor):\n\n        def __init__(self):\n            pass\n\n        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n            assert scores.ndim == 2\n            assert input_ids.ndim == 2\n            self.regeneration_required = False\n            self.halt = False\n\n            if(vars.standalone):\n                return scores\n\n            scores_shape = scores.shape\n            scores_list = scores.tolist()\n            vars.lua_koboldbridge.logits = vars.lua_state.table()\n            for r, row in enumerate(scores_list):\n                vars.lua_koboldbridge.logits[r+1] = vars.lua_state.table(*row)\n            vars.lua_koboldbridge.vocab_size = scores_shape[-1]\n\n            execute_genmod()\n\n            scores = torch.tensor(\n                tuple(tuple(row.values()) for row in vars.lua_koboldbridge.logits.values()),\n                device=scores.device,\n                dtype=scores.dtype,\n            )\n            assert scores.shape == scores_shape\n\n            return scores\n\n    from torch.nn import functional as F\n\n    def visualize_probabilities(scores: torch.FloatTensor) -> None:\n        assert scores.ndim == 2\n\n        if vars.numseqs > 1 or not vars.show_probs:\n            return\n\n        probs = F.softmax(scores, dim = -1).cpu().numpy()[0]\n        token_prob_info = []\n        for token_id, score in sorted(enumerate(probs), key=lambda x: x[1], reverse=True)[:8]:\n            token_prob_info.append({\n                \"tokenId\": token_id,\n                \"decoded\": utils.decodenewlines(tokenizer.decode(token_id)),\n                \"score\": float(score),\n            })\n\n        vars.token_stream_queue.probability_buffer = token_prob_info\n    \n    def new_get_logits_processor(*args, **kwargs) -> LogitsProcessorList:\n        processors = new_get_logits_processor.old_get_logits_processor(*args, **kwargs)\n        processors.insert(0, LuaLogitsProcessor())\n        return processors\n    new_get_logits_processor.old_get_logits_processor = transformers.generation_utils.GenerationMixin._get_logits_processor\n    transformers.generation_utils.GenerationMixin._get_logits_processor = new_get_logits_processor\n\n    class KoboldLogitsWarperList(LogitsProcessorList):\n        def __init__(self, beams: int = 1, **kwargs):\n            self.__warper_list: List[LogitsWarper] = []\n            self.__warper_list.append(TopKLogitsWarper(top_k=1, min_tokens_to_keep=1 + (beams > 1)))\n            self.__warper_list.append(TopALogitsWarper(top_a=0.5, min_tokens_to_keep=1 + (beams > 1)))\n            self.__warper_list.append(TopPLogitsWarper(top_p=0.5, min_tokens_to_keep=1 + (beams > 1)))\n            self.__warper_list.append(TailFreeLogitsWarper(tfs=0.5, min_tokens_to_keep=1 + (beams > 1)))\n            self.__warper_list.append(TypicalLogitsWarper(typical=0.5, min_tokens_to_keep=1 + (beams > 1)))\n            self.__warper_list.append(TemperatureLogitsWarper(temperature=0.5))\n            self.__warper_list.append(AdvancedRepetitionPenaltyLogitsProcessor())\n\n        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, *args, **kwargs):\n            sampler_order = vars.sampler_order[:]\n            if len(sampler_order) < 7:  # Add repetition penalty at beginning if it's not present\n                sampler_order = [6] + sampler_order\n            for k in sampler_order:\n                scores = self.__warper_list[k](input_ids, scores, *args, **kwargs)\n            visualize_probabilities(scores)\n            return scores\n\n    def new_get_logits_warper(beams: int = 1,) -> LogitsProcessorList:\n        return KoboldLogitsWarperList(beams=beams)\n    \n    def new_sample(self, *args, **kwargs):\n        assert kwargs.pop(\"logits_warper\", None) is not None\n        kwargs[\"logits_warper\"] = new_get_logits_warper(\n            beams=1,\n        )\n        if(vars.newlinemode == \"s\") or (vars.newlinemode == \"ns\"):\n            kwargs[\"eos_token_id\"] = -1\n            kwargs.setdefault(\"pad_token_id\", 2)\n        return new_sample.old_sample(self, *args, **kwargs)\n    new_sample.old_sample = transformers.generation_utils.GenerationMixin.sample\n    transformers.generation_utils.GenerationMixin.sample = new_sample\n\n\n    # Allow bad words filter to ban <|endoftext|> token\n    import transformers.generation_logits_process\n    def new_init(self, bad_words_ids: List[List[int]], eos_token_id: int):\n        return new_init.old_init(self, bad_words_ids, -1)\n    new_init.old_init = transformers.generation_logits_process.NoBadWordsLogitsProcessor.__init__\n    transformers.generation_logits_process.NoBadWordsLogitsProcessor.__init__ = new_init\n\n    class TokenStreamer(StoppingCriteria):\n        # A StoppingCriteria is used here because it seems to run after\n        # everything has been evaluated score-wise. \n        def __init__(self, tokenizer):\n            self.tokenizer = tokenizer\n\n        def __call__(\n            self,\n            input_ids: torch.LongTensor,\n            scores: torch.FloatTensor,\n            **kwargs,\n        ) -> bool:\n            # Do not intermingle multiple generations' outputs!\n            if vars.numseqs > 1:\n                return False\n\n            if not (vars.show_probs or vars.output_streaming):\n                return False\n\n            if vars.chatmode:\n                return False\n            tokenizer_text = utils.decodenewlines(tokenizer.decode(input_ids[0, -1]))\n            vars.token_stream_queue.add_text(tokenizer_text)\n            return False\n\n\n    # Sets up dynamic world info scanner\n    class DynamicWorldInfoScanCriteria(StoppingCriteria):\n        def __init__(\n            self,\n            tokenizer,\n            excluded_world_info: List[Set],\n        ):\n            self.regeneration_required = False\n            self.halt = False\n            self.tokenizer = tokenizer\n            self.excluded_world_info = excluded_world_info\n        def __call__(\n            self,\n            input_ids: torch.LongTensor,\n            scores: torch.FloatTensor,\n            **kwargs,\n        ) -> bool:\n            vars.generated_tkns += 1\n            if(not vars.standalone and vars.lua_koboldbridge.generated_cols and vars.generated_tkns != vars.lua_koboldbridge.generated_cols):\n                raise RuntimeError(f\"Inconsistency detected between KoboldAI Python and Lua backends ({vars.generated_tkns} != {vars.lua_koboldbridge.generated_cols})\")\n            if(vars.abort or vars.generated_tkns >= vars.genamt):\n                self.regeneration_required = False\n                self.halt = False\n                return True\n            if(vars.standalone):\n                return False\n\n            assert input_ids.ndim == 2\n            assert len(self.excluded_world_info) == input_ids.shape[0]\n            self.regeneration_required = vars.lua_koboldbridge.regeneration_required\n            self.halt = not vars.lua_koboldbridge.generating\n            vars.lua_koboldbridge.regeneration_required = False\n\n            for i in range(vars.numseqs):\n                vars.lua_koboldbridge.generated[i+1][vars.generated_tkns] = int(input_ids[i, -1].item())\n\n            if(not vars.dynamicscan):\n                return self.regeneration_required or self.halt\n            tail = input_ids[..., -vars.generated_tkns:]\n            for i, t in enumerate(tail):\n                decoded = utils.decodenewlines(tokenizer.decode(t))\n                _, found = checkworldinfo(decoded, force_use_txt=True, actions=vars._actions)\n                found -= self.excluded_world_info[i]\n                if(len(found) != 0):\n                    self.regeneration_required = True\n                    break\n            return self.regeneration_required or self.halt\n    old_get_stopping_criteria = transformers.generation_utils.GenerationMixin._get_stopping_criteria\n    def new_get_stopping_criteria(self, *args, **kwargs):\n        stopping_criteria = old_get_stopping_criteria(self, *args, **kwargs)\n        global tokenizer\n        self.kai_scanner = DynamicWorldInfoScanCriteria(\n            tokenizer=tokenizer,\n            excluded_world_info=self.kai_scanner_excluded_world_info,\n        )\n        token_streamer = TokenStreamer(tokenizer=tokenizer)\n\n        stopping_criteria.insert(0, self.kai_scanner)\n        stopping_criteria.insert(0, token_streamer)\n        return stopping_criteria\n    transformers.generation_utils.GenerationMixin._get_stopping_criteria = new_get_stopping_criteria\n\ndef reset_model_settings():\n    vars.socketio = socketio\n    vars.max_length  = 1024    # Maximum number of tokens to submit per action\n    vars.ikmax       = 3000    # Maximum number of characters to submit to InferKit\n    vars.genamt      = 80      # Amount of text for each action to generate\n    vars.ikgen       = 200     # Number of characters for InferKit to generate\n    vars.rep_pen     = 1.1     # Default generator repetition_penalty\n    vars.rep_pen_slope = 0.7   # Default generator repetition penalty slope\n    vars.rep_pen_range = 1024  # Default generator repetition penalty range\n    vars.temp        = 0.5     # Default generator temperature\n    vars.top_p       = 0.9     # Default generator top_p\n    vars.top_k       = 0       # Default generator top_k\n    vars.top_a       = 0.0     # Default generator top-a\n    vars.tfs         = 1.0     # Default generator tfs (tail-free sampling)\n    vars.typical     = 1.0     # Default generator typical sampling threshold\n    vars.numseqs     = 1       # Number of sequences to ask the generator to create\n    vars.generated_tkns = 0    # If using a backend that supports Lua generation modifiers, how many tokens have already been generated, otherwise 0\n    vars.badwordsids = []\n    vars.fp32_model  = False  # Whether or not the most recently loaded HF model was in fp32 format\n    vars.modeldim    = -1     # Embedding dimension of your model (e.g. it's 4096 for GPT-J-6B and 2560 for GPT-Neo-2.7B)\n    vars.sampler_order = [6, 0, 1, 2, 3, 4, 5]\n    vars.newlinemode = \"n\"\n    vars.revision    = None\n    vars.lazy_load = True\n    \n\ndef load_model(use_gpu=True, gpu_layers=None, disk_layers=None, initial_load=False, online_model=\"\", use_breakmodel_args=False, breakmodel_args_default_to_cpu=False):\n    global model\n    global generator\n    global torch\n    global model_config\n    global GPT2Tokenizer\n    global tokenizer\n    if(initial_load):\n        use_breakmodel_args = True\n    reset_model_settings()\n    if not utils.HAS_ACCELERATE:\n        disk_layers = None\n    vars.noai = False\n    if not use_breakmodel_args:\n        set_aibusy(True)\n        if vars.model != 'ReadOnly':\n            emit('from_server', {'cmd': 'model_load_status', 'data': \"Loading {}\".format(vars.model)}, broadcast=True)\n            #Have to add a sleep so the server will send the emit for some reason\n            time.sleep(0.1)\n    if gpu_layers is not None:\n        args.breakmodel_gpulayers = gpu_layers\n    elif use_breakmodel_args:\n        gpu_layers = args.breakmodel_gpulayers\n    if breakmodel_args_default_to_cpu and gpu_layers is None:\n        gpu_layers = args.breakmodel_gpulayers = []\n    if disk_layers is not None:\n        args.breakmodel_disklayers = int(disk_layers)\n    elif use_breakmodel_args:\n        disk_layers = args.breakmodel_disklayers\n    if breakmodel_args_default_to_cpu and disk_layers is None:\n        disk_layers = args.breakmodel_disklayers = 0\n    \n    #We need to wipe out the existing model and refresh the cuda cache\n    model = None\n    generator = None\n    model_config = None\n    vars.online_model = ''\n    with torch.no_grad():\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"torch.distributed.reduce_op is deprecated\")\n            for tensor in gc.get_objects():\n                try:\n                    if torch.is_tensor(tensor):\n                        tensor.set_(torch.tensor((), device=tensor.device, dtype=tensor.dtype))\n                except:\n                    pass\n    gc.collect()\n    try:\n        torch.cuda.empty_cache()\n    except:\n        pass\n        \n    #Reload our badwords\n    vars.badwordsids = vars.badwordsids_default\n    \n    if online_model == \"\":\n        vars.configname = getmodelname()\n    #Let's set the GooseAI or OpenAI server URLs if that's applicable\n    else:\n        vars.online_model = online_model\n        # Swap OAI Server if GooseAI was selected\n        if(vars.model == \"GooseAI\"):\n            vars.oaiengines = \"https://api.goose.ai/v1/engines\"\n            vars.model = \"OAI\"\n            vars.configname = f\"GooseAI_{online_model.replace('/', '_')}\"\n        elif(vars.model == \"CLUSTER\") and type(online_model) is list:\n                if len(online_model) != 1:\n                    vars.configname = vars.model\n                else:\n                    vars.configname = f\"{vars.model}_{online_model[0].replace('/', '_')}\"\n        else:\n            vars.configname = f\"{vars.model}_{online_model.replace('/', '_')}\"\n        if path.exists(get_config_filename()):\n            changed=False\n            with open(get_config_filename(), \"r\") as file:\n                # Check if API key exists\n                js = json.load(file)\n                if 'online_model' in js:\n                    if js['online_model'] != online_model:\n                        changed=True\n                        js['online_model'] = online_model\n                else:\n                    changed=True\n                    js['online_model'] = online_model\n            if changed:\n                with open(get_config_filename(), \"w\") as file:\n                    file.write(json.dumps(js, indent=3))\n\n        # Swap OAI Server if GooseAI was selected\n        if(vars.model == \"GooseAI\"):\n            vars.oaiengines = \"https://api.goose.ai/v1/engines\"\n            vars.model = \"OAI\"\n            args.configname = \"GooseAI\" + \"/\" + online_model\n        elif vars.model != \"CLUSTER\":\n            args.configname = vars.model + \"/\" + online_model\n        vars.oaiurl = vars.oaiengines + \"/{0}/completions\".format(online_model)\n    \n    \n    # If transformers model was selected & GPU available, ask to use CPU or GPU\n    if(vars.model not in [\"InferKit\", \"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"GooseAI\" , \"ReadOnly\", \"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\"]):\n        vars.allowsp = True\n        # Test for GPU support\n        \n        # Make model path the same as the model name to make this consistent with the other loading method if it isn't a known model type\n        # This code is not just a workaround for below, it is also used to make the behavior consistent with other loading methods - Henk717\n        if(not vars.model in [\"NeoCustom\", \"GPT2Custom\"]):\n            vars.custmodpth = vars.model\n        elif(vars.model == \"NeoCustom\"):\n            vars.model = os.path.basename(os.path.normpath(vars.custmodpth))\n\n        # Get the model_type from the config or assume a model type if it isn't present\n        from transformers import AutoConfig\n        if(os.path.isdir(vars.custmodpth.replace('/', '_'))):\n            try:\n                model_config = AutoConfig.from_pretrained(vars.custmodpth.replace('/', '_'), revision=args.revision, cache_dir=\"cache\")\n                vars.model_type = model_config.model_type\n            except ValueError as e:\n                vars.model_type = \"not_found\"\n        elif(os.path.isdir(\"models/{}\".format(vars.custmodpth.replace('/', '_')))):\n            try:\n                model_config = AutoConfig.from_pretrained(\"models/{}\".format(vars.custmodpth.replace('/', '_')), revision=args.revision, cache_dir=\"cache\")\n                vars.model_type = model_config.model_type\n            except ValueError as e:\n                vars.model_type = \"not_found\"\n        else:\n            try:\n                model_config = AutoConfig.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\")\n                vars.model_type = model_config.model_type\n            except ValueError as e:\n                vars.model_type = \"not_found\"\n        if(vars.model_type == \"not_found\" and vars.model == \"NeoCustom\"):\n            vars.model_type = \"gpt_neo\"\n        elif(vars.model_type == \"not_found\" and vars.model == \"GPT2Custom\"):\n            vars.model_type = \"gpt2\"\n        elif(vars.model_type == \"not_found\"):\n            logger.warning(\"No model type detected, assuming Neo (If this is a GPT2 model use the other menu option or --model GPT2Custom)\")\n            vars.model_type = \"gpt_neo\"\n\n    if(not vars.use_colab_tpu and vars.model not in [\"InferKit\", \"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"GooseAI\" , \"ReadOnly\", \"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\"]):\n        loadmodelsettings()\n        loadsettings()\n        logger.init(\"GPU support\", status=\"Searching\")\n        vars.hascuda = torch.cuda.is_available() and not args.cpu\n        vars.bmsupported = ((utils.HAS_ACCELERATE and vars.model_type != 'gpt2') or vars.model_type in (\"gpt_neo\", \"gptj\", \"xglm\", \"opt\")) and not vars.nobreakmodel\n        if(args.breakmodel is not None and args.breakmodel):\n            logger.warning(\"--breakmodel is no longer supported. Breakmodel mode is now automatically enabled when --breakmodel_gpulayers is used (see --help for details).\")\n        if(args.breakmodel_layers is not None):\n            logger.warning(\"--breakmodel_layers is deprecated. Use --breakmodel_gpulayers instead (see --help for details).\")\n        if(args.model and vars.bmsupported and not args.breakmodel_gpulayers and not args.breakmodel_layers and (not utils.HAS_ACCELERATE or not args.breakmodel_disklayers)):\n            logger.warning(\"Model launched without the --breakmodel_gpulayers argument, defaulting to GPU only mode.\")\n            vars.bmsupported = False\n        if(not vars.bmsupported and (args.breakmodel_gpulayers is not None or args.breakmodel_layers is not None or args.breakmodel_disklayers is not None)):\n            logger.warning(\"This model does not support hybrid generation. --breakmodel_gpulayers will be ignored.\")\n        if(vars.hascuda):\n            logger.init_ok(\"GPU support\", status=\"Found\")\n        else:\n            logger.init_warn(\"GPU support\", status=\"Not Found\")\n        \n        if args.cpu:\n            vars.usegpu = False\n            gpu_layers = None\n            disk_layers = None\n            vars.breakmodel = False\n        elif vars.hascuda:\n            if(vars.bmsupported):\n                vars.usegpu = False\n                vars.breakmodel = True\n            else:\n                vars.breakmodel = False\n                vars.usegpu = use_gpu\n\n\n    # Ask for API key if InferKit was selected\n    if(vars.model == \"InferKit\"):\n        vars.apikey = vars.oaiapikey\n                    \n    # Swap OAI Server if GooseAI was selected\n    if(vars.model == \"GooseAI\"):\n        vars.oaiengines = \"https://api.goose.ai/v1/engines\"\n        vars.model = \"OAI\"\n        vars.configname = \"GooseAI\"\n\n    # Ask for API key if OpenAI was selected\n    if(vars.model == \"OAI\"):\n        if not vars.configname:\n            vars.configname = \"OAI\"\n        \n    if(vars.model == \"ReadOnly\"):\n        vars.noai = True\n\n    # Start transformers and create pipeline\n    if(not vars.use_colab_tpu and vars.model not in [\"InferKit\", \"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"GooseAI\" , \"ReadOnly\", \"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\"]):\n        if(not vars.noai):\n            logger.init(\"Transformers\", status='Starting')\n            for m in (\"GPTJModel\", \"XGLMModel\"):\n                try:\n                    globals()[m] = getattr(__import__(\"transformers\"), m)\n                except:\n                    pass\n\n            # Lazy loader\n            import torch_lazy_loader\n            def get_lazy_load_callback(n_layers, convert_to_float16=True):\n                if not vars.lazy_load:\n                    return\n\n                from tqdm.auto import tqdm\n\n                global breakmodel\n                import breakmodel\n\n                if utils.HAS_ACCELERATE:\n                    import accelerate.utils\n\n                if args.breakmodel_disklayers is not None:\n                    breakmodel.disk_blocks = args.breakmodel_disklayers\n\n                disk_blocks = breakmodel.disk_blocks\n                gpu_blocks = breakmodel.gpu_blocks\n                ram_blocks = ram_blocks = n_layers - sum(gpu_blocks)\n                cumulative_gpu_blocks = tuple(itertools.accumulate(gpu_blocks))\n\n                def lazy_load_callback(model_dict: Dict[str, Union[torch_lazy_loader.LazyTensor, torch.Tensor]], f, **_):\n                    if lazy_load_callback.nested:\n                        return\n                    lazy_load_callback.nested = True\n\n                    device_map: Dict[str, Union[str, int]] = {}\n\n                    @functools.lru_cache(maxsize=None)\n                    def get_original_key(key):\n                        return max((original_key for original_key in utils.module_names if original_key.endswith(key)), key=len)\n\n                    for key, value in model_dict.items():\n                        original_key = get_original_key(key)\n                        if isinstance(value, torch_lazy_loader.LazyTensor) and not any(original_key.startswith(n) for n in utils.layers_module_names):\n                            device_map[key] = vars.gpu_device if vars.hascuda and vars.usegpu else \"cpu\" if not vars.hascuda or not vars.breakmodel else breakmodel.primary_device\n                        else:\n                            layer = int(max((n for n in utils.layers_module_names if original_key.startswith(n)), key=len).rsplit(\".\", 1)[1])\n                            device = vars.gpu_device if vars.hascuda and vars.usegpu else \"disk\" if layer < disk_blocks and layer < ram_blocks else \"cpu\" if not vars.hascuda or not vars.breakmodel else \"shared\" if layer < ram_blocks else bisect.bisect_right(cumulative_gpu_blocks, layer - ram_blocks)\n                            device_map[key] = device\n\n                    if utils.num_shards is None or utils.current_shard == 0:\n                        utils.offload_index = {}\n                        if utils.HAS_ACCELERATE:\n                            if os.path.isdir(\"accelerate-disk-cache\"):\n                                # Delete all of the files in the disk cache folder without deleting the folder itself to allow people to create symbolic links for this folder\n                                # (the folder doesn't contain any subfolders so os.remove will do just fine)\n                                for filename in os.listdir(\"accelerate-disk-cache\"):\n                                    try:\n                                        os.remove(os.path.join(\"accelerate-disk-cache\", filename))\n                                    except OSError:\n                                        pass\n                            os.makedirs(\"accelerate-disk-cache\", exist_ok=True)\n                        if utils.num_shards is not None:\n                            num_tensors = len(utils.get_sharded_checkpoint_num_tensors(utils.from_pretrained_model_name, utils.from_pretrained_index_filename, **utils.from_pretrained_kwargs))\n                        else:\n                            num_tensors = len(device_map)\n                        utils.bar = tqdm(total=num_tensors, desc=f\"{colors.PURPLE}INIT{colors.END}       | Loading model tensors\", file=Send_to_socketio())\n\n                    with zipfile.ZipFile(f, \"r\") as z:\n                        try:\n                            last_storage_key = None\n                            zipfolder = os.path.basename(os.path.normpath(f)).split('.')[0]\n                            f = None\n                            current_offset = 0\n                            able_to_pin_layers = True\n                            if utils.num_shards is not None:\n                                utils.current_shard += 1\n                            for key in sorted(device_map.keys(), key=lambda k: (model_dict[k].key, model_dict[k].seek_offset)):\n                                storage_key = model_dict[key].key\n                                if storage_key != last_storage_key or model_dict[key].seek_offset < current_offset:\n                                    last_storage_key = storage_key\n                                    if isinstance(f, zipfile.ZipExtFile):\n                                        f.close()\n                                    try:\n                                        f = z.open(f\"archive/data/{storage_key}\")\n                                    except:\n                                        f = z.open(f\"{zipfolder}/data/{storage_key}\")\n                                    current_offset = 0\n                                if current_offset != model_dict[key].seek_offset:\n                                    f.read(model_dict[key].seek_offset - current_offset)\n                                    current_offset = model_dict[key].seek_offset\n                                device = device_map[key]\n                                size = functools.reduce(lambda x, y: x * y, model_dict[key].shape, 1)\n                                dtype = model_dict[key].dtype\n                                nbytes = size if dtype is torch.bool else size * ((torch.finfo if dtype.is_floating_point else torch.iinfo)(dtype).bits >> 3)\n                                #print(f\"Transferring <{key}>  to  {f'({device.upper()})' if isinstance(device, str) else '[device ' + str(device) + ']'} ... \", end=\"\", flush=True)\n                                model_dict[key] = model_dict[key].materialize(f, map_location=\"cpu\")\n                                if model_dict[key].dtype is torch.float32:\n                                    vars.fp32_model = True\n                                if convert_to_float16 and breakmodel.primary_device != \"cpu\" and vars.hascuda and (vars.breakmodel or vars.usegpu) and model_dict[key].dtype is torch.float32:\n                                    model_dict[key] = model_dict[key].to(torch.float16)\n                                if breakmodel.primary_device == \"cpu\" or (not vars.usegpu and not vars.breakmodel and model_dict[key].dtype is torch.float16):\n                                    model_dict[key] = model_dict[key].to(torch.float32)\n                                if device == \"shared\":\n                                    model_dict[key] = model_dict[key].to(\"cpu\").detach_()\n                                    if able_to_pin_layers and utils.HAS_ACCELERATE:\n                                        try:\n                                            model_dict[key] = model_dict[key].pin_memory()\n                                        except:\n                                            able_to_pin_layers = False\n                                elif device == \"disk\":\n                                    accelerate.utils.offload_weight(model_dict[key], get_original_key(key), \"accelerate-disk-cache\", index=utils.offload_index)\n                                    model_dict[key] = model_dict[key].to(\"meta\")\n                                else:\n                                    model_dict[key] = model_dict[key].to(device)\n                                #print(\"OK\", flush=True)\n                                current_offset += nbytes\n                                utils.bar.update(1)\n                        finally:\n                            if utils.num_shards is None or utils.current_shard >= utils.num_shards:\n                                if utils.offload_index:\n                                    for name, tensor in utils.named_buffers:\n                                        dtype = tensor.dtype\n                                        if convert_to_float16 and breakmodel.primary_device != \"cpu\" and vars.hascuda and (vars.breakmodel or vars.usegpu):\n                                            dtype = torch.float16\n                                        if breakmodel.primary_device == \"cpu\" or (not vars.usegpu and not vars.breakmodel):\n                                            dtype = torch.float32\n                                        if name in model_dict and model_dict[name].dtype is not dtype:\n                                            model_dict[name] = model_dict[name].to(dtype)\n                                        if tensor.dtype is not dtype:\n                                            tensor = tensor.to(dtype)\n                                        if name not in utils.offload_index:\n                                            accelerate.utils.offload_weight(tensor, name, \"accelerate-disk-cache\", index=utils.offload_index)\n                                    accelerate.utils.save_offload_index(utils.offload_index, \"accelerate-disk-cache\")\n                                utils.bar.close()\n                                utils.bar = None\n                            lazy_load_callback.nested = False\n                            if isinstance(f, zipfile.ZipExtFile):\n                                f.close()\n\n                lazy_load_callback.nested = False\n                return lazy_load_callback\n\n\n            def maybe_low_cpu_mem_usage() -> Dict[str, Any]:\n                if(packaging.version.parse(transformers_version) < packaging.version.parse(\"4.11.0\")):\n                    logger.warning(f\"Please upgrade to transformers 4.11.0 for lower RAM usage. You have transformers {transformers_version}.\")\n                    return {}\n                return {\"low_cpu_mem_usage\": True}\n            \n            @contextlib.contextmanager\n            def maybe_use_float16(always_use=False):\n                if(always_use or (vars.hascuda and args.lowmem and (vars.usegpu or vars.breakmodel))):\n                    original_dtype = torch.get_default_dtype()\n                    torch.set_default_dtype(torch.float16)\n                    yield True\n                    torch.set_default_dtype(original_dtype)\n                else:\n                    yield False\n\n            # If custom GPT2 model was chosen\n            if(vars.model_type == \"gpt2\"):\n                vars.lazy_load = False\n                if os.path.exists(vars.custmodpth):\n                    model_config = open(vars.custmodpth + \"/config.json\", \"r\")\n                elif os.path.exists(os.path.join(\"models/\", vars.custmodpth)):\n                    config_path = os.path.join(\"models/\", vars.custmodpth)\n                    config_path = os.path.join(config_path, \"config.json\").replace(\"\\\\\", \"//\")\n                    model_config = open(config_path, \"r\")\n                #js   = json.load(model_config)\n                with(maybe_use_float16()):\n                    try:\n                        if os.path.exists(vars.custmodpth):\n                            model = GPT2LMHeadModel.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\")\n                            tokenizer = GPT2Tokenizer.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\")\n                        elif os.path.exists(os.path.join(\"models/\", vars.custmodpth)):\n                            model = GPT2LMHeadModel.from_pretrained(os.path.join(\"models/\", vars.custmodpth), revision=args.revision, cache_dir=\"cache\")\n                            tokenizer = GPT2Tokenizer.from_pretrained(os.path.join(\"models/\", vars.custmodpth), revision=args.revision, cache_dir=\"cache\")\n                        else:\n                            model = GPT2LMHeadModel.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\")\n                            tokenizer = GPT2Tokenizer.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\")\n                    except Exception as e:\n                        if(\"out of memory\" in traceback.format_exc().lower()):\n                            raise RuntimeError(\"One of your GPUs ran out of memory when KoboldAI tried to load your model.\")\n                        raise e\n                tokenizer = GPT2Tokenizer.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\")\n                model.save_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), max_shard_size=\"500MiB\")\n                tokenizer.save_pretrained(\"models/{}\".format(vars.model.replace('/', '_')))\n                vars.modeldim = get_hidden_size_from_model(model)\n                # Is CUDA available? If so, use GPU, otherwise fall back to CPU\n                if(vars.hascuda and vars.usegpu):\n                    model = model.half().to(vars.gpu_device)\n                    generator = model.generate\n                else:\n                    model = model.to('cpu').float()\n                    generator = model.generate\n                patch_causallm(model)\n            # Use the Generic implementation\n            else:\n                lowmem = maybe_low_cpu_mem_usage()\n                # We must disable low_cpu_mem_usage (by setting lowmem to {}) if\n                # using a GPT-2 model because GPT-2 is not compatible with this\n                # feature yet\n                if(vars.model_type == \"gpt2\"):\n                    lowmem = {}\n                    vars.lazy_load = False  # Also, lazy loader doesn't support GPT-2 models\n                \n                # If we're using torch_lazy_loader, we need to get breakmodel config\n                # early so that it knows where to load the individual model tensors\n                if (utils.HAS_ACCELERATE or vars.lazy_load and vars.hascuda and vars.breakmodel) and not vars.nobreakmodel:\n                    device_config(model_config)\n\n                # Download model from Huggingface if it does not exist, otherwise load locally\n                \n                #If we specify a model and it's in the root directory, we need to move it to the models directory (legacy folder structure to new)\n                if os.path.isdir(vars.model.replace('/', '_')):\n                    import shutil\n                    shutil.move(vars.model.replace('/', '_'), \"models/{}\".format(vars.model.replace('/', '_')))\n                if(vars.lazy_load):  # If we're using lazy loader, we need to figure out what the model's hidden layers are called\n                    with torch_lazy_loader.use_lazy_torch_load(dematerialized_modules=True, use_accelerate_init_empty_weights=True):\n                        try:\n                            metamodel = AutoModelForCausalLM.from_config(model_config)\n                        except Exception as e:\n                            metamodel = GPTNeoForCausalLM.from_config(model_config)\n                        utils.layers_module_names = utils.get_layers_module_names(metamodel)\n                        utils.module_names = list(metamodel.state_dict().keys())\n                        utils.named_buffers = list(metamodel.named_buffers(recurse=True))\n                with maybe_use_float16(), torch_lazy_loader.use_lazy_torch_load(enable=vars.lazy_load, callback=get_lazy_load_callback(utils.num_layers(model_config)) if vars.lazy_load else None, dematerialized_modules=True):\n                    if(vars.lazy_load):  # torch_lazy_loader.py and low_cpu_mem_usage can't be used at the same time\n                        lowmem = {}\n                    if(os.path.isdir(vars.custmodpth)):\n                        try:\n                            tokenizer = AutoTokenizer.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\", use_fast=False)\n                        except Exception as e:\n                            try:\n                                tokenizer = AutoTokenizer.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\")\n                            except Exception as e:\n                                try:\n                                    tokenizer = GPT2Tokenizer.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\")\n                                except Exception as e:\n                                    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=args.revision, cache_dir=\"cache\")\n                        try:\n                            model     = AutoModelForCausalLM.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\", **lowmem)\n                        except Exception as e:\n                            if(\"out of memory\" in traceback.format_exc().lower()):\n                                raise RuntimeError(\"One of your GPUs ran out of memory when KoboldAI tried to load your model.\")\n                            model     = GPTNeoForCausalLM.from_pretrained(vars.custmodpth, revision=args.revision, cache_dir=\"cache\", **lowmem)\n                    elif(os.path.isdir(\"models/{}\".format(vars.model.replace('/', '_')))):\n                        try:\n                            tokenizer = AutoTokenizer.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=args.revision, cache_dir=\"cache\", use_fast=False)\n                        except Exception as e:\n                            try:\n                                tokenizer = AutoTokenizer.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=args.revision, cache_dir=\"cache\")\n                            except Exception as e:\n                                try:\n                                    tokenizer = GPT2Tokenizer.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=args.revision, cache_dir=\"cache\")\n                                except Exception as e:\n                                    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=args.revision, cache_dir=\"cache\")\n                        try:\n                            model     = AutoModelForCausalLM.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=args.revision, cache_dir=\"cache\", **lowmem)\n                        except Exception as e:\n                            if(\"out of memory\" in traceback.format_exc().lower()):\n                                raise RuntimeError(\"One of your GPUs ran out of memory when KoboldAI tried to load your model.\")\n                            model     = GPTNeoForCausalLM.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=args.revision, cache_dir=\"cache\", **lowmem)\n                    else:\n                        old_rebuild_tensor = torch._utils._rebuild_tensor\n                        def new_rebuild_tensor(storage: Union[torch_lazy_loader.LazyTensor, torch.Storage], storage_offset, shape, stride):\n                            if(not isinstance(storage, torch_lazy_loader.LazyTensor)):\n                                dtype = storage.dtype\n                            else:\n                                dtype = storage.storage_type.dtype\n                                if(not isinstance(dtype, torch.dtype)):\n                                    dtype = storage.storage_type(0).dtype\n                            if(dtype is torch.float32 and len(shape) >= 2):\n                                vars.fp32_model = True\n                            return old_rebuild_tensor(storage, storage_offset, shape, stride)\n                        torch._utils._rebuild_tensor = new_rebuild_tensor\n\n                        try:\n                            tokenizer = AutoTokenizer.from_pretrained(vars.model, revision=args.revision, cache_dir=\"cache\", use_fast=False)\n                        except Exception as e:\n                            try:\n                                tokenizer = AutoTokenizer.from_pretrained(vars.model, revision=args.revision, cache_dir=\"cache\")\n                            except Exception as e:\n                                try:\n                                    tokenizer = GPT2Tokenizer.from_pretrained(vars.model, revision=args.revision, cache_dir=\"cache\")\n                                except Exception as e:\n                                    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=args.revision, cache_dir=\"cache\")\n                        try:\n                            model     = AutoModelForCausalLM.from_pretrained(vars.model, revision=args.revision, cache_dir=\"cache\", **lowmem)\n                        except Exception as e:\n                            if(\"out of memory\" in traceback.format_exc().lower()):\n                                raise RuntimeError(\"One of your GPUs ran out of memory when KoboldAI tried to load your model.\")\n                            model     = GPTNeoForCausalLM.from_pretrained(vars.model, revision=args.revision, cache_dir=\"cache\", **lowmem)\n\n                        torch._utils._rebuild_tensor = old_rebuild_tensor\n\n                        if not args.colab or args.savemodel:\n                            import shutil\n                            tokenizer.save_pretrained(\"models/{}\".format(vars.model.replace('/', '_')))\n                            if(vars.fp32_model and (\"breakmodel\" not in globals() or not breakmodel.disk_blocks)):  # Use save_pretrained to convert fp32 models to fp16, unless we are using disk cache because save_pretrained is not supported in that case\n                                model = model.half()\n                                model.save_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), max_shard_size=\"500MiB\")\n                            else:  # For fp16 models, we can just copy the model files directly\n                                import transformers.configuration_utils\n                                import transformers.modeling_utils\n                                import transformers.file_utils\n                                import huggingface_hub\n                                legacy = packaging.version.parse(transformers_version) < packaging.version.parse(\"4.22.0.dev0\")\n                                # Save the config.json\n                                shutil.move(os.path.realpath(huggingface_hub.hf_hub_download(vars.model, transformers.configuration_utils.CONFIG_NAME, revision=args.revision, cache_dir=\"cache\", local_files_only=True, legacy_cache_layout=legacy)), os.path.join(\"models/{}\".format(vars.model.replace('/', '_')), transformers.configuration_utils.CONFIG_NAME))\n                                if(utils.num_shards is None):\n                                    # Save the pytorch_model.bin of an unsharded model\n                                    shutil.move(os.path.realpath(huggingface_hub.hf_hub_download(vars.model, transformers.modeling_utils.WEIGHTS_NAME, revision=args.revision, cache_dir=\"cache\", local_files_only=True, legacy_cache_layout=legacy)), os.path.join(\"models/{}\".format(vars.model.replace('/', '_')), transformers.modeling_utils.WEIGHTS_NAME))\n                                else:\n                                    with open(utils.from_pretrained_index_filename) as f:\n                                        map_data = json.load(f)\n                                    filenames = set(map_data[\"weight_map\"].values())\n                                    # Save the pytorch_model.bin.index.json of a sharded model\n                                    shutil.move(os.path.realpath(utils.from_pretrained_index_filename), os.path.join(\"models/{}\".format(vars.model.replace('/', '_')), transformers.modeling_utils.WEIGHTS_INDEX_NAME))\n                                    # Then save the pytorch_model-#####-of-#####.bin files\n                                    for filename in filenames:\n                                        shutil.move(os.path.realpath(huggingface_hub.hf_hub_download(vars.model, filename, revision=args.revision, cache_dir=\"cache\", local_files_only=True, legacy_cache_layout=legacy)), os.path.join(\"models/{}\".format(vars.model.replace('/', '_')), filename))\n                            shutil.rmtree(\"cache/\")\n\n                if(vars.badwordsids is vars.badwordsids_default and vars.model_type not in (\"gpt2\", \"gpt_neo\", \"gptj\")):\n                    vars.badwordsids = [[v] for k, v in tokenizer.get_vocab().items() if any(c in str(k) for c in \"<>[]\") if vars.newlinemode != \"s\" or str(k) != \"</s>\"]\n\n                patch_causallm(model)\n\n                if(vars.hascuda):\n                    if(vars.usegpu):\n                        vars.modeldim = get_hidden_size_from_model(model)\n                        model = model.half().to(vars.gpu_device)\n                        generator = model.generate\n                    elif(vars.breakmodel):  # Use both RAM and VRAM (breakmodel)\n                        vars.modeldim = get_hidden_size_from_model(model)\n                        if(not vars.lazy_load):\n                            device_config(model.config)\n                        move_model_to_devices(model)\n                    elif(utils.HAS_ACCELERATE and __import__(\"breakmodel\").disk_blocks > 0):\n                        move_model_to_devices(model)\n                        vars.modeldim = get_hidden_size_from_model(model)\n                        generator = model.generate\n                    else:\n                        model = model.to('cpu').float()\n                        vars.modeldim = get_hidden_size_from_model(model)\n                        generator = model.generate\n                elif(utils.HAS_ACCELERATE and __import__(\"breakmodel\").disk_blocks > 0):\n                    move_model_to_devices(model)\n                    vars.modeldim = get_hidden_size_from_model(model)\n                    generator = model.generate\n                else:\n                    model.to('cpu').float()\n                    vars.modeldim = get_hidden_size_from_model(model)\n                    generator = model.generate\n            \n            # Suppress Author's Note by flagging square brackets (Old implementation)\n            #vocab         = tokenizer.get_vocab()\n            #vocab_keys    = vocab.keys()\n            #vars.badwords = gettokenids(\"[\")\n            #for key in vars.badwords:\n            #    vars.badwordsids.append([vocab[key]])\n            \n            logger.info(f\"Pipeline created: {vars.model}\")\n        \n        else:\n            from transformers import GPT2Tokenizer\n            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=args.revision, cache_dir=\"cache\")\n    else:\n        from transformers import PreTrainedModel\n        from transformers import modeling_utils\n        old_from_pretrained = PreTrainedModel.from_pretrained.__func__\n        @classmethod\n        def new_from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n            vars.fp32_model = False\n            utils.num_shards = None\n            utils.current_shard = 0\n            utils.from_pretrained_model_name = pretrained_model_name_or_path\n            utils.from_pretrained_index_filename = None\n            utils.from_pretrained_kwargs = kwargs\n            utils.bar = None\n            if not args.no_aria2:\n                utils.aria2_hook(pretrained_model_name_or_path, **kwargs)\n            return old_from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n        if(not hasattr(PreTrainedModel, \"_kai_patched\")):\n            PreTrainedModel.from_pretrained = new_from_pretrained\n            PreTrainedModel._kai_patched = True\n        if(hasattr(modeling_utils, \"get_checkpoint_shard_files\")):\n            old_get_checkpoint_shard_files = modeling_utils.get_checkpoint_shard_files\n            def new_get_checkpoint_shard_files(pretrained_model_name_or_path, index_filename, *args, **kwargs):\n                utils.num_shards = utils.get_num_shards(index_filename)\n                utils.from_pretrained_index_filename = index_filename\n                return old_get_checkpoint_shard_files(pretrained_model_name_or_path, index_filename, *args, **kwargs)\n            modeling_utils.get_checkpoint_shard_files = new_get_checkpoint_shard_files\n\n\n        def tpumtjgenerate_warper_callback(scores) -> \"np.array\":\n            scores_shape = scores.shape\n            scores_list = scores.tolist()\n            vars.lua_koboldbridge.logits = vars.lua_state.table()\n            for r, row in enumerate(scores_list):\n                vars.lua_koboldbridge.logits[r+1] = vars.lua_state.table(*row)\n            vars.lua_koboldbridge.vocab_size = scores_shape[-1]\n\n            execute_genmod()\n\n            scores = np.array(\n                tuple(tuple(row.values()) for row in vars.lua_koboldbridge.logits.values()),\n                dtype=scores.dtype,\n            )\n            assert scores.shape == scores_shape\n\n            return scores\n        \n        def tpumtjgenerate_stopping_callback(generated, n_generated, excluded_world_info) -> Tuple[List[set], bool, bool]:\n            vars.generated_tkns += 1\n\n            assert len(excluded_world_info) == len(generated)\n            regeneration_required = vars.lua_koboldbridge.regeneration_required\n            halt = vars.abort or not vars.lua_koboldbridge.generating or vars.generated_tkns >= vars.genamt\n            vars.lua_koboldbridge.regeneration_required = False\n\n            global past\n\n            for i in range(vars.numseqs):\n                vars.lua_koboldbridge.generated[i+1][vars.generated_tkns] = int(generated[i, tpu_mtj_backend.params[\"seq\"] + n_generated - 1].item())\n\n            if(not vars.dynamicscan or halt):\n                return excluded_world_info, regeneration_required, halt\n\n            for i, t in enumerate(generated):\n                decoded = utils.decodenewlines(tokenizer.decode(past[i])) + utils.decodenewlines(tokenizer.decode(t[tpu_mtj_backend.params[\"seq\"] : tpu_mtj_backend.params[\"seq\"] + n_generated]))\n                _, found = checkworldinfo(decoded, force_use_txt=True, actions=vars._actions)\n                found -= excluded_world_info[i]\n                if(len(found) != 0):\n                    regeneration_required = True\n                    break\n            return excluded_world_info, regeneration_required, halt\n\n        def tpumtjgenerate_compiling_callback() -> None:\n            print(colors.GREEN + \"TPU backend compilation triggered\" + colors.END)\n            vars.compiling = True\n\n        def tpumtjgenerate_stopped_compiling_callback() -> None:\n            vars.compiling = False\n        \n        def tpumtjgenerate_settings_callback() -> dict:\n            sampler_order = vars.sampler_order[:]\n            if len(sampler_order) < 7:  # Add repetition penalty at beginning if it's not present\n                sampler_order = [6] + sampler_order\n            return {\n                \"sampler_order\": sampler_order,\n                \"top_p\": float(vars.top_p),\n                \"temp\": float(vars.temp),\n                \"top_k\": int(vars.top_k),\n                \"tfs\": float(vars.tfs),\n                \"typical\": float(vars.typical),\n                \"top_a\": float(vars.top_a),\n                \"repetition_penalty\": float(vars.rep_pen),\n                \"rpslope\": float(vars.rep_pen_slope),\n                \"rprange\": int(vars.rep_pen_range),\n            }\n\n        # If we're running Colab or OAI, we still need a tokenizer.\n        if(vars.model in (\"Colab\", \"API\", \"CLUSTER\")):\n            from transformers import GPT2Tokenizer\n            tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\", revision=args.revision, cache_dir=\"cache\")\n            loadsettings()\n        elif(vars.model == \"OAI\"):\n            from transformers import GPT2Tokenizer\n            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=args.revision, cache_dir=\"cache\")\n            loadsettings()\n        # Load the TPU backend if requested\n        elif(vars.use_colab_tpu or vars.model in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")):\n            global tpu_mtj_backend\n            import tpu_mtj_backend\n            if(vars.model == \"TPUMeshTransformerGPTNeoX\"):\n                vars.badwordsids = vars.badwordsids_neox\n            print(\"{0}Initializing Mesh Transformer JAX, please wait...{1}\".format(colors.PURPLE, colors.END))\n            if vars.model in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\") and (not vars.custmodpth or not os.path.isdir(vars.custmodpth)):\n                raise FileNotFoundError(f\"The specified model path {repr(vars.custmodpth)} is not the path to a valid folder\")\n            import tpu_mtj_backend\n            if(vars.model == \"TPUMeshTransformerGPTNeoX\"):\n                tpu_mtj_backend.pad_token_id = 2\n            tpu_mtj_backend.vars = vars\n            tpu_mtj_backend.warper_callback = tpumtjgenerate_warper_callback\n            tpu_mtj_backend.stopping_callback = tpumtjgenerate_stopping_callback\n            tpu_mtj_backend.compiling_callback = tpumtjgenerate_compiling_callback\n            tpu_mtj_backend.stopped_compiling_callback = tpumtjgenerate_stopped_compiling_callback\n            tpu_mtj_backend.settings_callback = tpumtjgenerate_settings_callback\n            vars.allowsp = True\n            loadmodelsettings()\n            loadsettings()\n            tpu_mtj_backend.load_model(vars.custmodpth, hf_checkpoint=vars.model not in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\") and vars.use_colab_tpu, **vars.modelconfig)\n            vars.modeldim = int(tpu_mtj_backend.params.get(\"d_embed\", tpu_mtj_backend.params[\"d_model\"]))\n            tokenizer = tpu_mtj_backend.tokenizer\n            if(vars.badwordsids is vars.badwordsids_default and vars.model_type not in (\"gpt2\", \"gpt_neo\", \"gptj\")):\n                vars.badwordsids = [[v] for k, v in tokenizer.get_vocab().items() if any(c in str(k) for c in \"<>[]\") if vars.newlinemode != \"s\" or str(k) != \"</s>\"]\n        else:\n            loadsettings()\n    \n    lua_startup()\n    # Load scripts\n    load_lua_scripts()\n    \n    final_startup()\n    if not initial_load:\n        set_aibusy(False)\n        emit('from_server', {'cmd': 'hide_model_name'}, broadcast=True)\n        time.sleep(0.1)\n        \n        if not vars.gamestarted:\n            setStartState()\n            sendsettings()\n            refresh_settings()\n\n\n# Set up Flask routes\n@app.route('/')\n@app.route('/index')\ndef index():\n    if args.no_ui:\n        return redirect('/api/latest')\n    else:\n        return render_template('index.html', hide_ai_menu=args.noaimenu)\n@app.route('/api', strict_slashes=False)\ndef api():\n    return redirect('/api/latest')\n@app.route('/favicon.ico')\ndef favicon():\n    return send_from_directory(app.root_path,\n                                   'koboldai.ico', mimetype='image/vnd.microsoft.icon')    \n@app.route('/download')\ndef download():\n    if args.no_ui:\n        raise NotFound()\n\n    save_format = request.args.get(\"format\", \"json\").strip().lower()\n\n    if(save_format == \"plaintext\"):\n        txt = vars.prompt + \"\".join(vars.actions.values())\n        save = Response(txt)\n        filename = path.basename(vars.savedir)\n        if filename[-5:] == \".json\":\n            filename = filename[:-5]\n        save.headers.set('Content-Disposition', 'attachment', filename='%s.txt' % filename)\n        return(save)\n\n    # Build json to write\n    js = {}\n    js[\"gamestarted\"] = vars.gamestarted\n    js[\"prompt\"]      = vars.prompt\n    js[\"memory\"]      = vars.memory\n    js[\"authorsnote\"] = vars.authornote\n    js[\"anotetemplate\"] = vars.authornotetemplate\n    js[\"actions\"]     = tuple(vars.actions.values())\n    js[\"actions_metadata\"] = vars.actions_metadata\n    js[\"worldinfo\"]   = []\n        \n    # Extract only the important bits of WI\n    for wi in vars.worldinfo:\n        if(wi[\"constant\"] or wi[\"key\"] != \"\"):\n            js[\"worldinfo\"].append({\n                \"key\": wi[\"key\"],\n                \"keysecondary\": wi[\"keysecondary\"],\n                \"content\": wi[\"content\"],\n                \"comment\": wi[\"comment\"],\n                \"folder\": wi[\"folder\"],\n                \"selective\": wi[\"selective\"],\n                \"constant\": wi[\"constant\"]\n            })\n    \n    save = Response(json.dumps(js, indent=3))\n    filename = path.basename(vars.savedir)\n    if filename[-5:] == \".json\":\n        filename = filename[:-5]\n    save.headers.set('Content-Disposition', 'attachment', filename='%s.json' % filename)\n    return(save)\n\n\n#============================ LUA API =============================#\n_bridged = {}\nF = TypeVar(\"F\", bound=Callable)\ndef lua_startup():\n    global _bridged\n    global F\n    global bridged\n    if(path.exists(get_config_filename())):\n        file = open(get_config_filename(), \"r\")\n        js   = json.load(file)\n        if(\"userscripts\" in js):\n            vars.userscripts = []\n            for userscript in js[\"userscripts\"]:\n                if type(userscript) is not str:\n                    continue\n                userscript = userscript.strip()\n                if len(userscript) != 0 and all(q not in userscript for q in (\"..\", \":\")) and all(userscript[0] not in q for q in (\"/\", \"\\\\\")) and os.path.exists(fileops.uspath(userscript)):\n                    vars.userscripts.append(userscript)\n        if(\"corescript\" in js and type(js[\"corescript\"]) is str and all(q not in js[\"corescript\"] for q in (\"..\", \":\")) and all(js[\"corescript\"][0] not in q for q in (\"/\", \"\\\\\"))):\n            vars.corescript = js[\"corescript\"]\n        else:\n            vars.corescript = \"default.lua\"\n        file.close()\n        \n    #==================================================================#\n    #  Lua runtime startup\n    #==================================================================#\n\n    print(\"\", end=\"\", flush=True)\n    logger.init(\"LUA bridge\", status=\"Starting\")\n\n    # Set up Lua state\n    vars.lua_state = lupa.LuaRuntime(unpack_returned_tuples=True)\n\n    # Load bridge.lua\n    bridged = {\n        \"corescript_path\": \"cores\",\n        \"userscript_path\": \"userscripts\",\n        \"config_path\": \"userscripts\",\n        \"lib_paths\": vars.lua_state.table(\"lualibs\", os.path.join(\"extern\", \"lualibs\")),\n        \"vars\": vars,\n    }\n    for kwarg in _bridged:\n        bridged[kwarg] = _bridged[kwarg]\n    try:\n        vars.lua_kobold, vars.lua_koboldcore, vars.lua_koboldbridge = vars.lua_state.globals().dofile(\"bridge.lua\")(\n            vars.lua_state.globals().python,\n            bridged,\n        )\n    except lupa.LuaError as e:\n        print(colors.RED + \"ERROR!\" + colors.END)\n        vars.lua_koboldbridge.obliterate_multiverse()\n        logger.error('LUA ERROR: ' + str(e).replace(\"\\033\", \"\"))\n        logger.warning(\"Lua engine stopped; please open 'Userscripts' and press Load to reinitialize scripts.\")\n        exit(1)\n    logger.init_ok(\"LUA bridge\", status=\"OK\")\n\n\ndef lua_log_format_name(name):\n    return f\"[{name}]\" if type(name) is str else \"CORE\"\n\n\ndef bridged_kwarg(name=None):\n    def _bridged_kwarg(f: F):\n        _bridged[name if name is not None else f.__name__[4:] if f.__name__[:4] == \"lua_\" else f.__name__] = f\n        return f\n    return _bridged_kwarg\n\n#==================================================================#\n#  Event triggered when a userscript is loaded\n#==================================================================#\n@bridged_kwarg()\ndef load_callback(filename, modulename):\n    print(colors.GREEN + f\"Loading Userscript [{modulename}] <{filename}>\" + colors.END)\n\n#==================================================================#\n#  Load all Lua scripts\n#==================================================================#\ndef load_lua_scripts():\n    logger.init(\"LUA Scripts\", status=\"Starting\")\n\n    filenames = []\n    modulenames = []\n    descriptions = []\n\n    lst = fileops.getusfiles(long_desc=True)\n    filenames_dict = {ob[\"filename\"]: i for i, ob in enumerate(lst)}\n\n    for filename in vars.userscripts:\n        if filename in filenames_dict:\n            i = filenames_dict[filename]\n            filenames.append(filename)\n            modulenames.append(lst[i][\"modulename\"])\n            descriptions.append(lst[i][\"description\"])\n\n    vars.has_genmod = False\n\n    try:\n        vars.lua_koboldbridge.obliterate_multiverse()\n        tpool.execute(vars.lua_koboldbridge.load_corescript, vars.corescript)\n        vars.has_genmod = tpool.execute(vars.lua_koboldbridge.load_userscripts, filenames, modulenames, descriptions)\n        vars.lua_running = True\n    except lupa.LuaError as e:\n        try:\n            vars.lua_koboldbridge.obliterate_multiverse()\n        except:\n            pass\n        vars.lua_running = False\n        if(vars.serverstarted):\n            emit('from_server', {'cmd': 'errmsg', 'data': 'Lua script error; please check console.'}, broadcast=True)\n            sendUSStatItems()\n        logger.error('LUA ERROR: ' + str(e).replace(\"\\033\", \"\"))\n        logger.warning(\"Lua engine stopped; please open 'Userscripts' and press Load to reinitialize scripts.\")\n        if(vars.serverstarted):\n            set_aibusy(0)\n    logger.init_ok(\"LUA Scripts\", status=\"OK\")\n\n#==================================================================#\n#  Print message that originates from the userscript with the given name\n#==================================================================#\n@bridged_kwarg()\ndef lua_print(msg):\n    if(vars.lua_logname != vars.lua_koboldbridge.logging_name):\n        vars.lua_logname = vars.lua_koboldbridge.logging_name\n        print(colors.BLUE + lua_log_format_name(vars.lua_logname) + \":\" + colors.END, file=sys.stderr)\n    print(colors.PURPLE + msg.replace(\"\\033\", \"\") + colors.END)\n\n#==================================================================#\n#  Print warning that originates from the userscript with the given name\n#==================================================================#\n@bridged_kwarg()\ndef lua_warn(msg):\n    if(vars.lua_logname != vars.lua_koboldbridge.logging_name):\n        vars.lua_logname = vars.lua_koboldbridge.logging_name\n        print(colors.BLUE + lua_log_format_name(vars.lua_logname) + \":\" + colors.END, file=sys.stderr)\n    print(colors.YELLOW + msg.replace(\"\\033\", \"\") + colors.END)\n\n#==================================================================#\n#  Decode tokens into a string using current tokenizer\n#==================================================================#\n@bridged_kwarg()\ndef lua_decode(tokens):\n    tokens = list(tokens.values())\n    assert type(tokens) is list\n    if(\"tokenizer\" not in globals()):\n        from transformers import GPT2Tokenizer\n        global tokenizer\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=args.revision, cache_dir=\"cache\")\n    return utils.decodenewlines(tokenizer.decode(tokens))\n\n#==================================================================#\n#  Encode string into list of token IDs using current tokenizer\n#==================================================================#\n@bridged_kwarg()\ndef lua_encode(string):\n    assert type(string) is str\n    if(\"tokenizer\" not in globals()):\n        from transformers import GPT2Tokenizer\n        global tokenizer\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=args.revision, cache_dir=\"cache\")\n    return tokenizer.encode(utils.encodenewlines(string), max_length=int(4e9), truncation=True)\n\n#==================================================================#\n#  Computes context given a submission, Lua array of entry UIDs and a Lua array\n#  of folder UIDs\n#==================================================================#\n@bridged_kwarg()\ndef lua_compute_context(submission, entries, folders, kwargs):\n    assert type(submission) is str\n    if(kwargs is None):\n        kwargs = vars.lua_state.table()\n    actions = vars._actions if vars.lua_koboldbridge.userstate == \"genmod\" else vars.actions\n    allowed_entries = None\n    allowed_folders = None\n    if(entries is not None):\n        allowed_entries = set()\n        i = 1\n        while(entries[i] is not None):\n            allowed_entries.add(int(entries[i]))\n            i += 1\n    if(folders is not None):\n        allowed_folders = set()\n        i = 1\n        while(folders[i] is not None):\n            allowed_folders.add(int(folders[i]))\n            i += 1\n    winfo, mem, anotetxt, _ = calcsubmitbudgetheader(\n        submission,\n        allowed_entries=allowed_entries,\n        allowed_folders=allowed_folders,\n        force_use_txt=True,\n        scan_story=kwargs[\"scan_story\"] if kwargs[\"scan_story\"] != None else True,\n    )\n    if kwargs[\"include_anote\"] is not None and not kwargs[\"include_anote\"]:\n        anotetxt = \"\"\n    txt, _, _ = calcsubmitbudget(\n        len(actions),\n        winfo,\n        mem,\n        anotetxt,\n        actions,\n    )\n    return utils.decodenewlines(tokenizer.decode(txt))\n\n#==================================================================#\n#  Get property of a world info entry given its UID and property name\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_attr(uid, k):\n    assert type(uid) is int and type(k) is str\n    if(uid in vars.worldinfo_u and k in (\n        \"key\",\n        \"keysecondary\",\n        \"content\",\n        \"comment\",\n        \"folder\",\n        \"num\",\n        \"selective\",\n        \"constant\",\n        \"uid\",\n    )):\n        return vars.worldinfo_u[uid][k]\n\n#==================================================================#\n#  Set property of a world info entry given its UID, property name and new value\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_attr(uid, k, v):\n    assert type(uid) is int and type(k) is str\n    assert uid in vars.worldinfo_u and k in (\n        \"key\",\n        \"keysecondary\",\n        \"content\",\n        \"comment\",\n        \"selective\",\n        \"constant\",\n    )\n    if(type(vars.worldinfo_u[uid][k]) is int and type(v) is float):\n        v = int(v)\n    assert type(vars.worldinfo_u[uid][k]) is type(v)\n    vars.worldinfo_u[uid][k] = v\n    print(colors.GREEN + f\"{lua_log_format_name(vars.lua_koboldbridge.logging_name)} set {k} of world info entry {uid} to {v}\" + colors.END)\n\n#==================================================================#\n#  Get property of a world info folder given its UID and property name\n#==================================================================#\n@bridged_kwarg()\ndef lua_folder_get_attr(uid, k):\n    assert type(uid) is int and type(k) is str\n    if(uid in vars.wifolders_d and k in (\n        \"name\",\n    )):\n        return vars.wifolders_d[uid][k]\n\n#==================================================================#\n#  Set property of a world info folder given its UID, property name and new value\n#==================================================================#\n@bridged_kwarg()\ndef lua_folder_set_attr(uid, k, v):\n    assert type(uid) is int and type(k) is str\n    assert uid in vars.wifolders_d and k in (\n        \"name\",\n    )\n    if(type(vars.wifolders_d[uid][k]) is int and type(v) is float):\n        v = int(v)\n    assert type(vars.wifolders_d[uid][k]) is type(v)\n    vars.wifolders_d[uid][k] = v\n    print(colors.GREEN + f\"{lua_log_format_name(vars.lua_koboldbridge.logging_name)} set {k} of world info folder {uid} to {v}\" + colors.END)\n\n#==================================================================#\n#  Get the \"Amount to Generate\"\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_genamt():\n    return vars.genamt\n\n#==================================================================#\n#  Set the \"Amount to Generate\"\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_genamt(genamt):\n    assert vars.lua_koboldbridge.userstate != \"genmod\" and type(genamt) in (int, float) and genamt >= 0\n    print(colors.GREEN + f\"{lua_log_format_name(vars.lua_koboldbridge.logging_name)} set genamt to {int(genamt)}\" + colors.END)\n    vars.genamt = int(genamt)\n\n#==================================================================#\n#  Get the \"Gens Per Action\"\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_numseqs():\n    return vars.numseqs\n\n#==================================================================#\n#  Set the \"Gens Per Action\"\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_numseqs(numseqs):\n    assert type(numseqs) in (int, float) and numseqs >= 1\n    print(colors.GREEN + f\"{lua_log_format_name(vars.lua_koboldbridge.logging_name)} set numseqs to {int(numseqs)}\" + colors.END)\n    vars.numseqs = int(numseqs)\n\n#==================================================================#\n#  Check if a setting exists with the given name\n#==================================================================#\n@bridged_kwarg()\ndef lua_has_setting(setting):\n    return setting in (\n        \"anotedepth\",\n        \"settemp\",\n        \"settopp\",\n        \"settopk\",\n        \"settfs\",\n        \"settypical\",\n        \"settopa\",\n        \"setreppen\",\n        \"setreppenslope\",\n        \"setreppenrange\",\n        \"settknmax\",\n        \"setwidepth\",\n        \"setuseprompt\",\n        \"setadventure\",\n        \"setchatmode\",\n        \"setdynamicscan\",\n        \"setnopromptgen\",\n        \"autosave\",\n        \"setrngpersist\",\n        \"temp\",\n        \"topp\",\n        \"top_p\",\n        \"topk\",\n        \"top_k\",\n        \"tfs\",\n        \"typical\",\n        \"topa\",\n        \"reppen\",\n        \"reppenslope\",\n        \"reppenrange\",\n        \"tknmax\",\n        \"widepth\",\n        \"useprompt\",\n        \"chatmode\",\n        \"chatname\",\n        \"adventure\",\n        \"dynamicscan\",\n        \"nopromptgen\",\n        \"rngpersist\",\n        \"frmttriminc\",\n        \"frmtrmblln\",\n        \"frmtrmspch\",\n        \"frmtadsnsp\",\n        \"frmtsingleline\",\n        \"triminc\",\n        \"rmblln\",\n        \"rmspch\",\n        \"adsnsp\",\n        \"singleline\",\n        \"output_streaming\",\n        \"show_probs\"\n    )\n\n#==================================================================#\n#  Return the setting with the given name if it exists\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_setting(setting):\n    if(setting in (\"settemp\", \"temp\")): return vars.temp\n    if(setting in (\"settopp\", \"topp\", \"top_p\")): return vars.top_p\n    if(setting in (\"settopk\", \"topk\", \"top_k\")): return vars.top_k\n    if(setting in (\"settfs\", \"tfs\")): return vars.tfs\n    if(setting in (\"settypical\", \"typical\")): return vars.typical\n    if(setting in (\"settopa\", \"topa\")): return vars.top_a\n    if(setting in (\"setreppen\", \"reppen\")): return vars.rep_pen\n    if(setting in (\"setreppenslope\", \"reppenslope\")): return vars.rep_pen_slope\n    if(setting in (\"setreppenrange\", \"reppenrange\")): return vars.rep_pen_range\n    if(setting in (\"settknmax\", \"tknmax\")): return vars.max_length\n    if(setting == \"anotedepth\"): return vars.andepth\n    if(setting in (\"setwidepth\", \"widepth\")): return vars.widepth\n    if(setting in (\"setuseprompt\", \"useprompt\")): return vars.useprompt\n    if(setting in (\"setadventure\", \"adventure\")): return vars.adventure\n    if(setting in (\"setchatmode\", \"chatmode\")): return vars.chatmode\n    if(setting in (\"setdynamicscan\", \"dynamicscan\")): return vars.dynamicscan\n    if(setting in (\"setnopromptgen\", \"nopromptgen\")): return vars.nopromptgen\n    if(setting in (\"autosave\", \"autosave\")): return vars.autosave\n    if(setting in (\"setrngpersist\", \"rngpersist\")): return vars.rngpersist\n    if(setting in (\"frmttriminc\", \"triminc\")): return vars.formatoptns[\"frmttriminc\"]\n    if(setting in (\"frmtrmblln\", \"rmblln\")): return vars.formatoptns[\"frmttrmblln\"]\n    if(setting in (\"frmtrmspch\", \"rmspch\")): return vars.formatoptns[\"frmttrmspch\"]\n    if(setting in (\"frmtadsnsp\", \"adsnsp\")): return vars.formatoptns[\"frmtadsnsp\"]\n    if(setting in (\"frmtsingleline\", \"singleline\")): return vars.formatoptns[\"singleline\"]\n    if(setting == \"output_streaming\"): return vars.output_streaming\n    if(setting == \"show_probs\"): return vars.show_probs\n\n#==================================================================#\n#  Set the setting with the given name if it exists\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_setting(setting, v):\n    actual_type = type(lua_get_setting(setting))\n    assert v is not None and (actual_type is type(v) or (actual_type is int and type(v) is float))\n    v = actual_type(v)\n    print(colors.GREEN + f\"{lua_log_format_name(vars.lua_koboldbridge.logging_name)} set {setting} to {v}\" + colors.END)\n    if(setting in (\"setadventure\", \"adventure\") and v):\n        vars.actionmode = 1\n    if(setting in (\"settemp\", \"temp\")): vars.temp = v\n    if(setting in (\"settopp\", \"topp\")): vars.top_p = v\n    if(setting in (\"settopk\", \"topk\")): vars.top_k = v\n    if(setting in (\"settfs\", \"tfs\")): vars.tfs = v\n    if(setting in (\"settypical\", \"typical\")): vars.typical = v\n    if(setting in (\"settopa\", \"topa\")): vars.top_a = v\n    if(setting in (\"setreppen\", \"reppen\")): vars.rep_pen = v\n    if(setting in (\"setreppenslope\", \"reppenslope\")): vars.rep_pen_slope = v\n    if(setting in (\"setreppenrange\", \"reppenrange\")): vars.rep_pen_range = v\n    if(setting in (\"settknmax\", \"tknmax\")): vars.max_length = v; return True\n    if(setting == \"anotedepth\"): vars.andepth = v; return True\n    if(setting in (\"setwidepth\", \"widepth\")): vars.widepth = v; return True\n    if(setting in (\"setuseprompt\", \"useprompt\")): vars.useprompt = v; return True\n    if(setting in (\"setadventure\", \"adventure\")): vars.adventure = v\n    if(setting in (\"setdynamicscan\", \"dynamicscan\")): vars.dynamicscan = v\n    if(setting in (\"setnopromptgen\", \"nopromptgen\")): vars.nopromptgen = v\n    if(setting in (\"autosave\", \"noautosave\")): vars.autosave = v\n    if(setting in (\"setrngpersist\", \"rngpersist\")): vars.rngpersist = v\n    if(setting in (\"setchatmode\", \"chatmode\")): vars.chatmode = v\n    if(setting in (\"frmttriminc\", \"triminc\")): vars.formatoptns[\"frmttriminc\"] = v\n    if(setting in (\"frmtrmblln\", \"rmblln\")): vars.formatoptns[\"frmttrmblln\"] = v\n    if(setting in (\"frmtrmspch\", \"rmspch\")): vars.formatoptns[\"frmttrmspch\"] = v\n    if(setting in (\"frmtadsnsp\", \"adsnsp\")): vars.formatoptns[\"frmtadsnsp\"] = v\n    if(setting in (\"frmtsingleline\", \"singleline\")): vars.formatoptns[\"singleline\"] = v\n    if(setting == \"output_streaming\"): vars.output_streaming = v\n    if(setting == \"show_probs\"): vars.show_probs = v\n\n#==================================================================#\n#  Get contents of memory\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_memory():\n    return vars.memory\n\n#==================================================================#\n#  Set contents of memory\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_memory(m):\n    assert type(m) is str\n    vars.memory = m\n\n#==================================================================#\n#  Get contents of author's note\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_authorsnote():\n    return vars.authornote\n\n#==================================================================#\n#  Set contents of author's note\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_authorsnote(m):\n    assert type(m) is str\n    vars.authornote = m\n\n#==================================================================#\n#  Get contents of author's note template\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_authorsnotetemplate():\n    return vars.authornotetemplate\n\n#==================================================================#\n#  Set contents of author's note template\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_authorsnotetemplate(m):\n    assert type(m) is str\n    vars.authornotetemplate = m\n\n#==================================================================#\n#  Save settings and send them to client\n#==================================================================#\n@bridged_kwarg()\ndef lua_resend_settings():\n    settingschanged()\n    refresh_settings()\n\n#==================================================================#\n#  Set story chunk text and delete the chunk if the new chunk is empty\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_chunk(k, v):\n    assert type(k) in (int, None) and type(v) is str\n    assert k >= 0\n    assert k != 0 or len(v) != 0\n    if(len(v) == 0):\n        print(colors.GREEN + f\"{lua_log_format_name(vars.lua_koboldbridge.logging_name)} deleted story chunk {k}\" + colors.END)\n        chunk = int(k)\n        if(vars.lua_koboldbridge.userstate == \"genmod\"):\n            del vars._actions[chunk-1]\n        vars.lua_deleted.add(chunk)\n        if(not hasattr(vars, \"_actions\") or vars._actions is not vars.actions):\n            #Instead of deleting we'll blank out the text. This way our actions and actions_metadata stay in sync and we can restore the chunk on an undo\n            vars.actions[chunk-1] = \"\"\n            vars.actions_metadata[chunk-1]['Alternative Text'] = [{\"Text\": vars.actions_metadata[chunk-1]['Selected Text'], \"Pinned\": False, \"Editted\": True}] + vars.actions_metadata[chunk-1]['Alternative Text']\n            vars.actions_metadata[chunk-1]['Selected Text'] = ''\n            send_debug()\n    else:\n        if(k == 0):\n            print(colors.GREEN + f\"{lua_log_format_name(vars.lua_koboldbridge.logging_name)} edited prompt chunk\" + colors.END)\n        else:\n            print(colors.GREEN + f\"{lua_log_format_name(vars.lua_koboldbridge.logging_name)} edited story chunk {k}\" + colors.END)\n        chunk = int(k)\n        if(chunk == 0):\n            if(vars.lua_koboldbridge.userstate == \"genmod\"):\n                vars._prompt = v\n            vars.lua_edited.add(chunk)\n            vars.prompt = v\n        else:\n            if(vars.lua_koboldbridge.userstate == \"genmod\"):\n                vars._actions[chunk-1] = v\n            vars.lua_edited.add(chunk)\n            vars.actions[chunk-1] = v\n            vars.actions_metadata[chunk-1]['Alternative Text'] = [{\"Text\": vars.actions_metadata[chunk-1]['Selected Text'], \"Pinned\": False, \"Editted\": True}] + vars.actions_metadata[chunk-1]['Alternative Text']\n            vars.actions_metadata[chunk-1]['Selected Text'] = v\n            send_debug()\n\n#==================================================================#\n#  Get model type as \"gpt-2-xl\", \"gpt-neo-2.7B\", etc.\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_modeltype():\n    if(vars.noai):\n        return \"readonly\"\n    if(vars.model in (\"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"InferKit\")):\n        return \"api\"\n    if(not vars.use_colab_tpu and vars.model not in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\") and (vars.model in (\"GPT2Custom\", \"NeoCustom\") or vars.model_type in (\"gpt2\", \"gpt_neo\", \"gptj\"))):\n        hidden_size = get_hidden_size_from_model(model)\n    if(vars.model in (\"gpt2\",) or (vars.model_type == \"gpt2\" and hidden_size == 768)):\n        return \"gpt2\"\n    if(vars.model in (\"gpt2-medium\",) or (vars.model_type == \"gpt2\" and hidden_size == 1024)):\n        return \"gpt2-medium\"\n    if(vars.model in (\"gpt2-large\",) or (vars.model_type == \"gpt2\" and hidden_size == 1280)):\n        return \"gpt2-large\"\n    if(vars.model in (\"gpt2-xl\",) or (vars.model_type == \"gpt2\" and hidden_size == 1600)):\n        return \"gpt2-xl\"\n    if(vars.model_type == \"gpt_neo\" and hidden_size == 768):\n        return \"gpt-neo-125M\"\n    if(vars.model in (\"EleutherAI/gpt-neo-1.3B\",) or (vars.model_type == \"gpt_neo\" and hidden_size == 2048)):\n        return \"gpt-neo-1.3B\"\n    if(vars.model in (\"EleutherAI/gpt-neo-2.7B\",) or (vars.model_type == \"gpt_neo\" and hidden_size == 2560)):\n        return \"gpt-neo-2.7B\"\n    if(vars.model in (\"EleutherAI/gpt-j-6B\",) or ((vars.use_colab_tpu or vars.model == \"TPUMeshTransformerGPTJ\") and tpu_mtj_backend.params[\"d_model\"] == 4096) or (vars.model_type in (\"gpt_neo\", \"gptj\") and hidden_size == 4096)):\n        return \"gpt-j-6B\"\n    return \"unknown\"\n\n#==================================================================#\n#  Get model backend as \"transformers\" or \"mtj\"\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_modelbackend():\n    if(vars.noai):\n        return \"readonly\"\n    if(vars.model in (\"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"InferKit\")):\n        return \"api\"\n    if(vars.use_colab_tpu or vars.model in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")):\n        return \"mtj\"\n    return \"transformers\"\n\n#==================================================================#\n#  Check whether model is loaded from a custom path\n#==================================================================#\n@bridged_kwarg()\ndef lua_is_custommodel():\n    return vars.model in (\"GPT2Custom\", \"NeoCustom\", \"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")\n\n#==================================================================#\n#  Return the filename (as a string) of the current soft prompt, or\n#  None if no soft prompt is loaded\n#==================================================================#\n@bridged_kwarg()\ndef lua_get_spfilename():\n    return vars.spfilename.strip() or None\n\n#==================================================================#\n#  When called with a string as argument, sets the current soft prompt;\n#  when called with None as argument, uses no soft prompt.\n#  Returns True if soft prompt changed, False otherwise.\n#==================================================================#\n@bridged_kwarg()\ndef lua_set_spfilename(filename: Union[str, None]):\n    if(filename is None):\n        filename = \"\"\n    filename = str(filename).strip()\n    changed = lua_get_spfilename() != filename\n    assert all(q not in filename for q in (\"/\", \"\\\\\"))\n    spRequest(filename)\n    return changed\n\n#==================================================================#\n#  \n#==================================================================#\ndef execute_inmod():\n    setgamesaved(False)\n    vars.lua_logname = ...\n    vars.lua_edited = set()\n    vars.lua_deleted = set()\n    try:\n        tpool.execute(vars.lua_koboldbridge.execute_inmod)\n    except lupa.LuaError as e:\n        vars.lua_koboldbridge.obliterate_multiverse()\n        vars.lua_running = False\n        emit('from_server', {'cmd': 'errmsg', 'data': 'Lua script error; please check console.'}, broadcast=True)\n        sendUSStatItems()\n        logger.error('LUA ERROR: ' + str(e).replace(\"\\033\", \"\"))\n        logger.warning(\"Lua engine stopped; please open 'Userscripts' and press Load to reinitialize scripts.\")\n        set_aibusy(0)\n\ndef execute_genmod():\n    vars.lua_koboldbridge.execute_genmod()\n\ndef execute_outmod():\n    setgamesaved(False)\n    emit('from_server', {'cmd': 'hidemsg', 'data': ''}, broadcast=True)\n    try:\n        tpool.execute(vars.lua_koboldbridge.execute_outmod)\n    except lupa.LuaError as e:\n        vars.lua_koboldbridge.obliterate_multiverse()\n        vars.lua_running = False\n        emit('from_server', {'cmd': 'errmsg', 'data': 'Lua script error; please check console.'}, broadcast=True)\n        sendUSStatItems()\n        logger.error('LUA ERROR: ' + str(e).replace(\"\\033\", \"\"))\n        logger.warning(\"Lua engine stopped; please open 'Userscripts' and press Load to reinitialize scripts.\")\n        set_aibusy(0)\n    if(vars.lua_koboldbridge.resend_settings_required):\n        vars.lua_koboldbridge.resend_settings_required = False\n        lua_resend_settings()\n    for k in vars.lua_edited:\n        inlineedit(k, vars.actions[k])\n    for k in vars.lua_deleted:\n        inlinedelete(k)\n\n\n\n\n#============================ METHODS =============================#    \n\n#==================================================================#\n# Event triggered when browser SocketIO is loaded and connects to server\n#==================================================================#\n@socketio.on('connect')\ndef do_connect():\n    logger.info(\"Client connected!\")\n    emit('from_server', {'cmd': 'setchatname', 'data': vars.chatname})\n    emit('from_server', {'cmd': 'setanotetemplate', 'data': vars.authornotetemplate})\n    emit('from_server', {'cmd': 'connected', 'smandelete': vars.smandelete, 'smanrename': vars.smanrename, 'modelname': getmodelname()})\n    if(vars.host):\n        emit('from_server', {'cmd': 'runs_remotely'})\n    if(vars.allowsp):\n        emit('from_server', {'cmd': 'allowsp', 'data': vars.allowsp})\n\n    sendUSStatItems()\n    emit('from_server', {'cmd': 'spstatitems', 'data': {vars.spfilename: vars.spmeta} if vars.allowsp and len(vars.spfilename) else {}}, broadcast=True)\n\n    if(not vars.gamestarted):\n        setStartState()\n        sendsettings()\n        refresh_settings()\n        vars.laststory = None\n        emit('from_server', {'cmd': 'setstoryname', 'data': vars.laststory})\n        sendwi()\n        emit('from_server', {'cmd': 'setmemory', 'data': vars.memory})\n        emit('from_server', {'cmd': 'setanote', 'data': vars.authornote})\n        vars.mode = \"play\"\n    else:\n        # Game in session, send current game data and ready state to browser\n        refresh_story()\n        sendsettings()\n        refresh_settings()\n        emit('from_server', {'cmd': 'setstoryname', 'data': vars.laststory})\n        sendwi()\n        emit('from_server', {'cmd': 'setmemory', 'data': vars.memory})\n        emit('from_server', {'cmd': 'setanote', 'data': vars.authornote})\n        if(vars.mode == \"play\"):\n            if(not vars.aibusy):\n                emit('from_server', {'cmd': 'setgamestate', 'data': 'ready'})\n            else:\n                emit('from_server', {'cmd': 'setgamestate', 'data': 'wait'})\n        elif(vars.mode == \"edit\"):\n            emit('from_server', {'cmd': 'editmode', 'data': 'true'})\n        elif(vars.mode == \"memory\"):\n            emit('from_server', {'cmd': 'memmode', 'data': 'true'})\n        elif(vars.mode == \"wi\"):\n            emit('from_server', {'cmd': 'wimode', 'data': 'true'})\n\n    emit('from_server', {'cmd': 'gamesaved', 'data': vars.gamesaved}, broadcast=True)\n\n#==================================================================#\n# Event triggered when browser SocketIO sends data to the server\n#==================================================================#\n@socketio.on('message')\ndef get_message(msg):\n    if not vars.quiet:\n        logger.debug(f\"Data received: {msg}\")\n    # Submit action\n    if(msg['cmd'] == 'submit'):\n        if(vars.mode == \"play\"):\n            if(vars.aibusy):\n                if(msg.get('allowabort', False)):\n                    vars.abort = True\n                return\n            vars.abort = False\n            vars.lua_koboldbridge.feedback = None\n            if(vars.chatmode):\n                if(type(msg['chatname']) is not str):\n                    raise ValueError(\"Chatname must be a string\")\n                vars.chatname = msg['chatname']\n                settingschanged()\n                emit('from_server', {'cmd': 'setchatname', 'data': vars.chatname})\n            vars.recentrng = vars.recentrngm = None\n            actionsubmit(msg['data'], actionmode=msg['actionmode'])\n        elif(vars.mode == \"edit\"):\n            editsubmit(msg['data'])\n        elif(vars.mode == \"memory\"):\n            memsubmit(msg['data'])\n    # Retry Action\n    elif(msg['cmd'] == 'retry'):\n        if(vars.aibusy):\n            if(msg.get('allowabort', False)):\n                vars.abort = True\n            return\n        vars.abort = False\n        if(vars.chatmode):\n            if(type(msg['chatname']) is not str):\n                raise ValueError(\"Chatname must be a string\")\n            vars.chatname = msg['chatname']\n            settingschanged()\n            emit('from_server', {'cmd': 'setchatname', 'data': vars.chatname})\n        actionretry(msg['data'])\n    # Back/Undo Action\n    elif(msg['cmd'] == 'back'):\n        ignore = actionback()\n    # Forward/Redo Action\n    elif(msg['cmd'] == 'redo'):\n        actionredo()\n    # EditMode Action (old)\n    elif(msg['cmd'] == 'edit'):\n        if(vars.mode == \"play\"):\n            vars.mode = \"edit\"\n            emit('from_server', {'cmd': 'editmode', 'data': 'true'}, broadcast=True)\n        elif(vars.mode == \"edit\"):\n            vars.mode = \"play\"\n            emit('from_server', {'cmd': 'editmode', 'data': 'false'}, broadcast=True)\n    # EditLine Action (old)\n    elif(msg['cmd'] == 'editline'):\n        editrequest(int(msg['data']))\n    # Inline edit\n    elif(msg['cmd'] == 'inlineedit'):\n        inlineedit(msg['chunk'], msg['data'])\n    elif(msg['cmd'] == 'inlinedelete'):\n        inlinedelete(msg['data'])\n    # DeleteLine Action (old)\n    elif(msg['cmd'] == 'delete'):\n        deleterequest()\n    elif(msg['cmd'] == 'memory'):\n        togglememorymode()\n    elif(not vars.host and msg['cmd'] == 'savetofile'):\n        savetofile()\n    elif(not vars.host and msg['cmd'] == 'loadfromfile'):\n        loadfromfile()\n    elif(msg['cmd'] == 'loadfromstring'):\n        loadRequest(json.loads(msg['data']), filename=msg['filename'])\n    elif(not vars.host and msg['cmd'] == 'import'):\n        importRequest()\n    elif(msg['cmd'] == 'newgame'):\n        newGameRequest()\n    elif(msg['cmd'] == 'rndgame'):\n        randomGameRequest(msg['data'], memory=msg['memory'])\n    elif(msg['cmd'] == 'settemp'):\n        vars.temp = float(msg['data'])\n        emit('from_server', {'cmd': 'setlabeltemp', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'settopp'):\n        vars.top_p = float(msg['data'])\n        emit('from_server', {'cmd': 'setlabeltopp', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'settopk'):\n        vars.top_k = int(msg['data'])\n        emit('from_server', {'cmd': 'setlabeltopk', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'settfs'):\n        vars.tfs = float(msg['data'])\n        emit('from_server', {'cmd': 'setlabeltfs', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'settypical'):\n        vars.typical = float(msg['data'])\n        emit('from_server', {'cmd': 'setlabeltypical', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'settopa'):\n        vars.top_a = float(msg['data'])\n        emit('from_server', {'cmd': 'setlabeltopa', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setreppen'):\n        vars.rep_pen = float(msg['data'])\n        emit('from_server', {'cmd': 'setlabelreppen', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setreppenslope'):\n        vars.rep_pen_slope = float(msg['data'])\n        emit('from_server', {'cmd': 'setlabelreppenslope', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setreppenrange'):\n        vars.rep_pen_range = float(msg['data'])\n        emit('from_server', {'cmd': 'setlabelreppenrange', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setoutput'):\n        vars.genamt = int(msg['data'])\n        emit('from_server', {'cmd': 'setlabeloutput', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'settknmax'):\n        vars.max_length = int(msg['data'])\n        emit('from_server', {'cmd': 'setlabeltknmax', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setikgen'):\n        vars.ikgen = int(msg['data'])\n        emit('from_server', {'cmd': 'setlabelikgen', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    # Author's Note field update\n    elif(msg['cmd'] == 'anote'):\n        anotesubmit(msg['data'], template=msg['template'])\n    # Author's Note depth update\n    elif(msg['cmd'] == 'anotedepth'):\n        vars.andepth = int(msg['data'])\n        emit('from_server', {'cmd': 'setlabelanotedepth', 'data': msg['data']}, broadcast=True)\n        settingschanged()\n        refresh_settings()\n    # Format - Trim incomplete sentences\n    elif(msg['cmd'] == 'frmttriminc'):\n        if('frmttriminc' in vars.formatoptns):\n            vars.formatoptns[\"frmttriminc\"] = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'frmtrmblln'):\n        if('frmtrmblln' in vars.formatoptns):\n            vars.formatoptns[\"frmtrmblln\"] = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'frmtrmspch'):\n        if('frmtrmspch' in vars.formatoptns):\n            vars.formatoptns[\"frmtrmspch\"] = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'frmtadsnsp'):\n        if('frmtadsnsp' in vars.formatoptns):\n            vars.formatoptns[\"frmtadsnsp\"] = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'singleline'):\n        if('singleline' in vars.formatoptns):\n            vars.formatoptns[\"singleline\"] = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'importselect'):\n        vars.importnum = int(msg[\"data\"].replace(\"import\", \"\"))\n    elif(msg['cmd'] == 'importcancel'):\n        emit('from_server', {'cmd': 'popupshow', 'data': False})\n        vars.importjs  = {}\n    elif(msg['cmd'] == 'importaccept'):\n        emit('from_server', {'cmd': 'popupshow', 'data': False})\n        importgame()\n    elif(msg['cmd'] == 'wi'):\n        togglewimode()\n    elif(msg['cmd'] == 'wiinit'):\n        if(int(msg['data']) < len(vars.worldinfo)):\n            setgamesaved(False)\n            vars.worldinfo[msg['data']][\"init\"] = True\n            addwiitem(folder_uid=msg['folder'])\n    elif(msg['cmd'] == 'wifolderinit'):\n        addwifolder()\n    elif(msg['cmd'] == 'wimoveitem'):\n        movewiitem(msg['destination'], msg['data'])\n    elif(msg['cmd'] == 'wimovefolder'):\n        movewifolder(msg['destination'], msg['data'])\n    elif(msg['cmd'] == 'widelete'):\n        deletewi(msg['data'])\n    elif(msg['cmd'] == 'wifolderdelete'):\n        deletewifolder(msg['data'])\n    elif(msg['cmd'] == 'wiexpand'):\n        assert 0 <= int(msg['data']) < len(vars.worldinfo)\n        setgamesaved(False)\n        emit('from_server', {'cmd': 'wiexpand', 'data': msg['data']}, broadcast=True)\n    elif(msg['cmd'] == 'wiexpandfolder'):\n        assert 0 <= int(msg['data']) < len(vars.worldinfo)\n        setgamesaved(False)\n        emit('from_server', {'cmd': 'wiexpandfolder', 'data': msg['data']}, broadcast=True)\n    elif(msg['cmd'] == 'wifoldercollapsecontent'):\n        setgamesaved(False)\n        vars.wifolders_d[msg['data']]['collapsed'] = True\n        emit('from_server', {'cmd': 'wifoldercollapsecontent', 'data': msg['data']}, broadcast=True)\n    elif(msg['cmd'] == 'wifolderexpandcontent'):\n        setgamesaved(False)\n        vars.wifolders_d[msg['data']]['collapsed'] = False\n        emit('from_server', {'cmd': 'wifolderexpandcontent', 'data': msg['data']}, broadcast=True)\n    elif(msg['cmd'] == 'wiupdate'):\n        setgamesaved(False)\n        num = int(msg['num'])\n        fields = (\"key\", \"keysecondary\", \"content\", \"comment\")\n        for field in fields:\n            if(field in msg['data'] and type(msg['data'][field]) is str):\n                vars.worldinfo[num][field] = msg['data'][field]\n        emit('from_server', {'cmd': 'wiupdate', 'num': msg['num'], 'data': {field: vars.worldinfo[num][field] for field in fields}}, broadcast=True)\n    elif(msg['cmd'] == 'wifolderupdate'):\n        setgamesaved(False)\n        uid = int(msg['uid'])\n        fields = (\"name\", \"collapsed\")\n        for field in fields:\n            if(field in msg['data'] and type(msg['data'][field]) is (str if field != \"collapsed\" else bool)):\n                vars.wifolders_d[uid][field] = msg['data'][field]\n        emit('from_server', {'cmd': 'wifolderupdate', 'uid': msg['uid'], 'data': {field: vars.wifolders_d[uid][field] for field in fields}}, broadcast=True)\n    elif(msg['cmd'] == 'wiselon'):\n        setgamesaved(False)\n        vars.worldinfo[msg['data']][\"selective\"] = True\n        emit('from_server', {'cmd': 'wiselon', 'data': msg['data']}, broadcast=True)\n    elif(msg['cmd'] == 'wiseloff'):\n        setgamesaved(False)\n        vars.worldinfo[msg['data']][\"selective\"] = False\n        emit('from_server', {'cmd': 'wiseloff', 'data': msg['data']}, broadcast=True)\n    elif(msg['cmd'] == 'wiconstanton'):\n        setgamesaved(False)\n        vars.worldinfo[msg['data']][\"constant\"] = True\n        emit('from_server', {'cmd': 'wiconstanton', 'data': msg['data']}, broadcast=True)\n    elif(msg['cmd'] == 'wiconstantoff'):\n        setgamesaved(False)\n        vars.worldinfo[msg['data']][\"constant\"] = False\n        emit('from_server', {'cmd': 'wiconstantoff', 'data': msg['data']}, broadcast=True)\n    elif(msg['cmd'] == 'sendwilist'):\n        commitwi(msg['data'])\n    elif(msg['cmd'] == 'aidgimport'):\n        importAidgRequest(msg['data'])\n    elif(msg['cmd'] == 'saveasrequest'):\n        saveas(msg['data'])\n    elif(msg['cmd'] == 'saverequest'):\n        save()\n    elif(msg['cmd'] == 'loadlistrequest'):\n        getloadlist()\n    elif(msg['cmd'] == 'splistrequest'):\n        getsplist()\n    elif(msg['cmd'] == 'uslistrequest'):\n        unloaded, loaded = getuslist()\n        emit('from_server', {'cmd': 'buildus', 'data': {\"unloaded\": unloaded, \"loaded\": loaded}})\n    elif(msg['cmd'] == 'samplerlistrequest'):\n        emit('from_server', {'cmd': 'buildsamplers', 'data': vars.sampler_order})\n    elif(msg['cmd'] == 'usloaded'):\n        vars.userscripts = []\n        for userscript in msg['data']:\n            if type(userscript) is not str:\n                continue\n            userscript = userscript.strip()\n            if len(userscript) != 0 and all(q not in userscript for q in (\"..\", \":\")) and all(userscript[0] not in q for q in (\"/\", \"\\\\\")) and os.path.exists(fileops.uspath(userscript)):\n                vars.userscripts.append(userscript)\n        settingschanged()\n    elif(msg['cmd'] == 'usload'):\n        load_lua_scripts()\n        unloaded, loaded = getuslist()\n        sendUSStatItems()\n    elif(msg['cmd'] == 'samplers'):\n        sampler_order = msg[\"data\"]\n        sampler_order_min_length = 6\n        sampler_order_max_length = 7\n        if(not isinstance(sampler_order, list)):\n            raise ValueError(f\"Sampler order must be a list, but got a {type(sampler_order)}\")\n        if(not (sampler_order_min_length <= len(sampler_order) <= sampler_order_max_length)):\n            raise ValueError(f\"Sampler order must be a list of length greater than or equal to {sampler_order_min_length} and less than or equal to {sampler_order_max_length}, but got a list of length {len(sampler_order)}\")\n        if(not all(isinstance(e, int) for e in sampler_order)):\n            raise ValueError(f\"Sampler order must be a list of ints, but got a list with at least one non-int element\")\n        if(min(sampler_order) != 0 or max(sampler_order) != len(sampler_order) - 1 or len(set(sampler_order)) != len(sampler_order)):\n            raise ValueError(f\"Sampler order list of length {len(sampler_order)} must be a permutation of the first {len(sampler_order)} nonnegative integers\")\n        vars.sampler_order = sampler_order\n        settingschanged()\n    elif(msg['cmd'] == 'list_model'):\n        sendModelSelection(menu=msg['data'])\n    elif(msg['cmd'] == 'load_model'):\n        logger.debug(f\"Selected Model: {vars.model_selected}\")\n        if not os.path.exists(\"settings/\"):\n            os.mkdir(\"settings\")\n        changed = True\n        if not utils.HAS_ACCELERATE:\n            msg['disk_layers'] = \"0\"\n        if os.path.exists(\"settings/\" + vars.model_selected.replace('/', '_') + \".breakmodel\"):\n            with open(\"settings/\" + vars.model_selected.replace('/', '_') + \".breakmodel\", \"r\") as file:\n                data = file.read().split('\\n')[:2]\n                if len(data) < 2:\n                    data.append(\"0\")\n                gpu_layers, disk_layers = data\n                if gpu_layers == msg['gpu_layers'] and disk_layers == msg['disk_layers']:\n                    changed = False\n        if changed:\n            if vars.model_selected in [\"NeoCustom\", \"GPT2Custom\"]:\n                filename = \"settings/{}.breakmodel\".format(os.path.basename(os.path.normpath(vars.custmodpth)))\n            else:\n                filename = \"settings/{}.breakmodel\".format(vars.model_selected.replace('/', '_'))\n            f = open(filename, \"w\")\n            f.write(str(msg['gpu_layers']) + '\\n' + str(msg['disk_layers']))\n            f.close()\n        vars.colaburl = msg['url'] + \"/request\"\n        vars.model = vars.model_selected\n        if vars.model == \"CLUSTER\":\n            if type(msg['online_model']) is not list:\n                if msg['online_model'] == '':\n                    vars.cluster_requested_models = []\n                else:\n                    vars.cluster_requested_models = [msg['online_model']]\n            else:\n                vars.cluster_requested_models = msg['online_model']\n        load_model(use_gpu=msg['use_gpu'], gpu_layers=msg['gpu_layers'], disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n    elif(msg['cmd'] == 'show_model'):\n        logger.info(f\"Model Name: {getmodelname()}\")\n        emit('from_server', {'cmd': 'show_model_name', 'data': getmodelname()}, broadcast=True)\n    elif(msg['cmd'] == 'selectmodel'):\n        # This is run when a model line is selected from the UI (line from the model_menu variable) that is tagged as not a menu\n        # otherwise we should be running the msg['cmd'] == 'list_model'\n        \n        # We have to do a bit of processing though, if we select a custom path, we need to list out the contents of folders\n        # But if we select something else, we need to potentially show model layers for each GPU\n        # We might also need to show key input. All of that happens here\n        \n        # The data variable will contain the model name. But our Custom lines need a bit more processing\n        # If we're on a custom line that we have selected a model for, the path variable will be in msg\n        # so if that's missing we need to run the menu to show the model folders in the models folder\n        if msg['data'] in ('NeoCustom', 'GPT2Custom') and 'path' not in msg and 'path_modelname' not in msg:\n            if 'folder' not in msg or vars.host:\n                folder = \"./models\"\n            else:\n                folder = msg['folder']\n            sendModelSelection(menu=msg['data'], folder=folder)\n        elif msg['data'] in ('NeoCustom', 'GPT2Custom') and 'path_modelname' in msg:\n            #Here the user entered custom text in the text box. This could be either a model name or a path.\n            if check_if_dir_is_model(msg['path_modelname']):\n                vars.model_selected = msg['data']\n                vars.custmodpth = msg['path_modelname']\n                get_model_info(msg['data'], directory=msg['path'])\n            else:\n                vars.model_selected = msg['path_modelname']\n                try:\n                    get_model_info(vars.model_selected)\n                except:\n                    emit('from_server', {'cmd': 'errmsg', 'data': \"The model entered doesn't exist.\"})\n        elif msg['data'] in ('NeoCustom', 'GPT2Custom'):\n            if check_if_dir_is_model(msg['path']):\n                vars.model_selected = msg['data']\n                vars.custmodpth = msg['path']\n                get_model_info(msg['data'], directory=msg['path'])\n            else:\n                if vars.host:\n                    sendModelSelection(menu=msg['data'], folder=\"./models\")\n                else:\n                    sendModelSelection(menu=msg['data'], folder=msg['path'])\n        else:\n            vars.model_selected = msg['data'] \n            if 'path' in msg:\n                vars.custmodpth = msg['path']\n                get_model_info(msg['data'], directory=msg['path'])\n            else:\n                get_model_info(vars.model_selected)\n    elif(msg['cmd'] == 'delete_model'):\n        if \"{}/models\".format(os.getcwd()) in os.path.abspath(msg['data']) or \"{}\\\\models\".format(os.getcwd()) in os.path.abspath(msg['data']):\n            if check_if_dir_is_model(msg['data']):\n                logger.warning(f\"Someone deleted {msg['data']}\")\n                import shutil\n                shutil.rmtree(msg['data'])\n                sendModelSelection(menu=msg['menu'])\n            else:\n                logger.error(f\"Someone attempted to delete {msg['data']} but this is not a valid model\")\n        else:\n            logger.critical(f\"Someone maliciously attempted to delete {msg['data']}. The attempt has been blocked.\")\n    elif(msg['cmd'] == 'OAI_Key_Update'):\n        get_oai_models(msg['key'])\n    elif(msg['cmd'] == 'Cluster_Key_Update'):\n        get_cluster_models(msg)\n    elif(msg['cmd'] == 'loadselect'):\n        vars.loadselect = msg[\"data\"]\n    elif(msg['cmd'] == 'spselect'):\n        vars.spselect = msg[\"data\"]\n    elif(msg['cmd'] == 'loadrequest'):\n        loadRequest(fileops.storypath(vars.loadselect))\n    elif(msg['cmd'] == 'sprequest'):\n        spRequest(vars.spselect)\n    elif(msg['cmd'] == 'deletestory'):\n        deletesave(msg['data'])\n    elif(msg['cmd'] == 'renamestory'):\n        renamesave(msg['data'], msg['newname'])\n    elif(msg['cmd'] == 'clearoverwrite'):    \n        vars.svowname = \"\"\n        vars.saveow   = False\n    elif(msg['cmd'] == 'seqsel'):\n        selectsequence(msg['data'])\n    elif(msg['cmd'] == 'seqpin'):\n        pinsequence(msg['data'])\n    elif(msg['cmd'] == 'setnumseq'):\n        vars.numseqs = int(msg['data'])\n        emit('from_server', {'cmd': 'setlabelnumseq', 'data': msg['data']})\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setwidepth'):\n        vars.widepth = int(msg['data'])\n        emit('from_server', {'cmd': 'setlabelwidepth', 'data': msg['data']})\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setuseprompt'):\n        vars.useprompt = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setadventure'):\n        vars.adventure = msg['data']\n        vars.chatmode = False\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'autosave'):\n        vars.autosave = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setchatmode'):\n        vars.chatmode = msg['data']\n        vars.adventure = False\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setdynamicscan'):\n        vars.dynamicscan = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setnopromptgen'):\n        vars.nopromptgen = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setrngpersist'):\n        vars.rngpersist = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setnogenmod'):\n        vars.nogenmod = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setfulldeterminism'):\n        vars.full_determinism = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setoutputstreaming'):\n        vars.output_streaming = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setshowbudget'):\n        vars.show_budget = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(msg['cmd'] == 'setshowprobs'):\n        vars.show_probs = msg['data']\n        settingschanged()\n        refresh_settings()\n    elif(not vars.host and msg['cmd'] == 'importwi'):\n        wiimportrequest()\n    elif(msg['cmd'] == 'debug'):\n        vars.debug = msg['data']\n        emit('from_server', {'cmd': 'set_debug', 'data': msg['data']}, broadcast=True)\n        if vars.debug:\n            send_debug()\n    elif(msg['cmd'] == 'getfieldbudget'):\n        unencoded = msg[\"data\"][\"unencoded\"]\n        field = msg[\"data\"][\"field\"]\n\n        # Tokenizer may be undefined here when a model has not been chosen.\n        if \"tokenizer\" not in globals():\n            # We don't have a tokenizer, just return nulls.\n            emit(\n                'from_server',\n                {'cmd': 'showfieldbudget', 'data': {\"length\": None, \"max\": None, \"field\": field}},\n            )\n            return\n\n        header_length = len(tokenizer._koboldai_header)\n        max_tokens = vars.max_length - header_length - vars.sp_length - vars.genamt\n\n        if not unencoded:\n            # Unencoded is empty, just return 0\n            emit(\n                'from_server',\n                {'cmd': 'showfieldbudget', 'data': {\"length\": 0, \"max\": max_tokens, \"field\": field}},\n                broadcast=True\n            )\n        else:\n            if field == \"anoteinput\":\n                unencoded = buildauthorsnote(unencoded, msg[\"data\"][\"anotetemplate\"])\n            tokens_length = len(tokenizer.encode(unencoded))\n\n            emit(\n                'from_server',\n                {'cmd': 'showfieldbudget', 'data': {\"length\": tokens_length, \"max\": max_tokens, \"field\": field}},\n                broadcast=True\n            )\n\n#==================================================================#\n#  Send userscripts list to client\n#==================================================================#\ndef sendUSStatItems():\n    _, loaded = getuslist()\n    loaded = loaded if vars.lua_running else []\n    last_userscripts = [e[\"filename\"] for e in loaded]\n    emit('from_server', {'cmd': 'usstatitems', 'data': loaded, 'flash': last_userscripts != vars.last_userscripts}, broadcast=True)\n    vars.last_userscripts = last_userscripts\n\n#==================================================================#\n#  KoboldAI Markup Formatting (Mixture of Markdown and sanitized html)\n#==================================================================#\ndef kml(txt):\n   txt = txt.replace('>', '&gt;')\n   txt = bleach.clean(markdown.markdown(txt), tags = ['p', 'em', 'strong', 'code', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'ul', 'b', 'i', 'a', 'span', 'button'], styles = ['color', 'font-weight'], attributes=['id', 'class', 'style', 'href'])\n   return txt\n\n#==================================================================#\n#  Send start message and tell Javascript to set UI state\n#==================================================================#\ndef setStartState():\n    if(vars.welcome):\n        txt = kml(vars.welcome) + \"<br/>\"\n    else:\n        txt = \"<span>Welcome to <span class=\\\"color_cyan\\\">KoboldAI</span>! You are running <span class=\\\"color_green\\\">\"+getmodelname()+\"</span>.<br/>\"\n    if(not vars.noai and not vars.welcome):\n        txt = txt + \"Please load a game or enter a prompt below to begin!</span>\"\n    if(vars.noai):\n        txt = txt + \"Please load or import a story to read. There is no AI in this mode.\"\n    emit('from_server', {'cmd': 'updatescreen', 'gamestarted': vars.gamestarted, 'data': txt}, broadcast=True)\n    emit('from_server', {'cmd': 'setgamestate', 'data': 'start'}, broadcast=True)\n\n#==================================================================#\n#  Transmit applicable settings to SocketIO to build UI sliders/toggles\n#==================================================================#\ndef sendsettings():\n    # Send settings for selected AI type\n    emit('from_server', {'cmd': 'reset_menus'})\n    if(vars.model != \"InferKit\"):\n        for set in gensettings.gensettingstf:\n            emit('from_server', {'cmd': 'addsetting', 'data': set})\n    else:\n        for set in gensettings.gensettingsik:\n            emit('from_server', {'cmd': 'addsetting', 'data': set})\n    \n    # Send formatting options\n    for frm in gensettings.formatcontrols:\n        emit('from_server', {'cmd': 'addformat', 'data': frm})\n        # Add format key to vars if it wasn't loaded with client.settings\n        if(not frm[\"id\"] in vars.formatoptns):\n            vars.formatoptns[frm[\"id\"]] = False;\n\n#==================================================================#\n#  Set value of gamesaved\n#==================================================================#\ndef setgamesaved(gamesaved):\n    assert type(gamesaved) is bool\n    if(gamesaved != vars.gamesaved):\n        emit('from_server', {'cmd': 'gamesaved', 'data': gamesaved}, broadcast=True)\n    vars.gamesaved = gamesaved\n\n#==================================================================#\n#  Take input text from SocketIO and decide what to do with it\n#==================================================================#\n\ndef check_for_backend_compilation():\n    if(vars.checking):\n        return\n    vars.checking = True\n    for _ in range(31):\n        time.sleep(0.06276680299820175)\n        if(vars.compiling):\n            emit('from_server', {'cmd': 'warnmsg', 'data': 'Compiling TPU backend&mdash;this usually takes 1&ndash;2 minutes...'}, broadcast=True)\n            break\n    vars.checking = False\n\ndef actionsubmit(data, actionmode=0, force_submit=False, force_prompt_gen=False, disable_recentrng=False, no_generate=False, ignore_aibusy=False):\n    # Ignore new submissions if the AI is currently busy\n    if(not ignore_aibusy and vars.aibusy):\n        return\n    \n    while(True):\n        set_aibusy(1)\n\n        if(vars.model in [\"API\",\"CLUSTER\"]):\n            global tokenizer\n            if vars.model == \"API\":\n                tokenizer_id = requests.get(\n                    vars.colaburl[:-8] + \"/api/v1/model\",\n                ).json()[\"result\"]\n            elif len(vars.cluster_requested_models) >= 1:\n                # If the player has requested one or more models, we use the first one for the tokenizer\n                tokenizer_id = vars.cluster_requested_models[0]\n            # The cluster can return any number of possible models for each gen, but this happens after this step\n            # So at this point, this is unknown\n            else:\n                tokenizer_id = \"\"\n            if tokenizer_id != vars.api_tokenizer_id:\n                try:\n                    if(os.path.isdir(tokenizer_id)):\n                        try:\n                            tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, revision=args.revision, cache_dir=\"cache\")\n                        except:\n                            tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, revision=args.revision, cache_dir=\"cache\", use_fast=False)\n                    elif(os.path.isdir(\"models/{}\".format(tokenizer_id.replace('/', '_')))):\n                        try:\n                            tokenizer = AutoTokenizer.from_pretrained(\"models/{}\".format(tokenizer_id.replace('/', '_')), revision=args.revision, cache_dir=\"cache\")\n                        except:\n                            tokenizer = AutoTokenizer.from_pretrained(\"models/{}\".format(tokenizer_id.replace('/', '_')), revision=args.revision, cache_dir=\"cache\", use_fast=False)\n                    else:\n                        try:\n                            tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, revision=args.revision, cache_dir=\"cache\")\n                        except:\n                            tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, revision=args.revision, cache_dir=\"cache\", use_fast=False)\n                except:\n                    logger.warning(f\"Unknown tokenizer {repr(tokenizer_id)}\")\n                vars.api_tokenizer_id = tokenizer_id\n\n        if(disable_recentrng):\n            vars.recentrng = vars.recentrngm = None\n\n        vars.recentback = False\n        vars.recentedit = False\n        vars.actionmode = actionmode\n\n        # \"Action\" mode\n        if(actionmode == 1):\n            data = data.strip().lstrip('>')\n            data = re.sub(r'\\n+', ' ', data)\n            if(len(data)):\n                data = f\"\\n\\n> {data}\\n\"\n        \n        # \"Chat\" mode\n        if(vars.chatmode and vars.gamestarted):\n            data = re.sub(r'\\n+', ' ', data)\n            if(len(data)):\n                data = f\"\\n{vars.chatname}: {data}\\n\"\n        \n        # If we're not continuing, store a copy of the raw input\n        if(data != \"\"):\n            vars.lastact = data\n        \n        if(not vars.gamestarted):\n            vars.submission = data\n            if(not no_generate):\n                execute_inmod()\n            vars.submission = re.sub(r\"[^\\S\\r\\n]*([\\r\\n]*)$\", r\"\\1\", vars.submission)  # Remove trailing whitespace, excluding newlines\n            data = vars.submission\n            if(not force_submit and len(data.strip()) == 0):\n                assert False\n            # Start the game\n            vars.gamestarted = True\n            if(not no_generate and not vars.noai and vars.lua_koboldbridge.generating and (not vars.nopromptgen or force_prompt_gen)):\n                # Save this first action as the prompt\n                vars.prompt = data\n                # Clear the startup text from game screen\n                emit('from_server', {'cmd': 'updatescreen', 'gamestarted': False, 'data': 'Please wait, generating story...'}, broadcast=True)\n                calcsubmit(data) # Run the first action through the generator\n                if(not no_generate and not vars.abort and vars.lua_koboldbridge.restart_sequence is not None and len(vars.genseqs) == 0):\n                    data = \"\"\n                    force_submit = True\n                    disable_recentrng = True\n                    continue\n                emit('from_server', {'cmd': 'scrolldown', 'data': ''}, broadcast=True)\n                break\n            else:\n                # Save this first action as the prompt\n                vars.prompt = data if len(data) > 0 else '\"'\n                for i in range(vars.numseqs):\n                    vars.lua_koboldbridge.outputs[i+1] = \"\"\n                if(not no_generate):\n                    execute_outmod()\n                vars.lua_koboldbridge.regeneration_required = False\n                genout = []\n                for i in range(vars.numseqs):\n                    genout.append({\"generated_text\": vars.lua_koboldbridge.outputs[i+1]})\n                    assert type(genout[-1][\"generated_text\"]) is str\n                if(len(genout) == 1):\n                    genresult(genout[0][\"generated_text\"], flash=False)\n                    refresh_story()\n                    if(len(vars.actions) > 0):\n                        emit('from_server', {'cmd': 'texteffect', 'data': vars.actions.get_last_key() + 1}, broadcast=True)\n                    if(not vars.abort and vars.lua_koboldbridge.restart_sequence is not None):\n                        data = \"\"\n                        force_submit = True\n                        disable_recentrng = True\n                        continue\n                else:\n                    if(not vars.abort and vars.lua_koboldbridge.restart_sequence is not None and vars.lua_koboldbridge.restart_sequence > 0):\n                        genresult(genout[vars.lua_koboldbridge.restart_sequence-1][\"generated_text\"], flash=False)\n                        refresh_story()\n                        data = \"\"\n                        force_submit = True\n                        disable_recentrng = True\n                        continue\n                    genselect(genout)\n                    refresh_story()\n                set_aibusy(0)\n                emit('from_server', {'cmd': 'scrolldown', 'data': ''}, broadcast=True)\n                break\n        else:\n            # Apply input formatting & scripts before sending to tokenizer\n            if(vars.actionmode == 0):\n                data = applyinputformatting(data)\n            vars.submission = data\n            if(not no_generate):\n                execute_inmod()\n            vars.submission = re.sub(r\"[^\\S\\r\\n]*([\\r\\n]*)$\", r\"\\1\", vars.submission)  # Remove trailing whitespace, excluding newlines\n            data = vars.submission\n            # Dont append submission if it's a blank/continue action\n            if(data != \"\"):\n                # Store the result in the Action log\n                if(len(vars.prompt.strip()) == 0):\n                    vars.prompt = data\n                else:\n                    vars.actions.append(data)\n                    # we now need to update the actions_metadata\n                    # we'll have two conditions. \n                    # 1. This is totally new (user entered) \n                    if vars.actions.get_last_key() not in vars.actions_metadata:\n                        vars.actions_metadata[vars.actions.get_last_key()] = {\"Selected Text\": data, \"Alternative Text\": []}\n                    else:\n                    # 2. We've selected a chunk of text that is was presented previously\n                        try:\n                            alternatives = [item['Text'] for item in vars.actions_metadata[len(vars.actions)-1][\"Alternative Text\"]]\n                        except:\n                            logger.debug(len(vars.actions))\n                            logger.debug(vars.actions_metadata)\n                            raise\n                        if data in alternatives:\n                            alternatives = [item for item in vars.actions_metadata[vars.actions.get_last_key() ][\"Alternative Text\"] if item['Text'] != data]\n                            vars.actions_metadata[vars.actions.get_last_key()][\"Alternative Text\"] = alternatives\n                        vars.actions_metadata[vars.actions.get_last_key()][\"Selected Text\"] = data\n                update_story_chunk('last')\n                send_debug()\n\n            if(not no_generate and not vars.noai and vars.lua_koboldbridge.generating):\n                # Off to the tokenizer!\n                calcsubmit(data)\n                if(not vars.abort and vars.lua_koboldbridge.restart_sequence is not None and len(vars.genseqs) == 0):\n                    data = \"\"\n                    force_submit = True\n                    disable_recentrng = True\n                    continue\n                emit('from_server', {'cmd': 'scrolldown', 'data': ''}, broadcast=True)\n                break\n            else:\n                if(not no_generate):\n                    for i in range(vars.numseqs):\n                        vars.lua_koboldbridge.outputs[i+1] = \"\"\n                    execute_outmod()\n                    vars.lua_koboldbridge.regeneration_required = False\n                genout = []\n                for i in range(vars.numseqs):\n                    genout.append({\"generated_text\": vars.lua_koboldbridge.outputs[i+1] if not no_generate else \"\"})\n                    assert type(genout[-1][\"generated_text\"]) is str\n                if(len(genout) == 1):\n                    genresult(genout[0][\"generated_text\"])\n                    if(not no_generate and not vars.abort and vars.lua_koboldbridge.restart_sequence is not None):\n                        data = \"\"\n                        force_submit = True\n                        disable_recentrng = True\n                        continue\n                else:\n                    if(not no_generate and not vars.abort and vars.lua_koboldbridge.restart_sequence is not None and vars.lua_koboldbridge.restart_sequence > 0):\n                        genresult(genout[vars.lua_koboldbridge.restart_sequence-1][\"generated_text\"])\n                        data = \"\"\n                        force_submit = True\n                        disable_recentrng = True\n                        continue\n                    genselect(genout)\n                set_aibusy(0)\n                emit('from_server', {'cmd': 'scrolldown', 'data': ''}, broadcast=True)\n                break\n\ndef apiactionsubmit_generate(txt, minimum, maximum):\n    vars.generated_tkns = 0\n\n    if not vars.quiet:\n        logger.debug(f\"Prompt Min:{minimum}, Max:{maximum}\")\n        logger.prompt(utils.decodenewlines(tokenizer.decode(txt)).encode(\"unicode_escape\").decode(\"utf-8\"))\n\n    # Clear CUDA cache if using GPU\n    if(vars.hascuda and (vars.usegpu or vars.breakmodel)):\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # Submit input text to generator\n    _genout, already_generated = tpool.execute(_generate, txt, minimum, maximum, set())\n\n    genout = [applyoutputformatting(utils.decodenewlines(tokenizer.decode(tokens[-already_generated:]))) for tokens in _genout]\n\n    # Clear CUDA cache again if using GPU\n    if(vars.hascuda and (vars.usegpu or vars.breakmodel)):\n        del _genout\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    return genout\n\ndef apiactionsubmit_tpumtjgenerate(txt, minimum, maximum):\n    vars.generated_tkns = 0\n\n    if(vars.full_determinism):\n        tpu_mtj_backend.set_rng_seed(vars.seed)\n\n    if not vars.quiet:\n        logger.debug(f\"Prompt Min:{minimum}, Max:{maximum}\")\n        logger.prompt(utils.decodenewlines(tokenizer.decode(txt)).encode(\"unicode_escape\").decode(\"utf-8\"))\n\n    vars._actions = vars.actions\n    vars._prompt = vars.prompt\n    if(vars.dynamicscan):\n        vars._actions = vars._actions.copy()\n\n    # Submit input text to generator\n    soft_tokens = tpumtjgetsofttokens()\n    genout = tpool.execute(\n        tpu_mtj_backend.infer_static,\n        np.uint32(txt),\n        gen_len = maximum-minimum+1,\n        temp=vars.temp,\n        top_p=vars.top_p,\n        top_k=vars.top_k,\n        tfs=vars.tfs,\n        typical=vars.typical,\n        top_a=vars.top_a,\n        numseqs=vars.numseqs,\n        repetition_penalty=vars.rep_pen,\n        rpslope=vars.rep_pen_slope,\n        rprange=vars.rep_pen_range,\n        soft_embeddings=vars.sp,\n        soft_tokens=soft_tokens,\n        sampler_order=vars.sampler_order,\n    )\n    genout = [applyoutputformatting(utils.decodenewlines(tokenizer.decode(txt))) for txt in genout]\n\n    return genout\n\ndef apiactionsubmit(data, use_memory=False, use_world_info=False, use_story=False, use_authors_note=False):\n    if(vars.model == \"Colab\"):\n        raise NotImplementedError(\"API generation is not supported in old Colab API mode.\")\n    elif(vars.model == \"API\"):\n        raise NotImplementedError(\"API generation is not supported in API mode.\")\n    elif(vars.model == \"CLUSTER\"):\n        raise NotImplementedError(\"API generation is not supported in API mode.\")\n    elif(vars.model == \"OAI\"):\n        raise NotImplementedError(\"API generation is not supported in OpenAI/GooseAI mode.\")\n    elif(vars.model == \"ReadOnly\"):\n        raise NotImplementedError(\"API generation is not supported in read-only mode; please load a model and then try again.\")\n\n    data = applyinputformatting(data)\n\n    if(vars.memory != \"\" and vars.memory[-1] != \"\\n\"):\n        mem = vars.memory + \"\\n\"\n    else:\n        mem = vars.memory\n    if(use_authors_note and vars.authornote != \"\"):\n        anotetxt  = (\"\\n\" + vars.authornotetemplate + \"\\n\").replace(\"<|>\", vars.authornote)\n    else:\n        anotetxt = \"\"\n    MIN_STORY_TOKENS = 8\n    story_tokens = []\n    mem_tokens = []\n    wi_tokens = []\n    story_budget = lambda: vars.max_length - vars.sp_length - vars.genamt - len(tokenizer._koboldai_header) - len(story_tokens) - len(mem_tokens) - len(wi_tokens)\n    budget = lambda: story_budget() + MIN_STORY_TOKENS\n    if budget() < 0:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": f\"Your Max Tokens setting is too low for your current soft prompt and tokenizer to handle. It needs to be at least {vars.max_length - budget()}.\",\n            \"type\": \"token_overflow\",\n        }}), mimetype=\"application/json\", status=500))\n    if use_memory:\n        mem_tokens = tokenizer.encode(utils.encodenewlines(mem))[-budget():]\n    if use_world_info:\n        world_info, _ = checkworldinfo(data, force_use_txt=True, scan_story=use_story)\n        wi_tokens = tokenizer.encode(utils.encodenewlines(world_info))[-budget():]\n    if use_story:\n        if vars.useprompt:\n            story_tokens = tokenizer.encode(utils.encodenewlines(vars.prompt))[-budget():]\n    story_tokens = tokenizer.encode(utils.encodenewlines(data))[-story_budget():] + story_tokens\n    if use_story:\n        for i, action in enumerate(reversed(vars.actions.values())):\n            if story_budget() <= 0:\n                assert story_budget() == 0\n                break\n            story_tokens = tokenizer.encode(utils.encodenewlines(action))[-story_budget():] + story_tokens\n            if i == vars.andepth - 1:\n                story_tokens = tokenizer.encode(utils.encodenewlines(anotetxt))[-story_budget():] + story_tokens\n        if not vars.useprompt:\n            story_tokens = tokenizer.encode(utils.encodenewlines(vars.prompt))[-budget():] + story_tokens\n    tokens = tokenizer._koboldai_header + mem_tokens + wi_tokens + story_tokens\n    assert story_budget() >= 0\n    minimum = len(tokens) + 1\n    maximum = len(tokens) + vars.genamt\n\n    if(not vars.use_colab_tpu and vars.model not in [\"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\"]):\n        genout = apiactionsubmit_generate(tokens, minimum, maximum)\n    elif(vars.use_colab_tpu or vars.model in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")):\n        genout = apiactionsubmit_tpumtjgenerate(tokens, minimum, maximum)\n\n    return genout\n\n#==================================================================#\n#  \n#==================================================================#\ndef actionretry(data):\n    if(vars.noai):\n        emit('from_server', {'cmd': 'errmsg', 'data': \"Retry function unavailable in Read Only mode.\"})\n        return\n    if(vars.recentrng is not None):\n        if(not vars.aibusy):\n            randomGameRequest(vars.recentrng, memory=vars.recentrngm)\n        return\n    if actionback():\n        actionsubmit(\"\", actionmode=vars.actionmode, force_submit=True)\n        send_debug()\n    elif(not vars.useprompt):\n        emit('from_server', {'cmd': 'errmsg', 'data': \"Please enable \\\"Always Add Prompt\\\" to retry with your prompt.\"})\n\n#==================================================================#\n#  \n#==================================================================#\ndef actionback():\n    if(vars.aibusy):\n        return\n    # Remove last index of actions and refresh game screen\n    if(len(vars.genseqs) == 0 and len(vars.actions) > 0):\n        # We are going to move the selected text to alternative text in the actions_metadata variable so we can redo this action\n        vars.actions_metadata[vars.actions.get_last_key() ]['Alternative Text'] = [{'Text': vars.actions_metadata[vars.actions.get_last_key() ]['Selected Text'],\n                                                                    'Pinned': False,\n                                                                    \"Previous Selection\": True,\n                                                                    \"Edited\": False}] + vars.actions_metadata[vars.actions.get_last_key() ]['Alternative Text']\n        vars.actions_metadata[vars.actions.get_last_key() ]['Selected Text'] = \"\"\n    \n        last_key = vars.actions.get_last_key()\n        vars.actions.pop()\n        vars.recentback = True\n        remove_story_chunk(last_key + 1)\n        #for the redo to not get out of whack, need to reset the max # in the actions sequence\n        vars.actions.set_next_id(last_key)\n        success = True\n    elif(len(vars.genseqs) == 0):\n        emit('from_server', {'cmd': 'errmsg', 'data': \"Cannot delete the prompt.\"})\n        success =  False\n    else:\n        vars.genseqs = []\n        success = True\n    send_debug()\n    return success\n        \ndef actionredo():\n    i = 0\n    #First we need to find the next valid key\n    #We might have deleted text so we don't want to show a redo for that blank chunk\n    \n    restore_id = vars.actions.get_last_key()+1\n    if restore_id in vars.actions_metadata:\n        ok_to_use = False\n        while not ok_to_use:\n            for item in vars.actions_metadata[restore_id]['Alternative Text']:\n                if item['Previous Selection'] and item['Text'] != \"\":\n                    ok_to_use = True\n            if not ok_to_use:\n                restore_id+=1\n                if restore_id not in vars.actions_metadata:\n                    return\n            else:\n                vars.actions.set_next_id(restore_id)\n                \n    \n    if restore_id in vars.actions_metadata:\n        genout = [{\"generated_text\": item['Text']} for item in vars.actions_metadata[restore_id]['Alternative Text'] if (item[\"Previous Selection\"]==True)]\n        if len(genout) > 0:\n            genout = genout + [{\"generated_text\": item['Text']} for item in vars.actions_metadata[restore_id]['Alternative Text'] if (item[\"Pinned\"]==True) and (item[\"Previous Selection\"]==False)]\n            if len(genout) == 1:\n                vars.actions_metadata[restore_id]['Alternative Text'] = [item for item in vars.actions_metadata[restore_id]['Alternative Text'] if (item[\"Previous Selection\"]!=True)]\n                genresult(genout[0]['generated_text'], flash=True, ignore_formatting=True)\n            else:\n                # Store sequences in memory until selection is made\n                vars.genseqs = genout\n                \n                \n                # Send sequences to UI for selection\n                genout = [[item['Text'], \"redo\"] for item in vars.actions_metadata[restore_id]['Alternative Text'] if (item[\"Previous Selection\"]==True)]\n                \n                emit('from_server', {'cmd': 'genseqs', 'data': genout}, broadcast=True)\n    else:\n        emit('from_server', {'cmd': 'popuperror', 'data': \"There's nothing to undo\"}, broadcast=True)\n    send_debug()\n\n#==================================================================#\n#  \n#==================================================================#\ndef buildauthorsnote(authorsnote, template):\n    # Build Author's Note if set\n    if authorsnote == \"\":\n        return \"\"\n    return (\"\\n\" + template + \"\\n\").replace(\"<|>\", authorsnote)\n\ndef calcsubmitbudgetheader(txt, **kwargs):\n    # Scan for WorldInfo matches\n    winfo, found_entries = checkworldinfo(txt, **kwargs)\n\n    # Add a newline to the end of memory\n    if(vars.memory != \"\" and vars.memory[-1] != \"\\n\"):\n        mem = vars.memory + \"\\n\"\n    else:\n        mem = vars.memory\n\n    anotetxt = buildauthorsnote(vars.authornote, vars.authornotetemplate)\n\n    return winfo, mem, anotetxt, found_entries\n\ndef calcsubmitbudget(actionlen, winfo, mem, anotetxt, actions, submission=None, budget_deduction=0):\n    forceanote   = False # In case we don't have enough actions to hit A.N. depth\n    anoteadded   = False # In case our budget runs out before we hit A.N. depth\n    anotetkns    = []  # Placeholder for Author's Note tokens\n    lnanote      = 0   # Placeholder for Author's Note length\n\n    lnsp = vars.sp_length\n\n    if(\"tokenizer\" not in globals()):\n        from transformers import GPT2Tokenizer\n        global tokenizer\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=args.revision, cache_dir=\"cache\")\n\n    lnheader = len(tokenizer._koboldai_header)\n\n    # Calculate token budget\n    prompttkns = tokenizer.encode(utils.encodenewlines(vars.comregex_ai.sub('', vars.prompt)), max_length=int(2e9), truncation=True)\n    lnprompt   = len(prompttkns)\n\n    memtokens = tokenizer.encode(utils.encodenewlines(mem), max_length=int(2e9), truncation=True)\n    lnmem     = len(memtokens)\n    if(lnmem > vars.max_length - lnheader - lnsp - vars.genamt - budget_deduction):\n        raise OverflowError(\"The memory in your story is too long. Please either write a shorter memory text or increase the Max Tokens setting. If you are using a soft prompt, additionally consider using a smaller soft prompt.\")\n\n    witokens  = tokenizer.encode(utils.encodenewlines(winfo), max_length=int(2e9), truncation=True)\n    lnwi      = len(witokens)\n    if(lnmem + lnwi > vars.max_length - lnheader - lnsp - vars.genamt - budget_deduction):\n        raise OverflowError(\"The current active world info keys take up too many tokens. Please either write shorter world info, decrease World Info Depth or increase the Max Tokens setting. If you are using a soft prompt, additionally consider using a smaller soft prompt.\")\n\n    if(anotetxt != \"\"):\n        anotetkns = tokenizer.encode(utils.encodenewlines(anotetxt), max_length=int(2e9), truncation=True)\n        lnanote   = len(anotetkns)\n        if(lnmem + lnwi + lnanote > vars.max_length - lnheader - lnsp - vars.genamt - budget_deduction):\n            raise OverflowError(\"The author's note in your story is too long. Please either write a shorter author's note or increase the Max Tokens setting. If you are using a soft prompt, additionally consider using a smaller soft prompt.\")\n\n    if(vars.useprompt):\n        budget = vars.max_length - lnheader - lnsp - lnprompt - lnmem - lnanote - lnwi - vars.genamt - budget_deduction\n    else:\n        budget = vars.max_length - lnheader - lnsp - lnmem - lnanote - lnwi - vars.genamt - budget_deduction\n\n    lnsubmission = len(tokenizer.encode(utils.encodenewlines(vars.comregex_ai.sub('', submission)), max_length=int(2e9), truncation=True)) if submission is not None else 0\n    maybe_lnprompt = lnprompt if vars.useprompt and actionlen > 0 else 0\n\n    if(lnmem + lnwi + lnanote + maybe_lnprompt + lnsubmission > vars.max_length - lnheader - lnsp - vars.genamt - budget_deduction):\n        raise OverflowError(\"Your submission is too long. Please either write a shorter submission or increase the Max Tokens setting. If you are using a soft prompt, additionally consider using a smaller soft prompt. If you are using the Always Add Prompt setting, turning it off may help.\")\n\n    assert budget >= 0\n\n    if(actionlen == 0):\n        # First/Prompt action\n        tokens = (tokenizer._koboldai_header if vars.model not in (\"Colab\", \"API\", \"CLUSTER\", \"OAI\") else []) + memtokens + witokens + anotetkns + prompttkns\n        assert len(tokens) <= vars.max_length - lnsp - vars.genamt - budget_deduction\n        ln = len(tokens) + lnsp\n        return tokens, ln+1, ln+vars.genamt\n    else:\n        tokens     = []\n        \n        # Check if we have the action depth to hit our A.N. depth\n        if(anotetxt != \"\" and actionlen < vars.andepth):\n            forceanote = True\n        \n        # Get most recent action tokens up to our budget\n        n = 0\n        for key in reversed(actions):\n            chunk = vars.comregex_ai.sub('', actions[key])\n            \n            assert budget >= 0\n            if(budget <= 0):\n                break\n            acttkns = tokenizer.encode(utils.encodenewlines(chunk), max_length=int(2e9), truncation=True)\n            tknlen = len(acttkns)\n            if(tknlen < budget):\n                tokens = acttkns + tokens\n                budget -= tknlen\n            else:\n                count = budget * -1\n                tokens = acttkns[count:] + tokens\n                budget = 0\n                break\n            \n            # Inject Author's Note if we've reached the desired depth\n            if(n == vars.andepth-1):\n                if(anotetxt != \"\"):\n                    tokens = anotetkns + tokens # A.N. len already taken from bdgt\n                    anoteadded = True\n            n += 1\n        \n        # If we're not using the prompt every time and there's still budget left,\n        # add some prompt.\n        if(not vars.useprompt):\n            if(budget > 0):\n                prompttkns = prompttkns[-budget:]\n            else:\n                prompttkns = []\n\n        # Did we get to add the A.N.? If not, do it here\n        if(anotetxt != \"\"):\n            if((not anoteadded) or forceanote):\n                tokens = (tokenizer._koboldai_header if vars.model not in (\"Colab\", \"API\", \"CLUSTER\", \"OAI\") else []) + memtokens + witokens + anotetkns + prompttkns + tokens\n            else:\n                tokens = (tokenizer._koboldai_header if vars.model not in (\"Colab\", \"API\", \"CLUSTER\", \"OAI\") else []) + memtokens + witokens + prompttkns + tokens\n        else:\n            # Prepend Memory, WI, and Prompt before action tokens\n            tokens = (tokenizer._koboldai_header if vars.model not in (\"Colab\", \"API\", \"CLUSTER\", \"OAI\") else []) + memtokens + witokens + prompttkns + tokens\n\n        # Send completed bundle to generator\n        assert len(tokens) <= vars.max_length - lnsp - vars.genamt - budget_deduction\n        ln = len(tokens) + lnsp\n        return tokens, ln+1, ln+vars.genamt\n\n#==================================================================#\n# Take submitted text and build the text to be given to generator\n#==================================================================#\ndef calcsubmit(txt):\n    anotetxt     = \"\"    # Placeholder for Author's Note text\n    forceanote   = False # In case we don't have enough actions to hit A.N. depth\n    anoteadded   = False # In case our budget runs out before we hit A.N. depth\n    actionlen    = len(vars.actions)\n\n    winfo, mem, anotetxt, found_entries = calcsubmitbudgetheader(txt)\n \n    # For all transformers models\n    if(vars.model != \"InferKit\"):\n        subtxt, min, max = calcsubmitbudget(actionlen, winfo, mem, anotetxt, vars.actions, submission=txt)\n        if(actionlen == 0):\n            if(not vars.use_colab_tpu and vars.model not in [\"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\"]):\n                generate(subtxt, min, max, found_entries=found_entries)\n            elif(vars.model == \"Colab\"):\n                sendtocolab(utils.decodenewlines(tokenizer.decode(subtxt)), min, max)\n            elif(vars.model == \"API\"):\n                sendtoapi(utils.decodenewlines(tokenizer.decode(subtxt)), min, max)\n            elif(vars.model == \"CLUSTER\"):\n                sendtocluster(utils.decodenewlines(tokenizer.decode(subtxt)), min, max)\n            elif(vars.model == \"OAI\"):\n                oairequest(utils.decodenewlines(tokenizer.decode(subtxt)), min, max)\n            elif(vars.use_colab_tpu or vars.model in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")):\n                tpumtjgenerate(subtxt, min, max, found_entries=found_entries)\n        else:\n            if(not vars.use_colab_tpu and vars.model not in [\"Colab\", \"API\", \"CLUSTER\", \"OAI\", \"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\"]):\n                generate(subtxt, min, max, found_entries=found_entries)\n            elif(vars.model == \"Colab\"):\n                sendtocolab(utils.decodenewlines(tokenizer.decode(subtxt)), min, max)\n            elif(vars.model == \"API\"):\n                sendtoapi(utils.decodenewlines(tokenizer.decode(subtxt)), min, max)\n            elif(vars.model == \"CLUSTER\"):\n                sendtocluster(utils.decodenewlines(tokenizer.decode(subtxt)), min, max)\n            elif(vars.model == \"OAI\"):\n                oairequest(utils.decodenewlines(tokenizer.decode(subtxt)), min, max)\n            elif(vars.use_colab_tpu or vars.model in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")):\n                tpumtjgenerate(subtxt, min, max, found_entries=found_entries)\n                    \n    # For InferKit web API\n    else:\n        # Check if we have the action depth to hit our A.N. depth\n        if(anotetxt != \"\" and actionlen < vars.andepth):\n            forceanote = True\n        \n        if(vars.useprompt):\n            budget = vars.ikmax - len(vars.comregex_ai.sub('', vars.prompt)) - len(anotetxt) - len(mem) - len(winfo) - 1\n        else:\n            budget = vars.ikmax - len(anotetxt) - len(mem) - len(winfo) - 1\n            \n        subtxt = \"\"\n        prompt = vars.comregex_ai.sub('', vars.prompt)\n        n = 0\n        for key in reversed(vars.actions):\n            chunk = vars.actions[key]\n            \n            if(budget <= 0):\n                    break\n            actlen = len(chunk)\n            if(actlen < budget):\n                subtxt = chunk + subtxt\n                budget -= actlen\n            else:\n                count = budget * -1\n                subtxt = chunk[count:] + subtxt\n                budget = 0\n                break\n            \n            # If we're not using the prompt every time and there's still budget left,\n            # add some prompt.\n            if(not vars.useprompt):\n                if(budget > 0):\n                    prompt = vars.comregex_ai.sub('', vars.prompt)[-budget:]\n                else:\n                    prompt = \"\"\n            \n            # Inject Author's Note if we've reached the desired depth\n            if(n == vars.andepth-1):\n                if(anotetxt != \"\"):\n                    subtxt = anotetxt + subtxt # A.N. len already taken from bdgt\n                    anoteadded = True\n            n += 1\n        \n        # Did we get to add the A.N.? If not, do it here\n        if(anotetxt != \"\"):\n            if((not anoteadded) or forceanote):\n                subtxt = mem + winfo + anotetxt + prompt + subtxt\n            else:\n                subtxt = mem + winfo + prompt + subtxt\n        else:\n            subtxt = mem + winfo + prompt + subtxt\n        \n        # Send it!\n        ikrequest(subtxt)\n\n#==================================================================#\n# Send text to generator and deal with output\n#==================================================================#\n\ndef _generate(txt, minimum, maximum, found_entries):\n    if(vars.full_determinism):\n        torch.manual_seed(vars.seed)\n\n    gen_in = torch.tensor(txt, dtype=torch.long)[None]\n    if(vars.sp is not None):\n        soft_tokens = torch.arange(\n            model.config.vocab_size,\n            model.config.vocab_size + vars.sp.shape[0],\n        )\n        gen_in = torch.cat((soft_tokens[None], gen_in), dim=-1)\n    assert gen_in.shape[-1] + vars.genamt <= vars.max_length\n\n    if(vars.hascuda and vars.usegpu):\n        gen_in = gen_in.to(vars.gpu_device)\n    elif(vars.hascuda and vars.breakmodel):\n        gen_in = gen_in.to(breakmodel.primary_device)\n    else:\n        gen_in = gen_in.to('cpu')\n\n    model.kai_scanner_excluded_world_info = found_entries\n\n    vars._actions = vars.actions\n    vars._prompt = vars.prompt\n    if(vars.dynamicscan):\n        vars._actions = vars._actions.copy()\n\n    with torch.no_grad():\n        already_generated = 0\n        numseqs = vars.numseqs\n        while True:\n            genout = generator(\n                gen_in, \n                do_sample=True, \n                max_length=int(2e9),\n                repetition_penalty=1.0,\n                bad_words_ids=vars.badwordsids,\n                use_cache=True,\n                num_return_sequences=numseqs\n                )\n            already_generated += len(genout[0]) - len(gen_in[0])\n            assert already_generated <= vars.genamt\n            if(model.kai_scanner.halt or not model.kai_scanner.regeneration_required):\n                break\n            assert genout.ndim >= 2\n            assert genout.shape[0] == vars.numseqs\n            if(vars.lua_koboldbridge.generated_cols and vars.generated_tkns != vars.lua_koboldbridge.generated_cols):\n                raise RuntimeError(\"Inconsistency detected between KoboldAI Python and Lua backends\")\n            if(already_generated != vars.generated_tkns):\n                raise RuntimeError(\"WI scanning error\")\n            for r in range(vars.numseqs):\n                for c in range(already_generated):\n                    assert vars.lua_koboldbridge.generated[r+1][c+1] is not None\n                    genout[r][genout.shape[-1] - already_generated + c] = vars.lua_koboldbridge.generated[r+1][c+1]\n            encoded = []\n            for i in range(vars.numseqs):\n                txt = utils.decodenewlines(tokenizer.decode(genout[i, -already_generated:]))\n                winfo, mem, anotetxt, _found_entries = calcsubmitbudgetheader(txt, force_use_txt=True, actions=vars._actions)\n                found_entries[i].update(_found_entries)\n                txt, _, _ = calcsubmitbudget(len(vars._actions), winfo, mem, anotetxt, vars._actions, submission=txt)\n                encoded.append(torch.tensor(txt, dtype=torch.long, device=genout.device))\n            max_length = len(max(encoded, key=len))\n            encoded = torch.stack(tuple(torch.nn.functional.pad(e, (max_length - len(e), 0), value=model.config.pad_token_id or model.config.eos_token_id) for e in encoded))\n            genout = torch.cat(\n                (\n                    encoded,\n                    genout[..., -already_generated:],\n                ),\n                dim=-1\n            )\n            if(vars.sp is not None):\n                soft_tokens = torch.arange(\n                    model.config.vocab_size,\n                    model.config.vocab_size + vars.sp.shape[0],\n                    device=genout.device,\n                )\n                genout = torch.cat((soft_tokens.tile(vars.numseqs, 1), genout), dim=-1)\n            assert genout.shape[-1] + vars.genamt - already_generated <= vars.max_length\n            diff = genout.shape[-1] - gen_in.shape[-1]\n            minimum += diff\n            maximum += diff\n            gen_in = genout\n            numseqs = 1\n    \n    return genout, already_generated\n    \n\ndef generate(txt, minimum, maximum, found_entries=None):    \n    vars.generated_tkns = 0\n\n    if(found_entries is None):\n        found_entries = set()\n    found_entries = tuple(found_entries.copy() for _ in range(vars.numseqs))\n\n    if not vars.quiet:\n        logger.debug(f\"Prompt Min:{minimum}, Max:{maximum}\")\n        logger.prompt(utils.decodenewlines(tokenizer.decode(txt)).encode(\"unicode_escape\").decode(\"utf-8\"))\n\n    # Store context in memory to use it for comparison with generated content\n    vars.lastctx = utils.decodenewlines(tokenizer.decode(txt))\n\n    # Clear CUDA cache if using GPU\n    if(vars.hascuda and (vars.usegpu or vars.breakmodel)):\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # Submit input text to generator\n    try:\n        genout, already_generated = tpool.execute(_generate, txt, minimum, maximum, found_entries)\n    except Exception as e:\n        if(issubclass(type(e), lupa.LuaError)):\n            vars.lua_koboldbridge.obliterate_multiverse()\n            vars.lua_running = False\n            emit('from_server', {'cmd': 'errmsg', 'data': 'Lua script error; please check console.'}, broadcast=True)\n            sendUSStatItems()\n            logger.error('LUA ERROR: ' + str(e).replace(\"\\033\", \"\"))\n            logger.warning(\"Lua engine stopped; please open 'Userscripts' and press Load to reinitialize scripts.\")\n        else:\n            emit('from_server', {'cmd': 'errmsg', 'data': 'Error occurred during generator call; please check console.'}, broadcast=True)\n            logger.error(traceback.format_exc().replace(\"\\033\", \"\"))\n        set_aibusy(0)\n        return\n\n    for i in range(vars.numseqs):\n        vars.lua_koboldbridge.generated[i+1][vars.generated_tkns] = int(genout[i, -1].item())\n        vars.lua_koboldbridge.outputs[i+1] = utils.decodenewlines(tokenizer.decode(genout[i, -already_generated:]))\n\n    execute_outmod()\n    if(vars.lua_koboldbridge.regeneration_required):\n        vars.lua_koboldbridge.regeneration_required = False\n        genout = []\n        for i in range(vars.numseqs):\n            genout.append({\"generated_text\": vars.lua_koboldbridge.outputs[i+1]})\n            assert type(genout[-1][\"generated_text\"]) is str\n    else:\n        genout = [{\"generated_text\": utils.decodenewlines(tokenizer.decode(tokens[-already_generated:]))} for tokens in genout]\n    \n    if(len(genout) == 1):\n        genresult(genout[0][\"generated_text\"])\n    else:\n        if(vars.lua_koboldbridge.restart_sequence is not None and vars.lua_koboldbridge.restart_sequence > 0):\n            genresult(genout[vars.lua_koboldbridge.restart_sequence-1][\"generated_text\"])\n        else:\n            genselect(genout)\n    \n    # Clear CUDA cache again if using GPU\n    if(vars.hascuda and (vars.usegpu or vars.breakmodel)):\n        del genout\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    set_aibusy(0)\n\n#==================================================================#\n#  Deal with a single return sequence from generate()\n#==================================================================#\ndef genresult(genout, flash=True, ignore_formatting=False):\n    if not vars.quiet:\n        logger.generation(genout.encode(\"unicode_escape\").decode(\"utf-8\"))\n    \n    # Format output before continuing\n    if not ignore_formatting:\n        genout = applyoutputformatting(genout)\n\n    vars.lua_koboldbridge.feedback = genout\n\n    if(len(genout) == 0):\n        return\n    \n    # Add formatted text to Actions array and refresh the game screen\n    if(len(vars.prompt.strip()) == 0):\n        vars.prompt = genout\n    else:\n        vars.actions.append(genout)\n        if vars.actions.get_last_key() not in vars.actions_metadata:\n            vars.actions_metadata[vars.actions.get_last_key()] = {'Selected Text': genout, 'Alternative Text': []}\n        else:\n            vars.actions_metadata[vars.actions.get_last_key()]['Selected Text'] = genout\n    update_story_chunk('last')\n    if(flash):\n        emit('from_server', {'cmd': 'texteffect', 'data': vars.actions.get_last_key() + 1 if len(vars.actions) else 0}, broadcast=True)\n    send_debug()\n\n#==================================================================#\n#  Send generator sequences to the UI for selection\n#==================================================================#\ndef genselect(genout):\n    i = 0\n    for result in genout:\n        # Apply output formatting rules to sequences\n        result[\"generated_text\"] = applyoutputformatting(result[\"generated_text\"])\n        if not vars.quiet:\n            logger.info(f\"Generation Result {i}\")\n            logger.generation(result[\"generated_text\"].encode(\"unicode_escape\").decode(\"utf-8\"))\n        i += 1\n    \n    # Add the options to the actions metadata\n    # If we've already generated text for this action but haven't selected one we'll want to kill all non-pinned, non-previous selection, and non-edited options then add the new ones\n    if vars.actions.get_next_id() in vars.actions_metadata:\n        if (vars.actions_metadata[vars.actions.get_next_id()]['Selected Text'] == \"\"):\n            vars.actions_metadata[vars.actions.get_next_id()]['Alternative Text'] = [{\"Text\": item['Text'], \"Pinned\": item['Pinned'], \n                                                                             \"Previous Selection\": item[\"Previous Selection\"], \n                                                                             \"Edited\": item[\"Edited\"]} for item in vars.actions_metadata[vars.actions.get_next_id()]['Alternative Text'] \n                                                                             if item['Pinned'] or item[\"Previous Selection\"] or item[\"Edited\"]] + [{\"Text\": text[\"generated_text\"], \n                                                                                    \"Pinned\": False, \"Previous Selection\": False, \"Edited\": False} for text in genout]\n        else:\n            vars.actions_metadata[vars.actions.get_next_id()] = {'Selected Text': '', 'Alternative Text': [{\"Text\": text[\"generated_text\"], \"Pinned\": False, \"Previous Selection\": False, \"Edited\": False} for text in genout]}\n    else:\n        vars.actions_metadata[vars.actions.get_next_id()] = {'Selected Text': '', 'Alternative Text': [{\"Text\": text[\"generated_text\"], \"Pinned\": False, \"Previous Selection\": False, \"Edited\": False} for text in genout]}\n    \n    genout = [{\"generated_text\": item['Text']} for item in vars.actions_metadata[vars.actions.get_next_id()]['Alternative Text'] if (item[\"Previous Selection\"]==False) and (item[\"Edited\"]==False)]\n\n    # Store sequences in memory until selection is made\n    vars.genseqs = genout\n    \n    genout = [[item['Text'], \"pinned\" if item['Pinned'] else \"normal\"] for item in vars.actions_metadata[vars.actions.get_next_id()]['Alternative Text']  if (item[\"Previous Selection\"]==False) and (item[\"Edited\"]==False)]\n\n    # Send sequences to UI for selection\n    emit('from_server', {'cmd': 'genseqs', 'data': genout}, broadcast=True)\n    send_debug()\n\n#==================================================================#\n#  Send selected sequence to action log and refresh UI\n#==================================================================#\ndef selectsequence(n):\n    if(len(vars.genseqs) == 0):\n        return\n    vars.lua_koboldbridge.feedback = vars.genseqs[int(n)][\"generated_text\"]\n    if(len(vars.lua_koboldbridge.feedback) != 0):\n        vars.actions.append(vars.lua_koboldbridge.feedback)\n        #We'll want to remove the option from the alternative text and put it in selected text\n        vars.actions_metadata[vars.actions.get_last_key() ]['Alternative Text'] = [item for item in vars.actions_metadata[vars.actions.get_last_key()]['Alternative Text'] if item['Text'] != vars.lua_koboldbridge.feedback]\n        vars.actions_metadata[vars.actions.get_last_key() ]['Selected Text'] = vars.lua_koboldbridge.feedback\n        update_story_chunk('last')\n        emit('from_server', {'cmd': 'texteffect', 'data': vars.actions.get_last_key() + 1 if len(vars.actions) else 0}, broadcast=True)\n    emit('from_server', {'cmd': 'hidegenseqs', 'data': ''}, broadcast=True)\n    vars.genseqs = []\n\n    if(vars.lua_koboldbridge.restart_sequence is not None):\n        actionsubmit(\"\", actionmode=vars.actionmode, force_submit=True, disable_recentrng=True)\n    send_debug()\n\n#==================================================================#\n#  Pin/Unpin the selected sequence\n#==================================================================#\ndef pinsequence(n):\n    if n.isnumeric():\n        text = vars.genseqs[int(n)]['generated_text']\n        if text in [item['Text'] for item in vars.actions_metadata[vars.actions.get_next_id()]['Alternative Text']]:\n            alternatives = vars.actions_metadata[vars.actions.get_next_id()]['Alternative Text']\n            for i in range(len(alternatives)):\n                if alternatives[i]['Text'] == text:\n                    alternatives[i]['Pinned'] = not alternatives[i]['Pinned']\n                    break\n            vars.actions_metadata[vars.actions.get_next_id()]['Alternative Text'] = alternatives\n    send_debug()\n\n\n#==================================================================#\n#  Send transformers-style request to ngrok/colab host\n#==================================================================#\ndef sendtocolab(txt, min, max):\n    # Log request to console\n    if not vars.quiet:\n        print(\"{0}Tokens:{1}, Txt:{2}{3}\".format(colors.YELLOW, min-1, txt, colors.END))\n    \n    # Store context in memory to use it for comparison with generated content\n    vars.lastctx = txt\n    \n    # Build request JSON data\n    reqdata = {\n        'text': txt,\n        'min': min,\n        'max': max,\n        'rep_pen': vars.rep_pen,\n        'rep_pen_slope': vars.rep_pen_slope,\n        'rep_pen_range': vars.rep_pen_range,\n        'temperature': vars.temp,\n        'top_p': vars.top_p,\n        'top_k': vars.top_k,\n        'tfs': vars.tfs,\n        'typical': vars.typical,\n        'topa': vars.top_a,\n        'numseqs': vars.numseqs,\n        'retfultxt': False\n    }\n    \n    # Create request\n    req = requests.post(\n        vars.colaburl, \n        json = reqdata\n        )\n    \n    # Deal with the response\n    if(req.status_code == 200):\n        js = req.json()[\"data\"]\n        \n        # Try to be backwards compatible with outdated colab\n        if(\"text\" in js):\n            genout = [getnewcontent(js[\"text\"])]\n        else:\n            genout = js[\"seqs\"]\n        \n        for i in range(vars.numseqs):\n            vars.lua_koboldbridge.outputs[i+1] = genout[i]\n\n        execute_outmod()\n        if(vars.lua_koboldbridge.regeneration_required):\n            vars.lua_koboldbridge.regeneration_required = False\n            genout = []\n            for i in range(vars.numseqs):\n                genout.append(vars.lua_koboldbridge.outputs[i+1])\n                assert type(genout[-1]) is str\n\n        if(len(genout) == 1):\n            genresult(genout[0])\n        else:\n            # Convert torch output format to transformers\n            seqs = []\n            for seq in genout:\n                seqs.append({\"generated_text\": seq})\n            if(vars.lua_koboldbridge.restart_sequence is not None and vars.lua_koboldbridge.restart_sequence > 0):\n                genresult(genout[vars.lua_koboldbridge.restart_sequence-1][\"generated_text\"])\n            else:\n                genselect(genout)\n        \n        # Format output before continuing\n        #genout = applyoutputformatting(getnewcontent(genout))\n        \n        # Add formatted text to Actions array and refresh the game screen\n        #vars.actions.append(genout)\n        #refresh_story()\n        #emit('from_server', {'cmd': 'texteffect', 'data': vars.actions.get_last_key() + 1 if len(vars.actions) else 0})\n        \n        set_aibusy(0)\n    else:\n        errmsg = \"Colab API Error: Failed to get a reply from the server. Please check the colab console.\"\n        print(\"{0}{1}{2}\".format(colors.RED, errmsg, colors.END))\n        emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n        set_aibusy(0)\n\n\n#==================================================================#\n#  Send transformers-style request to KoboldAI API\n#==================================================================#\ndef sendtoapi(txt, min, max):\n    # Log request to console\n    if not vars.quiet:\n        print(\"{0}Tokens:{1}, Txt:{2}{3}\".format(colors.YELLOW, min-1, txt, colors.END))\n    \n    # Store context in memory to use it for comparison with generated content\n    vars.lastctx = txt\n    \n    # Build request JSON data\n    reqdata = {\n        'prompt': txt,\n        'max_length': max - min + 1,\n        'max_context_length': vars.max_length,\n        'rep_pen': vars.rep_pen,\n        'rep_pen_slope': vars.rep_pen_slope,\n        'rep_pen_range': vars.rep_pen_range,\n        'temperature': vars.temp,\n        'top_p': vars.top_p,\n        'top_k': vars.top_k,\n        'top_a': vars.top_a,\n        'tfs': vars.tfs,\n        'typical': vars.typical,\n        'n': vars.numseqs,\n    }\n    \n    # Create request\n    while True:\n        req = requests.post(\n            vars.colaburl[:-8] + \"/api/v1/generate\",\n            json=reqdata,\n        )\n        if(req.status_code == 503):  # Server is currently generating something else so poll until it's our turn\n            time.sleep(1)\n            continue\n        js = req.json()\n        if(req.status_code != 200):\n            errmsg = \"KoboldAI API Error: Failed to get a reply from the server. Please check the console.\"\n            print(\"{0}{1}{2}\".format(colors.RED, json.dumps(js, indent=2), colors.END))\n            emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n            set_aibusy(0)\n            return\n\n        genout = [obj[\"text\"] for obj in js[\"results\"]]\n\n        for i in range(vars.numseqs):\n            vars.lua_koboldbridge.outputs[i+1] = genout[i]\n\n        execute_outmod()\n        if(vars.lua_koboldbridge.regeneration_required):\n            vars.lua_koboldbridge.regeneration_required = False\n            genout = []\n            for i in range(vars.numseqs):\n                genout.append(vars.lua_koboldbridge.outputs[i+1])\n                assert type(genout[-1]) is str\n\n        if(len(genout) == 1):\n            genresult(genout[0])\n        else:\n            adjusted_genout = []\n            for item in genout:\n                adjusted_genout.append({\"generated_text\": item})\n            # Convert torch output format to transformers\n            seqs = []\n            for seq in adjusted_genout:\n                seqs.append({\"generated_text\": seq})\n            if(vars.lua_koboldbridge.restart_sequence is not None and vars.lua_koboldbridge.restart_sequence > 0):\n                genresult(adjusted_genout[vars.lua_koboldbridge.restart_sequence-1][\"generated_text\"])\n            else:\n                genselect(adjusted_genout)\n\n        set_aibusy(0)\n        return\n\n#==================================================================#\n#  Send transformers-style request to KoboldAI Cluster\n#==================================================================#\ndef sendtocluster(txt, min, max):\n    # Log request to console\n    if not vars.quiet:\n        logger.debug(f\"Tokens Min:{min-1}\")\n        logger.prompt(txt.encode(\"unicode_escape\").decode(\"utf-8\"))\n\n    # Store context in memory to use it for comparison with generated content\n    vars.lastctx = txt\n    # Build request JSON data\n    reqdata = {\n        'max_length': max - min + 1,\n        'max_context_length': vars.max_length,\n        'rep_pen': vars.rep_pen,\n        'rep_pen_slope': vars.rep_pen_slope,\n        'rep_pen_range': vars.rep_pen_range,\n        'temperature': vars.temp,\n        'top_p': vars.top_p,\n        'top_k': vars.top_k,\n        'top_a': vars.top_a,\n        'tfs': vars.tfs,\n        'typical': vars.typical,\n        'n': vars.numseqs,\n    }\n    cluster_metadata = {\n        'prompt': txt,\n        'params': reqdata,\n        'models': vars.cluster_requested_models,\n        'trusted_workers': False,\n    }    \n    client_agent = \"KoboldAI:1.19.3:koboldai.org\"\n    cluster_headers = {\n        'apikey': vars.apikey,\n        \"Client-Agent\": client_agent\n    }    \n    logger.debug(f\"Horde Payload: {cluster_metadata}\")\n    try:\n        # Create request\n        req = requests.post(\n            vars.colaburl[:-8] + \"/api/v2/generate/text/async\",\n            json=cluster_metadata,\n            headers=cluster_headers,\n        )\n    except requests.exceptions.ConnectionError:\n        errmsg = f\"Horde unavailable. Please try again later\"\n        logger.error(errmsg)\n        emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n        set_aibusy(0)\n        return\n    if(req.status_code == 503):\n        errmsg = f\"KoboldAI API Error: No available KoboldAI servers found in Horde to fulfil this request using the selected models or other properties.\"\n        logger.error(req.text)\n        emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n        set_aibusy(0)\n        return\n    if(not req.ok):\n        errmsg = f\"KoboldAI API Error: Failed to get a standard reply from the Horde. Please check the console.\"\n        logger.error(req.text)\n        emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n        set_aibusy(0)\n        return\n    try:\n        js = req.json()\n    except requests.exceptions.JSONDecodeError:\n        errmsg = f\"Unexpected message received from the Horde: '{req.text}'\"\n        logger.error(errmsg)\n        emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n        set_aibusy(0)\n        return\n\n    request_id = js[\"id\"]\n    logger.debug(\"Horde Request ID: {}\".format(request_id))\n    \n    cluster_agent_headers = {\n        \"Client-Agent\": client_agent\n    }            \n    finished = False\n\n    while not finished:\n        try: \n            req = requests.get(vars.colaburl[:-8] + \"/api/v2/generate/text/status/\" + request_id, headers=cluster_agent_headers)\n        except requests.exceptions.ConnectionError:\n            errmsg = f\"Horde unavailable. Please try again later\"\n            logger.error(errmsg)\n            emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n            set_aibusy(0)\n            return\n\n        if not req.ok:\n            errmsg = f\"KoboldAI API Error: Failed to get a standard reply from the Horde. Please check the console.\"\n            logger.error(req.text)\n            emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n            set_aibusy(0)\n            return\n\n        try:\n            req_status = req.json()\n        except requests.exceptions.JSONDecodeError:\n            errmsg = f\"Unexpected message received from the KoboldAI Horde: '{req.text}'\"\n            logger.error(errmsg)\n            emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n            set_aibusy(0)\n            return\n\n        if \"done\" not in req_status:\n            errmsg = f\"Unexpected response received from the KoboldAI Horde: '{js}'\"\n            logger.error(errmsg)\n            emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n            set_aibusy(0)\n            return\n\n        finished = req_status[\"done\"]\n\n        if not finished:\n            logger.debug(req_status)\n            time.sleep(1)\n    \n    logger.debug(\"Last Horde Status Message: {}\".format(js))\n    if req_status[\"faulted\"]:\n        errmsg = \"Horde Text generation faulted! Please try again\"\n        logger.error(errmsg)\n        emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n        set_aibusy(0)\n        return\n    \n    generations = req_status['generations']\n    gen_workers = [(cgen['worker_name'],cgen['worker_id']) for cgen in generations]\n    logger.info(f\"Generations by: {gen_workers}\")\n\n\n\n\n\n\n    # Just in case we want to announce it to the user\n    if len(generations) == 1:        \n        warnmsg = f\"Text generated by {[w[0] for w in gen_workers]}\"\n        emit('from_server', {'cmd': 'warnmsg', 'data': warnmsg}, broadcast=True)\n    genout = [cgen['text'] for cgen in generations]\n\n    for i in range(vars.numseqs):\n        vars.lua_koboldbridge.outputs[i+1] = genout[i]\n\n    execute_outmod()\n    if(vars.lua_koboldbridge.regeneration_required):\n        vars.lua_koboldbridge.regeneration_required = False\n        genout = []\n        for i in range(vars.numseqs):\n            genout.append(vars.lua_koboldbridge.outputs[i+1])\n            assert type(genout[-1]) is str\n\n    if(len(genout) == 1):\n        genresult(genout[0])\n    else:\n        adjusted_genout = []\n        for item in genout:\n            adjusted_genout.append({\"generated_text\": item})\n        # Convert torch output format to transformers\n        seqs = []\n        for seq in adjusted_genout:\n            seqs.append({\"generated_text\": seq})\n        if(vars.lua_koboldbridge.restart_sequence is not None and vars.lua_koboldbridge.restart_sequence > 0):\n            genresult(adjusted_genout[vars.lua_koboldbridge.restart_sequence-1][\"generated_text\"])\n        else:\n            genselect(adjusted_genout)\n\n    set_aibusy(0)\n    return\n\n#==================================================================#\n#  Send text to TPU mesh transformer backend\n#==================================================================#\ndef tpumtjgenerate(txt, minimum, maximum, found_entries=None):\n    if(vars.full_determinism):\n        tpu_mtj_backend.set_rng_seed(vars.seed)\n\n    vars.generated_tkns = 0\n\n    if(found_entries is None):\n        found_entries = set()\n    found_entries = tuple(found_entries.copy() for _ in range(vars.numseqs))\n\n    if not vars.quiet:\n        logger.debug(f\"Prompt Min:{minimum}, Max:{maximum}\")\n        logger.prompt(utils.decodenewlines(tokenizer.decode(txt)).encode(\"unicode_escape\").decode(\"utf-8\"))\n\n    vars._actions = vars.actions\n    vars._prompt = vars.prompt\n    if(vars.dynamicscan):\n        vars._actions = vars._actions.copy()\n\n    # Submit input text to generator\n    try:\n        soft_tokens = tpumtjgetsofttokens()\n\n        global past\n\n        socketio.start_background_task(copy_current_request_context(check_for_backend_compilation))\n\n        if(vars.dynamicscan or (not vars.nogenmod and vars.has_genmod)):\n\n            context = np.tile(np.uint32(txt), (vars.numseqs, 1))\n            past = np.empty((vars.numseqs, 0), dtype=np.uint32)\n\n            while(True):\n                genout, n_generated, regeneration_required, halt = tpool.execute(\n                    tpu_mtj_backend.infer_dynamic,\n                    context,\n                    gen_len = maximum-minimum+1,\n                    numseqs=vars.numseqs,\n                    soft_embeddings=vars.sp,\n                    soft_tokens=soft_tokens,\n                    excluded_world_info=found_entries,\n                )\n\n                past = np.pad(past, ((0, 0), (0, n_generated)))\n                for r in range(vars.numseqs):\n                    for c in range(vars.lua_koboldbridge.generated_cols):\n                        assert vars.lua_koboldbridge.generated[r+1][c+1] is not None\n                        past[r, c] = vars.lua_koboldbridge.generated[r+1][c+1]\n\n                if(vars.abort or halt or not regeneration_required):\n                    break\n                print(\"(regeneration triggered)\")\n\n                encoded = []\n                for i in range(vars.numseqs):\n                    txt = utils.decodenewlines(tokenizer.decode(past[i]))\n                    winfo, mem, anotetxt, _found_entries = calcsubmitbudgetheader(txt, force_use_txt=True, actions=vars._actions)\n                    found_entries[i].update(_found_entries)\n                    txt, _, _ = calcsubmitbudget(len(vars._actions), winfo, mem, anotetxt, vars._actions, submission=txt)\n                    encoded.append(np.array(txt, dtype=np.uint32))\n                max_length = len(max(encoded, key=len))\n                encoded = np.stack(tuple(np.pad(e, (max_length - len(e), 0), constant_values=tpu_mtj_backend.pad_token_id) for e in encoded))\n                context = np.concatenate(\n                    (\n                        encoded,\n                        past,\n                    ),\n                    axis=-1,\n                )\n\n        else:\n            genout = tpool.execute(\n                tpu_mtj_backend.infer_static,\n                np.uint32(txt),\n                gen_len = maximum-minimum+1,\n                temp=vars.temp,\n                top_p=vars.top_p,\n                top_k=vars.top_k,\n                tfs=vars.tfs,\n                typical=vars.typical,\n                top_a=vars.top_a,\n                numseqs=vars.numseqs,\n                repetition_penalty=vars.rep_pen,\n                rpslope=vars.rep_pen_slope,\n                rprange=vars.rep_pen_range,\n                soft_embeddings=vars.sp,\n                soft_tokens=soft_tokens,\n                sampler_order=vars.sampler_order,\n            )\n            past = genout\n            for i in range(vars.numseqs):\n                vars.lua_koboldbridge.generated[i+1] = vars.lua_state.table(*genout[i].tolist())\n            vars.lua_koboldbridge.generated_cols = vars.generated_tkns = genout[0].shape[-1]\n\n    except Exception as e:\n        if(issubclass(type(e), lupa.LuaError)):\n            vars.lua_koboldbridge.obliterate_multiverse()\n            vars.lua_running = False\n            emit('from_server', {'cmd': 'errmsg', 'data': 'Lua script error; please check console.'}, broadcast=True)\n            sendUSStatItems()\n            logger.error('LUA ERROR: ' + str(e).replace(\"\\033\", \"\"))\n            logger.warning(\"Lua engine stopped; please open 'Userscripts' and press Load to reinitialize scripts.\")\n        else:\n            emit('from_server', {'cmd': 'errmsg', 'data': 'Error occurred during generator call; please check console.'}, broadcast=True)\n            print(\"{0}{1}{2}\".format(colors.RED, traceback.format_exc().replace(\"\\033\", \"\"), colors.END), file=sys.stderr)\n        set_aibusy(0)\n        return\n\n    for i in range(vars.numseqs):\n        vars.lua_koboldbridge.outputs[i+1] = utils.decodenewlines(tokenizer.decode(past[i]))\n    genout = past\n\n    execute_outmod()\n    if(vars.lua_koboldbridge.regeneration_required):\n        vars.lua_koboldbridge.regeneration_required = False\n        genout = []\n        for i in range(vars.numseqs):\n            genout.append({\"generated_text\": vars.lua_koboldbridge.outputs[i+1]})\n            assert type(genout[-1][\"generated_text\"]) is str\n    else:\n        genout = [{\"generated_text\": utils.decodenewlines(tokenizer.decode(txt))} for txt in genout]\n\n    if(len(genout) == 1):\n        genresult(genout[0][\"generated_text\"])\n    else:\n        if(vars.lua_koboldbridge.restart_sequence is not None and vars.lua_koboldbridge.restart_sequence > 0):\n            genresult(genout[vars.lua_koboldbridge.restart_sequence-1][\"generated_text\"])\n        else:\n            genselect(genout)\n\n    set_aibusy(0)\n\n\n#==================================================================#\n# Replaces returns and newlines with HTML breaks\n#==================================================================#\ndef formatforhtml(txt):\n    return txt.replace(\"\\\\r\\\\n\", \"<br/>\").replace(\"\\\\r\", \"<br/>\").replace(\"\\\\n\", \"<br/>\").replace(\"\\r\\n\", \"<br/>\").replace('\\n', '<br/>').replace('\\r', '<br/>').replace('&lt;/s&gt;', '<br/>')\n\n#==================================================================#\n# Strips submitted text from the text returned by the AI\n#==================================================================#\ndef getnewcontent(txt):\n    # If the submitted context was blank, then everything is new\n    if(vars.lastctx == \"\"):\n        return txt\n    \n    # Tokenize the last context and the generated content\n    ctxtokens = tokenizer.encode(utils.encodenewlines(vars.lastctx), max_length=int(2e9), truncation=True)\n    txttokens = tokenizer.encode(utils.encodenewlines(txt), max_length=int(2e9), truncation=True)\n    dif       = (len(txttokens) - len(ctxtokens)) * -1\n    \n    # Remove the context from the returned text\n    newtokens = txttokens[dif:]\n    \n    return utils.decodenewlines(tokenizer.decode(newtokens))\n\n#==================================================================#\n# Applies chosen formatting options to text submitted to AI\n#==================================================================#\ndef applyinputformatting(txt):\n    # Add sentence spacing\n    if(vars.formatoptns[\"frmtadsnsp\"]):\n        txt = utils.addsentencespacing(txt, vars)\n \n    return txt\n\n#==================================================================#\n# Applies chosen formatting options to text returned from AI\n#==================================================================#\ndef applyoutputformatting(txt):\n    # Use standard quotes and apostrophes\n    txt = utils.fixquotes(txt)\n\n    # Adventure mode clipping of all characters after '>'\n    if(vars.adventure):\n        txt = vars.acregex_ai.sub('', txt)\n    \n    # Trim incomplete sentences\n    if(vars.formatoptns[\"frmttriminc\"] and not vars.chatmode):\n        txt = utils.trimincompletesentence(txt)\n    # Replace blank lines\n    if(vars.formatoptns[\"frmtrmblln\"] or vars.chatmode):\n        txt = utils.replaceblanklines(txt)\n    # Remove special characters\n    if(vars.formatoptns[\"frmtrmspch\"]):\n        txt = utils.removespecialchars(txt, vars)\n\t# Single Line Mode\n    if(vars.formatoptns[\"singleline\"] or vars.chatmode):\n        txt = utils.singlelineprocessing(txt, vars)\n    \n    return txt\n\n#==================================================================#\n# Sends the current story content to the Game Screen\n#==================================================================#\ndef refresh_story():\n    text_parts = ['<chunk n=\"0\" id=\"n0\" tabindex=\"-1\">', vars.comregex_ui.sub(lambda m: '\\n'.join('<comment>' + l + '</comment>' for l in m.group().split('\\n')), html.escape(vars.prompt)), '</chunk>']\n    for idx in vars.actions:\n        item = vars.actions[idx]\n        idx += 1\n        item = html.escape(item)\n        item = vars.comregex_ui.sub(lambda m: '\\n'.join('<comment>' + l + '</comment>' for l in m.group().split('\\n')), item)  # Add special formatting to comments\n        item = vars.acregex_ui.sub('<action>\\\\1</action>', item)  # Add special formatting to adventure actions\n        text_parts.extend(('<chunk n=\"', str(idx), '\" id=\"n', str(idx), '\" tabindex=\"-1\">', item, '</chunk>'))\n    emit('from_server', {'cmd': 'updatescreen', 'gamestarted': vars.gamestarted, 'data': formatforhtml(''.join(text_parts))}, broadcast=True)\n\n\n#==================================================================#\n# Signals the Game Screen to update one of the chunks\n#==================================================================#\ndef update_story_chunk(idx: Union[int, str]):\n    if idx == 'last':\n        if len(vars.actions) <= 1:\n            # In this case, we are better off just refreshing the whole thing as the\n            # prompt might not have been shown yet (with a \"Generating story...\"\n            # message instead).\n            refresh_story()\n            setgamesaved(False)\n            return\n\n        idx = (vars.actions.get_last_key() if len(vars.actions) else 0) + 1\n\n    if idx == 0:\n        text = vars.prompt\n    else:\n        # Actions are 0 based, but in chunks 0 is the prompt.\n        # So the chunk index is one more than the corresponding action index.\n        if(idx - 1 not in vars.actions):\n            return\n        text = vars.actions[idx - 1]\n\n    item = html.escape(text)\n    item = vars.comregex_ui.sub(lambda m: '\\n'.join('<comment>' + l + '</comment>' for l in m.group().split('\\n')), item)  # Add special formatting to comments\n    item = vars.acregex_ui.sub('<action>\\\\1</action>', item)  # Add special formatting to adventure actions\n\n    chunk_text = f'<chunk n=\"{idx}\" id=\"n{idx}\" tabindex=\"-1\">{formatforhtml(item)}</chunk>'\n    emit('from_server', {'cmd': 'updatechunk', 'data': {'index': idx, 'html': chunk_text}}, broadcast=True)\n\n    setgamesaved(False)\n\n    #If we've set the auto save flag, we'll now save the file\n    if vars.autosave and (\".json\" in vars.savedir):\n        save()\n\n\n#==================================================================#\n# Signals the Game Screen to remove one of the chunks\n#==================================================================#\ndef remove_story_chunk(idx: int):\n    emit('from_server', {'cmd': 'removechunk', 'data': idx}, broadcast=True)\n    setgamesaved(False)\n\n\n#==================================================================#\n# Sends the current generator settings to the Game Menu\n#==================================================================#\ndef refresh_settings():\n    # Suppress toggle change events while loading state\n    emit('from_server', {'cmd': 'allowtoggle', 'data': False}, broadcast=True)\n    \n    if(vars.model != \"InferKit\"):\n        emit('from_server', {'cmd': 'updatetemp', 'data': vars.temp}, broadcast=True)\n        emit('from_server', {'cmd': 'updatetopp', 'data': vars.top_p}, broadcast=True)\n        emit('from_server', {'cmd': 'updatetopk', 'data': vars.top_k}, broadcast=True)\n        emit('from_server', {'cmd': 'updatetfs', 'data': vars.tfs}, broadcast=True)\n        emit('from_server', {'cmd': 'updatetypical', 'data': vars.typical}, broadcast=True)\n        emit('from_server', {'cmd': 'updatetopa', 'data': vars.top_a}, broadcast=True)\n        emit('from_server', {'cmd': 'updatereppen', 'data': vars.rep_pen}, broadcast=True)\n        emit('from_server', {'cmd': 'updatereppenslope', 'data': vars.rep_pen_slope}, broadcast=True)\n        emit('from_server', {'cmd': 'updatereppenrange', 'data': vars.rep_pen_range}, broadcast=True)\n        emit('from_server', {'cmd': 'updateoutlen', 'data': vars.genamt}, broadcast=True)\n        emit('from_server', {'cmd': 'updatetknmax', 'data': vars.max_length}, broadcast=True)\n        emit('from_server', {'cmd': 'updatenumseq', 'data': vars.numseqs}, broadcast=True)\n    else:\n        emit('from_server', {'cmd': 'updatetemp', 'data': vars.temp}, broadcast=True)\n        emit('from_server', {'cmd': 'updatetopp', 'data': vars.top_p}, broadcast=True)\n        emit('from_server', {'cmd': 'updateikgen', 'data': vars.ikgen}, broadcast=True)\n    \n    emit('from_server', {'cmd': 'updateanotedepth', 'data': vars.andepth}, broadcast=True)\n    emit('from_server', {'cmd': 'updatewidepth', 'data': vars.widepth}, broadcast=True)\n    emit('from_server', {'cmd': 'updateuseprompt', 'data': vars.useprompt}, broadcast=True)\n    emit('from_server', {'cmd': 'updateadventure', 'data': vars.adventure}, broadcast=True)\n    emit('from_server', {'cmd': 'updatechatmode', 'data': vars.chatmode}, broadcast=True)\n    emit('from_server', {'cmd': 'updatedynamicscan', 'data': vars.dynamicscan}, broadcast=True)\n    emit('from_server', {'cmd': 'updateautosave', 'data': vars.autosave}, broadcast=True)\n    emit('from_server', {'cmd': 'updatenopromptgen', 'data': vars.nopromptgen}, broadcast=True)\n    emit('from_server', {'cmd': 'updaterngpersist', 'data': vars.rngpersist}, broadcast=True)\n    emit('from_server', {'cmd': 'updatenogenmod', 'data': vars.nogenmod}, broadcast=True)\n    emit('from_server', {'cmd': 'updatefulldeterminism', 'data': vars.full_determinism}, broadcast=True)\n    \n    emit('from_server', {'cmd': 'updatefrmttriminc', 'data': vars.formatoptns[\"frmttriminc\"]}, broadcast=True)\n    emit('from_server', {'cmd': 'updatefrmtrmblln', 'data': vars.formatoptns[\"frmtrmblln\"]}, broadcast=True)\n    emit('from_server', {'cmd': 'updatefrmtrmspch', 'data': vars.formatoptns[\"frmtrmspch\"]}, broadcast=True)\n    emit('from_server', {'cmd': 'updatefrmtadsnsp', 'data': vars.formatoptns[\"frmtadsnsp\"]}, broadcast=True)\n    emit('from_server', {'cmd': 'updatesingleline', 'data': vars.formatoptns[\"singleline\"]}, broadcast=True)\n    emit('from_server', {'cmd': 'updateoutputstreaming', 'data': vars.output_streaming}, broadcast=True)\n    emit('from_server', {'cmd': 'updateshowbudget', 'data': vars.show_budget}, broadcast=True)\n    emit('from_server', {'cmd': 'updateshowprobs', 'data': vars.show_probs}, broadcast=True)\n    \n    # Allow toggle events again\n    emit('from_server', {'cmd': 'allowtoggle', 'data': True}, broadcast=True)\n\n#==================================================================#\n#  Sets the logical and display states for the AI Busy condition\n#==================================================================#\ndef set_aibusy(state):\n    if(vars.disable_set_aibusy):\n        return\n    if(state):\n        vars.aibusy = True\n        emit('from_server', {'cmd': 'setgamestate', 'data': 'wait'}, broadcast=True)\n    else:\n        vars.aibusy = False\n        emit('from_server', {'cmd': 'setgamestate', 'data': 'ready'}, broadcast=True)\n\n#==================================================================#\n# \n#==================================================================#\ndef editrequest(n):\n    if(n == 0):\n        txt = vars.prompt\n    else:\n        txt = vars.actions[n-1]\n    \n    vars.editln = n\n    emit('from_server', {'cmd': 'setinputtext', 'data': txt}, broadcast=True)\n    emit('from_server', {'cmd': 'enablesubmit', 'data': ''}, broadcast=True)\n\n#==================================================================#\n# \n#==================================================================#\ndef editsubmit(data):\n    vars.recentedit = True\n    if(vars.editln == 0):\n        vars.prompt = data\n    else:\n        vars.actions_metadata[vars.editln-1]['Alternative Text'] = vars.actions_metadata[vars.editln-1]['Alternative Text'] + [{\"Text\": vars.actions[vars.editln-1], \"Pinned\": False, \n                                                                         \"Previous Selection\": False, \n                                                                         \"Edited\": True}]\n        vars.actions_metadata[vars.editln-1]['Selected Text'] = data\n        vars.actions[vars.editln-1] = data\n    \n    vars.mode = \"play\"\n    update_story_chunk(vars.editln)\n    emit('from_server', {'cmd': 'texteffect', 'data': vars.editln}, broadcast=True)\n    emit('from_server', {'cmd': 'editmode', 'data': 'false'})\n    send_debug()\n\n#==================================================================#\n#  \n#==================================================================#\ndef deleterequest():\n    vars.recentedit = True\n    # Don't delete prompt\n    if(vars.editln == 0):\n        # Send error message\n        pass\n    else:\n        vars.actions_metadata[vars.editln-1]['Alternative Text'] = [{\"Text\": vars.actions[vars.editln-1], \"Pinned\": False, \n                                                      \"Previous Selection\": True, \"Edited\": False}] + vars.actions_metadata[vars.editln-1]['Alternative Text']\n        vars.actions_metadata[vars.editln-1]['Selected Text'] = ''\n        vars.actions[vars.editln-1] = ''\n        vars.mode = \"play\"\n        remove_story_chunk(vars.editln)\n        emit('from_server', {'cmd': 'editmode', 'data': 'false'})\n    send_debug()\n\n#==================================================================#\n# \n#==================================================================#\ndef inlineedit(chunk, data):\n    vars.recentedit = True\n    chunk = int(chunk)\n    if(chunk == 0):\n        if(len(data.strip()) == 0):\n            return\n        vars.prompt = data\n    else:\n        if(chunk-1 in vars.actions):\n            vars.actions_metadata[chunk-1]['Alternative Text'] = vars.actions_metadata[chunk-1]['Alternative Text'] + [{\"Text\": vars.actions[chunk-1], \"Pinned\": False, \n                                                                             \"Previous Selection\": False, \n                                                                             \"Edited\": True}]\n            vars.actions_metadata[chunk-1]['Selected Text'] = data\n            vars.actions[chunk-1] = data\n        else:\n            logger.warning(f\"Attempted to edit non-existent chunk {chunk}\")\n\n    setgamesaved(False)\n    update_story_chunk(chunk)\n    emit('from_server', {'cmd': 'texteffect', 'data': chunk}, broadcast=True)\n    emit('from_server', {'cmd': 'editmode', 'data': 'false'}, broadcast=True)\n    send_debug()\n\n#==================================================================#\n#  \n#==================================================================#\ndef inlinedelete(chunk):\n    vars.recentedit = True\n    chunk = int(chunk)\n    # Don't delete prompt\n    if(chunk == 0):\n        # Send error message\n        update_story_chunk(chunk)\n        emit('from_server', {'cmd': 'errmsg', 'data': \"Cannot delete the prompt.\"})\n        emit('from_server', {'cmd': 'editmode', 'data': 'false'}, broadcast=True)\n    else:\n        if(chunk-1 in vars.actions):\n            vars.actions_metadata[chunk-1]['Alternative Text'] = [{\"Text\": vars.actions[chunk-1], \"Pinned\": False, \n                                                                             \"Previous Selection\": True, \n                                                                             \"Edited\": False}] + vars.actions_metadata[chunk-1]['Alternative Text']\n            vars.actions_metadata[chunk-1]['Selected Text'] = ''\n            del vars.actions[chunk-1]\n        else:\n            logger.warning(f\"Attempted to delete non-existent chunk {chunk}\")\n        setgamesaved(False)\n        remove_story_chunk(chunk)\n        emit('from_server', {'cmd': 'editmode', 'data': 'false'}, broadcast=True)\n    send_debug()\n\n#==================================================================#\n#   Toggles the game mode for memory editing and sends UI commands\n#==================================================================#\ndef togglememorymode():\n    if(vars.mode == \"play\"):\n        vars.mode = \"memory\"\n        emit('from_server', {'cmd': 'memmode', 'data': 'true'}, broadcast=True)\n        emit('from_server', {'cmd': 'setinputtext', 'data': vars.memory}, broadcast=True)\n        emit('from_server', {'cmd': 'setanote', 'data': vars.authornote}, broadcast=True)\n        emit('from_server', {'cmd': 'setanotetemplate', 'data': vars.authornotetemplate}, broadcast=True)\n    elif(vars.mode == \"memory\"):\n        vars.mode = \"play\"\n        emit('from_server', {'cmd': 'memmode', 'data': 'false'}, broadcast=True)\n\n#==================================================================#\n#   Toggles the game mode for WI editing and sends UI commands\n#==================================================================#\ndef togglewimode():\n    if(vars.mode == \"play\"):\n        vars.mode = \"wi\"\n        emit('from_server', {'cmd': 'wimode', 'data': 'true'}, broadcast=True)\n    elif(vars.mode == \"wi\"):\n        # Commit WI fields first\n        requestwi()\n        # Then set UI state back to Play\n        vars.mode = \"play\"\n        emit('from_server', {'cmd': 'wimode', 'data': 'false'}, broadcast=True)\n    sendwi()\n\n#==================================================================#\n#   \n#==================================================================#\ndef addwiitem(folder_uid=None):\n    assert folder_uid is None or folder_uid in vars.wifolders_d\n    ob = {\"key\": \"\", \"keysecondary\": \"\", \"content\": \"\", \"comment\": \"\", \"folder\": folder_uid, \"num\": len(vars.worldinfo), \"init\": False, \"selective\": False, \"constant\": False}\n    vars.worldinfo.append(ob)\n    while(True):\n        uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n        if(uid not in vars.worldinfo_u):\n            break\n    vars.worldinfo_u[uid] = vars.worldinfo[-1]\n    vars.worldinfo[-1][\"uid\"] = uid\n    if(folder_uid is not None):\n        vars.wifolders_u[folder_uid].append(vars.worldinfo[-1])\n    emit('from_server', {'cmd': 'addwiitem', 'data': ob}, broadcast=True)\n\n#==================================================================#\n#   Creates a new WI folder with an unused cryptographically secure random UID\n#==================================================================#\ndef addwifolder():\n    while(True):\n        uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n        if(uid not in vars.wifolders_d):\n            break\n    ob = {\"name\": \"\", \"collapsed\": False}\n    vars.wifolders_d[uid] = ob\n    vars.wifolders_l.append(uid)\n    vars.wifolders_u[uid] = []\n    emit('from_server', {'cmd': 'addwifolder', 'uid': uid, 'data': ob}, broadcast=True)\n    addwiitem(folder_uid=uid)\n\n#==================================================================#\n#   Move the WI entry with UID src so that it immediately precedes\n#   the WI entry with UID dst\n#==================================================================#\ndef movewiitem(dst, src):\n    setgamesaved(False)\n    if(vars.worldinfo_u[src][\"folder\"] is not None):\n        for i, e in enumerate(vars.wifolders_u[vars.worldinfo_u[src][\"folder\"]]):\n            if(e is vars.worldinfo_u[src]):\n                vars.wifolders_u[vars.worldinfo_u[src][\"folder\"]].pop(i)\n                break\n    if(vars.worldinfo_u[dst][\"folder\"] is not None):\n        vars.wifolders_u[vars.worldinfo_u[dst][\"folder\"]].append(vars.worldinfo_u[src])\n    vars.worldinfo_u[src][\"folder\"] = vars.worldinfo_u[dst][\"folder\"]\n    for i, e in enumerate(vars.worldinfo):\n        if(e is vars.worldinfo_u[src]):\n            _src = i\n        elif(e is vars.worldinfo_u[dst]):\n            _dst = i\n    vars.worldinfo.insert(_dst - (_dst >= _src), vars.worldinfo.pop(_src))\n    sendwi()\n\n#==================================================================#\n#   Move the WI folder with UID src so that it immediately precedes\n#   the WI folder with UID dst\n#==================================================================#\ndef movewifolder(dst, src):\n    setgamesaved(False)\n    vars.wifolders_l.remove(src)\n    if(dst is None):\n        # If dst is None, that means we should move src to be the last folder\n        vars.wifolders_l.append(src)\n    else:\n        vars.wifolders_l.insert(vars.wifolders_l.index(dst), src)\n    sendwi()\n\n#==================================================================#\n#   \n#==================================================================#\ndef sendwi():\n    # Cache len of WI\n    ln = len(vars.worldinfo)\n\n    # Clear contents of WI container\n    emit('from_server', {'cmd': 'wistart', 'wifolders_d': vars.wifolders_d, 'wifolders_l': vars.wifolders_l, 'data': ''}, broadcast=True)\n\n    # Stable-sort WI entries in order of folder\n    stablesortwi()\n\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n\n    # If there are no WI entries, send an empty WI object\n    if(ln == 0):\n        addwiitem()\n    else:\n        # Send contents of WI array\n        last_folder = ...\n        for wi in vars.worldinfo:\n            if(wi[\"folder\"] != last_folder):\n                emit('from_server', {'cmd': 'addwifolder', 'uid': wi[\"folder\"], 'data': vars.wifolders_d[wi[\"folder\"]] if wi[\"folder\"] is not None else None}, broadcast=True)\n                last_folder = wi[\"folder\"]\n            ob = wi\n            emit('from_server', {'cmd': 'addwiitem', 'data': ob}, broadcast=True)\n    \n    emit('from_server', {'cmd': 'wifinish', 'data': ''}, broadcast=True)\n\n#==================================================================#\n#  Request current contents of all WI HTML elements\n#==================================================================#\ndef requestwi():\n    list = []\n    for wi in vars.worldinfo:\n        list.append(wi[\"num\"])\n    emit('from_server', {'cmd': 'requestwiitem', 'data': list})\n\n#==================================================================#\n#  Stable-sort WI items so that items in the same folder are adjacent,\n#  and items in different folders are sorted based on the order of the folders\n#==================================================================#\ndef stablesortwi():\n    mapping = {uid: index for index, uid in enumerate(vars.wifolders_l)}\n    vars.worldinfo.sort(key=lambda x: mapping[x[\"folder\"]] if x[\"folder\"] is not None else float(\"inf\"))\n    last_folder = ...\n    last_wi = None\n    for i, wi in enumerate(vars.worldinfo):\n        wi[\"num\"] = i\n        wi[\"init\"] = True\n        if(wi[\"folder\"] != last_folder):\n            if(last_wi is not None and last_folder is not ...):\n                last_wi[\"init\"] = False\n            last_folder = wi[\"folder\"]\n        last_wi = wi\n    if(last_wi is not None):\n        last_wi[\"init\"] = False\n    for folder in vars.wifolders_u:\n        vars.wifolders_u[folder].sort(key=lambda x: x[\"num\"])\n\n#==================================================================#\n#  Extract object from server and send it to WI objects\n#==================================================================#\ndef commitwi(ar):\n    for ob in ar:\n        ob[\"uid\"] = int(ob[\"uid\"])\n        vars.worldinfo_u[ob[\"uid\"]][\"key\"]          = ob[\"key\"]\n        vars.worldinfo_u[ob[\"uid\"]][\"keysecondary\"] = ob[\"keysecondary\"]\n        vars.worldinfo_u[ob[\"uid\"]][\"content\"]      = ob[\"content\"]\n        vars.worldinfo_u[ob[\"uid\"]][\"comment\"]      = ob.get(\"comment\", \"\")\n        vars.worldinfo_u[ob[\"uid\"]][\"folder\"]       = ob.get(\"folder\", None)\n        vars.worldinfo_u[ob[\"uid\"]][\"selective\"]    = ob[\"selective\"]\n        vars.worldinfo_u[ob[\"uid\"]][\"constant\"]     = ob.get(\"constant\", False)\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n\n#==================================================================#\n#  \n#==================================================================#\ndef deletewi(uid):\n    if(uid in vars.worldinfo_u):\n        setgamesaved(False)\n        # Store UID of deletion request\n        vars.deletewi = uid\n        if(vars.deletewi is not None):\n            if(vars.worldinfo_u[vars.deletewi][\"folder\"] is not None):\n                for i, e in enumerate(vars.wifolders_u[vars.worldinfo_u[vars.deletewi][\"folder\"]]):\n                    if(e is vars.worldinfo_u[vars.deletewi]):\n                        vars.wifolders_u[vars.worldinfo_u[vars.deletewi][\"folder\"]].pop(i)\n            for i, e in enumerate(vars.worldinfo):\n                if(e is vars.worldinfo_u[vars.deletewi]):\n                    del vars.worldinfo[i]\n                    break\n            del vars.worldinfo_u[vars.deletewi]\n            # Send the new WI array structure\n            sendwi()\n            # And reset deletewi\n            vars.deletewi = None\n\n#==================================================================#\n#  \n#==================================================================#\ndef deletewifolder(uid):\n    uid = int(uid)\n    del vars.wifolders_u[uid]\n    del vars.wifolders_d[uid]\n    del vars.wifolders_l[vars.wifolders_l.index(uid)]\n    setgamesaved(False)\n    # Delete uninitialized entries in the folder we're going to delete\n    vars.worldinfo = [wi for wi in vars.worldinfo if wi[\"folder\"] != uid or wi[\"init\"]]\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    # Move WI entries that are inside of the folder we're going to delete\n    # so that they're outside of all folders\n    for wi in vars.worldinfo:\n        if(wi[\"folder\"] == uid):\n            wi[\"folder\"] = None\n\n    sendwi()\n\n#==================================================================#\n#  Look for WI keys in text to generator \n#==================================================================#\ndef checkworldinfo(txt, allowed_entries=None, allowed_folders=None, force_use_txt=False, scan_story=True, actions=None):\n    original_txt = txt\n\n    if(actions is None):\n        actions = vars.actions\n\n    # Dont go any further if WI is empty\n    if(len(vars.worldinfo) == 0):\n        return \"\", set()\n    \n    # Cache actions length\n    ln = len(actions)\n    \n    # Don't bother calculating action history if widepth is 0\n    if(vars.widepth > 0 and scan_story):\n        depth = vars.widepth\n        # If this is not a continue, add 1 to widepth since submitted\n        # text is already in action history @ -1\n        if(not force_use_txt and (txt != \"\" and vars.prompt != txt)):\n            txt    = \"\"\n            depth += 1\n        \n        if(ln > 0):\n            chunks = collections.deque()\n            i = 0\n            for key in reversed(actions):\n                chunk = actions[key]\n                chunks.appendleft(chunk)\n                i += 1\n                if(i == depth):\n                    break\n        \n        if(ln >= depth):\n            txt = \"\".join(chunks)\n        elif(ln > 0):\n            txt = vars.comregex_ai.sub('', vars.prompt) + \"\".join(chunks)\n        elif(ln == 0):\n            txt = vars.comregex_ai.sub('', vars.prompt)\n\n    if(force_use_txt):\n        txt += original_txt\n\n    # Scan text for matches on WI keys\n    wimem = \"\"\n    found_entries = set()\n    for wi in vars.worldinfo:\n        if(allowed_entries is not None and wi[\"uid\"] not in allowed_entries):\n            continue\n        if(allowed_folders is not None and wi[\"folder\"] not in allowed_folders):\n            continue\n\n        if(wi.get(\"constant\", False)):\n            wimem = wimem + wi[\"content\"] + \"\\n\"\n            found_entries.add(id(wi))\n            continue\n\n        if(len(wi[\"key\"].strip()) > 0 and (not wi.get(\"selective\", False) or len(wi.get(\"keysecondary\", \"\").strip()) > 0)):\n            # Split comma-separated keys\n            keys = wi[\"key\"].split(\",\")\n            keys_secondary = wi.get(\"keysecondary\", \"\").split(\",\")\n\n            for k in keys:\n                ky = k\n                # Remove leading/trailing spaces if the option is enabled\n                if(vars.wirmvwhtsp):\n                    ky = k.strip()\n                if ky.lower() in txt.lower():\n                    if wi.get(\"selective\", False) and len(keys_secondary):\n                        found = False\n                        for ks in keys_secondary:\n                            ksy = ks\n                            if(vars.wirmvwhtsp):\n                                ksy = ks.strip()\n                            if ksy.lower() in txt.lower():\n                                wimem = wimem + wi[\"content\"] + \"\\n\"\n                                found_entries.add(id(wi))\n                                found = True\n                                break\n                        if found:\n                            break\n                    else:\n                        wimem = wimem + wi[\"content\"] + \"\\n\"\n                        found_entries.add(id(wi))\n                        break\n    \n    return wimem, found_entries\n    \n#==================================================================#\n#  Commit changes to Memory storage\n#==================================================================#\ndef memsubmit(data):\n    emit('from_server', {'cmd': 'setinputtext', 'data': data}, broadcast=True)\n    # Maybe check for length at some point\n    # For now just send it to storage\n    if(data != vars.memory):\n        setgamesaved(False)\n    vars.memory = data\n    vars.mode = \"play\"\n    emit('from_server', {'cmd': 'memmode', 'data': 'false'}, broadcast=True)\n    \n    # Ask for contents of Author's Note field\n    emit('from_server', {'cmd': 'getanote', 'data': ''})\n\n#==================================================================#\n#  Commit changes to Author's Note\n#==================================================================#\ndef anotesubmit(data, template=\"\"):\n    assert type(data) is str and type(template) is str\n    # Maybe check for length at some point\n    # For now just send it to storage\n    if(data != vars.authornote):\n        setgamesaved(False)\n    vars.authornote = data\n\n    if(vars.authornotetemplate != template):\n        vars.setauthornotetemplate = template\n        settingschanged()\n    vars.authornotetemplate = template\n\n    emit('from_server', {'cmd': 'setanote', 'data': vars.authornote}, broadcast=True)\n    emit('from_server', {'cmd': 'setanotetemplate', 'data': vars.authornotetemplate}, broadcast=True)\n\n#==================================================================#\n#  Assembles game data into a request to InferKit API\n#==================================================================#\ndef ikrequest(txt):\n    # Log request to console\n    if not vars.quiet:\n        print(\"{0}Len:{1}, Txt:{2}{3}\".format(colors.YELLOW, len(txt), txt, colors.END))\n    \n    # Build request JSON data\n    reqdata = {\n        'forceNoEnd': True,\n        'length': vars.ikgen,\n        'prompt': {\n            'isContinuation': False,\n            'text': txt\n        },\n        'startFromBeginning': False,\n        'streamResponse': False,\n        'temperature': vars.temp,\n        'topP': vars.top_p\n    }\n    \n    # Create request\n    req = requests.post(\n        vars.url, \n        json    = reqdata,\n        headers = {\n            'Authorization': 'Bearer '+vars.apikey\n            }\n        )\n    \n    # Deal with the response\n    if(req.status_code == 200):\n        genout = req.json()[\"data\"][\"text\"]\n\n        vars.lua_koboldbridge.outputs[1] = genout\n\n        execute_outmod()\n        if(vars.lua_koboldbridge.regeneration_required):\n            vars.lua_koboldbridge.regeneration_required = False\n            genout = vars.lua_koboldbridge.outputs[1]\n            assert genout is str\n\n        if not vars.quiet:\n            print(\"{0}{1}{2}\".format(colors.CYAN, genout, colors.END))\n        vars.actions.append(genout)\n        if vars.actions.get_last_key() in vars.actions_metadata:\n            vars.actions_metadata[vars.actions.get_last_key()] = {\"Selected Text\": genout, \"Alternative Text\": []}\n        else:\n        # 2. We've selected a chunk of text that is was presented previously\n            alternatives = [item['Text'] for item in vars.actions_metadata[vars.actions.get_last_key()][\"Alternative Text\"]]\n            if genout in alternatives:\n                alternatives = [item for item in vars.actions_metadata[vars.actions.get_last_key()][\"Alternative Text\"] if item['Text'] != genout]\n                vars.actions_metadata[vars.actions.get_last_key()][\"Alternative Text\"] = alternatives\n            vars.actions_metadata[vars.actions.get_last_key()][\"Selected Text\"] = genout\n        update_story_chunk('last')\n        emit('from_server', {'cmd': 'texteffect', 'data': vars.actions.get_last_key() + 1 if len(vars.actions) else 0}, broadcast=True)\n        send_debug()\n        set_aibusy(0)\n    else:\n        # Send error message to web client\n        er = req.json()\n        if(\"error\" in er):\n            code = er[\"error\"][\"extensions\"][\"code\"]\n        elif(\"errors\" in er):\n            code = er[\"errors\"][0][\"extensions\"][\"code\"]\n            \n        errmsg = \"InferKit API Error: {0} - {1}\".format(req.status_code, code)\n        emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n        set_aibusy(0)\n\n#==================================================================#\n#  Assembles game data into a request to OpenAI API\n#==================================================================#\ndef oairequest(txt, min, max):\n    # Log request to console\n    if not vars.quiet:\n        print(\"{0}Len:{1}, Txt:{2}{3}\".format(colors.YELLOW, len(txt), txt, colors.END))\n    \n    # Store context in memory to use it for comparison with generated content\n    vars.lastctx = txt\n    \n    # Build request JSON data\n    # GooseAI is a subntype of OAI. So to check if it's this type, we check the configname as a workaround\n    # as the vars.model will always be OAI\n    if 'GooseAI' in vars.configname:\n        reqdata = {\n            'prompt': txt,\n            'max_tokens': vars.genamt,\n            'temperature': vars.temp,\n            'top_a': vars.top_a,\n            'top_p': vars.top_p,\n            'top_k': vars.top_k,\n            'tfs': vars.tfs,\n            'typical_p': vars.typical,\n            'repetition_penalty': vars.rep_pen,\n            'repetition_penalty_slope': vars.rep_pen_slope,\n            'repetition_penalty_range': vars.rep_pen_range,\n            'n': vars.numseqs,\n            'stream': False\n        }\n    else:\n        reqdata = {\n            'prompt': txt,\n            'max_tokens': vars.genamt,\n            'temperature': vars.temp,\n            'top_p': vars.top_p,\n            'n': vars.numseqs,\n            'stream': False\n        }\n    \n    req = requests.post(\n        vars.oaiurl, \n        json    = reqdata,\n        headers = {\n            'Authorization': 'Bearer '+vars.oaiapikey,\n            'Content-Type': 'application/json'\n            }\n        )\n    \n    # Deal with the response\n    if(req.status_code == 200):\n        outputs = [out[\"text\"] for out in req.json()[\"choices\"]]\n\n        for idx in range(len(outputs)):\n            vars.lua_koboldbridge.outputs[idx+1] = outputs[idx]\n\n        execute_outmod()\n        if (vars.lua_koboldbridge.regeneration_required):\n            vars.lua_koboldbridge.regeneration_required = False\n            genout = []\n            for i in range(len(outputs)):\n                genout.append(\n                    {\"generated_text\": vars.lua_koboldbridge.outputs[i + 1]})\n                assert type(genout[-1][\"generated_text\"]) is str\n        else:\n            genout = [\n                {\"generated_text\": utils.decodenewlines(txt)}\n                for txt in outputs]\n\n        if vars.actions.get_last_key() not in vars.actions_metadata:\n            vars.actions_metadata[vars.actions.get_last_key()] = {\n                \"Selected Text\": genout[0], \"Alternative Text\": []}\n        else:\n        # 2. We've selected a chunk of text that is was presented previously\n            try:\n                alternatives = [item['Text'] for item in vars.actions_metadata[len(vars.actions)-1][\"Alternative Text\"]]\n            except:\n                print(len(vars.actions))\n                print(vars.actions_metadata)\n                raise\n            if genout in alternatives:\n                alternatives = [item for item in vars.actions_metadata[vars.actions.get_last_key() ][\"Alternative Text\"] if item['Text'] != genout]\n                vars.actions_metadata[vars.actions.get_last_key()][\"Alternative Text\"] = alternatives\n            vars.actions_metadata[vars.actions.get_last_key()][\"Selected Text\"] = genout\n\n        if (len(genout) == 1):\n            genresult(genout[0][\"generated_text\"])\n        else:\n            if (vars.lua_koboldbridge.restart_sequence is not None and\n                    vars.lua_koboldbridge.restart_sequence > 0):\n                genresult(genout[vars.lua_koboldbridge.restart_sequence - 1][\n                              \"generated_text\"])\n            else:\n                genselect(genout)\n\n        if not vars.quiet:\n            print(\"{0}{1}{2}\".format(colors.CYAN, genout, colors.END))\n\n        set_aibusy(0)\n    else:\n        # Send error message to web client            \n        er = req.json()\n        if(\"error\" in er):\n            type    = er[\"error\"][\"type\"]\n            message = er[\"error\"][\"message\"]\n            \n        errmsg = \"OpenAI API Error: {0} - {1}\".format(type, message)\n        emit('from_server', {'cmd': 'errmsg', 'data': errmsg}, broadcast=True)\n        set_aibusy(0)\n\n#==================================================================#\n#  Forces UI to Play mode\n#==================================================================#\ndef exitModes():\n    if(vars.mode == \"edit\"):\n        emit('from_server', {'cmd': 'editmode', 'data': 'false'}, broadcast=True)\n    elif(vars.mode == \"memory\"):\n        emit('from_server', {'cmd': 'memmode', 'data': 'false'}, broadcast=True)\n    elif(vars.mode == \"wi\"):\n        emit('from_server', {'cmd': 'wimode', 'data': 'false'}, broadcast=True)\n    vars.mode = \"play\"\n\n#==================================================================#\n#  Launch in-browser save prompt\n#==================================================================#\ndef saveas(data):\n    \n    name = data['name']\n    savepins = data['pins']\n    # Check if filename exists already\n    name = utils.cleanfilename(name)\n    if(not fileops.saveexists(name) or (vars.saveow and vars.svowname == name)):\n        # All clear to save\n        e = saveRequest(fileops.storypath(name), savepins=savepins)\n        vars.saveow = False\n        vars.svowname = \"\"\n        if(e is None):\n            emit('from_server', {'cmd': 'hidesaveas', 'data': ''})\n        else:\n            print(\"{0}{1}{2}\".format(colors.RED, str(e), colors.END))\n            emit('from_server', {'cmd': 'popuperror', 'data': str(e)})\n    else:\n        # File exists, prompt for overwrite\n        vars.saveow   = True\n        vars.svowname = name\n        emit('from_server', {'cmd': 'askforoverwrite', 'data': ''})\n\n#==================================================================#\n#  Launch in-browser story-delete prompt\n#==================================================================#\ndef deletesave(name):\n    name = utils.cleanfilename(name)\n    e = fileops.deletesave(name)\n    if(e is None):\n        if(vars.smandelete):\n            emit('from_server', {'cmd': 'hidepopupdelete', 'data': ''})\n            getloadlist()\n        else:\n            emit('from_server', {'cmd': 'popuperror', 'data': \"The server denied your request to delete this story\"})\n    else:\n        print(\"{0}{1}{2}\".format(colors.RED, str(e), colors.END))\n        emit('from_server', {'cmd': 'popuperror', 'data': str(e)})\n\n#==================================================================#\n#  Launch in-browser story-rename prompt\n#==================================================================#\ndef renamesave(name, newname):\n    # Check if filename exists already\n    name = utils.cleanfilename(name)\n    newname = utils.cleanfilename(newname)\n    if(not fileops.saveexists(newname) or name == newname or (vars.saveow and vars.svowname == newname)):\n        e = fileops.renamesave(name, newname)\n        vars.saveow = False\n        vars.svowname = \"\"\n        if(e is None):\n            if(vars.smanrename):\n                emit('from_server', {'cmd': 'hidepopuprename', 'data': ''})\n                getloadlist()\n            else:\n                emit('from_server', {'cmd': 'popuperror', 'data': \"The server denied your request to rename this story\"})\n        else:\n            print(\"{0}{1}{2}\".format(colors.RED, str(e), colors.END))\n            emit('from_server', {'cmd': 'popuperror', 'data': str(e)})\n    else:\n        # File exists, prompt for overwrite\n        vars.saveow   = True\n        vars.svowname = newname\n        emit('from_server', {'cmd': 'askforoverwrite', 'data': ''})\n\n#==================================================================#\n#  Save the currently running story\n#==================================================================#\ndef save():\n    # Check if a file is currently open\n    if(\".json\" in vars.savedir):\n        saveRequest(vars.savedir)\n    else:\n        emit('from_server', {'cmd': 'saveas', 'data': ''})\n\n#==================================================================#\n#  Save the story via file browser\n#==================================================================#\ndef savetofile():\n    savpath = fileops.getsavepath(vars.savedir, \"Save Story As\", [(\"Json\", \"*.json\")])\n    saveRequest(savpath)\n\n#==================================================================#\n#  Save the story to specified path\n#==================================================================#\ndef saveRequest(savpath, savepins=True):    \n    if(savpath):\n        # Leave Edit/Memory mode before continuing\n        exitModes()\n        \n        # Save path for future saves\n        vars.savedir = savpath\n        txtpath = os.path.splitext(savpath)[0] + \".txt\"\n        # Build json to write\n        js = {}\n        js[\"gamestarted\"] = vars.gamestarted\n        js[\"prompt\"]      = vars.prompt\n        js[\"memory\"]      = vars.memory\n        js[\"authorsnote\"] = vars.authornote\n        js[\"anotetemplate\"] = vars.authornotetemplate\n        js[\"actions\"]     = tuple(vars.actions.values())\n        if savepins:\n            js[\"actions_metadata\"]     = vars.actions_metadata\n        js[\"worldinfo\"]   = []\n        js[\"wifolders_d\"] = vars.wifolders_d\n        js[\"wifolders_l\"] = vars.wifolders_l\n\t\t\n        # Extract only the important bits of WI\n        for wi in vars.worldinfo_i:\n            if(True):\n                js[\"worldinfo\"].append({\n                    \"key\": wi[\"key\"],\n                    \"keysecondary\": wi[\"keysecondary\"],\n                    \"content\": wi[\"content\"],\n                    \"comment\": wi[\"comment\"],\n                    \"folder\": wi[\"folder\"],\n                    \"selective\": wi[\"selective\"],\n                    \"constant\": wi[\"constant\"]\n                })\n                \n        txt = vars.prompt + \"\".join(vars.actions.values())\n\n        # Write it\n        try:\n            file = open(savpath, \"w\")\n        except Exception as e:\n            return e\n        try:\n            file.write(json.dumps(js, indent=3))\n        except Exception as e:\n            file.close()\n            return e\n        file.close()\n        \n        try:\n            file = open(txtpath, \"w\")\n        except Exception as e:\n            return e\n        try:\n            file.write(txt)\n        except Exception as e:\n            file.close()\n            return e\n        file.close()\n\n        filename = path.basename(savpath)\n        if(filename.endswith('.json')):\n            filename = filename[:-5]\n        vars.laststory = filename\n        emit('from_server', {'cmd': 'setstoryname', 'data': vars.laststory}, broadcast=True)\n        setgamesaved(True)\n        print(\"{0}Story saved to {1}!{2}\".format(colors.GREEN, path.basename(savpath), colors.END))\n\n#==================================================================#\n#  Show list of saved stories\n#==================================================================#\ndef getloadlist():\n    emit('from_server', {'cmd': 'buildload', 'data': fileops.getstoryfiles()})\n\n#==================================================================#\n#  Show list of soft prompts\n#==================================================================#\ndef getsplist():\n    if(vars.allowsp):\n        emit('from_server', {'cmd': 'buildsp', 'data': fileops.getspfiles(vars.modeldim)})\n\n#==================================================================#\n#  Get list of userscripts\n#==================================================================#\ndef getuslist():\n    files = {i: v for i, v in enumerate(fileops.getusfiles())}\n    loaded = []\n    unloaded = []\n    userscripts = set(vars.userscripts)\n    for i in range(len(files)):\n        if files[i][\"filename\"] not in userscripts:\n            unloaded.append(files[i])\n    files = {files[k][\"filename\"]: files[k] for k in files}\n    userscripts = set(files.keys())\n    for filename in vars.userscripts:\n        if filename in userscripts:\n            loaded.append(files[filename])\n    return unloaded, loaded\n\n#==================================================================#\n#  Load a saved story via file browser\n#==================================================================#\ndef loadfromfile():\n    loadpath = fileops.getloadpath(vars.savedir, \"Select Story File\", [(\"Json\", \"*.json\")])\n    loadRequest(loadpath)\n\n#==================================================================#\n#  Load a stored story from a file\n#==================================================================#\ndef loadRequest(loadpath, filename=None):\n    if(loadpath):\n        # Leave Edit/Memory mode before continuing\n        exitModes()\n        \n        # Read file contents into JSON object\n        if(isinstance(loadpath, str)):\n            with open(loadpath, \"r\") as file:\n                js = json.load(file)\n            if(filename is None):\n                filename = path.basename(loadpath)\n        else:\n            js = loadpath\n            if(filename is None):\n                filename = \"untitled.json\"\n        \n        # Copy file contents to vars\n        vars.gamestarted = js[\"gamestarted\"]\n        vars.prompt      = js[\"prompt\"]\n        vars.memory      = js[\"memory\"]\n        vars.worldinfo   = []\n        vars.worldinfo   = []\n        vars.worldinfo_u = {}\n        vars.wifolders_d = {int(k): v for k, v in js.get(\"wifolders_d\", {}).items()}\n        vars.wifolders_l = js.get(\"wifolders_l\", [])\n        vars.wifolders_u = {uid: [] for uid in vars.wifolders_d}\n        vars.lastact     = \"\"\n        vars.submission  = \"\"\n        vars.lastctx     = \"\"\n        vars.genseqs = []\n\n        del vars.actions\n        vars.actions = structures.KoboldStoryRegister()\n        actions = collections.deque(js[\"actions\"])\n        \n\n        if \"actions_metadata\" in js:\n            \n            if type(js[\"actions_metadata\"]) == dict:\n                temp = js[\"actions_metadata\"]\n                vars.actions_metadata = {}\n                #we need to redo the numbering of the actions_metadata since the actions list doesn't preserve it's number on saving\n                if len(temp) > 0:\n                    counter = 0\n                    temp = {int(k):v for k,v in temp.items()}\n                    for i in range(max(temp)+1):\n                        if i in temp:\n                            vars.actions_metadata[counter] = temp[i]\n                            counter += 1\n                del temp\n            else:\n                #fix if we're using the old metadata format\n                vars.actions_metadata = {}\n                i = 0\n                \n                for text in js['actions']:\n                    vars.actions_metadata[i] = {'Selected Text': text, 'Alternative Text': []}\n                    i+=1\n        else:\n            vars.actions_metadata = {}\n            i = 0\n            \n            for text in js['actions']:\n                vars.actions_metadata[i] = {'Selected Text': text, 'Alternative Text': []}\n                i+=1\n\n        footer = \"\"                \n\n        if(len(vars.prompt.strip()) == 0):\n            while(len(actions)):\n                action = actions.popleft()\n                if(len(action.strip()) != 0):\n                    vars.prompt = action\n                    break\n            else:\n                vars.gamestarted = False\n        vars.prompt = vars.prompt.lstrip()\n        ln = len(vars.prompt.rstrip())\n        footer += vars.prompt[ln:]\n        vars.prompt = vars.prompt[:ln]\n        if(vars.gamestarted):\n            for s in actions:\n                if(len(s.strip()) == 0):\n                    # If this action only contains whitespace, we merge it with the next action\n                    footer += s\n                    continue\n                vars.actions.append(footer + s)\n                footer = \"\"\n                # If there is trailing whitespace at the end of an action, we move that whitespace to the beginning of the next action\n                ln = len(vars.actions[vars.actions.get_last_key()].rstrip())\n                footer += vars.actions[vars.actions.get_last_key()][ln:]\n                vars.actions[vars.actions.get_last_key()] = vars.actions[vars.actions.get_last_key()][:ln]\n        \n        # Try not to break older save files\n        if(\"authorsnote\" in js):\n            vars.authornote = js[\"authorsnote\"]\n        else:\n            vars.authornote = \"\"\n        if(\"anotetemplate\" in js):\n            vars.authornotetemplate = js[\"anotetemplate\"]\n        else:\n            vars.authornotetemplate = \"[Author's note: <|>]\"\n        \n        if(\"worldinfo\" in js):\n            num = 0\n            for wi in js[\"worldinfo\"]:\n                vars.worldinfo.append({\n                    \"key\": wi[\"key\"],\n                    \"keysecondary\": wi.get(\"keysecondary\", \"\"),\n                    \"content\": wi[\"content\"],\n                    \"comment\": wi.get(\"comment\", \"\"),\n                    \"folder\": wi.get(\"folder\", None),\n                    \"num\": num,\n                    \"init\": True,\n                    \"selective\": wi.get(\"selective\", False),\n                    \"constant\": wi.get(\"constant\", False),\n                    \"uid\": None,\n                })\n                while(True):\n                    uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n                    if(uid not in vars.worldinfo_u):\n                        break\n                vars.worldinfo_u[uid] = vars.worldinfo[-1]\n                vars.worldinfo[-1][\"uid\"] = uid\n                if(vars.worldinfo[-1][\"folder\"] is not None):\n                    vars.wifolders_u[vars.worldinfo[-1][\"folder\"]].append(vars.worldinfo[-1])\n                num += 1\n\n        for uid in vars.wifolders_l + [None]:\n            vars.worldinfo.append({\"key\": \"\", \"keysecondary\": \"\", \"content\": \"\", \"comment\": \"\", \"folder\": uid, \"num\": None, \"init\": False, \"selective\": False, \"constant\": False, \"uid\": None})\n            while(True):\n                uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n                if(uid not in vars.worldinfo_u):\n                    break\n            vars.worldinfo_u[uid] = vars.worldinfo[-1]\n            vars.worldinfo[-1][\"uid\"] = uid\n            if(vars.worldinfo[-1][\"folder\"] is not None):\n                vars.wifolders_u[vars.worldinfo[-1][\"folder\"]].append(vars.worldinfo[-1])\n        stablesortwi()\n        vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n\n        # Save path for save button\n        vars.savedir = loadpath\n        \n        # Clear loadselect var\n        vars.loadselect = \"\"\n        \n        # Refresh game screen\n        _filename = filename\n        if(filename.endswith('.json')):\n            _filename = filename[:-5]\n        vars.laststory = _filename\n        emit('from_server', {'cmd': 'setstoryname', 'data': vars.laststory}, broadcast=True)\n        setgamesaved(True)\n        sendwi()\n        emit('from_server', {'cmd': 'setmemory', 'data': vars.memory}, broadcast=True)\n        emit('from_server', {'cmd': 'setanote', 'data': vars.authornote}, broadcast=True)\n        emit('from_server', {'cmd': 'setanotetemplate', 'data': vars.authornotetemplate}, broadcast=True)\n        refresh_story()\n        emit('from_server', {'cmd': 'setgamestate', 'data': 'ready'}, broadcast=True)\n        emit('from_server', {'cmd': 'hidegenseqs', 'data': ''}, broadcast=True)\n        print(\"{0}Story loaded from {1}!{2}\".format(colors.GREEN, filename, colors.END))\n        \n        send_debug()\n\n#==================================================================#\n# Import an AIDungon game exported with Mimi's tool\n#==================================================================#\ndef importRequest():\n    importpath = fileops.getloadpath(vars.savedir, \"Select AID CAT File\", [(\"Json\", \"*.json\")])\n    \n    if(importpath):\n        # Leave Edit/Memory mode before continuing\n        exitModes()\n        \n        # Read file contents into JSON object\n        file = open(importpath, \"rb\")\n        vars.importjs = json.load(file)\n        \n        # If a bundle file is being imported, select just the Adventures object\n        if type(vars.importjs) is dict and \"stories\" in vars.importjs:\n            vars.importjs = vars.importjs[\"stories\"]\n        \n        # Clear Popup Contents\n        emit('from_server', {'cmd': 'clearpopup', 'data': ''}, broadcast=True)\n        \n        # Initialize vars\n        num = 0\n        vars.importnum = -1\n        \n        # Get list of stories\n        for story in vars.importjs:\n            ob = {}\n            ob[\"num\"]   = num\n            if(story[\"title\"] != \"\" and story[\"title\"] != None):\n                ob[\"title\"] = story[\"title\"]\n            else:\n                ob[\"title\"] = \"(No Title)\"\n            if(story[\"description\"] != \"\" and story[\"description\"] != None):\n                ob[\"descr\"] = story[\"description\"]\n            else:\n                ob[\"descr\"] = \"(No Description)\"\n            if(\"actions\" in story):\n                ob[\"acts\"]  = len(story[\"actions\"])\n            elif(\"actionWindow\" in story):\n                ob[\"acts\"]  = len(story[\"actionWindow\"])\n            emit('from_server', {'cmd': 'addimportline', 'data': ob})\n            num += 1\n        \n        # Show Popup\n        emit('from_server', {'cmd': 'popupshow', 'data': True})\n\n#==================================================================#\n# Import an AIDungon game selected in popup\n#==================================================================#\ndef importgame():\n    if(vars.importnum >= 0):\n        # Cache reference to selected game\n        ref = vars.importjs[vars.importnum]\n        \n        # Copy game contents to vars\n        vars.gamestarted = True\n        \n        # Support for different versions of export script\n        if(\"actions\" in ref):\n            if(len(ref[\"actions\"]) > 0):\n                vars.prompt = ref[\"actions\"][0][\"text\"]\n            else:\n                vars.prompt = \"\"\n        elif(\"actionWindow\" in ref):\n            if(len(ref[\"actionWindow\"]) > 0):\n                vars.prompt = ref[\"actionWindow\"][0][\"text\"]\n            else:\n                vars.prompt = \"\"\n        else:\n            vars.prompt = \"\"\n        vars.memory      = ref[\"memory\"]\n        vars.authornote  = ref[\"authorsNote\"] if type(ref[\"authorsNote\"]) is str else \"\"\n        vars.authornotetemplate = \"[Author's note: <|>]\"\n        vars.actions     = structures.KoboldStoryRegister()\n        vars.actions_metadata = {}\n        vars.worldinfo   = []\n        vars.worldinfo_i = []\n        vars.worldinfo_u = {}\n        vars.wifolders_d = {}\n        vars.wifolders_l = []\n        vars.wifolders_u = {uid: [] for uid in vars.wifolders_d}\n        vars.lastact     = \"\"\n        vars.submission  = \"\"\n        vars.lastctx     = \"\"\n        \n        # Get all actions except for prompt\n        if(\"actions\" in ref):\n            if(len(ref[\"actions\"]) > 1):\n                for act in ref[\"actions\"][1:]:\n                    vars.actions.append(act[\"text\"])\n        elif(\"actionWindow\" in ref):\n            if(len(ref[\"actionWindow\"]) > 1):\n                for act in ref[\"actionWindow\"][1:]:\n                    vars.actions.append(act[\"text\"])\n        \n        # Get just the important parts of world info\n        if(ref[\"worldInfo\"] != None):\n            if(len(ref[\"worldInfo\"]) > 1):\n                num = 0\n                for wi in ref[\"worldInfo\"]:\n                    vars.worldinfo.append({\n                        \"key\": wi[\"keys\"],\n                        \"keysecondary\": wi.get(\"keysecondary\", \"\"),\n                        \"content\": wi[\"entry\"],\n                        \"comment\": wi.get(\"comment\", \"\"),\n                        \"folder\": wi.get(\"folder\", None),\n                        \"num\": num,\n                        \"init\": True,\n                        \"selective\": wi.get(\"selective\", False),\n                        \"constant\": wi.get(\"constant\", False),\n                        \"uid\": None,\n                    })\n                    while(True):\n                        uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n                        if(uid not in vars.worldinfo_u):\n                            break\n                    vars.worldinfo_u[uid] = vars.worldinfo[-1]\n                    vars.worldinfo[-1][\"uid\"] = uid\n                    if(vars.worldinfo[-1][\"folder\"]) is not None:\n                        vars.wifolders_u[vars.worldinfo[-1][\"folder\"]].append(vars.worldinfo[-1])\n                    num += 1\n\n        for uid in vars.wifolders_l + [None]:\n            vars.worldinfo.append({\"key\": \"\", \"keysecondary\": \"\", \"content\": \"\", \"comment\": \"\", \"folder\": uid, \"num\": None, \"init\": False, \"selective\": False, \"constant\": False, \"uid\": None})\n            while(True):\n                uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n                if(uid not in vars.worldinfo_u):\n                    break\n            vars.worldinfo_u[uid] = vars.worldinfo[-1]\n            vars.worldinfo[-1][\"uid\"] = uid\n            if(vars.worldinfo[-1][\"folder\"] is not None):\n                vars.wifolders_u[vars.worldinfo[-1][\"folder\"]].append(vars.worldinfo[-1])\n        stablesortwi()\n        vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n        \n        # Clear import data\n        vars.importjs = {}\n        \n        # Reset current save\n        vars.savedir = getcwd()+\"\\\\stories\"\n        \n        # Refresh game screen\n        vars.laststory = None\n        emit('from_server', {'cmd': 'setstoryname', 'data': vars.laststory}, broadcast=True)\n        setgamesaved(False)\n        sendwi()\n        emit('from_server', {'cmd': 'setmemory', 'data': vars.memory}, broadcast=True)\n        emit('from_server', {'cmd': 'setanote', 'data': vars.authornote}, broadcast=True)\n        emit('from_server', {'cmd': 'setanotetemplate', 'data': vars.authornotetemplate}, broadcast=True)\n        refresh_story()\n        emit('from_server', {'cmd': 'setgamestate', 'data': 'ready'}, broadcast=True)\n        emit('from_server', {'cmd': 'hidegenseqs', 'data': ''}, broadcast=True)\n\n#==================================================================#\n# Import an aidg.club prompt and start a new game with it.\n#==================================================================#\ndef importAidgRequest(id):    \n    exitModes()\n    \n    urlformat = \"https://aetherroom.club/api/\"\n    req = requests.get(urlformat+id)\n\n    if(req.status_code == 200):\n        js = req.json()\n        \n        # Import game state\n        vars.gamestarted = True\n        vars.prompt      = js[\"promptContent\"]\n        vars.memory      = js[\"memory\"]\n        vars.authornote  = js[\"authorsNote\"]\n        vars.authornotetemplate = \"[Author's note: <|>]\"\n        vars.actions     = structures.KoboldStoryRegister()\n        vars.actions_metadata = {}\n        vars.worldinfo   = []\n        vars.worldinfo_i = []\n        vars.worldinfo_u = {}\n        vars.wifolders_d = {}\n        vars.wifolders_l = []\n        vars.wifolders_u = {uid: [] for uid in vars.wifolders_d}\n        vars.lastact     = \"\"\n        vars.submission  = \"\"\n        vars.lastctx     = \"\"\n        \n        if not vars.memory:\n            vars.memory = \"\"\n        if not vars.authornote:\n            vars.authornote = \"\"\n        \n        num = 0\n        for wi in js[\"worldInfos\"]:\n            vars.worldinfo.append({\n                \"key\": wi[\"keys\"],\n                \"keysecondary\": wi.get(\"keysecondary\", \"\"),\n                \"content\": wi[\"entry\"],\n                \"comment\": wi.get(\"comment\", \"\"),\n                \"folder\": wi.get(\"folder\", None),\n                \"num\": num,\n                \"init\": True,\n                \"selective\": wi.get(\"selective\", False),\n                \"constant\": wi.get(\"constant\", False),\n                \"uid\": None,\n            })\n            while(True):\n                uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n                if(uid not in vars.worldinfo_u):\n                    break\n            vars.worldinfo_u[uid] = vars.worldinfo[-1]\n            vars.worldinfo[-1][\"uid\"] = uid\n            if(vars.worldinfo[-1][\"folder\"]) is not None:\n                vars.wifolders_u[vars.worldinfo[-1][\"folder\"]].append(vars.worldinfo[-1])\n            num += 1\n\n        for uid in vars.wifolders_l + [None]:\n            vars.worldinfo.append({\"key\": \"\", \"keysecondary\": \"\", \"content\": \"\", \"comment\": \"\", \"folder\": uid, \"num\": None, \"init\": False, \"selective\": False, \"constant\": False, \"uid\": None})\n            while(True):\n                uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n                if(uid not in vars.worldinfo_u):\n                    break\n            vars.worldinfo_u[uid] = vars.worldinfo[-1]\n            vars.worldinfo[-1][\"uid\"] = uid\n            if(vars.worldinfo[-1][\"folder\"] is not None):\n                vars.wifolders_u[vars.worldinfo[-1][\"folder\"]].append(vars.worldinfo[-1])\n        stablesortwi()\n        vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n\n        # Reset current save\n        vars.savedir = getcwd()+\"\\\\stories\"\n        \n        # Refresh game screen\n        vars.laststory = None\n        emit('from_server', {'cmd': 'setstoryname', 'data': vars.laststory}, broadcast=True)\n        setgamesaved(False)\n        sendwi()\n        emit('from_server', {'cmd': 'setmemory', 'data': vars.memory}, broadcast=True)\n        emit('from_server', {'cmd': 'setanote', 'data': vars.authornote}, broadcast=True)\n        emit('from_server', {'cmd': 'setanotetemplate', 'data': vars.authornotetemplate}, broadcast=True)\n        refresh_story()\n        emit('from_server', {'cmd': 'setgamestate', 'data': 'ready'}, broadcast=True)\n\n#==================================================================#\n#  Import World Info JSON file\n#==================================================================#\ndef wiimportrequest():\n    importpath = fileops.getloadpath(vars.savedir, \"Select World Info File\", [(\"Json\", \"*.json\")])\n    if(importpath):\n        file = open(importpath, \"rb\")\n        js = json.load(file)\n        if(len(js) > 0):\n            # If the most recent WI entry is blank, remove it.\n            if(not vars.worldinfo[-1][\"init\"]):\n                del vars.worldinfo[-1]\n            # Now grab the new stuff\n            num = len(vars.worldinfo)\n            for wi in js:\n                vars.worldinfo.append({\n                    \"key\": wi[\"keys\"],\n                    \"keysecondary\": wi.get(\"keysecondary\", \"\"),\n                    \"content\": wi[\"entry\"],\n                    \"comment\": wi.get(\"comment\", \"\"),\n                    \"folder\": wi.get(\"folder\", None),\n                    \"num\": num,\n                    \"init\": True,\n                    \"selective\": wi.get(\"selective\", False),\n                    \"constant\": wi.get(\"constant\", False),\n                    \"uid\": None,\n                })\n                while(True):\n                    uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n                    if(uid not in vars.worldinfo_u):\n                        break\n                vars.worldinfo_u[uid] = vars.worldinfo[-1]\n                vars.worldinfo[-1][\"uid\"] = uid\n                if(vars.worldinfo[-1][\"folder\"]) is not None:\n                    vars.wifolders_u[vars.worldinfo[-1][\"folder\"]].append(vars.worldinfo[-1])\n                num += 1\n            for uid in [None]:\n                vars.worldinfo.append({\"key\": \"\", \"keysecondary\": \"\", \"content\": \"\", \"comment\": \"\", \"folder\": uid, \"num\": None, \"init\": False, \"selective\": False, \"constant\": False, \"uid\": None})\n                while(True):\n                    uid = int.from_bytes(os.urandom(4), \"little\", signed=True)\n                    if(uid not in vars.worldinfo_u):\n                        break\n                vars.worldinfo_u[uid] = vars.worldinfo[-1]\n                vars.worldinfo[-1][\"uid\"] = uid\n                if(vars.worldinfo[-1][\"folder\"] is not None):\n                    vars.wifolders_u[vars.worldinfo[-1][\"folder\"]].append(vars.worldinfo[-1])\n        \n        if not vars.quiet:\n            print(\"{0}\".format(vars.worldinfo[0]))\n                \n        # Refresh game screen\n        setgamesaved(False)\n        sendwi()\n\n#==================================================================#\n#  Starts a new story\n#==================================================================#\ndef newGameRequest(): \n    # Leave Edit/Memory mode before continuing\n    exitModes()\n    \n    # Clear vars values\n    vars.gamestarted = False\n    vars.prompt      = \"\"\n    vars.memory      = \"\"\n    vars.actions     = structures.KoboldStoryRegister()\n    vars.actions_metadata = {}\n    \n    vars.authornote  = \"\"\n    vars.authornotetemplate = vars.setauthornotetemplate\n    vars.worldinfo   = []\n    vars.worldinfo_i = []\n    vars.worldinfo_u = {}\n    vars.wifolders_d = {}\n    vars.wifolders_l = []\n    vars.lastact     = \"\"\n    vars.submission  = \"\"\n    vars.lastctx     = \"\"\n    \n    # Reset current save\n    vars.savedir = getcwd()+\"\\\\stories\"\n    \n    # Refresh game screen\n    vars.laststory = None\n    emit('from_server', {'cmd': 'setstoryname', 'data': vars.laststory}, broadcast=True)\n    setgamesaved(True)\n    sendwi()\n    emit('from_server', {'cmd': 'setmemory', 'data': vars.memory}, broadcast=True)\n    emit('from_server', {'cmd': 'setanote', 'data': vars.authornote}, broadcast=True)\n    emit('from_server', {'cmd': 'setanotetemplate', 'data': vars.authornotetemplate}, broadcast=True)\n    setStartState()\n\ndef randomGameRequest(topic, memory=\"\"): \n    if(vars.noai):\n        newGameRequest()\n        vars.memory = memory\n        emit('from_server', {'cmd': 'setmemory', 'data': vars.memory}, broadcast=True)\n        return\n    vars.recentrng = topic\n    vars.recentrngm = memory\n    newGameRequest()\n    setgamesaved(False)\n    _memory = memory\n    if(len(memory) > 0):\n        _memory = memory.rstrip() + \"\\n\\n\"\n    vars.memory      = _memory + \"You generate the following \" + topic + \" story concept :\"\n    vars.lua_koboldbridge.feedback = None\n    actionsubmit(\"\", force_submit=True, force_prompt_gen=True)\n    vars.memory      = memory\n    emit('from_server', {'cmd': 'setmemory', 'data': vars.memory}, broadcast=True)\n\ndef final_startup():\n    # Prevent tokenizer from taking extra time the first time it's used\n    def __preempt_tokenizer():\n        if(\"tokenizer\" not in globals()):\n            return\n        utils.decodenewlines(tokenizer.decode([25678, 559]))\n        tokenizer.encode(utils.encodenewlines(\"eunoia\"))\n    threading.Thread(target=__preempt_tokenizer).start()\n\n    # Load soft prompt specified by the settings file, if applicable\n    if(path.exists(get_config_filename())):\n        file = open(get_config_filename(), \"r\")\n        js   = json.load(file)\n        if(vars.allowsp and \"softprompt\" in js and type(js[\"softprompt\"]) is str and all(q not in js[\"softprompt\"] for q in (\"..\", \":\")) and (len(js[\"softprompt\"]) == 0 or all(js[\"softprompt\"][0] not in q for q in (\"/\", \"\\\\\")))):\n            spRequest(js[\"softprompt\"])\n        else:\n            vars.spfilename = \"\"\n        file.close()\n\n    # Precompile TPU backend if required\n    if(vars.use_colab_tpu or vars.model in (\"TPUMeshTransformerGPTJ\", \"TPUMeshTransformerGPTNeoX\")):\n        soft_tokens = tpumtjgetsofttokens()\n        if(vars.dynamicscan or (not vars.nogenmod and vars.has_genmod)):\n            threading.Thread(\n                target=tpu_mtj_backend.infer_dynamic,\n                args=(np.tile(np.uint32((23403, 727, 20185)), (vars.numseqs, 1)),),\n                kwargs={\n                    \"soft_embeddings\": vars.sp,\n                    \"soft_tokens\": soft_tokens,\n                    \"gen_len\": 1,\n                    \"use_callback\": False,\n                    \"numseqs\": vars.numseqs,\n                    \"excluded_world_info\": list(set() for _ in range(vars.numseqs)),\n                },\n            ).start()\n        else:\n            threading.Thread(\n                target=tpu_mtj_backend.infer_static,\n                args=(np.uint32((23403, 727, 20185)),),\n                kwargs={\n                    \"soft_embeddings\": vars.sp,\n                    \"soft_tokens\": soft_tokens,\n                    \"gen_len\": 1,\n                    \"numseqs\": vars.numseqs,\n                },\n            ).start()\n\n    # Set the initial RNG seed\n    if(vars.seed is not None):\n        if(vars.use_colab_tpu):\n            if(vars.seed_specified):\n                __import__(\"tpu_mtj_backend\").set_rng_seed(vars.seed)\n            else:\n                __import__(\"tpu_mtj_backend\").randomize_rng_seed()\n        else:\n            if(vars.seed_specified):\n                __import__(\"torch\").manual_seed(vars.seed)\n            else:\n                __import__(\"torch\").seed()\n    vars.seed = __import__(\"tpu_mtj_backend\").get_rng_seed() if vars.use_colab_tpu else __import__(\"torch\").initial_seed()\n\ndef send_debug():\n    if vars.debug:\n        debug_info = \"\"\n        try:\n            debug_info = \"{}Seed: {} ({})\\n\".format(debug_info, repr(__import__(\"tpu_mtj_backend\").get_rng_seed() if vars.use_colab_tpu else __import__(\"torch\").initial_seed()), \"specified by user in settings file\" if vars.seed_specified else \"randomly generated\")\n        except:\n            pass\n        try:\n            debug_info = \"{}Newline Mode: {}\\n\".format(debug_info, vars.newlinemode)\n        except:\n            pass\n        try:\n            debug_info = \"{}Action Length: {}\\n\".format(debug_info, vars.actions.get_last_key())\n        except:\n            pass\n        try:\n            debug_info = \"{}Actions Metadata Length: {}\\n\".format(debug_info, max(vars.actions_metadata) if len(vars.actions_metadata) > 0 else 0)\n        except:\n            pass\n        try:\n            debug_info = \"{}Actions: {}\\n\".format(debug_info, [k for k in vars.actions])\n        except:\n            pass\n        try:\n            debug_info = \"{}Actions Metadata: {}\\n\".format(debug_info, [k for k in vars.actions_metadata])\n        except:\n            pass\n        try:\n            debug_info = \"{}Last Action: {}\\n\".format(debug_info, vars.actions[vars.actions.get_last_key()])\n        except:\n            pass\n        try:\n            debug_info = \"{}Last Metadata: {}\\n\".format(debug_info, vars.actions_metadata[max(vars.actions_metadata)])\n        except:\n            pass\n\n        emit('from_server', {'cmd': 'debug_info', 'data': debug_info}, broadcast=True)\n\n#==================================================================#\n# Load file browser for soft prompts\n#==================================================================#\n@socketio.on('show_folder_soft_prompt')\ndef show_folder_soft_prompt(data):\n    file_popup(\"Load Softprompt\", \"./softprompts\", \"\", renameable=True, folder_only=False, editable=False, deleteable=True, jailed=True, item_check=None)\n\n#==================================================================#\n# Load file browser for user scripts\n#==================================================================#\n@socketio.on('show_folder_usersripts')\ndef show_folder_usersripts(data):\n    file_popup(\"Load Softprompt\", \"./userscripts\", \"\", renameable=True, folder_only=False, editable=True, deleteable=True, jailed=True, item_check=None)\n\n\n\n#==================================================================#\n# File Popup options\n#==================================================================#\n\n@socketio.on('upload_file')\ndef upload_file(data):\n    print(\"upload_file {}\".format(data['filename']))\n    print('current_folder' in session)\n    print('popup_jailed_dir' not in session)\n    print(session['popup_jailed_dir'])\n    print(session['current_folder'])    \n    if 'current_folder' in session:\n        path = os.path.abspath(os.path.join(session['current_folder'], data['filename']).replace(\"\\\\\", \"/\")).replace(\"\\\\\", \"/\")\n        print(path)\n        print(os.path.exists(path))\n        if 'popup_jailed_dir' not in session:\n            print(\"Someone is trying to upload a file to your server. Blocked.\")\n        elif session['popup_jailed_dir'] is None:\n            if os.path.exists(path):\n                print(\"popup error\")\n                emit(\"error_popup\", \"The file already exists. Please delete it or rename the file before uploading\", room=\"UI_2\");\n            else:\n                with open(path, \"wb\") as f:\n                    f.write(data['data'])\n                get_files_folders(session['current_folder'])\n                print(\"saved\")\n        elif session['popup_jailed_dir'] in session['current_folder']:\n            if os.path.exists(path):\n                print(\"popup error\")\n                emit(\"error_popup\", \"The file already exists. Please delete it or rename the file before uploading\", room=\"UI_2\");\n            else:\n                with open(path, \"wb\") as f:\n                    f.write(data['data'])\n                get_files_folders(session['current_folder'])\n                print(\"saved\")\n\n@socketio.on('popup_change_folder')\ndef popup_change_folder(data):\n    print(\"Doing popup change folder: {}\".format(data))\n    if 'popup_jailed_dir' not in session:\n        print(\"Someone is trying to get at files in your server. Blocked.\")\n        return\n    if session['popup_jailed_dir'] is None:\n        get_files_folders(data)\n    elif session['popup_jailed_dir'] in data:\n        get_files_folders(data)\n    else:\n        print(\"User is trying to get at files in your server outside the jail. Blocked. Jailed Dir: {}  Requested Dir: {}\".format(session['popup_jailed_dir'], data))\n\n@socketio.on('popup_rename')\ndef popup_rename(data):\n    if 'popup_renameable' not in session:\n        print(\"Someone is trying to rename a file in your server. Blocked.\")\n        return\n    if not session['popup_renameable']:\n        print(\"Someone is trying to rename a file in your server. Blocked.\")\n        return\n    \n    if session['popup_jailed_dir'] is None:\n        os.rename(data['file'], data['new_name'])\n        get_files_folders(os.path.dirname(data['file']))\n    elif session['popup_jailed_dir'] in data:\n        os.rename(data['file'], data['new_name'])\n        get_files_folders(os.path.dirname(data['file']))\n    else:\n        print(\"User is trying to rename files in your server outside the jail. Blocked. Jailed Dir: {}  Requested Dir: {}\".format(session['popup_jailed_dir'], data['file']))\n\n\n@socketio.on('popup_delete')\ndef popup_delete(data):\n    if 'popup_deletable' not in session:\n        print(\"Someone is trying to delete a file in your server. Blocked.\")\n        return\n    if not session['popup_deletable']:\n        print(\"Someone is trying to delete a file in your server. Blocked.\")\n        return\n    \n    if session['popup_jailed_dir'] is None:\n        import shutil\n        if os.path.isdir(data):\n            shutil.rmtree(data)\n        else:\n            os.remove(data)\n        path = os.path.abspath(data).replace(\"\\\\\", \"/\")\n        if path[-1] == \"/\":\n            path = path[:-1]\n        path = \"/\".join(path.split(\"/\")[:-1])\n        get_files_folders(path)\n    elif session['popup_jailed_dir'] in data:\n        import shutil\n        if os.path.isdir(data):\n            shutil.rmtree(data)\n        else:\n            os.remove(data)\n        path = os.path.abspath(data).replace(\"\\\\\", \"/\")\n        if path[-1] == \"/\":\n            path = path[:-1]\n        path = \"/\".join(path.split(\"/\")[:-1])\n        get_files_folders(path)\n    else:\n        print(\"User is trying to delete files in your server outside the jail. Blocked. Jailed Dir: {}  Requested Dir: {}\".format(session['popup_jailed_dir'], data))\n\n@socketio.on('popup_edit')\ndef popup_edit(data):\n    if 'popup_editable' not in session:\n        print(\"Someone is trying to edit a file in your server. Blocked.\")\n        return\n    if not session['popup_editable']:\n        print(\"Someone is trying to edit a file in your server. Blocked.\")\n        return\n    \n    if session['popup_jailed_dir'] is None:\n        emit(\"popup_edit_file\", {\"file\": data, \"text\": open(data, 'r', encoding='utf-8').read()});\n    elif session['popup_jailed_dir'] in data:\n        emit(\"popup_edit_file\", {\"file\": data, \"text\": open(data, 'r', encoding='utf-8').read()});\n    else:\n        print(\"User is trying to delete files in your server outside the jail. Blocked. Jailed Dir: {}  Requested Dir: {}\".format(session['popup_jailed_dir'], data))\n\n@socketio.on('popup_change_file')\ndef popup_change_file(data):\n    if 'popup_editable' not in session:\n        print(\"Someone is trying to edit a file in your server. Blocked.\")\n        return\n    if not session['popup_editable']:\n        print(\"Someone is trying to edit a file in your server. Blocked.\")\n        return\n    \n    if session['popup_jailed_dir'] is None:\n        with open(data['file'], 'w') as f:\n            f.write(data['data'])\n    elif session['popup_jailed_dir'] in data['file']:\n        with open(data['file'], 'w') as f:\n            f.write(data['data'])\n    else:\n        print(\"User is trying to delete files in your server outside the jail. Blocked. Jailed Dir: {}  Requested Dir: {}\".format(session['popup_jailed_dir'], data))\n\ndef file_popup(popup_title, starting_folder, return_event, upload=True, jailed=True, folder_only=True, renameable=False, deleteable=False, editable=False, show_breadcrumbs=True, item_check=None, show_hidden=False):\n    #starting_folder = The folder we're going to get folders and/or items from\n    #return_event = the socketio event that will be emitted when the load button is clicked\n    #jailed = if set to true will look for the session variable jailed_folder and prevent navigation outside of that folder\n    #folder_only = will only show folders, no files\n    #deletable = will show the delete icons/methods.\n    #editable = will show the edit icons/methods\n    #show_breadcrumbs = will show the breadcrumbs at the top of the screen\n    #item_check will call this function to check if the item is valid as a selection if not none. Will pass absolute directory as only argument to function\n    #show_hidden = ... really, you have to ask?\n    if jailed:\n        session['popup_jailed_dir'] = os.path.abspath(starting_folder).replace(\"\\\\\", \"/\")\n    else:\n        session['popup_jailed_dir'] = None\n    session['popup_deletable'] = deleteable\n    session['popup_renameable'] = renameable\n    session['popup_editable'] = editable\n    session['popup_show_hidden'] = show_hidden\n    session['popup_item_check'] = item_check\n    session['popup_folder_only'] = folder_only\n    session['popup_show_breadcrumbs'] = show_breadcrumbs\n    session['upload'] = upload\n    \n    socketio.emit(\"load_popup\", {\"popup_title\": popup_title, \"call_back\": return_event, \"renameable\": renameable, \"deleteable\": deleteable, \"editable\": editable, 'upload': upload}, broadcast=True)\n    \n    get_files_folders(starting_folder)\n    \n    \ndef get_files_folders(starting_folder):\n    import stat\n    session['current_folder'] = os.path.abspath(starting_folder).replace(\"\\\\\", \"/\")\n    item_check = session['popup_item_check']\n    show_breadcrumbs = session['popup_show_breadcrumbs']\n    show_hidden = session['popup_show_hidden']\n    folder_only = session['popup_folder_only']\n    \n    if starting_folder == 'This PC':\n        breadcrumbs = [['This PC', 'This PC']]\n        items = [[\"{}:/\".format(chr(i)), \"{}:\\\\\".format(chr(i))] for i in range(65, 91) if os.path.exists(\"{}:\".format(chr(i)))]\n    else:\n        path = os.path.abspath(starting_folder).replace(\"\\\\\", \"/\")\n        if path[-1] == \"/\":\n            path = path[:-1]\n        breadcrumbs = []\n        for i in range(len(path.split(\"/\"))):\n            breadcrumbs.append([\"/\".join(path.split(\"/\")[:i+1]),\n                                 path.split(\"/\")[i]])\n        if len(breadcrumbs) == 1:\n            breadcrumbs = [[\"{}:/\".format(chr(i)), \"{}:\\\\\".format(chr(i))] for i in range(65, 91) if os.path.exists(\"{}:\".format(chr(i)))]\n        else:\n            if len([[\"{}:/\".format(chr(i)), \"{}:\\\\\".format(chr(i))] for i in range(65, 91) if os.path.exists(\"{}:\".format(chr(i)))]) > 0:\n                breadcrumbs.insert(0, ['This PC', 'This PC'])\n        \n        #if we're jailed, remove the stuff before the jail from the breadcrumbs\n        if session['popup_jailed_dir'] is not None:\n            \n            breadcrumbs = breadcrumbs[len(session['popup_jailed_dir'].split(\"/\")):]\n        \n        folders = []\n        files = []\n        base_path = os.path.abspath(starting_folder).replace(\"\\\\\", \"/\")\n        for item in os.listdir(base_path):\n            item_full_path = os.path.join(base_path, item).replace(\"\\\\\", \"/\")\n            if hasattr(os.stat(item_full_path), \"st_file_attributes\"):\n                hidden = bool(os.stat(item_full_path).st_file_attributes & stat.FILE_ATTRIBUTE_HIDDEN)\n            else:\n                hidden = item[0] == \".\"\n            if item_check is None:\n                valid_selection = True\n            else:\n                valid_selection = item_check(item_full_path)\n                \n            if (show_hidden and hidden) or not hidden:\n                if os.path.isdir(os.path.join(base_path, item)):\n                    folders.append([True, item_full_path, item,  valid_selection])\n                else:\n                    files.append([False, item_full_path, item,  valid_selection])\n        items = folders\n        if not folder_only:\n            items += files\n            \n    socketio.emit(\"popup_items\", items, broadcast=True, include_self=True)\n    if show_breadcrumbs:\n        socketio.emit(\"popup_breadcrumbs\", breadcrumbs, broadcast=True)\n\n\nclass EmptySchema(KoboldSchema):\n    pass\n\nclass BasicTextResultInnerSchema(KoboldSchema):\n    text: str = fields.String(required=True)\n\nclass BasicTextResultSchema(KoboldSchema):\n    result: BasicTextResultInnerSchema = fields.Nested(BasicTextResultInnerSchema)\n\nclass BasicResultInnerSchema(KoboldSchema):\n    result: str = fields.String(required=True)\n\nclass BasicResultSchema(KoboldSchema):\n    result: BasicResultInnerSchema = fields.Nested(BasicResultInnerSchema, required=True)\n\nclass BasicResultsSchema(KoboldSchema):\n    results: BasicResultInnerSchema = fields.List(fields.Nested(BasicResultInnerSchema), required=True)\n\nclass BasicStringSchema(KoboldSchema):\n    value: str = fields.String(required=True)\n\nclass BasicBooleanSchema(KoboldSchema):\n    value: bool = fields.Boolean(required=True)\n\nclass BasicUIDSchema(KoboldSchema):\n    uid: str = fields.Integer(required=True, validate=validate.Range(min=-2147483648, max=2147483647), metadata={\"description\": \"32-bit signed integer unique to this world info entry/folder.\"})\n\nclass BasicErrorSchema(KoboldSchema):\n    msg: str = fields.String(required=True)\n    type: str = fields.String(required=True)\n\nclass StoryEmptyErrorSchema(KoboldSchema):\n    detail: BasicErrorSchema = fields.Nested(BasicErrorSchema, required=True)\n\nclass StoryTooShortErrorSchema(KoboldSchema):\n    detail: BasicErrorSchema = fields.Nested(BasicErrorSchema, required=True)\n\nclass OutOfMemoryErrorSchema(KoboldSchema):\n    detail: BasicErrorSchema = fields.Nested(BasicErrorSchema, required=True)\n\nclass NotFoundErrorSchema(KoboldSchema):\n    detail: BasicErrorSchema = fields.Nested(BasicErrorSchema, required=True)\n\napi_out_of_memory_response = \"\"\"507:\n          description: Out of memory\n          content:\n            application/json:\n              schema: OutOfMemoryErrorSchema\n              examples:\n                gpu.cuda:\n                  value:\n                    detail:\n                      msg: \"KoboldAI ran out of memory: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.97 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch)\"\n                      type: out_of_memory.gpu.cuda\n                gpu.hip:\n                  value:\n                    detail:\n                      msg: \"KoboldAI ran out of memory: HIP out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.97 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch)\"\n                      type: out_of_memory.gpu.hip\n                tpu.hbm:\n                  value:\n                    detail:\n                      msg: \"KoboldAI ran out of memory: Compilation failed: Compilation failure: Ran out of memory in memory space hbm. Used 8.83G of 8.00G hbm. Exceeded hbm capacity by 848.88M.\"\n                      type: out_of_memory.tpu.hbm\n                cpu.default_cpu_allocator:\n                  value:\n                    detail:\n                      msg: \"KoboldAI ran out of memory: DefaultCPUAllocator: not enough memory: you tried to allocate 209715200 bytes.\"\n                      type: out_of_memory.cpu.default_cpu_allocator\n                unknown.unknown:\n                  value:\n                    detail:\n                      msg: \"KoboldAI ran out of memory.\"\n                      type: out_of_memory.unknown.unknown\"\"\"\n\nclass ValidationErrorSchema(KoboldSchema):\n    detail: Dict[str, List[str]] = fields.Dict(keys=fields.String(), values=fields.List(fields.String(), validate=validate.Length(min=1)), required=True)\n\napi_validation_error_response = \"\"\"422:\n          description: Validation error\n          content:\n            application/json:\n              schema: ValidationErrorSchema\"\"\"\n\nclass ServerBusyErrorSchema(KoboldSchema):\n    detail: BasicErrorSchema = fields.Nested(BasicErrorSchema, required=True)\n\napi_server_busy_response = \"\"\"503:\n          description: Server is busy\n          content:\n            application/json:\n              schema: ServerBusyErrorSchema\n              example:\n                detail:\n                  msg: Server is busy; please try again later.\n                  type: service_unavailable\"\"\"\n\nclass NotImplementedErrorSchema(KoboldSchema):\n    detail: BasicErrorSchema = fields.Nested(BasicErrorSchema, required=True)\n\napi_not_implemented_response = \"\"\"501:\n          description: Not implemented\n          content:\n            application/json:\n              schema: NotImplementedErrorSchema\n              example:\n                detail:\n                  msg: API generation is not supported in read-only mode; please load a model and then try again.\n                  type: not_implemented\"\"\"\n\nclass SamplerSettingsSchema(KoboldSchema):\n    rep_pen: Optional[float] = fields.Float(validate=validate.Range(min=1), metadata={\"description\": \"Base repetition penalty value.\"})\n    rep_pen_range: Optional[int] = fields.Integer(validate=validate.Range(min=0), metadata={\"description\": \"Repetition penalty range.\"})\n    rep_pen_slope: Optional[float] = fields.Float(validate=validate.Range(min=0), metadata={\"description\": \"Repetition penalty slope.\"})\n    top_k: Optional[int] = fields.Integer(validate=validate.Range(min=0), metadata={\"description\": \"Top-k sampling value.\"})\n    top_a: Optional[float] = fields.Float(validate=validate.Range(min=0), metadata={\"description\": \"Top-a sampling value.\"})\n    top_p: Optional[float] = fields.Float(validate=validate.Range(min=0, max=1), metadata={\"description\": \"Top-p sampling value.\"})\n    tfs: Optional[float] = fields.Float(validate=validate.Range(min=0, max=1), metadata={\"description\": \"Tail free sampling value.\"})\n    typical: Optional[float] = fields.Float(validate=validate.Range(min=0, max=1), metadata={\"description\": \"Typical sampling value.\"})\n    temperature: Optional[float] = fields.Float(validate=validate.Range(min=0, min_inclusive=False), metadata={\"description\": \"Temperature value.\"})\n\ndef soft_prompt_validator(soft_prompt: str):\n    if len(soft_prompt.strip()) == 0:\n        return\n    if not vars.allowsp:\n        raise ValidationError(\"Cannot use soft prompts with current backend.\")\n    if any(q in soft_prompt for q in (\"/\", \"\\\\\")):\n        return\n    z, _, _, _, _ = fileops.checksp(soft_prompt.strip(), vars.modeldim)\n    if isinstance(z, int):\n        raise ValidationError(\"Must be a valid soft prompt name.\")\n    z.close()\n    return True\n\ndef story_load_validator(name: str):\n    if any(q in name for q in (\"/\", \"\\\\\")):\n        return\n    if len(name.strip()) == 0 or not os.path.isfile(fileops.storypath(name)):\n        raise ValidationError(\"Must be a valid story name.\")\n    return True\n\ndef permutation_validator(lst: list):\n    if any(not isinstance(e, int) for e in lst):\n        return\n    if min(lst) != 0 or max(lst) != len(lst) - 1 or len(set(lst)) != len(lst):\n        raise ValidationError(\"Must be a permutation of the first N non-negative integers, where N is the length of this array\")\n    return True\n\nclass GenerationInputSchema(SamplerSettingsSchema):\n    prompt: str = fields.String(required=True, metadata={\"description\": \"This is the submission.\"})\n    use_memory: bool = fields.Boolean(load_default=False, metadata={\"description\": \"Whether or not to use the memory from the KoboldAI GUI when generating text.\"})\n    use_story: bool = fields.Boolean(load_default=False, metadata={\"description\": \"Whether or not to use the story from the KoboldAI GUI when generating text.\"})\n    use_authors_note: bool = fields.Boolean(load_default=False, metadata={\"description\": \"Whether or not to use the author's note from the KoboldAI GUI when generating text. This has no effect unless `use_story` is also enabled.\"})\n    use_world_info: bool = fields.Boolean(load_default=False, metadata={\"description\": \"Whether or not to use the world info from the KoboldAI GUI when generating text.\"})\n    use_userscripts: bool = fields.Boolean(load_default=False, metadata={\"description\": \"Whether or not to use the userscripts from the KoboldAI GUI when generating text.\"})\n    soft_prompt: Optional[str] = fields.String(metadata={\"description\": \"Soft prompt to use when generating. If set to the empty string or any other string containing no non-whitespace characters, uses no soft prompt.\"}, validate=[soft_prompt_validator, validate.Regexp(r\"^[^/\\\\]*$\")])\n    max_length: int = fields.Integer(validate=validate.Range(min=1, max=512), metadata={\"description\": \"Number of tokens to generate.\"})\n    max_context_length: int = fields.Integer(validate=validate.Range(min=512, max=2048), metadata={\"description\": \"Maximum number of tokens to send to the model.\"})\n    n: int = fields.Integer(validate=validate.Range(min=1, max=5), metadata={\"description\": \"Number of outputs to generate.\"})\n    disable_output_formatting: bool = fields.Boolean(load_default=True, metadata={\"description\": \"When enabled, all output formatting options default to `false` instead of the value in the KoboldAI GUI.\"})\n    frmttriminc: Optional[bool] = fields.Boolean(metadata={\"description\": \"Output formatting option. When enabled, removes some characters from the end of the output such that the output doesn't end in the middle of a sentence. If the output is less than one sentence long, does nothing.\\n\\nIf `disable_output_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.\"})\n    frmtrmblln: Optional[bool] = fields.Boolean(metadata={\"description\": \"Output formatting option. When enabled, replaces all occurrences of two or more consecutive newlines in the output with one newline.\\n\\nIf `disable_output_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.\"})\n    frmtrmspch: Optional[bool] = fields.Boolean(metadata={\"description\": \"Output formatting option. When enabled, removes `#/@%{}+=~|\\^<>` from the output.\\n\\nIf `disable_output_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.\"})\n    singleline: Optional[bool] = fields.Boolean(metadata={\"description\": \"Output formatting option. When enabled, removes everything after the first line of the output, including the newline.\\n\\nIf `disable_output_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.\"})\n    disable_input_formatting: bool = fields.Boolean(load_default=True, metadata={\"description\": \"When enabled, all input formatting options default to `false` instead of the value in the KoboldAI GUI\"})\n    frmtadsnsp: Optional[bool] = fields.Boolean(metadata={\"description\": \"Input formatting option. When enabled, adds a leading space to your input if there is no trailing whitespace at the end of the previous action.\\n\\nIf `disable_input_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.\"})\n    quiet: Optional[bool] = fields.Boolean(metadata={\"description\": \"When enabled, Generated output will not be displayed in the console.\"})\n    sampler_order: Optional[List[int]] = fields.List(fields.Integer(), validate=[validate.Length(min=6), permutation_validator], metadata={\"description\": \"Sampler order to be used. If N is the length of this array, then N must be greater than or equal to 6 and the array must be a permutation of the first N non-negative integers.\"})\n    sampler_seed: Optional[int] = fields.Integer(validate=validate.Range(min=0, max=2**64 - 1), metadata={\"description\": \"RNG seed to use for sampling. If not specified, the global RNG will be used.\"})\n    sampler_full_determinism: Optional[bool] = fields.Boolean(metadata={\"description\": \"If enabled, the generated text will always be the same as long as you use the same RNG seed, input and settings. If disabled, only the *sequence* of generated texts that you get when repeatedly generating text will be the same given the same RNG seed, input and settings.\"})\n\nclass GenerationResultSchema(KoboldSchema):\n    text: str = fields.String(required=True, metadata={\"description\": \"Generated output as plain text.\"})\n\nclass GenerationOutputSchema(KoboldSchema):\n    results: List[GenerationResultSchema] = fields.List(fields.Nested(GenerationResultSchema), required=True, metadata={\"description\": \"Array of generated outputs.\"})\n\nclass StoryNumsChunkSchema(KoboldSchema):\n    num: int = fields.Integer(required=True, metadata={\"description\": \"Guaranteed to not equal the `num` of any other active story chunk. Equals 0 iff this is the first action of the story (the prompt).\"})\n\nclass StoryChunkSchema(StoryNumsChunkSchema, KoboldSchema):\n    text: str = fields.String(required=True, metadata={\"description\": \"The text inside this story chunk.\"})\n\nclass StorySchema(KoboldSchema):\n    results: List[StoryChunkSchema] = fields.List(fields.Nested(StoryChunkSchema), required=True, metadata={\"description\": \"Array of story actions. The array is sorted such that actions closer to the end of this array are closer to the end of the story.\"})\n\nclass BasicBooleanSchema(KoboldSchema):\n    result: bool = fields.Boolean(required=True)\n\nclass StoryNumsSchema(KoboldSchema):\n    results: List[int] = fields.List(fields.Integer(), required=True, metadata={\"description\": \"Array of story action nums. The array is sorted such that actions closer to the end of this array are closer to the end of the story.\"})\n\nclass StoryChunkResultSchema(KoboldSchema):\n    result: StoryChunkSchema = fields.Nested(StoryChunkSchema, required=True)\n\nclass StoryChunkNumSchema(KoboldSchema):\n    value: int = fields.Integer(required=True)\n\nclass StoryChunkTextSchema(KoboldSchema):\n    value: str = fields.String(required=True)\n\nclass StoryChunkSetTextSchema(KoboldSchema):\n    value: str = fields.String(required=True, validate=validate.Regexp(r\"^(.|\\n)*\\S$\"))\n\nclass StoryLoadSchema(KoboldSchema):\n    name: str = fields.String(required=True, validate=[story_load_validator, validate.Regexp(r\"^[^/\\\\]*$\")])\n\nclass StorySaveSchema(KoboldSchema):\n    name: str = fields.String(required=True, validate=validate.Regexp(r\"^(?=.*\\S)(?!.*[/\\\\]).*$\"))\n\nclass WorldInfoEntrySchema(KoboldSchema):\n    uid: int = fields.Integer(required=True, validate=validate.Range(min=-2147483648, max=2147483647), metadata={\"description\": \"32-bit signed integer unique to this world info entry.\"})\n    content: str = fields.String(required=True, metadata={\"description\": \"The \\\"What To Remember\\\" for this entry.\"})\n    key: str = fields.String(required=True, metadata={\"description\": \"Comma-separated list of keys, or of primary keys if selective mode is enabled.\"})\n    keysecondary: str = fields.String(metadata={\"description\": \"Comma-separated list of secondary keys if selective mode is enabled.\"})\n    selective: bool = fields.Boolean(required=True, metadata={\"description\": \"Whether or not selective mode is enabled for this world info entry.\"})\n    constant: bool = fields.Boolean(required=True, metadata={\"description\": \"Whether or not constant mode is enabled for this world info entry.\"})\n    comment: bool = fields.String(required=True, metadata={\"description\": \"The comment/description/title for this world info entry.\"})\n\nclass WorldInfoEntryResultSchema(KoboldSchema):\n    result: WorldInfoEntrySchema = fields.Nested(WorldInfoEntrySchema, required=True)\n\nclass WorldInfoFolderBasicSchema(KoboldSchema):\n    uid: int = fields.Integer(required=True, validate=validate.Range(min=-2147483648, max=2147483647), metadata={\"description\": \"32-bit signed integer unique to this world info folder.\"})\n    name: str = fields.String(required=True, metadata={\"description\": \"Name of this world info folder.\"})\n\nclass WorldInfoFolderSchema(WorldInfoFolderBasicSchema):\n    entries: List[WorldInfoEntrySchema] = fields.List(fields.Nested(WorldInfoEntrySchema), required=True)\n\nclass WorldInfoFolderUIDsSchema(KoboldSchema):\n    uid: int = fields.Integer(required=True, validate=validate.Range(min=-2147483648, max=2147483647), metadata={\"description\": \"32-bit signed integer unique to this world info folder.\"})\n    entries: List[int] = fields.List(fields.Integer(required=True, validate=validate.Range(min=-2147483648, max=2147483647), metadata={\"description\": \"32-bit signed integer unique to this world info entry.\"}), required=True)\n\nclass WorldInfoEntriesSchema(KoboldSchema):\n    entries: List[WorldInfoEntrySchema] = fields.List(fields.Nested(WorldInfoEntrySchema), required=True)\n\nclass WorldInfoFoldersSchema(KoboldSchema):\n    folders: List[WorldInfoFolderBasicSchema] = fields.List(fields.Nested(WorldInfoFolderBasicSchema), required=True)\n\nclass WorldInfoSchema(WorldInfoEntriesSchema):\n    folders: List[WorldInfoFolderSchema] = fields.List(fields.Nested(WorldInfoFolderSchema), required=True)\n\nclass WorldInfoEntriesUIDsSchema(KoboldSchema):\n    entries: List[int] = fields.List(fields.Integer(required=True, validate=validate.Range(min=-2147483648, max=2147483647), metadata={\"description\": \"32-bit signed integer unique to this world info entry.\"}), required=True)\n\nclass WorldInfoFoldersUIDsSchema(KoboldSchema):\n    folders: List[int] = fields.List(fields.Integer(required=True, validate=validate.Range(min=-2147483648, max=2147483647), metadata={\"description\": \"32-bit signed integer unique to this world info folder.\"}), required=True)\n\nclass WorldInfoUIDsSchema(WorldInfoEntriesUIDsSchema):\n    folders: List[WorldInfoFolderSchema] = fields.List(fields.Nested(WorldInfoFolderUIDsSchema), required=True)\n\nclass ModelSelectionSchema(KoboldSchema):\n    model: str = fields.String(required=True, validate=validate.Regexp(r\"^(?!\\s*NeoCustom)(?!\\s*GPT2Custom)(?!\\s*TPUMeshTransformerGPTJ)(?!\\s*TPUMeshTransformerGPTNeoX)(?!\\s*GooseAI)(?!\\s*OAI)(?!\\s*InferKit)(?!\\s*Colab)(?!\\s*API).*$\"), metadata={\"description\": 'Hugging Face model ID, the path to a model folder (relative to the \"models\" folder in the KoboldAI root folder) or \"ReadOnly\" for no model'})\n\ndef _generate_text(body: GenerationInputSchema):\n    if vars.aibusy or vars.genseqs:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Server is busy; please try again later.\",\n            \"type\": \"service_unavailable\",\n        }}), mimetype=\"application/json\", status=503))\n    if vars.use_colab_tpu:\n        import tpu_mtj_backend\n    if hasattr(body, \"sampler_seed\"):\n        # If a seed was specified, we need to save the global RNG state so we\n        # can restore it later\n        old_seed = vars.seed\n        old_rng_state = tpu_mtj_backend.get_rng_state() if vars.use_colab_tpu else torch.get_rng_state()\n        vars.seed = body.sampler_seed\n        # We should try to use a previously saved RNG state with the same seed\n        if body.sampler_seed in vars.rng_states:\n            if vars.use_colab_tpu:\n                tpu_mtj_backend.set_rng_state(vars.rng_states[body.sampler_seed])\n            else:\n                torch.set_rng_state(vars.rng_states[body.sampler_seed])\n        else:\n            if vars.use_colab_tpu:\n                tpu_mtj_backend.set_rng_state(tpu_mtj_backend.new_rng_state(body.sampler_seed))\n            else:\n                torch.manual_seed(body.sampler_seed)\n        vars.rng_states[body.sampler_seed] = tpu_mtj_backend.get_rng_state() if vars.use_colab_tpu else torch.get_rng_state()\n    if hasattr(body, \"sampler_order\"):\n        if len(body.sampler_order) < 7:\n            body.sampler_order = [6] + body.sampler_order\n    # This maps each property of the setting to use when sending the generate idempotently\n    # To the object which typically contains it's value\n    # This allows to set the property only for the API generation, and then revert the setting\n    # To what it was before.\n    mapping = {\n        \"disable_input_formatting\": (\"vars\", \"disable_input_formatting\", None),\n        \"disable_output_formatting\": (\"vars\", \"disable_output_formatting\", None),\n        \"rep_pen\": (\"vars\", \"rep_pen\", None),\n        \"rep_pen_range\": (\"vars\", \"rep_pen_range\", None),\n        \"rep_pen_slope\": (\"vars\", \"rep_pen_slope\", None),\n        \"top_k\": (\"vars\", \"top_k\", None),\n        \"top_a\": (\"vars\", \"top_a\", None),\n        \"top_p\": (\"vars\", \"top_p\", None),\n        \"tfs\": (\"vars\", \"tfs\", None),\n        \"typical\": (\"vars\", \"typical\", None),\n        \"temperature\": (\"vars\", \"temp\", None),\n        \"frmtadsnsp\": (\"vars.formatoptns\", \"@frmtadsnsp\", \"input\"),\n        \"frmttriminc\": (\"vars.formatoptns\", \"@frmttriminc\", \"output\"),\n        \"frmtrmblln\": (\"vars.formatoptns\", \"@frmtrmblln\", \"output\"),\n        \"frmtrmspch\": (\"vars.formatoptns\", \"@frmtrmspch\", \"output\"),\n        \"singleline\": (\"vars.formatoptns\", \"@singleline\", \"output\"),\n        \"max_length\": (\"vars\", \"genamt\", None),\n        \"max_context_length\": (\"vars\", \"max_length\", None),\n        \"n\": (\"vars\", \"numseqs\", None),\n        \"quiet\": (\"vars\", \"quiet\", None),\n        \"sampler_order\": (\"vars\", \"sampler_order\", None),\n        \"sampler_full_determinism\": (\"vars\", \"full_determinism\", None),\n    }\n    saved_settings = {}\n    set_aibusy(1)\n    disable_set_aibusy = vars.disable_set_aibusy\n    vars.disable_set_aibusy = True\n    _standalone = vars.standalone\n    vars.standalone = True\n    show_probs = vars.show_probs\n    vars.show_probs = False\n    output_streaming = vars.output_streaming\n    vars.output_streaming = False\n    for key, entry in mapping.items():\n        obj = {\"vars\": vars, \"vars.formatoptns\": vars.formatoptns}[entry[0]]\n        if entry[2] == \"input\" and vars.disable_input_formatting and not hasattr(body, key):\n            setattr(body, key, False)\n        if entry[2] == \"output\" and vars.disable_output_formatting and not hasattr(body, key):\n            setattr(body, key, False)\n        if getattr(body, key, None) is not None:\n            if entry[1].startswith(\"@\"):\n                saved_settings[key] = obj[entry[1][1:]]\n                obj[entry[1][1:]] = getattr(body, key)\n            else:\n                saved_settings[key] = getattr(obj, entry[1])\n                setattr(obj, entry[1], getattr(body, key))\n    try:\n        if vars.allowsp and getattr(body, \"soft_prompt\", None) is not None:\n            if any(q in body.soft_prompt for q in (\"/\", \"\\\\\")):\n                raise RuntimeError\n            old_spfilename = vars.spfilename\n            spRequest(body.soft_prompt.strip())\n        genout = apiactionsubmit(body.prompt, use_memory=body.use_memory, use_story=body.use_story, use_world_info=body.use_world_info, use_authors_note=body.use_authors_note)\n        output = {\"results\": [{\"text\": txt} for txt in genout]}\n    finally:\n        for key in saved_settings:\n            entry = mapping[key]\n            obj = {\"vars\": vars, \"vars.formatoptns\": vars.formatoptns}[entry[0]]\n            if getattr(body, key, None) is not None:\n                if entry[1].startswith(\"@\"):\n                    if obj[entry[1][1:]] == getattr(body, key):\n                        obj[entry[1][1:]] = saved_settings[key]\n                else:\n                    if getattr(obj, entry[1]) == getattr(body, key):\n                        setattr(obj, entry[1], saved_settings[key])\n        vars.disable_set_aibusy = disable_set_aibusy\n        vars.standalone = _standalone\n        vars.show_probs = show_probs\n        vars.output_streaming = output_streaming\n        if vars.allowsp and getattr(body, \"soft_prompt\", None) is not None:\n            spRequest(old_spfilename)\n        if hasattr(body, \"sampler_seed\"):\n            vars.seed = old_seed\n            if vars.use_colab_tpu:\n                tpu_mtj_backend.set_rng_state(old_rng_state)\n            else:\n                torch.set_rng_state(old_rng_state)\n        set_aibusy(0)\n    return output\n\n\n@api_v1.get(\"/info/version\")\n@api_schema_wrap\ndef get_version():\n    \"\"\"---\n    get:\n      summary: Current API version\n      tags:\n        - info\n      description: |-2\n        Returns the version of the API that you are currently using.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicResultSchema\n              example:\n                result: 1.0.0\n    \"\"\"\n    return {\"result\": api_version}\n\n\n@api_v1.get(\"/info/version/latest\")\n@api_schema_wrap\ndef get_version_latest():\n    \"\"\"---\n    get:\n      summary: Latest API version\n      tags:\n        - info\n      description: |-2\n        Returns the latest API version available.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicResultSchema\n              example:\n                result: 1.0.0\n    \"\"\"\n    return {\"result\": api_versions[-1]}\n\n\n@api_v1.get(\"/info/version/list\")\n@api_schema_wrap\ndef get_version_list():\n    \"\"\"---\n    get:\n      summary: List API versions\n      tags:\n        - info\n      description: |-2\n        Returns a list of available API versions sorted in ascending order.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicResultsSchema\n              example:\n                results:\n                  - 1.0.0\n    \"\"\"\n    return {\"results\": api_versions}\n\n\n@api_v1.post(\"/generate\")\n@api_schema_wrap\ndef post_generate(body: GenerationInputSchema):\n    \"\"\"---\n    post:\n      summary: Generate text\n      tags:\n        - generate\n      description: |-2\n        Generates text given a submission, sampler settings, soft prompt and number of return sequences.\n\n        By default, the story, userscripts, memory, author's note and world info are disabled.\n\n        Unless otherwise specified, optional values default to the values in the KoboldAI GUI.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: GenerationInputSchema\n            example:\n              prompt: |-2\n                Niko the kobold stalked carefully down the alley, his small scaly figure obscured by a dusky cloak that fluttered lightly in the cold winter breeze.\n              top_p: 0.9\n              temperature: 0.5\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: GenerationOutputSchema\n              example:\n                results:\n                  - text: |-2\n                       Holding up his tail to keep it from dragging in the dirty snow that covered the cobblestone, he waited patiently for the butcher to turn his attention from his stall so that he could pilfer his next meal: a tender-looking chicken.\n        {api_validation_error_response}\n        {api_not_implemented_response}\n        {api_server_busy_response}\n        {api_out_of_memory_response}\n    \"\"\"\n    return _generate_text(body)\n\n\n@api_v1.get(\"/model\")\n@api_schema_wrap\ndef get_model():\n    \"\"\"---\n    get:\n      summary: Retrieve the current model string\n      description: |-2\n        Gets the current model string, which is shown in the title of the KoboldAI GUI in parentheses, e.g. \"KoboldAI Client (KoboldAI/fairseq-dense-13B-Nerys-v2)\".\n      tags:\n        - model\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicResultSchema\n              example:\n                result: KoboldAI/fairseq-dense-13B-Nerys-v2\n    \"\"\"\n    return {\"result\": vars.model}\n\n\n@api_v1.put(\"/model\")\n@api_schema_wrap\ndef put_model(body: ModelSelectionSchema):\n    \"\"\"---\n    put:\n      summary: Load a model\n      description: |-2\n        Loads a model given its Hugging Face model ID, the path to a model folder (relative to the \"models\" folder in the KoboldAI root folder) or \"ReadOnly\" for no model.\n      tags:\n        - model\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: ModelSelectionSchema\n            example:\n              model: ReadOnly\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        {api_validation_error_response}\n        {api_server_busy_response}\n    \"\"\"\n    if vars.aibusy or vars.genseqs:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Server is busy; please try again later.\",\n            \"type\": \"service_unavailable\",\n        }}), mimetype=\"application/json\", status=503))\n    set_aibusy(1)\n    old_model = vars.model\n    vars.model = body.model.strip()\n    try:\n        load_model(use_breakmodel_args=True, breakmodel_args_default_to_cpu=True)\n    except Exception as e:\n        vars.model = old_model\n        raise e\n    set_aibusy(0)\n    return {}\n\n\ndef prompt_validator(prompt: str):\n    if len(prompt.strip()) == 0:\n        raise ValidationError(\"String does not match expected pattern.\")\n\nclass SubmissionInputSchema(KoboldSchema):\n    prompt: str = fields.String(required=True, validate=prompt_validator, metadata={\"pattern\": r\"^[\\S\\s]*\\S[\\S\\s]*$\", \"description\": \"This is the submission.\"})\n    disable_input_formatting: bool = fields.Boolean(load_default=True, metadata={\"description\": \"When enabled, disables all input formatting options, overriding their individual enabled/disabled states.\"})\n    frmtadsnsp: Optional[bool] = fields.Boolean(metadata={\"description\": \"Input formatting option. When enabled, adds a leading space to your input if there is no trailing whitespace at the end of the previous action.\"})\n\n@api_v1.post(\"/story/end\")\n@api_schema_wrap\ndef post_story_end(body: SubmissionInputSchema):\n    \"\"\"---\n    post:\n      summary: Add an action to the end of the story\n      tags:\n        - story\n      description: |-2\n        Inserts a single action at the end of the story in the KoboldAI GUI without generating text.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: SubmissionInputSchema\n            example:\n              prompt: |-2\n                 This is some text to put at the end of the story.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        {api_validation_error_response}\n        {api_server_busy_response}\n    \"\"\"\n    if vars.aibusy or vars.genseqs:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Server is busy; please try again later.\",\n            \"type\": \"service_unavailable\",\n        }}), mimetype=\"application/json\", status=503))\n    set_aibusy(1)\n    disable_set_aibusy = vars.disable_set_aibusy\n    vars.disable_set_aibusy = True\n    _standalone = vars.standalone\n    vars.standalone = True\n    numseqs = vars.numseqs\n    vars.numseqs = 1\n    try:\n        actionsubmit(body.prompt, force_submit=True, no_generate=True, ignore_aibusy=True)\n    finally:\n        vars.disable_set_aibusy = disable_set_aibusy\n        vars.standalone = _standalone\n        vars.numseqs = numseqs\n    set_aibusy(0)\n    return {}\n\n\n@api_v1.get(\"/story/end\")\n@api_schema_wrap\ndef get_story_end():\n    \"\"\"---\n    get:\n      summary: Retrieve the last action of the story\n      tags:\n        - story\n      description: |-2\n        Returns the last action of the story in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: StoryChunkResultSchema\n        510:\n          description: Story is empty\n          content:\n            application/json:\n              schema: StoryEmptyErrorSchema\n              example:\n                detail:\n                  msg: Could not retrieve the last action of the story because the story is empty.\n                  type: story_empty\n    \"\"\"\n    if not vars.gamestarted:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Could not retrieve the last action of the story because the story is empty.\",\n            \"type\": \"story_empty\",\n        }}), mimetype=\"application/json\", status=510))\n    if len(vars.actions) == 0:\n        return {\"result\": {\"text\": vars.prompt, \"num\": 0}}\n    return {\"result\": {\"text\": vars.actions[vars.actions.get_last_key()], \"num\": vars.actions.get_last_key() + 1}}\n\n\n@api_v1.get(\"/story/end/num\")\n@api_schema_wrap\ndef get_story_end_num():\n    \"\"\"---\n    get:\n      summary: Retrieve the num of the last action of the story\n      tags:\n        - story\n      description: |-2\n        Returns the `num` of the last action of the story in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: StoryChunkNumSchema\n        510:\n          description: Story is empty\n          content:\n            application/json:\n              schema: StoryEmptyErrorSchema\n              example:\n                detail:\n                  msg: Could not retrieve the last action of the story because the story is empty.\n                  type: story_empty\n    \"\"\"\n    if not vars.gamestarted:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Could not retrieve the last action of the story because the story is empty.\",\n            \"type\": \"story_empty\",\n        }}), mimetype=\"application/json\", status=510))\n    if len(vars.actions) == 0:\n        return {\"result\": {\"text\": 0}}\n    return {\"result\": {\"text\": vars.actions.get_last_key() + 1}}\n\n\n@api_v1.get(\"/story/end/text\")\n@api_schema_wrap\ndef get_story_end_text():\n    \"\"\"---\n    get:\n      summary: Retrieve the text of the last action of the story\n      tags:\n        - story\n      description: |-2\n        Returns the text of the last action of the story in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: StoryChunkTextSchema\n        510:\n          description: Story is empty\n          content:\n            application/json:\n              schema: StoryEmptyErrorSchema\n              example:\n                detail:\n                  msg: Could not retrieve the last action of the story because the story is empty.\n                  type: story_empty\n    \"\"\"\n    if not vars.gamestarted:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Could not retrieve the last action of the story because the story is empty.\",\n            \"type\": \"story_empty\",\n        }}), mimetype=\"application/json\", status=510))\n    if len(vars.actions) == 0:\n        return {\"result\": {\"text\": vars.prompt}}\n    return {\"result\": {\"text\": vars.actions[vars.actions.get_last_key()]}}\n\n\n@api_v1.put(\"/story/end/text\")\n@api_schema_wrap\ndef put_story_end_text(body: StoryChunkSetTextSchema):\n    \"\"\"---\n    put:\n      summary: Set the text of the last action of the story\n      tags:\n        - story\n      description: |-2\n        Sets the text of the last action of the story in the KoboldAI GUI to the desired value.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: StoryChunkSetTextSchema\n            example:\n              value: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        510:\n          description: Story is empty\n          content:\n            application/json:\n              schema: StoryEmptyErrorSchema\n              example:\n                detail:\n                  msg: Could not retrieve the last action of the story because the story is empty.\n                  type: story_empty\n        {api_validation_error_response}\n    \"\"\"\n    if not vars.gamestarted:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Could not retrieve the last action of the story because the story is empty.\",\n            \"type\": \"story_empty\",\n        }}), mimetype=\"application/json\", status=510))\n    value = body.value.rstrip()\n    if len(vars.actions) == 0:\n        inlineedit(0, value)\n    else:\n        inlineedit(vars.actions.get_last_key() + 1, value)\n    return {}\n\n\n@api_v1.post(\"/story/end/delete\")\n@api_schema_wrap\ndef post_story_end_delete(body: EmptySchema):\n    \"\"\"---\n    post:\n      summary: Remove the last action of the story\n      tags:\n        - story\n      description: |-2\n        Removes the last action of the story in the KoboldAI GUI.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: EmptySchema\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        510:\n          description: Story too short\n          content:\n            application/json:\n              schema: StoryTooShortErrorSchema\n              example:\n                detail:\n                  msg: Could not delete the last action of the story because the number of actions in the story is less than or equal to 1.\n                  type: story_too_short\n        {api_validation_error_response}\n        {api_server_busy_response}\n    \"\"\"\n    if vars.aibusy or vars.genseqs:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Server is busy; please try again later.\",\n            \"type\": \"service_unavailable\",\n        }}), mimetype=\"application/json\", status=503))\n    if not vars.gamestarted or not len(vars.actions):\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Could not delete the last action of the story because the number of actions in the story is less than or equal to 1.\",\n            \"type\": \"story_too_short\",\n        }}), mimetype=\"application/json\", status=510))\n    actionback()\n    return {}\n\n\n@api_v1.get(\"/story\")\n@api_schema_wrap\ndef get_story():\n    \"\"\"---\n    get:\n      summary: Retrieve the entire story\n      tags:\n        - story\n      description: |-2\n        Returns the entire story currently shown in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: StorySchema\n    \"\"\"\n    chunks = []\n    if vars.gamestarted:\n        chunks.append({\"num\": 0, \"text\": vars.prompt})\n    for num, action in vars.actions.items():\n        chunks.append({\"num\": num + 1, \"text\": action})\n    return {\"results\": chunks}\n\n\n@api_v1.get(\"/story/nums\")\n@api_schema_wrap\ndef get_story_nums():\n    \"\"\"---\n    get:\n      summary: Retrieve a list of the nums of the chunks in the current story\n      tags:\n        - story\n      description: |-2\n        Returns the `num`s of the story chunks currently shown in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: StorySchema\n    \"\"\"\n    chunks = []\n    if vars.gamestarted:\n        chunks.append(0)\n    for num in vars.actions.keys():\n        chunks.append(num + 1)\n    return {\"results\": chunks}\n\n\n@api_v1.get(\"/story/nums/<int(signed=True):num>\")\n@api_schema_wrap\ndef get_story_nums_num(num: int):\n    \"\"\"---\n    get:\n      summary: Determine whether or not there is a story chunk with the given num\n      tags:\n        - story\n      parameters:\n        - name: num\n          in: path\n          description: |-2\n            `num` of the desired story chunk.\n          schema:\n            type: integer\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicBooleanSchema\n    \"\"\"\n    if num == 0:\n        return {\"result\": vars.gamestarted}\n    return {\"result\": num - 1 in vars.actions}\n\n\n@api_v1.get(\"/story/<int(signed=True):num>\")\n@api_schema_wrap\ndef get_story_num(num: int):\n    \"\"\"---\n    get:\n      summary: Retrieve a story chunk\n      tags:\n        - story\n      description: |-2\n        Returns information about a story chunk given its `num`.\n      parameters:\n        - name: num\n          in: path\n          description: |-2\n            `num` of the desired story chunk.\n          schema:\n            type: integer\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: StoryChunkResultSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No chunk with the given num exists.\n                  type: key_error\n    \"\"\"\n    if num == 0:\n        if not vars.gamestarted:\n            abort(Response(json.dumps({\"detail\": {\n                \"msg\": \"No chunk with the given num exists.\",\n                \"type\": \"key_error\",\n            }}), mimetype=\"application/json\", status=404))\n        return {\"result\": {\"text\": vars.prompt, \"num\": num}}\n    if num - 1 not in vars.actions:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No chunk with the given num exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"result\": {\"text\": vars.actions[num - 1], \"num\": num}}\n\n\n@api_v1.get(\"/story/<int(signed=True):num>/text\")\n@api_schema_wrap\ndef get_story_num_text(num: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the text of a story chunk\n      tags:\n        - story\n      description: |-2\n        Returns the text inside a story chunk given its `num`.\n      parameters:\n        - name: num\n          in: path\n          description: |-2\n            `num` of the desired story chunk.\n          schema:\n            type: integer\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: StoryChunkTextSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No chunk with the given num exists.\n                  type: key_error\n    \"\"\"\n    if num == 0:\n        if not vars.gamestarted:\n            abort(Response(json.dumps({\"detail\": {\n                \"msg\": \"No chunk with the given num exists.\",\n                \"type\": \"key_error\",\n            }}), mimetype=\"application/json\", status=404))\n        return {\"value\": vars.prompt}\n    if num - 1 not in vars.actions:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No chunk with the given num exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"value\": vars.actions[num - 1]}\n\n\n@api_v1.put(\"/story/<int(signed=True):num>/text\")\n@api_schema_wrap\ndef put_story_num_text(body: StoryChunkSetTextSchema, num: int):\n    \"\"\"---\n    put:\n      summary: Set the text of a story chunk\n      tags:\n        - story\n      description: |-2\n        Sets the text inside a story chunk given its `num`.\n      parameters:\n        - name: num\n          in: path\n          description: |-2\n            `num` of the desired story chunk.\n          schema:\n            type: integer\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: StoryChunkSetTextSchema\n            example:\n              value: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No chunk with the given num exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if num == 0:\n        if not vars.gamestarted:\n            abort(Response(json.dumps({\"detail\": {\n                \"msg\": \"No chunk with the given num exists.\",\n                \"type\": \"key_error\",\n            }}), mimetype=\"application/json\", status=404))\n        inlineedit(0, body.value.rstrip())\n        return {}\n    if num - 1 not in vars.actions:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No chunk with the given num exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    inlineedit(num, body.value.rstrip())\n    return {}\n\n\n@api_v1.delete(\"/story/<int(signed=True):num>\")\n@api_schema_wrap\ndef post_story_num_delete(num: int):\n    \"\"\"---\n    delete:\n      summary: Remove a story chunk\n      tags:\n        - story\n      description: |-2\n        Removes a story chunk from the story in the KoboldAI GUI given its `num`. Cannot be used to delete the first action (the prompt).\n      parameters:\n        - name: num\n          in: path\n          description: |-2\n            `num` of the desired story chunk. Must be larger than or equal to 1.\n          schema:\n            type: integer\n            minimum: 1\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No chunk with the given num exists.\n                  type: key_error\n        {api_server_busy_response}\n    \"\"\"\n    if num < 1:\n        abort(Response(json.dumps({\"detail\": {\n            \"num\": [\"Must be greater than or equal to 1.\"],\n        }}), mimetype=\"application/json\", status=422))\n    if num - 1 not in vars.actions:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No chunk with the given num exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    if vars.aibusy or vars.genseqs:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Server is busy; please try again later.\",\n            \"type\": \"service_unavailable\",\n        }}), mimetype=\"application/json\", status=503))\n    inlinedelete(num)\n    return {}\n\n\n@api_v1.delete(\"/story\")\n@api_schema_wrap\ndef delete_story():\n    \"\"\"---\n    delete:\n      summary: Clear the story\n      tags:\n        - story\n      description: |-2\n        Starts a new blank story.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        {api_server_busy_response}\n    \"\"\"\n    if vars.aibusy or vars.genseqs:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Server is busy; please try again later.\",\n            \"type\": \"service_unavailable\",\n        }}), mimetype=\"application/json\", status=503))\n    newGameRequest()\n    return {}\n\n\n@api_v1.put(\"/story/load\")\n@api_schema_wrap\ndef put_story_load(body: StoryLoadSchema):\n    \"\"\"---\n    put:\n      summary: Load a story\n      tags:\n        - story\n      description: |-2\n        Loads a story given its filename (without the .json).\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: StoryLoadSchema\n            example:\n              name: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        {api_validation_error_response}\n        {api_server_busy_response}\n    \"\"\"\n    if vars.aibusy or vars.genseqs:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"Server is busy; please try again later.\",\n            \"type\": \"service_unavailable\",\n        }}), mimetype=\"application/json\", status=503))\n    loadRequest(fileops.storypath(body.name.strip()))\n    return {}\n\n\n@api_v1.put(\"/story/save\")\n@api_schema_wrap\ndef put_story_save(body: StorySaveSchema):\n    \"\"\"---\n    put:\n      summary: Save the current story\n      tags:\n        - story\n      description: |-2\n        Saves the current story given its destination filename (without the .json).\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: StorySaveSchema\n            example:\n              name: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        {api_validation_error_response}\n    \"\"\"\n    saveRequest(fileops.storypath(body.name.strip()))\n    return {}\n\n\n@api_v1.get(\"/world_info\")\n@api_schema_wrap\ndef get_world_info():\n    \"\"\"---\n    get:\n      summary: Retrieve all world info entries\n      tags:\n        - world_info\n      description: |-2\n        Returns all world info entries currently shown in the KoboldAI GUI.\n\n        The `folders` are sorted in the same order as they are in the GUI and the `entries` within the folders and within the parent `result` object are all sorted in the same order as they are in their respective parts of the GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoSchema\n    \"\"\"\n    folders = []\n    entries = []\n    ln = len(vars.worldinfo)\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    folder: Optional[list] = None\n    if ln:\n        last_folder = ...\n        for wi in vars.worldinfo_i:\n            if wi[\"folder\"] != last_folder:\n                folder = []\n                if wi[\"folder\"] is not None:\n                    folders.append({\"uid\": wi[\"folder\"], \"name\": vars.wifolders_d[wi[\"folder\"]][\"name\"], \"entries\": folder})\n                last_folder = wi[\"folder\"]\n            (folder if wi[\"folder\"] is not None else entries).append({k: v for k, v in wi.items() if k not in (\"init\", \"folder\", \"num\") and (wi[\"selective\"] or k != \"keysecondary\")})\n    return {\"folders\": folders, \"entries\": entries}\n\n@api_v1.get(\"/world_info/uids\")\n@api_schema_wrap\ndef get_world_info_uids():\n    \"\"\"---\n    get:\n      summary: Retrieve the UIDs of all world info entries\n      tags:\n        - world_info\n      description: |-2\n        Returns in a similar format as GET /world_info except only the `uid`s are returned.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoUIDsSchema\n    \"\"\"\n    folders = []\n    entries = []\n    ln = len(vars.worldinfo)\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    folder: Optional[list] = None\n    if ln:\n        last_folder = ...\n        for wi in vars.worldinfo_i:\n            if wi[\"folder\"] != last_folder:\n                folder = []\n                if wi[\"folder\"] is not None:\n                    folders.append({\"uid\": wi[\"folder\"], \"entries\": folder})\n                last_folder = wi[\"folder\"]\n            (folder if wi[\"folder\"] is not None else entries).append(wi[\"uid\"])\n    return {\"folders\": folders, \"entries\": entries}\n\n\n@api_v1.get(\"/world_info/uids/<int(signed=True):uid>\")\n@api_schema_wrap\ndef get_world_info_uids_uid(uid: int):\n    \"\"\"---\n    get:\n      summary: Determine whether or not there is a world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicBooleanSchema\n    \"\"\"\n    return {\"result\": uid in vars.worldinfo_u and vars.worldinfo_u[uid][\"init\"]}\n\n\n@api_v1.get(\"/world_info/folders\")\n@api_schema_wrap\ndef get_world_info_folders():\n    \"\"\"---\n    get:\n      summary: Retrieve all world info folders\n      tags:\n        - world_info\n      description: |-2\n        Returns details about all world info folders currently shown in the KoboldAI GUI.\n\n        The `folders` are sorted in the same order as they are in the GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoFoldersSchema\n    \"\"\"\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    return {\"folders\": [{\"uid\": folder, **{k: v for k, v in vars.wifolders_d[folder].items() if k != \"collapsed\"}} for folder in vars.wifolders_l]}\n\n\n@api_v1.get(\"/world_info/folders/uids\")\n@api_schema_wrap\ndef get_world_info_folders_uids():\n    \"\"\"---\n    get:\n      summary: Retrieve the UIDs all world info folders\n      tags:\n        - world_info\n      description: |-2\n        Returns the `uid`s of all world info folders currently shown in the KoboldAI GUI.\n\n        The `folders` are sorted in the same order as they are in the GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoFoldersUIDsSchema\n    \"\"\"\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    return {\"folders\": vars.wifolders_l}\n\n\n@api_v1.get(\"/world_info/folders/none\")\n@api_schema_wrap\ndef get_world_info_folders_none():\n    \"\"\"---\n    get:\n      summary: Retrieve all world info entries not in a folder\n      tags:\n        - world_info\n      description: |-2\n        Returns all world info entries that are not in a world info folder.\n\n        The `entries` are sorted in the same order as they are in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoEntriesSchema\n    \"\"\"\n    entries = []\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    for wi in reversed(vars.worldinfo_i):\n        if wi[\"folder\"] is not None:\n            break\n        entries.append({k: v for k, v in wi.items() if k not in (\"init\", \"folder\", \"num\") and (wi[\"selective\"] or k != \"keysecondary\")})\n    return {\"entries\": list(reversed(entries))}\n\n\n@api_v1.get(\"/world_info/folders/none/uids\")\n@api_schema_wrap\ndef get_world_info_folders_none_uids():\n    \"\"\"---\n    get:\n      summary: Retrieve the UIDs of all world info entries not in a folder\n      tags:\n        - world_info\n      description: |-2\n        Returns the `uid`s of all world info entries that are not in a world info folder.\n\n        The `entries` are sorted in the same order as they are in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoEntriesUIDsSchema\n    \"\"\"\n    entries = []\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    for wi in reversed(vars.worldinfo_i):\n        if wi[\"folder\"] is not None:\n            break\n        entries.append(wi[\"uid\"])\n    return {\"entries\": list(reversed(entries))}\n\n\n@api_v1.get(\"/world_info/folders/none/uids/<int(signed=True):uid>\")\n@api_schema_wrap\ndef get_world_info_folders_none_uids_uid(uid: int):\n    \"\"\"---\n    get:\n      summary: Determine whether or not there is a world info entry with the given UID that is not in a world info folder\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicBooleanSchema\n    \"\"\"\n    return {\"result\": uid in vars.worldinfo_u and vars.worldinfo_u[uid][\"folder\"] is None and vars.worldinfo_u[uid][\"init\"]}\n\n\n@api_v1.get(\"/world_info/folders/<int(signed=True):uid>\")\n@api_schema_wrap\ndef get_world_info_folders_uid(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve all world info entries in the given folder\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info folder.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      description: |-2\n        Returns all world info entries that are in the world info folder with the given `uid`.\n\n        The `entries` are sorted in the same order as they are in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoEntriesSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info folder with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.wifolders_d:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info folder with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    entries = []\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    for wi in vars.wifolders_u[uid]:\n        if wi[\"init\"]:\n            entries.append({k: v for k, v in wi.items() if k not in (\"init\", \"folder\", \"num\") and (wi[\"selective\"] or k != \"keysecondary\")})\n    return {\"entries\": entries}\n\n\n@api_v1.get(\"/world_info/folders/<int(signed=True):uid>/uids\")\n@api_schema_wrap\ndef get_world_info_folders_uid_uids(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the UIDs of all world info entries in the given folder\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info folder.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      description: |-2\n        Returns the `uid`s of all world info entries that are in the world info folder with the given `uid`.\n\n        The `entries` are sorted in the same order as they are in the KoboldAI GUI.\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoEntriesUIDsSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info folder with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.wifolders_d:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info folder with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    entries = []\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    for wi in vars.wifolders_u[uid]:\n        if wi[\"init\"]:\n            entries.append(wi[\"uid\"])\n    return {\"entries\": entries}\n\n\n@api_v1.get(\"/world_info/folders/<int(signed=True):folder_uid>/uids/<int(signed=True):entry_uid>\")\n@api_schema_wrap\ndef get_world_info_folders_folder_uid_uids_entry_uid(folder_uid: int, entry_uid: int):\n    \"\"\"---\n    get:\n      summary: Determine whether or not there is a world info entry with the given UID in the world info folder with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: folder_uid\n          in: path\n          description: |-2\n            `uid` of the desired world info folder.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n        - name: entry_uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicBooleanSchema\n    \"\"\"\n    return {\"result\": entry_uid in vars.worldinfo_u and vars.worldinfo_u[entry_uid][\"folder\"] == folder_uid and vars.worldinfo_u[entry_uid][\"init\"]}\n\n\n@api_v1.get(\"/world_info/folders/<int(signed=True):uid>/name\")\n@api_schema_wrap\ndef get_world_info_folders_uid_name(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the name of the world info folder with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info folder.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicStringSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info folder with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.wifolders_d:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info folder with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"value\": vars.wifolders_d[uid][\"name\"]}\n\n\n@api_v1.put(\"/world_info/folders/<int(signed=True):uid>/name\")\n@api_schema_wrap\ndef put_world_info_folders_uid_name(body: BasicStringSchema, uid: int):\n    \"\"\"---\n    put:\n      summary: Set the name of the world info folder with the given UID to the specified value\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info folder.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: BasicStringSchema\n            example:\n              value: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info folder with the given uid exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if uid not in vars.wifolders_d:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info folder with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    vars.wifolders_d[uid][\"name\"] = body.value\n    setgamesaved(False)\n    return {}\n\n\n@api_v1.get(\"/world_info/<int(signed=True):uid>\")\n@api_schema_wrap\ndef get_world_info_uid(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve information about the world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: WorldInfoEntrySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    wi = vars.worldinfo_u[uid]\n    return {k: v for k, v in wi.items() if k not in (\"init\", \"folder\", \"num\") and (wi[\"selective\"] or k != \"keysecondary\")}\n\n\n@api_v1.get(\"/world_info/<int(signed=True):uid>/comment\")\n@api_schema_wrap\ndef get_world_info_uid_comment(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the comment of the world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicStringSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"value\": vars.worldinfo_u[uid][\"comment\"]}\n\n\n@api_v1.put(\"/world_info/<int(signed=True):uid>/comment\")\n@api_schema_wrap\ndef put_world_info_uid_comment(body: BasicStringSchema, uid: int):\n    \"\"\"---\n    put:\n      summary: Set the comment of the world info entry with the given UID to the specified value\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: BasicStringSchema\n            example:\n              value: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    vars.worldinfo_u[uid][\"comment\"] = body.value\n    setgamesaved(False)\n    return {}\n\n\n@api_v1.get(\"/world_info/<int(signed=True):uid>/content\")\n@api_schema_wrap\ndef get_world_info_uid_content(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the content of the world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicStringSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"value\": vars.worldinfo_u[uid][\"content\"]}\n\n\n@api_v1.put(\"/world_info/<int(signed=True):uid>/content\")\n@api_schema_wrap\ndef put_world_info_uid_content(body: BasicStringSchema, uid: int):\n    \"\"\"---\n    put:\n      summary: Set the content of the world info entry with the given UID to the specified value\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: BasicStringSchema\n            example:\n              value: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    vars.worldinfo_u[uid][\"content\"] = body.value\n    setgamesaved(False)\n    return {}\n\n\n@api_v1.get(\"/world_info/<int(signed=True):uid>/key\")\n@api_schema_wrap\ndef get_world_info_uid_key(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the keys or primary keys of the world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicStringSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"value\": vars.worldinfo_u[uid][\"key\"]}\n\n\n@api_v1.put(\"/world_info/<int(signed=True):uid>/key\")\n@api_schema_wrap\ndef put_world_info_uid_key(body: BasicStringSchema, uid: int):\n    \"\"\"---\n    put:\n      summary: Set the keys or primary keys of the world info entry with the given UID to the specified value\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: BasicStringSchema\n            example:\n              value: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    vars.worldinfo_u[uid][\"key\"] = body.value\n    setgamesaved(False)\n    return {}\n\n\n@api_v1.get(\"/world_info/<int(signed=True):uid>/keysecondary\")\n@api_schema_wrap\ndef get_world_info_uid_keysecondary(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the secondary keys of the world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicStringSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"value\": vars.worldinfo_u[uid][\"keysecondary\"]}\n\n\n@api_v1.put(\"/world_info/<int(signed=True):uid>/keysecondary\")\n@api_schema_wrap\ndef put_world_info_uid_keysecondary(body: BasicStringSchema, uid: int):\n    \"\"\"---\n    put:\n      summary: Set the secondary keys of the world info entry with the given UID to the specified value\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: BasicStringSchema\n            example:\n              value: string\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    vars.worldinfo_u[uid][\"keysecondary\"] = body.value\n    setgamesaved(False)\n    return {}\n\n\n@api_v1.get(\"/world_info/<int(signed=True):uid>/selective\")\n@api_schema_wrap\ndef get_world_info_uid_selective(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the selective mode state of the world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicBooleanSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"value\": vars.worldinfo_u[uid][\"selective\"]}\n\n\n@api_v1.put(\"/world_info/<int(signed=True):uid>/selective\")\n@api_schema_wrap\ndef put_world_info_uid_selective(body: BasicBooleanSchema, uid: int):\n    \"\"\"---\n    put:\n      summary: Set the selective mode state of the world info entry with the given UID to the specified value\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: BasicBooleanSchema\n            example:\n              value: true\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    vars.worldinfo_u[uid][\"selective\"] = body.value\n    setgamesaved(False)\n    return {}\n\n\n@api_v1.get(\"/world_info/<int(signed=True):uid>/constant\")\n@api_schema_wrap\ndef get_world_info_uid_constant(uid: int):\n    \"\"\"---\n    get:\n      summary: Retrieve the constant mode state of the world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicBooleanSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    return {\"value\": vars.worldinfo_u[uid][\"constant\"]}\n\n\n@api_v1.put(\"/world_info/<int(signed=True):uid>/constant\")\n@api_schema_wrap\ndef put_world_info_uid_constant(body: BasicBooleanSchema, uid: int):\n    \"\"\"---\n    put:\n      summary: Set the constant mode state of the world info entry with the given UID to the specified value\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: BasicBooleanSchema\n            example:\n              value: true\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    vars.worldinfo_u[uid][\"constant\"] = body.value\n    setgamesaved(False)\n    return {}\n\n\n@api_v1.post(\"/world_info/folders/none\")\n@api_schema_wrap\ndef post_world_info_folders_none(body: EmptySchema):\n    \"\"\"---\n    post:\n      summary: Create a new world info entry outside of a world info folder, at the end of the world info\n      tags:\n        - world_info\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: EmptySchema\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicUIDSchema\n        {api_validation_error_response}\n    \"\"\"\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    setgamesaved(False)\n    emit('from_server', {'cmd': 'wiexpand', 'data': vars.worldinfo[-1][\"num\"]}, broadcast=True)\n    vars.worldinfo[-1][\"init\"] = True\n    addwiitem(folder_uid=None)\n    return {\"uid\": vars.worldinfo[-2][\"uid\"]}\n\n\n@api_v1.post(\"/world_info/folders/<int(signed=True):uid>\")\n@api_schema_wrap\ndef post_world_info_folders_uid(body: EmptySchema, uid: int):\n    \"\"\"---\n    post:\n      summary: Create a new world info entry at the end of the world info folder with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info folder.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: EmptySchema\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicUIDSchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info folder with the given uid exists.\n                  type: key_error\n        {api_validation_error_response}\n    \"\"\"\n    if uid not in vars.wifolders_d:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info folder with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    stablesortwi()\n    vars.worldinfo_i = [wi for wi in vars.worldinfo if wi[\"init\"]]\n    setgamesaved(False)\n    emit('from_server', {'cmd': 'wiexpand', 'data': vars.wifolders_u[uid][-1][\"num\"]}, broadcast=True)\n    vars.wifolders_u[uid][-1][\"init\"] = True\n    addwiitem(folder_uid=uid)\n    return {\"uid\": vars.wifolders_u[uid][-2][\"uid\"]}\n\n\n@api_v1.delete(\"/world_info/<int(signed=True):uid>\")\n@api_schema_wrap\ndef delete_world_info_uid(uid: int):\n    \"\"\"---\n    delete:\n      summary: Delete the world info entry with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info entry.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info entry with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.worldinfo_u:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info entry with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    deletewi(uid)\n    return {}\n\n\n@api_v1.post(\"/world_info/folders\")\n@api_schema_wrap\ndef post_world_info_folders(body: EmptySchema):\n    \"\"\"---\n    post:\n      summary: Create a new world info folder at the end of the world info\n      tags:\n        - world_info\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: EmptySchema\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: BasicUIDSchema\n        {api_validation_error_response}\n    \"\"\"\n    addwifolder()\n    return {\"uid\": vars.wifolders_l[-1]}\n\n\n@api_v1.delete(\"/world_info/folders/<int(signed=True):uid>\")\n@api_schema_wrap\ndef delete_world_info_folders_uid(uid: int):\n    \"\"\"---\n    delete:\n      summary: Delete the world info folder with the given UID\n      tags:\n        - world_info\n      parameters:\n        - name: uid\n          in: path\n          description: |-2\n            `uid` of the desired world info folder.\n          schema:\n            type: integer\n            minimum: -2147483648\n            maximum: 2147483647\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        404:\n          description: Not found\n          content:\n            application/json:\n              schema: NotFoundErrorSchema\n              example:\n                detail:\n                  msg: No world info folders with the given uid exists.\n                  type: key_error\n    \"\"\"\n    if uid not in vars.wifolders_d:\n        abort(Response(json.dumps({\"detail\": {\n            \"msg\": \"No world info folder with the given uid exists.\",\n            \"type\": \"key_error\",\n        }}), mimetype=\"application/json\", status=404))\n    deletewifolder(uid)\n    return {}\n\n\ndef _make_f_get(obj, _var_name, _name, _schema, _example_yaml_value):\n    def f_get():\n        \"\"\"---\n    get:\n      summary: Retrieve the current {} setting value\n      tags:\n        - config\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: {}\n              example:\n                value: {}\n        \"\"\"\n        _obj = {\"vars\": vars, \"vars.formatoptns\": vars.formatoptns}[obj]\n        if _var_name.startswith(\"@\"):\n            return {\"value\": _obj[_var_name[1:]]}\n        else:\n            return {\"value\": getattr(_obj, _var_name)}\n    f_get.__doc__ = f_get.__doc__.format(_name, _schema, _example_yaml_value)\n    return f_get\n\ndef _make_f_put(schema_class: Type[KoboldSchema], obj, _var_name, _name, _schema, _example_yaml_value):\n    def f_put(body: schema_class):\n        \"\"\"---\n    put:\n      summary: Set {} setting to specified value\n      tags:\n        - config\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: {}\n            example:\n              value: {}\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        {api_validation_error_response}\n        \"\"\"\n        _obj = {\"vars\": vars, \"vars.formatoptns\": vars.formatoptns}[obj]\n        if _var_name.startswith(\"@\"):\n            _obj[_var_name[1:]] = body.value\n        else:\n            setattr(_obj, _var_name, body.value)\n        settingschanged()\n        refresh_settings()\n        return {}\n    f_put.__doc__ = f_put.__doc__.format(_name, _schema, _example_yaml_value, api_validation_error_response=api_validation_error_response)\n    return f_put\n\ndef create_config_endpoint(method=\"GET\", schema=\"MemorySchema\"):\n    _name = globals()[schema].KoboldMeta.name\n    _var_name = globals()[schema].KoboldMeta.var_name\n    _route_name = globals()[schema].KoboldMeta.route_name\n    _obj = globals()[schema].KoboldMeta.obj\n    _example_yaml_value = globals()[schema].KoboldMeta.example_yaml_value\n    _schema = schema\n    f = _make_f_get(_obj, _var_name, _name, _schema, _example_yaml_value) if method == \"GET\" else _make_f_put(globals()[schema], _obj, _var_name, _name, _schema, _example_yaml_value)\n    f.__name__ = f\"{method.lower()}_config_{_name}\"\n    f = api_schema_wrap(f)\n    for api in (api_v1,):\n        f = api.route(f\"/config/{_route_name}\", methods=[method])(f)\n\nclass SoftPromptSettingSchema(KoboldSchema):\n    value: str = fields.String(required=True, validate=[soft_prompt_validator, validate.Regexp(r\"^[^/\\\\]*$\")], metadata={\"description\": \"Soft prompt name, or a string containing only whitespace for no soft prompt. If using the GET method and no soft prompt is loaded, this will always be the empty string.\"})\n\n@api_v1.get(\"/config/soft_prompt\")\n@api_schema_wrap\ndef get_config_soft_prompt():\n    \"\"\"---\n    get:\n      summary: Retrieve the current soft prompt name\n      tags:\n        - config\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: SoftPromptSettingSchema\n              example:\n                value: \"\"\n    \"\"\"\n    return {\"value\": vars.spfilename.strip()}\n\nclass SoftPromptsListSchema(KoboldSchema):\n    values: List[SoftPromptSettingSchema] = fields.List(fields.Nested(SoftPromptSettingSchema), required=True, metadata={\"description\": \"Array of available softprompts.\"})\n\n@api_v1.get(\"/config/soft_prompts_list\")\n@api_schema_wrap\ndef get_config_soft_prompts_list():\n    \"\"\"---\n    get:\n      summary: Retrieve all available softprompt filenames\n      tags:\n        - config\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: SoftPromptsListSchema\n              example:\n                values: []\n    \"\"\"\n    splist = []\n    for sp in fileops.getspfiles(vars.modeldim):\n\n        splist.append({\"value\":sp[\"filename\"]})\n    return {\"values\": splist}\n\n@api_v1.put(\"/config/soft_prompt\")\n@api_schema_wrap\ndef put_config_soft_prompt(body: SoftPromptSettingSchema):\n    \"\"\"---\n    put:\n      summary: Set soft prompt by name\n      tags:\n        - config\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: SoftPromptSettingSchema\n            example:\n              value: \"\"\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        {api_validation_error_response}\n    \"\"\"\n    if vars.allowsp:\n        spRequest(body.value)\n        settingschanged()\n    return {}\n\nclass SamplerSeedSettingSchema(KoboldSchema):\n    value: int = fields.Integer(validate=validate.Range(min=0, max=2**64 - 1), required=True)\n\n@api_v1.get(\"/config/sampler_seed\")\n@api_schema_wrap\ndef get_config_sampler_seed():\n    \"\"\"---\n    get:\n      summary: Retrieve the current global sampler seed value\n      tags:\n        - config\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: SamplerSeedSettingSchema\n              example:\n                value: 3475097509890965500\n    \"\"\"\n    return {\"value\": __import__(\"tpu_mtj_backend\").get_rng_seed() if vars.use_colab_tpu else __import__(\"torch\").initial_seed()}\n\n@api_v1.put(\"/config/sampler_seed\")\n@api_schema_wrap\ndef put_config_sampler_seed(body: SamplerSeedSettingSchema):\n    \"\"\"---\n    put:\n      summary: Set the global sampler seed value\n      tags:\n        - config\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema: SamplerSeedSettingSchema\n            example:\n              value: 3475097509890965500\n      responses:\n        200:\n          description: Successful request\n          content:\n            application/json:\n              schema: EmptySchema\n        {api_validation_error_response}\n    \"\"\"\n    if vars.use_colab_tpu:\n        import tpu_mtj_backend\n        tpu_mtj_backend.set_rng_seed(body.value)\n    else:\n        import torch\n        torch.manual_seed(body.value)\n    vars.seed = body.value\n    return {}\n\nconfig_endpoint_schemas: List[Type[KoboldSchema]] = []\n\ndef config_endpoint_schema(c: Type[KoboldSchema]):\n    config_endpoint_schemas.append(c)\n    return c\n\n\n@config_endpoint_schema\nclass MemorySettingSchema(KoboldSchema):\n    value = fields.String(required=True)\n    class KoboldMeta:\n        route_name = \"memory\"\n        obj = \"vars\"\n        var_name = \"memory\"\n        name = \"memory\"\n        example_yaml_value = \"Memory\"\n\n@config_endpoint_schema\nclass AuthorsNoteSettingSchema(KoboldSchema):\n    value = fields.String(required=True)\n    class KoboldMeta:\n        route_name = \"authors_note\"\n        obj = \"vars\"\n        var_name = \"authornote\"\n        name = \"author's note\"\n        example_yaml_value = \"''\"\n\n@config_endpoint_schema\nclass AuthorsNoteTemplateSettingSchema(KoboldSchema):\n    value = fields.String(required=True)\n    class KoboldMeta:\n        route_name = \"authors_note_template\"\n        obj = \"vars\"\n        var_name = \"authornotetemplate\"\n        name = \"author's note template\"\n        example_yaml_value = \"\\\"[Author's note: <|>]\\\"\"\n\n@config_endpoint_schema\nclass TopKSamplingSettingSchema(KoboldSchema):\n    value = fields.Integer(validate=validate.Range(min=0), required=True)\n    class KoboldMeta:\n        route_name = \"top_k\"\n        obj = \"vars\"\n        var_name = \"top_k\"\n        name = \"top-k sampling\"\n        example_yaml_value = \"0\"\n\n@config_endpoint_schema\nclass TopASamplingSettingSchema(KoboldSchema):\n    value = fields.Float(validate=validate.Range(min=0), required=True)\n    class KoboldMeta:\n        route_name = \"top_a\"\n        obj = \"vars\"\n        var_name = \"top_a\"\n        name = \"top-a sampling\"\n        example_yaml_value = \"0.0\"\n\n@config_endpoint_schema\nclass TopPSamplingSettingSchema(KoboldSchema):\n    value = fields.Float(validate=validate.Range(min=0, max=1), required=True)\n    class KoboldMeta:\n        route_name = \"top_p\"\n        obj = \"vars\"\n        var_name = \"top_p\"\n        name = \"top-p sampling\"\n        example_yaml_value = \"0.9\"\n\n@config_endpoint_schema\nclass TailFreeSamplingSettingSchema(KoboldSchema):\n    value = fields.Float(validate=validate.Range(min=0, max=1), required=True)\n    class KoboldMeta:\n        route_name = \"tfs\"\n        obj = \"vars\"\n        var_name = \"tfs\"\n        name = \"tail free sampling\"\n        example_yaml_value = \"1.0\"\n\n@config_endpoint_schema\nclass TypicalSamplingSettingSchema(KoboldSchema):\n    value = fields.Float(validate=validate.Range(min=0, max=1), required=True)\n    class KoboldMeta:\n        route_name = \"typical\"\n        obj = \"vars\"\n        var_name = \"typical\"\n        name = \"typical sampling\"\n        example_yaml_value = \"1.0\"\n\n@config_endpoint_schema\nclass TemperatureSamplingSettingSchema(KoboldSchema):\n    value = fields.Float(validate=validate.Range(min=0, min_inclusive=False), required=True)\n    class KoboldMeta:\n        route_name = \"temperature\"\n        obj = \"vars\"\n        var_name = \"temp\"\n        name = \"temperature\"\n        example_yaml_value = \"0.5\"\n\n@config_endpoint_schema\nclass GensPerActionSettingSchema(KoboldSchema):\n    value = fields.Integer(validate=validate.Range(min=0, max=5), required=True)\n    class KoboldMeta:\n        route_name = \"n\"\n        obj = \"vars\"\n        var_name = \"numseqs\"\n        name = \"Gens Per Action\"\n        example_yaml_value = \"1\"\n\n@config_endpoint_schema\nclass MaxLengthSettingSchema(KoboldSchema):\n    value = fields.Integer(validate=validate.Range(min=1, max=512), required=True)\n    class KoboldMeta:\n        route_name = \"max_length\"\n        obj = \"vars\"\n        var_name = \"genamt\"\n        name = \"max length\"\n        example_yaml_value = \"80\"\n\n@config_endpoint_schema\nclass WorldInfoDepthSettingSchema(KoboldSchema):\n    value = fields.Integer(validate=validate.Range(min=1, max=5), required=True)\n    class KoboldMeta:\n        route_name = \"world_info_depth\"\n        obj = \"vars\"\n        var_name = \"widepth\"\n        name = \"world info depth\"\n        example_yaml_value = \"3\"\n\n@config_endpoint_schema\nclass AuthorsNoteDepthSettingSchema(KoboldSchema):\n    value = fields.Integer(validate=validate.Range(min=1, max=5), required=True)\n    class KoboldMeta:\n        route_name = \"authors_note_depth\"\n        obj = \"vars\"\n        var_name = \"andepth\"\n        name = \"author's note depth\"\n        example_yaml_value = \"3\"\n\n@config_endpoint_schema\nclass MaxContextLengthSettingSchema(KoboldSchema):\n    value = fields.Integer(validate=validate.Range(min=512, max=2048), required=True)\n    class KoboldMeta:\n        route_name = \"max_context_length\"\n        obj = \"vars\"\n        var_name = \"max_length\"\n        name = \"max context length\"\n        example_yaml_value = \"2048\"\n\n@config_endpoint_schema\nclass TrimIncompleteSentencesSettingsSchema(KoboldSchema):\n    value = fields.Boolean(required=True)\n    class KoboldMeta:\n        route_name = \"frmttriminc\"\n        obj = \"vars.formatoptns\"\n        var_name = \"@frmttriminc\"\n        name = \"trim incomplete sentences (output formatting)\"\n        example_yaml_value = \"false\"\n\n@config_endpoint_schema\nclass RemoveBlankLinesSettingsSchema(KoboldSchema):\n    value = fields.Boolean(required=True)\n    class KoboldMeta:\n        route_name = \"frmtrmblln\"\n        obj = \"vars.formatoptns\"\n        var_name = \"@frmtrmblln\"\n        name = \"remove blank lines (output formatting)\"\n        example_yaml_value = \"false\"\n\n@config_endpoint_schema\nclass RemoveSpecialCharactersSettingsSchema(KoboldSchema):\n    value = fields.Boolean(required=True)\n    class KoboldMeta:\n        route_name = \"frmtrmspch\"\n        obj = \"vars.formatoptns\"\n        var_name = \"@frmtrmspch\"\n        name = \"remove special characters (output formatting)\"\n        example_yaml_value = \"false\"\n\n@config_endpoint_schema\nclass SingleLineSettingsSchema(KoboldSchema):\n    value = fields.Boolean(required=True)\n    class KoboldMeta:\n        route_name = \"singleline\"\n        obj = \"vars.formatoptns\"\n        var_name = \"@singleline\"\n        name = \"single line (output formatting)\"\n        example_yaml_value = \"false\"\n\n@config_endpoint_schema\nclass AddSentenceSpacingSettingsSchema(KoboldSchema):\n    value = fields.Boolean(required=True)\n    class KoboldMeta:\n        route_name = \"frmtadsnsp\"\n        obj = \"vars.formatoptns\"\n        var_name = \"@frmtadsnsp\"\n        name = \"add sentence spacing (input formatting)\"\n        example_yaml_value = \"false\"\n\n@config_endpoint_schema\nclass SamplerOrderSettingSchema(KoboldSchema):\n    value = fields.List(fields.Integer(), validate=[validate.Length(min=6), permutation_validator], required=True)\n    class KoboldMeta:\n        route_name = \"sampler_order\"\n        obj = \"vars\"\n        var_name = \"sampler_order\"\n        name = \"sampler order\"\n        example_yaml_value = \"[6, 0, 1, 2, 3, 4, 5]\"\n\n@config_endpoint_schema\nclass SamplerFullDeterminismSettingSchema(KoboldSchema):\n    value = fields.Boolean(required=True)\n    class KoboldMeta:\n        route_name = \"sampler_full_determinism\"\n        obj = \"vars\"\n        var_name = \"full_determinism\"\n        name = \"sampler full determinism\"\n        example_yaml_value = \"false\"\n\n\nfor schema in config_endpoint_schemas:\n    create_config_endpoint(schema=schema.__name__, method=\"GET\")\n    create_config_endpoint(schema=schema.__name__, method=\"PUT\")\n\n\n#==================================================================#\n#  Final startup commands to launch Flask app\n#==================================================================#\nif __name__ == \"__main__\":\n\n    general_startup()\n    # Start flask & SocketIO\n    logger.init(\"Flask\", status=\"Starting\")\n    Session(app)\n    logger.init_ok(\"Flask\", status=\"OK\")\n    logger.init(\"Webserver\", status=\"Starting\")\n    patch_transformers()\n    #show_select_model_list()\n    if vars.model == \"\" or vars.model is None:\n        vars.model = \"ReadOnly\"\n    load_model(initial_load=True)\n\n    # Start Flask/SocketIO (Blocking, so this must be last method!)\n    port = args.port if \"port\" in args and args.port is not None else 5000\n    \n    #socketio.run(app, host='0.0.0.0', port=port)\n    if(vars.host):\n        if(args.localtunnel):\n            import subprocess, shutil\n            localtunnel = subprocess.Popen([shutil.which('lt'), '-p', str(port), 'http'], stdout=subprocess.PIPE)\n            attempts = 0\n            while attempts < 10:\n                try:\n                    cloudflare = str(localtunnel.stdout.readline())\n                    cloudflare = (re.search(\"(?P<url>https?:\\/\\/[^\\s]+loca.lt)\", cloudflare).group(\"url\"))\n                    break\n                except:\n                    attempts += 1\n                    time.sleep(3)\n                    continue\n            if attempts == 10:\n                print(\"LocalTunnel could not be created, falling back to cloudflare...\")\n                from flask_cloudflared import _run_cloudflared\n                cloudflare = _run_cloudflared(port)\n        elif(args.ngrok):\n            from flask_ngrok import _run_ngrok\n            cloudflare = _run_ngrok()\n        elif(args.remote):\n           from flask_cloudflared import _run_cloudflared\n           cloudflare = _run_cloudflared(port)\n        if(args.localtunnel or args.ngrok or args.remote):\n            with open('cloudflare.log', 'w') as cloudflarelog:\n                cloudflarelog.write(\"KoboldAI has finished loading and is available at the following link : \" + cloudflare)\n                logger.init_ok(\"Webserver\", status=\"OK\")\n                logger.message(f\"KoboldAI has finished loading and is available at the following link: {cloudflare}\")\n        else:\n            logger.init_ok(\"Webserver\", status=\"OK\")\n            logger.message(f\"Webserver has started, you can now connect to this machine at port: {port}\")\n        vars.serverstarted = True\n        socketio.run(app, host='0.0.0.0', port=port)\n    else:\n        if args.unblock:\n            if not args.no_ui:\n                try:\n                    import webbrowser\n                    webbrowser.open_new('http://localhost:{0}'.format(port))\n                except:\n                    pass\n            logger.init_ok(\"Webserver\", status=\"OK\")\n            logger.message(f\"Webserver started! You may now connect with a browser at http://127.0.0.1:{port}\")\n            vars.serverstarted = True\n            socketio.run(app, port=port, host='0.0.0.0')\n        else:\n            if not args.no_ui:\n                try:\n                    import webbrowser\n                    webbrowser.open_new('http://localhost:{0}'.format(port))\n                except:\n                    pass\n            logger.init_ok(\"Webserver\", status=\"OK\")\n            logger.message(f\"Webserver started! You may now connect with a browser at http://127.0.0.1:{port}\")\n            vars.serverstarted = True\n            socketio.run(app, port=port)\n    logger.init(\"Webserver\", status=\"Closed\")\n\n\nelse:\n    general_startup()\n    # Start flask & SocketIO\n    logger.init(\"Flask\", status=\"Starting\")\n    Session(app)\n    logger.init_ok(\"Flask\", status=\"OK\")\n    patch_transformers()\n    #show_select_model_list()\n    if vars.model == \"\" or vars.model is None:\n        vars.model = \"ReadOnly\"\n    load_model(initial_load=True)\n    print(\"{0}\\nServer started in WSGI mode!{1}\".format(colors.GREEN, colors.END), flush=True)\n"
        },
        {
          "name": "breakmodel.py",
          "type": "blob",
          "size": 42.3427734375,
          "content": "'''\nThis is a MODIFIED version of arrmansa's low VRAM patch.\nhttps://github.com/arrmansa/Basic-UI-for-GPT-J-6B-with-low-vram/blob/main/GPT-J-6B-Low-Vram-UI.ipynb\nThe ORIGINAL version of the patch is released under the Apache License 2.0\nCopyright 2021 arrmansa\nCopyright 2021 finetuneanon\nCopyright 2018, 2022 The Hugging Face team\n\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n'''\n\n\nimport torch\nfrom torch import nn\nimport torch.cuda.comm\nimport copy\nimport gc\nimport os\nimport sys\nimport itertools\nimport bisect\nimport random\nimport utils\nfrom typing import Dict, List, Optional, Union\n\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, BaseModelOutputWithPastAndCrossAttentions\n\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nbreakmodel = True\ngpu_blocks = []\ndisk_blocks = 0\nprimary_device = 0 if torch.cuda.device_count() > 0 else \"cpu\"\n\n\nif utils.HAS_ACCELERATE:\n    from accelerate.hooks import attach_align_device_hook_on_blocks\n    from accelerate.utils import OffloadedWeightsLoader, check_device_map, extract_submodules_state_dict, offload_state_dict\n    from accelerate import dispatch_model\n\ndef dispatch_model_ex(\n    model: nn.Module,\n    device_map: Dict[str, Union[str, int, torch.device]],\n    main_device: Optional[torch.device] = None,\n    state_dict: Optional[Dict[str, torch.Tensor]] = None,\n    offload_dir: Union[str, os.PathLike] = None,\n    offload_buffers: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    This is a modified version of\n    https://github.com/huggingface/accelerate/blob/eeaba598f455fbd2c48661d7e816d3ff25ab050b/src/accelerate/big_modeling.py#L130\n    that still works when the main device is the CPU.\n\n    Dispatches a model according to a given device map. Layers of the model might be spread across GPUs, offloaded on\n    the CPU or even the disk.\n\n    Args:\n        model (`torch.nn.Module`):\n            The model to dispatch.\n        device_map (`Dict[str, Union[str, int, torch.device]]`):\n            A dictionary mapping module names in the models `state_dict` to the device they should go to. Note that\n            `\"disk\"` is accepted even if it's not a proper value for `torch.device`.\n        main_device (`str`, `int` or `torch.device`, *optional*):\n            The main execution device. Will default to the first device in the `device_map` different from `\"cpu\"` or\n            `\"disk\"`.\n        state_dict (`Dict[str, torch.Tensor]`, *optional*):\n            The state dict of the part of the model that will be kept on CPU.\n        offload_dir (`str` or `os.PathLike`):\n            The folder in which to offload the model weights (or where the model weights are already offloaded).\n        offload_buffers (`bool`, *optional*, defaults to `False`):\n            Whether or not to offload the buffers with the model parameters.\n        preload_module_classes (`List[str]`, *optional*):\n            A list of classes whose instances should load all their weights (even in the submodules) at the beginning\n            of the forward. This should only be used for classes that have submodules which are registered but not\n            called directly during the forward, for instance if a `dense` linear layer is registered, but at forward,\n            `dense.weight` and `dense.bias` are used in some operations instead of calling `dense` directly.\n    \"\"\"\n    if main_device != \"cpu\":\n        return dispatch_model(model, device_map, main_device, state_dict, offload_dir=offload_dir, offload_buffers=offload_buffers, **kwargs)\n\n    # Error early if the device map is incomplete.\n    check_device_map(model, device_map)\n\n    offload_devices = [\"cpu\", \"disk\"] if main_device != \"cpu\" else [\"disk\"]\n\n    if main_device is None:\n        main_device = [d for d in device_map.values() if d not in offload_devices][0]\n\n    cpu_modules = [name for name, device in device_map.items() if device == \"cpu\"] if main_device != \"cpu\" else []\n    if state_dict is None and len(cpu_modules) > 0:\n        state_dict = extract_submodules_state_dict(model.state_dict(), cpu_modules)\n\n    disk_modules = [name for name, device in device_map.items() if device == \"disk\"]\n    if offload_dir is None and len(disk_modules) > 0:\n        raise ValueError(\n            \"We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules \"\n            f\"need to be offloaded: {', '.join(disk_modules)}.\"\n        )\n    if len(disk_modules) > 0 and (\n        not os.path.isdir(offload_dir) or not os.path.isfile(os.path.join(offload_dir, \"index.json\"))\n    ):\n        disk_state_dict = extract_submodules_state_dict(model.state_dict(), disk_modules)\n        offload_state_dict(offload_dir, disk_state_dict)\n\n    execution_device = {\n        name: main_device if device in offload_devices else device for name, device in device_map.items()\n    }\n    offload = {name: device in offload_devices for name, device in device_map.items()}\n    save_folder = offload_dir if len(disk_modules) > 0 else None\n    if state_dict is not None or save_folder is not None:\n        weights_map = OffloadedWeightsLoader(state_dict=state_dict, save_folder=save_folder)\n    else:\n        weights_map = None\n\n    attach_align_device_hook_on_blocks(\n        model,\n        execution_device=execution_device,\n        offload=offload,\n        offload_buffers=offload_buffers,\n        weights_map=weights_map,\n        **kwargs,\n    )\n    model.hf_device_map = device_map\n    return model\n\n\n# Copied from transformers.models.bart.modeling_bart._expand_mask\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)\n\n\ndef move_hidden_layers(transformer, h=None):\n    if h is None:\n        h = transformer.h\n\n    assert len(gpu_blocks) <= torch.cuda.device_count()\n    assert sum(gpu_blocks) <= len(h)\n    ram_blocks = len(h) - sum(gpu_blocks)\n\n    transformer.extrastorage = {}\n    torch.cuda.empty_cache()\n    \n    able_to_pin_layers = True\n    for i in range(ram_blocks):\n        h[i].to(\"cpu\")\n        transformer.extrastorage[i] = copy.deepcopy(h[i])\n        smalltensor = torch.tensor(0).to(primary_device)\n        for param1 in h[i].parameters():\n            param1.data = smalltensor\n        h[i].to(primary_device)\n        for param in transformer.extrastorage[i].parameters():\n            param.requires_grad = False\n            param.data = param.data.detach()\n            if able_to_pin_layers:\n                try:\n                    param.data = param.data.pin_memory()\n                except:\n                    able_to_pin_layers = False\n                    print(f\"WARNING:  You only have enough shared GPU memory for {i} out of {ram_blocks} CPU layers.  Expect suboptimal speed.\", file=sys.stderr)\n            gc.collect()\n            torch.cuda.empty_cache()\n\n    if ram_blocks:\n        for param1,param2 in zip(h[0].parameters(),transformer.extrastorage[0].parameters()):\n            param1.data = param2.data.to(primary_device, non_blocking=False).detach()\n\n        for param1,param2 in zip(h[ram_blocks-1].parameters(),transformer.extrastorage[ram_blocks-1].parameters()):\n            param1.data = param2.data.to(primary_device, non_blocking=False).detach()\n\n    i = ram_blocks\n    for j in range(len(gpu_blocks)):\n        for _ in range(gpu_blocks[j]):\n            h[i].to(j)\n            i += 1\n\n\ndef new_forward_neo(\n    self,\n    input_ids=None,\n    past_key_values=None,\n    attention_mask=None,\n    token_type_ids=None,\n    position_ids=None,\n    head_mask=None,\n    inputs_embeds=None,\n    use_cache=None,\n    output_attentions=None,\n    output_hidden_states=None,\n    return_dict=None,\n    embs=None,\n):\n    assert len(gpu_blocks) <= torch.cuda.device_count()\n    assert sum(gpu_blocks) <= len(self.h)\n    ram_blocks = len(self.h) - sum(gpu_blocks)\n    cumulative_gpu_blocks = tuple(itertools.accumulate(gpu_blocks))\n\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    use_cache = use_cache if use_cache is not None else self.config.use_cache\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    elif input_ids is not None:\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_shape[-1])\n        batch_size = input_ids.shape[0]\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n        batch_size = inputs_embeds.shape[0]\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.view(-1, input_shape[-1])\n    if position_ids is not None:\n        position_ids = position_ids.view(-1, input_shape[-1])\n\n    if past_key_values is None:\n        past_length = 0\n        past_key_values = tuple([None] * len(self.h))\n    else:\n        past_length = past_key_values[0][0].size(-2)\n\n    device = primary_device if breakmodel else input_ids.device if input_ids is not None else inputs_embeds.device\n    if position_ids is None:\n        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n\n    # Attention mask.\n    if attention_mask is not None:\n        assert batch_size > 0, \"batch_size has to be defined and > 0\"\n        attention_mask = attention_mask.view(batch_size, -1)\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        attention_mask = attention_mask[:, None, None, :]\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        attention_mask = (1.0 - attention_mask) * -10000.0\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x num_heads x N x N\n    # head_mask has shape n_layer x batch x num_heads x N x N\n    head_mask = self.get_head_mask(head_mask, getattr(self.config, \"num_layers\", None) or self.config.n_layer)\n\n    if inputs_embeds is None:\n        if breakmodel:\n            input_ids = input_ids.to(primary_device)\n        inputs_embeds = self.wte(input_ids)\n\n    if embs is not None and not (use_cache is not None and use_cache and past_key_values is not None and len(past_key_values) > 0 and past_key_values[0] is not None):\n        offset = 0\n        for pos, emb in embs:\n            pos += offset\n            if len(emb.shape) == 2:\n                emb = emb.repeat(input_shape[0], 1, 1)\n            inputs_embeds[:, pos:pos+emb.shape[1]] = emb\n            offset += emb.shape[1]\n\n    if getattr(self, \"wpe\", None) is None:\n        hidden_states = inputs_embeds\n    else:\n        if breakmodel:\n            position_ids = position_ids.to(primary_device)\n        position_embeds = self.wpe(position_ids)\n        if breakmodel:\n            position_embeds = position_embeds.to(primary_device)\n        hidden_states = inputs_embeds + position_embeds\n\n    if token_type_ids is not None:\n        token_type_embeds = self.wte(token_type_ids)\n        hidden_states = hidden_states + token_type_embeds\n\n    hidden_states = self.drop(hidden_states)\n\n    output_shape = input_shape + (hidden_states.size(-1),)\n\n    presents = () if use_cache else None\n    all_self_attentions = () if output_attentions else None\n    all_hidden_states = () if output_hidden_states else None\n\n    if breakmodel and ram_blocks:\n        copystream = torch.cuda.Stream(device=primary_device, priority=-1)\n\n    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n\n        if breakmodel:\n            if i in range(ram_blocks):\n                index1 = (i+1)%ram_blocks\n                for param1,param2 in zip(self.h[index1].parameters(),self.h[(i-1)%ram_blocks].parameters()):\n                    param1.data = param2.data\n                for param1,param2 in zip(self.h[index1].parameters(),self.extrastorage[index1].parameters()):\n                    with torch.cuda.stream(copystream):\n                        torch.cuda.comm.broadcast(param2.data,out = [param1.data])\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states.cpu(),)\n\n        if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n\n            if use_cache:\n                logger.warning(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n            def create_custom_forward(module):\n                def custom_forward(*inputs):\n                    # None for past_key_value\n                    return module(*inputs, use_cache, output_attentions)\n\n                return custom_forward\n\n            outputs = torch.utils.checkpoint.checkpoint(\n                create_custom_forward(block),\n                hidden_states,\n                None,\n                attention_mask,\n                head_mask[i],\n            )\n        else:\n            if breakmodel:\n                device = primary_device if i < ram_blocks else bisect.bisect_right(cumulative_gpu_blocks, i - ram_blocks)\n            outputs = block(\n                hidden_states.to(device) if breakmodel and hidden_states is not None else hidden_states,\n                layer_past=tuple(v.to(device) for v in layer_past if v is not None) if breakmodel and layer_past is not None and i >= ram_blocks and len(layer_past) and layer_past[0].device.index != device else layer_past,\n                attention_mask=attention_mask.to(device) if breakmodel and attention_mask is not None else attention_mask,\n                head_mask=head_mask[i].to(device) if breakmodel and head_mask[i] is not None else head_mask[i],\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n            )\n\n        hidden_states = outputs[0]\n        if use_cache is True:\n            presents = presents + (outputs[1],)\n\n        if output_attentions:\n            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n\n\n        if breakmodel:\n            if i in range(ram_blocks):\n                torch.cuda.synchronize()\n                torch.cuda.empty_cache()\n\n    if breakmodel:\n        if ram_blocks:\n            del copystream\n        torch.cuda.empty_cache()\n        hidden_states = hidden_states.to(primary_device)\n    hidden_states = self.ln_f(hidden_states)\n    if breakmodel:\n        hidden_states = hidden_states.to(primary_device)\n\n    hidden_states = hidden_states.view(*output_shape)\n    # Add last hidden state\n    if output_hidden_states:\n        all_hidden_states = all_hidden_states + (hidden_states,)\n\n    if not return_dict:\n        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n    return BaseModelOutputWithPast(\n        last_hidden_state=hidden_states,\n        past_key_values=presents,\n        hidden_states=all_hidden_states,\n        attentions=all_self_attentions,\n    )\n\n\ndef new_forward_xglm(\n    self,\n    input_ids=None,\n    attention_mask=None,\n    encoder_hidden_states=None,\n    encoder_attention_mask=None,\n    head_mask=None,\n    cross_attn_head_mask=None,\n    past_key_values=None,\n    inputs_embeds=None,\n    use_cache=None,\n    output_attentions=None,\n    output_hidden_states=None,\n    return_dict=None,\n):\n    assert len(gpu_blocks) <= torch.cuda.device_count()\n    assert sum(gpu_blocks) <= len(self.layers)\n    ram_blocks = len(self.layers) - sum(gpu_blocks)\n    cumulative_gpu_blocks = tuple(itertools.accumulate(gpu_blocks))\n\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    use_cache = use_cache if use_cache is not None else self.config.use_cache\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    # retrieve input_ids and inputs_embeds\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    elif input_ids is not None:\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_shape[-1])\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    # past_key_values_length\n    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n    if inputs_embeds is None:\n        if breakmodel:\n            input_ids = input_ids.to(primary_device)\n        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n\n    attention_mask = self._prepare_decoder_attention_mask(\n        attention_mask, input_shape, inputs_embeds, past_key_values_length\n    )\n\n    # expand encoder attention mask\n    if encoder_hidden_states is not None and encoder_attention_mask is not None:\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n\n    # embed positions\n    if breakmodel:\n        inputs_embeds = inputs_embeds.to(primary_device)\n    positions = self.embed_positions(input_ids, inputs_embeds, past_key_values_length)\n    if breakmodel:\n        positions = positions.to(primary_device)\n\n    hidden_states = inputs_embeds + positions\n\n    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n    # decoder layers\n    all_hidden_states = () if output_hidden_states else None\n    all_self_attns = () if output_attentions else None\n    all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n    next_decoder_cache = () if use_cache else None\n\n    if breakmodel and ram_blocks:\n        copystream = torch.cuda.Stream(device=primary_device, priority=-1)\n\n    # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n    for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n        if attn_mask is not None:\n            assert attn_mask.size()[0] == (\n                len(self.layers)\n            ), f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n    for idx, decoder_layer in enumerate(self.layers):\n        i = idx\n        if breakmodel:\n            if i in range(ram_blocks):\n                index1 = (i+1)%ram_blocks\n                for param1,param2 in zip(self.layers[index1].parameters(),self.layers[(i-1)%ram_blocks].parameters()):\n                    param1.data = param2.data\n                for param1,param2 in zip(self.layers[index1].parameters(),self.extrastorage[index1].parameters()):\n                    with torch.cuda.stream(copystream):\n                        torch.cuda.comm.broadcast(param2.data,out = [param1.data])\n\n        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        dropout_probability = random.uniform(0, 1)\n        if self.training and (dropout_probability < self.layerdrop):\n            continue\n\n        past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n        if self.gradient_checkpointing and self.training:\n\n            if use_cache:\n                logger.warning(\n                    \"`use_cache = True` is incompatible with gradient checkpointing`. Setting `use_cache = False`...\"\n                )\n                use_cache = False\n\n            def create_custom_forward(module):\n                def custom_forward(*inputs):\n                    # None for past_key_value\n                    return module(*inputs, output_attentions, use_cache)\n\n                return custom_forward\n\n            layer_outputs = torch.utils.checkpoint.checkpoint(\n                create_custom_forward(decoder_layer),\n                hidden_states,\n                attention_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                head_mask[idx] if head_mask is not None else None,\n                cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                None,\n            )\n        else:\n            if breakmodel:\n                device = primary_device if i < ram_blocks else bisect.bisect_right(cumulative_gpu_blocks, i - ram_blocks)\n            layer_outputs = decoder_layer(\n                hidden_states.to(device) if breakmodel and hidden_states is not None else hidden_states,\n                attention_mask=attention_mask.to(device) if breakmodel and attention_mask is not None else attention_mask,\n                encoder_hidden_states=encoder_hidden_states.to(device) if breakmodel and encoder_hidden_states is not None else encoder_hidden_states,\n                encoder_attention_mask=encoder_attention_mask.to(device) if breakmodel and encoder_attention_mask is not None else encoder_attention_mask,\n                layer_head_mask=((head_mask[idx].to(device) if breakmodel and head_mask[idx] is not None else head_mask[idx]) if head_mask is not None else None),\n                cross_attn_layer_head_mask=(\n                    (cross_attn_head_mask[idx].to(device) if breakmodel and cross_attn_head_mask[idx] is not None else cross_attn_head_mask[idx]) if cross_attn_head_mask is not None else None\n                ),\n                past_key_value=tuple(v.to(device) for v in past_key_value if v is not None) if breakmodel and past_key_value is not None and i >= ram_blocks and len(past_key_value) and past_key_value[0].device.index != device else past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n            )\n        hidden_states = layer_outputs[0]\n\n        if use_cache:\n            next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n\n        if output_attentions:\n            all_self_attns += (layer_outputs[1],)\n\n            if encoder_hidden_states is not None:\n                all_cross_attentions += (layer_outputs[2],)\n        \n        if breakmodel:\n            if i in range(ram_blocks):\n                torch.cuda.synchronize()\n                torch.cuda.empty_cache()\n\n    if breakmodel:\n        if ram_blocks:\n            del copystream\n        torch.cuda.empty_cache()\n        hidden_states = hidden_states.to(primary_device)\n    hidden_states = self.layer_norm(hidden_states)\n    if breakmodel:\n        hidden_states = hidden_states.to(primary_device)\n\n    # add hidden states from the last decoder layer\n    if output_hidden_states:\n        all_hidden_states += (hidden_states,)\n\n    next_cache = next_decoder_cache if use_cache else None\n    if not return_dict:\n        return tuple(\n            v\n            for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n            if v is not None\n        )\n    return BaseModelOutputWithPastAndCrossAttentions(\n        last_hidden_state=hidden_states,\n        past_key_values=next_cache,\n        hidden_states=all_hidden_states,\n        attentions=all_self_attns,\n        cross_attentions=all_cross_attentions,\n    )\n\n\ndef new_forward_opt(\n    self,\n    input_ids=None,\n    attention_mask=None,\n    head_mask=None,\n    past_key_values=None,\n    inputs_embeds=None,\n    use_cache=None,\n    output_attentions=None,\n    output_hidden_states=None,\n    return_dict=None,\n):\n    assert len(gpu_blocks) <= torch.cuda.device_count()\n    assert sum(gpu_blocks) <= len(self.layers)\n    ram_blocks = len(self.layers) - sum(gpu_blocks)\n    cumulative_gpu_blocks = tuple(itertools.accumulate(gpu_blocks))\n\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    # retrieve input_ids and inputs_embeds\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n    elif input_ids is not None:\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_shape[-1])\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n    if inputs_embeds is None:\n        if breakmodel:\n            input_ids = input_ids.to(primary_device) \n        inputs_embeds = self.embed_tokens(input_ids)\n\n    # embed positions\n    if breakmodel:\n        inputs_embeds = inputs_embeds.to(primary_device) \n    if attention_mask is None:\n        attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.bool, device=inputs_embeds.device)\n\n    positions = self.embed_positions(attention_mask)[:, past_key_values_length:, :]\n    if breakmodel:\n        positions = positions.to(primary_device) \n\n    attention_mask = self._prepare_decoder_attention_mask(\n        attention_mask, input_shape, inputs_embeds, past_key_values_length\n    )\n\n    if self.project_in is not None:\n        inputs_embeds = self.project_in(inputs_embeds)\n\n    hidden_states = inputs_embeds + positions\n\n    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n    # decoder layers\n    all_hidden_states = () if output_hidden_states else None\n    all_self_attns = () if output_attentions else None\n    next_decoder_cache = () if use_cache else None\n\n    if breakmodel and ram_blocks:\n        copystream = torch.cuda.Stream(device=primary_device, priority=-1)\n\n    # check if head_mask has a correct number of layers specified if desired\n    for attn_mask, mask_name in zip([head_mask], [\"head_mask\"]):\n        if attn_mask is not None:\n            if attn_mask.size()[0] != (len(self.layers)):\n                raise ValueError(\n                    f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n                    f\" {head_mask.size()[0]}.\"\n                )\n\n    for idx, decoder_layer in enumerate(self.layers):\n        i = idx\n        if breakmodel:\n            if i in range(ram_blocks):\n                index1 = (i+1)%ram_blocks\n                for param1,param2 in zip(self.layers[index1].parameters(),self.layers[(i-1)%ram_blocks].parameters()):\n                    param1.data = param2.data\n                for param1,param2 in zip(self.layers[index1].parameters(),self.extrastorage[index1].parameters()):\n                    with torch.cuda.stream(copystream):\n                        torch.cuda.comm.broadcast(param2.data,out = [param1.data])\n\n        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        dropout_probability = random.uniform(0, 1)\n        if self.training and (dropout_probability < self.layerdrop):\n            continue\n\n        past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n        if self.gradient_checkpointing and self.training:\n\n            if use_cache:\n                logger.warning(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n            def create_custom_forward(module):\n                def custom_forward(*inputs):\n                    # None for past_key_value\n                    return module(*inputs, output_attentions, None)\n\n                return custom_forward\n\n            layer_outputs = torch.utils.checkpoint.checkpoint(\n                create_custom_forward(decoder_layer),\n                hidden_states,\n                attention_mask,\n                head_mask[idx] if head_mask is not None else None,\n                None,\n            )\n        else:\n            if breakmodel:\n                device = primary_device if i < ram_blocks else bisect.bisect_right(cumulative_gpu_blocks, i - ram_blocks)\n            layer_outputs = decoder_layer(\n                hidden_states.to(device) if breakmodel and hidden_states is not None else hidden_states,\n                attention_mask=attention_mask.to(device) if breakmodel and attention_mask is not None else attention_mask,\n                layer_head_mask=((head_mask[idx].to(device) if breakmodel and head_mask[idx] is not None else head_mask[idx]) if head_mask is not None else None),\n                past_key_value=tuple(v.to(device) for v in past_key_value if v is not None) if breakmodel and past_key_value is not None and i >= ram_blocks and len(past_key_value) and past_key_value[0].device.index != device else past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n            )\n\n        hidden_states = layer_outputs[0]\n\n        if use_cache:\n            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n        if output_attentions:\n            all_self_attns += (layer_outputs[1],)\n        \n        if breakmodel:\n            if i in range(ram_blocks):\n                torch.cuda.synchronize()\n                torch.cuda.empty_cache()\n\n    if breakmodel:\n        if ram_blocks:\n            del copystream\n        torch.cuda.empty_cache()\n        hidden_states = hidden_states.to(primary_device)\n    if self.project_out is not None:\n        hidden_states = self.project_out(hidden_states)\n    if breakmodel:\n        hidden_states = hidden_states.to(primary_device)\n\n    # add hidden states from the last decoder layer\n    if output_hidden_states:\n        all_hidden_states += (hidden_states,)\n\n    next_cache = next_decoder_cache if use_cache else None\n    if not return_dict:\n        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n    return BaseModelOutputWithPast(\n        last_hidden_state=hidden_states,\n        past_key_values=next_cache,\n        hidden_states=all_hidden_states,\n        attentions=all_self_attns,\n    )\n"
        },
        {
          "name": "bridge.lua",
          "type": "blob",
          "size": 68.560546875,
          "content": "-- KoboldAI Lua 5.4 Bridge\n\n\n---@param _python? table<string, any>\n---@param _bridged? table<string, any>\n---@return KoboldLib, KoboldCoreLib?\nreturn function(_python, _bridged)\n\n    --==========================================================================\n    -- Globally allows using a _kobold_next metamethod for \"Kobold\" classes only\n    --==========================================================================\n\n    local old_next = next\n    ---@generic K, V\n    ---@param t table<K, V>\n    ---@param k? K\n    ---@return K?, V?\n    function next(t, k)\n        local meta = getmetatable(t)\n        return ((meta ~= nil and type(rawget(t, \"_name\")) == \"string\" and string.match(rawget(t, \"_name\"), \"^Kobold\") and type(meta._kobold_next) == \"function\") and meta._kobold_next or old_next)(t, k)\n    end\n\n\n    --==========================================================================\n    -- General utility functions\n    --==========================================================================\n\n    ---@generic T\n    ---@param original T\n    ---@return T\n    local function deepcopy(original)\n        if type(original) == \"table\" then\n            local copy = {}\n            for k, v in old_next, original, nil do\n                copy[k] = deepcopy(v)\n            end\n            setmetatable(copy, deepcopy(getmetatable(original)))\n            return copy\n        end\n        return original\n    end\n\n    ---@param paths string|table<integer, string>\n    ---@return nil\n    local function set_require_path(paths)\n        if type(paths) == \"string\" then\n            paths = {paths}\n        end\n        local config = {}\n        local i = 1\n        for substring in string.gmatch(package.config, \"[^\\n]+\") do\n            config[i] = substring\n            i = i + 1\n        end\n        local _paths = {}\n        for i, path in ipairs(paths) do\n            _paths[i] = path .. config[1] .. config[3] .. \".lua\" .. config[2] .. path .. config[1] .. config[3] .. config[1] .. \"init.lua\"\n        end\n        package.path = table.concat(_paths, config[2])\n        package.cpath = \"\"\n    end\n\n    ---@param path string\n    ---@param filename string\n    ---@return string\n    local function join_folder_and_filename(path, filename)\n        return path .. string.match(package.config, \"[^\\n]+\") .. filename\n    end\n\n\n    --==========================================================================\n    -- _bridged preprocessing\n    --==========================================================================\n\n    local bridged = {}\n    for k in _python.iter(_bridged) do\n        local v = _bridged[k]\n        bridged[k] = type(v) == \"userdata\" and _python.as_attrgetter(v) or v\n    end\n    set_require_path(bridged.lib_paths)\n\n\n    --==========================================================================\n    -- Wraps most functions in this file so that they restore original\n    -- metatables prior to executing the function body\n    --==========================================================================\n\n    local wrapped = false\n\n    ---@class Metatables\n    local metatables = {}\n    local type_map = {\n        _nil = nil,\n        _boolean = false,\n        _number = 0,\n        _string = \"\",\n        _function = type,\n        _thread = coroutine.create(function() end),\n    }\n\n    function metatables:overwrite()\n        for k, v in pairs(type_map) do\n            self[k] = debug.getmetatable(v)\n        end\n    end\n\n    function metatables:restore()\n        for k, v in pairs(type_map) do\n            debug.setmetatable(v, self[k])\n        end\n    end\n\n    local metatables_original = deepcopy(metatables)\n    metatables_original:overwrite()\n\n    local metawrapper = {}\n    ---@generic T : table\n    ---@param t T\n    ---@return T\n    function metawrapper.__newindex(t, k, wrapped_func)\n        if type(wrapped_func) == \"function\" then\n            return rawset(t, k, function(...)\n                local _needs_unwrap = false\n                if not wrapped then\n                    metatables:overwrite()\n                    metatables_original:restore()\n                    _needs_unwrap = true\n                    wrapped = true\n                end\n                local r = table.pack(wrapped_func(...))\n                if _needs_unwrap then\n                    metatables:restore()\n                    wrapped = false\n                end\n                return table.unpack(r, 1, r.n)\n            end)\n        else\n            return rawset(t, k, wrapped_func)\n        end\n    end\n\n\n    --==========================================================================\n    -- Modules\n    --==========================================================================\n\n    ---@class KoboldLib\n    ---@field API_VERSION number\n    ---@field authorsnote string\n    ---@field authorsnotetemplate string\n    ---@field memory string\n    ---@field submission string\n    ---@field model string\n    ---@field modeltype \"'readonly'\"|\"'api'\"|\"'unknown'\"|\"'gpt2'\"|\"'gpt2-medium'\"|\"'gpt2-large'\"|\"'gpt2-xl'\"|\"'gpt-neo-125M'\"|\"'gpt-neo-1.3B'\"|\"'gpt-neo-2.7B'\"|\"'gpt-j-6B'\"\n    ---@field modelbackend \"'readonly'\"|\"'api'\"|\"'transformers'\"|\"'mtj'\"\n    ---@field is_custommodel boolean\n    ---@field custmodpth string\n    ---@field logits table<integer, table<integer, number>>\n    ---@field logits_rows integer\n    ---@field logits_cols integer\n    ---@field generated table<integer, table<integer, integer>>\n    ---@field generated_rows integer\n    ---@field generated_cols integer\n    ---@field outputs table<integer, string>\n    ---@field num_outputs integer\n    ---@field feedback string\n    ---@field is_config_file_open boolean\n    local kobold = setmetatable({API_VERSION = 1.2}, metawrapper)\n    local KoboldLib_mt = setmetatable({}, metawrapper)\n    local KoboldLib_getters = setmetatable({}, metawrapper)\n    local KoboldLib_setters = setmetatable({}, metawrapper)\n\n    ---@param t KoboldLib\n    function KoboldLib_mt.__index(t, k)\n        local getter = KoboldLib_getters[k]\n        if getter ~= nil then\n            return getter(t)\n        end\n        return rawget(t, k)\n    end\n\n    ---@param t KoboldLib\n    function KoboldLib_mt.__newindex(t, k, v)\n        local setter = KoboldLib_setters[k]\n        if setter ~= nil then\n            return setter(t, v)\n        end\n        return rawset(t, k, v)\n    end\n\n    ---@class KoboldCoreLib\n    ---@field userscripts KoboldUserScriptList\n    local koboldcore = setmetatable({}, metawrapper)\n    local KoboldCoreLib_mt = setmetatable({}, metawrapper)\n    local KoboldCoreLib_getters = setmetatable({}, metawrapper)\n    local KoboldCoreLib_setters = setmetatable({}, metawrapper)\n\n    ---@param t KoboldCoreLib\n    function KoboldCoreLib_mt.__index(t, k)\n        local getter = KoboldCoreLib_getters[k]\n        if getter ~= nil then\n            return getter(t, k)\n        end\n        return rawget(t, k)\n    end\n\n    ---@param t KoboldCoreLib\n    function KoboldCoreLib_mt.__newindex(t, k, v)\n        local setter = KoboldCoreLib_setters[k]\n        if setter ~= nil then\n            return setter(t, k, v)\n        end\n        return rawset(t, k, v)\n    end\n\n    ---@class KoboldBridgeLib\n    local koboldbridge = {}\n\n    koboldbridge.regeneration_required = false\n    koboldbridge.resend_settings_required = false\n    koboldbridge.generating = true\n    koboldbridge.restart_sequence = nil\n    koboldbridge.userstate = nil\n    koboldbridge.logits = {}\n    koboldbridge.vocab_size = 0\n    koboldbridge.generated = {}\n    koboldbridge.generated_cols = 0\n    koboldbridge.outputs = {}\n    koboldbridge.feedback = nil  ---@type string?\n\n    function koboldbridge:clear_userscript_metadata()\n        self.logging_name = nil\n        self.filename = nil\n    end\n\n    ---@return nil\n    local function maybe_require_regeneration()\n        if koboldbridge.userstate == \"genmod\" or koboldbridge.userstate == \"outmod\" then\n            koboldbridge.regeneration_required = true\n        end\n    end\n\n\n    --==========================================================================\n    -- Userscript API: Configuration\n    --==========================================================================\n\n    local config_files = {}  ---@type table<string, file*>\n    local config_file_filename_map = {}  ---@type table<file*, string>\n\n    ---@return file*?\n    local function open_and_handle_errors(...)\n        local file, err_msg = io.open(...)\n        if err_msg ~= nil then\n            koboldbridge.obliterate_multiverse()\n            error(err_msg)\n            return\n        end\n        return file\n    end\n\n    ---@param file? file*\n    local function new_close_pre(file)\n        if file == nil then\n            file = io.output()\n        end\n        local filename = config_file_filename_map[file]\n        if filename ~= nil then\n            config_file_filename_map[file] = nil\n            config_files[filename] = nil\n        end\n    end\n\n    ---@param f fun(file?: file*)\n    local function _new_close(f)\n        ---@param file? file*\n        return function(file)\n            new_close_pre(file)\n            return f(file)\n        end\n    end\n    debug.getmetatable(io.stdout).__index.close = _new_close(io.stdout.close)\n    debug.getmetatable(io.stdout).__close = _new_close(io.stdout.close)\n\n    ---@param filename string\n    ---@return boolean\n    local function is_config_file_open(filename)\n        return config_files[filename] ~= nil\n    end\n\n    ---@param filename string\n    ---@param clear? boolean\n    ---@return file*\n    local function get_config_file(filename, clear)\n        if not is_config_file_open(filename) then\n            local config_filepath = join_folder_and_filename(bridged.config_path, filename .. \".conf\")\n            open_and_handle_errors(config_filepath, \"a\"):close()\n            config_files[filename] = open_and_handle_errors(config_filepath, clear and \"w+b\" or \"r+b\")\n            config_file_filename_map[config_files[filename]] = filename\n        end\n        return config_files[filename]\n    end\n\n    ---@param clear? boolean\n    ---@return file*\n    function kobold.get_config_file(clear)\n        return get_config_file(koboldbridge.filename, clear)\n    end\n\n    ---@param t KoboldLib\n    ---@return boolean\n    function KoboldLib_getters.is_config_file_open(t)\n        return is_config_file_open(koboldbridge.filename)\n    end\n\n    ---@param t KoboldLib\n    ---@param v boolean\n    function KoboldLib_setters.is_config_file_open(t, v)\n        error(\"`KoboldLib.is_config_file_open` is a read-only attribute\")\n    end\n\n\n    --==========================================================================\n    -- Userscript API: World Info\n    --==========================================================================\n\n    local fields = setmetatable({}, metawrapper)\n\n    ---@param t KoboldWorldInfoEntry|KoboldWorldInfoFolder|KoboldWorldInfo|KoboldWorldInfoFolderSelector\n    ---@return boolean\n    local function check_validity(t)\n        if not t:is_valid() then\n            error(\"Attempted to use a nonexistent/deleted `\"..rawget(t, \"_name\")..\"`\")\n            return false\n        end\n        return true\n    end\n\n\n    ----------------------------------------------------------------------------\n\n    ---@class KoboldWorldInfoEntry_base\n    ---@type table<integer, nil>\n    local _ = {}\n\n    ---@class KoboldWorldInfoEntry : KoboldWorldInfoEntry_base\n    ---@field key string\n    ---@field keysecondary string\n    ---@field content string\n    ---@field comment string\n    ---@field folder integer\n    ---@field num integer\n    ---@field selective boolean\n    ---@field constant boolean\n    ---@field uid integer\n    local KoboldWorldInfoEntry = setmetatable({\n        _name = \"KoboldWorldInfoEntry\",\n    }, metawrapper)\n    fields.KoboldWorldInfoEntry = {\n        \"key\",\n        \"keysecondary\",\n        \"content\",\n        \"comment\",\n        \"folder\",\n        \"num\",\n        \"selective\",\n        \"constant\",\n        \"uid\",\n    }\n    local KoboldWorldInfoEntry_mt = setmetatable({}, metawrapper)\n\n    local KoboldWorldInfoEntry_fieldtypes = {\n        key = \"string\",\n        keysecondary = \"string\",\n        content = \"string\",\n        comment = \"string\",\n        selective = \"boolean\",\n        constant = \"boolean\",\n    }\n\n    ---@return boolean\n    function KoboldWorldInfoEntry:is_valid()\n        return _python.as_attrgetter(bridged.vars.worldinfo_u).get(rawget(self, \"_uid\")) ~= nil\n    end\n\n    ---@param submission? string\n    ---@param kwargs? table<string, any>\n    ---@return string\n    function KoboldWorldInfoEntry:compute_context(submission, kwargs)\n        if not check_validity(self) then\n            return \"\"\n        elseif submission == nil then\n            submission = kobold.submission\n        elseif type(submission) ~= \"string\" then\n            error(\"`compute_context` takes a string or nil as argument #1, but got a \" .. type(submission))\n            return \"\"\n        end\n        return bridged.compute_context(submission, {self.uid}, nil, kwargs)\n    end\n\n    ---@generic K\n    ---@param t KoboldWorldInfoEntry|KoboldWorldInfoFolder|KoboldWorldInfo|KoboldWorldInfoFolderSelector\n    ---@param k K\n    ---@return K, any\n    function KoboldWorldInfoEntry_mt._kobold_next(t, k)\n        local _t = fields[rawget(t, \"_name\")]\n        if _t == nil then\n            return\n        end\n        return next(_t, k)\n    end\n\n    ---@param t KoboldWorldInfoEntry|KoboldWorldInfoFolder|KoboldWorldInfo|KoboldWorldInfoFolderSelector\n    ---@return function, KoboldWorldInfoEntry|KoboldWorldInfoFolder|KoboldWorldInfo|KoboldWorldInfoFolderSelector, nil\n    function KoboldWorldInfoEntry_mt.__pairs(t)\n        return next, t, nil\n    end\n\n    ---@param t KoboldWorldInfoEntry\n    function KoboldWorldInfoEntry_mt.__index(t, k)\n        if not check_validity(t) then\n            return\n        elseif k == \"uid\" then\n            return rawget(t, \"_uid\")\n        elseif type(k) == \"string\" then\n            return bridged.get_attr(t.uid, k)\n        end\n    end\n\n    ---@param t KoboldWorldInfoEntry\n    ---@return KoboldWorldInfoEntry\n    function KoboldWorldInfoEntry_mt.__newindex(t, k, v)\n        if not check_validity(t) then\n            return\n        elseif fields[rawget(t, \"_name\")] then\n            if type(k) == \"string\" and KoboldWorldInfoEntry_fieldtypes[k] == nil then\n                error(\"`\"..rawget(t, \"_name\")..\".\"..k..\"` is a read-only attribute\")\n                return\n            elseif type(k) == \"string\" and type(v) ~= KoboldWorldInfoEntry_fieldtypes[k] then\n                error(\"`\"..rawget(t, \"_name\")..\".\"..k..\"` must be a \"..KoboldWorldInfoEntry_fieldtypes[k]..\"; you attempted to set it to a \"..type(v))\n                return\n            else\n                if k ~= \"comment\" and not (t.selective and k == \"keysecondary\") then\n                    maybe_require_regeneration()\n                end\n                bridged.set_attr(t.uid, k, v)\n                return t\n            end\n        end\n        return rawset(t, k, v)\n    end\n\n\n    ----------------------------------------------------------------------------\n\n    ---@class KoboldWorldInfoFolder_base\n    ---@type table<integer, KoboldWorldInfoEntry>\n    local _ = {}\n\n    ---@class KoboldWorldInfoFolder : KoboldWorldInfoFolder_base\n    ---@field uid integer\n    ---@field name string\n    local KoboldWorldInfoFolder = setmetatable({\n        _name = \"KoboldWorldInfoFolder\",\n    }, metawrapper)\n\n    fields.KoboldWorldInfoFolder = {\n        \"uid\",\n    }\n    local KoboldWorldInfoFolder_mt = setmetatable({}, metawrapper)\n\n    ---@param u integer\n    ---@return KoboldWorldInfoEntry?\n    function KoboldWorldInfoFolder:finduid(u)\n        if not check_validity(self) or type(u) ~= \"number\" then\n            return\n        end\n        local query = _python.as_attrgetter(bridged.vars.worldinfo_u).get(u)\n        if query == nil or (rawget(self, \"_name\") == \"KoboldWorldInfoFolder\" and self.uid ~= _python.as_attrgetter(query).get(\"folder\")) then\n            return\n        end\n        local entry = deepcopy(KoboldWorldInfoEntry)\n        rawset(entry, \"_uid\", u)\n        return entry\n    end\n\n    ---@param submission? string\n    ---@param entries? KoboldWorldInfoEntry|table<any, KoboldWorldInfoEntry>\n    ---@param kwargs? table<string, any>\n    ---@return string\n    function KoboldWorldInfoFolder:compute_context(submission, entries, kwargs)\n        if not check_validity(self) then\n            return \"\"\n        elseif submission == nil then\n            submission = kobold.submission\n        elseif type(submission) ~= \"string\" then\n            error(\"`compute_context` takes a string or nil as argument #1, but got a \" .. type(submission))\n            return \"\"\n        end\n        local _entries\n        if entries ~= nil then\n            if type(entries) ~= \"table\" or (entries.name ~= nil and entries.name ~= \"KoboldWorldInfoEntry\") then\n                error(\"`compute_context` takes a KoboldWorldInfoEntry, table of KoboldWorldInfoEntries or nil as argument #2, but got a \" .. type(entries))\n                return \"\"\n            elseif entries.name == \"KoboldWorldInfoEntry\" then\n                _entries = {entries}\n            else\n                _entries = {}\n                for k, v in pairs(entries) do\n                    if type(v) == \"table\" and v.name == \"KoboldWorldInfoEntry\" and v:is_valid() then\n                        _entries[k] = v.uid\n                    end\n                end\n            end\n        end\n        local folders\n        if self.name == \"KoboldWorldInfoFolder\" then\n            folders = {rawget(self, \"_uid\")}\n        end\n        return bridged.compute_context(submission, _entries, folders, kwargs)\n    end\n\n    ---@return boolean\n    function KoboldWorldInfoFolder:is_valid()\n        return _python.as_attrgetter(bridged.vars.wifolders_d).get(rawget(self, \"_uid\")) ~= nil\n    end\n\n    ---@param t KoboldWorldInfoFolder\n    ---@return integer\n    function KoboldWorldInfoFolder_mt.__len(t)\n        if not check_validity(t) then\n            return 0\n        end\n        return math.tointeger(_python.builtins.len(_python.as_attrgetter(bridged.vars.wifolders_u).get(t.uid))) - 1\n    end\n\n    KoboldWorldInfoFolder_mt._kobold_next = KoboldWorldInfoEntry_mt._kobold_next\n\n    KoboldWorldInfoFolder_mt.__pairs = KoboldWorldInfoEntry_mt.__pairs\n\n    ---@param t KoboldWorldInfoFolder|KoboldWorldInfo\n    ---@return KoboldWorldInfoEntry?\n    function KoboldWorldInfoFolder_mt.__index(t, k)\n        if not check_validity(t) then\n            return\n        elseif rawget(t, \"_name\") == \"KoboldWorldInfoFolder\" and k == \"uid\" then\n            return rawget(t, \"_uid\")\n        elseif rawget(t, \"_name\") == \"KoboldWorldInfoFolder\" and k == \"name\" then\n            return bridged.folder_get_attr(t.uid, k)\n        elseif type(k) == \"number\" then\n            local query = rawget(t, \"_name\") == \"KoboldWorldInfoFolder\" and _python.as_attrgetter(bridged.vars.wifolders_u).get(t.uid) or bridged.vars.worldinfo_i\n            k = math.tointeger(k)\n            if k == nil or k < 1 or k > #t then\n                return\n            end\n            local entry = deepcopy(KoboldWorldInfoEntry)\n            rawset(entry, \"_uid\", math.tointeger(query[k-1].uid))\n            return entry\n        end\n    end\n\n    ---@param t KoboldWorldInfoFolder|KoboldWorldInfo\n    ---@return KoboldWorldInfoFolder|KoboldWorldInfo\n    function KoboldWorldInfoFolder_mt.__newindex(t, k, v)\n        if not check_validity(t) then\n            return\n        elseif type(k) == \"number\" and math.tointeger(k) ~= nil then\n            error(\"Cannot write to integer indices of `\"..rawget(t, \"_name\")..\"`\")\n        elseif rawget(t, \"_name\") == \"KoboldWorldInfoFolder\" and k == \"uid\" then\n            error(\"`\"..rawget(t, \"_name\")..\".\"..k..\"` is a read-only attribute\")\n        elseif t == \"name\" then\n            if type(v) ~= \"string\" then\n                error(\"`\"..rawget(t, \"_name\")..\".\"..k..\"` must be a string; you attempted to set it to a \"..type(v))\n                return\n            end\n            bridged.folder_set_attr(t.uid, k, v)\n            return t\n        else\n            return rawset(t, k, v)\n        end\n    end\n\n\n    ----------------------------------------------------------------------------\n\n    ---@class KoboldWorldInfoFolderSelector_base\n    ---@type table<integer, KoboldWorldInfoFolder>\n    local _ = {}\n\n    ---@class KoboldWorldInfoFolderSelector : KoboldWorldInfoFolderSelector_base\n    local KoboldWorldInfoFolderSelector = setmetatable({\n        _name = \"KoboldWorldInfoFolderSelector\",\n    }, metawrapper)\n    local KoboldWorldInfoFolderSelector_mt = setmetatable({}, metawrapper)\n\n    ---@param u integer\n    ---@return KoboldWorldInfoFolder?\n    function KoboldWorldInfoFolderSelector:finduid(u)\n        if not check_validity(self) or type(u) ~= \"number\" then\n            return\n        end\n        local query = _python.as_attrgetter(bridged.vars.wifolders_d).get(u)\n        if query == nil then\n            return\n        end\n        local folder = deepcopy(KoboldWorldInfoFolder)\n        rawset(folder, \"_uid\", u)\n        return folder\n    end\n\n    ---@return boolean\n    function KoboldWorldInfoFolderSelector:is_valid()\n        return true\n    end\n\n    ---@param t KoboldWorldInfoFolderSelector\n    ---@return integer\n    function KoboldWorldInfoFolderSelector_mt.__len(t)\n        if not check_validity(t) then\n            return 0\n        end\n        return _python.builtins.len(bridged.vars.wifolders_l)\n    end\n\n    KoboldWorldInfoFolderSelector_mt._kobold_next = KoboldWorldInfoEntry_mt._kobold_next\n\n    KoboldWorldInfoFolderSelector_mt.__pairs = KoboldWorldInfoEntry_mt.__pairs\n\n    ---@param t KoboldWorldInfoFolderSelector\n    ---@return KoboldWorldInfoFolder?\n    function KoboldWorldInfoFolderSelector_mt.__index(t, k)\n        if not check_validity(t) or type(k) ~= \"number\" or math.tointeger(k) == nil or k < 1 or k > #t then\n            return\n        end\n        local folder = deepcopy(KoboldWorldInfoFolder)\n        rawset(folder, \"_uid\", math.tointeger(bridged.vars.wifolders_l[k-1]))\n        return folder\n    end\n\n    ---@param t KoboldWorldInfoFolderSelector\n    ---@return KoboldWorldInfoFolderSelector\n    function KoboldWorldInfoFolderSelector_mt.__newindex(t, k, v)\n        if check_validity(t) or (type(k) == \"number\" and math.tointeger(k) ~= nil) then\n            error(\"Cannot write to integer indices of `\"..rawget(t, \"_name\")..\"`\")\n        end\n        return rawset(t, k, v)\n    end\n\n\n    ----------------------------------------------------------------------------\n\n    ---@class KoboldWorldInfo : KoboldWorldInfoFolder_base\n    local KoboldWorldInfo = setmetatable({\n        _name = \"KoboldWorldInfo\",\n    }, metawrapper)\n    local KoboldWorldInfo_mt = setmetatable({}, metawrapper)\n\n    KoboldWorldInfo.folders = KoboldWorldInfoFolderSelector\n\n    KoboldWorldInfo.finduid = KoboldWorldInfoFolder.finduid\n\n    KoboldWorldInfo.compute_context = KoboldWorldInfoFolder.compute_context\n\n    ---@return boolean\n    function KoboldWorldInfo:is_valid()\n        return true\n    end\n\n    ---@param t KoboldWorldInfo\n    ---@return integer\n    function KoboldWorldInfo_mt.__len(t)\n        if not check_validity(t) then\n            return 0\n        end\n        return math.tointeger(_python.builtins.len(bridged.vars.worldinfo)) - math.tointeger(_python.builtins.len(bridged.vars.wifolders_l)) - 1\n    end\n\n    KoboldWorldInfo_mt._kobold_next = KoboldWorldInfoEntry_mt._kobold_next\n\n    KoboldWorldInfo_mt.__pairs = KoboldWorldInfoEntry_mt.__pairs\n\n    KoboldWorldInfo_mt.__index = KoboldWorldInfoFolder_mt.__index\n\n    KoboldWorldInfo_mt.__newindex = KoboldWorldInfoFolder_mt.__newindex\n\n    kobold.worldinfo = KoboldWorldInfo\n\n\n    --==========================================================================\n    -- Userscript API: Story chunks\n    --==========================================================================\n\n    ---@class KoboldStoryChunk\n    ---@field num integer\n    ---@field content string\n    local KoboldStoryChunk = setmetatable({\n        _name = \"KoboldStoryChunk\",\n    }, metawrapper)\n    local KoboldStoryChunk_mt = setmetatable({}, metawrapper)\n\n    local KoboldStoryChunk_fields = {\n        num = false,\n        content = false,\n    }\n\n    ---@generic K\n    ---@param t KoboldStoryChunk\n    ---@param k K\n    ---@return K, any\n    function KoboldStoryChunk_mt._kobold_next(t, k)\n        k = (next(KoboldStoryChunk_fields, k))\n        return k, t[k]\n    end\n\n    ---@param t KoboldStoryChunk\n    ---@return function, KoboldStoryChunk, nil\n    function KoboldStoryChunk_mt.__pairs(t)\n        return next, t, nil\n    end\n\n    ---@param t KoboldStoryChunk\n    function KoboldStoryChunk_mt.__index(t, k)\n        if k == \"num\" then\n            return rawget(t, \"_num\")\n        end\n        if k == \"content\" then\n            if rawget(t, \"_num\") == 0 then\n                if bridged.vars.gamestarted then\n                    local prompt = koboldbridge.userstate == \"genmod\" and bridged.vars._prompt or bridged.vars.prompt\n                    return prompt\n                end\n            end\n            local actions = koboldbridge.userstate == \"genmod\" and bridged.vars._actions or bridged.vars.actions\n            return _python.as_attrgetter(actions).get(math.tointeger(rawget(t, \"_num\")) - 1)\n        end\n    end\n\n    ---@param t KoboldStoryChunk\n    function KoboldStoryChunk_mt.__newindex(t, k, v)\n        if k == \"num\" then\n            error(\"`\"..rawget(t, \"_name\")..\".\"..k..\"` is a read-only attribute\")\n            return\n        elseif k == \"content\" then\n            if type(v) ~= \"string\" then\n                error(\"`\"..rawget(t, \"_name\")..\".\"..k..\"` must be a string; you attempted to set it to a \"..type(v))\n                return\n            end\n            local _k = math.tointeger(rawget(t, \"_num\"))\n            if _k == nil or _k < 0 then\n                return\n            elseif _k == 0 and v == \"\" then\n                error(\"Attempted to set the prompt chunk's content to the empty string; this is not allowed\")\n                return\n            end\n            local actions = koboldbridge.userstate == \"genmod\" and bridged.vars._actions or bridged.vars.actions\n            if _k ~= 0 and _python.as_attrgetter(actions).get(_k-1) == nil then\n                return\n            end\n            bridged.set_chunk(_k, v)\n            maybe_require_regeneration()\n            return t\n        end\n    end\n\n\n    ----------------------------------------------------------------------------\n\n    ---@class KoboldStory_base\n    ---@type table<integer, KoboldStoryChunk>\n    local _ = {}\n\n    ---@class KoboldStory : KoboldStory_base\n    local KoboldStory = setmetatable({\n        _name = \"KoboldStory\",\n    }, metawrapper)\n    local KoboldStory_mt = setmetatable({}, metawrapper)\n\n    ---@return fun(): KoboldStoryChunk, table, nil\n    function KoboldStory:forward_iter()\n        local actions = koboldbridge.userstate == \"genmod\" and bridged.vars._actions or bridged.vars.actions\n        local nxt, iterator = _python.iter(actions)\n        local run_once = false\n        local function f()\n            if not bridged.vars.gamestarted then\n                return\n            end\n            local chunk = deepcopy(KoboldStoryChunk)\n            local _k\n            if not run_once then\n                _k = -1\n                run_once = true\n            else\n                _k = nxt(iterator)\n            end\n            if _k == nil then\n                return\n            else\n                _k = math.tointeger(_k) + 1\n            end\n            rawset(chunk, \"_num\", _k)\n            return chunk\n        end\n        return f, {}, nil\n    end\n\n    ---@return fun(): KoboldStoryChunk, table, nil\n    function KoboldStory:reverse_iter()\n        local actions = koboldbridge.userstate == \"genmod\" and bridged.vars._actions or bridged.vars.actions\n        local nxt, iterator = _python.iter(_python.builtins.reversed(actions))\n        local last_run = false\n        local function f()\n            if not bridged.vars.gamestarted or last_run then\n                return\n            end\n            local chunk = deepcopy(KoboldStoryChunk)\n            local _k = nxt(iterator)\n            if _k == nil then\n                _k = 0\n                last_run = true\n            else\n                _k = math.tointeger(_k) + 1\n            end\n            rawset(chunk, \"_num\", _k)\n            return chunk\n        end\n        return f, {}, nil\n    end\n\n    ---@param t KoboldStory\n    function KoboldStory_mt.__pairs(t)\n        return function() return nil end, t, nil\n    end\n\n    ---@param t KoboldStory\n    function KoboldStory_mt.__index(t, k)\n        if type(k) == \"number\" and math.tointeger(k) ~= nil then\n            local chunk = deepcopy(KoboldStoryChunk)\n            rawset(chunk, \"_num\", math.tointeger(k))\n            if chunk.content == nil then\n                return nil\n            end\n            return chunk\n        end\n    end\n\n    ---@param t KoboldStory\n    function KoboldStory_mt.__newindex(t, k, v)\n        error(\"`\"..rawget(t, \"_name\")..\"` is a read-only class\")\n    end\n\n    kobold.story = KoboldStory\n\n\n    --==========================================================================\n    -- Userscript API: Settings\n    --==========================================================================\n\n    ---@class KoboldSettings_base\n    ---@type table<string, any>\n    local _ = {}\n\n    ---@class KoboldSettings : KoboldSettings_base\n    ---@field numseqs integer\n    ---@field genamt integer\n    ---@field anotedepth integer\n    ---@field settemp number\n    ---@field settopp number\n    ---@field settopk integer\n    ---@field settfs number\n    ---@field settypical number\n    ---@field settopa number\n    ---@field setreppen number\n    ---@field setreppenslope number\n    ---@field setreppenrange number\n    ---@field settknmax integer\n    ---@field setwidepth integer\n    ---@field setuseprompt boolean\n    ---@field setadventure boolean\n    ---@field setdynamicscan boolean\n    ---@field setnopromptgen boolean\n    ---@field setrngpersist boolean\n    ---@field temp number\n    ---@field topp number\n    ---@field topk integer\n    ---@field top_p number\n    ---@field top_k integer\n    ---@field tfs number\n    ---@field typical number\n    ---@field topa number\n    ---@field reppen number\n    ---@field reppenslope number\n    ---@field reppenrange number\n    ---@field tknmax integer\n    ---@field widepth integer\n    ---@field useprompt boolean\n    ---@field adventure boolean\n    ---@field dynamicscan boolean\n    ---@field nopromptgen boolean\n    ---@field rngpersist boolean\n    ---@field frmttriminc boolean\n    ---@field frmtrmblln boolean\n    ---@field frmtrmspch boolean\n    ---@field frmtadsnsp boolean\n    ---@field frmtsingleline boolean\n    ---@field triminc boolean\n    ---@field rmblln boolean\n    ---@field rmspch boolean\n    ---@field adsnsp boolean\n    ---@field singleline boolean\n    ---@field chatmode boolean\n    ---@field chatname string\n    local KoboldSettings = setmetatable({\n        _name = \"KoboldSettings\",\n    }, metawrapper)\n    local KoboldSettings_mt = setmetatable({}, metawrapper)\n\n    ---@generic K\n    ---@param t KoboldSettings\n    ---@param k K\n    ---@return K, any\n    function KoboldSettings_mt._kobold_next(t, k)\n        local v\n        repeat\n            k, v = next()\n        until type(k) ~= \"string\" or k ~= \"_name\"\n        return k, v\n    end\n\n    ---@param t KoboldSettings\n    ---@return function, KoboldSettings, nil\n    function KoboldSettings_mt.__pairs(t)\n        return next, t, nil\n    end\n\n    ---@param t KoboldSettings\n    ---@return any, boolean\n    function KoboldSettings_mt.__index(t, k)\n        if type(k) ~= \"string\" then\n            return\n        end\n        if k == \"genamt\" or k == \"output\" or k == \"setoutput\" then\n            return math.tointeger(bridged.get_genamt()), true\n        elseif k == \"numseqs\" or k == \"numseq\" or k == \"setnumseq\" then\n            return math.tointeger(bridged.get_numseqs()), true\n        elseif bridged.has_setting(k) then\n            return bridged.get_setting(k), true\n        else\n            return nil, false\n        end\n    end\n\n    ---@param t KoboldSettings_base\n    function KoboldSettings_mt.__newindex(t, k, v)\n        if (k == \"genamt\" or k == \"output\" or k == \"setoutput\") and type(v) == \"number\" and math.tointeger(v) ~= nil and v >= 0 then\n            bridged.set_genamt(v)\n            koboldbridge.resend_settings_required = true\n        elseif (k == \"numseqs\" or k == \"numseq\" or k == \"setnumseq\") and type(v) == \"number\" and math.tointeger(v) ~= nil and v >= 1 then\n            if koboldbridge.userstate == \"genmod\" then\n                error(\"Cannot set numseqs from a generation modifier\")\n                return\n            end\n            bridged.set_numseqs(v)\n            koboldbridge.resend_settings_required = true\n        elseif type(k) == \"string\" and bridged.has_setting(k) and type(v) == type(bridged.get_setting(k)) then\n            if bridged.set_setting(k, v) == true then\n                maybe_require_regeneration()\n            end\n            koboldbridge.resend_settings_required = true\n        end\n        return t\n    end\n\n    kobold.settings = KoboldSettings\n\n\n    --==========================================================================\n    -- Userscript API: Memory / Author's Note\n    --==========================================================================\n\n    ---@param t KoboldLib\n    ---@return string\n    function KoboldLib_getters.memory(t)\n        return bridged.get_memory()\n    end\n\n    ---@param t KoboldLib\n    ---@param v string\n    ---@return KoboldLib\n    function KoboldLib_setters.memory(t, v)\n        if type(v) ~= \"string\" then\n            error(\"`KoboldLib.memory` must be a string; you attempted to set it to a \"..type(v))\n            return\n        end\n        maybe_require_regeneration()\n        bridged.set_memory(v)\n    end\n\n    ---@param t KoboldLib\n    ---@return string\n    function KoboldLib_getters.authorsnote(t)\n        return bridged.get_authorsnote()\n    end\n\n    ---@param t KoboldLib\n    ---@param v string\n    ---@return KoboldLib\n    function KoboldLib_setters.authorsnote(t, v)\n        if type(v) ~= \"string\" then\n            error(\"`KoboldLib.authorsnote` must be a string; you attempted to set it to a \"..type(v))\n            return\n        end\n        maybe_require_regeneration()\n        bridged.set_authorsnote(v)\n    end\n\n    ---@param t KoboldLib\n    ---@return string\n    function KoboldLib_getters.authorsnotetemplate(t)\n        return bridged.get_authorsnotetemplate()\n    end\n\n    ---@param t KoboldLib\n    ---@param v string\n    ---@return KoboldLib\n    function KoboldLib_setters.authorsnotetemplate(t, v)\n        if type(v) ~= \"string\" then\n            error(\"`KoboldLib.authorsnotetemplate` must be a string; you attempted to set it to a \"..type(v))\n            return\n        end\n        maybe_require_regeneration()\n        bridged.set_authorsnotetemplate(v)\n    end\n\n\n    --==========================================================================\n    -- Userscript API: User-submitted text (after applying input formatting)\n    --==========================================================================\n\n    ---@param t KoboldLib\n    ---@return string\n    function KoboldLib_getters.submission(t)\n        return bridged.vars.submission\n    end\n\n    ---@param t KoboldLib\n    ---@param v string\n    function KoboldLib_setters.submission(t, v)\n        if koboldbridge.userstate ~= \"inmod\" then\n            error(\"Cannot write to `KoboldLib.submission` from outside of an input modifier\")\n            return\n        elseif type(v) ~= \"string\" then\n            error(\"`KoboldLib.submission` must be a string; you attempted to set it to a \" .. type(v))\n            return\n        elseif not bridged.vars.gamestarted and v == \"\" then\n            error(\"`KoboldLib.submission` must not be set to the empty string when the story is empty\")\n            return\n        end\n        bridged.vars.submission = v\n    end\n\n\n    --==========================================================================\n    -- Userscript API: Soft prompt\n    --==========================================================================\n\n    ---@param t KoboldLib\n    ---@return string?\n    function KoboldLib_getters.spfilename(t)\n        return bridged.get_spfilename()\n    end\n\n    ---@param t KoboldLib\n    ---@param v string?\n    function KoboldLib_setters.spfilename(t, v)\n        if v:find(\"/\") or v:find(\"\\\\\") then\n            error(\"Cannot set `KoboldLib.spfilename` to a string that contains slashes\")\n        end\n        if bridged.set_spfilename(v) then\n            maybe_require_regeneration()\n        end\n    end\n\n\n    --==========================================================================\n    -- Userscript API: Model information\n    --==========================================================================\n\n    ---@param t KoboldLib\n    ---@return string\n    function KoboldLib_getters.modeltype(t)\n        return bridged.get_modeltype()\n    end\n\n    ---@param t KoboldLib\n    ---@param v string\n    function KoboldLib_setters.modeltype(t, v)\n        error(\"`KoboldLib.modeltype` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return string\n    function KoboldLib_getters.model(t)\n        return bridged.vars.model\n    end\n\n    ---@param t KoboldLib\n    ---@param v string\n    function KoboldLib_setters.model(t, v)\n        error(\"`KoboldLib.model` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return string\n    function KoboldLib_getters.modelbackend(t)\n        return bridged.get_modelbackend()\n    end\n\n    ---@param t KoboldLib\n    ---@param v string\n    function KoboldLib_setters.modelbackend(t, v)\n        error(\"`KoboldLib.modelbackend` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return boolean\n    function KoboldLib_getters.is_custommodel(t)\n        return bridged.is_custommodel()\n    end\n\n    ---@param t KoboldLib\n    ---@param v boolean\n    function KoboldLib_setters.is_custommodel(t, v)\n        error(\"`KoboldLib.is_custommodel` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return string\n    function KoboldLib_getters.custmodpth(t)\n        return bridged.vars.custmodpth\n    end\n\n    ---@param t KoboldLib\n    ---@param v string\n    function KoboldLib_setters.custmodpth(t, v)\n        error(\"`KoboldLib.custmodpth` is a read-only attribute\")\n    end\n\n\n    --==========================================================================\n    -- Userscript API: Logit Warping\n    --==========================================================================\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_getters.logits_rows(t)\n        if koboldbridge.userstate ~= \"genmod\" then\n            return 0\n        end\n        local backend = kobold.modelbackend\n        if backend == \"readonly\" or backend == \"api\" then\n            return 0\n        end\n        return kobold.settings.numseqs\n    end\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_setters.logits_rows(t)\n        error(\"`KoboldLib.logits_rows` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_getters.logits_cols(t)\n        if koboldbridge.userstate ~= \"genmod\" then\n            return 0\n        end\n        local backend = kobold.modelbackend\n        if backend == \"readonly\" or backend == \"api\" then\n            return 0\n        end\n        return math.tointeger(koboldbridge.vocab_size)\n    end\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_setters.logits_cols(t)\n        error(\"`KoboldLib.logits_cols` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return table<integer, table<integer, number>>\n    function KoboldLib_getters.logits(t)\n        if koboldbridge.userstate ~= \"genmod\" then\n            return\n        end\n        return koboldbridge.logits\n    end\n\n    ---@param t KoboldLib\n    ---@param v table<integer, table<integer, number>>\n    function KoboldLib_setters.logits(t, v)\n        if koboldbridge.userstate ~= \"genmod\" then\n            error(\"Cannot write to `KoboldLib.logits` from outside of a generation modifer\")\n            return\n        elseif type(v) ~= \"table\" then\n            error(\"`KoboldLib.logits` must be a 2D array of numbers; you attempted to set it to a \" .. type(v))\n            return\n        end\n        koboldbridge.logits = v\n    end\n\n\n    --==========================================================================\n    -- Userscript API: Generated Tokens\n    --==========================================================================\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_getters.generated_rows(t)\n        local backend = kobold.modelbackend\n        if backend == \"readonly\" or backend == \"api\" then\n            return 0\n        elseif koboldbridge.userstate == \"outmod\" then\n            return koboldbridge.num_outputs\n        end\n        return kobold.settings.numseqs\n    end\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_setters.generated_rows(t)\n        error(\"`KoboldLib.generated_rows` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_getters.generated_cols(t)\n        if koboldbridge.userstate ~= \"genmod\" then\n            return 0\n        end\n        local backend = kobold.modelbackend\n        if backend == \"readonly\" or backend == \"api\" then\n            return 0\n        end\n        return math.tointeger(koboldbridge.generated_cols)\n    end\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_setters.generated_cols(t)\n        error(\"`KoboldLib.generated_cols` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return table<integer, table<integer, integer>>\n    function KoboldLib_getters.generated(t)\n        if koboldbridge.userstate ~= \"genmod\" and koboldbridge.userstate ~= \"outmod\" then\n            return\n        end\n        local backend = kobold.modelbackend\n        if backend == \"readonly\" or backend == \"api\" then\n            return\n        end\n        return koboldbridge.generated\n    end\n\n    ---@param t KoboldLib\n    ---@param v table<integer, table<integer, integer>>\n    function KoboldLib_setters.generated(t, v)\n        if koboldbridge.userstate ~= \"genmod\" then\n            error(\"Cannot write to `KoboldLib.generated` from outside of a generation modifier\")\n            return\n        elseif type(v) ~= \"table\" then\n            error(\"`KoboldLib.generated` must be a 2D array of integers; you attempted to set it to a \" .. type(v))\n            return\n        end\n        koboldbridge.generated = v\n    end\n\n\n    --==========================================================================\n    -- Userscript API: Output\n    --==========================================================================\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_getters.num_outputs(t)\n        local model = kobold.model\n        if model == \"OAI\" or model == \"InferKit\" then\n            return 1\n        end\n        if koboldbridge.userstate == \"outmod\" then\n            return koboldbridge.num_outputs\n        end\n        return kobold.settings.numseqs\n    end\n\n    ---@param t KoboldLib\n    ---@return integer\n    function KoboldLib_setters.num_outputs(t)\n        error(\"`KoboldLib.num_outputs` is a read-only attribute\")\n    end\n\n    ---@param t KoboldLib\n    ---@return table<integer, string>\n    function KoboldLib_getters.outputs(t)\n        if koboldbridge.userstate ~= \"outmod\" then\n            return\n        end\n        return koboldbridge.outputs\n    end\n\n    ---@param t KoboldLib\n    ---@param v table<integer, string>\n    function KoboldLib_setters.outputs(t, v)\n        if koboldbridge.userstate ~= \"outmod\" then\n            error(\"Cannot write to `KoboldLib.outputs` from outside of an output modifier\")\n            return\n        elseif type(v) ~= \"table\" then\n            error(\"`KoboldLib.outputs` must be a 1D array of strings; you attempted to set it to a \" .. type(v))\n            return\n        end\n        koboldbridge.outputs = v\n    end\n\n\n    --==========================================================================\n    -- Userscript API: Utilities\n    --==========================================================================\n\n    ---@param str string\n    ---@return table<integer, integer>\n    function kobold.encode(str)\n        if type(str) ~= \"string\" then\n            error(\"`encode` takes a string as argument, but got a \" .. type(str))\n            return\n        end\n        local encoded = {}\n        for i, token in _python.enumerate(bridged.encode(str)) do\n            encoded[i+1] = math.tointeger(token)\n        end\n        return encoded\n    end\n\n    ---@param tok integer|table<integer, integer>\n    ---@return string\n    function kobold.decode(tok)\n        if type(tok) ~= \"number\" and type(tok) ~= \"table\" then\n            error(\"`decode` takes a number or table of numbers as argument, but got a \" .. type(tok))\n            return\n        end\n        if type(tok) == \"number\" then\n            tok = {tok}\n        end\n        local _tok = {}\n        local _v\n        for k, v in ipairs(tok) do\n            _v = math.tointeger(v)\n            if _v == nil then\n                error \"`decode` got a table with one or more non-integer values\"\n                return\n            end\n            _tok[k] = _v\n        end\n        return bridged.decode(_tok)\n    end\n\n    ---@return nil\n    function kobold.halt_generation()\n        koboldbridge.generating = false\n    end\n\n    ---@param sequence? integer\n    ---@return nil\n    function kobold.restart_generation(sequence)\n        if sequence == nil then\n            sequence = 0\n        end\n        sequence_type = type(sequence)\n        sequence = math.tointeger(sequence)\n        if sequence_type ~= \"number\" then\n            error(\"`kobold.restart_generation` takes an integer greater than or equal to 0 or nil as argument, but got a \" .. sequence_type)\n            return\n        elseif sequence < 0 then\n            error(\"`kobold.restart_generation` takes an integer greater than or equal to 0 or nil as argument, but got `\" .. sequence .. \"`\")\n            return\n        end\n        if koboldbridge.userstate ~= \"outmod\" then\n            error(\"Can only call `kobold.restart_generation()` from an output modifier\")\n            return\n        end\n        koboldbridge.restart_sequence = sequence\n    end\n\n    ---@param t KoboldCoreLib\n    ---@return string\n    function KoboldLib_getters.feedback(t)\n        return koboldbridge.feedback\n    end\n\n    ---@param t KoboldCoreLib\n    ---@param v string\n    ---@return KoboldCoreLib\n    function KoboldLib_setters.feedback(t, v)\n        error(\"`KoboldLib.feedback` is a read-only attribute\")\n    end\n\n\n    --==========================================================================\n    -- Core script API\n    --==========================================================================\n\n    koboldbridge.userscripts = {}  ---@type table<integer, KoboldUserScriptModule>\n    koboldbridge.userscriptmodule_filename_map = {}  ---@type table<KoboldUserScriptModule, string>\n    koboldbridge.num_userscripts = 0\n    koboldbridge.inmod = nil  ---@type function?\n    koboldbridge.genmod = nil  ---@type function?\n    koboldbridge.outmod = nil  ---@type function?\n\n    ---@class KoboldUserScript\n    ---@field inmod? function\n    ---@field genmod? function\n    ---@field outmod? function\n\n    ---@class KoboldCoreScript\n    ---@field inmod? function\n    ---@field genmod? function\n    ---@field outmod? function\n\n\n    ----------------------------------------------------------------------------\n\n    ---@class KoboldUserScriptModule\n    ---@field filename string\n    ---@field modulename string\n    ---@field description string\n    ---@field is_config_file_open boolean\n    ---@field inmod? function\n    ---@field genmod? function\n    ---@field outmod? function\n    local KoboldUserScriptModule = setmetatable({\n        _name = \"KoboldUserScriptModule\",\n    }, metawrapper)\n    local KoboldUserScriptModule_mt = setmetatable({}, metawrapper)\n\n    local KoboldUserScriptModule_fields = {\n        filename = false,\n        modulename = false,\n        description = false,\n        inmod = false,\n        genmod = false,\n        outmod = false,\n    }\n\n    ---@param clear? boolean\n    ---@return file*\n    function KoboldUserScriptModule:get_config_file(clear)\n        return get_config_file(koboldbridge.userscriptmodule_filename_map[self], clear)\n    end\n\n    ---@generic K\n    ---@param t KoboldUserScriptModule\n    ---@param k K\n    ---@return K, any\n    function KoboldUserScriptModule_mt._kobold_next(t, k)\n        k = (next(KoboldUserScriptModule_fields, k))\n        return k, t[k]\n    end\n\n    ---@param t KoboldUserScriptModule\n    ---@return function, KoboldUserScriptModule, nil\n    function KoboldUserScriptModule_mt.__pairs(t)\n        return next, t, nil\n    end\n\n    ---@param t KoboldUserScriptModule\n    function KoboldUserScriptModule_mt.__index(t, k)\n        if type(k) == \"string\" and KoboldUserScriptModule_fields[k] ~= nil then\n            return rawget(t, \"_\" .. k)\n        elseif k == \"is_config_file_open\" then\n            return is_config_file_open(koboldbridge.userscriptmodule_filename_map[t])\n        end\n        return rawget(t, k)\n    end\n\n    ---@param t KoboldUserScriptModule\n    function KoboldUserScriptModule_mt.__newindex(t, k, v)\n        error(\"`\"..rawget(t, \"_name\")..\"` is a read-only class\")\n    end\n\n\n    ----------------------------------------------------------------------------\n\n    ---@class KoboldUserScriptList_base\n    ---@type table<integer, KoboldUserScriptModule>\n    local _ = {}\n\n    ---@class KoboldUserScriptList : KoboldUserScriptList_base\n    local KoboldUserScriptList = setmetatable({\n        _name = \"KoboldUserScriptList\",\n    }, metawrapper)\n    local KoboldUserScriptList_mt = setmetatable({}, metawrapper)\n\n    ---@param t KoboldUserScriptList\n    ---@return integer\n    function KoboldUserScriptList_mt.__len(t)\n        return koboldbridge.num_userscripts\n    end\n\n    ---@param t KoboldUserScriptList\n    ---@param k integer\n    ---@return KoboldUserScriptModule?\n    function KoboldUserScriptList_mt.__index(t, k)\n        if type(k) == \"number\" and math.tointeger(k) ~= nil then\n            return koboldbridge.userscripts[k]\n        end\n    end\n\n    ---@generic K\n    ---@param t KoboldUserScriptList\n    ---@param k K\n    ---@return K, any\n    function KoboldUserScriptList_mt._kobold_next(t, k)\n        if k == nil then\n            k = 0\n        elseif type(k) ~= \"number\" then\n            return nil\n        end\n        k = k + 1\n        local v = t[k]\n        if v == nil then\n            return nil\n        end\n        return v.filename, v\n    end\n\n    ---@param t KoboldUserScriptList\n    ---@return function, KoboldUserScriptList, nil\n    function KoboldUserScriptList_mt.__pairs(t)\n        return next, t, nil\n    end\n\n    ---@param t KoboldUserScriptList\n    function KoboldUserScriptList_mt.__newindex(t, k, v)\n        error(\"`\"..rawget(t, \"_name\")..\"` is a read-only class\")\n    end\n\n\n    ----------------------------------------------------------------------------\n\n    ---@param t KoboldCoreLib\n    ---@return string\n    function KoboldCoreLib_getters.userscripts(t)\n        return koboldbridge.userscripts\n    end\n\n    ---@param t KoboldCoreLib\n    ---@param v string\n    ---@return KoboldCoreLib\n    function KoboldCoreLib_setters.userscripts(t, v)\n        error(\"`KoboldCoreLib.userscripts` is a read-only attribute\")\n    end\n\n\n    --==========================================================================\n    -- Sandboxing code\n    --==========================================================================\n\n    local envs = {}\n    koboldbridge.logging_name = nil\n    koboldbridge.filename = nil\n\n    local sandbox_require_builtins = {\n        coroutine = true,\n        package = true,\n        string = true,\n        utf8 = true,\n        table = true,\n        math = true,\n        io = true,\n        os = true,\n        debug = true,\n    }\n\n    local old_load = load\n    local function _safe_load(_g)\n        return function(chunk, chunkname, mode, env)\n            if mode == nil then\n                mode = \"t\"\n            elseif mode ~= \"t\" then\n                error(\"Calling `load` with a `mode` other than 't' is disabled for security reasons\")\n                return\n            end\n            if env == nil then\n                env = _g\n            end\n            return old_load(chunk, chunkname, mode, env)\n        end\n    end\n\n    local old_loadfile = loadfile\n    local package_loaded = {}  ---@type table<table, table>\n    local old_package_searchers = package.searchers\n    ---@param modname string\n    ---@param env table<string, any>\n    ---@param search_paths? string|table<integer, string>\n    ---@return any, string?\n    local function requirex(modname, env, search_paths)\n        if search_paths == nil then\n            search_paths = bridged.lib_paths\n        end\n        if modname == \"bridge\" then\n            return function() return env.kobold, env.koboldcore end\n        end\n        if type(modname) == \"number\" then\n            modname = tostring(modname)\n        elseif type(modname) ~= \"string\" then\n            error(\"bad argument #1 to 'require' (string expected, got \"..type(modname)..\")\")\n            return\n        end\n        if sandbox_require_builtins[modname] then\n            return env[modname]\n        end\n        local allowsearch = type(modname) == \"string\" and string.match(modname, \"[^%w._-]\") == nil and string.match(modname, \"%.%.\") == nil\n        if allowsearch and package_loaded[env] == nil then\n            package_loaded[env] = {}\n        elseif allowsearch and package_loaded[env][modname] then\n            return package_loaded[env][modname]\n        end\n        local loader, path\n        local errors = {}\n        local n_errors = 0\n        set_require_path(search_paths)\n        for k, v in ipairs(old_package_searchers) do\n            loader, path = v(modname)\n            if allowsearch and type(loader) == \"function\" then\n                break\n            elseif type(loader) == \"string\" then\n                n_errors = n_errors + 1\n                errors[n_errors] = \"\\n\\t\" .. loader\n            end\n        end\n        set_require_path(bridged.lib_paths)\n        if not allowsearch or type(loader) ~= \"function\" then\n            error(\"module '\" .. modname .. \"' not found:\" .. table.concat(errors))\n            return\n        end\n        local f, err = old_loadfile(path, \"t\", env)\n        if err ~= nil then\n            error(err)\n            return\n        end\n        local retval = (f())\n        package_loaded[env][modname] = retval == nil or retval\n        return package_loaded[env][modname], path\n    end\n    local function _safe_require(_g)\n        ---@param modname string\n        ---@return any, string?\n        return function(modname)\n            return requirex(modname, _g)\n        end\n    end\n\n    local old_input = io.input\n    ---@param file? string|file*\n    local function safe_input(file)\n        if type(file) == \"string\" then\n            error(\"Calling `io.input` with a string as argument is disabled for security reasons\")\n            return\n        end\n        return old_input(file)\n    end\n\n    local old_output = io.output\n    ---@param file? string|file*\n    local function safe_output(file)\n        if type(file) == \"string\" then\n            error(\"Calling `io.output` with a string as argument is disabled for security reasons\")\n            return\n        end\n        return old_output(file)\n    end\n\n    local old_lines = io.lines\n    ---@param filename? string\n    local function safe_lines(filename, ...)\n        if type(filename) == \"string\" then\n            error(\"Calling `io.lines` with a string as first argument is disabled for security reasons\")\n            return\n        end\n        return old_lines(filename, ...)\n    end\n\n    local function redirected_print(...)\n        local args = table.pack(...)\n        for i = 1, args.n do\n            args[i] = tostring(args[i])\n        end\n        bridged.print(table.concat(args, \"\\t\"))\n    end\n\n    local function _redirected_warn()\n        local do_warning = true\n        local control_table = {\n            [\"@on\"] = function()\n                do_warning = true\n            end,\n            [\"@off\"] = function()\n                do_warning = false\n            end,\n        }\n        return function(...)\n            local args = table.pack(...)\n            if args.n == 1 and type(args[1]) == \"string\" and args[1]:sub(1, 1) == \"@\" then\n                local f = control_table[args[1]]\n                if f ~= nil then\n                    f()\n                end\n                return\n            end\n            if not do_warning then\n                return\n            end\n            for i = 1, args.n do\n                args[i] = tostring(args[i])\n            end\n            bridged.warn(table.concat(args, \"\\t\"))\n        end\n    end\n\n    local sandbox_template_env = {\n        assert = assert,\n        connectgarbage = collectgarbage,\n        error = error,\n        getmetatable = getmetatable,\n        ipairs = ipairs,\n        load = nil,  ---@type function\n        next = next,\n        pairs = pairs,\n        pcall = pcall,\n        print = nil,   ---@type function\n        rawequal = rawequal,\n        rawget = rawget,\n        rawlen = rawlen,\n        rawset = rawset,\n        select = select,\n        setmetatable = setmetatable,\n        tonumber = tonumber,\n        tostring = tostring,\n        type = type,\n        _VERSION = _VERSION,\n        warn = nil,   ---@type function\n        xpcall = xpcall,\n        coroutine = {\n            close = coroutine.close,\n            create = coroutine.create,\n            isyieldable = coroutine.isyieldable,\n            resume = coroutine.resume,\n            running = coroutine.running,\n            status = coroutine.status,\n            wrap = coroutine.wrap,\n            yield = coroutine.yield,\n        },\n        require = nil,  ---@type function\n        package = {\n            config = package.config,\n        },\n        string = {\n            byte = string.byte,\n            char = string.char,\n            dump = string.dump,\n            find = string.find,\n            format = string.format,\n            gmatch = string.gmatch,\n            gsub = string.gsub,\n            len = string.len,\n            lower = string.lower,\n            match = string.match,\n            pack = string.pack,\n            packsize = string.packsize,\n            rep = string.rep,\n            reverse = string.reverse,\n            sub = string.sub,\n            unpack = string.unpack,\n            upper = string.upper,\n        },\n        utf8 = {\n            char = utf8.char,\n            charpattern = utf8.charpattern,\n            codes = utf8.codes,\n            codepoint = utf8.codepoint,\n            len = utf8.len,\n            offset = utf8.offset,\n        },\n        table = {\n            concat = table.concat,\n            insert = table.insert,\n            move = table.move,\n            pack = table.pack,\n            remove = table.remove,\n            sort = table.sort,\n            unpack = table.unpack,\n        },\n        math = {\n            abs = math.abs,\n            acos = math.acos,\n            asin = math.asin,\n            atan = math.atan,\n            atan2 = math.atan2,\n            ceil = math.ceil,\n            cos = math.cos,\n            cosh = math.cosh,\n            deg = math.deg,\n            exp = math.exp,\n            floor = math.floor,\n            fmod = math.fmod,\n            frexp = math.frexp,\n            huge = math.huge,\n            ldexp = math.ldexp,\n            log = math.log,\n            log10 = math.log10,\n            max = math.max,\n            maxinteger = math.maxinteger,\n            min = math.min,\n            mininteger = math.mininteger,\n            modf = math.modf,\n            pi = math.pi,\n            pow = math.pow,\n            rad = math.rad,\n            random = math.random,\n            randomseed = function() warn(\"WARNING: math.randomseed() is not permitted; please use the mt19937ar library instead\") end,\n            sin = math.sin,\n            sinh = math.sinh,\n            sqrt = math.sqrt,\n            tan = math.tan,\n            tanh = math.tanh,\n            tointeger = math.tointeger,\n            type = math.type,\n            ult = math.ult,\n        },\n        io = {\n            stdin = io.stdin,\n            stdout = io.stdout,\n            stderr = io.stderr,\n            input = safe_input,\n            output = safe_output,\n            read = io.read,\n            write = io.write,\n            close = _new_close(io.close),\n            lines = safe_lines,\n            flush = io.flush,\n            type = io.type,\n        },\n        os = {\n            clock = os.clock,\n            date = os.date,\n            difftime = os.difftime,\n            exit = function() end,\n            getenv = os.getenv,\n            time = os.time,\n            tmpname = os.tmpname,\n        },\n        debug = {\n            getinfo = debug.getinfo,\n            gethook = debug.gethook,\n            getmetatable = debug.getmetatable,\n            getuservalue = debug.getuservalue,\n            sethook = debug.sethook,\n            setmetatable = debug.setmetatable,\n            setuservalue = debug.setuservalue,\n            traceback = debug.traceback,\n            upvalueid = debug.upvalueid,\n        },\n    }\n\n    function koboldbridge.get_universe(universe)\n        local env = envs[universe]\n        if env == nil then\n            envs[universe] = deepcopy(sandbox_template_env)\n            env = envs[universe]\n            envs[universe].kobold = deepcopy(kobold)\n            if universe == 0 then\n                envs[universe].koboldcore = deepcopy(koboldcore)\n            end\n            envs[universe].load = _safe_load(env)\n            envs[universe].require = _safe_require(env)\n            envs[universe].print = redirected_print\n            envs[universe].warn = _redirected_warn()\n            env._G = env\n        end\n        return env\n    end\n\n    function koboldbridge.obliterate_multiverse()\n        for k, v in pairs(config_files) do\n            pcall(v.close, v)\n        end\n        envs = {}\n        koboldbridge.userscripts = {}\n        koboldbridge.num_userscripts = 0\n        koboldbridge.inmod = nil\n        koboldbridge.genmod = nil\n        koboldbridge.outmod = nil\n    end\n\n\n    --==========================================================================\n    -- API for aiserver.py\n    --==========================================================================\n\n    ---@return boolean\n    function koboldbridge.load_userscripts(filenames, modulenames, descriptions)\n        config_files = {}\n        config_file_filename_map = {}\n        koboldbridge.userscripts = {}\n        koboldbridge.userscriptmodule_filename_map = {}\n        koboldbridge.num_userscripts = 0\n        local has_genmod = false\n        for i, filename in _python.enumerate(filenames) do\n            bridged.load_callback(filename, modulenames[i])\n            koboldbridge.logging_name = modulenames[i]\n            koboldbridge.filename = filename\n            local f, err = old_loadfile(join_folder_and_filename(bridged.userscript_path, filename), \"t\", koboldbridge.get_universe(filename))\n            if err ~= nil then\n                error(err)\n                return false\n            end\n            ---@type KoboldUserScript\n            local _userscript = f()\n            koboldbridge.logging_name = nil\n            koboldbridge.filename = nil\n            if _userscript.genmod ~= nil then\n                has_genmod = true\n            end\n            local userscript = deepcopy(KoboldUserScriptModule)\n            rawset(userscript, \"_inmod\", function()\n                koboldbridge.logging_name = modulenames[i]\n                koboldbridge.filename = filename\n                if _userscript.inmod ~= nil then\n                    _userscript.inmod()\n                end\n                koboldbridge:clear_userscript_metadata()\n            end)\n            rawset(userscript, \"_genmod\", function()\n                koboldbridge.logging_name = modulenames[i]\n                koboldbridge.filename = filename\n                if _userscript.genmod ~= nil then\n                    _userscript.genmod()\n                end\n                koboldbridge:clear_userscript_metadata()\n            end)\n            rawset(userscript, \"_outmod\", function()\n                koboldbridge.logging_name = modulenames[i]\n                koboldbridge.filename = filename\n                if _userscript.outmod ~= nil then\n                    _userscript.outmod()\n                end\n                koboldbridge:clear_userscript_metadata()\n            end)\n            rawset(userscript, \"_filename\", filename)\n            rawset(userscript, \"_modulename\", modulenames[i])\n            rawset(userscript, \"_description\", descriptions[i])\n            koboldbridge.userscripts[i+1] = userscript\n            koboldbridge.userscriptmodule_filename_map[userscript] = filename\n            koboldbridge.num_userscripts = i + 1\n        end\n        return has_genmod\n    end\n\n    ---@return nil\n    function koboldbridge.load_corescript(filename)\n        local f, err = old_loadfile(join_folder_and_filename(bridged.corescript_path, filename), \"t\", koboldbridge.get_universe(0))\n        if err ~= nil then\n            error(err)\n            return\n        end\n        ---@type KoboldCoreScript\n        local corescript = f()\n        koboldbridge.inmod = corescript.inmod\n        koboldbridge.genmod = corescript.genmod\n        koboldbridge.outmod = corescript.outmod\n    end\n\n    function koboldbridge.execute_inmod()\n        local r\n        koboldbridge:clear_userscript_metadata()\n        koboldbridge.restart_sequence = nil\n        koboldbridge.userstate = \"inmod\"\n        koboldbridge.regeneration_required = false\n        koboldbridge.generating = true\n        koboldbridge.generated_cols = 0\n        koboldbridge.generated = {}\n        if koboldbridge.inmod ~= nil then\n            r = koboldbridge.inmod()\n        end\n        for i = 1, kobold.settings.numseqs do\n            koboldbridge.generated[i] = {}\n        end\n        koboldbridge.outputs = {}\n        for i = 1, kobold.num_outputs do\n            koboldbridge.outputs[i] = {}\n        end\n        return r\n    end\n\n    ---@return any, boolean\n    function koboldbridge.execute_genmod()\n        local r\n        koboldbridge:clear_userscript_metadata()\n        koboldbridge.generating = true\n        koboldbridge.userstate = \"genmod\"\n        if koboldbridge.genmod ~= nil then\n            local _generated = deepcopy(koboldbridge.generated)\n            if not bridged.vars.nogenmod then\n                r = koboldbridge.genmod()\n            end\n            setmetatable(koboldbridge.logits, nil)\n            for kr, vr in old_next, koboldbridge.logits, nil do\n                setmetatable(vr, nil)\n                for kc, vc in old_next, vr, nil do\n                    if type(vc) ~= \"number\" then\n                        error(\"`kobold.logits` must be a 2D table of numbers, but found a non-number element at row \" .. kr .. \", column \" .. kc)\n                        return r\n                    end\n                end\n            end\n            setmetatable(koboldbridge.generated, nil)\n            for kr, vr in old_next, koboldbridge.generated, nil do\n                setmetatable(vr, nil)\n                for kc, vc in old_next, vr, nil do\n                    if math.tointeger(vc) == nil then\n                        error(\"`kobold.generated` must be a 2D table of integers, but found a non-integer element at row \" .. kr .. \", column \" .. kc)\n                        return r\n                    end\n                    vr[kc] = math.tointeger(vc)\n                    if vr[kc] ~= _generated[kr][kc] then\n                        maybe_require_regeneration()\n                    end\n                end\n            end\n        end\n        koboldbridge.generated_cols = koboldbridge.generated_cols + 1\n        return r\n    end\n\n    function koboldbridge.execute_outmod()\n        local r\n        koboldbridge:clear_userscript_metadata()\n        koboldbridge.generating = false\n        koboldbridge.userstate = \"outmod\"\n        koboldbridge.num_outputs = kobold.settings.numseqs\n        if koboldbridge.outmod ~= nil then\n            local _outputs = deepcopy(koboldbridge.outputs)\n            r = koboldbridge.outmod()\n            setmetatable(koboldbridge.outputs, nil)\n            for k, v in old_next, koboldbridge.outputs, nil do\n                if type(v) ~= \"string\" then\n                    error(\"`kobold.outputs` must be a 1D array of strings, but found a non-string element at index \" .. k)\n                    return r\n                end\n                if v ~= _outputs[k] then\n                    maybe_require_regeneration()\n                end\n            end\n        end\n        koboldbridge.userstate = nil\n        return r\n    end\n\n\n    --==========================================================================\n    -- Footer\n    --==========================================================================\n\n    metawrapper.__newindex = nil\n    setmetatable(KoboldWorldInfoEntry, KoboldWorldInfoEntry_mt)\n    setmetatable(KoboldWorldInfoFolder, KoboldWorldInfoFolder_mt)\n    setmetatable(KoboldWorldInfoFolderSelector, KoboldWorldInfoFolderSelector_mt)\n    setmetatable(KoboldWorldInfo, KoboldWorldInfo_mt)\n    setmetatable(KoboldStoryChunk, KoboldStoryChunk_mt)\n    setmetatable(KoboldStory, KoboldStory_mt)\n    setmetatable(KoboldSettings, KoboldSettings_mt)\n    setmetatable(KoboldUserScriptModule, KoboldUserScriptModule_mt)\n    setmetatable(KoboldUserScriptList, KoboldUserScriptList_mt)\n    setmetatable(kobold, KoboldLib_mt)\n    setmetatable(koboldcore, KoboldCoreLib_mt)\n\n    return kobold, koboldcore, koboldbridge\nend\n"
        },
        {
          "name": "colab",
          "type": "tree",
          "content": null
        },
        {
          "name": "colabkobold.sh",
          "type": "blob",
          "size": 5.9453125,
          "content": "#!/bin/bash\n# KoboldAI Easy Colab Deployment Script by Henk717\n\n# read the options\nTEMP=`getopt -o m:i:p:c:d:x:a:l:z:g:t:n:b:s: --long model:,init:,path:,configname:,download:,aria2:,dloc:,xloc:,7z:,git:,tar:,ngrok:,branch:,savemodel:,localtunnel:,lt: -- \"$@\"`\neval set -- \"$TEMP\"\n\n# extract options and their arguments into variables.\nwhile true ; do\n    case \"$1\" in\n        -m|--model)\n            model=\" --model $2\" ; shift 2 ;;\n        -i|--init)\n            init=$2 ; shift 2 ;;\n        -p|--path)\n            mpath=\"$2\" ; shift 2 ;;\n        -c|--configname)\n            configname=\" --configname $2\" ; shift 2 ;;\n        -n|--ngrok)\n            ngrok=\" --ngrok\" ; shift 2 ;;\n        --lt|--localtunnel)\n            localtunnel=\" --localtunnel\" ; shift 2 ;;\n        -d|--download)\n            download=\"$2\" ; shift 2 ;;\n        -a|--aria2)\n            aria2=\"$2\" ; shift 2 ;;\n        -l|--dloc)\n            dloc=\"$2\" ; shift 2 ;;\n        -x|--xloc)\n            xloc=\"$2\" ; shift 2 ;;\n        -z|--7z)\n            z7=\"$2\" ; shift 2 ;;\n        -t|--tar)\n            tar=\"$2\" ; shift 2 ;;\n        -g|--git)\n            git=\"$2\" ; shift 2 ;;\n        -b|--branch)\n            branch=\"$2\" ; shift 2 ;;\n        -s|--savemodel)\n            savemodel=\" --savemodel\" ; shift 2 ;;\n        --) shift ; break ;;\n        *) echo \"Internal error!\" ; exit 1 ;;\n    esac\ndone\n\n# Create the Launch function so we can run KoboldAI at different places in the script\nfunction launch\n{\n    #End the script if \"--init only\" was specified.\n    if [ \"$init\" == \"only\" ]; then\n        echo Initialization complete...\n        exit 0\n    else\n    cd /content/KoboldAI-Client\n    echo \"Launching KoboldAI with the following options : python3 aiserver.py$model$kmpath$configname$ngrok$localtunnel$savemodel --colab\"\n    python3 aiserver.py$model$kmpath$configname$ngrok$localtunnel$savemodel --colab\n    exit\n    fi\n}\n\ngit_default_branch() {\n  (git remote show $git | grep 'HEAD branch' | cut -d' ' -f5) 2>/dev/null\n}\n\n# Don't allow people to mess up their system\nif [[ ! -d \"/content\" ]]; then\n    echo You can only use this script on Google Colab\n    echo Use aiserver.py to play KoboldAI locally.\n    echo Check our Readme for Colab links if you wish to play on Colab.\n    exit\nfi\n\n# Redefine the download location\nif [ \"$dloc\" == \"colab\" ]; then\n    dloc=\"/content\"\nelse\n    dloc=\"/content/drive/MyDrive/KoboldAI/models\"\nfi\n\n# Redefine the extraction location\nif [ \"$xloc\" == \"drive\" ]; then\n    xloc=\"/content/drive/MyDrive/KoboldAI/models/\"\n    dloc=\"/content\"\nelse\n    xloc=\"/content/\"\nfi\n\n# Redefine the Path to be in the relevant location\nif [[ -v mpath ]];then\nmpath=\"$xloc$mpath\"\nkmpath=\" --path $mpath\"\nfi\n\n# Create folders on Google Drive\nmkdir /content/drive/MyDrive/KoboldAI/\nmkdir /content/drive/MyDrive/KoboldAI/stories/\nmkdir /content/drive/MyDrive/KoboldAI/models/\nmkdir /content/drive/MyDrive/KoboldAI/settings/\nmkdir /content/drive/MyDrive/KoboldAI/softprompts/\nmkdir /content/drive/MyDrive/KoboldAI/userscripts/\nif [ \"$init\" == \"drive\" ]; then\n\techo Google Drive folders created.\n\texit 0\nfi\n    \n# Install and/or Update KoboldAI\nif [ \"$init\" != \"skip\" ]; then\n    cd /content\n    if [ ! -z ${git+x} ]; then\n        if [ \"$git\" == \"Official\" ]; then\n            git=https://github.com/koboldai/KoboldAI-Client\n        fi\n        if [ \"$git\" == \"United\" ]; then\n            git=https://github.com/henk717/KoboldAI-Client\n        fi\n        if [ \"$git\" == \"united\" ]; then\n            git=https://github.com/henk717/KoboldAI-Client\n        fi\n    else\n        git=https://github.com/koboldai/KoboldAI-Client\n    fi\n\n    mkdir /content/KoboldAI-Client\n    cd /content/KoboldAI-Client\n\n    git init\n    git remote remove origin\n    git remote add origin $git\n    git fetch --all\n\n    if [ ! -z ${branch+x} ]; then\n        git checkout $branch -f\n        git reset --hard origin/$branch\n    else\n        git checkout $(git_default_branch) -f\n        git reset --hard origin/$(git_default_branch)\n    fi\n\n    cd /content/KoboldAI-Client\n\n    cp -rn stories/* /content/drive/MyDrive/KoboldAI/stories/\n    cp -rn userscripts/* /content/drive/MyDrive/KoboldAI/userscripts/\n    cp -rn softprompts/* /content/drive/MyDrive/KoboldAI/softprompts/\n    rm stories\n    rm -rf stories/\n    rm userscripts\n    rm -rf userscripts/\n    rm softprompts\n    rm -rf softprompts/\n    rm models\n    rm -rf models/\n    ln -s /content/drive/MyDrive/KoboldAI/stories/ stories\n    ln -s /content/drive/MyDrive/KoboldAI/settings/ settings\n    ln -s /content/drive/MyDrive/KoboldAI/softprompts/ softprompts\n    ln -s /content/drive/MyDrive/KoboldAI/userscripts/ userscripts\n    ln -s /content/drive/MyDrive/KoboldAI/models/ models\n\n    if [ -n \"${COLAB_TPU_ADDR+set}\" ]; then\n        pip install -r requirements_mtj.txt\n    else\n        pip install -r requirements.txt\n    fi\n    \n    # Make sure Colab has the system dependencies\n    sudo apt install netbase aria2 -y\n    npm install -g localtunnel\nfi\n\ncd /content\n\n# Models extracted? Then we skip anything beyond this point for faster loading.\nif [ -f \"/content/extracted\" ]; then\n    launch\nfi\n\n# Is the model extracted on Google Drive? Skip the download and extraction\n# Only on Google Drive since it has a big impact there if we don't, and locally we have better checks in place\nif [ \"$xloc\" == \"/content/drive/MyDrive/KoboldAI/models/\"  ] && [[ -d $mpath ]];then\n    launch\nfi\n\n#Download routine for regular Downloads\nif [ ! -z ${download+x} ]; then\n    wget -c $download -P $dloc\nfi\n\n#Download routine for Aria2c scripts\nif [ ! -z ${aria2+x} ]; then\n    curl -L $aria2 | aria2c -x 10 -s 10 -j 10 -c -i- -d$dloc --user-agent=KoboldAI --file-allocation=none\nfi\n\n#Extract the model with 7z\nif [ ! -z ${z7+x} ]; then\n    7z x -o$xloc $dloc/$z7 -aos\n    touch /content/extracted\nfi\n\n#Extract the model in a ZSTD Tar file\nif [ ! -z ${tar+x} ]; then\n    git clone https://github.com/VE-FORBRYDERNE/pv\n    cd pv\n    ./configure\n    make\n    make install\n    cd ..\n    apt install zstd -y\n    pv $dloc/$tar | tar -I zstd -C $xloc -x\n    touch /content/extracted\nfi\n\nlaunch\n"
        },
        {
          "name": "commandline-rocm.sh",
          "type": "blob",
          "size": 0.05078125,
          "content": "bin/micromamba run -r runtime -n koboldai-rocm bash\n"
        },
        {
          "name": "commandline.bat",
          "type": "blob",
          "size": 0.48046875,
          "content": "@echo off\ncd /D %~dp0\nSET CONDA_SHLVL=\n\nTITLE CMD for KoboldAI Runtime\nSET /P M=<loader.settings\nIF %M%==1 GOTO drivemap\nIF %M%==2 GOTO subfolder\nIF %M%==3 GOTO drivemap_B\n\n:subfolder\nSET TEMP=%~DP0MINICONDA3\nSET TMP=%~DP0MINICONDA3\ncall miniconda3\\condabin\\activate\ncmd /k \"%*\"\n\n:drivemap\nsubst K: miniconda3 >nul\nSET TEMP=K:\\\nSET TMP=K:\\\ncall K:\\python\\condabin\\activate\ncmd /k \"%*\"\n\n:drivemap_B\nsubst B: miniconda3 >nul\nSET TEMP=B:\\\nSET TMP=B:\\\ncall B:\\python\\condabin\\activate\ncmd /k \"%*\""
        },
        {
          "name": "commandline.sh",
          "type": "blob",
          "size": 0.0458984375,
          "content": "bin/micromamba run -r runtime -n koboldai bash\n"
        },
        {
          "name": "cores",
          "type": "tree",
          "content": null
        },
        {
          "name": "customsettings_template.json",
          "type": "blob",
          "size": 0.443359375,
          "content": "{\"aria2_port\":null, \"breakmodel\":null, \"breakmodel_disklayers\":null, \"breakmodel_gpulayers\":null, \"breakmodel_layers\":null, \"colab\":null, \"configname\":null, \"cpu\":null, \"host\":null, \"localtunnel\":null, \"lowmem\":null, \"model\":null, \"ngrok\":null, \"no_aria2\":null, \"noaimenu\":null, \"nobreakmodel\":null, \"override_delete\":null, \"override_rename\":null, \"path\":null, \"port\":null, \"quiet\":null, \"remote\":null, \"revision\":null, \"savemodel\":null, \"unblock\":null}\n"
        },
        {
          "name": "disconnect-kobold-drive.bat",
          "type": "blob",
          "size": 0.119140625,
          "content": "@echo off\nSET /P M=<loader.settings\nIF %M%==3 subst /D B:\nIF %M%==1 subst /D K:\ncls\necho KoboldAI Drive disconnected\npause"
        },
        {
          "name": "docker-cuda.sh",
          "type": "blob",
          "size": 0.166015625,
          "content": "cd docker-cuda\nxhost +local:docker\ncp ../environments/huggingface.yml env.yml\ndocker-compose run --service-ports koboldai bash -c \"cd /content && python3 aiserver.py $*\"\n"
        },
        {
          "name": "docker-cuda",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-rocm.sh",
          "type": "blob",
          "size": 0.1591796875,
          "content": "cd docker-rocm\nxhost +local:docker\ncp ../environments/rocm.yml env.yml\ndocker-compose run --service-ports koboldai bash -c \"cd /content && python3 aiserver.py $*\"\n"
        },
        {
          "name": "docker-rocm",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-standalone",
          "type": "tree",
          "content": null
        },
        {
          "name": "environments",
          "type": "tree",
          "content": null
        },
        {
          "name": "extern",
          "type": "tree",
          "content": null
        },
        {
          "name": "fileops.py",
          "type": "blob",
          "size": 9.427734375,
          "content": "from os import getcwd, listdir, path\nfrom typing import Tuple, Union, Optional\nimport os\nimport json\nimport zipfile\nfrom logger import logger\n\n#==================================================================#\n#  Generic Method for prompting for file path\n#==================================================================#\ndef getsavepath(dir, title, types):\n    import tkinter as tk\n    from tkinter import filedialog\n    root = tk.Tk()\n    root.attributes(\"-topmost\", True)\n    path = tk.filedialog.asksaveasfile(\n        initialdir=dir, \n        title=title, \n        filetypes = types,\n        defaultextension=\"*.*\"\n        )\n    root.destroy()\n    if(path != \"\" and path != None):\n        return path.name\n    else:\n        return None\n\n#==================================================================#\n#  Generic Method for prompting for file path\n#==================================================================#\ndef getloadpath(dir, title, types):\n    import tkinter as tk\n    from tkinter import filedialog\n    root = tk.Tk()\n    root.attributes(\"-topmost\", True)\n    path = tk.filedialog.askopenfilename(\n        initialdir=dir, \n        title=title, \n        filetypes = types\n        )\n    root.destroy()\n    if(path != \"\" and path != None):\n        return path\n    else:\n        return None\n\n#==================================================================#\n#  Generic Method for prompting for directory path\n#==================================================================#\ndef getdirpath(dir, title):\n    import tkinter as tk\n    from tkinter import filedialog\n    root = tk.Tk()\n    root.attributes(\"-topmost\", True)\n    path = filedialog.askdirectory(\n        initialdir=dir, \n        title=title\n        )\n    root.destroy()\n    if(path != \"\" and path != None):\n        return path\n    else:\n        return None\n\n#==================================================================#\n#  Returns the path (as a string) to the given story by its name\n#==================================================================#\ndef storypath(name):\n    return path.join(\"stories\", name + \".json\")\n\n#==================================================================#\n#  Returns the path (as a string) to the given soft prompt by its filename\n#==================================================================#\ndef sppath(filename):\n    return path.join(\"softprompts\", filename)\n\n#==================================================================#\n#  Returns the path (as a string) to the given username by its filename\n#==================================================================#\ndef uspath(filename):\n    return path.join(\"userscripts\", filename)\n\n#==================================================================#\n#  Returns an array of dicts containing story files in /stories\n#==================================================================#\ndef getstoryfiles():\n    list = []\n    for file in listdir(\"stories\"):\n        if file.endswith(\".json\") and not file.endswith(\".v2.json\"):\n            ob = {}\n            ob[\"name\"] = file.replace(\".json\", \"\")\n            f = open(\"stories/\"+file, \"r\")\n            try:\n                js = json.load(f)\n            except:\n                print(f\"Browser loading error: {file} is malformed or not a JSON file.\")\n                f.close()\n                continue\n            f.close()\n            try:\n                ob[\"actions\"] = len(js[\"actions\"])\n            except TypeError:\n                print(f\"Browser loading error: {file} has incorrect format.\")\n                continue\n            list.append(ob)\n    return list\n\n#==================================================================#\n#  Checks if the given soft prompt file is valid\n#==================================================================#\ndef checksp(filename: str, model_dimension: int) -> Tuple[Union[zipfile.ZipFile, int], Optional[Tuple[int, int]], Optional[Tuple[int, int]], Optional[bool], Optional['np.dtype']]:\n    global np\n    if 'np' not in globals():\n        import numpy as np\n    try:\n        z = zipfile.ZipFile(\"softprompts/\"+filename)\n        with z.open('tensor.npy') as f:\n            # Read only the header of the npy file, for efficiency reasons\n            version: Tuple[int, int] = np.lib.format.read_magic(f)\n            shape: Tuple[int, int]\n            fortran_order: bool\n            dtype: np.dtype\n            shape, fortran_order, dtype = np.lib.format._read_array_header(f, version)\n            assert len(shape) == 2\n    except:\n        try:\n            z.close()\n        except UnboundLocalError:\n            pass\n        return 1, None, None, None, None\n    if dtype not in ('V2', np.float16, np.float32):\n        z.close()\n        return 2, version, shape, fortran_order, dtype\n    if shape[1] != model_dimension:\n        z.close()\n        return 3, version, shape, fortran_order, dtype\n    if shape[0] >= 2048:\n        z.close()\n        return 4, version, shape, fortran_order, dtype\n    return z, version, shape, fortran_order, dtype\n\n#==================================================================#\n#  Returns an array of dicts containing softprompt files in /softprompts\n#==================================================================#\ndef getspfiles(model_dimension: int):\n    lst = []\n    os.makedirs(\"softprompts\", exist_ok=True)\n    for file in listdir(\"softprompts\"):\n        if not file.endswith(\".zip\"):\n            continue\n        z, version, shape, fortran_order, dtype = checksp(file, model_dimension)\n        if z == 1:\n            logger.warning(f\"Softprompt {file} is malformed or not a soft prompt ZIP file.\")\n            continue\n        if z == 2:\n            logger.warning(f\"Softprompt {file} tensor.npy has unsupported dtype '{dtype.name}'.\")\n            continue\n        if z == 3:\n            logger.debug(f\"Softprompt {file} tensor.npy has model dimension {shape[1]} which does not match your model's model dimension of {model_dimension}. This usually means this soft prompt is not compatible with your model.\")\n            continue\n        if z == 4:\n            logger.warning(f\"Softprompt {file} tensor.npy has {shape[0]} tokens but it is supposed to have less than 2048 tokens.\")\n            continue\n        assert isinstance(z, zipfile.ZipFile)\n        try:\n            with z.open('meta.json') as f:\n                ob = json.load(f)\n        except:\n            ob = {}\n        z.close()\n        ob[\"filename\"] = file\n        ob[\"n_tokens\"] = shape[-2]\n        lst.append(ob)\n    return lst\n\n#==================================================================#\n#  Returns an array of dicts containing userscript files in /userscripts\n#==================================================================#\ndef getusfiles(long_desc=False):\n    lst = []\n    os.makedirs(\"userscripts\", exist_ok=True)\n    for file in listdir(\"userscripts\"):\n        if file.endswith(\".lua\"):\n            ob = {}\n            ob[\"filename\"] = file\n            description = []\n            multiline = False\n            with open(uspath(file)) as f:\n                ob[\"modulename\"] = f.readline().strip().replace(\"\\033\", \"\")\n                if ob[\"modulename\"][:2] != \"--\":\n                    ob[\"modulename\"] = file\n                else:\n                    ob[\"modulename\"] = ob[\"modulename\"][2:]\n                    if ob[\"modulename\"][:2] == \"[[\":\n                        ob[\"modulename\"] = ob[\"modulename\"][2:]\n                        multiline = True\n                    ob[\"modulename\"] = ob[\"modulename\"].lstrip(\"-\").strip()\n                    for line in f:\n                        line = line.strip().replace(\"\\033\", \"\")\n                        if multiline:\n                            index = line.find(\"]]\")\n                            if index > -1:\n                                description.append(line[:index])\n                                if index != len(line) - 2:\n                                    break\n                                multiline = False\n                            else:\n                                description.append(line)\n                        else:\n                            if line[:2] != \"--\":\n                                break\n                            line = line[2:]\n                            if line[:2] == \"[[\":\n                                multiline = True\n                                line = line[2:]\n                            description.append(line.strip())\n            ob[\"description\"] = \"\\n\".join(description)\n            if not long_desc:\n                if len(ob[\"description\"]) > 250:\n                    ob[\"description\"] = ob[\"description\"][:247] + \"...\"\n            lst.append(ob)\n    return lst\n\n#==================================================================#\n#  Returns True if json file exists with requested save name\n#==================================================================#\ndef saveexists(name):\n    return path.exists(storypath(name))\n\n#==================================================================#\n#  Delete save file by name; returns None if successful, or the exception if not\n#==================================================================#\ndef deletesave(name):\n    try:\n        os.remove(storypath(name))\n    except Exception as e:\n        return e\n\n#==================================================================#\n#  Rename save file; returns None if successful, or the exception if not\n#==================================================================#\ndef renamesave(name, new_name):\n    try:\n        os.replace(storypath(name), storypath(new_name))\n    except Exception as e:\n        return e\n"
        },
        {
          "name": "gensettings.py",
          "type": "blob",
          "size": 11.4404296875,
          "content": "gensettingstf = [\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"Amount to Generate\",\n\t\"id\": \"setoutput\", \n\t\"min\": 16,\n\t\"max\": 512,\n\t\"step\": 2,\n\t\"default\": 80,\n    \"tooltip\": \"Number of tokens the AI should generate. Higher numbers will take longer to generate.\"\n\t},\n   {\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Temperature\",\n\t\"id\": \"settemp\", \n\t\"min\": 0.1,\n\t\"max\": 2.0,\n\t\"step\": 0.01,\n\t\"default\": 0.5,\n    \"tooltip\": \"Randomness of sampling. High values can increase creativity but may make text less sensible. Lower values will make text more predictable but can become repetitious.\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Top p Sampling\",\n\t\"id\": \"settopp\", \n\t\"min\": 0.0,\n\t\"max\": 1.0,\n\t\"step\": 0.01,\n\t\"default\": 0.9,\n    \"tooltip\": \"Used to discard unlikely text in the sampling process. Lower values will make text more predictable but can become repetitious. (Put this value on 1 to disable its effect)\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"Top k Sampling\",\n\t\"id\": \"settopk\",\n\t\"min\": 0,\n\t\"max\": 100,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"Alternative sampling method, can be combined with top_p. (Put this value on 0 to disable its effect)\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Tail-free Sampling\",\n\t\"id\": \"settfs\", \n\t\"min\": 0.0,\n\t\"max\": 1.0,\n\t\"step\": 0.01,\n\t\"default\": 1.0,\n    \"tooltip\": \"Alternative sampling method; it is recommended to disable top_p and top_k (set top_p to 1 and top_k to 0) if using this. 0.95 is thought to be a good value. (Put this value on 1 to disable its effect)\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Typical Sampling\",\n\t\"id\": \"settypical\", \n\t\"min\": 0.0,\n\t\"max\": 1.0,\n\t\"step\": 0.01,\n\t\"default\": 1.0,\n    \"tooltip\": \"Alternative sampling method described in the paper \\\"Typical Decoding for Natural Language Generation\\\" (10.48550/ARXIV.2202.00666). The paper suggests 0.2 as a good value for this setting. Set this setting to 1 to disable its effect.\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Top a Sampling\",\n\t\"id\": \"settopa\", \n\t\"min\": 0.0,\n\t\"max\": 1.0,\n\t\"step\": 0.01,\n\t\"default\": 0.0,\n    \"tooltip\": \"Alternative sampling method that reduces the randomness of the AI whenever the probability of one token is much higher than all the others. Higher values have a stronger effect. Set this setting to 0 to disable its effect.\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Repetition Penalty\",\n\t\"id\": \"setreppen\", \n\t\"min\": 1.0,\n\t\"max\": 3.0,\n\t\"step\": 0.01,\n\t\"default\": 1.1,\n    \"tooltip\": \"Used to penalize words that were already generated or belong to the context (Going over 1.2 breaks 6B models).\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"Rep Penalty Range\",\n\t\"id\": \"setreppenrange\", \n\t\"min\": 0,\n\t\"max\": 4096,\n\t\"step\": 4,\n\t\"default\": 0,\n    \"tooltip\": \"Repetition penalty range. If set higher than 0, only applies repetition penalty to the last few tokens of your story rather than applying it to the entire story. This slider controls the amount of tokens at the end of your story to apply it to.\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Rep Penalty Slope\",\n\t\"id\": \"setreppenslope\", \n\t\"min\": 0.0,\n\t\"max\": 10.0,\n\t\"step\": 0.1,\n\t\"default\": 0.0,\n    \"tooltip\": \"Repetition penalty slope. If BOTH this setting and Rep Penalty Range are set higher than 0, will use sigmoid interpolation to apply repetition penalty more strongly on tokens that are closer to the end of your story. This setting controls the tension of the sigmoid curve; higher settings will result in the repetition penalty difference between the start and end of your story being more apparent. Setting this to 1 uses linear interpolation; setting this to 0 disables interpolation.\"\n\t},\n    {\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"Max Tokens\",\n\t\"id\": \"settknmax\", \n\t\"min\": 512,\n\t\"max\": 2048,\n\t\"step\": 8,\n\t\"default\": 1024,\n    \"tooltip\": \"Max number of tokens of context to submit to the AI for sampling. Make sure this is higher than Amount to Generate. Higher values increase VRAM/RAM usage.\"\n\t},\n    {\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"Gens Per Action\",\n\t\"id\": \"setnumseq\", \n\t\"min\": 1,\n\t\"max\": 5,\n\t\"step\": 1,\n\t\"default\": 1,\n    \"tooltip\": \"Number of results to generate per submission. Increases VRAM/RAM usage.\"\n\t},\n    {\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"W Info Depth\",\n\t\"id\": \"setwidepth\", \n\t\"min\": 1,\n\t\"max\": 5,\n\t\"step\": 1,\n\t\"default\": 3,\n    \"tooltip\": \"Number of historic actions to scan for W Info keys.\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Auto Save\",\n\t\"id\": \"autosave\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"Whether the game is saved after each action.\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Always Add Prompt\",\n\t\"id\": \"setuseprompt\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 1,\n    \"tooltip\": \"Whether the prompt should be sent in the context of every action.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Adventure Mode\",\n\t\"id\": \"setadventure\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"Turn this on if you are playing a Choose your Adventure model.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Chat Mode\",\n\t\"id\": \"setchatmode\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"This mode optimizes KoboldAI for chatting.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Dynamic WI Scan\",\n\t\"id\": \"setdynamicscan\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"Scan the AI's output for world info keys as it's generating the output.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"No Prompt Generation\",\n\t\"id\": \"setnopromptgen\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"When enabled the AI does not generate when you enter the prompt, instead you need to do an action first.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Random Story Persist\",\n\t\"id\": \"setrngpersist\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"When enabled, the Memory text box in the Random Story dialog will be prefilled by default with your current story's memory instead of being empty.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"No Genmod\",\n\t\"id\": \"setnogenmod\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n  \"tooltip\": \"Disables userscript generation modifiers.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Full Determinism\",\n\t\"id\": \"setfulldeterminism\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n  \"tooltip\": \"Causes generation to be fully deterministic -- the model will always output the same thing as long as your story, settings and RNG seed are the same. If this is off, only the sequence of outputs that the model makes will be deterministic.\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Token Streaming\",\n\t\"id\": \"setoutputstreaming\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n  \"tooltip\": \"Shows outputs to you as they are made. Does not work with more than one gens per action.\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Probability Viewer\",\n\t\"id\": \"setshowprobs\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n  \"tooltip\": \"Shows token selection probabilities. Does not work with more than one gens per action.\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Show Field Budget\",\n\t\"id\": \"setshowbudget\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n  \"tooltip\": \"Shows token usage when typing in relevant text boxes. <b>May lag slower devices.</b>\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Debug\",\n\t\"id\": \"debug\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n  \"tooltip\": \"Show debug info\"\n\t},\n]\n\ngensettingsik =[{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Temperature\",\n\t\"id\": \"settemp\", \n\t\"min\": 0.1,\n\t\"max\": 2.0,\n\t\"step\": 0.05,\n\t\"default\": 0.5,\n    \"tooltip\": \"Randomness of sampling. High values can increase creativity but may make text less sensible. Lower values will make text more predictable but can become repetitious.\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Top p Sampling\",\n\t\"id\": \"settopp\", \n\t\"min\": 0.0,\n\t\"max\": 1.0,\n\t\"step\": 0.05,\n\t\"default\": 1.1,\n    \"tooltip\": \"Used to discard unlikely text in the sampling process. Lower values will make text more predictable but can become repetitious.\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"Top k Sampling\",\n\t\"id\": \"settopk\",\n\t\"min\": 0,\n\t\"max\": 100,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"Alternative sampling method, can be combined with top_p.\"\n\t},\n\t{\n\t\"uitype\": \"slider\",\n\t\"unit\": \"float\",\n\t\"label\": \"Tail-free Sampling\",\n\t\"id\": \"settfs\", \n\t\"min\": 0.0,\n\t\"max\": 1.0,\n\t\"step\": 0.05,\n\t\"default\": 0.0,\n    \"tooltip\": \"Alternative sampling method; it is recommended to disable (set to 0) top_p and top_k if using this. 0.95 is thought to be a good value.\"\n\t},\n    {\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"Amount to Generate\",\n\t\"id\": \"setikgen\", \n\t\"min\": 50,\n\t\"max\": 3000,\n\t\"step\": 2,\n\t\"default\": 200,\n    \"tooltip\": \"Number of characters the AI should generate.\"\n\t},\n    {\n\t\"uitype\": \"slider\",\n\t\"unit\": \"int\",\n\t\"label\": \"W Info Depth\",\n\t\"id\": \"setwidepth\", \n\t\"min\": 1,\n\t\"max\": 5,\n\t\"step\": 1,\n\t\"default\": 3,\n    \"tooltip\": \"Number of historic actions to scan for W Info keys.\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Auto Save\",\n\t\"id\": \"autosave\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"Whether the game is saved after each action.\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Always Add Prompt\",\n\t\"id\": \"setuseprompt\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 1,\n    \"tooltip\": \"Whether the prompt should be sent in the context of every action.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Adventure Mode\",\n\t\"id\": \"setadventure\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"Turn this on if you are playing a Choose your Adventure model.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Chat Mode\",\n\t\"id\": \"setchatmode\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"This mode optimizes KoboldAI for chatting.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"No Prompt Generation\",\n\t\"id\": \"setnopromptgen\", \n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"When enabled the AI does not generate when you enter the prompt, instead you need to do an action first.\"\n\t},\n\t{\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Random Story Persist\",\n\t\"id\": \"setrngpersist\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n    \"tooltip\": \"When enabled, the Memory text box in the Random Story dialog will be prefilled by default with your current story's memory instead of being empty.\"\n\t},\n    {\n\t\"uitype\": \"toggle\",\n\t\"unit\": \"bool\",\n\t\"label\": \"Debug\",\n\t\"id\": \"debug\",\n\t\"min\": 0,\n\t\"max\": 1,\n\t\"step\": 1,\n\t\"default\": 0,\n  \"tooltip\": \"Show debug info\"\n\t}\n]\n\nformatcontrols = [{\n    \"label\": \"Trim incomplete sentences\",\n    \"id\": \"frmttriminc\",\n    \"tooltip\": \"Remove text after last sentence closure.  If no closure is found, all tokens will be returned.\"\n    },\n    {\n    \"label\": \"Remove blank lines\",\n    \"id\": \"frmtrmblln\",\n    \"tooltip\": \"Replace double newlines (\\\\n\\\\n) with single newlines to avoid blank lines.\"\n    },\n    {\n    \"label\": \"Remove special characters\",\n    \"id\": \"frmtrmspch\",\n    \"tooltip\": \"Remove special characters (@,#,%,^, etc)\"\n    },\n    {\n    \"label\": \"Automatic spacing\",\n    \"id\": \"frmtadsnsp\",\n    \"tooltip\": \"Add spaces automatically if needed\"\n    },\n    {\n    \"label\": \"Single Line\",\n    \"id\": \"singleline\",\n    \"tooltip\": \"Only allows the AI to output anything before the enter\"\n    }]\n"
        },
        {
          "name": "install_requirements.bat",
          "type": "blob",
          "size": 1.904296875,
          "content": "@echo off\ntitle KoboldAI Runtime Installer (MicroMamba)\n\necho Errors? Rerun this as admin so it can add the needed LongPathsEnabled registery tweak.\necho Installer failed or crashed? Run it again so it can continue.\necho Only Windows 10 and higher officially supported, older Windows installations can't handle the paths.\necho.\n\nReg add \"HKLM\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v \"LongPathsEnabled\" /t REG_DWORD /d \"1\" /f 2>nul\ncd /D %~dp0\nSET CONDA_SHLVL=\n\nif exist miniconda3\\ (\n  echo Delete existing installation?\n  echo This is required if you are switching modes, or if you get dependency errors in the game.\n  echo 1. Yes\n  echo 2. No\n  SET /P D=Type the number of the desired option and then press ENTER:\n) ELSE (\n\tSET D=Workaround\n)\nIF %D%==1 rmdir /s /q miniconda3\n\n:Mode\necho Which installation mode would you like?\necho 1. Temporary Drive Letter (Mounts the folder as drive B:, more stable and portable)\necho 2. Subfolder (Traditional method, can't run in folder paths that contain spaces)\necho.\nSET /P M=Type the number of the desired option and then press ENTER:\nIF %M%==1 GOTO drivemap\nIF %M%==2 GOTO subfolder\nECHO Incorrect choice\nGOTO MODE\n\n\n:drivemap\necho 3 > loader.settings\nsubst B: /D >nul\nmkdir miniconda3\nsubst B: miniconda3\nSET TEMP=B:\\\nSET TMP=B:\\\ncopy umamba.exe B:\\umamba.exe\ncopy loader.settings B:\\loader.settings\ncopy disconnect-kobold-drive.bat B:\\disconnect-kobold-drive.bat\nB:\numamba.exe create -r B:\\python\\ -n base\numamba.exe install --no-shortcuts -r B:\\python\\ -n base -f \"%~dp0\\environments\\huggingface.yml\" -y --always-copy\numamba.exe -r B:\\ clean -a -y\nrd B:\\Python\\pkgs /S /Q\nsubst B: /d\npause\nexit\n\n:subfolder\necho 2 > loader.settings\nSET TEMP=%~DP0MINICONDA3\nSET TMP=%~DP0MINICONDA3\numamba.exe create -r miniconda3\\ -n base\numamba.exe install --no-shortcuts -r miniconda3 -n base -f environments\\huggingface.yml -y --always-copy\numamba.exe clean -a -y\nrd miniconda3\\Python\\pkgs /S /Q\npause\nexit\n"
        },
        {
          "name": "install_requirements.sh",
          "type": "blob",
          "size": 0.888671875,
          "content": "#!/bin/bash\nif [[ $1 = \"cuda\" || $1 = \"CUDA\" ]]; then\nwget -qO- https://micromamba.snakepit.net/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\nbin/micromamba create -f environments/huggingface.yml -r runtime -n koboldai -y\n# Weird micromamba bug causes it to fail the first time, running it twice just to be safe, the second time is much faster\nbin/micromamba create -f environments/huggingface.yml -r runtime -n koboldai -y\nexit\nfi\nif [[ $1 = \"rocm\" || $1 = \"ROCM\" ]]; then\nwget -qO- https://micromamba.snakepit.net/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\nbin/micromamba create -f environments/rocm.yml -r runtime -n koboldai-rocm -y\n# Weird micromamba bug causes it to fail the first time, running it twice just to be safe, the second time is much faster\nbin/micromamba create -f environments/rocm.yml -r runtime -n koboldai-rocm -y\nexit\nfi\necho Please specify either CUDA or ROCM\n"
        },
        {
          "name": "koboldai.ico",
          "type": "blob",
          "size": 149.9697265625,
          "content": null
        },
        {
          "name": "koboldaiblue.ico",
          "type": "blob",
          "size": 151.505859375,
          "content": null
        },
        {
          "name": "koboldaigreen.ico",
          "type": "blob",
          "size": 151.267578125,
          "content": null
        },
        {
          "name": "logger.py",
          "type": "blob",
          "size": 4.0361328125,
          "content": "import sys\r\nfrom functools import partialmethod\r\nfrom loguru import logger\r\n\r\nSTDOUT_LEVELS = [\"GENERATION\", \"PROMPT\"]\r\nINIT_LEVELS = [\"INIT\", \"INIT_OK\", \"INIT_WARN\", \"INIT_ERR\"]\r\nMESSAGE_LEVELS = [\"MESSAGE\"]\r\n# By default we're at error level or higher\r\nverbosity = 20\r\nquiet = 0\r\n\r\ndef set_logger_verbosity(count):\r\n    global verbosity\r\n    # The count comes reversed. So count = 0 means minimum verbosity\r\n    # While count 5 means maximum verbosity\r\n    # So the more count we have, the lowe we drop the versbosity maximum\r\n    verbosity = 20 - (count * 10)\r\n\r\ndef quiesce_logger(count):\r\n    global quiet\r\n    # The bigger the count, the more silent we want our logger\r\n    quiet = count * 10\r\n\r\ndef is_stdout_log(record):\r\n    if record[\"level\"].name not in STDOUT_LEVELS:\r\n        return(False)\r\n    if record[\"level\"].no < verbosity + quiet:\r\n        return(False)\r\n    return(True)\r\n\r\ndef is_init_log(record):\r\n    if record[\"level\"].name not in INIT_LEVELS:\r\n        return(False)\r\n    if record[\"level\"].no < verbosity + quiet:\r\n        return(False)\r\n    return(True)\r\n\r\ndef is_msg_log(record):\r\n    if record[\"level\"].name not in MESSAGE_LEVELS:\r\n        return(False)\r\n    if record[\"level\"].no < verbosity + quiet:\r\n        return(False)\r\n    return(True)\r\n\r\ndef is_stderr_log(record):\r\n    if record[\"level\"].name in STDOUT_LEVELS + INIT_LEVELS + MESSAGE_LEVELS:\r\n        return(False)\r\n    if record[\"level\"].no < verbosity + quiet:\r\n        return(False)\r\n    return(True)\r\n\r\ndef test_logger():\r\n    logger.generation(\"This is a generation message\\nIt is typically multiline\\nThee Lines\".encode(\"unicode_escape\").decode(\"utf-8\"))\r\n    logger.prompt(\"This is a prompt message\")\r\n    logger.debug(\"Debug Message\")\r\n    logger.info(\"Info Message\")\r\n    logger.warning(\"Info Warning\")\r\n    logger.error(\"Error Message\")\r\n    logger.critical(\"Critical Message\")\r\n    logger.init(\"This is an init message\", status=\"Starting\")\r\n    logger.init_ok(\"This is an init message\", status=\"OK\")\r\n    logger.init_warn(\"This is an init message\", status=\"Warning\")\r\n    logger.init_err(\"This is an init message\", status=\"Error\")\r\n    logger.message(\"This is user message\")\r\n    sys.exit()\r\n\r\n\r\nlogfmt = \"<level>{level: <10}</level> | <green>{name}</green>:<green>{function}</green>:<green>{line}</green> - <level>{message}</level>\"\r\ngenfmt = \"<level>{level: <10}</level> @ <green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{message}</level>\"\r\ninitfmt = \"<magenta>INIT      </magenta> | <level>{extra[status]: <10}</level> | <magenta>{message}</magenta>\"\r\nmsgfmt = \"<level>{level: <10}</level> | <level>{message}</level>\"\r\n\r\nlogger.level(\"GENERATION\", no=24, color=\"<cyan>\")\r\nlogger.level(\"PROMPT\", no=23, color=\"<yellow>\")\r\nlogger.level(\"INIT\", no=31, color=\"<white>\")\r\nlogger.level(\"INIT_OK\", no=31, color=\"<green>\")\r\nlogger.level(\"INIT_WARN\", no=31, color=\"<yellow>\")\r\nlogger.level(\"INIT_ERR\", no=31, color=\"<red>\")\r\n# Messages contain important information without which this application might not be able to be used\r\n# As such, they have the highest priority\r\nlogger.level(\"MESSAGE\", no=61, color=\"<green>\")\r\n\r\nlogger.__class__.generation = partialmethod(logger.__class__.log, \"GENERATION\")\r\nlogger.__class__.prompt = partialmethod(logger.__class__.log, \"PROMPT\")\r\nlogger.__class__.init = partialmethod(logger.__class__.log, \"INIT\")\r\nlogger.__class__.init_ok = partialmethod(logger.__class__.log, \"INIT_OK\")\r\nlogger.__class__.init_warn = partialmethod(logger.__class__.log, \"INIT_WARN\")\r\nlogger.__class__.init_err = partialmethod(logger.__class__.log, \"INIT_ERR\")\r\nlogger.__class__.message = partialmethod(logger.__class__.log, \"MESSAGE\")\r\n\r\nconfig = {\r\n    \"handlers\": [\r\n        {\"sink\": sys.stderr, \"format\": logfmt, \"colorize\":True, \"filter\": is_stderr_log},\r\n        {\"sink\": sys.stdout, \"format\": genfmt, \"level\": \"PROMPT\", \"colorize\":True, \"filter\": is_stdout_log},\r\n        {\"sink\": sys.stdout, \"format\": initfmt, \"level\": \"INIT\", \"colorize\":True, \"filter\": is_init_log},\r\n        {\"sink\": sys.stdout, \"format\": msgfmt, \"level\": \"MESSAGE\", \"colorize\":True, \"filter\": is_msg_log}\r\n    ],\r\n}\r\nlogger.configure(**config)\r\n"
        },
        {
          "name": "maps",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "play-rocm.sh",
          "type": "blob",
          "size": 0.1689453125,
          "content": "#!/bin/bash\nif [ ! -f \"runtime/envs/koboldai-rocm/bin/python\" ]; then\n./install_requirements.sh rocm\nfi\nbin/micromamba run -r runtime -n koboldai-rocm python aiserver.py $*\n"
        },
        {
          "name": "play.bat",
          "type": "blob",
          "size": 0.701171875,
          "content": "@echo off\ncd /D %~dp0\nSET CONDA_SHLVL=\n\nrmdir /S /Q flask_session\n\nTITLE KoboldAI - Server\nSET /P M=<loader.settings\nIF %M%==1 GOTO drivemap\nIF %M%==2 GOTO subfolder\nIF %M%==3 GOTO drivemap_B\n\n:subfolder\nECHO Runtime launching in subfolder mode\nSET TEMP=%~DP0MINICONDA3\nSET TMP=%~DP0MINICONDA3\ncall miniconda3\\condabin\\activate\npython aiserver.py %*\ncmd /k\n\n:drivemap\nECHO Runtime launching in K: drive mode\nsubst /D K: >nul\nsubst K: miniconda3 >nul\nSET TEMP=K:\\\nSET TMP=K:\\\ncall K:\\python\\condabin\\activate\npython aiserver.py %*\ncmd /k\n\n:drivemap_B\nECHO Runtime launching in B: drive mode\nsubst /D B: >nul\nsubst B: miniconda3 >nul\nSET TEMP=B:\\\nSET TMP=B:\\\ncall B:\\python\\condabin\\activate\npython aiserver.py %*\ncmd /k"
        },
        {
          "name": "play.sh",
          "type": "blob",
          "size": 0.1591796875,
          "content": "#!/bin/bash\nif [ ! -f \"runtime/envs/koboldai/bin/python\" ]; then\n./install_requirements.sh cuda\nfi\nbin/micromamba run -r runtime -n koboldai python aiserver.py $*\n"
        },
        {
          "name": "prompt_tuner.py",
          "type": "blob",
          "size": 47.46484375,
          "content": "import abc\nimport os\nimport sys\nimport math\nimport numpy as np\nimport termcolor\nimport contextlib\nimport traceback\nimport random\nimport zipfile\nimport json\nimport uuid\nimport datetime\nimport base64\nimport pickle\nimport hashlib\nimport itertools\nimport functools\nimport bisect\nimport eventlet\nimport packaging\nimport gc\nimport time\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Embedding, CrossEntropyLoss\nimport transformers\nfrom transformers import __version__ as transformers_version\nfrom transformers import AutoTokenizer, GPT2Tokenizer, AutoConfig, AutoModelForCausalLM, GPTNeoForCausalLM, PreTrainedModel, modeling_utils\nimport accelerate\nimport accelerate.utils\nfrom mkultra.tuning import GPTPromptTuningMixin, GPTNeoPromptTuningLM\nfrom mkultra.soft_prompt import SoftPrompt\nfrom typing import Dict, List, Optional, TextIO, Union\n\nimport logging\nlogging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n\nimport breakmodel\nimport torch_lazy_loader\nimport utils\n\nuse_breakmodel = True\n\n\nclass colors:\n    PURPLE    = '\\033[95m'\n    BLUE      = '\\033[94m'\n    CYAN      = '\\033[96m'\n    GREEN     = '\\033[92m'\n    YELLOW    = '\\033[93m'\n    RED       = '\\033[91m'\n    END       = '\\033[0m'\n    UNDERLINE = '\\033[4m'\n\nclass Send_to_socketio(object):\n    def write(self, bar):\n        print(bar, end=\"\")\n        time.sleep(0.01)\n        try:\n            if utils.emit is not None:\n                utils.emit('from_server', {'cmd': 'model_load_status', 'data': bar.replace(\" \", \"&nbsp;\")}, broadcast=True)\n        except:\n            pass\n\ndef patch_transformers_download():\n    global transformers\n    import copy, requests, tqdm, time\n    class Send_to_socketio(object):\n        def write(self, bar):\n            bar = bar.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n            if bar != \"\":\n                try:\n                    print(bar, end=\"\\r\")\n                    if utils.emit is not None:\n                        utils.emit('from_server', {'cmd': 'model_load_status', 'data': bar.replace(\" \", \"&nbsp;\")}, broadcast=True)\n                    eventlet.sleep(seconds=0)\n                except:\n                    pass\n    def http_get(\n        url: str,\n        temp_file: transformers.utils.hub.BinaryIO,\n        proxies=None,\n        resume_size=0,\n        headers: transformers.utils.hub.Optional[transformers.utils.hub.Dict[str, str]] = None,\n        file_name: transformers.utils.hub.Optional[str] = None,\n    ):\n        \"\"\"\n        Download remote file. Do not gobble up errors.\n        \"\"\"\n        headers = copy.deepcopy(headers)\n        if resume_size > 0:\n            headers[\"Range\"] = f\"bytes={resume_size}-\"\n        r = requests.get(url, stream=True, proxies=proxies, headers=headers)\n        transformers.utils.hub._raise_for_status(r)\n        content_length = r.headers.get(\"Content-Length\")\n        total = resume_size + int(content_length) if content_length is not None else None\n        # `tqdm` behavior is determined by `utils.logging.is_progress_bar_enabled()`\n        # and can be set using `utils.logging.enable/disable_progress_bar()`\n        if url[-11:] != 'config.json':\n            progress = tqdm.tqdm(\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n                total=total,\n                initial=resume_size,\n                desc=f\"Downloading {file_name}\" if file_name is not None else \"Downloading\",\n                file=Send_to_socketio(),\n            )\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                if url[-11:] != 'config.json':\n                    progress.update(len(chunk))\n                temp_file.write(chunk)\n        if url[-11:] != 'config.json':\n            progress.close()\n\n    transformers.utils.hub.http_get = http_get\n\n\ndef patch_transformers():\n    global transformers\n    \n    patch_transformers_download()\n    \n    old_from_pretrained = PreTrainedModel.from_pretrained.__func__\n    @classmethod\n    def new_from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        utils.num_shards = None\n        utils.current_shard = 0\n        utils.from_pretrained_model_name = pretrained_model_name_or_path\n        utils.from_pretrained_index_filename = None\n        utils.from_pretrained_kwargs = kwargs\n        utils.bar = None\n        if utils.args is None or not utils.args.no_aria2:\n            utils.aria2_hook(pretrained_model_name_or_path, **kwargs)\n        return old_from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n    if(not hasattr(PreTrainedModel, \"_kai_patched\")):\n        PreTrainedModel.from_pretrained = new_from_pretrained\n        PreTrainedModel._kai_patched = True\n    if(hasattr(modeling_utils, \"get_checkpoint_shard_files\")):\n        old_get_checkpoint_shard_files = modeling_utils.get_checkpoint_shard_files\n        def new_get_checkpoint_shard_files(pretrained_model_name_or_path, index_filename, *args, **kwargs):\n            utils.num_shards = utils.get_num_shards(index_filename)\n            utils.from_pretrained_index_filename = index_filename\n            return old_get_checkpoint_shard_files(pretrained_model_name_or_path, index_filename, *args, **kwargs)\n        modeling_utils.get_checkpoint_shard_files = new_get_checkpoint_shard_files\n        \n    # Some versions of transformers 4.17.0.dev0 are affected by\n    # https://github.com/huggingface/transformers/issues/15736\n    # This is a workaround for those versions of transformers.\n    if(transformers_version == \"4.17.0.dev0\"):\n        try:\n            from transformers.models.xglm.modeling_xglm import XGLMSinusoidalPositionalEmbedding\n        except ImportError:\n            pass\n        else:\n            @torch.no_grad()\n            def new_forward(self, input_ids: torch.Tensor = None, inputs_embeds: torch.Tensor = None, past_key_values_length: int = 0):\n                bsz, seq_len = inputs_embeds.size()[:-1]\n                input_shape = inputs_embeds.size()[:-1]\n                sequence_length = input_shape[1]\n                position_ids = torch.arange(\n                    past_key_values_length + self.padding_idx + 1, past_key_values_length + sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n                ).unsqueeze(0).expand(input_shape).contiguous()\n                max_pos = self.padding_idx + 1 + seq_len + past_key_values_length\n                if max_pos > self.weights.size(0):\n                    self.make_weights(max_pos + self.offset, self.embedding_dim, self.padding_idx)\n                return self.weights.index_select(0, position_ids.view(-1)).view(bsz, seq_len, -1).detach()\n            XGLMSinusoidalPositionalEmbedding.forward = new_forward\n\n\n    # Fix a bug in OPTForCausalLM where self.lm_head is the wrong size\n    if(packaging.version.parse(\"4.19.0.dev0\") <= packaging.version.parse(transformers_version) < packaging.version.parse(\"4.20.0\")):\n        try:\n            from transformers import OPTForCausalLM, OPTModel\n        except ImportError:\n            pass\n        else:\n            # This is the same as the original __init__ but with\n            # config.hidden_size\n            # replaced with\n            # config.word_embed_proj_dim\n            def new_init(self, config):\n                super(OPTForCausalLM, self).__init__(config)\n                self.model = OPTModel(config)\n                self.lm_head = torch.nn.Linear(config.word_embed_proj_dim, config.vocab_size, bias=False)\n                self.post_init()\n            OPTForCausalLM.__init__ = new_init\n\n\ndef device_list(n_layers, primary=None, selected=None):\n    device_count = torch.cuda.device_count()\n    if(device_count < 2):\n        primary = None\n    gpu_blocks = breakmodel.gpu_blocks + (device_count - len(breakmodel.gpu_blocks))*[0]\n    print(f\"{colors.YELLOW}       DEVICE ID  |  LAYERS  |  DEVICE NAME{colors.END}\")\n    for i in range(device_count):\n        name = torch.cuda.get_device_name(i)\n        if(len(name) > 47):\n            name = \"...\" + name[-44:]\n        row_color = colors.END\n        sep_color = colors.YELLOW\n        print(f\"{row_color}{colors.YELLOW + '->' + row_color if i == selected else '  '} {'(primary)' if i == primary else ' '*9} {i:3}  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}\")\n    row_color = colors.END\n    sep_color = colors.YELLOW\n    print(f\"{row_color}{colors.YELLOW + '->' + row_color if -1 == selected else '  '} {' '*9} N/A  {sep_color}|{row_color}     {breakmodel.disk_blocks:3}  {sep_color}|{row_color}  (Disk cache){colors.END}\")\n    print(f\"{row_color}   {' '*9} N/A  {sep_color}|{row_color}     {n_layers:3}  {sep_color}|{row_color}  (CPU){colors.END}\")\n\n\ndef move_model_to_devices(model, usegpu, gpu_device):\n    global generator\n\n    if(not use_breakmodel):\n        if(usegpu):\n            model = model.half().to(gpu_device)\n        else:\n            model = model.to('cpu').float()\n        generator = model.generate\n        return\n\n    for key, value in model.state_dict().items():\n        target_dtype = torch.float32 if breakmodel.primary_device == \"cpu\" else torch.float16\n        if(value.dtype is not target_dtype):\n            accelerate.utils.set_module_tensor_to_device(model, key, target_dtype)\n    disk_blocks = breakmodel.disk_blocks\n    gpu_blocks = breakmodel.gpu_blocks\n    ram_blocks = len(utils.layers_module_names) - sum(gpu_blocks)\n    cumulative_gpu_blocks = tuple(itertools.accumulate(gpu_blocks))\n    device_map = {}\n    for name in utils.layers_module_names:\n        layer = int(name.rsplit(\".\", 1)[1])\n        device = (\"disk\" if layer < disk_blocks else \"cpu\") if layer < ram_blocks else bisect.bisect_right(cumulative_gpu_blocks, layer - ram_blocks)\n        device_map[name] = device\n    for name in utils.get_missing_module_names(model, list(device_map.keys())):\n        device_map[name] = breakmodel.primary_device\n    breakmodel.dispatch_model_ex(model, device_map, main_device=breakmodel.primary_device, offload_buffers=True, offload_dir=\"accelerate-disk-cache\")\n    gc.collect()\n    generator = model.generate\n    return\n\n\n_PromptTuningPreTrainedModel = Union[\"UniversalPromptTuningMixin\", GPTPromptTuningMixin, transformers.PreTrainedModel]\n\nclass _WTEDummy:\n    def __init__(self, model: transformers.PreTrainedModel):\n        self.model = model\n\n    @property\n    def wte(self: \"_WTEDummy\"):\n        return self.model.get_input_embeddings()\n\n    @wte.setter\n    def wte(self: \"_WTEDummy\", v):\n        self.model.set_input_embeddings(v)\n\nclass _WTEMixin:\n    @property\n    def wte(self: Union[\"_WTEMixin\", transformers.PreTrainedModel]):\n        return self.get_input_embeddings()\n\n    @wte.setter\n    def wte(self: Union[\"_WTEMixin\", transformers.PreTrainedModel], v):\n        self.set_input_embeddings(v)\n\n\nclass UniversalPromptTuningMixin:\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: str, **kwargs):\n        model: _PromptTuningPreTrainedModel = super().from_pretrained(pretrained_model_name_or_path, **kwargs)\n\n        if not hasattr(model, \"transformer\"):\n            model.transformer = _WTEDummy(model)\n        elif not hasattr(model.transformer, \"wte\"):\n            assert isinstance(model.transformer, type)\n            model.transformer.__class__ = type(\"_UniversalPromptTuning\" + model.transformer.__class__.__name__, (_WTEMixin, model.transformer.__class__), {})\n\n        model.__class__ = type(\"_UniversalPromptTuning\" + model.__class__.__name__, (UniversalPromptTuningMixin, model.__class__), {})\n\n        for param in model.parameters():\n            param.requires_grad = False\n        model.initialize_soft_prompt()\n\n        return model\n\n    def forward(\n        self: _PromptTuningPreTrainedModel,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ):\n        assert input_ids is not None\n        assert input_ids.ndim == 2\n\n        input_ids = F.pad(input_ids, (self.learned_embedding.size(0), 0, 0, 0), value=self.transformer.wte.weight.size(0) // 2)\n\n        if labels is not None:\n            labels = self._extend_labels(labels)\n\n        if attention_mask is not None:\n            attention_mask = self._extend_attention_mask(attention_mask)\n\n        old_embedding_call = Embedding.__call__\n        model = self\n\n        def new_embedding_call(self, input_ids, *args, **kwargs):\n            inputs_embeds = old_embedding_call(self, input_ids, *args, **kwargs)\n            if model.transformer.wte is self:\n                assert inputs_embeds.ndim == 3\n                inputs_embeds[:, :model.learned_embedding.size(0), :] = model.learned_embedding[None]\n            return inputs_embeds\n\n        Embedding.__call__ = new_embedding_call\n\n        try:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n                use_cache=use_cache,\n                return_dict=return_dict,\n            )\n        finally:\n            Embedding.__call__ = old_embedding_call\n\nfor k in dir(GPTPromptTuningMixin):\n    v = getattr(GPTPromptTuningMixin, k)\n    _v = getattr(UniversalPromptTuningMixin, k, None)\n    if _v is None or (_v is getattr(object, k, None) and callable(_v) and not isinstance(_v, type)):\n        setattr(UniversalPromptTuningMixin, k, v)\n\n\nclass AutoPromptTuningLM(UniversalPromptTuningMixin, transformers.AutoModelForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n\n\ndefault_quiet = False\n\n\ndef get_tokenizer(model_id, revision=None) -> transformers.PreTrainedTokenizerBase:\n    if(os.path.isdir(model_id)):\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision, cache_dir=\"cache\", use_fast=False)\n        except Exception as e:\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision, cache_dir=\"cache\")\n            except Exception as e:\n                try:\n                    tokenizer = GPT2Tokenizer.from_pretrained(model_id, revision=revision, cache_dir=\"cache\")\n                except Exception as e:\n                    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=revision, cache_dir=\"cache\")\n    elif(os.path.isdir(\"models/{}\".format(model_id.replace('/', '_')))):\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\"models/{}\".format(model_id.replace('/', '_')), revision=revision, cache_dir=\"cache\", use_fast=False)\n        except Exception as e:\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(\"models/{}\".format(model_id.replace('/', '_')), revision=revision, cache_dir=\"cache\")\n            except Exception as e:\n                try:\n                    tokenizer = GPT2Tokenizer.from_pretrained(\"models/{}\".format(model_id.replace('/', '_')), revision=revision, cache_dir=\"cache\")\n                except Exception as e:\n                    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=revision, cache_dir=\"cache\")\n    else:\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision, cache_dir=\"cache\", use_fast=False)\n        except Exception as e:\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision, cache_dir=\"cache\")\n            except Exception as e:\n                try:\n                    tokenizer = GPT2Tokenizer.from_pretrained(model_id, revision=revision, cache_dir=\"cache\")\n                except Exception as e:\n                    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=revision, cache_dir=\"cache\")\n\n    @contextlib.contextmanager\n    def _kai_no_prefix():\n        add_bos_token = getattr(tokenizer, \"add_bos_token\", False)\n        add_prefix_space = getattr(tokenizer, \"add_prefix_space\", False)\n        tokenizer.add_bos_token = False\n        tokenizer.add_prefix_space = False\n        try:\n            yield\n        finally:\n            tokenizer.add_bos_token = add_bos_token\n            tokenizer.add_prefix_space = add_prefix_space\n\n    tokenizer._kai_no_prefix = _kai_no_prefix\n    return tokenizer\n\n\nclass ConfigurationError(Exception):\n    def __init__(self, msg: str = \"Unknown error\", code: int = 1, quiet: Optional[bool] = None):\n        if quiet is None:\n            quiet = default_quiet\n        super().__init__(msg)\n        self.code = code\n        self.quiet = quiet\n\n\nclass TrainerBase(abc.ABC):\n    @abc.abstractmethod\n    def startup(self, step: int) -> None:\n        ...\n\n    @abc.abstractmethod\n    def get_batch(self, step: int, size: int) -> np.ndarray:\n        ...\n\n    @abc.abstractmethod\n    def get_num_sequences(self) -> int:\n        ...\n\n    @abc.abstractmethod\n    def get_initial_soft_embeddings(self, model: transformers.PreTrainedModel) -> SoftPrompt:\n        ...\n\n    @abc.abstractmethod\n    def tokenize_dataset_callback(self, tokenizer: transformers.PreTrainedTokenizerBase, text: str) -> List[int]:\n        ...\n\n    class TrainerData:\n        def __init__(self):\n            self.__lazy_load_spec: Optional[dict] = None\n            self.model_spec: Optional[dict] = None\n            self.tokenizer_id: Optional[str] = None\n            self.newlinemode: Optional[str] = None\n            self.ckpt_path: Optional[str] = None\n            self.save_file: Optional[str] = None\n            self.params: Optional[dict] = None\n            self.stparams: Optional[dict] = None\n            self.gradient_accumulation_steps = -1\n            self.soft_in_dim = -1\n            self.prompt_method = \"tokens\"\n            self.prompt_seed = 42\n\n        @property\n        def lazy_load_spec(self):\n            print(\"WARNING:  `TrainerData.lazy_load_spec` is currently unused\", file=sys.stderr)\n            return self.__lazy_load_spec\n\n        @lazy_load_spec.setter\n        def lazy_load_spec(self, value: Optional[dict]):\n            print(\"WARNING:  `TrainerData.lazy_load_spec` is currently unused\", file=sys.stderr)\n            self.__lazy_load_spec = value\n\n        @property\n        def kaiming_size(self):  # backwards compatibility\n            return self.soft_in_dim\n\n        @kaiming_size.setter\n        def kaiming_size(self, value: int):  # backwards compatibility\n            self.prompt_method = \"kaiming\"\n            self.soft_in_dim = value\n\n    data: TrainerData\n\n    def __init__(self, universe: Optional[int] = None, quiet=False):\n        self.quiet = quiet\n        self.universe = universe\n        self.data = self.TrainerData()\n        self._spmodule: Optional[str] = None\n        if universe is not None:\n            print(\"WARNING:  The `universe` argument of `TrainerBase.__init__` is currently unused\", file=sys.stderr)\n\n    def raise_configuration_error(self, msg, **kwargs):\n        if \"quiet\" not in kwargs:\n            kwargs[\"quiet\"] = self.quiet\n        raise ConfigurationError(msg, **kwargs)\n    \n    def _get_model_config(self) -> transformers.configuration_utils.PretrainedConfig:\n        REVISION = None\n        if(os.path.isdir(self.data.ckpt_path)):\n            model_config     = AutoConfig.from_pretrained(self.data.ckpt_path, revision=REVISION, cache_dir=\"cache\")\n        elif(os.path.isdir(\"models/{}\".format(self.data.ckpt_path.replace('/', '_')))):\n            model_config     = AutoConfig.from_pretrained(\"models/{}\".format(self.data.ckpt_path.replace('/', '_')), revision=REVISION, cache_dir=\"cache\")\n        else:\n            model_config     = AutoConfig.from_pretrained(self.data.ckpt_path, revision=REVISION, cache_dir=\"cache\")\n        return model_config\n\n    def get_hf_checkpoint_metadata(self) -> bool:\n        params = {}\n        model_config = self._get_model_config()\n        params[\"tokenizer_id\"] = self.data.ckpt_path\n        tokenizer = get_tokenizer(self.data.ckpt_path)\n        params[\"newlinemode\"] = params.get(\n            \"newlinemode\", \"s\" if model_config.model_type == \"xglm\" else \"n\"\n        )\n        params[\"max_batch_size\"] = 2048\n        with tokenizer._kai_no_prefix():\n            params[\"eos_token\"] = (\n                [50259, 50259] if model_config.model_type == \"xglm\" and model_config.eos_token_id == 50259 else [model_config.eos_token_id]\n            )\n        params[\"seq\"] = 2048\n        self.data.params = params\n        return True\n\n    def get_tokenizer(self) -> transformers.PreTrainedTokenizerBase:\n        return get_tokenizer(self.data.ckpt_path)\n    \n    def save_data(self):\n        pass\n\n    def export_to_kobold(self, output_file: str, name: str, author: str, supported: str, description: str):\n        try:\n            z = torch.load(self.data.save_file)\n            assert z[\"step\"] > 0\n            assert z[\"tensor\"].ndim == 2 and \"opt_state\" in z\n            assert z[\"tensor\"].shape[0] < self.data.params[\"max_batch_size\"]\n            self.data.soft_in_dim = z[\"tensor\"].shape[0]\n        except AssertionError:\n            self.raise_configuration_error(\"MKUSP file is corrupted.\", code=14)\n\n        tensor = z[\"tensor\"]\n\n        meta = {\n            \"name\": name,\n            \"author\": author,\n            \"supported\": supported,\n            \"description\": description,\n        }\n        if len(meta[\"author\"].strip()) == 0:\n            meta.pop(\"author\")\n        meta[\"supported\"] = list(map(lambda m: m.strip(), supported.split(\",\")))\n\n        with zipfile.ZipFile(output_file, \"w\", compression=zipfile.ZIP_LZMA) as z:\n            with z.open(\"tensor.npy\", \"w\") as f:\n                np.save(f, tensor.detach().cpu().numpy(), allow_pickle=False)\n        with zipfile.ZipFile(output_file, \"a\", compression=zipfile.ZIP_STORED) as z:\n            with z.open(\"meta.json\", \"w\") as f:\n                f.write(json.dumps(meta, indent=2).encode(\"utf-8\"))\n\n    def export_to_mkultra(self, output_file: str, soft_prompt_name: str, soft_prompt_description: str):\n        try:\n            z = torch.load(self.data.save_file)\n            assert z[\"step\"] > 0\n            assert z[\"tensor\"].ndim == 2 and \"opt_state\" in z\n            assert z[\"tensor\"].shape[0] < self.data.params[\"max_batch_size\"]\n            self.data.soft_in_dim = z[\"tensor\"].shape[0]\n            _step = z[\"step\"]\n        except AssertionError:\n            self.raise_configuration_error(\"MKUSP file is corrupted.\", code=14)\n\n        tensor = z[\"tensor\"]\n\n        with open(output_file, \"w\") as f:\n            json.dump(\n                {\n                    \"metadata\": {\n                        \"step\": _step,\n                        \"loss\": float(z[\"loss\"]),\n                        \"uuid\": str(uuid.uuid4()),\n                        \"name\": soft_prompt_name,\n                        \"description\": soft_prompt_description,\n                        \"epoch\": datetime.datetime.now().timestamp(),\n                    },\n                    \"tensor\": base64.b64encode(\n                        pickle.dumps(\n                            tensor.detach().cpu(),\n                            protocol=4,\n                        ),\n                    ).decode(\"ascii\"),\n                },\n                f,\n            )\n\n    def tokenize_dataset(\n        self,\n        dataset_path: Union[str, TextIO],\n        output_file: Union[str, TextIO],\n        batch_size=2048,\n        epochs=1,\n        use_ftfy=True,\n        shuffle_seed: Optional[Union[int, float, str, bytes, bytearray]] = 1729,\n    ):\n        dataset_path = dataset_path.replace(\"\\\\\", \"/\")\n        output_file = output_file.replace(\"\\\\\", \"/\")\n        if not isinstance(batch_size, int) or batch_size < 1:\n            self.raise_configuration_error(\n                \"batch_size must be an integer greater than zero.\", code=9\n            )\n        if (\n            not isinstance(epochs, int) and not isinstance(epochs, float)\n        ) or epochs <= 0:\n            self.raise_configuration_error(\n                \"epochs must be an int or float greater than zero.\", code=10\n            )\n        if isinstance(output_file, str) and output_file.endswith(\"/\"):\n            self.raise_configuration_error(\n                \"output_file should be the path to a file, not a directory.\", code=11\n            )\n        if isinstance(dataset_path, str) and not os.path.exists(dataset_path):\n            self.raise_configuration_error(\n                \"dataset_path is not set to a valid file or directory.\", code=12\n            )\n\n        if use_ftfy:\n            import ftfy\n\n        tokenizer = self.get_tokenizer()\n\n        batch_size = min(\n            batch_size,\n            self.data.params[\"max_batch_size\"] - self.data.soft_in_dim,\n        )\n        assert batch_size >= 0\n        print(\n            termcolor.colored(\n                \"\\nIf you see a warning somewhere below about token indices, ignore it.  That warning is normal.\\n\",\n                \"magenta\",\n            )\n        )\n        print(\"Batch size:\", batch_size)\n        print(termcolor.colored(\"Tokenizing your dataset...\\n\", \"magenta\"))\n\n        if not isinstance(dataset_path, str):\n            files = [dataset_path]\n        elif os.path.isfile(dataset_path):\n            files = [dataset_path]\n        else:\n            files = sorted(\n                os.path.join(dataset_path, filename)\n                for filename in os.listdir(dataset_path)\n            )\n        if shuffle_seed is not None:\n            random.Random(shuffle_seed).shuffle(files)\n        tokens = []\n        eos = tokenizer.decode(self.data.params[\"eos_token\"])\n        for path in files:\n            if isinstance(path, str):\n                f = open(path)\n            else:\n                f = path\n            try:\n                text = f.read()\n                if use_ftfy:\n                    text = ftfy.fix_text(text)\n                text = text.replace(\"<|endoftext|>\", eos)\n                tokens.extend(self.tokenize_dataset_callback(tokenizer, text))\n            finally:\n                if isinstance(path, str):\n                    f.close()\n\n        print(\"Dataset size (in tokens):\", len(tokens))\n        if len(tokens) < batch_size + 1:\n            self.raise_configuration_error(\n                \"Your dataset is too small!  The number of tokens has to be greater than the batch size.  Try increasing the epochs.\",\n                code=13,\n            )\n        tail = len(tokens) % (batch_size + 1)\n        if tail:\n            print(\n                f\"We're removing the last {tail} tokens from your dataset to make the length a multiple of {batch_size+1}.\"\n            )\n            tokens = tokens[:-tail]\n\n        tokens = np.array(tokens, dtype=np.uint16).reshape((-1, batch_size + 1))\n        sequences_per_epoch = tokens.shape[0]\n        _epochs = math.ceil(epochs)\n        if _epochs > 1:\n            rng = np.random.Generator(np.random.PCG64(1729))\n            tokens = np.concatenate(\n                (\n                    tokens,\n                    *(rng.permutation(tokens, axis=0) for i in range(_epochs - 1)),\n                ),\n                axis=0,\n            )\n        tokens = tokens[: math.ceil(epochs * sequences_per_epoch)]\n        print(f\"Total sequences in your dataset: {tokens.shape[0]}\")\n\n        if isinstance(output_file, str):\n            f = open(output_file, \"w\")\n        else:\n            f = output_file\n        try:\n            np.save(output_file, tokens)\n        finally:\n            if isinstance(output_file, str):\n                f.close()\n\n    def train(\n        self,\n        breakmodel_primary_device: Optional[Union[str, int, torch.device]] = None,\n        breakmodel_gpulayers: Optional[List[int]] = None,\n        breakmodel_disklayers = 0,\n    ):\n        if breakmodel_gpulayers is None:\n            breakmodel_gpulayers = []\n        if breakmodel_primary_device is None:\n            breakmodel_primary_device = 0 if sum(x if x >= 0 else 1 for x in breakmodel_gpulayers) else \"cpu\"\n\n        if self.data.params is not None and \"max_batch_size\" not in self.data.params:\n            self.data.params[\"max_batch_size\"] = 2048\n\n        if not os.path.exists(self.data.save_file):\n            print(\"We are starting a brand new soft-tuning session.\\n\")\n            self.startup(step=-1)\n            if self.data.soft_in_dim <= 0:\n                self.raise_configuration_error(\n                    \"You have not set a soft prompt size.\", code=6\n                )\n            step = 0\n        else:\n            # If we're resuming a soft-tuning session, the soft prompt tensor is\n            # already in the save file and we just have to decode it.\n            try:\n                z = torch.load(self.data.save_file)\n                assert z[\"step\"] > 0\n                assert z[\"tensor\"].ndim == 2 and \"opt_state\" in z\n                assert z[\"tensor\"].shape[0] < self.data.params[\"max_batch_size\"]\n                self.data.soft_in_dim = z[\"tensor\"].shape[0]\n                step = z[\"step\"]\n                opt_state = z[\"opt_state\"]\n            except AssertionError:\n                self.raise_configuration_error(\"MKUSP file is corrupted.\", code=14)\n            print(f\"We're resuming a previous soft-tuning session at step {step+1}.\\n\")\n            self.startup(step=step + 1)\n            soft_embeddings = z[\"tensor\"]\n\n        REVISION = None\n\n        patch_transformers()\n\n        model: _PromptTuningPreTrainedModel\n\n        model_config = self._get_model_config()\n        n_layers = utils.num_layers(model_config)\n        breakmodel_gpulayers = [x if x >= 0 else n_layers for x in breakmodel_gpulayers]\n\n        convert_to_float16 = True\n        hascuda = torch.cuda.is_available()\n        usegpu = hascuda and not breakmodel_disklayers and len(breakmodel_gpulayers) == 1 and breakmodel_gpulayers[0] == n_layers\n        gpu_device = breakmodel_primary_device\n        use_breakmodel = bool(hascuda or breakmodel_disklayers or sum(breakmodel_gpulayers))\n\n        assert len(breakmodel_gpulayers) <= torch.cuda.device_count()\n        assert sum(breakmodel_gpulayers) + breakmodel_disklayers <= n_layers\n\n        breakmodel.gpu_blocks = breakmodel_gpulayers\n        breakmodel.disk_blocks = breakmodel_disklayers\n        disk_blocks = breakmodel.disk_blocks\n        gpu_blocks = breakmodel.gpu_blocks\n        ram_blocks = ram_blocks = n_layers - sum(gpu_blocks)\n        cumulative_gpu_blocks = tuple(itertools.accumulate(gpu_blocks))\n\n        device_list(ram_blocks, primary=breakmodel.primary_device)\n\n        def lazy_load_callback(model_dict: Dict[str, Union[torch_lazy_loader.LazyTensor, torch.Tensor]], f, **_):\n            if lazy_load_callback.nested:\n                return\n            lazy_load_callback.nested = True\n\n            device_map: Dict[str, Union[str, int]] = {}\n\n            @functools.lru_cache(maxsize=None)\n            def get_original_key(key):\n                return max((original_key for original_key in utils.module_names if original_key.endswith(key)), key=len)\n\n            for key, value in model_dict.items():\n                original_key = get_original_key(key)\n                if isinstance(value, torch_lazy_loader.LazyTensor) and not any(original_key.startswith(n) for n in utils.layers_module_names):\n                    device_map[key] = gpu_device if hascuda and usegpu else \"cpu\" if not hascuda or not use_breakmodel else breakmodel.primary_device\n                else:\n                    layer = int(max((n for n in utils.layers_module_names if original_key.startswith(n)), key=len).rsplit(\".\", 1)[1])\n                    device = gpu_device if hascuda and usegpu else \"disk\" if layer < disk_blocks and layer < ram_blocks else \"cpu\" if not hascuda or not use_breakmodel else \"shared\" if layer < ram_blocks else bisect.bisect_right(cumulative_gpu_blocks, layer - ram_blocks)\n                    device_map[key] = device\n\n            if utils.num_shards is None or utils.current_shard == 0:\n                utils.offload_index = {}\n                if os.path.isdir(\"accelerate-disk-cache\"):\n                    # Delete all of the files in the disk cache folder without deleting the folder itself to allow people to create symbolic links for this folder\n                    # (the folder doesn't contain any subfolders so os.remove will do just fine)\n                    for filename in os.listdir(\"accelerate-disk-cache\"):\n                        try:\n                            os.remove(os.path.join(\"accelerate-disk-cache\", filename))\n                        except OSError:\n                            pass\n                os.makedirs(\"accelerate-disk-cache\", exist_ok=True)\n                if utils.num_shards is not None:\n                    num_tensors = len(utils.get_sharded_checkpoint_num_tensors(utils.from_pretrained_model_name, utils.from_pretrained_index_filename, **utils.from_pretrained_kwargs))\n                else:\n                    num_tensors = len(device_map)\n                print(flush=True)\n                utils.bar = tqdm(total=num_tensors, desc=\"Loading model tensors\", file=Send_to_socketio())\n\n            with zipfile.ZipFile(f, \"r\") as z:\n                try:\n                    last_storage_key = None\n                    f = None\n                    current_offset = 0\n                    able_to_pin_layers = True\n                    if utils.num_shards is not None:\n                        utils.current_shard += 1\n                    for key in sorted(device_map.keys(), key=lambda k: (model_dict[k].key, model_dict[k].seek_offset)):\n                        storage_key = model_dict[key].key\n                        if storage_key != last_storage_key or model_dict[key].seek_offset < current_offset:\n                            last_storage_key = storage_key\n                            if isinstance(f, zipfile.ZipExtFile):\n                                f.close()\n                            f = z.open(f\"archive/data/{storage_key}\")\n                            current_offset = 0\n                        if current_offset != model_dict[key].seek_offset:\n                            f.read(model_dict[key].seek_offset - current_offset)\n                            current_offset = model_dict[key].seek_offset\n                        device = device_map[key]\n                        size = functools.reduce(lambda x, y: x * y, model_dict[key].shape, 1)\n                        dtype = model_dict[key].dtype\n                        nbytes = size if dtype is torch.bool else size * ((torch.finfo if dtype.is_floating_point else torch.iinfo)(dtype).bits >> 3)\n                        #print(f\"Transferring <{key}>  to  {f'({device.upper()})' if isinstance(device, str) else '[device ' + str(device) + ']'} ... \", end=\"\", flush=True)\n                        model_dict[key] = model_dict[key].materialize(f, map_location=\"cpu\")\n                        # if model_dict[key].dtype is torch.float32:\n                        #     fp32_model = True\n                        if convert_to_float16 and breakmodel.primary_device != \"cpu\" and hascuda and (use_breakmodel or usegpu) and model_dict[key].dtype is torch.float32:\n                            model_dict[key] = model_dict[key].to(torch.float16)\n                        if breakmodel.primary_device == \"cpu\" or (not usegpu and not use_breakmodel and model_dict[key].dtype is torch.float16):\n                            model_dict[key] = model_dict[key].to(torch.float32)\n                        if device == \"shared\":\n                            model_dict[key] = model_dict[key].to(\"cpu\").detach_()\n                            if able_to_pin_layers:\n                                try:\n                                    model_dict[key] = model_dict[key].pin_memory()\n                                except:\n                                    able_to_pin_layers = False\n                        elif device == \"disk\":\n                            accelerate.utils.offload_weight(model_dict[key], get_original_key(key), \"accelerate-disk-cache\", index=utils.offload_index)\n                            model_dict[key] = model_dict[key].to(\"meta\")\n                        else:\n                            model_dict[key] = model_dict[key].to(device)\n                        #print(\"OK\", flush=True)\n                        current_offset += nbytes\n                        utils.bar.update(1)\n                finally:\n                    if utils.num_shards is None or utils.current_shard >= utils.num_shards:\n                        if utils.offload_index:\n                            for name, tensor in utils.named_buffers:\n                                if name not in utils.offload_index:\n                                    accelerate.utils.offload_weight(tensor, name, \"accelerate-disk-cache\", index=utils.offload_index)\n                            accelerate.utils.save_offload_index(utils.offload_index, \"accelerate-disk-cache\")\n                        utils.bar.close()\n                        utils.bar = None\n                    lazy_load_callback.nested = False\n                    if isinstance(f, zipfile.ZipExtFile):\n                        f.close()\n\n        lazy_load_callback.nested = False\n\n        # Since we're using lazy loader, we need to figure out what the model's hidden layers are called\n        with torch_lazy_loader.use_lazy_torch_load(dematerialized_modules=True, use_accelerate_init_empty_weights=True):\n            try:\n                metamodel = AutoModelForCausalLM.from_config(model_config)\n            except Exception as e:\n                metamodel = GPTNeoForCausalLM.from_config(model_config)\n            utils.layers_module_names = utils.get_layers_module_names(metamodel)\n            utils.module_names = list(metamodel.state_dict().keys())\n            utils.named_buffers = list(metamodel.named_buffers(recurse=True))\n\n        with torch_lazy_loader.use_lazy_torch_load(callback=lazy_load_callback, dematerialized_modules=True):\n            if(os.path.isdir(self.data.ckpt_path)):\n                try:\n                    model     = AutoPromptTuningLM.from_pretrained(self.data.ckpt_path, revision=REVISION, cache_dir=\"cache\")\n                except Exception as e:\n                    if(\"out of memory\" in traceback.format_exc().lower()):\n                        raise RuntimeError(\"One of your GPUs ran out of memory when KoboldAI tried to load your model.\")\n                    model     = GPTNeoPromptTuningLM.from_pretrained(self.data.ckpt_path, revision=REVISION, cache_dir=\"cache\")\n            elif(os.path.isdir(\"models/{}\".format(self.data.ckpt_path.replace('/', '_')))):\n                try:\n                    model     = AutoPromptTuningLM.from_pretrained(\"models/{}\".format(self.data.ckpt_path.replace('/', '_')), revision=REVISION, cache_dir=\"cache\")\n                except Exception as e:\n                    if(\"out of memory\" in traceback.format_exc().lower()):\n                        raise RuntimeError(\"One of your GPUs ran out of memory when KoboldAI tried to load your model.\")\n                    model     = GPTNeoPromptTuningLM.from_pretrained(\"models/{}\".format(self.data.ckpt_path.replace('/', '_')), revision=REVISION, cache_dir=\"cache\")\n            else:\n                try:\n                    model     = AutoPromptTuningLM.from_pretrained(self.data.ckpt_path, revision=REVISION, cache_dir=\"cache\")\n                except Exception as e:\n                    if(\"out of memory\" in traceback.format_exc().lower()):\n                        raise RuntimeError(\"One of your GPUs ran out of memory when KoboldAI tried to load your model.\")\n                    model     = GPTNeoPromptTuningLM.from_pretrained(self.data.ckpt_path, revision=REVISION, cache_dir=\"cache\")\n\n        if(hascuda):\n            if(usegpu):\n                model = model.half().to(gpu_device)\n            elif(use_breakmodel):  # Use both RAM and VRAM (breakmodel)\n                move_model_to_devices(model, usegpu, gpu_device)\n            elif(__import__(\"breakmodel\").disk_blocks > 0):\n                move_model_to_devices(model, usegpu, gpu_device)\n            else:\n                model = model.to('cpu').float()\n        elif(__import__(\"breakmodel\").disk_blocks > 0):\n            move_model_to_devices(model, usegpu, gpu_device)\n        else:\n            model.to('cpu').float()\n\n        if step == 0:\n            soft_embeddings = self.get_initial_soft_embeddings(model)\n        else:\n            soft_embeddings = SoftPrompt.from_inputs_embeds(soft_embeddings)\n        model.set_soft_prompt(soft_embeddings)\n\n        steps = self.get_num_sequences() // self.data.gradient_accumulation_steps\n        warmup_steps = max(1, round(steps * self.data.stparams[\"warmup\"]))\n\n        beta1: Optional[float] = self.data.stparams.get(\"beta1\", 0.0)\n        if beta1 == 0.0:\n            beta1 = None\n        optimizer = transformers.Adafactor(\n            params=(model.get_soft_params(),),\n            scale_parameter=False,\n            relative_step=False,\n            warmup_init=False,\n            lr=self.data.stparams[\"lr\"],\n            beta1=beta1,\n            decay_rate=self.data.stparams.get(\"decay_rate\", -0.8),\n            weight_decay=self.data.stparams.get(\"weight_decay\", 0.1),\n        )\n        if step != 0:\n            optimizer.load_state_dict(opt_state)\n        scheduler = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(\n            optimizer=optimizer,\n            num_warmup_steps=warmup_steps,\n            num_training_steps=steps - warmup_steps,\n            num_cycles=(steps - warmup_steps) // self.data.stparams.get(\"training_steps_per_cycle\", 56),\n        )\n\n        torch.cuda.empty_cache()\n        optimizer.state['step'] = step\n        cross_entropy_loss = CrossEntropyLoss()\n\n        def save_mkusp(\n            loss,\n            grad_norm,\n        ):\n            with open(self.data.save_file, \"wb\") as f:\n                torch.save(\n                    {\n                        \"tensor\": soft_embeddings.get_inputs_embeds(),\n                        \"opt_state\": optimizer.state_dict(),\n                        \"step\": step,\n                        \"loss\": loss,\n                        \"grad_norm\": grad_norm,\n                    },\n                    f,\n                )\n            self.save_data()\n        \n        bar1 = tqdm(initial=step + 1, total=steps, desc=\"CURRENT TRAINING STEP\")\n\n        while step < steps:\n            step += 1\n            model.train()\n\n            total_loss = total_grad = total_grad_norm = 0\n\n            # Get the next sequences from the dataset\n            block = torch.tensor(np.int32(self.get_batch(step, self.data.gradient_accumulation_steps))).to(model.transformer.wte.weight.device)\n\n            for sequence in tqdm(block, desc=\"GRADIENT ACCUMULATION\", leave=False):\n                # input_ids is the context to the model (without the soft prompt) and labels is what we expect the model to generate (the -100s represent soft prompt tokens for which loss is not calculated)\n                input_ids = sequence[:-1].unsqueeze(0).detach()\n                labels = torch.cat((torch.full((model.get_soft_params().size(0) - 1,), -100, device=sequence.device), sequence), dim=-1).unsqueeze(0).detach()\n\n                # Give the context to the model and compare the model's output logits with the labels to compute the loss\n                logits = model(input_ids=input_ids, labels=input_ids).logits\n                loss: torch.Tensor = cross_entropy_loss(logits.view(-1, model.transformer.wte.weight.size(0)), labels.view(-1))\n                total_loss += loss.detach()\n\n                # Compute the gradient of the loss function and add it to model.get_soft_params().grad (model.get_soft_params().grad += gradient)\n                loss.backward()\n\n                total_grad_norm += torch.linalg.norm(model.get_soft_params().grad.detach() - total_grad)\n                total_grad = model.get_soft_params().grad.detach()\n\n                del input_ids\n                del labels\n                del logits\n                torch.cuda.empty_cache()\n\n            mean_loss = (total_loss / self.data.gradient_accumulation_steps).item()\n            mean_grad_norm = (total_grad_norm / self.data.gradient_accumulation_steps).item()\n\n            # Apply the optimization algorithm using the accumulated gradients, which changes the contents of the soft prompt matrix very slightly to reduce the loss\n            optimizer.step()\n            lr = optimizer.param_groups[0][\"lr\"]\n            scheduler.step()\n            optimizer.zero_grad()\n\n            # Save checkpoint every few steps\n            if step == 1 or step % self.data.stparams[\"save_every\"] == 0:\n                save_mkusp(mean_loss, mean_grad_norm)\n\n            bar1.set_postfix({\"loss\": mean_loss, \"grad_norm\": mean_grad_norm, \"learning_rate\": lr})\n            bar1.update()\n\n\nclass BasicTrainer(TrainerBase):\n    class TrainerData(TrainerBase.TrainerData):\n        def __init__(self):\n            super().__init__()\n            self.dataset_file: Optional[str] = None\n            self.initial_softprompt: Optional[List[int]] = None\n\n    data: \"BasicTrainer.TrainerData\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dataset: Optional[np.ndarray] = None\n\n    def startup(self, step: int) -> None:\n        if self.get_num_sequences() < self.data.gradient_accumulation_steps:\n            self.raise_configuration_error(\n                \"Your dataset is too small!  gradient_accumulation_steps must be less than or equal to the number of sequences.\",\n                code=101,\n            )\n        if (\n            self.data.prompt_method == \"tokens\"\n            and step < 0\n            and self.data.initial_softprompt is None\n        ):\n            self.raise_configuration_error(\n                \"You have not set an initial soft prompt string.\", code=103\n            )\n        if self.data.prompt_method == \"tokens\" and step < 0:\n            self.data.soft_in_dim = len(self.data.initial_softprompt)\n\n    def get_batch(self, step: int, size: int) -> np.ndarray:\n        return self.dataset[(step - 1) * size : step * size]\n\n    def get_num_sequences(self) -> int:\n        if self.dataset is None:\n            if self.data.dataset_file is None or not os.path.exists(\n                self.data.dataset_file\n            ):\n                self.raise_configuration_error(\n                    f\"Dataset file not found at {repr(self.data.dataset_file)}\",\n                    code=102,\n                )\n            self.dataset = np.load(self.data.dataset_file, mmap_mode=\"r\")\n        assert self.dataset.ndim >= 2\n        assert self.dataset.shape[0] >= 2\n        return self.dataset.shape[0]\n\n    def get_initial_soft_embeddings(self, model: transformers.PreTrainedModel) -> SoftPrompt:\n        if self.data.prompt_method == \"vocab_sample\":\n            rng = np.random.Generator(\n                np.random.PCG64(\n                    [\n                        self.data.prompt_seed,\n                        int.from_bytes(hashlib.sha256(model.config.model_type.encode(\"utf8\")).digest()[:4], \"little\"),\n                    ]\n                )\n            )\n            tokenizer = self.get_tokenizer()\n            with tokenizer._kai_no_prefix():\n                special_tokens = set(\n                    itertools.chain.from_iterable(\n                        tokenizer.encode(str(v))\n                        for v in tokenizer.special_tokens_map_extended.values()\n                    )\n                )\n            sample_space = [\n                k for k in range(model.get_input_embeddings().weight.shape[-2]) if k not in special_tokens\n            ]\n            sample = rng.choice(sample_space, self.data.soft_in_dim, False)\n            return SoftPrompt.from_inputs_embeds(model.get_input_embeddings()(torch.tensor(sample, dtype=torch.int32, device=model.get_input_embeddings().weight.device)))\n        elif self.data.prompt_method == \"tokens\":\n            return SoftPrompt.from_inputs_embeds(model.get_input_embeddings()(torch.tensor(self.data.initial_softprompt, dtype=torch.int32, device=model.get_input_embeddings().weight.device)))\n        self.raise_configuration_error(\n            f\"Unknown prompt method {repr(self.data.prompt_method)}\", code=104\n        )\n\n    def tokenize_dataset_callback(\n        self, tokenizer: transformers.PreTrainedTokenizerBase, text: str\n    ) -> List[int]:\n        if self.data.newlinemode == \"s\":\n            text = text.replace(\"\\n\", \"</s>\")\n        with tokenizer._kai_no_prefix():\n            return tokenizer.encode(text) + self.data.params[\"eos_token\"]\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.1064453125,
          "content": "[pytest]\naddopts = --ignore=miniconda3 --ignore=runtime --html=unit_test_report.html --self-contained-html -v"
        },
        {
          "name": "remote-play.bat",
          "type": "blob",
          "size": 0.015625,
          "content": "play --remote %*"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.4375,
          "content": "transformers==4.24.0\r\nhuggingface_hub==0.12.1\r\nFlask==2.2.3\r\nFlask-SocketIO==5.3.2\r\nWerkzeug==2.3.7\r\npython-socketio==5.7.2\r\nrequests\r\ntorch >= 1.9, < 1.13\r\nflask-cloudflared==0.0.10\r\nflask-ngrok\r\neventlet==0.33.3\r\ndnspython==2.2.1\r\nlupa==1.10\r\nmarkdown\r\nbleach==4.1.0\r\nsentencepiece\r\nprotobuf\r\naccelerate\r\nflask-session==0.4.0\r\nmarshmallow>=3.13\r\napispec-webframeworks\r\nloguru\r\ntermcolor\r\nsafetensors\r\ngit+https://github.com/VE-FORBRYDERNE/mkultra"
        },
        {
          "name": "requirements_mtj.txt",
          "type": "blob",
          "size": 0.45703125,
          "content": "torch >= 1.9, < 1.13\nnumpy\ntqdm\nrequests\ndm-haiku==0.0.9\njax==0.3.25\njaxlib==0.3.25\nchex == 0.1.5\ntransformers == 4.24.0\nhuggingface_hub==0.12.1\nprogressbar2\ngit+https://github.com/VE-FORBRYDERNE/mesh-transformer-jax@ck\nFlask==2.2.3\nFlask-SocketIO==5.3.2\npython-socketio==5.7.2\nflask-cloudflared==0.0.10\nflask-ngrok\nWerkzeug==2.3.7\neventlet==0.33.3\ndnspython==2.2.1\nlupa==1.10\nmarkdown\nbleach==4.1.0\nflask-session==0.4.0\nmarshmallow>=3.13\napispec-webframeworks\nloguru\n"
        },
        {
          "name": "static",
          "type": "tree",
          "content": null
        },
        {
          "name": "stories",
          "type": "tree",
          "content": null
        },
        {
          "name": "structures.py",
          "type": "blob",
          "size": 1.1259765625,
          "content": "import collections\nfrom typing import Iterable, Tuple\n\n\nclass KoboldStoryRegister(collections.OrderedDict):\n    '''\n    Complexity-optimized class for keeping track of story chunks\n    '''\n\n    def __init__(self, sequence: Iterable[Tuple[int, str]] = ()):\n        super().__init__(sequence)\n        self.__next_id: int = len(sequence)\n\n    def append(self, v: str) -> None:\n        self[self.__next_id] = v\n        self.increment_id()\n    \n    def pop(self) -> str:\n        return self.popitem()[1]\n    \n    def get_first_key(self) -> int:\n        if len(self) == 0:\n            return -1\n        else:\n            return next(iter(self))\n\n    def get_last_key(self) -> int:\n        if len(self) == 0:\n            return -1\n        else:\n            return next(reversed(self))\n\n    def __getitem__(self, k: int) -> str:\n        return super().__getitem__(k)\n\n    def __setitem__(self, k: int, v: str) -> None:\n        return super().__setitem__(k, v)\n\n    def increment_id(self) -> None:\n        self.__next_id += 1\n\n    def get_next_id(self) -> int:\n        return self.__next_id\n\n    def set_next_id(self, x: int) -> None:\n        self.__next_id = x\n"
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_aiserver.py",
          "type": "blob",
          "size": 11.392578125,
          "content": "import pytest, time\nimport aiserver\n\n#Test Model List:\ntest_models = [\n                ('EleutherAI/gpt-neo-1.3B', {'key': False, 'gpu': False, 'layer_count': 24, 'breakmodel': True, 'url': False}), \n                ('gpt2', {'key': False, 'gpu': False, 'layer_count': 12, 'breakmodel': True, 'url': False}), \n                ('facebook/opt-350m', {'key': False, 'gpu': False, 'layer_count': 24, 'breakmodel': True, 'url': False})\n              ]\n\n@pytest.fixture\ndef client_data():\n    app = aiserver.app\n    #app.test_client_class = FlaskLoginClient\n    client_conn = app.test_client()\n    socketio_client = aiserver.socketio.test_client(app, flask_test_client=client_conn)\n    #Clear out the connection message\n    response = socketio_client.get_received()\n    return (client_conn, app, socketio_client)\n\n\ndef get_model_menu(model):\n    for menu in aiserver.model_menu:\n        for item in aiserver.model_menu[menu]:\n            if item[1] == model:\n                for main_menu_line in aiserver.model_menu['mainmenu']:\n                    if main_menu_line[1] == menu:\n                        return (menu, main_menu_line, item)\n    return None\n    \ndef generate_story_data(client_data):\n    (client, app, socketio_client) = client_data\n    socketio_client.emit('message',{'cmd': 'submit', 'allowabort': False, 'actionmode': 0, 'chatname': None, 'data': ''})\n    \n    #wait until the game state turns back to start\n    state = 'wait'\n    new_text = None\n    start_time = time.time()\n    timeout = time.time() + 60*1\n    while state == 'wait':\n        if time.time() > timeout:\n            break\n        responses = socketio_client.get_received()\n        for response in responses:\n            response = response['args'][0]\n            print(response)\n            if response['cmd'] == 'setgamestate':\n                state = response['data']\n            elif response['cmd'] == 'updatechunk' or response['cmd'] == 'genseqs':\n                new_text = response['data']\n        time.sleep(0.1)\n    \n    assert new_text is not None\n\ndef test_basic_connection(client_data):\n    (client, app, socketio_client) = client_data\n    response = client.get(\"/\")\n    assert response.status_code == 200\n\ndef test_load_story_from_web_ui(client_data):\n    (client, app, socketio_client) = client_data\n    \n    #List out the stories and make sure we have the sample story\n    socketio_client.emit('message',{'cmd': 'loadlistrequest', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]['data']\n    found_sample_story = False\n    for story in response:\n        if story['name'] == 'sample_story':\n            found_sample_story = True\n    assert found_sample_story\n    \n    #Click on the sample story, then click load\n    socketio_client.emit('message',{'cmd': 'loadselect', 'data': 'sample_story'})\n    socketio_client.emit('message',{'cmd': 'loadrequest', 'data': ''})\n    \n    #Wait until we get the data back from the load\n    loaded_story = False\n    timeout = time.time() + 60*2\n    while not loaded_story:\n        if time.time() > timeout:\n            break\n        responses = socketio_client.get_received()\n        for response in responses:\n            response = response['args'][0]\n            if 'cmd' not in response:\n                print(response)\n                assert False\n            if response['cmd'] == 'updatescreen':\n                loaded_story = True\n                story_text = response['data']\n                break\n    assert loaded_story\n    \n    #Verify that it's the right story data\n    assert story_text == '<chunk n=\"0\" id=\"n0\" tabindex=\"-1\">Niko the kobold stalked carefully down the alley, his small scaly figure obscured by a dusky cloak that fluttered lightly in the cold winter breeze. Holding up his tail to keep it from dragging in the dirty snow that covered the cobblestone, he waited patiently for the butcher to turn his attention from his stall so that he could pilfer his next meal: a tender-looking</chunk><chunk n=\"1\" id=\"n1\" tabindex=\"-1\"> chicken. He crouched just slightly as he neared the stall to ensure that no one was watching, not that anyone would be dumb enough to hassle a small kobold. What else was there for a lowly kobold to</chunk><chunk n=\"2\" id=\"n2\" tabindex=\"-1\"> do in a city? All that Niko needed to know was</chunk><chunk n=\"3\" id=\"n3\" tabindex=\"-1\"> where to find the chicken and then how to make off with it.<br/><br/>A soft thud caused Niko to quickly lift his head. Standing behind the stall where the butcher had been cutting his chicken,</chunk>'  \n\n@pytest.mark.parametrize(\"model, expected_load_options\", test_models)\ndef test_load_model_from_web_ui(client_data, model, expected_load_options):\n    (client, app, socketio_client) = client_data\n    \n    #Clear out any old messages\n    response = socketio_client.get_received()\n    \n    (menu, menu_line, model_line) = get_model_menu(model)\n    \n    #Send the ai load model menu option\n    socketio_client.emit('message',{'cmd': 'list_model', 'data': 'mainmenu'})\n    response = socketio_client.get_received()[0]['args'][0]['data']\n    assert menu_line in response\n    \n    #Send the click model menu option\n    socketio_client.emit('message',{'cmd': 'list_model', 'data': menu, 'pretty_name': \"\"})\n    response = socketio_client.get_received()[0]['args'][0]['data']\n    assert model_line in response\n    \n    #Click the model\n    socketio_client.emit('message',{'cmd': 'selectmodel', 'data': model})\n    response = socketio_client.get_received()[0]['args'][0]\n    #Check that we're getting the right load options\n    print(response)\n    assert response['key'] == expected_load_options['key']\n    assert response['gpu'] == expected_load_options['gpu']\n    assert response['layer_count'] == expected_load_options['layer_count']\n    assert response['breakmodel'] == expected_load_options['breakmodel']\n    assert response['url'] == expected_load_options['url']\n    \n    #Now send the load \n    socketio_client.emit('message',{'cmd': 'load_model', 'use_gpu': True, 'key': '', 'gpu_layers': str(expected_load_options['layer_count']), 'disk_layers': '0', 'url': '', 'online_model': ''})\n    #wait until the game state turns back to start\n    state = 'wait'\n    start_time = time.time()\n    timeout = time.time() + 60*2\n    while state == 'wait':\n        if time.time() > timeout:\n            break\n        responses = socketio_client.get_received()\n        for response in responses:\n            response = response['args'][0]\n            if response['cmd'] == 'setgamestate':\n                state = response['data']\n        time.sleep(0.1)\n    \n    #Give it a second to get all of the settings, etc and clear out the messages\n    responses = socketio_client.get_received()\n    \n    #check the model info to see if it's loaded\n    socketio_client.emit('message',{'cmd': 'show_model', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]\n    assert response == {'cmd': 'show_model_name', 'data': model}\n    \n    generate_story_data(client_data)\n  \ndef test_load_GooseAI_from_web_ui(client_data):\n    \n    pytest.skip(\"unsupported configuration\")\n\n@pytest.mark.parametrize(\"model, expected_load_options\", test_models)\ndef test_load_model_from_command_line(client_data, model, expected_load_options):\n    (client, app, socketio_client) = client_data\n    \n    #Clear out any old messages\n    response = socketio_client.get_received()\n    \n    (menu, menu_line, model_line) = get_model_menu(model)\n    \n    aiserver.general_startup(\"--model {}\".format(model))\n    \n    aiserver.load_model(initial_load=True)\n    \n    #check the model info to see if it's loaded\n    socketio_client.emit('message',{'cmd': 'show_model', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]\n    assert response == {'cmd': 'show_model_name', 'data': model}\n    \n    generate_story_data(client_data)\n\ndef test_back_redo(client_data):\n    (client, app, socketio_client) = client_data\n    \n    \n    #Make sure we have known story in the ui\n    test_load_story_from_web_ui(client_data)\n    \n    #Clear out any old messages\n    response = socketio_client.get_received()\n    \n    #run a back action\n    socketio_client.emit('message',{'cmd': 'back', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]\n    assert response == {'cmd': 'removechunk', 'data': 3}\n    \n    #Run a redo action\n    socketio_client.emit('message',{'cmd': 'redo', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]\n    assert response == {'cmd': 'updatechunk', 'data': {'index': 3, 'html': '<chunk n=\"3\" id=\"n3\" tabindex=\"-1\"> where to find the chicken and then how to make off with it.<br/><br/>A soft thud caused Niko to quickly lift his head. Standing behind the stall where the butcher had been cutting his chicken,</chunk>'}}\n    \n    #Go all the way back, then all the way forward\n    socketio_client.emit('message',{'cmd': 'back', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]\n    assert response == {'cmd': 'removechunk', 'data': 3}\n    socketio_client.emit('message',{'cmd': 'back', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]\n    assert response == {'cmd': 'removechunk', 'data': 2}\n    socketio_client.emit('message',{'cmd': 'back', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]\n    assert response == {'cmd': 'removechunk', 'data': 1}\n    socketio_client.emit('message',{'cmd': 'back', 'data': ''})\n    response = socketio_client.get_received()[0]['args'][0]\n    assert response == {'cmd': 'errmsg', 'data': 'Cannot delete the prompt.'}\n    socketio_client.emit('message',{'cmd': 'redo', 'data': ''})\n    response = socketio_client.get_received()\n    assert response == [{'name': 'from_server', 'args': [{'cmd': 'updatescreen', 'gamestarted': True, 'data': '<chunk n=\"0\" id=\"n0\" tabindex=\"-1\">Niko the kobold stalked carefully down the alley, his small scaly figure obscured by a dusky cloak that fluttered lightly in the cold winter breeze. Holding up his tail to keep it from dragging in the dirty snow that covered the cobblestone, he waited patiently for the butcher to turn his attention from his stall so that he could pilfer his next meal: a tender-looking</chunk><chunk n=\"1\" id=\"n1\" tabindex=\"-1\"> chicken. He crouched just slightly as he neared the stall to ensure that no one was watching, not that anyone would be dumb enough to hassle a small kobold. What else was there for a lowly kobold to</chunk>'}], 'namespace': '/'}, \n                        {'name': 'from_server', 'args': [{'cmd': 'texteffect', 'data': 1}], 'namespace': '/'}]\n    socketio_client.emit('message',{'cmd': 'redo', 'data': ''})\n    response = socketio_client.get_received()\n    assert response == [{'name': 'from_server', 'args': [{'cmd': 'updatechunk', 'data': {'index': 2, 'html': '<chunk n=\"2\" id=\"n2\" tabindex=\"-1\"> do in a city? All that Niko needed to know was</chunk>'}}], 'namespace': '/'}, \n                        {'name': 'from_server', 'args': [{'cmd': 'texteffect', 'data': 2}], 'namespace': '/'}]\n    socketio_client.emit('message',{'cmd': 'redo', 'data': ''})\n    response = socketio_client.get_received()\n    assert response == [{'name': 'from_server', 'args': [{'cmd': 'updatechunk', 'data': {'index': 3, 'html': '<chunk n=\"3\" id=\"n3\" tabindex=\"-1\"> where to find the chicken and then how to make off with it.<br/><br/>A soft thud caused Niko to quickly lift his head. Standing behind the stall where the butcher had been cutting his chicken,</chunk>'}}], 'namespace': '/'}, \n                        {'name': 'from_server', 'args': [{'cmd': 'texteffect', 'data': 3}], 'namespace': '/'}]\n    \n    \n    \n\n    "
        },
        {
          "name": "torch_lazy_loader.py",
          "type": "blob",
          "size": 15.439453125,
          "content": "'''\nThis file is AGPL-licensed.\n\nSome of the code in this file is copied from PyTorch.\n\nThe license for PyTorch is shown below:\n\nCopyright (c) 2016-     Facebook, Inc            (Adam Paszke)\nCopyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\nCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\nCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\nCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\nCopyright (c) 2011-2013 NYU                      (Clement Farabet)\nCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\nCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\nCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America\n   and IDIAP Research Institute nor the names of its contributors may be\n   used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n'''\n\n\nimport contextlib\nfrom functools import reduce\nimport itertools\nimport zipfile\nimport pickle\nimport torch\nimport numpy as np\nimport collections\nimport _codecs\nimport utils\nimport os\nfrom torch.nn import Module\nfrom typing import Any, Callable, Dict, Optional, Tuple, Type, Union\n\n\n_EXTRA_STATE_KEY_SUFFIX = '_extra_state'\n\n\nSTORAGE_TYPE_MAP = {\n    torch.float64: torch.DoubleStorage,\n    torch.float32: torch.FloatStorage,\n    torch.float16: torch.HalfStorage,\n    torch.int64: torch.LongStorage,\n    torch.int32: torch.IntStorage,\n    torch.int16: torch.ShortStorage,\n    torch.int8: torch.CharStorage,\n    torch.uint8: torch.ByteStorage,\n    torch.bool: torch.BoolStorage,\n    torch.bfloat16: torch.BFloat16Storage,\n}\n\n\nclass LazyTensor:\n    def __init__(self, storage_type, key: str, location: str, dtype: Optional[torch.dtype] = None, seek_offset: Optional[int] = None, shape: Optional[Tuple[int, ...]] = None, stride: Optional[Tuple[int, ...]] = None, requires_grad=False, backward_hooks: Any = None):\n        self.storage_type = storage_type\n        self.key = key\n        self.location = location\n        self.dtype = dtype\n        self.seek_offset = seek_offset\n        self.shape = shape\n        self.stride = stride\n        self.requires_grad = requires_grad\n        self.backward_hooks = backward_hooks\n\n    def __view(self, f: Callable):\n        return f\"{type(self).__name__}(storage_type={f(self.storage_type)}, key={f(self.key)}, location={f(self.location)}, dtype={f(self.dtype)}, seek_offset={f(self.seek_offset)}, shape={f(self.shape)}, stride={f(self.stride)}, requires_grad={f(self.requires_grad)}, backward_hooks={f(self.backward_hooks)})\"\n\n    def __repr__(self):\n        return self.__view(repr)\n\n    def materialize(self, checkpoint: Union[zipfile.ZipFile, zipfile.ZipExtFile], map_location=None, no_grad=True, filename=\"pytorch_model.bin\") -> torch.Tensor:\n        filename = os.path.basename(os.path.normpath(filename)).split('.')[0]\n        size = reduce(lambda x, y: x * y, self.shape, 1)\n        dtype = self.dtype\n        nbytes = size if dtype is torch.bool else size * ((torch.finfo if dtype.is_floating_point else torch.iinfo)(dtype).bits >> 3)\n        if isinstance(checkpoint, zipfile.ZipFile):\n            try:\n                f = checkpoint.open(f\"archive/data/{self.key}\", \"r\")\n            except:\n                f = checkpoint.open(f\"{filename}/data/{self.key}\", \"r\")\n            f.read(self.seek_offset)\n        else:\n            f = checkpoint\n        try:\n            storage = STORAGE_TYPE_MAP[dtype].from_buffer(f.read(nbytes), \"little\")\n        finally:\n            if isinstance(checkpoint, zipfile.ZipFile):\n                f.close()\n        storage = torch.serialization._get_restore_location(map_location)(storage, self.location)\n        tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)\n        tensor.set_(storage, 0, self.shape, self.stride)\n        tensor.requires_grad = not no_grad and self.requires_grad\n        tensor._backward_hooks = self.backward_hooks\n        return tensor\n\nclass RestrictedUnpickler(pickle.Unpickler):\n    def original_persistent_load(self, saved_id):\n        return super().persistent_load(saved_id)\n\n    def forced_persistent_load(self, saved_id):\n        if saved_id[0] != \"storage\":\n            raise pickle.UnpicklingError(\"`saved_id[0]` must be 'storage'\")\n        return self.original_persistent_load(saved_id)\n\n    def find_class(self, module, name):\n        if module == \"collections\" and name == \"OrderedDict\":\n            return collections.OrderedDict\n        elif module == \"torch._utils\" and name == \"_rebuild_tensor_v2\":\n            return torch._utils._rebuild_tensor_v2\n        elif module == \"torch\" and name in (\n            \"DoubleStorage\",\n            \"FloatStorage\",\n            \"HalfStorage\",\n            \"LongStorage\",\n            \"IntStorage\",\n            \"ShortStorage\",\n            \"CharStorage\",\n            \"ByteStorage\",\n            \"BoolStorage\",\n            \"BFloat16Storage\",\n        ):\n            return getattr(torch, name)\n        elif module == \"numpy.core.multiarray\" and name == \"scalar\":\n            return np.core.multiarray.scalar\n        elif module == \"numpy\" and name == \"dtype\":\n            return np.dtype\n        elif module == \"_codecs\" and name == \"encode\":\n            return _codecs.encode\n        else:\n            # Forbid everything else.\n            qualified_name = name if module == \"__builtin__\" else f\"{module}.{name}\"\n            raise pickle.UnpicklingError(f\"`{qualified_name}` is forbidden; the model you are loading probably contains malicious code\")\n\n    def load(self, *args, **kwargs):\n        self.original_persistent_load = getattr(self, \"persistent_load\", pickle.Unpickler.persistent_load)\n        self.persistent_load = self.forced_persistent_load\n        return super().load(*args, **kwargs)\n\nclass _LazyUnpickler(RestrictedUnpickler):\n    lazy_loaded_storages: Dict[str, LazyTensor]\n\n    def __init__(self, *args, **kwargs):\n        self.lazy_loaded_storages = {}\n        return super().__init__(*args, **kwargs)\n\n    def forced_persistent_load(self, saved_id):\n        assert isinstance(saved_id, tuple)\n        typename = saved_id[0]\n        assert typename == \"storage\", f\"Unknown typename for persistent_load, expected 'storage' but got '{typename}'\"\n        storage_type, key, location, _ = saved_id[1:]\n        return LazyTensor(storage_type, key, location)\n\n    def load(self, *args, **kwargs):\n        retval = super().load(*args, **kwargs)\n        self.lazy_loaded_storages = {}\n        return retval\n\n\ndef _rebuild_tensor(lazy_storage: LazyTensor, storage_offset, shape, stride):\n    lazy_storage.shape = shape\n    lazy_storage.stride = stride\n    dtype = lazy_storage.storage_type.dtype\n    if not isinstance(dtype, torch.dtype):\n        dtype = lazy_storage.storage_type(0).dtype\n    lazy_storage.dtype = dtype\n    lazy_storage.seek_offset = storage_offset if dtype is torch.bool else storage_offset * ((torch.finfo if dtype.is_floating_point else torch.iinfo)(dtype).bits >> 3)\n    return lazy_storage\n\n\n# Modified version of https://github.com/pytorch/pytorch/blob/v1.11.0-rc4/torch/nn/modules/module.py#L1346-L1438\ndef _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    for hook in self._load_state_dict_pre_hooks.values():\n        hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n\n    persistent_buffers = {k: v for k, v in self._buffers.items() if k not in self._non_persistent_buffers_set}\n    local_name_params = itertools.chain(self._parameters.items(), persistent_buffers.items())\n    local_state = {k: v for k, v in local_name_params if v is not None}\n\n    for name, param in local_state.items():\n        key = prefix + name\n        if key in state_dict:\n            input_param = state_dict[key]\n            if not torch.overrides.is_tensor_like(input_param):\n                error_msgs.append('While copying the parameter named \"{}\", '\n                                    'expected torch.Tensor or Tensor-like object from checkpoint but '\n                                    'received {}'\n                                    .format(key, type(input_param)))\n                continue\n\n            # This is used to avoid copying uninitialized parameters into\n            # non-lazy modules, since they dont have the hook to do the checks\n            # in such case, it will error when accessing the .shape attribute.\n            is_param_lazy = torch.nn.parameter.is_lazy(param)\n            # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n            if not is_param_lazy and len(param.shape) == 0 and len(input_param.shape) == 1:\n                input_param = input_param[0]\n\n            if not is_param_lazy and input_param.shape != param.shape:\n                # local shape should match the one in checkpoint\n                error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n                                    'the shape in current model is {}.'\n                                    .format(key, input_param.shape, param.shape))\n                continue\n            try:\n                with torch.no_grad():\n                    #param.copy_(input_param)\n                    new_param = torch.nn.Parameter(input_param, requires_grad=param.requires_grad)  # This line is new\n                    if name in self._parameters:  # This line is new\n                        self._parameters[name] = new_param  # This line is new\n                    if name in persistent_buffers:  # This line is new\n                        self._buffers[name] = new_param  # This line is new\n            except Exception as ex:\n                error_msgs.append('While copying the parameter named \"{}\", '\n                                    'whose dimensions in the model are {} and '\n                                    'whose dimensions in the checkpoint are {}, '\n                                    'an exception occurred : {}.'\n                                    .format(key, param.size(), input_param.size(), ex.args))\n        elif strict:\n            missing_keys.append(key)\n\n    extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n    if hasattr(Module, \"set_extra_state\") and getattr(self.__class__, \"set_extra_state\", Module.set_extra_state) is not Module.set_extra_state:  # if getattr(self.__class__, \"set_extra_state\", Module.set_extra_state) is not Module.set_extra_state:\n        if extra_state_key in state_dict:\n            self.set_extra_state(state_dict[extra_state_key])\n        elif strict:\n            missing_keys.append(extra_state_key)\n    elif strict and (extra_state_key in state_dict):\n        unexpected_keys.append(extra_state_key)\n\n    if strict:\n        for key in state_dict.keys():\n            if key.startswith(prefix) and key != extra_state_key:\n                input_name = key[len(prefix):]\n                input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n                if input_name not in self._modules and input_name not in local_state:\n                    unexpected_keys.append(key)\n\n\n@contextlib.contextmanager\ndef use_custom_unpickler(unpickler: Type[pickle.Unpickler] = RestrictedUnpickler):\n    try:\n        old_unpickler = pickle.Unpickler\n        pickle.Unpickler = unpickler\n\n        old_pickle_load = pickle.load\n\n        def new_pickle_load(*args, **kwargs):\n            return pickle.Unpickler(*args, **kwargs).load()\n\n        pickle.load = new_pickle_load\n\n        yield\n\n    finally:\n        pickle.Unpickler = old_unpickler\n        pickle.load = old_pickle_load\n\n@contextlib.contextmanager\ndef use_lazy_torch_load(enable=True, callback: Optional[Callable] = None, dematerialized_modules=False, use_accelerate_init_empty_weights=False):\n    if not enable:\n        with use_custom_unpickler(RestrictedUnpickler):\n            yield False\n        return\n\n    try:\n        old_rebuild_tensor = torch._utils._rebuild_tensor\n        torch._utils._rebuild_tensor = _rebuild_tensor\n\n        old_torch_load = torch.load\n\n        def torch_load(f, map_location=None, pickle_module=pickle, **pickle_load_args):\n            retval = old_torch_load(f=f, map_location=map_location, pickle_module=pickle_module, **pickle_load_args)\n            if callback is not None:\n                callback(retval, f=f, map_location=map_location, pickle_module=pickle_module, **pickle_load_args)\n            return retval\n\n        torch.load = torch_load\n\n        if dematerialized_modules:\n            if use_accelerate_init_empty_weights and utils.HAS_ACCELERATE:\n                import accelerate\n                init_empty_weights = accelerate.init_empty_weights()\n                init_empty_weights.__enter__()\n            else:\n                old_linear_init = torch.nn.Linear.__init__\n                old_embedding_init = torch.nn.Embedding.__init__\n                old_layernorm_init = torch.nn.LayerNorm.__init__\n\n                def linear_init(self, *args, device=None, **kwargs):\n                    return old_linear_init(self, *args, device=\"meta\", **kwargs)\n\n                def embedding_init(self, *args, device=None, **kwargs):\n                    return old_embedding_init(self, *args, device=\"meta\", **kwargs)\n\n                def layernorm_init(self, *args, device=None, **kwargs):\n                    return old_layernorm_init(self, *args, device=\"meta\", **kwargs)\n\n                torch.nn.Linear.__init__ = linear_init\n                torch.nn.Embedding.__init__ = embedding_init\n                torch.nn.LayerNorm.__init__ = layernorm_init\n                old_load_from_state_dict = torch.nn.Module._load_from_state_dict\n                torch.nn.Module._load_from_state_dict = _load_from_state_dict\n\n        with use_custom_unpickler(_LazyUnpickler):\n            yield True\n\n    finally:\n        torch._utils._rebuild_tensor = old_rebuild_tensor\n        torch.load = old_torch_load\n        if dematerialized_modules:\n            if use_accelerate_init_empty_weights and utils.HAS_ACCELERATE:\n                init_empty_weights.__exit__(None, None, None)\n            else:\n                torch.nn.Linear.__init__ = old_linear_init\n                torch.nn.Embedding.__init__ = old_embedding_init\n                torch.nn.LayerNorm.__init__ = old_layernorm_init\n                torch.nn.Module._load_from_state_dict = old_load_from_state_dict\n"
        },
        {
          "name": "tpu_mtj_backend.py",
          "type": "blob",
          "size": 66.9521484375,
          "content": "'''\nThis file is AGPL-licensed.\n\nSome of the code in this file is from Clover Edition:\nhttps://github.com/cloveranon/Clover-Edition/blob/master/aidungeon/gpt2generator.py\n\nThe license for Clover Edition is shown below:\n\nCopyright (c) 2019 Nick Walton\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n'''\n\nimport utils\n\nimport multiprocessing\nfrom typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple, TypeVar\nimport progressbar\nimport time\nimport os\nimport sys\nimport json\nimport zipfile\nimport requests\nimport random\nimport jax\nimport jax.dlpack\nfrom jax.config import config\nfrom jax.experimental import maps\nimport jax.numpy as jnp\nimport numpy as np\nimport haiku as hk\nfrom transformers import AutoTokenizer, GPT2Tokenizer, AutoModelForCausalLM, GPTNeoForCausalLM\nfrom tokenizers import Tokenizer\nfrom mesh_transformer.checkpoint import read_ckpt_lowmem\nfrom mesh_transformer.transformer_shard import CausalTransformer, CausalTransformerShard, PlaceholderTensor\nfrom mesh_transformer.util import to_bf16\n\n\nparams: Dict[str, Any] = {}\n\n__seed = random.randrange(2**64)\nrng = random.Random(__seed)\n\n\ndef get_rng_seed():\n    return __seed\n\ndef set_rng_seed(seed: int):\n    global __seed, rng\n    rng = random.Random(seed)\n    __seed = seed\n    return seed\n\ndef randomize_rng_seed():\n    return set_rng_seed(random.randrange(2**64))\n\ndef get_rng_state():\n    return rng\n\ndef set_rng_state(state):\n    global rng\n    rng = state\n\ndef new_rng_state(seed: int):\n    return random.Random(seed)\n\ndef warper_callback(logits) -> np.array:\n    raise NotImplementedError(\"`tpu_mtj_backend.warper_callback()` needs to be defined\")\n\ndef stopping_callback(generated, n_generated, excluded_world_info) -> Tuple[List[set], bool, bool]:\n    raise NotImplementedError(\"`tpu_mtj_backend.stopping_callback()` needs to be defined\")\n\ndef settings_callback() -> dict:\n    return {\n        \"sampler_order\": utils.default_sampler_order.copy(),\n        \"top_p\": 0.9,\n        \"temp\": 0.5,\n        \"top_k\": 0,\n        \"tfs\": 1.0,\n        \"typical\": 1.0,\n        \"top_a\": 0.0,\n        \"repetition_penalty\": 1.0,\n        \"rpslope\": 0.0,\n        \"rprange\": 0,\n    }\n\ndef started_compiling_callback() -> None:\n    pass\n\ndef stopped_compiling_callback() -> None:\n    pass\n\ndef compiling_callback() -> None:\n    pass\n\n\ndef show_spinner():\n    bar = progressbar.ProgressBar(max_value=progressbar.UnknownLength, widgets=[progressbar.Timer(), '  ', progressbar.BouncingBar(left='[', right=']', marker='█')])\n    i = 0\n    while True:\n        bar.update(i)\n        time.sleep(0.1)\n        i += 1\n\n\n__F = TypeVar(\"__F\", bound=Callable)\n__T = TypeVar(\"__T\")\n\ndef __move_xmap(f: __F, out_axis: str) -> __F:\n    return maps.xmap(\n        f,\n        in_axes=([\"shard\", ...], [\"batch\", ...]),\n        out_axes=[out_axis, ...],\n        axis_resources={'shard': 'mp', 'batch': 'dp'},\n    )\n\ndef __shard_xmap(batch_dim=1):\n    xmap = __move_xmap(lambda s, b: s, \"shard\")\n    def inner(x: __T) -> __T:\n        return xmap(x, np.empty(batch_dim))\n    return inner\n\ndef __batch_xmap(shard_dim=1):\n    xmap = __move_xmap(lambda s, b: b, \"batch\")\n    def inner(x: __T) -> __T:\n        return xmap(np.empty(shard_dim), x)\n    return inner\n\n\nclass _EmptyState(NamedTuple):\n    pass\n\nclass _DummyOptimizer:\n    def init(*args, **kwargs):\n        return _EmptyState()\n\n\ndef apply_repetition_penalty_dynamic(logits, tokens, repetition_penalty, generated_index, gen_length, rpslope, rprange):\n    '''\n    This gets called by generate_loop_fn to apply repetition penalty\n    to the 1D array logits using the provided 1D array of tokens to penalize\n    '''\n    tokens = np.minimum(tokens, params[\"n_vocab\"]-1)  # https://github.com/google/jax/issues/3774\n    rpslope = np.int32(rpslope)\n    rprange = np.int32(rprange)\n    clipped_rprange = rprange if rprange > 0 else tokens.shape[-1]\n    penalty_arange = np.roll(np.arange(tokens.shape[-1]) + (clipped_rprange - tokens.shape[-1]), generated_index, axis=-1)\n    # Make a new array with the same length as the tokens array but with\n    # each element replaced by the value at the corresponding index in the\n    # logits array; e.g.\n    # if logits is [77, 5, 3, 98] and tokens is [0, 1, 2, 3, 2, 3, 1],\n    # then penalty_logits will be [77, 5, 3, 98, 3, 98, 5]\n    penalty_logits = np.take(logits, tokens)\n    # Repetition penalty slope\n    if rpslope != 0.0 and rprange > 0:\n        _penalty = (penalty_arange/(rprange - 1)) * 2 - 1\n        _penalty = (rpslope * _penalty) / (1 + np.abs(_penalty) * (rpslope - 1))\n        _penalty = 1 + ((_penalty + 1) / 2) * (repetition_penalty - 1)\n        repetition_penalty = _penalty\n    # Divide positive values by repetition_penalty and multiply negative\n    # values by repetition_penalty (the academic publication that described\n    # this technique actually just only divided, but that would cause tokens\n    # with negative logits to become more likely, which is obviously wrong)\n    penalty_logits = np.where(\n        penalty_arange >= 0,\n        np.where(\n            penalty_logits > 0,\n            penalty_logits/repetition_penalty,\n            penalty_logits*repetition_penalty,\n        ),\n        penalty_logits,\n    )\n    # Finally, put those penalized logit values back into their original\n    # positions in the logits array\n    logits[tokens] = penalty_logits\n    return logits\n\ndef kobold_sample_dynamic(key, logits, rpargs, sampler_order: Optional[np.ndarray] = None, top_p=0.9, temp=0.5, top_k=0, tfs=1.0, typical=1.0, top_a=0.0):\n    '''\n    This gets called by generate_loop_fn to apply a series of 6 filters\n    to the logits (top-k, then top-a, then top-p, then TFS, then typical, then temperature)\n    before picking one token using the modified logits\n    '''\n    # Top-k (keep only the k tokens with the highest logits and remove\n    # the rest, by setting their logits to negative infinity)\n    def top_k_filter(logits):\n        # After sorting the logits array in descending order,\n        # sorted_indices_to_remove is a 1D array that is True for tokens\n        # in the sorted logits array we want to remove and False for ones\n        # we want to keep, in this case the first top_k elements will be\n        # False and the rest will be True\n        sorted_indices_to_remove = np.arange(len(logits)) >= top_k\n        # Unsort the logits array back to its original configuration and\n        # remove tokens we need to remove\n        _, indices_to_remove = jax.lax.sort_key_val(\n            np.argsort(-logits),\n            sorted_indices_to_remove,\n        )\n        return np.where(indices_to_remove, -np.inf, logits)\n    # Top-a (remove all tokens that have softmax probability less than\n    # a*m^2 where m is the maximum softmax probability)\n    def top_a_filter(logits):\n        # Replace every element in the logits array\n        # with e (Euler's number) to the power of that element, and divide\n        # each element of the new array by the sum of the elements in the\n        # new array\n        probabilities = np.array(jax.nn.softmax(logits), copy=True)\n        # Find the largest probability\n        probs_max = probabilities.max()\n        # Remove tokens\n        return np.where(probabilities < probs_max * probs_max * top_a, -np.inf, logits)\n    # Top-p (after sorting the remaining tokens again in descending order of\n    # logit, remove the ones that have cumulative softmax probability\n    # greater than p)\n    def top_p_filter(logits):\n        # Sort the logits array in descending order, replace every element\n        # with e (Euler's number) to the power of that element, and divide\n        # each element of the new array by the sum of the elements in the\n        # new array\n        sorted_logits = -np.sort(-logits)\n        probabilities = np.array(jax.nn.softmax(sorted_logits), copy=True)\n        # Calculate cumulative_probabilities as the prefix-sum array of\n        # probabilities\n        cumulative_probabilities = np.cumsum(probabilities, axis=-1)\n        # We want to remove tokens with cumulative probability higher\n        # than top_p\n        sorted_indices_to_remove = cumulative_probabilities > top_p\n        # Don't ever remove the token with the highest logit, even if\n        # the probability is higher than top_p\n        sorted_indices_to_remove[0] = False\n        # Unsort and remove\n        _, indices_to_remove = jax.lax.sort_key_val(\n            np.argsort(-logits),\n            sorted_indices_to_remove,\n        )\n        return np.where(indices_to_remove, -np.inf, logits)\n    # Tail free sampling (basically top-p a second time on remaining tokens\n    # except it's the \"cumulative normalized absolute second finite\n    # differences of the softmax probabilities\" instead of just the\n    # cumulative softmax probabilities)\n    def tail_free_filter(logits):\n        # Sort in descending order\n        sorted_logits = -np.sort(-logits)\n        # Softmax again\n        probabilities = np.array(jax.nn.softmax(sorted_logits), copy=True)\n        # Calculate the second finite differences of that array (i.e.\n        # calculate the difference array and then calculate the difference\n        # array of the difference array)\n        d2 = np.diff(np.diff(probabilities))\n        # Get the absolute values of all those second finite differences\n        d2 = np.abs(d2)\n        # Normalize (all elements in the array are divided by the sum of the\n        # array's elements)\n        d2 = d2 / d2.sum(axis=-1, keepdims=True)\n        # Get the prefix-sum array\n        cumulative_d2 = np.cumsum(d2, axis=-1)\n        # We will remove the tokens with a cumulative normalized absolute\n        # second finite difference larger than the TFS value\n        sorted_indices_to_remove = cumulative_d2 > tfs\n        # Don't remove the token with the highest logit\n        sorted_indices_to_remove[0] = False\n        # Since the d2 array has two fewer elements than the logits array,\n        # we'll add two extra Trues to the end\n        sorted_indices_to_remove = np.pad(\n            sorted_indices_to_remove,\n            (0, 2),\n            constant_values=True,\n        )\n        # Unsort and remove\n        _, indices_to_remove = jax.lax.sort_key_val(\n            np.argsort(-logits),\n            sorted_indices_to_remove,\n        )\n        return np.where(indices_to_remove, -np.inf, logits)\n    # Typical sampling (https://arxiv.org/pdf/2202.00666.pdf)\n    def typical_filter(logits):\n        # Compute softmax probabilities and the natural logarithms of them\n        probs = jax.nn.softmax(logits)\n        with np.errstate(divide=\"ignore\"):\n            log_probs = np.log(probs)\n        # Compute the negative of entropy, which is the sum of p*ln(p) for all p\n        # in the set of softmax probabilities of the logits\n        neg_entropy = np.nansum(probs * log_probs, axis=-1, keepdims=True)\n        # Determine absolute difference between the negative entropy and the\n        # log probabilities\n        entropy_deviation = np.abs(neg_entropy - log_probs)\n        # Keep certain tokens such that the sum of the entropy_deviation of the\n        # kept tokens is the smallest possible value such that the sum of the\n        # softmax probabilities of the kept tokens is at least the threshold\n        # value (by sorting the tokens in ascending order of entropy_deviation\n        # and then keeping the smallest possible number of tokens from the\n        # beginning such that sum of softmax probabilities is at or above the\n        # threshold)\n        _, sorted_logits = jax.lax.sort_key_val(entropy_deviation, probs)\n        sorted_indices_to_remove = np.cumsum(sorted_logits, axis=-1) >= typical\n        sorted_indices_to_remove = np.roll(sorted_indices_to_remove, 1, axis=-1)\n        sorted_indices_to_remove[0] = False\n        # Unsort and remove\n        _, indices_to_remove = jax.lax.sort_key_val(\n            jnp.argsort(entropy_deviation),\n            sorted_indices_to_remove,\n        )\n        return np.where(indices_to_remove, -jnp.inf, logits)\n    # Temperature (just divide the logits by the temperature)\n    def temp_filter(logits):\n        return logits / temp\n    for k in sampler_order:\n        if k == 0 and top_k > 0: logits = top_k_filter(logits)\n        if k == 1 and top_a > 0.0: logits = top_a_filter(logits)\n        if k == 2 and top_p < 1.0: logits = top_p_filter(logits)\n        if k == 3 and tfs < 1.0: logits = tail_free_filter(logits)\n        if k == 4 and typical < 1.0: logits = typical_filter(logits)\n        if k == 5 and temp != 1.0: logits = temp_filter(logits)\n        if k == 6 and rpargs[1] != 1.0: logits = apply_repetition_penalty_dynamic(logits, *rpargs)\n    # Finally, pick one token using the softmax thingy again (it gives\n    # an array whose elements sum to 1 so it can be used nicely as a\n    # probability distribution)\n    return jax.random.categorical(key, logits, -1).astype(np.uint32)\n\ndef apply_repetition_penalty_static(logits, tokens, repetition_penalty, generated_index, gen_length, rpslope, rprange):\n    '''\n    This gets called by generate_loop_fn to apply repetition penalty\n    to the 1D array logits using the provided 1D array of tokens to penalize\n    '''\n    rpslope = jnp.int32(rpslope)\n    rprange = jnp.int32(rprange)\n    clipped_rprange = jax.lax.cond(rprange > 0, lambda x: x, lambda x: tokens.shape[-1], rprange)\n    penalty_arange = jnp.roll(jnp.arange(tokens.shape[-1]) + (clipped_rprange - tokens.shape[-1]), generated_index, axis=-1)\n    # Make a new array with the same length as the tokens array but with\n    # each element replaced by the value at the corresponding index in the\n    # logits array; e.g.\n    # if logits is [77, 5, 3, 98] and tokens is [0, 1, 2, 3, 2, 3, 1],\n    # then penalty_logits will be [77, 5, 3, 98, 3, 98, 5]\n    penalty_logits = jnp.take(logits, tokens)\n    # Repetition penalty slope\n    def apply_slope(carry):\n        repetition_penalty, rprange = carry\n        _penalty = (penalty_arange/(rprange - 1)) * 2 - 1\n        _penalty = (rpslope * _penalty) / (1 + jnp.abs(_penalty) * (rpslope - 1))\n        _penalty = 1 + ((_penalty + 1) / 2) * (repetition_penalty - 1)\n        return _penalty\n    repetition_penalty = jax.lax.cond(\n        (rpslope != 0.0) & (rprange > 0),  # Not a typo; do not use `and` here, it makes JAX crash\n        apply_slope,\n        lambda carry: jnp.full(tokens.shape, carry[0]),\n        (repetition_penalty, rprange),\n    )\n    # Divide positive values by repetition_penalty and multiply negative\n    # values by repetition_penalty (the academic publication that described\n    # this technique actually just only divided, but that would cause tokens\n    # with negative logits to become more likely, which is obviously wrong)\n    penalty_logits = jnp.where(\n        penalty_arange >= 0,\n        jnp.where(\n            penalty_logits > 0,\n            penalty_logits/repetition_penalty,\n            penalty_logits*repetition_penalty,\n        ),\n        penalty_logits,\n    )\n    # Finally, put those penalized logit values back into their original\n    # positions in the logits array\n    return logits.at[tokens].set(penalty_logits)\n\ndef kobold_sample_static(key, logits, rpargs, sampler_order: Optional[np.ndarray] = None, top_p=0.9, temp=0.5, top_k=0, tfs=1.0, typical=1.0, top_a=0.0):\n    '''\n    This gets called by generate_loop_fn to apply a series of 6 filters\n    to the logits (top-k, then top-a, then top-p, then TFS, then typical, then temperature)\n    before picking one token using the modified logits\n    '''\n    # Top-k (keep only the k tokens with the highest logits and remove\n    # the rest, by setting their logits to negative infinity)\n    def top_k_filter(logits):\n        # After sorting the logits array in descending order,\n        # sorted_indices_to_remove is a 1D array that is True for tokens\n        # in the sorted logits array we want to remove and False for ones\n        # we want to keep, in this case the first top_k elements will be\n        # False and the rest will be True\n        sorted_indices_to_remove = jnp.arange(len(logits)) >= top_k\n        # Unsort the logits array back to its original configuration and\n        # remove tokens we need to remove\n        _, indices_to_remove = jax.lax.sort_key_val(\n            jnp.argsort(-logits),\n            sorted_indices_to_remove,\n        )\n        return jnp.where(indices_to_remove, -jnp.inf, logits)\n    # Top-a (remove all tokens that have softmax probability less than\n    # a*m^2 where m is the maximum softmax probability)\n    def top_a_filter(logits):\n        # Replace every element in the logits array\n        # with e (Euler's number) to the power of that element, and divide\n        # each element of the new array by the sum of the elements in the\n        # new array\n        probabilities = jax.nn.softmax(logits)\n        # Find the largest probability\n        probs_max = probabilities.max()\n        # Remove tokens\n        return jnp.where(probabilities < probs_max * probs_max * top_a, -jnp.inf, logits)\n    # Top-p (after sorting the remaining tokens again in descending order of\n    # logit, remove the ones that have cumulative softmax probability\n    # greater than p)\n    def top_p_filter(logits):\n        # Sort the logits array in descending order, replace every element\n        # with e (Euler's number) to the power of that element, and divide\n        # each element of the new array by the sum of the elements in the\n        # new array\n        sorted_logits = -jnp.sort(-logits)\n        probabilities = jax.nn.softmax(sorted_logits)\n        # Calculate cumulative_probabilities as the prefix-sum array of\n        # probabilities\n        cumulative_probabilities = jnp.cumsum(probabilities, axis=-1)\n        # We want to remove tokens with cumulative probability higher\n        # than top_p\n        sorted_indices_to_remove = cumulative_probabilities > top_p\n        # Don't ever remove the token with the highest logit, even if\n        # the probability is higher than top_p\n        sorted_indices_to_remove = sorted_indices_to_remove.at[0].set(False)\n        # Unsort and remove\n        _, indices_to_remove = jax.lax.sort_key_val(\n            jnp.argsort(-logits),\n            sorted_indices_to_remove,\n        )\n        return jnp.where(indices_to_remove, -jnp.inf, logits)\n    # Tail free sampling (basically top-p a second time on remaining tokens\n    # except it's the \"cumulative normalized absolute second finite\n    # differences of the softmax probabilities\" instead of just the\n    # cumulative softmax probabilities)\n    def tail_free_filter(logits):\n        # Sort in descending order\n        sorted_logits = -jnp.sort(-logits)\n        # Softmax again\n        probabilities = jax.nn.softmax(sorted_logits)\n        # Calculate the second finite differences of that array (i.e.\n        # calculate the difference array and then calculate the difference\n        # array of the difference array)\n        d2 = jnp.diff(jnp.diff(probabilities))\n        # Get the absolute values of all those second finite differences\n        d2 = jnp.abs(d2)\n        # Normalize (all elements in the array are divided by the sum of the\n        # array's elements)\n        d2 = d2 / d2.sum(axis=-1, keepdims=True)\n        # Get the prefix-sum array\n        cumulative_d2 = jnp.cumsum(d2, axis=-1)\n        # We will remove the tokens with a cumulative normalized absolute\n        # second finite difference larger than the TFS value\n        sorted_indices_to_remove = cumulative_d2 > tfs\n        # Don't remove the token with the highest logit\n        sorted_indices_to_remove = sorted_indices_to_remove.at[0].set(False)\n        # Since the d2 array has two fewer elements than the logits array,\n        # we'll add two extra Trues to the end\n        sorted_indices_to_remove = jnp.pad(\n            sorted_indices_to_remove,\n            (0, 2),\n            constant_values=True,\n        )\n        # Unsort and remove\n        _, indices_to_remove = jax.lax.sort_key_val(\n            jnp.argsort(-logits),\n            sorted_indices_to_remove,\n        )\n        return jnp.where(indices_to_remove, -jnp.inf, logits)\n    # Typical sampling (https://arxiv.org/pdf/2202.00666.pdf)\n    def typical_filter(logits):\n        # Compute softmax probabilities and the natural logarithms of them\n        probs = jax.nn.softmax(logits)\n        log_probs = jnp.log(probs)\n        # Compute the negative of entropy, which is the sum of p*ln(p) for all p\n        # in the set of softmax probabilities of the logits\n        neg_entropy = jnp.nansum(probs * log_probs, axis=-1, keepdims=True)\n        # Determine absolute difference between the negative entropy and the\n        # log probabilities\n        entropy_deviation = jnp.abs(neg_entropy - log_probs)\n        # Keep certain tokens such that the sum of the entropy_deviation of the\n        # kept tokens is the smallest possible value such that the sum of the\n        # softmax probabilities of the kept tokens is at least the threshold\n        # value (by sorting the tokens in ascending order of entropy_deviation\n        # and then keeping the smallest possible number of tokens from the\n        # beginning such that sum of softmax probabilities is at or above the\n        # threshold)\n        _, sorted_logits = jax.lax.sort_key_val(entropy_deviation, probs)\n        sorted_indices_to_remove = jnp.cumsum(sorted_logits, axis=-1) >= typical\n        sorted_indices_to_remove = jnp.roll(sorted_indices_to_remove, 1, axis=-1)\n        sorted_indices_to_remove = sorted_indices_to_remove.at[0].set(False)\n        # Unsort and remove\n        _, indices_to_remove = jax.lax.sort_key_val(\n            jnp.argsort(entropy_deviation),\n            sorted_indices_to_remove,\n        )\n        return jnp.where(indices_to_remove, -jnp.inf, logits)\n    # Temperature (just divide the logits by the temperature)\n    def temp_filter(logits):\n        return logits / temp\n    for k in sampler_order:\n        logits = jax.lax.cond(jnp.logical_and(k == 0, top_k > 0), top_k_filter, lambda x: x, logits)\n        logits = jax.lax.cond(jnp.logical_and(k == 1, top_a > 0.0), top_a_filter, lambda x: x, logits)\n        logits = jax.lax.cond(jnp.logical_and(k == 2, top_p < 1.0), top_p_filter, lambda x: x, logits)\n        logits = jax.lax.cond(jnp.logical_and(k == 3, tfs < 1.0), tail_free_filter, lambda x: x, logits)\n        logits = jax.lax.cond(jnp.logical_and(k == 4, typical < 1.0), typical_filter, lambda x: x, logits)\n        logits = jax.lax.cond(jnp.logical_and(k == 5, temp != 1.0), temp_filter, lambda x: x, logits)\n        logits = jax.lax.cond(jnp.logical_and(k == 6, rpargs[1] != 1.0), lambda x: apply_repetition_penalty_static(*x), lambda x: x[0], (logits, *rpargs))\n    # Finally, pick one token using the softmax thingy again (it gives\n    # an array whose elements sum to 1 so it can be used nicely as a\n    # probability distribution)\n    return jax.random.categorical(key, logits, -1).astype(jnp.uint32)\n\npad_token_id = 50256\n\ndef sample_func(data, key, numseqs_aux, badwords, repetition_penalty, generated_index, gen_length, rpslope, rprange, sampler_options):\n    numseqs = numseqs_aux.shape[0]\n    gi = data[0][1]\n    def sample_loop_fn(carry):\n        generated, generated_index, logits, _ = carry[0][0]\n        sample_key = carry[1]\n        # Get the pseudo-random number generator key that will\n        # be used by kobold_sample_dynamic to randomly pick a token\n        sample_key, new_key = jax.random.split(sample_key, num=2)\n        # Remove any tokens in the badwords list by setting\n        # their logits to negative infinity which effectively\n        # makes their probabilities of being chosen zero\n        logits[badwords] = -np.inf\n        # Use the sampler (kobold_sample_dynamic) to pick one token\n        # based on the logits array as a 0D uint32 array\n        # (higher logit means higher probability of being\n        # picked, non-linearly)\n        next_token = kobold_sample_dynamic(\n            sample_key,\n            logits,\n            (\n                generated,\n                repetition_penalty,\n                generated_index, \n                gen_length,\n                rpslope,\n                rprange,\n            ),\n            **sampler_options,\n        )\n        # Remember what token was picked\n        generated[generated_index] = next_token\n        generated_index += 1\n        # Re-pack the current sample_loop_fn's state so we can\n        # get back the same variables the next time\n        carry[0][0] = [generated, generated_index, logits, next_token]\n        carry[0].append(carry[0].pop(0))\n        return carry[0], new_key\n    # return jax.lax.while_loop(\n    #     lambda carry: carry[0][0][1] == gi,\n    #     sample_loop_fn,\n    #     (data, key),\n    # )\n    carry = (data, key)\n    while carry[0][0][1] == gi:\n        carry = sample_loop_fn(carry)\n    return carry\n\nclass PenalizingCausalTransformer(CausalTransformer):\n    def __init__(self, config, **kwargs):\n        # Initialize\n        super().__init__(config, **kwargs)\n        def generate_static(state, key, ctx, ctx_length, gen_length, numseqs_aux, sampler_options, soft_embeddings=None):\n            compiling_callback()\n            numseqs = numseqs_aux.shape[0]\n            # These are the tokens that we don't want the AI to ever write\n            badwords = jnp.array(vars.badwordsids).squeeze()\n            @hk.transform\n            def generate_sample(context, ctx_length):\n                # Give the initial context to the transformer\n                transformer = CausalTransformerShard(config)\n                def generate_initial_scan_fn(sequence_index, _):\n                    _, initial_state = transformer.generate_initial(context, ctx_length, soft_embeddings=soft_embeddings)\n                    # The \"generated\" array will contain the tokens from the\n                    # context as well as the tokens picked by the sampler at\n                    # each stage, padded with a bunch of 50256s, so we know\n                    # which tokens have to be repetition penalized\n                    generated = jnp.pad(context, (0, config[\"seq\"]), constant_values=pad_token_id)  # Let it start off with just the 2048 context tokens, plus some 50256s which will be eventually filled with sampler-chosen tokens\n                    generated_index = config[\"seq\"]\n                    # Add that information to generate_loop_fn's starting state\n                    initial_state = (generated, generated_index, sequence_index) + initial_state\n                    return sequence_index+1, initial_state\n                _, initial_states = jax.lax.scan(generate_initial_scan_fn, 0, None, numseqs)\n                sample_key = initial_states[-1][0]\n                initial_states = list(jax.tree_map(lambda x: x[i], initial_states[:-1]) for i in range(numseqs))\n                # Get repetition penalty from the arguments\n                repetition_penalty = sampler_options.pop('repetition_penalty', None)\n                rpslope = sampler_options.pop('rpslope', None)\n                rprange = sampler_options.pop('rprange', None)\n                # This is the main generation loop\n                def generate_loop_fn(carry):\n                    # Unpack current generate_loop_fn state\n                    generated, generated_index, sequence_index, next_token, decode_state = carry[0][0]\n                    sample_key = carry[1]\n                    # Get the pseudo-random number generator key that will\n                    # be used by kobold_sample_static to randomly pick a token\n                    sample_key, new_key = jax.random.split(sample_key)\n                    # Give the context to the model and get the logits it\n                    # spits out\n                    # (a 2D array with 1 row and 50400 columns representing\n                    # how strongly it thinks each of the 50257 tokens in its\n                    # vocabulary should be appended to the context, followed\n                    # by 143 apparently useless columns ???)\n                    logits, new_state = transformer.generate_once(next_token, decode_state, soft_embeddings=soft_embeddings)\n                    # Verify that logits does indeed have that many rows and\n                    # columns (if you get an error here, pray for mercy)\n                    assert logits.shape == (1, config[\"n_vocab\"])\n                    # Flatten it into a 1D array to make it easier to use\n                    logits = logits[0]\n                    # Remove any tokens in the badwords list by setting\n                    # their logits to negative infinity which effectively\n                    # makes their probabilities of being chosen zero\n                    logits = logits.at[badwords].set(-jnp.inf)\n                    # Use the sampler (kobold_sample_static) to pick one token\n                    # based on the logits array as a 0D uint32 array\n                    # (higher logit means higher probability of being\n                    # picked, non-linearly)\n                    next_token = kobold_sample_static(\n                        sample_key,\n                        logits,\n                        (\n                            generated,\n                            repetition_penalty,\n                            generated_index,\n                            gen_length,\n                            rpslope,\n                            rprange,\n                        ),\n                        **sampler_options,\n                    )\n                    # Remember what token was picked\n                    generated = generated.at[generated_index].set(next_token)\n                    generated_index += 1\n                    # Re-pack the current generate_loop_fn's state so we can\n                    # get back the same variables the next time\n                    carry[0][0] = (generated, generated_index, sequence_index, next_token[jnp.newaxis], new_state)\n                    carry[0].append(carry[0].pop(0))\n                    return carry[0], new_key\n                return jax.lax.while_loop(\n                    lambda carry: carry[0][0][1] - config[\"seq\"] < gen_length,\n                    generate_loop_fn,\n                    (initial_states, sample_key),\n                )\n            return generate_sample.apply(state[\"params\"], key, ctx, ctx_length)\n        self.generate_static_xmap = jax.experimental.maps.xmap(\n            fun=generate_static,\n            in_axes=(\n                [\"shard\", ...],\n                [\"batch\", ...],\n                [\"batch\", ...],\n                [\"batch\", ...],\n                [\"batch\", ...],\n                [\"batch\", ...],\n                [\"batch\", ...],\n                [\"shard\", ...],\n            ),\n            out_axes=[\"shard\", \"batch\", ...],\n            axis_resources={'shard': 'mp', 'batch': 'dp'},\n        )\n        def generate_initial(state, key, ctx, ctx_length, numseqs_aux, soft_embeddings=None):\n            compiling_callback()\n            numseqs = numseqs_aux.shape[0]\n            @hk.transform\n            def generate_initial_inner(context, ctx_length):\n                # Give the initial context to the transformer\n                transformer = CausalTransformerShard(config)\n                def generate_initial_scan_fn(sequence_index, c):\n                    _, initial_state = transformer.generate_initial(c, ctx_length, soft_embeddings=soft_embeddings)\n                    generated_index = config[\"seq\"]\n                    # Add that information to generate_loop_fn's starting state\n                    initial_state = (jnp.empty(config[\"n_vocab\"], dtype=jnp.float32), generated_index, sequence_index) + initial_state\n                    return sequence_index+1, initial_state\n                _, initial_states = jax.lax.scan(generate_initial_scan_fn, 0, context, numseqs)\n                sample_key = initial_states[-1][0]\n                initial_states = list(list(jax.tree_map(lambda x: x[i], initial_states[:-1])) for i in range(numseqs))\n                return initial_states, sample_key\n            return generate_initial_inner.apply(state[\"params\"], key, ctx, ctx_length)\n        self.generate_initial_xmap = jax.experimental.maps.xmap(\n            fun=generate_initial,\n            in_axes=(\n                [\"shard\", ...],\n                [\"batch\", ...],\n                [\"batch\", ...],\n                [\"batch\", ...],\n                [\"batch\", ...],\n                [\"shard\", ...],\n            ),\n            out_axes=[\"shard\", \"batch\", ...],\n            axis_resources={'shard': 'mp', 'batch': 'dp'},\n        )\n        def generate_once(data, state, numseqs_aux, soft_embeddings=None):\n            numseqs = numseqs_aux.shape[0]\n            @hk.without_apply_rng\n            @hk.transform\n            def generate_once_inner():\n                gi = data[0][1]\n                # Give the initial context to the transformer\n                transformer = CausalTransformerShard(config)\n                # This is the main generation loop\n                def generate_loop_fn(carry):\n                    # Unpack current generate_loop_fn state\n                    _, generated_index, sequence_index, next_token, decode_state = carry[0][0]\n                    # Give the context to the model and get the logits it\n                    # spits out\n                    # (a 2D array with 1 row and 50400 columns representing\n                    # how strongly it thinks each of the 50257 tokens in its\n                    # vocabulary should be appended to the context, followed\n                    # by 143 apparently useless columns ???)\n                    logits, new_state = transformer.generate_once(next_token, decode_state, soft_embeddings=soft_embeddings)\n                    # Verify that logits does indeed have that many rows and\n                    # columns (if you get an error here, pray for mercy)\n                    assert logits.shape == (1, config[\"n_vocab\"])\n                    assert logits.dtype == jnp.float32\n                    # Flatten it into a 1D array to make it easier to use\n                    logits = logits[0]\n                    # Re-pack the current generate_loop_fn's state so we can\n                    # get back the same variables the next time\n                    generated_index += 1\n                    carry[0][0] = [logits, generated_index, sequence_index, next_token, new_state]\n                    carry[0].append(carry[0].pop(0))\n                    return carry[0],\n                return jax.lax.while_loop(\n                    lambda carry: carry[0][0][1] == gi,\n                    generate_loop_fn,\n                    (data,),\n                )\n            return generate_once_inner.apply(state[\"params\"])\n        self.generate_once_xmap = jax.experimental.maps.xmap(\n            fun=generate_once,\n            in_axes=(\n                [\"shard\", \"batch\", ...],\n                [\"shard\", ...],\n                [\"batch\", ...],\n                [\"shard\", ...],\n            ),\n            out_axes=[\"shard\", \"batch\", ...],\n            axis_resources={'shard': 'mp', 'batch': 'dp'},\n        )\n    def generate_dynamic(self, ctx, ctx_length, gen_length, numseqs, return_logits=False, soft_embeddings=None, excluded_world_info=None, use_callback=True):\n        assert excluded_world_info is not None\n        assert not return_logits\n        assert gen_length.ndim == 1\n        assert soft_embeddings is not None\n        key = hk.PRNGSequence(rng.randint(0, 2 ** 60))\n        batch_size = ctx.shape[0]\n        self.batch_size = batch_size\n        _numseqs_aux = jnp.empty((batch_size, numseqs), dtype=np.uint32)\n        numseqs_aux = batch_xmap(_numseqs_aux)\n        sample_data = [\n            [\n                np.pad(ctx[0][i], (0, params[\"seq\"]), constant_values=pad_token_id),\n                params[\"seq\"],\n                None,\n                np.empty((), dtype=np.uint32),\n            ]\n            for i in range(numseqs)\n        ]\n        n_generated = 0\n        regeneration_required = False\n        halt = False\n        started_compiling_callback()\n        generate_data, sample_key = self.generate_initial_xmap(self.state, jnp.array(key.take(batch_size)), ctx, ctx_length, numseqs_aux, soft_embeddings)\n        sample_key = np.asarray(sample_key[0, 0])\n        while True:\n            generate_data, = self.generate_once_xmap(generate_data, self.state, numseqs_aux, soft_embeddings)\n            for i in range(numseqs):\n                sample_data[i][2] = np.array(generate_data[i][0][0, 0], copy=True)\n            if use_callback:\n                logits = np.float32(tuple(d[2] for d in sample_data))\n                logits = warper_callback(logits)\n                for i in range(numseqs):\n                    sample_data[i][2] = logits[i]\n            sampler_options = settings_callback()\n            repetition_penalty = sampler_options.pop(\"repetition_penalty\", 1.0)\n            rpslope = sampler_options.pop(\"rpslope\", 0.0)\n            rprange = sampler_options.pop(\"rprange\", 0)\n            sample_data, sample_key = sample_func(sample_data, sample_key, _numseqs_aux, badwords, repetition_penalty, params[\"seq\"] + n_generated, gen_length, rpslope, rprange, sampler_options)\n            n_generated += 1\n            for i in range(numseqs):\n                generate_data[i][3] = np.tile(sample_data[i][0][sample_data[i][1]-1][np.newaxis, np.newaxis], (params[\"cores_per_replica\"], 1, 1))\n            if use_callback:\n                generated = np.uint32(tuple(d[0] for d in sample_data))\n                excluded_world_info, regeneration_required, halt = stopping_callback(generated, n_generated, excluded_world_info)\n                if regeneration_required or halt:\n                    break\n            else:\n                break\n        stopped_compiling_callback()\n        return sample_data, n_generated, regeneration_required, halt\n    def generate_static(self, ctx, ctx_length, gen_length, numseqs, sampler_options, return_logits=False, soft_embeddings=None):\n        assert not return_logits\n        key = hk.PRNGSequence(rng.randint(0, 2 ** 60))\n        batch_size = ctx.shape[0]\n        self.batch_size = batch_size\n        started_compiling_callback()\n        result = self.generate_static_xmap(\n            self.state,\n            jnp.array(key.take(batch_size)),\n            ctx,\n            np.array(ctx_length, dtype=np.uint32),\n            np.array(gen_length, dtype=np.uint32),\n            np.empty((batch_size, numseqs), dtype=np.uint8),\n            sampler_options,\n            soft_embeddings,\n        )\n        stopped_compiling_callback()\n        return result\n\n\ndef infer_dynamic(\n    context: np.array,\n    numseqs=1,\n    gen_len=80,\n    soft_embeddings: Optional[np.array] = None,\n    soft_tokens: Optional[np.array] = None,\n    excluded_world_info = None,\n    use_callback=True,\n) -> Tuple[List[np.array], int, bool, bool]:\n    assert excluded_world_info is not None\n    maps.thread_resources.env = thread_resources_env\n    total_batch = 1\n    tokens = context\n    if(soft_tokens is not None):\n        tokens = np.uint32(np.concatenate((np.tile(soft_tokens, (tokens.shape[0], 1)), tokens), axis=-1))\n    provided_ctx = tokens.shape[-1]\n    pad_amount = seq - provided_ctx\n    padded_tokens = np.pad(tokens, ((0, 0), (pad_amount, 0)), constant_values=pad_token_id)\n    batched_tokens = np.array([padded_tokens] * total_batch)\n    samples = []\n    output = network.generate_dynamic(\n        batched_tokens,\n        np.ones(total_batch, dtype=np.uint32) * provided_ctx,\n        np.ones(total_batch, dtype=np.uint32) * gen_len,\n        numseqs,\n        soft_embeddings=soft_embeddings,\n        excluded_world_info=excluded_world_info,\n        use_callback=use_callback,\n    )\n    for out in output[0]:\n        samples.append(out[0][params[\"seq\"] : params[\"seq\"] + gen_len])\n    return (samples,) + output[1:]\n\ndef infer_static(\n    context: np.array,\n    top_p=0.9,\n    temp=0.5,\n    top_k=0,\n    tfs=1.0,\n    typical=1.0,\n    top_a=0.0,\n    repetition_penalty=1.0,\n    rpslope=0.0,\n    rprange=0,\n    numseqs=1,\n    gen_len=80,\n    soft_embeddings: Optional[np.array] = None,\n    soft_tokens: Optional[np.array] = None,\n    sampler_order: Optional[List[int]] = None,\n) -> List[np.array]:\n    maps.thread_resources.env = thread_resources_env\n    if sampler_order is None:\n        sampler_order = utils.default_sampler_order.copy()\n    sampler_order = sampler_order[:]\n    if len(sampler_order) < 7:  # Add repetition penalty at beginning if it's not present\n        sampler_order = [6] + sampler_order\n    sampler_order = np.uint32(sampler_order)\n    total_batch = 1\n    tokens = context\n    if(soft_tokens is not None):\n        tokens = np.uint32(np.concatenate((soft_tokens, tokens)))\n    provided_ctx = tokens.shape[0]\n    pad_amount = seq - provided_ctx\n    padded_tokens = np.pad(tokens, ((pad_amount, 0),), constant_values=pad_token_id)\n    batched_tokens = np.array([padded_tokens] * total_batch)\n    samples = []\n    batched_generator_params = {\n        \"sampler_order\": np.repeat(sampler_order[np.newaxis], total_batch, axis=0),\n        \"temp\": temp * np.ones(total_batch),\n        \"top_p\": top_p * np.ones(total_batch),\n        \"tfs\": tfs * np.ones(total_batch),\n        \"typical\": typical * np.ones(total_batch),\n        \"top_a\": top_a * np.ones(total_batch),\n        \"repetition_penalty\": repetition_penalty * np.ones(total_batch),\n        \"rpslope\": rpslope * np.ones(total_batch),\n        \"rprange\": np.full(total_batch, rprange, dtype=np.uint32),\n        \"top_k\": np.full(total_batch, top_k, dtype=np.uint32)\n    }\n    output = network.generate_static(\n        batched_tokens,\n        np.ones(total_batch, dtype=np.uint32) * provided_ctx,\n        np.ones(total_batch, dtype=np.uint32) * gen_len,\n        numseqs,\n        batched_generator_params,\n        soft_embeddings=soft_embeddings,\n    )[0]\n    for o in output:\n        samples.append(o[0][0, 0, params[\"seq\"] : params[\"seq\"] + gen_len])\n    return samples\n\n\ndef reshard_reverse(x, total_shards, old_shape):\n    assert len(x.shape) != 1\n    if len(x.shape) == 2:\n        if old_shape[1] == x.shape[1]:\n            out = x[0:1].tile((total_shards, 1))\n        else:\n            out = x.reshape(old_shape)\n    elif len(x.shape) == 3:\n        if x.shape[0] * x.shape[2] == old_shape[2]:\n            out = x.reshape(old_shape)\n        elif x.shape[0] * x.shape[1] == old_shape[1]:\n            out = x.reshape((old_shape[1], old_shape[0], old_shape[2])).permute((1, 0, 2))\n        else:\n            assert False\n    else:\n        assert False\n    return out\n\n\ndef get_old_shape(t, total_shards, dim=2):\n    if len(t.shape) == 2:\n        shard_shape = t.shape\n        if dim == 1:\n            assert shard_shape[0] % total_shards == 0\n            return (shard_shape[0] // total_shards, shard_shape[1])\n        elif dim == 2:\n            assert shard_shape[1] % total_shards == 0\n            return (shard_shape[0], shard_shape[1] // total_shards)\n        else:\n            raise ValueError(f\"Unsupported dim {dim}\")\n    if len(t.shape) == 1:\n        assert t.shape[0] % total_shards == 0\n        return (t.shape[0] // total_shards,)\n    else:\n        raise ValueError(f\"Unsupported shape {t.shape}\")\n\n\ndef read_neox_checkpoint(state, path, config, checkpoint_shards=2):\n    assert config[\"cores_per_replica\"] % checkpoint_shards == 0\n    output_shards = config[\"cores_per_replica\"] // checkpoint_shards\n\n    import torch\n    import torch.utils.dlpack\n    import torch_lazy_loader\n    from tqdm.auto import tqdm\n\n    move_xmap = jax.experimental.maps.xmap(\n        fun=lambda x, _: to_bf16(x),\n        in_axes=([\"shard\", ...], [\"batch\", ...]),\n        out_axes=[\"shard\", ...],\n        axis_resources={'shard': 'mp', 'batch': 'dp'}\n    )\n\n    path_template = os.path.join(path, \"layer_{layer:02d}-model_{shard:02d}-model_states.pt\")\n\n    static_mapping = {\n        \"word_embeddings.weight\": {\"module\": \"embedding_shard/~/linear\", \"param\": \"w\", \"axis\": 1},\n        \"final_linear.weight\": {\"module\": \"projection_shard/~/linear\", \"param\": \"w\", \"axis\": 2},\n        \"norm.weight\": {\"module\": \"projection_shard/~/replicated_layer_norm\", \"param\": \"scale\", \"axis\": None},\n        \"norm.bias\": {\"module\": \"projection_shard/~/replicated_layer_norm\", \"param\": \"offset\", \"axis\": None},\n    }\n\n    layer_mapping = {\n        \"attention.query_key_value.weight\": {\"module\": \"combined_qkv\", \"param\": \"w\", \"axis\": 2},\n        \"attention.query_key_value.bias\": {\"module\": \"combined_qkv\", \"param\": \"b\", \"axis\": 1},\n        \"attention.dense.weight\": {\"module\": \"linear_3\", \"param\": \"w\", \"axis\": 1},\n        \"attention.dense.bias\": {\"module\": \"linear_3\", \"param\": \"b\", \"axis\": None},\n        \"mlp.dense_h_to_4h.weight\": {\"module\": \"linear_4\", \"param\": \"w\", \"axis\": 2},\n        \"mlp.dense_h_to_4h.bias\": {\"module\": \"linear_4\", \"param\": \"b\", \"axis\": 1},\n        \"mlp.dense_4h_to_h.weight\": {\"module\": \"linear_5\", \"param\": \"w\", \"axis\": 1},\n        \"mlp.dense_4h_to_h.bias\": {\"module\": \"linear_5\", \"param\": \"b\", \"axis\": None},\n        \"input_layernorm.weight\": {\"module\": \"replicated_layer_norm\", \"param\": \"scale\", \"axis\": None},\n        \"input_layernorm.bias\": {\"module\": \"replicated_layer_norm\", \"param\": \"offset\", \"axis\": None},\n        \"post_attention_layernorm.weight\": {\"module\": \"replicated_layer_norm_1\", \"param\": \"scale\", \"axis\": None},\n        \"post_attention_layernorm.bias\": {\"module\": \"replicated_layer_norm_1\", \"param\": \"offset\", \"axis\": None},\n    }\n\n    tqdm_length = len(static_mapping) + config[\"layers\"]*len(layer_mapping)\n    bar = tqdm(total=tqdm_length, desc=\"Loading from NeoX checkpoint\")\n\n    for checkpoint_layer in range(config[\"layers\"] + 5):\n        if checkpoint_layer in (1, config[\"layers\"] + 2):\n            continue\n        layer = checkpoint_layer - 2\n        shards = []\n        with torch_lazy_loader.use_custom_unpickler(torch_lazy_loader.RestrictedUnpickler):\n            for checkpoint_shard in range(checkpoint_shards):\n                shards.append(torch.load(path_template.format(layer=checkpoint_layer, shard=checkpoint_shard), map_location=\"cpu\"))\n        for key in shards[0]:\n            if key == \"attention.rotary_emb.inv_freq\":\n                continue\n            elif key in static_mapping:\n                target_module = \"causal_transformer_shard/~/\" + static_mapping[key][\"module\"]\n                target_param = static_mapping[key][\"param\"]\n                target_axis = static_mapping[key][\"axis\"]\n            elif key in layer_mapping:\n                target_module = f\"causal_transformer_shard/~/layer_{layer}/~/\" + layer_mapping[key][\"module\"]\n                target_param = layer_mapping[key][\"param\"]\n                target_axis = layer_mapping[key][\"axis\"]\n            else:\n                error = f\"{repr(key)} not found in mapping\"\n                print(\"\\n\\nERROR: \", error, file=sys.stderr)\n                raise RuntimeError(error)\n            original_shape = shards[0][key].shape\n            for checkpoint_shard in range(checkpoint_shards):\n                if key in (\"attention.dense.bias\", \"mlp.dense_4h_to_h.bias\"):\n                    shards[checkpoint_shard][key] /= output_shards\n                if key != \"word_embeddings.weight\" and shards[checkpoint_shard][key].ndim == 2:\n                    shards[checkpoint_shard][key] = shards[checkpoint_shard][key].T\n                tensor = shards[checkpoint_shard][key]\n                if target_axis is not None:\n                    target_shape = (output_shards,) + get_old_shape(tensor, total_shards=output_shards, dim=target_axis)\n                else:\n                    target_shape = (output_shards, tensor.shape[0])\n                shards[checkpoint_shard][key] = reshard_reverse(tensor.unsqueeze_(0), output_shards, target_shape)\n            #print(key, \":\", original_shape, \"->\", shards[0][key].shape)\n            tensor = torch.cat([shards[s][key] for s in range(checkpoint_shards)], dim=0)\n            target_shape = state[\"params\"][target_module][target_param].shape\n            if tensor.shape != target_shape:\n                error = f\"Weight {repr(key)} has shape {tensor.shape} in checkpoint but shape {target_shape} was requested by MTJ for {target_module} {target_param}\"\n                print(\"\\n\\nERROR: \", error, file=sys.stderr)\n                raise RuntimeError(error)\n            if tensor.dtype is torch.float16 or tensor.dtype is torch.float32:\n                tensor = tensor.bfloat16()\n            state[\"params\"][target_module][target_param] = move_xmap(\n                jax.dlpack.from_dlpack(torch.utils.dlpack.to_dlpack(tensor)).copy(),\n                np.zeros(config[\"cores_per_replica\"]),\n            )\n            bar.update(1)\n    for mk, mv in state[\"params\"].items():\n        for pk, pv in mv.items():\n            if isinstance(pv, PlaceholderTensor):\n                error = f\"{mk} {pk} could not be found in the model checkpoint\"\n                print(\"\\n\\nERROR:  \" + error, file=sys.stderr)\n                raise RuntimeError(error)\n\n\ndef load_model(path: str, driver_version=\"tpu_driver_20221109\", hf_checkpoint=False, socketio_queue=None, initial_load=False, logger=None, **kwargs) -> None:\n    global thread_resources_env, seq, tokenizer, network, params, pad_token_id\n\n    if \"pad_token_id\" in kwargs:\n        pad_token_id = kwargs[\"pad_token_id\"]\n    elif \"eos_token_id\" in kwargs:\n        pad_token_id = kwargs[\"eos_token_id\"]\n\n    if not hasattr(vars, \"sampler_order\") or not vars.sampler_order:\n        vars.sampler_order = utils.default_sampler_order.copy()\n\n    default_params = {\n        \"compat\": \"j\",\n        \"layers\": 28,\n        \"d_model\": 4096,\n        \"n_heads\": 16,\n        \"n_vocab\": 50400,\n        \"n_vocab_padding\": 0,\n        \"norm\": \"layernorm\",\n        \"pe\": \"rotary\",\n        \"pe_rotary_dims\": 64,\n        \"seq\": 2048,\n        \"cores_per_replica\": 8,\n        \"tokenizer_class\": \"GPT2Tokenizer\",\n        \"tokenizer\": \"gpt2\",\n    }\n    params = kwargs\n\n    if vars.model == \"TPUMeshTransformerGPTNeoX\":\n        default_params = {\n            \"compat\": \"neox\",\n            \"layers\": 44,\n            \"d_model\": 6144,\n            \"n_heads\": 64,\n            \"n_vocab\": 50432,\n            \"n_vocab_padding\": 0,\n            \"norm\": \"doublelayernorm\",\n            \"pe\": \"neox_rotary\",\n            \"pe_rotary_dims\": 24,\n            \"seq\": 2048,\n            \"cores_per_replica\": 8,\n            \"tokenizer_class\": \"GPT2Tokenizer\",\n            \"tokenizer\": \"gpt2\",\n        }\n\n    # Try to convert HF config.json to MTJ config\n    if hf_checkpoint:\n        spec_path = os.path.join(\"maps\", vars.model_type + \".json\")\n        if not os.path.isfile(spec_path):\n            raise NotImplementedError(f\"Unsupported model type {repr(vars.model_type)}\")\n        with open(spec_path) as f:\n            lazy_load_spec = json.load(f)\n\n        if \"mtj_compat\" in lazy_load_spec:\n            params[\"compat\"] = lazy_load_spec[\"mtj_compat\"]\n        if \"mtj_pe\" in lazy_load_spec:\n            params[\"pe\"] = lazy_load_spec[\"mtj_pe\"]\n        for k, v in lazy_load_spec.get(\"mtj_config_map\", {}).items():\n            if type(v) is not list:\n                params[k] = params[v]\n                continue\n            for i in range(len(v)):\n                if i == len(v) - 1:\n                    params[k] = v[i]\n                elif v[i] in params:\n                    params[k] = params[v[i]]\n                    break\n\n        params[\"n_vocab\"] = params[\"vocab_size\"]\n\n        if \"activation_function\" in params:\n            params[\"activation\"] = params[\"activation_function\"]\n\n        # Both the number of attention heads in the model and the embedding\n        # dimension of the model need to be divisible by the number of TPU cores\n        # that we use, and JAX also requires the number of TPU cores used to be\n        # an even number if we're using more than one core, so logically we try\n        # to pick the largest possible even number of TPU cores such that the\n        # number of attention heads and embedding dimension are both divisible\n        # by the number of TPU cores, and fall back to one core if an even\n        # number of TPU cores is not possible.\n        for c in (8, 6, 4, 2, 1):\n            if 0 == params[\"n_heads\"] % c == params.get(\"d_embed\", params[\"d_model\"]) % c:\n                params[\"cores_per_replica\"] = c\n                break\n\n        # The vocabulary size of the model also has to be divisible by the\n        # number of TPU cores, so we pad the vocabulary with the minimum\n        # possible number of dummy tokens such that it's divisible.\n        params[\"n_vocab_padding\"] = -(params[\"n_vocab\"] % -params[\"cores_per_replica\"])\n\n    if \"compat\" in params:\n        default_params[\"compat\"] = params[\"compat\"]\n    if default_params[\"compat\"] == \"fairseq_lm\":\n        default_params[\"tokenizer\"] = \"KoboldAI/fairseq-dense-125M\"\n    for param in default_params:\n        if param not in params:\n            params[param] = default_params[param]\n\n    # Use an optimization that will allow us to avoid one extra transpose operation\n    if hf_checkpoint:\n        params[\"transposed_linear\"] = True\n\n    # Load tokenizer\n    if vars.model == \"TPUMeshTransformerGPTNeoX\":\n        tokenizer = Tokenizer.from_file(os.path.join(path, \"20B_tokenizer.json\"))\n        def new_encode(old_encode):\n            def encode(s, *args, **kwargs):\n                return old_encode(s).ids\n            return encode\n        tokenizer.encode = new_encode(tokenizer.encode)\n        tokenizer._koboldai_header = []\n    elif not hf_checkpoint:\n        if not isinstance(params[\"tokenizer_class\"], str) or not any(params[\"tokenizer_class\"].endswith(s) for s in (\"Tokenizer\", \"TokenizerFast\")):\n            raise ValueError(\"`tokenizer_class` must be a string ending in 'Tokenizer' or 'TokenizerFast'\")\n        tokenizer_class = getattr(__import__(\"transformers\"), params[\"tokenizer_class\"])\n        tokenizer = tokenizer_class.from_pretrained(params[\"tokenizer\"])\n\n    # Disable JAX warnings about these two functions having been renamed\n    jax.host_count = jax.process_count\n    jax.host_id = jax.process_index\n\n    print(\"Connecting to your Colab instance's TPU\", flush=True)\n    spinner = multiprocessing.Process(target=show_spinner, args=())\n    spinner.start()\n    if os.environ.get('COLAB_TPU_ADDR', '') != '':\n        tpu_address = os.environ['COLAB_TPU_ADDR']  # Colab\n    else:\n        tpu_address = os.environ['TPU_NAME']  # Kaggle\n    tpu_address = tpu_address.replace(\"grpc://\", \"\")\n    tpu_address_without_port = tpu_address.split(':', 1)[0]\n    url = f'http://{tpu_address_without_port}:8475/requestversion/{driver_version}'\n    requests.post(url)\n    config.FLAGS.jax_xla_backend = \"tpu_driver\"\n    config.FLAGS.jax_backend_target = \"grpc://\" + tpu_address\n    spinner.terminate()\n    print()\n\n    cores_per_replica = params[\"cores_per_replica\"]\n    seq = params[\"seq\"]\n    params[\"optimizer\"] = _DummyOptimizer()\n    mesh_shape = (1, cores_per_replica)\n    devices = np.array(jax.devices()[:cores_per_replica]).reshape(mesh_shape)\n    thread_resources_env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')), ())\n    maps.thread_resources.env = thread_resources_env\n\n    global badwords\n    # These are the tokens that we don't want the AI to ever write\n    badwords = jnp.array(vars.badwordsids).squeeze()\n\n    if not path.endswith(\"/\"):\n        path += \"/\"\n\n    network = PenalizingCausalTransformer(params, dematerialized=True)\n\n    if not hf_checkpoint and vars.model != \"TPUMeshTransformerGPTNeoX\":\n        network.state = read_ckpt_lowmem(network.state, path, devices.shape[1])\n        #network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))\n        return\n\n    if vars.model == \"TPUMeshTransformerGPTNeoX\":\n        print(\"\\n\\n\\nThis model has  \", f\"{hk.data_structures.tree_size(network.state['params']):,d}\".replace(\",\", \" \"), \"  parameters.\\n\")\n        read_neox_checkpoint(network.state, path, params)\n        return\n\n    # Convert from HF checkpoint\n\n    move_xmap = jax.experimental.maps.xmap(\n        fun=lambda x, _: to_bf16(x),\n        in_axes=([\"shard\", ...], [\"batch\", ...]),\n        out_axes=[\"shard\", ...],\n        axis_resources={'shard': 'mp', 'batch': 'dp'}\n    )\n\n    model_spec = {}\n    for key, spec in lazy_load_spec.get(\"static_weights\", {}).items():\n        if spec.get(\"mtj\") is not None:\n            model_spec[key] = spec[\"mtj\"].copy()\n            model_spec[key][\"module\"] = \"causal_transformer_shard/~/\" + model_spec[key][\"module\"]\n    for _key, spec in lazy_load_spec.get(\"layer_weights\", {}).items():\n        for layer in range(params[\"layers\"]):\n            if spec.get(\"mtj\") is not None:\n                key = _key.format(layer=layer)\n                model_spec[key] = spec[\"mtj\"].copy()\n                model_spec[key][\"module\"] = \"causal_transformer_shard/~/\" + model_spec[key][\"module\"].format(layer=layer)\n\n    import torch_lazy_loader\n    import torch\n    from tqdm.auto import tqdm\n    import functools\n\n\n    def callback(model_dict, f, **_):\n        if callback.nested:\n            return\n        callback.nested = True\n        with zipfile.ZipFile(f, \"r\") as z:\n            try:\n                last_storage_key = None\n                zipfolder = os.path.basename(os.path.normpath(f)).split('.')[0]\n                f = None\n                current_offset = 0\n                if utils.current_shard == 0:\n                    print(\"\\n\\n\\nThis model has  \", f\"{hk.data_structures.tree_size(network.state['params']):,d}\".replace(\",\", \" \"), \"  parameters.\\n\")\n\n                if utils.num_shards is None or utils.current_shard == 0:\n                    if utils.num_shards is not None:\n                        num_tensors = len(utils.get_sharded_checkpoint_num_tensors(utils.from_pretrained_model_name, utils.from_pretrained_index_filename, **utils.from_pretrained_kwargs))\n                    else:\n                        num_tensors = len(model_dict)\n                    utils.bar = tqdm(total=num_tensors, desc=\"Loading model tensors\")\n\n                if utils.num_shards is not None:\n                    utils.current_shard += 1\n                for key in sorted(model_dict.keys(), key=lambda k: (model_dict[k].key, model_dict[k].seek_offset)):\n                    model_spec_key = max((k for k in model_spec.keys() if key.endswith(k)), key=len, default=None)\n\n                    # Some model weights are used by transformers but not by MTJ.\n                    # We have to materialize these weights anyways because\n                    # transformers will throw a tantrum otherwise.  To attain\n                    # the least possible memory usage, we create them as meta\n                    # tensors, which don't take up any actual CPU or TPU memory.\n                    if model_spec_key is None:\n                        model_dict[key] = torch.empty(model_dict[key].shape, dtype=model_dict[key].dtype, device=\"meta\")\n                        utils.bar.update(1)\n                        continue\n\n                    storage_key = model_dict[key].key\n                    if storage_key != last_storage_key or model_dict[key].seek_offset < current_offset:\n                        last_storage_key = storage_key\n                        if isinstance(f, zipfile.ZipExtFile):\n                            f.close()\n                        try:\n                            f = z.open(f\"archive/data/{storage_key}\")\n                        except:\n                            f = z.open(f\"{zipfolder}/data/{storage_key}\")\n                        current_offset = 0\n                    if current_offset != model_dict[key].seek_offset:\n                        f.read(model_dict[key].seek_offset - current_offset)\n                        current_offset = model_dict[key].seek_offset\n                    spec = model_spec[model_spec_key]\n                    transforms = set(spec.get(\"transforms\", ()))\n                    if not isinstance(model_dict[key], torch_lazy_loader.LazyTensor):\n                        error = f\"Duplicate key {repr(key)}\"\n                        print(\"\\n\\nERROR:  \" + error, file=sys.stderr)\n                        raise RuntimeError(error)\n                    size = functools.reduce(lambda x, y: x * y, model_dict[key].shape, 1)\n                    dtype = model_dict[key].dtype\n                    nbytes = size if dtype is torch.bool else size * ((torch.finfo if dtype.is_floating_point else torch.iinfo)(dtype).bits >> 3)\n                    tensor = model_dict[key].materialize(f, map_location=\"cpu\")\n                    model_dict[key] = tensor.to(\"meta\")\n                    current_offset += nbytes\n\n                    # MTJ requires certain mathematical operations to be performed\n                    # on tensors in order for them to be in the correct format\n                    if \"remove_first_two_rows\" in transforms:\n                        tensor = tensor[2:]\n                    if \"divide_by_shards\" in transforms:\n                        tensor /= params[\"cores_per_replica\"]\n                    if \"vocab_pad\" in transforms:\n                        tensor = torch.nn.functional.pad(tensor, (0,) * (tensor.ndim * 2 - 1) + (params[\"n_vocab_padding\"],))\n                    # We don't need to transpose linear module weights anymore because MTJ will do it for us if `transposed_linear` is set to True in the config\n                    #if \"no_transpose\" not in transforms and tensor.ndim == 2:\n                    #    tensor = tensor.T\n                    tensor.unsqueeze_(0)\n                    \n\n                    # Shard the tensor so that parts of the tensor can be used\n                    # on different TPU cores\n                    tensor = reshard_reverse(\n                        tensor,\n                        params[\"cores_per_replica\"],\n                        network.state[\"params\"][spec[\"module\"]][spec[\"param\"]].shape,\n                    )\n                    tensor = jnp.array(tensor.detach())\n                    if tensor.dtype is torch.float16 or tensor.dtype is torch.float32:\n                        tensor = tensor.bfloat16()\n                    network.state[\"params\"][spec[\"module\"]][spec[\"param\"]] = move_xmap(\n                        tensor,\n                        np.empty(params[\"cores_per_replica\"]),\n                    )\n\n                    utils.bar.update(1)\n\n                if utils.num_shards is not None and utils.current_shard < utils.num_shards:\n                    return\n\n                # Check for tensors that MTJ needs that were not provided in the\n                # HF model\n                for mk, mv in network.state[\"params\"].items():\n                    for pk, pv in mv.items():\n                        if isinstance(pv, PlaceholderTensor):\n                            # The transformers GPT-J models apparently do not\n                            # have embedding bias, whereas MTJ GPT-J models do,\n                            # so we have to supplement an embedding bias tensor\n                            # by creating a tensor with the necessary shape, filled\n                            # with zeros.\n                            if mk == \"causal_transformer_shard/~/embedding_shard/~/linear\" and pk == \"b\":\n                                mv[pk] = move_xmap(jnp.zeros(mv[pk].shape, dtype=jnp.bfloat16), np.empty(params[\"cores_per_replica\"]))\n\n                            else:\n                                error = f\"{mk} {pk} could not be found in the model checkpoint\"\n                                print(\"\\n\\nERROR:  \" + error, file=sys.stderr)\n                                raise RuntimeError(error)\n            finally:\n                if utils.num_shards is None or utils.current_shard >= utils.num_shards:\n                    utils.bar.close()\n                    utils.bar = None\n                callback.nested = False\n                if isinstance(f, zipfile.ZipExtFile):\n                    f.close()\n    callback.nested = False\n\n    if os.path.isdir(vars.model.replace('/', '_')):\n        import shutil\n        shutil.move(vars.model.replace('/', '_'), \"models/{}\".format(vars.model.replace('/', '_')))\n    print(\"\\n\", flush=True)\n    with torch_lazy_loader.use_lazy_torch_load(callback=callback, dematerialized_modules=True):\n        if(os.path.isdir(vars.custmodpth)):\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(vars.custmodpth, revision=vars.revision, cache_dir=\"cache\", use_fast=False)\n            except Exception as e:\n                try:\n                    tokenizer = AutoTokenizer.from_pretrained(vars.custmodpth, revision=vars.revision, cache_dir=\"cache\")\n                except Exception as e:\n                    try:\n                        tokenizer = GPT2Tokenizer.from_pretrained(vars.custmodpth, revision=vars.revision, cache_dir=\"cache\")\n                    except Exception as e:\n                        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=vars.revision, cache_dir=\"cache\")\n            try:\n                model     = AutoModelForCausalLM.from_pretrained(vars.custmodpth, revision=vars.revision, cache_dir=\"cache\")\n            except Exception as e:\n                model     = GPTNeoForCausalLM.from_pretrained(vars.custmodpth, revision=vars.revision, cache_dir=\"cache\")\n        elif(os.path.isdir(\"models/{}\".format(vars.model.replace('/', '_')))):\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=vars.revision, cache_dir=\"cache\", use_fast=False)\n            except Exception as e:\n                try:\n                    tokenizer = AutoTokenizer.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=vars.revision, cache_dir=\"cache\")\n                except Exception as e:\n                    try:\n                        tokenizer = GPT2Tokenizer.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=vars.revision, cache_dir=\"cache\")\n                    except Exception as e:\n                        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=vars.revision, cache_dir=\"cache\")\n            try:\n                model     = AutoModelForCausalLM.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=vars.revision, cache_dir=\"cache\")\n            except Exception as e:\n                model     = GPTNeoForCausalLM.from_pretrained(\"models/{}\".format(vars.model.replace('/', '_')), revision=vars.revision, cache_dir=\"cache\")\n        else:\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(vars.model, revision=vars.revision, cache_dir=\"cache\", use_fast=False)\n            except Exception as e:\n                try:\n                    tokenizer = AutoTokenizer.from_pretrained(vars.model, revision=vars.revision, cache_dir=\"cache\")\n                except Exception as e:\n                    try:\n                        tokenizer = GPT2Tokenizer.from_pretrained(vars.model, revision=vars.revision, cache_dir=\"cache\")\n                    except Exception as e:\n                        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", revision=vars.revision, cache_dir=\"cache\")\n            try:\n                model     = AutoModelForCausalLM.from_pretrained(vars.model, revision=vars.revision, cache_dir=\"cache\")\n            except Exception as e:\n                model     = GPTNeoForCausalLM.from_pretrained(vars.model, revision=vars.revision, cache_dir=\"cache\")\n\n    #network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))\n    global shard_xmap, batch_xmap\n    shard_xmap = __shard_xmap()\n    batch_xmap = __batch_xmap(shard_dim=cores_per_replica)\n"
        },
        {
          "name": "umamba.exe",
          "type": "blob",
          "size": 9507,
          "content": ""
        },
        {
          "name": "update-koboldai.bat",
          "type": "blob",
          "size": 1.4501953125,
          "content": "@echo off\ncd /d %~dp0\nSET CONDA_SHLVL=\n\nTITLE KoboldAI - Updater\nSET /P M=<loader.settings\nIF %M%==1 GOTO drivemap\nIF %M%==2 GOTO subfolder\nIF %M%==3 GOTO drivemap_B\n\n:subfolder\nSET TEMP=%~DP0MINICONDA3\nSET TMP=%~DP0MINICONDA3\ncall miniconda3\\condabin\\activate\nGOTO GIT\n\n:drivemap\nsubst K: miniconda3 >nul\nSET TEMP=K:\\\nSET TMP=K:\\\ncall K:\\python\\condabin\\activate\nGOTO GIT\n\n:drivemap_B\nsubst B: miniconda3 >nul\nSET TEMP=B:\\\nSET TMP=B:\\\ncall B:\\python\\condabin\\activate\nGOTO GIT\n\n:GIT\nECHO 1. KoboldAI Main (The Official stable version of KoboldAI)\nECHO 2. KoboldAI United (Development Version, new features but may break at any time)\nSET /P V=Enter your desired version or type your own GIT URL:\nIF %V%==1 (\nSET origin=https://github.com/koboldai/koboldai-client\nSET branch=main\n) ELSE (\n\tIF %V%==2 (\n\t\tSET origin=https://github.com/henk717/koboldai\n\t\tSET branch=united\n\t) ELSE (\n\t\tSET origin=%v%\n\t\tSET /P branch=Specify the GIT Branch:\n\t)\n)\n\ngit init     \ngit remote remove origin\ngit remote add origin %origin%    \ngit fetch --all\ngit checkout %branch% -f\ngit reset --hard origin/%branch%\nIF %M%==1 umamba.exe install --no-shortcuts -r K:\\python\\ -n base -f \"%~dp0\\environments\\huggingface.yml\" -y --always-copy\nIF %M%==2 umamba.exe install --no-shortcuts -r miniconda3 -n base -f environments\\huggingface.yml -y --always-copy\nIF %M%==3 umamba.exe install --no-shortcuts -r B:\\python\\ -n base -f \"%~dp0\\environments\\huggingface.yml\" -y --always-copy\n\n\n%windir%\\system32\\timeout -t 10"
        },
        {
          "name": "userscripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 29.2880859375,
          "content": "from threading import Timer\nimport re\nimport shutil\nimport json\nimport subprocess\nimport tempfile\nfrom urllib.error import HTTPError\nimport requests\nimport requests.adapters\nimport time\nfrom transformers import __version__ as transformers_version\nfrom transformers import PreTrainedModel\nimport packaging.version\nfrom tqdm.auto import tqdm\nimport os\nimport itertools\nimport hashlib\nimport huggingface_hub\nimport packaging.version\nfrom pathlib import Path\nfrom typing import List, Optional\n\nHAS_ACCELERATE = packaging.version.parse(transformers_version) >= packaging.version.parse(\"4.20.0.dev0\")\ntry:\n    import accelerate\nexcept ImportError:\n    HAS_ACCELERATE = False\n\nvars = None\nargs = None\nnum_shards: Optional[int] = None\ncurrent_shard = 0\nfrom_pretrained_model_name = \"\"\nfrom_pretrained_index_filename: Optional[str] = None\nfrom_pretrained_kwargs = {}\nbar = None\n\nlayers_module_names: Optional[List[str]] = None\nmodule_names: Optional[List[str]] = None\nnamed_buffers: Optional[List[tuple]] = None\n\ndefault_sampler_order = [6, 0, 1, 2, 3, 4, 5]\n\nemit = None\n\n#==================================================================#\n# Decorator to prevent a function's actions from being run until\n# at least x seconds have passed without the function being called\n#==================================================================#\ndef debounce(wait): \n    def decorator(fun):\n        def debounced(*args, **kwargs):\n            def call_it():\n                fun(*args, **kwargs)\n \n            try:\n                debounced.t.cancel()\n            except AttributeError:\n                pass\n \n            debounced.t = Timer(wait, call_it)\n            debounced.t.start()\n \n        return debounced\n \n    return decorator\n\n#==================================================================#\n# Replace fancy quotes and apostrope's with standard ones\n#==================================================================#\ndef fixquotes(txt):\n    txt = txt.replace(\"“\", '\"')\n    txt = txt.replace(\"”\", '\"')\n    txt = txt.replace(\"’\", \"'\")\n    txt = txt.replace(\"`\", \"'\")\n    return txt\n\n#==================================================================#\n# \n#==================================================================#\ndef trimincompletesentence(txt):\n    # Cache length of text\n    ln = len(txt)\n    # Find last instance of punctuation (Borrowed from Clover-Edition by cloveranon)\n    lastpunc = max(txt.rfind(\".\"), txt.rfind(\"!\"), txt.rfind(\"?\"))\n    # Is this the end of a quote?\n    if(lastpunc < ln-1):\n        if(txt[lastpunc+1] == '\"'):\n            lastpunc = lastpunc + 1\n    if(lastpunc >= 0):\n        txt = txt[:lastpunc+1]\n    return txt\n\n#==================================================================#\n# \n#==================================================================#\ndef replaceblanklines(txt):\n    txt = txt.replace(\"\\n\\n\", \"\\n\")\n    return txt\n\n#==================================================================#\n# \n#==================================================================#\ndef removespecialchars(txt, vars=None):\n    if vars is None or vars.actionmode == 0:\n        txt = re.sub(r\"[#/@%<>{}+=~|\\^]\", \"\", txt)\n    else:\n        txt = re.sub(r\"[#/@%{}+=~|\\^]\", \"\", txt)\n    return txt\n\n#==================================================================#\n# If the next action follows a sentence closure, add a space\n#==================================================================#\ndef addsentencespacing(txt, vars):\n    # Don't add sentence spacing if submission is empty or starts with whitespace\n    if(len(txt) == 0 or len(txt) != len(txt.lstrip())):\n        return txt\n    # Get last character of last action\n    if(len(vars.actions) > 0):\n        if(len(vars.actions[vars.actions.get_last_key()]) > 0):\n            action = vars.actions[vars.actions.get_last_key()]\n            lastchar = action[-1] if len(action) else \"\"\n        else:\n            # Last action is blank, this should never happen, but\n            # since it did let's bail out.\n            return txt\n    else:\n        action = vars.prompt\n        lastchar = action[-1] if len(action) else \"\"\n    if(lastchar != \" \"):\n        txt = \" \" + txt\n    return txt\n\t\ndef singlelineprocessing(txt, vars):\n    txt = vars.regex_sl.sub('', txt)\n    if(len(vars.actions) > 0):\n        if(len(vars.actions[vars.actions.get_last_key()]) > 0):\n            action = vars.actions[vars.actions.get_last_key()]\n            lastchar = action[-1] if len(action) else \"\"\n        else:\n            # Last action is blank, this should never happen, but\n            # since it did let's bail out.\n            return txt\n    else:\n        action = vars.prompt\n        lastchar = action[-1] if len(action) else \"\"\n    if(lastchar != \"\\n\"):\n        txt = txt + \"\\n\"\n    return txt\n\n#==================================================================#\n#  Cleans string for use in file name\n#==================================================================#\ndef cleanfilename(filename):\n    filteredcharacters = ('/','\\\\')\n    filename = \"\".join(c for c in filename if c not in filteredcharacters).rstrip()\n    return filename\n    \n#==================================================================#\n#  Newline substitution for fairseq models\n#==================================================================#\ndef encodenewlines(txt):\n    if(vars.newlinemode == \"s\"):\n        return txt.replace('\\n', \"</s>\")\n    return txt\n\ndef decodenewlines(txt):\n    if(vars.newlinemode == \"s\"):\n        return txt.replace(\"</s>\", '\\n')\n    if(vars.newlinemode == \"ns\"):\n        return txt.replace(\"</s>\", '')\n    return txt\n\n#==================================================================#\n#  Returns number of layers given an HF model config\n#==================================================================#\ndef num_layers(config):\n    return config[\"n_layer\"] if isinstance(config, dict) else config.num_layers if hasattr(config, \"num_layers\") else config.n_layer if hasattr(config, \"n_layer\") else config.num_hidden_layers if hasattr(config, 'num_hidden_layers') else None\n\n#==================================================================#\n#  Downloads huggingface checkpoints using aria2c if possible\n#==================================================================#\nfrom flask_socketio import emit\n            \ndef _download_with_aria2(aria2_config: str, total_length: int, directory: str = \".\", user_agent=None, force_download=False, use_auth_token=None):\n    class Send_to_socketio(object):\n        def write(self, bar):\n            bar = bar.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n            \n            if bar != \"\":\n                try:\n                    print('\\r' + bar, end='')\n                    try:\n                        emit('from_server', {'cmd': 'model_load_status', 'data': bar.replace(\" \", \"&nbsp;\")}, broadcast=True)\n                    except:\n                        pass\n                    eventlet.sleep(seconds=0)\n                except:\n                    pass\n        def flush(self):\n            pass\n    \n    import transformers\n    aria2_port = 6799 if vars is None else vars.aria2_port\n    lengths = {}\n    s = requests.Session()\n    s.mount(\"http://\", requests.adapters.HTTPAdapter(max_retries=requests.adapters.Retry(total=120, backoff_factor=1)))\n    bar = None\n    done = False\n    secret = os.urandom(17).hex()\n    try:\n        with tempfile.NamedTemporaryFile(\"w+b\", delete=False) as f:\n            f.write(aria2_config)\n            f.flush()\n            p = subprocess.Popen([\"aria2c\", \"-x\", \"10\", \"-s\", \"10\", \"-j\", \"10\", \"--enable-rpc=true\", f\"--rpc-secret={secret}\", \"--rpc-listen-port\", str(aria2_port), \"--disable-ipv6\", \"--file-allocation=trunc\", \"--allow-overwrite\", \"--auto-file-renaming=false\", \"-d\", directory, \"-i\", f.name, \"-U\", transformers.file_utils.http_user_agent(user_agent)] + ([\"-c\"] if not force_download else []) + ([f\"--header='Authorization: Bearer {use_auth_token}'\"] if use_auth_token else []), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n            while p.poll() is None:\n                r = s.post(f\"http://localhost:{aria2_port}/jsonrpc\", json={\"jsonrpc\": \"2.0\", \"id\": \"kai\", \"method\": \"aria2.tellActive\", \"params\": [f\"token:{secret}\"]}).json()[\"result\"]\n                if not r:\n                    s.close()\n                    if bar is not None:\n                        bar.n = bar.total\n                        bar.close()\n                    p.terminate()\n                    done = True\n                    break\n                if bar is None:\n                    bar = tqdm(total=total_length, desc=f\"[aria2] Downloading model\", unit=\"B\", unit_scale=True, unit_divisor=1000, file=Send_to_socketio())\n                visited = set()\n                for x in r:\n                    filename = x[\"files\"][0][\"path\"]\n                    lengths[filename] = (int(x[\"completedLength\"]), int(x[\"totalLength\"]))\n                    visited.add(filename)\n                for k, v in lengths.items():\n                    if k not in visited:\n                        lengths[k] = (v[1], v[1])\n                bar.n = sum(v[0] for v in lengths.values())\n                bar.update()\n                time.sleep(0.1)\n            path = f.name\n    except Exception as e:\n        p.terminate()\n        raise e\n    finally:\n        try:\n            os.remove(path)\n        except OSError:\n            pass\n    code = p.wait()\n    if not done and code:\n        raise OSError(f\"aria2 exited with exit code {code}\")\n\ndef _transformers22_aria2_hook(pretrained_model_name_or_path: str, force_download=False, cache_dir=None, proxies=None, resume_download=False, local_files_only=False, use_auth_token=None, user_agent=None, revision=None, **kwargs):\n    import transformers\n    import transformers.modeling_utils\n    from huggingface_hub import HfFolder\n    if use_auth_token:\n        if isinstance(use_auth_token, str):\n            token = use_auth_token\n        else:\n            token = HfFolder.get_token()\n            if token is None:\n                raise EnvironmentError(\"You specified use_auth_token=True, but a huggingface token was not found.\")\n    _cache_dir = str(cache_dir) if cache_dir is not None else transformers.TRANSFORMERS_CACHE\n    _revision = args.revision if args.revision is not None else huggingface_hub.constants.DEFAULT_REVISION\n    sharded = False\n    headers = {\"user-agent\": transformers.file_utils.http_user_agent(user_agent)}\n    if use_auth_token:\n        headers[\"authorization\"] = f\"Bearer {use_auth_token}\"\n\n    storage_folder = os.path.join(_cache_dir, huggingface_hub.file_download.repo_folder_name(repo_id=pretrained_model_name_or_path, repo_type=\"model\"))\n    os.makedirs(storage_folder, exist_ok=True)\n\n    def is_cached(filename):\n        try:\n            huggingface_hub.hf_hub_download(pretrained_model_name_or_path, filename, cache_dir=cache_dir, local_files_only=True, revision=_revision)\n        except ValueError:\n            return False\n        return True\n    while True:  # Try to get the huggingface.co URL of the model's pytorch_model.bin or pytorch_model.bin.index.json file\n        try:\n            filename = transformers.modeling_utils.WEIGHTS_INDEX_NAME if sharded else transformers.modeling_utils.WEIGHTS_NAME\n        except AttributeError:\n            return\n        url = huggingface_hub.hf_hub_url(pretrained_model_name_or_path, filename, revision=_revision)\n        if is_cached(filename) or requests.head(url, allow_redirects=True, proxies=proxies, headers=headers):\n            break\n        if sharded:\n            return\n        else:\n            sharded = True\n    if not sharded:  # If the model has a pytorch_model.bin file, that's the only file to download\n        filenames = [transformers.modeling_utils.WEIGHTS_NAME]\n    else:  # Otherwise download the pytorch_model.bin.index.json and then let aria2 download all the pytorch_model-#####-of-#####.bin files mentioned inside it\n        map_filename = huggingface_hub.hf_hub_download(pretrained_model_name_or_path, filename, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, use_auth_token=use_auth_token, user_agent=user_agent)\n        with open(map_filename) as f:\n            map_data = json.load(f)\n        filenames = set(map_data[\"weight_map\"].values())\n    urls = [huggingface_hub.hf_hub_url(pretrained_model_name_or_path, n, revision=_revision) for n in filenames]\n    if not force_download:\n        urls = [u for u, n in zip(urls, filenames) if not is_cached(n)]\n        if not urls:\n            return\n    \n    blob_paths = []\n\n    # This section is a modified version of hf_hub_download from huggingface_hub\n    # See https://github.com/huggingface/huggingface_hub/blob/main/LICENSE for license\n    for u, n in zip(urls, filenames):\n        relative_filename = os.path.join(*n.split(\"/\"))\n        if not local_files_only:\n            try:\n                r = huggingface_hub.file_download._request_wrapper(\n                    method=\"HEAD\",\n                    url=u,\n                    headers=headers,\n                    allow_redirects=False,\n                    follow_relative_redirects=True,\n                    proxies=proxies,\n                    timeout=10,\n                )\n                try:\n                    r.raise_for_status()\n                except HTTPError as e:\n                    error_code = r.headers.get(\"X-Error-Code\")\n                    if error_code != \"EntryNotFound\":\n                        raise RuntimeError(f\"HEAD {u} failed with error code {r.status_code}\")\n                    commit_hash = r.headers.get(huggingface_hub.file_download.HUGGINGFACE_HEADER_X_REPO_COMMIT)\n                    if commit_hash is not None:\n                        no_exist_file_path = (\n                            Path(storage_folder)\n                            / \".no_exist\"\n                            / commit_hash\n                            / relative_filename\n                        )\n                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)\n                        no_exist_file_path.touch()\n                        huggingface_hub.file_download._cache_commit_hash_for_specific_revision(\n                            storage_folder, _revision, commit_hash\n                        )\n                    raise\n                commit_hash = r.headers[huggingface_hub.file_download.HUGGINGFACE_HEADER_X_REPO_COMMIT]\n                if commit_hash is None:\n                    raise OSError(\n                        \"Distant resource does not seem to be on huggingface.co (missing\"\n                        \" commit header).\"\n                    )\n                etag = r.headers.get(huggingface_hub.file_download.HUGGINGFACE_HEADER_X_LINKED_ETAG) or r.headers.get(\n                    \"ETag\"\n                )\n                # We favor a custom header indicating the etag of the linked resource, and\n                # we fallback to the regular etag header.\n                # If we don't have any of those, raise an error.\n                if etag is None:\n                    raise OSError(\n                        \"Distant resource does not have an ETag, we won't be able to\"\n                        \" reliably ensure reproducibility.\"\n                    )\n                etag = huggingface_hub.file_download._normalize_etag(etag)\n                # In case of a redirect, save an extra redirect on the request.get call,\n                # and ensure we download the exact atomic version even if it changed\n                # between the HEAD and the GET (unlikely, but hey).\n                # Useful for lfs blobs that are stored on a CDN.\n                if 300 <= r.status_code <= 399:\n                    url_to_download = r.headers[\"Location\"]\n                    if (\n                        \"lfs.huggingface.co\" in url_to_download\n                        or \"lfs-staging.huggingface.co\" in url_to_download\n                    ):\n                        # Remove authorization header when downloading a LFS blob\n                        headers.pop(\"authorization\", None)\n            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):\n                # Actually raise for those subclasses of ConnectionError\n                raise\n            except (\n                requests.exceptions.ConnectionError,\n                requests.exceptions.Timeout,\n                huggingface_hub.file_download.OfflineModeIsEnabled,\n            ):\n                # Otherwise, our Internet connection is down.\n                # etag is None\n                pass\n        if etag is None:\n            # In those cases, we cannot force download.\n            if force_download:\n                raise ValueError(\n                    \"We have no connection or you passed local_files_only, so\"\n                    \" force_download is not an accepted option.\"\n                )\n            if huggingface_hub.file_download.REGEX_COMMIT_HASH.match(_revision):\n                commit_hash = _revision\n            else:\n                ref_path = os.path.join(storage_folder, \"refs\", _revision)\n                with open(ref_path) as f:\n                    commit_hash = f.read()\n            pointer_path = os.path.join(\n                storage_folder, \"snapshots\", commit_hash, relative_filename\n            )\n            if os.path.exists(pointer_path):\n                return pointer_path\n            # If we couldn't find an appropriate file on disk,\n            # raise an error.\n            # If files cannot be found and local_files_only=True,\n            # the models might've been found if local_files_only=False\n            # Notify the user about that\n            if local_files_only:\n                raise huggingface_hub.file_download.LocalEntryNotFoundError(\n                    \"Cannot find the requested files in the disk cache and\"\n                    \" outgoing traffic has been disabled. To enable hf.co look-ups\"\n                    \" and downloads online, set 'local_files_only' to False.\"\n                )\n            else:\n                raise huggingface_hub.file_download.LocalEntryNotFoundError(\n                    \"Connection error, and we cannot find the requested files in\"\n                    \" the disk cache. Please try again or make sure your Internet\"\n                    \" connection is on.\"\n                )\n        # From now on, etag and commit_hash are not None.\n        blob_path = os.path.join(storage_folder, \"blobs\", etag)\n        pointer_path = os.path.join(\n            storage_folder, \"snapshots\", commit_hash, relative_filename\n        )\n        os.makedirs(os.path.dirname(blob_path), exist_ok=True)\n        os.makedirs(os.path.dirname(pointer_path), exist_ok=True)\n        # if passed revision is not identical to commit_hash\n        # then revision has to be a branch name or tag name.\n        # In that case store a ref.\n        huggingface_hub.file_download._cache_commit_hash_for_specific_revision(storage_folder, _revision, commit_hash)\n        if os.path.exists(pointer_path) and not force_download:\n            return pointer_path\n        if os.path.exists(blob_path) and not force_download:\n            # we have the blob already, but not the pointer\n            huggingface_hub.file_download.logger.info(\"creating pointer to %s from %s\", blob_path, pointer_path)\n            huggingface_hub.file_download._create_relative_symlink(blob_path, pointer_path)\n            return pointer_path\n        # Some Windows versions do not allow for paths longer than 255 characters.\n        # In this case, we must specify it is an extended path by using the \"\\\\?\\\" prefix.\n        if os.name == \"nt\" and len(os.path.abspath(blob_path)) > 255:\n            blob_path = \"\\\\\\\\?\\\\\" + os.path.abspath(blob_path)\n        blob_paths.append(blob_path)\n\n    filenames = blob_paths\n    headers = [requests.head(u, headers=headers, allow_redirects=True, proxies=proxies, timeout=10).headers for u in urls]\n\n    for n in filenames:\n        prefix, suffix = n.rsplit(os.sep, 1)\n        path = os.path.join(prefix, \"kai-tempfile.\" + suffix + \".aria2\")\n        if os.path.exists(path):\n            os.remove(path)\n        path = os.path.join(prefix, \"kai-tempfile.\" + suffix)\n        if os.path.exists(path):\n            os.remove(path)\n    total_length = sum(int(h[\"Content-Length\"]) for h in headers)\n    aria2_config = \"\\n\".join(f\"{u}\\n  out={os.path.join(prefix, 'kai-tempfile.' + suffix)}\" for u, n in zip(urls, filenames) for prefix, suffix in [n.rsplit(os.sep, 1)]).encode()\n    _download_with_aria2(aria2_config, total_length, use_auth_token=token if use_auth_token else None, user_agent=user_agent, force_download=force_download)\n    for u, n in zip(urls, filenames):\n        prefix, suffix = n.rsplit(os.sep, 1)\n        os.rename(os.path.join(prefix, \"kai-tempfile.\" + suffix), os.path.join(prefix, suffix))\n\ndef aria2_hook(pretrained_model_name_or_path: str, force_download=False, cache_dir=None, proxies=None, resume_download=False, local_files_only=False, use_auth_token=None, user_agent=None, revision=None, **kwargs):\n    import transformers\n    import transformers.modeling_utils\n    from huggingface_hub import HfFolder\n    _revision = args.revision if args.revision is not None else huggingface_hub.constants.DEFAULT_REVISION\n    if shutil.which(\"aria2c\") is None:  # Don't do anything if aria2 is not installed\n        return\n    if local_files_only:  # If local_files_only is true, we obviously don't need to download anything\n        return\n    if os.path.isdir(pretrained_model_name_or_path) or os.path.isfile(pretrained_model_name_or_path) or os.path.isfile(pretrained_model_name_or_path + \".index\") or transformers.modeling_utils.is_remote_url(pretrained_model_name_or_path):\n        return\n    if proxies:\n        print(\"WARNING:  KoboldAI does not support using aria2 to download models from huggingface.co through a proxy.  Disabling aria2 download mode.\")\n        return\n    if packaging.version.parse(transformers.__version__) >= packaging.version.parse(\"4.22.0.dev0\"):\n        return _transformers22_aria2_hook(pretrained_model_name_or_path, force_download=force_download, cache_dir=cache_dir, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, revision=revision, **kwargs)\n    if use_auth_token:\n        if isinstance(use_auth_token, str):\n            token = use_auth_token\n        else:\n            token = HfFolder.get_token()\n            if token is None:\n                raise EnvironmentError(\"You specified use_auth_token=True, but a huggingface token was not found.\")\n    _cache_dir = str(cache_dir) if cache_dir is not None else transformers.TRANSFORMERS_CACHE\n    sharded = False\n    headers = {\"user-agent\": transformers.file_utils.http_user_agent(user_agent)}\n    if use_auth_token:\n        headers[\"authorization\"] = f\"Bearer {use_auth_token}\"\n    def is_cached(url):\n        try:\n            huggingface_hub.cached_download(url, cache_dir=cache_dir, local_files_only=True)\n        except ValueError:\n            return False\n        return True\n    while True:  # Try to get the huggingface.co URL of the model's pytorch_model.bin or pytorch_model.bin.index.json file\n        try:\n            filename = transformers.modeling_utils.WEIGHTS_INDEX_NAME if sharded else transformers.modeling_utils.WEIGHTS_NAME\n        except AttributeError:\n            return\n        url = huggingface_hub.hf_hub_url(pretrained_model_name_or_path, filename, revision=_revision)\n        if is_cached(url) or requests.head(url, allow_redirects=True, proxies=proxies, headers=headers):\n            break\n        if sharded:\n            return\n        else:\n            sharded = True\n    if not sharded:  # If the model has a pytorch_model.bin file, that's the only file to download\n        filenames = [transformers.modeling_utils.WEIGHTS_NAME]\n    else:  # Otherwise download the pytorch_model.bin.index.json and then let aria2 download all the pytorch_model-#####-of-#####.bin files mentioned inside it\n        map_filename = huggingface_hub.cached_download(url, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, use_auth_token=use_auth_token, user_agent=user_agent)\n        with open(map_filename) as f:\n            map_data = json.load(f)\n        filenames = set(map_data[\"weight_map\"].values())\n    urls = [huggingface_hub.hf_hub_url(pretrained_model_name_or_path, n, revision=_revision) for n in filenames]\n    if not force_download:\n        urls = [u for u in urls if not is_cached(u)]\n        if not urls:\n            return\n    etags = [h.get(\"X-Linked-Etag\") or h.get(\"ETag\") for u in urls for h in [requests.head(u, headers=headers, allow_redirects=False, proxies=proxies, timeout=10).headers]]\n    headers = [requests.head(u, headers=headers, allow_redirects=True, proxies=proxies, timeout=10).headers for u in urls]\n    filenames = [hashlib.sha256(u.encode(\"utf-8\")).hexdigest() + \".\" + hashlib.sha256(t.encode(\"utf-8\")).hexdigest() for u, t in zip(urls, etags)]\n    for n in filenames:\n        path = os.path.join(_cache_dir, \"kai-tempfile.\" + n + \".aria2\")\n        if os.path.exists(path):\n            os.remove(path)\n        path = os.path.join(_cache_dir, \"kai-tempfile.\" + n)\n        if os.path.exists(path):\n            os.remove(path)\n        if force_download:\n            path = os.path.join(_cache_dir, n + \".json\")\n            if os.path.exists(path):\n                os.remove(path)\n            path = os.path.join(_cache_dir, n)\n            if os.path.exists(path):\n                os.remove(path)\n    total_length = sum(int(h[\"Content-Length\"]) for h in headers)\n    aria2_config = \"\\n\".join(f\"{u}\\n  out=kai-tempfile.{n}\" for u, n in zip(urls, filenames)).encode()\n    _download_with_aria2(aria2_config, total_length, directory=_cache_dir, use_auth_token=token if use_auth_token else None, user_agent=user_agent, force_download=force_download)\n    for u, t, n in zip(urls, etags, filenames):\n        os.rename(os.path.join(_cache_dir, \"kai-tempfile.\" + n), os.path.join(_cache_dir, n))\n        with open(os.path.join(_cache_dir, n + \".json\"), \"w\") as f:\n            json.dump({\"url\": u, \"etag\": t}, f)\n\n#==================================================================#\n#  Given the path to a pytorch_model.bin.index.json, returns how many\n#  shards there are in the model\n#==================================================================#\ndef get_num_shards(filename):\n    with open(filename) as f:\n        map_data = json.load(f)\n    return len(set(map_data[\"weight_map\"].values()))\n\n#==================================================================#\n#  Given the name/path of a sharded model and the path to a\n#  pytorch_model.bin.index.json, returns a list of weight names in the\n#  sharded model.  Requires lazy loader to be enabled to work properl\n#==================================================================#\ndef get_sharded_checkpoint_num_tensors(pretrained_model_name_or_path, filename, cache_dir=None, force_download=False, proxies=None, resume_download=False, local_files_only=False, use_auth_token=None, user_agent=None, revision=None, **kwargs):\n    import transformers.modeling_utils\n    import torch\n    _revision = args.revision if args.revision is not None else huggingface_hub.constants.DEFAULT_REVISION\n    shard_paths, _ = transformers.modeling_utils.get_checkpoint_shard_files(pretrained_model_name_or_path, filename, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, user_agent=user_agent, revision=_revision)\n    return list(itertools.chain(*(torch.load(p, map_location=\"cpu\").keys() for p in shard_paths)))\n\n#==================================================================#\n#  Given a PreTrainedModel, returns the list of module names that correspond\n#  to the model's hidden layers.\n#==================================================================#\ndef get_layers_module_names(model: PreTrainedModel) -> List[str]:\n    names: List[str] = []\n    def recurse(module, head=\"\"):\n        for c in module.named_children():\n            name = head + c[0]\n            if c[0].isnumeric() and any(c[1].__class__.__name__.endswith(suffix) for suffix in (\"Block\", \"Layer\")):\n                names.append(name)\n            else:\n                recurse(c[1], head=name + \".\")\n    recurse(model)\n    return names\n\n#==================================================================#\n#  Given a PreTrainedModel, returns the module name that corresponds\n#  to the model's input embeddings.\n#==================================================================#\ndef get_input_embeddings_module_name(model: PreTrainedModel) -> str:\n    embeddings = model.get_input_embeddings()\n    def recurse(module, head=\"\"):\n        for c in module.named_children():\n            name = head + c[0]\n            if c[1] is embeddings:\n                return name\n            else:\n                return recurse(c[1], head=name + \".\")\n    return recurse(model)\n\n#==================================================================#\n#  Given a PreTrainedModel and a list of module names, returns a list\n#  of module names such that the union of the set of modules given as input\n#  and the set of modules returned as output contains all modules in the model.\n#==================================================================#\ndef get_missing_module_names(model: PreTrainedModel, names: List[str]) -> List[str]:\n    missing_names: List[str] = []\n    def recurse(module, head=\"\"):\n        for c in module.named_children():\n            name = head + c[0]\n            if any(name.startswith(n) for n in names):\n                continue\n            if next(c[1].named_children(), None) is None:\n                missing_names.append(name)\n            else:\n                recurse(c[1], head=name + \".\")\n    recurse(model)\n    return missing_names"
        },
        {
          "name": "warpers.py",
          "type": "blob",
          "size": 7.865234375,
          "content": "'''\nThis file is AGPL-licensed.\n\nSome of the code in this file is from Clover Edition:\nhttps://github.com/cloveranon/Clover-Edition/blob/master/aidungeon/gpt2generator.py\n\nThe license for Clover Edition is shown below:\n\nCopyright (c) 2019 Nick Walton\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n'''\n\nimport torch\nfrom transformers import LogitsWarper\n\n\nclass AdvancedRepetitionPenaltyLogitsProcessor(LogitsWarper):\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        self.penalty_range = int(self.penalty_range)\n        clipped_penalty_range = min(input_ids.shape[-1], self.penalty_range)\n\n        if self.penalty != 1.0:\n            if self.penalty_range > 0:\n                if clipped_penalty_range < input_ids.shape[1]:\n                    input_ids = input_ids[..., -clipped_penalty_range:]\n\n                if self.penalty_slope != 0:\n                    _penalty = (torch.arange(self.penalty_range, dtype=scores.dtype, device=scores.device)/(self.penalty_range - 1)) * 2. - 1\n                    _penalty = (self.penalty_slope * _penalty) / (1 + torch.abs(_penalty) * (self.penalty_slope - 1))\n                    _penalty = 1 + ((_penalty + 1) / 2).unsqueeze(0) * (self.penalty - 1)\n                    self.penalty = _penalty[..., -clipped_penalty_range:]\n\n            score = torch.gather(scores, 1, input_ids)\n            score = torch.where(score <= 0, score * self.penalty, score / self.penalty)\n            scores.scatter_(1, input_ids, score)\n\n        return scores\n\n\nclass TailFreeLogitsWarper(LogitsWarper):\n\n    def __init__(self, tfs: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n        tfs = float(tfs)\n        if tfs < 0 or tfs > 1.0:\n            raise ValueError(f\"`tfs` has to be a float >= 0 and <= 1, but is {tfs}\")\n        self.tfs = tfs\n        self.filter_value = filter_value\n        self.min_tokens_to_keep = min_tokens_to_keep\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if self.filter_value >= 1.0:\n            return scores\n        sorted_logits, sorted_indices = torch.sort(scores, descending=True)\n        probs = sorted_logits.softmax(dim=-1)\n\n        # Compute second derivative normalized CDF\n        d2 = probs.diff().diff().abs()\n        normalized_d2 = d2 / d2.sum(dim=-1, keepdim=True)\n        normalized_d2_cdf = normalized_d2.cumsum(dim=-1)\n\n        # Remove tokens with CDF value above the threshold (token with 0 are kept)\n        sorted_indices_to_remove = normalized_d2_cdf > self.tfs\n\n        # Centre the distribution around the cutoff as in the original implementation of the algorithm\n        sorted_indices_to_remove = torch.cat(\n            (\n                torch.zeros(scores.shape[0], 1, dtype=torch.bool, device=scores.device),\n                sorted_indices_to_remove,\n                torch.ones(scores.shape[0], 1, dtype=torch.bool, device=scores.device),\n            ),\n            dim=-1,\n        )\n\n        if self.min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep\n            sorted_indices_to_remove[..., : self.min_tokens_to_keep] = 0\n\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores\n\n\nclass TypicalLogitsWarper(LogitsWarper):\n    '''\n    Typical sampling, described in https://arxiv.org/pdf/2202.00666.pdf\n    '''\n\n    def __init__(self, typical: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n        typical = float(typical)\n        if typical < 0 or typical > 1.0:\n            raise ValueError(f\"`typical` has to be a float >= 0 and <= 1, but is {typical}\")\n        self.typical = typical\n        self.filter_value = filter_value\n        self.min_tokens_to_keep = min_tokens_to_keep\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if self.filter_value >= 1.0:\n            return scores\n\n        # Compute softmax probabilities and the natural logarithms of them\n        probs = scores.softmax(dim=-1)\n        log_probs = probs.log()\n\n        # Compute the negative of entropy, which is the sum of p*ln(p) for all p\n        # in the set of softmax probabilities of the logits\n        neg_entropy = (probs * log_probs).nansum(dim=-1, keepdim=True)\n\n        # Determine absolute difference between the negative entropy and the\n        # log probabilities\n        entropy_deviation = (neg_entropy - log_probs).abs()\n\n        # Keep certain tokens such that the sum of the entropy_deviation of the\n        # kept tokens is the smallest possible value such that the sum of the\n        # softmax probabilities of the kept tokens is at least the threshold\n        # value (by sorting the tokens in ascending order of entropy_deviation\n        # and then keeping the smallest possible number of tokens from the\n        # beginning such that sum of softmax probabilities is at or above the\n        # threshold)\n        _, sorted_indices = torch.sort(entropy_deviation)\n        sorted_logits = probs.gather(-1, sorted_indices)\n        sorted_indices_to_remove = sorted_logits.cumsum(dim=-1) >= self.typical\n        sorted_indices_to_remove = sorted_indices_to_remove.roll(1, dims=-1)\n\n        min_tokens_to_keep = max(self.min_tokens_to_keep, 1)\n        # Keep at least min_tokens_to_keep\n        sorted_indices_to_remove[..., : min_tokens_to_keep] = 0\n\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores\n\n\nclass TopALogitsWarper(LogitsWarper):\n    def __init__(self, top_a: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n        top_a = float(top_a)\n        if top_a < 0 or top_a > 1.0:\n            raise ValueError(f\"`top_a` has to be a float >= 0 and <= 1, but is {top_a}\")\n        self.top_a = top_a\n        self.filter_value = filter_value\n        self.min_tokens_to_keep = min_tokens_to_keep\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if self.filter_value >= 1.0:\n            return scores\n\n        sorted_logits, sorted_indices = torch.sort(scores, descending=True)\n        probs = sorted_logits.softmax(dim=-1)\n\n        # Remove tokens with probability less than top_a*(max(probs))^2 (token with 0 are kept)\n        probs_max = probs[..., 0, None]\n        sorted_indices_to_remove = probs < probs_max * probs_max * self.top_a\n\n        if self.min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep\n            sorted_indices_to_remove[..., : self.min_tokens_to_keep] = 0\n\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores\n"
        }
      ]
    }
  ]
}