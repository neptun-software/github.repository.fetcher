{
  "metadata": {
    "timestamp": 1736559818607,
    "page": 554,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/dlrm",
      "stars": 3808,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.9873046875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.23828125,
          "content": "# Code of Conduct\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to.\nPlease read the [full text](https://code.fb.com/codeofconduct/)\nso that you can understand what actions will and will not be tolerated.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.373046875,
          "content": "# Contributing to DLRM\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style\n* 4 spaces for indentation rather than tabs\n* 80 character line length\n* in general, please maintain a consistent style with the rest of the code\n\n## License\nBy contributing to DLRM, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.3828125,
          "content": "# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nARG FROM_IMAGE_NAME=pytorch/pytorch:1.3-cuda10.1-cudnn7-runtime\nFROM ${FROM_IMAGE_NAME}\n\nADD requirements.txt .\nRUN pip install -r requirements.txt\n\nRUN pip install torch==1.3.1\n\nWORKDIR /code\nADD . .\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.060546875,
          "content": "MIT License\n\nCopyright (c) Facebook, Inc. and its affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.16015625,
          "content": "Deep Learning Recommendation Model for Personalization and Recommendation Systems:\n=================================================================================\n*Copyright (c) Facebook, Inc. and its affiliates.*\n\nDescription:\n------------\nAn implementation of a deep learning recommendation model (DLRM).\nThe model input consists of dense and sparse features. The former is a vector\nof floating point values. The latter is a list of sparse indices into\nembedding tables, which consist of vectors of floating point values.\nThe selected vectors are passed to mlp networks denoted by triangles,\nin some cases the vectors are interacted through operators (Ops).\n```\noutput:\n                    probability of a click\nmodel:                        |\n                             /\\\n                            /__\\\n                              |\n      _____________________> Op  <___________________\n    /                         |                      \\\n   /\\                        /\\                      /\\\n  /__\\                      /__\\           ...      /__\\\n   |                          |                       |\n   |                         Op                      Op\n   |                    ____/__\\_____           ____/__\\____\n   |                   |_Emb_|____|__|    ...  |_Emb_|__|___|\ninput:\n[ dense features ]     [sparse indices] , ..., [sparse indices]\n```\n More precise definition of model layers:\n 1) fully connected layers of an mlp\n\n    z = f(y)\n\n    y = Wx + b\n\n 2) embedding lookup (for a list of sparse indices p=[p1,...,pk])\n\n    z = Op(e1,...,ek)\n\n    obtain vectors e1=E[:,p1], ..., ek=E[:,pk]\n\n 3) Operator Op can be one of the following\n\n    Sum(e1,...,ek) = e1 + ... + ek\n\n    Dot(e1,...,ek) = [e1'e1, ..., e1'ek, ..., ek'e1, ..., ek'ek]\n\n    Cat(e1,...,ek) = [e1', ..., ek']'\n\n    where ' denotes transpose operation\n\nSee our blog post to learn more about DLRM: [https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/](https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/).\n\nCite [Work](https://arxiv.org/abs/1906.00091):\n```\n@article{DLRM19,\n  author    = {Maxim Naumov and Dheevatsa Mudigere and Hao{-}Jun Michael Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole{-}Jean Wu and Alisson G. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and Ilia Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and Misha Smelyanskiy},\n  title     = {Deep Learning Recommendation Model for Personalization and Recommendation Systems},\n  journal   = {CoRR},\n  volume    = {abs/1906.00091},\n  year      = {2019},\n  url       = {https://arxiv.org/abs/1906.00091},\n}\n```\n\nRelated Work:\n\nOn the [system architecture implications](https://arxiv.org/abs/1906.03109), with DLRM as one of the benchmarks,\n```\n@article{ArchImpl19,\n  author    = {Udit Gupta and Xiaodong Wang and Maxim Naumov and Carole{-}Jean Wu and Brandon Reagen and David Brooks and Bradford Cottel and Kim M. Hazelwood and Bill Jia and Hsien{-}Hsin S. Lee and Andrey Malevich and Dheevatsa Mudigere and Mikhail Smelyanskiy and Liang Xiong and Xuan Zhang},\n  title     = {The Architectural Implications of Facebook's DNN-based Personalized Recommendation},\n  journal   = {CoRR},\n  volume    = {abs/1906.03109},\n  year      = {2019},\n  url       = {https://arxiv.org/abs/1906.03109},\n}\n```\n\nOn the [embedding compression techniques (for number of vectors)](https://arxiv.org/abs/1909.02107), with DLRM as one of the benchmarks,\n```\n@article{QuoRemTrick19,\n  author    = {Hao{-}Jun Michael Shi and Dheevatsa Mudigere and Maxim Naumov and Jiyan Yang},\n  title     = {Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems},\n  journal   = {CoRR},\n  volume    = {abs/1909.02107},\n  year      = {2019},\n  url       = {https://arxiv.org/abs/1909.02107},\n}\n```\n\nOn the [embedding compression techniques (for dimension of vectors)](https://arxiv.org/abs/1909.11810), with DLRM as one of the benchmarks,\n```\n@article{MixDimTrick19,\n  author    = {Antonio Ginart and Maxim Naumov and Dheevatsa Mudigere and Jiyan Yang and James Zou},\n  title     = {Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems},\n  journal   = {CoRR},\n  volume    = {abs/1909.11810},\n  year      = {2019},\n  url       = {https://arxiv.org/abs/1909.11810},\n}\n```\n\nImplementation\n--------------\n**DLRM PyTorch**. Implementation of DLRM in PyTorch framework:\n\n       dlrm_s_pytorch.py\n\n**DLRM Caffe2**. Implementation of DLRM in Caffe2 framework:\n\n       dlrm_s_caffe2.py\n\n**DLRM Data**. Implementation of DLRM data generation and loading:\n\n       dlrm_data_pytorch.py, dlrm_data_caffe2.py, data_utils.py\n\n**DLRM Tests**. Implementation of DLRM tests in ./test\n\n       dlrm_s_test.sh\n\n**DLRM Benchmarks**. Implementation of DLRM benchmarks in ./bench\n\n       dlrm_s_criteo_kaggle.sh, dlrm_s_criteo_terabyte.sh, dlrm_s_benchmark.sh\n\nRelated Work:\n\nOn the [Glow framework](https://github.com/pytorch/glow) implementation\n```\nhttps://github.com/pytorch/glow/blob/master/tests/unittests/RecommendationSystemTest.cpp\n```\nOn the [FlexFlow framework](https://github.com/flexflow/FlexFlow) distributed implementation with Legion backend\n```\nhttps://github.com/flexflow/FlexFlow/blob/master/examples/cpp/DLRM/dlrm.cc\n```\n\nHow to run dlrm code?\n--------------------\n1) A sample run of the code, with a tiny model is shown below\n```\n$ python dlrm_s_pytorch.py --mini-batch-size=2 --data-size=6\ntime/loss/accuracy (if enabled):\nFinished training it 1/3 of epoch 0, -1.00 ms/it, loss 0.451893, accuracy 0.000%\nFinished training it 2/3 of epoch 0, -1.00 ms/it, loss 0.402002, accuracy 0.000%\nFinished training it 3/3 of epoch 0, -1.00 ms/it, loss 0.275460, accuracy 0.000%\n```\n2) A sample run of the code, with a tiny model in debug mode\n```\n$ python dlrm_s_pytorch.py --mini-batch-size=2 --data-size=6 --debug-mode\nmodel arch:\nmlp top arch 3 layers, with input to output dimensions:\n[8 4 2 1]\n# of interactions\n8\nmlp bot arch 2 layers, with input to output dimensions:\n[4 3 2]\n# of features (sparse and dense)\n4\ndense feature size\n4\nsparse feature size\n2\n# of embeddings (= # of sparse features) 3, with dimensions 2x:\n[4 3 2]\ndata (inputs and targets):\nmini-batch: 0\n[[0.69647 0.28614 0.22685 0.55131]\n [0.71947 0.42311 0.98076 0.68483]]\n[[[1], [0, 1]], [[0], [1]], [[1], [0]]]\n[[0.55679]\n [0.15896]]\nmini-batch: 1\n[[0.36179 0.22826 0.29371 0.63098]\n [0.0921  0.4337  0.43086 0.49369]]\n[[[1], [0, 2, 3]], [[1], [1, 2]], [[1], [1]]]\n[[0.15307]\n [0.69553]]\nmini-batch: 2\n[[0.60306 0.54507 0.34276 0.30412]\n [0.41702 0.6813  0.87546 0.51042]]\n[[[2], [0, 1, 2]], [[1], [2]], [[1], [1]]]\n[[0.31877]\n [0.69197]]\ninitial parameters (weights and bias):\n[[ 0.05438 -0.11105]\n [ 0.42513  0.34167]\n [-0.1426  -0.45641]\n [-0.19523 -0.10181]]\n[[ 0.23667  0.57199]\n [-0.16638  0.30316]\n [ 0.10759  0.22136]]\n[[-0.49338 -0.14301]\n [-0.36649 -0.22139]]\n[[0.51313 0.66662 0.10591 0.13089]\n [0.32198 0.66156 0.84651 0.55326]\n [0.85445 0.38484 0.31679 0.35426]]\n[0.17108 0.82911 0.33867]\n[[0.55237 0.57855 0.52153]\n [0.00269 0.98835 0.90534]]\n[0.20764 0.29249]\n[[0.52001 0.90191 0.98363 0.25754 0.56436 0.80697 0.39437 0.73107]\n [0.16107 0.6007  0.86586 0.98352 0.07937 0.42835 0.20454 0.45064]\n [0.54776 0.09333 0.29686 0.92758 0.569   0.45741 0.75353 0.74186]\n [0.04858 0.7087  0.83924 0.16594 0.781   0.28654 0.30647 0.66526]]\n[0.11139 0.66487 0.88786 0.69631]\n[[0.44033 0.43821 0.7651  0.56564]\n [0.0849  0.58267 0.81484 0.33707]]\n[0.92758 0.75072]\n[[0.57406 0.75164]]\n[0.07915]\nDLRM_Net(\n  (emb_l): ModuleList(\n    (0): EmbeddingBag(4, 2, mode=sum)\n    (1): EmbeddingBag(3, 2, mode=sum)\n    (2): EmbeddingBag(2, 2, mode=sum)\n  )\n  (bot_l): Sequential(\n    (0): Linear(in_features=4, out_features=3, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=3, out_features=2, bias=True)\n    (3): ReLU()\n  )\n  (top_l): Sequential(\n    (0): Linear(in_features=8, out_features=4, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=4, out_features=2, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=2, out_features=1, bias=True)\n    (5): Sigmoid()\n  )\n)\ntime/loss/accuracy (if enabled):\nFinished training it 1/3 of epoch 0, -1.00 ms/it, loss 0.451893, accuracy 0.000%\nFinished training it 2/3 of epoch 0, -1.00 ms/it, loss 0.402002, accuracy 0.000%\nFinished training it 3/3 of epoch 0, -1.00 ms/it, loss 0.275460, accuracy 0.000%\nupdated parameters (weights and bias):\n[[ 0.0543  -0.1112 ]\n [ 0.42513  0.34167]\n [-0.14283 -0.45679]\n [-0.19532 -0.10197]]\n[[ 0.23667  0.57199]\n [-0.1666   0.30285]\n [ 0.10751  0.22124]]\n[[-0.49338 -0.14301]\n [-0.36664 -0.22164]]\n[[0.51313 0.66663 0.10591 0.1309 ]\n [0.32196 0.66154 0.84649 0.55324]\n [0.85444 0.38482 0.31677 0.35425]]\n[0.17109 0.82907 0.33863]\n[[0.55238 0.57857 0.52154]\n [0.00265 0.98825 0.90528]]\n[0.20764 0.29244]\n[[0.51996 0.90184 0.98368 0.25752 0.56436 0.807   0.39437 0.73107]\n [0.16096 0.60055 0.86596 0.98348 0.07938 0.42842 0.20453 0.45064]\n [0.5476  0.0931  0.29701 0.92752 0.56902 0.45752 0.75351 0.74187]\n [0.04849 0.70857 0.83933 0.1659  0.78101 0.2866  0.30646 0.66526]]\n[0.11137 0.66482 0.88778 0.69627]\n[[0.44029 0.43816 0.76502 0.56561]\n [0.08485 0.5826  0.81474 0.33702]]\n[0.92754 0.75067]\n[[0.57379 0.7514 ]]\n[0.07908]\n```\n\nTesting\n-------\nTesting scripts to confirm functional correctness of the code\n```\n./test/dlrm_s_test.sh\nRunning commands ...\npython dlrm_s_pytorch.py\npython dlrm_s_caffe2.py\nChecking results ...\ndiff test1 (no numeric values in the output = SUCCESS)\ndiff test2 (no numeric values in the output = SUCCESS)\ndiff test3 (no numeric values in the output = SUCCESS)\ndiff test4 (no numeric values in the output = SUCCESS)\n```\n\n*NOTE: Testing scripts accept extra arguments which will be passed along to the model, such as --use-gpu*\n\nBenchmarking\n------------\n1) Performance benchmarking\n    ```\n    ./bench/dlrm_s_benchmark.sh\n    ```\n\n2) The code supports interface with the [Criteo Kaggle Display Advertising Challenge Dataset](https://ailab.criteo.com/ressources/).\n   - Please do the following to prepare the dataset for use with DLRM code:\n     - First, specify the raw data file (train.txt) as downloaded with --raw-data-file=<path/train.txt>\n     - This is then pre-processed (categorize, concat across days...) to allow using with dlrm code\n     - The processed data is stored as *.npz file in <root_dir>/input/*.npz\n     - The processed file (*.npz) can be used for subsequent runs with --processed-data-file=<path/*.npz>\n   - The model can be trained using the following script\n     ```\n     ./bench/dlrm_s_criteo_kaggle.sh [--test-freq=1024]\n     ```\n\n<img src=\"./kaggle_dac_loss_accuracy_plots.png\" width=\"900\" height=\"320\">\n\n3) The code supports interface with the [Criteo Terabyte Dataset](https://labs.criteo.com/2013/12/download-terabyte-click-logs/).\n   - Please do the following to prepare the dataset for use with DLRM code:\n     - First, download the raw data files day_0.gz, ...,day_23.gz and unzip them\n     - Specify the location of the unzipped text files day_0, ...,day_23, using --raw-data-file=<path/day> (the day number will be appended automatically)\n     - These are then pre-processed (categorize, concat across days...) to allow using with dlrm code\n     - The processed data is stored as *.npz file in <root_dir>/input/*.npz\n     - The processed file (*.npz) can be used for subsequent runs with --processed-data-file=<path/*.npz>\n   - The model can be trained using the following script\n    ```\n      ./bench/dlrm_s_criteo_terabyte.sh [\"--test-freq=10240 --memory-map --data-sub-sample-rate=0.875\"]\n    ```\n    - Corresponding pre-trained model is available under [CC-BY-NC license](https://creativecommons.org/licenses/by-nc/2.0/) and can be downloaded here\n    [dlrm_emb64_subsample0.875_maxindrange10M_pretrained.pt](https://dlrm.s3-us-west-1.amazonaws.com/models/tb0875_10M.pt)\n\n<img src=\"./terabyte_0875_loss_accuracy_plots.png\" width=\"900\" height=\"320\">\n\n*NOTE: Benchmarking scripts accept extra arguments which will be passed along to the model, such as --num-batches=100 to limit the number of data samples*\n\n4) The code supports interface with [MLPerf benchmark](https://mlperf.org).\n   - Please refer to the following training parameters\n   ```\n     --mlperf-logging that keeps track of multiple metrics, including area under the curve (AUC)\n\n     --mlperf-acc-threshold that allows early stopping based on accuracy metric\n\n     --mlperf-auc-threshold that allows early stopping based on AUC metric\n\n     --mlperf-bin-loader that enables preprocessing of data into a single binary file\n\n     --mlperf-bin-shuffle that controls whether a random shuffle of mini-batches is performed\n   ```\n   - The MLPerf training model is completely specified and can be trained using the following script\n   ```\n     ./bench/run_and_time.sh [--use-gpu]\n   ```\n   - Corresponding pre-trained model is available under [CC-BY-NC license](https://creativecommons.org/licenses/by-nc/2.0/) and can be downloaded here\n     [dlrm_emb128_subsample0.0_maxindrange40M_pretrained.pt](https://dlrm.s3-us-west-1.amazonaws.com/models/tb00_40M.pt)\n\n5) The code now supports synchronous distributed training, we support gloo/nccl/mpi backend, we provide launching mode for [pytorch distributed launcher](https://pytorch.org/docs/stable/distributed.html#launch-utility) and Mpirun. For MPI, users need to write their own MPI launching scripts for configuring the running hosts. For example, using pytorch distributed launcher, we can have the following command as launching scripts:\n```\n# for single node 8 gpus and nccl as backend on randomly generated dataset:\npython -m torch.distributed.launch --nproc_per_node=8 dlrm_s_pytorch.py --arch-embedding-size=\"80000-80000-80000-80000-80000-80000-80000-80000\" --arch-sparse-feature-size=64 --arch-mlp-bot=\"128-128-128-128\" --arch-mlp-top=\"512-512-512-256-1\" --max-ind-range=40000000\n--data-generation=random --loss-function=bce --round-targets=True --learning-rate=1.0 --mini-batch-size=2048 --print-freq=2 --print-time --test-freq=2 --test-mini-batch-size=2048 --memory-map --use-gpu --num-batches=100 --dist-backend=nccl\n\n# for multiple nodes, user can add the related argument according to the launcher manual like:\n--nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234\n```\n\n\nModel checkpoint saving/loading\n-------------------------------\nDuring training, the model can be saved using --save-model=<path/model.pt>\n\nThe model is saved if there is an improvement in test accuracy (which is checked at --test-freq intervals).\n\nA previously saved model can be loaded using --load-model=<path/model.pt>\n\nOnce loaded the model can be used to continue training, with the saved model being a checkpoint.\nAlternatively, the saved model can be used to evaluate only on the test data-set by specifying --inference-only option.\n\n\nVersion\n-------\n0.1 : Initial release of the DLRM code\n\n1.0 : DLRM with distributed training, cpu support for row-wise adagrad optimizer\n\nRequirements\n------------\npytorch-nightly (*11/10/20*)\n\nscikit-learn\n\nnumpy\n\nonnx (*optional*)\n\npydot (*optional*)\n\ntorchviz (*optional*)\n\nmpi (*optional for distributed backend*)\n\n\nLicense\n-------\nThis source code is licensed under the MIT license found in the\nLICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "bench",
          "type": "tree",
          "content": null
        },
        {
          "name": "cython",
          "type": "tree",
          "content": null
        },
        {
          "name": "data_loader_terabyte.py",
          "type": "blob",
          "size": 11.794921875,
          "content": "# @lint-ignore-every LICENSELINT\n\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\nimport math\n\nimport os\nimport time\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\n\nclass DataLoader:\n    \"\"\"\n    DataLoader dedicated for the Criteo Terabyte Click Logs dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        data_filename,\n        data_directory,\n        days,\n        batch_size,\n        max_ind_range=-1,\n        split=\"train\",\n        drop_last_batch=False,\n    ):\n        self.data_filename = data_filename\n        self.data_directory = data_directory\n        self.days = days\n        self.batch_size = batch_size\n        self.max_ind_range = max_ind_range\n\n        total_file = os.path.join(data_directory, data_filename + \"_day_count.npz\")\n        with np.load(total_file) as data:\n            total_per_file = data[\"total_per_file\"][np.array(days)]\n\n        self.length = sum(total_per_file)\n        if split == \"test\" or split == \"val\":\n            self.length = int(np.ceil(self.length / 2.0))\n        self.split = split\n        self.drop_last_batch = drop_last_batch\n\n    def __iter__(self):\n        return iter(\n            _batch_generator(\n                self.data_filename,\n                self.data_directory,\n                self.days,\n                self.batch_size,\n                self.split,\n                self.drop_last_batch,\n                self.max_ind_range,\n            )\n        )\n\n    def __len__(self):\n        if self.drop_last_batch:\n            return self.length // self.batch_size\n        else:\n            return math.ceil(self.length / self.batch_size)\n\n\ndef _transform_features(\n    x_int_batch, x_cat_batch, y_batch, max_ind_range, flag_input_torch_tensor=False\n):\n    if max_ind_range > 0:\n        x_cat_batch = x_cat_batch % max_ind_range\n\n    if flag_input_torch_tensor:\n        x_int_batch = torch.log(x_int_batch.clone().detach().type(torch.float) + 1)\n        x_cat_batch = x_cat_batch.clone().detach().type(torch.long)\n        y_batch = y_batch.clone().detach().type(torch.float32).view(-1, 1)\n    else:\n        x_int_batch = torch.log(torch.tensor(x_int_batch, dtype=torch.float) + 1)\n        x_cat_batch = torch.tensor(x_cat_batch, dtype=torch.long)\n        y_batch = torch.tensor(y_batch, dtype=torch.float32).view(-1, 1)\n\n    batch_size = x_cat_batch.shape[0]\n    feature_count = x_cat_batch.shape[1]\n    lS_o = torch.arange(batch_size).reshape(1, -1).repeat(feature_count, 1)\n\n    return x_int_batch, lS_o, x_cat_batch.t(), y_batch.view(-1, 1)\n\n\ndef _batch_generator(\n    data_filename, data_directory, days, batch_size, split, drop_last, max_ind_range\n):\n    previous_file = None\n    for day in days:\n        filepath = os.path.join(\n            data_directory, data_filename + \"_{}_reordered.npz\".format(day)\n        )\n\n        # print('Loading file: ', filepath)\n        with np.load(filepath) as data:\n            x_int = data[\"X_int\"]\n            x_cat = data[\"X_cat\"]\n            y = data[\"y\"]\n\n        samples_in_file = y.shape[0]\n        batch_start_idx = 0\n        if split == \"test\" or split == \"val\":\n            length = int(np.ceil(samples_in_file / 2.0))\n            if split == \"test\":\n                samples_in_file = length\n            elif split == \"val\":\n                batch_start_idx = samples_in_file - length\n\n        while batch_start_idx < samples_in_file - batch_size:\n            missing_samples = batch_size\n            if previous_file is not None:\n                missing_samples -= previous_file[\"y\"].shape[0]\n\n            current_slice = slice(batch_start_idx, batch_start_idx + missing_samples)\n\n            x_int_batch = x_int[current_slice]\n            x_cat_batch = x_cat[current_slice]\n            y_batch = y[current_slice]\n\n            if previous_file is not None:\n                x_int_batch = np.concatenate(\n                    [previous_file[\"x_int\"], x_int_batch], axis=0\n                )\n                x_cat_batch = np.concatenate(\n                    [previous_file[\"x_cat\"], x_cat_batch], axis=0\n                )\n                y_batch = np.concatenate([previous_file[\"y\"], y_batch], axis=0)\n                previous_file = None\n\n            if x_int_batch.shape[0] != batch_size:\n                raise ValueError(\"should not happen\")\n\n            yield _transform_features(x_int_batch, x_cat_batch, y_batch, max_ind_range)\n\n            batch_start_idx += missing_samples\n        if batch_start_idx != samples_in_file:\n            current_slice = slice(batch_start_idx, samples_in_file)\n            if previous_file is not None:\n                previous_file = {\n                    \"x_int\": np.concatenate(\n                        [previous_file[\"x_int\"], x_int[current_slice]], axis=0\n                    ),\n                    \"x_cat\": np.concatenate(\n                        [previous_file[\"x_cat\"], x_cat[current_slice]], axis=0\n                    ),\n                    \"y\": np.concatenate([previous_file[\"y\"], y[current_slice]], axis=0),\n                }\n            else:\n                previous_file = {\n                    \"x_int\": x_int[current_slice],\n                    \"x_cat\": x_cat[current_slice],\n                    \"y\": y[current_slice],\n                }\n\n    if not drop_last:\n        yield _transform_features(\n            previous_file[\"x_int\"],\n            previous_file[\"x_cat\"],\n            previous_file[\"y\"],\n            max_ind_range,\n        )\n\n\ndef _test():\n    generator = _batch_generator(\n        data_filename=\"day\",\n        data_directory=\"./input\",\n        days=range(23),\n        split=\"train\",\n        batch_size=2048,\n        drop_last=True,\n        max_ind_range=-1,\n    )\n    t1 = time.time()\n    for x_int, lS_o, x_cat, y in generator:\n        t2 = time.time()\n        time_diff = t2 - t1\n        t1 = t2\n        print(\n            \"time {} x_int.shape: {} lS_o.shape: {} x_cat.shape: {} y.shape: {}\".format(\n                time_diff, x_int.shape, lS_o.shape, x_cat.shape, y.shape\n            )\n        )\n\n\nclass CriteoBinDataset(Dataset):\n    \"\"\"Binary version of criteo dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_file,\n        counts_file,\n        batch_size=1,\n        max_ind_range=-1,\n        bytes_per_feature=4,\n    ):\n        # dataset\n        self.tar_fea = 1  # single target\n        self.den_fea = 13  # 13 dense  features\n        self.spa_fea = 26  # 26 sparse features\n        self.tad_fea = self.tar_fea + self.den_fea\n        self.tot_fea = self.tad_fea + self.spa_fea\n\n        self.batch_size = batch_size\n        self.max_ind_range = max_ind_range\n        self.bytes_per_entry = bytes_per_feature * self.tot_fea * batch_size\n\n        self.num_entries = math.ceil(os.path.getsize(data_file) / self.bytes_per_entry)\n\n        print(\"data file:\", data_file, \"number of batches:\", self.num_entries)\n        self.file = open(data_file, \"rb\")\n\n        with np.load(counts_file) as data:\n            self.counts = data[\"counts\"]\n\n        # hardcoded for now\n        self.m_den = 13\n\n    def __len__(self):\n        return self.num_entries\n\n    def __getitem__(self, idx):\n        self.file.seek(idx * self.bytes_per_entry, 0)\n        raw_data = self.file.read(self.bytes_per_entry)\n        array = np.frombuffer(raw_data, dtype=np.int32)\n        tensor = torch.from_numpy(array).view((-1, self.tot_fea))\n\n        return _transform_features(\n            x_int_batch=tensor[:, 1:14],\n            x_cat_batch=tensor[:, 14:],\n            y_batch=tensor[:, 0],\n            max_ind_range=self.max_ind_range,\n            flag_input_torch_tensor=True,\n        )\n\n    def __del__(self):\n        self.file.close()\n\n\ndef numpy_to_binary(input_files, output_file_path, split=\"train\"):\n    \"\"\"Convert the data to a binary format to be read with CriteoBinDataset.\"\"\"\n\n    # WARNING - both categorical and numerical data must fit into int32 for\n    # the following code to work correctly\n\n    with open(output_file_path, \"wb\") as output_file:\n        if split == \"train\":\n            for input_file in input_files:\n                print(\"Processing file: \", input_file)\n\n                np_data = np.load(input_file)\n                np_data = np.concatenate(\n                    [np_data[\"y\"].reshape(-1, 1), np_data[\"X_int\"], np_data[\"X_cat\"]],\n                    axis=1,\n                )\n                np_data = np_data.astype(np.int32)\n\n                output_file.write(np_data.tobytes())\n        else:\n            assert len(input_files) == 1\n            np_data = np.load(input_files[0])\n            np_data = np.concatenate(\n                [np_data[\"y\"].reshape(-1, 1), np_data[\"X_int\"], np_data[\"X_cat\"]],\n                axis=1,\n            )\n            np_data = np_data.astype(np.int32)\n\n            samples_in_file = np_data.shape[0]\n            midpoint = int(np.ceil(samples_in_file / 2.0))\n            if split == \"test\":\n                begin = 0\n                end = midpoint\n            elif split == \"val\":\n                begin = midpoint\n                end = samples_in_file\n            else:\n                raise ValueError(\"Unknown split value: \", split)\n\n            output_file.write(np_data[begin:end].tobytes())\n\n\ndef _preprocess(args):\n    train_files = [\n        \"{}_{}_reordered.npz\".format(args.input_data_prefix, day)\n        for day in range(0, 23)\n    ]\n\n    test_valid_file = args.input_data_prefix + \"_23_reordered.npz\"\n\n    os.makedirs(args.output_directory, exist_ok=True)\n    for split in [\"train\", \"val\", \"test\"]:\n        print(\"Running preprocessing for split =\", split)\n\n        output_file = os.path.join(args.output_directory, \"{}_data.bin\".format(split))\n\n        input_files = train_files if split == \"train\" else [test_valid_file]\n        numpy_to_binary(\n            input_files=input_files, output_file_path=output_file, split=split\n        )\n\n\ndef _test_bin():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--output_directory\", required=True)\n    parser.add_argument(\"--input_data_prefix\", required=True)\n    parser.add_argument(\"--split\", choices=[\"train\", \"test\", \"val\"], required=True)\n    args = parser.parse_args()\n\n    _preprocess(args)\n\n    binary_data_file = os.path.join(\n        args.output_directory, \"{}_data.bin\".format(args.split)\n    )\n\n    counts_file = os.path.join(args.output_directory, \"day_fea_count.npz\")\n    dataset_binary = CriteoBinDataset(\n        data_file=binary_data_file,\n        counts_file=counts_file,\n        batch_size=2048,\n    )\n    from dlrm_data_pytorch import (\n        collate_wrapper_criteo_offset as collate_wrapper_criteo,\n        CriteoDataset,\n    )\n\n    binary_loader = torch.utils.data.DataLoader(\n        dataset_binary,\n        batch_size=None,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=None,\n        pin_memory=False,\n        drop_last=False,\n    )\n\n    original_dataset = CriteoDataset(\n        dataset=\"terabyte\",\n        max_ind_range=10 * 1000 * 1000,\n        sub_sample_rate=1,\n        randomize=True,\n        split=args.split,\n        raw_path=args.input_data_prefix,\n        pro_data=\"dummy_string\",\n        memory_map=True,\n    )\n\n    original_loader = torch.utils.data.DataLoader(\n        original_dataset,\n        batch_size=2048,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=collate_wrapper_criteo,\n        pin_memory=False,\n        drop_last=False,\n    )\n\n    assert len(dataset_binary) == len(original_loader)\n    for i, (old_batch, new_batch) in tqdm(\n        enumerate(zip(original_loader, binary_loader)), total=len(dataset_binary)\n    ):\n        for j in range(len(new_batch)):\n            if not np.array_equal(old_batch[j], new_batch[j]):\n                raise ValueError(\"FAILED: Datasets not equal\")\n        if i > len(dataset_binary):\n            break\n    print(\"PASSED\")\n\n\nif __name__ == \"__main__\":\n    _test()\n    _test_bin()\n"
        },
        {
          "name": "data_utils.py",
          "type": "blob",
          "size": 51.0068359375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Description: generate inputs and targets for the DLRM benchmark\n#\n# Utility function(s) to download and pre-process public data sets\n#   - Criteo Kaggle Display Advertising Challenge Dataset\n#     https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset\n#   - Criteo Terabyte Dataset\n#     https://labs.criteo.com/2013/12/download-terabyte-click-logs\n#\n# After downloading dataset, run:\n#   getCriteoAdData(\n#       datafile=\"<path-to-train.txt>\",\n#       o_filename=kaggleAdDisplayChallenge_processed.npz,\n#       max_ind_range=-1,\n#       sub_sample_rate=0.0,\n#       days=7,\n#       data_split='train',\n#       randomize='total',\n#       criteo_kaggle=True,\n#       memory_map=False\n#   )\n#   getCriteoAdData(\n#       datafile=\"<path-to-day_{0,...,23}>\",\n#       o_filename=terabyte_processed.npz,\n#       max_ind_range=-1,\n#       sub_sample_rate=0.0,\n#       days=24,\n#       data_split='train',\n#       randomize='total',\n#       criteo_kaggle=False,\n#       memory_map=False\n#   )\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport sys\nfrom multiprocessing import Manager, Process\n\n# import os\nfrom os import path\n\n# import io\n# from io import StringIO\n# import collections as coll\n\nimport numpy as np\n\n\ndef convertUStringToDistinctIntsDict(mat, convertDicts, counts):\n    # Converts matrix of unicode strings into distinct integers.\n    #\n    # Inputs:\n    #     mat (np.array): array of unicode strings to convert\n    #     convertDicts (list): dictionary for each column\n    #     counts (list): number of different categories in each column\n    #\n    # Outputs:\n    #     out (np.array): array of output integers\n    #     convertDicts (list): dictionary for each column\n    #     counts (list): number of different categories in each column\n\n    # check if convertDicts and counts match correct length of mat\n    if len(convertDicts) != mat.shape[1] or len(counts) != mat.shape[1]:\n        print(\"Length of convertDicts or counts does not match input shape\")\n        print(\"Generating convertDicts and counts...\")\n\n        convertDicts = [{} for _ in range(mat.shape[1])]\n        counts = [0 for _ in range(mat.shape[1])]\n\n    # initialize output\n    out = np.zeros(mat.shape)\n\n    for j in range(mat.shape[1]):\n        for i in range(mat.shape[0]):\n            # add to convertDict and increment count\n            if mat[i, j] not in convertDicts[j]:\n                convertDicts[j][mat[i, j]] = counts[j]\n                counts[j] += 1\n            out[i, j] = convertDicts[j][mat[i, j]]\n\n    return out, convertDicts, counts\n\n\ndef convertUStringToDistinctIntsUnique(mat, mat_uni, counts):\n    # mat is an array of 0,...,# samples, with each being 26 categorical features\n\n    # check if mat_unique and counts match correct length of mat\n    if len(mat_uni) != mat.shape[1] or len(counts) != mat.shape[1]:\n        print(\"Length of mat_unique or counts does not match input shape\")\n        print(\"Generating mat_unique and counts...\")\n\n        mat_uni = [np.array([]) for _ in range(mat.shape[1])]\n        counts = [0 for _ in range(mat.shape[1])]\n\n    # initialize output\n    out = np.zeros(mat.shape)\n    ind_map = [np.array([]) for _ in range(mat.shape[1])]\n\n    # find out and assign unique ids to features\n    for j in range(mat.shape[1]):\n        m = mat_uni[j].size\n        mat_concat = np.concatenate((mat_uni[j], mat[:, j]))\n        mat_uni[j], ind_map[j] = np.unique(mat_concat, return_inverse=True)\n        out[:, j] = ind_map[j][m:]\n        counts[j] = mat_uni[j].size\n\n    return out, mat_uni, counts\n\n\ndef processCriteoAdData(d_path, d_file, npzfile, i, convertDicts, pre_comp_counts):\n    # Process Kaggle Display Advertising Challenge or Terabyte Dataset\n    # by converting unicode strings in X_cat to integers and\n    # converting negative integer values in X_int.\n    #\n    # Loads data in the form \"{kaggle|terabyte}_day_i.npz\" where i is the day.\n    #\n    # Inputs:\n    #   d_path (str): path for {kaggle|terabyte}_day_i.npz files\n    #   i (int): splits in the dataset (typically 0 to 7 or 0 to 24)\n\n    # process data if not all files exist\n    filename_i = npzfile + \"_{0}_processed.npz\".format(i)\n\n    if path.exists(filename_i):\n        print(\"Using existing \" + filename_i, end=\"\\n\")\n    else:\n        print(\"Not existing \" + filename_i)\n        with np.load(npzfile + \"_{0}.npz\".format(i)) as data:\n            # categorical features\n            \"\"\"\n            # Approach 1a: using empty dictionaries\n            X_cat, convertDicts, counts = convertUStringToDistinctIntsDict(\n                data[\"X_cat\"], convertDicts, counts\n            )\n            \"\"\"\n            \"\"\"\n            # Approach 1b: using empty np.unique\n            X_cat, convertDicts, counts = convertUStringToDistinctIntsUnique(\n                data[\"X_cat\"], convertDicts, counts\n            )\n            \"\"\"\n            # Approach 2a: using pre-computed dictionaries\n            X_cat_t = np.zeros(data[\"X_cat_t\"].shape)\n            for j in range(26):\n                for k, x in enumerate(data[\"X_cat_t\"][j, :]):\n                    X_cat_t[j, k] = convertDicts[j][x]\n            # continuous features\n            X_int = data[\"X_int\"]\n            X_int[X_int < 0] = 0\n            # targets\n            y = data[\"y\"]\n\n        np.savez_compressed(\n            filename_i,\n            # X_cat = X_cat,\n            X_cat=np.transpose(X_cat_t),  # transpose of the data\n            X_int=X_int,\n            y=y,\n        )\n        print(\"Processed \" + filename_i, end=\"\\n\")\n    # sanity check (applicable only if counts have been pre-computed & are re-computed)\n    # for j in range(26):\n    #    if pre_comp_counts[j] != counts[j]:\n    #        sys.exit(\"ERROR: Sanity check on counts has failed\")\n    # print(\"\\nSanity check on counts passed\")\n\n    return\n\n\ndef concatCriteoAdData(\n    d_path,\n    d_file,\n    npzfile,\n    trafile,\n    days,\n    data_split,\n    randomize,\n    total_per_file,\n    total_count,\n    memory_map,\n    o_filename,\n):\n    # Concatenates different days and saves the result.\n    #\n    # Inputs:\n    #   days (int): total number of days in the dataset (typically 7 or 24)\n    #   d_path (str): path for {kaggle|terabyte}_day_i.npz files\n    #   o_filename (str): output file name\n    #\n    # Output:\n    #   o_file (str): output file path\n\n    if memory_map:\n        # dataset break up per fea\n        # tar_fea = 1   # single target\n        den_fea = 13  # 13 dense  features\n        spa_fea = 26  # 26 sparse features\n        # tad_fea = tar_fea + den_fea\n        # tot_fea = tad_fea + spa_fea\n        # create offset per file\n        offset_per_file = np.array([0] + [x for x in total_per_file])\n        for i in range(days):\n            offset_per_file[i + 1] += offset_per_file[i]\n\n        \"\"\"\n        # Approach 1, 2 and 3 use indices, while Approach 4 does not use them\n        # create indices\n        indices = np.arange(total_count)\n        if data_split == \"none\":\n            if randomize == \"total\":\n                indices = np.random.permutation(indices)\n        else:\n            indices = np.array_split(indices, offset_per_file[1:-1])\n\n            # randomize train data (per day)\n            if randomize == \"day\":  # or randomize == \"total\":\n                for i in range(len(indices) - 1):\n                    indices[i] = np.random.permutation(indices[i])\n                print(\"Randomized indices per day ...\")\n\n            train_indices = np.concatenate(indices[:-1])\n            test_indices = indices[-1]\n\n            # randomize train data (across days)\n            if randomize == \"total\":\n                train_indices = np.random.permutation(train_indices)\n                print(\"Randomized indices across days ...\")\n\n            indices = np.concatenate((train_indices, test_indices))\n        # no reordering\n        # indices = np.arange(total_count)\n        \"\"\"\n        \"\"\"\n        # Approach 1: simple and slow (no grouping is used)\n        # check if data already exists\n        recreate_flag = False\n        for j in range(tot_fea):\n            filename_j = trafile + \"_{0}_reordered.npy\".format(j)\n            if path.exists(filename_j):\n                print(\"Using existing \" + filename_j)\n            else:\n                recreate_flag = True\n        # load, reorder and concatenate data (memmap all reordered files per feature)\n        if recreate_flag:\n            # init reordered files (.npy appended automatically)\n            z = np.zeros((total_count))\n            for j in range(tot_fea):\n                filename_j = trafile + \"_{0}_reordered\".format(j)\n                np.save(filename_j, z)\n                print(\"Creating \" + filename_j)\n\n            for i in range(days):\n                filename_i = d_path + npzfile + \"_{0}_processed.npz\".format(i)\n                with np.load(filename_i) as data:\n                    X_cat_t = np.transpose(data[\"X_cat\"])\n                    X_int_t = np.transpose(data[\"X_int\"])\n                    y = data[\"y\"]\n                size = len(y)\n                # sanity check\n                if total_per_file[i] != size:\n                    sys.exit(\"ERROR: sanity check on number of samples failed\")\n                # setup start and end ranges\n                start = offset_per_file[i]\n                end = offset_per_file[i + 1]\n                # print(filename_i)\n                # print(\"start=\" + str(start) + \" end=\" + str(end)\n                #     + \" diff=\" + str(end - start) + \"=\" + str(total_per_file[i]))\n\n                for j in range(tot_fea):\n                    filename_j = trafile + \"_{0}_reordered.npy\".format(j)\n                    fj = np.load(filename_j, mmap_mode='r+')\n                    if j < tar_fea:\n                        fj[indices[start:end]] = y\n                    elif tar_fea <= j and j < tad_fea:\n                        fj[indices[start:end]] = X_int_t[j - tar_fea, :]\n                    else:\n                        fj[indices[start:end]] = X_cat_t[j - tad_fea, :]\n                    del fj\n        else:\n            print(\"Reordered fea files already exist, skipping ...\")\n\n        # check if data already exists\n        recreate_flag = False\n        for i in range(days):\n            filename_i = d_path + npzfile + \"_{0}_reordered.npz\".format(i)\n            if path.exists(filename_i):\n                print(\"Using existing \" + filename_i)\n            else:\n                recreate_flag = True\n        # split reordered data by files (memmap all reordered files per feature)\n        # on the day boundary del the file object and memmap again\n        if recreate_flag:\n            for i in range(days):\n                filename_i = d_path + npzfile + \"_{0}_reordered.npz\".format(i)\n                size = total_per_file[i]\n                X_int_t = np.zeros((den_fea, size))\n                X_cat_t = np.zeros((spa_fea, size))\n                # setup start and end ranges\n                start = offset_per_file[i]\n                end = offset_per_file[i + 1]\n                print(\"Creating \" + filename_i)\n                # print(\"start=\" + str(start) + \" end=\" + str(end)\n                #     + \" diff=\" + str(end - start) + \"=\" + str(total_per_file[i]))\n\n                for j in range(tot_fea):\n                    filename_j = trafile + \"_{0}_reordered.npy\".format(j)\n                    fj = np.load(filename_j, mmap_mode='r')\n                    if j < tar_fea:\n                        y = fj[start:end]\n                    elif tar_fea <= j and j < tad_fea:\n                        X_int_t[j - tar_fea, :] = fj[start:end]\n                    else:\n                        X_cat_t[j - tad_fea, :] = fj[start:end]\n                    del fj\n\n                np.savez_compressed(\n                    filename_i,\n                    X_cat=np.transpose(X_cat_t),  # transpose of the data\n                    X_int=np.transpose(X_int_t),  # transpose of the data\n                    y=y,\n                )\n        else:\n            print(\"Reordered day files already exist, skipping ...\")\n        \"\"\"\n        \"\"\"\n        # Approach 2: group days\n        # check if data already exists\n        recreate_flag = False\n        for j in range(tot_fea):\n            filename_j = trafile + \"_{0}_reordered.npy\".format(j)\n            if path.exists(filename_j):\n                print(\"Using existing \" + filename_j)\n            else:\n                recreate_flag = True\n        # load, reorder and concatenate data (memmap all reordered files per feature)\n        if recreate_flag:\n            # init reordered files (.npy appended automatically)\n            z = np.zeros((total_count))\n            for j in range(tot_fea):\n                filename_j = trafile + \"_{0}_reordered\".format(j)\n                np.save(filename_j, z)\n                print(\"Creating \" + filename_j)\n\n            group_day = 3  # e.g. 8, 4 or 3\n            group_num = days // group_day\n            file_group = [i*group_day for i in range(group_num)] + [days]\n            for ii in range(group_num):\n                # for last may be group_size != group_num, therefore reset it below\n                group_size = file_group[ii + 1] - file_group[ii]\n                X_cat_t = [0]*group_size\n                X_int_t = [0]*group_size\n                y = [0]*group_size\n                start = [0]*group_size\n                end  = [0]*group_size\n                for ig in range(group_size):\n                    i = file_group[ii] + ig\n                    filename_i = d_path + npzfile + \"_{0}_processed.npz\".format(i)\n                    # setup start and end ranges\n                    start[ig] = offset_per_file[i]\n                    end[ig] = offset_per_file[i + 1]\n                    # print(filename_i)\n                    # load a group of files\n                    with np.load(filename_i) as data:\n                        X_cat_t[ig] = np.transpose(data[\"X_cat\"])\n                        X_int_t[ig] = np.transpose(data[\"X_int\"])\n                        y[ig] = data[\"y\"]\n                    # sanity check\n                    if total_per_file[i] != len(y[ig]):\n                        sys.exit(\"ERROR: sanity check on number of samples failed\")\n                # print(\"start=\" + str(start) + \" end=\" + str(end)\n                #  + \" diff=\" + str(end[ig]-start[ig]) + \"=\" + str(total_per_file[i]))\n\n                for j in range(tot_fea):\n                    filename_j = trafile + \"_{0}_reordered.npy\".format(j)\n                    fj = np.load(filename_j, mmap_mode='r+')\n                    for ig in range(group_size):\n                        if j < tar_fea:\n                            fj[indices[start[ig]:end[ig]]] = y[ig]\n                        elif tar_fea <= j and j < tad_fea:\n                            fj[indices[start[ig]:end[ig]]] = X_int_t[ig][j - tar_fea, :]\n                        else:\n                            fj[indices[start[ig]:end[ig]]] = X_cat_t[ig][j - tad_fea, :]\n                    del fj\n        else:\n            print(\"Reordered fea files already exist, skipping ...\")\n\n        # check if data already exists\n        recreate_flag = False\n        for i in range(days):\n            filename_i = d_path + npzfile + \"_{0}_reordered.npz\".format(i)\n            if path.exists(filename_i):\n                print(\"Using existing \" + filename_i)\n            else:\n                recreate_flag = True\n        # split reordered data by files (memmap all reordered files per feature)\n        # on the day boundary del the file object and memmap again\n        if recreate_flag:\n            for ii in range(group_num):\n                # for last may be group_size != group_num, therefore reset it below\n                group_size = file_group[ii + 1] - file_group[ii]\n                X_cat_t= []; X_int_t = []\n                for ig in range(group_size):\n                    i = file_group[ii] + ig\n                    X_int_t.append(np.zeros((den_fea, total_per_file[i])))\n                    X_cat_t.append(np.zeros((spa_fea, total_per_file[i])))\n                y = [0]*group_size\n                start = [0]*group_size\n                end  = [0]*group_size\n\n                for j in range(tot_fea):\n                    filename_j = trafile + \"_{0}_reordered.npy\".format(j)\n                    fj = np.load(filename_j, mmap_mode='r')\n                    # load a group of files\n                    for ig in range(group_size):\n                        i = file_group[ii] + ig\n                        # setup start and end ranges\n                        start[ig] = offset_per_file[i]\n                        end[ig] = offset_per_file[i + 1]\n                        # load data for the group of files\n                        if j < tar_fea:\n                            y[ig] = fj[start[ig]:end[ig]]\n                        elif tar_fea <= j and j < tad_fea:\n                            X_int_t[ig][j - tar_fea, :] = fj[start[ig]:end[ig]]\n                        else:\n                            X_cat_t[ig][j - tad_fea, :] = fj[start[ig]:end[ig]]\n                    del fj\n\n                for ig in range(group_size):\n                    i = file_group[ii] + ig\n                    filename_i = d_path + npzfile + \"_{0}_reordered.npz\".format(i)\n                    print(\"Creating \" + filename_i)\n                    np.savez_compressed(\n                        filename_i,\n                        X_cat=np.transpose(X_cat_t[ig]),  # transpose of the data\n                        X_int=np.transpose(X_int_t[ig]),  # transpose of the data\n                        y=y[ig],\n                    )\n        else:\n            print(\"Reordered day files already exist, skipping ...\")\n        \"\"\"\n        \"\"\"\n        # Approach 3: group features\n        # check if data already exists\n        group_fea = 5  # e.g. 8, 5 or 4\n        group_num = tot_fea // group_fea\n        if tot_fea % group_fea != 0:  # sanity check\n            sys.exit(\"ERROR: the group_fea must divided tot_fea evenly.\")\n        recreate_flag = False\n        for jn in range(group_num):\n            filename_j = trafile + \"_{0}_reordered{1}.npy\".format(\n                jn, group_fea\n            )\n            if path.exists(filename_j):\n                print(\"Using existing \" + filename_j)\n            else:\n                recreate_flag = True\n        # load, reorder and concatenate data (memmap all reordered files per feature)\n        if recreate_flag:\n            # init reordered files (.npy appended automatically)\n            z = np.zeros((group_fea, total_count))\n            for jn in range(group_num):\n                filename_j = trafile + \"_{0}_reordered{1}\".format(\n                    jn, group_fea\n                )\n                np.save(filename_j, z)\n                print(\"Creating \" + filename_j)\n\n            for i in range(days):\n                filename_i = d_path + npzfile + \"_{0}_processed.npz\".format(i)\n                with np.load(filename_i) as data:\n                    X_cat_t = np.transpose(data[\"X_cat\"])\n                    X_int_t = np.transpose(data[\"X_int\"])\n                    y = data[\"y\"]\n                size = len(y)\n                # sanity check\n                if total_per_file[i] != size:\n                    sys.exit(\"ERROR: sanity check on number of samples failed\")\n                # setup start and end ranges\n                start = offset_per_file[i]\n                end = offset_per_file[i + 1]\n                # print(filename_i)\n                # print(\"start=\" + str(start) + \" end=\" + str(end)\n                #      + \" diff=\" + str(end - start) + \"=\" + str(total_per_file[i]))\n\n                for jn in range(group_num):\n                    filename_j = trafile + \"_{0}_reordered{1}.npy\".format(\n                        jn, group_fea\n                    )\n                    fj = np.load(filename_j, mmap_mode='r+')\n                    for jg in range(group_fea):\n                        j = jn * group_fea + jg\n                        # print(\"j=\" + str(j) + \" jn=\" + str(jn) + \" jg=\" + str(jg))\n                        if j < tar_fea:\n                            fj[jg, indices[start:end]] = y\n                        elif tar_fea <= j and j < tad_fea:\n                            fj[jg, indices[start:end]] = X_int_t[j - tar_fea, :]\n                        else:\n                            fj[jg, indices[start:end]] = X_cat_t[j - tad_fea, :]\n                    del fj\n        else:\n            print(\"Reordered fea files already exist, skipping ...\")\n\n        # check if data already exists\n        recreate_flag = False\n        for i in range(days):\n            filename_i = d_path + npzfile + \"_{0}_reordered.npz\".format(i)\n            if path.exists(filename_i):\n                print(\"Using existing\" + filename_i)\n            else:\n                recreate_flag = True\n        # split reordered data by files (memmap all reordered files per feature)\n        # on the day boundary del the file object and memmap again\n        if recreate_flag:\n            for i in range(days):\n                filename_i = d_path + npzfile + \"_{0}_reordered.npz\".format(i)\n                size = total_per_file[i]\n                X_int_t = np.zeros((den_fea, size))\n                X_cat_t = np.zeros((spa_fea, size))\n                # setup start and end ranges\n                start = offset_per_file[i]\n                end = offset_per_file[i + 1]\n                print(\"Creating \" + filename_i)\n                # print(\"start=\" + str(start) + \" end=\" + str(end)\n                #      + \" diff=\" + str(end - start) + \"=\" + str(total_per_file[i]))\n\n                for jn in range(group_num):\n                    filename_j = trafile + \"_{0}_reordered{1}.npy\".format(\n                        jn, group_fea\n                    )\n                    fj = np.load(filename_j, mmap_mode='r')\n                    for jg in range(group_fea):\n                        j = jn * group_fea + jg\n                        # print(\"j=\" + str(j) + \" jn=\" + str(jn) + \" jg=\" + str(jg))\n                        if j < tar_fea:\n                            y = fj[jg, start:end]\n                        elif tar_fea <= j and j < tad_fea:\n                            X_int_t[j - tar_fea, :] = fj[jg, start:end]\n                        else:\n                            X_cat_t[j - tad_fea, :] = fj[jg, start:end]\n                    del fj\n\n                np.savez_compressed(\n                    filename_i,\n                    X_cat=np.transpose(X_cat_t),  # transpose of the data\n                    X_int=np.transpose(X_int_t),  # transpose of the data\n                    y=y,\n                )\n\n        else:\n            print(\"Reordered day files already exist, skipping ...\")\n        \"\"\"\n\n        # Approach 4: Fisher-Yates-Rao (FYR) shuffle algorithm\n        # 1st pass of FYR shuffle\n        # check if data already exists\n        recreate_flag = False\n        for j in range(days):\n            filename_j_y = npzfile + \"_{0}_intermediate_y.npy\".format(j)\n            filename_j_d = npzfile + \"_{0}_intermediate_d.npy\".format(j)\n            filename_j_s = npzfile + \"_{0}_intermediate_s.npy\".format(j)\n            if (\n                path.exists(filename_j_y)\n                and path.exists(filename_j_d)\n                and path.exists(filename_j_s)\n            ):\n                print(\n                    \"Using existing\\n\"\n                    + filename_j_y\n                    + \"\\n\"\n                    + filename_j_d\n                    + \"\\n\"\n                    + filename_j_s\n                )\n            else:\n                recreate_flag = True\n        # reorder across buckets using sampling\n        if recreate_flag:\n            # init intermediate files (.npy appended automatically)\n            for j in range(days):\n                filename_j_y = npzfile + \"_{0}_intermediate_y\".format(j)\n                filename_j_d = npzfile + \"_{0}_intermediate_d\".format(j)\n                filename_j_s = npzfile + \"_{0}_intermediate_s\".format(j)\n                np.save(filename_j_y, np.zeros((total_per_file[j])))\n                np.save(filename_j_d, np.zeros((total_per_file[j], den_fea)))\n                np.save(filename_j_s, np.zeros((total_per_file[j], spa_fea)))\n            # start processing files\n            total_counter = [0] * days\n            for i in range(days):\n                filename_i = npzfile + \"_{0}_processed.npz\".format(i)\n                with np.load(filename_i) as data:\n                    X_cat = data[\"X_cat\"]\n                    X_int = data[\"X_int\"]\n                    y = data[\"y\"]\n                size = len(y)\n                # sanity check\n                if total_per_file[i] != size:\n                    sys.exit(\"ERROR: sanity check on number of samples failed\")\n                # debug prints\n                print(\"Reordering (1st pass) \" + filename_i)\n\n                # create buckets using sampling of random ints\n                # from (discrete) uniform distribution\n                buckets = []\n                for _j in range(days):\n                    buckets.append([])\n                counter = [0] * days\n                days_to_sample = days if data_split == \"none\" else days - 1\n                if randomize == \"total\":\n                    rand_u = np.random.randint(low=0, high=days_to_sample, size=size)\n                    for k in range(size):\n                        # sample and make sure elements per buckets do not overflow\n                        if data_split == \"none\" or i < days - 1:\n                            # choose bucket\n                            p = rand_u[k]\n                            # retry of the bucket is full\n                            while total_counter[p] + counter[p] >= total_per_file[p]:\n                                p = np.random.randint(low=0, high=days_to_sample)\n                        else:  # preserve the last day/bucket if needed\n                            p = i\n                        buckets[p].append(k)\n                        counter[p] += 1\n                else:  # randomize is day or none\n                    for k in range(size):\n                        # do not sample, preserve the data in this bucket\n                        p = i\n                        buckets[p].append(k)\n                        counter[p] += 1\n\n                # sanity check\n                if np.sum(counter) != size:\n                    sys.exit(\"ERROR: sanity check on number of samples failed\")\n                # debug prints\n                # print(counter)\n                # print(str(np.sum(counter)) + \" = \" + str(size))\n                # print([len(x) for x in buckets])\n                # print(total_counter)\n\n                # partially feel the buckets\n                for j in range(days):\n                    filename_j_y = npzfile + \"_{0}_intermediate_y.npy\".format(j)\n                    filename_j_d = npzfile + \"_{0}_intermediate_d.npy\".format(j)\n                    filename_j_s = npzfile + \"_{0}_intermediate_s.npy\".format(j)\n                    start = total_counter[j]\n                    end = total_counter[j] + counter[j]\n                    # target buckets\n                    fj_y = np.load(filename_j_y, mmap_mode=\"r+\")\n                    # print(\"start=\" + str(start) + \" end=\" + str(end)\n                    #       + \" end - start=\" + str(end - start) + \" \"\n                    #       + str(fj_y[start:end].shape) + \" \"\n                    #       + str(len(buckets[j])))\n                    fj_y[start:end] = y[buckets[j]]\n                    del fj_y\n                    # dense buckets\n                    fj_d = np.load(filename_j_d, mmap_mode=\"r+\")\n                    # print(\"start=\" + str(start) + \" end=\" + str(end)\n                    #       + \" end - start=\" + str(end - start) + \" \"\n                    #       + str(fj_d[start:end, :].shape) + \" \"\n                    #       + str(len(buckets[j])))\n                    fj_d[start:end, :] = X_int[buckets[j], :]\n                    del fj_d\n                    # sparse buckets\n                    fj_s = np.load(filename_j_s, mmap_mode=\"r+\")\n                    # print(\"start=\" + str(start) + \" end=\" + str(end)\n                    #       + \" end - start=\" + str(end - start) + \" \"\n                    #       + str(fj_s[start:end, :].shape) + \" \"\n                    #       + str(len(buckets[j])))\n                    fj_s[start:end, :] = X_cat[buckets[j], :]\n                    del fj_s\n                    # update counters for next step\n                    total_counter[j] += counter[j]\n\n        # 2nd pass of FYR shuffle\n        # check if data already exists\n        for j in range(days):\n            filename_j = npzfile + \"_{0}_reordered.npz\".format(j)\n            if path.exists(filename_j):\n                print(\"Using existing \" + filename_j)\n            else:\n                recreate_flag = True\n        # reorder within buckets\n        if recreate_flag:\n            for j in range(days):\n                filename_j_y = npzfile + \"_{0}_intermediate_y.npy\".format(j)\n                filename_j_d = npzfile + \"_{0}_intermediate_d.npy\".format(j)\n                filename_j_s = npzfile + \"_{0}_intermediate_s.npy\".format(j)\n                fj_y = np.load(filename_j_y)\n                fj_d = np.load(filename_j_d)\n                fj_s = np.load(filename_j_s)\n\n                indices = range(total_per_file[j])\n                if randomize == \"day\" or randomize == \"total\":\n                    if data_split == \"none\" or j < days - 1:\n                        indices = np.random.permutation(range(total_per_file[j]))\n\n                filename_r = npzfile + \"_{0}_reordered.npz\".format(j)\n                print(\"Reordering (2nd pass) \" + filename_r)\n                np.savez_compressed(\n                    filename_r,\n                    X_cat=fj_s[indices, :],\n                    X_int=fj_d[indices, :],\n                    y=fj_y[indices],\n                )\n\n        \"\"\"\n        # sanity check (under no reordering norms should be zero)\n        for i in range(days):\n            filename_i_o = npzfile + \"_{0}_processed.npz\".format(i)\n            print(filename_i_o)\n            with np.load(filename_i_o) as data_original:\n                X_cat_o = data_original[\"X_cat\"]\n                X_int_o = data_original[\"X_int\"]\n                y_o = data_original[\"y\"]\n            filename_i_r = npzfile + \"_{0}_reordered.npz\".format(i)\n            print(filename_i_r)\n            with np.load(filename_i_r) as data_reordered:\n                X_cat_r = data_reordered[\"X_cat\"]\n                X_int_r = data_reordered[\"X_int\"]\n                y_r = data_reordered[\"y\"]\n            print(np.linalg.norm(y_o - y_r))\n            print(np.linalg.norm(X_int_o - X_int_r))\n            print(np.linalg.norm(X_cat_o - X_cat_r))\n        \"\"\"\n\n    else:\n        print(\"Concatenating multiple days into %s.npz file\" % str(d_path + o_filename))\n\n        # load and concatenate data\n        for i in range(days):\n            filename_i = npzfile + \"_{0}_processed.npz\".format(i)\n            with np.load(filename_i) as data:\n                if i == 0:\n                    X_cat = data[\"X_cat\"]\n                    X_int = data[\"X_int\"]\n                    y = data[\"y\"]\n                else:\n                    X_cat = np.concatenate((X_cat, data[\"X_cat\"]))\n                    X_int = np.concatenate((X_int, data[\"X_int\"]))\n                    y = np.concatenate((y, data[\"y\"]))\n            print(\"Loaded day:\", i, \"y = 1:\", len(y[y == 1]), \"y = 0:\", len(y[y == 0]))\n\n        with np.load(d_path + d_file + \"_fea_count.npz\") as data:\n            counts = data[\"counts\"]\n        print(\"Loaded counts!\")\n\n        np.savez_compressed(\n            d_path + o_filename + \".npz\",\n            X_cat=X_cat,\n            X_int=X_int,\n            y=y,\n            counts=counts,\n        )\n\n    return d_path + o_filename + \".npz\"\n\n\ndef transformCriteoAdData(X_cat, X_int, y, days, data_split, randomize, total_per_file):\n    # Transforms Criteo Kaggle or terabyte data by applying log transformation\n    # on dense features and converting everything to appropriate tensors.\n    #\n    # Inputs:\n    #     X_cat (ndarray): array of integers corresponding to preprocessed\n    #                      categorical features\n    #     X_int (ndarray): array of integers corresponding to dense features\n    #     y (ndarray):     array of bool corresponding to labels\n    #     data_split(str): flag for splitting dataset into training/validation/test\n    #                      sets\n    #     randomize (str): determines randomization scheme\n    #         \"none\": no randomization\n    #         \"day\": randomizes each day\"s data (only works if split = True)\n    #         \"total\": randomizes total dataset\n    #\n    # Outputs:\n    #     if split:\n    #         X_cat_train (tensor): sparse features for training set\n    #         X_int_train (tensor): dense features for training set\n    #         y_train (tensor): labels for training set\n    #         X_cat_val (tensor): sparse features for validation set\n    #         X_int_val (tensor): dense features for validation set\n    #         y_val (tensor): labels for validation set\n    #         X_cat_test (tensor): sparse features for test set\n    #         X_int_test (tensor): dense features for test set\n    #         y_test (tensor): labels for test set\n    #     else:\n    #         X_cat (tensor): sparse features\n    #         X_int (tensor): dense features\n    #         y (tensor): label\n\n    # define initial set of indices\n    indices = np.arange(len(y))\n\n    # create offset per file\n    offset_per_file = np.array([0] + [x for x in total_per_file])\n    for i in range(days):\n        offset_per_file[i + 1] += offset_per_file[i]\n\n    # split dataset\n    if data_split == \"train\":\n        indices = np.array_split(indices, offset_per_file[1:-1])\n\n        # randomize train data (per day)\n        if randomize == \"day\":  # or randomize == \"total\":\n            for i in range(len(indices) - 1):\n                indices[i] = np.random.permutation(indices[i])\n            print(\"Randomized indices per day ...\")\n\n        train_indices = np.concatenate(indices[:-1])\n        test_indices = indices[-1]\n        test_indices, val_indices = np.array_split(test_indices, 2)\n\n        print(\"Defined training and testing indices...\")\n\n        # randomize train data (across days)\n        if randomize == \"total\":\n            train_indices = np.random.permutation(train_indices)\n            print(\"Randomized indices across days ...\")\n\n        # indices = np.concatenate((train_indices, test_indices))\n\n        # create training, validation, and test sets\n        X_cat_train = X_cat[train_indices]\n        X_int_train = X_int[train_indices]\n        y_train = y[train_indices]\n\n        X_cat_val = X_cat[val_indices]\n        X_int_val = X_int[val_indices]\n        y_val = y[val_indices]\n\n        X_cat_test = X_cat[test_indices]\n        X_int_test = X_int[test_indices]\n        y_test = y[test_indices]\n\n        print(\"Split data according to indices...\")\n\n        X_cat_train = X_cat_train.astype(int)\n        X_int_train = np.log(X_int_train.astype(np.float32) + 1)\n        y_train = y_train.astype(np.float32)\n\n        X_cat_val = X_cat_val.astype(int)\n        X_int_val = np.log(X_int_val.astype(np.float32) + 1)\n        y_val = y_val.astype(np.float32)\n\n        X_cat_test = X_cat_test.astype(int)\n        X_int_test = np.log(X_int_test.astype(np.float32) + 1)\n        y_test = y_test.astype(np.float32)\n\n        print(\"Converted to tensors...done!\")\n\n        return (\n            X_cat_train,\n            X_int_train,\n            y_train,\n            X_cat_val,\n            X_int_val,\n            y_val,\n            X_cat_test,\n            X_int_test,\n            y_test,\n        )\n\n    else:\n        # randomize data\n        if randomize == \"total\":\n            indices = np.random.permutation(indices)\n            print(\"Randomized indices...\")\n\n        X_cat = X_cat[indices].astype(int)\n        X_int = np.log(X_int[indices].astype(np.float32) + 1)\n        y = y[indices].astype(np.float32)\n\n        print(\"Converted to tensors...done!\")\n\n        return (X_cat, X_int, y, [], [], [], [], [], [])\n\n\ndef getCriteoAdData(\n    datafile,\n    o_filename,\n    max_ind_range=-1,\n    sub_sample_rate=0.0,\n    days=7,\n    data_split=\"train\",\n    randomize=\"total\",\n    criteo_kaggle=True,\n    memory_map=False,\n    dataset_multiprocessing=False,\n):\n    # Passes through entire dataset and defines dictionaries for categorical\n    # features and determines the number of total categories.\n    #\n    # Inputs:\n    #    datafile : path to downloaded raw data file\n    #    o_filename (str): saves results under o_filename if filename is not \"\"\n    #\n    # Output:\n    #   o_file (str): output file path\n\n    # split the datafile into path and filename\n    lstr = datafile.split(\"/\")\n    d_path = \"/\".join(lstr[0:-1]) + \"/\"\n    d_file = lstr[-1].split(\".\")[0] if criteo_kaggle else lstr[-1]\n    npzfile = d_path + ((d_file + \"_day\") if criteo_kaggle else d_file)\n    trafile = d_path + ((d_file + \"_fea\") if criteo_kaggle else \"fea\")\n\n    # count number of datapoints in training set\n    total_file = d_path + d_file + \"_day_count.npz\"\n    if path.exists(total_file):\n        with np.load(total_file) as data:\n            total_per_file = list(data[\"total_per_file\"])\n        total_count = np.sum(total_per_file)\n        print(\"Skipping counts per file (already exist)\")\n    else:\n        total_count = 0\n        total_per_file = []\n        if criteo_kaggle:\n            # WARNING: The raw data consists of a single train.txt file\n            # Each line in the file is a sample, consisting of 13 continuous and\n            # 26 categorical features (an extra space indicates that feature is\n            # missing and will be interpreted as 0).\n            if path.exists(datafile):\n                print(\"Reading data from path=%s\" % (datafile))\n                with open(str(datafile)) as f:\n                    for _ in f:\n                        total_count += 1\n                total_per_file.append(total_count)\n                # reset total per file due to split\n                num_data_per_split, extras = divmod(total_count, days)\n                total_per_file = [num_data_per_split] * days\n                for j in range(extras):\n                    total_per_file[j] += 1\n                # split into days (simplifies code later on)\n                file_id = 0\n                boundary = total_per_file[file_id]\n                nf = open(npzfile + \"_\" + str(file_id), \"w\")\n                with open(str(datafile)) as f:\n                    for j, line in enumerate(f):\n                        if j == boundary:\n                            nf.close()\n                            file_id += 1\n                            nf = open(npzfile + \"_\" + str(file_id), \"w\")\n                            boundary += total_per_file[file_id]\n                        nf.write(line)\n                nf.close()\n            else:\n                sys.exit(\n                    \"ERROR: Criteo Kaggle Display Ad Challenge Dataset path is invalid; please download from https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset\"\n                )\n        else:\n            # WARNING: The raw data consist of day_0.gz,... ,day_23.gz text files\n            # Each line in the file is a sample, consisting of 13 continuous and\n            # 26 categorical features (an extra space indicates that feature is\n            # missing and will be interpreted as 0).\n            for i in range(days):\n                datafile_i = datafile + \"_\" + str(i)  # + \".gz\"\n                if path.exists(str(datafile_i)):\n                    print(\"Reading data from path=%s\" % (str(datafile_i)))\n                    # file day_<number>\n                    total_per_file_count = 0\n                    with open(str(datafile_i)) as f:\n                        for _ in f:\n                            total_per_file_count += 1\n                    total_per_file.append(total_per_file_count)\n                    total_count += total_per_file_count\n                else:\n                    sys.exit(\n                        \"ERROR: Criteo Terabyte Dataset path is invalid; please download from https://labs.criteo.com/2013/12/download-terabyte-click-logs\"\n                    )\n\n    # process a file worth of data and reinitialize data\n    # note that a file main contain a single or multiple splits\n    def process_one_file(\n        datfile,\n        npzfile,\n        split,\n        num_data_in_split,\n        dataset_multiprocessing,\n        convertDictsDay=None,\n        resultDay=None,\n    ):\n        if dataset_multiprocessing:\n            convertDicts_day = [{} for _ in range(26)]\n\n        with open(str(datfile)) as f:\n            y = np.zeros(num_data_in_split, dtype=\"i4\")  # 4 byte int\n            X_int = np.zeros((num_data_in_split, 13), dtype=\"i4\")  # 4 byte int\n            X_cat = np.zeros((num_data_in_split, 26), dtype=\"i4\")  # 4 byte int\n            if sub_sample_rate == 0.0:\n                rand_u = 1.0\n            else:\n                rand_u = np.random.uniform(low=0.0, high=1.0, size=num_data_in_split)\n\n            i = 0\n            percent = 0\n            for k, line in enumerate(f):\n                # process a line (data point)\n                line = line.split(\"\\t\")\n                # set missing values to zero\n                for j in range(len(line)):\n                    if (line[j] == \"\") or (line[j] == \"\\n\"):\n                        line[j] = \"0\"\n                # sub-sample data by dropping zero targets, if needed\n                target = np.int32(line[0])\n                if (\n                    target == 0\n                    and (rand_u if sub_sample_rate == 0.0 else rand_u[k])\n                    < sub_sample_rate\n                ):\n                    continue\n\n                y[i] = target\n                X_int[i] = np.array(line[1:14], dtype=np.int32)\n                if max_ind_range > 0:\n                    X_cat[i] = np.array(\n                        list(map(lambda x: int(x, 16) % max_ind_range, line[14:])),\n                        dtype=np.int32,\n                    )\n                else:\n                    X_cat[i] = np.array(\n                        list(map(lambda x: int(x, 16), line[14:])), dtype=np.int32\n                    )\n\n                # count uniques\n                if dataset_multiprocessing:\n                    for j in range(26):\n                        convertDicts_day[j][X_cat[i][j]] = 1\n                    # debug prints\n                    if float(i) / num_data_in_split * 100 > percent + 1:\n                        percent = int(float(i) / num_data_in_split * 100)\n                        print(\n                            \"Load %d/%d (%d%%) Split: %d  Label True: %d  Stored: %d\"\n                            % (\n                                i,\n                                num_data_in_split,\n                                percent,\n                                split,\n                                target,\n                                y[i],\n                            ),\n                            end=\"\\n\",\n                        )\n                else:\n                    for j in range(26):\n                        convertDicts[j][X_cat[i][j]] = 1\n                    # debug prints\n                    print(\n                        \"Load %d/%d  Split: %d  Label True: %d  Stored: %d\"\n                        % (\n                            i,\n                            num_data_in_split,\n                            split,\n                            target,\n                            y[i],\n                        ),\n                        end=\"\\r\",\n                    )\n                i += 1\n\n            # store num_data_in_split samples or extras at the end of file\n            # count uniques\n            # X_cat_t  = np.transpose(X_cat)\n            # for j in range(26):\n            #     for x in X_cat_t[j,:]:\n            #         convertDicts[j][x] = 1\n            # store parsed\n            filename_s = npzfile + \"_{0}.npz\".format(split)\n            if path.exists(filename_s):\n                print(\"\\nSkip existing \" + filename_s)\n            else:\n                np.savez_compressed(\n                    filename_s,\n                    X_int=X_int[0:i, :],\n                    # X_cat=X_cat[0:i, :],\n                    X_cat_t=np.transpose(X_cat[0:i, :]),  # transpose of the data\n                    y=y[0:i],\n                )\n                print(\"\\nSaved \" + npzfile + \"_{0}.npz!\".format(split))\n\n        if dataset_multiprocessing:\n            resultDay[split] = i\n            convertDictsDay[split] = convertDicts_day\n            return\n        else:\n            return i\n\n    # create all splits (reuse existing files if possible)\n    recreate_flag = False\n    convertDicts = [{} for _ in range(26)]\n    # WARNING: to get reproducable sub-sampling results you must reset the seed below\n    # np.random.seed(123)\n    # in this case there is a single split in each day\n    for i in range(days):\n        npzfile_i = npzfile + \"_{0}.npz\".format(i)\n        npzfile_p = npzfile + \"_{0}_processed.npz\".format(i)\n        if path.exists(npzfile_i):\n            print(\"Skip existing \" + npzfile_i)\n        elif path.exists(npzfile_p):\n            print(\"Skip existing \" + npzfile_p)\n        else:\n            recreate_flag = True\n\n    if recreate_flag:\n        if dataset_multiprocessing:\n            resultDay = Manager().dict()\n            convertDictsDay = Manager().dict()\n            processes = [\n                Process(\n                    target=process_one_file,\n                    name=\"process_one_file:%i\" % i,\n                    args=(\n                        npzfile + \"_{0}\".format(i),\n                        npzfile,\n                        i,\n                        total_per_file[i],\n                        dataset_multiprocessing,\n                        convertDictsDay,\n                        resultDay,\n                    ),\n                )\n                for i in range(0, days)\n            ]\n            for process in processes:\n                process.start()\n            for process in processes:\n                process.join()\n            for day in range(days):\n                total_per_file[day] = resultDay[day]\n                print(\"Constructing convertDicts Split: {}\".format(day))\n                convertDicts_tmp = convertDictsDay[day]\n                for i in range(26):\n                    for j in convertDicts_tmp[i]:\n                        convertDicts[i][j] = 1\n        else:\n            for i in range(days):\n                total_per_file[i] = process_one_file(\n                    npzfile + \"_{0}\".format(i),\n                    npzfile,\n                    i,\n                    total_per_file[i],\n                    dataset_multiprocessing,\n                )\n\n    # report and save total into a file\n    total_count = np.sum(total_per_file)\n    if not path.exists(total_file):\n        np.savez_compressed(total_file, total_per_file=total_per_file)\n    print(\"Total number of samples:\", total_count)\n    print(\"Divided into days/splits:\\n\", total_per_file)\n\n    # dictionary files\n    counts = np.zeros(26, dtype=np.int32)\n    if recreate_flag:\n        # create dictionaries\n        for j in range(26):\n            for i, x in enumerate(convertDicts[j]):\n                convertDicts[j][x] = i\n            dict_file_j = d_path + d_file + \"_fea_dict_{0}.npz\".format(j)\n            if not path.exists(dict_file_j):\n                np.savez_compressed(\n                    dict_file_j, unique=np.array(list(convertDicts[j]), dtype=np.int32)\n                )\n            counts[j] = len(convertDicts[j])\n        # store (uniques and) counts\n        count_file = d_path + d_file + \"_fea_count.npz\"\n        if not path.exists(count_file):\n            np.savez_compressed(count_file, counts=counts)\n    else:\n        # create dictionaries (from existing files)\n        for j in range(26):\n            with np.load(d_path + d_file + \"_fea_dict_{0}.npz\".format(j)) as data:\n                unique = data[\"unique\"]\n            for i, x in enumerate(unique):\n                convertDicts[j][x] = i\n        # load (uniques and) counts\n        with np.load(d_path + d_file + \"_fea_count.npz\") as data:\n            counts = data[\"counts\"]\n\n    # process all splits\n    if dataset_multiprocessing:\n        processes = [\n            Process(\n                target=processCriteoAdData,\n                name=\"processCriteoAdData:%i\" % i,\n                args=(\n                    d_path,\n                    d_file,\n                    npzfile,\n                    i,\n                    convertDicts,\n                    counts,\n                ),\n            )\n            for i in range(0, days)\n        ]\n        for process in processes:\n            process.start()\n        for process in processes:\n            process.join()\n\n    else:\n        for i in range(days):\n            processCriteoAdData(d_path, d_file, npzfile, i, convertDicts, counts)\n\n    o_file = concatCriteoAdData(\n        d_path,\n        d_file,\n        npzfile,\n        trafile,\n        days,\n        data_split,\n        randomize,\n        total_per_file,\n        total_count,\n        memory_map,\n        o_filename,\n    )\n\n    return o_file\n\n\ndef loadDataset(\n    dataset,\n    max_ind_range,\n    sub_sample_rate,\n    randomize,\n    data_split,\n    raw_path=\"\",\n    pro_data=\"\",\n    memory_map=False,\n):\n    # dataset\n    if dataset == \"kaggle\":\n        days = 7\n        o_filename = \"kaggleAdDisplayChallenge_processed\"\n    elif dataset == \"terabyte\":\n        days = 24\n        o_filename = \"terabyte_processed\"\n    else:\n        raise (ValueError(\"Data set option is not supported\"))\n\n    # split the datafile into path and filename\n    lstr = raw_path.split(\"/\")\n    d_path = \"/\".join(lstr[0:-1]) + \"/\"\n    d_file = lstr[-1].split(\".\")[0] if dataset == \"kaggle\" else lstr[-1]\n    npzfile = (d_file + \"_day\") if dataset == \"kaggle\" else d_file\n    # trafile = d_path + ((d_file + \"_fea\") if dataset == \"kaggle\" else \"fea\")\n\n    # check if pre-processed data is available\n    data_ready = True\n    if memory_map:\n        for i in range(days):\n            reo_data = d_path + npzfile + \"_{0}_reordered.npz\".format(i)\n            if not path.exists(str(reo_data)):\n                data_ready = False\n    else:\n        if not path.exists(str(pro_data)):\n            data_ready = False\n\n    # pre-process data if needed\n    # WARNNING: when memory mapping is used we get a collection of files\n    if data_ready:\n        print(\"Reading pre-processed data=%s\" % (str(pro_data)))\n        file = str(pro_data)\n    else:\n        print(\"Reading raw data=%s\" % (str(raw_path)))\n        file = getCriteoAdData(\n            raw_path,\n            o_filename,\n            max_ind_range,\n            sub_sample_rate,\n            days,\n            data_split,\n            randomize,\n            dataset == \"kaggle\",\n            memory_map,\n        )\n\n    return file, days\n\n\nif __name__ == \"__main__\":\n    ### import packages ###\n    import argparse\n\n    ### parse arguments ###\n    parser = argparse.ArgumentParser(description=\"Preprocess Criteo dataset\")\n    # model related parameters\n    parser.add_argument(\"--max-ind-range\", type=int, default=-1)\n    parser.add_argument(\"--data-sub-sample-rate\", type=float, default=0.0)  # in [0, 1]\n    parser.add_argument(\"--data-randomize\", type=str, default=\"total\")  # or day or none\n    parser.add_argument(\"--memory-map\", action=\"store_true\", default=False)\n    parser.add_argument(\"--data-set\", type=str, default=\"kaggle\")  # or terabyte\n    parser.add_argument(\"--raw-data-file\", type=str, default=\"\")\n    parser.add_argument(\"--processed-data-file\", type=str, default=\"\")\n    args = parser.parse_args()\n\n    loadDataset(\n        args.data_set,\n        args.max_ind_range,\n        args.data_sub_sample_rate,\n        args.data_randomize,\n        \"train\",\n        args.raw_data_file,\n        args.processed_data_file,\n        args.memory_map,\n    )\n"
        },
        {
          "name": "dlrm_data_caffe2.py",
          "type": "blob",
          "size": 29.3603515625,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Description: generate inputs and targets for the dlrm benchmark\n# The inpts and outputs are generated according to the following three option(s)\n# 1) random distribution\n# 2) synthetic distribution, based on unique accesses and distances between them\n#    i) R. Hassan, A. Harris, N. Topham and A. Efthymiou \"Synthetic Trace-Driven\n#    Simulation of Cache Memory\", IEEE AINAM'07\n# 3) public data set\n#    i)  Criteo Kaggle Display Advertising Challenge Dataset\n#    https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset\n#    ii) Criteo Terabyte Dataset\n#    https://labs.criteo.com/2013/12/download-terabyte-click-logs\n\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport bisect\nimport collections\n\n# others\n# from os import path\nimport sys\n\nimport data_utils\n\n# numpy\nimport numpy as np\n\n# pytorch\nimport torch\nfrom numpy import random as ra\nfrom torch.utils.data import Dataset\n\n\n# Kaggle Display Advertising Challenge Dataset\n# dataset (str): name of dataset (Kaggle or Terabyte)\n# randomize (str): determines randomization scheme\n#            'none': no randomization\n#            'day': randomizes each day's data (only works if split = True)\n#            'total': randomizes total dataset\n# split (bool) : to split into train, test, validation data-sets\n\n\nclass CriteoDatasetWMemoryMap(Dataset):\n    def __init__(\n        self,\n        dataset,\n        max_ind_range,\n        sub_sample_rate,\n        randomize,\n        split=\"train\",\n        raw_path=\"\",\n        pro_data=\"\",\n    ):\n        # dataset\n        # tar_fea = 1   # single target\n        den_fea = 13  # 13 dense  features\n        # spa_fea = 26  # 26 sparse features\n        # tad_fea = tar_fea + den_fea\n        # tot_fea = tad_fea + spa_fea\n        if dataset == \"kaggle\":\n            days = 7\n        elif dataset == \"terabyte\":\n            days = 24\n        else:\n            raise (ValueError(\"Data set option is not supported\"))\n        self.max_ind_range = max_ind_range\n\n        # split the datafile into path and filename\n        lstr = raw_path.split(\"/\")\n        self.d_path = \"/\".join(lstr[0:-1]) + \"/\"\n        self.d_file = lstr[-1].split(\".\")[0] if dataset == \"kaggle\" else lstr[-1]\n        self.npzfile = self.d_path + (\n            (self.d_file + \"_day\") if dataset == \"kaggle\" else self.d_file\n        )\n        self.trafile = self.d_path + (\n            (self.d_file + \"_fea\") if dataset == \"kaggle\" else \"fea\"\n        )\n\n        # get a number of samples per day\n        total_file = self.d_path + self.d_file + \"_day_count.npz\"\n        with np.load(total_file) as data:\n            total_per_file = data[\"total_per_file\"]\n        # compute offsets per file\n        self.offset_per_file = np.array([0] + list(total_per_file))\n        for i in range(days):\n            self.offset_per_file[i + 1] += self.offset_per_file[i]\n        # print(self.offset_per_file)\n\n        # setup data\n        self.split = split\n        if split == \"none\" or split == \"train\":\n            self.day = 0\n            self.max_day_range = days if split == \"none\" else days - 1\n        elif split == \"test\" or split == \"val\":\n            self.day = days - 1\n            num_samples = self.offset_per_file[days] - self.offset_per_file[days - 1]\n            self.test_size = int(np.ceil(num_samples / 2.0))\n            self.val_size = num_samples - self.test_size\n        else:\n            sys.exit(\"ERROR: dataset split is neither none, nor train or test.\")\n\n        # load unique counts\n        with np.load(self.d_path + self.d_file + \"_fea_count.npz\") as data:\n            self.counts = data[\"counts\"]\n        self.m_den = den_fea  # X_int.shape[1]\n        self.n_emb = len(self.counts)\n        print(\"Sparse features= %d, Dense features= %d\" % (self.n_emb, self.m_den))\n\n        # Load the test data\n        # Only a single day is used for testing\n        if self.split == \"test\" or self.split == \"val\":\n            # only a single day is used for testing\n            fi = self.npzfile + \"_{0}_reordered.npz\".format(self.day)\n            with np.load(fi) as data:\n                self.X_int = data[\"X_int\"]  # continuous  feature\n                self.X_cat = data[\"X_cat\"]  # categorical feature\n                self.y = data[\"y\"]  # target\n\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            return [\n                self[idx]\n                for idx in range(\n                    index.start or 0, index.stop or len(self), index.step or 1\n                )\n            ]\n        if self.split == \"none\" or self.split == \"train\":\n            # check if need to swicth to next day and load data\n            if index == self.offset_per_file[self.day]:\n                # print(\"day_boundary switch\", index)\n                self.day_boundary = self.offset_per_file[self.day]\n                fi = self.npzfile + \"_{0}_reordered.npz\".format(self.day)\n                # print('Loading file: ', fi)\n                with np.load(fi) as data:\n                    self.X_int = data[\"X_int\"]  # continuous  feature\n                    self.X_cat = data[\"X_cat\"]  # categorical feature\n                    self.y = data[\"y\"]  # target\n                self.day = (self.day + 1) % self.max_day_range\n\n            i = index - self.day_boundary\n        elif self.split == \"test\" or self.split == \"val\":\n            # only a single day is used for testing\n            i = index + (0 if self.split == \"test\" else self.test_size)\n        else:\n            sys.exit(\"ERROR: dataset split is neither none, nor train or test.\")\n\n        if self.max_ind_range > 0:\n            return self.X_int[i], self.X_cat[i] % self.max_ind_range, self.y[i]\n        else:\n            return self.X_int[i], self.X_cat[i], self.y[i]\n\n    def _default_preprocess(self, X_int, X_cat, y):\n        X_int = torch.log(torch.tensor(X_int, dtype=torch.float) + 1)\n        if self.max_ind_range > 0:\n            X_cat = torch.tensor(X_cat % self.max_ind_range, dtype=torch.long)\n        else:\n            X_cat = torch.tensor(X_cat, dtype=torch.long)\n        y = torch.tensor(y.astype(np.float32))\n\n        return X_int, X_cat, y\n\n    def __len__(self):\n        if self.split == \"none\":\n            return self.offset_per_file[-1]\n        elif self.split == \"train\":\n            return self.offset_per_file[-2]\n        elif self.split == \"test\":\n            return self.test_size\n        elif self.split == \"val\":\n            return self.val_size\n        else:\n            sys.exit(\"ERROR: dataset split is neither none, nor train nor test.\")\n\n\ndef collate_wrapper_criteo(list_of_tuples):\n    # where each tuple is (X_int, X_cat, y)\n    transposed_data = list(zip(*list_of_tuples))\n    X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)\n    X_cat = torch.tensor(transposed_data[1], dtype=torch.long)\n    T = torch.tensor(transposed_data[2], dtype=torch.float32).view(-1, 1)\n\n    batchSize = X_cat.shape[0]\n    featureCnt = X_cat.shape[1]\n\n    lS_i = [X_cat[:, i] for i in range(featureCnt)]\n    lS_o = [torch.tensor(range(batchSize)) for _ in range(featureCnt)]\n\n    return X_int, torch.stack(lS_o), torch.stack(lS_i), T\n\n\n# Conversion from offset to length\ndef offset_to_length_convertor(lS_o, lS_i):\n    def diff(tensor):\n        return tensor[1:] - tensor[:-1]\n\n    return torch.stack(\n        [\n            diff(torch.cat((S_o, torch.tensor(lS_i[ind].shape))).int())\n            for ind, S_o in enumerate(lS_o)\n        ]\n    )\n\n\ndef unpack_batch(b, data_gen, data_set):\n    return b[0], b[1], b[2], b[3], torch.ones(b[3].size())\n\n\ndef read_dataset(\n    dataset,\n    max_ind_range,\n    sub_sample_rate,\n    mini_batch_size,\n    num_batches,\n    randomize,\n    split=\"train\",\n    raw_data=\"\",\n    processed_data=\"\",\n    memory_map=False,\n    inference_only=False,\n    test_mini_batch_size=1,\n):\n    # split the datafile into path and filename\n    lstr = raw_data.split(\"/\")\n    d_path = \"/\".join(lstr[0:-1]) + \"/\"\n    d_file = lstr[-1].split(\".\")[0] if dataset == \"kaggle\" else lstr[-1]\n    # npzfile = d_path + ((d_file + \"_day\") if dataset == \"kaggle\" else d_file)\n    # trafile = d_path + ((d_file + \"_fea\") if dataset == \"kaggle\" else \"fea\")\n\n    # load\n    print(\"Loading %s dataset...\" % dataset)\n    nbatches = 0\n    file, days = data_utils.loadDataset(\n        dataset,\n        max_ind_range,\n        sub_sample_rate,\n        randomize,\n        split,\n        raw_data,\n        processed_data,\n        memory_map,\n    )\n\n    if memory_map:\n        # WARNING: at this point the data has been reordered and shuffled across files\n        # e.g. day_<number>_reordered.npz, what remains is simply to read and feed\n        # the data from each file, going in the order of days file-by-file, to the\n        # model during training.\n        train_data = CriteoDatasetWMemoryMap(\n            dataset,\n            max_ind_range,\n            sub_sample_rate,\n            randomize,\n            \"train\",\n            raw_data,\n            processed_data,\n        )\n\n        test_data = CriteoDatasetWMemoryMap(\n            dataset,\n            max_ind_range,\n            sub_sample_rate,\n            randomize,\n            \"test\",\n            raw_data,\n            processed_data,\n        )\n\n        train_loader = torch.utils.data.DataLoader(\n            train_data,\n            batch_size=mini_batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=collate_wrapper_criteo,\n            pin_memory=False,\n            drop_last=False,  # True\n        )\n\n        test_loader = torch.utils.data.DataLoader(\n            test_data,\n            batch_size=test_mini_batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=collate_wrapper_criteo,\n            pin_memory=False,\n            drop_last=False,  # True\n        )\n\n        return train_data, train_loader, test_data, test_loader\n\n    else:\n        # load and preprocess data\n        with np.load(file) as data:\n            X_int = data[\"X_int\"]\n            X_cat = data[\"X_cat\"]\n            y = data[\"y\"]\n            counts = data[\"counts\"]\n\n        # get a number of samples per day\n        total_file = d_path + d_file + \"_day_count.npz\"\n        with np.load(total_file) as data:\n            total_per_file = data[\"total_per_file\"]\n\n        # transform\n        (\n            X_cat_train,\n            X_int_train,\n            y_train,\n            X_cat_val,\n            X_int_val,\n            y_val,\n            X_cat_test,\n            X_int_test,\n            y_test,\n        ) = data_utils.transformCriteoAdData(\n            X_cat, X_int, y, days, split, randomize, total_per_file\n        )\n        ln_emb = counts\n        m_den = X_int_train.shape[1]\n        n_emb = len(counts)\n        print(\"Sparse features = %d, Dense features = %d\" % (n_emb, m_den))\n\n        # adjust parameters\n        def assemble_samples(X_cat, X_int, y, max_ind_range, print_message):\n            if max_ind_range > 0:\n                X_cat = X_cat % max_ind_range\n\n            nsamples = len(y)\n            data_size = nsamples\n            # using floor is equivalent to dropping last mini-batch (drop_last = True)\n            nbatches = int(np.floor((data_size * 1.0) / mini_batch_size))\n            print(print_message)\n            if num_batches != 0 and num_batches < nbatches:\n                print(\n                    \"Limiting to %d batches of the total % d batches\"\n                    % (num_batches, nbatches)\n                )\n                nbatches = num_batches\n            else:\n                print(\"Total number of batches %d\" % nbatches)\n\n            # data main loop\n            lX = []\n            lS_lengths = []\n            lS_indices = []\n            lT = []\n            for j in range(0, nbatches):\n                # number of data points in a batch\n                print(\"Reading in batch: %d / %d\" % (j + 1, nbatches), end=\"\\r\")\n                n = min(mini_batch_size, data_size - (j * mini_batch_size))\n                # dense feature\n                idx_start = j * mini_batch_size\n                lX.append((X_int[idx_start : (idx_start + n)]).astype(np.float32))\n                # Targets - outputs\n                lT.append(\n                    (y[idx_start : idx_start + n]).reshape(-1, 1).astype(np.int32)\n                )\n                # sparse feature (sparse indices)\n                lS_emb_indices = []\n                # for each embedding generate a list of n lookups,\n                # where each lookup is composed of multiple sparse indices\n                for size in range(n_emb):\n                    lS_batch_indices = []\n                    for _b in range(n):\n                        # num of sparse indices to be used per embedding, e.g. for\n                        # store lengths and indices\n                        lS_batch_indices += (\n                            (X_cat[idx_start + _b][size].reshape(-1)).astype(np.int32)\n                        ).tolist()\n                    lS_emb_indices.append(lS_batch_indices)\n                lS_indices.append(lS_emb_indices)\n                # Criteo Kaggle data it is 1 because data is categorical\n                lS_lengths.append(\n                    [(list(np.ones(n).astype(np.int32))) for _ in range(n_emb)]\n                )\n            print(\"\\n\")\n\n            return nbatches, lX, lS_lengths, lS_indices, lT\n\n        # adjust training data\n        (nbatches, lX, lS_lengths, lS_indices, lT) = assemble_samples(\n            X_cat_train, X_int_train, y_train, max_ind_range, \"Training data\"\n        )\n\n        # adjust testing data\n        (nbatches_t, lX_t, lS_lengths_t, lS_indices_t, lT_t) = assemble_samples(\n            X_cat_test, X_int_test, y_test, max_ind_range, \"Testing data\"\n        )\n    # end if memory_map\n\n    return (\n        nbatches,\n        lX,\n        lS_lengths,\n        lS_indices,\n        lT,\n        nbatches_t,\n        lX_t,\n        lS_lengths_t,\n        lS_indices_t,\n        lT_t,\n        ln_emb,\n        m_den,\n    )\n\n\ndef generate_random_data(\n    m_den,\n    ln_emb,\n    data_size,\n    num_batches,\n    mini_batch_size,\n    num_indices_per_lookup,\n    num_indices_per_lookup_fixed,\n    num_targets=1,\n    round_targets=False,\n    data_generation=\"random\",\n    trace_file=\"\",\n    enable_padding=False,\n):\n    nbatches = int(np.ceil((data_size * 1.0) / mini_batch_size))\n    if num_batches != 0:\n        nbatches = num_batches\n        data_size = nbatches * mini_batch_size\n    # print(\"Total number of batches %d\" % nbatches)\n\n    # inputs and targets\n    lT = []\n    lX = []\n    lS_lengths = []\n    lS_indices = []\n    for j in range(0, nbatches):\n        # number of data points in a batch\n        n = min(mini_batch_size, data_size - (j * mini_batch_size))\n\n        # generate a batch of dense and sparse features\n        if data_generation == \"random\":\n            (Xt, lS_emb_lengths, lS_emb_indices) = generate_uniform_input_batch(\n                m_den, ln_emb, n, num_indices_per_lookup, num_indices_per_lookup_fixed\n            )\n        elif data_generation == \"synthetic\":\n            (Xt, lS_emb_lengths, lS_emb_indices) = generate_synthetic_input_batch(\n                m_den,\n                ln_emb,\n                n,\n                num_indices_per_lookup,\n                num_indices_per_lookup_fixed,\n                trace_file,\n                enable_padding,\n            )\n        else:\n            sys.exit(\n                \"ERROR: --data-generation=\" + data_generation + \" is not supported\"\n            )\n        # dense feature\n        lX.append(Xt)\n        # sparse feature (sparse indices)\n        lS_lengths.append(lS_emb_lengths)\n        lS_indices.append(lS_emb_indices)\n\n        # generate a batch of target (probability of a click)\n        P = generate_random_output_batch(n, num_targets, round_targets)\n        lT.append(P)\n\n    return (nbatches, lX, lS_lengths, lS_indices, lT)\n\n\ndef generate_random_output_batch(n, num_targets=1, round_targets=False):\n    # target (probability of a click)\n    if round_targets:\n        P = np.round(ra.rand(n, num_targets).astype(np.float32)).astype(np.int32)\n    else:\n        P = ra.rand(n, num_targets).astype(np.float32)\n\n    return P\n\n\n# uniform ditribution (input data)\ndef generate_uniform_input_batch(\n    m_den,\n    ln_emb,\n    n,\n    num_indices_per_lookup,\n    num_indices_per_lookup_fixed,\n):\n    # dense feature\n    Xt = ra.rand(n, m_den).astype(np.float32)\n\n    # sparse feature (sparse indices)\n    lS_emb_lengths = []\n    lS_emb_indices = []\n    # for each embedding generate a list of n lookups,\n    # where each lookup is composed of multiple sparse indices\n    for size in ln_emb:\n        lS_batch_lengths = []\n        lS_batch_indices = []\n        for _ in range(n):\n            # num of sparse indices to be used per embedding (between\n            if num_indices_per_lookup_fixed:\n                sparse_group_size = np.int32(num_indices_per_lookup)\n            else:\n                # random between [1,num_indices_per_lookup])\n                r = ra.random(1)\n                sparse_group_size = np.int32(\n                    max(1, np.round(r * min(size, num_indices_per_lookup))[0])\n                )\n            # sparse indices to be used per embedding\n            r = ra.random(sparse_group_size)\n            sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int32))\n            # reset sparse_group_size in case some index duplicates were removed\n            sparse_group_size = np.int32(sparse_group.size)\n            # store lengths and indices\n            lS_batch_lengths += [sparse_group_size]\n            lS_batch_indices += sparse_group.tolist()\n        lS_emb_lengths.append(lS_batch_lengths)\n        lS_emb_indices.append(lS_batch_indices)\n\n    return (Xt, lS_emb_lengths, lS_emb_indices)\n\n\n# synthetic distribution (input data)\ndef generate_synthetic_input_batch(\n    m_den,\n    ln_emb,\n    n,\n    num_indices_per_lookup,\n    num_indices_per_lookup_fixed,\n    trace_file,\n    enable_padding=False,\n):\n    # dense feature\n    Xt = ra.rand(n, m_den).astype(np.float32)\n\n    # sparse feature (sparse indices)\n    lS_emb_lengths = []\n    lS_emb_indices = []\n    # for each embedding generate a list of n lookups,\n    # where each lookup is composed of multiple sparse indices\n    for i, size in enumerate(ln_emb):\n        lS_batch_lengths = []\n        lS_batch_indices = []\n        for _ in range(n):\n            # num of sparse indices to be used per embedding (between\n            if num_indices_per_lookup_fixed:\n                sparse_group_size = np.int32(num_indices_per_lookup)\n            else:\n                # random between [1,num_indices_per_lookup])\n                r = ra.random(1)\n                sparse_group_size = np.int32(\n                    max(1, np.round(r * min(size, num_indices_per_lookup))[0])\n                )\n            # sparse indices to be used per embedding\n            file_path = trace_file\n            line_accesses, list_sd, cumm_sd = read_dist_from_file(\n                file_path.replace(\"j\", str(i))\n            )\n            # debug print\n            # print('input')\n            # print(line_accesses); print(list_sd); print(cumm_sd);\n            # print(sparse_group_size)\n            # approach 1: rand\n            # r = trace_generate_rand(\n            #     line_accesses, list_sd, cumm_sd, sparse_group_size, enable_padding\n            # )\n            # approach 2: lru\n            r = trace_generate_lru(\n                line_accesses, list_sd, cumm_sd, sparse_group_size, enable_padding\n            )\n            # WARNING: if the distribution in the file is not consistent with\n            # embedding table dimensions, below mod guards against out of\n            # range access\n            sparse_group = np.unique(r).astype(np.int32)\n            minsg = np.min(sparse_group)\n            maxsg = np.max(sparse_group)\n            if (minsg < 0) or (size <= maxsg):\n                print(\n                    \"WARNING: distribution is inconsistent with embedding \"\n                    + \"table size (using mod to recover and continue)\"\n                )\n                sparse_group = np.mod(sparse_group, size).astype(np.int32)\n            # sparse_group = np.unique(np.array(np.mod(r, size-1)).astype(np.int32))\n            # reset sparse_group_size in case some index duplicates were removed\n            sparse_group_size = np.int32(sparse_group.size)\n            # store lengths and indices\n            lS_batch_lengths += [sparse_group_size]\n            lS_batch_indices += sparse_group.tolist()\n        lS_emb_lengths.append(lS_batch_lengths)\n        lS_emb_indices.append(lS_batch_indices)\n\n    return (Xt, lS_emb_lengths, lS_emb_indices)\n\n\ndef generate_stack_distance(cumm_val, cumm_dist, max_i, i, enable_padding=False):\n    u = ra.rand(1)\n    if i < max_i:\n        # only generate stack distances up to the number of new references seen so far\n        j = bisect.bisect(cumm_val, i) - 1\n        fi = cumm_dist[j]\n        u *= fi  # shrink distribution support to exclude last values\n    elif enable_padding:\n        # WARNING: disable generation of new references (once all have been seen)\n        fi = cumm_dist[0]\n        u = (1.0 - fi) * u + fi  # remap distribution support to exclude first value\n\n    for j, f in enumerate(cumm_dist):\n        if u <= f:\n            return cumm_val[j]\n\n\n# WARNING: global define, must be consistent across all synthetic functions\ncache_line_size = 1\n\n\ndef trace_generate_lru(\n    line_accesses, list_sd, cumm_sd, out_trace_len, enable_padding=False\n):\n    max_sd = list_sd[-1]\n    l = len(line_accesses)\n    i = 0\n    ztrace = []\n    for _ in range(out_trace_len):\n        sd = generate_stack_distance(list_sd, cumm_sd, max_sd, i, enable_padding)\n        mem_ref_within_line = 0  # floor(ra.rand(1)*cache_line_size) #0\n        # generate memory reference\n        if sd == 0:  # new reference #\n            line_ref = line_accesses.pop(0)\n            line_accesses.append(line_ref)\n            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n            i += 1\n        else:  # existing reference #\n            line_ref = line_accesses[l - sd]\n            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n            line_accesses.pop(l - sd)\n            line_accesses.append(line_ref)\n        # save generated memory reference\n        ztrace.append(mem_ref)\n\n    return ztrace\n\n\ndef trace_generate_rand(\n    line_accesses, list_sd, cumm_sd, out_trace_len, enable_padding=False\n):\n    max_sd = list_sd[-1]\n    l = len(line_accesses)  # !!!Unique,\n    i = 0\n    ztrace = []\n    for _ in range(out_trace_len):\n        sd = generate_stack_distance(list_sd, cumm_sd, max_sd, i, enable_padding)\n        mem_ref_within_line = 0  # floor(ra.rand(1)*cache_line_size) #0\n        # generate memory reference\n        if sd == 0:  # new reference #\n            line_ref = line_accesses.pop(0)\n            line_accesses.append(line_ref)\n            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n            i += 1\n        else:  # existing reference #\n            line_ref = line_accesses[l - sd]\n            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n        ztrace.append(mem_ref)\n\n    return ztrace\n\n\ndef trace_profile(trace, enable_padding=False):\n    # number of elements in the array (assuming 1D)\n    # n = trace.size\n\n    rstack = []  # S\n    stack_distances = []  # SDS\n    line_accesses = []  # L\n    for x in trace:\n        r = np.uint64(x / cache_line_size)\n        l = len(rstack)\n        try:  # found #\n            i = rstack.index(r)\n            # WARNING: I believe below is the correct depth in terms of meaning of the\n            #          algorithm, but that is not what seems to be in the paper alg.\n            #          -1 can be subtracted if we defined the distance between\n            #          consecutive accesses (e.g. r, r) as 0 rather than 1.\n            sd = l - i  # - 1\n            # push r to the end of stack_distances\n            stack_distances.insert(0, sd)\n            # remove r from its position and insert to the top of stack\n            rstack.pop(i)  # rstack.remove(r)\n            rstack.insert(l - 1, r)\n        except ValueError:  # not found #\n            sd = 0  # -1\n            # push r to the end of stack_distances/line_accesses\n            stack_distances.insert(0, sd)\n            line_accesses.insert(0, r)\n            # push r to the top of stack\n            rstack.insert(l, r)\n\n    if enable_padding:\n        # WARNING: notice that as the ratio between the number of samples (l)\n        # and cardinality [c] of a sample increases the probability of\n        # generating a sample gets smaller and smaller because there are\n        # few new samples compared to repeated samples. This means that for a\n        # long trace with relatively small cardinality it will take longer to\n        # generate all new samples and therefore obtain full distribution support\n        # and hence it takes longer for distribution to resemble the original.\n        # Therefore, we may pad the number of new samples to be on par with\n        # average number of samples l/c artificially.\n        l = len(stack_distances)\n        c = max(stack_distances)\n        padding = int(np.ceil(l / c))\n        stack_distances = stack_distances + [0] * padding\n\n    return (rstack, stack_distances, line_accesses)\n\n\n# auxiliary read/write routines\ndef read_trace_from_file(file_path):\n    try:\n        with open(file_path) as f:\n            if args.trace_file_binary_type:\n                array = np.fromfile(f, dtype=np.uint64)\n                trace = array.astype(np.uint64).tolist()\n            else:\n                line = f.readline()\n                trace = list(map(lambda x: np.uint64(x), line.split(\", \")))\n            return trace\n    except Exception:\n        print(\"ERROR: no input trace file has been provided\")\n\n\ndef write_trace_to_file(file_path, trace):\n    try:\n        if args.trace_file_binary_type:\n            with open(file_path, \"wb+\") as f:\n                np.array(trace).astype(np.uint64).tofile(f)\n        else:\n            with open(file_path, \"w+\") as f:\n                s = str(trace)\n                f.write(s[1 : len(s) - 1])\n    except Exception:\n        print(\"ERROR: no output trace file has been provided\")\n\n\ndef read_dist_from_file(file_path):\n    try:\n        with open(file_path, \"r\") as f:\n            lines = f.read().splitlines()\n    except Exception:\n        print(\"Wrong file or file path\")\n    # read unique accesses\n    unique_accesses = [int(el) for el in lines[0].split(\", \")]\n    # read cumulative distribution (elements are passed as two separate lists)\n    list_sd = [int(el) for el in lines[1].split(\", \")]\n    cumm_sd = [float(el) for el in lines[2].split(\", \")]\n\n    return unique_accesses, list_sd, cumm_sd\n\n\ndef write_dist_to_file(file_path, unique_accesses, list_sd, cumm_sd):\n    try:\n        with open(file_path, \"w\") as f:\n            # unique_acesses\n            s = str(unique_accesses)\n            f.write(s[1 : len(s) - 1] + \"\\n\")\n            # list_sd\n            s = str(list_sd)\n            f.write(s[1 : len(s) - 1] + \"\\n\")\n            # cumm_sd\n            s = str(cumm_sd)\n            f.write(s[1 : len(s) - 1] + \"\\n\")\n    except Exception:\n        print(\"Wrong file or file path\")\n\n\nif __name__ == \"__main__\":\n    import argparse\n    import operator\n    import sys\n\n    ### parse arguments ###\n    parser = argparse.ArgumentParser(description=\"Generate Synthetic Distributions\")\n    parser.add_argument(\"--trace-file\", type=str, default=\"./input/trace.log\")\n    parser.add_argument(\"--trace-file-binary-type\", type=bool, default=False)\n    parser.add_argument(\"--trace-enable-padding\", type=bool, default=False)\n    parser.add_argument(\"--dist-file\", type=str, default=\"./input/dist.log\")\n    parser.add_argument(\n        \"--synthetic-file\", type=str, default=\"./input/trace_synthetic.log\"\n    )\n    parser.add_argument(\"--numpy-rand-seed\", type=int, default=123)\n    parser.add_argument(\"--print-precision\", type=int, default=5)\n    args = parser.parse_args()\n\n    ### some basic setup ###\n    np.random.seed(args.numpy_rand_seed)\n    np.set_printoptions(precision=args.print_precision)\n\n    ### read trace ###\n    trace = read_trace_from_file(args.trace_file)\n    # print(trace)\n\n    ### profile trace ###\n    (_, stack_distances, line_accesses) = trace_profile(\n        trace, args.trace_enable_padding\n    )\n    stack_distances.reverse()\n    line_accesses.reverse()\n    # print(line_accesses)\n    # print(stack_distances)\n\n    ### compute probability distribution ###\n    # count items\n    l = len(stack_distances)\n    dc = sorted(\n        collections.Counter(stack_distances).items(), key=operator.itemgetter(0)\n    )\n\n    # create a distribution\n    list_sd = list(map(lambda tuple_x_k: tuple_x_k[0], dc))  # x = tuple_x_k[0]\n    dist_sd = list(\n        map(lambda tuple_x_k: tuple_x_k[1] / float(l), dc)\n    )  # k = tuple_x_k[1]\n    cumm_sd = []  # np.cumsum(dc).tolist() #prefixsum\n    for i, (_, k) in enumerate(dc):\n        if i == 0:\n            cumm_sd.append(k / float(l))\n        else:\n            # add the 2nd element of the i-th tuple in the dist_sd list\n            cumm_sd.append(cumm_sd[i - 1] + (k / float(l)))\n\n    ### write stack_distance and line_accesses to a file ###\n    write_dist_to_file(args.dist_file, line_accesses, list_sd, cumm_sd)\n\n    ### generate correspondinf synthetic ###\n    # line_accesses, list_sd, cumm_sd = read_dist_from_file(args.dist_file)\n    synthetic_trace = trace_generate_lru(\n        line_accesses, list_sd, cumm_sd, len(trace), args.trace_enable_padding\n    )\n    # synthetic_trace = trace_generate_rand(\n    #     line_accesses, list_sd, cumm_sd, len(trace), args.trace_enable_padding\n    # )\n    write_trace_to_file(args.synthetic_file, synthetic_trace)\n"
        },
        {
          "name": "dlrm_data_pytorch.py",
          "type": "blob",
          "size": 45.34765625,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Description: generate inputs and targets for the dlrm benchmark\n# The inputs and outputs are generated according to the following three option(s)\n# 1) random distribution\n# 2) synthetic distribution, based on unique accesses and distances between them\n#    i) R. Hassan, A. Harris, N. Topham and A. Efthymiou \"Synthetic Trace-Driven\n#    Simulation of Cache Memory\", IEEE AINAM'07\n# 3) public data set\n#    i)  Criteo Kaggle Display Advertising Challenge Dataset\n#    https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset\n#    ii) Criteo Terabyte Dataset\n#    https://labs.criteo.com/2013/12/download-terabyte-click-logs\n\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport bisect\nimport collections\nimport sys\nfrom collections import deque\n\n# others\nfrom os import path\n\nimport data_loader_terabyte\n\nimport data_utils\nimport mlperf_logger\n\n# numpy\nimport numpy as np\n\n# pytorch\nimport torch\nfrom numpy import random as ra\nfrom torch.utils.data import Dataset, RandomSampler\n\n\n# Kaggle Display Advertising Challenge Dataset\n# dataset (str): name of dataset (Kaggle or Terabyte)\n# randomize (str): determines randomization scheme\n#            \"none\": no randomization\n#            \"day\": randomizes each day\"s data (only works if split = True)\n#            \"total\": randomizes total dataset\n# split (bool) : to split into train, test, validation data-sets\nclass CriteoDataset(Dataset):\n    def __init__(\n        self,\n        dataset,\n        max_ind_range,\n        sub_sample_rate,\n        randomize,\n        split=\"train\",\n        raw_path=\"\",\n        pro_data=\"\",\n        memory_map=False,\n        dataset_multiprocessing=False,\n    ):\n        # dataset\n        # tar_fea = 1   # single target\n        den_fea = 13  # 13 dense  features\n        # spa_fea = 26  # 26 sparse features\n        # tad_fea = tar_fea + den_fea\n        # tot_fea = tad_fea + spa_fea\n        if dataset == \"kaggle\":\n            days = 7\n            out_file = \"kaggleAdDisplayChallenge_processed\"\n        elif dataset == \"terabyte\":\n            days = 24\n            out_file = \"terabyte_processed\"\n        else:\n            raise (ValueError(\"Data set option is not supported\"))\n        self.max_ind_range = max_ind_range\n        self.memory_map = memory_map\n\n        # split the datafile into path and filename\n        lstr = raw_path.split(\"/\")\n        self.d_path = \"/\".join(lstr[0:-1]) + \"/\"\n        self.d_file = lstr[-1].split(\".\")[0] if dataset == \"kaggle\" else lstr[-1]\n        self.npzfile = self.d_path + (\n            (self.d_file + \"_day\") if dataset == \"kaggle\" else self.d_file\n        )\n        self.trafile = self.d_path + (\n            (self.d_file + \"_fea\") if dataset == \"kaggle\" else \"fea\"\n        )\n\n        # check if pre-processed data is available\n        data_ready = True\n        if memory_map:\n            for i in range(days):\n                reo_data = self.npzfile + \"_{0}_reordered.npz\".format(i)\n                if not path.exists(str(reo_data)):\n                    data_ready = False\n        else:\n            if not path.exists(str(pro_data)):\n                data_ready = False\n\n        # pre-process data if needed\n        # WARNNING: when memory mapping is used we get a collection of files\n        if data_ready:\n            print(\"Reading pre-processed data=%s\" % (str(pro_data)))\n            file = str(pro_data)\n        else:\n            print(\"Reading raw data=%s\" % (str(raw_path)))\n            file = data_utils.getCriteoAdData(\n                raw_path,\n                out_file,\n                max_ind_range,\n                sub_sample_rate,\n                days,\n                split,\n                randomize,\n                dataset == \"kaggle\",\n                memory_map,\n                dataset_multiprocessing,\n            )\n\n        # get a number of samples per day\n        total_file = self.d_path + self.d_file + \"_day_count.npz\"\n        with np.load(total_file) as data:\n            total_per_file = data[\"total_per_file\"]\n        # compute offsets per file\n        self.offset_per_file = np.array([0] + [x for x in total_per_file])\n        for i in range(days):\n            self.offset_per_file[i + 1] += self.offset_per_file[i]\n        # print(self.offset_per_file)\n\n        # setup data\n        if memory_map:\n            # setup the training/testing split\n            self.split = split\n            if split == \"none\" or split == \"train\":\n                self.day = 0\n                self.max_day_range = days if split == \"none\" else days - 1\n            elif split == \"test\" or split == \"val\":\n                self.day = days - 1\n                num_samples = (\n                    self.offset_per_file[days] - self.offset_per_file[days - 1]\n                )\n                self.test_size = int(np.ceil(num_samples / 2.0))\n                self.val_size = num_samples - self.test_size\n            else:\n                sys.exit(\"ERROR: dataset split is neither none, nor train or test.\")\n\n            \"\"\"\n            # text\n            print(\"text\")\n            for i in range(days):\n                fi = self.npzfile + \"_{0}\".format(i)\n                with open(fi) as data:\n                    ttt = 0; nnn = 0\n                    for _j, line in enumerate(data):\n                        ttt +=1\n                        if np.int32(line[0]) > 0:\n                            nnn +=1\n                    print(\"day=\" + str(i) + \" total=\" + str(ttt) + \" non-zeros=\"\n                          + str(nnn) + \" ratio=\" +str((nnn * 100.) / ttt) + \"%\")\n            # processed\n            print(\"processed\")\n            for i in range(days):\n                fi = self.npzfile + \"_{0}_processed.npz\".format(i)\n                with np.load(fi) as data:\n                    yyy = data[\"y\"]\n                ttt = len(yyy)\n                nnn = np.count_nonzero(yyy)\n                print(\"day=\" + str(i) + \" total=\" + str(ttt) + \" non-zeros=\"\n                      + str(nnn) + \" ratio=\" +str((nnn * 100.) / ttt) + \"%\")\n            # reordered\n            print(\"reordered\")\n            for i in range(days):\n                fi = self.npzfile + \"_{0}_reordered.npz\".format(i)\n                with np.load(fi) as data:\n                    yyy = data[\"y\"]\n                ttt = len(yyy)\n                nnn = np.count_nonzero(yyy)\n                print(\"day=\" + str(i) + \" total=\" + str(ttt) + \" non-zeros=\"\n                      + str(nnn) + \" ratio=\" +str((nnn * 100.) / ttt) + \"%\")\n            \"\"\"\n\n            # load unique counts\n            with np.load(self.d_path + self.d_file + \"_fea_count.npz\") as data:\n                self.counts = data[\"counts\"]\n            self.m_den = den_fea  # X_int.shape[1]\n            self.n_emb = len(self.counts)\n            print(\"Sparse features= %d, Dense features= %d\" % (self.n_emb, self.m_den))\n\n            # Load the test data\n            # Only a single day is used for testing\n            if self.split == \"test\" or self.split == \"val\":\n                # only a single day is used for testing\n                fi = self.npzfile + \"_{0}_reordered.npz\".format(self.day)\n                with np.load(fi) as data:\n                    self.X_int = data[\"X_int\"]  # continuous  feature\n                    self.X_cat = data[\"X_cat\"]  # categorical feature\n                    self.y = data[\"y\"]  # target\n\n        else:\n            # load and preprocess data\n            with np.load(file) as data:\n                X_int = data[\"X_int\"]  # continuous  feature\n                X_cat = data[\"X_cat\"]  # categorical feature\n                y = data[\"y\"]  # target\n                self.counts = data[\"counts\"]\n            self.m_den = X_int.shape[1]  # den_fea\n            self.n_emb = len(self.counts)\n            print(\"Sparse fea = %d, Dense fea = %d\" % (self.n_emb, self.m_den))\n\n            # create reordering\n            indices = np.arange(len(y))\n\n            if split == \"none\":\n                # randomize all data\n                if randomize == \"total\":\n                    indices = np.random.permutation(indices)\n                    print(\"Randomized indices...\")\n\n                X_int[indices] = X_int\n                X_cat[indices] = X_cat\n                y[indices] = y\n\n            else:\n                indices = np.array_split(indices, self.offset_per_file[1:-1])\n\n                # randomize train data (per day)\n                if randomize == \"day\":  # or randomize == \"total\":\n                    for i in range(len(indices) - 1):\n                        indices[i] = np.random.permutation(indices[i])\n                    print(\"Randomized indices per day ...\")\n\n                train_indices = np.concatenate(indices[:-1])\n                test_indices = indices[-1]\n                test_indices, val_indices = np.array_split(test_indices, 2)\n\n                print(\"Defined %s indices...\" % (split))\n\n                # randomize train data (across days)\n                if randomize == \"total\":\n                    train_indices = np.random.permutation(train_indices)\n                    print(\"Randomized indices across days ...\")\n\n                # create training, validation, and test sets\n                if split == \"train\":\n                    self.X_int = [X_int[i] for i in train_indices]\n                    self.X_cat = [X_cat[i] for i in train_indices]\n                    self.y = [y[i] for i in train_indices]\n                elif split == \"val\":\n                    self.X_int = [X_int[i] for i in val_indices]\n                    self.X_cat = [X_cat[i] for i in val_indices]\n                    self.y = [y[i] for i in val_indices]\n                elif split == \"test\":\n                    self.X_int = [X_int[i] for i in test_indices]\n                    self.X_cat = [X_cat[i] for i in test_indices]\n                    self.y = [y[i] for i in test_indices]\n\n            print(\"Split data according to indices...\")\n\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            return [\n                self[idx]\n                for idx in range(\n                    index.start or 0, index.stop or len(self), index.step or 1\n                )\n            ]\n\n        if self.memory_map:\n            if self.split == \"none\" or self.split == \"train\":\n                # check if need to swicth to next day and load data\n                if index == self.offset_per_file[self.day]:\n                    # print(\"day_boundary switch\", index)\n                    self.day_boundary = self.offset_per_file[self.day]\n                    fi = self.npzfile + \"_{0}_reordered.npz\".format(self.day)\n                    # print('Loading file: ', fi)\n                    with np.load(fi) as data:\n                        self.X_int = data[\"X_int\"]  # continuous  feature\n                        self.X_cat = data[\"X_cat\"]  # categorical feature\n                        self.y = data[\"y\"]  # target\n                    self.day = (self.day + 1) % self.max_day_range\n\n                i = index - self.day_boundary\n            elif self.split == \"test\" or self.split == \"val\":\n                # only a single day is used for testing\n                i = index + (0 if self.split == \"test\" else self.test_size)\n            else:\n                sys.exit(\"ERROR: dataset split is neither none, nor train or test.\")\n        else:\n            i = index\n\n        if self.max_ind_range > 0:\n            return self.X_int[i], self.X_cat[i] % self.max_ind_range, self.y[i]\n        else:\n            return self.X_int[i], self.X_cat[i], self.y[i]\n\n    def _default_preprocess(self, X_int, X_cat, y):\n        X_int = torch.log(torch.tensor(X_int, dtype=torch.float) + 1)\n        if self.max_ind_range > 0:\n            X_cat = torch.tensor(X_cat % self.max_ind_range, dtype=torch.long)\n        else:\n            X_cat = torch.tensor(X_cat, dtype=torch.long)\n        y = torch.tensor(y.astype(np.float32))\n\n        return X_int, X_cat, y\n\n    def __len__(self):\n        if self.memory_map:\n            if self.split == \"none\":\n                return self.offset_per_file[-1]\n            elif self.split == \"train\":\n                return self.offset_per_file[-2]\n            elif self.split == \"test\":\n                return self.test_size\n            elif self.split == \"val\":\n                return self.val_size\n            else:\n                sys.exit(\"ERROR: dataset split is neither none, nor train nor test.\")\n        else:\n            return len(self.y)\n\n\ndef collate_wrapper_criteo_offset(list_of_tuples):\n    # where each tuple is (X_int, X_cat, y)\n    transposed_data = list(zip(*list_of_tuples))\n    X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)\n    X_cat = torch.tensor(transposed_data[1], dtype=torch.long)\n    T = torch.tensor(transposed_data[2], dtype=torch.float32).view(-1, 1)\n\n    batchSize = X_cat.shape[0]\n    featureCnt = X_cat.shape[1]\n\n    lS_i = [X_cat[:, i] for i in range(featureCnt)]\n    lS_o = [torch.tensor(range(batchSize)) for _ in range(featureCnt)]\n\n    return X_int, torch.stack(lS_o), torch.stack(lS_i), T\n\n\ndef ensure_dataset_preprocessed(args, d_path):\n    _ = CriteoDataset(\n        args.data_set,\n        args.max_ind_range,\n        args.data_sub_sample_rate,\n        args.data_randomize,\n        \"train\",\n        args.raw_data_file,\n        args.processed_data_file,\n        args.memory_map,\n        args.dataset_multiprocessing,\n    )\n\n    _ = CriteoDataset(\n        args.data_set,\n        args.max_ind_range,\n        args.data_sub_sample_rate,\n        args.data_randomize,\n        \"test\",\n        args.raw_data_file,\n        args.processed_data_file,\n        args.memory_map,\n        args.dataset_multiprocessing,\n    )\n\n    for split in [\"train\", \"val\", \"test\"]:\n        print(\"Running preprocessing for split =\", split)\n\n        train_files = [\n            \"{}_{}_reordered.npz\".format(args.raw_data_file, day)\n            for day in range(0, 23)\n        ]\n\n        test_valid_file = args.raw_data_file + \"_23_reordered.npz\"\n\n        output_file = d_path + \"_{}.bin\".format(split)\n\n        input_files = train_files if split == \"train\" else [test_valid_file]\n        data_loader_terabyte.numpy_to_binary(\n            input_files=input_files, output_file_path=output_file, split=split\n        )\n\n\n# Conversion from offset to length\ndef offset_to_length_converter(lS_o, lS_i):\n    def diff(tensor):\n        return tensor[1:] - tensor[:-1]\n\n    return torch.stack(\n        [\n            diff(torch.cat((S_o, torch.tensor(lS_i[ind].shape))).int())\n            for ind, S_o in enumerate(lS_o)\n        ]\n    )\n\n\ndef collate_wrapper_criteo_length(list_of_tuples):\n    # where each tuple is (X_int, X_cat, y)\n    transposed_data = list(zip(*list_of_tuples))\n    X_int = torch.log(torch.tensor(transposed_data[0], dtype=torch.float) + 1)\n    X_cat = torch.tensor(transposed_data[1], dtype=torch.long)\n    T = torch.tensor(transposed_data[2], dtype=torch.float32).view(-1, 1)\n\n    batchSize = X_cat.shape[0]\n    featureCnt = X_cat.shape[1]\n\n    lS_i = torch.stack([X_cat[:, i] for i in range(featureCnt)])\n    lS_o = torch.stack([torch.tensor(range(batchSize)) for _ in range(featureCnt)])\n\n    lS_l = offset_to_length_converter(lS_o, lS_i)\n\n    return X_int, lS_l, lS_i, T\n\n\ndef make_criteo_data_and_loaders(args, offset_to_length_converter=False):\n    if args.mlperf_logging and args.memory_map and args.data_set == \"terabyte\":\n        # more efficient for larger batches\n        data_directory = path.dirname(args.raw_data_file)\n\n        if args.mlperf_bin_loader:\n            lstr = args.processed_data_file.split(\"/\")\n            d_path = \"/\".join(lstr[0:-1]) + \"/\" + lstr[-1].split(\".\")[0]\n            train_file = d_path + \"_train.bin\"\n            test_file = d_path + \"_test.bin\"\n            # val_file = d_path + \"_val.bin\"\n            counts_file = args.raw_data_file + \"_fea_count.npz\"\n\n            if any(not path.exists(p) for p in [train_file, test_file, counts_file]):\n                ensure_dataset_preprocessed(args, d_path)\n\n            train_data = data_loader_terabyte.CriteoBinDataset(\n                data_file=train_file,\n                counts_file=counts_file,\n                batch_size=args.mini_batch_size,\n                max_ind_range=args.max_ind_range,\n            )\n\n            mlperf_logger.log_event(\n                key=mlperf_logger.constants.TRAIN_SAMPLES, value=train_data.num_samples\n            )\n\n            train_loader = torch.utils.data.DataLoader(\n                train_data,\n                batch_size=None,\n                batch_sampler=None,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=None,\n                pin_memory=False,\n                drop_last=False,\n                sampler=RandomSampler(train_data) if args.mlperf_bin_shuffle else None,\n            )\n\n            test_data = data_loader_terabyte.CriteoBinDataset(\n                data_file=test_file,\n                counts_file=counts_file,\n                batch_size=args.test_mini_batch_size,\n                max_ind_range=args.max_ind_range,\n            )\n\n            mlperf_logger.log_event(\n                key=mlperf_logger.constants.EVAL_SAMPLES, value=test_data.num_samples\n            )\n\n            test_loader = torch.utils.data.DataLoader(\n                test_data,\n                batch_size=None,\n                batch_sampler=None,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=None,\n                pin_memory=False,\n                drop_last=False,\n            )\n        else:\n            data_filename = args.raw_data_file.split(\"/\")[-1]\n\n            train_data = CriteoDataset(\n                args.data_set,\n                args.max_ind_range,\n                args.data_sub_sample_rate,\n                args.data_randomize,\n                \"train\",\n                args.raw_data_file,\n                args.processed_data_file,\n                args.memory_map,\n                args.dataset_multiprocessing,\n            )\n\n            test_data = CriteoDataset(\n                args.data_set,\n                args.max_ind_range,\n                args.data_sub_sample_rate,\n                args.data_randomize,\n                \"test\",\n                args.raw_data_file,\n                args.processed_data_file,\n                args.memory_map,\n                args.dataset_multiprocessing,\n            )\n\n            train_loader = data_loader_terabyte.DataLoader(\n                data_directory=data_directory,\n                data_filename=data_filename,\n                days=list(range(23)),\n                batch_size=args.mini_batch_size,\n                max_ind_range=args.max_ind_range,\n                split=\"train\",\n            )\n\n            test_loader = data_loader_terabyte.DataLoader(\n                data_directory=data_directory,\n                data_filename=data_filename,\n                days=[23],\n                batch_size=args.test_mini_batch_size,\n                max_ind_range=args.max_ind_range,\n                split=\"test\",\n            )\n    else:\n        train_data = CriteoDataset(\n            args.data_set,\n            args.max_ind_range,\n            args.data_sub_sample_rate,\n            args.data_randomize,\n            \"train\",\n            args.raw_data_file,\n            args.processed_data_file,\n            args.memory_map,\n            args.dataset_multiprocessing,\n        )\n\n        test_data = CriteoDataset(\n            args.data_set,\n            args.max_ind_range,\n            args.data_sub_sample_rate,\n            args.data_randomize,\n            \"test\",\n            args.raw_data_file,\n            args.processed_data_file,\n            args.memory_map,\n            args.dataset_multiprocessing,\n        )\n\n        collate_wrapper_criteo = collate_wrapper_criteo_offset\n        if offset_to_length_converter:\n            collate_wrapper_criteo = collate_wrapper_criteo_length\n\n        train_loader = torch.utils.data.DataLoader(\n            train_data,\n            batch_size=args.mini_batch_size,\n            shuffle=False,\n            num_workers=args.num_workers,\n            collate_fn=collate_wrapper_criteo,\n            pin_memory=False,\n            drop_last=False,  # True\n        )\n\n        test_loader = torch.utils.data.DataLoader(\n            test_data,\n            batch_size=args.test_mini_batch_size,\n            shuffle=False,\n            num_workers=args.test_num_workers,\n            collate_fn=collate_wrapper_criteo,\n            pin_memory=False,\n            drop_last=False,  # True\n        )\n\n    return train_data, train_loader, test_data, test_loader\n\n\n# uniform ditribution (input data)\nclass RandomDataset(Dataset):\n    def __init__(\n        self,\n        m_den,\n        ln_emb,\n        data_size,\n        num_batches,\n        mini_batch_size,\n        num_indices_per_lookup,\n        num_indices_per_lookup_fixed,\n        num_targets=1,\n        round_targets=False,\n        data_generation=\"random\",\n        trace_file=\"\",\n        enable_padding=False,\n        reset_seed_on_access=False,\n        rand_data_dist=\"uniform\",\n        rand_data_min=1,\n        rand_data_max=1,\n        rand_data_mu=-1,\n        rand_data_sigma=1,\n        rand_seed=0,\n    ):\n        # compute batch size\n        nbatches = int(np.ceil((data_size * 1.0) / mini_batch_size))\n        if num_batches != 0:\n            nbatches = num_batches\n            data_size = nbatches * mini_batch_size\n            # print(\"Total number of batches %d\" % nbatches)\n\n        # save args (recompute data_size if needed)\n        self.m_den = m_den\n        self.ln_emb = ln_emb\n        self.data_size = data_size\n        self.num_batches = nbatches\n        self.mini_batch_size = mini_batch_size\n        self.num_indices_per_lookup = num_indices_per_lookup\n        self.num_indices_per_lookup_fixed = num_indices_per_lookup_fixed\n        self.num_targets = num_targets\n        self.round_targets = round_targets\n        self.data_generation = data_generation\n        self.trace_file = trace_file\n        self.enable_padding = enable_padding\n        self.reset_seed_on_access = reset_seed_on_access\n        self.rand_seed = rand_seed\n        self.rand_data_dist = rand_data_dist\n        self.rand_data_min = rand_data_min\n        self.rand_data_max = rand_data_max\n        self.rand_data_mu = rand_data_mu\n        self.rand_data_sigma = rand_data_sigma\n\n    def reset_numpy_seed(self, numpy_rand_seed):\n        np.random.seed(numpy_rand_seed)\n        # torch.manual_seed(numpy_rand_seed)\n\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            return [\n                self[idx]\n                for idx in range(\n                    index.start or 0, index.stop or len(self), index.step or 1\n                )\n            ]\n\n        # WARNING: reset seed on access to first element\n        # (e.g. if same random samples needed across epochs)\n        if self.reset_seed_on_access and index == 0:\n            self.reset_numpy_seed(self.rand_seed)\n\n        # number of data points in a batch\n        n = min(self.mini_batch_size, self.data_size - (index * self.mini_batch_size))\n\n        # generate a batch of dense and sparse features\n        if self.data_generation == \"random\":\n            (X, lS_o, lS_i) = generate_dist_input_batch(\n                self.m_den,\n                self.ln_emb,\n                n,\n                self.num_indices_per_lookup,\n                self.num_indices_per_lookup_fixed,\n                rand_data_dist=self.rand_data_dist,\n                rand_data_min=self.rand_data_min,\n                rand_data_max=self.rand_data_max,\n                rand_data_mu=self.rand_data_mu,\n                rand_data_sigma=self.rand_data_sigma,\n            )\n        elif self.data_generation == \"synthetic\":\n            (X, lS_o, lS_i) = generate_synthetic_input_batch(\n                self.m_den,\n                self.ln_emb,\n                n,\n                self.num_indices_per_lookup,\n                self.num_indices_per_lookup_fixed,\n                self.trace_file,\n                self.enable_padding,\n            )\n        else:\n            sys.exit(\n                \"ERROR: --data-generation=\" + self.data_generation + \" is not supported\"\n            )\n\n        # generate a batch of target (probability of a click)\n        T = generate_random_output_batch(n, self.num_targets, self.round_targets)\n\n        return (X, lS_o, lS_i, T)\n\n    def __len__(self):\n        # WARNING: note that we produce bacthes of outputs in __getitem__\n        # therefore we should use num_batches rather than data_size below\n        return self.num_batches\n\n\ndef collate_wrapper_random_offset(list_of_tuples):\n    # where each tuple is (X, lS_o, lS_i, T)\n    (X, lS_o, lS_i, T) = list_of_tuples[0]\n    return (X, torch.stack(lS_o), lS_i, T)\n\n\ndef collate_wrapper_random_length(list_of_tuples):\n    # where each tuple is (X, lS_o, lS_i, T)\n    (X, lS_o, lS_i, T) = list_of_tuples[0]\n    return (X, offset_to_length_converter(torch.stack(lS_o), lS_i), lS_i, T)\n\n\ndef make_random_data_and_loader(\n    args,\n    ln_emb,\n    m_den,\n    offset_to_length_converter=False,\n):\n    train_data = RandomDataset(\n        m_den,\n        ln_emb,\n        args.data_size,\n        args.num_batches,\n        args.mini_batch_size,\n        args.num_indices_per_lookup,\n        args.num_indices_per_lookup_fixed,\n        1,  # num_targets\n        args.round_targets,\n        args.data_generation,\n        args.data_trace_file,\n        args.data_trace_enable_padding,\n        reset_seed_on_access=True,\n        rand_data_dist=args.rand_data_dist,\n        rand_data_min=args.rand_data_min,\n        rand_data_max=args.rand_data_max,\n        rand_data_mu=args.rand_data_mu,\n        rand_data_sigma=args.rand_data_sigma,\n        rand_seed=args.numpy_rand_seed,\n    )  # WARNING: generates a batch of lookups at once\n\n    test_data = RandomDataset(\n        m_den,\n        ln_emb,\n        args.data_size,\n        args.num_batches,\n        args.mini_batch_size,\n        args.num_indices_per_lookup,\n        args.num_indices_per_lookup_fixed,\n        1,  # num_targets\n        args.round_targets,\n        args.data_generation,\n        args.data_trace_file,\n        args.data_trace_enable_padding,\n        reset_seed_on_access=True,\n        rand_data_dist=args.rand_data_dist,\n        rand_data_min=args.rand_data_min,\n        rand_data_max=args.rand_data_max,\n        rand_data_mu=args.rand_data_mu,\n        rand_data_sigma=args.rand_data_sigma,\n        rand_seed=args.numpy_rand_seed,\n    )\n\n    collate_wrapper_random = collate_wrapper_random_offset\n    if offset_to_length_converter:\n        collate_wrapper_random = collate_wrapper_random_length\n\n    train_loader = torch.utils.data.DataLoader(\n        train_data,\n        batch_size=1,\n        shuffle=False,\n        num_workers=args.num_workers,\n        collate_fn=collate_wrapper_random,\n        pin_memory=False,\n        drop_last=False,  # True\n    )\n\n    test_loader = torch.utils.data.DataLoader(\n        test_data,\n        batch_size=1,\n        shuffle=False,\n        num_workers=args.num_workers,\n        collate_fn=collate_wrapper_random,\n        pin_memory=False,\n        drop_last=False,  # True\n    )\n    return train_data, train_loader, test_data, test_loader\n\n\ndef generate_random_data(\n    m_den,\n    ln_emb,\n    data_size,\n    num_batches,\n    mini_batch_size,\n    num_indices_per_lookup,\n    num_indices_per_lookup_fixed,\n    num_targets=1,\n    round_targets=False,\n    data_generation=\"random\",\n    trace_file=\"\",\n    enable_padding=False,\n    length=False,  # length for caffe2 version (except dlrm_s_caffe2)\n):\n    nbatches = int(np.ceil((data_size * 1.0) / mini_batch_size))\n    if num_batches != 0:\n        nbatches = num_batches\n        data_size = nbatches * mini_batch_size\n    # print(\"Total number of batches %d\" % nbatches)\n\n    # inputs\n    lT = []\n    lX = []\n    lS_offsets = []\n    lS_indices = []\n    for j in range(0, nbatches):\n        # number of data points in a batch\n        n = min(mini_batch_size, data_size - (j * mini_batch_size))\n\n        # generate a batch of dense and sparse features\n        if data_generation == \"random\":\n            (Xt, lS_emb_offsets, lS_emb_indices) = generate_uniform_input_batch(\n                m_den,\n                ln_emb,\n                n,\n                num_indices_per_lookup,\n                num_indices_per_lookup_fixed,\n                length,\n            )\n        elif data_generation == \"synthetic\":\n            (Xt, lS_emb_offsets, lS_emb_indices) = generate_synthetic_input_batch(\n                m_den,\n                ln_emb,\n                n,\n                num_indices_per_lookup,\n                num_indices_per_lookup_fixed,\n                trace_file,\n                enable_padding,\n            )\n        else:\n            sys.exit(\n                \"ERROR: --data-generation=\" + data_generation + \" is not supported\"\n            )\n        # dense feature\n        lX.append(Xt)\n        # sparse feature (sparse indices)\n        lS_offsets.append(lS_emb_offsets)\n        lS_indices.append(lS_emb_indices)\n\n        # generate a batch of target (probability of a click)\n        P = generate_random_output_batch(n, num_targets, round_targets)\n        lT.append(P)\n\n    return (nbatches, lX, lS_offsets, lS_indices, lT)\n\n\ndef generate_random_output_batch(n, num_targets, round_targets=False):\n    # target (probability of a click)\n    if round_targets:\n        P = np.round(ra.rand(n, num_targets).astype(np.float32)).astype(np.float32)\n    else:\n        P = ra.rand(n, num_targets).astype(np.float32)\n\n    return torch.tensor(P)\n\n\n# uniform ditribution (input data)\ndef generate_uniform_input_batch(\n    m_den,\n    ln_emb,\n    n,\n    num_indices_per_lookup,\n    num_indices_per_lookup_fixed,\n    length,\n):\n    # dense feature\n    Xt = torch.tensor(ra.rand(n, m_den).astype(np.float32))\n\n    # sparse feature (sparse indices)\n    lS_emb_offsets = []\n    lS_emb_indices = []\n    # for each embedding generate a list of n lookups,\n    # where each lookup is composed of multiple sparse indices\n    for size in ln_emb:\n        lS_batch_offsets = []\n        lS_batch_indices = []\n        offset = 0\n        for _ in range(n):\n            # num of sparse indices to be used per embedding (between\n            if num_indices_per_lookup_fixed:\n                sparse_group_size = np.int64(num_indices_per_lookup)\n            else:\n                # random between [1,num_indices_per_lookup])\n                r = ra.random(1)\n                sparse_group_size = np.int64(\n                    np.round(max([1.0], r * min(size, num_indices_per_lookup)))\n                )\n            # sparse indices to be used per embedding\n            r = ra.random(sparse_group_size)\n            sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))\n            # reset sparse_group_size in case some index duplicates were removed\n            sparse_group_size = np.int32(sparse_group.size)\n            # store lengths and indices\n            if length:  # for caffe2 version\n                lS_batch_offsets += [sparse_group_size]\n            else:\n                lS_batch_offsets += [offset]\n            lS_batch_indices += sparse_group.tolist()\n            # update offset for next iteration\n            offset += sparse_group_size\n        lS_emb_offsets.append(torch.tensor(lS_batch_offsets))\n        lS_emb_indices.append(torch.tensor(lS_batch_indices))\n\n    return (Xt, lS_emb_offsets, lS_emb_indices)\n\n\n# random data from uniform or gaussian ditribution (input data)\ndef generate_dist_input_batch(\n    m_den,\n    ln_emb,\n    n,\n    num_indices_per_lookup,\n    num_indices_per_lookup_fixed,\n    rand_data_dist,\n    rand_data_min,\n    rand_data_max,\n    rand_data_mu,\n    rand_data_sigma,\n):\n    # dense feature\n    Xt = torch.tensor(ra.rand(n, m_den).astype(np.float32))\n\n    # sparse feature (sparse indices)\n    lS_emb_offsets = []\n    lS_emb_indices = []\n    # for each embedding generate a list of n lookups,\n    # where each lookup is composed of multiple sparse indices\n    for size in ln_emb:\n        lS_batch_offsets = []\n        lS_batch_indices = []\n        offset = 0\n        for _ in range(n):\n            # num of sparse indices to be used per embedding (between\n            if num_indices_per_lookup_fixed:\n                sparse_group_size = np.int64(num_indices_per_lookup)\n            else:\n                # random between [1,num_indices_per_lookup])\n                r = ra.random(1)\n                sparse_group_size = np.int64(\n                    np.round(max([1.0], r * min(size, num_indices_per_lookup)))\n                )\n            # sparse indices to be used per embedding\n            if rand_data_dist == \"gaussian\":\n                if rand_data_mu == -1:\n                    rand_data_mu = (rand_data_max + rand_data_min) / 2.0\n                r = ra.normal(rand_data_mu, rand_data_sigma, sparse_group_size)\n                sparse_group = np.clip(r, rand_data_min, rand_data_max)\n                sparse_group = np.unique(sparse_group).astype(np.int64)\n            elif rand_data_dist == \"uniform\":\n                r = ra.random(sparse_group_size)\n                sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))\n            else:\n                raise (\n                    rand_data_dist,\n                    \"distribution is not supported. \\\n                     please select uniform or gaussian\",\n                )\n\n            # reset sparse_group_size in case some index duplicates were removed\n            sparse_group_size = np.int64(sparse_group.size)\n            # store lengths and indices\n            lS_batch_offsets += [offset]\n            lS_batch_indices += sparse_group.tolist()\n            # update offset for next iteration\n            offset += sparse_group_size\n        lS_emb_offsets.append(torch.tensor(lS_batch_offsets))\n        lS_emb_indices.append(torch.tensor(lS_batch_indices))\n\n    return (Xt, lS_emb_offsets, lS_emb_indices)\n\n\n# synthetic distribution (input data)\ndef generate_synthetic_input_batch(\n    m_den,\n    ln_emb,\n    n,\n    num_indices_per_lookup,\n    num_indices_per_lookup_fixed,\n    trace_file,\n    enable_padding=False,\n):\n    # dense feature\n    Xt = torch.tensor(ra.rand(n, m_den).astype(np.float32))\n\n    # sparse feature (sparse indices)\n    lS_emb_offsets = []\n    lS_emb_indices = []\n    # for each embedding generate a list of n lookups,\n    # where each lookup is composed of multiple sparse indices\n    for i, size in enumerate(ln_emb):\n        lS_batch_offsets = []\n        lS_batch_indices = []\n        offset = 0\n        for _ in range(n):\n            # num of sparse indices to be used per embedding (between\n            if num_indices_per_lookup_fixed:\n                sparse_group_size = np.int64(num_indices_per_lookup)\n            else:\n                # random between [1,num_indices_per_lookup])\n                r = ra.random(1)\n                sparse_group_size = np.int64(\n                    max(1, np.round(r * min(size, num_indices_per_lookup))[0])\n                )\n            # sparse indices to be used per embedding\n            file_path = trace_file\n            line_accesses, list_sd, cumm_sd = read_dist_from_file(\n                file_path.replace(\"j\", str(i))\n            )\n            # debug prints\n            # print(\"input\")\n            # print(line_accesses); print(list_sd); print(cumm_sd);\n            # print(sparse_group_size)\n            # approach 1: rand\n            # r = trace_generate_rand(\n            #     line_accesses, list_sd, cumm_sd, sparse_group_size, enable_padding\n            # )\n            # approach 2: lru\n            r = trace_generate_lru(\n                line_accesses, list_sd, cumm_sd, sparse_group_size, enable_padding\n            )\n            # WARNING: if the distribution in the file is not consistent\n            # with embedding table dimensions, below mod guards against out\n            # of range access\n            sparse_group = np.unique(r).astype(np.int64)\n            minsg = np.min(sparse_group)\n            maxsg = np.max(sparse_group)\n            if (minsg < 0) or (size <= maxsg):\n                print(\n                    \"WARNING: distribution is inconsistent with embedding \"\n                    + \"table size (using mod to recover and continue)\"\n                )\n                sparse_group = np.mod(sparse_group, size).astype(np.int64)\n            # sparse_group = np.unique(np.array(np.mod(r, size-1)).astype(np.int64))\n            # reset sparse_group_size in case some index duplicates were removed\n            sparse_group_size = np.int64(sparse_group.size)\n            # store lengths and indices\n            lS_batch_offsets += [offset]\n            lS_batch_indices += sparse_group.tolist()\n            # update offset for next iteration\n            offset += sparse_group_size\n        lS_emb_offsets.append(torch.tensor(lS_batch_offsets))\n        lS_emb_indices.append(torch.tensor(lS_batch_indices))\n\n    return (Xt, lS_emb_offsets, lS_emb_indices)\n\n\ndef generate_stack_distance(cumm_val, cumm_dist, max_i, i, enable_padding=False):\n    u = ra.rand(1)\n    if i < max_i:\n        # only generate stack distances up to the number of new references seen so far\n        j = bisect.bisect(cumm_val, i) - 1\n        fi = cumm_dist[j]\n        u *= fi  # shrink distribution support to exclude last values\n    elif enable_padding:\n        # WARNING: disable generation of new references (once all have been seen)\n        fi = cumm_dist[0]\n        u = (1.0 - fi) * u + fi  # remap distribution support to exclude first value\n\n    for j, f in enumerate(cumm_dist):\n        if u <= f:\n            return cumm_val[j]\n\n\n# WARNING: global define, must be consistent across all synthetic functions\ncache_line_size = 1\n\n\ndef trace_generate_lru(\n    line_accesses, list_sd, cumm_sd, out_trace_len, enable_padding=False\n):\n    max_sd = list_sd[-1]\n    l = len(line_accesses)\n    i = 0\n    ztrace = deque()\n    for _ in range(out_trace_len):\n        sd = generate_stack_distance(list_sd, cumm_sd, max_sd, i, enable_padding)\n        mem_ref_within_line = 0  # floor(ra.rand(1)*cache_line_size) #0\n\n        # generate memory reference\n        if sd == 0:  # new reference #\n            line_ref = line_accesses[0]\n            del line_accesses[0]\n            line_accesses.append(line_ref)\n            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n            i += 1\n        else:  # existing reference #\n            line_ref = line_accesses[l - sd]\n            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n            del line_accesses[l - sd]\n            line_accesses.append(line_ref)\n        # save generated memory reference\n        ztrace.append(mem_ref)\n\n    return ztrace\n\n\ndef trace_generate_rand(\n    line_accesses, list_sd, cumm_sd, out_trace_len, enable_padding=False\n):\n    max_sd = list_sd[-1]\n    l = len(line_accesses)  # !!!Unique,\n    i = 0\n    ztrace = []\n    for _ in range(out_trace_len):\n        sd = generate_stack_distance(list_sd, cumm_sd, max_sd, i, enable_padding)\n        mem_ref_within_line = 0  # floor(ra.rand(1)*cache_line_size) #0\n        # generate memory reference\n        if sd == 0:  # new reference #\n            line_ref = line_accesses.pop(0)\n            line_accesses.append(line_ref)\n            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n            i += 1\n        else:  # existing reference #\n            line_ref = line_accesses[l - sd]\n            mem_ref = np.uint64(line_ref * cache_line_size + mem_ref_within_line)\n        ztrace.append(mem_ref)\n\n    return ztrace\n\n\ndef trace_profile(trace, enable_padding=False):\n    # number of elements in the array (assuming 1D)\n    # n = trace.size\n\n    rstack = deque()  # S\n    stack_distances = deque()  # SDS\n    line_accesses = deque()  # L\n    for x in trace:\n        r = np.uint64(x / cache_line_size)\n        l = len(rstack)\n        try:  # found #\n            i = rstack.index(r)\n            # WARNING: I believe below is the correct depth in terms of meaning of the\n            #          algorithm, but that is not what seems to be in the paper alg.\n            #          -1 can be subtracted if we defined the distance between\n            #          consecutive accesses (e.g. r, r) as 0 rather than 1.\n            sd = l - i  # - 1\n            # push r to the end of stack_distances\n            stack_distances.appendleft(sd)\n            # remove r from its position and insert to the top of stack\n            del rstack[i]  # rstack.remove(r)\n            rstack.append(r)\n        except ValueError:  # not found #\n            sd = 0  # -1\n            # push r to the end of stack_distances/line_accesses\n            stack_distances.appendleft(sd)\n            line_accesses.appendleft(r)\n            # push r to the top of stack\n            rstack.append(r)\n\n    if enable_padding:\n        # WARNING: notice that as the ratio between the number of samples (l)\n        # and cardinality [c] of a sample increases the probability of\n        # generating a sample gets smaller and smaller because there are\n        # few new samples compared to repeated samples. This means that for a\n        # long trace with relatively small cardinality it will take longer to\n        # generate all new samples and therefore obtain full distribution support\n        # and hence it takes longer for distribution to resemble the original.\n        # Therefore, we may pad the number of new samples to be on par with\n        # average number of samples l/c artificially.\n        l = len(stack_distances)\n        c = max(stack_distances)\n        padding = int(np.ceil(l / c))\n        stack_distances = stack_distances + [0] * padding\n\n    return (rstack, stack_distances, line_accesses)\n\n\n# auxiliary read/write routines\ndef read_trace_from_file(file_path):\n    try:\n        with open(file_path) as f:\n            if args.trace_file_binary_type:\n                array = np.fromfile(f, dtype=np.uint64)\n                trace = array.astype(np.uint64).tolist()\n            else:\n                line = f.readline()\n                trace = list(map(lambda x: np.uint64(x), line.split(\", \")))\n            return trace\n    except Exception:\n        print(f\"ERROR: trace file '{file_path}' is not available.\")\n\n\ndef write_trace_to_file(file_path, trace):\n    try:\n        if args.trace_file_binary_type:\n            with open(file_path, \"wb+\") as f:\n                np.array(trace).astype(np.uint64).tofile(f)\n        else:\n            with open(file_path, \"w+\") as f:\n                s = str(list(trace))\n                f.write(s[1 : len(s) - 1])\n    except Exception:\n        print(\"ERROR: no output trace file has been provided\")\n\n\ndef read_dist_from_file(file_path):\n    try:\n        with open(file_path, \"r\") as f:\n            lines = f.read().splitlines()\n    except Exception:\n        print(\"{file_path} Wrong file or file path\")\n    # read unique accesses\n    unique_accesses = [int(el) for el in lines[0].split(\", \")]\n    # read cumulative distribution (elements are passed as two separate lists)\n    list_sd = [int(el) for el in lines[1].split(\", \")]\n    cumm_sd = [float(el) for el in lines[2].split(\", \")]\n\n    return unique_accesses, list_sd, cumm_sd\n\n\ndef write_dist_to_file(file_path, unique_accesses, list_sd, cumm_sd):\n    try:\n        with open(file_path, \"w\") as f:\n            # unique_acesses\n            s = str(list(unique_accesses))\n            f.write(s[1 : len(s) - 1] + \"\\n\")\n            # list_sd\n            s = str(list_sd)\n            f.write(s[1 : len(s) - 1] + \"\\n\")\n            # cumm_sd\n            s = str(list(cumm_sd))\n            f.write(s[1 : len(s) - 1] + \"\\n\")\n    except Exception:\n        print(\"Wrong file or file path\")\n\n\nif __name__ == \"__main__\":\n    import argparse\n    import operator\n\n    ### parse arguments ###\n    parser = argparse.ArgumentParser(description=\"Generate Synthetic Distributions\")\n    parser.add_argument(\"--trace-file\", type=str, default=\"./input/trace.log\")\n    parser.add_argument(\"--trace-file-binary-type\", type=bool, default=False)\n    parser.add_argument(\"--trace-enable-padding\", type=bool, default=False)\n    parser.add_argument(\"--dist-file\", type=str, default=\"./input/dist.log\")\n    parser.add_argument(\n        \"--synthetic-file\", type=str, default=\"./input/trace_synthetic.log\"\n    )\n    parser.add_argument(\"--numpy-rand-seed\", type=int, default=123)\n    parser.add_argument(\"--print-precision\", type=int, default=5)\n    args = parser.parse_args()\n\n    ### some basic setup ###\n    np.random.seed(args.numpy_rand_seed)\n    np.set_printoptions(precision=args.print_precision)\n\n    ### read trace ###\n    trace = read_trace_from_file(args.trace_file)\n    # print(trace)\n\n    ### profile trace ###\n    (_, stack_distances, line_accesses) = trace_profile(\n        trace, args.trace_enable_padding\n    )\n    stack_distances.reverse()\n    line_accesses.reverse()\n    # print(line_accesses)\n    # print(stack_distances)\n\n    ### compute probability distribution ###\n    # count items\n    l = len(stack_distances)\n    dc = sorted(\n        collections.Counter(stack_distances).items(), key=operator.itemgetter(0)\n    )\n\n    # create a distribution\n    list_sd = list(map(lambda tuple_x_k: tuple_x_k[0], dc))  # x = tuple_x_k[0]\n    dist_sd = list(\n        map(lambda tuple_x_k: tuple_x_k[1] / float(l), dc)\n    )  # k = tuple_x_k[1]\n    cumm_sd = deque()  # np.cumsum(dc).tolist() #prefixsum\n    for i, (_, k) in enumerate(dc):\n        if i == 0:\n            cumm_sd.append(k / float(l))\n        else:\n            # add the 2nd element of the i-th tuple in the dist_sd list\n            cumm_sd.append(cumm_sd[i - 1] + (k / float(l)))\n\n    ### write stack_distance and line_accesses to a file ###\n    write_dist_to_file(args.dist_file, line_accesses, list_sd, cumm_sd)\n\n    ### generate corresponding synthetic ###\n    # line_accesses, list_sd, cumm_sd = read_dist_from_file(args.dist_file)\n    synthetic_trace = trace_generate_lru(\n        line_accesses, list_sd, cumm_sd, len(trace), args.trace_enable_padding\n    )\n    # synthetic_trace = trace_generate_rand(\n    #     line_accesses, list_sd, cumm_sd, len(trace), args.trace_enable_padding\n    # )\n    write_trace_to_file(args.synthetic_file, synthetic_trace)\n"
        },
        {
          "name": "dlrm_s_caffe2.py",
          "type": "blob",
          "size": 72.7841796875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Description: an implementation of a deep learning recommendation model (DLRM)\n# The model input consists of dense and sparse features. The former is a vector\n# of floating point values. The latter is a list of sparse indices into\n# embedding tables, which consist of vectors of floating point values.\n# The selected vectors are passed to mlp networks denoted by triangles,\n# in some cases the vectors are interacted through operators (Ops).\n#\n# output:\n#                         vector of values\n# model:                        |\n#                              /\\\n#                             /__\\\n#                               |\n#       _____________________> Op  <___________________\n#     /                         |                      \\\n#    /\\                        /\\                      /\\\n#   /__\\                      /__\\           ...      /__\\\n#    |                          |                       |\n#    |                         Op                      Op\n#    |                    ____/__\\_____           ____/__\\____\n#    |                   |_Emb_|____|__|    ...  |_Emb_|__|___|\n# input:\n# [ dense features ]     [sparse indices] , ..., [sparse indices]\n#\n# More precise definition of model layers:\n# 1) fully connected layers of an mlp\n# z = f(y)\n# y = Wx + b\n#\n# 2) embedding lookup (for a list of sparse indices p=[p1,...,pk])\n# z = Op(e1,...,ek)\n# obtain vectors e1=E[:,p1], ..., ek=E[:,pk]\n#\n# 3) Operator Op can be one of the following\n# Sum(e1,...,ek) = e1 + ... + ek\n# Dot(e1,...,ek) = [e1'e1, ..., e1'ek, ..., ek'e1, ..., ek'ek]\n# Cat(e1,...,ek) = [e1', ..., ek']'\n# where ' denotes transpose operation\n#\n# References:\n# [1] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,\n# Narayanan Sundaram, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,\n# Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii,\n# Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko,\n# Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong,\n# Misha Smelyanskiy, \"Deep Learning Recommendation Model for Personalization and\n# Recommendation Systems\", CoRR, arXiv:1906.00091, 2019\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport copy\n\nimport functools\n\n# others\nimport operator\nimport time\n\n# onnx\n# The onnx import causes deprecation warnings every time workers\n# are spawned during testing. So, we filter out those warnings.\nimport warnings\n\n# data generation\nimport dlrm_data_pytorch as dp\n\n# numpy\nimport numpy as np\nimport sklearn.metrics\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n    try:\n        import caffe2.python.onnx.frontend\n        import onnx\n    except ImportError as error:\n        print(\"Unable to import onnx or caffe2.python.onnx.frontend \", error)\n\n# from caffe2.python import data_parallel_model\n\n# caffe2\nfrom caffe2.proto import caffe2_pb2\nfrom caffe2.python import brew, core, dyndep, model_helper, net_drawer, workspace\n\n\"\"\"\n# auxiliary routine used to split input on the mini-bacth dimension\ndef where_to_split(mini_batch_size, ndevices, _add_leftover=False):\n    n = (mini_batch_size + ndevices - 1) // ndevices  # ceiling\n    l = mini_batch_size - n * (ndevices - 1)  # leftover\n    s = [n] * (ndevices - 1)\n    if _add_leftover:\n        ls += [l if l > 0 else n]\n    return ls\n\"\"\"\n\n\n### define dlrm in Caffe2 ###\nclass DLRM_Net(object):\n    def FeedBlobWrapper(self, tag, val, add_prefix=True, split=False, device_id=-1):\n        if self.ndevices > 1 and add_prefix:\n            if split:\n                # split across devices\n                mini_batch_size = val.shape[0]\n                # approach 1: np and caffe2 operators assume the mini-batch size is\n                # divisible exactly by the number of available devices\n                if mini_batch_size % self.ndevices != 0:\n                    sys.exit(\n                        \"ERROR: caffe2 net assumes that the mini_batch_size \"\n                        + str(mini_batch_size)\n                        + \" is evenly divisible by the number of available devices\"\n                        + str(self.ndevices)\n                    )\n                vals = np.split(val, self.ndevices, axis=0)\n                \"\"\"\n                # approach 2: np and caffe2 operators do not assume exact divisibility\n                if args.mini_batch_size != mini_batch_size:\n                    sys.exit(\"ERROR: caffe2 net was prepared for mini-batch size \"\n                             + str(args.mini_batch_size)\n                             + \" which is different from current mini-batch size \"\n                             + str(mini_batch_size) + \" being passed to it. \"\n                             + \"This is common for the last mini-batch, when \"\n                             + \"mini-batch size does not evenly divided the number of \"\n                             + \"elements in the data set.\")\n                ls = where_to_split(mini_batch_size, self.ndevices)\n                vals = np.split(val, ls, axis=0)\n                \"\"\"\n                # feed to multiple devices\n                for d in range(self.ndevices):\n                    tag_on_device = \"gpu_\" + str(d) + \"/\" + tag\n                    _d = core.DeviceOption(workspace.GpuDeviceType, d)\n                    workspace.FeedBlob(tag_on_device, vals[d], device_option=_d)\n            else:\n                # feed to multiple devices\n                for d in range(self.ndevices):\n                    tag_on_device = \"gpu_\" + str(d) + \"/\" + tag\n                    _d = core.DeviceOption(workspace.GpuDeviceType, d)\n                    workspace.FeedBlob(tag_on_device, val, device_option=_d)\n        else:\n            # feed to a single device (named or not)\n            if device_id >= 0:\n                _d = core.DeviceOption(workspace.GpuDeviceType, device_id)\n                workspace.FeedBlob(tag, val, device_option=_d)\n            else:\n                workspace.FeedBlob(tag, val)\n\n    def FetchBlobWrapper(self, tag, add_prefix=True, reduce_across=None, device_id=-1):\n        if self.ndevices > 1 and add_prefix:\n            # fetch from multiple devices\n            vals = []\n            for d in range(self.ndevices):\n                if tag.__class__ == list:\n                    tag_on_device = tag[d]\n                else:\n                    tag_on_device = \"gpu_\" + str(0) + \"/\" + tag\n                val = workspace.FetchBlob(tag_on_device)\n                vals.append(val)\n            # reduce across devices\n            if reduce_across == \"add\":\n                return functools.reduce(operator.add, vals)\n            elif reduce_across == \"concat\":\n                return np.concatenate(vals)\n            else:\n                return vals\n        else:\n            # fetch from a single device (named or not)\n            if device_id >= 0:\n                tag_on_device = \"gpu_\" + str(device_id) + \"/\" + tag\n                return workspace.FetchBlob(tag_on_device)\n            else:\n                return workspace.FetchBlob(tag)\n\n    def AddLayerWrapper(\n        self, layer, inp_blobs, out_blobs, add_prefix=True, reset_grad=False, **kwargs\n    ):\n        # auxiliary routine to adjust tags\n        def adjust_tag(blobs, on_device):\n            if blobs.__class__ == str:\n                _blobs = on_device + blobs\n            elif blobs.__class__ == list:\n                _blobs = list(map(lambda tag: on_device + tag, blobs))\n            else:  # blobs.__class__ == model_helper.ModelHelper or something else\n                _blobs = blobs\n            return _blobs\n\n        if self.ndevices > 1 and add_prefix:\n            # add layer on multiple devices\n            ll = []\n            for d in range(self.ndevices):\n                # add prefix on_device\n                on_device = \"gpu_\" + str(d) + \"/\"\n                _inp_blobs = adjust_tag(inp_blobs, on_device)\n                _out_blobs = adjust_tag(out_blobs, on_device)\n                # WARNING: reset_grad option was exlusively designed for WeightedSum\n                #         with inp_blobs=[w, tag_one, \"\", lr], where \"\" will be replaced\n                if reset_grad:\n                    w_grad = self.gradientMap[_inp_blobs[0]]\n                    _inp_blobs[2] = w_grad\n                # add layer to the model\n                with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):\n                    if kwargs:\n                        new_layer = layer(_inp_blobs, _out_blobs, **kwargs)\n                    else:\n                        new_layer = layer(_inp_blobs, _out_blobs)\n                ll.append(new_layer)\n            return ll\n        else:\n            # add layer on a single device\n            # WARNING: reset_grad option was exlusively designed for WeightedSum\n            #          with inp_blobs=[w, tag_one, \"\", lr], where \"\" will be replaced\n            if reset_grad:\n                w_grad = self.gradientMap[inp_blobs[0]]\n                inp_blobs[2] = w_grad\n            # add layer to the model\n            if kwargs:\n                new_layer = layer(inp_blobs, out_blobs, **kwargs)\n            else:\n                new_layer = layer(inp_blobs, out_blobs)\n            return new_layer\n\n    def create_mlp(self, ln, sigmoid_layer, model, tag):\n        (tag_layer, tag_in, tag_out) = tag\n\n        # build MLP layer by layer\n        layers = []\n        weights = []\n        for i in range(1, ln.size):\n            n = ln[i - 1]\n            m = ln[i]\n\n            # create tags\n            tag_fc_w = tag_layer + \":::\" + \"fc\" + str(i) + \"_w\"\n            tag_fc_b = tag_layer + \":::\" + \"fc\" + str(i) + \"_b\"\n            tag_fc_y = tag_layer + \":::\" + \"fc\" + str(i) + \"_y\"\n            tag_fc_z = tag_layer + \":::\" + \"fc\" + str(i) + \"_z\"\n            if i == ln.size - 1:\n                tag_fc_z = tag_out\n            weights.append(tag_fc_w)\n            weights.append(tag_fc_b)\n\n            # initialize the weights\n            # approach 1: custom Xavier input, output or two-sided fill\n            mean = 0.0  # std_dev = np.sqrt(variance)\n            std_dev = np.sqrt(2 / (m + n))  # np.sqrt(1 / m) # np.sqrt(1 / n)\n            W = np.random.normal(mean, std_dev, size=(m, n)).astype(np.float32)\n            std_dev = np.sqrt(1 / m)  # np.sqrt(2 / (m + 1))\n            b = np.random.normal(mean, std_dev, size=m).astype(np.float32)\n            self.FeedBlobWrapper(tag_fc_w, W)\n            self.FeedBlobWrapper(tag_fc_b, b)\n            # approach 2: caffe2 xavier\n            # W = self.AddLayerWrapper(\n            #     model.param_init_net.XavierFill,\n            #     [],\n            #     tag_fc_w,\n            #     shape=[m, n]\n            # )\n            # b = self.AddLayerWrapper(\n            #     model.param_init_net.ConstantFill,\n            #     [],\n            #     tag_fc_b,\n            #     shape=[m]\n            # )\n\n            # initialize the MLP's momentum for the Adagrad optimizer\n            if self.emb_optimizer in [\"adagrad\", \"rwsadagrad\"]:\n                # momentum of the weights\n                self.FeedBlobWrapper(\n                    \"momentum_mlp_{}_{}\".format(tag_layer, 2 * i - 1),\n                    np.full((m, n), 0, dtype=np.float32),\n                )\n                # momentum of the biases\n                self.FeedBlobWrapper(\n                    \"momentum_mlp_{}_{}\".format(tag_layer, 2 * i),\n                    np.full((m), 0, dtype=np.float32),\n                )\n\n            # save the blob shapes for latter (only needed if onnx is requested)\n            if self.save_onnx:\n                self.onnx_tsd[tag_fc_w] = (onnx.TensorProto.FLOAT, W.shape)\n                self.onnx_tsd[tag_fc_b] = (onnx.TensorProto.FLOAT, b.shape)\n\n            # approach 1: construct fully connected operator using model.net\n            fc = self.AddLayerWrapper(\n                model.net.FC, [tag_in, tag_fc_w, tag_fc_b], tag_fc_y\n            )\n            # approach 2: construct fully connected operator using brew\n            # https://github.com/caffe2/tutorials/blob/master/MNIST.ipynb\n            # fc = brew.fc(model, layer, tag_fc_w, dim_in=m, dim_out=n)\n            layers.append(fc)\n\n            if i == sigmoid_layer:\n                # approach 1: construct sigmoid operator using model.net\n                layer = self.AddLayerWrapper(model.net.Sigmoid, tag_fc_y, tag_fc_z)\n                # approach 2: using brew (which currently does not support sigmoid)\n                # tag_sigm = tag_layer + \":::\" + \"sigmoid\" + str(i)\n                # layer = brew.sigmoid(model,fc,tag_sigmoid)\n            else:\n                # approach 1: construct relu operator using model.net\n                layer = self.AddLayerWrapper(model.net.Relu, tag_fc_y, tag_fc_z)\n                # approach 2: using brew\n                # tag_relu = tag_layer + \":::\" + \"relu\" + str(i)\n                # layer = brew.relu(model,fc,tag_relu)\n            tag_in = tag_fc_z\n            layers.append(layer)\n\n        # WARNING: the dependency between layers is implicit in the tags,\n        # so only the last layer is added to the layers list. It will\n        # later be used for interactions.\n        return layers, weights\n\n    def create_emb(self, m, ln, model, tag):\n        (tag_layer, tag_in, tag_out) = tag\n        emb_l = []\n        weights_l = []\n        vw_l = []\n        for i in range(0, ln.size):\n            n = ln[i]\n\n            # select device\n            if self.ndevices > 1:\n                d = i % self.ndevices\n            else:\n                d = -1\n\n            # create tags\n            on_device = \"\" if self.ndevices <= 1 else \"gpu_\" + str(d) + \"/\"\n            len_s = on_device + tag_layer + \":::\" + \"sls\" + str(i) + \"_l\"\n            ind_s = on_device + tag_layer + \":::\" + \"sls\" + str(i) + \"_i\"\n            tbl_s = on_device + tag_layer + \":::\" + \"sls\" + str(i) + \"_w\"\n            sum_s = on_device + tag_layer + \":::\" + \"sls\" + str(i) + \"_z\"\n            weights_l.append(tbl_s)\n\n            # initialize the weights\n            # approach 1a: custom\n            W = np.random.uniform(\n                low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)\n            ).astype(np.float32)\n            # approach 1b: numpy rand\n            # W = ra.rand(n, m).astype(np.float32)\n            self.FeedBlobWrapper(tbl_s, W, False, device_id=d)\n            # approach 2: caffe2 xavier\n            # with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):\n            #     W = model.param_init_net.XavierFill([], tbl_s, shape=[n, m])\n            # save the blob shapes for latter (only needed if onnx is requested)\n\n            # initialize the embedding's momentum for the Adagrad optimizer\n            if self.emb_optimizer == \"adagrad\":\n                self.FeedBlobWrapper(\n                    \"momentum_emb_{}\".format(i),\n                    np.full((n, m), 0),\n                    add_prefix=False,\n                    device_id=d,\n                )\n            elif self.emb_optimizer == \"rwsadagrad\":\n                self.FeedBlobWrapper(\n                    \"momentum_emb_{}\".format(i),\n                    np.full((n), 0),\n                    add_prefix=False,\n                    device_id=d,\n                )\n\n            if self.save_onnx:\n                self.onnx_tsd[tbl_s] = (onnx.TensorProto.FLOAT, W.shape)\n\n            # create operator\n            if self.weighted_pooling is not None:\n                vw_s = on_device + tag_layer + \":::\" + \"sls\" + str(i) + \"_v\"\n                psw_s = on_device + tag_layer + \":::\" + \"sls\" + str(i) + \"_s\"\n                VW = np.ones(n).astype(np.float32)\n                self.FeedBlobWrapper(vw_s, VW, False, device_id=d)\n                if self.weighted_pooling == \"learned\":\n                    vw_l.append(vw_s)\n                    grad_on_weights = True\n                else:\n                    grad_on_weights = False\n                if self.save_onnx:\n                    self.onnx_tsd[vw_s] = (onnx.TensorProto.FLOAT, VW.shape)\n                if self.ndevices <= 1:\n                    PSW = model.net.Gather([vw_s, ind_s], [psw_s])\n                    EE = model.net.SparseLengthsWeightedSum(\n                        [tbl_s, PSW, ind_s, len_s],\n                        [sum_s],\n                        grad_on_weights=grad_on_weights,\n                    )\n                else:\n                    with core.DeviceScope(\n                        core.DeviceOption(workspace.GpuDeviceType, d)\n                    ):\n                        PSW = model.net.Gather([vw_s, ind_s], [psw_s])\n                        EE = model.net.SparseLengthsWeightedSum(\n                            [tbl_s, PSW, ind_s, len_s],\n                            [sum_s],\n                            grad_on_weights=grad_on_weights,\n                        )\n            else:\n                if self.ndevices <= 1:\n                    EE = model.net.SparseLengthsSum([tbl_s, ind_s, len_s], [sum_s])\n                else:\n                    with core.DeviceScope(\n                        core.DeviceOption(workspace.GpuDeviceType, d)\n                    ):\n                        EE = model.net.SparseLengthsSum([tbl_s, ind_s, len_s], [sum_s])\n            emb_l.append(EE)\n\n        return emb_l, weights_l, vw_l\n\n    def create_interactions(self, x, ly, model, tag):\n        (tag_dense_in, tag_sparse_in, tag_int_out) = tag\n\n        if self.arch_interaction_op == \"dot\":\n            # concatenate dense and sparse features\n            tag_int_out_info = tag_int_out + \"_info\"\n            T, T_info = model.net.Concat(\n                x + ly,\n                [tag_int_out + \"_cat_axis0\", tag_int_out_info + \"_cat_axis0\"],\n                axis=1,\n                add_axis=1,\n            )\n            # perform a dot product\n            Z = model.net.BatchMatMul([T, T], tag_int_out + \"_matmul\", trans_b=1)\n            # append dense feature with the interactions (into a row vector)\n            # approach 1: all\n            # Zflat = model.net.Flatten(Z, tag_int_out + \"_flatten\", axis=1)\n            # approach 2: unique\n            Zflat_all = model.net.Flatten(Z, tag_int_out + \"_flatten_all\", axis=1)\n            Zflat = model.net.BatchGather(\n                [Zflat_all, tag_int_out + \"_tril_indices\"], tag_int_out + \"_flatten\"\n            )\n            R, R_info = model.net.Concat(\n                x + [Zflat], [tag_int_out, tag_int_out_info], axis=1\n            )\n        elif self.arch_interaction_op == \"cat\":\n            # concatenation features (into a row vector)\n            tag_int_out_info = tag_int_out + \"_info\"\n            R, R_info = model.net.Concat(\n                x + ly, [tag_int_out, tag_int_out_info], axis=1\n            )\n        else:\n            sys.exit(\n                \"ERROR: --arch-interaction-op=\"\n                + self.arch_interaction_op\n                + \" is not supported\"\n            )\n\n        return R\n\n    def create_sequential_forward_ops(self):\n        # embeddings\n        tag = (self.temb, self.tsin, self.tsout)\n        self.emb_l, self.emb_w, self.emb_vw = self.create_emb(\n            self.m_spa, self.ln_emb, self.model, tag\n        )\n        # bottom mlp\n        tag = (self.tbot, self.tdin, self.tdout)\n        self.bot_l, self.bot_w = self.create_mlp(\n            self.ln_bot, self.sigmoid_bot, self.model, tag\n        )\n        # interactions\n        tag = (self.tdout, self.tsout, self.tint)\n        Z = self.create_interactions([self.bot_l[-1]], self.emb_l, self.model, tag)\n\n        # top mlp\n        tag = (self.ttop, Z, self.tout)\n        self.top_l, self.top_w = self.create_mlp(\n            self.ln_top, self.sigmoid_top, self.model, tag\n        )\n        # debug prints\n        # print(self.emb_l)\n        # print(self.bot_l)\n        # print(self.top_l)\n\n        # setup the last output variable\n        self.last_output = self.top_l[-1]\n\n    def create_parallel_forward_ops(self):\n        # distribute embeddings (model parallelism)\n        tag = (self.temb, self.tsin, self.tsout)\n        self.emb_l, self.emb_w, self.emb_vw = self.create_emb(\n            self.m_spa, self.ln_emb, self.model, tag\n        )\n        # replicate mlp (data parallelism)\n        tag = (self.tbot, self.tdin, self.tdout)\n        self.bot_l, self.bot_w = self.create_mlp(\n            self.ln_bot, self.sigmoid_bot, self.model, tag\n        )\n\n        # add communication (butterfly shuffle)\n        t_list = []\n        for i, emb_output in enumerate(self.emb_l):\n            # split input\n            src_d = i % self.ndevices\n            lo = [emb_output + \"_split_\" + str(d) for d in range(self.ndevices)]\n            # approach 1: np and caffe2 operators assume the mini-batch size is\n            # divisible exactly by the number of available devices\n            with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, src_d)):\n                self.model.net.Split(emb_output, lo, axis=0)\n            \"\"\"\n            # approach 2: np and caffe2 operators do not assume exact divisibility\n            ls = where_to_split(args.mini_batch_size, self.ndevices, _add_leftover=True)\n            with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, src_d)):\n                emb_output_split = self.model.net.Split(\n                    emb_output, lo, split=lp, axis=0\n                )\n            \"\"\"\n            # scatter\n            y = []\n            for dst_d in range(len(lo)):\n                src_blob = lo[dst_d]\n                dst_blob = str(src_blob).replace(\n                    \"gpu_\" + str(src_d), \"gpu_\" + str(dst_d), 1\n                )\n                if src_blob != dst_blob:\n                    with core.DeviceScope(\n                        core.DeviceOption(workspace.GpuDeviceType, dst_d)\n                    ):\n                        blob = self.model.Copy(src_blob, dst_blob)\n                else:\n                    blob = dst_blob\n                y.append(blob)\n            t_list.append(y)\n        # adjust lists to be ordered per device\n        x = list(map(lambda x: list(x), zip(*self.bot_l)))\n        ly = list(map(lambda y: list(y), zip(*t_list)))\n\n        # interactions\n        for d in range(self.ndevices):\n            on_device = \"gpu_\" + str(d) + \"/\"\n            tag = (\n                on_device + self.tdout,\n                on_device + self.tsout,\n                on_device + self.tint,\n            )\n            with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):\n                self.create_interactions([x[d][-1]], ly[d], self.model, tag)\n\n        # replicate mlp (data parallelism)\n        tag = (self.ttop, self.tint, self.tout)\n        self.top_l, self.top_w = self.create_mlp(\n            self.ln_top, self.sigmoid_top, self.model, tag\n        )\n\n        # debug prints\n        # print(self.model.net.Proto(),end='\\n')\n        # sys.exit(\"ERROR: debugging\")\n\n        # setup the last output variable\n        self.last_output = self.top_l[-1]\n\n    def __init__(\n        self,\n        m_spa,\n        ln_emb,\n        ln_bot,\n        ln_top,\n        arch_interaction_op,\n        arch_interaction_itself=False,\n        sigmoid_bot=-1,\n        sigmoid_top=-1,\n        save_onnx=False,\n        model=None,\n        test_net=None,\n        tag=None,\n        ndevices=-1,\n        forward_ops=True,\n        enable_prof=False,\n        weighted_pooling=None,\n        emb_optimizer=\"sgd\",\n    ):\n        super(DLRM_Net, self).__init__()\n\n        # init model\n        if model is None:\n            global_init_opt = [\"caffe2\", \"--caffe2_log_level=0\"]\n            if enable_prof:\n                global_init_opt += [\n                    \"--logtostderr=0\",\n                    \"--log_dir=$HOME\",\n                    \"--caffe2_logging_print_net_summary=1\",\n                ]\n            workspace.GlobalInit(global_init_opt)\n            self.set_tags()\n            self.model = model_helper.ModelHelper(name=\"DLRM\", init_params=True)\n            self.test_net = None\n        else:\n            # WARNING: assume that workspace and tags have been initialized elsewhere\n            self.set_tags(\n                tag[0],\n                tag[1],\n                tag[2],\n                tag[3],\n                tag[4],\n                tag[5],\n                tag[6],\n                tag[7],\n                tag[8],\n                tag[9],\n            )\n            self.model = model\n            self.test_net = test_net\n\n        # save arguments\n        self.m_spa = m_spa\n        self.ln_emb = ln_emb\n        self.ln_bot = ln_bot\n        self.ln_top = ln_top\n        self.arch_interaction_op = arch_interaction_op\n        self.arch_interaction_itself = arch_interaction_itself\n        self.sigmoid_bot = sigmoid_bot\n        self.sigmoid_top = sigmoid_top\n        self.save_onnx = save_onnx\n        self.ndevices = ndevices\n        self.emb_optimizer = emb_optimizer\n        if weighted_pooling is not None and weighted_pooling != \"fixed\":\n            self.weighted_pooling = \"learned\"\n        else:\n            self.weighted_pooling = weighted_pooling\n        # onnx types and shapes dictionary\n        if self.save_onnx:\n            self.onnx_tsd = {}\n        # create forward operators\n        if forward_ops:\n            if self.ndevices <= 1:\n                return self.create_sequential_forward_ops()\n            else:\n                return self.create_parallel_forward_ops()\n\n    def set_tags(\n        self,\n        _tag_layer_top_mlp=\"top\",\n        _tag_layer_bot_mlp=\"bot\",\n        _tag_layer_embedding=\"emb\",\n        _tag_feature_dense_in=\"dense_in\",\n        _tag_feature_dense_out=\"dense_out\",\n        _tag_feature_sparse_in=\"sparse_in\",\n        _tag_feature_sparse_out=\"sparse_out\",\n        _tag_interaction=\"interaction\",\n        _tag_dense_output=\"prob_click\",\n        _tag_dense_target=\"target\",\n    ):\n        # layer tags\n        self.ttop = _tag_layer_top_mlp\n        self.tbot = _tag_layer_bot_mlp\n        self.temb = _tag_layer_embedding\n        # dense feature tags\n        self.tdin = _tag_feature_dense_in\n        self.tdout = _tag_feature_dense_out\n        # sparse feature tags\n        self.tsin = _tag_feature_sparse_in\n        self.tsout = _tag_feature_sparse_out\n        # output and target tags\n        self.tint = _tag_interaction\n        self.ttar = _tag_dense_target\n        self.tout = _tag_dense_output\n\n    def parameters(self):\n        return self.model\n\n    def get_loss(self):\n        return self.FetchBlobWrapper(self.loss, reduce_across=\"add\")\n\n    def get_output(self):\n        return self.FetchBlobWrapper(self.last_output, reduce_across=\"concat\")\n\n    def create(self, X, S_lengths, S_indices, T):\n        self.create_input(X, S_lengths, S_indices, T)\n        self.create_model(X, S_lengths, S_indices, T)\n\n    def create_input(self, X, S_lengths, S_indices, T):\n        # feed input data to blobs\n        self.FeedBlobWrapper(self.tdin, X, split=True)\n        # save the blob shapes for latter (only needed if onnx is requested)\n        if self.save_onnx:\n            self.onnx_tsd[self.tdin] = (onnx.TensorProto.FLOAT, X.shape)\n\n        for i in range(len(self.emb_l)):\n            # select device\n            if self.ndevices > 1:\n                d = i % self.ndevices\n            else:\n                d = -1\n            # create tags\n            on_device = \"\" if self.ndevices <= 1 else \"gpu_\" + str(d) + \"/\"\n            len_s = on_device + self.temb + \":::\" + \"sls\" + str(i) + \"_l\"\n            ind_s = on_device + self.temb + \":::\" + \"sls\" + str(i) + \"_i\"\n            self.FeedBlobWrapper(len_s, np.array(S_lengths[i]), False, device_id=d)\n            self.FeedBlobWrapper(ind_s, np.array(S_indices[i]), False, device_id=d)\n            # save the blob shapes for latter (only needed if onnx is requested)\n            if self.save_onnx:\n                lshape = (len(S_lengths[i]),)  # =args.mini_batch_size\n                ishape = (len(S_indices[i]),)\n                self.onnx_tsd[len_s] = (onnx.TensorProto.INT32, lshape)\n                self.onnx_tsd[ind_s] = (onnx.TensorProto.INT32, ishape)\n\n        # feed target data to blobs\n        if T is not None:\n            zeros_fp32 = np.zeros(T.shape).astype(np.float32)\n            self.FeedBlobWrapper(self.ttar, zeros_fp32, split=True)\n            # save the blob shapes for latter (only needed if onnx is requested)\n            if self.save_onnx:\n                self.onnx_tsd[self.ttar] = (onnx.TensorProto.FLOAT, T.shape)\n\n    def create_model(self, X, S_lengths, S_indices, T):\n        # setup tril indices for the interactions\n        offset = 1 if self.arch_interaction_itself else 0\n        num_fea = len(self.emb_l) + 1\n        tril_indices = np.array(\n            [j + i * num_fea for i in range(num_fea) for j in range(i + offset)]\n        )\n        self.FeedBlobWrapper(self.tint + \"_tril_indices\", tril_indices)\n\n        # create compute graph\n        if T is not None:\n            # WARNING: RunNetOnce call is needed only if we use brew and ConstantFill.\n            # We could use direct calls to self.model functions above to avoid it\n            workspace.RunNetOnce(self.model.param_init_net)\n            workspace.CreateNet(self.model.net)\n            if self.test_net is not None:\n                workspace.CreateNet(self.test_net)\n\n    def run(self, X, S_lengths, S_indices, T, test_net=False, enable_prof=False):\n        # feed input data to blobs\n        # dense features\n        self.FeedBlobWrapper(self.tdin, X, split=True)\n        # sparse features\n        for i in range(len(self.emb_l)):\n            # select device\n            if self.ndevices > 1:\n                d = i % self.ndevices\n            else:\n                d = -1\n            # create tags\n            on_device = \"\" if self.ndevices <= 1 else \"gpu_\" + str(d) + \"/\"\n            len_s = on_device + self.temb + \":::\" + \"sls\" + str(i) + \"_l\"\n            ind_s = on_device + self.temb + \":::\" + \"sls\" + str(i) + \"_i\"\n            self.FeedBlobWrapper(len_s, np.array(S_lengths[i]), False, device_id=d)\n            self.FeedBlobWrapper(ind_s, np.array(S_indices[i]), False, device_id=d)\n\n        # feed target data to blobs if needed\n        if T is not None:\n            self.FeedBlobWrapper(self.ttar, T, split=True)\n            # execute compute graph\n            if test_net:\n                workspace.RunNet(self.test_net)\n            else:\n                if enable_prof:\n                    workspace.C.benchmark_net(self.model.net.Name(), 0, 1, True)\n                else:\n                    workspace.RunNet(self.model.net)\n        # debug prints\n        # print(\"intermediate\")\n        # print(self.FetchBlobWrapper(self.bot_l[-1]))\n        # for tag_emb in self.emb_l:\n        #     print(self.FetchBlobWrapper(tag_emb))\n        # print(self.FetchBlobWrapper(self.tint))\n\n    def MSEloss(self, scale=1.0):\n        # add MSEloss to the model\n        self.AddLayerWrapper(self.model.SquaredL2Distance, [self.tout, self.ttar], \"sd\")\n        self.AddLayerWrapper(self.model.Scale, \"sd\", \"sd2\", scale=2.0 * scale)\n        # WARNING: \"loss\" is a special tag and should not be changed\n        self.loss = self.AddLayerWrapper(self.model.AveragedLoss, \"sd2\", \"loss\")\n\n    def BCEloss(self, scale=1.0, threshold=0.0):\n        # add BCEloss to the mode\n        if 0.0 < threshold and threshold < 1.0:\n            self.AddLayerWrapper(\n                self.model.Clip,\n                self.tout,\n                \"tout_c\",\n                min=threshold,\n                max=(1.0 - threshold),\n            )\n            self.AddLayerWrapper(self.model.MakeTwoClass, \"tout_c\", \"tout_2c\")\n        else:\n            self.AddLayerWrapper(self.model.MakeTwoClass, self.tout, \"tout_2c\")\n        self.AddLayerWrapper(self.model.LabelCrossEntropy, [\"tout_2c\", self.ttar], \"sd\")\n        # WARNING: \"loss\" is a special tag and should not be changed\n        if scale == 1.0:\n            self.loss = self.AddLayerWrapper(self.model.AveragedLoss, \"sd\", \"loss\")\n        else:\n            self.AddLayerWrapper(self.model.Scale, \"sd\", \"sd2\", scale=scale)\n            self.loss = self.AddLayerWrapper(self.model.AveragedLoss, \"sd2\", \"loss\")\n\n    def sgd_optimizer(\n        self, learning_rate, T=None, _gradientMap=None, sync_dense_params=True\n    ):\n        # create one, it and lr tags (or use them if already present)\n        if T is not None:\n            (tag_one, tag_it, tag_lr) = T\n        else:\n            (tag_one, tag_it, tag_lr) = (\"const_one\", \"optim_it\", \"optim_lr\")\n\n            # approach 1: feed values directly\n            # self.FeedBlobWrapper(tag_one, np.ones(1).astype(np.float32))\n            # self.FeedBlobWrapper(tag_it, np.zeros(1).astype(np.int64))\n            # it = self.AddLayerWrapper(self.model.Iter, tag_it, tag_it)\n            # lr = self.AddLayerWrapper(self.model.LearningRate, tag_it, tag_lr,\n            #                           base_lr=-1 * learning_rate, policy=\"fixed\")\n            # approach 2: use brew\n            self.AddLayerWrapper(\n                self.model.param_init_net.ConstantFill,\n                [],\n                tag_one,\n                shape=[1],\n                value=1.0,\n            )\n            self.AddLayerWrapper(brew.iter, self.model, tag_it)\n            self.AddLayerWrapper(\n                self.model.LearningRate,\n                tag_it,\n                tag_lr,\n                base_lr=-1 * learning_rate,\n                policy=\"fixed\",\n            )\n            # save the blob shapes for latter (only needed if onnx is requested)\n            if self.save_onnx:\n                self.onnx_tsd[tag_one] = (onnx.TensorProto.FLOAT, (1,))\n                self.onnx_tsd[tag_it] = (onnx.TensorProto.INT64, (1,))\n\n        # create gradient maps (or use them if already present)\n        if _gradientMap is not None:\n            self.gradientMap = _gradientMap\n        else:\n            if self.loss.__class__ == list:\n                self.gradientMap = self.model.AddGradientOperators(self.loss)\n            else:\n                self.gradientMap = self.model.AddGradientOperators([self.loss])\n\n        # update weights\n        # approach 1: builtin function\n        # optimizer.build_sgd(self.model, base_learning_rate=learning_rate)\n        # approach 2: custom code\n        # top MLP weight and bias\n        for w in self.top_w:\n            # allreduce across devices if needed\n            if sync_dense_params and self.ndevices > 1:\n                grad_blobs = [\n                    self.gradientMap[\"gpu_{}/\".format(d) + w]\n                    for d in range(self.ndevices)\n                ]\n                self.model.NCCLAllreduce(grad_blobs, grad_blobs)\n            # update weights\n            self.AddLayerWrapper(\n                self.model.WeightedSum, [w, tag_one, \"\", tag_lr], w, reset_grad=True\n            )\n        # bottom MLP weight and bias\n        for w in self.bot_w:\n            # allreduce across devices if needed\n            if sync_dense_params and self.ndevices > 1:\n                grad_blobs = [\n                    self.gradientMap[\"gpu_{}/\".format(d) + w]\n                    for d in range(self.ndevices)\n                ]\n                self.model.NCCLAllreduce(grad_blobs, grad_blobs)\n            # update weights\n            self.AddLayerWrapper(\n                self.model.WeightedSum, [w, tag_one, \"\", tag_lr], w, reset_grad=True\n            )\n        # update embeddings\n        for i, w in enumerate(self.emb_w):\n            # select device\n            if self.ndevices > 1:\n                d = i % self.ndevices\n            # create tags\n            on_device = \"\" if self.ndevices <= 1 else \"gpu_\" + str(d) + \"/\"\n            _tag_one = on_device + tag_one\n            _tag_lr = on_device + tag_lr\n            # pickup gradient\n            w_grad = self.gradientMap[w]\n            # update weights\n            if self.ndevices > 1:\n                with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):\n                    self.model.ScatterWeightedSum(\n                        [w, _tag_one, w_grad.indices, w_grad.values, _tag_lr], w\n                    )\n            else:\n                self.model.ScatterWeightedSum(\n                    [w, _tag_one, w_grad.indices, w_grad.values, _tag_lr], w\n                )\n\n        # update per sample weights\n        if self.weighted_pooling == \"learned\":\n            for i, w in enumerate(self.emb_vw):\n                # select device\n                if self.ndevices > 1:\n                    d = i % self.ndevices\n                # create tags\n                on_device = \"\" if self.ndevices <= 1 else \"gpu_\" + str(d) + \"/\"\n                _tag_one = on_device + tag_one\n                _tag_lr = on_device + tag_lr\n                # pickup gradient\n                w_grad = self.gradientMap[w]\n                # update weights\n                if self.ndevices > 1:\n                    with core.DeviceScope(\n                        core.DeviceOption(workspace.GpuDeviceType, d)\n                    ):\n                        self.model.ScatterWeightedSum(\n                            [w, _tag_one, w_grad.indices, w_grad.values, _tag_lr], w\n                        )\n                else:\n                    self.model.ScatterWeightedSum(\n                        [w, _tag_one, w_grad.indices, w_grad.values, _tag_lr], w\n                    )\n\n    def adagrad_optimizer(\n        self,\n        learning_rate,\n        T=None,\n        _gradientMap=None,\n        sync_dense_params=True,\n        epsilon=1e-10,\n        decay_=0.0,\n        weight_decay_=0.0,\n    ):\n        # create one, it and lr tags (or use them if already present)\n        if T is not None:\n            (tag_one, tag_it, tag_lr) = T\n        else:\n            (tag_one, tag_it, tag_lr) = (\"const_one\", \"optim_it\", \"optim_lr\")\n\n            # approach 1: feed values directly\n            # self.FeedBlobWrapper(tag_one, np.ones(1).astype(np.float32))\n            # self.FeedBlobWrapper(tag_it, np.zeros(1).astype(np.int64))\n            # it = self.AddLayerWrapper(self.model.Iter, tag_it, tag_it)\n            # lr = self.AddLayerWrapper(self.model.LearningRate, tag_it, tag_lr,\n            #                           base_lr=-1 * learning_rate, policy=\"fixed\")\n            # approach 2: use brew\n            self.AddLayerWrapper(\n                self.model.param_init_net.ConstantFill,\n                [],\n                tag_one,\n                shape=[1],\n                value=1.0,\n            )\n            self.AddLayerWrapper(brew.iter, self.model, tag_it)\n            self.AddLayerWrapper(\n                self.model.LearningRate,\n                tag_it,\n                tag_lr,\n                base_lr=-1 * learning_rate,\n                policy=\"fixed\",\n            )\n            # save the blob shapes for latter (only needed if onnx is requested)\n            if self.save_onnx:\n                self.onnx_tsd[tag_one] = (onnx.TensorProto.FLOAT, (1,))\n                self.onnx_tsd[tag_it] = (onnx.TensorProto.INT64, (1,))\n\n        # create gradient maps (or use them if already present)\n        if _gradientMap is not None:\n            self.gradientMap = _gradientMap\n        else:\n            if self.loss.__class__ == list:\n                self.gradientMap = self.model.AddGradientOperators(self.loss)\n            else:\n                self.gradientMap = self.model.AddGradientOperators([self.loss])\n\n        # update weights\n        # approach 1: builtin function\n        # optimizer.build_sgd(self.model, base_learning_rate=learning_rate)\n        # approach 2: custom code\n        # top MLP weight and bias\n        for i, w in enumerate(self.top_w):\n            # allreduce across devices if needed\n            if sync_dense_params and self.ndevices > 1:\n                grad_blobs = [\n                    self.gradientMap[\"gpu_{}/\".format(d) + w]\n                    for d in range(self.ndevices)\n                ]\n                self.model.NCCLAllreduce(grad_blobs, grad_blobs)\n            # update weights\n            self.model.Adagrad(\n                [w, \"momentum_mlp_top_{}\".format(i + 1), self.gradientMap[w], tag_lr],\n                [w, \"momentum_mlp_top_{}\".format(i + 1)],\n                epsilon=epsilon,\n                decay_=decay_,\n                weight_decay_=weight_decay_,\n            )\n\n        # bottom MLP weight and bias\n        for i, w in enumerate(self.bot_w):\n            # allreduce across devices if needed\n            if sync_dense_params and self.ndevices > 1:\n                grad_blobs = [\n                    self.gradientMap[\"gpu_{}/\".format(d) + w]\n                    for d in range(self.ndevices)\n                ]\n                self.model.NCCLAllreduce(grad_blobs, grad_blobs)\n            # update weights\n            self.model.Adagrad(\n                [w, \"momentum_mlp_bot_{}\".format(i + 1), self.gradientMap[w], tag_lr],\n                [w, \"momentum_mlp_bot_{}\".format(i + 1)],\n                epsilon=epsilon,\n                decay_=decay_,\n                weight_decay_=weight_decay_,\n            )\n\n        # update embeddings\n        for i, w in enumerate(self.emb_w):\n            # select device\n            if self.ndevices > 1:\n                d = i % self.ndevices\n            # create tags\n            on_device = \"\" if self.ndevices <= 1 else \"gpu_\" + str(d) + \"/\"\n            _tag_one = on_device + tag_one\n            _tag_lr = on_device + tag_lr\n            # pickup gradient\n            w_grad = self.gradientMap[w]\n\n            # update weights\n            def add_optimizer():\n                self.model.Unique(\n                    w_grad.indices,\n                    [\"unique_w_grad_indices\", \"remapping_w_grad_indices\"],\n                )\n                self.model.UnsortedSegmentSum(\n                    [w_grad.values, \"remapping_w_grad_indices\"], \"unique_w_grad_values\"\n                )\n\n                if self.emb_optimizer == \"adagrad\":\n                    self.model.SparseAdagrad(\n                        [\n                            w,\n                            \"momentum_emb_{}\".format(i),\n                            \"unique_w_grad_indices\",\n                            \"unique_w_grad_values\",\n                            _tag_lr,\n                        ],\n                        [w, \"momentum_emb_{}\".format(i)],\n                        epsilon=epsilon,\n                        decay_=decay_,\n                        weight_decay_=weight_decay_,\n                    )\n\n                elif self.emb_optimizer == \"rwsadagrad\":\n                    self.model.RowWiseSparseAdagrad(\n                        [\n                            w,\n                            \"momentum_emb_{}\".format(i),\n                            \"unique_w_grad_indices\",\n                            \"unique_w_grad_values\",\n                            _tag_lr,\n                        ],\n                        [w, \"momentum_emb_{}\".format(i)],\n                        epsilon=epsilon,\n                        decay_=decay_,\n                        weight_decay_=weight_decay_,\n                    )\n\n            if self.ndevices > 1:\n                with core.DeviceScope(core.DeviceOption(workspace.GpuDeviceType, d)):\n                    add_optimizer()\n            else:\n                add_optimizer()\n\n        # update per sample weights\n        if self.weighted_pooling == \"learned\":\n            for i, w in enumerate(self.emb_vw):\n                # select device\n                if self.ndevices > 1:\n                    d = i % self.ndevices\n                # create tags\n                on_device = \"\" if self.ndevices <= 1 else \"gpu_\" + str(d) + \"/\"\n                _tag_one = on_device + tag_one\n                _tag_lr = on_device + tag_lr\n                # pickup gradient\n                w_grad = self.gradientMap[w]\n                # update weights\n                if self.ndevices > 1:\n                    with core.DeviceScope(\n                        core.DeviceOption(workspace.GpuDeviceType, d)\n                    ):\n                        self.model.ScatterWeightedSum(\n                            [w, _tag_one, w_grad.indices, w_grad.values, _tag_lr], w\n                        )\n                else:\n                    self.model.ScatterWeightedSum(\n                        [w, _tag_one, w_grad.indices, w_grad.values, _tag_lr], w\n                    )\n\n    def print_all(self):\n        # approach 1: all\n        print(workspace.Blobs(), end=\"\\n\")\n        for _, l in enumerate(workspace.Blobs()):\n            print(l)\n            print(self.FetchBlobWrapper(l))\n        # approach 2: only summary\n        # for param in self.model.params:\n        #    self.model.Summarize(param, [], to_file=1)\n        #    self.model.Summarize(self.model.param_to_grad[param], [], to_file=1)\n\n    def print_weights(self):\n        for _, l in enumerate(self.emb_w):\n            # print(l)\n            print(self.FetchBlobWrapper(l, False))\n        if self.weighted_pooling == \"learned\":\n            for _, l in enumerate(self.emb_vw):\n                # print(l)\n                print(self.FetchBlobWrapper(l, False))\n        for _, l in enumerate(self.bot_w):\n            # print(l)\n            if self.ndevices > 1:\n                print(self.FetchBlobWrapper(l, False, device_id=0))\n            else:\n                print(self.FetchBlobWrapper(l))\n        for _, l in enumerate(self.top_w):\n            # print(l)\n            if self.ndevices > 1:\n                print(self.FetchBlobWrapper(l, False, device_id=0))\n            else:\n                print(self.FetchBlobWrapper(l))\n\n    def print_activations(self):\n        for _, l in enumerate(self.emb_l):\n            print(l)\n            print(self.FetchBlobWrapper(l, False))\n        for _, l in enumerate(self.bot_l):\n            print(l)\n            print(self.FetchBlobWrapper(l))\n        print(self.tint)\n        print(self.FetchBlobWrapper(self.tint))\n        for _, l in enumerate(self.top_l):\n            print(l)\n            print(self.FetchBlobWrapper(l))\n\n\ndef define_metrics():\n    metrics = {\n        \"loss\": lambda y_true, y_score: sklearn.metrics.log_loss(\n            y_true=y_true, y_pred=y_score, labels=[0, 1]\n        ),\n        \"recall\": lambda y_true, y_score: sklearn.metrics.recall_score(\n            y_true=y_true, y_pred=np.round(y_score)\n        ),\n        \"precision\": lambda y_true, y_score: sklearn.metrics.precision_score(\n            y_true=y_true, y_pred=np.round(y_score)\n        ),\n        \"f1\": lambda y_true, y_score: sklearn.metrics.f1_score(\n            y_true=y_true, y_pred=np.round(y_score)\n        ),\n        \"ap\": sklearn.metrics.average_precision_score,\n        \"roc_auc\": sklearn.metrics.roc_auc_score,\n        \"accuracy\": lambda y_true, y_score: sklearn.metrics.accuracy_score(\n            y_true=y_true, y_pred=np.round(y_score)\n        ),\n        # 'pre_curve' : sklearn.metrics.precision_recall_curve,\n        # 'roc_curve' :  sklearn.metrics.roc_curve,\n    }\n    return metrics\n\n\ndef calculate_metrics(targets, scores):\n    scores = np.concatenate(scores, axis=0)\n    targets = np.concatenate(targets, axis=0)\n\n    metrics = define_metrics()\n\n    # print(\"Compute time for validation metric : \", end=\"\")\n    # first_it = True\n    validation_results = {}\n    for metric_name, metric_function in metrics.items():\n        # if first_it:\n        #     first_it = False\n        # else:\n        #     print(\", \", end=\"\")\n        # metric_compute_start = time_wrap(False)\n        try:\n            validation_results[metric_name] = metric_function(targets, scores)\n        except Exception as error:\n            validation_results[metric_name] = -1\n            print(\"{} in calculating {}\".format(error, metric_name))\n        # metric_compute_end = time_wrap(False)\n        # met_time = metric_compute_end - metric_compute_start\n        # print(\"{} {:.4f}\".format(metric_name, 1000 * (met_time)),\n        #      end=\"\")\n    # print(\" ms\")\n    return validation_results\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    ### import packages ###\n    import sys\n\n    ### parse arguments ###\n    parser = argparse.ArgumentParser(\n        description=\"Train Deep Learning Recommendation Model (DLRM)\"\n    )\n    # model related parameters\n    parser.add_argument(\"--arch-sparse-feature-size\", type=int, default=2)\n    parser.add_argument(\"--arch-embedding-size\", type=str, default=\"4-3-2\")\n    parser.add_argument(\"--arch-mlp-bot\", type=str, default=\"4-3-2\")\n    parser.add_argument(\"--arch-mlp-top\", type=str, default=\"4-2-1\")\n    parser.add_argument(\"--arch-interaction-op\", type=str, default=\"dot\")\n    parser.add_argument(\"--arch-interaction-itself\", action=\"store_true\", default=False)\n    # activations and loss\n    parser.add_argument(\"--activation-function\", type=str, default=\"relu\")\n    parser.add_argument(\"--loss-function\", type=str, default=\"mse\")  # or bce\n    parser.add_argument(\"--loss-threshold\", type=float, default=0.0)  # 1.0e-7\n    parser.add_argument(\"--round-targets\", type=bool, default=False)\n    parser.add_argument(\"--weighted-pooling\", type=str, default=None)\n    # data\n    parser.add_argument(\"--data-size\", type=int, default=1)\n    parser.add_argument(\"--num-batches\", type=int, default=0)\n    parser.add_argument(\n        \"--data-generation\", type=str, default=\"random\"\n    )  # or synthetic or dataset\n    parser.add_argument(\n        \"--rand-data-dist\", type=str, default=\"uniform\"\n    )  # uniform or gaussian\n    parser.add_argument(\"--rand-data-min\", type=float, default=0)\n    parser.add_argument(\"--rand-data-max\", type=float, default=1)\n    parser.add_argument(\"--rand-data-mu\", type=float, default=-1)\n    parser.add_argument(\"--rand-data-sigma\", type=float, default=1)\n    parser.add_argument(\"--data-trace-file\", type=str, default=\"./input/dist_emb_j.log\")\n    parser.add_argument(\"--data-set\", type=str, default=\"kaggle\")  # or terabyte\n    parser.add_argument(\"--raw-data-file\", type=str, default=\"\")\n    parser.add_argument(\"--processed-data-file\", type=str, default=\"\")\n    parser.add_argument(\"--data-randomize\", type=str, default=\"total\")  # or day or none\n    parser.add_argument(\"--data-trace-enable-padding\", type=bool, default=False)\n    parser.add_argument(\"--max-ind-range\", type=int, default=-1)\n    parser.add_argument(\"--data-sub-sample-rate\", type=float, default=0.0)  # in [0, 1]\n    parser.add_argument(\"--num-indices-per-lookup\", type=int, default=10)\n    parser.add_argument(\"--num-indices-per-lookup-fixed\", type=bool, default=False)\n    parser.add_argument(\"--num-workers\", type=int, default=0)\n    parser.add_argument(\"--memory-map\", action=\"store_true\", default=False)\n    # training\n    parser.add_argument(\"--mini-batch-size\", type=int, default=1)\n    parser.add_argument(\"--nepochs\", type=int, default=1)\n    parser.add_argument(\"--learning-rate\", type=float, default=0.01)\n    parser.add_argument(\"--print-precision\", type=int, default=5)\n    parser.add_argument(\"--numpy-rand-seed\", type=int, default=123)\n    parser.add_argument(\"--sync-dense-params\", type=bool, default=True)\n    parser.add_argument(\"--caffe2-net-type\", type=str, default=\"\")\n    parser.add_argument(\n        \"--optimizer\",\n        type=str,\n        default=\"sgd\",\n        help=\"\"\"This is the optimizer for embedding tables.\"\"\",\n    )\n    parser.add_argument(\n        \"--dataset-multiprocessing\",\n        action=\"store_true\",\n        default=False,\n        help=\"The Kaggle dataset can be multiprocessed in an environment \\\n                        with more than 7 CPU cores and more than 20 GB of memory. \\n \\\n                        The Terabyte dataset can be multiprocessed in an environment \\\n                        with more than 24 CPU cores and at least 1 TB of memory.\",\n    )\n    # inference\n    parser.add_argument(\"--inference-only\", action=\"store_true\", default=False)\n    # onnx (or protobuf with shapes)\n    parser.add_argument(\"--save-onnx\", action=\"store_true\", default=False)\n    parser.add_argument(\"--save-proto-types-shapes\", action=\"store_true\", default=False)\n    # gpu\n    parser.add_argument(\"--use-gpu\", action=\"store_true\", default=False)\n    # debugging and profiling\n    parser.add_argument(\"--print-freq\", type=int, default=1)\n    parser.add_argument(\"--test-freq\", type=int, default=-1)\n    parser.add_argument(\"--test-mini-batch-size\", type=int, default=-1)\n    parser.add_argument(\"--test-num-workers\", type=int, default=-1)\n    parser.add_argument(\"--print-time\", action=\"store_true\", default=False)\n    parser.add_argument(\"--debug-mode\", action=\"store_true\", default=False)\n    parser.add_argument(\"--enable-profiling\", action=\"store_true\", default=False)\n    parser.add_argument(\"--plot-compute-graph\", action=\"store_true\", default=False)\n    # mlperf logging (disables other output and stops early)\n    parser.add_argument(\"--mlperf-logging\", action=\"store_true\", default=False)\n    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107\n    parser.add_argument(\"--mlperf-acc-threshold\", type=float, default=0.0)\n    # stop at target AUC Terabyte (no subsampling) 0.8025\n    parser.add_argument(\"--mlperf-auc-threshold\", type=float, default=0.0)\n    args = parser.parse_args()\n\n    if args.dataset_multiprocessing:\n        assert sys.version_info[0] >= 3 and sys.version_info[1] > 7, (\n            \"The dataset_multiprocessing \"\n            + \"flag is susceptible to a bug in Python 3.7 and under. \"\n            + \"https://github.com/facebookresearch/dlrm/issues/172\"\n        )\n\n    ### some basic setup ###\n    # WARNING: to obtain exactly the same initialization for\n    # the weights we need to start from the same random seed.\n    np.random.seed(args.numpy_rand_seed)\n\n    np.set_printoptions(precision=args.print_precision)\n    if args.test_mini_batch_size < 0:\n        # if the parameter is not set, use the training batch size\n        args.test_mini_batch_size = args.mini_batch_size\n    if args.test_num_workers < 0:\n        # if the parameter is not set, use the same parameter for training\n        args.test_num_workers = args.num_workers\n\n    use_gpu = args.use_gpu\n    if use_gpu:\n        device_opt = core.DeviceOption(workspace.GpuDeviceType, 0)\n        ngpus = workspace.NumGpuDevices()  # 1\n        print(\"Using {} GPU(s)...\".format(ngpus))\n    else:\n        device_opt = core.DeviceOption(caffe2_pb2.CPU)\n        print(\"Using CPU...\")\n\n    ### prepare training data ###\n    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=\"-\")\n    if args.data_generation == \"dataset\":\n        if args.num_workers > 0 or args.test_num_workers > 0:\n            print(\n                \"WARNING: non default --num-workers or --test-num-workers options\"\n                + \" are not supported and will be ignored\"\n            )\n        if args.mini_batch_size != args.test_mini_batch_size:\n            print(\n                \"WARNING: non default ----test-mini-batch-size option\"\n                + \" is not supported and will be ignored\"\n            )\n\n        # input and target from dataset\n\n        train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(\n            args,\n            offset_to_length_converter=True,\n        )\n\n        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n\n        nbatches_test = len(test_ld)\n\n        ln_emb = train_data.counts\n        m_den = train_data.m_den\n\n        # enforce maximum limit on number of vectors per embedding\n        if args.max_ind_range > 0:\n            ln_emb = np.array(\n                list(\n                    map(\n                        lambda x: x if x < args.max_ind_range else args.max_ind_range,\n                        ln_emb,\n                    )\n                )\n            )\n        ln_bot[0] = m_den\n\n    else:\n        if args.num_workers > 0 or args.test_num_workers > 0:\n            print(\n                \"WARNING: non default --num-workers or --test-num-workers options\"\n                + \" are not supported and will be ignored\"\n            )\n        if args.mini_batch_size != args.test_mini_batch_size:\n            print(\n                \"WARNING: non default ----test-mini-batch-size option\"\n                + \" is not supported and will be ignored\"\n            )\n\n        # input and target at random\n        ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=\"-\")\n        m_den = ln_bot[0]\n        train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(\n            args,\n            ln_emb,\n            m_den,\n            offset_to_length_converter=True,\n        )\n        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n        nbatches_test = len(test_ld)\n        # table_feature_map = {idx : idx for idx in range(len(ln_emb))}\n\n    ### parse command line arguments ###\n    m_spa = args.arch_sparse_feature_size\n    ln_emb = np.asarray(ln_emb)\n    num_fea = ln_emb.size + 1  # num sparse + num dense features\n    m_den_out = ln_bot[ln_bot.size - 1]\n    if args.arch_interaction_op == \"dot\":\n        # approach 1: all\n        # num_int = num_fea * num_fea + m_den_out\n        # approach 2: unique\n        if args.arch_interaction_itself:\n            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out\n        else:\n            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out\n    elif args.arch_interaction_op == \"cat\":\n        num_int = num_fea * m_den_out\n    else:\n        sys.exit(\n            \"ERROR: --arch-interaction-op=\"\n            + args.arch_interaction_op\n            + \" is not supported\"\n        )\n    arch_mlp_top_adjusted = str(num_int) + \"-\" + args.arch_mlp_top\n    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=\"-\")\n    # sanity check: feature sizes and mlp dimensions must match\n    if m_den != ln_bot[0]:\n        sys.exit(\n            \"ERROR: arch-dense-feature-size \"\n            + str(m_den)\n            + \" does not match first dim of bottom mlp \"\n            + str(ln_bot[0])\n        )\n    if m_spa != m_den_out:\n        sys.exit(\n            \"ERROR: arch-sparse-feature-size \"\n            + str(m_spa)\n            + \" does not match last dim of bottom mlp \"\n            + str(m_den_out)\n        )\n    if num_int != ln_top[0]:\n        sys.exit(\n            \"ERROR: # of feature interactions \"\n            + str(num_int)\n            + \" does not match first dim of top mlp \"\n            + str(ln_top[0])\n        )\n\n    # test prints (model arch)\n    if args.debug_mode:\n        print(\"model arch:\")\n        print(\n            \"mlp top arch \"\n            + str(ln_top.size - 1)\n            + \" layers, with input to output dimensions:\"\n        )\n        print(ln_top)\n\n        print(\"# of interactions\")\n        print(num_int)\n        print(\n            \"mlp bot arch \"\n            + str(ln_bot.size - 1)\n            + \" layers, with input to output dimensions:\"\n        )\n        print(ln_bot)\n        print(\"# of features (sparse and dense)\")\n        print(num_fea)\n        print(\"dense feature size\")\n        print(m_den)\n        print(\"sparse feature size\")\n        print(m_spa)\n        print(\n            \"# of embeddings (= # of sparse features) \"\n            + str(ln_emb.size)\n            + \", with dimensions \"\n            + str(m_spa)\n            + \"x:\"\n        )\n        print(ln_emb)\n\n        print(\"data (inputs and targets):\")\n        for j, inputBatch in enumerate(train_ld):\n            lX_j, lS_l_j, lS_i_j, lT_j = inputBatch\n            print(\"mini-batch: %d\" % j)\n            print(lX_j)\n            print(lS_l_j)\n            print(lS_i_j)\n            print(lT_j)\n\n    ### construct the neural network specified above ###\n    # WARNING: to obtain exactly the same initialization for\n    # the weights we need to start from the same random seed.\n    # np.random.seed(args.numpy_rand_seed)\n    ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1\n    flag_types_shapes = args.save_onnx or args.save_proto_types_shapes\n    flag_forward_ops = not (use_gpu and ndevices > 1)\n    with core.DeviceScope(device_opt):\n        dlrm = DLRM_Net(\n            m_spa,\n            ln_emb,\n            ln_bot,\n            ln_top,\n            args.arch_interaction_op,\n            arch_interaction_itself=args.arch_interaction_itself,\n            sigmoid_bot=-1,\n            sigmoid_top=ln_top.size - 1,\n            save_onnx=flag_types_shapes,\n            ndevices=ndevices,\n            # forward_ops = flag_forward_ops\n            enable_prof=args.enable_profiling,\n            weighted_pooling=args.weighted_pooling,\n            emb_optimizer=args.optimizer,\n        )\n    # load nccl if using multiple devices\n    if args.sync_dense_params and ndevices > 1:\n        dyndep.InitOpsLibrary(\"//caffe2/caffe2/contrib/nccl:nccl_ops\")\n    # set the net type for better performance (dag, async_scheduling, etc)\n    if args.caffe2_net_type:\n        dlrm.parameters().net.Proto().type = args.caffe2_net_type\n    # plot compute graph\n    if args.plot_compute_graph:\n        graph = net_drawer.GetPydotGraph(\n            dlrm.parameters().net, \"dlrm_s_caffe2_graph\", \"BT\"\n        )\n        graph.write_pdf(graph.get_name() + \".pdf\")\n    # test prints\n    if args.debug_mode:\n        print(\"initial parameters (weights and bias):\")\n        dlrm.print_weights()\n\n    # add training loss if needed\n    if not args.inference_only:\n        with core.DeviceScope(device_opt):\n            # specify the loss function\n            nd = 1.0 if dlrm.ndevices <= 1 else 1.0 / dlrm.ndevices  # 1\n            if args.loss_function == \"mse\":\n                dlrm.MSEloss(scale=nd)\n            elif args.loss_function == \"bce\":\n                dlrm.BCEloss(scale=nd, threshold=args.loss_threshold)\n            else:\n                sys.exit(\n                    \"ERROR: --loss-function=\" + args.loss_function + \" is not supported\"\n                )\n\n            # define test net (as train net without gradients)\n            dlrm.test_net = core.Net(copy.deepcopy(dlrm.model.net.Proto()))\n\n            # specify the optimizer algorithm\n            if args.optimizer == \"sgd\":\n                dlrm.sgd_optimizer(\n                    args.learning_rate, sync_dense_params=args.sync_dense_params\n                )\n            elif args.optimizer in [\"adagrad\", \"rwsadagrad\"]:\n                dlrm.adagrad_optimizer(\n                    args.learning_rate, sync_dense_params=args.sync_dense_params\n                )\n            else:\n                sys.exit(\n                    \"\"\"ERROR: Select an optimizer for\n                                embedding tables : 'sgd', 'adagrad',\n                                or 'rwsadagrad' \"\"\"\n                )\n\n    # init/create\n    X, lS_l, lS_i, T = next(\n        iter(train_ld)\n    )  # does not affect the enumerate(train_ld) in the main loop\n    dlrm.create(X, lS_l, lS_i, T.int())\n\n    ### main loop ###\n    best_gA_test = 0\n    best_auc_test = 0\n    total_time = 0\n    total_loss = 0\n    total_accu = 0\n    total_iter = 0\n    total_samp = 0\n    k = 0\n\n    print(\"time/loss/accuracy (if enabled):\")\n    while k < args.nepochs:\n        j = 0\n        for j, inputBatch in enumerate(train_ld):\n            # forward and backward pass, where the latter runs only\n            # when gradients and loss have been added to the net\n            time1 = time.time()\n            lX_j, lS_l_j, lS_i_j, lT_j = inputBatch\n            lT_j = lT_j.int() if args.loss_function == \"bce\" else lT_j\n            dlrm.run(lX_j, lS_l_j, lS_i_j, lT_j)\n\n            time2 = time.time()\n            total_time += time2 - time1\n\n            # compte loss and accuracy\n            Z = dlrm.get_output()  # numpy array\n            T = lT_j.numpy()\n            \"\"\"\n            # debug prints\n            print(\"output and loss\")\n            print(Z)\n            print(dlrm.get_loss())\n            \"\"\"\n            mbs = T.shape[0]  # = args.mini_batch_size except maybe for last\n            A = np.sum((np.round(Z, 0) == T).astype(np.uint8))\n            total_accu += 0 if args.inference_only else A\n            total_loss += 0 if args.inference_only else dlrm.get_loss() * mbs\n            total_iter += 1\n            total_samp += mbs\n\n            # print time, loss and accuracy\n            should_print = ((j + 1) % args.print_freq == 0) or (j + 1 == nbatches)\n            should_test = (\n                (args.test_freq > 0)\n                and (args.data_generation in [\"dataset\", \"random\"])\n                and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))\n            )\n            if should_print or should_test:\n                gT = 1000.0 * total_time / total_iter if args.print_time else -1\n                total_time = 0\n\n                gA = total_accu / total_samp\n                total_accu = 0\n\n                gL = total_loss / total_samp\n                total_loss = 0\n\n                str_run_type = \"inference\" if args.inference_only else \"training\"\n                print(\n                    \"Finished {} it {}/{} of epoch {}, {:.2f} ms/it,\".format(\n                        str_run_type, j + 1, nbatches, k, gT\n                    )\n                    + \" loss {:.6f}\".format(gL)\n                )\n                total_iter = 0\n                total_samp = 0\n                # debug prints\n                # print(Z)\n                # print(T)\n\n                # testing\n                if should_test and not args.inference_only:\n                    # don't measure training iter time in a test iteration\n                    if args.mlperf_logging:\n                        previous_iteration_time = None\n\n                    test_accu = 0\n                    test_loss = 0\n                    test_samp = 0\n\n                    if args.mlperf_logging:\n                        scores = []\n                        targets = []\n\n                    for i, testBatch in enumerate(test_ld):\n                        # early exit if nbatches was set by the user and was exceeded\n                        if nbatches > 0 and i >= nbatches:\n                            break\n\n                        # forward pass\n\n                        lX_test_i, lS_l_test_i, lS_i_test_i, lT_test_i = testBatch\n                        lT_test_i = (\n                            lT_test_i.int()\n                            if args.loss_function == \"bce\"\n                            else lT_test_i\n                        )\n                        dlrm.run(\n                            lX_test_i,\n                            lS_l_test_i,\n                            lS_i_test_i,\n                            lT_test_i,\n                            test_net=True,\n                        )\n\n                        Z_test = dlrm.get_output()\n                        T_test = lT_test_i.numpy()\n\n                        if args.mlperf_logging:\n                            scores.append(Z_test)\n                            targets.append(T_test)\n                        else:\n                            # compte loss and accuracy\n                            L_test = dlrm.get_loss()\n                            mbs_test = T_test.shape[0]  # = mini_batch_size except last\n                            A_test = np.sum(\n                                (np.round(Z_test, 0) == T_test).astype(np.uint8)\n                            )\n                            test_accu += A_test\n                            test_loss += L_test * mbs_test\n                            test_samp += mbs_test\n\n                    # compute metrics (after test loop has finished)\n                    if args.mlperf_logging:\n                        validation_results = calculate_metrics(targets, scores)\n                        gA_test = validation_results[\"accuracy\"]\n                        gL_test = validation_results[\"loss\"]\n                    else:\n                        gA_test = test_accu / test_samp\n                        gL_test = test_loss / test_samp\n\n                    # print metrics\n                    is_best = gA_test > best_gA_test\n                    if is_best:\n                        best_gA_test = gA_test\n\n                    if args.mlperf_logging:\n                        is_best = validation_results[\"roc_auc\"] > best_auc_test\n                        if is_best:\n                            best_auc_test = validation_results[\"roc_auc\"]\n\n                        print(\n                            \"Testing at - {}/{} of epoch {},\".format(j + 1, nbatches, k)\n                            + \" loss {:.6f}, recall {:.4f}, precision {:.4f},\".format(\n                                validation_results[\"loss\"],\n                                validation_results[\"recall\"],\n                                validation_results[\"precision\"],\n                            )\n                            + \" f1 {:.4f}, ap {:.4f},\".format(\n                                validation_results[\"f1\"],\n                                validation_results[\"ap\"],\n                            )\n                            + \" auc {:.4f}, best auc {:.4f},\".format(\n                                validation_results[\"roc_auc\"], best_auc_test\n                            )\n                            + \" accuracy {:3.3f} %, best accuracy {:3.3f} %\".format(\n                                validation_results[\"accuracy\"] * 100, best_gA_test * 100\n                            )\n                        )\n                    else:\n                        print(\n                            \"Testing at - {}/{} of epoch {},\".format(j + 1, nbatches, 0)\n                            + \" loss {:.6f}, accuracy {:3.3f} %, best {:3.3f} %\".format(\n                                gL_test, gA_test * 100, best_gA_test * 100\n                            )\n                        )\n\n                    # check thresholds\n                    if (\n                        args.mlperf_logging\n                        and (args.mlperf_acc_threshold > 0)\n                        and (best_gA_test > args.mlperf_acc_threshold)\n                    ):\n                        print(\n                            \"MLPerf testing accuracy threshold \"\n                            + str(args.mlperf_acc_threshold)\n                            + \" reached, stop training\"\n                        )\n                        break\n\n                    if (\n                        args.mlperf_logging\n                        and (args.mlperf_auc_threshold > 0)\n                        and (best_auc_test > args.mlperf_auc_threshold)\n                    ):\n                        print(\n                            \"MLPerf testing auc threshold \"\n                            + str(args.mlperf_auc_threshold)\n                            + \" reached, stop training\"\n                        )\n                        break\n\n            j += 1  # nbatches\n        k += 1  # nepochs\n\n    # test prints\n    if not args.inference_only and args.debug_mode:\n        print(\"updated parameters (weights and bias):\")\n        dlrm.print_weights()\n\n    # build onnx model from caffe2\n    if args.save_onnx:\n        pnet = dlrm.parameters().net.Proto()\n        inet = dlrm.parameters().param_init_net.Proto()\n        value_info = dlrm.onnx_tsd  # None\n        # debug prints\n        # print(value_info)\n\n        # WARNING: Why Caffe2 to ONNX net transformation currently does not work?\n        # 1. ONNX does not support SparseLengthsSum operator directly. A workaround\n        # could be for the Caffe2 ONNX frontend to indirectly map this operator to\n        # Gather and ReducedSum ONNX operators, following the PyTorch approach.\n        c2f = caffe2.python.onnx.frontend.Caffe2Frontend()\n        dlrm_caffe2_onnx = c2f.caffe2_net_to_onnx_model(pnet, inet, value_info)\n        # check the onnx model\n        onnx.checker.check_model(dlrm_caffe2_onnx)\n\n        # save model to a file\n        with open(\"dlrm_s_caffe2.onnx\", \"w+\") as dlrm_caffe2_onnx_file:\n            dlrm_caffe2_onnx_file.write(str(dlrm_caffe2_onnx))\n\n    # build protobuf with types and shapes\n    if args.save_proto_types_shapes:\n        # add types and shapes to protobuf\n        __TYPE_MAPPING = {\n            onnx.TensorProto.FLOAT: caffe2_pb2.TensorProto.FLOAT,\n            onnx.TensorProto.UINT8: caffe2_pb2.TensorProto.UINT8,\n            onnx.TensorProto.INT8: caffe2_pb2.TensorProto.INT8,\n            onnx.TensorProto.UINT16: caffe2_pb2.TensorProto.UINT16,\n            onnx.TensorProto.INT16: caffe2_pb2.TensorProto.INT16,\n            onnx.TensorProto.INT32: caffe2_pb2.TensorProto.INT32,\n            onnx.TensorProto.INT64: caffe2_pb2.TensorProto.INT64,\n            onnx.TensorProto.STRING: caffe2_pb2.TensorProto.STRING,\n            onnx.TensorProto.BOOL: caffe2_pb2.TensorProto.BOOL,\n            onnx.TensorProto.FLOAT16: caffe2_pb2.TensorProto.FLOAT16,\n            onnx.TensorProto.DOUBLE: caffe2_pb2.TensorProto.DOUBLE,\n        }\n\n        pnet = dlrm.parameters().net.Proto()\n        arg = pnet.arg.add()\n        arg.name = \"input_shape_info\"\n        for i in pnet.external_input:\n            if i in dlrm.onnx_tsd:\n                onnx_dtype, shape = dlrm.onnx_tsd[i]\n                t = arg.tensors.add()\n                t.name = i\n                t.data_type = __TYPE_MAPPING[onnx_dtype]\n                t.dims.extend(shape)\n            else:\n                print(\"Warning: we don't have shape/type info for input: {}\".format(i))\n        # debug print\n        # print(pnet)\n\n        # export the protobuf with types and shapes\n        with open(\"dlrm_s_caffe2.proto\", \"w+\") as dlrm_s_proto_file:\n            dlrm_s_proto_file.write(str(pnet))\n\n        \"\"\"\n        # export the protobuf with types and shapes as well as weights\n        # see https://github.com/pytorch/pytorch/issues/9533\n        #save\n        net = dlrm.parameters().net\n        params = dlrm.parameters().params\n        init_net, predict_net = mobile_exporter.Export(workspace, net, params)\n        with open(\"dlrm_s_caffe2.predict\", \"wb\") as dlrm_s_predict_file:\n            dlrm_s_predict_file.write(predict_net.SerializeToString())\n        with open(\"dlrm_s_caffe2.init\", \"wb\") as dlrm_s_init_file:\n            dlrm_s_init_file.write(init_net.SerializeToString())\n        #load\n        net_def = caffe2_pb2.NetDef()\n        init_def= caffe2_pb2.NetDef()\n        with open(\"dlrm_s_caffe2.predict\", \"rb\") as dlrm_s_predict_file:\n            net_def.ParseFromString(dlrm_s_predict_file.read())\n            print(net_def)\n        with open(\"dlrm_s_caffe2.init\", \"rb\") as dlrm_s_init_file:\n            init_def.ParseFromString(dlrm_s_init_file.read())\n            print(init_def)\n        \"\"\"\n"
        },
        {
          "name": "dlrm_s_pytorch.py",
          "type": "blob",
          "size": 73.251953125,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Description: an implementation of a deep learning recommendation model (DLRM)\n# The model input consists of dense and sparse features. The former is a vector\n# of floating point values. The latter is a list of sparse indices into\n# embedding tables, which consist of vectors of floating point values.\n# The selected vectors are passed to mlp networks denoted by triangles,\n# in some cases the vectors are interacted through operators (Ops).\n#\n# output:\n#                         vector of values\n# model:                        |\n#                              /\\\n#                             /__\\\n#                               |\n#       _____________________> Op  <___________________\n#     /                         |                      \\\n#    /\\                        /\\                      /\\\n#   /__\\                      /__\\           ...      /__\\\n#    |                          |                       |\n#    |                         Op                      Op\n#    |                    ____/__\\_____           ____/__\\____\n#    |                   |_Emb_|____|__|    ...  |_Emb_|__|___|\n# input:\n# [ dense features ]     [sparse indices] , ..., [sparse indices]\n#\n# More precise definition of model layers:\n# 1) fully connected layers of an mlp\n# z = f(y)\n# y = Wx + b\n#\n# 2) embedding lookup (for a list of sparse indices p=[p1,...,pk])\n# z = Op(e1,...,ek)\n# obtain vectors e1=E[:,p1], ..., ek=E[:,pk]\n#\n# 3) Operator Op can be one of the following\n# Sum(e1,...,ek) = e1 + ... + ek\n# Dot(e1,...,ek) = [e1'e1, ..., e1'ek, ..., ek'e1, ..., ek'ek]\n# Cat(e1,...,ek) = [e1', ..., ek']'\n# where ' denotes transpose operation\n#\n# References:\n# [1] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,\n# Narayanan Sundaram, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,\n# Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii,\n# Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko,\n# Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong,\n# Misha Smelyanskiy, \"Deep Learning Recommendation Model for Personalization and\n# Recommendation Systems\", CoRR, arXiv:1906.00091, 2019\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\n\n# miscellaneous\nimport builtins\nimport datetime\nimport json\nimport sys\nimport time\n\n# onnx\n# The onnx import causes deprecation warnings every time workers\n# are spawned during testing. So, we filter out those warnings.\nimport warnings\n\n# data generation\nimport dlrm_data_pytorch as dp\n\n# For distributed run\nimport extend_distributed as ext_dist\nimport mlperf_logger\n\n# numpy\nimport numpy as np\nimport optim.rwsadagrad as RowWiseSparseAdagrad\nimport sklearn.metrics\n\n# pytorch\nimport torch\nimport torch.nn as nn\n\n# dataloader\ntry:\n    from internals import fbDataLoader, fbInputBatchFormatter\n\n    has_internal_libs = True\nexcept ImportError:\n    has_internal_libs = False\n\nfrom torch._ops import ops\nfrom torch.autograd.profiler import record_function\nfrom torch.nn.parallel.parallel_apply import parallel_apply\nfrom torch.nn.parallel.replicate import replicate\nfrom torch.nn.parallel.scatter_gather import gather, scatter\nfrom torch.nn.parameter import Parameter\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.utils.tensorboard import SummaryWriter\n\n# mixed-dimension trick\nfrom tricks.md_embedding_bag import md_solver, PrEmbeddingBag\n\n# quotient-remainder trick\nfrom tricks.qr_embedding_bag import QREmbeddingBag\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n    try:\n        import onnx\n    except ImportError as error:\n        print(\"Unable to import onnx. \", error)\n\n# from torchviz import make_dot\n# import torch.nn.functional as Functional\n# from torch.nn.parameter import Parameter\n\nexc = getattr(builtins, \"IOError\", \"FileNotFoundError\")\n\n\ndef time_wrap(use_gpu):\n    if use_gpu:\n        torch.cuda.synchronize()\n    return time.time()\n\n\ndef dlrm_wrap(X, lS_o, lS_i, use_gpu, device, ndevices=1):\n    with record_function(\"DLRM forward\"):\n        if use_gpu:  # .cuda()\n            # lS_i can be either a list of tensors or a stacked tensor.\n            # Handle each case below:\n            if ndevices == 1:\n                lS_i = (\n                    [S_i.to(device) for S_i in lS_i]\n                    if isinstance(lS_i, list)\n                    else lS_i.to(device)\n                )\n                lS_o = (\n                    [S_o.to(device) for S_o in lS_o]\n                    if isinstance(lS_o, list)\n                    else lS_o.to(device)\n                )\n        return dlrm(X.to(device), lS_o, lS_i)\n\n\ndef loss_fn_wrap(Z, T, use_gpu, device):\n    with record_function(\"DLRM loss compute\"):\n        if args.loss_function == \"mse\" or args.loss_function == \"bce\":\n            return dlrm.loss_fn(Z, T.to(device))\n        elif args.loss_function == \"wbce\":\n            loss_ws_ = dlrm.loss_ws[T.data.view(-1).long()].view_as(T).to(device)\n            loss_fn_ = dlrm.loss_fn(Z, T.to(device))\n            loss_sc_ = loss_ws_ * loss_fn_\n            return loss_sc_.mean()\n\n\n# The following function is a wrapper to avoid checking this multiple times in th\n# loop below.\ndef unpack_batch(b):\n    if args.data_generation == \"internal\":\n        return fbInputBatchFormatter(b, args.data_size)\n    else:\n        # Experiment with unweighted samples\n        return b[0], b[1], b[2], b[3], torch.ones(b[3].size()), None\n\n\nclass LRPolicyScheduler(_LRScheduler):\n    def __init__(self, optimizer, num_warmup_steps, decay_start_step, num_decay_steps):\n        self.num_warmup_steps = num_warmup_steps\n        self.decay_start_step = decay_start_step\n        self.decay_end_step = decay_start_step + num_decay_steps\n        self.num_decay_steps = num_decay_steps\n\n        if self.decay_start_step < self.num_warmup_steps:\n            sys.exit(\"Learning rate warmup must finish before the decay starts\")\n\n        super(LRPolicyScheduler, self).__init__(optimizer)\n\n    def get_lr(self):\n        step_count = self._step_count\n        if step_count < self.num_warmup_steps:\n            # warmup\n            scale = 1.0 - (self.num_warmup_steps - step_count) / self.num_warmup_steps\n            lr = [base_lr * scale for base_lr in self.base_lrs]\n            self.last_lr = lr\n        elif self.decay_start_step <= step_count and step_count < self.decay_end_step:\n            # decay\n            decayed_steps = step_count - self.decay_start_step\n            scale = ((self.num_decay_steps - decayed_steps) / self.num_decay_steps) ** 2\n            min_lr = 0.0000001\n            lr = [max(min_lr, base_lr * scale) for base_lr in self.base_lrs]\n            self.last_lr = lr\n        else:\n            if self.num_decay_steps > 0:\n                # freeze at last, either because we're after decay\n                # or because we're between warmup and decay\n                lr = self.last_lr\n            else:\n                # do not adjust\n                lr = self.base_lrs\n        return lr\n\n\n### define dlrm in PyTorch ###\nclass DLRM_Net(nn.Module):\n    def create_mlp(self, ln, sigmoid_layer):\n        # build MLP layer by layer\n        layers = nn.ModuleList()\n        for i in range(0, ln.size - 1):\n            n = ln[i]\n            m = ln[i + 1]\n\n            # construct fully connected operator\n            LL = nn.Linear(int(n), int(m), bias=True)\n\n            # initialize the weights\n            # with torch.no_grad():\n            # custom Xavier input, output or two-sided fill\n            mean = 0.0  # std_dev = np.sqrt(variance)\n            std_dev = np.sqrt(2 / (m + n))  # np.sqrt(1 / m) # np.sqrt(1 / n)\n            W = np.random.normal(mean, std_dev, size=(m, n)).astype(np.float32)\n            std_dev = np.sqrt(1 / m)  # np.sqrt(2 / (m + 1))\n            bt = np.random.normal(mean, std_dev, size=m).astype(np.float32)\n            # approach 1\n            LL.weight.data = torch.tensor(W, requires_grad=True)\n            LL.bias.data = torch.tensor(bt, requires_grad=True)\n            # approach 2\n            # LL.weight.data.copy_(torch.tensor(W))\n            # LL.bias.data.copy_(torch.tensor(bt))\n            # approach 3\n            # LL.weight = Parameter(torch.tensor(W),requires_grad=True)\n            # LL.bias = Parameter(torch.tensor(bt),requires_grad=True)\n            layers.append(LL)\n\n            # construct sigmoid or relu operator\n            if i == sigmoid_layer:\n                layers.append(nn.Sigmoid())\n            else:\n                layers.append(nn.ReLU())\n\n        # approach 1: use ModuleList\n        # return layers\n        # approach 2: use Sequential container to wrap all layers\n        return torch.nn.Sequential(*layers)\n\n    def create_emb(self, m, ln, weighted_pooling=None):\n        emb_l = nn.ModuleList()\n        v_W_l = []\n        for i in range(0, ln.size):\n            if ext_dist.my_size > 1:\n                if i not in self.local_emb_indices:\n                    continue\n            n = ln[i]\n\n            # construct embedding operator\n            if self.qr_flag and n > self.qr_threshold:\n                EE = QREmbeddingBag(\n                    n,\n                    m,\n                    self.qr_collisions,\n                    operation=self.qr_operation,\n                    mode=\"sum\",\n                    sparse=True,\n                )\n            elif self.md_flag and n > self.md_threshold:\n                base = max(m)\n                _m = m[i] if n > self.md_threshold else base\n                EE = PrEmbeddingBag(n, _m, base)\n                # use np initialization as below for consistency...\n                W = np.random.uniform(\n                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, _m)\n                ).astype(np.float32)\n                EE.embs.weight.data = torch.tensor(W, requires_grad=True)\n            else:\n                EE = nn.EmbeddingBag(n, m, mode=\"sum\", sparse=True)\n                # initialize embeddings\n                # nn.init.uniform_(EE.weight, a=-np.sqrt(1 / n), b=np.sqrt(1 / n))\n                W = np.random.uniform(\n                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)\n                ).astype(np.float32)\n                # approach 1\n                EE.weight.data = torch.tensor(W, requires_grad=True)\n                # approach 2\n                # EE.weight.data.copy_(torch.tensor(W))\n                # approach 3\n                # EE.weight = Parameter(torch.tensor(W),requires_grad=True)\n            if weighted_pooling is None:\n                v_W_l.append(None)\n            else:\n                v_W_l.append(torch.ones(n, dtype=torch.float32))\n            emb_l.append(EE)\n        return emb_l, v_W_l\n\n    def __init__(\n        self,\n        m_spa=None,\n        ln_emb=None,\n        ln_bot=None,\n        ln_top=None,\n        arch_interaction_op=None,\n        arch_interaction_itself=False,\n        sigmoid_bot=-1,\n        sigmoid_top=-1,\n        sync_dense_params=True,\n        loss_threshold=0.0,\n        ndevices=-1,\n        qr_flag=False,\n        qr_operation=\"mult\",\n        qr_collisions=0,\n        qr_threshold=200,\n        md_flag=False,\n        md_threshold=200,\n        weighted_pooling=None,\n        loss_function=\"bce\",\n    ):\n        super(DLRM_Net, self).__init__()\n\n        if (\n            (m_spa is not None)\n            and (ln_emb is not None)\n            and (ln_bot is not None)\n            and (ln_top is not None)\n            and (arch_interaction_op is not None)\n        ):\n            # save arguments\n            self.ndevices = ndevices\n            self.output_d = 0\n            self.parallel_model_batch_size = -1\n            self.parallel_model_is_not_prepared = True\n            self.arch_interaction_op = arch_interaction_op\n            self.arch_interaction_itself = arch_interaction_itself\n            self.sync_dense_params = sync_dense_params\n            self.loss_threshold = loss_threshold\n            self.loss_function = loss_function\n            if weighted_pooling is not None and weighted_pooling != \"fixed\":\n                self.weighted_pooling = \"learned\"\n            else:\n                self.weighted_pooling = weighted_pooling\n            # create variables for QR embedding if applicable\n            self.qr_flag = qr_flag\n            if self.qr_flag:\n                self.qr_collisions = qr_collisions\n                self.qr_operation = qr_operation\n                self.qr_threshold = qr_threshold\n            # create variables for MD embedding if applicable\n            self.md_flag = md_flag\n            if self.md_flag:\n                self.md_threshold = md_threshold\n\n            # If running distributed, get local slice of embedding tables\n            if ext_dist.my_size > 1:\n                n_emb = len(ln_emb)\n                if n_emb < ext_dist.my_size:\n                    sys.exit(\n                        \"only (%d) sparse features for (%d) devices, table partitions will fail\"\n                        % (n_emb, ext_dist.my_size)\n                    )\n                self.n_global_emb = n_emb\n                self.n_local_emb, self.n_emb_per_rank = ext_dist.get_split_lengths(\n                    n_emb\n                )\n                self.local_emb_slice = ext_dist.get_my_slice(n_emb)\n                self.local_emb_indices = list(range(n_emb))[self.local_emb_slice]\n\n            # create operators\n            if ndevices <= 1:\n                self.emb_l, w_list = self.create_emb(m_spa, ln_emb, weighted_pooling)\n                if self.weighted_pooling == \"learned\":\n                    self.v_W_l = nn.ParameterList()\n                    for w in w_list:\n                        self.v_W_l.append(Parameter(w))\n                else:\n                    self.v_W_l = w_list\n            self.bot_l = self.create_mlp(ln_bot, sigmoid_bot)\n            self.top_l = self.create_mlp(ln_top, sigmoid_top)\n\n            # quantization\n            self.quantize_emb = False\n            self.emb_l_q = []\n            self.quantize_bits = 32\n\n            # specify the loss function\n            if self.loss_function == \"mse\":\n                self.loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n            elif self.loss_function == \"bce\":\n                self.loss_fn = torch.nn.BCELoss(reduction=\"mean\")\n            elif self.loss_function == \"wbce\":\n                self.loss_ws = torch.tensor(\n                    np.fromstring(args.loss_weights, dtype=float, sep=\"-\")\n                )\n                self.loss_fn = torch.nn.BCELoss(reduction=\"none\")\n            else:\n                sys.exit(\n                    \"ERROR: --loss-function=\" + self.loss_function + \" is not supported\"\n                )\n\n    def apply_mlp(self, x, layers):\n        # approach 1: use ModuleList\n        # for layer in layers:\n        #     x = layer(x)\n        # return x\n        # approach 2: use Sequential container to wrap all layers\n        return layers(x)\n\n    def apply_emb(self, lS_o, lS_i, emb_l, v_W_l):\n        # WARNING: notice that we are processing the batch at once. We implicitly\n        # assume that the data is laid out such that:\n        # 1. each embedding is indexed with a group of sparse indices,\n        #   corresponding to a single lookup\n        # 2. for each embedding the lookups are further organized into a batch\n        # 3. for a list of embedding tables there is a list of batched lookups\n\n        ly = []\n        for k, sparse_index_group_batch in enumerate(lS_i):\n            sparse_offset_group_batch = lS_o[k]\n\n            # embedding lookup\n            # We are using EmbeddingBag, which implicitly uses sum operator.\n            # The embeddings are represented as tall matrices, with sum\n            # happening vertically across 0 axis, resulting in a row vector\n            # E = emb_l[k]\n\n            if v_W_l[k] is not None:\n                per_sample_weights = v_W_l[k].gather(0, sparse_index_group_batch)\n            else:\n                per_sample_weights = None\n\n            if self.quantize_emb:\n                s1 = self.emb_l_q[k].element_size() * self.emb_l_q[k].nelement()\n                s2 = self.emb_l_q[k].element_size() * self.emb_l_q[k].nelement()\n                print(\"quantized emb sizes:\", s1, s2)\n\n                if self.quantize_bits == 4:\n                    QV = ops.quantized.embedding_bag_4bit_rowwise_offsets(\n                        self.emb_l_q[k],\n                        sparse_index_group_batch,\n                        sparse_offset_group_batch,\n                        per_sample_weights=per_sample_weights,\n                    )\n                elif self.quantize_bits == 8:\n                    QV = ops.quantized.embedding_bag_byte_rowwise_offsets(\n                        self.emb_l_q[k],\n                        sparse_index_group_batch,\n                        sparse_offset_group_batch,\n                        per_sample_weights=per_sample_weights,\n                    )\n\n                ly.append(QV)\n            else:\n                E = emb_l[k]\n                V = E(\n                    sparse_index_group_batch,\n                    sparse_offset_group_batch,\n                    per_sample_weights=per_sample_weights,\n                )\n\n                ly.append(V)\n\n        # print(ly)\n        return ly\n\n    #  using quantizing functions from caffe2/aten/src/ATen/native/quantized/cpu\n    def quantize_embedding(self, bits):\n        n = len(self.emb_l)\n        self.emb_l_q = [None] * n\n        for k in range(n):\n            if bits == 4:\n                self.emb_l_q[k] = ops.quantized.embedding_bag_4bit_prepack(\n                    self.emb_l[k].weight\n                )\n            elif bits == 8:\n                self.emb_l_q[k] = ops.quantized.embedding_bag_byte_prepack(\n                    self.emb_l[k].weight\n                )\n            else:\n                return\n        self.emb_l = None\n        self.quantize_emb = True\n        self.quantize_bits = bits\n\n    def interact_features(self, x, ly):\n        if self.arch_interaction_op == \"dot\":\n            # concatenate dense and sparse features\n            (batch_size, d) = x.shape\n            T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))\n            # perform a dot product\n            Z = torch.bmm(T, torch.transpose(T, 1, 2))\n            # append dense feature with the interactions (into a row vector)\n            # approach 1: all\n            # Zflat = Z.view((batch_size, -1))\n            # approach 2: unique\n            _, ni, nj = Z.shape\n            # approach 1: tril_indices\n            # offset = 0 if self.arch_interaction_itself else -1\n            # li, lj = torch.tril_indices(ni, nj, offset=offset)\n            # approach 2: custom\n            offset = 1 if self.arch_interaction_itself else 0\n            li = torch.tensor([i for i in range(ni) for j in range(i + offset)])\n            lj = torch.tensor([j for i in range(nj) for j in range(i + offset)])\n            Zflat = Z[:, li, lj]\n            # concatenate dense features and interactions\n            R = torch.cat([x] + [Zflat], dim=1)\n        elif self.arch_interaction_op == \"cat\":\n            # concatenation features (into a row vector)\n            R = torch.cat([x] + ly, dim=1)\n        else:\n            sys.exit(\n                \"ERROR: --arch-interaction-op=\"\n                + self.arch_interaction_op\n                + \" is not supported\"\n            )\n\n        return R\n\n    def forward(self, dense_x, lS_o, lS_i):\n        if ext_dist.my_size > 1:\n            # multi-node multi-device run\n            return self.distributed_forward(dense_x, lS_o, lS_i)\n        elif self.ndevices <= 1:\n            # single device run\n            return self.sequential_forward(dense_x, lS_o, lS_i)\n        else:\n            # single-node multi-device run\n            return self.parallel_forward(dense_x, lS_o, lS_i)\n\n    def distributed_forward(self, dense_x, lS_o, lS_i):\n        batch_size = dense_x.size()[0]\n        # WARNING: # of ranks must be <= batch size in distributed_forward call\n        if batch_size < ext_dist.my_size:\n            sys.exit(\n                \"ERROR: batch_size (%d) must be larger than number of ranks (%d)\"\n                % (batch_size, ext_dist.my_size)\n            )\n        if batch_size % ext_dist.my_size != 0:\n            sys.exit(\n                \"ERROR: batch_size %d can not split across %d ranks evenly\"\n                % (batch_size, ext_dist.my_size)\n            )\n\n        dense_x = dense_x[ext_dist.get_my_slice(batch_size)]\n        lS_o = lS_o[self.local_emb_slice]\n        lS_i = lS_i[self.local_emb_slice]\n\n        if (len(self.emb_l) != len(lS_o)) or (len(self.emb_l) != len(lS_i)):\n            sys.exit(\n                \"ERROR: corrupted model input detected in distributed_forward call\"\n            )\n\n        # embeddings\n        with record_function(\"DLRM embedding forward\"):\n            ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)\n\n        # WARNING: Note that at this point we have the result of the embedding lookup\n        # for the entire batch on each rank. We would like to obtain partial results\n        # corresponding to all embedding lookups, but part of the batch on each rank.\n        # Therefore, matching the distribution of output of bottom mlp, so that both\n        # could be used for subsequent interactions on each device.\n        if len(self.emb_l) != len(ly):\n            sys.exit(\"ERROR: corrupted intermediate result in distributed_forward call\")\n\n        a2a_req = ext_dist.alltoall(ly, self.n_emb_per_rank)\n\n        with record_function(\"DLRM bottom nlp forward\"):\n            x = self.apply_mlp(dense_x, self.bot_l)\n\n        ly = a2a_req.wait()\n        ly = list(ly)\n\n        # interactions\n        with record_function(\"DLRM interaction forward\"):\n            z = self.interact_features(x, ly)\n\n        # top mlp\n        with record_function(\"DLRM top nlp forward\"):\n            p = self.apply_mlp(z, self.top_l)\n\n        # clamp output if needed\n        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:\n            z = torch.clamp(p, min=self.loss_threshold, max=(1.0 - self.loss_threshold))\n        else:\n            z = p\n\n        return z\n\n    def sequential_forward(self, dense_x, lS_o, lS_i):\n        # process dense features (using bottom mlp), resulting in a row vector\n        x = self.apply_mlp(dense_x, self.bot_l)\n        # debug prints\n        # print(\"intermediate\")\n        # print(x.detach().cpu().numpy())\n\n        # process sparse features(using embeddings), resulting in a list of row vectors\n        ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)\n        # for y in ly:\n        #     print(y.detach().cpu().numpy())\n\n        # interact features (dense and sparse)\n        z = self.interact_features(x, ly)\n        # print(z.detach().cpu().numpy())\n\n        # obtain probability of a click (using top mlp)\n        p = self.apply_mlp(z, self.top_l)\n\n        # clamp output if needed\n        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:\n            z = torch.clamp(p, min=self.loss_threshold, max=(1.0 - self.loss_threshold))\n        else:\n            z = p\n\n        return z\n\n    def parallel_forward(self, dense_x, lS_o, lS_i):\n        ### prepare model (overwrite) ###\n        # WARNING: # of devices must be >= batch size in parallel_forward call\n        batch_size = dense_x.size()[0]\n        ndevices = min(self.ndevices, batch_size, len(self.emb_l))\n        device_ids = range(ndevices)\n        # WARNING: must redistribute the model if mini-batch size changes(this is common\n        # for last mini-batch, when # of elements in the dataset/batch size is not even\n        if self.parallel_model_batch_size != batch_size:\n            self.parallel_model_is_not_prepared = True\n\n        if self.parallel_model_is_not_prepared or self.sync_dense_params:\n            # replicate mlp (data parallelism)\n            self.bot_l_replicas = replicate(self.bot_l, device_ids)\n            self.top_l_replicas = replicate(self.top_l, device_ids)\n            self.parallel_model_batch_size = batch_size\n\n        if self.parallel_model_is_not_prepared:\n            # distribute embeddings (model parallelism)\n            t_list = []\n            w_list = []\n            for k, emb in enumerate(self.emb_l):\n                d = torch.device(\"cuda:\" + str(k % ndevices))\n                t_list.append(emb.to(d))\n                if self.weighted_pooling == \"learned\":\n                    w_list.append(Parameter(self.v_W_l[k].to(d)))\n                elif self.weighted_pooling == \"fixed\":\n                    w_list.append(self.v_W_l[k].to(d))\n                else:\n                    w_list.append(None)\n            self.emb_l = nn.ModuleList(t_list)\n            if self.weighted_pooling == \"learned\":\n                self.v_W_l = nn.ParameterList(w_list)\n            else:\n                self.v_W_l = w_list\n            self.parallel_model_is_not_prepared = False\n\n        ### prepare input (overwrite) ###\n        # scatter dense features (data parallelism)\n        # print(dense_x.device)\n        dense_x = scatter(dense_x, device_ids, dim=0)\n        # distribute sparse features (model parallelism)\n        if (len(self.emb_l) != len(lS_o)) or (len(self.emb_l) != len(lS_i)):\n            sys.exit(\"ERROR: corrupted model input detected in parallel_forward call\")\n\n        t_list = []\n        i_list = []\n        for k, _ in enumerate(self.emb_l):\n            d = torch.device(\"cuda:\" + str(k % ndevices))\n            t_list.append(lS_o[k].to(d))\n            i_list.append(lS_i[k].to(d))\n        lS_o = t_list\n        lS_i = i_list\n\n        ### compute results in parallel ###\n        # bottom mlp\n        # WARNING: Note that the self.bot_l is a list of bottom mlp modules\n        # that have been replicated across devices, while dense_x is a tuple of dense\n        # inputs that has been scattered across devices on the first (batch) dimension.\n        # The output is a list of tensors scattered across devices according to the\n        # distribution of dense_x.\n        x = parallel_apply(self.bot_l_replicas, dense_x, None, device_ids)\n        # debug prints\n        # print(x)\n\n        # embeddings\n        ly = self.apply_emb(lS_o, lS_i, self.emb_l, self.v_W_l)\n        # debug prints\n        # print(ly)\n\n        # butterfly shuffle (implemented inefficiently for now)\n        # WARNING: Note that at this point we have the result of the embedding lookup\n        # for the entire batch on each device. We would like to obtain partial results\n        # corresponding to all embedding lookups, but part of the batch on each device.\n        # Therefore, matching the distribution of output of bottom mlp, so that both\n        # could be used for subsequent interactions on each device.\n        if len(self.emb_l) != len(ly):\n            sys.exit(\"ERROR: corrupted intermediate result in parallel_forward call\")\n\n        t_list = []\n        for k, _ in enumerate(self.emb_l):\n            d = torch.device(\"cuda:\" + str(k % ndevices))\n            y = scatter(ly[k], device_ids, dim=0)\n            t_list.append(y)\n        # adjust the list to be ordered per device\n        ly = list(map(lambda y: list(y), zip(*t_list)))\n        # debug prints\n        # print(ly)\n\n        # interactions\n        z = []\n        for k in range(ndevices):\n            zk = self.interact_features(x[k], ly[k])\n            z.append(zk)\n        # debug prints\n        # print(z)\n\n        # top mlp\n        # WARNING: Note that the self.top_l is a list of top mlp modules that\n        # have been replicated across devices, while z is a list of interaction results\n        # that by construction are scattered across devices on the first (batch) dim.\n        # The output is a list of tensors scattered across devices according to the\n        # distribution of z.\n        p = parallel_apply(self.top_l_replicas, z, None, device_ids)\n\n        ### gather the distributed results ###\n        p0 = gather(p, self.output_d, dim=0)\n\n        # clamp output if needed\n        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:\n            z0 = torch.clamp(\n                p0, min=self.loss_threshold, max=(1.0 - self.loss_threshold)\n            )\n        else:\n            z0 = p0\n\n        return z0\n\n\ndef dash_separated_ints(value):\n    vals = value.split(\"-\")\n    for val in vals:\n        try:\n            int(val)\n        except ValueError:\n            raise argparse.ArgumentTypeError(\n                \"%s is not a valid dash separated list of ints\" % value\n            )\n\n    return value\n\n\ndef dash_separated_floats(value):\n    vals = value.split(\"-\")\n    for val in vals:\n        try:\n            float(val)\n        except ValueError:\n            raise argparse.ArgumentTypeError(\n                \"%s is not a valid dash separated list of floats\" % value\n            )\n\n    return value\n\n\ndef inference(\n    args,\n    dlrm,\n    best_acc_test,\n    best_auc_test,\n    test_ld,\n    device,\n    use_gpu,\n    log_iter=-1,\n):\n    test_accu = 0\n    test_samp = 0\n\n    if args.mlperf_logging:\n        scores = []\n        targets = []\n\n    for i, testBatch in enumerate(test_ld):\n        # early exit if nbatches was set by the user and was exceeded\n        if nbatches > 0 and i >= nbatches:\n            break\n\n        X_test, lS_o_test, lS_i_test, T_test, W_test, CBPP_test = unpack_batch(\n            testBatch\n        )\n\n        # Skip the batch if batch size not multiple of total ranks\n        if ext_dist.my_size > 1 and X_test.size(0) % ext_dist.my_size != 0:\n            print(\"Warning: Skiping the batch %d with size %d\" % (i, X_test.size(0)))\n            continue\n\n        # forward pass\n        Z_test = dlrm_wrap(\n            X_test,\n            lS_o_test,\n            lS_i_test,\n            use_gpu,\n            device,\n            ndevices=ndevices,\n        )\n        ### gather the distributed results on each rank ###\n        # For some reason it requires explicit sync before all_gather call if\n        # tensor is on GPU memory\n        if Z_test.is_cuda:\n            torch.cuda.synchronize()\n        (_, batch_split_lengths) = ext_dist.get_split_lengths(X_test.size(0))\n        if ext_dist.my_size > 1:\n            Z_test = ext_dist.all_gather(Z_test, batch_split_lengths)\n\n        if args.mlperf_logging:\n            S_test = Z_test.detach().cpu().numpy()  # numpy array\n            T_test = T_test.detach().cpu().numpy()  # numpy array\n            scores.append(S_test)\n            targets.append(T_test)\n        else:\n            with record_function(\"DLRM accuracy compute\"):\n                # compute loss and accuracy\n                S_test = Z_test.detach().cpu().numpy()  # numpy array\n                T_test = T_test.detach().cpu().numpy()  # numpy array\n\n                mbs_test = T_test.shape[0]  # = mini_batch_size except last\n                A_test = np.sum((np.round(S_test, 0) == T_test).astype(np.uint8))\n\n                test_accu += A_test\n                test_samp += mbs_test\n\n    if args.mlperf_logging:\n        with record_function(\"DLRM mlperf sklearn metrics compute\"):\n            scores = np.concatenate(scores, axis=0)\n            targets = np.concatenate(targets, axis=0)\n\n            metrics = {\n                \"recall\": lambda y_true, y_score: sklearn.metrics.recall_score(\n                    y_true=y_true, y_pred=np.round(y_score)\n                ),\n                \"precision\": lambda y_true, y_score: sklearn.metrics.precision_score(\n                    y_true=y_true, y_pred=np.round(y_score)\n                ),\n                \"f1\": lambda y_true, y_score: sklearn.metrics.f1_score(\n                    y_true=y_true, y_pred=np.round(y_score)\n                ),\n                \"ap\": sklearn.metrics.average_precision_score,\n                \"roc_auc\": sklearn.metrics.roc_auc_score,\n                \"accuracy\": lambda y_true, y_score: sklearn.metrics.accuracy_score(\n                    y_true=y_true, y_pred=np.round(y_score)\n                ),\n            }\n\n        validation_results = {}\n        for metric_name, metric_function in metrics.items():\n            validation_results[metric_name] = metric_function(targets, scores)\n            writer.add_scalar(\n                \"mlperf-metrics-test/\" + metric_name,\n                validation_results[metric_name],\n                log_iter,\n            )\n        acc_test = validation_results[\"accuracy\"]\n    else:\n        acc_test = test_accu / test_samp\n        writer.add_scalar(\"Test/Acc\", acc_test, log_iter)\n\n    model_metrics_dict = {\n        \"nepochs\": args.nepochs,\n        \"nbatches\": nbatches,\n        \"nbatches_test\": nbatches_test,\n        \"state_dict\": dlrm.state_dict(),\n        \"test_acc\": acc_test,\n    }\n\n    if args.mlperf_logging:\n        is_best = validation_results[\"roc_auc\"] > best_auc_test\n        if is_best:\n            best_auc_test = validation_results[\"roc_auc\"]\n            model_metrics_dict[\"test_auc\"] = best_auc_test\n        print(\n            \"recall {:.4f}, precision {:.4f},\".format(\n                validation_results[\"recall\"],\n                validation_results[\"precision\"],\n            )\n            + \" f1 {:.4f}, ap {:.4f},\".format(\n                validation_results[\"f1\"], validation_results[\"ap\"]\n            )\n            + \" auc {:.4f}, best auc {:.4f},\".format(\n                validation_results[\"roc_auc\"], best_auc_test\n            )\n            + \" accuracy {:3.3f} %, best accuracy {:3.3f} %\".format(\n                validation_results[\"accuracy\"] * 100, best_acc_test * 100\n            ),\n            flush=True,\n        )\n    else:\n        is_best = acc_test > best_acc_test\n        if is_best:\n            best_acc_test = acc_test\n        print(\n            \" accuracy {:3.3f} %, best {:3.3f} %\".format(\n                acc_test * 100, best_acc_test * 100\n            ),\n            flush=True,\n        )\n    return model_metrics_dict, is_best\n\n\ndef run():\n    ### parse arguments ###\n    parser = argparse.ArgumentParser(\n        description=\"Train Deep Learning Recommendation Model (DLRM)\"\n    )\n    # model related parameters\n    parser.add_argument(\"--arch-sparse-feature-size\", type=int, default=2)\n    parser.add_argument(\n        \"--arch-embedding-size\", type=dash_separated_ints, default=\"4-3-2\"\n    )\n    # j will be replaced with the table number\n    parser.add_argument(\"--arch-mlp-bot\", type=dash_separated_ints, default=\"4-3-2\")\n    parser.add_argument(\"--arch-mlp-top\", type=dash_separated_ints, default=\"4-2-1\")\n    parser.add_argument(\n        \"--arch-interaction-op\", type=str, choices=[\"dot\", \"cat\"], default=\"dot\"\n    )\n    parser.add_argument(\"--arch-interaction-itself\", action=\"store_true\", default=False)\n    parser.add_argument(\"--weighted-pooling\", type=str, default=None)\n    # embedding table options\n    parser.add_argument(\"--md-flag\", action=\"store_true\", default=False)\n    parser.add_argument(\"--md-threshold\", type=int, default=200)\n    parser.add_argument(\"--md-temperature\", type=float, default=0.3)\n    parser.add_argument(\"--md-round-dims\", action=\"store_true\", default=False)\n    parser.add_argument(\"--qr-flag\", action=\"store_true\", default=False)\n    parser.add_argument(\"--qr-threshold\", type=int, default=200)\n    parser.add_argument(\"--qr-operation\", type=str, default=\"mult\")\n    parser.add_argument(\"--qr-collisions\", type=int, default=4)\n    # activations and loss\n    parser.add_argument(\"--activation-function\", type=str, default=\"relu\")\n    parser.add_argument(\"--loss-function\", type=str, default=\"mse\")  # or bce or wbce\n    parser.add_argument(\n        \"--loss-weights\", type=dash_separated_floats, default=\"1.0-1.0\"\n    )  # for wbce\n    parser.add_argument(\"--loss-threshold\", type=float, default=0.0)  # 1.0e-7\n    parser.add_argument(\"--round-targets\", type=bool, default=False)\n    # data\n    parser.add_argument(\"--data-size\", type=int, default=1)\n    parser.add_argument(\"--num-batches\", type=int, default=0)\n    parser.add_argument(\n        \"--data-generation\",\n        type=str,\n        choices=[\"random\", \"dataset\", \"internal\"],\n        default=\"random\",\n    )  # synthetic, dataset or internal\n    parser.add_argument(\n        \"--rand-data-dist\", type=str, default=\"uniform\"\n    )  # uniform or gaussian\n    parser.add_argument(\"--rand-data-min\", type=float, default=0)\n    parser.add_argument(\"--rand-data-max\", type=float, default=1)\n    parser.add_argument(\"--rand-data-mu\", type=float, default=-1)\n    parser.add_argument(\"--rand-data-sigma\", type=float, default=1)\n    parser.add_argument(\"--data-trace-file\", type=str, default=\"./input/dist_emb_j.log\")\n    parser.add_argument(\"--data-set\", type=str, default=\"kaggle\")  # or terabyte\n    parser.add_argument(\"--raw-data-file\", type=str, default=\"\")\n    parser.add_argument(\"--processed-data-file\", type=str, default=\"\")\n    parser.add_argument(\"--data-randomize\", type=str, default=\"total\")  # or day or none\n    parser.add_argument(\"--data-trace-enable-padding\", type=bool, default=False)\n    parser.add_argument(\"--max-ind-range\", type=int, default=-1)\n    parser.add_argument(\"--data-sub-sample-rate\", type=float, default=0.0)  # in [0, 1]\n    parser.add_argument(\"--num-indices-per-lookup\", type=int, default=10)\n    parser.add_argument(\"--num-indices-per-lookup-fixed\", type=bool, default=False)\n    parser.add_argument(\"--num-workers\", type=int, default=0)\n    parser.add_argument(\"--memory-map\", action=\"store_true\", default=False)\n    # training\n    parser.add_argument(\"--mini-batch-size\", type=int, default=1)\n    parser.add_argument(\"--nepochs\", type=int, default=1)\n    parser.add_argument(\"--learning-rate\", type=float, default=0.01)\n    parser.add_argument(\"--print-precision\", type=int, default=5)\n    parser.add_argument(\"--numpy-rand-seed\", type=int, default=123)\n    parser.add_argument(\"--sync-dense-params\", type=bool, default=True)\n    parser.add_argument(\"--optimizer\", type=str, default=\"sgd\")\n    parser.add_argument(\n        \"--dataset-multiprocessing\",\n        action=\"store_true\",\n        default=False,\n        help=\"The Kaggle dataset can be multiprocessed in an environment \\\n                        with more than 7 CPU cores and more than 20 GB of memory. \\n \\\n                        The Terabyte dataset can be multiprocessed in an environment \\\n                        with more than 24 CPU cores and at least 1 TB of memory.\",\n    )\n    # inference\n    parser.add_argument(\"--inference-only\", action=\"store_true\", default=False)\n    # quantize\n    parser.add_argument(\"--quantize-mlp-with-bit\", type=int, default=32)\n    parser.add_argument(\"--quantize-emb-with-bit\", type=int, default=32)\n    # onnx\n    parser.add_argument(\"--save-onnx\", action=\"store_true\", default=False)\n    # gpu\n    parser.add_argument(\"--use-gpu\", action=\"store_true\", default=False)\n    # distributed\n    parser.add_argument(\"--local_rank\", type=int, default=-1)\n    parser.add_argument(\"--dist-backend\", type=str, default=\"\")\n    # debugging and profiling\n    parser.add_argument(\"--print-freq\", type=int, default=1)\n    parser.add_argument(\"--test-freq\", type=int, default=-1)\n    parser.add_argument(\"--test-mini-batch-size\", type=int, default=-1)\n    parser.add_argument(\"--test-num-workers\", type=int, default=-1)\n    parser.add_argument(\"--print-time\", action=\"store_true\", default=False)\n    parser.add_argument(\"--print-wall-time\", action=\"store_true\", default=False)\n    parser.add_argument(\"--debug-mode\", action=\"store_true\", default=False)\n    parser.add_argument(\"--enable-profiling\", action=\"store_true\", default=False)\n    parser.add_argument(\"--plot-compute-graph\", action=\"store_true\", default=False)\n    parser.add_argument(\"--tensor-board-filename\", type=str, default=\"run_kaggle_pt\")\n    # store/load model\n    parser.add_argument(\"--save-model\", type=str, default=\"\")\n    parser.add_argument(\"--load-model\", type=str, default=\"\")\n    # mlperf logging (disables other output and stops early)\n    parser.add_argument(\"--mlperf-logging\", action=\"store_true\", default=False)\n    # stop at target accuracy Kaggle 0.789, Terabyte (sub-sampled=0.875) 0.8107\n    parser.add_argument(\"--mlperf-acc-threshold\", type=float, default=0.0)\n    # stop at target AUC Terabyte (no subsampling) 0.8025\n    parser.add_argument(\"--mlperf-auc-threshold\", type=float, default=0.0)\n    parser.add_argument(\"--mlperf-bin-loader\", action=\"store_true\", default=False)\n    parser.add_argument(\"--mlperf-bin-shuffle\", action=\"store_true\", default=False)\n    # mlperf gradient accumulation iterations\n    parser.add_argument(\"--mlperf-grad-accum-iter\", type=int, default=1)\n    # LR policy\n    parser.add_argument(\"--lr-num-warmup-steps\", type=int, default=0)\n    parser.add_argument(\"--lr-decay-start-step\", type=int, default=0)\n    parser.add_argument(\"--lr-num-decay-steps\", type=int, default=0)\n\n    global args\n    global nbatches\n    global nbatches_test\n    global writer\n    args = parser.parse_args()\n\n    if args.dataset_multiprocessing:\n        assert sys.version_info[0] >= 3 and sys.version_info[1] > 7, (\n            \"The dataset_multiprocessing \"\n            + \"flag is susceptible to a bug in Python 3.7 and under. \"\n            + \"https://github.com/facebookresearch/dlrm/issues/172\"\n        )\n\n    if args.mlperf_logging:\n        mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)\n        mlperf_logger.log_start(\n            key=mlperf_logger.constants.INIT_START, log_all_ranks=True\n        )\n\n    if args.weighted_pooling is not None:\n        if args.qr_flag:\n            sys.exit(\"ERROR: quotient remainder with weighted pooling is not supported\")\n        if args.md_flag:\n            sys.exit(\"ERROR: mixed dimensions with weighted pooling is not supported\")\n    if args.quantize_emb_with_bit in [4, 8]:\n        if args.qr_flag:\n            sys.exit(\n                \"ERROR: 4 and 8-bit quantization with quotient remainder is not supported\"\n            )\n        if args.md_flag:\n            sys.exit(\n                \"ERROR: 4 and 8-bit quantization with mixed dimensions is not supported\"\n            )\n        if args.use_gpu:\n            sys.exit(\"ERROR: 4 and 8-bit quantization on GPU is not supported\")\n\n    ### some basic setup ###\n    np.random.seed(args.numpy_rand_seed)\n    np.set_printoptions(precision=args.print_precision)\n    torch.set_printoptions(precision=args.print_precision)\n    torch.manual_seed(args.numpy_rand_seed)\n\n    if args.test_mini_batch_size < 0:\n        # if the parameter is not set, use the training batch size\n        args.test_mini_batch_size = args.mini_batch_size\n    if args.test_num_workers < 0:\n        # if the parameter is not set, use the same parameter for training\n        args.test_num_workers = args.num_workers\n\n    use_gpu = args.use_gpu and torch.cuda.is_available()\n\n    if not args.debug_mode:\n        ext_dist.init_distributed(\n            local_rank=args.local_rank, use_gpu=use_gpu, backend=args.dist_backend\n        )\n\n    if use_gpu:\n        torch.cuda.manual_seed_all(args.numpy_rand_seed)\n        torch.backends.cudnn.deterministic = True\n        if ext_dist.my_size > 1:\n            ngpus = 1\n            device = torch.device(\"cuda\", ext_dist.my_local_rank)\n        else:\n            ngpus = torch.cuda.device_count()\n            device = torch.device(\"cuda\", 0)\n        print(\"Using {} GPU(s)...\".format(ngpus))\n    else:\n        device = torch.device(\"cpu\")\n        print(\"Using CPU...\")\n\n    ### prepare training data ###\n    ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=\"-\")\n    # input data\n\n    if args.mlperf_logging:\n        mlperf_logger.barrier()\n        mlperf_logger.log_end(key=mlperf_logger.constants.INIT_STOP)\n        mlperf_logger.barrier()\n        mlperf_logger.log_start(key=mlperf_logger.constants.RUN_START)\n        mlperf_logger.barrier()\n\n    if args.data_generation == \"dataset\":\n        train_data, train_ld, test_data, test_ld = dp.make_criteo_data_and_loaders(args)\n        table_feature_map = {idx: idx for idx in range(len(train_data.counts))}\n        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n        nbatches_test = len(test_ld)\n\n        ln_emb = train_data.counts\n        # enforce maximum limit on number of vectors per embedding\n        if args.max_ind_range > 0:\n            ln_emb = np.array(\n                list(\n                    map(\n                        lambda x: x if x < args.max_ind_range else args.max_ind_range,\n                        ln_emb,\n                    )\n                )\n            )\n        else:\n            ln_emb = np.array(ln_emb)\n        m_den = train_data.m_den\n        ln_bot[0] = m_den\n    elif args.data_generation == \"internal\":\n        if not has_internal_libs:\n            raise Exception(\"Internal libraries are not available.\")\n        NUM_BATCHES = 5000\n        nbatches = args.num_batches if args.num_batches > 0 else NUM_BATCHES\n        train_ld, feature_to_num_embeddings = fbDataLoader(args.data_size, nbatches)\n        ln_emb = np.array(list(feature_to_num_embeddings.values()))\n        m_den = ln_bot[0]\n    else:\n        # input and target at random\n        ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=\"-\")\n        m_den = ln_bot[0]\n        train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(\n            args, ln_emb, m_den\n        )\n        nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n        nbatches_test = len(test_ld)\n\n    args.ln_emb = ln_emb.tolist()\n    if args.mlperf_logging:\n        print(\"command line args: \", json.dumps(vars(args)))\n\n    ### parse command line arguments ###\n    m_spa = args.arch_sparse_feature_size\n    ln_emb = np.asarray(ln_emb)\n    num_fea = ln_emb.size + 1  # num sparse + num dense features\n\n    m_den_out = ln_bot[ln_bot.size - 1]\n    if args.arch_interaction_op == \"dot\":\n        # approach 1: all\n        # num_int = num_fea * num_fea + m_den_out\n        # approach 2: unique\n        if args.arch_interaction_itself:\n            num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out\n        else:\n            num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out\n    elif args.arch_interaction_op == \"cat\":\n        num_int = num_fea * m_den_out\n    else:\n        sys.exit(\n            \"ERROR: --arch-interaction-op=\"\n            + args.arch_interaction_op\n            + \" is not supported\"\n        )\n    arch_mlp_top_adjusted = str(num_int) + \"-\" + args.arch_mlp_top\n    ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=\"-\")\n\n    # sanity check: feature sizes and mlp dimensions must match\n    if m_den != ln_bot[0]:\n        sys.exit(\n            \"ERROR: arch-dense-feature-size \"\n            + str(m_den)\n            + \" does not match first dim of bottom mlp \"\n            + str(ln_bot[0])\n        )\n    if args.qr_flag:\n        if args.qr_operation == \"concat\" and 2 * m_spa != m_den_out:\n            sys.exit(\n                \"ERROR: 2 arch-sparse-feature-size \"\n                + str(2 * m_spa)\n                + \" does not match last dim of bottom mlp \"\n                + str(m_den_out)\n                + \" (note that the last dim of bottom mlp must be 2x the embedding dim)\"\n            )\n        if args.qr_operation != \"concat\" and m_spa != m_den_out:\n            sys.exit(\n                \"ERROR: arch-sparse-feature-size \"\n                + str(m_spa)\n                + \" does not match last dim of bottom mlp \"\n                + str(m_den_out)\n            )\n    else:\n        if m_spa != m_den_out:\n            sys.exit(\n                \"ERROR: arch-sparse-feature-size \"\n                + str(m_spa)\n                + \" does not match last dim of bottom mlp \"\n                + str(m_den_out)\n            )\n    if num_int != ln_top[0]:\n        sys.exit(\n            \"ERROR: # of feature interactions \"\n            + str(num_int)\n            + \" does not match first dimension of top mlp \"\n            + str(ln_top[0])\n        )\n\n    # assign mixed dimensions if applicable\n    if args.md_flag:\n        m_spa = md_solver(\n            torch.tensor(ln_emb),\n            args.md_temperature,  # alpha\n            d0=m_spa,\n            round_dim=args.md_round_dims,\n        ).tolist()\n\n    # test prints (model arch)\n    if args.debug_mode:\n        print(\"model arch:\")\n        print(\n            \"mlp top arch \"\n            + str(ln_top.size - 1)\n            + \" layers, with input to output dimensions:\"\n        )\n        print(ln_top)\n        print(\"# of interactions\")\n        print(num_int)\n        print(\n            \"mlp bot arch \"\n            + str(ln_bot.size - 1)\n            + \" layers, with input to output dimensions:\"\n        )\n        print(ln_bot)\n        print(\"# of features (sparse and dense)\")\n        print(num_fea)\n        print(\"dense feature size\")\n        print(m_den)\n        print(\"sparse feature size\")\n        print(m_spa)\n        print(\n            \"# of embeddings (= # of sparse features) \"\n            + str(ln_emb.size)\n            + \", with dimensions \"\n            + str(m_spa)\n            + \"x:\"\n        )\n        print(ln_emb)\n\n        print(\"data (inputs and targets):\")\n        for j, inputBatch in enumerate(train_ld):\n            X, lS_o, lS_i, T, W, CBPP = unpack_batch(inputBatch)\n\n            torch.set_printoptions(precision=4)\n            # early exit if nbatches was set by the user and has been exceeded\n            if nbatches > 0 and j >= nbatches:\n                break\n            print(\"mini-batch: %d\" % j)\n            print(X.detach().cpu())\n            # transform offsets to lengths when printing\n            print(\n                torch.IntTensor(\n                    [\n                        np.diff(\n                            S_o.detach().cpu().tolist() + list(lS_i[i].shape)\n                        ).tolist()\n                        for i, S_o in enumerate(lS_o)\n                    ]\n                )\n            )\n            print([S_i.detach().cpu() for S_i in lS_i])\n            print(T.detach().cpu())\n\n    global ndevices\n    ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1\n\n    ### construct the neural network specified above ###\n    # WARNING: to obtain exactly the same initialization for\n    # the weights we need to start from the same random seed.\n    # np.random.seed(args.numpy_rand_seed)\n    global dlrm\n    dlrm = DLRM_Net(\n        m_spa,\n        ln_emb,\n        ln_bot,\n        ln_top,\n        arch_interaction_op=args.arch_interaction_op,\n        arch_interaction_itself=args.arch_interaction_itself,\n        sigmoid_bot=-1,\n        sigmoid_top=ln_top.size - 2,\n        sync_dense_params=args.sync_dense_params,\n        loss_threshold=args.loss_threshold,\n        ndevices=ndevices,\n        qr_flag=args.qr_flag,\n        qr_operation=args.qr_operation,\n        qr_collisions=args.qr_collisions,\n        qr_threshold=args.qr_threshold,\n        md_flag=args.md_flag,\n        md_threshold=args.md_threshold,\n        weighted_pooling=args.weighted_pooling,\n        loss_function=args.loss_function,\n    )\n\n    # test prints\n    if args.debug_mode:\n        print(\"initial parameters (weights and bias):\")\n        for param in dlrm.parameters():\n            print(param.detach().cpu().numpy())\n        # print(dlrm)\n\n    if use_gpu:\n        # Custom Model-Data Parallel\n        # the mlps are replicated and use data parallelism, while\n        # the embeddings are distributed and use model parallelism\n        dlrm = dlrm.to(device)  # .cuda()\n        if dlrm.ndevices > 1:\n            dlrm.emb_l, dlrm.v_W_l = dlrm.create_emb(\n                m_spa, ln_emb, args.weighted_pooling\n            )\n        else:\n            if dlrm.weighted_pooling == \"fixed\":\n                for k, w in enumerate(dlrm.v_W_l):\n                    dlrm.v_W_l[k] = w.cuda()\n\n    # distribute data parallel mlps\n    if ext_dist.my_size > 1:\n        if use_gpu:\n            device_ids = [ext_dist.my_local_rank]\n            dlrm.bot_l = ext_dist.DDP(dlrm.bot_l, device_ids=device_ids)\n            dlrm.top_l = ext_dist.DDP(dlrm.top_l, device_ids=device_ids)\n        else:\n            dlrm.bot_l = ext_dist.DDP(dlrm.bot_l)\n            dlrm.top_l = ext_dist.DDP(dlrm.top_l)\n\n    if not args.inference_only:\n        if use_gpu and args.optimizer in [\"rwsadagrad\", \"adagrad\"]:\n            sys.exit(\"GPU version of Adagrad is not supported by PyTorch.\")\n        # specify the optimizer algorithm\n        opts = {\n            \"sgd\": torch.optim.SGD,\n            \"rwsadagrad\": RowWiseSparseAdagrad.RWSAdagrad,\n            \"adagrad\": torch.optim.Adagrad,\n        }\n\n        parameters = (\n            dlrm.parameters()\n            if ext_dist.my_size == 1\n            else [\n                {\n                    \"params\": [p for emb in dlrm.emb_l for p in emb.parameters()],\n                    \"lr\": args.learning_rate,\n                },\n                # TODO check this lr setup\n                # bottom mlp has no data parallelism\n                # need to check how do we deal with top mlp\n                {\n                    \"params\": dlrm.bot_l.parameters(),\n                    \"lr\": args.learning_rate,\n                },\n                {\n                    \"params\": dlrm.top_l.parameters(),\n                    \"lr\": args.learning_rate,\n                },\n            ]\n        )\n        optimizer = opts[args.optimizer](parameters, lr=args.learning_rate)\n        lr_scheduler = LRPolicyScheduler(\n            optimizer,\n            args.lr_num_warmup_steps,\n            args.lr_decay_start_step,\n            args.lr_num_decay_steps,\n        )\n\n    ### main loop ###\n\n    # training or inference\n    best_acc_test = 0\n    best_auc_test = 0\n    skip_upto_epoch = 0\n    skip_upto_batch = 0\n    total_time = 0\n    total_loss = 0\n    total_iter = 0\n    total_samp = 0\n\n    if args.mlperf_logging:\n        mlperf_logger.mlperf_submission_log(\"dlrm\")\n        mlperf_logger.log_event(\n            key=mlperf_logger.constants.SEED, value=args.numpy_rand_seed\n        )\n        mlperf_logger.log_event(\n            key=mlperf_logger.constants.GLOBAL_BATCH_SIZE, value=args.mini_batch_size\n        )\n\n    # Load model is specified\n    if not (args.load_model == \"\"):\n        print(\"Loading saved model {}\".format(args.load_model))\n        if use_gpu:\n            if dlrm.ndevices > 1:\n                # NOTE: when targeting inference on multiple GPUs,\n                # load the model as is on CPU or GPU, with the move\n                # to multiple GPUs to be done in parallel_forward\n                ld_model = torch.load(args.load_model)\n            else:\n                # NOTE: when targeting inference on single GPU,\n                # note that the call to .to(device) has already happened\n                ld_model = torch.load(\n                    args.load_model,\n                    map_location=torch.device(\"cuda\"),\n                    # map_location=lambda storage, loc: storage.cuda(0)\n                )\n        else:\n            # when targeting inference on CPU\n            ld_model = torch.load(args.load_model, map_location=torch.device(\"cpu\"))\n        dlrm.load_state_dict(ld_model[\"state_dict\"])\n        ld_j = ld_model[\"iter\"]\n        ld_k = ld_model[\"epoch\"]\n        ld_nepochs = ld_model[\"nepochs\"]\n        ld_nbatches = ld_model[\"nbatches\"]\n        ld_nbatches_test = ld_model[\"nbatches_test\"]\n        ld_train_loss = ld_model[\"train_loss\"]\n        ld_total_loss = ld_model[\"total_loss\"]\n        if args.mlperf_logging:\n            ld_gAUC_test = ld_model[\"test_auc\"]\n        ld_acc_test = ld_model[\"test_acc\"]\n        if not args.inference_only:\n            optimizer.load_state_dict(ld_model[\"opt_state_dict\"])\n            best_acc_test = ld_acc_test\n            total_loss = ld_total_loss\n            skip_upto_epoch = ld_k  # epochs\n            skip_upto_batch = ld_j  # batches\n        else:\n            args.print_freq = ld_nbatches\n            args.test_freq = 0\n\n        print(\n            \"Saved at: epoch = {:d}/{:d}, batch = {:d}/{:d}, ntbatch = {:d}\".format(\n                ld_k, ld_nepochs, ld_j, ld_nbatches, ld_nbatches_test\n            )\n        )\n        print(\n            \"Training state: loss = {:.6f}\".format(\n                ld_train_loss,\n            )\n        )\n        if args.mlperf_logging:\n            print(\n                \"Testing state: accuracy = {:3.3f} %, auc = {:.3f}\".format(\n                    ld_acc_test * 100, ld_gAUC_test\n                )\n            )\n        else:\n            print(\"Testing state: accuracy = {:3.3f} %\".format(ld_acc_test * 100))\n\n    if args.inference_only:\n        # Currently only dynamic quantization with INT8 and FP16 weights are\n        # supported for MLPs and INT4 and INT8 weights for EmbeddingBag\n        # post-training quantization during the inference.\n        # By default we don't do the quantization: quantize_{mlp,emb}_with_bit == 32 (FP32)\n        assert args.quantize_mlp_with_bit in [\n            8,\n            16,\n            32,\n        ], \"only support 8/16/32-bit but got {}\".format(args.quantize_mlp_with_bit)\n        assert args.quantize_emb_with_bit in [\n            4,\n            8,\n            32,\n        ], \"only support 4/8/32-bit but got {}\".format(args.quantize_emb_with_bit)\n        if args.quantize_mlp_with_bit != 32:\n            if args.quantize_mlp_with_bit in [8]:\n                quantize_dtype = torch.qint8\n            else:\n                quantize_dtype = torch.float16\n            dlrm = torch.quantization.quantize_dynamic(\n                dlrm, {torch.nn.Linear}, quantize_dtype\n            )\n        if args.quantize_emb_with_bit != 32:\n            dlrm.quantize_embedding(args.quantize_emb_with_bit)\n            # print(dlrm)\n\n    print(\"time/loss/accuracy (if enabled):\")\n\n    if args.mlperf_logging:\n        # LR is logged twice for now because of a compliance checker bug\n        mlperf_logger.log_event(\n            key=mlperf_logger.constants.OPT_BASE_LR, value=args.learning_rate\n        )\n        mlperf_logger.log_event(\n            key=mlperf_logger.constants.OPT_LR_WARMUP_STEPS,\n            value=args.lr_num_warmup_steps,\n        )\n\n        # use logging keys from the official HP table and not from the logging library\n        mlperf_logger.log_event(\n            key=\"sgd_opt_base_learning_rate\", value=args.learning_rate\n        )\n        mlperf_logger.log_event(\n            key=\"lr_decay_start_steps\", value=args.lr_decay_start_step\n        )\n        mlperf_logger.log_event(\n            key=\"sgd_opt_learning_rate_decay_steps\", value=args.lr_num_decay_steps\n        )\n        mlperf_logger.log_event(key=\"sgd_opt_learning_rate_decay_poly_power\", value=2)\n\n    tb_file = \"./\" + args.tensor_board_filename\n    writer = SummaryWriter(tb_file)\n\n    ext_dist.barrier()\n    with torch.autograd.profiler.profile(\n        args.enable_profiling, use_cuda=use_gpu, record_shapes=True\n    ) as prof:\n        if not args.inference_only:\n            k = 0\n            total_time_begin = 0\n            while k < args.nepochs:\n                if args.mlperf_logging:\n                    mlperf_logger.barrier()\n                    mlperf_logger.log_start(\n                        key=mlperf_logger.constants.BLOCK_START,\n                        metadata={\n                            mlperf_logger.constants.FIRST_EPOCH_NUM: (k + 1),\n                            mlperf_logger.constants.EPOCH_COUNT: 1,\n                        },\n                    )\n                    mlperf_logger.barrier()\n                    mlperf_logger.log_start(\n                        key=mlperf_logger.constants.EPOCH_START,\n                        metadata={mlperf_logger.constants.EPOCH_NUM: (k + 1)},\n                    )\n\n                if k < skip_upto_epoch:\n                    continue\n\n                if args.mlperf_logging:\n                    previous_iteration_time = None\n\n                for j, inputBatch in enumerate(train_ld):\n                    if j == 0 and args.save_onnx:\n                        X_onnx, lS_o_onnx, lS_i_onnx, _, _, _ = unpack_batch(inputBatch)\n\n                    if j < skip_upto_batch:\n                        continue\n\n                    X, lS_o, lS_i, T, W, CBPP = unpack_batch(inputBatch)\n\n                    if args.mlperf_logging:\n                        current_time = time_wrap(use_gpu)\n                        if previous_iteration_time:\n                            iteration_time = current_time - previous_iteration_time\n                        else:\n                            iteration_time = 0\n                        previous_iteration_time = current_time\n                    else:\n                        t1 = time_wrap(use_gpu)\n\n                    # early exit if nbatches was set by the user and has been exceeded\n                    if nbatches > 0 and j >= nbatches:\n                        break\n\n                    # Skip the batch if batch size not multiple of total ranks\n                    if ext_dist.my_size > 1 and X.size(0) % ext_dist.my_size != 0:\n                        print(\n                            \"Warning: Skiping the batch %d with size %d\"\n                            % (j, X.size(0))\n                        )\n                        continue\n\n                    mbs = T.shape[0]  # = args.mini_batch_size except maybe for last\n\n                    # forward pass\n                    Z = dlrm_wrap(\n                        X,\n                        lS_o,\n                        lS_i,\n                        use_gpu,\n                        device,\n                        ndevices=ndevices,\n                    )\n\n                    if ext_dist.my_size > 1:\n                        T = T[ext_dist.get_my_slice(mbs)]\n                        W = W[ext_dist.get_my_slice(mbs)]\n\n                    # loss\n                    E = loss_fn_wrap(Z, T, use_gpu, device)\n\n                    # compute loss and accuracy\n                    L = E.detach().cpu().numpy()  # numpy array\n                    # training accuracy is not disabled\n                    # S = Z.detach().cpu().numpy()  # numpy array\n                    # T = T.detach().cpu().numpy()  # numpy array\n\n                    # # print(\"res: \", S)\n\n                    # # print(\"j, train: BCE \", j, L)\n\n                    # mbs = T.shape[0]  # = args.mini_batch_size except maybe for last\n                    # A = np.sum((np.round(S, 0) == T).astype(np.uint8))\n\n                    with record_function(\"DLRM backward\"):\n                        # scaled error gradient propagation\n                        # (where we do not accumulate gradients across mini-batches)\n                        if (\n                            args.mlperf_logging\n                            and (j + 1) % args.mlperf_grad_accum_iter == 0\n                        ) or not args.mlperf_logging:\n                            optimizer.zero_grad()\n                        # backward pass\n                        E.backward()\n\n                        # optimizer\n                        if (\n                            args.mlperf_logging\n                            and (j + 1) % args.mlperf_grad_accum_iter == 0\n                        ) or not args.mlperf_logging:\n                            optimizer.step()\n                            lr_scheduler.step()\n\n                    if args.mlperf_logging:\n                        total_time += iteration_time\n                    else:\n                        t2 = time_wrap(use_gpu)\n                        total_time += t2 - t1\n\n                    total_loss += L * mbs\n                    total_iter += 1\n                    total_samp += mbs\n\n                    should_print = ((j + 1) % args.print_freq == 0) or (\n                        j + 1 == nbatches\n                    )\n                    should_test = (\n                        (args.test_freq > 0)\n                        and (args.data_generation in [\"dataset\", \"random\"])\n                        and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))\n                    )\n\n                    # print time, loss and accuracy\n                    if should_print or should_test:\n                        gT = 1000.0 * total_time / total_iter if args.print_time else -1\n                        total_time = 0\n\n                        train_loss = total_loss / total_samp\n                        total_loss = 0\n\n                        str_run_type = (\n                            \"inference\" if args.inference_only else \"training\"\n                        )\n\n                        wall_time = \"\"\n                        if args.print_wall_time:\n                            wall_time = \" ({})\".format(time.strftime(\"%H:%M\"))\n\n                        print(\n                            \"Finished {} it {}/{} of epoch {}, {:.2f} ms/it,\".format(\n                                str_run_type, j + 1, nbatches, k, gT\n                            )\n                            + \" loss {:.6f}\".format(train_loss)\n                            + wall_time,\n                            flush=True,\n                        )\n\n                        log_iter = nbatches * k + j + 1\n                        writer.add_scalar(\"Train/Loss\", train_loss, log_iter)\n\n                        total_iter = 0\n                        total_samp = 0\n\n                    # testing\n                    if should_test:\n                        epoch_num_float = (j + 1) / len(train_ld) + k + 1\n                        if args.mlperf_logging:\n                            mlperf_logger.barrier()\n                            mlperf_logger.log_start(\n                                key=mlperf_logger.constants.EVAL_START,\n                                metadata={\n                                    mlperf_logger.constants.EPOCH_NUM: epoch_num_float\n                                },\n                            )\n\n                        # don't measure training iter time in a test iteration\n                        if args.mlperf_logging:\n                            previous_iteration_time = None\n                        print(\n                            \"Testing at - {}/{} of epoch {},\".format(j + 1, nbatches, k)\n                        )\n                        model_metrics_dict, is_best = inference(\n                            args,\n                            dlrm,\n                            best_acc_test,\n                            best_auc_test,\n                            test_ld,\n                            device,\n                            use_gpu,\n                            log_iter,\n                        )\n\n                        if (\n                            is_best\n                            and not (args.save_model == \"\")\n                            and not args.inference_only\n                        ):\n                            model_metrics_dict[\"epoch\"] = k\n                            model_metrics_dict[\"iter\"] = j + 1\n                            model_metrics_dict[\"train_loss\"] = train_loss\n                            model_metrics_dict[\"total_loss\"] = total_loss\n                            model_metrics_dict[\"opt_state_dict\"] = (\n                                optimizer.state_dict()\n                            )\n                            print(\"Saving model to {}\".format(args.save_model))\n                            torch.save(model_metrics_dict, args.save_model)\n\n                        if args.mlperf_logging:\n                            mlperf_logger.barrier()\n                            mlperf_logger.log_end(\n                                key=mlperf_logger.constants.EVAL_STOP,\n                                metadata={\n                                    mlperf_logger.constants.EPOCH_NUM: epoch_num_float\n                                },\n                            )\n\n                        # Uncomment the line below to print out the total time with overhead\n                        # print(\"Total test time for this group: {}\" \\\n                        # .format(time_wrap(use_gpu) - accum_test_time_begin))\n\n                        if (\n                            args.mlperf_logging\n                            and (args.mlperf_acc_threshold > 0)\n                            and (best_acc_test > args.mlperf_acc_threshold)\n                        ):\n                            print(\n                                \"MLPerf testing accuracy threshold \"\n                                + str(args.mlperf_acc_threshold)\n                                + \" reached, stop training\"\n                            )\n                            break\n\n                        if (\n                            args.mlperf_logging\n                            and (args.mlperf_auc_threshold > 0)\n                            and (best_auc_test > args.mlperf_auc_threshold)\n                        ):\n                            print(\n                                \"MLPerf testing auc threshold \"\n                                + str(args.mlperf_auc_threshold)\n                                + \" reached, stop training\"\n                            )\n                            if args.mlperf_logging:\n                                mlperf_logger.barrier()\n                                mlperf_logger.log_end(\n                                    key=mlperf_logger.constants.RUN_STOP,\n                                    metadata={\n                                        mlperf_logger.constants.STATUS: mlperf_logger.constants.SUCCESS\n                                    },\n                                )\n                            break\n\n                if args.mlperf_logging:\n                    mlperf_logger.barrier()\n                    mlperf_logger.log_end(\n                        key=mlperf_logger.constants.EPOCH_STOP,\n                        metadata={mlperf_logger.constants.EPOCH_NUM: (k + 1)},\n                    )\n                    mlperf_logger.barrier()\n                    mlperf_logger.log_end(\n                        key=mlperf_logger.constants.BLOCK_STOP,\n                        metadata={mlperf_logger.constants.FIRST_EPOCH_NUM: (k + 1)},\n                    )\n                k += 1  # nepochs\n            if args.mlperf_logging and best_auc_test <= args.mlperf_auc_threshold:\n                mlperf_logger.barrier()\n                mlperf_logger.log_end(\n                    key=mlperf_logger.constants.RUN_STOP,\n                    metadata={\n                        mlperf_logger.constants.STATUS: mlperf_logger.constants.ABORTED\n                    },\n                )\n        else:\n            print(\"Testing for inference only\")\n            inference(\n                args,\n                dlrm,\n                best_acc_test,\n                best_auc_test,\n                test_ld,\n                device,\n                use_gpu,\n            )\n\n    # profiling\n    if args.enable_profiling:\n        time_stamp = str(datetime.datetime.now()).replace(\" \", \"_\")\n        with open(\"dlrm_s_pytorch\" + time_stamp + \"_shape.prof\", \"w\") as prof_f:\n            prof_f.write(\n                prof.key_averages(group_by_input_shape=True).table(\n                    sort_by=\"self_cpu_time_total\"\n                )\n            )\n        with open(\"dlrm_s_pytorch\" + time_stamp + \"_total.prof\", \"w\") as prof_f:\n            prof_f.write(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n        prof.export_chrome_trace(\"dlrm_s_pytorch\" + time_stamp + \".json\")\n        # print(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n\n    # plot compute graph\n    if args.plot_compute_graph:\n        sys.exit(\n            \"ERROR: Please install pytorchviz package in order to use the\"\n            + \" visualization. Then, uncomment its import above as well as\"\n            + \" three lines below and run the code again.\"\n        )\n        # V = Z.mean() if args.inference_only else E\n        # dot = make_dot(V, params=dict(dlrm.named_parameters()))\n        # dot.render('dlrm_s_pytorch_graph') # write .pdf file\n\n    # test prints\n    if not args.inference_only and args.debug_mode:\n        print(\"updated parameters (weights and bias):\")\n        for param in dlrm.parameters():\n            print(param.detach().cpu().numpy())\n\n    # export the model in onnx\n    if args.save_onnx:\n        \"\"\"\n        # workaround 1: tensor -> list\n        if torch.is_tensor(lS_i_onnx):\n            lS_i_onnx = [lS_i_onnx[j] for j in range(len(lS_i_onnx))]\n        # workaound 2: list -> tensor\n        lS_i_onnx = torch.stack(lS_i_onnx)\n        \"\"\"\n        # debug prints\n        # print(\"inputs\", X_onnx, lS_o_onnx, lS_i_onnx)\n        # print(\"output\", dlrm_wrap(X_onnx, lS_o_onnx, lS_i_onnx, use_gpu, device))\n        dlrm_pytorch_onnx_file = \"dlrm_s_pytorch.onnx\"\n        batch_size = X_onnx.shape[0]\n        print(\"X_onnx.shape\", X_onnx.shape)\n        if torch.is_tensor(lS_o_onnx):\n            print(\"lS_o_onnx.shape\", lS_o_onnx.shape)\n        else:\n            for oo in lS_o_onnx:\n                print(\"oo.shape\", oo.shape)\n        if torch.is_tensor(lS_i_onnx):\n            print(\"lS_i_onnx.shape\", lS_i_onnx.shape)\n        else:\n            for ii in lS_i_onnx:\n                print(\"ii.shape\", ii.shape)\n\n        # name inputs and outputs\n        o_inputs = (\n            [\"offsets\"]\n            if torch.is_tensor(lS_o_onnx)\n            else [\"offsets_\" + str(i) for i in range(len(lS_o_onnx))]\n        )\n        i_inputs = (\n            [\"indices\"]\n            if torch.is_tensor(lS_i_onnx)\n            else [\"indices_\" + str(i) for i in range(len(lS_i_onnx))]\n        )\n        all_inputs = [\"dense_x\"] + o_inputs + i_inputs\n        # debug prints\n        print(\"inputs\", all_inputs)\n\n        # create dynamic_axis dictionaries\n        do_inputs = (\n            [{\"offsets\": {1: \"batch_size\"}}]\n            if torch.is_tensor(lS_o_onnx)\n            else [\n                {\"offsets_\" + str(i): {0: \"batch_size\"}} for i in range(len(lS_o_onnx))\n            ]\n        )\n        di_inputs = (\n            [{\"indices\": {1: \"batch_size\"}}]\n            if torch.is_tensor(lS_i_onnx)\n            else [\n                {\"indices_\" + str(i): {0: \"batch_size\"}} for i in range(len(lS_i_onnx))\n            ]\n        )\n        dynamic_axes = {\"dense_x\": {0: \"batch_size\"}, \"pred\": {0: \"batch_size\"}}\n        for do in do_inputs:\n            dynamic_axes.update(do)\n        for di in di_inputs:\n            dynamic_axes.update(di)\n        # debug prints\n        print(dynamic_axes)\n        # export model\n        torch.onnx.export(\n            dlrm,\n            (X_onnx, lS_o_onnx, lS_i_onnx),\n            dlrm_pytorch_onnx_file,\n            verbose=True,\n            opset_version=11,\n            input_names=all_inputs,\n            output_names=[\"pred\"],\n            dynamic_axes=dynamic_axes,\n        )\n        # recover the model back\n        dlrm_pytorch_onnx = onnx.load(\"dlrm_s_pytorch.onnx\")\n        # check the onnx model\n        onnx.checker.check_model(dlrm_pytorch_onnx)\n    total_time_end = time_wrap(use_gpu)\n\n\nif __name__ == \"__main__\":\n    run()\n"
        },
        {
          "name": "extend_distributed.py",
          "type": "blob",
          "size": 18.9912109375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport builtins\nimport os\nimport sys\n\nimport torch\nimport torch.distributed as dist\nfrom torch.autograd import Function\nfrom torch.autograd.profiler import record_function\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\ntry:\n    import torch_ccl\nexcept ImportError as e:\n    # print(e)\n    torch_ccl = False\n\ntry:\n    import torch_ucc\nexcept ImportError as e:\n    torch_ucc = False\n\n\nmy_rank = -1\nmy_size = -1\nmy_local_rank = -1\nmy_local_size = -1\nalltoall_supported = False\na2a_impl = os.environ.get(\"DLRM_ALLTOALL_IMPL\", \"\")\n\nmyreq = None\n\n\ndef env2int(env_list, default=-1):\n    for e in env_list:\n        val = int(os.environ.get(e, -1))\n        if val >= 0:\n            return val\n    return default\n\n\ndef get_my_slice(n):\n    k, m = divmod(n, my_size)\n    return slice(\n        my_rank * k + min(my_rank, m), (my_rank + 1) * k + min(my_rank + 1, m), 1\n    )\n\n\ndef get_split_lengths(n):\n    k, m = divmod(n, my_size)\n    if m == 0:\n        splits = None\n        my_len = k\n    else:\n        splits = [(k + 1) if i < m else k for i in range(my_size)]\n        my_len = splits[my_rank]\n    return (my_len, splits)\n\n\ndef init_distributed(rank=-1, local_rank=-1, size=-1, use_gpu=False, backend=\"\"):\n    global myreq\n    global my_rank\n    global my_size\n    global my_local_rank\n    global my_local_size\n    global a2a_impl\n    global alltoall_supported\n\n    # guess MPI ranks from env (works for IMPI, OMPI and MVAPICH2)\n    num_mpi_ranks = env2int(\n        [\"PMI_SIZE\", \"OMPI_COMM_WORLD_SIZE\", \"MV2_COMM_WORLD_SIZE\", \"WORLD_SIZE\"]\n    )\n    if backend == \"\" and num_mpi_ranks > 1:\n        if torch_ccl and env2int([\"CCL_WORKER_COUNT\"]) > 0:\n            backend = \"ccl\"\n        elif use_gpu and dist.is_nccl_available():\n            backend = \"nccl\"\n        elif dist.is_mpi_available():\n            backend = \"mpi\"\n        else:\n            print(\n                \"WARNING: MPI multi-process launch detected but PyTorch MPI backend not available.\"\n            )\n            backend = \"gloo\"\n\n    if backend != \"\":\n        # guess Rank and size\n        if rank == -1:\n            rank = env2int(\n                [\"PMI_RANK\", \"OMPI_COMM_WORLD_RANK\", \"MV2_COMM_WORLD_RANK\", \"RANK\"], 0\n            )\n        if size == -1:\n            size = env2int(\n                [\n                    \"PMI_SIZE\",\n                    \"OMPI_COMM_WORLD_SIZE\",\n                    \"MV2_COMM_WORLD_SIZE\",\n                    \"WORLD_SIZE\",\n                ],\n                1,\n            )\n        if not os.environ.get(\"RANK\", None) and rank != -1:\n            os.environ[\"RANK\"] = str(rank)\n        if not os.environ.get(\"WORLD_SIZE\", None) and size != -1:\n            os.environ[\"WORLD_SIZE\"] = str(size)\n        if not os.environ.get(\"MASTER_PORT\", None):\n            os.environ[\"MASTER_PORT\"] = \"29500\"\n        if not os.environ.get(\"MASTER_ADDR\", None):\n            local_size = env2int(\n                [\n                    \"MPI_LOCALNRANKS\",\n                    \"OMPI_COMM_WORLD_LOCAL_SIZE\",\n                    \"MV2_COMM_WORLD_LOCAL_SIZE\",\n                ],\n                1,\n            )\n            if local_size != size and backend != \"mpi\":\n                print(\n                    \"Warning: Looks like distributed multinode run but MASTER_ADDR env not set, using '127.0.0.1' as default\"\n                )\n                print(\n                    \"If this run hangs, try exporting rank 0's hostname as MASTER_ADDR\"\n                )\n            os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n\n    if size > 1:\n        if local_rank == -1:\n            my_local_rank = env2int(\n                [\n                    \"MPI_LOCALRANKID\",\n                    \"OMPI_COMM_WORLD_LOCAL_RANK\",\n                    \"MV2_COMM_WORLD_LOCAL_RANK\",\n                    \"LOCAL_RANK\",\n                ],\n                0,\n            )\n        else:\n            my_local_rank = local_rank\n        my_local_size = env2int(\n            [\n                \"MPI_LOCALNRANKS\",\n                \"OMPI_COMM_WORLD_LOCAL_SIZE\",\n                \"MV2_COMM_WORLD_LOCAL_SIZE\",\n            ],\n            1,\n        )\n        if use_gpu:\n            if my_local_size > torch.cuda.device_count():\n                print(\n                    \"Not sufficient GPUs available... local_size = %d, ngpus = %d\"\n                    % (my_local_size, torch.cuda.device_count())\n                )\n                sys.exit(1)\n            torch.cuda.set_device(my_local_rank)\n        dist.init_process_group(backend, rank=rank, world_size=size)\n        my_rank = dist.get_rank()\n        my_size = dist.get_world_size()\n        if my_rank == 0:\n            print(\"Running on %d ranks using %s backend\" % (my_size, backend))\n        if hasattr(dist, \"all_to_all_single\"):\n            try:\n                t = torch.zeros([4])\n                if use_gpu:\n                    t = t.cuda()\n                dist.all_to_all_single(t, t)\n                alltoall_supported = True\n            except RuntimeError as err:\n                print(\"fail to enable all_to_all_single primitive: %s\" % err)\n        if a2a_impl == \"alltoall\" and alltoall_supported == False:\n            print(\n                \"Requested DLRM_ALLTOALL_IMPL=%s but backend %s does not support it, use scatter/gather based alltoall\"\n                % (a2a_impl, backend)\n            )\n            a2a_impl = \"scatter\"\n        if a2a_impl != \"\":\n            print(\"Using DLRM_ALLTOALL_IMPL=%s\" % a2a_impl)\n    else:\n        my_rank = 0\n        my_size = 1\n        my_local_rank = 0\n        my_local_size = 1\n    print_all(\n        \"world size: %d, current rank: %d, local rank: %d\"\n        % (my_size, my_rank, my_local_rank)\n    )\n    myreq = Request()\n\n\nclass Request(object):\n    def __init__(self):\n        self.req = None\n        self.tensor = None\n        self.WaitFunction = All2All_Scatter_Wait\n\n    def wait(self):\n        ret = self.WaitFunction.apply(*self.tensor)\n        self.req = None\n        self.tensor = None\n        return ret\n\n\nclass All2All_ScatterList_Req(Function):\n    @staticmethod\n    def forward(ctx, a2a_info, *inputs):\n        global myreq\n        batch_split_lengths = (\n            a2a_info.global_batch_partition_slices\n            if a2a_info.global_batch_partition_slices\n            else a2a_info.local_batch_num\n        )\n        table_split_lengths = (\n            a2a_info.global_table_wise_parition_slices\n            if a2a_info.global_table_wise_parition_slices\n            else [a2a_info.local_table_num] * my_size\n        )\n        gather_list = []\n        req_list = []\n        for i in range(my_size):\n            for j in range(table_split_lengths[i]):\n                out_tensor = inputs[0].new_empty(\n                    [a2a_info.local_batch_num, a2a_info.emb_dim]\n                )\n                scatter_list = (\n                    list(inputs[j].split(batch_split_lengths, dim=0))\n                    if i == my_rank\n                    else []\n                )\n                req = dist.scatter(out_tensor, scatter_list, src=i, async_op=True)\n                gather_list.append(out_tensor)\n                req_list.append(req)\n        myreq.req = req_list\n        myreq.tensor = tuple(gather_list)\n        myreq.a2a_info = a2a_info\n        return myreq.tensor\n\n    @staticmethod\n    def backward(ctx, *grad_output):\n        global myreq\n        for r in myreq.req:\n            r.wait()\n        myreq.req = None\n        grad_inputs = myreq.tensor\n        myreq.tensor = None\n        return (None, *grad_inputs)\n\n\nclass All2All_ScatterList_Wait(Function):\n    @staticmethod\n    def forward(ctx, *output):\n        global myreq\n        ctx.a2a_info = myreq.a2a_info\n        for r in myreq.req:\n            r.wait()\n        myreq.req = None\n        myreq.tensor = None\n        return output\n\n    @staticmethod\n    def backward(ctx, *grad_output):\n        global myreq\n        a2a_info = ctx.a2a_info\n        grad_output = [t.contiguous() for t in grad_output]\n        batch_split_lengths = (\n            a2a_info.global_batch_partition_slices\n            if a2a_info.global_batch_partition_slices\n            else [a2a_info.local_batch_num] * my_size\n        )\n        per_rank_table_splits = (\n            a2a_info.global_table_wise_parition_slices\n            if a2a_info.global_table_wise_parition_slices\n            else [a2a_info.local_table_num] * my_size\n        )\n        grad_inputs = [\n            grad_output[0].new_empty([ctx.a2a_info.batch_size, ctx.a2a_info.emb_dim])\n            for _ in range(a2a_info.local_table_num)\n        ]\n        req_list = []\n        ind = 0\n        for i in range(my_size):\n            for j in range(per_rank_table_splits[i]):\n                gather_list = (\n                    list(grad_inputs[j].split(batch_split_lengths, dim=0))\n                    if i == my_rank\n                    else None\n                )\n                req = dist.gather(grad_output[ind], gather_list, dst=i, async_op=True)\n                req_list.append(req)\n                ind += 1\n        myreq.req = req_list\n        myreq.tensor = grad_inputs\n        return tuple(grad_output)\n\n\nclass All2All_Scatter_Req(Function):\n    @staticmethod\n    def forward(ctx, a2a_info, *inputs):\n        global myreq\n        batch_split_lengths = (\n            a2a_info.global_batch_partition_slices\n            if a2a_info.global_batch_partition_slices\n            else a2a_info.local_batch_num\n        )\n        table_split_lengths = (\n            a2a_info.global_table_wise_parition_slices\n            if a2a_info.global_table_wise_parition_slices\n            else [a2a_info.local_table_num] * my_size\n        )\n        input = torch.cat(inputs, dim=1)\n        scatter_list = list(input.split(batch_split_lengths, dim=0))\n        gather_list = []\n        req_list = []\n        for i in range(my_size):\n            out_tensor = input.new_empty(\n                [a2a_info.local_batch_num, table_split_lengths[i] * a2a_info.emb_dim]\n            )\n            req = dist.scatter(\n                out_tensor, scatter_list if i == my_rank else [], src=i, async_op=True\n            )\n            gather_list.append(out_tensor)\n            req_list.append(req)\n        myreq.req = req_list\n        myreq.tensor = tuple(gather_list)\n        myreq.a2a_info = a2a_info\n        ctx.a2a_info = a2a_info\n        return myreq.tensor\n\n    @staticmethod\n    def backward(ctx, *grad_output):\n        global myreq\n        for r in myreq.req:\n            r.wait()\n        myreq.req = None\n        grad_input = myreq.tensor\n        grad_inputs = grad_input.split(ctx.a2a_info.emb_dim, dim=1)\n        myreq.tensor = None\n        return (None, *grad_inputs)\n\n\nclass All2All_Scatter_Wait(Function):\n    @staticmethod\n    def forward(ctx, *output):\n        global myreq\n        ctx.a2a_info = myreq.a2a_info\n        for r in myreq.req:\n            r.wait()\n        myreq.req = None\n        myreq.tensor = None\n        return output\n\n    @staticmethod\n    def backward(ctx, *grad_output):\n        global myreq\n        assert len(grad_output) == my_size\n        scatter_list = [t.contiguous() for t in grad_output]\n        a2a_info = ctx.a2a_info\n        batch_split_lengths = (\n            a2a_info.global_batch_partition_slices\n            if a2a_info.global_batch_partition_slices\n            else a2a_info.local_batch_num\n        )\n        table_split_lengths = (\n            a2a_info.global_table_wise_parition_slices\n            if a2a_info.global_table_wise_parition_slices\n            else [a2a_info.local_table_num] * my_size\n        )\n        grad_input = grad_output[0].new_empty(\n            [a2a_info.batch_size, a2a_info.emb_dim * a2a_info.local_table_num]\n        )\n        gather_list = list(grad_input.split(batch_split_lengths, dim=0))\n        req_list = []\n        for i in range(my_size):\n            req = dist.gather(\n                scatter_list[i],\n                gather_list if i == my_rank else [],\n                dst=i,\n                async_op=True,\n            )\n            req_list.append(req)\n        myreq.req = req_list\n        myreq.tensor = grad_input\n        return grad_output\n\n\nclass All2All_Req(Function):\n    @staticmethod\n    def forward(ctx, a2a_info, *inputs):\n        global myreq\n        with record_function(\"DLRM alltoall_req_fwd_single\"):\n            batch_split_lengths = a2a_info.global_batch_partition_slices\n            if batch_split_lengths:\n                batch_split_lengths = [\n                    m * a2a_info.emb_dim * a2a_info.local_table_num\n                    for m in batch_split_lengths\n                ]\n            table_split_lengths = a2a_info.global_table_wise_parition_slices\n            if table_split_lengths:\n                table_split_lengths = [\n                    a2a_info.local_batch_num * e * a2a_info.emb_dim\n                    for e in table_split_lengths\n                ]\n            input = torch.cat(inputs, dim=1).view([-1])\n            output = input.new_empty(\n                [\n                    a2a_info.global_table_num\n                    * a2a_info.local_batch_num\n                    * a2a_info.emb_dim\n                ]\n            )\n            req = dist.all_to_all_single(\n                output, input, table_split_lengths, batch_split_lengths, async_op=True\n            )\n\n            myreq.req = req\n            myreq.tensor = []\n            myreq.tensor.append(output)\n            myreq.tensor = tuple(myreq.tensor)\n            a2a_info.batch_split_lengths = batch_split_lengths\n            a2a_info.table_split_lengths = table_split_lengths\n            myreq.a2a_info = a2a_info\n            ctx.a2a_info = a2a_info\n            return myreq.tensor\n\n    @staticmethod\n    def backward(ctx, *grad_output):\n        global myreq\n        with record_function(\"DLRM alltoall_req_bwd_single\"):\n            a2a_info = ctx.a2a_info\n            myreq.req.wait()\n            myreq.req = None\n            grad_input = myreq.tensor\n            grad_inputs = grad_input.view([a2a_info.batch_size, -1]).split(\n                a2a_info.emb_dim, dim=1\n            )\n            grad_inputs = [gin.contiguous() for gin in grad_inputs]\n            myreq.tensor = None\n            return (None, *grad_inputs)\n\n\nclass All2All_Wait(Function):\n    @staticmethod\n    def forward(ctx, *output):\n        global myreq\n        with record_function(\"DLRM alltoall_wait_fwd_single\"):\n            a2a_info = myreq.a2a_info\n            ctx.a2a_info = a2a_info\n            myreq.req.wait()\n            myreq.req = None\n            myreq.tensor = None\n            table_split_lengths = (\n                a2a_info.table_split_lengths\n                if a2a_info.table_split_lengths\n                else a2a_info.local_table_num\n                * a2a_info.local_batch_num\n                * a2a_info.emb_dim\n            )\n            outputs = output[0].split(table_split_lengths)\n            outputs = tuple(\n                [out.view([a2a_info.local_batch_num, -1]) for out in outputs]\n            )\n            return outputs\n\n    @staticmethod\n    def backward(ctx, *grad_outputs):\n        global myreq\n        with record_function(\"DLRM alltoall_wait_bwd_single\"):\n            a2a_info = ctx.a2a_info\n            grad_outputs = [gout.contiguous().view([-1]) for gout in grad_outputs]\n            grad_output = torch.cat(grad_outputs)\n            grad_input = grad_output.new_empty(\n                [a2a_info.batch_size * a2a_info.local_table_num * a2a_info.emb_dim]\n            )\n            req = dist.all_to_all_single(\n                grad_input,\n                grad_output,\n                a2a_info.batch_split_lengths,\n                a2a_info.table_split_lengths,\n                async_op=True,\n            )\n            myreq.req = req\n            myreq.tensor = grad_input\n            return (grad_output,)\n\n\nclass AllGather(Function):\n    @staticmethod\n    def forward(ctx, input, global_lengths, dim=0):\n        if not isinstance(global_lengths, (list, tuple)):\n            global_lengths = [global_lengths] * my_size\n\n        assert len(global_lengths) == my_size\n        assert global_lengths[my_rank] == input.size(dim)\n        local_start = sum(global_lengths[:my_rank])\n\n        output_size = list(input.size())\n\n        ctx.dim = dim\n        ctx.local_start = local_start\n        ctx.local_length = global_lengths[my_rank]\n\n        input = input.contiguous()\n        if dim == 0:\n            out_len = sum(global_lengths)\n            output_size[dim] = out_len\n            output = input.new_empty(output_size)\n            gather_list = list(output.split(global_lengths, dim=0))\n        else:\n            gather_list = [torch.empty_like(input) for _ in range(my_size)]\n            gather_list = []\n            for length in global_lengths:\n                output_size[dim] = length\n                gather_list.append(input.new_empty(output_size))\n\n        dist.all_gather(gather_list, input)\n\n        if dim != 0:\n            output = torch.cat(gather_list, dim=dim)\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # print(\"Inside All2AllBackward\")\n        dim = ctx.dim\n        start = ctx.local_start\n        length = ctx.local_length\n\n        grad_input = grad_output.narrow(dim, start, length)\n\n        return (grad_input, None, None)\n\n\nclass All2AllInfo(object):\n    pass\n\n\ndef alltoall(inputs, per_rank_table_splits):\n    global myreq\n    batch_size, emb_dim = inputs[0].size()\n    a2a_info = All2AllInfo()\n    a2a_info.local_table_num = len(inputs)\n    a2a_info.global_table_wise_parition_slices = per_rank_table_splits\n    (\n        a2a_info.local_batch_num,\n        a2a_info.global_batch_partition_slices,\n    ) = get_split_lengths(batch_size)\n    a2a_info.emb_dim = emb_dim\n    a2a_info.batch_size = batch_size\n    a2a_info.global_table_num = (\n        sum(per_rank_table_splits)\n        if per_rank_table_splits\n        else a2a_info.local_table_num * my_size\n    )\n\n    if a2a_impl == \"\" and alltoall_supported or a2a_impl == \"alltoall\":\n        # print(\"Using All2All_Req\")\n        output = All2All_Req.apply(a2a_info, *inputs)\n        myreq.WaitFunction = All2All_Wait\n    elif a2a_impl == \"\" or a2a_impl == \"scatter\":\n        # print(\"Using All2All_Scatter_Req\")\n        output = All2All_Scatter_Req.apply(a2a_info, *inputs)\n        myreq.WaitFunction = All2All_Scatter_Wait\n    elif a2a_impl == \"scatter_list\":\n        # print(\"Using All2All_ScatterList_Req\")\n        output = All2All_ScatterList_Req.apply(a2a_info, *inputs)\n        myreq.WaitFunction = All2All_ScatterList_Wait\n    else:\n        print(\n            \"Unknown value set for DLRM_ALLTOALL_IMPL (%s), \"\n            \"please use one of [alltoall, scatter, scatter_list]\" % a2a_impl\n        )\n    return myreq\n\n\ndef all_gather(input, lengths, dim=0):\n    if not lengths:\n        lengths = [input.size(0)] * my_size\n    return AllGather.apply(input, lengths, dim)\n\n\ndef barrier():\n    if my_size > 1:\n        dist.barrier()\n\n\n# Override builtin print function to print only from rank 0\norig_print = builtins.print\n\n\ndef rank0_print(*args, **kwargs):\n    if my_rank <= 0 or kwargs.get(\"print_all\", False):\n        orig_print(*args, **kwargs)\n\n\nbuiltins.print = rank0_print\n\n\n# Allow printing from all rank with explicit print_all\ndef print_all(*args, **kwargs):\n    orig_print(*args, **kwargs)\n"
        },
        {
          "name": "input",
          "type": "tree",
          "content": null
        },
        {
          "name": "kaggle_dac_loss_accuracy_plots.png",
          "type": "blob",
          "size": 534.59375,
          "content": null
        },
        {
          "name": "mlperf_logger.py",
          "type": "blob",
          "size": 2.8046875,
          "content": "# @lint-ignore-every LICENSELINT\n# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\"\"\"\nUtilities for MLPerf logging\n\"\"\"\n\nimport os\n\nimport torch\n\ntry:\n    from mlperf_logging import mllog\n    from mlperf_logging.mllog import constants\n\n    _MLLOGGER = mllog.get_mllogger()\nexcept ImportError as error:\n    print(\"Unable to import mlperf_logging, \", error)\n\n\ndef log_start(*args, **kwargs):\n    \"log with start tag\"\n    _log_print(_MLLOGGER.start, *args, **kwargs)\n\n\ndef log_end(*args, **kwargs):\n    \"log with end tag\"\n    _log_print(_MLLOGGER.end, *args, **kwargs)\n\n\ndef log_event(*args, **kwargs):\n    \"log with event tag\"\n    _log_print(_MLLOGGER.event, *args, **kwargs)\n\n\ndef _log_print(logger, *args, **kwargs):\n    \"makes mlperf logger aware of distributed execution\"\n    if \"stack_offset\" not in kwargs:\n        kwargs[\"stack_offset\"] = 3\n    if \"value\" not in kwargs:\n        kwargs[\"value\"] = None\n\n    if kwargs.pop(\"log_all_ranks\", False):\n        log = True\n    else:\n        log = get_rank() == 0\n\n    if log:\n        logger(*args, **kwargs)\n\n\ndef config_logger(benchmark):\n    \"initiates mlperf logger\"\n    mllog.config(\n        filename=os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), f\"{benchmark}.log\"\n        )\n    )\n    _MLLOGGER.logger.propagate = False\n\n\ndef barrier():\n    \"\"\"\n    Works as a temporary distributed barrier, currently pytorch\n    doesn't implement barrier for NCCL backend.\n    Calls all_reduce on dummy tensor and synchronizes with GPU.\n    \"\"\"\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        torch.distributed.all_reduce(torch.cuda.FloatTensor(1))\n        torch.cuda.synchronize()\n\n\ndef get_rank():\n    \"\"\"\n    Gets distributed rank or returns zero if distributed is not initialized.\n    \"\"\"\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        rank = torch.distributed.get_rank()\n    else:\n        rank = 0\n    return rank\n\n\ndef mlperf_submission_log(benchmark):\n    \"\"\"\n    Logs information needed for MLPerf submission\n    \"\"\"\n\n    config_logger(benchmark)\n\n    log_event(\n        key=constants.SUBMISSION_BENCHMARK,\n        value=benchmark,\n    )\n\n    log_event(key=constants.SUBMISSION_ORG, value=\"reference_implementation\")\n\n    log_event(key=constants.SUBMISSION_DIVISION, value=\"closed\")\n\n    log_event(key=constants.SUBMISSION_STATUS, value=\"onprem\")\n\n    log_event(key=constants.SUBMISSION_PLATFORM, value=\"reference_implementation\")\n\n    log_event(key=constants.SUBMISSION_ENTRY, value=\"reference_implementation\")\n\n    log_event(key=constants.SUBMISSION_POC_NAME, value=\"reference_implementation\")\n\n    log_event(key=constants.SUBMISSION_POC_EMAIL, value=\"reference_implementation\")\n"
        },
        {
          "name": "optim",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0869140625,
          "content": "future\nnumpy\nonnx\npydot\ntorch\ntorchviz\nscikit-learn\ntqdm\ntorchrec-nightly\ntorchx-nightly\n"
        },
        {
          "name": "terabyte_0875_loss_accuracy_plots.png",
          "type": "blob",
          "size": 253.2119140625,
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "torchrec_dlrm",
          "type": "tree",
          "content": null
        },
        {
          "name": "tricks",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}