{
  "metadata": {
    "timestamp": 1736560377181,
    "page": 918,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "google/deepvariant",
      "stars": 3288,
      "defaultBranch": "r1.8",
      "files": [
        {
          "name": ".bazelrc",
          "type": "blob",
          "size": 2.9619140625,
          "content": "# For more information about the format and semantics of this file, see:\n# https://docs.bazel.build/versions/master/bazel-user-manual.html#bazelrc\n#\n# Test inherits all of the rules of build, so we need only specify many of\n# our configuration options for build and they will also apply to test.\n\n# We need the same basic configuration defined by tensorflow, plus\n# the local configuration.  (Our WORKSPACE file assumes tensorflow is\n# installed in a specific location, and there doesn't seem to be a\n# way to refer to it symbolically, like BUILD files can.)\nimport %workspace%/../tensorflow/.bazelrc\nimport %workspace%/../tensorflow/.tf_configure.bazelrc\n\n# Use 128 jobs to build. Unfortunately there doesn't seem to be any way to set\n# the number of jobs to the number of actual cores on the machine (e.g., by\n# looking at cpuinfo. See https://github.com/bazelbuild/bazel/issues/3814.\nbuild --jobs 128\n\n# We have to use the monolithic build configuration of TF. See internal.\nbuild --config=monolithic\n\n# A timestamp is added to each message generated by Bazel specifying the time at\n# which the message was displayed.\nbuild --show_timestamps\n\n# Print additional information when there's a failure.\nbuild --verbose_failures\n\n# Use CPP protos for python, not the reflection ones.\nbuild --define=use_fast_cpp_protos=true\n\n# Turn off meddlesome mostly false positive warnings. Unfortunately these only\n# affect the Nucleus build itself, not our deps, which is where most of the\n# problems are coming from.\n#\n# These are largely errors due to protobuf.size(). We want to see them in our\n# own build.\n# build --copt=\"-Wno-sign-compare\"\n\n# These are essentially all false positives from our CLIF bindings, we disable\n# them in our own build.\nbuild --copt=\"-Wno-maybe-uninitialized\"\nbuild --copt=\"-Wno-unused-function\"\n\n# Set c++ version.\nbuild --cxxopt=\"-std=c++17\"\n\n# These are errors coming from the protobuf library itself. We'd like to see\n# them in our own build.\n# build --copt=\"-Wno-write-strings\"\n\n# Errors sends combined stdout/stderr output from failed tests only into the\n# stdout immediately after test is completed, ensuring that test output from\n# simultaneous tests is not interleaved with each other. Prints a summary at the\n# build as per summary output above.\ntest --test_output=errors\n\n# Until https://github.com/bazelbuild/bazel/issues/4815 is fixed, we need\n# to force the use of our py_runtime.\n# Versions of bazel used by TensorFlow 2.1+ disable the use of --python_top by\n# default with the below error message, necessitating the additional flag.\n#  \"ERROR: `--python_top` is disabled by `--incompatible_use_python_toolchains`.\n#    Instead of configuring the Python runtime directly, register a Python\n#    toolchain. See https://github.com/bazelbuild/bazel/issues/7899. You can\n#    temporarily revert to the legacy flag-based way of specifying toolchains by\n#    setting `--incompatible_use_python_toolchains=false`.\"\nbuild --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1162109375,
          "content": "# Compiled python files.\n*.pyc\n\n# Emacs temporary files\n*~\n\n# Other temporary files\n*#\n\nbazel-*\n\n**/.ipynb_checkpoints\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.306640625,
          "content": "# This is the official list of DeepVariant authors for copyright purposes.\n# This file is distinct from the CONTRIBUTORS files.\n# See the latter for an explanation.\n\n# Names should be added to this file as:\n# Name or Organization <email address>\n# The email address is not required for organizations.\n\nGoogle LLC.\n"
        },
        {
          "name": "BUILD",
          "type": "blob",
          "size": 3.5751953125,
          "content": "load(\"//third_party/nucleus/tools:zip_dir.bzl\", \"zip_dir\")\n\npackage(\n    default_visibility = [\n        \"//visibility:public\",\n    ],\n)\n\ntest_suite(\n    name = \"smoke_tests\",\n    tests = [\n        \"//deepvariant/core:smoke_tests\",\n        \"//deepvariant/environment_tests:smoke_tests\",\n        \"//deepvariant/testing:smoke_tests\",\n    ],\n)\n\nfilegroup(\n    name = \"binaries\",\n    srcs = [\n        \"//deepvariant:binaries\",\n    ],\n)\n\nfilegroup(\n    name = \"binaries-deeptrio\",\n    srcs = [\n        \"//deeptrio:binaries\",\n    ],\n)\n\nexports_files([\"LICENSE\"])\n\nfilegroup(\n    name = \"licenses\",\n    srcs = [\n        \":LICENSE\",\n        \"//third_party:abseil_cpp.LICENSE\",  # TODO\n        \"//third_party:boost.LICENSE\",\n        \"@com_google_protobuf//:LICENSE\",\n        \"@com_googlesource_code_re2//:LICENSE\",\n        \"@gbwt//:LICENSE\",\n        \"@gbwtgraph//:LICENSE\",\n        \"@htslib//:LICENSE\",\n        \"@libdivsufsort//:LICENSE\",\n        \"@libssw//:README.md\",  # SSW license embedded in the README.\n        \"@org_tensorflow//:LICENSE\",\n        \"@sdsl_lite//:COPYING\",\n    ],\n)\n\nzip_dir(\n    name = \"licenses_zip\",\n    srcs = [\":licenses\"],\n    zipname = \"licenses.zip\",\n)\n\ncc_library(\n    name = \"all_extensions\",\n    srcs = [],\n    deps = [\n        \"//deepvariant/python:allelecounter_cclib\",\n        \"//deepvariant/python:direct_phasing_cclib\",\n        \"//deepvariant/python:make_examples_native_cclib\",\n        \"//deepvariant/python:pileup_image_native_cclib\",\n        \"//deepvariant/python:postprocess_variants_cclib\",\n        \"//deepvariant/python:variant_calling_cclib\",\n        \"//deepvariant/python:variant_calling_multisample_cclib\",\n        \"//deepvariant/realigner/python:debruijn_graph_cclib\",\n        \"//deepvariant/realigner/python:fast_pass_aligner_cclib\",\n        \"//deepvariant/realigner/python:ssw_cclib\",\n        \"//deepvariant/realigner/python:window_selector_cclib\",\n        \"//third_party/nucleus/core/python:statusor_examples_cclib\",\n        \"//third_party/nucleus/io/python:bed_reader_cclib\",\n        \"//third_party/nucleus/io/python:bed_writer_cclib\",\n        \"//third_party/nucleus/io/python:bedgraph_reader_cclib\",\n        \"//third_party/nucleus/io/python:bedgraph_writer_cclib\",\n        \"//third_party/nucleus/io/python:fastq_reader_cclib\",\n        \"//third_party/nucleus/io/python:fastq_writer_cclib\",\n        \"//third_party/nucleus/io/python:gbz_reader_cclib\",\n        \"//third_party/nucleus/io/python:gff_reader_cclib\",\n        \"//third_party/nucleus/io/python:gff_writer_cclib\",\n        \"//third_party/nucleus/io/python:gfile_cclib\",\n        \"//third_party/nucleus/io/python:hts_verbose_cclib\",\n        \"//third_party/nucleus/io/python:merge_variants_cclib\",\n        \"//third_party/nucleus/io/python:reference_cclib\",\n        \"//third_party/nucleus/io/python:sam_reader_cclib\",\n        \"//third_party/nucleus/io/python:sam_writer_cclib\",\n        \"//third_party/nucleus/io/python:tabix_indexer_cclib\",\n        \"//third_party/nucleus/io/python:tfrecord_reader_cclib\",\n        \"//third_party/nucleus/io/python:tfrecord_writer_cclib\",\n        \"//third_party/nucleus/io/python:vcf_concat_cclib\",\n        \"//third_party/nucleus/io/python:vcf_reader_cclib\",\n        \"//third_party/nucleus/io/python:vcf_writer_cclib\",\n        \"//third_party/nucleus/util/python:math_cclib\",\n        \"//third_party/nucleus/util/python:utils_cclib\",\n    ],\n)\n\n# Until https://github.com/bazelbuild/bazel/issues/4815 is fixed,\n# we need to specify and force the use of a py_runtime.\npy_runtime(\n    name = \"deepvariant_python_runtime\",\n    files = [],\n    interpreter_path = select({\n        \"@bazel_tools//tools/python:PY3\": \"/usr/bin/python3\",\n    }),\n)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.5771484375,
          "content": "# How to Contribute\n\nWe cannot merge external pull requests into the DeepVariant repository at this\ntime. The source of truth for DeepVariant lives in an internal Google codebase,\nand changes must first be made internally.\n\nHowever, we still welcome community contributions! Please feel free to fork the\nDeepVariant repository and open a pull request or issue with suggested edits. To\nincorporate your contributions, we will make the changes internally and then\npush them to GitHub in the subsequent release. We will attribute the changes to\nyou in the commit description and release notes.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 14.49609375,
          "content": "# Copyright 2019 Google LLC.\n# This is used to build the DeepVariant release docker image.\n# It can also be used to build local images, especially if you've made changes\n# to the code.\n# Example command:\n# $ git clone https://github.com/google/deepvariant.git\n# $ cd deepvariant\n# $ sudo docker build -t deepvariant .\n#\n# To build for GPU, use a command like:\n# $ sudo docker build --build-arg=FROM_IMAGE=nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 --build-arg=DV_GPU_BUILD=1 -t deepvariant_gpu .\n\n\nARG FROM_IMAGE=ubuntu:22.04\n# PYTHON_VERSION is also set in settings.sh.\nARG PYTHON_VERSION=3.10\nARG DV_GPU_BUILD=0\nARG VERSION=1.8.0\nARG TF_ENABLE_ONEDNN_OPTS=1\n\nFROM continuumio/miniconda3 as conda_setup\nRUN conda config --add channels defaults && \\\n    conda config --add channels bioconda && \\\n    conda config --add channels conda-forge\nRUN conda create -n bio \\\n                    bioconda::bcftools=1.15 \\\n                    bioconda::samtools=1.15 \\\n    && conda clean -a\n\nFROM ${FROM_IMAGE} as builder\nCOPY --from=conda_setup /opt/conda /opt/conda\nLABEL maintainer=\"https://github.com/google/deepvariant/issues\"\n\nARG DV_GPU_BUILD\nENV DV_GPU_BUILD=${DV_GPU_BUILD}\nENV DV_BIN_PATH=/opt/deepvariant/bin\n\n# Copying DeepVariant source code\nCOPY . /opt/deepvariant\n\nARG VERSION\nENV VERSION=${VERSION}\n\nWORKDIR /opt/deepvariant\n\nRUN echo \"Acquire::http::proxy \\\"$http_proxy\\\";\\n\" \\\n         \"Acquire::https::proxy \\\"$https_proxy\\\";\" > \"/etc/apt/apt.conf\"\n\nRUN ./build-prereq.sh \\\n  && PATH=\"${HOME}/bin:${PATH}\" ./build_release_binaries.sh  # PATH for bazel\n\nFROM ${FROM_IMAGE}\nARG DV_GPU_BUILD\nARG VERSION\nARG PYTHON_VERSION\nARG TF_ENABLE_ONEDNN_OPTS\nENV DV_GPU_BUILD=${DV_GPU_BUILD}\nENV VERSION ${VERSION}\nENV PYTHON_VERSION ${PYTHON_VERSION}\nENV TF_ENABLE_ONEDNN_OPTS ${TF_ENABLE_ONEDNN_OPTS}\n\nRUN echo \"Acquire::http::proxy \\\"$http_proxy\\\";\\n\" \\\n         \"Acquire::https::proxy \\\"$https_proxy\\\";\" > \"/etc/apt/apt.conf\"\n\nWORKDIR /opt/\nCOPY --from=builder /opt/deepvariant/bazel-bin/licenses.zip .\n\nWORKDIR /opt/deepvariant/bin/\nCOPY --from=builder /opt/conda /opt/conda\nCOPY --from=builder /opt/deepvariant/run-prereq.sh .\nCOPY --from=builder /opt/deepvariant/settings.sh .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/vcf_stats_report.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/show_examples.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/runtime_by_region_vis.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/multisample_make_examples.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_somatic.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/train.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/fast_pipeline .\nCOPY --from=builder /opt/deepvariant/scripts/run_deepvariant.py .\nCOPY --from=builder /opt/deepvariant/scripts/run_deepsomatic.py .\n\nRUN ./run-prereq.sh\n\nRUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 0 && \\\n    update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 0\n\n# Create shell wrappers for python zip files for easier use.\nRUN \\\n  BASH_HEADER='#!/bin/bash' && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/make_examples.zip \"$@\"' > \\\n    /opt/deepvariant/bin/make_examples && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/call_variants.zip \"$@\"' > \\\n    /opt/deepvariant/bin/call_variants && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/postprocess_variants.zip \"$@\"' > \\\n    /opt/deepvariant/bin/postprocess_variants && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/vcf_stats_report.zip \"$@\"' > \\\n    /opt/deepvariant/bin/vcf_stats_report && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/show_examples.zip \"$@\"' > \\\n    /opt/deepvariant/bin/show_examples && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/runtime_by_region_vis.zip \"$@\"' > \\\n    /opt/deepvariant/bin/runtime_by_region_vis && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/multisample_make_examples.zip \"$@\"' > \\\n    /opt/deepvariant/bin/multisample_make_examples && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/labeled_examples_to_vcf.zip \"$@\"' > \\\n    /opt/deepvariant/bin/labeled_examples_to_vcf && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/make_examples_somatic.zip \"$@\"' > \\\n    /opt/deepvariant/bin/make_examples_somatic && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/run_deepvariant.py \"$@\"' > \\\n    /opt/deepvariant/bin/run_deepvariant && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/run_deepsomatic.py \"$@\"' > \\\n    /opt/deepvariant/bin/run_deepsomatic && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/train.zip \"$@\"' > \\\n    /opt/deepvariant/bin/train && \\\n  chmod +x /opt/deepvariant/bin/make_examples \\\n    /opt/deepvariant/bin/call_variants \\\n    /opt/deepvariant/bin/postprocess_variants \\\n    /opt/deepvariant/bin/vcf_stats_report \\\n    /opt/deepvariant/bin/show_examples \\\n    /opt/deepvariant/bin/runtime_by_region_vis \\\n    /opt/deepvariant/bin/multisample_make_examples \\\n    /opt/deepvariant/bin/run_deepvariant \\\n    /opt/deepvariant/bin/run_deepsomatic \\\n    /opt/deepvariant/bin/labeled_examples_to_vcf \\\n    /opt/deepvariant/bin/make_examples_somatic \\\n    /opt/deepvariant/bin/train\n\n# Copy models\nWORKDIR /opt/models/wgs\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wgs.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wgs.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wgs.savedmodel/example_info.json .\nWORKDIR /opt/models/wgs/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wgs.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wgs.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/wgs/*\n\nWORKDIR /opt/models/wes\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wes.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wes.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wes.savedmodel/example_info.json .\nWORKDIR /opt/models/wes/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wes.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.wes.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/wes/*\n\nWORKDIR /opt/models/pacbio\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.pacbio.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.pacbio.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.pacbio.savedmodel/example_info.json .\nWORKDIR /opt/models/pacbio/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.pacbio.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.pacbio.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/pacbio/*\n\nWORKDIR /opt/models/hybrid_pacbio_illumina\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.hybrid.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.hybrid.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.hybrid.savedmodel/example_info.json .\nWORKDIR /opt/models/hybrid_pacbio_illumina/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.hybrid.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.hybrid.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/hybrid_pacbio_illumina/*\n\nWORKDIR /opt/models/ont_r104\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.ont.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.ont.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.ont.savedmodel/example_info.json .\nWORKDIR /opt/models/ont_r104/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.ont.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.ont.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/ont_r104/*\n\nWORKDIR /opt/models/masseq\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.masseq.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.masseq.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.masseq.savedmodel/example_info.json .\nWORKDIR /opt/models/masseq/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.masseq.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/savedmodels/deepvariant.masseq.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/masseq/*\n\n# Copy small models\nWORKDIR /opt/smallmodels/wgs\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.wgs.smallmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.wgs.smallmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.wgs.smallmodel/keras_metadata.pb .\nWORKDIR /opt/smallmodels/wgs/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.wgs.smallmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.wgs.smallmodel/variables/variables.index .\nRUN chmod -R +r /opt/smallmodels/wgs/*\n\nWORKDIR /opt/smallmodels/pacbio\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.pacbio.smallmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.pacbio.smallmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.pacbio.smallmodel/keras_metadata.pb .\nWORKDIR /opt/smallmodels/pacbio/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.pacbio.smallmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.pacbio.smallmodel/variables/variables.index .\nRUN chmod -R +r /opt/smallmodels/pacbio/*\n\nWORKDIR /opt/smallmodels/hybrid_pacbio_illumina\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.hybrid.smallmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.hybrid.smallmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.hybrid.smallmodel/keras_metadata.pb .\nWORKDIR /opt/smallmodels/hybrid_pacbio_illumina/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.hybrid.smallmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.hybrid.smallmodel/variables/variables.index .\nRUN chmod -R +r /opt/smallmodels/hybrid_pacbio_illumina/*\n\nWORKDIR /opt/smallmodels/ont_r104\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.ont.smallmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.ont.smallmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.ont.smallmodel/keras_metadata.pb .\nWORKDIR /opt/smallmodels/ont_r104/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.ont.smallmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION}/smallmodels/deepvariant.ont.smallmodel/variables/variables.index .\nRUN chmod -R +r /opt/smallmodels/ont_r104/*\n\nENV PATH=\"${PATH}\":/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin\n\nRUN apt-get -y update && \\\n  apt-get install -y parallel python3-pip && \\\n  PATH=\"${HOME}/.local/bin:$PATH\" python3 -m pip install absl-py==0.13.0 && \\\n  apt-get clean autoclean && \\\n  apt-get autoremove -y --purge && \\\n  rm -rf /var/lib/apt/lists/*\n\nWORKDIR /opt/deepvariant\n\nCMD [\"/opt/deepvariant/bin/run_deepvariant\", \"--help\"]\n"
        },
        {
          "name": "Dockerfile.deepsomatic",
          "type": "blob",
          "size": 14.203125,
          "content": "# Copyright 2019 Google LLC.\n# This is used to build the DeepSomatic release docker image.\n# It can also be used to build local images, especially if you've made changes\n# to the code.\n# Example command:\n# $ git clone https://github.com/google/deepvariant.git\n# $ cd deepvariant\n# $ sudo docker build -f Dockerfile.deepsomatic -t deepsomatic .\n#\n# To build for GPU, use a command like:\n# $ sudo docker build -f Dockerfile.deepsomatic --build-arg=FROM_IMAGE=nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 --build-arg=DV_GPU_BUILD=1 -t deepsomatic_gpu .\n\n\nARG FROM_IMAGE=ubuntu:22.04\n# PYTHON_VERSION is also set in settings.sh.\nARG PYTHON_VERSION=3.10\nARG DV_GPU_BUILD=0\nARG VERSION_DEEPSOMATIC=1.8.0\nARG TF_ENABLE_ONEDNN_OPTS=1\n\nFROM continuumio/miniconda3 as conda_setup\nRUN conda config --add channels defaults && \\\n    conda config --add channels bioconda && \\\n    conda config --add channels conda-forge\nRUN conda create -n bio \\\n                    bioconda::bcftools=1.15 \\\n                    bioconda::samtools=1.15 \\\n    && conda clean -a\n\nFROM ${FROM_IMAGE} as builder\nCOPY --from=conda_setup /opt/conda /opt/conda\nLABEL maintainer=\"https://github.com/google/deepvariant/issues\"\n\nARG DV_GPU_BUILD\nENV DV_GPU_BUILD=${DV_GPU_BUILD}\n\n# Copying DeepVariant source code\nCOPY . /opt/deepvariant\n\nWORKDIR /opt/deepvariant\n\nRUN ./build-prereq.sh \\\n  && PATH=\"${HOME}/bin:${PATH}\" ./build_release_binaries.sh  # PATH for bazel\n\nFROM ${FROM_IMAGE}\nARG DV_GPU_BUILD\nARG VERSION_DEEPSOMATIC\nARG PYTHON_VERSION\nARG TF_ENABLE_ONEDNN_OPTS\nENV DV_GPU_BUILD=${DV_GPU_BUILD}\nENV VERSION_DEEPSOMATIC ${VERSION_DEEPSOMATIC}\nENV PYTHON_VERSION ${PYTHON_VERSION}\nENV TF_ENABLE_ONEDNN_OPTS ${TF_ENABLE_ONEDNN_OPTS}\n\nWORKDIR /opt/\nCOPY --from=builder /opt/deepvariant/bazel-bin/licenses.zip .\n\nWORKDIR /opt/deepvariant/bin/\nCOPY --from=builder /opt/conda /opt/conda\nCOPY --from=builder /opt/deepvariant/run-prereq.sh .\nCOPY --from=builder /opt/deepvariant/settings.sh .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_somatic.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/vcf_stats_report.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/show_examples.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/runtime_by_region_vis.zip  .\nCOPY --from=builder /opt/deepvariant/scripts/run_deepsomatic.py ./deepsomatic/\nRUN ./run-prereq.sh\n\nRUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 0 && \\\n    update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 0\n\n# Create shell wrappers for python zip files for easier use.\nRUN \\\n  BASH_HEADER='#!/bin/bash' && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/make_examples_somatic.zip \"$@\"' > \\\n    /opt/deepvariant/bin/make_examples_somatic && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/call_variants.zip \"$@\"' > \\\n    /opt/deepvariant/bin/call_variants && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/postprocess_variants.zip \"$@\"' > \\\n    /opt/deepvariant/bin/postprocess_variants && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/vcf_stats_report.zip \"$@\"' > \\\n    /opt/deepvariant/bin/vcf_stats_report && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/show_examples.zip \"$@\"' > \\\n    /opt/deepvariant/bin/show_examples && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/runtime_by_region_vis.zip \"$@\"' > \\\n    /opt/deepvariant/bin/runtime_by_region_vis && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/deepsomatic/run_deepsomatic.py \"$@\"' > \\\n    /opt/deepvariant/bin/deepsomatic/run_deepsomatic && \\\n  chmod +x /opt/deepvariant/bin/make_examples_somatic \\\n    /opt/deepvariant/bin/call_variants \\\n    /opt/deepvariant/bin/postprocess_variants \\\n    /opt/deepvariant/bin/vcf_stats_report \\\n    /opt/deepvariant/bin/show_examples \\\n    /opt/deepvariant/bin/runtime_by_region_vis \\\n    /opt/deepvariant/bin/deepsomatic/run_deepsomatic\n\n# Copy models\nWORKDIR /opt/models/deepsomatic/wgs\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/wgs/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/wgs/*\n\nWORKDIR /opt/models/deepsomatic/wes\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wes.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wes.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wes.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/wes/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wes.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wes.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/wes/*\n\nWORKDIR /opt/models/deepsomatic/pacbio\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/pacbio/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/pacbio/*\n\nWORKDIR /opt/models/deepsomatic/ont\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/ont/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/ont/*\n\nWORKDIR /opt/models/deepsomatic/ffpe_wgs\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wgs.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wgs.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wgs.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/ffpe_wgs/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wgs.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wgs.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/ffpe_wgs/*\n\nWORKDIR /opt/models/deepsomatic/ffpe_wes\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wes.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wes.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wes.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/ffpe_wes/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wes.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ffpe_wes.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/ffpe_wes/*\n\n# Tumor-only models\nWORKDIR /opt/models/deepsomatic/wgs_tumor_only\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs_tumor_only.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs_tumor_only.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs_tumor_only.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/wgs_tumor_only/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs_tumor_only.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.wgs_tumor_only.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/wgs_tumor_only/*\n\nWORKDIR /opt/models/deepsomatic/pacbio_tumor_only\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio_tumor_only.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio_tumor_only.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio_tumor_only.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/pacbio_tumor_only/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio_tumor_only.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.pacbio_tumor_only.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/pacbio_tumor_only/*\n\nWORKDIR /opt/models/deepsomatic/ont_tumor_only\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont_tumor_only.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont_tumor_only.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont_tumor_only.savedmodel/example_info.json .\nWORKDIR /opt/models/deepsomatic/ont_tumor_only/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont_tumor_only.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/savedmodels/deepsomatic.ont_tumor_only.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deepsomatic/ont_tumor_only/*\n\n# PONs and AF VCF files\nWORKDIR /opt/models/deepsomatic/pons\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/pons/AF_ilmn_PON_DeepVariant.GRCh38.AF0.05.vcf.gz .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/pons/AF_ilmn_PON_DeepVariant.GRCh38.AF0.05.vcf.gz.tbi .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/pons/AF_pacbio_PON_CoLoRSdb.GRCh38.AF0.05.vcf.gz .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/pons/AF_pacbio_PON_CoLoRSdb.GRCh38.AF0.05.vcf.gz.tbi .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/pons/PON_dbsnp138_gnomad_ILMN1000g_pon.vcf.gz .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/pons/PON_dbsnp138_gnomad_ILMN1000g_pon.vcf.gz.tbi .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/pons/PON_dbsnp138_gnomad_PB1000g_pon.vcf.gz .\nADD https://storage.googleapis.com/deepvariant/models/DeepSomatic/${VERSION_DEEPSOMATIC}/pons/PON_dbsnp138_gnomad_PB1000g_pon.vcf.gz.tbi .\nRUN chmod -R +r /opt/models/deepsomatic/pons/*\n\nENV PATH=\"${PATH}\":/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin/deepsomatic:/opt/deepvariant/bin\n\nRUN apt-get -y update && \\\n  apt-get install -y parallel python3-pip && \\\n  PATH=\"${HOME}/.local/bin:$PATH\" python3 -m pip install absl-py==0.13.0 && \\\n  apt-get clean autoclean && \\\n  apt-get autoremove -y --purge && \\\n  rm -rf /var/lib/apt/lists/*\n\n\nWORKDIR /opt/deepvariant\n\nCMD [\"/opt/deepvariant/bin/deepsomatic/run_deepsomatic\", \"--help\"]\n"
        },
        {
          "name": "Dockerfile.deeptrio",
          "type": "blob",
          "size": 11.6982421875,
          "content": "# Copyright 2019 Google LLC.\n# This is used to build the DeepTrio release docker image.\n# It can also be used to build local images, especially if you've made changes\n# to the code.\n# Example command:\n# $ git clone https://github.com/google/deepvariant.git\n# $ cd deepvariant\n# $ sudo docker build -f Dockerfile.deeptrio -t deeptrio .\n#\n# To build for GPU, use a command like:\n# $ sudo docker build -f Dockerfile.deeptrio --build-arg=FROM_IMAGE=nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 --build-arg=DV_GPU_BUILD=1 -t deeptrio_gpu .\n\n\nARG FROM_IMAGE=ubuntu:22.04\n# PYTHON_VERSION is also set in settings.sh.\nARG PYTHON_VERSION=3.10\nARG DV_GPU_BUILD=0\nARG VERSION_DEEPTRIO=1.8.0\nARG TF_ENABLE_ONEDNN_OPTS=1\n\nFROM continuumio/miniconda3 as conda_setup\nRUN conda config --add channels defaults && \\\n    conda config --add channels bioconda && \\\n    conda config --add channels conda-forge\nRUN conda create -n bio \\\n                    bioconda::bcftools=1.15 \\\n                    bioconda::samtools=1.15 \\\n    && conda clean -a\n\nFROM ${FROM_IMAGE} as builder\nCOPY --from=conda_setup /opt/conda /opt/conda\nLABEL maintainer=\"https://github.com/google/deepvariant/issues\"\n\nARG DV_GPU_BUILD\nENV DV_GPU_BUILD=${DV_GPU_BUILD}\n\n# Copying DeepVariant source code\nCOPY . /opt/deepvariant\n\nWORKDIR /opt/deepvariant\n\nRUN ./build-prereq.sh \\\n  && PATH=\"${HOME}/bin:${PATH}\" ./build_release_binaries.sh  # PATH for bazel\n\nFROM ${FROM_IMAGE}\nARG DV_GPU_BUILD\nARG VERSION_DEEPTRIO\nARG PYTHON_VERSION\nARG TF_ENABLE_ONEDNN_OPTS\nENV DV_GPU_BUILD=${DV_GPU_BUILD}\nENV VERSION_DEEPTRIO ${VERSION_DEEPTRIO}\nENV PYTHON_VERSION ${PYTHON_VERSION}\nENV TF_ENABLE_ONEDNN_OPTS ${TF_ENABLE_ONEDNN_OPTS}\n\nWORKDIR /opt/\nCOPY --from=builder /opt/deepvariant/bazel-bin/licenses.zip .\n\nWORKDIR /opt/deepvariant/bin/\nCOPY --from=builder /opt/conda /opt/conda\nCOPY --from=builder /opt/deepvariant/run-prereq.sh .\nCOPY --from=builder /opt/deepvariant/settings.sh .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/vcf_stats_report.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/show_examples.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/runtime_by_region_vis.zip  .\nCOPY --from=builder /opt/deepvariant/scripts/run_deeptrio.py ./deeptrio/\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deeptrio/make_examples.zip  ./deeptrio/\nRUN ./run-prereq.sh\n\nRUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 0 && \\\n    update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 0\n\n# Create shell wrappers for python zip files for easier use.\nRUN \\\n  BASH_HEADER='#!/bin/bash' && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/deeptrio/make_examples.zip \"$@\"' > \\\n    /opt/deepvariant/bin/deeptrio/make_examples && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/call_variants.zip \"$@\"' > \\\n    /opt/deepvariant/bin/call_variants && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/postprocess_variants.zip \"$@\"' > \\\n    /opt/deepvariant/bin/postprocess_variants && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/vcf_stats_report.zip \"$@\"' > \\\n    /opt/deepvariant/bin/vcf_stats_report && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/show_examples.zip \"$@\"' > \\\n    /opt/deepvariant/bin/show_examples && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/runtime_by_region_vis.zip \"$@\"' > \\\n    /opt/deepvariant/bin/runtime_by_region_vis && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/deeptrio/run_deeptrio.py \"$@\"' > \\\n    /opt/deepvariant/bin/deeptrio/run_deeptrio && \\\n  chmod +x /opt/deepvariant/bin/deeptrio/make_examples \\\n    /opt/deepvariant/bin/call_variants \\\n    /opt/deepvariant/bin/postprocess_variants \\\n    /opt/deepvariant/bin/vcf_stats_report \\\n    /opt/deepvariant/bin/show_examples \\\n    /opt/deepvariant/bin/runtime_by_region_vis \\\n    /opt/deepvariant/bin/deeptrio/run_deeptrio\n\n# Copy models\nWORKDIR /opt/models/deeptrio/wgs/child\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_child.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_child.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_child.savedmodel/example_info.json .\nWORKDIR /opt/models/deeptrio/wgs/child/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_child.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_child.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deeptrio/wgs/child/*\n\nWORKDIR /opt/models/deeptrio/wgs/parent\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_parent.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_parent.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_parent.savedmodel/example_info.json .\nWORKDIR /opt/models/deeptrio/wgs/parent/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_parent.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wgs_parent.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deeptrio/wgs/parent/*\n\nWORKDIR /opt/models/deeptrio/pacbio/child\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_child.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_child.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_child.savedmodel/example_info.json .\nWORKDIR /opt/models/deeptrio/pacbio/child/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_child.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_child.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deeptrio/pacbio/child/*\n\nWORKDIR /opt/models/deeptrio/pacbio/parent\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_parent.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_parent.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_parent.savedmodel/example_info.json .\nWORKDIR /opt/models/deeptrio/pacbio/parent/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_parent.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.pacbio_parent.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deeptrio/pacbio/parent/*\n\nWORKDIR /opt/models/deeptrio/wes/child\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_child.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_child.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_child.savedmodel/example_info.json .\nWORKDIR /opt/models/deeptrio/wes/child/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_child.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_child.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deeptrio/wes/child/*\n\nWORKDIR /opt/models/deeptrio/wes/parent\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_parent.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_parent.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_parent.savedmodel/example_info.json .\nWORKDIR /opt/models/deeptrio/wes/parent/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_parent.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.wes_parent.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deeptrio/wes/parent/*\n\nWORKDIR /opt/models/deeptrio/ont/child\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_child.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_child.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_child.savedmodel/example_info.json .\nWORKDIR /opt/models/deeptrio/ont/child/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_child.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_child.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deeptrio/ont/child/*\n\nWORKDIR /opt/models/deeptrio/ont/parent\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_parent.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_parent.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_parent.savedmodel/example_info.json .\nWORKDIR /opt/models/deeptrio/ont/parent/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_parent.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepTrio/${VERSION_DEEPTRIO}/savedmodels/deeptrio.ont_parent.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/deeptrio/ont/parent/*\n\nENV PATH=\"${PATH}\":/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin/deeptrio:/opt/deepvariant/bin\n\nRUN apt-get -y update && \\\n  apt-get install -y parallel python3-pip && \\\n  PATH=\"${HOME}/.local/bin:$PATH\" python3 -m pip install absl-py==0.13.0 && \\\n  apt-get clean autoclean && \\\n  apt-get autoremove -y --purge && \\\n  rm -rf /var/lib/apt/lists/*\n\n\nWORKDIR /opt/deepvariant\n\nCMD [\"/opt/deepvariant/bin/deeptrio/run_deeptrio\", \"--help\"]\n"
        },
        {
          "name": "Dockerfile.pangenome_aware_deepvariant",
          "type": "blob",
          "size": 7.396484375,
          "content": "# Copyright 2019 Google LLC.\n# This is used to build the pangenome-aware DeepVariant release docker image.\n# It can also be used to build local images, especially if you've made changes\n# to the code.\n# Example command:\n# $ git clone https://github.com/google/deepvariant.git\n# $ cd deepvariant\n# $ sudo docker build -f Dockerfile.pangenome_aware_deepvariant -t pangenome_aware_deepvariant .\n#\n# To build for GPU, use a command like:\n# $ sudo docker build -f Dockerfile.pangenome_aware_deepvariant --build-arg=FROM_IMAGE=nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 --build-arg=DV_GPU_BUILD=1 -t pangenome_aware_deepvariant_gpu .\n\n\nARG FROM_IMAGE=ubuntu:22.04\n# PYTHON_VERSION is also set in settings.sh.\nARG PYTHON_VERSION=3.10\nARG DV_GPU_BUILD=0\nARG VERSION_DEEPVARIANT=1.8.0\nARG TF_ENABLE_ONEDNN_OPTS=1\n\nFROM continuumio/miniconda3 AS conda_setup\nRUN conda config --add channels defaults && \\\n    conda config --add channels bioconda && \\\n    conda config --add channels conda-forge\nRUN conda create -n bio \\\n                    bioconda::bcftools=1.15 \\\n                    bioconda::samtools=1.15 \\\n    && conda clean -a\n\nFROM ${FROM_IMAGE} AS builder\nCOPY --from=conda_setup /opt/conda /opt/conda\nLABEL maintainer=\"https://github.com/google/deepvariant/issues\"\n\nARG DV_GPU_BUILD\nENV DV_GPU_BUILD=${DV_GPU_BUILD}\n\n# Copying DeepVariant source code\nCOPY . /opt/deepvariant\n\nWORKDIR /opt/deepvariant\n\nRUN ./build-prereq.sh \\\n  && PATH=\"${HOME}/bin:${PATH}\" ./build_release_binaries.sh  # PATH for bazel\n\nFROM ${FROM_IMAGE}\nARG DV_GPU_BUILD\nARG VERSION_DEEPVARIANT\nARG PYTHON_VERSION\nARG TF_ENABLE_ONEDNN_OPTS\nENV DV_GPU_BUILD=${DV_GPU_BUILD}\nENV VERSION_DEEPVARIANT=${VERSION_DEEPVARIANT}\nENV PYTHON_VERSION=${PYTHON_VERSION}\nENV TF_ENABLE_ONEDNN_OPTS=${TF_ENABLE_ONEDNN_OPTS}\n\nWORKDIR /opt/\nCOPY --from=builder /opt/deepvariant/bazel-bin/licenses.zip .\n\nWORKDIR /opt/deepvariant/bin/\nCOPY --from=builder /opt/conda /opt/conda\nCOPY --from=builder /opt/deepvariant/run-prereq.sh .\nCOPY --from=builder /opt/deepvariant/settings.sh .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_pangenome_aware_dv.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/vcf_stats_report.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/show_examples.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/runtime_by_region_vis.zip  .\nCOPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/load_gbz_into_shared_memory.zip  .\nCOPY --from=builder /opt/deepvariant/scripts/run_pangenome_aware_deepvariant.py .\nRUN ./run-prereq.sh\n\nRUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 0 && \\\n    update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 0\n\n# Create shell wrappers for python zip files for easier use.\nRUN \\\n  BASH_HEADER='#!/bin/bash' && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/make_examples_pangenome_aware_dv.zip \"$@\"' > \\\n    /opt/deepvariant/bin/make_examples_pangenome_aware_dv && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/call_variants.zip \"$@\"' > \\\n    /opt/deepvariant/bin/call_variants && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/postprocess_variants.zip \"$@\"' > \\\n    /opt/deepvariant/bin/postprocess_variants && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/vcf_stats_report.zip \"$@\"' > \\\n    /opt/deepvariant/bin/vcf_stats_report && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/show_examples.zip \"$@\"' > \\\n    /opt/deepvariant/bin/show_examples && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/runtime_by_region_vis.zip \"$@\"' > \\\n    /opt/deepvariant/bin/runtime_by_region_vis && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 /opt/deepvariant/bin/load_gbz_into_shared_memory.zip \"$@\"' > \\\n    /opt/deepvariant/bin/load_gbz_into_shared_memory && \\\n  printf \"%s\\n%s\\n\" \\\n    \"${BASH_HEADER}\" \\\n    'python3 -u /opt/deepvariant/bin/run_pangenome_aware_deepvariant.py \"$@\"' > \\\n    /opt/deepvariant/bin/run_pangenome_aware_deepvariant && \\\n  chmod +x /opt/deepvariant/bin/make_examples_pangenome_aware_dv \\\n    /opt/deepvariant/bin/call_variants \\\n    /opt/deepvariant/bin/postprocess_variants \\\n    /opt/deepvariant/bin/vcf_stats_report \\\n    /opt/deepvariant/bin/show_examples \\\n    /opt/deepvariant/bin/runtime_by_region_vis \\\n    /opt/deepvariant/bin/load_gbz_into_shared_memory \\\n    /opt/deepvariant/bin/run_pangenome_aware_deepvariant\n\n# Copy models\nWORKDIR /opt/models/pangenome_aware_deepvariant/wgs\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wgs.pangenome.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wgs.pangenome.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wgs.pangenome.savedmodel/example_info.json .\nWORKDIR /opt/models/pangenome_aware_deepvariant/wgs/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wgs.pangenome.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wgs.pangenome.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/pangenome_aware_deepvariant/wgs/*\n\nWORKDIR /opt/models/pangenome_aware_deepvariant/wes\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wes.pangenome.savedmodel/fingerprint.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wes.pangenome.savedmodel/saved_model.pb .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wes.pangenome.savedmodel/example_info.json .\nWORKDIR /opt/models/pangenome_aware_deepvariant/wes/variables\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wes.pangenome.savedmodel/variables/variables.data-00000-of-00001 .\nADD https://storage.googleapis.com/deepvariant/models/DeepVariant/${VERSION_DEEPVARIANT}/pangenome_aware_models/savedmodels/deepvariant.wes.pangenome.savedmodel/variables/variables.index .\nRUN chmod -R +r /opt/models/pangenome_aware_deepvariant/wes/*\n\nENV PATH=\"${PATH}\":/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin/pangenome_aware_deepvariant:/opt/deepvariant/bin\n\nRUN apt-get -y update && \\\n  apt-get install -y parallel python3-pip && \\\n  PATH=\"${HOME}/.local/bin:$PATH\" python3 -m pip install absl-py==0.13.0 && \\\n  apt-get clean autoclean && \\\n  apt-get autoremove -y --purge && \\\n  rm -rf /var/lib/apt/lists/*\n\n\nWORKDIR /opt/deepvariant\n\nCMD [\"/opt/deepvariant/bin/run_pangenome_aware_deepvariant\", \"--help\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4365234375,
          "content": "Copyright 2020 Google LLC.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors\n   may be used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.66015625,
          "content": "<img src=\"docs/images/dv_logo.png\" width=50% height=50%>\n\n[![release](https://img.shields.io/badge/release-v1.8-green?logo=github)](https://github.com/google/deepvariant/releases)\n[![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements)\n[![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant)\n\nDeepVariant is a deep learning-based variant caller that takes aligned reads (in\nBAM or CRAM format), produces pileup image tensors from them, classifies each\ntensor using a convolutional neural network, and finally reports the results in\na standard VCF or gVCF file.\n\nDeepVariant supports germline variant-calling in diploid organisms.\n\n**DeepVariant case-studies for germline variant calling:**\n\n*   NGS (Illumina or Element) data for either a\n    [whole genome](docs/deepvariant-case-study.md) or\n    [whole exome](docs/deepvariant-exome-case-study.md).\n*   PacBio HiFi data\n    [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).\n*   Oxford Nanopore R10.4.1\n    [Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md),\n    [Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).\n*   Complete Genomics\n    [T7 case study](docs/deepvariant-complete-t7-case-study.md);\n    [G400 case study](docs/deepvariant-complete-g400-case-study.md).\n*   Pangenome-mapping-based case-study:\n    [vg case study](docs/deepvariant-vg-case-study.md).\n*   RNA data for\n    [PacBio Iso-Seq/MAS-Seq case study](docs/deepvariant-masseq-case-study.md)\n    and [Illumina RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md).\n*   Hybrid PacBio HiFi + Illumina WGS, see the\n    [hybrid case study](docs/deepvariant-hybrid-case-study.md).\n\n**Pangenome-aware DeepVariant case-studies:**\n\n*   Pangenome-aware DeepVariant WGS (Illumina or Element):\n    [Mapped with BWA](docs/pangenome-aware-wgs-bwa-case-study.md),\n    [Mapped with VG](docs/pangenome-aware-wgs-vg-case-study.md).\n*   Pangenome-aware DeepVariant WES (Illumina or Element):\n    [Mapped with BWA](docs/pangenome-aware-wes-bwa-case-study.md).\n\nWe have also adapted DeepVariant for somatic calling. See the\n[DeepSomatic](https://github.com/google/deepsomatic) repo for details.\n\nPlease also note:\n\n*   DeepVariant currently supports variant calling on organisms where the\n    ploidy/copy-number is two. This is because the genotypes supported are\n    hom-alt, het, and hom-ref.\n*   The models included with DeepVariant are only trained on human data. For\n    other organisms, see the\n    [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)\n    for some possible pitfalls and how to handle them.\n\n## DeepTrio\n\nDeepTrio is a deep learning-based trio variant caller built on top of\nDeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to\nutilize the power of neural networks to predict genomic variants in trios or\nduos. See [this page](docs/deeptrio-details.md) for more details and\ninstructions on how to run DeepTrio.\n\nDeepTrio supports germline variant-calling in diploid organisms for the\nfollowing types of input data:\n\n*   NGS (Illumina) data for either\n    [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.\n*   PacBio HiFi data, see the\n    [PacBio case study](docs/deeptrio-pacbio-case-study.md).\n\nPlease also note:\n\n*   All DeepTrio models were trained on human data.\n*   It is possible to use DeepTrio with only 2 samples (child, and one parent).\n*   External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to\n    merge output VCFs.\n\n## How to run DeepVariant\n\nWe recommend using our Docker solution. The command will look like this:\n\n```\nBIN_VERSION=\"1.8.0\"\ndocker run \\\n  -v \"YOUR_INPUT_DIR\":\"/input\" \\\n  -v \"YOUR_OUTPUT_DIR:/output\" \\\n  google/deepvariant:\"${BIN_VERSION}\" \\\n  /opt/deepvariant/bin/run_deepvariant \\\n  --model_type=WGS \\ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**\n  --ref=/input/YOUR_REF \\\n  --reads=/input/YOUR_BAM \\\n  --output_vcf=/output/YOUR_OUTPUT_VCF \\\n  --output_gvcf=/output/YOUR_OUTPUT_GVCF \\\n  --num_shards=$(nproc) \\ **This will use all your cores to run make_examples. Feel free to change.**\n  --vcf_stats_report=true \\ **Optional. Creates VCF statistics report in html file. Default is false.\n  --disable_small_model=true \\ **Optional. Disables the small model from make_examples stage. Default is false.\n  --logging_dir=/output/logs \\ **Optional. This saves the log output for each stage separately.\n  --haploid_contigs=\"chrX,chrY\" \\ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to \"chrX,chrY\" for GRCh38 and \"X,Y\" for GRCh37. For a sample with karyotype XX, this should not be used.\n  --par_regions_bed=\"/input/GRCh3X_par.bed\" \\ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.\n  --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.\n```\n\nFor details on X,Y support, please see\n[DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case\nstudy in\n[DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You\ncan download the PAR bed files from here:\n[GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),\n[GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed).\n\nTo see all flags you can use, run: `docker run\ngoogle/deepvariant:\"${BIN_VERSION}\"`\n\nIf you're using GPUs, or want to use Singularity instead, see\n[Quick Start](docs/deepvariant-quick-start.md) for more details.\n\nIf you are running on a machine with a GPU, an experimental mode is available\nthat enables running the `make_examples` stage on the CPU while the\n `call_variants` stage runs on the GPU simultaneously.\nFor more details, refer to the [Fast Pipeline case study](docs/deepvariant-fast-pipeline-case-study.md).\n\nFor more information, also see:\n\n*   [Full documentation list](docs/README.md)\n*   [Detailed usage guide](docs/deepvariant-details.md) with more information on\n    the input and output file formats and how to work with them.\n*   [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md)\n*   [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md)\n*   [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md)\n\n## How to cite\n\nIf you're using DeepVariant in your work, please cite:\n\n[A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983987 (2018).](https://rdcu.be/7Dhl) <br/>\nRyan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>\ndoi: https://doi.org/10.1038/nbt.4235\n\nAdditionally, if you are generating multi-sample calls using our\n[DeepVariant and GLnexus Best Practices](docs/trio-merge-case-study.md), please\ncite:\n\n[Accurate, scalable cohort variant calls using DeepVariant and GLnexus.\n_Bioinformatics_ (2021).](https://doi.org/10.1093/bioinformatics/btaa1081)<br/>\nTaedong Yun, Helen Li, Pi-Chuan Chang, Michael F. Lin, Andrew Carroll, and Cory\nY. McLean.<br/>\ndoi: https://doi.org/10.1093/bioinformatics/btaa1081\n\n## Why Use DeepVariant?\n\n*   **High accuracy** - DeepVariant won 2020\n    [PrecisionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results)\n    for All Benchmark Regions for ONT, PacBio, and Multiple Technologies\n    categories, and 2016\n    [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results)\n    for best SNP Performance. DeepVariant maintains high accuracy across data\n    from different sequencing technologies, prep methods, and species. For\n    [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),\n    using DeepVariant makes an especially great difference. See\n    [metrics](docs/metrics.md) for the latest accuracy numbers on each of the\n    sequencing types.\n*   **Flexibility** - Out-of-the-box use for\n    [PCR-positive](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html)\n    samples and\n    [low quality sequencing runs](https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/),\n    and easy adjustments for\n    [different sequencing technologies](https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/)\n    and\n    [non-human species](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).\n*   **Ease of use** - No filtering is needed beyond setting your preferred\n    minimum quality threshold.\n*   **Cost effectiveness** - With a single non-preemptible n1-standard-16\n    machine on Google Cloud, it costs ~$11.8 to call a 30x whole genome and\n    ~$0.89 to call an exome. With preemptible pricing, the cost is $2.84 for a\n    30x whole genome and $0.21 for whole exome (not considering preemption).\n*   **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported\n    datatypes on a 96-core CPU-only machine</sup>. Multiple options for\n    acceleration exist.\n*   **Usage options** - DeepVariant can be run via Docker or binaries, using\n    both on-premise hardware or in the cloud, with support for hardware\n    accelerators like GPUs and TPUs.\n\n<a name=\"myfootnote1\">(1)</a>: Time estimates do not include mapping.\n\n## How DeepVariant works\n\n![Stages in DeepVariant](docs/images/inference_flow_diagram.svg)\n\nFor more information on the pileup images and how to read them, please see the\n[\"Looking through DeepVariant's Eyes\" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/).\n\nDeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of\nPython and C++ code for reading and writing data in common genomics file formats\n(like SAM and VCF) designed for painless integration with the\n[TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus\nwas built with DeepVariant in mind and open-sourced separately so it can be used\nby anyone in the genomics research community for other projects. See this blog\npost on\n[Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/).\n\n## DeepVariant Setup\n\n### Prerequisites\n\n*   Unix-like operating system (cannot run on Windows)\n*   Python 3.10\n\n### Official Solutions\n\nBelow are the official solutions provided by the\n[Genomics team in Google Health](https://health.google/health-research/).\n\nName                                                                                                | Description\n:-------------------------------------------------------------------------------------------------: | -----------\n[Docker](docs/deepvariant-quick-start.md)           | This is the recommended method.\n[Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.\nPrebuilt Binaries                                                                                   | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under \"flags\".\n\n## Contribution Guidelines\n\nPlease [open a pull request](https://github.com/google/deepvariant/compare) if\nyou wish to contribute to DeepVariant. Note, we have not set up the\ninfrastructure to merge pull requests externally. If you agree, we will test and\nsubmit the changes internally and mention your contributions in our\n[release notes](https://github.com/google/deepvariant/releases). We apologize\nfor any inconvenience.\n\nIf you have any difficulty using DeepVariant, feel free to\n[open an issue](https://github.com/google/deepvariant/issues/new). If you have\ngeneral questions not specific to DeepVariant, we recommend that you post on a\ncommunity discussion forum such as [BioStars](https://www.biostars.org/).\n\n## License\n\n[BSD-3-Clause license](LICENSE)\n\n## Acknowledgements\n\nDeepVariant happily makes use of many open source packages. We would like to\nspecifically call out a few key ones:\n\n*   [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html)\n*   [abseil-cpp](https://github.com/abseil/abseil-cpp) and\n    [abseil-py](https://github.com/abseil/abseil-py)\n*   [pybind11](https://github.com/pybind/pybind11)\n*   [GNU Parallel](https://www.gnu.org/software/parallel/)\n*   [htslib & samtools](http://www.htslib.org/)\n*   [Nucleus](https://github.com/google/nucleus)\n*   [numpy](http://www.numpy.org/)\n*   [SSW Library](https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library)\n*   [TensorFlow](https://www.tensorflow.org/)\n\nWe thank all of the developers and contributors to these packages for their\nwork.\n\n## Disclaimer\n\nThis is not an official Google product.\n\nNOTE: the content of this research code repository (i) is not intended to be a\nmedical device; and (ii) is not intended for clinical use of any kind, including\nbut not limited to diagnosis or prognosis.\n"
        },
        {
          "name": "WORKSPACE",
          "type": "blob",
          "size": 6.6328125,
          "content": "# The workspace name appears at the top of the runfiles tree,\n# and in paths to tests, so to keep python happy it is best\n# if it is unique.\nworkspace(name = \"com_google_deepvariant\")\n\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n\n# Note: absl_py and com_google_absl (the Python and C++ abseil libraries) are\n# provided by TensorFlow.\n\n# CCTZ (Time-zone framework).\n# TODO: transitive WORKSPACE dependency resolution doesn't\n# work in bazel, so we need to include this to enable nucleus's use of\n# //absl/{time,synchronization}\nhttp_archive(\n    name = \"com_googlesource_code_cctz\",\n    strip_prefix = \"cctz-master\",\n    urls = [\"https://github.com/google/cctz/archive/master.zip\"],\n)\n\n# This is the 1.18 release of htslib.\nhttp_archive(\n    name = \"htslib\",\n    build_file = \"//:third_party/htslib.BUILD\",\n    sha256 = \"f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f\",\n    strip_prefix = \"htslib-1.18\",\n    urls = [\n        \"https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2\",\n    ],\n)\n\nhttp_archive(\n    name = \"libssw\",\n    build_file = \"//:third_party/libssw.BUILD\",\n    sha256 = \"b294c0cb6f0f3d578db11b4112a88b20583b9d4190b0a9cf04d83bb6a8704d9a\",\n    # Note: HHBlits requires a patch (internal) in ssw_align to work.\n    strip_prefix = \"Complete-Striped-Smith-Waterman-Library-1.2.5\",\n    urls = [\n        \"https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/archive/v1.2.5.tar.gz\",\n    ],\n)\n\nhttp_archive(\n    name = \"gbwt\",\n    build_file = \"//:third_party/gbwt.BUILD\",\n    sha256 = \"21d3679349ef9809a886da50f0a2036eba0c172b97826a7882322f81649397e2\",\n    strip_prefix = \"gbwt-0b3aacbea6f7d285a3c5fbd0a22b4aa2ac8957d6\",\n    urls = [\n        \"https://github.com/mobinasri/gbwt/archive/0b3aacbea6f7d285a3c5fbd0a22b4aa2ac8957d6.zip\",\n    ],\n)\n\nhttp_archive(\n    name = \"libhandlegraph\",\n    build_file = \"//:third_party/libhandlegraph.BUILD\",\n    sha256 = \"078dee9ab07996193117b54d75f6e4cfd881851da5e3c3e3d8c669c7eefeaaa2\",\n    strip_prefix = \"libhandlegraph-b2fc22c552440076b340306fc660b4fa309fb005\",\n    urls = [\n        \"https://github.com/mobinasri/libhandlegraph/archive/b2fc22c552440076b340306fc660b4fa309fb005.zip\",\n    ],\n)\n\nhttp_archive(\n    name = \"sdsl_lite\",\n    build_file = \"//:third_party/sdsl_lite.BUILD\",\n    sha256 = \"24c454fae9f2b4e5d20ce7df9817027e1315bef2eca519e0f123a0b970b757d2\",\n    strip_prefix = \"sdsl_lite-4cb63b65854983bec395d799aaff342bd0cc376f\",\n    urls = [\n        \"https://github.com/mobinasri/sdsl_lite/archive/4cb63b65854983bec395d799aaff342bd0cc376f.zip\",\n    ],\n)\n\nhttp_archive(\n    name = \"gbwtgraph\",\n    build_file = \"//:third_party/gbwtgraph.BUILD\",\n    sha256 = \"40c41c34b152a1eea6991e1acfdad8875e0c738e24cd36ca22dab5187c99a910\",\n    strip_prefix = \"gbwtgraph-c96ca88b65fc40ac4bd371319a29111015d38904\",\n    urls = [\n        \"https://github.com/mobinasri/gbwtgraph/archive/c96ca88b65fc40ac4bd371319a29111015d38904.zip\",\n    ],\n)\n\nhttp_archive(\n    name = \"libdivsufsort\",\n    build_file = \"//:third_party/libdivsufsort.BUILD\",\n    sha256 = \"6a94e0ae99824b027a732062fab2ebd16091ada33ba1b90ba0e9892f2afec8b8\",\n    strip_prefix = \"libdivsufsort-22e6b23e619ff50fd086844b6e618d53ca9d53bd\",\n    urls = [\n        \"https://github.com/simongog/libdivsufsort/archive/22e6b23e619ff50fd086844b6e618d53ca9d53bd.zip\",\n    ],\n)\n\n# Import tensorflow.  Note path.\nlocal_repository(\n    name = \"org_tensorflow\",\n    path = \"../tensorflow\",\n)\n\n# Required boilerplate for tf_workspace().\n# This is copied from https://github.com/tensorflow/tensorflow/blob/v2.3.0/WORKSPACE.\nhttp_archive(\n    name = \"io_bazel_rules_closure\",\n    sha256 = \"5b00383d08dd71f28503736db0500b6fb4dda47489ff5fc6bed42557c07c6ba9\",\n    strip_prefix = \"rules_closure-308b05b2419edb5c8ee0471b67a40403df940149\",\n    urls = [\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",\n        \"https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",  # 2019-06-13\n    ],\n)\n\n# This needs to be in sync with the version of protobuf used by TensorFlow,\n# which is currently defined in @tensorflow/tensorflow/workspace.bzl.\n# We supply our own BUILD file, though, so we can prevent ODR violations by\n# putting all of Nucleus's C++ binary dependencies into a single library.\n# That BUILD file must be kept in sync with the version of protobuf used.\nhttp_archive(\n    name = \"com_google_protobuf\",\n    build_file = \"//:third_party/protobuf.BUILD\",\n    patch_args = [\"-p1\"],\n    patches = [\"//:third_party/protobuf.patch\"],\n    sha256 = \"cfcba2df10feec52a84208693937c17a4b5df7775e1635c1e3baffc487b24c9b\",\n    # This protobuf release is based on protobuf 3.9.2.\n    strip_prefix = \"protobuf-3.9.2\",\n    urls = [\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/v3.9.2.zip\",\n        \"https://github.com/protocolbuffers/protobuf/archive/v3.9.2.zip\",\n    ],\n)\n\nhttp_archive(\n    name = \"com_google_glog\",\n    sha256 = \"1ee310e5d0a19b9d584a855000434bb724aa744745d5b8ab1855c85bff8a8e21\",\n    strip_prefix = \"glog-028d37889a1e80e8a07da1b8945ac706259e5fd8\",\n    urls = [\n        \"https://mirror.bazel.build/github.com/google/glog/archive/028d37889a1e80e8a07da1b8945ac706259e5fd8.tar.gz\",\n        \"https://github.com/google/glog/archive/028d37889a1e80e8a07da1b8945ac706259e5fd8.tar.gz\",\n    ],\n)\n\n# bazel_skylib is now a required dependency of protobuf_archive.\nhttp_archive(\n    name = \"bazel_skylib\",\n    sha256 = \"74d544d96f4a5bb630d465ca8bbcfe231e3594e5aae57e1edbf17a6eb3ca2506\",\n    urls = [\n        \"https://mirror.bazel.build/github.com/bazelbuild/bazel-skylib/releases/download/1.3.0/bazel-skylib-1.3.0.tar.gz\",\n        \"https://github.com/bazelbuild/bazel-skylib/releases/download/1.3.0/bazel-skylib-1.3.0.tar.gz\",\n    ],\n)\n\nhttp_archive(\n    name = \"pybind11_protobuf\",\n    sha256 = \"21e0c32d81ece8039a3a8e6daafbd7f64cb0c2744492f3b00f11baa0e276d1a5\",\n    strip_prefix = \"pybind11_protobuf-de94308491982c32ddfe305a5dfc3c38bc9ff2bc\",\n    urls = [\n        \"https://github.com/pichuan/pybind11_protobuf/archive/de94308491982c32ddfe305a5dfc3c38bc9ff2bc.zip\",\n    ],\n)\n\n# Import all of the tensorflow dependencies.\n# Copied from tensorflow/WORKSPACE. Updated in v2.5.0:\nload(\"@org_tensorflow//tensorflow:workspace3.bzl\", \"tf_workspace3\")\n\ntf_workspace3()\n\nload(\"@org_tensorflow//tensorflow:workspace2.bzl\", \"tf_workspace2\")\n\ntf_workspace2()\n\nload(\"@org_tensorflow//tensorflow:workspace1.bzl\", \"tf_workspace1\")\n\ntf_workspace1()\n\nload(\"@org_tensorflow//tensorflow:workspace0.bzl\", \"tf_workspace0\")\n\ntf_workspace0()\n\nnew_local_repository(\n    name = \"clif\",\n    build_file = \"third_party/clif.BUILD\",\n    path = \"/usr/local\",\n)\n"
        },
        {
          "name": "build-prereq.sh",
          "type": "blob",
          "size": 7.6220703125,
          "content": "#!/bin/bash\nset -euo pipefail\n\n# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\necho ========== This script is only maintained for Ubuntu 22.04.\necho ========== Load config settings.\n\nsource settings.sh\n\n################################################################################\n# Misc. setup\n################################################################################\n\nnote_build_stage \"Install the runtime packages\"\n\n./run-prereq.sh\n\nnote_build_stage \"Update package list\"\n\nsudo -H apt-get -qq -y update\n\nnote_build_stage \"build-prereq.sh: Install development packages\"\n\n# Need to wait for dpkg lock (see internal)\nwait_for_dpkg_lock\nsudo -H NEEDRESTART_MODE=a apt-get -qq -y install pkg-config zip g++ zlib1g-dev unzip curl git wget > /dev/null\n\n\n################################################################################\n# bazel\n################################################################################\n\nnote_build_stage \"Install bazel\"\n\nfunction ensure_wanted_bazel_version {\n  local wanted_bazel_version=$1\n  rm -rf ~/bazel\n  mkdir ~/bazel\n\n  if\n    v=$(bazel --bazelrc=/dev/null --ignore_all_rc_files version) &&\n    echo \"$v\" | awk -v b=\"$wanted_bazel_version\" '/Build label/ { exit ($3 != b)}'\n  then\n    echo \"Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling\"\n  else\n    pushd ~/bazel\n    curl -L -O https://github.com/bazelbuild/bazel/releases/download/\"${wanted_bazel_version}\"/bazel-\"${wanted_bazel_version}\"-installer-linux-x86_64.sh\n    chmod +x bazel-*.sh\n    ./bazel-\"${wanted_bazel_version}\"-installer-linux-x86_64.sh --user > /dev/null\n    rm bazel-\"${wanted_bazel_version}\"-installer-linux-x86_64.sh\n    popd\n  fi\n}\n\nensure_wanted_bazel_version \"${DV_BAZEL_VERSION}\"\n\n# This is used for building examples_from_stream.so later.\ntime sudo ./tools/build_absl.sh\n\n################################################################################\n# TensorFlow\n################################################################################\n\nnote_build_stage \"Download and configure TensorFlow sources\"\n\n# Getting the directory before switching out.\nDV_DIR=$(pwd)\n\nif [[ ! -d ../tensorflow ]]; then\n  note_build_stage \"Cloning TensorFlow from github as ../tensorflow doesn't exist\"\n  (cd .. && git clone https://github.com/tensorflow/tensorflow)\nfi\n\n# PYTHON_BIN_PATH and PYTHON_LIB_PATH are set in settings.sh.\n# I had to remove this line in tensorflow v2.5.0 because I got an ERROR:\n# rule() got unexpected keyword argument 'incompatible_use_toolchain_transition'.\n# I changed the llvm path to zip to avoid flakiness.\n(cd ../tensorflow &&\n git checkout \"${DV_CPP_TENSORFLOW_TAG}\" &&\n echo | ./configure)\n\n# We want to use a newer absl version. So I grabbed the one from TensorFlow\n# r2.13. Eventually we'll want to update to TF 2.13. But for now this works.\n# TODO: After updating to v2.13, we can remove this.\nwget https://raw.githubusercontent.com/tensorflow/tensorflow/r2.13/third_party/absl/workspace.bzl -O ../tensorflow/third_party/absl/workspace.bzl\nrm -f ../tensorflow/third_party/absl/absl_designated_initializers.patch\n# To get the @com_google_absl//absl/strings:string_view target:\nsed -i -e 's|b971ac5250ea8de900eae9f95e06548d14cd95fe|29bf8085f3bf17b84d30e34b3d7ff8248fda404e|g' ../tensorflow/third_party/absl/workspace.bzl\nsed -i -e 's|8eeec9382fc0338ef5c60053f3a4b0e0708361375fe51c9e65d0ce46ccfe55a7|affb64f374b16877e47009df966d0a9403dbf7fe613fe1f18e49802c84f6421e|g' ../tensorflow/third_party/absl/workspace.bzl\nsed -i -e 's|patch_file = \\[\"//third_party/absl:absl_designated_initializers.patch\"\\],||g' ../tensorflow/third_party/absl/workspace.bzl\n\n# Update tensorflow.bzl. This updates the `pybind_extension` rule to use the\n# _message.so file.\npatch ../tensorflow/tensorflow/tensorflow.bzl \"${DV_DIR}\"/third_party/tensorflow.bzl.patch\n\n# I want to replace this part in ../tensorflow/tensorflow/workspace2.bzl\n# From:\n# tf_http_archive(\n#     name = \"pybind11\",\n#     urls = tf_mirror_urls(\"https://github.com/pybind/pybind11/archive/v2.10.0.tar.gz\"),\n#     sha256 = \"eacf582fa8f696227988d08cfc46121770823839fe9e301a20fbce67e7cd70ec\",\n#     strip_prefix = \"pybind11-2.10.0\",\n#     build_file = \"//third_party:pybind11.BUILD\",\n#     system_build_file = \"//third_party/systemlibs:pybind11.BUILD\",\n# )\n# To:\n# tf_http_archive(\n#     name = \"pybind11\",\n#     urls = tf_mirror_urls(\"https://github.com/pybind/pybind11/archive/a7b91e33269ab6f3f90167291af2c4179fc878f5.zip\"),\n#     sha256 = \"09d2ab67e91457c966eb335b361bdc4d27ece2d4dea681d22e5d8307e0e0c023\",\n#     strip_prefix = \"pybind11-a7b91e33269ab6f3f90167291af2c4179fc878f5\",\n#     build_file = \"//third_party:pybind11.BUILD\",\n#     system_build_file = \"//third_party/systemlibs:pybind11.BUILD\",\n# )\nsed -i -e 's|v2.10.0.tar.gz|a7b91e33269ab6f3f90167291af2c4179fc878f5.zip|g' ../tensorflow/tensorflow/workspace2.bzl\nsed -i -e 's|eacf582fa8f696227988d08cfc46121770823839fe9e301a20fbce67e7cd70ec|09d2ab67e91457c966eb335b361bdc4d27ece2d4dea681d22e5d8307e0e0c023|g' ../tensorflow/tensorflow/workspace2.bzl\nsed -i -e 's|pybind11-2.10.0|pybind11-a7b91e33269ab6f3f90167291af2c4179fc878f5|g' ../tensorflow/tensorflow/workspace2.bzl\n\n# Inspired by part of https://raw.githubusercontent.com/tensorflow/tensorflow/r2.11/third_party/protobuf/protobuf.patch.\n# This is necessary for Python 3.10.\ncat > third_party/protobuf.patch <<- EOM\ndiff --git a/python/google/protobuf/pyext/message.cc b/python/google/protobuf/pyext/message.cc\nindex 3530a9b37..c31fa8fcc 100644\n--- a/python/google/protobuf/pyext/message.cc\n+++ b/python/google/protobuf/pyext/message.cc\n@@ -2991,8 +2991,12 @@ bool InitProto2MessageModule(PyObject *m) {\n         reinterpret_cast<PyObject*>(\n             &RepeatedCompositeContainer_Type));\n \n-    // Register them as collections.Sequence\n+    // Register them as MutableSequence.\n+#if PY_MAJOR_VERSION >= 3\n+    ScopedPyObjectPtr collections(PyImport_ImportModule(\"collections.abc\"));\n+#else\n     ScopedPyObjectPtr collections(PyImport_ImportModule(\"collections\"));\n+#endif\n     if (collections == NULL) {\n       return false;\n     }\nEOM\n\n# TODO: Test removing this version pinning.\nnote_build_stage \"Set pyparsing to 2.2.2 for CLIF.\"\nexport PATH=\"$HOME/.local/bin\":$PATH\npip3 uninstall -y pyparsing && pip3 install -Iv 'pyparsing==2.2.2'\n\nnote_build_stage \"build-prereq.sh complete\"\n"
        },
        {
          "name": "build_and_test.sh",
          "type": "blob",
          "size": 3.3994140625,
          "content": "#!/bin/bash\n\n# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n# NOLINT\nset -eux -o pipefail\n\nsource settings.sh\n\n# bazel should have been installed in build-prereq.sh, but the PATH might\n# need to be added in this script.\nif ! bazel; then\n  PATH=\"$HOME/bin:$PATH\"\nfi\n\n# Building examples_from_stream.so C++ library. It cannot be built correctly\n# with the default bazel setup, so we build it manually.\n# examples_from_stream.so is used by call_variants target therefore it has to\n# be built before :binaries.\nTF_CFLAGS=( $(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\nTF_LFLAGS=( $(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\n\n# shellcheck disable=SC2068\ng++ -std=c++14 -shared \\\n        deepvariant/stream_examples_kernel.cc  \\\n        deepvariant/stream_examples_ops.cc \\\n        -o deepvariant/examples_from_stream.so \\\n        -fPIC \\\n        -l:libtensorflow_framework.so.2  \\\n        -I. \\\n        ${TF_CFLAGS[@]} \\\n        ${TF_LFLAGS[@]} \\\n        -D_GLIBCXX_USE_CXX11_ABI=1 \\\n        --std=c++17 \\\n        -DEIGEN_MAX_ALIGN_BYTES=64 \\\n        -O2\n\n# Run all deepvariant tests.  Take bazel options from args, if any.\n# Note: If running with GPU, tests must be executed serially due to a GPU\n# contention issue.\nif [[ \"${DV_GPU_BUILD:-0}\" = \"1\" ]]; then\n  bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} \"$@\" \\\n    deepvariant/...\n  # GPU tests are commented out for now.\n  # Because they seem to be all filtered out, and as a result causing an error.\n  # See internal#comment5.\n  # TODO: Uncomment this once it's resolved.\n  # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} \"$@\" \\\n  #   deepvariant:gpu_tests\nelse\n  # Running parallel tests on CPU.\n  bazel test -c opt ${DV_COPT_FLAGS} \"$@\" deepvariant/...\nfi\n\n# Build the binary.\n./build_release_binaries.sh\n\necho 'Expect a usage message:'\n(python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help || : ) | grep '/call_variants.py:'\n\n"
        },
        {
          "name": "build_release_binaries.sh",
          "type": "blob",
          "size": 7.205078125,
          "content": "#!/bin/bash\n# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n#\n# Build release binaries using our standard compiler flags.\n# Note we use the lowest common denominator compiler optimization options (For\n# Google Cloud Engine chipsets lowest possible is Sandy Bridge).\n\n# NOLINT\nsource settings.sh\n\nset -e\n\n# Bazel's --build_python_zip replaces our carefully engineered symbolic links\n# with copies.  This function puts the symbolic links back.\nfunction fix_zip_file {\n  orig_zip_file=$1\n\n  # Step 1:  Copy the zip file to a temporary place.\n  TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX)\n  # The .zip version of the binary doesn't have the header that makes it\n  # self-executable.  We use that version because otherwise unzip would\n  # complain and raise an error code.\n  cp \"${orig_zip_file}.zip\" \"${TMPDIR}\"\n\n  # Step 2: Unzip it.\n  pushd \"${TMPDIR}\" > /dev/null\n  BN=$(basename \"${orig_zip_file}\")\n  unzip -qq \"${BN}.zip\"\n\n  # Step 3: Restore the symbolic links.\n  find \"runfiles/com_google_deepvariant\" -name '*.so' ! -name 'examples_from_stream.so' -exec ln --force -s --relative \"runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so\" {} \\;\n\n  # Step 4: Fix the __main__.py's use of zipfile, which can't handle\n  # symbolic links.  Replace it with an invocation of unzip, which can.\n  # The lines we replace are\n  # with zipfile.ZipFile(zip_path) as zf:\n  #   for info in zf.infolist():\n  #     zf.extract(info, dest_dir)\n  #     # UNC-prefixed paths must be absolute/normalized. See\n\n  sed -i 's/  with zipfile.ZipFile(zip_path) as zf:/  if True:/' __main__.py\n  sed -i 's/  for info in zf.infolist():/  if True:/' __main__.py\n  sed -i 's/  zf.extract(info, dest_dir)/  os.system(\"unzip -qq \" + zip_path + \" -d \" + dest_dir)/' __main__.py\n  sed -i 's/  # UNC-prefixed paths must be absolute\\/normalized. See/  return/' __main__.py\n\n  # Step 5: Zip it back up, with zip --symbolic\n  rm -f \"${BN}.zip\"\n  ZIP_OUT=\"/tmp/${BN}.zip\"\n  rm -f \"${ZIP_OUT}\"\n  zip -q --symlinks -r \"${ZIP_OUT}\" *\n\n  # Step 6: Make the zip file self-executable\n  SELF_ZIP=\"/tmp/${BN}\"\n  # If the Python interpreter discovers it is being run from part of a zip\n  # file, it will uncompress and run the __main__.py.  This is the trick that\n  # bazel uses to make a self-executable zip, see for example\n  # https://github.com/bazelbuild/bazel/blob/558b717e906156477b1c6bd29d049a0fb8e18b27/src/main/java/com/google/devtools/build/lib/bazel/rules/python/BazelPythonSemantics.java#L193\n  echo '#!/usr/bin/env python3' | cat - \"${ZIP_OUT}\" > \"${SELF_ZIP}\"\n\n  # Step 7: Copy it back and make it executable.\n  popd > /dev/null\n  rm -f \"${orig_zip_file}\"\n  mv \"${SELF_ZIP}\" \"${orig_zip_file}\"\n  chmod +x \"${orig_zip_file}\"\n\n  # Step 8: DeepVariant also uses \"${orig_zip_file}.zip\" in many of its\n  # instructions, so make sure that we also copy that.\n  rm -f \"${orig_zip_file}.zip\"\n  mv \"${ZIP_OUT}\" \"${orig_zip_file}.zip\"\n  # No executable bit because the .zip version is not self-executing and\n  # must be invoked as\n  #   python3 ${orig_zip_file}.zip\n}\n\n# Building examples_from_stream.so C++ library. It cannot be built correctly\n# with the default bazel setup, so we build it manually.\n# examples_from_stream.so is used by call_variants target therefore it has to\n# be built before :binaries.\nTF_CFLAGS=( $(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\nTF_LFLAGS=( $(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\n\n# shellcheck disable=SC2068\ng++ -std=c++14 -shared \\\n        deepvariant/stream_examples_kernel.cc  \\\n        deepvariant/stream_examples_ops.cc \\\n        -o deepvariant/examples_from_stream.so \\\n        -fPIC \\\n        -l:libtensorflow_framework.so.2  \\\n        -I. \\\n        ${TF_CFLAGS[@]} \\\n        ${TF_LFLAGS[@]} \\\n        -D_GLIBCXX_USE_CXX11_ABI=1 \\\n        --std=c++17 \\\n        -DEIGEN_MAX_ALIGN_BYTES=64 \\\n        -O2\n\n# shellcheck disable=SC2086\nbazel build -c opt \\\n  //deepvariant:fast_pipeline\n\n# shellcheck disable=SC2086\nbazel build -c opt \\\n  --output_filter=DONT_MATCH_ANYTHING \\\n  --noshow_loading_progress \\\n  --show_result=0 \\\n  ${DV_COPT_FLAGS} \\\n  --build_python_zip \\\n  :binaries\n\n# shellcheck disable=SC2086\nbazel build -c opt \\\n  --output_filter=DONT_MATCH_ANYTHING \\\n  --noshow_loading_progress \\\n  --show_result=0 \\\n  ${DV_COPT_FLAGS} \\\n  --build_python_zip \\\n  //deepvariant/labeler:labeled_examples_to_vcf\n\n# shellcheck disable=SC2086\nbazel build -c opt \\\n  --output_filter=DONT_MATCH_ANYTHING \\\n  --noshow_loading_progress \\\n  --show_result=0 \\\n  ${DV_COPT_FLAGS} \\\n  --build_python_zip \\\n  :binaries-deeptrio\n\n# shellcheck disable=SC2086\nbazel build  -c opt \\\n  --output_filter=DONT_MATCH_ANYTHING \\\n  --noshow_loading_progress \\\n  --show_result=0 \\\n  --noshow_progress \\\n  ${DV_COPT_FLAGS} \\\n  :licenses_zip\n\n# Bazel understandably doesn't like it when its output files are edited, so\n# make sure all the builds are done before we fix things.\n\n# TODO: Replace this hand-made list with a find command.\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/train\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/call_variants\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/load_gbz_into_shared_memory\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/make_examples\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/make_examples_pangenome_aware_dv\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/make_examples_somatic\"\nfix_zip_file \"bazel-out/k8-opt/bin/deeptrio/make_examples\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/postprocess_variants\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/vcf_stats_report\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/show_examples\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/runtime_by_region_vis\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/multisample_make_examples\"\nfix_zip_file \"bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf\"\n"
        },
        {
          "name": "deeptrio",
          "type": "tree",
          "content": null
        },
        {
          "name": "deepvariant",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "run-prereq.sh",
          "type": "blob",
          "size": 13.3681640625,
          "content": "#!/bin/bash\n\n# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n# This installs all the libraries (python, dso, etc) that are needed\n# by DeepVariant at runtime (except for tensorflow, which is special).\n# Some extra stuff may also be included.\n\nset -euo pipefail\n\necho ========== This script is only maintained for Ubuntu 22.04.\necho ========== Load config settings.\n\nsource settings.sh\n\n################################################################################\n# misc setup\n################################################################################\n\nnote_build_stage \"Misc setup\"\n\nAPT_ARGS=(\n\"-qq\"\n\"-y\"\n)\n\nif [[ \"$EUID\" = \"0\" ]]; then\n  # Just in case:\n  # https://github.com/NVIDIA/nvidia-docker/issues/1632#issuecomment-1112667716\n  rm -f /etc/apt/sources.list.d/cuda.list\n  rm -f /etc/apt/sources.list.d/nvidia-ml.list\n  # Ensure sudo exists, even if we don't need it.\n  apt-get update \"${APT_ARGS[@]}\" > /dev/null\n  apt-get install \"${APT_ARGS[@]}\" sudo > /dev/null\n  PIP_ARGS=(\n    \"-qq\")\nelse\n  PIP_ARGS=(\n    \"--user\"\n    \"-qq\")\nfi\n\nnote_build_stage \"Update package list\"\n\nsudo -H apt-get update \"${APT_ARGS[@]}\" > /dev/null\n\nnote_build_stage \"run-prereq.sh: Install development packages\"\n\n# Need to wait for dpkg lock (see internal)\nwait_for_dpkg_lock\n\n# See https://askubuntu.com/questions/909277.\nsudo -H DEBIAN_FRONTEND=noninteractive apt-get install \"${APT_ARGS[@]}\" pkg-config zip zlib1g-dev unzip curl git wget > /dev/null\nsudo -H apt-get install \"${APT_ARGS[@]}\" python3-distutils > /dev/null\n\nnote_build_stage \"Install python3 packaging infrastructure\"\n\n# Avoid issue with pip's dependency resolver not accounting for all installed\n# packages.\nsudo -H apt-get install \"${APT_ARGS[@]}\" \"python3-testresources\"\n\n# Fix this error:\n# \"error: command 'x86_64-linux-gnu-gcc' failed: No such file or directory\"\nsudo -H apt-get install \"${APT_ARGS[@]}\" \"gcc\"\n\n# If we install python3-pip directly, the pip3 version points to:\n#   pip 8.1.1 from /usr/lib/python3/dist-packages (python 3.5)\n# Use the following lines to ensure correct Python version.\ncurl -o get-pip.py https://bootstrap.pypa.io/get-pip.py\npython3 get-pip.py --force-reinstall --user\nrm -f get-pip.py\n\necho \"$(python3 --version)\"\n\nexport PATH=\"$HOME/.local/bin\":$PATH\necho \"$(pip3 --version)\"\n\n################################################################################\n# python packages\n################################################################################\n\nnote_build_stage \"Install python3 packages\"\n\npip3 install \"${PIP_ARGS[@]}\" contextlib2\npip3 install \"${PIP_ARGS[@]}\" etils typing_extensions importlib_resources\npip3 install \"${PIP_ARGS[@]}\" 'enum34==1.1.8'\npip3 install \"${PIP_ARGS[@]}\" 'sortedcontainers==2.1.0'\npip3 install \"${PIP_ARGS[@]}\" 'intervaltree==3.0.2'\npip3 install \"${PIP_ARGS[@]}\" 'mock>=2.0.0'\npip3 install \"${PIP_ARGS[@]}\" ml_collections\npip3 install \"${PIP_ARGS[@]}\" --ignore-installed PyYAML\npip3 install \"${PIP_ARGS[@]}\" 'clu==0.0.9'\n# Note that protobuf installed with pip needs to be 3.13 because of the pyclif\n# version we're using. This is currently inconsistent with C++ protobuf version\n# in WORKSPACE and protobuf.BUILD, but we can't update those, because those\n# files need to be consistent with what TensorFlow needs, which is currently\n# still 3.9.2.\n# Ideally we want to make these protobuf versions all match, eventually.\npip3 install \"${PIP_ARGS[@]}\" 'protobuf==3.13.0'\npip3 install \"${PIP_ARGS[@]}\" 'argparse==1.4.0'\n\npip3 install \"${PIP_ARGS[@]}\" \"numpy==${DV_TF_NUMPY_VERSION}\"\n\n# Reason:\n# ========== [Wed Dec 11 19:57:32 UTC 2019] Stage 'Install python3 packages' starting\n# ERROR: pyasn1-modules 0.2.7 has requirement pyasn1<0.5.0,>=0.4.6, but you'll have pyasn1 0.1.9 which is incompatible.\npip3 install \"${PIP_ARGS[@]}\" 'pyasn1<0.5.0,>=0.4.6'\npip3 install \"${PIP_ARGS[@]}\" 'requests>=2.18'\npip3 install \"${PIP_ARGS[@]}\" --ignore-installed 'oauth2client>=4.0.0'\npip3 install \"${PIP_ARGS[@]}\" 'crcmod>=1.7'\npip3 install \"${PIP_ARGS[@]}\" 'six>=1.11.0'\npip3 install \"${PIP_ARGS[@]}\" joblib\npip3 install \"${PIP_ARGS[@]}\" psutil\npip3 install \"${PIP_ARGS[@]}\" --upgrade google-api-python-client\npip3 install \"${PIP_ARGS[@]}\" 'pandas==1.3.4'\n# We manually install jsonschema here to pin it to v3.2.0, since\n# the latest v4.0.1 has issues with Altair v4.1.0.\n# See https://github.com/altair-viz/altair/issues/2496\n# If Altair version is updated below, the jsonschema version\n# should also be updated accordingly.\npip3 install \"${PIP_ARGS[@]}\" 'jsonschema==3.2.0'\npip3 install \"${PIP_ARGS[@]}\" 'altair==4.1.0'\npip3 install \"${PIP_ARGS[@]}\" 'Pillow==9.5.0'\npip3 install \"${PIP_ARGS[@]}\" 'ipython==8.22.2'\npip3 install \"${PIP_ARGS[@]}\" 'pysam==0.20.0'\npip3 install \"${PIP_ARGS[@]}\" 'scikit-learn==1.0.2'\npip3 install \"${PIP_ARGS[@]}\" 'tensorflow-addons==0.21.0'\n# This is to avoid ERROR: No matching distribution found for opencv-python-headless==4.5.2.52.\n# TODO: Make this the same as ${DV_GCP_OPTIMIZED_TF_WHL_VERSION}\" later\npip3 install \"${PIP_ARGS[@]}\"  \"tf-models-official==2.13.1\"\n\n################################################################################\n# TensorFlow\n################################################################################\n\nnote_build_stage \"Install TensorFlow pip package\"\n\nif [[ \"${DV_USE_PREINSTALLED_TF}\" = \"1\" ]]; then\n  echo \"Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.\"\nelse\n  # Also pip install the latest TensorFlow with cpu support. We don't build the\n  # full TF from source, but instead using prebuilt version. However, we still\n  # need the full source version to build DeepVariant.\n\n  # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is\n  # necessary right now if we aren't pinning the TF source. We have observed\n  # runtime failures if there's too much skew between the released TF package and\n  # the source.\n  if [[ \"${DV_TF_NIGHTLY_BUILD}\" = \"1\" ]]; then\n    if [[ \"${DV_GPU_BUILD}\" = \"1\" ]]; then\n      echo \"Installing GPU-enabled TensorFlow nightly wheel\"\n      pip3 install \"${PIP_ARGS[@]}\" --upgrade tf_nightly_gpu\n    else\n      echo \"Installing CPU-only TensorFlow nightly wheel\"\n      pip3 install \"${PIP_ARGS[@]}\" --upgrade tf_nightly\n    fi\n  else\n    # Use the official TF release pip package.\n    if [[ \"${DV_GPU_BUILD}\" = \"1\" ]]; then\n      echo \"Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel\"\n      pip3 install \"${PIP_ARGS[@]}\" --upgrade \"tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}\"\n    else\n      echo \"Installing CPU TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel\"\n      pip3 install \"${PIP_ARGS[@]}\" --upgrade \"tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}\"\n    fi\n  fi\nfi\n\n# A temporary fix.\n# Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,\n# which caused the issue here: https://github.com/pallets/markupsafe/issues/286.\n# Specifically:\n# ImportError: cannot import name 'soft_unicode' from 'markupsafe'.\n# So, forcing a downgrade. This isn't the best solution, but we need it to get\n# our tests pass.\npip3 install \"${PIP_ARGS[@]}\" --upgrade 'markupsafe==2.0.1'\n\n################################################################################\n# CUDA\n################################################################################\n\nnote_build_stage \"Install CUDA\"\n\n# See https://www.tensorflow.org/install/source#gpu for versions required.\nif [[ \"${DV_GPU_BUILD}\" = \"1\" ]]; then\n  if [[ \"${DV_INSTALL_GPU_DRIVERS}\" = \"1\" ]]; then\n    # This script is only maintained for Ubuntu 22.04.\n    echo \"Checking for CUDA...\"\n    if ! dpkg-query -W cuda-11-8; then\n      echo \"Installing CUDA...\"\n      UBUNTU_VERSION=\"2204\"\n      curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/cuda-ubuntu${UBUNTU_VERSION}.pin\n      sudo mv cuda-ubuntu${UBUNTU_VERSION}.pin /etc/apt/preferences.d/cuda-repository-pin-600\n\n      curl https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub | gpg --dearmor | sudo tee /usr/share/keyrings/nvidia-cuda-archive-keyring.gpg > /dev/null\n      echo \\\n        \"deb [signed-by=/usr/share/keyrings/nvidia-cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /\" | \\\n        sudo tee /etc/apt/sources.list.d/cuda.list > /dev/null\n      sudo -H NEEDRESTART_MODE=a apt-get update \"${APT_ARGS[@]}\"\n      sudo -H DEBIAN_FRONTEND=noninteractive NEEDRESTART_MODE=a apt-get full-upgrade \"${APT_ARGS[@]}\"\n      sudo -H DEBIAN_FRONTEND=noninteractive NEEDRESTART_MODE=a apt-get install \"${APT_ARGS[@]}\" cuda-11-8\n    fi\n    echo \"Checking for CUDNN...\"\n    if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then\n      echo \"Installing CUDNN...\"\n      CUDNN_TAR_FILE=\"cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz\"\n      wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.6.0/local_installers/11.8/${CUDNN_TAR_FILE}\n      tar -xvf ${CUDNN_TAR_FILE}\n      sudo cp -P cudnn-linux-x86_64-8.6.0.163_cuda11-archive/include/cudnn.h /usr/local/cuda-11/include\n      sudo cp -P cudnn-linux-x86_64-8.6.0.163_cuda11-archive/lib/libcudnn* /usr/local/cuda-11/lib64/\n      sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*\n      sudo ldconfig\n    fi\n    # Tensorflow says to do this.\n    sudo -H NEEDRESTART_MODE=a apt-get install \"${APT_ARGS[@]}\" libcupti-dev > /dev/null\n  fi\n\n  # If we are doing a gpu-build, nvidia-smi should be install. Run it so we\n  # can see what gpu is installed.\n  nvidia-smi || :\nfi\n\n################################################################################\n# TensorRT\n################################################################################\n\nnote_build_stage \"Install TensorRT\"\n\n# Address the issue:\n# 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'\n# It's unclear whether we need this or not. Setting up to get rid of the errors.\nif [[ \"${DV_GPU_BUILD}\" = \"1\" ]]; then\n  pip3 install \"${PIP_ARGS[@]}\" nvidia-tensorrt\n  echo \"For debugging:\"\n  pip3 show nvidia-tensorrt\n  TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])')\n  # In v8.6.1, the libs got moved to tensorrt_libs:\n  # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1\n  sudo ln -sf \"${TENSORRT_PATH}_libs/libnvinfer.so.8\" \"${TENSORRT_PATH}_libs/libnvinfer.so.7\"\n  sudo ln -sf \"${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8\" \"${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7\"\n  export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs\"\n  sudo ldconfig\n  # Just in case this still doesn't work, we link them.\n  # This is a workaround that we might want to get rid of, if we can make sure\n  # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.\n  if [[ ! -e /usr/local/nvidia/lib ]]; then\n    sudo mkdir -p /usr/local/nvidia/lib\n    sudo ln -sf \"${TENSORRT_PATH}_libs/libnvinfer.so.7\" /usr/local/nvidia/lib/libnvinfer.so.7\n    sudo ln -sf \"${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7\" /usr/local/nvidia/lib/libnvinfer_plugin.so.7\n  fi\nfi\n\n################################################################################\n# Misc dependencies\n################################################################################\n\nnote_build_stage \"Install other packages\"\n\n# for htslib\nsudo -H NEEDRESTART_MODE=a apt-get install \"${APT_ARGS[@]}\" libssl-dev libcurl4-openssl-dev liblz-dev libbz2-dev liblzma-dev > /dev/null\n\n# for the debruijn graph\nsudo -H NEEDRESTART_MODE=a apt-get install \"${APT_ARGS[@]}\" libboost-graph-dev > /dev/null\n\n# Pin tf-models-official back to 2.11.6 to be closer to\n# ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} (which is 2.11.0).\n# This is to avoid the issue:\n# ValueError: Addons>LAMB has already been registered to <class 'tensorflow_addons.optimizers.lamb.LAMB'>\n# However, it's important that the protobuf pinning happens after this!\n# TODO: Remove this later once the first dependency can be changed\n#                to ${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.\npip3 install \"${PIP_ARGS[@]}\"  \"tf-models-official==2.11.6\"\n\n# Just being safe, pin protobuf's version one more time.\npip3 install \"${PIP_ARGS[@]}\" 'protobuf==3.13.0'\n\nnote_build_stage \"run-prereq.sh complete\"\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "settings.sh",
          "type": "blob",
          "size": 5.822265625,
          "content": "# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n# Source this file---these options are needed for TF config and for\n# successive bazel runs.\n\n# Set this to 1 if the system image already has TensorFlow preinstalled.  This\n# will skip the installation of TensorFlow.\nexport DV_USE_PREINSTALLED_TF=\"${DV_USE_PREINSTALLED_TF:-0}\"\n\nexport TF_NEED_GCP=1\n\nexport CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\n\n# The version of bazel we want to build DeepVariant.\n# https://www.tensorflow.org/install/source#tested_build_configurations\nDV_BAZEL_VERSION=\"5.3.0\"\n\n# We need to make sure that $HOME/bin is first in the binary search path so that\n# `bazel` will find the latest version of bazel installed in the user's home\n# directory. This is set in setting.sh as all DeepVariant scripts source\n# settings.sh and assume that `bazel` will find the right version.\nexport PATH=\"$HOME/bin:$PATH\"\n\n# Path to the public bucket containing DeepVariant-related artifacts.\nexport DEEPVARIANT_BUCKET=\"gs://deepvariant\"\nexport DV_PACKAGE_BUCKET_PATH=\"${DEEPVARIANT_BUCKET}/packages\"\nexport DV_PACKAGE_CURL_PATH=\"https://storage.googleapis.com/deepvariant/packages\"\n\n# Set this to 1 to use the nightly (latest) build of TensorFlow instead of a\n# named release version. Set it to an already existing value in the environment\n# (allowing command line control of the build), defaulting to 0 (release build).\n# Note that setting this to 1 implies that the C++ code in DeepVariant will be\n# build using the master branch and not the pinned version to avoid\n# incompatibilities between TensorFlow C++ used to build DeepVariant and the\n# tf-nightly wheel.\nexport DV_TF_NIGHTLY_BUILD=\"${DV_TF_NIGHTLY_BUILD:-0}\"\n\n# The branch/tag we checkout to build our C++ dependencies against. This is not\n# the same as the python version of TensorFlow we use, but should be similar or\n# we risk having version incompatibilities between our C++ code and the Python\n# code we use at runtime.\nif [[ \"${DV_TF_NIGHTLY_BUILD}\" = \"1\" ]]; then\n  export DV_CPP_TENSORFLOW_TAG=\"master\"\nelse\n  export DV_CPP_TENSORFLOW_TAG=\"v2.11.0\"\nfi\n# These WHL_VERSIONs determine the Python version of TensorFlow we use.\nexport DV_GCP_OPTIMIZED_TF_WHL_VERSION=\"2.11.0\"\nexport DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=\"2.11.0\"\nexport DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=\"2.11.0\"\n\n# Set this to 1 to use DeepVariant with GPUs. Set it to an already existing\n# value in the environment (allowing command line control of the build),\n# defaulting to 0 (CPU only build).\nexport DV_GPU_BUILD=\"${DV_GPU_BUILD:-0}\"\n\n# NOTE: CPU TensorFlow has a TF_ENABLE_ONEDNN_OPTS option that can be used to\n# enable Intel-specific optimization.\nexport GCP_OPTIMIZED_TF_WHL_FILENAME=\"tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl\"\nexport GCP_OPTIMIZED_TF_WHL_PATH=\"${DV_PACKAGE_BUCKET_PATH}/tensorflow\"\nexport GCP_OPTIMIZED_TF_WHL_CURL_PATH=\"${DV_PACKAGE_CURL_PATH}/tensorflow\"\nexport DV_TF_NUMPY_VERSION=\"1.21.2\"  # Python 3.10 requires >= 1.21.2\n\n# Set this to 1 to make our prereq scripts install the CUDA libraries.\n# If you already have CUDA installed, such as on a properly provisioned\n# Docker image, it shouldn't be necessary.\nexport DV_INSTALL_GPU_DRIVERS=\"${DV_INSTALL_GPU_DRIVERS:-0}\"\n\nexport PYTHON_VERSION=3.10\n# shellcheck disable=SC2155\nexport PYTHON_BIN_PATH=\"$(which python${PYTHON_VERSION})\"\nexport PYTHON_LIB_PATH=\"/usr/local/lib/python${PYTHON_VERSION}/dist-packages\"\nexport USE_DEFAULT_PYTHON_LIB_PATH=1\n# N.B. The --experimental_build_setting_api had to be added on protobuf\n# upgrade to 3.9.2 to avoid error in bazel_skylib:\n#   \"parameter 'build_setting' is experimental and thus unavailable with the\n#    current flags. It may be enabled by setting\n#    --experimental_build_setting_api\"\n# Presumably it won't be needed at some later point when bazel_skylib is\n# upgraded again.\nexport DV_COPT_FLAGS=\"--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11\"\n\nfunction note_build_stage {\n  echo \"========== [$(date)] Stage '${1}' starting\"\n}\n\nfunction wait_for_dpkg_lock {\n  # Wait for at most 5 minutes.\n  echo \"Calling wait_for_dpkg_lock.\"\n  max_wait=300\n  i=0\n  while sudo fuser /var/lib/dpkg/{lock,lock-frontend} >/dev/null 2>&1 ; do\n    echo \"Waiting to obtain dpkg lock..\"\n    sleep 10\n    ((i=i+10))\n    if (( i > max_wait )); then\n      echo \"ERROR: Waited for dpkg lock for 5 minutes.\"\n      exit 1\n    fi\n  done\n}\n"
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}