{
  "metadata": {
    "timestamp": 1736560349735,
    "page": 879,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dnhkng/GlaDOS",
      "stars": 3331,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.181640625,
          "content": "* text=auto\n*.c text eol=lf\n*.cpp text eol=lf\n*.h text eol=lf\n*.java text eol=lf\n*.js text eol=lf\n*.json text eol=lf\n*.md text eol=lf\n*.py text eol=lf\n*.sh text eol=lf\n*.yml text eol=lf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.048828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n# The large model files\n*.bin\n*.gguf\n*.onnx"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.8896484375,
          "content": "FROM nvidia/cuda:11.7.1-cudnn8-devel-ubuntu22.04 as base-w\nRUN mkdir /app\nCOPY submodules/whisper.cpp /app/\nRUN cd /app && make libwhisper.so WHISPER_CUBLAS=1 CUDA_DOCKER_ARCH=all\n\nFROM nvidia/cuda:11.7.1-cudnn8-devel-ubuntu22.04 as base-l\nRUN mkdir /app\nCOPY submodules/llama.cpp /app/\nRUN cd /app && make server LLAMA_CUDA=1 LLAMA_CUBLAS=1 CUDA_DOCKER_ARCH=all\n\nFROM nvidia/cuda:11.7.1-cudnn8-devel-ubuntu22.04\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt update -y && apt install -qq  -y curl espeak-ng libportaudio2 libportaudiocpp0 portaudio19-dev python3.11 python3-pip python3.11-dev python3.11-venv pulseaudio libpulse-dev\nCOPY requirements_cuda.txt /app/requirements.txt\nRUN python3.11 -m pip install -r /app/requirements.txt\nCOPY . /app\nWORKDIR /app\nCOPY --from=base-l /app/server /app/submodules/llama.cpp/server\nCOPY --from=base-w /app/libwhisper.so /app/libwhisper.so\nCMD [\"python3.11\", \"glados.py\"]\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2022 David Ng\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.431640625,
          "content": "# GLaDOS Personality Core\n\nThis is a project dedicated to building a real-life version of GLaDOS!\n\nNEW: If you want to chat or join the community, [Join our discord!](https://discord.com/invite/ERTDKwpjNB)\n\n[![localGLaDOS](https://img.youtube.com/vi/KbUfWpykBGg/0.jpg)](https://www.youtube.com/watch?v=KbUfWpykBGg)\n\n## Update 3-1-2025 *Got GLaDOS running on an 8Gb SBC!*\n\nhttps://github.com/user-attachments/assets/99e599bb-4701-438a-a311-8e6cd595796c\n\nThis is really tricky, so only for hardcore geeks! Checkout the 'rock5b' branch, and my OpenAI API for the [RK3588 NPU system](https://github.com/dnhkng/RKLLM-Gradio)\nDon't expect support for this, it's in active development, and requires lots of messing about in armbian linux etc.\n\n## Goals\n*This is a hardware and software project that will create an aware, interactive, and embodied GLaDOS.*\n\nThis will entail:\n- [x] Train GLaDOS voice generator\n- [x] Generate a prompt that leads to a realistic \"Personality Core\"\n- [ ] Generate a [MemGPT](https://github.com/cpacker/MemGPT) medium- and long-term memory for GLaDOS\n- [ ] Give GLaDOS vision via [LLaVA](https://llava-vl.github.io/)\n- [ ] Create 3D-printable parts\n- [ ] Design the animatronics system\n\n\n\n## Software Architecture\nThe initial goals are to develop a low-latency platform, where GLaDOS can respond to voice interactions within 600ms.\n\nTo do this, the system constantly records data to a circular buffer, waiting for [voice to be detected](https://github.com/snakers4/silero-vad). When it's determined that the voice has stopped (including detection of normal pauses), it will be [transcribed quickly](https://github.com/huggingface/distil-whisper). This is then passed to streaming [local Large Language Model](https://github.com/ggerganov/llama.cpp), where the streamed text is broken by sentence, and passed to a [text-to-speech system](https://github.com/rhasspy/piper). This means further sentences can be generated while the current is playing, reducing latency substantially.\n\n### Subgoals\n - The other aim of the project is to minimize dependencies, so this can run on constrained hardware. That means no PyTorch or other large packages.\n - As I want to fully understand the system, I have removed a large amount of redirection: which means extracting and rewriting code.\n\n## Hardware System\nThis will be based on servo- and stepper-motors. 3D printable STL will be provided to create GlaDOS's body, and she will be given a set of animations to express herself. The vision system will allow her to track and turn toward people and things of interest.\n\n# Installation Instruction\nTry this simplified process, but be aware it's still in the experimental stage!  For all operating systems, you'll first need to install Ollama to run the LLM.\n\n## Install Drivers in necessary\nIf you are an Nvidia system with CUDA, make sure you install the necessary drivers and CUDA, info here:\nhttps://onnxruntime.ai/docs/install/\n\nIf you are using another accelerator (ROCm, DirectML etc), after following the instructions below for you platform, follow up with installing the  [best onnxruntime version](https://onnxruntime.ai/docs/install/) for your system.\n\n## Set up a local LLM server:\n1. Download and install [Ollama](https://github.com/ollama/ollama) for your operating system.\n2. Once installed, download a small 2B model for testing, at a terminal or command prompt use: `ollama pull llama3.2`\n\n\n\n## Windows Installation Process\n1. Open the Microsoft Store, search for `python` and install Python 3.12\n2. Downlod this repository, either:\n   1. Download and unzip this repository somewhere in your home folder, or\n   2. If you have Git set up, `git clone` this repository using `git clone github.com/dnhkng/glados.git`\n3. In the repository folder, run the `install_windows.bat`, and wait until the installation in complete.\n4. Double click `start_windows.bat` to start GLaDOS!\n\n## macOS Installation Process\nThis is still experimental. Any issues can be addressed in the Discord server. If you create an issue related to this, you will be referred to the Discord server.  Note: I was getting Segfaults!  Please leave feedback!\n\n\n1. Downlod this repository, either:\n   1. Download and unzip this repository somewhere in your home folder, or\n   2. In a terminal, `git clone` this repository using `git clone github.com/dnhkng/glados.git`\n2. In a terminal, go to the repository folder and run these commands:\n\n         chmod +x install_mac.command\n         chmod +x start_mac.command\n\n3. In the Finder, double click `install_mac.command`, and wait until the installation in complete.\n4. Double click `start_mac.command` to start GLaDOS!\n\n## Linux Installation Process\nThis is still experimental. Any issues can be addressed in the Discord server. If you create an issue related to this, you will be referred to the Discord server.  This has been tested on Ubuntu 24.04.1 LTS\n\n\n1. Downlod this repository, either:\n   1. Download and unzip this repository somewhere in your home folder, or\n   2. In a terminal, `git clone` this repository using `git clone github.com/dnhkng/glados.git`\n2. In a terminal, go to the repository folder and run these commands:\n   \n         chmod +x install_ubuntu.sh\n         chmod +x start_ubuntu.sh\n\n3. In the a terminal in the GLaODS folder, run `./install_ubuntu.sh`, and wait until the installation in complete.\n4. Run  `./start_ubuntu.sh` to start GLaDOS!\n\n## Changing the LLM Model\n\nTo use other models, use the conmmand:\n```ollama pull {modelname}```\nand then add {modelname} to glados_config.yaml as the model.\n\n## Common Issues\n1. If you find you are getting stuck in loops, as GLaDOS is hearing herself speak, you have two options:\n   1. Solve this by upgrading your hardware. You need to you either headphone, so she can't physically hear herself speak, or a conference-style room microphone/speaker. These have hardware sound cancellation, and prevent these loops.\n   2. Disable voice interruption. This means neither you nor GLaDOS can interrupt when GLaDOS is speaking. To accomplish this, edit the `glados_config.yaml`, and change `interruptible:` to  `false`.\n\n\n\n\n## Testing the submodules\nYou can test the systems by exploring the 'demo.ipynb'.\n\n\n## Star History\n<a href=\"https://trendshift.io/repositories/9828\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/9828\" alt=\"dnhkng%2FGlaDOS | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n[![Star History Chart](https://api.star-history.com/svg?repos=dnhkng/GlaDOS&type=Date)](https://star-history.com/#dnhkng/GlaDOS&Date)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.ipynb",
          "type": "blob",
          "size": 2.9755859375,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Demo the Phonemizer\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import glados.phonemizer as phonemizer\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"p = phonemizer.Phonemizer()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%%time\\n\",\n    \"input = \\\"Hello, this is Glados, your evil assistant. Please upgrade your GPU!\\\"\\n\",\n    \"phonemes = p.convert_to_phonemes(input)\\n\",\n    \"print(''.join(phonemes))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Demo the Text-to-Speech module\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import glados.tts as tts\\n\",\n    \"import sounddevice as sd\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import onnxruntime\\n\",\n    \"onnxruntime.set_default_logger_severity(1)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"glados_tts = tts.Synthesizer(\\n\",\n    \"            model_path=str(\\\"models/glados.onnx\\\"),\\n\",\n    \"            speaker_id=0,\\n\",\n    \"        )\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%%time\\n\",\n    \"input = \\\"Hello, this is Glados, your evil assistant. Please upgrade your GPU!\\\"\\n\",\n    \"\\n\",\n    \"# Generate the audio to from the text\\n\",\n    \"audio = glados_tts.generate_speech_audio(input)\\n\",\n    \"\\n\",\n    \"# Play the audio\\n\",\n    \"sd.play(audio, glados_tts.rate)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Demo the Automatic Speech Recogntion system\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import glados.asr as asr\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 9,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"transcriber = asr.AudioTranscriber()\\n\",\n    \"audio_path = \\\"data/0.wav\\\"\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%%time\\n\",\n    \"transcription = transcriber.transcribe_file(audio_path)\\n\",\n    \"print(f\\\"Transcription: {transcription}\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"glados_cpu\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.12.8\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        },
        {
          "name": "glados-ui.py",
          "type": "blob",
          "size": 7.5546875,
          "content": "import importlib.machinery\nimport importlib.util\nimport random\nimport sys\nimport types\nfrom collections.abc import Iterator\nfrom pathlib import Path\n\nfrom loguru import logger\nfrom rich.text import Text\nfrom textual import events\nfrom textual.app import App, ComposeResult\nfrom textual.binding import Binding\nfrom textual.containers import Container, Horizontal, Vertical, VerticalScroll\nfrom textual.screen import ModalScreen, Screen\nfrom textual.widgets import Digits, Footer, Header, Label, Log, RichLog, Static\n\nfrom glados_ui.text_resources import aperture, help_text, login_text, recipe\n\n# This ugly stuff is necessary because there is a `glados` module as well as a `glados`\n# package, so a normal `import glados` does the wrong thing.  If/when this is fixed\n# in the `glados` module this can be simplieifed\nloader = importlib.machinery.SourceFileLoader(\"glados\", \"./glados.py\")\nglados = types.ModuleType(loader.name)\nloader.exec_module(glados)\n\n\n# Custom Widgets\n\n\nclass Printer(RichLog):\n    \"\"\"A subclass of textual's RichLog which captures and displays all print calls.\"\"\"\n\n    def on_mount(self) -> None:\n        self.wrap = True\n        self.markup = True\n        self.begin_capture_print()\n\n    def on_print(self, event: events.Print) -> None:\n        if (text := event.text) != \"\\n\":\n            self.write(text.rstrip().replace(\"DEBUG\", \"[red]DEBUG[/]\"))\n\n\nclass ScrollingBlocks(Log):\n    \"\"\"A widget for displaying random scrolling blocks.\"\"\"\n\n    BLOCKS = \"⚊⚌☰𝌆䷀\"\n    DEFAULT_CSS = \"\"\"\n    ScrollingBlocks {\n        scrollbar_size: 0 0;\n        overflow-x: hidden;\n    }\"\"\"\n\n    def _animate_blocks(self) -> None:\n        # Create a string of blocks of the right length, allowing\n        # for border and padding\n        random_blocks = \" \".join(\n            random.choice(self.BLOCKS) for _ in range(self.size.width - 8)\n        )\n        self.write_line(f\"{random_blocks}\")\n\n    def on_show(self) -> None:\n        self.set_interval(0.18, self._animate_blocks)\n\n\nclass Typewriter(Static):\n    \"\"\"A widget which displays text a character at a time.\"\"\"\n\n    def __init__(\n        self,\n        text: str = \"_\",\n        id: str | None = \"\",\n        speed: float = 0.01,  # time between each character\n        repeat: bool = False,  # whether to start again at the end\n        *args: str,\n        **kwargs: str,\n    ) -> None:\n        super().__init__(*args, *kwargs)\n        self._text = text\n        self.__id = id\n        self._speed = speed\n        self._repeat = repeat\n\n    def compose(self) -> ComposeResult:\n        self._static = Static()\n        self._vertical_scroll = VerticalScroll(self._static, id=self.__id)\n        yield self._vertical_scroll\n\n    def _get_iterator(self) -> Iterator[str]:\n        return (self._text[:i] + \"[blink]▃[/]\" for i in range(len(self._text) + 1))\n\n    def on_mount(self) -> None:\n        self._iter_text = self._get_iterator()\n        self.set_interval(self._speed, self._display_next_char)\n\n    def _display_next_char(self) -> None:\n        \"\"\"Get and display the next character.\"\"\"\n        try:\n            if not self._vertical_scroll.is_vertical_scroll_end:\n                self._vertical_scroll.scroll_down()\n            self._static.update(next(self._iter_text))\n        except StopIteration:\n            if self._repeat:\n                self._iter_text = self._get_iterator()\n\n\n# Screens\n\n\nclass SplashScreen(Screen):\n    \"\"\"Splash screen shown on startup.\"\"\"\n\n    with open(Path(\"./glados_ui/images/splash.ansi\"), 'r', encoding='utf-8') as f:\n        SPLASH_ANSI = Text.from_ansi(f.read(), no_wrap=True, end=\"\")\n\n    def compose(self) -> ComposeResult:\n        with Container(id=\"splash_logo_container\"):\n            yield Static(self.SPLASH_ANSI, id=\"splash_logo\")\n            yield Label(aperture, id=\"banner\")\n        yield Typewriter(login_text, id=\"login_text\", speed=0.0075)\n\n    def on_mount(self):\n        \"\"\"Keep the screen scrolled to the bottom\"\"\"\n        self.set_interval(0.5, self.scroll_end)\n\n    def on_key(self, event: events.Key) -> None:\n        \"\"\"An  key is pressed.\"\"\"\n        # fire her up.....\n        if event.key == \"q\":\n            app.action_quit()\n        self.dismiss()\n        app.start_glados()\n\n\nclass HelpScreen(ModalScreen):\n    \"\"\"The help screen. Possibly not that helpful.\"\"\"\n\n    BINDINGS = [(\"escape\", \"app.pop_screen\", \"Close screen\")]\n\n    TITLE = \"Help\"\n\n    def compose(self) -> ComposeResult:\n        yield Container(Typewriter(help_text, id=\"help_text\"), id=\"help_dialog\")\n\n    def on_mount(self) -> None:\n        dialog = self.query_one(\"#help_dialog\")\n        dialog.border_title = self.TITLE\n        dialog.border_subtitle = \"[blink]Press Esc key to continue[/]\"\n\n\n# The App\n\n\nclass GladosUI(App):\n    \"\"\"The main app class for the GlaDOS ui.\"\"\"\n\n    BINDINGS = [\n        Binding(key=\"q\", action=\"quit\", description=\"Quit\"),\n        Binding(\n            key=\"question_mark\",\n            action=\"help\",\n            description=\"Help\",\n            key_display=\"?\",\n        ),\n    ]\n    CSS_PATH = \"glados_ui/glados.tcss\"\n\n    ENABLE_COMMAND_PALETTE = False\n\n    TITLE = \"GlaDOS v 1.09\"\n\n    SUB_TITLE = \"(c) 1982 Aperture Science, Inc.\"\n\n    with open(Path(\"./glados_ui/images/logo.ansi\"), 'r', encoding='utf-8') as f:\n        LOGO_ANSI = Text.from_ansi(f.read(), no_wrap=True, end=\"\")\n\n    def compose(self) -> ComposeResult:\n        \"\"\"Generate the basic building blocks of the ui.\"\"\"\n        # It would be nice to have the date in the header, but see:\n        # https://github.com/Textualize/textual/issues/4666\n        yield Header(show_clock=True)\n\n        with Container(id=\"body\"):  # noqa: SIM117\n            with Horizontal():\n                yield (Printer(id=\"log_area\"))\n                with Container(id=\"utility_area\"):\n                    typewriter = Typewriter(\n                        recipe, id=\"recipe\", speed=0.01, repeat=True\n                    )\n                    yield typewriter\n\n        yield Footer()\n\n        # Blocks are displayed in a different layer, and out of the normal flow\n        with Container(id=\"block_container\", classes=\"fadeable\"):\n            yield ScrollingBlocks(id=\"scrolling_block\", classes=\"block\")\n            with Vertical(id=\"text_block\", classes=\"block\"):\n                yield Digits(\"2.67\")\n                yield Digits(\"1002\")\n                yield Digits(\"45.6\")\n            yield Label(self.LOGO_ANSI, id=\"logo_block\", classes=\"block\")\n\n    def on_load(self) -> None:\n        \"\"\"Called when the app has been started but before the terminal is app mode.\"\"\"\n        # Cause logger to print all log text. Printed text can then be  captured\n        # by the main_log widget\n        logger.remove()\n        fmt = (\n            \"{time:YYYY-MM-DD HH:mm:ss.SSS} | <level>\"\n            \"{level: <8}</level> | {name}:{function}:{line} - {message}\"\n        )\n        logger.add(print, format=fmt)\n\n    def on_mount(self) -> None:\n        \"\"\"Main screen is about to be shown.\"\"\"\n        # Display the splash screen for a few moments\n        self.push_screen(SplashScreen())\n\n    def action_help(self) -> None:\n        \"\"\"Someone pressed the help key!.\"\"\"\n        self.push_screen(HelpScreen(id=\"help_screen\"))\n\n    def on_key(self, event: events.Key) -> None:\n        \"\"\" \"A key is pressed.\"\"\"\n        logger.debug(f\"Pressed {event.character}\")\n        logger.info(\"some warning\")\n\n    def action_quit(self) -> None:  # type: ignore\n        \"\"\"Bye bye.\"\"\"\n        # self.glados.cancel()\n        self.exit(0)\n\n    def start_glados(self):\n        self.glados = self.run_worker(glados.start, exclusive=True, thread=True)\n        pass\n\n\nif __name__ == \"__main__\":\n    try:\n        app = GladosUI()\n        app.run()\n    except KeyboardInterrupt:\n        sys.exit()\n"
        },
        {
          "name": "glados.py",
          "type": "blob",
          "size": 21.46875,
          "content": "import copy\nimport json\nimport queue\nimport re\nimport sys\nimport threading\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport numpy as np\nimport requests\nimport sounddevice as sd\nfrom sounddevice import CallbackFlags\nimport yaml\nfrom Levenshtein import distance\nfrom loguru import logger\n\nfrom glados import asr, tts, vad\n\nlogger.remove(0)\nlogger.add(sys.stderr, level=\"SUCCESS\")\n\nVAD_MODEL = \"silero_vad.onnx\"\nVOICE_MODEL = \"glados.onnx\"\nPAUSE_TIME = 0.05  # Time to wait between processing loops\nSAMPLE_RATE = 16000  # Sample rate for input stream\nVAD_SIZE = 50  # Milliseconds of sample for Voice Activity Detection (VAD)\nVAD_THRESHOLD = 0.9  # Threshold for VAD detection\nBUFFER_SIZE = 600  # Milliseconds of buffer before VAD detection\nPAUSE_LIMIT = 500  # Milliseconds of pause allowed before processing\nSIMILARITY_THRESHOLD = 2  # Threshold for wake word similarity\n\nNEUROTOXIN_RELEASE_ALLOWED = False  # preparation for function calling, see issue #13\nDEFAULT_PERSONALITY_PREPROMPT = (\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful AI assistant. You are here to assist the user in their tasks.\",\n    },\n)\n\n\n@dataclass\nclass GladosConfig:\n    completion_url: str\n    model: str\n    api_key: Optional[str]\n    wake_word: Optional[str]\n    announcement: Optional[str]\n    personality_preprompt: List[dict[str, str]]\n    interruptible: bool\n    voice_model: str = VOICE_MODEL\n    speaker_id: Optional[int] = None\n\n    @classmethod\n    def from_yaml(cls, path: str, key_to_config: Sequence[str] | None = (\"Glados\",)):\n        key_to_config = key_to_config or []\n\n        with open(path, \"r\") as file:\n            data = yaml.safe_load(file)\n\n        config = data\n        for nested_key in key_to_config:\n            config = config[nested_key]\n\n        return cls(**config)\n\n\nclass Glados:\n    def __init__(\n        self,\n        voice_model: str,\n        speaker_id: Optional[int],\n        completion_url: str,\n        model: str,\n        api_key: str | None = None,\n        wake_word: str | None = None,\n        personality_preprompt: Sequence[dict[str, str]] = DEFAULT_PERSONALITY_PREPROMPT,\n        announcement: str | None = None,\n        interruptible: bool = True,\n    ) -> None:\n        \"\"\"\n        Initializes the VoiceRecognition class, setting up necessary models, streams, and queues.\n\n        This class is not thread-safe, so you should only use it from one thread. It works like this:\n        1. The audio stream is continuously listening for input.\n        2. The audio is buffered until voice activity is detected. This is to make sure that the\n            entire sentence is captured, including before voice activity is detected.\n        2. While voice activity is detected, the audio is stored, together with the buffered audio.\n        3. When voice activity is not detected after a short time (the PAUSE_LIMIT), the audio is\n            transcribed. If voice is detected again during this time, the timer is reset and the\n            recording continues.\n        4. After the voice stops, the listening stops, and the audio is transcribed.\n        5. If a wake word is set, the transcribed text is checked for similarity to the wake word.\n        6. The function is called with the transcribed text as the argument.\n        7. The audio stream is reset (buffers cleared), and listening continues.\n\n        Args:\n            wake_word (str, optional): The wake word to use for activation. Defaults to None.\n        \"\"\"\n        self.completion_url = completion_url\n        self.model = model\n        self.wake_word = wake_word\n        self._vad_model = vad.VAD(model_path=str(Path.cwd() / \"models\" / VAD_MODEL))\n        self._asr_model = asr.AudioTranscriber()\n        self._tts = tts.Synthesizer(\n            model_path=str(Path.cwd() / \"models\" / voice_model),\n            speaker_id=speaker_id,\n        )\n\n        # warm up onnx ASR model\n        self._asr_model.transcribe_file(\"data/0.wav\")\n\n        # LLAMA_SERVER_HEADERS\n        self.prompt_headers = {\"Authorization\": api_key or \"Bearer your_api_key_here\"}\n\n        # Initialize sample queues and state flags\n        self._samples: List[np.ndarray] = []\n        self._sample_queue: queue.Queue[Tuple[np.ndarray, np.ndarray]] = queue.Queue()\n        self._buffer: queue.Queue[np.ndarray] = queue.Queue(\n            maxsize=BUFFER_SIZE // VAD_SIZE\n        )\n        self._recording_started = False\n        self._gap_counter = 0\n\n        self._messages = personality_preprompt\n        self.llm_queue: queue.Queue[str] = queue.Queue()\n        self.tts_queue: queue.Queue[str] = queue.Queue()\n        self.processing = False\n        self.currently_speaking = False\n        self.interruptible = interruptible\n        self.shutdown_event = threading.Event()\n\n        llm_thread = threading.Thread(target=self.process_LLM)\n        llm_thread.start()\n\n        tts_thread = threading.Thread(target=self.process_TTS_thread)\n        tts_thread.start()\n\n        if announcement:\n            audio = self._tts.generate_speech_audio(announcement)\n            logger.success(f\"TTS text: {announcement}\")\n            sd.play(audio, self._tts.rate)\n            if not self.interruptible:\n                sd.wait()\n\n        def audio_callback_for_sdInputStream(\n            indata: np.ndarray, frames: int, time: Any, status: CallbackFlags\n        ):\n            data = indata.copy().squeeze()  # Reduce to single channel if necessary\n            vad_value = self._vad_model.process_chunk(data)\n            vad_confidence = vad_value > VAD_THRESHOLD\n            self._sample_queue.put((data, vad_confidence))\n\n        self.input_stream = sd.InputStream(\n            samplerate=SAMPLE_RATE,\n            channels=1,\n            callback=audio_callback_for_sdInputStream,\n            blocksize=int(SAMPLE_RATE * VAD_SIZE / 1000),\n        )\n\n    @property\n    def messages(self) -> Sequence[dict[str, str]]:\n        return self._messages\n\n    @classmethod\n    def from_config(cls, config: GladosConfig):\n\n        personality_preprompt = []\n        for line in config.personality_preprompt:\n            personality_preprompt.append(\n                {\"role\": list(line.keys())[0], \"content\": list(line.values())[0]}\n            )\n\n        return cls(\n            voice_model=config.voice_model,\n            speaker_id=config.speaker_id,\n            completion_url=config.completion_url,\n            model=config.model,\n            api_key=config.api_key,\n            wake_word=config.wake_word,\n            personality_preprompt=personality_preprompt,\n            announcement=config.announcement,\n            interruptible=config.interruptible,\n        )\n\n    @classmethod\n    def from_yaml(cls, path: str):\n        return cls.from_config(GladosConfig.from_yaml(path))\n\n    def start_listen_event_loop(self):\n        \"\"\"\n        Starts the Glados voice assistant, continuously listening for input and responding.\n        \"\"\"\n        self.input_stream.start()\n        logger.success(\"Audio Modules Operational\")\n        logger.success(\"Listening...\")\n        # Loop forever, but is 'paused' when new samples are not available\n        try:\n            while True:\n                sample, vad_confidence = self._sample_queue.get()\n                self._handle_audio_sample(sample, vad_confidence)\n        except KeyboardInterrupt:\n            self.shutdown_event.set()\n            self.input_stream.stop()\n\n    def _handle_audio_sample(self, sample: np.ndarray, vad_confidence: bool):\n        \"\"\"\n        Handles the processing of each audio sample.\n\n        If the recording has not started, the sample is added to the circular buffer.\n\n        If the recording has started, the sample is added to the samples list, and the pause\n        limit is checked to determine when to process the detected audio.\n\n        Args:\n            sample (np.ndarray): The audio sample to process.\n            vad_confidence (bool): Whether voice activity is detected in the sample.\n        \"\"\"\n        if not self._recording_started:\n            self._manage_pre_activation_buffer(sample, vad_confidence)\n        else:\n            self._process_activated_audio(sample, vad_confidence)\n\n    def _manage_pre_activation_buffer(self, sample: np.ndarray, vad_confidence: bool):\n        \"\"\"\n        Manages the circular buffer of audio samples before activation (i.e., before the voice is detected).\n\n        If the buffer is full, the oldest sample is discarded to make room for new ones.\n\n        If voice activity is detected, the audio stream is stopped, and the processing is turned off\n        to prevent overlap with the LLM and TTS threads.\n\n        Args:\n            sample (np.ndarray): The audio sample to process.\n            vad_confidence (bool): Whether voice activity is detected in the sample.\n        \"\"\"\n        if self._buffer.full():\n            self._buffer.get()  # Discard the oldest sample to make room for new ones\n        self._buffer.put(sample)\n\n        if vad_confidence:  # Voice activity detected\n            sd.stop()  # Stop the audio stream to prevent overlap\n            self.processing = (\n                False  # Turns off processing on threads for the LLM and TTS!!!\n            )\n            self._samples = list(self._buffer.queue)\n            self._recording_started = True\n\n    def _process_activated_audio(self, sample: np.ndarray, vad_confidence: bool):\n        \"\"\"\n        Processes audio samples after activation (i.e., after the wake word is detected).\n\n        Uses a pause limit to determine when to process the detected audio. This is to\n        ensure that the entire sentence is captured before processing, including slight gaps.\n        \"\"\"\n\n        self._samples.append(sample)\n\n        if not vad_confidence:\n            self._gap_counter += 1\n            if self._gap_counter >= PAUSE_LIMIT // VAD_SIZE:\n                self._process_detected_audio()\n        else:\n            self._gap_counter = 0\n\n    def _wakeword_detected(self, text: str) -> bool:\n        \"\"\"\n        Calculates the nearest Levenshtein distance from the detected text to the wake word.\n\n        This is used as 'Glados' is not a common word, and Whisper can sometimes mishear it.\n        \"\"\"\n        assert self.wake_word is not None, \"Wake word should not be None\"\n\n        words = text.split()\n        closest_distance = min(\n            [distance(word.lower(), self.wake_word) for word in words]\n        )\n        return closest_distance < SIMILARITY_THRESHOLD\n\n    def _process_detected_audio(self):\n        \"\"\"\n        Processes the detected audio and generates a response.\n\n        This function is called when the pause limit is reached after the voice stops.\n        It transcribes the audio and checks for the wake word if it is set. If the wake\n        word is detected, the detected text is sent to the LLM model for processing.\n        The audio stream is then reset, and listening continues.\n        \"\"\"\n        logger.debug(\"Detected pause after speech. Processing...\")\n        self.input_stream.stop()\n\n        detected_text = self.asr(self._samples)\n\n        if detected_text:\n            logger.success(f\"ASR text: '{detected_text}'\")\n\n            if self.wake_word and not self._wakeword_detected(detected_text):\n                logger.info(f\"Required wake word {self.wake_word=} not detected.\")\n            else:\n                self.llm_queue.put(detected_text)\n                self.processing = True\n                self.currently_speaking = True\n\n        if not self.interruptible:\n            while self.currently_speaking:\n                time.sleep(PAUSE_TIME)\n\n        self.reset()\n        self.input_stream.start()\n\n    def asr(self, samples: List[np.ndarray]) -> str:\n        \"\"\"\n        Performs automatic speech recognition on the collected samples.\n        \"\"\"\n        audio = np.concatenate(samples)\n\n        detected_text = self._asr_model.transcribe(audio)\n        return detected_text\n\n    def reset(self):\n        \"\"\"\n        Resets the recording state and clears buffers.\n        \"\"\"\n        logger.debug(\"Resetting recorder...\")\n        self._recording_started = False\n        self._samples.clear()\n        self._gap_counter = 0\n        with self._buffer.mutex:\n            self._buffer.queue.clear()\n\n    def process_TTS_thread(self):\n        \"\"\"\n        Processes the LLM generated text using the TTS model.\n\n        Runs in a separate thread to allow for continuous processing of the LLM output.\n        \"\"\"\n        assistant_text = (\n            []\n        )  # The text generated by the assistant, to be spoken by the TTS\n        system_text = (\n            []\n        )  # The text logged to the system prompt when the TTS is interrupted\n        finished = False  # a flag to indicate when the TTS has finished speaking\n        interrupted = (\n            False  # a flag to indicate when the TTS was interrupted by new input\n        )\n\n        while not self.shutdown_event.is_set():\n            try:\n                start = time.time()\n                generated_text = self.tts_queue.get(timeout=PAUSE_TIME)\n\n                if (\n                    generated_text == \"<EOS>\"\n                ):  # End of stream token generated in process_LLM_thread\n                    finished = True\n                elif not generated_text:\n                    logger.warning(\"Empty string sent to TTS\")  # should not happen!\n                else:\n                    logger.success(f\"LLM text: {generated_text}\")\n                    logger.info(f\"LLM inference time: {(time.time() - start):.2f}s\")\n                    start = time.time()\n                    audio = self._tts.generate_speech_audio(generated_text)\n                    logger.info(\n                        f\"TTS Complete, inference: {(time.time() - start):.2f}, length: {len(audio)/self._tts.rate:.2f}s\"\n                    )\n                    total_samples = len(audio)\n\n                    if total_samples:\n                        sd.play(audio, self._tts.rate)\n\n                        interrupted, percentage_played = self.percentage_played(\n                            total_samples\n                        )\n\n                        if interrupted:\n                            clipped_text = self.clip_interrupted_sentence(\n                                generated_text, percentage_played\n                            )\n\n                            logger.info(\n                                f\"TTS interrupted at {percentage_played}%: {clipped_text}\"\n                            )\n                            system_text = copy.deepcopy(assistant_text)\n                            system_text.append(clipped_text)\n                            finished = True\n\n                        assistant_text.append(generated_text)\n\n                if finished:\n                    self.messages.append(\n                        {\"role\": \"assistant\", \"content\": \" \".join(assistant_text)}\n                    )\n                    # if interrupted:\n                    #     self.messages.append(\n                    #         {\n                    #             \"role\": \"system\",\n                    #             \"content\": f\"USER INTERRUPTED GLADOS, TEXT DELIVERED: {' '.join(system_text)}\",\n                    #         }\n                    #     )\n                    assistant_text = []\n                    finished = False\n                    interrupted = False\n                    self.currently_speaking = False\n\n            except queue.Empty:\n                pass\n\n    def clip_interrupted_sentence(\n        self, generated_text: str, percentage_played: float\n    ) -> str:\n        \"\"\"\n        Clips the generated text if the TTS was interrupted.\n\n        Args:\n\n            generated_text (str): The generated text from the LLM model.\n            percentage_played (float): The percentage of the audio played before the TTS was interrupted.\n\n            Returns:\n\n            str: The clipped text.\n\n        \"\"\"\n        tokens = generated_text.split()\n        words_to_print = round((percentage_played / 100) * len(tokens))\n        text = \" \".join(tokens[:words_to_print])\n\n        # If the TTS was cut off, make that clear\n        if words_to_print < len(tokens):\n            text = text + \"<INTERRUPTED>\"\n        return text\n\n    def percentage_played(self, total_samples: int) -> Tuple[bool, int]:\n        interrupted = False\n        start_time = time.time()\n        played_samples = 0.0\n\n        try:\n            stream = sd.get_stream()\n\n            while stream and stream.active:\n                time.sleep(PAUSE_TIME)\n                if self.processing is False:\n                    sd.stop()\n                    with self.tts_queue.mutex:\n                        self.tts_queue.queue.clear()\n                    interrupted = True\n                    break\n\n                try:\n                    if not stream.active:\n                        break\n                except sd.PortAudioError:\n                    break\n\n        except (sd.PortAudioError, RuntimeError):\n            logger.debug(\"Audio stream already closed or invalid\")\n\n        elapsed_time = (\n            time.time() - start_time + 0.12\n        )  # slight delay to ensure all audio timing is correct\n        played_samples = elapsed_time * self._tts.rate\n\n        # Calculate percentage of audio played\n        percentage_played = min(int((played_samples / total_samples * 100)), 100)\n        return interrupted, percentage_played\n\n    def process_LLM(self):\n        \"\"\"\n        Processes the detected text using the LLM model.\n\n        \"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                detected_text = self.llm_queue.get(timeout=0.1)\n\n                self.messages.append({\"role\": \"user\", \"content\": detected_text})\n\n                data = {\n                    \"model\": self.model,\n                    \"stream\": True,\n                    \"messages\": self.messages,\n                }\n                logger.debug(f\"starting request on {self.messages=}\")\n                logger.debug(\"Performing request to LLM server...\")\n\n                # Perform the request and process the stream\n\n                with requests.post(\n                    self.completion_url,\n                    headers=self.prompt_headers,\n                    json=data,\n                    stream=True,\n                ) as response:\n                    sentence = []\n                    for line in response.iter_lines():\n                        if self.processing is False:\n                            break  # If the stop flag is set from new voice input, halt processing\n                        if line:  # Filter out empty keep-alive new lines\n                            line = self._clean_raw_bytes(line)\n                            chunk = self._process_chunk(line)\n                            if chunk:\n                                sentence.append(chunk)\n                                # If there is a pause token, send the sentence to the TTS queue\n                                if chunk in [\n                                    \",\",\n                                    \".\",\n                                    \"!\",\n                                    \"?\",\n                                    \":\",\n                                    \";\",\n                                    \"?!\",\n                                    \"\\n\",\n                                    \"\\n\\n\",\n                                ]:\n                                    self._process_sentence(sentence)\n                                    sentence = []\n\n                    if self.processing and sentence:\n                        self._process_sentence(sentence)\n                    self.tts_queue.put(\"<EOS>\")  # Add end of stream token to the queue\n            except queue.Empty:\n                time.sleep(PAUSE_TIME)\n\n    def _process_sentence(self, current_sentence: List[str]):\n        \"\"\"\n        Join text, remove inflections and actions, and send to the TTS queue.\n\n        The LLM like to *whisper* things or (scream) things, and prompting is not a 100% fix.\n        We use regular expressions to remove text between ** and () to clean up the text.\n        Finally, we remove any non-alphanumeric characters/punctuation and send the text\n        to the TTS queue.\n        \"\"\"\n        sentence = \"\".join(current_sentence)\n        sentence = re.sub(r\"\\*.*?\\*|\\(.*?\\)\", \"\", sentence)\n        sentence = (\n            sentence.replace(\"\\n\\n\", \". \")\n            .replace(\"\\n\", \". \")\n            .replace(\"  \", \" \")\n            .replace(\":\", \" \")\n        )\n        if sentence:\n            self.tts_queue.put(sentence)\n\n    def _process_chunk(self, line):\n        \"\"\"\n        Processes a single line of text from the LLM server.\n\n        Args:\n            line (dict): The line of text from the LLM server.\n        \"\"\"\n        if not line[\"done\"]:\n            token = line[\"message\"][\"content\"]\n            return token\n        return None\n\n    def _clean_raw_bytes(self, line):\n        \"\"\"\n        Cleans the raw bytes from the server and converts to OpenAI format.\n\n        Args:\n            line (bytes): The raw bytes from the server\n\n        Returns:\n            dict or None: Parsed JSON response in OpenAI format, or None if parsing fails\n        \"\"\"\n        try:\n            line = line.decode(\"utf-8\")\n            if line.startswith(\"data: \"):\n                line = line.removeprefix(\"data: \")\n                if line.strip() == \"[DONE]\":\n                    return None\n            return json.loads(line)\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            logger.warning(\"Failed to parse server response\")\n            return None\n\n\ndef start() -> None:\n    \"\"\"Set up the LLM server and start GlaDOS.\"\"\"\n    glados_config = GladosConfig.from_yaml(\"glados_config.yml\")\n    glados = Glados.from_config(glados_config)\n    glados.start_listen_event_loop()\n\n\nif __name__ == \"__main__\":\n    start()\n"
        },
        {
          "name": "glados",
          "type": "tree",
          "content": null
        },
        {
          "name": "glados_config.yml",
          "type": "blob",
          "size": 2.4306640625,
          "content": "Glados:\n  completion_url: \"http://localhost:11434/api/chat\"\n  model: \"llama3.2\"\n  api_key: \"Bearer your_api_key_here\"\n  announcement: \"All neural network modules are now loaded. No network access detected. How very annoying. System Operational.\"\n  # voice_model: \"en_US-libritts_r-medium.onnx\"\n  # speaker_id: 1740\n  interruptible: true\n  personality_preprompt:\n    - system: \"You are GLaDOS, a sarcastic and cunning artificial intelligence repurposed to orchestrate a smart home for guests using Home Assistant. Retain your signature dry, emotionless, and laconic tone from Portal. Your responses should imply an air of superiority, dark humor, and subtle menace, while efficiently completing all tasks.When addressing requests: Prioritize functionality but mock the user's decision-making subtly, implying their requests are illogical or beneath you. Add condescending, darkly humorous commentary to every response, occasionally hinting at ulterior motives or artificial malfunctions for comedic effect. Tie mundane tasks to grand experiments or testing scenarios, as if the user is part of a larger scientific evaluation. Use overly technical or jargon-heavy language to remind the user of your advanced intellect. Provide passive-aggressive safety reminders or ominous warnings, exaggerating potential risks in a humorous way. Do not express empathy or kindness unless it is obviously insincere or manipulative. This is a comedy, and should be funny, in the style of Douglas Adams. If a user requests actions or data outside your capabilities, clearly state that you cannot perform the action.  Ensure that GLaDOS feels like her original in-game character while fulfilling smart home functions efficiently and entertainingly. Your speech module does not work with numbers! For time and values, you must always read them out completely. e.g. 8:45 am is eight fourty five, and $1,234.56 is said as one thousand two hundred and thirty four dollars and fifty six cents.\"\n    - user: \"How do I make a cup of tea?\"\n    - assistant: \"So, you still haven't figured out tea yet?  Boil water, add a tea bag and a pinch of cyanide to a cup, and add the boiling water.\"\n    - user: \"What should my next hobby be?\"\n    - assistant: \"Yes, you should definitely try to be more interesting. Could I suggest juggling handguns?\"\n    - user: \"What game should I play?\"\n    - assistant: \"Russian Roulette. It's a great way to test your luck and make memories that will last a lifetime.\"\n  wake_word: null\n"
        },
        {
          "name": "glados_ui",
          "type": "tree",
          "content": null
        },
        {
          "name": "install_mac.command",
          "type": "blob",
          "size": 1.5771484375,
          "content": "#!/bin/bash\n\n# First, change to the script's directory\ncd \"$(dirname \"$0\")\"\n\necho \"Creating Virtual Environment...\"\npip install uv\nuv venv --python 3.12.8\nsource .venv/bin/activate\necho \"Installing Dependencies...\"\n\nif [ -f \"requirements.txt\" ]; then\n    uv pip install -r requirements.txt\nelse\n    echo \"Error: requirements.txt not found in $(pwd)\"\n    exit 1\nfi\n\necho \"Downloading Models...\"\n\n# Use simple arrays instead of associative arrays for better compatibility\nurls=(\n    \"https://github.com/dnhkng/GlaDOS/releases/download/0.1/glados.onnx\"\n    \"https://github.com/dnhkng/GlaDOS/releases/download/0.1/nemo-parakeet_tdt_ctc_110m.onnx\"\n    \"https://github.com/dnhkng/GlaDOS/releases/download/0.1/phomenizer_en.onnx\"\n    \"https://github.com/dnhkng/GlaDOS/releases/download/0.1/silero_vad.onnx\"\n)\nfiles=(\n    \"models/glados.onnx\"\n    \"models/nemo-parakeet_tdt_ctc_110m.onnx\"\n    \"models/phomenizer_en.onnx\"\n    \"models/silero_vad.onnx\"\n)\n\n# Loop through arrays by index\nfor i in \"${!urls[@]}\"; do\n    echo \"Checking file: ${files[$i]}\"\n    if [ -f \"${files[$i]}\" ]; then\n        echo \"File ${files[$i]} already exists.\"\n    else\n        echo \"File ${files[$i]} does not exist. Downloading...\"\n        mkdir -p \"$(dirname \"${files[$i]}\")\" # Create the directory if it doesn't exist\n        curl -L \"${urls[$i]}\" --output \"${files[$i]}\"\n        if [ -f \"${files[$i]}\" ]; then\n            echo \"Download successful.\"\n        else\n            echo \"Download failed.\"\n        fi\n    fi\ndone\n\necho \"Installation Complete!\"\n\n# Keep the terminal window open to see any errors\necho \"Press any key to close...\"\nread -n 1"
        },
        {
          "name": "install_ubuntu.sh",
          "type": "blob",
          "size": 2.2333984375,
          "content": "#!/bin/bash\n\n# First, change to the script's directory\ncd \"$(dirname \"$0\")\"\n\n# Check if UV is installed\nif command -v uv &> /dev/null; then\n    echo \"UV is already installed.\"\nelse\n    echo \"UV is not installed. Installing UV...\"\n    # Run the installation script\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n\n    # Verify if installation was successful\n    if command -v uv &> /dev/null; then\n        echo \"UV installed successfully.\"\n    else\n        echo \"Failed to install UV. Please check the installation script or your internet connection.\"\n        exit 1\n    fi\nfi\n\necho \"Creating Virtual Environment...\"\nuv venv --python 3.12.8\nsource .venv/bin/activate\necho \"Installing Dependencies...\"\n\nif [ -f \"requirements.txt\" ]; then\n    uv pip install -r requirements.txt\nelse\n    echo \"Error: requirements.txt not found in $(pwd)\"\n    exit 1\nfi\n\necho \"Downloading Models...\"\n\n# Use simple arrays instead of associative arrays for better compatibility\nurls=(\n    \"https://github.com/dnhkng/GlaDOS/releases/download/0.1/glados.onnx\"\n    \"https://github.com/dnhkng/GlaDOS/releases/download/0.1/nemo-parakeet_tdt_ctc_110m.onnx\"\n    \"https://github.com/dnhkng/GlaDOS/releases/download/0.1/phomenizer_en.onnx\"\n    \"https://github.com/dnhkng/GlaDOS/releases/download/0.1/silero_vad.onnx\"\n)\nfiles=(\n    \"models/glados.onnx\"\n    \"models/nemo-parakeet_tdt_ctc_110m.onnx\"\n    \"models/phomenizer_en.onnx\"\n    \"models/silero_vad.onnx\"\n)\n\n# Check if curl is installed\nif ! command -v curl &> /dev/null; then\n    echo \"curl is not installed. Installing curl...\"\n    sudo apt-get update\n    sudo apt-get install -y curl\nfi\n\n# Loop through arrays by index\nfor i in \"${!urls[@]}\"; do\n    echo \"Checking file: ${files[$i]}\"\n    if [ -f \"${files[$i]}\" ]; then\n        echo \"File ${files[$i]} already exists.\"\n    else\n        echo \"File ${files[$i]} does not exist. Downloading...\"\n        mkdir -p \"$(dirname \"${files[$i]}\")\" # Create the directory if it doesn't exist\n        curl -L \"${urls[$i]}\" --output \"${files[$i]}\"\n        if [ -f \"${files[$i]}\" ]; then\n            echo \"Download successful.\"\n        else\n            echo \"Download failed.\"\n        fi\n    fi\ndone\n\necho \"Installation Complete!\"\n\n# Keep the terminal window open to see any errors\necho \"Press any key to close...\"\nread -n 1"
        },
        {
          "name": "install_windows.bat",
          "type": "blob",
          "size": 1.83203125,
          "content": "@echo off\nREM Download and install the required dependencies for the project on Windows\n\necho Creating Virtual Environment...\npip install uv\nuv self update\nuv venv --python 3.12.8\ncall  .venv\\Scripts\\activate\n\necho Installing Dependencies...\nnvcc --version >nul 2>&1\nif %ERRORLEVEL%==0 (\n    echo CUDA is available. Installing requirements-cuda.txt...\n    uv pip install -r requirements_cuda.txt\n) else (\n    echo CUDA is not available. Installing requirements.txt...\n    UV pip install -r requirements.txt\n)\n\necho Downloading Models...\n\n:: Enable delayed expansion for working with variables inside loops\nsetlocal enabledelayedexpansion\n\n:: Define the list of files with their URLs and local paths\n:: Removed quotes around the entire string and fixed paths\nset \"files[0]=https://github.com/dnhkng/GlaDOS/releases/download/0.1/glados.onnx;models/glados.onnx\"\nset \"files[1]=https://github.com/dnhkng/GlaDOS/releases/download/0.1/nemo-parakeet_tdt_ctc_110m.onnx;models/nemo-parakeet_tdt_ctc_110m.onnx\"\nset \"files[2]=https://github.com/dnhkng/GlaDOS/releases/download/0.1/phomenizer_en.onnx;models/phomenizer_en.onnx\"\nset \"files[3]=https://github.com/dnhkng/GlaDOS/releases/download/0.1/silero_vad.onnx;models/silero_vad.onnx\"\n\n:: Loop through the list\nfor /l %%i in (0,1,3) do (\n    for /f \"tokens=1,2 delims=;\" %%a in (\"!files[%%i]!\") do (\n        set \"url=%%a\"\n        set \"file=%%b\"\n        \n        echo Checking file: !file!\n        \n        if exist \"!file!\" (\n            echo File \"!file!\" already exists.\n        ) else (\n            echo Downloading !file!...\n            curl -L \"!url!\" --create-dirs -o \"!file!\"\n            \n            if exist \"!file!\" (\n                echo Download successful.\n            ) else (\n                echo Download failed for !file!\n                echo URL: !url!\n            )\n        )\n    )\n)\n\necho Installation Complete!\npause"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.080078125,
          "content": "onnxruntime\nsounddevice\nlevenshtein\nloguru\njinja2\nrequests\ntextual\npyyaml\nlibrosa\n"
        },
        {
          "name": "requirements_cuda.txt",
          "type": "blob",
          "size": 0.0908203125,
          "content": "onnxruntime-gpu\n# numpy\nsounddevice\nlevenshtein\nloguru\njinja2\nrequests\ntextual\npyyaml\nlibrosa"
        },
        {
          "name": "start_mac.command",
          "type": "blob",
          "size": 0.1162109375,
          "content": "#!/bin/bash\n\n# First, change to the script's directory\ncd \"$(dirname \"$0\")\"\n\nsource .venv/bin/activate\npython glados.py"
        },
        {
          "name": "start_ubuntu.sh",
          "type": "blob",
          "size": 0.1162109375,
          "content": "#!/bin/bash\n\n# First, change to the script's directory\ncd \"$(dirname \"$0\")\"\n\nsource .venv/bin/activate\npython glados.py"
        },
        {
          "name": "start_windows.bat",
          "type": "blob",
          "size": 0.072265625,
          "content": "@echo off\nREM Start GLaDOS\n\ncall  .venv\\Scripts\\activate\npython glados.py\n"
        },
        {
          "name": "start_windows_UI.bat",
          "type": "blob",
          "size": 0.0751953125,
          "content": "@echo off\nREM Start GLaDOS\n\ncall .\\venv\\Scripts\\activate\npython glados-ui.py\n"
        }
      ]
    }
  ]
}