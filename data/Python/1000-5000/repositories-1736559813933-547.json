{
  "metadata": {
    "timestamp": 1736559813933,
    "page": 547,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "adbar/trafilatura",
      "stars": 3817,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.23046875,
          "content": "[run]\nsource = trafilatura\n\nomit =\n    tests/*\n    setup.py\n\n[report]\nexclude_lines =\n    pragma: no cover\n    if __name__ == .__main__.:\n    except .*ImportError.*:\n    except .*UnicodeDecodeError.*:\n    except .*urllib3.exceptions.*:\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.1318359375,
          "content": "# override github/linguist settings\ntests/cache/*\tlinguist-vendored\ntests/eval/*\tlinguist-vendored\ntests/resources/*\tlinguist-vendored\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3056640625,
          "content": "# Compiled python modules.\n*.pyc\n\n# logs\n*.log\n\n# packaging\ndist/\nbuild/\n*.egg-info/\n.idea/\n\n# tests\n.cache/\n.eggs/\n.pytest_cache/\n.tox/\n.coverage\n\n# evaluation\nresults/\nvenv/\n\n# docs\ndocs/_autosummary/\ndocs/_build/\ndocs/_static/\ndocs/_template/\n\n# profiling\n*.tmp\n*.lprof\n\n# pipenv\nPipfile*\n\n# older stuff\nold/\n\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 1.005859375,
          "content": "# Read the Docs configuration file for Sphinx projects\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the OS, Python version and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n    # You can also specify other tool versions:\n    # nodejs: \"20\"\n    # rust: \"1.70\"\n    # golang: \"1.20\"\n\n# Build documentation in the \"docs/\" directory with Sphinx\nsphinx:\n  configuration: docs/conf.py\n  # You can configure Sphinx to use a different builder, for instance use the dirhtml builder for simpler URLs\n  # builder: \"dirhtml\"\n  # Fail on all warnings to avoid broken references\n  # fail_on_warning: true\n\n# Optionally build your docs in additional formats such as PDF and ePub\n# formats:\n#    - pdf\n#    - epub\n\n# Optional but recommended, declare the Python requirements required\n# to build your documentation\n# See https://docs.readthedocs.io/en/stable/guides/reproducible-builds.html\npython:\n  install:\n    - requirements: docs/requirements.txt\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.77734375,
          "content": "authors:\n  - family-names: Barbaresi\n    given-names: Adrien\n    orcid: https://orcid.org/0000-0002-8079-8694\ncff-version: 1.2.0\nidentifiers:\n  - description: \"This is the collection of archived snapshots of all versions of Trafilatura\"\n    type: doi\n    value: 10.5281/zenodo.3460969\nmessage: \"If you use this software, please cite both the article from preferred-citation and the software itself.\"\npreferred-citation:\n  authors:\n    - family-names: Barbaresi\n      given-names: Adrien\n  title: \"Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction\"\n  type: article\n  year: 2021\nrepository: https://github.com/adbar/trafilatura\nrepository-code: https://github.com/adbar/trafilatura\ntitle: Trafilatura\ntype: software\nurl: https://trafilatura.readthedocs.io/"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.98828125,
          "content": "## How to contribute\n\n\nIf you value this software or depend on it for your product,\nconsider sponsoring it and contributing to its codebase.\nYour support will help ensure the sustainability and growth of the project.\n\nThere are many ways to contribute:\n\n  * Sponsor the project: Show your appreciation [on GitHub](https://github.com/sponsors/adbar) or [ko-fi.com](https://ko-fi.com/adbarbaresi).\n  * Find bugs and submit bug reports: Help making Trafilatura an even more robust tool.\n  * Write code: Fix bugs or add new features by writing [pull requests](https://docs.github.com/en/pull-requests) with a list of what you have done.\n  * Improve the documentation: Write tutorials and guides, correct mistakes, or translate existing content.\n  * Submit feature requests: Share your feedback and suggestions.\n\n\nHere are some important resources:\n\n  * [List of currently open issues](https://github.com/adbar/trafilatura/issues) (no pretention to exhaustivity!)\n  * [How to contribute to open source](https://opensource.guide/how-to-contribute/)\n\nA special thanks to all the [contributors](https://github.com/adbar/trafilatura/graphs/contributors) who have played a part in Trafilatura.\n\n\n## Testing and evaluating the code\n\nHere is how you can run the tests and code quality checks. Pull requests will only be accepted if the changes are tested and if they there are no errors. \n\n- Install the necessary packages with `pip install trafilatura[dev]`\n- Run `pytest` from trafilatura's directory, or select a particular test suite, for example `realworld_tests.py`, and run `pytest realworld_tests.py` or simply `python3 realworld_tests.py`\n- Run `mypy` on the directory: `mypy trafilatura/`\n\nIf you work on text extraction it is useful to check if performance is equal or better on the benchmark.\n\nSee the [tests Readme](tests/README.rst) for more information.\n\n\nFor further questions you can use [GitHub issues](https://github.com/adbar/trafilatura/issues) and discussion pages, or [E-Mail](https://adrien.barbaresi.eu/).\n\nThanks,\n\nAdrien\n"
        },
        {
          "name": "HISTORY.md",
          "type": "blob",
          "size": 18.904296875,
          "content": "## History / Changelog\n\n\n## 2.0.0\n\nBreaking changes:\n- Python 3.6 and 3.7 deprecated (#709)\n- `bare_extraction()`:\n   - now returns an instance of the `Document` class by default\n   - `as_dict` deprecation warning → use `.as_dict()` method on return value (#730)\n- `bare_extraction()` and `extract()`: `no_fallback` deprecation warning → use `fast` instead (#730)\n- downloads: remove `decode` argument in `fetch_url()` → use `fetch_response` instead (#724)\n- deprecated graphical user interface now removed (#713)\n- extraction: move `max_tree_size` parameter to `settings.cfg` (#742)\n- use type hinting (#721, #723, #748)\n- see [Python](https://trafilatura.readthedocs.io/en/latest/usage-python.html#deprecations) and [CLI](https://trafilatura.readthedocs.io/en/latest/usage-cli.html#deprecations) deprecations in the docs\n\nFixes:\n- set `options.source` before raising error on empty doc tree by @dmoklaf (#707)\n- robust encoding in `options.source` (#717)\n- more robust mapping for conversion to HTML (#721)\n- CLI downloads: use all information in settings file (#734)\n- downloads: cleaner urllib3 code (#736)\n- refine table markdown output by @unsleepy22 (#752)\n- extraction fix: images in text nodes by @unsleepy22 (#757)\n\nMetadata:\n- more robust URL extraction (#710)\n\nCommand-line interface:\n- CLI: print URLs early for feeds and sitemaps with `--list` with @gremid (#744)\n- CLI: add 126 exit code for high error ratio (#747)\n\nMaintenance:\n- remove already deprecated functions and args (#716)\n- add type hints (#723, #728)\n- setup: use `pyproject.toml` file (#715)\n- simplify code (#708, #709, #727)\n- better debug messages in `main_extractor` (#714)\n- evaluation: review data, update packages, add magic_html (#731)\n- setup: explicit exports through `__all__` (#740)\n- tests: extend coverage (#753)\n\nDocumentation:\n- fix link in `docs/index.html` by @nzw0301 (#711)\n- remove docs from published packages (#743)\n- update docs (#745)\n\n\n## 1.12.2\n\n- downloads: add support for SOCKS proxies with @gremid (#682)\n- extraction fix: ValueError in table spans (#685)\n- spider: `prune_xpath` parameter added by @felipehertzer (#684)\n- spider: relax strict parameter for link extraction (#687)\n- sitemaps: `max_sitemaps` parameter added by @felipehertzer (#690)\n- maintenance: make compression libraries optional (#691)\n- metadata: review and lint code (#694)\n\n\n### 1.12.1\n\nNavigation:\n- spider: restrict search to sections containing URL path (#673)\n- crawler: add parameter class and types, **breaking change** for undocumented functions (#675)\n- maintenance: simplify link discovery and extend tests (#674)\n- CLI: review code, add types and tests (#677)\n\nBugfixes:\n- fix `AttributeError` in element deletion (#668)\n- fix `MemoryError` in table header columns (#665)\n\nDocs:\n- docs: fix variable name for extract_metadata in quickstart by @jpigla in #678\n\n\n### 1.12.0\n\nBreaking change:\n- enforce fixed list of output formats, deprecate `-out` on the CLI (#647)\n\nFaster, more accurate extraction:\n- review link and structure checks (#653)\n- improve justext fallback (#652)\n- baseline: prevent LXML error in JSON-LD (#643), do not use as backup extraction (#646)\n- review XPaths for undesirable content (#645)\n\nBugfixes and maintenance:\n- CLI fix: markdown format should trigger `include_formatting` (#649)\n- images fix: use a length threshold on src attribute (#654)\n- XML-TEI: replace RelaxNG by DTD, remove pickle, and update (#655)\n- formatting & markdown fix: add newlines (#656)\n- table fix: prevent `MemoryError` & `ValueError` during conversion to text (#658)\n\nDocumentation:\n- update `crawls.rst`: `known` is an unexpected argument, by @tommytyc in #638\n\n\n### 1.11.0\n\nBreaking change:\n- metadata now skipped by default (#613), to trigger inclusion in all output formats:\n   - `with_metadata=True` (Python)\n   - `--with-metadata` (CLI)\n\nExtraction:\n- add HTML as output format (#614)\n- better and faster baseline extraction (#619)\n- better handling of HTML/XML elements (#628)\n- XPath rules added with @felipehertzer (#540)\n- fix: avoid faulty readability_lxml content (#635)\n\nEvaluation:\n- new scripts and data with @LydiaKoerber (#606, #615)\n- additional data with @swetepete (#197)\n\nMaintenance:\n- docs extended and updated, added page on deduplication (#618)\n- review code, add tests and types in part of the submodules (#620, #623, #624, #625)\n\n\n### 1.10.0\n\nBreaking changes:\n- raise errors on deprecated CLI and function arguments (#581)\n- regroup classes and functions linked to deduplication (#582)\n``trafilatura.hashing`` → ``trafilatura.deduplication``\n\nExtraction:\n- port of is_probably_readerable from readability.js by @zirkelc in #587\n- Markdown table fixes by @naktinis in #601\n- fix list spacing in TXT output (#598)\n- CLI fixes: file processing options, mtime, and tests (#605)\n- CLI fix: read standard input as binary (#607)\n\nDownloads:\n- fix deflate and add optional zstd to accepted encodings (#594)\n- spider fix: use internal download utilities for robots.txt (#590)\n\nMaintenance:\n- add author XPaths (#567)\n- update justext and lxml dependencies (#593)\n- simplify code: unique function for length tests (#591)\n\nDocs:\n- fix typos by @RainRat in #603\n\n\n### 1.9.0\n\nExtraction:\n- add markdown as explicit output (#550)\n- improve recall preset (#571)\n- speedup for readability-lxml (#547)\n- add global options object for extraction and use it in CLI (#552)\n- fix: better encoding detection (#548)\n- recall: fix for lists inside tables with @mikhainin (#534)\n- add symbol to preserve vertical spacing in Markdown (#499)\n- fix: table cell separators in non-XML output (#563)\n- slightly better accuracy and execution speed overall\n\nMetadata:\n- add file creation date (date extraction, JSON & XML-TEI) (#561)\n- fix: empty content in meta tag by @felipehertzer (#545)\n\nMaintenance:\n- restructure and simplify code (#543, #556)\n- CLI & downloads: revamp and use global options (#565)\n- eval: review code, add guidelines and small benchmark (#542)\n- fix: raise error if config file does not exist (#554)\n- deprecate `process_record()` (#549)\n- docs: convert readme to markdown and update info (#564, #578)\n\n\n### 1.8.1\n\nMaintenance:\n- Pin LXML to prevent broken dependency (#535)\n\nExtraction:\n- Improve extraction accuracy for major news outlets (#530)\n- Fix formatting by correcting order of element generation and space handling with @dlwh (#528)\n- Fix: prevent tail insertion before children in nested elements by @knit-bee (#536)\n\n\n### 1.8.0\n\nExtraction:\n- Better precision by @felipehertzer (#509, #520)\n- Code formatting in TXT/Markdown output added (#498)\n- Improved CSV output (#496)\n- LXML: compile XPath expressions (#504)\n- Overall speedup about +5%\n\nDownloads and Navigation:\n- More robust scans with `is_live_page()` (#501)\n- Better sitemap start and safeguards (#503, #506)\n- Fix for headers in response object (#513)\n\nMaintenance:\n- License changed to Apache 2.0\n- `Response` class: convenience functions added (#497)\n- `lxml.html.Cleaner` removed (#491)\n- CLI fixes: parallel cores and processing (#524)\n\n\n### 1.7.0\n\nExtraction:\n- improved `html2txt()` function\n\nDownloads:\n- add advanced `fetch_response()` function\n→ pending deprecation for `fetch_url(decode=False)`\n\nMaintenance:\n- support for LXML v5+ (#484 by @knit-bee, #485)\n- update [htmldate](https://github.com/adbar/htmldate/releases/tag/v1.7.0)\n\n\n### 1.6.4\n\nMaintenance:\n- MacOS: fix setup, update htmldate and add tests (#460)\n- drop invalid XML element attributes with @vbarbaresi in #462\n- remove cyclic imports (#458)\n\nNavigation:\n- introduce `MAX_REDIRECTS` config setting and fix urllib3 redirect handling by @vbarbaresi in #461\n- improve feed detection (#457)\n\nDocumentation:\n- enhancements to documentation and testing with @Maddesea in #456\n\n\n### 1.6.3\n\nExtraction:\n- preserve space in certain elements with @idoshamun (#429)\n- optional list of xPaths to prune by @HeLehm (#414)\n\nMetadata:\n- more precise date extraction (see [htmldate](https://github.com/adbar/htmldate/releases/tag/v1.6.0))\n- new `htmldate` extensive search parameter in config (#434)\n- changes in URLs: normalization, trackers removed (see [courlan](https://github.com/adbar/courlan/releases/tag/v0.9.5))\n\nNavigation:\n- reviewed code for feeds (#443)\n- new config option: external URLs for feeds/sitemaps (#441)\n\nDocumentation:\n- update, add page on text embeddings with @tonyyanga (#428, #435, #447)\n- fix quickstart by @sashkab (#419)\n\n\n### 1.6.2\n\nExtraction:\n- more lenient HTML parsing (#370)\n- improved code block support with @idoshamun (#372, #401)\n- conversion of relative links to absolute by @feltcat (#377)\n- remove use of signal from core functions (#384)\n\nMetadata:\n- JSON-LD fix for sitenames by @felipehertzer (#383)\n\nCommand-line interface:\n- more robust batch processing (#381)\n- added `--probe` option to CLI to check for extractable content (#378, #392)\n\nMaintenance:\n- simplified code (#408)\n- support for Python 3.12\n- pinned LXML version for MacOS (#393)\n- updated dependencies and parameters (notably `htmldate` and `courlan`)\n- code cleaning by @marksmayo (#406)\n\n\n### 1.6.1\n\nExtraction:\n- minor fixes: tables in figures (#301), headings (#354) and lists (#318)\n\nMetadata:\n- simplify and fully test JSON parsing code, with @felipehertzer (#352, #368)\n- authors, JSON and unicode fixes by @felipehertzer in #365\n- fix for authors without `additionalName` by @awwitecki in #363\n\nNavigation:\n- reviewed link processing in feeds and sitemaps (#340, #350)\n- more robust spider (#359)\n- updated underlying courlan package (#360)\n\n\n### 1.6.0\n\nExtraction:\n- new content hashes and default file names (#314)\n- fix deprecation warning with @sdondley in #321\n- fix for metadata image by @andremacola in #328\n- fix potential unicode issue in third-party extraction with @Korben00 in #331 \n- review logging levels (#347)\n\nCommand-line interface:\n- more efficient sitemap processing (#326)\n- more efficient downloads (#338)\n- fix for single URL processing (#324) and URL blacklisting (#339)\n\nNavigation:\n- additional safety check on domain similarity for feeds and sitemaps\n- new function ``is_live test()`` using HTTP HEAD request (#327)\n- code parts supported by new courlan version\n\nMaintenance:\n- allow ``urllib3`` version 2.0+\n- minor code simplification and fixes\n\n\n### 1.5.0\n\nExtraction:\n- fixes for metadata extraction with @felipehertzer (#295, #296),  @andremacola (#282, #310), and @edkrueger (#303)\n- pagetype and image urls added to metadata by @andremacola (#282, #310)\n- add as_dict method to Document class with @edkrueger in #306\n- XML output fix with @knit-bee in #315\n- various smaller fixes: lists (#309), XPaths, metadata hardening\n\nNavigation:\n- transfer URL management to courlan.UrlStore (#232, #312)\n- fixes for spider module\n\nMaintenance:\n- simplify code and extend tests\n- underlying packages htmldate and courlan, update setup and docs\n\n\n### 1.4.1\n\nExtraction:\n- XML output improvements with @knit-bee (#273, #274)\n- extraction bugs fixed (#263, #266), more robust HTML doctype parsing\n- adjust thresholds for link density in paragraphs\n\nMetadata:\n- improved title and sitename detection (#284)\n- faster author, categories, domain name, and tags extraction\n- fixes to author emoji regexes by @felipehertzer (#269)\n\nCommand-line interface:\n- review argument consistency and add deprecation warnings (#261)\n\nSetup:\n- make download timeout configurable (#263)\n- updated dependencies, use of faust-cchardet for Python 3.11\n\n\n### 1.4.0\n\nImpact on extraction and output format:\n- better extraction (#233, #243 & #250 with @knit-bee, #246 with @mrienstra, #258)\n- XML: preserve list type as attribute (#229)\n- XML TEI: better conformity with @knit-bee (#238, #242, #253, #254)\n- faster text cleaning and shorter code (#237 with @deedy5, #245)\n- metadata: add language when detector is activated (#224)\n- metadata: extend fallbacks and test coverage for json_metadata functions by @felipehertzer (#235)\n- TXT: change markdown formatting of headers by @LaundroMat (#257)\n\nSmaller changes in convenience functions:\n- add function to clear caches (#219)\n- CLI: change exit code if download fails (#223)\n- settings: use \"\\n\" for multiple user agents by @k-sareen (#241)\n\nUpdates:\n- docs updated (and #244 by @dsgibbons)\n- package dependencies updated\n\n\n### 1.3.0\n- fast and robust `html2txt()` function added (#221)\n- more robust parsing (#228)\n- fixed bugs in metadata extraction, with @felipehertzer in #213 & #226 \n- extraction about 10-20% faster, slightly better recall\n- partial fixes for memory leaks (#216)\n- docs extended and updated (#217, #225)\n- prepared deprecation of old `process_record()` function\n- more stable processing with updated dependencies\n\n\n### 1.2.2\n- more efficient rules for extraction\n- metadata: further attributes used (with @felipehertzer)\n- better baseline extraction\n- issues fixed: #202, #204, #205\n- evaluation updated\n\n\n### 1.2.1\n- ``--precision`` and ``--recall`` arguments added to the CLI\n- better text cleaning: paywalls and comments\n- improvements for Chinese websites (with @glacierck & @immortal-autumn): #186, #187, #188\n- further bugs fixed: #189, #192 (with @felipehertzer), #200\n- efficiency: faster module loading and improved RAM footprint\n\n\n### 1.2.0\n- efficiency: replaced module readability-lxml by trimmed fork\n- bug fixed: (#179, #180, #183, #184)\n- improved baseline extraction\n- cleaner metadata (with @felipehertzer)\n\n\n### 1.1.0\n- encodings: better detection, output NFC-normalized Unicode\n- maintenance and performance: more efficient code\n- bugs fixed (#119, #136, #147, #160, #161, #162, #164, #167 and others)\n- prepare compatibility with upcoming Python 3.11\n- changed default settings\n- extended documentation\n\n\n### 1.0.0\n- compress HTML backup files & seamlessly open .gz files\n- support JSON web feeds\n- graphical user interface integrated into main package\n- faster downloads: reviewed backoff, compressed data\n- optional modules: downloads with `pycurl`, language identification with `py3langid`\n- bugs fixed (#111, #125, #132, #136, #140)\n- minor optimizations and fixes by @vbarbaresi in [#124](https://github.com/adbar/trafilatura/pull/124) & [#130](https://github.com/adbar/trafilatura/pull/130)\n- fixed array with single or multiples entries on json extractor by @felipehertzer in [#143](https://github.com/adbar/trafilatura/pull/143)\n- code base refactored with @sourcery-ai [#121](https://github.com/adbar/trafilatura/pull/121), improved and optimized for Python 3.6+\n- drop support for Python 3.5\n\n\n### 0.9.3\n- better, faster encoding detection: replaced `chardet` with `charset_normalizer`\n- faster execution: updated `justext` to 3.0\n- better extraction of sub-elements in tables (#78, #90)\n- more robust web feed parsing\n- further defined precision- and recall-oriented settings\n- license extraction in footers (#118)\n\n\n### 0.9.2\n- first precision- and recall-oriented presets defined\n- improvements in authorship extraction (thanks @felipehertzer)\n- requesting TXT output with formatting now results in Markdown format\n- bugs fixed: notably extraction robustness and consistency (#109, #111, #113)\n- setting for cookies in request headers (thanks @muellermartin)\n- better date extraction thanks to htmldate update\n\n\n### 0.9.1\n- improved author extraction (thanks @felipehertzer!)\n- bugs fixed: HTML element handling, HTML meta attributes, spider, CLI, ...\n- docs updated and extended\n- CLI: option names normalized (heed deprecation warnings), new option `explore`\n\n\n### 0.9.0\n- focused crawling functions including politeness rules\n- more efficient multi-threaded downloads + use as Python functions\n- documentation extended\n- bugs fixed: extraction and URL handling\n- removed support for Python 3.4\n\n\n### 0.8.2\n- better handling of formatting, links and images, title type as attribute in XML formats\n- more robust sitemaps and feeds processing\n- more accurate extraction\n- further consolidation: code simplified and bugs fixed\n\n\n### 0.8.1\n- extraction trade-off: slightly better recall\n- code robustness: requests, configuration and navigation\n- bugfixes: image data extraction\n\n\n### 0.8.0\n- improved link discovery and handling\n- fixes in metadata extraction, feeds and sitemaps processing\n- breaking change: the `extract` function now reads target format from `output_format` argument only\n- new extraction option: preserve links, CLI options re-ordered\n- more opportunistic backup extraction\n\n\n### 0.7.0\n- customizable configuration file to parametrize extraction and downloads\n- better handling of feeds and sitemaps\n- additional CLI options: crytographic hash for file name, use Internet Archive as backup\n- more precise extraction\n- faster downloads: `requests` replaced with bare `urllib3` and custom decoding\n- consolidation: bug fixes and improvements, many thanks to the issues reporters!\n\n\n### 0.6.1\n- added `bare_extraction` function returning Python variables\n- improved link discovery in feeds and sitemaps\n- option to preserve image info\n- fixes (many thanks to bug reporters!)\n\n\n### 0.6.0\n- link discovery in sitemaps\n- compatibility with Python 3.9\n- extraction coverage improved\n- deduplication now optional\n- bug fixes\n\n\n### 0.5.2\n- optional language detector changed: `langid` → `pycld3`\n- helper function `bare_extraction()`\n- optional deduplication off by default\n- better URL handling (`courlan`), more complete metadata\n- code consolidation (cleaner and shorter)\n\n\n### 0.5.1\n- extended and more convenient command-line options\n- output in JSON format\n- bug fixes\n\n\n### 0.5.0\n- faster and more robust text and metadata extraction\n- more efficient batch processing (parallel processing, URL queues)\n- extraction and processing of ATOM/RSS feeds\n- complete command-line tool with corresponding options\n\n\n### 0.4.1\n- better metadata extraction and integration (XML & XML-TEI)\n- more efficient processing\n- output directory as CLI-option\n\n\n### 0.4\n- improved \"fast\" mode (accuracy and speed)\n- better fallbacks with readability-lxml and justext\n- metadata extraction added\n- more robust processing (tests, encoding handling)\n\n\n### 0.3.1\n- support for Python 3.4 reactivated\n- bugs in XML output and discarding sections solved\n- new tests and documentation\n\n\n### 0.3.0\n- code base re-structured for clarity and readability\n- streamlined HTML processing and conversion\n- internal less-recently-used cache (LRU) for deduplication\n- export as CSV\n- better test coverage, extraction recall and precision\n- further documentation (trafilatura.readthedocs.org)\n- optional processing of text formatting\n- more complete settings file\n\n\n### 0.2.1\n- added metadata to the XML output\n- production of valid XML TEI for simple documents\n\n\n### 0.2.0\n- better handling of nested elements, quotes and tables\n- validation of XML TEI documents\n- bulk download and processing\n\n\n### 0.1.1\n- handling of line breaks\n- element trimming simplified\n\n\n### 0.1.0\n- first release used in production and meant to be archived for reproducibility and citability\n- better extraction precision\n\n\n### 0.0.5: last version compatible with Python 3.4\n- optional dependencies\n- bugs in parsing removed\n\n\n### 0.0.4\n- code profiling and speed-up\n\n\n### 0.0.3\n- tables included in extraction\n- bypass justext in arguments\n- better handling of non-p elements\n\n\n### 0.0.2\n- better handling of text nodes\n- improvements in extraction recall\n\n\n### 0.0.1\n- first release, minimum viable package\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 9.9345703125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.3447265625,
          "content": "include CITATION.cff CONTRIBUTING.md HISTORY.md README.rst LICENSE\ngraft trafilatura/data/\ninclude trafilatura/settings.cfg\ninclude trafilatura/py.typed\n\ninclude tests/__init__.py\ninclude tests/*test*.py\ngraft tests/resources/\nexclude tests/realworld_tests.py\nrecursive-exclude tests/cache/\n\nrecursive-exclude * __pycache__\nrecursive-exclude * *.py[co]\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.4091796875,
          "content": "# Trafilatura: Discover and Extract Text Data on the Web\n\n<br/>\n\n<img alt=\"Trafilatura Logo\" src=\"https://raw.githubusercontent.com/adbar/trafilatura/master/docs/trafilatura-logo.png\" align=\"center\" width=\"60%\"/>\n\n<br/>\n\n[![Python package](https://img.shields.io/pypi/v/trafilatura.svg)](https://pypi.python.org/pypi/trafilatura)\n[![Python versions](https://img.shields.io/pypi/pyversions/trafilatura.svg)](https://pypi.python.org/pypi/trafilatura)\n[![Documentation Status](https://readthedocs.org/projects/trafilatura/badge/?version=latest)](http://trafilatura.readthedocs.org/en/latest/?badge=latest)\n[![Code Coverage](https://img.shields.io/codecov/c/github/adbar/trafilatura.svg)](https://codecov.io/gh/adbar/trafilatura)\n[![Downloads](https://static.pepy.tech/badge/trafilatura/month)](https://pepy.tech/project/trafilatura)\n[![Reference DOI: 10.18653/v1/2021.acl-demo.15](https://img.shields.io/badge/DOI-10.18653%2Fv1%2F2021.acl--demo.15-blue)](https://aclanthology.org/2021.acl-demo.15/)\n\n<br/>\n\n<img alt=\"Demo as GIF image\" src=\"https://raw.githubusercontent.com/adbar/trafilatura/master/docs/trafilatura-demo.gif\" align=\"center\" width=\"80%\"/>\n\n<br/>\n\n\n## Introduction\n\nTrafilatura is a cutting-edge **Python package and command-line tool**\ndesigned to **gather text on the Web and simplify the process of turning\nraw HTML into structured, meaningful data**. It includes all necessary\ndiscovery and text processing components to perform **web crawling,\ndownloads, scraping, and extraction** of main texts, metadata and\ncomments. It aims at staying **handy and modular**: no database is\nrequired, the output can be converted to commonly used formats.\n\nGoing from HTML bulk to essential parts can alleviate many problems\nrelated to text quality, by **focusing on the actual content**,\n**avoiding the noise** caused by recurring elements like headers and footers\nand by **making sense of the data and metadata** with selected information.\nThe extractor strikes a balance between limiting noise (precision) and\nincluding all valid parts (recall). It is **robust and reasonably fast**.\n\nTrafilatura is [widely used](https://trafilatura.readthedocs.io/en/latest/used-by.html)\nand integrated into [thousands of projects](https://github.com/adbar/trafilatura/network/dependents>)\nby companies like HuggingFace, IBM, and Microsoft Research as well as institutions like\nthe Allen Institute, Stanford, the Tokyo Institute of Technology, and\nthe University of Munich.\n\n\n### Features\n\n- Advanced web crawling and text discovery:\n   - Support for sitemaps (TXT, XML) and feeds (ATOM, JSON, RSS)\n   - Smart crawling and URL management (filtering and deduplication)\n\n- Parallel processing of online and offline input:\n   - Live URLs, efficient and polite processing of download queues\n   - Previously downloaded HTML files and parsed HTML trees\n\n- Robust and configurable extraction of key elements:\n   - Main text (common patterns and generic algorithms like jusText and readability)\n   - Metadata (title, author, date, site name, categories and tags)\n   - Formatting and structure: paragraphs, titles, lists, quotes, code, line breaks, in-line text formatting\n   - Optional elements: comments, links, images, tables\n\n- Multiple output formats:\n   - TXT and Markdown\n   - CSV\n   - JSON\n   - HTML, XML and [XML-TEI](https://tei-c.org/)\n\n- Optional add-ons:\n   - Language detection on extracted content\n   - Speed optimizations\n\n- Actively maintained with support from the open-source community:\n   - Regular updates, feature additions, and optimizations\n   - Comprehensive documentation\n\n\n### Evaluation and alternatives\n\nTrafilatura consistently outperforms other open-source libraries in text\nextraction benchmarks, showcasing its efficiency and accuracy in\nextracting web content. The extractor tries to strike a balance between\nlimiting noise and including all valid parts.\n\nFor more information see the [benchmark section](https://trafilatura.readthedocs.io/en/latest/evaluation.html)\nand the [evaluation readme](https://github.com/adbar/trafilatura/blob/master/tests/README.rst)\nto run the evaluation with the latest data and packages.\n\n\n#### Other evaluations:\n\n- Most efficient open-source library in *ScrapingHub*'s [article extraction benchmark](https://github.com/scrapinghub/article-extraction-benchmark)\n- Best overall tool according to [Bien choisir son outil d'extraction de contenu à partir du Web](https://hal.archives-ouvertes.fr/hal-02768510v3/document)\n  (Lejeune & Barbaresi 2020)\n- Best single tool by ROUGE-LSum Mean F1 Page Scores in [An Empirical Comparison of Web Content Extraction Algorithms](https://webis.de/downloads/publications/papers/bevendorff_2023b.pdf)\n  (Bevendorff et al. 2023)\n\n\n## Usage and documentation\n\n[Getting started with Trafilatura](https://trafilatura.readthedocs.io/en/latest/quickstart.html)\nis straightforward. For more information and detailed guides, visit\n[Trafilatura's documentation](https://trafilatura.readthedocs.io/):\n\n- [Installation](https://trafilatura.readthedocs.io/en/latest/installation.html)\n- Usage:\n  [On the command-line](https://trafilatura.readthedocs.io/en/latest/usage-cli.html),\n  [With Python](https://trafilatura.readthedocs.io/en/latest/usage-python.html),\n  [With R](https://trafilatura.readthedocs.io/en/latest/usage-r.html)\n- [Core Python functions](https://trafilatura.readthedocs.io/en/latest/corefunctions.html)\n- Interactive Python Notebook: [Trafilatura Overview](docs/Trafilatura_Overview.ipynb)\n- [Tutorials and use cases](https://trafilatura.readthedocs.io/en/latest/tutorials.html)\n\nYoutube playlist with video tutorials in several languages:\n\n- [Web scraping tutorials and how-tos](https://www.youtube.com/watch?v=8GkiOM17t0Q&list=PL-pKWbySIRGMgxXQOtGIz1-nbfYLvqrci)\n\n\n## License\n\nThis package is distributed under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0.html).\n\nVersions prior to v1.8.0 are under GPLv3+ license.\n\n\n### Contributing\n\nContributions of all kinds are welcome. Visit the [Contributing\npage](https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md)\nfor more information. Bug reports can be filed on the [dedicated issue\npage](https://github.com/adbar/trafilatura/issues).\n\nMany thanks to the\n[contributors](https://github.com/adbar/trafilatura/graphs/contributors)\nwho extended the docs or submitted bug reports, features and bugfixes!\n\n\n## Context\n\nThis work started as a PhD project at the crossroads of linguistics and\nNLP, this expertise has been instrumental in shaping Trafilatura over\nthe years. Initially launched to create text databases for research purposes\nat the Berlin-Brandenburg Academy of Sciences (DWDS and ZDL units),\nthis package continues to be maintained but its future depends on community support.\n\n**If you value this software or depend on it for your product, consider\nsponsoring it and contributing to its codebase**. Your support\n[on GitHub](https://github.com/sponsors/adbar) or [ko-fi.com](https://ko-fi.com/adbarbaresi)\nwill help maintain and enhance this popular package.\n\n*Trafilatura* is an Italian word for [wire\ndrawing](https://en.wikipedia.org/wiki/Wire_drawing) symbolizing the\nrefinement and conversion process. It is also the way shapes of pasta\nare formed.\n\n### Author\n\nReach out via ia the software repository or the [contact\npage](https://adrien.barbaresi.eu/) for inquiries, collaborations, or\nfeedback. See also social networks for the latest updates.\n\n-   Barbaresi, A. [Trafilatura: A Web Scraping Library and Command-Line\n    Tool for Text Discovery and\n    Extraction](https://aclanthology.org/2021.acl-demo.15/), Proceedings\n    of ACL/IJCNLP 2021: System Demonstrations, 2021, p. 122-131.\n-   Barbaresi, A. \"[Generic Web Content Extraction with Open-Source\n    Software](https://hal.archives-ouvertes.fr/hal-02447264/document)\",\n    Proceedings of KONVENS 2019, Kaleidoscope Abstracts, 2019.\n-   Barbaresi, A. \"[Efficient construction of metadata-enhanced web\n    corpora](https://hal.archives-ouvertes.fr/hal-01371704v2/document)\",\n    Proceedings of the [10th Web as Corpus Workshop\n    (WAC-X)](https://www.sigwac.org.uk/wiki/WAC-X), 2016.\n\n\n### Citing Trafilatura\n\nTrafilatura is widely used in the academic domain, chiefly for data\nacquisition. Here is how to cite it:\n\n[![Reference DOI: 10.18653/v1/2021.acl-demo.15](https://img.shields.io/badge/DOI-10.18653%2Fv1%2F2021.acl--demo.15-blue)](https://aclanthology.org/2021.acl-demo.15/)\n[![Zenodo archive DOI: 10.5281/zenodo.3460969](https://zenodo.org/badge/DOI/10.5281/zenodo.3460969.svg)](https://doi.org/10.5281/zenodo.3460969)\n\n``` shell\n@inproceedings{barbaresi-2021-trafilatura,\n  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},\n  author = \"Barbaresi, Adrien\",\n  booktitle = \"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations\",\n  pages = \"122--131\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.acl-demo.15\",\n  year = 2021,\n}\n```\n\n\n### Software ecosystem\n\nJointly developed plugins and additional packages also contribute to the\nfield of web data extraction and analysis:\n\n<img alt=\"Software ecosystem\" src=\"https://raw.githubusercontent.com/adbar/htmldate/master/docs/software-ecosystem.png\" align=\"center\" width=\"65%\"/>\n\nCorresponding posts can be found on [Bits of\nLanguage](https://adrien.barbaresi.eu/blog/tag/trafilatura.html).\n\nImpressive, you have reached the end of the page: Thank you for your\ninterest!\n"
        },
        {
          "name": "compose.yml",
          "type": "blob",
          "size": 0.310546875,
          "content": "services:\n  socks_proxy:\n    image: serjs/go-socks5-proxy\n    ports:\n      - 1080:1080\n  socks_proxy_auth:\n    image: serjs/go-socks5-proxy\n    ports:\n      - 1081:1080\n    environment:\n      PROXY_USER: user\n      PROXY_PASSWORD: pass\n#  tor_proxy:\n#    image: dperson/torproxy\n#    ports:\n#      - 9050:9050\n      \n\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 3.5,
          "content": "# https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/\n[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"trafilatura\"\ndescription = \"Python & Command-line tool to gather text and metadata on the Web: Crawling, scraping, extraction, output as CSV, JSON, HTML, MD, TXT, XML.\"\nreadme = \"README.md\"\nlicense = { text = \"Apache 2.0\" }\ndynamic = [\"version\"]\nrequires-python = \">=3.8\"\nauthors = [\n  {name = \"Adrien Barbaresi\", email = \"barbaresi@bbaw.de\"}\n]\nkeywords=[\n    \"corpus\",\n    \"html2text\",\n    \"news-crawler\",\n    \"natural-language-processing\",\n    \"scraper\",\n    \"tei-xml\",\n    \"text-extraction\",\n    \"webscraping\",\n    \"web-scraping\",\n]\nclassifiers = [\n    # https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    \"Development Status :: 5 - Production/Stable\",\n    #'Development Status :: 6 - Mature',\n    \"Environment :: Console\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Education\",\n    \"Intended Audience :: Information Technology\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: Apache Software License\",\n    \"Operating System :: MacOS\",\n    \"Operating System :: Microsoft\",\n    \"Operating System :: POSIX\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Topic :: Internet :: WWW/HTTP\",\n    \"Topic :: Scientific/Engineering :: Information Analysis\",\n    \"Topic :: Security\",\n    \"Topic :: Text Editors :: Text Processing\",\n    \"Topic :: Text Processing :: Linguistic\",\n    \"Topic :: Text Processing :: Markup :: HTML\",\n    \"Topic :: Text Processing :: Markup :: Markdown\",\n    \"Topic :: Text Processing :: Markup :: XML\",\n    \"Topic :: Utilities\",\n]\ndependencies = [\n    \"certifi\",\n    \"charset_normalizer >= 3.4.0\",\n    \"courlan >= 1.3.2\",\n    \"htmldate >= 1.9.2\",\n    \"justext >= 3.0.1\",\n    # see tests on Github Actions\n    \"lxml == 4.9.2 ; platform_system == 'Darwin' and python_version <= '3.8'\",\n    \"lxml >= 5.3.0 ; platform_system != 'Darwin' or python_version > '3.8'\",\n    \"urllib3 >= 1.26, < 3\",\n]\n\n# https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html\n[tool.setuptools]\npackages = [\"trafilatura\"]\n\n# https://packaging.python.org/en/latest/guides/single-sourcing-package-version/\n[tool.setuptools.dynamic]\nversion = {attr = \"trafilatura.__version__\"}\n\n# https://setuptools.pypa.io/en/stable/userguide/datafiles.html\n[tool.setuptools.package-data]\ntrafilatura = [\n    \"data/tei_corpus.dtd\",\n    \"settings.cfg\",\n]\n\n[project.scripts]\ntrafilatura = \"trafilatura.cli:main\"\n\n[project.urls]\n\"Homepage\" = \"https://trafilatura.readthedocs.io\"\n\"Source\" = \"https://github.com/adbar/trafilatura\"\n\"Blog\" = \"https://adrien.barbaresi.eu/blog/tag/trafilatura.html\"\n\"Tracker\" = \"https://github.com/adbar/trafilatura/issues\"\n\n# Development extras\n[project.optional-dependencies]\ndev = [\n    \"flake8\",\n    \"mypy\",\n    \"pytest\",\n    \"pytest-cov\",\n    \"types-lxml\",\n    \"types-urllib3\",\n]\nall = [\n    \"brotli\",\n    \"cchardet >= 2.1.7; python_version < '3.11'\",  # build issue\n    \"faust-cchardet >= 2.1.19; python_version >= '3.11'\",\n    \"htmldate[speed] >= 1.9.2\",\n    \"py3langid >= 0.3.0\",\n    \"pycurl >= 7.45.3\",\n    \"urllib3[socks]\",\n    \"zstandard >= 0.23.0\",\n]\n\n[tool.pytest.ini_options]\ntestpaths = \"tests/*test*.py\"\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "trafilatura",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}