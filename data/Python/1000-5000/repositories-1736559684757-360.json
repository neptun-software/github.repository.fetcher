{
  "metadata": {
    "timestamp": 1736559684757,
    "page": 360,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "commaai/research",
      "stars": 4126,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.224609375,
          "content": ".ipynb_checkpoints\n.*.swp\n*.pyc\noutputs/*.json\noutputs/*.h5\noutputs/*.keras\noutputs/*.log\n*.c\n*.so\nsamples_*/\nresults_*/\nvideo_*/\ndataset/*\noutputs/*\ncomma-dataset.zip\npaper/*.log\npaper/*.out\npaper/*.aux\npaper/commalds.tex.backup\n"
        },
        {
          "name": "DriveSim.md",
          "type": "blob",
          "size": 1.5908203125,
          "content": "# Drive Simulator\nFollow the instructions to train a generative model as described by\n[Learning a Driving Simulator](http://arxiv.org/abs/1608.01230).\n\n1) Download dataset\n```\n./get_data.sh\n```\n\n2) Start data server in the first terminal session\n```bash\n./server.py --batch 64\n```\nFor maximum performance we recommend using tmux. screen is not good.  \n3) Train auto encoder in the second terminal\n```bash\n./train_generative_model.py autoencoder --batch 64\n```\nThis will create two folders called `outputs/results_autoencoder` with model checkpoints\nand `outputs/samples_autoencoder` with samples from the generative model. In the sampled\nimages odd columns are generated and even column images are target images.  \n\nOnce the autoencoder model is trained you can stop the previous server and\nstart training the transition model\n\n4). Run server for transition model\n```bash\n./server.py --time 60 --batch 64\n```\n\n5) Train transition model\n```bash\n./train_generative_model.py transition --batch 64 --name transition\n```\nThis will create two folders called `outputs/results_transition` with model checkpoints\nand `outputs/samples_transition` with samples from the generative model\n\nTraining logs will be saved to `/tmp/logs/<model_name>`. You can visualize\nlogs using Tensorboard by typing\n`tensorboard --logdir /tmp/logs/autoencoder` or  \n`tensorboard --logdir /tmp/logs/transition`\n\n6) Make a gif of the transition model\n```bash\n./view_generative_model.py transition --name transition\n```\n<img src=\"./images/drive_simulator.gif\">\n\nHere your job is to make the video look so great that it could be used train a steering angle model.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4609375,
          "content": "Copyright (c) 2016, comma.ai\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and/or other materials provided with the distribution.\n    * Neither the name of the <organization> nor the\n      names of its contributors may be used to endorse or promote products\n      derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n"
        },
        {
          "name": "Logs.md",
          "type": "blob",
          "size": 5.2060546875,
          "content": "# logs\n\nKeys starting with \"app_\" refer to the Applanix POS LV 220E. The X, Y, Z axes are aligned with Forward, Right, Down with respect to the car.\n\nKeys starting with \"fiber_\" refer to the KVH 1775 fiber-optic gyro. The X, Y, Z axes are aligned with 45 degrees Right, 45 degrees Left, Up with respect to the car.\n\nKeys starting with \"velodyne_\" refer to the HDL-32 Velodyne unit. \n\n| HDF5 key          | Description                                                              |\n|-------------------|--------------------------------------------------------------------------|\n| UN_D_cam1_ptr     |                                                                          |\n| UN_D_cam2_ptr     |                                                                          |\n| UN_D_lidar_ptr    |                                                                          |\n| UN_D_radar_msg    |                                                                          |\n| UN_D_rawgps       |                                                                          |\n| UN_T_cam1_ptr     |                                                                          |\n| UN_T_cam2_ptr     |                                                                          |\n| UN_T_lidar_ptr    |                                                                          |\n| UN_T_radar_msg    |                                                                          |\n| UN_T_rawgps       |                                                                          |\n| app_accel         | m/s^2, <Forward, Right, Down>                                            |\n| app_heading       | deg                                                                      |\n| app_pos           | <Lat (deg), Lon (deg), Alt (m)>                                          |\n| app_speed         | m/s                                                                      |\n| app_status        |                                                                          |\n| app_v_yaw         | deg/s, yaw velocity                                                      |\n| blinker           |                                                                          |\n| brake             | brake_computer + brake_user                                              |\n| brake_computer    | Commanded brake [0-4095]                                                 |\n| brake_user        | User brake pedal depression [0-4095]                                     |\n| cam1_ptr          | Camera frame index at this time                                          |\n| cam2_ptr          |                                                                          |\n| car_accel         | m/s^2, from derivative of wheel speed                                    |\n| fiber_accel       | m/s^2                                                                    |\n| fiber_compass     |                                                                          |\n| fiber_compass_x   |                                                                          |\n| fiber_compass_y   |                                                                          |\n| fiber_compass_z   |                                                                          |\n| fiber_gyro        | deg/s                                                                    |\n| fiber_temperature |                                                                          |\n| gas               | [0-1)                                                                    |\n| gear_choice       | Selected gear. 0- park/neutral, 10- reverse, 11- gear currently changing |\n| idx               |                                                                          |\n| rpm               |                                                                          |\n| rpm_post_torque   | post torque-converter                                                    |\n| selfdrive         |                                                                          |\n| speed             | m/s, from encoder after transmission, negative when gear is Revese       |\n| speed_abs         | m/s, from encoder after transmission                                     |\n| speed_fl          | Individual wheels speeds (m/s)                                           |\n| speed_fr          |                                                                          |\n| speed_rl          |                                                                          |\n| speed_rr          |                                                                          |\n| standstill        | Is the car stopped?                                                      |\n| steering_angle    |                                                                          |\n| steering_torque   | deg/s, despite the name, this is the steering angle rate                 |\n| times             | seconds                                                                  |\n| velodyne_gps      |                                                                          |\n| velodyne_heading  |                                                                          |\n| velodyne_imu      |                                                                          |\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.779296875,
          "content": "# the people's comma\n\n## the paper\n\n[Learning a Driving Simulator](http://arxiv.org/abs/1608.01230)\n\n## the comma.ai driving dataset\n\n7 and a quarter hours of largely highway driving. Enough to train what we had in [Bloomberg](http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/).\n\n## Examples\n\nWe present two Machine Learning Experiments to show\npossible ways to use this dataset:\n\n\n<img src=\"./images/selfsteer.gif\">\n\n[Training a steering angle predictor](SelfSteering.md)\n\n\n<img src=\"./images/drive_simulator.gif\">\n\n[Training a generative image model](DriveSim.md)\n\n## Downloading the dataset\n\n```\n./get_data.sh\n```\n\nor get it at [archive.org comma dataset](https://archive.org/details/comma-dataset)\n\n45 GB compressed, 80 GB uncompressed\n\n```\ndog/2016-01-30--11-24-51 (7.7G)\ndog/2016-01-30--13-46-00 (8.5G)\ndog/2016-01-31--19-19-25 (3.0G)\ndog/2016-02-02--10-16-58 (8.1G)\ndog/2016-02-08--14-56-28 (3.9G)\ndog/2016-02-11--21-32-47 (13G)\ndog/2016-03-29--10-50-20 (12G)\nemily/2016-04-21--14-48-08 (4.4G)\nemily/2016-05-12--22-20-00 (7.5G)\nfrodo/2016-06-02--21-39-29 (6.5G)\nfrodo/2016-06-08--11-46-01 (2.7G)\n```\n\nDataset referenced on this page is copyrighted by comma.ai and published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License. This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license.\n\n## Dataset structure\nThe dataset consists of 10 videos clips of variable size recorded at 20 Hz\nwith a camera mounted on the windshield of an Acura ILX 2016. In parallel to the videos\nwe also recorded some measurements such as car's speed, acceleration,\nsteering angle, GPS coordinates, gyroscope angles. See the full `log` list [here](Logs.md).\nThese measurements are transformed into a uniform 100 Hz time base.\n\nThe dataset folder structure is the following:\n```bash\n+-- dataset\n|   +-- camera\n|   |   +-- 2016-04-21--14-48-08\n|   |   ...\n|   +-- log\n|   |   +-- 2016-04-21--14-48-08\n|   |   ...\n```\n\nAll the files come in hdf5 format and are named with the time they were recorded.\nThe camera dataset has shape `number_frames x 3 x 160 x 320` and `uint8` type.\nOne of the `log` hdf5-datasets is called `cam1_ptr` and addresses the alignment\nbetween camera frames and the other measurements.\n\n## Requirements\n[anaconda](https://www.continuum.io/downloads)  \n[tensorflow-0.9](https://github.com/tensorflow/tensorflow)  \n[keras-1.0.6](https://github.com/fchollet/keras)  \n[cv2](https://anaconda.org/menpo/opencv3)\n\n## Hiring\n\nWant a job at [comma.ai](http://comma.ai)?\n\nShow us amazing stuff on this dataset\n\n## Credits\n\nRiccardo Biasini, George Hotz, Sam Khalandovsky, Eder Santana, and Niel van der Westhuizen\n"
        },
        {
          "name": "SelfSteering.md",
          "type": "blob",
          "size": 0.8896484375,
          "content": "# Steering Angle model\nFollow the instructions to train a deep neural network for self-steering cars.\nThis experiment is similar to [End to End Learning for Self-Driving\nCars](https://arxiv.org/abs/1604.07316).\n\n1) Download dataset\n```\n./get_data.sh\n```\n\n2) Start training data server in the first terminal session\n```bash\n./server.py --batch 200 --port 5557\n```  \n\n3) Start validation data server in a second terminal session\n```bash\n./server.py --batch 200 --validation --port 5556\n```\n\n4) Train steering model in a third terminal\n```bash\n./train_steering_model.py --port 5557 --val_port 5556\n```\n\n4) Visualize results\n```bash\n./view_steering_model.py ./outputs/steering_model/steering_angle.json\n```\n<img src=\"./images/selfsteer.gif\">\n\nYour job is to make the predicted `green` path to be equal to the actual `blue` path. Once you get that you have a self-driving car. Next step: get yourself some funding.\n\n"
        },
        {
          "name": "dask_generator.py",
          "type": "blob",
          "size": 3.9033203125,
          "content": "\"\"\"\nThis file is named after `dask` for historical reasons. We first tried to\nuse dask to coordinate the hdf5 buckets but it was slow and we wrote our own\nstuff.\n\"\"\"\nimport numpy as np\nimport h5py\nimport time\nimport logging\nimport traceback\n\n# logging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n\ndef concatenate(camera_names, time_len):\n  logs_names = [x.replace('camera', 'log') for x in camera_names]\n\n  angle = []  # steering angle of the car\n  speed = []  # steering angle of the car\n  hdf5_camera = []  # the camera hdf5 files need to continue open\n  c5x = []\n  filters = []\n  lastidx = 0\n\n  for cword, tword in zip(camera_names, logs_names):\n    try:\n      with h5py.File(tword, \"r\") as t5:\n        c5 = h5py.File(cword, \"r\")\n        hdf5_camera.append(c5)\n        x = c5[\"X\"]\n        c5x.append((lastidx, lastidx+x.shape[0], x))\n\n        speed_value = t5[\"speed\"][:]\n        steering_angle = t5[\"steering_angle\"][:]\n        idxs = np.linspace(0, steering_angle.shape[0]-1, x.shape[0]).astype(\"int\")  # approximate alignment\n        angle.append(steering_angle[idxs])\n        speed.append(speed_value[idxs])\n\n        goods = np.abs(angle[-1]) <= 200\n\n        filters.append(np.argwhere(goods)[time_len-1:] + (lastidx+time_len-1))\n        lastidx += goods.shape[0]\n        # check for mismatched length bug\n        print(\"x {} | t {} | f {}\".format(x.shape[0], steering_angle.shape[0], goods.shape[0]))\n        if x.shape[0] != angle[-1].shape[0] or x.shape[0] != goods.shape[0]:\n          raise Exception(\"bad shape\")\n\n    except IOError:\n      import traceback\n      traceback.print_exc()\n      print \"failed to open\", tword\n\n  angle = np.concatenate(angle, axis=0)\n  speed = np.concatenate(speed, axis=0)\n  filters = np.concatenate(filters, axis=0).ravel()\n  print \"training on %d/%d examples\" % (filters.shape[0], angle.shape[0])\n  return c5x, angle, speed, filters, hdf5_camera\n\n\nfirst = True\n\n\ndef datagen(filter_files, time_len=1, batch_size=256, ignore_goods=False):\n  \"\"\"\n  Parameters:\n  -----------\n  leads : bool, should we use all x, y and speed radar leads? default is false, uses only x\n  \"\"\"\n  global first\n  assert time_len > 0\n  filter_names = sorted(filter_files)\n\n  logger.info(\"Loading {} hdf5 buckets.\".format(len(filter_names)))\n\n  c5x, angle, speed, filters, hdf5_camera = concatenate(filter_names, time_len=time_len)\n  filters_set = set(filters)\n\n  logger.info(\"camera files {}\".format(len(c5x)))\n\n  X_batch = np.zeros((batch_size, time_len, 3, 160, 320), dtype='uint8')\n  angle_batch = np.zeros((batch_size, time_len, 1), dtype='float32')\n  speed_batch = np.zeros((batch_size, time_len, 1), dtype='float32')\n\n  while True:\n    try:\n      t = time.time()\n\n      count = 0\n      start = time.time()\n      while count < batch_size:\n        if not ignore_goods:\n          i = np.random.choice(filters)\n          # check the time history for goods\n          good = True\n          for j in (i-time_len+1, i+1):\n            if j not in filters_set:\n              good = False\n          if not good:\n            continue\n\n        else:\n          i = np.random.randint(time_len+1, len(angle), 1)\n\n        # GET X_BATCH\n        # low quality loop\n        for es, ee, x in c5x:\n          if i >= es and i < ee:\n            X_batch[count] = x[i-es-time_len+1:i-es+1]\n            break\n\n        angle_batch[count] = np.copy(angle[i-time_len+1:i+1])[:, None]\n        speed_batch[count] = np.copy(speed[i-time_len+1:i+1])[:, None]\n\n        count += 1\n\n      # sanity check\n      assert X_batch.shape == (batch_size, time_len, 3, 160, 320)\n\n      logging.debug(\"load image : {}s\".format(time.time()-t))\n      print(\"%5.2f ms\" % ((time.time()-start)*1000.0))\n\n      if first:\n        print \"X\", X_batch.shape\n        print \"angle\", angle_batch.shape\n        print \"speed\", speed_batch.shape\n        first = False\n\n      yield (X_batch, angle_batch, speed_batch)\n\n    except KeyboardInterrupt:\n      raise\n    except:\n      traceback.print_exc()\n      pass\n"
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "get_data.sh",
          "type": "blob",
          "size": 0.1767578125,
          "content": "#!/bin/bash\nmkdir -p dataset/camera dataset/log\n\nwget --continue https://archive.org/download/comma-dataset/comma-dataset.zip\nmkdir -p dataset\ncd dataset\nunzip ../comma-dataset.zip\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "outputs",
          "type": "tree",
          "content": null
        },
        {
          "name": "paper",
          "type": "tree",
          "content": null
        },
        {
          "name": "ros",
          "type": "tree",
          "content": null
        },
        {
          "name": "server.py",
          "type": "blob",
          "size": 6.681640625,
          "content": "#!/usr/bin/env python\n\"\"\"\nNote:\nPart of this code was copied and modified from github.com/mila-udem/fuel.git (MIT License)\n\"\"\"\nimport logging\n\nimport numpy\nimport zmq\nfrom numpy.lib.format import header_data_from_array_1_0\n\nimport six\nif six.PY3:\n  buffer_ = memoryview\nelse:\n  buffer_ = buffer  # noqa\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_arrays(socket, arrays, stop=False):\n  \"\"\"Send NumPy arrays using the buffer interface and some metadata.\n\n  Parameters\n  ----------\n  socket : :class:`zmq.Socket`\n  The socket to send data over.\n  arrays : list\n  A list of :class:`numpy.ndarray` to transfer.\n  stop : bool, optional\n  Instead of sending a series of NumPy arrays, send a JSON object\n  with a single `stop` key. The :func:`recv_arrays` will raise\n  ``StopIteration`` when it receives this.\n\n  Notes\n  -----\n  The protocol is very simple: A single JSON object describing the array\n  format (using the same specification as ``.npy`` files) is sent first.\n  Subsequently the arrays are sent as bytestreams (through NumPy's\n  support of the buffering protocol).\n\n  \"\"\"\n  if arrays:\n    # The buffer protocol only works on contiguous arrays\n    arrays = [numpy.ascontiguousarray(array) for array in arrays]\n  if stop:\n    headers = {'stop': True}\n    socket.send_json(headers)\n  else:\n    headers = [header_data_from_array_1_0(array) for array in arrays]\n    socket.send_json(headers, zmq.SNDMORE)\n    for array in arrays[:-1]:\n      socket.send(array, zmq.SNDMORE)\n    socket.send(arrays[-1])\n\n\ndef recv_arrays(socket):\n  \"\"\"Receive a list of NumPy arrays.\n\n  Parameters\n  ----------\n  socket : :class:`zmq.Socket`\n  The socket to receive the arrays on.\n\n  Returns\n  -------\n  list\n  A list of :class:`numpy.ndarray` objects.\n\n  Raises\n  ------\n  StopIteration\n  If the first JSON object received contains the key `stop`,\n  signifying that the server has finished a single epoch.\n\n  \"\"\"\n  headers = socket.recv_json()\n  if 'stop' in headers:\n    raise StopIteration\n  arrays = []\n  for header in headers:\n    data = socket.recv()\n    buf = buffer_(data)\n    array = numpy.frombuffer(buf, dtype=numpy.dtype(header['descr']))\n    array.shape = header['shape']\n    if header['fortran_order']:\n      array.shape = header['shape'][::-1]\n      array = array.transpose()\n    arrays.append(array)\n  return arrays\n\n\ndef client_generator(port=5557, host=\"localhost\", hwm=20):\n  \"\"\"Generator in client side should extend this generator\n\n  Parameters\n  ----------\n\n  port : int\n  hwm : int, optional\n  The `ZeroMQ high-water mark (HWM)\n  <http://zguide.zeromq.org/page:all#High-Water-Marks>`_ on the\n  sending socket. Increasing this increases the buffer, which can be\n  useful if your data preprocessing times are very random.  However,\n  it will increase memory usage. There is no easy way to tell how\n  many batches will actually be queued with a particular HWM.\n  Defaults to 10. Be sure to set the corresponding HWM on the\n  receiving end as well.\n  \"\"\"\n  context = zmq.Context()\n  socket = context.socket(zmq.PULL)\n  socket.set_hwm(hwm)\n  socket.connect(\"tcp://{}:{}\".format(host, port))\n  logger.info('client started')\n  while True:\n    data = recv_arrays(socket)\n    yield tuple(data)\n\n\ndef start_server(data_stream, port=5557, hwm=20):\n  \"\"\"Start a data processing server.\n\n  This command starts a server in the current process that performs the\n  actual data processing (by retrieving data from the given data stream).\n  It also starts a second process, the broker, which mediates between the\n  server and the client. The broker also keeps a buffer of batches in\n  memory.\n\n  Parameters\n  ----------\n  data_stream : generator\n  The data stream to return examples from.\n  port : int, optional\n  The port the server and the client (training loop) will use to\n  communicate. Defaults to 5557.\n  hwm : int, optional\n  The `ZeroMQ high-water mark (HWM)\n  <http://zguide.zeromq.org/page:all#High-Water-Marks>`_ on the\n  sending socket. Increasing this increases the buffer, which can be\n  useful if your data preprocessing times are very random.  However,\n  it will increase memory usage. There is no easy way to tell how\n  many batches will actually be queued with a particular HWM.\n  Defaults to 10. Be sure to set the corresponding HWM on the\n  receiving end as well.\n  \"\"\"\n  logging.basicConfig(level='INFO')\n\n  context = zmq.Context()\n  socket = context.socket(zmq.PUSH)\n  socket.set_hwm(hwm)\n  socket.bind('tcp://*:{}'.format(port))\n\n  # it = itertools.tee(data_stream)\n  it = data_stream\n\n  logger.info('server started')\n  while True:\n    try:\n      data = next(it)\n      stop = False\n      logger.debug(\"sending {} arrays\".format(len(data)))\n    except StopIteration:\n      it = data_stream\n      data = None\n      stop = True\n      logger.debug(\"sending StopIteration\")\n    send_arrays(socket, data, stop=stop)\n\n# Example\nif __name__ == \"__main__\":\n  from dask_generator import datagen\n  import argparse\n\n  # Parameters\n  parser = argparse.ArgumentParser(description='MiniBatch server')\n  parser.add_argument('--batch', dest='batch', type=int, default=256, help='Batch size')\n  parser.add_argument('--time', dest='time', type=int, default=1, help='Number of frames per sample')\n  parser.add_argument('--port', dest='port', type=int, default=5557, help='Port of the ZMQ server')\n  parser.add_argument('--buffer', dest='buffer', type=int, default=20, help='High-water mark. Increasing this increses buffer and memory usage.')\n  parser.add_argument('--prep', dest='prep', action='store_true', default=False, help='Use images preprocessed by vision model.')\n  parser.add_argument('--leads', dest='leads', action='store_true', default=False, help='Use x, y and speed radar lead info.')\n  parser.add_argument('--nogood', dest='nogood', action='store_true', default=False, help='Ignore `goods` filters.')\n  parser.add_argument('--validation', dest='validation', action='store_true', default=False, help='Serve validation dataset instead.')\n  args, more = parser.parse_known_args()\n\n  # 9 for training\n  train_path = [\n    './dataset/camera/2016-01-30--11-24-51.h5',\n    './dataset/camera/2016-01-30--13-46-00.h5',\n    './dataset/camera/2016-01-31--19-19-25.h5',\n    './dataset/camera/2016-02-02--10-16-58.h5',\n    './dataset/camera/2016-02-08--14-56-28.h5',\n    './dataset/camera/2016-02-11--21-32-47.h5',\n    './dataset/camera/2016-03-29--10-50-20.h5',\n    './dataset/camera/2016-04-21--14-48-08.h5',\n    './dataset/camera/2016-05-12--22-20-00.h5',\n  ]\n\n  # 2 for validation\n  validation_path = [\n    './dataset/camera/2016-06-02--21-39-29.h5',\n    './dataset/camera/2016-06-08--11-46-01.h5'\n  ]\n\n  if args.validation:\n    datapath = validation_path\n  else:\n    datapath = train_path\n\n  gen = datagen(datapath, time_len=args.time, batch_size=args.batch, ignore_goods=args.nogood)\n  start_server(gen, port=args.port, hwm=args.buffer)\n\n"
        },
        {
          "name": "train_generative_model.py",
          "type": "blob",
          "size": 5.548828125,
          "content": "#!/usr/bin/env python\n\n\"\"\"\nUsage:\n>> ./server.py\n>> ./train_generator.py autoencoder\n\"\"\"\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport argparse\nimport time\nfrom keras import callbacks as cbks\nimport logging\nimport tensorflow as tf\nimport numpy as np\n\nfrom server import client_generator\nfrom models.utils import save_images\nmixtures = 1\n\n\ndef old_cleanup(data):\n  X = data[0]\n  if X.shape[1] == 1:\n    X = X[:, -1, :]/127.5 - 1.\n  return X\n\n\ndef gen(hwm, host, port):\n  for tup in client_generator(hwm=hwm, host=host, port=port):\n    X = cleanup(tup)\n    yield X\n\n\ndef train_model(name, g_train, d_train, sampler, generator, samples_per_epoch, nb_epoch,\n                z_dim=100, verbose=1, callbacks=[],\n                validation_data=None, nb_val_samples=None,\n                saver=None):\n    \"\"\"\n    Main training loop.\n    modified from Keras fit_generator\n    \"\"\"\n    self = {}\n    epoch = 0\n    counter = 0\n    out_labels = ['g_loss', 'd_loss', 'd_loss_fake', 'd_loss_legit', 'time']  # self.metrics_names\n    callback_metrics = out_labels + ['val_' + n for n in out_labels]\n\n    # prepare callbacks\n    history = cbks.History()\n    callbacks = [cbks.BaseLogger()] + callbacks + [history]\n    if verbose:\n        callbacks += [cbks.ProgbarLogger()]\n    callbacks = cbks.CallbackList(callbacks)\n\n    callbacks._set_params({\n        'nb_epoch': nb_epoch,\n        'nb_sample': samples_per_epoch,\n        'verbose': verbose,\n        'metrics': callback_metrics,\n    })\n    callbacks.on_train_begin()\n\n    while epoch < nb_epoch:\n      callbacks.on_epoch_begin(epoch)\n      samples_seen = 0\n      batch_index = 0\n      while samples_seen < samples_per_epoch:\n        z, x = next(generator)\n        # build batch logs\n        batch_logs = {}\n        if type(x) is list:\n          batch_size = len(x[0])\n        elif type(x) is dict:\n          batch_size = len(list(x.values())[0])\n        else:\n          batch_size = len(x)\n        batch_logs['batch'] = batch_index\n        batch_logs['size'] = batch_size\n        callbacks.on_batch_begin(batch_index, batch_logs)\n\n        t1 = time.time()\n        d_losses = d_train(x, z, counter)\n        z, x = next(generator)\n        g_loss, samples, xs = g_train(x, z, counter)\n        outs = (g_loss, ) + d_losses + (time.time() - t1, )\n        counter += 1\n\n        # save samples\n        if batch_index % 100 == 0:\n          join_image = np.zeros_like(np.concatenate([samples[:64], xs[:64]], axis=0))\n          for j, (i1, i2) in enumerate(zip(samples[:64], xs[:64])):\n            join_image[j*2] = i1\n            join_image[j*2+1] = i2\n          save_images(join_image, [8*2, 8],\n                      './outputs/samples_%s/train_%s_%s.png' % (name, epoch, batch_index))\n\n          samples, xs = sampler(z, x)\n          join_image = np.zeros_like(np.concatenate([samples[:64], xs[:64]], axis=0))\n          for j, (i1, i2) in enumerate(zip(samples[:64], xs[:64])):\n            join_image[j*2] = i1\n            join_image[j*2+1] = i2\n          save_images(join_image, [8*2, 8],\n                      './outputs/samples_%s/test_%s_%s.png' % (name, epoch, batch_index))\n\n        for l, o in zip(out_labels, outs):\n            batch_logs[l] = o\n\n        callbacks.on_batch_end(batch_index, batch_logs)\n\n        # construct epoch logs\n        epoch_logs = {}\n        batch_index += 1\n        samples_seen += batch_size\n\n      if saver is not None:\n        saver(epoch)\n\n      callbacks.on_epoch_end(epoch, epoch_logs)\n      epoch += 1\n\n    # _stop.set()\n    callbacks.on_train_end()\n\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser(description='Generative model trainer')\n  parser.add_argument('model', type=str, default=\"bn_model\", help='Model definitnion file')\n  parser.add_argument('--name', type=str, default=\"autoencoder\", help='Name of the model.')\n  parser.add_argument('--host', type=str, default=\"localhost\", help='Data server ip address.')\n  parser.add_argument('--port', type=int, default=5557, help='Port of server.')\n  # parser.add_argument('--time', type=int, default=1, help='How many temporal frames in a single input.')\n  parser.add_argument('--batch', type=int, default=64, help='Batch size.')\n  parser.add_argument('--epoch', type=int, default=200, help='Number of epochs.')\n  parser.add_argument('--gpu', type=int, default=0, help='Which gpu to use')\n  parser.add_argument('--epochsize', type=int, default=10000, help='How many frames per epoch.')\n  parser.add_argument('--loadweights', dest='loadweights', action='store_true', help='Start from checkpoint.')\n  parser.set_defaults(skipvalidate=False)\n  parser.set_defaults(loadweights=False)\n  args = parser.parse_args()\n\n  MODEL_NAME = args.model\n  logging.info(\"Importing get_model from {}\".format(args.model))\n  exec(\"from models.\"+MODEL_NAME+\" import get_model\")\n  # try to import `cleanup` from model file\n  try:\n    exec(\"from models.\"+MODEL_NAME+\" import cleanup\")\n  except:\n    cleanup = old_cleanup\n\n  model_code = open('models/'+MODEL_NAME+'.py').read()\n\n  if not os.path.exists(\"./outputs/results_\"+args.name):\n      os.makedirs(\"./outputs/results_\"+args.name)\n  if not os.path.exists(\"./outputs/samples_\"+args.name):\n      os.makedirs(\"./outputs/samples_\"+args.name)\n\n  with tf.Session() as sess:\n    g_train, d_train, sampler, saver, loader, extras = get_model(sess=sess, name=args.name, batch_size=args.batch, gpu=args.gpu)\n\n    # start from checkpoint\n    if args.loadweights:\n      loader()\n\n    train_model(args.name, g_train, d_train, sampler,\n                gen(20, args.host, port=args.port),\n                samples_per_epoch=args.epochsize,\n                nb_epoch=args.epoch, verbose=1, saver=saver\n                )\n"
        },
        {
          "name": "train_steering_model.py",
          "type": "blob",
          "size": 2.5849609375,
          "content": "#!/usr/bin/env python\n\"\"\"\nSteering angle prediction model\n\"\"\"\nimport os\nimport argparse\nimport json\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Lambda, ELU\nfrom keras.layers.convolutional import Convolution2D\n\nfrom server import client_generator\n\n\ndef gen(hwm, host, port):\n  for tup in client_generator(hwm=hwm, host=host, port=port):\n    X, Y, _ = tup\n    Y = Y[:, -1]\n    if X.shape[1] == 1:  # no temporal context\n      X = X[:, -1]\n    yield X, Y\n\n\ndef get_model(time_len=1):\n  ch, row, col = 3, 160, 320  # camera format\n\n  model = Sequential()\n  model.add(Lambda(lambda x: x/127.5 - 1.,\n            input_shape=(ch, row, col),\n            output_shape=(ch, row, col)))\n  model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\"))\n  model.add(ELU())\n  model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n  model.add(ELU())\n  model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n  model.add(Flatten())\n  model.add(Dropout(.2))\n  model.add(ELU())\n  model.add(Dense(512))\n  model.add(Dropout(.5))\n  model.add(ELU())\n  model.add(Dense(1))\n\n  model.compile(optimizer=\"adam\", loss=\"mse\")\n\n  return model\n\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser(description='Steering angle model trainer')\n  parser.add_argument('--host', type=str, default=\"localhost\", help='Data server ip address.')\n  parser.add_argument('--port', type=int, default=5557, help='Port of server.')\n  parser.add_argument('--val_port', type=int, default=5556, help='Port of server for validation dataset.')\n  parser.add_argument('--batch', type=int, default=64, help='Batch size.')\n  parser.add_argument('--epoch', type=int, default=200, help='Number of epochs.')\n  parser.add_argument('--epochsize', type=int, default=10000, help='How many frames per epoch.')\n  parser.add_argument('--skipvalidate', dest='skipvalidate', action='store_true', help='Multiple path output.')\n  parser.set_defaults(skipvalidate=False)\n  parser.set_defaults(loadweights=False)\n  args = parser.parse_args()\n\n  model = get_model()\n  model.fit_generator(\n    gen(20, args.host, port=args.port),\n    samples_per_epoch=10000,\n    nb_epoch=args.epoch,\n    validation_data=gen(20, args.host, port=args.val_port),\n    nb_val_samples=1000\n  )\n  print(\"Saving model weights and configuration file.\")\n\n  if not os.path.exists(\"./outputs/steering_model\"):\n      os.makedirs(\"./outputs/steering_model\")\n\n  model.save_weights(\"./outputs/steering_model/steering_angle.keras\", True)\n  with open('./outputs/steering_model/steering_angle.json', 'w') as outfile:\n    json.dump(model.to_json(), outfile)\n"
        },
        {
          "name": "view_generative_model.py",
          "type": "blob",
          "size": 3.6611328125,
          "content": "#!/usr/bin/env python\n\n\"\"\"\nUsage:\n>> server.py --time 60 --batch 64\n>> ./make_gif.py transition --name transition --time 15 --batch 64\n\"\"\"\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport time\nimport cv2\nfrom keras import callbacks as cbks\nfrom keras import backend as K\nimport logging\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.misc import imsave, imresize\nfrom tqdm import *\n\nfrom server import client_generator\nmixtures = 1\n\n\nif __name__ == \"__main__\":\n  import argparse\n  parser = argparse.ArgumentParser(description='MiniBatch server')\n  parser.add_argument('model', type=str, default=\"transition\", help='Model definitnion file')\n  parser.add_argument('--name', type=str, default=\"transition\", help='Name of the model.')\n  parser.add_argument('--host', type=str, default=\"localhost\", help='Data server ip address.')\n  parser.add_argument('--port', type=int, default=5557, help='Port of server.')\n  parser.add_argument('--time', type=int, default=1, help='How many temporal frames in a single input.')\n  parser.add_argument('--batch', type=int, default=256, help='Batch size.')\n  parser.add_argument('--epoch', type=int, default=200, help='Number of epochs.')\n  parser.add_argument('--gpu', type=int, default=0, help='Which gpu to use')\n  parser.add_argument('--loadweights', dest='loadweights', action='store_true', help='Start from checkpoint.')\n  parser.set_defaults(skipvalidate=False)\n  parser.set_defaults(loadweights=False)\n  args = parser.parse_args()\n\n  MODEL_NAME = args.model\n  logging.info(\"Importing get_model from {}\".format(args.model))\n  exec(\"from models.\"+MODEL_NAME+\" import get_model, load, save\")\n  # try to import `cleanup` from model file\n  try:\n    exec(\"from models.\"+MODEL_NAME+\" import cleanup\")\n  except:\n    cleanup = old_cleanup\n\n  model_code = open('models/'+MODEL_NAME+'.py').read()\n\n  with tf.Session() as sess:\n    K.set_session(sess)\n    g_train, d_train, sampler, saver, loader, [G, E, T] = get_model(sess=sess, name=args.name, batch_size=args.batch, gpu=args.gpu)\n\n    print(\"loading weights...\")\n    G.load_weights(\"./outputs/results_autoencoder/G_weights.keras\".format(args.name))\n    E.load_weights(\"./outputs/results_autoencoder/E_weights.keras\".format(args.name))\n    checkpoint_dir = './outputs/results_' + args.name\n    T.load_weights(checkpoint_dir+\"/T_weights.keras\")\n\n    if not os.path.exists(\"./video_\"+args.name):\n      os.makedirs(\"./video_\"+args.name)\n\n    # get data\n    data = client_generator(hwm=20, host=\"localhost\", port=5557)\n    X = next(data)[0]  # [:, ::2]\n    sh = X.shape\n    X = X.reshape((-1, 3, 160, 320))\n    X = np.asarray([cv2.resize(x.transpose(1, 2, 0), (160, 80)) for x in X])\n    X = X/127.5 - 1.\n    x = X.reshape((sh[0], args.time, 80, 160, 3))\n\n    # estimate frames\n    z_dim = 512\n    I = E.input\n    E_out = E(I)\n    O = G.input\n    G_out = G(O)\n    print \"Sampling...\"\n    for i in tqdm(range(128)):\n      x = x.reshape((-1, 80, 160, 3))\n      # code = E.predict(x, batch_size=args.batch*args.time)[0]\n      code = sess.run([E_out[0]], feed_dict={I: x, K.learning_phase(): 1})[0]\n      code = code.reshape((args.batch, args.time, z_dim))\n      inp = code[:, :5]  # context is based on the first 5 frames only\n      outs = T.predict(inp, batch_size=args.batch)\n      imgs = sess.run([G_out], feed_dict={O: outs.reshape((-1, z_dim)), K.learning_phase(): 1})[0]\n      # imgs = G.predict(outs[:, 0], batch_size=args.batch)\n      x = x.reshape((args.batch, args.time, 80, 160, 3))\n      x[0, :-1] = x[0, 1:]\n      x[0, -1] = imgs[0]\n      imsave(\"video_\"+args.name+\"/%03d.png\" % i, imresize(imgs[0], (160, 320)))\n\n    cmd = \"ffmpeg -y -i ./video_\"+args.name+\"/%03d.png ./video_\"+args.name+\"/output.gif -vf fps=1\"\n    print(cmd)\n    os.system(cmd)\n"
        },
        {
          "name": "view_steering_model.py",
          "type": "blob",
          "size": 4.42578125,
          "content": "#!/usr/bin/env python\nimport argparse\nimport sys\nimport numpy as np\nimport h5py\nimport pygame\nimport json\nfrom keras.models import model_from_json\n\npygame.init()\nsize = (320*2, 160*2)\npygame.display.set_caption(\"comma.ai data viewer\")\nscreen = pygame.display.set_mode(size, pygame.DOUBLEBUF)\n\ncamera_surface = pygame.surface.Surface((320,160),0,24).convert()\n\n# ***** get perspective transform for images *****\nfrom skimage import transform as tf\n\nrsrc = \\\n [[43.45456230828867, 118.00743250075844],\n  [104.5055617352614, 69.46865203761757],\n  [114.86050156739812, 60.83953551083698],\n  [129.74572757609468, 50.48459567870026],\n  [132.98164627363735, 46.38576532847949],\n  [301.0336906326895, 98.16046448916306],\n  [238.25686790036065, 62.56535881619311],\n  [227.2547443287154, 56.30924933427718],\n  [209.13359962247614, 46.817221154818526],\n  [203.9561297064078, 43.5813024572758]]\nrdst = \\\n [[10.822125594094452, 1.42189132706374],\n  [21.177065426231174, 1.5297552836484982],\n  [25.275895776451954, 1.42189132706374],\n  [36.062291434927694, 1.6376192402332563],\n  [40.376849698318004, 1.42189132706374],\n  [11.900765159942026, -2.1376192402332563],\n  [22.25570499207874, -2.1376192402332563],\n  [26.785991168638553, -2.029755283648498],\n  [37.033067044190524, -2.029755283648498],\n  [41.67121717733509, -2.029755283648498]]\n\ntform3_img = tf.ProjectiveTransform()\ntform3_img.estimate(np.array(rdst), np.array(rsrc))\n\ndef perspective_tform(x, y):\n  p1, p2 = tform3_img((x,y))[0]\n  return p2, p1\n\n# ***** functions to draw lines *****\ndef draw_pt(img, x, y, color, sz=1):\n  row, col = perspective_tform(x, y)\n  if row >= 0 and row < img.shape[0] and\\\n     col >= 0 and col < img.shape[1]:\n    img[row-sz:row+sz, col-sz:col+sz] = color\n\ndef draw_path(img, path_x, path_y, color):\n  for x, y in zip(path_x, path_y):\n    draw_pt(img, x, y, color)\n\n# ***** functions to draw predicted path *****\n\ndef calc_curvature(v_ego, angle_steers, angle_offset=0):\n  deg_to_rad = np.pi/180.\n  slip_fator = 0.0014 # slip factor obtained from real data\n  steer_ratio = 15.3  # from http://www.edmunds.com/acura/ilx/2016/road-test-specs/\n  wheel_base = 2.67   # from http://www.edmunds.com/acura/ilx/2016/sedan/features-specs/\n\n  angle_steers_rad = (angle_steers - angle_offset) * deg_to_rad\n  curvature = angle_steers_rad/(steer_ratio * wheel_base * (1. + slip_fator * v_ego**2))\n  return curvature\n\ndef calc_lookahead_offset(v_ego, angle_steers, d_lookahead, angle_offset=0):\n  #*** this function returns the lateral offset given the steering angle, speed and the lookahead distance\n  curvature = calc_curvature(v_ego, angle_steers, angle_offset)\n\n  # clip is to avoid arcsin NaNs due to too sharp turns\n  y_actual = d_lookahead * np.tan(np.arcsin(np.clip(d_lookahead * curvature, -0.999, 0.999))/2.)\n  return y_actual, curvature\n\ndef draw_path_on(img, speed_ms, angle_steers, color=(0,0,255)):\n  path_x = np.arange(0., 50.1, 0.5)\n  path_y, _ = calc_lookahead_offset(speed_ms, angle_steers, path_x)\n  draw_path(img, path_x, path_y, color)\n\n# ***** main loop *****\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser(description='Path viewer')\n  parser.add_argument('model', type=str, help='Path to model definition json. Model weights should be on the same path.')\n  parser.add_argument('--dataset', type=str, default=\"2016-06-08--11-46-01\", help='Dataset/video clip name')\n  args = parser.parse_args()\n\n  with open(args.model, 'r') as jfile:\n    model = model_from_json(json.load(jfile))\n\n  model.compile(\"sgd\", \"mse\")\n  weights_file = args.model.replace('json', 'keras')\n  model.load_weights(weights_file)\n\n  # default dataset is the validation data on the highway\n  dataset = args.dataset\n  skip = 300\n\n  log = h5py.File(\"dataset/log/\"+dataset+\".h5\", \"r\")\n  cam = h5py.File(\"dataset/camera/\"+dataset+\".h5\", \"r\")\n\n  print log.keys()\n\n  # skip to highway\n  for i in range(skip*100, log['times'].shape[0]):\n    if i%100 == 0:\n      print \"%.2f seconds elapsed\" % (i/100.0)\n    img = cam['X'][log['cam1_ptr'][i]].swapaxes(0,2).swapaxes(0,1)\n\n    predicted_steers = model.predict(img[None, :, :, :].transpose(0, 3, 1, 2))[0][0]\n\n    angle_steers = log['steering_angle'][i]\n    speed_ms = log['speed'][i]\n\n    draw_path_on(img, speed_ms, -angle_steers/10.0)\n    draw_path_on(img, speed_ms, -predicted_steers/10.0, (0, 255, 0))\n\n    # draw on\n    pygame.surfarray.blit_array(camera_surface, img.swapaxes(0,1))\n    camera_surface_2x = pygame.transform.scale2x(camera_surface)\n    screen.blit(camera_surface_2x, (0,0))\n    pygame.display.flip()\n"
        }
      ]
    }
  ]
}