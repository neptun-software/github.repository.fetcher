{
  "metadata": {
    "timestamp": 1736559620870,
    "page": 265,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "weiaicunzai/pytorch-cifar100",
      "stars": 4362,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0341796875,
          "content": "**/__pycache__\ndata\ncheckpoint\nruns"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.818359375,
          "content": "# Pytorch-cifar100\n\npractice on cifar100 using pytorch\n\n## Requirements\n\nThis is my experiment eviroument\n- python3.6\n- pytorch1.6.0+cu101\n- tensorboard 2.2.2(optional)\n\n\n## Usage\n\n### 1. enter directory\n```bash\n$ cd pytorch-cifar100\n```\n\n### 2. dataset\nI will use cifar100 dataset from torchvision since it's more convenient, but I also\nkept the sample code for writing your own dataset module in dataset folder, as an\nexample for people don't know how to write it.\n\n### 3. run tensorbard(optional)\nInstall tensorboard\n```bash\n$ pip install tensorboard\n$ mkdir runs\nRun tensorboard\n$ tensorboard --logdir='runs' --port=6006 --host='localhost'\n```\n\n### 4. train the model\nYou need to specify the net you want to train using arg -net\n\n```bash\n# use gpu to train vgg16\n$ python train.py -net vgg16 -gpu\n```\n\nsometimes, you might want to use warmup training by set ```-warm``` to 1 or 2, to prevent network\ndiverge during early training phase.\n\nThe supported net args are:\n```\nsqueezenet\nmobilenet\nmobilenetv2\nshufflenet\nshufflenetv2\nvgg11\nvgg13\nvgg16\nvgg19\ndensenet121\ndensenet161\ndensenet201\ngooglenet\ninceptionv3\ninceptionv4\ninceptionresnetv2\nxception\nresnet18\nresnet34\nresnet50\nresnet101\nresnet152\npreactresnet18\npreactresnet34\npreactresnet50\npreactresnet101\npreactresnet152\nresnext50\nresnext101\nresnext152\nattention56\nattention92\nseresnet18\nseresnet34\nseresnet50\nseresnet101\nseresnet152\nnasnet\nwideresnet\nstochasticdepth18\nstochasticdepth34\nstochasticdepth50\nstochasticdepth101\n```\nNormally, the weights file with the best accuracy would be written to the disk with name suffix 'best'(default in checkpoint folder).\n\n\n### 5. test the model\nTest the model using test.py\n```bash\n$ python test.py -net vgg16 -weights path_to_vgg16_weights_file\n```\n\n## Implementated NetWork\n\n- vgg [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556v6)\n- googlenet [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842v1)\n- inceptionv3 [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567v3)\n- inceptionv4, inception_resnet_v2 [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)\n- xception [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)\n- resnet [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385v1)\n- resnext [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431v2)\n- resnet in resnet [Resnet in Resnet: Generalizing Residual Architectures](https://arxiv.org/abs/1603.08029v1)\n- densenet [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993v5)\n- shufflenet [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/abs/1707.01083v2)\n- shufflenetv2 [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://arxiv.org/abs/1807.11164v1)\n- mobilenet [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n- mobilenetv2 [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)\n- residual attention network [Residual Attention Network for Image Classification](https://arxiv.org/abs/1704.06904)\n- senet [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)\n- squeezenet [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://arxiv.org/abs/1602.07360v4)\n- nasnet [Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012v4)\n- wide residual network[Wide Residual Networks](https://arxiv.org/abs/1605.07146)\n- stochastic depth networks[Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382)\n\n## Training Details\nI didn't use any training tricks to improve accuray, if you want to learn more about training tricks,\nplease refer to my another [repo](https://github.com/weiaicunzai/Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks), contains\nvarious common training tricks and their pytorch implementations.\n\n\nI follow the hyperparameter settings in paper [Improved Regularization of Convolutional Neural Networks with Cutout](https://arxiv.org/abs/1708.04552v2), which is init lr = 0.1 divide by 5 at 60th, 120th, 160th epochs, train for 200\nepochs with batchsize 128 and weight decay 5e-4, Nesterov momentum of 0.9. You could also use the hyperparameters from paper [Regularizing Neural Networks by Penalizing Confident Output Distributions](https://arxiv.org/abs/1701.06548v1) and [Random Erasing Data Augmentation](https://arxiv.org/abs/1708.04896v2), which is initial lr = 0.1, lr divied by 10 at 150th and 225th epochs, and training for 300 epochs with batchsize 128, this is more commonly used. You could decrese the batchsize to 64 or whatever suits you, if you dont have enough gpu memory.\n\nYou can choose whether to use TensorBoard to visualize your training procedure\n\n## Results\nThe result I can get from a certain model, since I use the same hyperparameters to train all the networks, some networks might not get the best result from these hyperparameters, you could try yourself by finetuning the hyperparameters to get\nbetter result.\n\n|dataset|network|params|top1 err|top5 err|epoch(lr = 0.1)|epoch(lr = 0.02)|epoch(lr = 0.004)|epoch(lr = 0.0008)|total epoch|\n|:-----:|:-----:|:----:|:------:|:------:|:-------------:|:--------------:|:---------------:|:----------------:|:---------:|\n|cifar100|mobilenet|3.3M|34.02|10.56|60|60|40|40|200|\n|cifar100|mobilenetv2|2.36M|31.92|09.02|60|60|40|40|200|\n|cifar100|squeezenet|0.78M|30.59|8.36|60|60|40|40|200|\n|cifar100|shufflenet|1.0M|29.94|8.35|60|60|40|40|200|\n|cifar100|shufflenetv2|1.3M|30.49|8.49|60|60|40|40|200|\n|cifar100|vgg11_bn|28.5M|31.36|11.85|60|60|40|40|200|\n|cifar100|vgg13_bn|28.7M|28.00|9.71|60|60|40|40|200|\n|cifar100|vgg16_bn|34.0M|27.07|8.84|60|60|40|40|200|\n|cifar100|vgg19_bn|39.0M|27.77|8.84|60|60|40|40|200|\n|cifar100|resnet18|11.2M|24.39|6.95|60|60|40|40|200|\n|cifar100|resnet34|21.3M|23.24|6.63|60|60|40|40|200|\n|cifar100|resnet50|23.7M|22.61|6.04|60|60|40|40|200|\n|cifar100|resnet101|42.7M|22.22|5.61|60|60|40|40|200|\n|cifar100|resnet152|58.3M|22.31|5.81|60|60|40|40|200|\n|cifar100|preactresnet18|11.3M|27.08|8.53|60|60|40|40|200|\n|cifar100|preactresnet34|21.5M|24.79|7.68|60|60|40|40|200|\n|cifar100|preactresnet50|23.9M|25.73|8.15|60|60|40|40|200|\n|cifar100|preactresnet101|42.9M|24.84|7.83|60|60|40|40|200|\n|cifar100|preactresnet152|58.6M|22.71|6.62|60|60|40|40|200|\n|cifar100|resnext50|14.8M|22.23|6.00|60|60|40|40|200|\n|cifar100|resnext101|25.3M|22.22|5.99|60|60|40|40|200|\n|cifar100|resnext152|33.3M|22.40|5.58|60|60|40|40|200|\n|cifar100|attention59|55.7M|33.75|12.90|60|60|40|40|200|\n|cifar100|attention92|102.5M|36.52|11.47|60|60|40|40|200|\n|cifar100|densenet121|7.0M|22.99|6.45|60|60|40|40|200|\n|cifar100|densenet161|26M|21.56|6.04|60|60|60|40|200|\n|cifar100|densenet201|18M|21.46|5.9|60|60|40|40|200|\n|cifar100|googlenet|6.2M|21.97|5.94|60|60|40|40|200|\n|cifar100|inceptionv3|22.3M|22.81|6.39|60|60|40|40|200|\n|cifar100|inceptionv4|41.3M|24.14|6.90|60|60|40|40|200|\n|cifar100|inceptionresnetv2|65.4M|27.51|9.11|60|60|40|40|200|\n|cifar100|xception|21.0M|25.07|7.32|60|60|40|40|200|\n|cifar100|seresnet18|11.4M|23.56|6.68|60|60|40|40|200|\n|cifar100|seresnet34|21.6M|22.07|6.12|60|60|40|40|200|\n|cifar100|seresnet50|26.5M|21.42|5.58|60|60|40|40|200|\n|cifar100|seresnet101|47.7M|20.98|5.41|60|60|40|40|200|\n|cifar100|seresnet152|66.2M|20.66|5.19|60|60|40|40|200|\n|cifar100|nasnet|5.2M|22.71|5.91|60|60|40|40|200|\n|cifar100|wideresnet-40-10|55.9M|21.25|5.77|60|60|40|40|200|\n|cifar100|stochasticdepth18|11.22M|31.40|8.84|60|60|40|40|200|\n|cifar100|stochasticdepth34|21.36M|27.72|7.32|60|60|40|40|200|\n|cifar100|stochasticdepth50|23.71M|23.35|5.76|60|60|40|40|200|\n|cifar100|stochasticdepth101|42.69M|21.28|5.39|60|60|40|40|200|\n\n\n\n"
        },
        {
          "name": "conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset.py",
          "type": "blob",
          "size": 1.88671875,
          "content": "\"\"\" train and test dataset\n\nauthor baiyu\n\"\"\"\nimport os\nimport sys\nimport pickle\n\nfrom skimage import io\nimport matplotlib.pyplot as plt\nimport numpy\nimport torch\nfrom torch.utils.data import Dataset\n\nclass CIFAR100Train(Dataset):\n    \"\"\"cifar100 test dataset, derived from\n    torch.utils.data.DataSet\n    \"\"\"\n\n    def __init__(self, path, transform=None):\n        #if transform is given, we transoform data using\n        with open(os.path.join(path, 'train'), 'rb') as cifar100:\n            self.data = pickle.load(cifar100, encoding='bytes')\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data['fine_labels'.encode()])\n\n    def __getitem__(self, index):\n        label = self.data['fine_labels'.encode()][index]\n        r = self.data['data'.encode()][index, :1024].reshape(32, 32)\n        g = self.data['data'.encode()][index, 1024:2048].reshape(32, 32)\n        b = self.data['data'.encode()][index, 2048:].reshape(32, 32)\n        image = numpy.dstack((r, g, b))\n\n        if self.transform:\n            image = self.transform(image)\n        return label, image\n\nclass CIFAR100Test(Dataset):\n    \"\"\"cifar100 test dataset, derived from\n    torch.utils.data.DataSet\n    \"\"\"\n\n    def __init__(self, path, transform=None):\n        with open(os.path.join(path, 'test'), 'rb') as cifar100:\n            self.data = pickle.load(cifar100, encoding='bytes')\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data['data'.encode()])\n\n    def __getitem__(self, index):\n        label = self.data['fine_labels'.encode()][index]\n        r = self.data['data'.encode()][index, :1024].reshape(32, 32)\n        g = self.data['data'.encode()][index, 1024:2048].reshape(32, 32)\n        b = self.data['data'.encode()][index, 2048:].reshape(32, 32)\n        image = numpy.dstack((r, g, b))\n\n        if self.transform:\n            image = self.transform(image)\n        return label, image\n\n"
        },
        {
          "name": "lr_finder.py",
          "type": "blob",
          "size": 3.5517578125,
          "content": "\nimport argparse\nimport glob\nimport os\n\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\nfrom torchvision import transforms\nfrom conf import settings\nfrom utils import *\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass FindLR(_LRScheduler):\n    \"\"\"exponentially increasing learning rate\n\n    Args:\n        optimizer: optimzier(e.g. SGD)\n        num_iter: totoal_iters\n        max_lr: maximum  learning rate\n    \"\"\"\n    def __init__(self, optimizer, max_lr=10, num_iter=100, last_epoch=-1):\n\n        self.total_iters = num_iter\n        self.max_lr = max_lr\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n\n        return [base_lr * (self.max_lr / base_lr) ** (self.last_epoch / (self.total_iters + 1e-32)) for base_lr in self.base_lrs]\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-net', type=str, required=True, help='net type')\n    parser.add_argument('-b', type=int, default=64, help='batch size for dataloader')\n    parser.add_argument('-base_lr', type=float, default=1e-7, help='min learning rate')\n    parser.add_argument('-max_lr', type=float, default=10, help='max learning rate')\n    parser.add_argument('-num_iter', type=int, default=100, help='num of iteration')\n    parser.add_argument('-gpu', type=bool, default=True, help='use gpu or not')\n    parser.add_argument('-gpus', nargs='+', type=int, default=0, help='gpu device')\n    args = parser.parse_args()\n\n    cifar100_training_loader = get_training_dataloader(\n        settings.CIFAR100_TRAIN_MEAN,\n        settings.CIFAR100_TRAIN_STD,\n        num_workers=4,\n        batch_size=args.b,\n    )\n\n    net = get_network(args)\n\n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=args.base_lr, momentum=0.9, weight_decay=1e-4, nesterov=True)\n\n    #set up warmup phase learning rate scheduler\n    lr_scheduler = FindLR(optimizer, max_lr=args.max_lr, num_iter=args.num_iter)\n    epoches = int(args.num_iter / len(cifar100_training_loader)) + 1\n\n    n = 0\n\n    learning_rate = []\n    losses = []\n    for epoch in range(epoches):\n\n        #training procedure\n        net.train()\n\n        for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n            if n > args.num_iter:\n                break\n\n            lr_scheduler.step()\n\n            images = images.cuda()\n            labels = labels.cuda()\n\n            optimizer.zero_grad()\n            predicts = net(images)\n            loss = loss_function(predicts, labels)\n            if torch.isnan(loss).any():\n                n += 1e8\n                break\n            loss.backward()\n            optimizer.step()\n\n            print('Iterations: {iter_num} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.8f}'.format(\n                loss.item(),\n                optimizer.param_groups[0]['lr'],\n                iter_num=n,\n                trained_samples=batch_index * args.b + len(images),\n                total_samples=len(cifar100_training_loader.dataset),\n            ))\n\n            learning_rate.append(optimizer.param_groups[0]['lr'])\n            losses.append(loss.item())\n            n += 1\n\n    learning_rate = learning_rate[10:-5]\n    losses = losses[10:-5]\n\n    fig, ax = plt.subplots(1,1)\n    ax.plot(learning_rate, losses)\n    ax.set_xlabel('learning rate')\n    ax.set_ylabel('losses')\n    ax.set_xscale('log')\n    ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n\n    fig.savefig('result.jpg')\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 2.251953125,
          "content": "#test.py\n#!/usr/bin/env python3\n\n\"\"\" test neuron network performace\nprint top1 and top5 err on test dataset\nof a model\n\nauthor baiyu\n\"\"\"\n\nimport argparse\n\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nfrom conf import settings\nfrom utils import get_network, get_test_dataloader\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-net', type=str, required=True, help='net type')\n    parser.add_argument('-weights', type=str, required=True, help='the weights file you want to test')\n    parser.add_argument('-gpu', action='store_true', default=False, help='use gpu or not')\n    parser.add_argument('-b', type=int, default=16, help='batch size for dataloader')\n    args = parser.parse_args()\n\n    net = get_network(args)\n\n    cifar100_test_loader = get_test_dataloader(\n        settings.CIFAR100_TRAIN_MEAN,\n        settings.CIFAR100_TRAIN_STD,\n        #settings.CIFAR100_PATH,\n        num_workers=4,\n        batch_size=args.b,\n    )\n\n    net.load_state_dict(torch.load(args.weights))\n    print(net)\n    net.eval()\n\n    correct_1 = 0.0\n    correct_5 = 0.0\n    total = 0\n\n    with torch.no_grad():\n        for n_iter, (image, label) in enumerate(cifar100_test_loader):\n            print(\"iteration: {}\\ttotal {} iterations\".format(n_iter + 1, len(cifar100_test_loader)))\n\n            if args.gpu:\n                image = image.cuda()\n                label = label.cuda()\n                print('GPU INFO.....')\n                print(torch.cuda.memory_summary(), end='')\n\n\n            output = net(image)\n            _, pred = output.topk(5, 1, largest=True, sorted=True)\n\n            label = label.view(label.size(0), -1).expand_as(pred)\n            correct = pred.eq(label).float()\n\n            #compute top 5\n            correct_5 += correct[:, :5].sum()\n\n            #compute top1\n            correct_1 += correct[:, :1].sum()\n\n    if args.gpu:\n        print('GPU INFO.....')\n        print(torch.cuda.memory_summary(), end='')\n\n    print()\n    print(\"Top 1 err: \", 1 - correct_1 / len(cifar100_test_loader.dataset))\n    print(\"Top 5 err: \", 1 - correct_5 / len(cifar100_test_loader.dataset))\n    print(\"Parameter numbers: {}\".format(sum(p.numel() for p in net.parameters())))\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 7.9443359375,
          "content": "# train.py\n#!/usr/bin/env\tpython3\n\n\"\"\" train network using pytorch\n\nauthor baiyu\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport time\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom conf import settings\nfrom utils import get_network, get_training_dataloader, get_test_dataloader, WarmUpLR, \\\n    most_recent_folder, most_recent_weights, last_epoch, best_acc_weights\n\ndef train(epoch):\n\n    start = time.time()\n    net.train()\n    for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n\n        if args.gpu:\n            labels = labels.cuda()\n            images = images.cuda()\n\n        optimizer.zero_grad()\n        outputs = net(images)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        n_iter = (epoch - 1) * len(cifar100_training_loader) + batch_index + 1\n\n        last_layer = list(net.children())[-1]\n        for name, para in last_layer.named_parameters():\n            if 'weight' in name:\n                writer.add_scalar('LastLayerGradients/grad_norm2_weights', para.grad.norm(), n_iter)\n            if 'bias' in name:\n                writer.add_scalar('LastLayerGradients/grad_norm2_bias', para.grad.norm(), n_iter)\n\n        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n            loss.item(),\n            optimizer.param_groups[0]['lr'],\n            epoch=epoch,\n            trained_samples=batch_index * args.b + len(images),\n            total_samples=len(cifar100_training_loader.dataset)\n        ))\n\n        #update training loss for each iteration\n        writer.add_scalar('Train/loss', loss.item(), n_iter)\n\n        if epoch <= args.warm:\n            warmup_scheduler.step()\n\n    for name, param in net.named_parameters():\n        layer, attr = os.path.splitext(name)\n        attr = attr[1:]\n        writer.add_histogram(\"{}/{}\".format(layer, attr), param, epoch)\n\n    finish = time.time()\n\n    print('epoch {} training time consumed: {:.2f}s'.format(epoch, finish - start))\n\n@torch.no_grad()\ndef eval_training(epoch=0, tb=True):\n\n    start = time.time()\n    net.eval()\n\n    test_loss = 0.0 # cost function error\n    correct = 0.0\n\n    for (images, labels) in cifar100_test_loader:\n\n        if args.gpu:\n            images = images.cuda()\n            labels = labels.cuda()\n\n        outputs = net(images)\n        loss = loss_function(outputs, labels)\n\n        test_loss += loss.item()\n        _, preds = outputs.max(1)\n        correct += preds.eq(labels).sum()\n\n    finish = time.time()\n    if args.gpu:\n        print('GPU INFO.....')\n        print(torch.cuda.memory_summary(), end='')\n    print('Evaluating Network.....')\n    print('Test set: Epoch: {}, Average loss: {:.4f}, Accuracy: {:.4f}, Time consumed:{:.2f}s'.format(\n        epoch,\n        test_loss / len(cifar100_test_loader.dataset),\n        correct.float() / len(cifar100_test_loader.dataset),\n        finish - start\n    ))\n    print()\n\n    #add informations to tensorboard\n    if tb:\n        writer.add_scalar('Test/Average loss', test_loss / len(cifar100_test_loader.dataset), epoch)\n        writer.add_scalar('Test/Accuracy', correct.float() / len(cifar100_test_loader.dataset), epoch)\n\n    return correct.float() / len(cifar100_test_loader.dataset)\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-net', type=str, required=True, help='net type')\n    parser.add_argument('-gpu', action='store_true', default=False, help='use gpu or not')\n    parser.add_argument('-b', type=int, default=128, help='batch size for dataloader')\n    parser.add_argument('-warm', type=int, default=1, help='warm up training phase')\n    parser.add_argument('-lr', type=float, default=0.1, help='initial learning rate')\n    parser.add_argument('-resume', action='store_true', default=False, help='resume training')\n    args = parser.parse_args()\n\n    net = get_network(args)\n\n    #data preprocessing:\n    cifar100_training_loader = get_training_dataloader(\n        settings.CIFAR100_TRAIN_MEAN,\n        settings.CIFAR100_TRAIN_STD,\n        num_workers=4,\n        batch_size=args.b,\n        shuffle=True\n    )\n\n    cifar100_test_loader = get_test_dataloader(\n        settings.CIFAR100_TRAIN_MEAN,\n        settings.CIFAR100_TRAIN_STD,\n        num_workers=4,\n        batch_size=args.b,\n        shuffle=True\n    )\n\n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=settings.MILESTONES, gamma=0.2) #learning rate decay\n    iter_per_epoch = len(cifar100_training_loader)\n    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * args.warm)\n\n    if args.resume:\n        recent_folder = most_recent_folder(os.path.join(settings.CHECKPOINT_PATH, args.net), fmt=settings.DATE_FORMAT)\n        if not recent_folder:\n            raise Exception('no recent folder were found')\n\n        checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder)\n\n    else:\n        checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, args.net, settings.TIME_NOW)\n\n    #use tensorboard\n    if not os.path.exists(settings.LOG_DIR):\n        os.mkdir(settings.LOG_DIR)\n\n    #since tensorboard can't overwrite old values\n    #so the only way is to create a new tensorboard log\n    writer = SummaryWriter(log_dir=os.path.join(\n            settings.LOG_DIR, args.net, settings.TIME_NOW))\n    input_tensor = torch.Tensor(1, 3, 32, 32)\n    if args.gpu:\n        input_tensor = input_tensor.cuda()\n    writer.add_graph(net, input_tensor)\n\n    #create checkpoint folder to save model\n    if not os.path.exists(checkpoint_path):\n        os.makedirs(checkpoint_path)\n    checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n\n    best_acc = 0.0\n    if args.resume:\n        best_weights = best_acc_weights(os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder))\n        if best_weights:\n            weights_path = os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder, best_weights)\n            print('found best acc weights file:{}'.format(weights_path))\n            print('load best training file to test acc...')\n            net.load_state_dict(torch.load(weights_path))\n            best_acc = eval_training(tb=False)\n            print('best acc is {:0.2f}'.format(best_acc))\n\n        recent_weights_file = most_recent_weights(os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder))\n        if not recent_weights_file:\n            raise Exception('no recent weights file were found')\n        weights_path = os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder, recent_weights_file)\n        print('loading weights file {} to resume training.....'.format(weights_path))\n        net.load_state_dict(torch.load(weights_path))\n\n        resume_epoch = last_epoch(os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder))\n\n\n    for epoch in range(1, settings.EPOCH + 1):\n        if epoch > args.warm:\n            train_scheduler.step(epoch)\n\n        if args.resume:\n            if epoch <= resume_epoch:\n                continue\n\n        train(epoch)\n        acc = eval_training(epoch)\n\n        #start to save best performance model after learning rate decay to 0.01\n        if epoch > settings.MILESTONES[1] and best_acc < acc:\n            weights_path = checkpoint_path.format(net=args.net, epoch=epoch, type='best')\n            print('saving weights file to {}'.format(weights_path))\n            torch.save(net.state_dict(), weights_path)\n            best_acc = acc\n            continue\n\n        if not epoch % settings.SAVE_EPOCH:\n            weights_path = checkpoint_path.format(net=args.net, epoch=epoch, type='regular')\n            print('saving weights file to {}'.format(weights_path))\n            torch.save(net.state_dict(), weights_path)\n\n    writer.close()\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 10.4501953125,
          "content": "\"\"\" helper function\n\nauthor baiyu\n\"\"\"\nimport os\nimport sys\nimport re\nimport datetime\n\nimport numpy\n\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\ndef get_network(args):\n    \"\"\" return given network\n    \"\"\"\n\n    if args.net == 'vgg16':\n        from models.vgg import vgg16_bn\n        net = vgg16_bn()\n    elif args.net == 'vgg13':\n        from models.vgg import vgg13_bn\n        net = vgg13_bn()\n    elif args.net == 'vgg11':\n        from models.vgg import vgg11_bn\n        net = vgg11_bn()\n    elif args.net == 'vgg19':\n        from models.vgg import vgg19_bn\n        net = vgg19_bn()\n    elif args.net == 'densenet121':\n        from models.densenet import densenet121\n        net = densenet121()\n    elif args.net == 'densenet161':\n        from models.densenet import densenet161\n        net = densenet161()\n    elif args.net == 'densenet169':\n        from models.densenet import densenet169\n        net = densenet169()\n    elif args.net == 'densenet201':\n        from models.densenet import densenet201\n        net = densenet201()\n    elif args.net == 'googlenet':\n        from models.googlenet import googlenet\n        net = googlenet()\n    elif args.net == 'inceptionv3':\n        from models.inceptionv3 import inceptionv3\n        net = inceptionv3()\n    elif args.net == 'inceptionv4':\n        from models.inceptionv4 import inceptionv4\n        net = inceptionv4()\n    elif args.net == 'inceptionresnetv2':\n        from models.inceptionv4 import inception_resnet_v2\n        net = inception_resnet_v2()\n    elif args.net == 'xception':\n        from models.xception import xception\n        net = xception()\n    elif args.net == 'resnet18':\n        from models.resnet import resnet18\n        net = resnet18()\n    elif args.net == 'resnet34':\n        from models.resnet import resnet34\n        net = resnet34()\n    elif args.net == 'resnet50':\n        from models.resnet import resnet50\n        net = resnet50()\n    elif args.net == 'resnet101':\n        from models.resnet import resnet101\n        net = resnet101()\n    elif args.net == 'resnet152':\n        from models.resnet import resnet152\n        net = resnet152()\n    elif args.net == 'preactresnet18':\n        from models.preactresnet import preactresnet18\n        net = preactresnet18()\n    elif args.net == 'preactresnet34':\n        from models.preactresnet import preactresnet34\n        net = preactresnet34()\n    elif args.net == 'preactresnet50':\n        from models.preactresnet import preactresnet50\n        net = preactresnet50()\n    elif args.net == 'preactresnet101':\n        from models.preactresnet import preactresnet101\n        net = preactresnet101()\n    elif args.net == 'preactresnet152':\n        from models.preactresnet import preactresnet152\n        net = preactresnet152()\n    elif args.net == 'resnext50':\n        from models.resnext import resnext50\n        net = resnext50()\n    elif args.net == 'resnext101':\n        from models.resnext import resnext101\n        net = resnext101()\n    elif args.net == 'resnext152':\n        from models.resnext import resnext152\n        net = resnext152()\n    elif args.net == 'shufflenet':\n        from models.shufflenet import shufflenet\n        net = shufflenet()\n    elif args.net == 'shufflenetv2':\n        from models.shufflenetv2 import shufflenetv2\n        net = shufflenetv2()\n    elif args.net == 'squeezenet':\n        from models.squeezenet import squeezenet\n        net = squeezenet()\n    elif args.net == 'mobilenet':\n        from models.mobilenet import mobilenet\n        net = mobilenet()\n    elif args.net == 'mobilenetv2':\n        from models.mobilenetv2 import mobilenetv2\n        net = mobilenetv2()\n    elif args.net == 'nasnet':\n        from models.nasnet import nasnet\n        net = nasnet()\n    elif args.net == 'attention56':\n        from models.attention import attention56\n        net = attention56()\n    elif args.net == 'attention92':\n        from models.attention import attention92\n        net = attention92()\n    elif args.net == 'seresnet18':\n        from models.senet import seresnet18\n        net = seresnet18()\n    elif args.net == 'seresnet34':\n        from models.senet import seresnet34\n        net = seresnet34()\n    elif args.net == 'seresnet50':\n        from models.senet import seresnet50\n        net = seresnet50()\n    elif args.net == 'seresnet101':\n        from models.senet import seresnet101\n        net = seresnet101()\n    elif args.net == 'seresnet152':\n        from models.senet import seresnet152\n        net = seresnet152()\n    elif args.net == 'wideresnet':\n        from models.wideresidual import wideresnet\n        net = wideresnet()\n    elif args.net == 'stochasticdepth18':\n        from models.stochasticdepth import stochastic_depth_resnet18\n        net = stochastic_depth_resnet18()\n    elif args.net == 'stochasticdepth34':\n        from models.stochasticdepth import stochastic_depth_resnet34\n        net = stochastic_depth_resnet34()\n    elif args.net == 'stochasticdepth50':\n        from models.stochasticdepth import stochastic_depth_resnet50\n        net = stochastic_depth_resnet50()\n    elif args.net == 'stochasticdepth101':\n        from models.stochasticdepth import stochastic_depth_resnet101\n        net = stochastic_depth_resnet101()\n\n    else:\n        print('the network name you have entered is not supported yet')\n        sys.exit()\n\n    if args.gpu: #use_gpu\n        net = net.cuda()\n\n    return net\n\n\ndef get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n    \"\"\" return training dataloader\n    Args:\n        mean: mean of cifar100 training dataset\n        std: std of cifar100 training dataset\n        path: path to cifar100 training python dataset\n        batch_size: dataloader batchsize\n        num_workers: dataloader num_works\n        shuffle: whether to shuffle\n    Returns: train_data_loader:torch dataloader object\n    \"\"\"\n\n    transform_train = transforms.Compose([\n        #transforms.ToPILImage(),\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    #cifar100_training = CIFAR100Train(path, transform=transform_train)\n    cifar100_training = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n    cifar100_training_loader = DataLoader(\n        cifar100_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n\n    return cifar100_training_loader\n\ndef get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n    \"\"\" return training dataloader\n    Args:\n        mean: mean of cifar100 test dataset\n        std: std of cifar100 test dataset\n        path: path to cifar100 test python dataset\n        batch_size: dataloader batchsize\n        num_workers: dataloader num_works\n        shuffle: whether to shuffle\n    Returns: cifar100_test_loader:torch dataloader object\n    \"\"\"\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    #cifar100_test = CIFAR100Test(path, transform=transform_test)\n    cifar100_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n    cifar100_test_loader = DataLoader(\n        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n\n    return cifar100_test_loader\n\ndef compute_mean_std(cifar100_dataset):\n    \"\"\"compute the mean and std of cifar100 dataset\n    Args:\n        cifar100_training_dataset or cifar100_test_dataset\n        witch derived from class torch.utils.data\n\n    Returns:\n        a tuple contains mean, std value of entire dataset\n    \"\"\"\n\n    data_r = numpy.dstack([cifar100_dataset[i][1][:, :, 0] for i in range(len(cifar100_dataset))])\n    data_g = numpy.dstack([cifar100_dataset[i][1][:, :, 1] for i in range(len(cifar100_dataset))])\n    data_b = numpy.dstack([cifar100_dataset[i][1][:, :, 2] for i in range(len(cifar100_dataset))])\n    mean = numpy.mean(data_r), numpy.mean(data_g), numpy.mean(data_b)\n    std = numpy.std(data_r), numpy.std(data_g), numpy.std(data_b)\n\n    return mean, std\n\nclass WarmUpLR(_LRScheduler):\n    \"\"\"warmup_training learning rate scheduler\n    Args:\n        optimizer: optimzier(e.g. SGD)\n        total_iters: totoal_iters of warmup phase\n    \"\"\"\n    def __init__(self, optimizer, total_iters, last_epoch=-1):\n\n        self.total_iters = total_iters\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \"\"\"we will use the first m batches, and set the learning\n        rate to base_lr * m / total_iters\n        \"\"\"\n        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n\n\ndef most_recent_folder(net_weights, fmt):\n    \"\"\"\n        return most recent created folder under net_weights\n        if no none-empty folder were found, return empty folder\n    \"\"\"\n    # get subfolders in net_weights\n    folders = os.listdir(net_weights)\n\n    # filter out empty folders\n    folders = [f for f in folders if len(os.listdir(os.path.join(net_weights, f)))]\n    if len(folders) == 0:\n        return ''\n\n    # sort folders by folder created time\n    folders = sorted(folders, key=lambda f: datetime.datetime.strptime(f, fmt))\n    return folders[-1]\n\ndef most_recent_weights(weights_folder):\n    \"\"\"\n        return most recent created weights file\n        if folder is empty return empty string\n    \"\"\"\n    weight_files = os.listdir(weights_folder)\n    if len(weights_folder) == 0:\n        return ''\n\n    regex_str = r'([A-Za-z0-9]+)-([0-9]+)-(regular|best)'\n\n    # sort files by epoch\n    weight_files = sorted(weight_files, key=lambda w: int(re.search(regex_str, w).groups()[1]))\n\n    return weight_files[-1]\n\ndef last_epoch(weights_folder):\n    weight_file = most_recent_weights(weights_folder)\n    if not weight_file:\n       raise Exception('no recent weights were found')\n    resume_epoch = int(weight_file.split('-')[1])\n\n    return resume_epoch\n\ndef best_acc_weights(weights_folder):\n    \"\"\"\n        return the best acc .pth file in given folder, if no\n        best acc weights file were found, return empty string\n    \"\"\"\n    files = os.listdir(weights_folder)\n    if len(files) == 0:\n        return ''\n\n    regex_str = r'([A-Za-z0-9]+)-([0-9]+)-(regular|best)'\n    best_files = [w for w in files if re.search(regex_str, w).groups()[2] == 'best']\n    if len(best_files) == 0:\n        return ''\n\n    best_files = sorted(best_files, key=lambda w: int(re.search(regex_str, w).groups()[1]))\n    return best_files[-1]"
        }
      ]
    }
  ]
}