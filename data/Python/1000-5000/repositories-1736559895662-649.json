{
  "metadata": {
    "timestamp": 1736559895662,
    "page": 649,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "YunYang1994/tensorflow-yolov3",
      "stars": 3633,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.341796875,
          "content": "# Build and Release Folders\nbin-debug/\nbin-release/\n[Oo]bj/\n[Bb]in/\n\n# Other files and folders\n.settings/\n\n# Executables\n*.swf\n*.air\n*.ipa\n*.apk\n\n# Project files, i.e. `.project`, `.actionScriptProperties` and `.flexProperties`\n# should NOT be excluded as they contain compiler settings and other important\n# information for Eclipse / Flash Builder.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0390625,
          "content": "MIT License\n\nCopyright (c) 2019 YangYun\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "LICENSE.fuck",
          "type": "blob",
          "size": 0.4951171875,
          "content": "           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n                   Version 1, JUNE 2019\n\nCopyright (C) 2019 YunYang1994 <dreameryangyun@sjtu.edu.cn>\n\nEveryone is permitted to copy and distribute verbatim or modified\ncopies of this license document, and changing it is allowed as long\nas the name is changed.\n\n           DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n  TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n\n 0. You just DO WHAT THE FUCK YOU WANT TO. DON'T ASK ME, JUST DO IT.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.591796875,
          "content": "\n## üÜï Are you looking for a new YOLOv3 implemented by TF2.0 ?\n\n>If you hate the fucking tensorflow1.x very much, no worries! I have implemented **a new YOLOv3 repo with TF2.0**, and also made a chinese blog on how to implement YOLOv3 object detector from scratch. <br>\n[code](https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3) | [blog](https://yunyang1994.gitee.io/2018/12/28/YOLOv3-ÁÆóÊ≥ïÁöÑ‰∏ÄÁÇπÁêÜËß£/)  | [issue](https://github.com/YunYang1994/tensorflow-yolov3/issues/39)\n\n## part 1. Quick start\n1. Clone this file\n```bashrc\n$ git clone https://github.com/YunYang1994/tensorflow-yolov3.git\n```\n2.  You are supposed  to install some dependencies before getting out hands with these codes.\n```bashrc\n$ cd tensorflow-yolov3\n$ pip install -r ./docs/requirements.txt\n```\n3. Exporting loaded COCO weights as TF checkpoint(`yolov3_coco.ckpt`)„Äê[BaiduCloud](https://pan.baidu.com/s/11mwiUy8KotjUVQXqkGGPFQ&shfl=sharepset)„Äë\n```bashrc\n$ cd checkpoint\n$ wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz\n$ tar -xvf yolov3_coco.tar.gz\n$ cd ..\n$ python convert_weight.py\n$ python freeze_graph.py\n```\n4. Then you will get some `.pb` files in the root path.,  and run the demo script\n```bashrc\n$ python image_demo.py\n$ python video_demo.py # if use camera, set video_path = 0\n```\n<p align=\"center\">\n    <img width=\"100%\" src=\"https://user-images.githubusercontent.com/30433053/68088581-9255e700-fe9b-11e9-8672-2672ab398abe.jpg\" style=\"max-width:100%;\">\n    </a>\n</p>\n\n## part 2. Train your own dataset\nTwo files are required as follows:\n\n- [`dataset.txt`](https://raw.githubusercontent.com/YunYang1994/tensorflow-yolov3/master/data/dataset/voc_train.txt): \n\n```\nxxx/xxx.jpg 18.19,6.32,424.13,421.83,20 323.86,2.65,640.0,421.94,20 \nxxx/xxx.jpg 48,240,195,371,11 8,12,352,498,14\n# image_path x_min, y_min, x_max, y_max, class_id  x_min, y_min ,..., class_id \n# make sure that x_max < width and y_max < height\n```\n\n- [`class.names`](https://github.com/YunYang1994/tensorflow-yolov3/blob/master/data/classes/coco.names):\n\n```\nperson\nbicycle\ncar\n...\ntoothbrush\n```\n\n### 2.1 Train on VOC dataset\nDownload VOC PASCAL trainval  and test data\n```bashrc\n$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n```\nExtract all of these tars into one directory and rename them, which should have the following basic structure.\n\n```bashrc\n\nVOC           # path:  /home/yang/dataset/VOC\n‚îú‚îÄ‚îÄ test\n|    ‚îî‚îÄ‚îÄVOCdevkit\n|        ‚îî‚îÄ‚îÄVOC2007 (from VOCtest_06-Nov-2007.tar)\n‚îî‚îÄ‚îÄ train\n     ‚îî‚îÄ‚îÄVOCdevkit\n         ‚îî‚îÄ‚îÄVOC2007 (from VOCtrainval_06-Nov-2007.tar)\n         ‚îî‚îÄ‚îÄVOC2012 (from VOCtrainval_11-May-2012.tar)\n                     \n$ python scripts/voc_annotation.py --data_path /home/yang/test/VOC\n```\nThen edit your `./core/config.py` to make some necessary configurations\n\n```bashrc\n__C.YOLO.CLASSES                = \"./data/classes/voc.names\"\n__C.TRAIN.ANNOT_PATH            = \"./data/dataset/voc_train.txt\"\n__C.TEST.ANNOT_PATH             = \"./data/dataset/voc_test.txt\"\n```\nHere are two kinds of training method: \n\n##### (1) train from scratch:\n\n```bashrc\n$ python train.py\n$ tensorboard --logdir ./data\n```\n##### (2) train from COCO weights(recommend):\n\n```bashrc\n$ cd checkpoint\n$ wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz\n$ tar -xvf yolov3_coco.tar.gz\n$ cd ..\n$ python convert_weight.py --train_from_coco\n$ python train.py\n```\n### 2.2 Evaluate on VOC dataset\n\n```\n$ python evaluate.py\n$ cd mAP\n$ python main.py -na\n```\n\nthe mAP on the VOC2012 dataset:\n\n<p align=\"center\">\n    <img width=\"50%\" src=\"https://user-images.githubusercontent.com/33013904/58227054-dd4fc800-7d5b-11e9-85aa-67854292fbe0.png\" style=\"max-width:50%;\">\n    </a>\n</p>\n\n\n## part 3. Other Implementations\n\n[-**`YOLOv3ÁõÆÊ†áÊ£ÄÊµãÊúâ‰∫ÜTensorFlowÂÆûÁé∞ÔºåÂèØÁî®Ëá™Â∑±ÁöÑÊï∞ÊçÆÊù•ËÆ≠ÁªÉ`**](https://mp.weixin.qq.com/s/cq7g1-4oFTftLbmKcpi_aQ)<br>\n\n[-**`Stronger-yolo`**](https://github.com/Stinky-Tofu/Stronger-yolo)<br>\n\n[- **`Implementing YOLO v3 in Tensorflow (TF-Slim)`**](https://itnext.io/implementing-yolo-v3-in-tensorflow-tf-slim-c3c55ff59dbe)\n\n[- **`YOLOv3_TensorFlow`**](https://github.com/wizyoung/YOLOv3_TensorFlow)\n\n[- **`Object Detection using YOLOv2 on Pascal VOC2012`**](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html)\n\n[-**`Understanding YOLO`**](https://hackernoon.com/understanding-yolo-f5a74bbc7967)\n\n"
        },
        {
          "name": "checkpoint",
          "type": "tree",
          "content": null
        },
        {
          "name": "convert_weight.py",
          "type": "blob",
          "size": 3.2978515625,
          "content": "#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : convert_weight.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 13:51:31\n#   Description :\n#\n#================================================================\n\nimport argparse\nimport tensorflow as tf\nfrom core.yolov3 import YOLOV3\nfrom core.config import cfg\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--train_from_coco\", action='store_true')\nflag = parser.parse_args()\n\norg_weights_path = cfg.YOLO.ORIGINAL_WEIGHT\ncur_weights_path = cfg.YOLO.DEMO_WEIGHT\npreserve_cur_names = ['conv_sbbox', 'conv_mbbox', 'conv_lbbox']\npreserve_org_names = ['Conv_6', 'Conv_14', 'Conv_22']\n\n\norg_weights_mess = []\ntf.Graph().as_default()\nload = tf.train.import_meta_graph(org_weights_path + '.meta')\nwith tf.Session() as sess:\n    load.restore(sess, org_weights_path)\n    for var in tf.global_variables():\n        var_name = var.op.name\n        var_name_mess = str(var_name).split('/')\n        var_shape = var.shape\n        if flag.train_from_coco:\n            if (var_name_mess[-1] not in ['weights', 'gamma', 'beta', 'moving_mean', 'moving_variance']) or \\\n                    (var_name_mess[1] == 'yolo-v3' and (var_name_mess[-2] in preserve_org_names)): continue\n        org_weights_mess.append([var_name, var_shape])\n        print(\"=> \" + str(var_name).ljust(50), var_shape)\nprint()\ntf.reset_default_graph()\n\ncur_weights_mess = []\ntf.Graph().as_default()\nwith tf.name_scope('input'):\n    input_data = tf.placeholder(dtype=tf.float32, shape=(1, 416, 416, 3), name='input_data')\n    training = tf.placeholder(dtype=tf.bool, name='trainable')\nmodel = YOLOV3(input_data, training)\nfor var in tf.global_variables():\n    var_name = var.op.name\n    var_name_mess = str(var_name).split('/')\n    var_shape = var.shape\n    print(var_name_mess[0])\n    if flag.train_from_coco:\n        if var_name_mess[0] in preserve_cur_names: continue\n    cur_weights_mess.append([var_name, var_shape])\n    print(\"=> \" + str(var_name).ljust(50), var_shape)\n\norg_weights_num = len(org_weights_mess)\ncur_weights_num = len(cur_weights_mess)\nif cur_weights_num != org_weights_num:\n    raise RuntimeError\n\nprint('=> Number of weights that will rename:\\t%d' % cur_weights_num)\ncur_to_org_dict = {}\nfor index in range(org_weights_num):\n    org_name, org_shape = org_weights_mess[index]\n    cur_name, cur_shape = cur_weights_mess[index]\n    if cur_shape != org_shape:\n        print(org_weights_mess[index])\n        print(cur_weights_mess[index])\n        raise RuntimeError\n    cur_to_org_dict[cur_name] = org_name\n    print(\"=> \" + str(cur_name).ljust(50) + ' : ' + org_name)\n\nwith tf.name_scope('load_save'):\n    name_to_var_dict = {var.op.name: var for var in tf.global_variables()}\n    restore_dict = {cur_to_org_dict[cur_name]: name_to_var_dict[cur_name] for cur_name in cur_to_org_dict}\n    load = tf.train.Saver(restore_dict)\n    save = tf.train.Saver(tf.global_variables())\n    for var in tf.global_variables():\n        print(\"=> \" + var.op.name)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print('=> Restoring weights from:\\t %s' % org_weights_path)\n    load.restore(sess, org_weights_path)\n    save.save(sess, cur_weights_path)\ntf.reset_default_graph()\n\n\n"
        },
        {
          "name": "core",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 6.98828125,
          "content": "#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : evaluate.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-21 15:30:26\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom core.config import cfg\nfrom core.yolov3 import YOLOV3\n\nclass YoloTest(object):\n    def __init__(self):\n        self.input_size       = cfg.TEST.INPUT_SIZE\n        self.anchor_per_scale = cfg.YOLO.ANCHOR_PER_SCALE\n        self.classes          = utils.read_class_names(cfg.YOLO.CLASSES)\n        self.num_classes      = len(self.classes)\n        self.anchors          = np.array(utils.get_anchors(cfg.YOLO.ANCHORS))\n        self.score_threshold  = cfg.TEST.SCORE_THRESHOLD\n        self.iou_threshold    = cfg.TEST.IOU_THRESHOLD\n        self.moving_ave_decay = cfg.YOLO.MOVING_AVE_DECAY\n        self.annotation_path  = cfg.TEST.ANNOT_PATH\n        self.weight_file      = cfg.TEST.WEIGHT_FILE\n        self.write_image      = cfg.TEST.WRITE_IMAGE\n        self.write_image_path = cfg.TEST.WRITE_IMAGE_PATH\n        self.show_label       = cfg.TEST.SHOW_LABEL\n\n        with tf.name_scope('input'):\n            self.input_data = tf.placeholder(dtype=tf.float32, name='input_data')\n            self.trainable  = tf.placeholder(dtype=tf.bool,    name='trainable')\n\n        model = YOLOV3(self.input_data, self.trainable)\n        self.pred_sbbox, self.pred_mbbox, self.pred_lbbox = model.pred_sbbox, model.pred_mbbox, model.pred_lbbox\n\n        with tf.name_scope('ema'):\n            ema_obj = tf.train.ExponentialMovingAverage(self.moving_ave_decay)\n\n        self.sess  = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n        self.saver = tf.train.Saver(ema_obj.variables_to_restore())\n        self.saver.restore(self.sess, self.weight_file)\n\n    def predict(self, image):\n\n        org_image = np.copy(image)\n        org_h, org_w, _ = org_image.shape\n\n        image_data = utils.image_preporcess(image, [self.input_size, self.input_size])\n        image_data = image_data[np.newaxis, ...]\n\n        pred_sbbox, pred_mbbox, pred_lbbox = self.sess.run(\n            [self.pred_sbbox, self.pred_mbbox, self.pred_lbbox],\n            feed_dict={\n                self.input_data: image_data,\n                self.trainable: False\n            }\n        )\n\n        pred_bbox = np.concatenate([np.reshape(pred_sbbox, (-1, 5 + self.num_classes)),\n                                    np.reshape(pred_mbbox, (-1, 5 + self.num_classes)),\n                                    np.reshape(pred_lbbox, (-1, 5 + self.num_classes))], axis=0)\n        bboxes = utils.postprocess_boxes(pred_bbox, (org_h, org_w), self.input_size, self.score_threshold)\n        bboxes = utils.nms(bboxes, self.iou_threshold)\n\n        return bboxes\n\n    def evaluate(self):\n        predicted_dir_path = './mAP/predicted'\n        ground_truth_dir_path = './mAP/ground-truth'\n        if os.path.exists(predicted_dir_path): shutil.rmtree(predicted_dir_path)\n        if os.path.exists(ground_truth_dir_path): shutil.rmtree(ground_truth_dir_path)\n        if os.path.exists(self.write_image_path): shutil.rmtree(self.write_image_path)\n        os.mkdir(predicted_dir_path)\n        os.mkdir(ground_truth_dir_path)\n        os.mkdir(self.write_image_path)\n\n        with open(self.annotation_path, 'r') as annotation_file:\n            for num, line in enumerate(annotation_file):\n                annotation = line.strip().split()\n                image_path = annotation[0]\n                image_name = image_path.split('/')[-1]\n                image = cv2.imread(image_path)\n                bbox_data_gt = np.array([list(map(int, box.split(','))) for box in annotation[1:]])\n\n                if len(bbox_data_gt) == 0:\n                    bboxes_gt=[]\n                    classes_gt=[]\n                else:\n                    bboxes_gt, classes_gt = bbox_data_gt[:, :4], bbox_data_gt[:, 4]\n                ground_truth_path = os.path.join(ground_truth_dir_path, str(num) + '.txt')\n\n                print('=> ground truth of %s:' % image_name)\n                num_bbox_gt = len(bboxes_gt)\n                with open(ground_truth_path, 'w') as f:\n                    for i in range(num_bbox_gt):\n                        class_name = self.classes[classes_gt[i]]\n                        xmin, ymin, xmax, ymax = list(map(str, bboxes_gt[i]))\n                        bbox_mess = ' '.join([class_name, xmin, ymin, xmax, ymax]) + '\\n'\n                        f.write(bbox_mess)\n                        print('\\t' + str(bbox_mess).strip())\n                print('=> predict result of %s:' % image_name)\n                predict_result_path = os.path.join(predicted_dir_path, str(num) + '.txt')\n                bboxes_pr = self.predict(image)\n\n                if self.write_image:\n                    image = utils.draw_bbox(image, bboxes_pr, show_label=self.show_label)\n                    cv2.imwrite(self.write_image_path+image_name, image)\n\n                with open(predict_result_path, 'w') as f:\n                    for bbox in bboxes_pr:\n                        coor = np.array(bbox[:4], dtype=np.int32)\n                        score = bbox[4]\n                        class_ind = int(bbox[5])\n                        class_name = self.classes[class_ind]\n                        score = '%.4f' % score\n                        xmin, ymin, xmax, ymax = list(map(str, coor))\n                        bbox_mess = ' '.join([class_name, score, xmin, ymin, xmax, ymax]) + '\\n'\n                        f.write(bbox_mess)\n                        print('\\t' + str(bbox_mess).strip())\n\n    def voc_2012_test(self, voc2012_test_path):\n\n        img_inds_file = os.path.join(voc2012_test_path, 'ImageSets', 'Main', 'test.txt')\n        with open(img_inds_file, 'r') as f:\n            txt = f.readlines()\n            image_inds = [line.strip() for line in txt]\n\n        results_path = 'results/VOC2012/Main'\n        if os.path.exists(results_path):\n            shutil.rmtree(results_path)\n        os.makedirs(results_path)\n\n        for image_ind in image_inds:\n            image_path = os.path.join(voc2012_test_path, 'JPEGImages', image_ind + '.jpg')\n            image = cv2.imread(image_path)\n\n            print('predict result of %s:' % image_ind)\n            bboxes_pr = self.predict(image)\n            for bbox in bboxes_pr:\n                coor = np.array(bbox[:4], dtype=np.int32)\n                score = bbox[4]\n                class_ind = int(bbox[5])\n                class_name = self.classes[class_ind]\n                score = '%.4f' % score\n                xmin, ymin, xmax, ymax = list(map(str, coor))\n                bbox_mess = ' '.join([image_ind, score, xmin, ymin, xmax, ymax]) + '\\n'\n                with open(os.path.join(results_path, 'comp4_det_test_' + class_name + '.txt'), 'a') as f:\n                    f.write(bbox_mess)\n                print('\\t' + str(bbox_mess).strip())\n\n\nif __name__ == '__main__': YoloTest().evaluate()\n\n\n\n"
        },
        {
          "name": "freeze_graph.py",
          "type": "blob",
          "size": 1.2275390625,
          "content": "#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : freeze_graph.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-20 15:57:33\n#   Description :\n#\n#================================================================\n\n\nimport tensorflow as tf\nfrom core.yolov3 import YOLOV3\n\npb_file = \"./yolov3_coco.pb\"\nckpt_file = \"./checkpoint/yolov3_coco_demo.ckpt\"\noutput_node_names = [\"input/input_data\", \"pred_sbbox/concat_2\", \"pred_mbbox/concat_2\", \"pred_lbbox/concat_2\"]\n\nwith tf.name_scope('input'):\n    input_data = tf.placeholder(dtype=tf.float32, name='input_data')\n\nmodel = YOLOV3(input_data, trainable=False)\nprint(model.conv_sbbox, model.conv_mbbox, model.conv_lbbox)\n\nsess  = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\nsaver = tf.train.Saver()\nsaver.restore(sess, ckpt_file)\n\nconverted_graph_def = tf.graph_util.convert_variables_to_constants(sess,\n                            input_graph_def  = sess.graph.as_graph_def(),\n                            output_node_names = output_node_names)\n\nwith tf.gfile.GFile(pb_file, \"wb\") as f:\n    f.write(converted_graph_def.SerializeToString())\n\n\n\n\n"
        },
        {
          "name": "from_darknet_weights_to_ckpt.py",
          "type": "blob",
          "size": 2.90234375,
          "content": "import tensorflow as tf\nfrom core.yolov3 import YOLOV3\n\niput_size = 416\ndarknet_weights = '<your yolov3.weights' path>'\nckpt_file = './checkpoint/yolov3_coco.ckpt'\n\ndef load_weights(var_list, weights_file):\n    \"\"\"\n    Loads and converts pre-trained weights.\n    :param var_list: list of network variables.\n    :param weights_file: name of the binary file.\n    :return: list of assign ops\n    \"\"\"\n    with open(weights_file, \"rb\") as fp:\n        _ = np.fromfile(fp, dtype=np.int32, count=5)\n        weights = np.fromfile(fp, dtype=np.float32)  # np.ndarray\n    print('weights_num:', weights.shape[0])\n    ptr = 0\n    i = 0\n    assign_ops = []\n    while i < len(var_list) - 1:\n        var1 = var_list[i]\n        var2 = var_list[i + 1]\n        # do something only if we process conv layer\n        if 'conv' in var1.name.split('/')[-2]:\n            # check type of next layer\n            if 'batch_normalization' in var2.name.split('/')[-2]:\n                # load batch norm params\n                gamma, beta, mean, var = var_list[i + 1:i + 5]\n                batch_norm_vars = [beta, gamma, mean, var]\n                for vari in batch_norm_vars:\n                    shape = vari.shape.as_list()\n                    num_params = np.prod(shape)\n                    vari_weights = weights[ptr:ptr + num_params].reshape(shape)\n                    ptr += num_params\n                    assign_ops.append(\n                        tf.assign(vari, vari_weights, validate_shape=True))\n                i += 4\n            elif 'conv' in var2.name.split('/')[-2]:\n                # load biases\n                bias = var2\n                bias_shape = bias.shape.as_list()\n                bias_params = np.prod(bias_shape)\n                bias_weights = weights[ptr:ptr +\n                                           bias_params].reshape(bias_shape)\n                ptr += bias_params\n                assign_ops.append(\n                    tf.assign(bias, bias_weights, validate_shape=True))\n                i += 1\n            shape = var1.shape.as_list()\n            num_params = np.prod(shape)\n\n            var_weights = weights[ptr:ptr + num_params].reshape(\n                (shape[3], shape[2], shape[0], shape[1]))\n            # remember to transpose to column-major\n            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n            ptr += num_params\n            assign_ops.append(\n                tf.assign(var1, var_weights, validate_shape=True))\n            i += 1\n    print('ptr:', ptr)\n    return assign_ops\n    \nwith tf.name_scope('input'):\n    input_data = tf.placeholder(dtype=tf.float32,shape=(None, iput_size, iput_size, 3), name='input_data')\nmodel = YOLOV3(input_data, trainable=False)\nload_ops = load_weights(tf.global_variables(), darknet_weights)\n\nsaver = tf.train.Saver(tf.global_variables())\n\nwith tf.Session() as sess:\n    sess.run(load_ops)\n    save_path = saver.save(sess, save_path=ckpt_file)\n    print('Model saved in path: {}'.format(save_path))\n"
        },
        {
          "name": "from_darknet_weights_to_pb.py",
          "type": "blob",
          "size": 0.9599609375,
          "content": "import tensorflow as tf\nfrom core.yolov3 import YOLOV3\nfrom from_darknet_weights_to_ckpt import load_weights\n\ninput_size = 416\ndarknet_weights = '<your darknet weights file path>'\npb_file = './yolov3.pb'\noutput_node_names = [\"input/input_data\", \"pred_sbbox/concat_2\", \"pred_mbbox/concat_2\", \"pred_lbbox/concat_2\"]\n\nwith tf.name_scope('input'):\n    input_data = tf.placeholder(dtype=tf.float32, shape=(None, input_size, input_size, 3), name='input_data')\nmodel = YOLOV3(input_data, trainable=False)\nload_ops = load_weights(tf.global_variables(), darknet_weights)\n\nwith tf.Session() as sess:\n    sess.run(load_ops)\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess,\n        tf.get_default_graph().as_graph_def(),\n        output_node_names=output_node_names\n    )\n\n    with tf.gfile.GFile(output_graph, \"wb\") as f:\n        f.write(output_graph_def.SerializeToString())\n\n    print(\"{} ops written to {}.\".format(len(output_graph_def.node), output_graph))\n"
        },
        {
          "name": "image_demo.py",
          "type": "blob",
          "size": 1.7373046875,
          "content": "#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : image_demo.py\n#   Author      : YunYang1994\n#   Created date: 2019-01-20 16:06:06\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport numpy as np\nimport core.utils as utils\nimport tensorflow as tf\nfrom PIL import Image\n\nreturn_elements = [\"input/input_data:0\", \"pred_sbbox/concat_2:0\", \"pred_mbbox/concat_2:0\", \"pred_lbbox/concat_2:0\"]\npb_file         = \"./yolov3_coco.pb\"\nimage_path      = \"./docs/images/road.jpeg\"\nnum_classes     = 80\ninput_size      = 416\ngraph           = tf.Graph()\n\noriginal_image = cv2.imread(image_path)\noriginal_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\noriginal_image_size = original_image.shape[:2]\nimage_data = utils.image_preporcess(np.copy(original_image), [input_size, input_size])\nimage_data = image_data[np.newaxis, ...]\n\nreturn_tensors = utils.read_pb_return_tensors(graph, pb_file, return_elements)\n\n\nwith tf.Session(graph=graph) as sess:\n    pred_sbbox, pred_mbbox, pred_lbbox = sess.run(\n        [return_tensors[1], return_tensors[2], return_tensors[3]],\n                feed_dict={ return_tensors[0]: image_data})\n\npred_bbox = np.concatenate([np.reshape(pred_sbbox, (-1, 5 + num_classes)),\n                            np.reshape(pred_mbbox, (-1, 5 + num_classes)),\n                            np.reshape(pred_lbbox, (-1, 5 + num_classes))], axis=0)\n\nbboxes = utils.postprocess_boxes(pred_bbox, original_image_size, input_size, 0.3)\nbboxes = utils.nms(bboxes, 0.45, method='nms')\nimage = utils.draw_bbox(original_image, bboxes)\nimage = Image.fromarray(image)\nimage.show()\n\n\n\n\n"
        },
        {
          "name": "mAP",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 9.5400390625,
          "content": "#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : train.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 17:50:26\n#   Description :\n#\n#================================================================\n\nimport os\nimport time\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom tqdm import tqdm\nfrom core.dataset import Dataset\nfrom core.yolov3 import YOLOV3\nfrom core.config import cfg\n\n\nclass YoloTrain(object):\n    def __init__(self):\n        self.anchor_per_scale    = cfg.YOLO.ANCHOR_PER_SCALE\n        self.classes             = utils.read_class_names(cfg.YOLO.CLASSES)\n        self.num_classes         = len(self.classes)\n        self.learn_rate_init     = cfg.TRAIN.LEARN_RATE_INIT\n        self.learn_rate_end      = cfg.TRAIN.LEARN_RATE_END\n        self.first_stage_epochs  = cfg.TRAIN.FISRT_STAGE_EPOCHS\n        self.second_stage_epochs = cfg.TRAIN.SECOND_STAGE_EPOCHS\n        self.warmup_periods      = cfg.TRAIN.WARMUP_EPOCHS\n        self.initial_weight      = cfg.TRAIN.INITIAL_WEIGHT\n        self.time                = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime(time.time()))\n        self.moving_ave_decay    = cfg.YOLO.MOVING_AVE_DECAY\n        self.max_bbox_per_scale  = 150\n        self.train_logdir        = \"./data/log/train\"\n        self.trainset            = Dataset('train')\n        self.testset             = Dataset('test')\n        self.steps_per_period    = len(self.trainset)\n        self.sess                = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\n        with tf.name_scope('define_input'):\n            self.input_data   = tf.placeholder(dtype=tf.float32, name='input_data')\n            self.label_sbbox  = tf.placeholder(dtype=tf.float32, name='label_sbbox')\n            self.label_mbbox  = tf.placeholder(dtype=tf.float32, name='label_mbbox')\n            self.label_lbbox  = tf.placeholder(dtype=tf.float32, name='label_lbbox')\n            self.true_sbboxes = tf.placeholder(dtype=tf.float32, name='sbboxes')\n            self.true_mbboxes = tf.placeholder(dtype=tf.float32, name='mbboxes')\n            self.true_lbboxes = tf.placeholder(dtype=tf.float32, name='lbboxes')\n            self.trainable     = tf.placeholder(dtype=tf.bool, name='training')\n\n        with tf.name_scope(\"define_loss\"):\n            self.model = YOLOV3(self.input_data, self.trainable)\n            self.net_var = tf.global_variables()\n            self.giou_loss, self.conf_loss, self.prob_loss = self.model.compute_loss(\n                                                    self.label_sbbox,  self.label_mbbox,  self.label_lbbox,\n                                                    self.true_sbboxes, self.true_mbboxes, self.true_lbboxes)\n            self.loss = self.giou_loss + self.conf_loss + self.prob_loss\n\n        with tf.name_scope('learn_rate'):\n            self.global_step = tf.Variable(1.0, dtype=tf.float64, trainable=False, name='global_step')\n            warmup_steps = tf.constant(self.warmup_periods * self.steps_per_period,\n                                        dtype=tf.float64, name='warmup_steps')\n            train_steps = tf.constant( (self.first_stage_epochs + self.second_stage_epochs)* self.steps_per_period,\n                                        dtype=tf.float64, name='train_steps')\n            self.learn_rate = tf.cond(\n                pred=self.global_step < warmup_steps,\n                true_fn=lambda: self.global_step / warmup_steps * self.learn_rate_init,\n                false_fn=lambda: self.learn_rate_end + 0.5 * (self.learn_rate_init - self.learn_rate_end) *\n                                    (1 + tf.cos(\n                                        (self.global_step - warmup_steps) / (train_steps - warmup_steps) * np.pi))\n            )\n            global_step_update = tf.assign_add(self.global_step, 1.0)\n\n        with tf.name_scope(\"define_weight_decay\"):\n            moving_ave = tf.train.ExponentialMovingAverage(self.moving_ave_decay).apply(tf.trainable_variables())\n\n        with tf.name_scope(\"define_first_stage_train\"):\n            self.first_stage_trainable_var_list = []\n            for var in tf.trainable_variables():\n                var_name = var.op.name\n                var_name_mess = str(var_name).split('/')\n                if var_name_mess[0] in ['conv_sbbox', 'conv_mbbox', 'conv_lbbox']:\n                    self.first_stage_trainable_var_list.append(var)\n\n            first_stage_optimizer = tf.train.AdamOptimizer(self.learn_rate).minimize(self.loss,\n                                                      var_list=self.first_stage_trainable_var_list)\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                with tf.control_dependencies([first_stage_optimizer, global_step_update]):\n                    with tf.control_dependencies([moving_ave]):\n                        self.train_op_with_frozen_variables = tf.no_op()\n\n        with tf.name_scope(\"define_second_stage_train\"):\n            second_stage_trainable_var_list = tf.trainable_variables()\n            second_stage_optimizer = tf.train.AdamOptimizer(self.learn_rate).minimize(self.loss,\n                                                      var_list=second_stage_trainable_var_list)\n\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                with tf.control_dependencies([second_stage_optimizer, global_step_update]):\n                    with tf.control_dependencies([moving_ave]):\n                        self.train_op_with_all_variables = tf.no_op()\n\n        with tf.name_scope('loader_and_saver'):\n            self.loader = tf.train.Saver(self.net_var)\n            self.saver  = tf.train.Saver(tf.global_variables(), max_to_keep=10)\n\n        with tf.name_scope('summary'):\n            tf.summary.scalar(\"learn_rate\",      self.learn_rate)\n            tf.summary.scalar(\"giou_loss\",  self.giou_loss)\n            tf.summary.scalar(\"conf_loss\",  self.conf_loss)\n            tf.summary.scalar(\"prob_loss\",  self.prob_loss)\n            tf.summary.scalar(\"total_loss\", self.loss)\n\n            logdir = \"./data/log/\"\n            if os.path.exists(logdir): shutil.rmtree(logdir)\n            os.mkdir(logdir)\n            self.write_op = tf.summary.merge_all()\n            self.summary_writer  = tf.summary.FileWriter(logdir, graph=self.sess.graph)\n\n\n    def train(self):\n        self.sess.run(tf.global_variables_initializer())\n        try:\n            print('=> Restoring weights from: %s ... ' % self.initial_weight)\n            self.loader.restore(self.sess, self.initial_weight)\n        except:\n            print('=> %s does not exist !!!' % self.initial_weight)\n            print('=> Now it starts to train YOLOV3 from scratch ...')\n            self.first_stage_epochs = 0\n\n        for epoch in range(1, 1+self.first_stage_epochs+self.second_stage_epochs):\n            if epoch <= self.first_stage_epochs:\n                train_op = self.train_op_with_frozen_variables\n            else:\n                train_op = self.train_op_with_all_variables\n\n            pbar = tqdm(self.trainset)\n            train_epoch_loss, test_epoch_loss = [], []\n\n            for train_data in pbar:\n                _, summary, train_step_loss, global_step_val = self.sess.run(\n                    [train_op, self.write_op, self.loss, self.global_step],feed_dict={\n                                                self.input_data:   train_data[0],\n                                                self.label_sbbox:  train_data[1],\n                                                self.label_mbbox:  train_data[2],\n                                                self.label_lbbox:  train_data[3],\n                                                self.true_sbboxes: train_data[4],\n                                                self.true_mbboxes: train_data[5],\n                                                self.true_lbboxes: train_data[6],\n                                                self.trainable:    True,\n                })\n\n                train_epoch_loss.append(train_step_loss)\n                self.summary_writer.add_summary(summary, global_step_val)\n                pbar.set_description(\"train loss: %.2f\" %train_step_loss)\n\n            for test_data in self.testset:\n                test_step_loss = self.sess.run( self.loss, feed_dict={\n                                                self.input_data:   test_data[0],\n                                                self.label_sbbox:  test_data[1],\n                                                self.label_mbbox:  test_data[2],\n                                                self.label_lbbox:  test_data[3],\n                                                self.true_sbboxes: test_data[4],\n                                                self.true_mbboxes: test_data[5],\n                                                self.true_lbboxes: test_data[6],\n                                                self.trainable:    False,\n                })\n\n                test_epoch_loss.append(test_step_loss)\n\n            train_epoch_loss, test_epoch_loss = np.mean(train_epoch_loss), np.mean(test_epoch_loss)\n            ckpt_file = \"./checkpoint/yolov3_test_loss=%.4f.ckpt\" % test_epoch_loss\n            log_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n            print(\"=> Epoch: %2d Time: %s Train loss: %.2f Test loss: %.2f Saving %s ...\"\n                            %(epoch, log_time, train_epoch_loss, test_epoch_loss, ckpt_file))\n            self.saver.save(self.sess, ckpt_file, global_step=epoch)\n\n\n\nif __name__ == '__main__': YoloTrain().train()\n\n\n\n\n"
        },
        {
          "name": "video_demo.py",
          "type": "blob",
          "size": 2.31640625,
          "content": "#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2018 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : video_demo.py\n#   Author      : YunYang1994\n#   Created date: 2018-11-30 15:56:37\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport time\nimport numpy as np\nimport core.utils as utils\nimport tensorflow as tf\nfrom PIL import Image\n\n\nreturn_elements = [\"input/input_data:0\", \"pred_sbbox/concat_2:0\", \"pred_mbbox/concat_2:0\", \"pred_lbbox/concat_2:0\"]\npb_file         = \"./yolov3_coco.pb\"\nvideo_path      = \"./docs/images/road.mp4\"\n# video_path      = 0\nnum_classes     = 80\ninput_size      = 416\ngraph           = tf.Graph()\nreturn_tensors  = utils.read_pb_return_tensors(graph, pb_file, return_elements)\n\nwith tf.Session(graph=graph) as sess:\n    vid = cv2.VideoCapture(video_path)\n    while True:\n        return_value, frame = vid.read()\n        if return_value:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            image = Image.fromarray(frame)\n        else:\n            raise ValueError(\"No image!\")\n        frame_size = frame.shape[:2]\n        image_data = utils.image_preporcess(np.copy(frame), [input_size, input_size])\n        image_data = image_data[np.newaxis, ...]\n        prev_time = time.time()\n\n        pred_sbbox, pred_mbbox, pred_lbbox = sess.run(\n            [return_tensors[1], return_tensors[2], return_tensors[3]],\n                    feed_dict={ return_tensors[0]: image_data})\n\n        pred_bbox = np.concatenate([np.reshape(pred_sbbox, (-1, 5 + num_classes)),\n                                    np.reshape(pred_mbbox, (-1, 5 + num_classes)),\n                                    np.reshape(pred_lbbox, (-1, 5 + num_classes))], axis=0)\n\n        bboxes = utils.postprocess_boxes(pred_bbox, frame_size, input_size, 0.3)\n        bboxes = utils.nms(bboxes, 0.45, method='nms')\n        image = utils.draw_bbox(frame, bboxes)\n\n        curr_time = time.time()\n        exec_time = curr_time - prev_time\n        result = np.asarray(image)\n        info = \"time: %.2f ms\" %(1000*exec_time)\n        cv2.namedWindow(\"result\", cv2.WINDOW_AUTOSIZE)\n        result = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        cv2.imshow(\"result\", result)\n        if cv2.waitKey(1) & 0xFF == ord('q'): break\n\n\n\n\n"
        }
      ]
    }
  ]
}