{
  "metadata": {
    "timestamp": 1736559798850,
    "page": 525,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "llSourcell/Doctor-Dignity",
      "stars": 3851,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.28515625,
          "content": "# Run the following command to reformat a file:\n# clang-format -i -style=Google <file>\n# Or use clang-format-diff to only reformat the changed lines:\n# https://clang.llvm.org/docs/ClangFormat.html\nBasedOnStyle: Google\nDerivePointerAlignment: false\nColumnLimit:     100\nPointerAlignment: Left\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.830078125,
          "content": "dist/\nparams/\n*.bak\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n.DS_Store\n\n*.S\n# C extensions\n*.so\n\nbuild/\n\n*.ll\n.npm\n# Distribution / packaging\n.Python\nenv/\nbuild/\nbuild-*/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n.conda/\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Generated by python/gen_requirements.py\npython/requirements/*.txt\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/_staging/\n\n# PyBuilder\ntarget/\n/target/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n*~\n*.pyc\n*~\nconfig.mk\nconfig.cmake\nWin32\n*.dir\nperf\n*.wasm\n.emscripten\n\n## IOS\nDerivedData/\n\n## Java\n*.class\njvm/*/target/\njvm/*/*/target/\njvm/native/*/generated\njvm/native/src/main/native/org_apache_tvm_native_c_api.h\n*.worksheet\n*.idea\n*.iml\n*.classpath\n*.project\n*.settings\n*/node_modules/\n\n## Various settings\n*.pbxuser\n!default.pbxuser\n*.mode1v3\n!default.mode1v3\n*.mode2v3\n!default.mode2v3\n*.perspectivev3\n!default.perspectivev3\nxcuserdata/\n.pkl_memoize_*\n\n.emscripten*\n.m2\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n## Other\n*.moved-aside\n*.xccheckout\n*.xcscmblueprint\n.DS_Store\ntags\ncscope*\n*.lock\n\n# vim temporary files\n*.swp\n*.swo\n\n# TVM generated code\nperf\n.bash_history\n# *.json\n*.params\n*.ro\n*.onnx\n*.h5\nsynset.txt\ncat.jpg\ncat.png\ndocs.tgz\ncat.png\n*.mlmodel\ntvm_u.*\ntvm_t.*\n# Mac OS X\n.DS_Store\n\n# Jetbrain\n.idea\n.ipython\n.jupyter\n.nv\n.pylint.d\n.python_history\n.pytest_cache\n.local\ncmake-build-debug\n\n# Visual Studio\n.vs\n\n# Visual Studio Code\n.vscode\n\n# tmp file\n.nfs*\n\n# keys\n*.pem\n*.p12\n*.pfx\n*.cer\n*.crt\n*.der\n\n# patch sentinel\npatched.txt\n\n# Python type checking\n.mypy_cache/\n.pyre/\n\n# pipenv files\nPipfile\nPipfile.lock\n\n# conda package artifacts\nconda/Dockerfile.cuda*\nconda/pkg\n.node_repl_history\n# nix files\n.envrc\n*.nix\n\n# Docker files\n.sudo_as_admin_successful\n\n# Downloaded models/datasets\n.tvm_test_data\n.dgl\n.caffe2\n\n# Local docs build\n_docs/\njvm/target\n.config/configstore/\n.ci-py-scripts/\n\n# Generated Hexagon files\nsrc/runtime/hexagon/rpc/hexagon_rpc.h\nsrc/runtime/hexagon/rpc/hexagon_rpc_skel.c\nsrc/runtime/hexagon/rpc/hexagon_rpc_stub.c\n\n# Local tvm-site checkout\ntvm-site/\n\n# Generated docs files\ngallery/how_to/work_with_microtvm/micro_tvmc.py\n\n# Test sample data files\n!tests/python/ci/sample_prs/*.json\n\n# Used in CI to communicate between Python and Jenkins\n.docker-image-names/\n\n# Printed TIR code on disk\n*.tir\n\n# GDB history file\n.gdb_history\n\ndist\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.41015625,
          "content": "[submodule \"3rdparty/argparse\"]\n\tpath = 3rdparty/argparse\n\turl = https://github.com/p-ranav/argparse\n[submodule \"3rdparty/tokenizers-cpp\"]\n\tpath = 3rdparty/tokenizers-cpp\n\turl = https://github.com/mlc-ai/tokenizers-cpp\n[submodule \"3rdparty/googletest\"]\n\tpath = 3rdparty/googletest\n\turl = https://github.com/google/googletest.git\n[submodule \"3rdparty/tvm\"]\n\tpath = 3rdparty/tvm\n\turl = https://github.com/mlc-ai/relax.git\n"
        },
        {
          "name": "3rdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 5.609375,
          "content": "cmake_minimum_required(VERSION 3.18)\nproject(mlc_llm C CXX)\n\ninclude(CheckCXXCompilerFlag)\nif(NOT MSVC)\n  check_cxx_compiler_flag(\"-std=c++17\" SUPPORT_CXX17)\n  set(CMAKE_CXX_FLAGS \"-std=c++17 ${CMAKE_CXX_FLAGS}\")\n  set(CMAKE_CUDA_STANDARD 17)\nelse()\n  check_cxx_compiler_flag(\"/std:c++17\" SUPPORT_CXX17)\n  set(CMAKE_CXX_FLAGS \"/std:c++17 ${CMAKE_CXX_FLAGS}\")\n  set(CMAKE_CUDA_STANDARD 17)\nendif()\n\nif(EXISTS ${CMAKE_BINARY_DIR}/config.cmake)\n  include(${CMAKE_BINARY_DIR}/config.cmake)\nelse()\n  if(EXISTS ${CMAKE_SOURCE_DIR}/config.cmake)\n    include(${CMAKE_SOURCE_DIR}/config.cmake)\n  endif()\nendif()\n\nif(NOT CMAKE_BUILD_TYPE)\n  set(\n    CMAKE_BUILD_TYPE RelWithDebInfo CACHE STRING \"Build type\" FORCE\n  )\n  message(STATUS \"Setting default build type to \" ${CMAKE_BUILD_TYPE})\nendif(NOT CMAKE_BUILD_TYPE)\n\noption(MLC_HIDE_PRIVATE_SYMBOLS \"Hide private symbols\" ON)\n\nif (MLC_LLM_INSTALL_STATIC_LIB)\n  set(BUILD_STATIC_RUNTIME ON)\nendif()\n\nset(MLC_VISIBILITY_FLAG \"\")\nif (MLC_HIDE_PRIVATE_SYMBOLS)\n  set(HIDE_PRIVATE_SYMBOLS ON)\n  if (NOT MSVC)\n    set(MLC_VISIBILITY_FLAG \"-fvisibility=hidden\")\n  endif()\n  message(STATUS \"Hide private symbols\")\nendif()\n\noption(BUILD_CPP_TEST \"Build cpp unittests\" OFF)\n\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\n# tvm runtime config: minimize runtime components\nset(USE_RPC OFF)\nset(USE_MICRO OFF)\nset(USE_GRAPH_EXECUTOR OFF)\nset(USE_GRAPH_EXECUTOR_DEBUG OFF)\nset(USE_AOT_EXECUTOR OFF)\nset(USE_PROFILER OFF)\nset(USE_GTEST OFF)\nset(USE_LIBBACKTRACE OFF)\nset(BUILD_DUMMY_LIBTVM ON)\nif (NOT DEFINED TVM_HOME)\n  set(TVM_HOME 3rdparty/tvm)\nendif (NOT DEFINED TVM_HOME)\nmessage(STATUS \"TVM_HOME: ${TVM_HOME}\")\nadd_subdirectory(${TVM_HOME} tvm EXCLUDE_FROM_ALL)\n\nset(MLC_LLM_RUNTIME_LINKER_LIB \"\")\nset(TOKENZIER_CPP_PATH 3rdparty/tokenizers-cpp)\nadd_subdirectory(${TOKENZIER_CPP_PATH} tokenizers EXCLUDE_FROM_ALL)\n\n\ntvm_file_glob(GLOB_RECURSE MLC_LLM_SRCS cpp/*.cc)\ntvm_file_glob(GLOB_RECURSE MLC_CLI_SRCS cpp/cli_main.cc)\nlist(REMOVE_ITEM MLC_LLM_SRCS ${MLC_CLI_SRCS})\n\nadd_library(mlc_llm_objs OBJECT ${MLC_LLM_SRCS})\nadd_library(mlc_cli_objs OBJECT ${MLC_CLI_SRCS})\n\nset(\n  MLC_LLM_INCLUDES\n  ${TVM_HOME}/include\n  ${TVM_HOME}/3rdparty/dlpack/include\n  ${TVM_HOME}/3rdparty/dmlc-core/include\n  ${TVM_HOME}/3rdparty/picojson\n)\n\nset(MLC_LLM_COMPILE_DEFS ${MLC_LLM_COMPILE_DEFS} DMLC_USE_LOGGING_LIBRARY=<tvm/runtime/logging.h>)\n\ntarget_include_directories(mlc_llm_objs PRIVATE ${MLC_LLM_INCLUDES})\ntarget_compile_definitions(mlc_llm_objs PRIVATE ${MLC_LLM_COMPILE_DEFS})\ntarget_include_directories(mlc_llm_objs PRIVATE ${TOKENZIER_CPP_PATH}/include)\ntarget_compile_definitions(mlc_llm_objs PRIVATE -DMLC_LLM_EXPORTS)\n\nadd_library(mlc_llm SHARED $<TARGET_OBJECTS:mlc_llm_objs>)\nadd_library(mlc_llm_static STATIC $<TARGET_OBJECTS:mlc_llm_objs>)\nadd_dependencies(mlc_llm_static tokenizers_cpp sentencepiece-static tokenizers_c tvm_runtime)\nset_target_properties(mlc_llm_static PROPERTIES OUTPUT_NAME mlc_llm)\n\ntarget_link_libraries(mlc_llm PUBLIC tvm_runtime)\ntarget_link_libraries(mlc_llm PRIVATE tokenizers_cpp)\n\nif (BUILD_CPP_TEST)\n  message(STATUS \"Building cpp unittests\")\n  add_subdirectory(3rdparty/googletest)\n  file(GLOB_RECURSE MLC_LLM_TEST_SRCS ${PROJECT_SOURCE_DIR}/tests/cpp/*unittest.cc)\n  add_executable(mlc_llm_cpp_tests ${MLC_LLM_TEST_SRCS})\n  target_include_directories(mlc_llm_cpp_tests PRIVATE ${MLC_LLM_INCLUDES})\n  target_include_directories(mlc_llm_cpp_tests PRIVATE ${PROJECT_SOURCE_DIR}/cpp)\n  target_include_directories(mlc_llm_cpp_tests PRIVATE ${gtest_SOURCE_DIR}/include ${gtest_SOURCE_DIR})\n  target_link_libraries(mlc_llm_cpp_tests PUBLIC mlc_llm gtest gtest_main)\nendif(BUILD_CPP_TEST)\n\n# Example app that may depends on mlc_llm\nadd_executable(mlc_chat_cli $<TARGET_OBJECTS:mlc_cli_objs>)\ntarget_include_directories(mlc_cli_objs PRIVATE ${MLC_LLM_INCLUDES})\ntarget_include_directories(mlc_cli_objs PRIVATE 3rdparty/argparse/include)\ntarget_compile_definitions(mlc_cli_objs PRIVATE ${MLC_LLM_COMPILE_DEFS})\n\nif (CMAKE_SYSTEM_NAME STREQUAL \"Android\")\n  target_link_libraries(mlc_llm PRIVATE log)\n  target_link_libraries(mlc_chat_cli PRIVATE log)\nendif()\n\nif (MLC_LLM_INSTALL_STATIC_LIB)\n  target_link_libraries(\n    mlc_chat_cli PRIVATE mlc_llm_static tokenizers_cpp sentencepiece-static tokenizers_c)\n  target_link_libraries(\n    mlc_chat_cli PRIVATE \"$<LINK_LIBRARY:WHOLE_ARCHIVE,tvm_runtime>\")\nelse()\n  target_link_libraries(mlc_chat_cli PUBLIC mlc_llm)\nendif()\n\nadd_library(mlc_llm_module SHARED $<TARGET_OBJECTS:mlc_llm_objs>)\ntarget_link_libraries(mlc_llm_module PUBLIC tvm)\ntarget_link_libraries(mlc_llm_module PRIVATE tokenizers_cpp)\n\n\nset_property(TARGET mlc_llm_module APPEND PROPERTY LINK_OPTIONS \"${MLC_VISIBILITY_FLAG}\")\nset_property(TARGET mlc_llm APPEND PROPERTY LINK_OPTIONS \"${MLC_VISIBILITY_FLAG}\")\n\nfind_program(CARGO_EXECUTABLE cargo)\n\nif(NOT CARGO_EXECUTABLE)\n    message(FATAL_ERROR \"Cargo is not found! Please install cargo.\")\nendif()\n\n# when this option is on,\n# we install all static lib deps into lib\nif (MLC_LLM_INSTALL_STATIC_LIB)\n  install(TARGETS\n    mlc_llm_static\n    tokenizers_cpp\n    sentencepiece-static\n    tvm_runtime\n    LIBRARY DESTINATION lib${LIB_SUFFIX}\n    )\n  # tokenizers need special handling as it builds from rust\n  if(MSVC)\n    install(FILES ${CMAKE_CURRENT_BINARY_DIR}/tokenizers/libtokenizers_c.lib\n      DESTINATION lib${LIB_SUFFIX}\n      )\n  else()\n    install(FILES ${CMAKE_CURRENT_BINARY_DIR}/tokenizers/libtokenizers_c.a\n      DESTINATION lib${LIB_SUFFIX}\n      )\n  endif()\nelse()\n  install(TARGETS mlc_chat_cli tvm_runtime mlc_llm mlc_llm_module\n    mlc_llm_static\n    tokenizers_cpp\n    sentencepiece-static\n    RUNTIME_DEPENDENCY_SET tokenizers_c\n    RUNTIME DESTINATION bin\n    LIBRARY DESTINATION lib${LIB_SUFFIX}\n  )\nendif()\n"
        },
        {
          "name": "CONTRIBUTORS.md",
          "type": "blob",
          "size": 0.1494140625,
          "content": "MLC LLM Contributors\n====================\n\n\n## List of Contributors\n- [Full List of Contributors](https://github.com/mlc-ai/mlc-llm/graphs/contributors)\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08984375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "Machine_Learning_Compilation_for_Beginners.ipynb",
          "type": "blob",
          "size": 100.705078125,
          "content": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": [],\n      \"gpuType\": \"V100\",\n      \"machine_shape\": \"hm\"\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"language_info\": {\n      \"name\": \"python\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"# Machine Learning Compilation for Beginners\\n\",\n        \"\\n\",\n        \"## Tutorial Overview\\n\",\n        \"\\n\",\n        \"Demo: Dr Dignity (Local Models are Private, Data Compliant, &Free (No OpenAI)\\n\",\n        \"1. Build a Simple Language Model in Python!\\n\",\n        \"2. Accelerate on NVIDIA GPUs using Compiler #1: CUDA\\n\",\n        \"3. Accelerate on iOS GPUs using Compiler #2: Metal\\n\",\n        \"4. Accelerate on Android GPUs using Compiler #3: Vulkan\\n\",\n        \"5. Accelerate on iOS & Android GPUs using Compiler #4: Tensor Virtual Machine\\n\",\n        \"6. Compile a Large Language Model from HuggingFace to iOS & Android with TVM\\n\",\n        \"7. Future Directions\\n\",\n        \"\\n\",\n        \"\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"5LJhJCt7KHRr\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## Dependencies\\n\",\n        \"\\n\",\n        \"- Python for Programming\\n\",\n        \"- Numpy for CPU speed up\\n\",\n        \"- CUDA for Nvidia GPU speed up\\n\",\n        \"- Metal for iOS GPU speed up\\n\",\n        \"- Vulkan for Android GPU speed up\\n\",\n        \"- Tensor Virtual Machine for iOS and Android GPU speed up\\n\",\n        \"- Relay to Optimize for iOS and Android\\n\",\n        \"- MLC-LLM for LLM-specific Compilation Tools\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"2IhKZxxCGjb2\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\",\n          \"height\": 741\n        },\n        \"id\": \"h8eCPpGoJm1p\",\n        \"outputId\": \"c08e1312-960f-4924-9368-448c467d0f10\"\n      },\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Looking in links: https://mlc.ai/wheels\\n\",\n            \"Collecting mlc_ai_nightly\\n\",\n            \"  Using cached https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly-0.12.dev1576-cp310-cp310-manylinux_2_28_x86_64.whl (86.3 MB)\\n\",\n            \"Collecting attrs (from mlc_ai_nightly)\\n\",\n            \"  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\\n\",\n            \"Collecting cloudpickle (from mlc_ai_nightly)\\n\",\n            \"  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\\n\",\n            \"Collecting decorator (from mlc_ai_nightly)\\n\",\n            \"  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\\n\",\n            \"Collecting ml-dtypes (from mlc_ai_nightly)\\n\",\n            \"  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\\n\",\n            \"Collecting numpy (from mlc_ai_nightly)\\n\",\n            \"  Using cached numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\\n\",\n            \"Collecting psutil (from mlc_ai_nightly)\\n\",\n            \"  Using cached psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\\n\",\n            \"Collecting scipy (from mlc_ai_nightly)\\n\",\n            \"  Using cached scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\\n\",\n            \"Collecting tornado (from mlc_ai_nightly)\\n\",\n            \"  Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\\n\",\n            \"Collecting typing-extensions (from mlc_ai_nightly)\\n\",\n            \"  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\\n\",\n            \"Installing collected packages: typing-extensions, tornado, psutil, numpy, decorator, cloudpickle, attrs, scipy, ml-dtypes, mlc_ai_nightly\\n\",\n            \"\\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\\n\",\n            \"ipython 7.34.0 requires jedi>=0.16, which is not installed.\\n\",\n            \"cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.0 which is incompatible.\\n\",\n            \"google-colab 1.0.0 requires tornado==6.3.2, but you have tornado 6.3.3 which is incompatible.\\n\",\n            \"moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\\n\",\n            \"numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0 which is incompatible.\\n\",\n            \"tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.0 which is incompatible.\\n\",\n            \"tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\\u001b[0m\\u001b[31m\\n\",\n            \"\\u001b[0mSuccessfully installed attrs-23.1.0 cloudpickle-2.2.1 decorator-4.4.2 ml-dtypes-0.2.0 mlc_ai_nightly-0.12.dev1576 numpy-1.23.5 psutil-5.9.5 scipy-1.11.2 tornado-6.3.2 typing-extensions-4.5.0\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"display_data\",\n          \"data\": {\n            \"application/vnd.colab-display-data+json\": {\n              \"pip_warning\": {\n                \"packages\": [\n                  \"decorator\",\n                  \"numpy\",\n                  \"psutil\",\n                  \"tornado\",\n                  \"tvm\"\n                ]\n              }\n            }\n          },\n          \"metadata\": {}\n        }\n      ],\n      \"source\": [\n        \"!pip install pycuda #CUDA, use an a100 instance\\n\",\n        \"!sudo apt install -y libvulkan-dev #Vulkan\\n\",\n        \"!pip install apache-tvm #Tensor Virtual Machine\\n\",\n        \"!pip install -I mlc_ai_nightly -f https://mlc.ai/wheels #MLC-LLM\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## Step 1 - Build a Simple Language Model in Python!\"\n      ],\n      \"metadata\": {\n        \"id\": \"TQ6uxfJUKDWc\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"\\n\",\n        \"- This Model aims to predict the next character in the sequence based on the current character.\\n\",\n        \"- The model is trained on a dataset of individual characters ('a', 'b', 'c', 'd', 'e')\\n\",\n        \"- This is a one-hot encoded character prediction model.\\n\",\n        \"- ![Alt Text](https://miro.medium.com/v2/resize:fit:714/0*TsV7C_p9Yhkeqthg.png)\\n\",\n        \"- ![Alt Text](https://raw.githubusercontent.com/geekquad/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg)\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"cLNs8HOJ6UX-\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# Import the NumPy library for numerical computations\\n\",\n        \"import numpy as np\\n\",\n        \"\\n\",\n        \"# ---- Data and Parameters ----\\n\",\n        \"\\n\",\n        \"# Initialize the data as a string \\\"abcde\\\"\\n\",\n        \"data = \\\"abcde\\\"\\n\",\n        \"# Create a dictionary to map each unique character in the data to an index\\n\",\n        \"vocab = {c: i for i, c in enumerate(set(data))}\\n\",\n        \"# Create an inverse dictionary to map each index back to its corresponding character\\n\",\n        \"inv_vocab = {i: c for c, i in vocab.items()}\\n\",\n        \"# Initialize the weight matrix with random values; its dimensions are based on the vocabulary size\\n\",\n        \"w = np.random.rand(len(vocab), len(vocab))\\n\",\n        \"# Set the learning rate for the model\\n\",\n        \"lr = 0.1\\n\",\n        \"\\n\",\n        \"# ---- Data Preprocessing ----\\n\",\n        \"\\n\",\n        \"# Convert each character in the data to its corresponding numerical index\\n\",\n        \"data_idx = [vocab[c] for c in data]\\n\",\n        \"\\n\",\n        \"# ---- Training Loop ----\\n\",\n        \"\\n\",\n        \"# Loop through the data (ignoring the last character)\\n\",\n        \"for i in range(len(data_idx) - 1):\\n\",\n        \"    # Extract the current and next characters' indices\\n\",\n        \"    x, y = data_idx[i], data_idx[i + 1]\\n\",\n        \"    # Generate a one-hot encoded vector for the current character\\n\",\n        \"    x_onehot = np.eye(len(vocab))[x]\\n\",\n        \"\\n\",\n        \"    # ---- Forward Pass ----\\n\",\n        \"\\n\",\n        \"    # Compute the dot product of the one-hot vector and the weight matrix, then apply the exponential function\\n\",\n        \"    pred = np.exp(np.dot(x_onehot, w))\\n\",\n        \"    # Normalize the resulting vector to form a probability distribution\\n\",\n        \"    pred /= np.sum(pred)\\n\",\n        \"\\n\",\n        \"    # ---- Loss and Gradient Calculation ----\\n\",\n        \"\\n\",\n        \"    # Compute the negative log-likelihood loss\\n\",\n        \"    loss = -np.log(pred[y])\\n\",\n        \"    # Compute the gradient based on the difference between the predicted and actual distributions\\n\",\n        \"    grad = pred - np.eye(len(vocab))[y]\\n\",\n        \"\\n\",\n        \"    # ---- Weight Update ----\\n\",\n        \"\\n\",\n        \"    # Update the weight matrix using stochastic gradient descent\\n\",\n        \"    w[:, x] -= lr * grad\\n\",\n        \"\\n\",\n        \"# ---- Inference ----\\n\",\n        \"\\n\",\n        \"# Set the input character for prediction\\n\",\n        \"input_char = 'a'\\n\",\n        \"# Convert the input character to its corresponding numerical index\\n\",\n        \"input_idx = vocab[input_char]\\n\",\n        \"# Generate a one-hot encoded vector for the input character\\n\",\n        \"input_onehot = np.eye(len(vocab))[input_idx]\\n\",\n        \"# Perform the forward pass to get the output probabilities\\n\",\n        \"output_prob = np.exp(np.dot(input_onehot, w))\\n\",\n        \"# Normalize the output probabilities\\n\",\n        \"output_prob /= np.sum(output_prob)\\n\",\n        \"# Find the index of the character with the highest predicted probability\\n\",\n        \"output_idx = np.argmax(output_prob)\\n\",\n        \"# Use the inverse vocabulary to map the index back to a character\\n\",\n        \"output_char = inv_vocab[output_idx]\\n\",\n        \"\\n\",\n        \"# Print the predicted next character given the input\\n\",\n        \"print(f\\\"Given '{input_char}', next predicted character is '{output_char}'\\\")\\n\"\n      ],\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"gnnVK1WVKWJ-\",\n        \"outputId\": \"59683f5e-e7fe-4794-8f17-060d561c18c4\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Given 'a', next predicted character is 'c'\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"### Measure Inference Speed on Intel CPU\\n\",\n        \"\\n\",\n        \"- CPU cycle\\n\",\n        \"- ![Alt Text](https://www.computerhope.com/jargon/m/machine-cycle.png)\\n\",\n        \"- In a CPU, the memory hierarchy includes multiple levels of cache to store data temporarily for time, space, & power efficiency.\\n\",\n        \"- Full Data Flow below\\n\",\n        \"- ![Alt Text](https://anilmaurya.files.wordpress.com/2016/02/cpu-block-diagram.gif)\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"CvrTMzvoKbR2\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# Import the 'time' library for timing operations\\n\",\n        \"import time\\n\",\n        \"# Import the 'numpy' library for numerical operations\\n\",\n        \"import numpy as np\\n\",\n        \"\\n\",\n        \"# Initialize an empty list to store the time taken for each inference operation\\n\",\n        \"times = []\\n\",\n        \"\\n\",\n        \"# Loop to run the inference process 1000 times\\n\",\n        \"for _ in range(1000):\\n\",\n        \"    # Record the start time before running inference\\n\",\n        \"    start = time.time()\\n\",\n        \"\\n\",\n        \"    # Create a one-hot encoded input array for inference (here, the first element is 1 and the rest are 0)\\n\",\n        \"    input_onehot = np.eye(5)[0]\\n\",\n        \"\\n\",\n        \"    # Perform the forward pass: dot product of the one-hot input with a random weight matrix, followed by exponentiation\\n\",\n        \"    output_prob = np.exp(np.dot(input_onehot, np.random.rand(5, 5)))\\n\",\n        \"\\n\",\n        \"    # Normalize the output to get a probability distribution\\n\",\n        \"    output_prob /= np.sum(output_prob)\\n\",\n        \"\\n\",\n        \"    # Record the time taken for this inference operation\\n\",\n        \"    elapsed_time = time.time() - start\\n\",\n        \"\\n\",\n        \"    # Append the elapsed time to the 'times' list\\n\",\n        \"    times.append(elapsed_time)\\n\",\n        \"\\n\",\n        \"# Calculate the average time taken for inference across all runs, and convert it to milliseconds\\n\",\n        \"average_time = np.mean(times) * 1000\\n\",\n        \"\\n\",\n        \"# Print the average inference time, rounded to 2 decimal places\\n\",\n        \"print(f\\\"Average Inference Time: {average_time:.2f} ms\\\")\"\n      ],\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"Dcus5YpRKfgM\",\n        \"outputId\": \"8ce858d9-7976-4b15-8e20-982631222120\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Average Inference Time: 0.07 ms\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## Step 2 - Accelerate on NVIDIA GPUs using Compiler #1: CUDA\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://data.embeddedcomputing.com/uploads/articles/wp/1462/54176dc2ec16c-Vivante-Sept-Fig1.jpg)\\n\",\n        \"- ![Alt Text](https://pbs.twimg.com/media/EbxlFreUwAAZ2sZ?format=jpg&name=900x900)\\n\",\n        \"- ![Alt Text](https://www.researchgate.net/profile/Joshua-Payne-2/publication/265291072/figure/fig1/AS:650829417680896@1532181233055/Performance-comparison-of-GPUs-vs-CPUs.png)\\n\",\n        \"- ![Alt Text](http://2.bp.blogspot.com/-UHviPBWWhR8/UPWSIVXHmLI/AAAAAAAAAsU/3gi4jAAtSIU/s1600/compilation.png)\\n\",\n        \"- CUDA tutorial https://web.engr.oregonstate.edu/~mjb/cs575/Handouts/cuda.1pp.pdf\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"x03DthaJKhNl\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# Import the CUDA driver interface from the PyCUDA library\\n\",\n        \"import pycuda.driver as cuda\\n\",\n        \"# Initialize the PyCUDA library\\n\",\n        \"import pycuda.autoinit\\n\",\n        \"# Import the SourceModule for compiling CUDA source code\\n\",\n        \"from pycuda.compiler import SourceModule\\n\",\n        \"# Import NumPy for numerical operations\\n\",\n        \"import numpy as np\\n\",\n        \"# Import time for timing the code\\n\",\n        \"import time\\n\",\n        \"\\n\",\n        \"# Define a vocabulary list\\n\",\n        \"vocab = ['a', 'b', 'c', 'd', 'e']\\n\",\n        \"\\n\",\n        \"# Define the CUDA kernel function for inference as a string\\n\",\n        \"mod = SourceModule(\\\"\\\"\\\"\\n\",\n        \"  __global__ void inference(float *x, float *w, float *y, int vocab_size)\\n\",\n        \"  {\\n\",\n        \"    const int i = threadIdx.x;  // Get the thread index\\n\",\n        \"    float sum = 0;  // Initialize sum variable\\n\",\n        \"\\n\",\n        \"    // Loop to calculate the sum of element-wise products of 'x' and 'w'\\n\",\n        \"    for(int j = 0; j < vocab_size; j++)\\n\",\n        \"    {\\n\",\n        \"        sum += exp(w[j * vocab_size + i] * x[j]);  // Exponential function applied to the element-wise product\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    // Store the sum in the output array 'y'\\n\",\n        \"    y[i] = sum;\\n\",\n        \"  }\\n\",\n        \"\\\"\\\"\\\")\\n\",\n        \"\\n\",\n        \"# Define the size of the vocabulary\\n\",\n        \"vocab_size = 5\\n\",\n        \"# Generate random weights and cast them to float32\\n\",\n        \"w = np.random.rand(vocab_size, vocab_size).astype(np.float32)\\n\",\n        \"# Create a one-hot vector representation for the first element 'a' in the vocabulary\\n\",\n        \"x_onehot = np.eye(vocab_size, dtype=np.float32)[0]  # One-hot vector for 'a'\\n\",\n        \"\\n\",\n        \"# Allocate GPU memory for weight, input, and output arrays\\n\",\n        \"w_gpu = cuda.mem_alloc(w.nbytes)\\n\",\n        \"x_gpu = cuda.mem_alloc(x_onehot.nbytes)\\n\",\n        \"y_gpu = cuda.mem_alloc(vocab_size * 4)  # 4 bytes for each float32 element\\n\",\n        \"\\n\",\n        \"# Copy data from host to device (CPU to GPU)\\n\",\n        \"cuda.memcpy_htod(w_gpu, w)\\n\",\n        \"cuda.memcpy_htod(x_gpu, x_onehot)\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"Vy-KsqfZK11p\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"### Measure Inference Speed on Nvidia GPU\"\n      ],\n      \"metadata\": {\n        \"id\": \"jhsPJnjmLBWr\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# Retrieve the compiled CUDA kernel function named \\\"inference\\\"\\n\",\n        \"func = mod.get_function(\\\"inference\\\")\\n\",\n        \"# Record the starting time for timing the execution\\n\",\n        \"start_time = time.time()\\n\",\n        \"\\n\",\n        \"# Execute the CUDA kernel function 1000 times\\n\",\n        \"for _ in range(1000):\\n\",\n        \"    # Run the CUDA function with the given parameters and thread configuration\\n\",\n        \"    func(x_gpu, w_gpu, y_gpu, np.int32(vocab_size), block=(vocab_size, 1, 1))\\n\",\n        \"\\n\",\n        \"# Synchronize the CUDA context to ensure all operations are complete\\n\",\n        \"cuda.Context.synchronize()\\n\",\n        \"# Record the ending time to calculate the elapsed time\\n\",\n        \"end_time = time.time()\\n\",\n        \"\\n\",\n        \"# Create an empty NumPy array with the same shape as x_onehot to store results\\n\",\n        \"y = np.empty_like(x_onehot)\\n\",\n        \"# Copy the result from GPU to CPU (device to host)\\n\",\n        \"cuda.memcpy_dtoh(y, y_gpu)\\n\",\n        \"# Apply the exponential function to the result to undo the previous log operation\\n\",\n        \"y = np.exp(y)\\n\",\n        \"# Normalize the probabilities so they sum to 1\\n\",\n        \"y /= np.sum(y)\\n\",\n        \"\\n\",\n        \"# Find the index of the maximum value in the array 'y'\\n\",\n        \"predicted_index = np.argmax(y)\\n\",\n        \"\\n\",\n        \"# Print the calculated probabilities\\n\",\n        \"print(f\\\"Predicted probabilities: {y}\\\")\\n\",\n        \"# Print the most probable next character based on the given input 'a'\\n\",\n        \"print(f\\\"Given 'a', the next predicted character is: '{vocab[predicted_index]}'\\\")\\n\",\n        \"# Print the average time taken for inference\\n\",\n        \"print(f\\\"Average Inference Time: {(end_time - start_time) / 1000 * 1000:.2f} ms\\\")\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"0hmDyTp3LIcW\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"828a5300-9037-4f1c-9d73-bb27c63e4ad6\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Predicted probabilities: [0.14863102 0.15203634 0.15777873 0.310623   0.23093095]\\n\",\n            \"Given 'a', the next predicted character is: 'd'\\n\",\n            \"Average Inference Time: 0.01 ms\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stderr\",\n          \"text\": [\n            \"/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\\n\",\n            \"  globals().clear()\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## Step 3 - Accelerate on iOS GPUs using Compiler #2: Metal\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"- There are many GPU providers, not just Nvidia\\n\",\n        \"- ![Alt Text](https://i.ytimg.com/vi/6GOzFB7bAqg/sddefault.jpg)\\n\",\n        \"- Apple has it's A Series Chips\\n\",\n        \"- ![Alt Text](https://static.wikia.nocookie.net/ipod/images/8/8d/Apple_A15_simplified_schematic_parts.jpg/revision/latest?cb=20220911035113)\\n\",\n        \"- The Metal Compiler lets us leverage it\\n\",\n        \"- ![Alt Text](https://devimages-cdn.apple.com/wwdc-services/images/124/6556/6556_wide_250x141_2x.jpg)\\n\",\n        \"\\n\",\n        \"\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"CZbnsy5hLqXo\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"Time to leave Colab...sorry, not sorry Nvidia.\\n\",\n        \"\\n\",\n        \"1. Open Xcode\\n\",\n        \"2. Create a new command-line project\\n\",\n        \"3. Paste the below inference function into it\\n\",\n        \"4. Create a .metal file\\n\",\n        \"7. Replace the .metal reference directory with the second cell block\\n\",\n        \"8. Build and run the project!\"\n      ],\n      \"metadata\": {\n        \"id\": \"0JGYKmHtHyzk\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"Language model in Objective-C\"\n      ],\n      \"metadata\": {\n        \"id\": \"mIkP-gm2hJ63\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"```objective-c\\n\",\n        \"\\n\",\n        \"#import <Foundation/Foundation.h>\\n\",\n        \"#import <Metal/Metal.h>\\n\",\n        \"\\n\",\n        \"#import <math.h>\\n\",\n        \"\\n\",\n        \"@interface SimpleLanguageModel : NSObject\\n\",\n        \"\\n\",\n        \"@property float *w;\\n\",\n        \"@property float lr;\\n\",\n        \"@property NSDictionary *vocab;\\n\",\n        \"@property NSDictionary *inv_vocab;\\n\",\n        \"@property NSUInteger vocabSize;\\n\",\n        \"\\n\",\n        \"- (instancetype)initWithData:(NSString *)data learningRate:(float)lr;\\n\",\n        \"- (void)train;\\n\",\n        \"- (NSString *)predictNextCharacter:(NSString *)inputChar;\\n\",\n        \"\\n\",\n        \"@end\\n\",\n        \"\\n\",\n        \"@implementation SimpleLanguageModel\\n\",\n        \"\\n\",\n        \"- (instancetype)initWithData:(NSString *)data learningRate:(float)lr {\\n\",\n        \"    self = [super init];\\n\",\n        \"    if (self) {\\n\",\n        \"        self.lr = lr;\\n\",\n        \"        self.vocab = [self buildVocab:data];\\n\",\n        \"        self.inv_vocab = [self buildInvVocab:self.vocab];\\n\",\n        \"        self.vocabSize = [self.vocab count];\\n\",\n        \"        self.w = malloc(self.vocabSize * self.vocabSize * sizeof(float));\\n\",\n        \"        for (int i = 0; i < self.vocabSize * self.vocabSize; i++) {\\n\",\n        \"            self.w[i] = ((float)rand() / RAND_MAX);\\n\",\n        \"        }\\n\",\n        \"    }\\n\",\n        \"    return self;\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"- (NSDictionary *)buildVocab:(NSString *)data {\\n\",\n        \"    NSMutableDictionary *vocab = [NSMutableDictionary dictionary];\\n\",\n        \"    for (int i = 0; i < [data length]; i++) {\\n\",\n        \"        NSString *charStr = [NSString stringWithFormat:@\\\"%C\\\", [data characterAtIndex:i]];\\n\",\n        \"        [vocab setObject:[NSNumber numberWithInt:i] forKey:charStr];\\n\",\n        \"    }\\n\",\n        \"    return [vocab copy];\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"- (NSDictionary *)buildInvVocab:(NSDictionary *)vocab {\\n\",\n        \"    NSMutableDictionary *inv_vocab = [NSMutableDictionary dictionary];\\n\",\n        \"    for (NSString *key in vocab) {\\n\",\n        \"        inv_vocab[vocab[key]] = key;\\n\",\n        \"    }\\n\",\n        \"    return [inv_vocab copy];\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"- (void)train {\\n\",\n        \"    NSArray *data = @[@0, @1, @2, @3, @4];  // Assuming the vocab indices are from 0 to 4\\n\",\n        \"    \\n\",\n        \"    for (NSNumber *x in data) {\\n\",\n        \"        NSNumber *y = [data objectAtIndex:([data indexOfObject:x] + 1) % [data count]];\\n\",\n        \"        \\n\",\n        \"        float pred[self.vocabSize];\\n\",\n        \"        for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"            pred[i] = exp(self.w[i * self.vocabSize + [x intValue]]);\\n\",\n        \"        }\\n\",\n        \"        float sumPred = 0;\\n\",\n        \"        for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"            sumPred += pred[i];\\n\",\n        \"        }\\n\",\n        \"        for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"            pred[i] /= sumPred;\\n\",\n        \"        }\\n\",\n        \"        \\n\",\n        \"        float loss = -log(pred[[y intValue]]);\\n\",\n        \"        NSLog(@\\\"Loss: %f\\\", loss);\\n\",\n        \"        \\n\",\n        \"        float grad[self.vocabSize];\\n\",\n        \"        for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"            grad[i] = pred[i] - (i == [y intValue] ? 1 : 0);\\n\",\n        \"        }\\n\",\n        \"        \\n\",\n        \"        for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"            self.w[i * self.vocabSize + [x intValue]] -= self.lr * grad[i];\\n\",\n        \"        }\\n\",\n        \"    }\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"- (NSString *)predictNextCharacter:(NSString *)inputChar {\\n\",\n        \"    int inputIdx = [[self.vocab objectForKey:inputChar] intValue];\\n\",\n        \"    \\n\",\n        \"    float output_prob[self.vocabSize];\\n\",\n        \"    for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"        output_prob[i] = exp(self.w[i * self.vocabSize + inputIdx]);\\n\",\n        \"    }\\n\",\n        \"    \\n\",\n        \"    float sumProb = 0;\\n\",\n        \"    for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"        sumProb += output_prob[i];\\n\",\n        \"    }\\n\",\n        \"    for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"        output_prob[i] /= sumProb;\\n\",\n        \"    }\\n\",\n        \"    \\n\",\n        \"    int outputIdx = 0;\\n\",\n        \"    float maxProb = 0;\\n\",\n        \"    for (int i = 0; i < self.vocabSize; i++) {\\n\",\n        \"        if (output_prob[i] > maxProb) {\\n\",\n        \"            maxProb = output_prob[i];\\n\",\n        \"            outputIdx = i;\\n\",\n        \"        }\\n\",\n        \"    }\\n\",\n        \"    \\n\",\n        \"    return [self.inv_vocab objectForKey:[NSNumber numberWithInt:outputIdx]];\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"@end\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"int main(int argc, const char * argv[]) {\\n\",\n        \"    @autoreleasepool {\\n\",\n        \"        \\n\",\n        \"        id<MTLDevice> device = MTLCreateSystemDefaultDevice();\\n\",\n        \"        NSError *error = nil;\\n\",\n        \"        \\n\",\n        \"        NSString *metalSrc = [NSString stringWithContentsOfFile:@\\\"/Users/sirajraval/Desktop/metal_test/metal_test/fun.metal\\\" encoding:NSUTF8StringEncoding error:&error];\\n\",\n        \"        id<MTLLibrary> library = [device newLibraryWithSource:metalSrc options:nil error:&error];\\n\",\n        \"        \\n\",\n        \"        id<MTLFunction> function = [library newFunctionWithName:@\\\"predict_next_character\\\"];\\n\",\n        \"        id<MTLComputePipelineState> pipelineState = [device newComputePipelineStateWithFunction:function error:&error];\\n\",\n        \"        \\n\",\n        \"        id<MTLCommandQueue> commandQueue = [device newCommandQueue];\\n\",\n        \"        id<MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];\\n\",\n        \"        id<MTLComputeCommandEncoder> commandEncoder = [commandBuffer computeCommandEncoder];\\n\",\n        \"        \\n\",\n        \"        [commandEncoder setComputePipelineState:pipelineState];\\n\",\n        \"        \\n\",\n        \"        SimpleLanguageModel *model = [[SimpleLanguageModel alloc] initWithData:@\\\"abcde\\\" learningRate:0.1];\\n\",\n        \"        \\n\",\n        \"        // Convert your weights to an MTLBuffer (in this example, I'm assuming the weights are a 5x5 matrix)\\n\",\n        \"        id<MTLBuffer> weightBuffer = [device newBufferWithBytes:model.w length:25*sizeof(float) options:MTLResourceStorageModeShared];\\n\",\n        \"        \\n\",\n        \"        float input_onehot[] = {1.0, 0.0, 0.0, 0.0, 0.0};  // Assume 'a' is encoded as this one-hot vector\\n\",\n        \"        id<MTLBuffer> inputBuffer = [device newBufferWithBytes:input_onehot length:5*sizeof(float) options:MTLResourceStorageModeShared];\\n\",\n        \"        \\n\",\n        \"        float output_prob[5] = {0};\\n\",\n        \"        id<MTLBuffer> outputBuffer = [device newBufferWithBytes:output_prob length:5*sizeof(float) options:MTLResourceStorageModeShared];\\n\",\n        \"        \\n\",\n        \"        [commandEncoder setBuffer:weightBuffer offset:0 atIndex:0];\\n\",\n        \"        [commandEncoder setBuffer:inputBuffer offset:0 atIndex:1];\\n\",\n        \"        [commandEncoder setBuffer:outputBuffer offset:0 atIndex:2];\\n\",\n        \"        \\n\",\n        \"        MTLSize gridSize = MTLSizeMake(5, 1, 1);\\n\",\n        \"        MTLSize threadGroupSize = MTLSizeMake(1, 1, 1);\\n\",\n        \"        \\n\",\n        \"        [commandEncoder dispatchThreadgroups:gridSize threadsPerThreadgroup:threadGroupSize];\\n\",\n        \"        \\n\",\n        \"        [commandEncoder endEncoding];\\n\",\n        \"        [commandBuffer commit];\\n\",\n        \"        \\n\",\n        \"\\n\",\n        \"\\n\",\n        \"               \\n\",\n        \"        \\n\",\n        \"        \\n\",\n        \"        [commandBuffer waitUntilCompleted];\\n\",\n        \"        \\n\",\n        \"        memcpy(output_prob, [outputBuffer contents], sizeof(output_prob));\\n\",\n        \"        \\n\",\n        \"        // Sum up all the probabilities to normalize them\\n\",\n        \"        double sum = 0.0;\\n\",\n        \"        for (int i = 0; i < model.vocabSize; i++) {\\n\",\n        \"            sum += output_prob[i];\\n\",\n        \"        }\\n\",\n        \"\\n\",\n        \"        // Normalize each output probability\\n\",\n        \"        for (int i = 0; i < model.vocabSize; i++) {\\n\",\n        \"            output_prob[i] /= sum;\\n\",\n        \"        }\\n\",\n        \"\\n\",\n        \"        // Find the index of the most likely next character\\n\",\n        \"        int maxIndex = 0;\\n\",\n        \"        double maxProb = 0.0;\\n\",\n        \"        for (int i = 0; i < model.vocabSize; i++) {\\n\",\n        \"            if (output_prob[i] > maxProb) {\\n\",\n        \"                maxProb = output_prob[i];\\n\",\n        \"                maxIndex = i;\\n\",\n        \"            }\\n\",\n        \"        }\\n\",\n        \"\\n\",\n        \"        // Convert the index back to a character\\n\",\n        \"        NSString *nextChar = [model.inv_vocab objectForKey:[NSNumber numberWithInt:maxIndex]];\\n\",\n        \"\\n\",\n        \"        NSLog(@\\\"Given '%s', the next predicted character is '%@'\\\", \\\"a\\\", nextChar);\\n\",\n        \"        \\n\",\n        \"\\n\",\n        \"        NSDate *startTime = [NSDate date];  // Capture the start time\\n\",\n        \"        NSString *nexty = [model predictNextCharacter:@\\\"a\\\"];\\n\",\n        \"        NSDate *endTime = [NSDate date];  // Capture the end time\\n\",\n        \"        NSTimeInterval timeInterval = [endTime timeIntervalSinceDate:startTime];\\n\",\n        \"\\n\",\n        \"        NSLog(@\\\"Inference took %f milliseconds\\\", timeInterval*1000);\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"    }\\n\",\n        \"    return 0;\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"```\"\n      ],\n      \"metadata\": {\n        \"id\": \"1XhHomwPg8RG\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"Metal Code for Inference Speed Up on Apple GPUs\"\n      ],\n      \"metadata\": {\n        \"id\": \"ZdiiaJmshX4G\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"```objective-c\\n\",\n        \"#include <metal_stdlib>\\n\",\n        \"using namespace metal;\\n\",\n        \"\\n\",\n        \"kernel void predict_next_character(\\n\",\n        \"    constant float *weights [[buffer(0)]],\\n\",\n        \"    device float *input_onehot [[buffer(1)]],\\n\",\n        \"    device float *output_prob [[buffer(2)]],\\n\",\n        \"    uint id [[thread_position_in_grid]]\\n\",\n        \") {\\n\",\n        \"    float prob = 0.0;\\n\",\n        \"    for (uint i = 0; i < 5; ++i) {\\n\",\n        \"        prob += weights[id * 5 + i] * input_onehot[i];\\n\",\n        \"    }\\n\",\n        \"    output_prob[id] = exp(prob);\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"```\"\n      ],\n      \"metadata\": {\n        \"id\": \"L_MFtbTuGkk1\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## Step 4 - Accelerate on Android GPUs using Compiler #3: Vulkan\\n\",\n        \"\\n\",\n        \"- Most Phones use Android\\n\",\n        \"- ![Alt Text](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjV3ABZMrT-oJ3vcUTwJr6xp4FKwyD0Gg2f6ZayIDpP9HyXDyWtq0nPZVVF0zye4gAr-OU-2ehNdJTpJ7b9no2Ebj7TMcHsoz72LKewuC5Y4IEbH_ghFFuFhtYn_GohJQcKbDbuubXEESwwBPaLxzk4CT7GnjfcE5hwTZxav6ekTvMck09Rc8ccRASp/s2092/os-market-share.png)\\n\",\n        \"\\n\",\n        \"- Lots of Different GPU vendors in the Android Ecosystem\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://www.techspot.com/images2/news/bigimage/2020/12/2020-12-26-image-2.png)\\n\",\n        \"\\n\",\n        \"Vulkan is the best API, it's a successor to OpenGL\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://i.ytimg.com/vi/rvCD9FaTKCA/maxresdefault.jpg)\\n\",\n        \"- ![Alt Text](https://architosh.com/wp-content/uploads/2015/03/opengl-v-vulkan.jpg)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://registry.khronos.org/vulkan/site/guide/latest/_images/platforms_overview.png)\\n\",\n        \"\\n\",\n        \"- Let's leverage the video memory, the vRAM\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Division_of_labor_cpu_and_gpu.svg/500px-Division_of_labor_cpu_and_gpu.svg.png)\\n\",\n        \"\\n\",\n        \"\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"4EgFaPyrNV7y\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"```c\\n\",\n        \"%%writefile vulkan.cpp\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"#include <iostream>\\n\",\n        \"#include <fstream>\\n\",\n        \"#include <unordered_map>\\n\",\n        \"#include <vector>\\n\",\n        \"#include <cmath>\\n\",\n        \"#include <numeric>\\n\",\n        \"#include <algorithm>\\n\",\n        \"#include <chrono>  \\n\",\n        \"#include <vulkan/vulkan.h>\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"VkInstance instance;\\n\",\n        \"VkPhysicalDevice physicalDevice;\\n\",\n        \"VkDevice device;\\n\",\n        \"VkPipelineLayout pipelineLayout;\\n\",\n        \"VkCommandPool commandPool;\\n\",\n        \"VkCommandBuffer commandBuffer;\\n\",\n        \"VkPipeline computePipeline;\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"using namespace std;\\n\",\n        \"\\n\",\n        \"void createInstance() {\\n\",\n        \"    VkApplicationInfo appInfo{};\\n\",\n        \"    appInfo.sType = VK_STRUCTURE_TYPE_APPLICATION_INFO;\\n\",\n        \"    appInfo.pApplicationName = \\\"Language Model\\\";\\n\",\n        \"    appInfo.applicationVersion = VK_MAKE_VERSION(1, 0, 0);\\n\",\n        \"    appInfo.pEngineName = \\\"No Engine\\\";\\n\",\n        \"    appInfo.engineVersion = VK_MAKE_VERSION(1, 0, 0);\\n\",\n        \"    appInfo.apiVersion = VK_API_VERSION_1_0;\\n\",\n        \"\\n\",\n        \"    VkInstanceCreateInfo createInfo{};\\n\",\n        \"    createInfo.sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO;\\n\",\n        \"    createInfo.pApplicationInfo = &appInfo;\\n\",\n        \"\\n\",\n        \"    if (vkCreateInstance(&createInfo, nullptr, &instance) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to create Vulkan instance!\\\");\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"void runComputationOnGPU() {\\n\",\n        \"    // Start recording commands\\n\",\n        \"    VkCommandBufferBeginInfo beginInfo{};\\n\",\n        \"    beginInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;\\n\",\n        \"\\n\",\n        \"    if (vkBeginCommandBuffer(commandBuffer, &beginInfo) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to begin recording command buffer!\\\");\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    // Add your Vulkan compute dispatch command here\\n\",\n        \"    vkCmdBindPipeline(commandBuffer, VK_PIPELINE_BIND_POINT_COMPUTE, computePipeline);\\n\",\n        \"    vkCmdDispatch(commandBuffer, 1, 1, 1);\\n\",\n        \"\\n\",\n        \"    // End recording\\n\",\n        \"    if (vkEndCommandBuffer(commandBuffer) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to record command buffer!\\\");\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    // Submit the command buffer to the queue\\n\",\n        \"    VkSubmitInfo submitInfo{};\\n\",\n        \"    submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;\\n\",\n        \"    submitInfo.commandBufferCount = 1;\\n\",\n        \"    submitInfo.pCommandBuffers = &commandBuffer;\\n\",\n        \"\\n\",\n        \"    VkQueue computeQueue;  // Should be initialized properly\\n\",\n        \"    if (vkQueueSubmit(computeQueue, 1, &submitInfo, VK_NULL_HANDLE) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to submit command buffer!\\\");\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    vkQueueWaitIdle(computeQueue);  // Ensure the computation is finished\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"void pickPhysicalDevice() {\\n\",\n        \"    uint32_t deviceCount = 0;\\n\",\n        \"    vkEnumeratePhysicalDevices(instance, &deviceCount, nullptr);\\n\",\n        \"\\n\",\n        \"    if (deviceCount == 0) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to find GPUs with Vulkan support!\\\");\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    std::vector<VkPhysicalDevice> devices(deviceCount);\\n\",\n        \"    vkEnumeratePhysicalDevices(instance, &deviceCount, devices.data());\\n\",\n        \"\\n\",\n        \"    for (const auto& dev : devices) {\\n\",\n        \"        if (true /* Here we can put conditions for picking the best device */) {\\n\",\n        \"            physicalDevice = dev;\\n\",\n        \"            break;\\n\",\n        \"        }\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    if (physicalDevice == VK_NULL_HANDLE) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to find a suitable GPU!\\\");\\n\",\n        \"    }\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"void createLogicalDevice() {\\n\",\n        \"    VkDeviceQueueCreateInfo queueCreateInfo{};\\n\",\n        \"    queueCreateInfo.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;\\n\",\n        \"    queueCreateInfo.queueFamilyIndex = 0; // Should be determined properly\\n\",\n        \"    queueCreateInfo.queueCount = 1;\\n\",\n        \"\\n\",\n        \"    float queuePriority = 1.0f;\\n\",\n        \"    queueCreateInfo.pQueuePriorities = &queuePriority;\\n\",\n        \"\\n\",\n        \"    VkDeviceCreateInfo deviceInfo{};\\n\",\n        \"    deviceInfo.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO;\\n\",\n        \"    deviceInfo.pQueueCreateInfos = &queueCreateInfo;\\n\",\n        \"    deviceInfo.queueCreateInfoCount = 1;\\n\",\n        \"\\n\",\n        \"    VkPhysicalDeviceFeatures deviceFeatures{};\\n\",\n        \"    deviceInfo.pEnabledFeatures = &deviceFeatures;\\n\",\n        \"\\n\",\n        \"    if (vkCreateDevice(physicalDevice, &deviceInfo, nullptr, &device) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to create logical device!\\\");\\n\",\n        \"    }\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"void createCommandPool() {\\n\",\n        \"    VkCommandPoolCreateInfo poolInfo{};\\n\",\n        \"    poolInfo.sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO;\\n\",\n        \"    poolInfo.queueFamilyIndex = 0;  // Should be determined properly\\n\",\n        \"\\n\",\n        \"    if (vkCreateCommandPool(device, &poolInfo, nullptr, &commandPool) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to create command pool!\\\");\\n\",\n        \"    }\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"void createCommandBuffer() {\\n\",\n        \"    VkCommandBufferAllocateInfo allocInfo{};\\n\",\n        \"    allocInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;\\n\",\n        \"    allocInfo.commandPool = commandPool;\\n\",\n        \"    allocInfo.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;\\n\",\n        \"    allocInfo.commandBufferCount = 1;\\n\",\n        \"\\n\",\n        \"    if (vkAllocateCommandBuffers(device, &allocInfo, &commandBuffer) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to allocate command buffer!\\\");\\n\",\n        \"    }\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"VkBuffer createBuffer(VkDevice device, VkPhysicalDevice physicalDevice, VkDeviceSize size, VkBufferUsageFlags usage) {\\n\",\n        \"    VkBufferCreateInfo bufferInfo{};\\n\",\n        \"    bufferInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;\\n\",\n        \"    bufferInfo.size = size;\\n\",\n        \"    bufferInfo.usage = usage;\\n\",\n        \"    bufferInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;\\n\",\n        \"\\n\",\n        \"    VkBuffer buffer;\\n\",\n        \"    if (vkCreateBuffer(device, &bufferInfo, nullptr, &buffer) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to create buffer.\\\");\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    return buffer;\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"void copyDataToBuffer(VkDevice device, VkBuffer buffer, void* data, VkDeviceSize size) {\\n\",\n        \"    // Code to map buffer memory and copy data to buffer\\n\",\n        \"    // Implementation can vary based on how you've set up memory\\n\",\n        \"    // Typically you'd use vkMapMemory, memcpy, and vkUnmapMemory\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"void createAndFillBuffer(VkDevice device, VkPhysicalDevice physicalDevice, void* data, VkDeviceSize size, VkBufferUsageFlags usage, VkBuffer& buffer) {\\n\",\n        \"    buffer = createBuffer(device, physicalDevice, size, usage);\\n\",\n        \"    copyDataToBuffer(device, buffer, data, size);\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"VkShaderModule createShaderModule(VkDevice device, const std::vector<char>& code) {\\n\",\n        \"    VkShaderModuleCreateInfo createInfo{};\\n\",\n        \"    createInfo.sType = VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO;\\n\",\n        \"    createInfo.codeSize = code.size();\\n\",\n        \"    createInfo.pCode = reinterpret_cast<const uint32_t*>(code.data());\\n\",\n        \"\\n\",\n        \"    VkShaderModule shaderModule;\\n\",\n        \"    if (vkCreateShaderModule(device, &createInfo, nullptr, &shaderModule) != VK_SUCCESS) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to create shader module\\\");\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    return shaderModule;\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"void createShaderModuler()  {\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"    VkPipelineLayout pipelineLayout;\\n\",\n        \"VkPipelineLayoutCreateInfo pipelineLayoutInfo{};\\n\",\n        \"pipelineLayoutInfo.sType = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO;\\n\",\n        \"pipelineLayoutInfo.setLayoutCount = 0; // Optional\\n\",\n        \"pipelineLayoutInfo.pSetLayouts = nullptr; // Optional\\n\",\n        \"pipelineLayoutInfo.pushConstantRangeCount = 0; // Optional\\n\",\n        \"pipelineLayoutInfo.pPushConstantRanges = nullptr; // Optional\\n\",\n        \"\\n\",\n        \"if (vkCreatePipelineLayout(device, &pipelineLayoutInfo, nullptr, &pipelineLayout) != VK_SUCCESS) {\\n\",\n        \"    throw std::runtime_error(\\\"Failed to create pipeline layout!\\\");\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"std::vector<char> readShaderFile(const std::string& filename) {\\n\",\n        \"    std::ifstream file(filename, std::ios::ate | std::ios::binary);\\n\",\n        \"\\n\",\n        \"    if (!file.is_open()) {\\n\",\n        \"        throw std::runtime_error(\\\"Failed to open file!\\\");\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    size_t fileSize = (size_t) file.tellg();\\n\",\n        \"    std::vector<char> buffer(fileSize);\\n\",\n        \"\\n\",\n        \"    file.seekg(0);\\n\",\n        \"    file.read(buffer.data(), fileSize);\\n\",\n        \"\\n\",\n        \"    file.close();\\n\",\n        \"\\n\",\n        \"    return buffer;\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"void createPipeline()  {\\n\",\n        \"\\n\",\n        \"VkPipeline computePipeline;\\n\",\n        \"VkComputePipelineCreateInfo pipelineInfo{};\\n\",\n        \"pipelineInfo.sType = VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO;\\n\",\n        \"pipelineInfo.stage.sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;\\n\",\n        \"pipelineInfo.stage.stage = VK_SHADER_STAGE_COMPUTE_BIT;\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"std::vector<char> shaderCode = readShaderFile(\\\"/comp.spv\\\");  // Make sure this path is correct\\n\",\n        \"VkShaderModule shaderModule = createShaderModule(device, shaderCode);\\n\",\n        \"\\n\",\n        \"pipelineInfo.stage.module = shaderModule; // The shaderModule you created\\n\",\n        \"pipelineInfo.stage.pName = \\\"main\\\";\\n\",\n        \"pipelineInfo.layout = pipelineLayout;\\n\",\n        \"\\n\",\n        \"if (vkCreateComputePipelines(device, VK_NULL_HANDLE, 1, &pipelineInfo, nullptr, &computePipeline) != VK_SUCCESS) {\\n\",\n        \"    throw std::runtime_error(\\\"Failed to create compute pipeline!\\\");\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"int main() {\\n\",\n        \"\\n\",\n        \"      try {\\n\",\n        \"        createInstance();\\n\",\n        \"        pickPhysicalDevice();\\n\",\n        \"        createLogicalDevice();\\n\",\n        \"        createShaderModuler();\\n\",\n        \"        createPipeline();\\n\",\n        \"        createCommandPool();\\n\",\n        \"        createCommandBuffer();\\n\",\n        \"    } catch (const std::exception& e) {\\n\",\n        \"        std::cerr << e.what() << std::endl;\\n\",\n        \"        return EXIT_FAILURE;\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"    string data = \\\"abcde\\\";\\n\",\n        \"    unordered_map<char, int> vocab;\\n\",\n        \"    unordered_map<int, char> inv_vocab;\\n\",\n        \"    int vocab_size = 0;\\n\",\n        \"\\n\",\n        \"    for (char c : data) {\\n\",\n        \"        vocab[c] = vocab_size;\\n\",\n        \"        inv_vocab[vocab_size] = c;\\n\",\n        \"        vocab_size++;\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    vector<vector<double>> w(vocab_size, vector<double>(vocab_size, rand() % 100 / 100.0));\\n\",\n        \"    double lr = 0.1;\\n\",\n        \"    vector<int> data_idx;\\n\",\n        \"\\n\",\n        \"    for (char c : data) {\\n\",\n        \"        data_idx.push_back(vocab[c]);\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    for (int i = 0; i < data_idx.size() - 1; ++i) {\\n\",\n        \"        int x = data_idx[i], y = data_idx[i + 1];\\n\",\n        \"        vector<double> x_onehot(vocab_size, 0);\\n\",\n        \"        x_onehot[x] = 1;\\n\",\n        \"\\n\",\n        \"        vector<double> pred(vocab_size);\\n\",\n        \"        for (int j = 0; j < vocab_size; ++j) {\\n\",\n        \"            pred[j] = exp(x_onehot[j] * w[j][x]);\\n\",\n        \"        }\\n\",\n        \"\\n\",\n        \"        double sum_pred = accumulate(pred.begin(), pred.end(), 0.0);\\n\",\n        \"        for (double &p : pred) p /= sum_pred;\\n\",\n        \"\\n\",\n        \"        double loss = -log(pred[y]);\\n\",\n        \"\\n\",\n        \"        vector<double> grad = pred;\\n\",\n        \"        grad[y] -= 1;\\n\",\n        \"\\n\",\n        \"        for (int j = 0; j < vocab_size; ++j) {\\n\",\n        \"            w[j][x] -= lr * grad[j];\\n\",\n        \"        }\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"\\n\",\n        \" // Start timer\\n\",\n        \"    auto start_time = std::chrono::high_resolution_clock::now();\\n\",\n        \"\\n\",\n        \"    char input_char = 'a';\\n\",\n        \"    int input_idx = vocab[input_char];\\n\",\n        \"    vector<double> input_onehot(vocab_size, 0);\\n\",\n        \"    input_onehot[input_idx] = 1;\\n\",\n        \"\\n\",\n        \"    runComputationOnGPU();\\n\",\n        \"    vector<double> output_prob(vocab_size);\\n\",\n        \"    for (int i = 0; i < vocab_size; ++i) {\\n\",\n        \"        output_prob[i] = exp(input_onehot[i] * w[i][input_idx]);\\n\",\n        \"    }\\n\",\n        \"\\n\",\n        \"    double sum_output = accumulate(output_prob.begin(), output_prob.end(), 0.0);\\n\",\n        \"    for (double &p : output_prob) p /= sum_output;\\n\",\n        \"\\n\",\n        \"    int output_idx = max_element(output_prob.begin(), output_prob.end()) -\\n\",\n        \"output_prob.begin();\\n\",\n        \"    char output_char = inv_vocab[output_idx];\\n\",\n        \"\\n\",\n        \"    // Stop timer and calculate elapsed time in milliseconds\\n\",\n        \"    auto end_time = std::chrono::high_resolution_clock::now();\\n\",\n        \"    auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(end_time -\\n\",\n        \"start_time).count();\\n\",\n        \"\\n\",\n        \"    cout << \\\"Given '\\\" << input_char << \\\"', next predicted character is '\\\" << output_char <<\\n\",\n        \"\\\"'\\\\n\\\";\\n\",\n        \"    cout << \\\"Inference time: \\\" << duration << \\\" nanosecondss\\\\n\\\";\\n\",\n        \"    return 0;\\n\",\n        \"}\\n\",\n        \"\\n\",\n        \"```\"\n      ],\n      \"metadata\": {\n        \"id\": \"0Sk50gUFxT_G\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"Run the following 2 commands to compile.\\n\",\n        \"\\n\",\n        \"- g++ vulkan.cpp -lvulkan -o vulkan -std=c++11\\n\",\n        \"- !./vulkan\"\n      ],\n      \"metadata\": {\n        \"id\": \"u_f_vctWxcPZ\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## Step 5 - Accelerate on iOS & Android GPUs using Compiler #4: Tensor Virtual Machine\"\n      ],\n      \"metadata\": {\n        \"id\": \"zkcnpOid4LVm\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"# Hardware Optimization Strategies for Machine Learning\\n\",\n        \"\\n\",\n        \"- There are many ways to make Inference faster.\\n\",\n        \"- Each GPU process data differently.\\n\",\n        \"- Below are some examples\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://ars.els-cdn.com/content/image/1-s2.0-S1383762120301430-gr26.jpg)\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"## General GPU Optimizations\\n\",\n        \"\\n\",\n        \"- **Memory Coalescing**: Structure your memory access patterns to make them contiguous and aligned, reducing memory latency.\\n\",\n        \"  \\n\",\n        \"- **Loop Unrolling**: Reduce the overhead of the loop control code to improve performance.\\n\",\n        \"\\n\",\n        \"- **Data Tiling**: Break the data into smaller chunks or \\\"tiles\\\" to fit into faster but smaller memory caches.\\n\",\n        \"\\n\",\n        \"- **Vectorization**: Use SIMD (Single Instruction, Multiple Data) operations to process multiple data points in a single instruction.\\n\",\n        \"\\n\",\n        \"- **Batching**: Process multiple inputs at the same time to make better use of the hardware.\\n\",\n        \"\\n\",\n        \"- **Prefetching**: Manually load data into cache before it's needed to reduce latency.\\n\",\n        \"\\n\",\n        \"- **Pipeline Parallelism**: Overlap the execution of independent operations.\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"## CUDA-Specific Optimizations\\n\",\n        \"\\n\",\n        \"- **Shared Memory**: Use shared memory to avoid redundant global memory access.\\n\",\n        \"\\n\",\n        \"- **Warp Shuffle**: Use warp shuffle functions for faster intra-warp communication.\\n\",\n        \"\\n\",\n        \"- **Stream Multiprocessor Occupancy**: Maximize the use of stream multiprocessors for higher throughput.\\n\",\n        \"\\n\",\n        \"- **Zero-Copy**: For host-device communication, use zero-copy memory to reduce data transfer times.\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"## Metal-Specific Optimizations\\n\",\n        \"\\n\",\n        \"- **Threadgroup Memory**: Use threadgroup shared memory for faster data sharing among threads.\\n\",\n        \"\\n\",\n        \"- **Metal Performance Shaders**: Use built-in high-performance shaders for common operations where available.\\n\",\n        \"\\n\",\n        \"- **Resource Groups**: Use argument buffers to efficiently manage resources.\\n\",\n        \"\\n\",\n        \"- **Half-Precision (fp16)**: Use half-precision floating-point arithmetic where full precision isn't required for performance gain.\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"## (AMD) ROCm-Specific Optimizations\\n\",\n        \"\\n\",\n        \"- **Local Data Share (LDS)**: Similar to CUDA's shared memory, use LDS to share data between threads in a wavefront.\\n\",\n        \"\\n\",\n        \"- **Wavefront Optimizations**: Utilize the full wavefront for computations to improve performance.\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"## OpenCL-Specific Optimizations\\n\",\n        \"\\n\",\n        \"- **Local Memory**: Like CUDA's shared memory and ROCm's LDS, use local memory to store frequently accessed data.\\n\",\n        \"\\n\",\n        \"- **Event-driven Execution**: Use OpenCL events to manage command execution dependencies effectively.\\n\",\n        \"\\n\",\n        \"- **Image Objects**: Use OpenCL image objects for better caching behavior if applicable.\\n\",\n        \"\\n\",\n        \"---\\n\",\n        \"\\n\",\n        \"..... this seems hard to learn\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"### Let's instead learn how to use the Tensor Virtual Machine, a cross platform compiler and runtime for Machine Learning models\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://www.opensourceforu.com/wp-content/uploads/2019/06/Figure-1-The-TVM-stack.jpg)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://developer.qualcomm.com/sites/default/files/attachments/qdn-blog-post-opencl-tvm-image01-800.png)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://i.imgur.com/BCg6gCz.png)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://russianblogs.com/images/240/46786653c6e5d7f59d4dee3d26aa6718.png)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://www.researchgate.net/publication/348753417/figure/fig6/AS:984105013497856@1611640327149/An-example-of-blocks-for-scheduling-optimization-with-their-corresponding-scheduling.png)\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"## 6 Step High Level Process\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"1. Model Import: First, the PyTorch model is imported into TVM.\\n\",\n        \"\\n\",\n        \"2. Optimization: TVM optimizes the computational graph of the model, performing various transformations and applying optimizations like quantization.\\n\",\n        \"\\n\",\n        \"3. Partitioning: The optimized model is then divided into smaller chunks, or \\\"shards\\\". This is typically done in such a way as to minimize dependencies between the shards so that they can be loaded and run independently.\\n\",\n        \"\\n\",\n        \"4. Code Generation: TVM generates the code for each shard, targeting the specific architecture (iOS or Android).\\n\",\n        \"\\n\",\n        \"5. Packaging: The shards are bundled into separate binary files, often with metadata to indicate their sequence and dependencies.\\n\",\n        \"\\n\",\n        \"6. Deployment: These binary shards can then be deployed onto the mobile device. The application logic is responsible for loading the shards as needed and performing the inference.\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"n7HfbfpTOcnd\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"!pip uninstall -y numpy\\n\",\n        \"!pip uninstall -y setuptools\\n\",\n        \"!pip install setuptools\\n\",\n        \"!pip install numpy\"\n      ],\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\",\n          \"height\": 374\n        },\n        \"id\": \"z_1q7Qm1l99A\",\n        \"outputId\": \"3a128d6f-906f-4ae3-f357-270f848c971a\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Found existing installation: numpy 1.23.5\\n\",\n            \"Uninstalling numpy-1.23.5:\\n\",\n            \"  Successfully uninstalled numpy-1.23.5\\n\",\n            \"Found existing installation: setuptools 67.7.2\\n\",\n            \"Uninstalling setuptools-67.7.2:\\n\",\n            \"  Successfully uninstalled setuptools-67.7.2\\n\",\n            \"Collecting setuptools\\n\",\n            \"  Using cached setuptools-68.2.2-py3-none-any.whl (807 kB)\\n\",\n            \"Installing collected packages: setuptools\\n\",\n            \"\\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\\n\",\n            \"ipython 7.34.0 requires jedi>=0.16, which is not installed.\\n\",\n            \"numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0 which is incompatible.\\n\",\n            \"tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.0 which is incompatible.\\u001b[0m\\u001b[31m\\n\",\n            \"\\u001b[0mSuccessfully installed setuptools-68.2.2\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"display_data\",\n          \"data\": {\n            \"application/vnd.colab-display-data+json\": {\n              \"pip_warning\": {\n                \"packages\": [\n                  \"_distutils_hack\",\n                  \"pkg_resources\",\n                  \"setuptools\"\n                ]\n              }\n            }\n          },\n          \"metadata\": {}\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.0)\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"!pip install apache-tvm\\n\"\n      ],\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"FneO_t8YmKn6\",\n        \"outputId\": \"a6006539-9d30-4072-ccc5-87205e7f453f\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Collecting apache-tvm\\n\",\n            \"  Downloading apache_tvm-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.2 MB)\\n\",\n            \"\\u001b[2K     \\u001b[90m\\u001b[0m \\u001b[32m47.2/47.2 MB\\u001b[0m \\u001b[31m30.1 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n\",\n            \"\\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (23.1.0)\\n\",\n            \"Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (2.2.1)\\n\",\n            \"Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.4.2)\\n\",\n            \"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.23.5)\\n\",\n            \"Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (5.9.5)\\n\",\n            \"Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.11.2)\\n\",\n            \"Collecting synr==0.6.0 (from apache-tvm)\\n\",\n            \"  Downloading synr-0.6.0-py3-none-any.whl (18 kB)\\n\",\n            \"Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (6.3.2)\\n\",\n            \"Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.5.0)\\n\",\n            \"Installing collected packages: synr, apache-tvm\\n\",\n            \"Successfully installed apache-tvm-0.11.1 synr-0.6.0\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"!python3 -m pip install mlc-ai-nightly -f https://mlc.ai/wheels\\n\",\n        \"!python3 -m pip install torch torchvision torchaudio torchsummary --extra-index-url https://download.pytorch.org/whl/cpu\"\n      ],\n      \"metadata\": {\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"id\": \"Yx4V-0s6n-9q\",\n        \"outputId\": \"40b86e0a-c043-410a-cb2d-ec0e4ac736a3\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Looking in links: https://mlc.ai/wheels\\n\",\n            \"Collecting mlc-ai-nightly\\n\",\n            \"  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly-0.12.dev1576-cp310-cp310-manylinux_2_28_x86_64.whl (86.3 MB)\\n\",\n            \"\\u001b[2K     \\u001b[90m\\u001b[0m \\u001b[32m86.3/86.3 MB\\u001b[0m \\u001b[31m15.5 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n\",\n            \"\\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (23.1.0)\\n\",\n            \"Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (2.2.1)\\n\",\n            \"Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (4.4.2)\\n\",\n            \"Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (0.2.0)\\n\",\n            \"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (1.23.5)\\n\",\n            \"Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (5.9.5)\\n\",\n            \"Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (1.11.2)\\n\",\n            \"Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (6.3.2)\\n\",\n            \"Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (4.5.0)\\n\",\n            \"Installing collected packages: mlc-ai-nightly\\n\",\n            \"Successfully installed mlc-ai-nightly-0.12.dev1576\\n\",\n            \"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\\n\",\n            \"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\\n\",\n            \"Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\\n\",\n            \"Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\\n\",\n            \"Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\\n\",\n            \"Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\\n\",\n            \"Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\\n\",\n            \"Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\\n\",\n            \"Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\\n\",\n            \"Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\\n\",\n            \"Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\\n\",\n            \"Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\\n\",\n            \"Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\\n\",\n            \"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\\n\",\n            \"Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\\n\",\n            \"Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\\n\",\n            \"Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\\n\",\n            \"Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\\n\",\n            \"Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\\n\",\n            \"Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\\n\",\n            \"Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\\n\",\n            \"Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"import IPython\\n\",\n        \"import numpy as np\\n\",\n        \"import tvm\\n\",\n        \"from tvm import relax\\n\",\n        \"from tvm.ir.module import IRModule\\n\",\n        \"from tvm.script import relax as R\\n\",\n        \"from tvm.script import tir as T\"\n      ],\n      \"metadata\": {\n        \"id\": \"IMZaf5hHN0KY\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"import torch\\n\",\n        \"import torchvision\\n\",\n        \"\\n\",\n        \"test_data = torchvision.datasets.FashionMNIST(\\n\",\n        \"    root=\\\"data\\\",\\n\",\n        \"    train=False,\\n\",\n        \"    download=True,\\n\",\n        \"    transform=torchvision.transforms.ToTensor()\\n\",\n        \")\\n\",\n        \"test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\\n\",\n        \"class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\\n\",\n        \"               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\\n\",\n        \"\\n\",\n        \"img, label = next(iter(test_loader))\\n\",\n        \"img = img.reshape(1, 28, 28).numpy()\"\n      ],\n      \"metadata\": {\n        \"id\": \"ddHa_AgVN05I\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"b113afde-8cb0-44c9-efd7-f37156afdf29\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\\n\",\n            \"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stderr\",\n          \"text\": [\n            \"100%|| 26421880/26421880 [00:00<00:00, 112959609.42it/s]\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\\n\",\n            \"\\n\",\n            \"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\\n\",\n            \"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stderr\",\n          \"text\": [\n            \"100%|| 29515/29515 [00:00<00:00, 4669742.84it/s]\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\\n\",\n            \"\\n\",\n            \"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\\n\",\n            \"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stderr\",\n          \"text\": [\n            \"100%|| 4422102/4422102 [00:00<00:00, 64735997.47it/s]\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\\n\",\n            \"\\n\",\n            \"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\\n\",\n            \"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stderr\",\n          \"text\": [\n            \"100%|| 5148/5148 [00:00<00:00, 5152058.46it/s]\\n\"\n          ]\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\\n\",\n            \"\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"import matplotlib.pyplot as plt\\n\",\n        \"\\n\",\n        \"plt.figure()\\n\",\n        \"plt.imshow(img[0])\\n\",\n        \"plt.colorbar()\\n\",\n        \"plt.grid(False)\\n\",\n        \"plt.show()\\n\",\n        \"print(\\\"Class:\\\", class_names[label[0]])\"\n      ],\n      \"metadata\": {\n        \"id\": \"ujJDpXLiN4ei\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\",\n          \"height\": 452\n        },\n        \"outputId\": \"58cb4a87-39c9-4c4f-867a-342b09e9a183\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"display_data\",\n          \"data\": {\n            \"text/plain\": [\n              \"<Figure size 640x480 with 2 Axes>\"\n            ],\n            \"image/png\": \"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\\n\"\n          },\n          \"metadata\": {}\n        },\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Class: Bag\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# Hide outputs\\n\",\n        \"!wget https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl\"\n      ],\n      \"metadata\": {\n        \"id\": \"jMcubOsiN6vv\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"d81155c6-20ab-4eb7-a927-3d0692ed0662\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"--2023-09-18 18:54:02--  https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl\\n\",\n            \"Resolving github.com (github.com)... 140.82.121.3\\n\",\n            \"Connecting to github.com (github.com)|140.82.121.3|:443... connected.\\n\",\n            \"HTTP request sent, awaiting response... 302 Found\\n\",\n            \"Location: https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl [following]\\n\",\n            \"--2023-09-18 18:54:02--  https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl\\n\",\n            \"Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\\n\",\n            \"Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\\n\",\n            \"HTTP request sent, awaiting response... 200 OK\\n\",\n            \"Length: 407396 (398K) [application/octet-stream]\\n\",\n            \"Saving to: fasionmnist_mlp_params.pkl\\n\",\n            \"\\n\",\n            \"fasionmnist_mlp_par 100%[===================>] 397.85K  --.-KB/s    in 0.01s   \\n\",\n            \"\\n\",\n            \"2023-09-18 18:54:02 (30.5 MB/s) - fasionmnist_mlp_params.pkl saved [407396/407396]\\n\",\n            \"\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"def numpy_mlp(data, w0, b0, w1, b1):\\n\",\n        \"    lv0 = data @ w0.T + b0\\n\",\n        \"    lv1 = np.maximum(lv0, 0)\\n\",\n        \"    lv2 = lv1 @ w1.T + b1\\n\",\n        \"    return lv2\"\n      ],\n      \"metadata\": {\n        \"id\": \"YV23AU8LN9Fe\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"import pickle as pkl\\n\",\n        \"\\n\",\n        \"mlp_params = pkl.load(open(\\\"fasionmnist_mlp_params.pkl\\\", \\\"rb\\\"))\\n\",\n        \"res = numpy_mlp(img.reshape(1, 784),\\n\",\n        \"                mlp_params[\\\"w0\\\"],\\n\",\n        \"                mlp_params[\\\"b0\\\"],\\n\",\n        \"                mlp_params[\\\"w1\\\"],\\n\",\n        \"                mlp_params[\\\"b1\\\"])\\n\",\n        \"print(res)\\n\",\n        \"pred_kind = res.argmax(axis=1)\\n\",\n        \"print(pred_kind)\\n\",\n        \"print(\\\"NumPy-MLP Prediction:\\\", class_names[pred_kind[0]])\"\n      ],\n      \"metadata\": {\n        \"id\": \"0GHkLb7KN_cX\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"4b174f5b-0fac-460d-d5a7-c6914c0cdda9\"\n      },\n      \"execution_count\": null,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"[[-36.13299   -47.72502    -9.275949  -18.921837   -7.161888   -9.535042\\n\",\n            \"   -9.7998495 -30.950085   29.353281  -22.556564 ]]\\n\",\n            \"[8]\\n\",\n            \"NumPy-MLP Prediction: Bag\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"#### Optimization\\n\",\n        \"1. Graph-Level Optimization: TVM starts with optimizations at the computation graph level. It takes the imported PyTorch model and tries to simplify its computational graph. This involves fusing nodes, eliminating redundancies, and reordering operations.\\n\",\n        \"\\n\",\n        \"2. Layout Transformation: The layout of the tensors (NCHW, NHWC, etc.) can be changed to best suit the hardware. Different layouts can have a significant impact on performance due to the access patterns of the memory.\\n\",\n        \"\\n\",\n        \"3. Kernel Matching: TVM uses a library of optimized kernel implementations for common operations. It attempts to map the operations in the computational graph to these optimized kernels.\\n\",\n        \"\\n\",\n        \"4. Quantization: If applicable, this is the stage where model weights and activations may be quantized to lower precision formats like INT8 to reduce model size and increase inference speed.\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://www.researchgate.net/publication/348753417/figure/fig6/AS:984105013497856@1611640327149/An-example-of-blocks-for-scheduling-optimization-with-their-corresponding-scheduling.png)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://www.allaboutcircuits.com/uploads/articles/qc-tech_quantization_gif-2_final.jpg)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://www.allaboutcircuits.com/uploads/articles/neuron_connections_quantization.jpg)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://tvm.apache.org/images/intro-auto-scheduler/search_overview.png)\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://www.nvidia.com/content/dam/en-zz/Solutions/glossary/data-science/xgboost/img-3.png)\\n\",\n        \"\\n\",\n        \"#### Partitioning\\n\",\n        \"1. Dependency Analysis: TVM analyzes the computation graph to identify dependencies between nodes. The goal is to break the graph into independent subgraphs that can be executed in parallel or loaded separately.\\n\",\n        \"\\n\",\n        \"2. Shard Formation: Once dependencies are mapped, the graph is divided into smaller segments, each forming a \\\"shard\\\". These shards are designed such that each can be loaded and executed independently of the others as much as possible.\\n\",\n        \"\\n\",\n        \"3. Metadata Creation: Metadata is generated for each shard, specifying its dependencies, the sequence in which it should be executed, and other information that will be necessary for loading and running the shard.\\n\",\n        \"\\n\",\n        \"4. Shard Optimization: After partitioning, each shard is subject to another round of optimization. This is because breaking the graph can sometimes introduce inefficiencies, like redundant operations that were previously fused.\\n\",\n        \"\\n\",\n        \"- ![Alt Text](https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs43588-021-00119-7/MediaObjects/43588_2021_119_Fig1_HTML.png)\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"#### Code Generation\\n\",\n        \"\\n\",\n        \"1. Backend Selection: TVM allows you to specify the target backend, such as Vulkan for Android or Metal for iOS. Depending on this choice, different code generation pathways may be used.\\n\",\n        \"\\n\",\n        \"2. Lowering: The high-level operations in each shard are transformed into low-level, hardware-specific instructions. This is often referred to as \\\"lowering\\\" the computation graph.\\n\",\n        \"\\n\",\n        \"3. Compilation: Once the operations have been lowered, they are compiled into machine code optimized for the target architecture. This code is what will actually be run on the mobile device.\\n\",\n        \"\\n\",\n        \"4. Packaging: Finally, the compiled machine code for each shard is packaged into a binary file format (.so for Android, .dylib for iOS, etc.). These binary files are now ready for deployment.\"\n      ],\n      \"metadata\": {\n        \"id\": \"hYbfG8J6Wf87\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"from tvm.script import ir as I\\n\",\n        \"from tvm.script import tir as T\\n\",\n        \"from tvm.script import relax as R\\n\",\n        \"#seperate computation and allocation\\n\",\n        \"\\n\",\n        \"@I.ir_module\\n\",\n        \"class Module:\\n\",\n        \"    @T.prim_func\\n\",\n        \"    def linear0(x: T.handle, w: T.handle, b: T.handle, z: T.handle):\\n\",\n        \"        m = T.int64()\\n\",\n        \"        X = T.match_buffer(x, (1, m))\\n\",\n        \"        n = T.int64()\\n\",\n        \"        W = T.match_buffer(w, (n, m))\\n\",\n        \"        B = T.match_buffer(b, (n,))\\n\",\n        \"        Z = T.match_buffer(z, (1, n))\\n\",\n        \"        # with T.block(\\\"root\\\"):\\n\",\n        \"        Y = T.alloc_buffer((1, n))\\n\",\n        \"        for i, j, k in T.grid(1, n, m):\\n\",\n        \"            with T.block(\\\"Y\\\"):\\n\",\n        \"                vi, vj, vk = T.axis.remap(\\\"SSR\\\", [i, j, k])\\n\",\n        \"                T.reads(X[vi, vk], W[vj, vk])\\n\",\n        \"                T.writes(Y[vi, vj])\\n\",\n        \"                with T.init():\\n\",\n        \"                    Y[vi, vj] = T.float32(0)\\n\",\n        \"                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]\\n\",\n        \"        for i, j in T.grid(1, n):\\n\",\n        \"            with T.block(\\\"Z\\\"):\\n\",\n        \"                vi, vj = T.axis.remap(\\\"SS\\\", [i, j])\\n\",\n        \"                T.reads(Y[vi, vj], B[vj])\\n\",\n        \"                T.writes(Z[vi, vj])\\n\",\n        \"                Z[vi, vj] = Y[vi, vj] + B[vj]\\n\",\n        \"\\n\",\n        \"    @T.prim_func\\n\",\n        \"    def relu0(x: T.handle, y: T.handle):\\n\",\n        \"        n = T.int64()\\n\",\n        \"        X = T.match_buffer(x, (1, n))\\n\",\n        \"        Y = T.match_buffer(y, (1, n))\\n\",\n        \"        # with T.block(\\\"root\\\"):\\n\",\n        \"        for i, j in T.grid(1, n):\\n\",\n        \"            with T.block(\\\"Y\\\"):\\n\",\n        \"                vi, vj = T.axis.remap(\\\"SS\\\", [i, j])\\n\",\n        \"                T.reads(X[vi, vj])\\n\",\n        \"                T.writes(Y[vi, vj])\\n\",\n        \"                Y[vi, vj] = T.max(X[vi, vj], T.float32(0))\\n\",\n        \"\\n\",\n        \"    @R.function\\n\",\n        \"    def main(x: R.Tensor((1, \\\"m\\\"), dtype=\\\"float32\\\"), w0: R.Tensor((\\\"n\\\", \\\"m\\\"), dtype=\\\"float32\\\"), b0: R.Tensor((\\\"n\\\",), dtype=\\\"float32\\\"), w1: R.Tensor((\\\"k\\\", \\\"n\\\"), dtype=\\\"float32\\\"), b1: R.Tensor((\\\"k\\\",), dtype=\\\"float32\\\")) -> R.Tensor((1, \\\"k\\\"), dtype=\\\"float32\\\"):\\n\",\n        \"        k = T.int64()\\n\",\n        \"        m = T.int64()\\n\",\n        \"        n = T.int64()\\n\",\n        \"        with R.dataflow():\\n\",\n        \"            lv0 = R.call_dps_packed(\\\"linear0\\\", (x, w0, b0), out_sinfo=R.Tensor((1, n), dtype=\\\"float32\\\"))\\n\",\n        \"            lv1 = R.call_dps_packed(\\\"relu0\\\", (lv0,), out_sinfo=R.Tensor((1, n), dtype=\\\"float32\\\"))\\n\",\n        \"            out = R.call_dps_packed(\\\"linear0\\\", (lv1, w1, b1), out_sinfo=R.Tensor((1, k), dtype=\\\"float32\\\"))\\n\",\n        \"            R.output(out)\\n\",\n        \"        return out\"\n      ],\n      \"metadata\": {\n        \"id\": \"PTykold3OB0c\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"ex = relax.build(MyModule, target=\\\"llvm\\\")\\n\",\n        \"type(ex)\"\n      ],\n      \"metadata\": {\n        \"id\": \"eEogZGA3ODyH\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"vm = relax.VirtualMachine(ex, tvm.cpu())\"\n      ],\n      \"metadata\": {\n        \"id\": \"B8RDSdUfOGrz\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"data_nd = tvm.nd.array(img.reshape(1, 784))\\n\",\n        \"nd_params = {k: tvm.nd.array(v) for k, v in mlp_params.items()}\"\n      ],\n      \"metadata\": {\n        \"id\": \"fEU2MCu9OIdW\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"nd_res = vm[\\\"main\\\"](data_nd,\\n\",\n        \"                    nd_params[\\\"w0\\\"],\\n\",\n        \"                    nd_params[\\\"b0\\\"],\\n\",\n        \"                    nd_params[\\\"w1\\\"],\\n\",\n        \"                    nd_params[\\\"b1\\\"])\\n\",\n        \"print(nd_res)\"\n      ],\n      \"metadata\": {\n        \"id\": \"czq7d0cJOJ1W\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"pred_kind = np.argmax(nd_res.numpy(), axis=1)\\n\",\n        \"print(\\\"MyModule Prediction:\\\", class_names[pred_kind[0]])\"\n      ],\n      \"metadata\": {\n        \"id\": \"fyEyr11yOLeC\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## Step 6 - Compile a Large Language Model from HuggingFace to iOS & Android with TVM\\n\",\n        \"- MLC LLM https://mlc.ai/mlc-llm/docs/install/tvm.html\"\n      ],\n      \"metadata\": {\n        \"id\": \"OBFI-dOI4TMH\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"## Step 7 - Future Directions\"\n      ],\n      \"metadata\": {\n        \"id\": \"MJHNTRsL4YtJ\"\n      }\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"\\n\",\n        \"\\n\",\n        \"## Dr Dignity Next Steps\\n\",\n        \"\\n\",\n        \"1. Port RWKV3B to an Android APK\\n\",\n        \"2. Fine-tuneMarx_3Bon medical data to pass the US Medical Licensing Exam (i'm calling it MedMarx_3b)\\n\",\n        \"3. Port MedMarx_3b to an Android APK\\n\",\n        \"4. Fine-Tune MedMarx_3B on code dataset (CodeMedMarx_3B)\\n\",\n        \"5. PortCodeMedMarx_3B to an Android APK as a Code Interpreter\\n\",\n        \"6. Use the Code Interpreter as a retrieval system on a local medical datastore to cite sources\\n\",\n        \"7. Fine-Tune CodeMedMarx_3B on biomedical image dataset for multimodal QA\\n\",\n        \"8. Continue Fine-tuning on all modalities of biomedical data\\n\",\n        \"9. PortMultiModalCodeMedMarx_3B to an Android APK\\n\",\n        \"10. Build Local Video Avatar Interface for MultiModalCodeMedMarx_3B\\n\",\n        \"\\n\",\n        \"The goal is to continually improve Dr Dignity's response accuracy until it replaces the need for human Doctors.\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"### Summary:\\n\",\n        \"\\n\",\n        \"Option 1: Learn Vulkan for Android and Learn Metal for iOS\\n\",\n        \"- Implement Android GPU Inference code in Vulkan\\n\",\n        \"- Implement iOS GPU Inference code in Metal\\n\",\n        \"\\n\",\n        \"(Better) Option 2: Learn Tensor Virtual Machine for iOS and Android\\n\",\n        \"- Implement Cross Platform GPU inference code in Relay\\n\",\n        \"- Optimize Relay code for Android or iOS\\n\",\n        \"- Compile Relay code to Android or iOS\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"\\n\"\n      ],\n      \"metadata\": {\n        \"id\": \"2GbMBszSxex9\"\n      }\n    }\n  ]\n}"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.4873046875,
          "content": "# Doctor Dignity\n<p align=\"center\">\n\n\nDISCLAIMER - Do not take any advice from Doctor Dignity seriously yet. This is a work in progress and taking any advice seriously could result in serious injury or even death. \n\n<img src=\"https://i.imgur.com/18jVWiV.png\" width=\"400\" height=\"400\">\n</p>\n\n## Overview\nDoctor Dignity is a Large Language Model that can pass the US Medical Licensing Exam. This is an open-source project with a mission to provide everyone their own private doctor. Doctor Dignity is a version of Meta's [Llama2](https://ai.meta.com/llama/) 7 billion parameter Large Language Model that was fine-tuned on a Medical Dialogue Dataset, then further improved using Reinforcement Learning & Constitutional AI. Since the model is only 3 Gigabytes in size, it fits on any local device, so there is no need to pay an API to use it. It's free, made for offline usage which preserves patient confidentiality, and it's available on iOS, Android, and Web. Pull requests for feature additions and improvements are encouraged.\n\n## Dependencies\n- [Numpy](https://numpy.org/install/)           (Use matrix math operations)\n- [PyTorch](https://pytorch.org/)         (Build Deep Learning models)\n- [Datasets](https://huggingface.co/docs/datasets/index)        (Access datasets from huggingface hub)\n- [Huggingface_hub](https://huggingface.co/docs/huggingface_hub/v0.5.1/en/package_reference/hf_api) (access huggingface data & models) \n- [Transformers](https://huggingface.co/docs/transformers/index)    (Access models from HuggingFace hub)\n- [Trl](https://huggingface.co/docs/trl/index)             (Transformer Reinforcement Learning. And fine-tuning.)\n- [Bitsandbytes](https://github.com/TimDettmers/bitsandbytes)    (makes models smaller, aka 'quantization')\n- [Sentencepiece](https://github.com/google/sentencepiece)       (Byte Pair Encoding scheme aka 'tokenization')\n- [OpenAI](https://openai.com)          (Create synthetic fine-tuning and reward model data)\n- [TVM](https://tvm.apache.org/)             (Tensor Virtual Machine, converts onnx model to efficient cross-platform use)\n- [Peft](https://huggingface.co/blog/peft)            (Parameter Efficient Fine Tuning, use low rank adaption (LoRa) to fine-tune)\n- [Onnx](https://onnx.ai/)            (Convert trained model to universal format)\n\n\n\n## Installation\n\nInstall all dependencies in one line using [pip](https://pip.pypa.io/en/stable/installation/)\n\n```bash\npip install numpy torch datasets huggingface_hub transformers trl bitsandbytes sentencepiece openai tvm peft onnx\n```\n\n## iOS QuickStart v2\n\n1. Clone this repository\n```bash\ngit clone https://github.com/llSourcell/Doctor-Dignity\n```\n2. Download the Weights\n```bash\nmkdir -p dist/prebuilt\ngit clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\ncd dist/prebuilt\ngit lfs install\nwget --no-check-certificate 'https://drive.google.com/file/d/1MLy8BDhuTTcXqagzLFMA07JDzqjQYUTB/view?pli=1'\ncd ../..\n```\n3. Build the Tensor Virtual Machine Runtime\n```bash\ngit submodule update --init --recursive\npip install apache-tvm\ncd ./ios\npip install --pre --force-reinstall mlc-ai-nightly mlc-chat-nightly -f https://mlc.ai/wheels \n./prepare_libs.sh\n```\n** Find the right version of MLC LLM for your system [here](https://mlc.ai/package/)\n4. Add Weights to Xcode\n```bash\ncd ./ios\nopen ./prepare_params.sh # make sure builtin_list only contains \"RedPajama-INCITE-Chat-3B-v1-q4f16_1\"\n./prepare_params.sh\n```\n5. Open Xcode Project and run! \n\n\n## DIY Training\n\nIn order to train the model, you can run the training.ipynb notebook locally or remotely via a cloud service like Google Colab Pro. The training process requires a GPU, and if you don't have one then the most accessible option i found was using Google Colab [Pro](https://colab.research.google.com/signup) which costs $10/month. The total training time for Doctor Dignity including supervised fine-tuning of the initial LLama model on custom medical data, as well as further improving it via Reinforcement Learning from Constitional AI Feedback took 24 hours on a paid instance of Google Colab. If you're interested in learning more about how this process works, details are in the training.ipynb notebook. \n\n#### Cloud Training\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/llSourcell/DoctorGPT/blob/main/llama2.ipynb)\nclick here: https://colab.research.google.com/github/llSourcell/Doctor-Dignity/blob/main/llama2.ipynb\n\n#### Local Training\n\n```bash\ngit clone https://github.com/llSourcell/Doctor-Dignity.git\njupyter training.ipynb\n```\nGet jupyter [here](https://jupyter.org/install)\n\n## Usage  https://huggingface.co/llSourcell/medllama2_7b\n\nThere are 2 huggingface repos, one which is quantized for mobile and one that is not.\n\n#### Old iOS app \n   \n- Step 1: [Download](https://github.com/mlc-ai/mlc-llm/tree/main/ios) the iOS Machine Learning Compilation Chat Repository\n- Step 2: Follow the [installation steps](https://mlc.ai/mlc-llm/docs/deploy/ios.html) \n- Step 3: Once the app is running on your iOS device or simulator, tap \"add model variant\"\n- Step 4: Enter the URL for the latest Doctor Dignity model to download it: [https://huggingface.co/llSourcell/doctorGPT_mini] (https://huggingface.co/llSourcell/doctorGPT_mini)\n- Step 5: Tap 'Add Model' and start chatting locally, inference runs on device. No internet connection needed!\n\n#### Android app (TODO)\n\n- Step 1: [Download](https://github.com/mlc-ai/mlc-llm/tree/main/android) the Android Machine Learning Compilation Chat Repository\n- Step 2: Follow the [installation steps]([https://mlc.ai/mlc-llm/docs/deploy/ios.html](https://mlc.ai/mlc-llm/docs/deploy/android.html)) \n- Step 3: Tap \"add model variant\"\n- Step 4: Enter the URL for the latest Doctor Dignity model to download it: [https://huggingface.co/llSourcell/doctorGPT_mini](https://huggingface.co/llSourcell/doctorGPT_mini)\n- Step 5: Tap 'Add Model' and start chatting locally! No internet needed. \n\n#### Web (TODO)\n\nAs an experiment in Online Learning using actual human feedback, i want to deploy the model as a Flask API with a React front-end. In this case, anyone can chat with the model at this URL. After each query, a human can rate the model's response. This rating is then used to further improve the model's performance through reinforcement learning. to run the app, download [flask](https://flask.palletsprojects.com/en/2.3.x/) and then you can run:\n\n```bash\nflask run\n```\n\nThen visit localhost:3000 to interact with it! You can also deploy to [vercel](https://vercel.com/templates/ai)\n\n## Credits\n\nMeta, MedAlpaca, Apache, MLC Chat & OctoML \n\n"
        },
        {
          "name": "android",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.py",
          "type": "blob",
          "size": 0.068359375,
          "content": "from mlc_llm.build import main\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "cpp",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "ios",
          "type": "tree",
          "content": null
        },
        {
          "name": "mlc_llm",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.8544921875,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n[tool.isort]\nprofile = \"black\"\n\n[tool.black]\nline-length = 100\ntarget-version = ['py310']\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.509765625,
          "content": "from distutils.core import setup\nfrom setuptools.dist import Distribution\nfrom setuptools import find_packages\nimport os\n\n# Note there is no need to setup when\n# running locally.\n\nCURRENT_DIR = os.path.dirname(__file__)\n\n\ndef git_describe_version(original_version):\n    \"\"\"Get git describe version.\"\"\"\n    ver_py = os.path.join(CURRENT_DIR, \"version.py\")\n    libver = {\"__file__\": ver_py}\n    exec(compile(open(ver_py, \"rb\").read(), ver_py, \"exec\"), libver, libver)\n    _, gd_version = libver[\"git_describe_version\"]()\n    if gd_version is not None and gd_version != original_version:\n        print(\"Use git describe based version %s\" % gd_version)\n    return gd_version\n\n\n__version__ = git_describe_version(None)\n\nsetup(\n    name=\"mlc_llm\",\n    version=__version__,\n    description=\"MLC LLM: Universal Compilation of Large Language Models\",\n    url=\"https://mlc.ai/mlc-llm/\",\n    author=\"MLC LLM Contributors\",\n    license=\"Apache 2.0\",\n    # See https://pypi.org/classifiers/\n    classifiers=[\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n    ],\n    keywords=\"machine learning\",\n    zip_safe=False,\n    packages=find_packages(),\n    package_dir={\"mlc_llm\": \"mlc_llm\"},\n    install_requires=[\"numpy\", \"torch\", \"transformers\", \"scipy\", \"timm\"],\n    entry_points={\"console_scripts\": [\"mlc_llm_build = mlc_llm.build:main\"]},\n    distclass=Distribution,\n)\n"
        },
        {
          "name": "site",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.py",
          "type": "blob",
          "size": 4.2685546875,
          "content": "# pylint: disable=missing-docstring\nimport argparse\nimport logging\nimport os\nimport subprocess\n\n# Modify the following value during release\n# ---------------------------------------------------\n# Current version:\n# We use the version of the incoming release for code\n# that is under development.\n#\n# It is also fallback version to be used when --git-describe\n# is not invoked, or when the repository does not present the\n# git tags in a format that this script can use.\n#\n# Two tag formats are supported:\n# - vMAJ.MIN.PATCH (e.g. v0.8.0) or\n# - vMAJ.MIN.devN (e.g. v0.8.dev0)\n\n# ---------------------------------------------------\n\n__version__ = \"0.1.dev0\"\nPROJ_ROOT = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))\n\n\ndef py_str(cstr):\n    return cstr.decode(\"utf-8\")\n\n\ndef git_describe_version():\n    \"\"\"Get PEP-440 compatible public and local version using git describe.\n\n    Returns\n    -------\n    pub_ver: str\n        Public version.\n\n    local_ver: str\n        Local version (with additional label appended to pub_ver).\n\n    Notes\n    -----\n    - We follow PEP 440's convention of public version\n      and local versions.\n    - Only tags conforming to vMAJOR.MINOR.REV (e.g. \"v0.7.0\")\n      are considered in order to generate the version string.\n      See the use of `--match` in the `git` command below.\n\n    Here are some examples:\n\n    - pub_ver = '0.7.0', local_ver = '0.7.0':\n      We are at the 0.7.0 release.\n    - pub_ver =  '0.8.dev94', local_ver = '0.8.dev94+g0d07a329e':\n      We are at the 0.8 development cycle.\n      The current source contains 94 additional commits\n      after the most recent tag(v0.7.0),\n      the git short hash tag of the current commit is 0d07a329e.\n    \"\"\"\n    cmd = [\n        \"git\",\n        \"describe\",\n        \"--tags\",\n        \"--match\",\n        \"v[0-9]*.[0-9]*.[0-9]*\",\n        \"--match\",\n        \"v[0-9]*.[0-9]*.dev[0-9]*\",\n    ]\n    with subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        cwd=PROJ_ROOT,\n    ) as proc:\n        (out, _) = proc.communicate()\n\n    if proc.returncode != 0:\n        msg = py_str(out)\n        logging.warning(\"git describe: %s\", msg)\n        return None, None\n    describe = py_str(out).strip()\n    arr_info = describe.split(\"-\")\n\n    # Remove the v prefix, mainly to be robust\n    # to the case where v is not presented as well.\n    if arr_info[0].startswith(\"v\"):\n        arr_info[0] = arr_info[0][1:]\n\n    # hit the exact tag\n    if len(arr_info) == 1:\n        return arr_info[0], arr_info[0]\n\n    if len(arr_info) != 3:\n        logging.warning(\"Invalid output from git describe %s\", describe)\n        return None, None\n\n    dev_pos = arr_info[0].find(\".dev\")\n\n    # Development versions:\n    # The code will reach this point in case it can't match a full release version, such as v0.7.0.\n    #\n    # 1. in case the last known label looks like vMAJ.MIN.devN e.g. v0.8.dev0, we use\n    # the current behavior of just using vMAJ.MIN.devNNNN+gGIT_REV\n    if dev_pos != -1:\n        dev_version = arr_info[0][: arr_info[0].find(\".dev\")]\n    # 2. in case the last known label looks like vMAJ.MIN.PATCH e.g. v0.8.0\n    # then we just carry on with a similar version to what git describe provides, which is\n    # vMAJ.MIN.PATCH.devNNNN+gGIT_REV\n    else:\n        dev_version = arr_info[0]\n\n    pub_ver = f\"{dev_version}.dev{arr_info[1]}\"\n    local_ver = f\"{pub_ver}+{arr_info[2]}\"\n    return pub_ver, local_ver\n\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n    parser = argparse.ArgumentParser(description=\"Detect and synchronize version.\")\n    parser.add_argument(\n        \"--print-version\",\n        action=\"store_true\",\n        help=\"Print version to the command line. No changes is applied to files.\",\n    )\n    parser.add_argument(\n        \"--git-describe\",\n        action=\"store_true\",\n        help=\"Use git describe to generate development version.\",\n    )\n    parser.add_argument(\"--dry-run\", action=\"store_true\")\n    pub_ver, local_ver = git_describe_version()\n    opt = parser.parse_args()\n    pub_ver, local_ver = None, None\n    if opt.git_describe:\n        pub_ver, local_ver = git_describe_version()\n    if pub_ver is None:\n        pub_ver = __version__\n    if local_ver is None:\n        local_ver = __version__\n    if opt.print_version:\n        print(local_ver)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}