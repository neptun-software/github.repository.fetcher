{
  "metadata": {
    "timestamp": 1736559968828,
    "page": 753,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "shibing624/MedicalGPT",
      "stars": 3489,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.763671875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n.idea/"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.3017578125,
          "content": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Xu\"\n  given-names: \"Ming\"\ntitle: \"MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline\"\nurl: \"https://github.com/shibing624/MedicalGPT\"\ndata-released: 2023-06-02\nversion: 0.0.4"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.4541015625,
          "content": "# Contributing\n\nWe are happy to accept your contributions to make this repo better and more awesome! To avoid unnecessary work on either\nside, please stick to the following process:\n\n1. Check if there is already an issue for your concern.\n2. If there is not, open a new one to start a discussion. We hate to close finished PRs!\n3. If we decide your concern needs code changes, we would be happy to accept a pull request. Please consider the\ncommit guidelines below."
        },
        {
          "name": "DISCLAIMER",
          "type": "blob",
          "size": 3.4443359375,
          "content": "The software project, data, and models provided by our GitHub project are provided \"as is,\" without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement.\n\nIn no event shall the project owners or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software project, data, or models, even if advised of the possibility of such damage.\n\nUsers of this software project, data, and models are solely responsible for any consequences of their use. The project owners and contributors shall not be held responsible for any subsequent or potential harm caused by the use of this software project, data, or models.\n\nBy using this software project, data, or models, users accept and agree to this disclaimer. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n\nIt is important to note that this software project, data, and models are still in the research phase and are provided for experimental purposes only. As such, the project owners and contributors do not guarantee the accuracy, completeness, or usefulness of the software project, data, or models.\n\nFurthermore, due to the experimental nature of this software project, data, and models, it is possible that they may contain or generate inappropriate responses, errors, or inconsistencies. Users should exercise caution when using this software project, data, or models, and should not rely solely on them for any critical or sensitive tasks.\n\nThe project owners and contributors shall not be held responsible for any damages, losses, or liabilities arising from the use of this software project, data, or models, including but not limited to, any inappropriate responses generated by the software project, data, or models.\n\nBy using this software project, data, or models, users acknowledge and accept the experimental nature of the software project, data, and models, and understand the potential risks and limitations associated with their use. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n\nThe software project, data, and models provided by our GitHub project are intended for research purposes only. They should not be used for any commercial, business, or legal purposes, and should not be relied upon as a substitute for professional advice or judgment.\n\nUsers of this software project, data, and models are strictly prohibited from using them for any commercial purposes, including but not limited to, selling, licensing, or distributing the software project, data, or models to third parties.\n\nThe project owners and contributors shall not be held responsible for any damages, losses, or liabilities arising from the use of this software project, data, or models for any commercial or business purposes.\n\nBy using this software project, data, or models, users agree to use them for research purposes only, and not for any commercial or business purposes. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 45.50390625,
          "content": "[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/MedicalGPT/blob/main/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/MedicalGPT/blob/main/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/MedicalGPT/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624)\n\n<div align=\"center\">\n  <a href=\"https://github.com/shibing624/MedicalGPT\">\n    <img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/logo.png\" height=\"100\" alt=\"Logo\">\n  </a>\n</div>\n\n-----------------\n\n# MedicalGPT: Training Medical GPT Model\n[![HF Models](https://img.shields.io/badge/Hugging%20Face-shibing624-green)](https://huggingface.co/shibing624)\n[![Github Stars](https://img.shields.io/github/stars/shibing624/MedicalGPT?color=yellow)](https://star-history.com/#shibing624/MedicalGPT&Timeline)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)\n[![Wechat Group](https://img.shields.io/badge/wechat-group-green.svg?logo=wechat)](#Contact)\n\n## ğŸ“– Introduction\n\n**MedicalGPT** training medical GPT model with ChatGPT training pipeline, implemantation of Pretraining,\nSupervised Finetuning, RLHF(Reward Modeling and Reinforcement Learning) and DPO(Direct Preference Optimization).\n\n**MedicalGPT** è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°äº†åŒ…æ‹¬å¢é‡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€RLHF(å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒ)å’ŒDPO(ç›´æ¥åå¥½ä¼˜åŒ–)ã€‚\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/dpo.jpg\" width=\"860\" />\n\n- RLHF training pipelineæ¥è‡ªAndrej Karpathyçš„æ¼”è®²PDF [State of GPT](https://karpathy.ai/stateofgpt.pdf)ï¼Œè§†é¢‘ [Video](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)\n- DPOæ–¹æ³•æ¥è‡ªè®ºæ–‡[Direct Preference Optimization:Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)\n- ORPOæ–¹æ³•æ¥è‡ªè®ºæ–‡[ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)\n## ğŸ”¥ News\n[2024/09/21] v2.3ç‰ˆæœ¬: æ”¯æŒäº† **[Qwen-2.5](https://qwenlm.github.io/zh/blog/qwen2.5/)** ç³»åˆ—æ¨¡å‹ï¼Œè¯¦è§[Release-v2.3](https://github.com/shibing624/MedicalGPT/releases/tag/2.3.0)\n\n[2024/08/02] v2.2ç‰ˆæœ¬ï¼šæ”¯æŒäº†è§’è‰²æ‰®æ¼”æ¨¡å‹è®­ç»ƒï¼Œæ–°å¢äº†åŒ»æ‚£å¯¹è¯SFTæ•°æ®ç”Ÿæˆè„šæœ¬[role_play_data](https://github.com/shibing624/MedicalGPT/blob/main/role_play_data/README.md)ï¼Œè¯¦è§[Release-v2.2](https://github.com/shibing624/MedicalGPT/releases/tag/2.2.0)\n\n[2024/06/11] v2.1ç‰ˆæœ¬ï¼šæ”¯æŒäº† **[Qwen-2](https://qwenlm.github.io/blog/qwen2/)** ç³»åˆ—æ¨¡å‹ï¼Œè¯¦è§[Release-v2.1](https://github.com/shibing624/MedicalGPT/releases/tag/2.1.0)\n\n[2024/04/24] v2.0ç‰ˆæœ¬ï¼šæ”¯æŒäº† **[Llama-3](https://huggingface.co/meta-llama)** ç³»åˆ—æ¨¡å‹ï¼Œè¯¦è§[Release-v2.0](https://github.com/shibing624/MedicalGPT/releases/tag/2.0.0)\n\n[2024/04/17] v1.9ç‰ˆæœ¬ï¼šæ”¯æŒäº† **[ORPO](https://arxiv.org/abs/2403.07691)**ï¼Œè¯¦ç»†ç”¨æ³•è¯·å‚ç…§ `run_orpo.sh`ã€‚è¯¦è§[Release-v1.9](https://github.com/shibing624/MedicalGPT/releases/tag/1.9.0)\n\n[2024/01/26] v1.8ç‰ˆæœ¬ï¼šæ”¯æŒå¾®è°ƒMixtralæ··åˆä¸“å®¶MoEæ¨¡å‹ **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)**ã€‚è¯¦è§[Release-v1.8](https://github.com/shibing624/MedicalGPT/releases/tag/1.8.0)\n\n[2024/01/14] v1.7ç‰ˆæœ¬ï¼šæ–°å¢æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çš„åŸºäºæ–‡ä»¶é—®ç­”[ChatPDF](https://github.com/shibing624/ChatPDF)åŠŸèƒ½ï¼Œä»£ç `chatpdf.py`ï¼Œå¯ä»¥åŸºäºå¾®è°ƒåçš„LLMç»“åˆçŸ¥è¯†åº“æ–‡ä»¶é—®ç­”æå‡è¡Œä¸šé—®ç­”å‡†ç¡®ç‡ã€‚è¯¦è§[Release-v1.7](https://github.com/shibing624/MedicalGPT/releases/tag/1.7.0)\n\n[2023/10/23] v1.6ç‰ˆæœ¬ï¼šæ–°å¢RoPEæ’å€¼æ¥æ‰©å±•GPTæ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼›é’ˆå¯¹LLaMAæ¨¡å‹æ”¯æŒäº†[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)å’Œ[LongLoRA](https://github.com/dvlab-research/LongLoRA) æå‡ºçš„ **$S^2$-Attn**ï¼›æ”¯æŒäº†[NEFTune](https://github.com/neelsjain/NEFTune)ç»™embeddingåŠ å™ªè®­ç»ƒæ–¹æ³•ã€‚è¯¦è§[Release-v1.6](https://github.com/shibing624/MedicalGPT/releases/tag/1.6.0)\n\n[2023/08/28] v1.5ç‰ˆæœ¬: æ–°å¢[DPO(ç›´æ¥åå¥½ä¼˜åŒ–)](https://arxiv.org/pdf/2305.18290.pdf)æ–¹æ³•ï¼ŒDPOé€šè¿‡ç›´æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹æ¥å®ç°å¯¹å…¶è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶ï¼Œå¯ä»¥æœ‰æ•ˆå­¦ä¹ åˆ°äººç±»åå¥½ã€‚è¯¦è§[Release-v1.5](https://github.com/shibing624/MedicalGPT/releases/tag/1.5.0)\n\n[2023/08/08] v1.4ç‰ˆæœ¬: å‘å¸ƒåŸºäºShareGPT4æ•°æ®é›†å¾®è°ƒçš„ä¸­è‹±æ–‡Vicuna-13Bæ¨¡å‹[shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat)ï¼Œå’Œå¯¹åº”çš„LoRAæ¨¡å‹[shibing624/vicuna-baichuan-13b-chat-lora](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat-lora)ï¼Œè¯¦è§[Release-v1.4](https://github.com/shibing624/MedicalGPT/releases/tag/1.4.0)\n\n[2023/08/02] v1.3ç‰ˆæœ¬: æ–°å¢LLaMA, LLaMA2, Bloom, ChatGLM, ChatGLM2, Baichuanæ¨¡å‹çš„å¤šè½®å¯¹è¯å¾®è°ƒè®­ç»ƒï¼›æ–°å¢é¢†åŸŸè¯è¡¨æ‰©å……åŠŸèƒ½ï¼›æ–°å¢ä¸­æ–‡é¢„è®­ç»ƒæ•°æ®é›†å’Œä¸­æ–‡ShareGPTå¾®è°ƒè®­ç»ƒé›†ï¼Œè¯¦è§[Release-v1.3](https://github.com/shibing624/MedicalGPT/releases/tag/1.3.0)\n\n[2023/07/13] v1.1ç‰ˆæœ¬: å‘å¸ƒä¸­æ–‡åŒ»ç–—LLaMA-13Bæ¨¡å‹[shibing624/ziya-llama-13b-medical-merged](https://huggingface.co/shibing624/ziya-llama-13b-medical-merged)ï¼ŒåŸºäºZiya-LLaMA-13B-v1æ¨¡å‹ï¼ŒSFTå¾®è°ƒäº†ä¸€ç‰ˆåŒ»ç–—æ¨¡å‹ï¼ŒåŒ»ç–—é—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„å®Œæ•´æ¨¡å‹æƒé‡ï¼Œè¯¦è§[Release-v1.1](https://github.com/shibing624/MedicalGPT/releases/tag/1.1)\n\n[2023/06/15] v1.0ç‰ˆæœ¬: å‘å¸ƒä¸­æ–‡åŒ»ç–—LoRAæ¨¡å‹[shibing624/ziya-llama-13b-medical-lora](https://huggingface.co/shibing624/ziya-llama-13b-medical-lora)ï¼ŒåŸºäºZiya-LLaMA-13B-v1æ¨¡å‹ï¼ŒSFTå¾®è°ƒäº†ä¸€ç‰ˆåŒ»ç–—æ¨¡å‹ï¼ŒåŒ»ç–—é—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡ï¼Œè¯¦è§[Release-v1.0](https://github.com/shibing624/MedicalGPT/releases/tag/1.0.0)\n\n[2023/06/05] v0.2ç‰ˆæœ¬: ä»¥åŒ»ç–—ä¸ºä¾‹ï¼Œè®­ç»ƒé¢†åŸŸå¤§æ¨¡å‹ï¼Œå®ç°äº†å››é˜¶æ®µè®­ç»ƒï¼šåŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚è¯¦è§[Release-v0.2](https://github.com/shibing624/MedicalGPT/releases/tag/0.2.0)\n\n\n## ğŸ˜Š Features\n\n\nåŸºäºChatGPT Training Pipelineï¼Œæœ¬é¡¹ç›®å®ç°äº†é¢†åŸŸæ¨¡å‹--åŒ»ç–—è¡Œä¸šè¯­è¨€å¤§æ¨¡å‹çš„è®­ç»ƒï¼š\n\n\n- ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æ¡£æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒGPTæ¨¡å‹ï¼Œä»¥é€‚åº”é¢†åŸŸæ•°æ®åˆ†å¸ƒï¼ˆå¯é€‰ï¼‰\n- ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾ï¼Œå¹¶æ³¨å…¥é¢†åŸŸçŸ¥è¯†\n- ç¬¬ä¸‰é˜¶æ®µ\n  - RLHF(Reinforcement Learning from Human Feedback)åŸºäºäººç±»åé¦ˆå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œåˆ†ä¸ºä¸¤æ­¥ï¼š\n    - RM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å»ºæ¨¡äººç±»åå¥½ï¼Œä¸»è¦æ˜¯\"HHH\"åŸåˆ™ï¼Œå…·ä½“æ˜¯\"helpful, honest, harmless\"\n    - RL(Reinforcement Learning)å¼ºåŒ–å­¦ä¹ ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬\n  - [DPO(Direct Preference Optimization)](https://arxiv.org/pdf/2305.18290.pdf)ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ï¼ŒDPOé€šè¿‡ç›´æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹æ¥å®ç°å¯¹å…¶è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶ï¼Œè€Œæ— éœ€ä½¿ç”¨å¤æ‚çš„å¼ºåŒ–å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥æœ‰æ•ˆå­¦ä¹ åˆ°äººç±»åå¥½ï¼ŒDPOç›¸è¾ƒäºRLHFæ›´å®¹æ˜“å®ç°ä¸”æ˜“äºè®­ç»ƒï¼Œæ•ˆæœæ›´å¥½\n  - [ORPO](https://arxiv.org/abs/2403.07691)æ¯”å€¼æ¯”åå¥½ä¼˜åŒ–ï¼Œä¸éœ€è¦å‚è€ƒæ¨¡å‹ï¼ˆref_modelï¼‰çš„ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡ORPOï¼ŒLLMå¯ä»¥åŒæ—¶å­¦ä¹ SFTå’Œå¯¹é½ï¼Œå°†ä¸¤ä¸ªè¿‡ç¨‹æ•´åˆä¸ºå•ä¸€æ­¥éª¤ï¼Œç¼“è§£æ¨¡å‹ç¾éš¾æ€§é—å¿˜é—®é¢˜\n\n\n### Release Models\n\n\n| Model                                                                                                             | Base Model                                                                              | Introduction                                                                                                                                                                 |\n|:------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [shibing624/ziya-llama-13b-medical-lora](https://huggingface.co/shibing624/ziya-llama-13b-medical-lora)           | [IDEA-CCNL/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1)       | åœ¨240ä¸‡æ¡ä¸­è‹±æ–‡åŒ»ç–—æ•°æ®é›†[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)ä¸ŠSFTå¾®è°ƒäº†ä¸€ç‰ˆZiya-LLaMA-13Bæ¨¡å‹ï¼ŒåŒ»ç–—é—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡(å•è½®å¯¹è¯)                                 |\n| [shibing624/ziya-llama-13b-medical-merged](https://huggingface.co/shibing624/ziya-llama-13b-medical-merged)       | [IDEA-CCNL/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1)       | åœ¨240ä¸‡æ¡ä¸­è‹±æ–‡åŒ»ç–—æ•°æ®é›†[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)ä¸ŠSFTå¾®è°ƒäº†ä¸€ç‰ˆZiya-LLaMA-13Bæ¨¡å‹ï¼ŒåŒ»ç–—é—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„å®Œæ•´æ¨¡å‹æƒé‡(å•è½®å¯¹è¯)                                 |\n| [shibing624/vicuna-baichuan-13b-chat-lora](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat-lora)       | [baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) | åœ¨10ä¸‡æ¡å¤šè¯­è¨€ShareGPT GPT4å¤šè½®å¯¹è¯æ•°æ®é›†[shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4) å’Œ åŒ»ç–—æ•°æ®é›†[shibing624/medical](https://huggingface.co/datasets/shibing624/medical) ä¸ŠSFTå¾®è°ƒäº†ä¸€ç‰ˆbaichuan-13b-chatå¤šè½®é—®ç­”æ¨¡å‹ï¼Œæ—¥å¸¸é—®ç­”å’ŒåŒ»ç–—é—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡ |\n| [shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat)                 | [baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) | åœ¨10ä¸‡æ¡å¤šè¯­è¨€ShareGPT GPT4å¤šè½®å¯¹è¯æ•°æ®é›†[shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4) å’Œ åŒ»ç–—æ•°æ®é›†[shibing624/medical](https://huggingface.co/datasets/shibing624/medical) ä¸ŠSFTå¾®è°ƒäº†ä¸€ç‰ˆbaichuan-13b-chatå¤šè½®é—®ç­”æ¨¡å‹ï¼Œæ—¥å¸¸é—®ç­”å’ŒåŒ»ç–—é—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„å®Œæ•´æ¨¡å‹æƒé‡ |\n| [shibing624/llama-3-8b-instruct-262k-chinese](https://huggingface.co/shibing624/llama-3-8b-instruct-262k-chinese) | [Llama-3-8B-Instruct-262k](https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k)  | åœ¨2ä¸‡æ¡ä¸­è‹±æ–‡åå¥½æ•°æ®é›†[shibing624/DPO-En-Zh-20k-Preference](https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference)ä¸Šä½¿ç”¨ORPOæ–¹æ³•å¾®è°ƒå¾—åˆ°çš„è¶…é•¿æ–‡æœ¬å¤šè½®å¯¹è¯æ¨¡å‹ï¼Œé€‚ç”¨äºRAGã€å¤šè½®å¯¹è¯                   |\n\næ¼”ç¤º[shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat)æ¨¡å‹æ•ˆæœï¼š\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/demo-screen.gif\" width=\"860\" />\nå…·ä½“caseè§[Inference Examples](#inference-examples)\n\n## â–¶ï¸ Demo\n\n\næˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€æ´çš„åŸºäºgradioçš„äº¤äº’å¼webç•Œé¢ï¼Œå¯åŠ¨æœåŠ¡åï¼Œå¯é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼Œè¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹ä¼šè¿”å›ç­”æ¡ˆã€‚\n\nå¯åŠ¨æœåŠ¡ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š\n```shell\nCUDA_VISIBLE_DEVICES=0 python gradio_demo.py --model_type base_model_type --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir\n```\n\nå‚æ•°è¯´æ˜ï¼š\n\n- `--model_type {base_model_type}`ï¼šé¢„è®­ç»ƒæ¨¡å‹ç±»å‹ï¼Œå¦‚llamaã€bloomã€chatglmç­‰\n- `--base_model {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°\n- `--lora_model {lora_model}`ï¼šLoRAæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚è‹¥loraæƒé‡å·²ç»åˆå¹¶åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åˆ é™¤--lora_modelå‚æ•°\n- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ\n- `--template_name`ï¼šæ¨¡æ¿åç§°ï¼Œå¦‚`vicuna`ã€`alpaca`ç­‰ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼æ˜¯vicuna\n- `--only_cpu`: ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†\n- `--resize_emb`ï¼šæ˜¯å¦è°ƒæ•´embeddingå¤§å°ï¼Œè‹¥ä¸è°ƒæ•´ï¼Œåˆ™ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„embeddingå¤§å°ï¼Œé»˜è®¤ä¸è°ƒæ•´\n\n\n## ğŸ’¾ Install\n#### Updating the requirements\n`requirements.txt`ä¼šä¸æ—¶æ›´æ–°ä»¥é€‚é…æœ€æ–°åŠŸèƒ½ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ›´æ–°ä¾èµ–:\n\n```markdown\ngit clone https://github.com/shibing624/MedicalGPT\ncd MedicalGPT\npip install -r requirements.txt --upgrade\n```\n\n#### Hardware Requirement (æ˜¾å­˜/VRAM)\n\n\n\\* *ä¼°ç®—å€¼*\n\n| è®­ç»ƒæ–¹æ³•  | ç²¾åº¦          |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |\n|-------|-------------| ----- | ----- | ----- | ------ | ------ | ----- | ------ |\n| å…¨å‚æ•°   | AMP(è‡ªåŠ¨æ··åˆç²¾åº¦) | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |\n| å…¨å‚æ•°   | 16          |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |\n| LoRA  | 16          |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |\n| QLoRA | 8           |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |\n| QLoRA | 4           |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |\n| QLoRA | 2           |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB |\n\n## ğŸš€ Training Pipeline\n\nTraining Stage:\n\n| Stage                          | Introduction | Python script                                                                                           | Shell script                                                                  |\n|:-------------------------------|:-------------|:--------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------|\n| Continue Pretraining           | å¢é‡é¢„è®­ç»ƒ        | [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py)                     | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)     |\n| Supervised Fine-tuning         | æœ‰ç›‘ç£å¾®è°ƒ        | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)   |\n| Direct Preference Optimization | ç›´æ¥åå¥½ä¼˜åŒ–       | [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py)                   | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)   |\n| Reward Modeling                | å¥–åŠ±æ¨¡å‹å»ºæ¨¡       | [reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/reward_modeling.py)             | [run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rm.sh)     |\n| Reinforcement Learning         | å¼ºåŒ–å­¦ä¹          | [ppo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/ppo_training.py)                   | [run_ppo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_ppo.sh)   |\n| ORPO                           | æ¦‚ç‡åå¥½ä¼˜åŒ–       | [orpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/orpo_training.py)                  | [run_orpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_orpo.sh) |\n\n- æä¾›å®Œæ•´PT+SFT+DPOå…¨é˜¶æ®µä¸²èµ·æ¥è®­ç»ƒçš„pipelineï¼š[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb) ï¼Œå…¶å¯¹åº”çš„colabï¼š [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)ï¼Œè¿è¡Œå®Œå¤§æ¦‚éœ€è¦15åˆ†é’Ÿ\n- æä¾›å®Œæ•´PT+SFT+RLHFå…¨é˜¶æ®µä¸²èµ·æ¥è®­ç»ƒçš„pipelineï¼š[run_training_ppo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_ppo_pipeline.ipynb) ï¼Œå…¶å¯¹åº”çš„colabï¼š [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_ppo_pipeline.ipynb) ï¼Œè¿è¡Œå®Œå¤§æ¦‚éœ€è¦20åˆ†é’Ÿ\n- æä¾›åŸºäºçŸ¥è¯†åº“æ–‡ä»¶çš„LLMé—®ç­”åŠŸèƒ½ï¼ˆRAGï¼‰ï¼š[chatpdf.py](https://github.com/shibing624/MedicalGPT/blob/main/chatpdf.py)\n- [è®­ç»ƒå‚æ•°è¯´æ˜](https://github.com/shibing624/MedicalGPT/blob/main/docs/training_params.md) | [è®­ç»ƒå‚æ•°è¯´æ˜wiki](https://github.com/shibing624/MedicalGPT/wiki/%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E)\n- [æ•°æ®é›†](https://github.com/shibing624/MedicalGPT/blob/main/docs/datasets.md) | [æ•°æ®é›†wiki](https://github.com/shibing624/MedicalGPT/wiki/%E6%95%B0%E6%8D%AE%E9%9B%86)\n- [æ‰©å……è¯è¡¨](https://github.com/shibing624/MedicalGPT/blob/main/docs/extend_vocab.md) | [æ‰©å……è¯è¡¨wiki](https://github.com/shibing624/MedicalGPT/wiki/%E6%89%A9%E5%85%85%E4%B8%AD%E6%96%87%E8%AF%8D%E8%A1%A8)\n- [FAQ](https://github.com/shibing624/MedicalGPT/blob/main/docs/FAQ.md) | [FAQ_wiki](https://github.com/shibing624/MedicalGPT/wiki/FAQ)\n\n#### Supported Models\n\n| Model Name                                                           | Model Size                    | Target Modules  | Template  |\n|----------------------------------------------------------------------|-------------------------------|-----------------|-----------|\n| [Baichuan](https://github.com/baichuan-inc/baichuan-13B)             | 7B/13B                        | W_pack          | baichuan  |\n| [Baichuan2](https://github.com/baichuan-inc/Baichuan2)               | 7B/13B                        | W_pack          | baichuan2 |\n| [BLOOMZ](https://huggingface.co/bigscience/bloomz)                   | 560M/1.1B/1.7B/3B/7.1B/176B   | query_key_value | vicuna    |\n| [ChatGLM](https://github.com/THUDM/ChatGLM-6B)                       | 6B                            | query_key_value | chatglm   |\n| [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)                     | 6B                            | query_key_value | chatglm2  |\n| [ChatGLM3](https://github.com/THUDM/ChatGLM3)                        | 6B                            | query_key_value | chatglm3  |\n| [Cohere](https://huggingface.co/CohereForAI/c4ai-command-r-plus)     | 104B                          | q_proj,v_proj   | cohere    |\n| [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM)              | 7B/16B/67B                    | q_proj,v_proj   | deepseek  |\n| [InternLM2](https://github.com/InternLM/InternLM)                    | 7B/20B                        | wqkv            | intern2   |\n| [LLaMA](https://github.com/facebookresearch/llama)                   | 7B/13B/33B/65B                | q_proj,v_proj   | alpaca    |\n| [LLaMA2](https://huggingface.co/meta-llama)                          | 7B/13B/70B                    | q_proj,v_proj   | llama2    |\n| [LLaMA3](https://huggingface.co/meta-llama)                          | 8B/70B                        | q_proj,v_proj   | llama3    |\n| [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) | 7B/8x7B                       | q_proj,v_proj   | mistral   |\n| [Orion](https://github.com/OrionStarAI/Orion)                        | 14B                           | q_proj,v_proj   | orion     |\n| [Qwen](https://github.com/QwenLM/Qwen)                               | 1.8B/7B/14B/72B               | c_attn          | qwen      |\n| [Qwen1.5](https://huggingface.co/Qwen/Qwen1.5-72B)                   | 0.5B/1.8B/4B/14B/32B/72B/110B | q_proj,v_proj   | qwen      |\n| [Qwen2](https://github.com/QwenLM/Qwen2)                             | 0.5B/1.5B/7B/72B              | q_proj,v_proj   | qwen      |\n| [XVERSE](https://github.com/xverse-ai/XVERSE-13B)                    | 13B                           | query_key_value | xverse    |\n| [Yi](https://github.com/01-ai/Yi)                                    | 6B/34B                        | q_proj,v_proj   | yi        |\n\n\n\n\n## ğŸ’» Inference\nè®­ç»ƒå®Œæˆåï¼Œç°åœ¨æˆ‘ä»¬åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ŒéªŒè¯æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ•ˆæœã€‚\n\n```shell\nCUDA_VISIBLE_DEVICES=0 python inference.py \\\n    --model_type base_model_type \\\n    --base_model path_to_model_hf_dir \\\n    --tokenizer_path path_to_model_hf_dir \\\n    --lora_model path_to_lora \\\n    --interactive\n```\n\nå‚æ•°è¯´æ˜ï¼š\n\n- `--model_type {base_model_type}`ï¼šé¢„è®­ç»ƒæ¨¡å‹ç±»å‹ï¼Œå¦‚llamaã€bloomã€chatglmç­‰\n- `--base_model {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•\n- `--tokenizer_path {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•\n- `--lora_model {lora_model}`ï¼šLoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚å¦‚æœå·²ç»åˆå¹¶äº†LoRAæƒé‡åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™å¯ä»¥ä¸æä¾›æ­¤å‚æ•°\n- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ\n- `--template_name`ï¼šæ¨¡æ¿åç§°ï¼Œå¦‚`vicuna`ã€`alpaca`ç­‰ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼æ˜¯vicuna\n- `--interactive`ï¼šä»¥äº¤äº’æ–¹å¼å¯åŠ¨å¤šè½®é—®ç­”ï¼Œä½¿ç”¨æµå¼æ¨ç†\n- `--data_file {file_name}`ï¼šéäº¤äº’æ–¹å¼å¯åŠ¨ä¸‹ï¼Œè¯»å–file_nameä¸­çš„çš„å†…å®¹è¿›è¡Œbatché¢„æµ‹\n- `--output_file {file_name}`ï¼šéäº¤äº’å¼æ–¹å¼ä¸‹ï¼Œå°†é¢„æµ‹çš„ç»“æœä»¥jsonlæ ¼å¼å†™å…¥file_name\n- `--resize_emb`ï¼šæ˜¯å¦è°ƒæ•´embeddingå¤§å°ï¼Œè‹¥ä¸è°ƒæ•´ï¼Œåˆ™ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„embeddingå¤§å°ï¼Œé»˜è®¤ä¸è°ƒæ•´\n- `--only_cpu`ï¼šä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†\n- `--gpus {gpu_ids}`ï¼šæŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2\n\n#### å¤šå¡æ¨ç†\nå¤šå¡æ•°æ®å¹¶è¡Œï¼Œbatchæ¨ç†\n```shell\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 inference_multigpu_demo.py --model_type baichuan --base_model shibing624/vicuna-baichuan-13b-chat\n```\n#### Vllmå¤šå¡éƒ¨ç½²\n```shell\nbash run_Multi-GPUs-deployment.sh\n```\n\n\n#### Inference Examples\n[shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat) inference examples:\n\n<details><summary>Model output case</summary>\n\n| Human                                                                              | Assitant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| å°å­©å‘çƒ§æ€ä¹ˆåŠ                                                                            | å‘çƒ§æ˜¯èº«ä½“å¯¹æ„ŸæŸ“æˆ–å…¶ä»–ç–¾ç—…çš„è‡ªç„¶ååº”ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ç¼“è§£å‘çƒ§ç—‡çŠ¶çš„å»ºè®®ï¼š<br/>1. ä¿æŒèº«ä½“æ°´åˆ†å……è¶³ï¼šå‘çƒ§æ—¶ï¼Œèº«ä½“ä¼šå¤±å»æ°´åˆ†ï¼Œå› æ­¤éœ€è¦ä¿æŒæ°´åˆ†å……è¶³ã€‚<br/>2. ä½¿ç”¨å†°è¢‹ï¼šå°†å†°è¢‹æ”¾åœ¨é¢å¤´æˆ–é¢ˆéƒ¨å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚<br/>3. ä½¿ç”¨é€€çƒ§è¯ï¼šé€€çƒ§è¯å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚<br/>4. ä¿æŒèº«ä½“æ¸©åº¦ï¼šä¿æŒèº«ä½“æ¸©åº¦å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚<br/>5. ä½¿ç”¨å†·æ°´æˆ–æ¸©æ°´æ“¦æ‹­èº«ä½“ï¼šä½¿ç”¨å†·æ°´æˆ–æ¸©æ°´æ“¦æ‹­èº«ä½“å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚<br/>6. ä½¿ç”¨é€€çƒ§è¯ï¼šé€€çƒ§è¯å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚<br/>7. ä¿æŒèº«ä½“æ¸©åº¦ï¼šä¿æŒèº«ä½“æ¸©åº¦å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚<br/>8. ä½¿ç”¨å†·æ°´æˆ–æ¸©æ°´æ“¦æ‹­èº«ä½“ï¼šä½¿ç”¨å†·æ°´æˆ–æ¸©æ°´æ“¦æ‹­èº«ä½“å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚<br/>9. ä½¿ç”¨é€€çƒ§è¯ï¼šé€€çƒ§è¯å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚<br/>10. ä¿æŒèº«ä½“æ¸©åº¦ï¼šä¿æŒèº«ä½“æ¸©åº¦å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ã€‚                                                                                                                                                                                                                                                         |\n| ä¸€å²å®å®å‘çƒ§èƒ½åƒå•¥è¯ï¼Ÿ                                                                        | 1å²å®å®å‘çƒ§æ—¶ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ç§è¯ç‰©ï¼š<br/>1. é€€çƒ§è¯ï¼šå¦‚å¸ƒæ´›èŠ¬(Ibuprofen)æˆ–å¯¹ä¹™é…°æ°¨åŸºé…š(Paracetamol)ã€‚è¿™äº›è¯ç‰©å¯ä»¥å¸®åŠ©é™ä½ä½“æ¸©ï¼Œç¼“è§£å‘çƒ§ç—‡çŠ¶ã€‚<br/>2. æŠ—ç”Ÿç´ ï¼šå¦‚æœå‘çƒ§æ˜¯ç”±ç»†èŒæ„ŸæŸ“å¼•èµ·çš„ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨æŠ—ç”Ÿç´ ã€‚ä½†è¯·åŠ¡å¿…åœ¨åŒ»ç”Ÿçš„å»ºè®®ä¸‹ä½¿ç”¨ã€‚<br/>3. æŠ—ç—…æ¯’è¯ç‰©ï¼šå¦‚æœå‘çƒ§æ˜¯ç”±ç—…æ¯’æ„ŸæŸ“å¼•èµ·çš„ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨æŠ—ç—…æ¯’è¯ç‰©ã€‚ä½†è¯·åŠ¡å¿…åœ¨åŒ»ç”Ÿçš„å»ºè®®ä¸‹ä½¿ç”¨ã€‚<br/>4. è§£çƒ­é•‡ç—›è¯ï¼šå¦‚é˜¿å¸åŒ¹æ—(Aspirin)æˆ–å¯¹ä¹™é…°æ°¨åŸºé…š(Paracetamol)ã€‚è¿™äº›è¯ç‰©å¯ä»¥å¸®åŠ©ç¼“è§£å‘çƒ§ç—‡çŠ¶ã€‚<br/>5. æŠ—ç»„èƒºè¯ï¼šå¦‚è‹¯æµ·æ‹‰æ˜(Diphenhydramine)æˆ–éç´¢éé‚£å®š(Naproxen)ã€‚è¿™äº›è¯ç‰©å¯ä»¥å¸®åŠ©ç¼“è§£å‘çƒ§ç—‡çŠ¶ã€‚<br/>è¯·åŠ¡å¿…åœ¨åŒ»ç”Ÿçš„å»ºè®®ä¸‹ä½¿ç”¨è¿™äº›è¯ç‰©ï¼Œå¹¶éµå¾ªè¯ç‰©è¯´æ˜ä¹¦ä¸Šçš„ç”¨é‡å’Œç”¨æ³•ã€‚åŒæ—¶ï¼Œä¿æŒå®å®å……è¶³çš„æ°´åˆ†æ‘„å…¥ï¼Œå¹¶ç¡®ä¿å®å®æœ‰è¶³å¤Ÿçš„ä¼‘æ¯ã€‚                                                                                                                                                                                                                             |\n| è‚›é—¨ç—…å˜å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…çš„ç—‡çŠ¶                                                                     | è‚›é—¨ç—…å˜å¯èƒ½ç”±å¤šç§åŸå› å¼•èµ·ï¼ŒåŒ…æ‹¬æ„ŸæŸ“ã€ç‚ç—‡ã€è‚¿ç˜¤æˆ–å…¶ä»–ç–¾ç—…ã€‚ä¸€äº›å¸¸è§çš„è‚›é—¨ç—…å˜åŒ…æ‹¬ï¼š<br/>1. è‚›è£‚ï¼šè¿™æ˜¯ä¸€ç§è‚›é—¨çš®è‚¤æ’•è£‚ï¼Œé€šå¸¸ç”±ä¾¿ç§˜æˆ–å…¶ä»–å‹åŠ›å› ç´ å¼•èµ·ã€‚<br/>2. è‚›ç˜˜ï¼šè¿™æ˜¯ä¸€ç§è‚›é—¨æ„ŸæŸ“ï¼Œå¯¼è‡´è‚›é—¨çš®è‚¤å’Œç›´è‚ ä¹‹é—´å½¢æˆç˜˜ç®¡ã€‚<br/>3. è‚›é—¨ç–£ï¼šè¿™æ˜¯ä¸€ç§è‚›é—¨çš®è‚¤ä¸Šå½¢æˆçš„ç–£çŠ¶ç—…å˜ï¼Œé€šå¸¸ç”±äººç±»ä¹³å¤´çŠ¶ç˜¤ç—…æ¯’(HPV)å¼•èµ·ã€‚<br/>4. è‚›é—¨è„“è‚¿ï¼šè¿™æ˜¯ä¸€ç§è‚›é—¨æ„ŸæŸ“ï¼Œå¯¼è‡´è‚›é—¨çš®è‚¤ä¸‹å½¢æˆè„“è‚¿ã€‚<br/>5. è‚›é—¨è‚¿ç˜¤ï¼šè¿™æ˜¯ä¸€ç§è‚›é—¨çš®è‚¤æˆ–ç›´è‚ å†…å½¢æˆçš„è‚¿ç˜¤ã€‚<br/>6. è‚›é—¨æºƒç–¡ï¼šè¿™æ˜¯ä¸€ç§è‚›é—¨çš®è‚¤ä¸Šå½¢æˆçš„æºƒç–¡ï¼Œé€šå¸¸ç”±æ„ŸæŸ“æˆ–ç‚ç—‡å¼•èµ·ã€‚<br/>7. è‚›é—¨ç˜™ç—’ï¼šè¿™æ˜¯ä¸€ç§è‚›é—¨çš®è‚¤ç˜™ç—’çš„ç—‡çŠ¶ï¼Œå¯èƒ½ç”±æ„ŸæŸ“ã€ç‚ç—‡æˆ–å…¶ä»–ç–¾ç—…å¼•èµ·ã€‚<br/>å¦‚æœæ‚¨æœ‰è‚›é—¨ç—…å˜çš„ç—‡çŠ¶ï¼Œè¯·å’¨è¯¢åŒ»ç–—ä¿å¥ä¸“ä¸šäººå‘˜ä»¥è·å¾—é€‚å½“çš„è¯Šæ–­å’Œæ²»ç–—ã€‚                                                                                                                                                                                                                                                                  |\n| åŒ»ç”Ÿï¼Œæ‚¨å¥½ç™½å¸¦å¢å¤šæ³›é»„æ€ä¹ˆå›äº‹ï¼Ÿ                                                                   | æ‚¨å¥½ï¼Œç™½å¸¦å¢å¤šå’Œå‘é»„å¯èƒ½æ˜¯ç”±å¤šç§åŸå› å¼•èµ·çš„ï¼ŒåŒ…æ‹¬æ„ŸæŸ“ã€è·å°”è’™å˜åŒ–ã€é˜´é“å¹²ç‡¥æˆ–å…¶ä»–å¦‡ç§‘é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„åŸå› ï¼š<br/>1. æ„ŸæŸ“ï¼šç™½å¸¦å¢å¤šå’Œå‘é»„å¯èƒ½æ˜¯ç”±é˜´é“æˆ–å°¿é“æ„ŸæŸ“å¼•èµ·çš„ã€‚æ„ŸæŸ“å¯èƒ½ç”±ç»†èŒã€ç—…æ¯’æˆ–çœŸèŒå¼•èµ·ã€‚<br/>2. è·å°”è’™å˜åŒ–ï¼šè·å°”è’™å˜åŒ–ï¼Œå¦‚æœˆç»å‘¨æœŸæˆ–æ›´å¹´æœŸï¼Œå¯èƒ½ä¼šå¯¼è‡´ç™½å¸¦å¢å¤šå’Œå‘é»„ã€‚<br/>3. é˜´é“å¹²ç‡¥ï¼šé˜´é“å¹²ç‡¥å¯èƒ½ä¼šå¯¼è‡´ç™½å¸¦å¢å¤šå’Œå‘é»„ã€‚<br/>4. å…¶ä»–å¦‡ç§‘é—®é¢˜ï¼šå…¶ä»–å¦‡ç§‘é—®é¢˜ï¼Œå¦‚å­å®«é¢ˆç‚ã€å­å®«å†…è†œç‚æˆ–å­å®«å†…è†œå¼‚ä½ç—‡ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´ç™½å¸¦å¢å¤šå’Œå‘é»„ã€‚<br/>å¦‚æœæ‚¨æ‹…å¿ƒè‡ªå·±çš„ç™½å¸¦ï¼Œå»ºè®®æ‚¨å’¨è¯¢å¦‡ç§‘åŒ»ç”Ÿè¿›è¡Œæ£€æŸ¥å’Œè¯Šæ–­ã€‚åŒ»ç”Ÿå¯ä»¥æ ¹æ®æ‚¨çš„ç—‡çŠ¶å’Œæ£€æŸ¥ç»“æœç¡®å®šåŸå› å¹¶æä¾›é€‚å½“çš„æ²»ç–—ã€‚                                                                                                                                                                                                                                                                                                                          |\n| è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—                                                                    | è¯æµåå‡ºç°å·§å…‹åŠ›ç™½å¸¦å¹¶ä¸ä¸€å®šæ„å‘³ç€æ²¡æœ‰æµå¹²å‡€ã€‚å·§å…‹åŠ›ç™½å¸¦æ˜¯ç”±äºå­å®«å†…è†œè„±è½å’Œå‡ºè¡€å¼•èµ·çš„ï¼Œé€šå¸¸åœ¨è¯æµåçš„ä¸€æ®µæ—¶é—´å†…ä¼šå‡ºç°ã€‚<br/><br/>è¯æµåå‡ºç°å·§å…‹åŠ›ç™½å¸¦çš„åŸå› å¯èƒ½åŒ…æ‹¬ï¼š<br/><br/>1. å­å®«å†…è†œè„±è½ï¼šè¯æµåï¼Œå­å®«å†…è†œä¼šè„±è½ï¼Œå¯¼è‡´å‡ºè¡€ã€‚è¿™ç§å‡ºè¡€å¯èƒ½è¡¨ç°ä¸ºå·§å…‹åŠ›ç™½å¸¦ã€‚<br/>2. æ„ŸæŸ“ï¼šè¯æµåï¼Œå­å®«å†…è†œå’Œå®«é¢ˆå£å¯èƒ½å­˜åœ¨åˆ›ä¼¤ï¼Œå®¹æ˜“å—åˆ°ç»†èŒæ„ŸæŸ“ã€‚æ„ŸæŸ“å¯èƒ½å¯¼è‡´å·§å…‹åŠ›ç™½å¸¦ã€‚<br/>3. æ¿€ç´ æ°´å¹³å˜åŒ–ï¼šè¯æµåï¼Œæ¿€ç´ æ°´å¹³å¯èƒ½å‘ç”Ÿå˜åŒ–ï¼Œå¯¼è‡´å·§å…‹åŠ›ç™½å¸¦ã€‚<br/><br/>å¦‚æœå·§å…‹åŠ›ç™½å¸¦æŒç»­æ—¶é—´è¾ƒé•¿ï¼Œæˆ–è€…ä¼´æœ‰å…¶ä»–ç—‡çŠ¶ï¼Œå¦‚è…¹ç—›ã€å‘çƒ­ã€åˆ†æ³Œç‰©å¼‚å‘³ç­‰ï¼Œå»ºè®®åŠæ—¶å°±è¯Šï¼Œè¿›è¡Œç›¸å…³æ£€æŸ¥ï¼Œå¦‚å¦‡ç§‘æ£€æŸ¥ã€Bè¶…ç­‰ï¼Œä»¥ç¡®å®šæ˜¯å¦å­˜åœ¨æ„ŸæŸ“æˆ–å…¶ä»–é—®é¢˜ã€‚åœ¨åŒ»ç”Ÿçš„æŒ‡å¯¼ä¸‹è¿›è¡Œç›¸åº”çš„æ²»ç–—ã€‚                                                                                                                                                                                                                                                                                      |\n| ç»´èƒºé…¯ç»´Eä¹³è†èƒ½æ²»ç†ä»€ä¹ˆç–¾ç—…                                                                     | ç»´èƒºé…¯ç»´Eä¹³è†æ˜¯ä¸€ç§å¤–ç”¨è¯ç‰©ï¼Œä¸»è¦ç”¨äºæ²»ç–—ç—¤ç–®ã€è„‚æº¢æ€§çš®ç‚ç­‰çš®è‚¤ç—…ã€‚å®ƒå¯ä»¥å¸®åŠ©å‡å°‘çš®è‚¤æ²¹è„‚åˆ†æ³Œï¼Œå‡è½»ç‚ç—‡ï¼Œä¿ƒè¿›çš®è‚¤ä¿®å¤ã€‚<br/><br/>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç»´èƒºé…¯ç»´Eä¹³è†å¹¶éä¸‡èƒ½è¯ï¼Œå¯¹äºæŸäº›çš®è‚¤ç—…å¯èƒ½å¹¶ä¸é€‚ç”¨ã€‚åœ¨ä½¿ç”¨å‰ï¼Œå»ºè®®å’¨è¯¢åŒ»ç”Ÿæˆ–è¯å¸ˆï¼Œäº†è§£è¯ç‰©çš„é€‚åº”ç—‡ã€ç¦å¿Œç—‡ã€ç”¨æ³•ç”¨é‡ç­‰ä¿¡æ¯ã€‚åŒæ—¶ï¼Œä½¿ç”¨è¯ç‰©æ—¶åº”éµå¾ªåŒ»å˜±ï¼Œé¿å…è‡ªè¡Œæ»¥ç”¨ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| ç—¤ç–®æ˜¯ä»€ä¹ˆåŸå› å¯¼è‡´çš„?                                                                        | ç—¤ç–®ï¼Œä¹Ÿç§°ä¸ºç²‰åˆºï¼Œæ˜¯ç”±çš®è‚¤æ¯›å­”å µå¡å¼•èµ·çš„ã€‚å µå¡æ¯›å­”çš„åŸå› æœ‰å¾ˆå¤šï¼ŒåŒ…æ‹¬ï¼š<br/>1. çš®è„‚è…ºè¿‡åº¦æ´»è·ƒï¼šçš®è„‚è…ºäº§ç”Ÿè¿‡å¤šæ²¹è„‚ï¼Œå¯¼è‡´æ¯›å­”å µå¡ã€‚<br/>2. æ¯›å­”å µå¡ï¼šçš®è‚¤è¡¨é¢çš„æ­»çš®ç»†èƒã€æ²¹è„‚å’Œç»†èŒå µå¡æ¯›å­”ã€‚<br/>3. ç‚ç—‡ï¼šç—¤ç–®é€šå¸¸ä¼´éšç€ç‚ç—‡ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºçš®è‚¤æ„ŸæŸ“æˆ–å…ç–«ç³»ç»Ÿå¯¹ç—¤ç–®çš„ååº”å¼•èµ·çš„ã€‚<br/>4. é—ä¼ å› ç´ ï¼šæŸäº›äººå¯èƒ½æ›´å®¹æ˜“æ‚£ç—¤ç–®ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºé—ä¼ å› ç´ å¼•èµ·çš„ã€‚<br/>5. è·å°”è’™å˜åŒ–ï¼šç—¤ç–®åœ¨é’æ˜¥æœŸå’Œæœˆç»æœŸé—´æ›´ä¸ºå¸¸è§ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè·å°”è’™å˜åŒ–å¼•èµ·çš„ã€‚<br/>6. ç”Ÿæ´»æ–¹å¼å› ç´ ï¼šæŸäº›ç”Ÿæ´»æ–¹å¼å› ç´ ï¼Œå¦‚å‹åŠ›ã€ä¸è‰¯é¥®é£Ÿå’Œç¼ºä¹è¿åŠ¨ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´ç—¤ç–®ã€‚<br/>7. åŒ–å¦†å“å’ŒæŠ¤è‚¤å“ï¼šæŸäº›åŒ–å¦†å“å’ŒæŠ¤è‚¤å“å¯èƒ½å¯¼è‡´ç—¤ç–®ï¼Œç‰¹åˆ«æ˜¯é‚£äº›å«æœ‰åˆºæ¿€æ€§æˆåˆ†çš„äº§å“ã€‚<br/>8. è¯ç‰©ï¼šæŸäº›è¯ç‰©ï¼Œå¦‚å£æœé¿å­•è¯ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´ç—¤ç–®ã€‚<br/>è¦é¢„é˜²ç—¤ç–®ï¼Œå¯ä»¥é‡‡å–ä¸€äº›æªæ–½ï¼Œå¦‚ä¿æŒçš®è‚¤æ¸…æ´ã€ä½¿ç”¨æ¸©å’Œçš„æŠ¤è‚¤å“ã€é¿å…è¿‡åº¦ä½¿ç”¨åŒ–å¦†å“å’ŒæŠ¤è‚¤å“ã€ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼å’Œé¥®é£Ÿä¹ æƒ¯ã€é¿å…å‹åŠ›å’Œè·å°”è’™å˜åŒ–ã€‚å¦‚æœç—¤ç–®ä¸¥é‡ï¼Œå¯èƒ½éœ€è¦å¯»æ±‚çš®è‚¤ç§‘åŒ»ç”Ÿçš„å¸®åŠ©ã€‚                                                                                                                                                         |\n| è„¸ä¸Šé•¿é’æ˜¥ç—˜æ€ä¹ˆåŠ,èƒ½åƒå•¥è¯,éœ€è¦æ³¨æ„å•¥                                                               | é’æ˜¥ç—˜ï¼Œåˆç§°ç—¤ç–®ï¼Œæ˜¯ä¸€ç§å¸¸è§çš„çš®è‚¤ç—…ã€‚é’æ˜¥ç—˜çš„æ²»ç–—éœ€è¦ç»¼åˆè€ƒè™‘é¥®é£Ÿã€ç”Ÿæ´»ä¹ æƒ¯ã€è¯ç‰©ç­‰å¤šä¸ªæ–¹é¢ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼š<br/><br/>1. é¥®é£Ÿï¼š<br/>* é¿å…é«˜ç³–ã€é«˜è„‚ã€é«˜æ²¹çš„é£Ÿç‰©ï¼Œå¦‚å¿«é¤ã€ç”œç‚¹ã€æ²¹ç‚¸é£Ÿå“ç­‰ã€‚<br/>* å¤šåƒæ–°é²œè”¬èœã€æ°´æœã€å…¨è°·ç±»é£Ÿç‰©ï¼Œä»¥å¢åŠ çº¤ç»´æ‘„å…¥ï¼Œä¿ƒè¿›è‚ é“è •åŠ¨ã€‚<br/>* é¿å…æ‘„å…¥è¿‡å¤šçš„å’–å•¡å› ã€é…’ç²¾ã€è¾›è¾£é£Ÿç‰©ç­‰åˆºæ¿€æ€§é£Ÿç‰©ã€‚<br/><br/>2. ç”Ÿæ´»ä¹ æƒ¯ï¼š<br/>* ä¿æŒè‰¯å¥½çš„ä½œæ¯æ—¶é—´ï¼Œä¿è¯å……è¶³çš„ç¡çœ ã€‚<br/>* é¿å…é•¿æ—¶é—´ä½¿ç”¨ç”µè„‘ã€æ‰‹æœºç­‰ç”µå­äº§å“ï¼Œä»¥å…è¾å°„å¯¹çš®è‚¤é€ æˆåˆºæ¿€ã€‚<br/>* ä¿æŒçš®è‚¤æ¸…æ´ï¼Œæ¯å¤©æ—©æ™šä½¿ç”¨æ¸©å’Œçš„æ´é¢äº§å“æ¸…æ´çš®è‚¤ã€‚<br/>* é¿å…æŒ¤å‹é’æ˜¥ç—˜ï¼Œä»¥å…æ„ŸæŸ“å’Œç•™ä¸‹ç–¤ç—•ã€‚<br/><br/>3. è¯ç‰©ï¼š<br/>* å¤–ç”¨è¯ç‰©ï¼šå¯ä»¥ä½¿ç”¨å«æœ‰æ°´æ¨é…¸ã€è‹¯ç”²é…¸ã€å£¬äºŒé…¸ç­‰æˆåˆ†çš„æŠ¤è‚¤å“ï¼Œå¸®åŠ©å»é™¤çš®è‚¤è¡¨é¢çš„è§’è´¨ï¼Œå‡å°‘ç—˜ç—˜çš„å½¢æˆã€‚<br/>* å£æœè¯ç‰©ï¼šå¦‚æœé’æ˜¥ç—˜ä¸¥é‡ï¼Œå¯ä»¥è€ƒè™‘å£æœæŠ—ç”Ÿç´ ã€ç»´ç”Ÿç´ Bæ—ã€é”Œç­‰è¯ç‰©ï¼Œä½†éœ€åœ¨åŒ»ç”ŸæŒ‡å¯¼ä¸‹ä½¿ç”¨ã€‚<br/><br/>4. æ³¨æ„ï¼š<br/>* ä¸è¦éšæ„ä½¿ç”¨åŒ–å¦†å“ã€æŠ¤è‚¤å“ï¼Œä»¥å…åˆºæ¿€çš®è‚¤ã€‚<br/>* ä¸è¦è¿‡åº¦æ¸…æ´çš®è‚¤ï¼Œä»¥å…ç ´åçš®è‚¤å±éšœã€‚<br/>* ä¿æŒå¿ƒæƒ…èˆ’ç•…ï¼Œé¿å…ç„¦è™‘ã€æŠ‘éƒç­‰è´Ÿé¢æƒ…ç»ªã€‚<br/><br/>æ€»ä¹‹ï¼Œé’æ˜¥ç—˜çš„æ²»ç–—éœ€è¦ç»¼åˆè€ƒè™‘é¥®é£Ÿã€ç”Ÿæ´»ä¹ æƒ¯ã€è¯ç‰©ç­‰å¤šä¸ªæ–¹é¢ã€‚åœ¨æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œè¦ä¿æŒè€å¿ƒå’Œä¿¡å¿ƒï¼Œéµå¾ªåŒ»ç”Ÿçš„å»ºè®®ï¼Œé€æ­¥æ”¹å–„çš®è‚¤çŠ¶å†µã€‚ |\n| ç”±äºè¾“ç²¾ç®¡å µå¡ä¼šä¸ä¼šå¯¼è‡´ç”·æ€§ä¸è‚²ï¼Ÿï¼ŒåŒ»ç”Ÿæ‚¨å¥½ï¼Œæˆ‘åŒäº‹è¿™å‡ å¤©ä¸å¼€å¿ƒï¼Œæˆ‘é—®å¥¹æ€ä¹ˆäº†ï¼Œå¥¹è¯´å¥¹è€å…¬è¾“ç²¾ç®¡å µå¡ï¼Œæ‰€ä»¥ä¸€ç›´æ²¡å­©å­ï¼Œä¸æ˜¯ä¸æƒ³è¦ï¼Œè¯·é—®è¾“ç²¾ç®¡å µå¡ä¼šä¸ä¼šå¯¼è‡´ç”·æ€§ä¸è‚²ï¼Ÿ | è¾“ç²¾ç®¡å µå¡å¯èƒ½ä¼šå¯¼è‡´ç”·æ€§ä¸è‚²ã€‚è¾“ç²¾ç®¡æ˜¯è¿æ¥ç¾ä¸¸å’Œå°¿é“çš„ä¸€æ¡ç®¡é“ï¼Œè´Ÿè´£å°†ç²¾å­ä»ç¾ä¸¸è¾“é€åˆ°å°¿é“ã€‚å¦‚æœè¾“ç²¾ç®¡å µå¡ï¼Œç²¾å­å°±æ— æ³•é€šè¿‡è¾“ç²¾ç®¡åˆ°è¾¾å°¿é“ï¼Œä»è€Œå¯¼è‡´ä¸è‚²ã€‚<br/><br/>è¾“ç²¾ç®¡å µå¡çš„åŸå› æœ‰å¾ˆå¤šï¼ŒåŒ…æ‹¬å…ˆå¤©æ€§å‘è‚²å¼‚å¸¸ã€ç‚ç—‡ã€è‚¿ç˜¤ã€å¤–ä¼¤ç­‰ã€‚è¾“ç²¾ç®¡å µå¡çš„æ²»ç–—æ–¹æ³•åŒ…æ‹¬æ‰‹æœ¯ç–é€šã€è¯ç‰©æ²»ç–—ç­‰ã€‚å¦‚æœè¾“ç²¾ç®¡å µå¡ä¸¥é‡ï¼Œå¯èƒ½éœ€è¦è¿›è¡Œæ‰‹æœ¯ç–é€šã€‚<br/><br/>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¾“ç²¾ç®¡å µå¡å¹¶ä¸ä¸€å®šå¯¼è‡´ä¸è‚²ï¼Œå› ä¸ºè¿˜æœ‰å…¶ä»–é€”å¾„å¯ä»¥å°†ç²¾å­è¾“é€åˆ°å¥³æ€§ä½“å†…ã€‚ä¾‹å¦‚ï¼Œç²¾å­å¯ä»¥é€šè¿‡é™„ç¾ã€å‰åˆ—è…ºç­‰ç»„ç»‡åˆ°è¾¾å¥³æ€§ä½“å†…ã€‚å› æ­¤ï¼Œå¦‚æœè¾“ç²¾ç®¡å µå¡ï¼Œå»ºè®®åŠæ—¶å°±åŒ»ï¼Œå¯»æ±‚ä¸“ä¸šåŒ»ç”Ÿçš„å¸®åŠ©ã€‚                                                                                                                                                                                                                                                                                                                                                     |\n| å¿ƒè‚Œæ¢—å¡èƒ½æ²»ç–—å—ï¼Ÿ                                                                          | å¿ƒè‚Œæ¢—æ­»æ˜¯ä¸€ç§ä¸¥é‡çš„ç–¾ç—…ï¼Œå¦‚æœåŠæ—¶æ²»ç–—ï¼Œå¯ä»¥æŒ½æ•‘ç”Ÿå‘½å¹¶é˜²æ­¢é•¿æœŸæ®‹ç–¾ã€‚æ²»ç–—å¿ƒè‚Œæ¢—æ­»çš„å…³é”®æ˜¯å°½æ—©è¯†åˆ«å’Œæ²»ç–—ã€‚<br/><br/>å¿ƒè‚Œæ¢—æ­»çš„æ²»ç–—é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š<br/><br/>1. ç«‹å³å°±åŒ»ï¼šå¦‚æœå‡ºç°èƒ¸ç—›ã€å‘¼å¸æ€¥ä¿ƒæˆ–æ™•å¥ç­‰ç—‡çŠ¶ï¼Œç«‹å³å°±åŒ»ã€‚<br/><br/>2. è¯ç‰©æ²»ç–—ï¼šåŒ»ç”Ÿå¯èƒ½ä¼šå¼€å¤„æ–¹è¯ç‰©ï¼Œå¦‚ç¡é…¸ç”˜æ²¹æˆ–é˜¿å¸åŒ¹æ—ï¼Œä»¥å¸®åŠ©ç¼“è§£ç—‡çŠ¶å¹¶é˜²æ­¢è¿›ä¸€æ­¥çš„æŸä¼¤ã€‚<br/><br/>3. æº¶æ “æ²»ç–—ï¼šå¦‚æœæ‚£è€…åœ¨åˆ°è¾¾åŒ»é™¢å‰å‡ºç°å¿ƒè‚Œæ¢—æ­»ï¼ŒåŒ»ç”Ÿå¯èƒ½ä¼šè¿›è¡Œæº¶æ “æ²»ç–—ï¼Œé€šè¿‡æ³¨å°„æº¶è§£è¡€æ “çš„è¯ç‰©æ¥å¸®åŠ©æ¢å¤è¡€æµã€‚<br/><br/>4. æ‰‹æœ¯æ²»ç–—ï¼šå¦‚æœè¯ç‰©æ²»ç–—æˆ–æº¶æ “æ²»ç–—æ— æ•ˆï¼ŒåŒ»ç”Ÿå¯èƒ½ä¼šå»ºè®®è¿›è¡Œæ‰‹æœ¯ï¼Œå¦‚å† çŠ¶åŠ¨è„‰æ­æ¡¥æ‰‹æœ¯æˆ–å¿ƒè„æ”¯æ¶æ¤å…¥æœ¯ã€‚<br/><br/>5. åº·å¤å’Œé¢„é˜²ï¼šå¿ƒè‚Œæ¢—æ­»çš„æ²»ç–—é€šå¸¸åŒ…æ‹¬åº·å¤è®¡åˆ’ï¼Œä»¥å¸®åŠ©æ‚£è€…æ¢å¤èº«ä½“åŠŸèƒ½å¹¶é¢„é˜²æœªæ¥çš„å¿ƒè„äº‹ä»¶ã€‚æ­¤å¤–ï¼Œæ‚£è€…è¿˜åº”éµå¾ªåŒ»ç”Ÿçš„å»ºè®®ï¼Œä»¥é™ä½æ‚£å¿ƒè„ç—…çš„é£é™©ã€‚<br/><br/>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¿ƒè‚Œæ¢—æ­»çš„æ²»ç–—å–å†³äºæ‚£è€…çš„å…·ä½“æƒ…å†µå’Œç—‡çŠ¶ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨æˆ–æ‚¨è®¤è¯†çš„äººå‡ºç°å¿ƒè‚Œæ¢—æ­»ç—‡çŠ¶ï¼Œè¯·ç«‹å³å°±åŒ»ã€‚                                                                                                                                                            |\n| æ€ä¹ˆè®©è‡ªå·±ç²¾åŠ›å……æ²›ï¼Œåˆ—5ç‚¹å»ºè®®                                                                    | 1. ä¿æŒå……è¶³çš„ç¡çœ ï¼šç¡çœ å¯¹äºä¿æŒç²¾åŠ›å……æ²›è‡³å…³é‡è¦ã€‚ç¡®ä¿æ¯æ™šè·å¾—7-9å°æ—¶çš„é«˜è´¨é‡ç¡çœ ã€‚<br/>2. å‡è¡¡é¥®é£Ÿï¼šå‡è¡¡é¥®é£Ÿå¯¹äºä¿æŒèƒ½é‡æ°´å¹³è‡³å…³é‡è¦ã€‚ç¡®ä¿æ‘„å…¥è¶³å¤Ÿçš„è›‹ç™½è´¨ã€ç¢³æ°´åŒ–åˆç‰©å’Œå¥åº·è„‚è‚ªã€‚<br/>3. å®šæœŸé”»ç‚¼ï¼šå®šæœŸé”»ç‚¼å¯ä»¥å¸®åŠ©æé«˜èƒ½é‡æ°´å¹³ã€‚å°è¯•æ¯å¤©è¿›è¡Œè‡³å°‘30åˆ†é’Ÿçš„æœ‰æ°§è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–éª‘è‡ªè¡Œè½¦ã€‚<br/>4. ç®¡ç†å‹åŠ›ï¼šå‹åŠ›ä¼šæ¶ˆè€—èƒ½é‡ï¼Œå› æ­¤ç®¡ç†å‹åŠ›å¯¹äºä¿æŒç²¾åŠ›å……æ²›è‡³å…³é‡è¦ã€‚å°è¯•æ‰¾åˆ°æ”¾æ¾å’Œå‡å‹çš„æ–¹æ³•ï¼Œå¦‚å†¥æƒ³ã€ç‘œä¼½æˆ–æ·±å‘¼å¸ç»ƒä¹ ã€‚<br/>5. ä¿æŒæ°´åˆ†ï¼šè„±æ°´ä¼šé™ä½èƒ½é‡æ°´å¹³ã€‚ç¡®ä¿æ¯å¤©å–è¶³å¤Ÿçš„æ°´ï¼Œå¹¶æ ¹æ®éœ€è¦è¡¥å……æ°´åˆ†ã€‚                                                                                                                                                                                                                                                                                                                                                         |\n| å†™ä¸ªæ˜¥å¤©çš„è¯—                                                                             | æ˜¥å¤©ï¼Œ<br/>æ˜¯å¤§åœ°è‹é†’çš„å­£èŠ‚ï¼Œ<br/>æ˜¯ä¸‡ç‰©å¤è‹çš„å­£èŠ‚ï¼Œ<br/>æ˜¯ç”Ÿå‘½ç»½æ”¾çš„å­£èŠ‚ã€‚<br/>æ˜¥å¤©ï¼Œ<br/>æ˜¯é˜³å…‰æ˜åªšçš„å­£èŠ‚ï¼Œ<br/>æ˜¯é¸Ÿè¯­èŠ±é¦™çš„å­£èŠ‚ï¼Œ<br/>æ˜¯ç»¿æ„ç›ç„¶çš„å­£èŠ‚ï¼Œ<br/>æ˜¯å¸Œæœ›æ— é™çš„å­£èŠ‚ã€‚<br/>æ˜¥å¤©ï¼Œ<br/>æ˜¯äººä»¬æ¬¢ç¬‘çš„å­£èŠ‚ï¼Œ<br/>æ˜¯å­©å­ä»¬å¬‰æˆçš„å­£èŠ‚ï¼Œ<br/>æ˜¯æ‹äººä»¬ç›¸æ‹¥çš„å­£èŠ‚ï¼Œ<br/>æ˜¯äººä»¬è¿½æ±‚æ¢¦æƒ³çš„å­£èŠ‚ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| å—ææ´²ä¸ºå•¥æ²¡æœ‰åŒ—æç†Š?                                                                        | å—ææ´²æ²¡æœ‰åŒ—æç†Šï¼Œå› ä¸ºåŒ—æç†Šç”Ÿæ´»åœ¨åŒ—æåœ°åŒºï¼Œè€Œå—ææ´²ä½äºå—åŠçƒï¼Œæ°”å€™å’Œç¯å¢ƒä¸åŒ—æåœ°åŒºæœ‰å¾ˆå¤§ä¸åŒã€‚å—ææ´²çš„æ°”å€™éå¸¸å¯’å†·ï¼Œå¸¸å¹´è¢«å†°é›ªè¦†ç›–ï¼Œè€ŒåŒ—æç†Šåˆ™ç”Ÿæ´»åœ¨åŒ—æåœ°åŒºï¼Œé‚£é‡Œæœ‰ä¸°å¯Œçš„æµ·æ´‹ç”Ÿç‰©å’Œé™†åœ°å“ºä¹³åŠ¨ç‰©ã€‚æ­¤å¤–ï¼Œå—ææ´²çš„ç”Ÿæ€ç³»ç»Ÿä¹Ÿä¸åŒ—æåœ°åŒºä¸åŒï¼Œæ²¡æœ‰åŒ—æç†Šæ‰€éœ€çš„ç”Ÿå­˜æ¡ä»¶ã€‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n\n</details>\n\n\n## ğŸ“š Dataset\n### åŒ»ç–—æ•°æ®é›†\n\n- 240ä¸‡æ¡ä¸­æ–‡åŒ»ç–—æ•°æ®é›†(åŒ…æ‹¬é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå¥–åŠ±æ•°æ®é›†)ï¼š[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)\n- 22ä¸‡æ¡ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†(åä½—é¡¹ç›®)ï¼š[shibing624/huatuo_medical_qa_sharegpt](https://huggingface.co/datasets/shibing624/huatuo_medical_qa_sharegpt) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘\n\n### é€šç”¨æ•°æ®é›†\n\n#### Pretraining datasets(é¢„è®­ç»ƒæ•°æ®é›†)\n- 16GBä¸­è‹±æ–‡æ— ç›‘ç£ã€å¹³è¡Œè¯­æ–™[Linly-AI/Chinese-pretraining-dataset](https://huggingface.co/datasets/Linly-AI/Chinese-pretraining-dataset)\n- 524MBä¸­æ–‡ç»´åŸºç™¾ç§‘è¯­æ–™[wikipedia-cn-20230720-filtered](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n#### Supervised fine-tuning datasets(æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†)\n- 10ä¸‡æ¡å¤šè¯­è¨€ShareGPT GPT4å¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘\n- 9ä¸‡æ¡è‹±æ–‡ShareGPTå¤šè½®å¯¹è¯æ•°é›†ï¼š[anon8231489123/ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘\n- 50ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- 100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- 5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)\n- 2ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)\n- 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)\n- 5ä¸‡æ¡è‹±æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)\n- 80ä¸‡æ¡ä¸­æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[BelleGroup/multiturn_chat_0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- 116ä¸‡æ¡ä¸­æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[fnlp/moss-002-sft-data](https://huggingface.co/datasets/fnlp/moss-002-sft-data)\n- 3.8ä¸‡æ¡ä¸­æ–‡ShareGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[FreedomIntelligence/ShareGPT-CN](https://huggingface.co/datasets/FreedomIntelligence/ShareGPT-CN)\n- 130ä¸‡æ¡ä¸­æ–‡å¾®è°ƒæ•°æ®é›†ï¼ˆæ±‡æ€»ï¼‰ï¼š[zhuangxialie/Llama3-Chinese-Dataset](https://modelscope.cn/datasets/zhuangxialie/Llama3-Chinese-Dataset/dataPeview) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘\n- 7åƒæ¡ä¸­æ–‡è§’è‰²æ‰®æ¼”å¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[shibing624/roleplay-zh-sharegpt-gpt4-data](https://huggingface.co/datasets/shibing624/roleplay-zh-sharegpt-gpt4-data) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘\n\n#### Preference datasets(åå¥½æ•°æ®é›†)\n- 2ä¸‡æ¡ä¸­è‹±æ–‡åå¥½æ•°æ®é›†ï¼š[shibing624/DPO-En-Zh-20k-Preference](https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘\n- åŸç‰ˆçš„oasst1æ•°æ®é›†ï¼š[OpenAssistant/oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)\n- 2ä¸‡æ¡å¤šè¯­è¨€oasst1çš„rewardæ•°æ®é›†ï¼š[tasksource/oasst1_pairwise_rlhf_reward](https://huggingface.co/datasets/tasksource/oasst1_pairwise_rlhf_reward)\n- 11ä¸‡æ¡è‹±æ–‡hh-rlhfçš„rewardæ•°æ®é›†ï¼š[Dahoas/full-hh-rlhf](https://huggingface.co/datasets/Dahoas/full-hh-rlhf)\n- 9ä¸‡æ¡è‹±æ–‡rewardæ•°æ®é›†(æ¥è‡ªAnthropic's Helpful Harmless dataset)ï¼š[Dahoas/static-hh](https://huggingface.co/datasets/Dahoas/static-hh)\n- 7ä¸‡æ¡è‹±æ–‡rewardæ•°æ®é›†ï¼ˆæ¥æºåŒä¸Šï¼‰ï¼š[Dahoas/rm-static](https://huggingface.co/datasets/Dahoas/rm-static)\n- 7ä¸‡æ¡ç¹ä½“ä¸­æ–‡çš„rewardæ•°æ®é›†ï¼ˆç¿»è¯‘è‡ªrm-staticï¼‰[liswei/rm-static-m2m100-zh](https://huggingface.co/datasets/liswei/rm-static-m2m100-zh)\n- 7ä¸‡æ¡è‹±æ–‡Rewardæ•°æ®é›†ï¼š[yitingxie/rlhf-reward-datasets](https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets)\n- 3åƒæ¡ä¸­æ–‡çŸ¥ä¹é—®ç­”åå¥½æ•°æ®é›†ï¼š[liyucheng/zhihu_rlhf_3k](https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k)\n\n\n## â˜ï¸ Contact\n\n- Issue(å»ºè®®)\n  ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)\n- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com\n- å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ï¼ˆåŠ æˆ‘æ‹‰ä½ è¿›ç¾¤ï¼‰ã€‚\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/wechat.jpeg\" width=\"200\" />\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/wechat_group.jpg\" width=\"200\" />\n\n## âš ï¸ LICENSE\n\næœ¬é¡¹ç›®ä»…å¯åº”ç”¨äºç ”ç©¶ç›®çš„ï¼Œé¡¹ç›®å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®ï¼ˆåŒ…å«ä½†ä¸é™äºæ•°æ®ã€æ¨¡å‹ã€ä»£ç ç­‰ï¼‰å¯¼è‡´çš„å±å®³æˆ–æŸå¤±ã€‚è¯¦ç»†è¯·å‚è€ƒ[å…è´£å£°æ˜](https://github.com/shibing624/MedicalGPT/blob/main/DISCLAIMER)ã€‚\n\nMedicalGPTé¡¹ç›®ä»£ç çš„æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œä»£ç å¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ï¼Œæ¨¡å‹æƒé‡å’Œæ•°æ®åªèƒ½ç”¨äºç ”ç©¶ç›®çš„ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ MedicalGPTçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚\n\n\n## ğŸ˜‡ Citation\n\nå¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†MedicalGPTï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š\n\n```latex\n@misc{MedicalGPT,\n  title={MedicalGPT: Training Medical GPT Model},\n  author={Ming Xu},\n  year={2023},\n  howpublished={\\url{https://github.com/shibing624/MedicalGPT}},\n}\n```\n\n## ğŸ˜ Contribute\n\né¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š\n\n- åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•\n- ä½¿ç”¨`python -m pytest`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„\n\nä¹‹åå³å¯æäº¤PRã€‚\n\n## ğŸ’• Acknowledgements\n\n- [Direct Preference Optimization:Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)\n- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)\n- [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n- [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)\n- [dvlab-research/LongLoRA](https://github.com/dvlab-research/LongLoRA)\n\nThanks for their great work!\n\n#### å…³è”é¡¹ç›®æ¨è\n- [shibing624/ChatPilot](https://github.com/shibing624/ChatPilot)ï¼šç»™ LLM Agentï¼ˆåŒ…æ‹¬RAGã€åœ¨çº¿æœç´¢ã€Code interpreterï¼‰ æä¾›ä¸€ä¸ªç®€å•å¥½ç”¨çš„Web UIç•Œé¢\n\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 20.4755859375,
          "content": "[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/MedicalGPT/blob/main/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/MedicalGPT/blob/main/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/MedicalGPT/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624)\n\n<div align=\"center\">\n  <a href=\"https://github.com/shibing624/MedicalGPT\">\n    <img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/logo.png\" width=\"120\" alt=\"Logo\">\n  </a>\n</div>\n\n-----------------\n\n# MedicalGPT: Training Medical GPT Model\n[![HF Models](https://img.shields.io/badge/Hugging%20Face-shibing624-green)](https://huggingface.co/shibing624)\n[![Github Stars](https://img.shields.io/github/stars/shibing624/MedicalGPT?color=yellow)](https://star-history.com/#shibing624/MedicalGPT&Timeline)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)\n[![Wechat Group](https://img.shields.io/badge/wechat-group-green.svg?logo=wechat)](#Contact)\n\n## ğŸ“– Introduction\n\n**MedicalGPT** trains a medical large language model using the ChatGPT training pipeline, implementing pretraining, supervised finetuning, RLHF (Reward Modeling and Reinforcement Learning), and DPO (Direct Preference Optimization).\n\n**MedicalGPT** trains medical large models, implementing incremental pretraining, supervised fine-tuning, RLHF (reward modeling, reinforcement learning training), and DPO (direct preference optimization).\n\n![DPO](https://github.com/shibing624/MedicalGPT/blob/main/docs/dpo.jpg)\n\n- The RLHF training pipeline is from Andrej Karpathy's presentation PDF [State of GPT](https://karpathy.ai/stateofgpt.pdf), video [Video](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)\n- The DPO method is from the paper [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)\n- The ORPO method is from the paper [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/GPT_Training.jpg\" width=\"860\" />\n\nTraining MedicalGPT modelï¼š\n\n- Stage 1ï¼šPT(Continue PreTraining), Pre-training the LLaMA model on massive domain document data to inject domain knowledge\n- Stage 2: SFT (Supervised Fine-tuning) has supervised fine-tuning, constructs instruction fine-tuning data sets, and performs instruction fine-tuning on the basis of pre-trained models to align instruction intentions\n- Stage 3: RM (Reward Model) reward model modeling, constructing a human preference ranking data set, training the reward model to align human preferences, mainly the \"HHH\" principle, specifically \"helpful, honest, harmless\"\n- Stage 4: RL (Reinforcement Learning) is based on human feedback reinforcement learning (RLHF), using the reward model to train the SFT model, and the generation model uses rewards or penalties to update its strategy in order to generate higher quality, more in line with human preferences text\n\n## ğŸ”¥ News\n\n- **[2024/09/21] v2.2 Release**: Supports the **[Qwen-2.5](https://qwenlm.github.io/zh/blog/qwen2.5/)** series of models. See [Release-v2.3](https://github.com/shibing624/MedicalGPT/releases/tag/2.3.0)\n\n- **[2024/08/02] v2.2 Release**: Supports role-playing model training, adds new scripts for generating patient-doctor dialogue SFT data [role_play_data](https://github.com/shibing624/MedicalGPT/blob/main/role_play_data/README.md). See [Release-v2.2](https://github.com/shibing624/MedicalGPT/releases/tag/2.2.0).\n  \n- **[2024/06/11] v2.1 Release**: Supports the **[Qwen-2](https://qwenlm.github.io/blog/qwen2/)** series of models. See [Release-v2.1](https://github.com/shibing624/MedicalGPT/releases/tag/2.1.0).\n\n- **[2024/04/24] v2.0 Release**: Supports the **[Llama-3](https://huggingface.co/meta-llama)** series of models. See [Release-v2.0](https://github.com/shibing624/MedicalGPT/releases/tag/2.0.0).\n\n- **[2024/04/17] v1.9 Release**: Supports **[ORPO](https://arxiv.org/abs/2403.07691)**. For detailed usage, refer to `run_orpo.sh`. See [Release-v1.9](https://github.com/shibing624/MedicalGPT/releases/tag/1.9.0).\n\n- **[2024/01/26] v1.8 Release**: Supports fine-tuning the Mixtral Mixture-of-Experts (MoE) model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)**. See [Release-v1.8](https://github.com/shibing624/MedicalGPT/releases/tag/1.8.0).\n\n- **[2024/01/14] v1.7 Release**: Adds retrieval-augmented generation (RAG) based file question answering [ChatPDF](https://github.com/shibing624/ChatPDF) functionality, code `chatpdf.py`, which can improve industry-specific Q&A accuracy by combining fine-tuned LLMs with knowledge base files. See [Release-v1.7](https://github.com/shibing624/MedicalGPT/releases/tag/1.7.0).\n\n- **[2023/10/23] v1.6 Release**: Adds RoPE interpolation to extend the context length of GPT models; supports **$S^2$-Attn** proposed by [FlashAttention-2](https://github.com/Dao-AILab/flash-attention) and [LongLoRA](https://github.com/dvlab-research/LongLoRA) for LLaMA models; supports the embedding noise training method [NEFTune](https://github.com/neelsjain/NEFTune). See [Release-v1.6](https://github.com/shibing624/MedicalGPT/releases/tag/1.6.0).\n\n- **[2023/08/28] v1.5 Release**: Adds the **DPO (Direct Preference Optimization)** method, which directly optimizes the behavior of language models to precisely align with human preferences. See [Release-v1.5](https://github.com/shibing624/MedicalGPT/releases/tag/1.5.0).\n\n- **[2023/08/08] v1.4 Release**: Releases the Chinese-English Vicuna-13B model fine-tuned on the ShareGPT4 dataset [shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat), and the corresponding LoRA model [shibing624/vicuna-baichuan-13b-chat-lora](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat-lora). See [Release-v1.4](https://github.com/shibing624/MedicalGPT/releases/tag/1.4.0).\n\n- **[2023/08/02] v1.3 Release**: Adds multi-turn dialogue finetuning for LLAMA, LLAMA2, Bloom, ChatGLM, ChatGLM2, and Baichuan models; adds domain vocabulary expansion functionality; adds Chinese pre-training datasets and Chinese ShareGPT finetuning datasets. See [Release-v1.3](https://github.com/shibing624/MedicalGPT/releases/tag/1.3.0).\n\n- **[2023/07/13] v1.1 Release**: Releases the Chinese medical LLAMA-13B model [shibing624/ziya-llama-13b-medical-merged](https://huggingface.co/shibing624/ziya-llama-13b-medical-merged), based on the Ziya-LLAMA-13B-v1 model, SFT fine-tunes a medical model, improving medical QA performance. See [Release-v1.1](https://github.com/shibing624/MedicalGPT/releases/tag/1.1).\n\n- **[2023/06/15] v1.0 Release**: Releases the Chinese medical LoRA model [shibing624/ziya-llama-13b-medical-lora](https://huggingface.co/shibing624/ziya-llama-13b-medical-lora), based on the Ziya-LLaMA-13B-v1 model, SFT fine-tunes a medical model, improving medical QA performance. See [Release-v1.0](https://github.com/shibing624/MedicalGPT/releases/tag/1.0.0).\n\n- **[2023/06/05] v0.2 Release**: Trains domain-specific large models using medicine as an example, implementing four stages of training: secondary pretraining, supervised fine-tuning, reward modeling, and reinforcement learning training. See [Release-v0.2](https://github.com/shibing624/MedicalGPT/releases/tag/0.2.0).\n## â–¶ï¸ Demo\n\n- Hugging Face Demo: doing\n\nWe provide a simple Gradio-based interactive web interface. After the service is started, it can be accessed through a browser, enter a question, and the model will return an answer. The command is as follows:\n```shell\npython gradio_demo.py --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir\n```\n\nParameter Description:\n\n- `--base_model {base_model}`: directory to store LLaMA model weights and configuration files in HF format, or use the HF Model Hub model call name\n- `--lora_model {lora_model}`: The directory where the LoRA file is located, and the name of the HF Model Hub model can also be used. If the lora weights have been merged into the pre-trained model, delete the --lora_model parameter\n- `--tokenizer_path {tokenizer_path}`: Store the directory corresponding to the tokenizer. If this parameter is not provided, its default value is the same as --lora_model; if the --lora_model parameter is not provided, its default value is the same as --base_model\n- `--use_cpu`: use only CPU for inference\n- `--gpus {gpu_ids}`: Specifies the number of GPU devices used, the default is 0. If using multiple GPUs, separate them with commas, such as 0,1,2\n\n\n\n\n## ğŸš€ Training Pipeline\n\n### Stage 1: Continue Pretraining\n\nBased on the llama-7b model, use medical encyclopedia data to continue pre-training, and expect to inject medical knowledge into the pre-training model to obtain the llama-7b-pt model. This step is optional\n\n\n```shell\nsh run_pt.sh\n```\n\n[Training Detail wiki](https://github.com/shibing624/MedicalGPT/wiki/Training-Details)\n\n### Stage 2: Supervised FineTuning\nBased on the llama-7b-pt model, the llama-7b-sft model is obtained by using medical question-and-answer data for supervised fine-tuning. This step is required\n\nSupervised fine-tuning of the base llama-7b-pt model to create llama-7b-sft\n\n```shell\nsh run_sft.sh\n```\n\n[Training Detail wiki](https://github.com/shibing624/MedicalGPT/wiki/Training-Details)\n\n### Stage 3: Reward Modeling\nRM(Reward Model): reward model modeling\n\nIn principle, we can directly use human annotations to fine-tune the model with RLHF.\n\nHowever, this will require us to send some samples to humans to be scored after each round of optimization. This is expensive and slow due to the large number of training samples required for convergence and the limited speed at which humans can read and annotate them.\nA better strategy than direct feedback is to train a reward model RM on the human annotated set before entering the RL loop. The purpose of the reward model is to simulate human scoring of text.\n\nThe best practice for building a reward model is to rank the prediction results, that is, for each prompt (input text) corresponding to two results (yk, yj), the model predicts which score the human annotation is higher.\nThe RM model is trained by manually marking the scoring results of the SFT model. The purpose is to replace manual scoring. It is essentially a regression model used to align human preferences, mainly based on the \"HHH\" principle, specifically \"helpful, honest, harmless\".\n\n\nBased on the llama-7b-sft model, the reward preference model is trained using medical question and answer preference data, and the llama-7b-reward model is obtained after training. This step is required\n\nReward modeling using dialog pairs from the reward dataset using the llama-7b-sft to create llama-7b-reward:\n\n```shell\nsh run_rm.sh\n```\n[Training Detail wiki](https://github.com/shibing624/MedicalGPT/wiki/Training-Details)\n\n### Stage 4: Reinforcement Learning\nThe purpose of the RL (Reinforcement Learning) model is to maximize the output of the reward model. Based on the above steps, we have a fine-tuned language model (llama-7b-sft) and reward model (llama-7b-reward).\nThe RL loop is ready to execute.\n\nThis process is roughly divided into three steps:\n\n1. Enter prompt, the model generates a reply\n2. Use a reward model to score responses\n3. Based on the score, a round of reinforcement learning for policy optimization (PPO)\n\n<img src=https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/trl_loop.png height=400 />\n\nReinforcement Learning fine-tuning of llama-7b-sft with the llama-7b-reward reward model to create llama-7b-rl\n\n```shell\nsh run_ppo.sh\n```\n[Training Detail wiki](https://github.com/shibing624/MedicalGPT/wiki/Training-Details)\n\n\n#### Supported Models\n\n\n| Model Name                                                           | Model Size                  | Target Modules  | Template  |\n|----------------------------------------------------------------------|-----------------------------|-----------------|-----------|\n| [Baichuan](https://github.com/baichuan-inc/baichuan-13B)             | 7B/13B                      | W_pack          | baichuan  |\n| [Baichuan2](https://github.com/baichuan-inc/Baichuan2)               | 7B/13B                      | W_pack          | baichuan2 |\n| [BLOOMZ](https://huggingface.co/bigscience/bloomz)                   | 560M/1.1B/1.7B/3B/7.1B/176B | query_key_value | vicuna    |\n| [ChatGLM](https://github.com/THUDM/ChatGLM-6B)                       | 6B                          | query_key_value | chatglm   |\n| [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)                     | 6B                          | query_key_value | chatglm2  |\n| [ChatGLM3](https://github.com/THUDM/ChatGLM3)                        | 6B                          | query_key_value | chatglm3  |\n| [Cohere](https://huggingface.co/CohereForAI/c4ai-command-r-plus)     | 104B                        | q_proj,v_proj   | cohere    |\n| [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM)              | 7B/16B/67B                  | q_proj,v_proj   | deepseek  |\n| [InternLM2](https://github.com/InternLM/InternLM)                    | 7B/20B                      | wqkv            | intern2    |\n| [LLaMA](https://github.com/facebookresearch/llama)                   | 7B/13B/33B/65B              | q_proj,v_proj   | alpaca    |\n| [LLaMA2](https://huggingface.co/meta-llama)                          | 7B/13B/70B                  | q_proj,v_proj   | llama2    |\n| [LLaMA3](https://huggingface.co/meta-llama)                          | 8B/70B                      | q_proj,v_proj   | llama3    |\n| [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) | 7B/8x7B                     | q_proj,v_proj   | mistral   |\n| [Orion](https://github.com/OrionStarAI/Orion)                        | 14B                         | q_proj,v_proj   | orion     |\n| [Qwen](https://github.com/QwenLM/Qwen)                               | 1.8B/7B/14B/72B             | c_attn          | chatml    |\n| [Qwen1.5](https://github.com/QwenLM/Qwen1.5)                         | 0.5B/1.8B/4B/14B/72B        | q_proj,v_proj   | qwen      |\n| [XVERSE](https://github.com/xverse-ai/XVERSE-13B)                    | 13B                         | query_key_value | xverse    |\n| [Yi](https://github.com/01-ai/Yi)                                    | 6B/34B                      | q_proj,v_proj   | yi        |\n\n\n## ğŸ’¾ Install\n#### Updating the requirements\nFrom time to time, the `requirements.txt` changes. To update, use this command:\n\n```markdown\ngit clone https://github.com/shibing624/MedicalGPT\ncd MedicalGPT\npip install -r requirements.txt --upgrade\n```\n\n### Hardware Requirement (VRAM)\n\n\n| Train Method  | Bits |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |\n|-------|------| ----- | ----- | ----- | ------ | ------ | ----- | ------ |\n| Full   | AMP  | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |\n| Full   | 16   |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |\n| LoRA  | 16   |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |\n| QLoRA | 8    |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |\n| QLoRA | 4    |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |\n| QLoRA | 2    |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB |\n\n\n## ğŸ”¥ Inference\nAfter the training is complete, now we load the trained model to verify the effect of the model generating text.\n\n```shell\npython inference.py \\\n    --base_model path_to_llama_hf_dir \\\n    --lora_model path_to_lora \\\n    --with_prompt \\\n    --interactive\n```\n\nParameter Description:\n\n- `--base_model {base_model}`: Directory to store LLaMA model weights and configuration files in HF format\n- `--lora_model {lora_model}`: The directory where the LoRA file is decompressed, and the name of the HF Model Hub model can also be used. If you have incorporated LoRA weights into the pre-trained model, you can not provide this parameter\n- `--tokenizer_path {tokenizer_path}`: Store the directory corresponding to the tokenizer. If this parameter is not provided, its default value is the same as --lora_model; if the --lora_model parameter is not provided, its default value is the same as --base_model\n- `--with_prompt`: Whether to merge the input with the prompt template. Be sure to enable this option if loading an Alpaca model!\n- `--interactive`: start interactively for multiple single rounds of question and answer\n- `--data_file {file_name}`: Start in non-interactive mode, read the contents of file_name line by line for prediction\n- `--predictions_file {file_name}`: In non-interactive mode, write the predicted results to file_name in json format\n- `--use_cpu`: use only CPU for inference\n- `--gpus {gpu_ids}`: Specifies the number of GPU devices used, the default is 0. If using multiple GPUs, separate them with commas, such as 0,1,2\n\n\n#### Inference Examples\n\n<details>\n| Human | Assistant |\n|-----|------|\n| How to make yourself energetic, list 5 suggestions | Here are some suggestions to keep you energetic: <br/>1. Get enough sleep. Make sure to get enough sleep every night. <br/>2. Exercise more. Doing moderate aerobic exercise, such as jogging and swimming, can help improve your energy and stamina. 3. Eat healthy food. Choose protein-rich foods such as chicken, fish, and eggs, as well as fresh fruits and vegetables. 4. Reduce caffeine intake. Try to avoid drinking caffeinated beverages or eating caffeinated foods during the day. <br/>5. Relax. Try meditation, yoga, or other forms of relaxation to reduce stress and anxiety. |\n</details>\n<br/>\n\n\n## ğŸ“š Dataset\n\n- 2.4 million Chinese medical datasets (including pre-training, instruction fine-tuning and reward datasets): [shibing624/medical](https://huggingface.co/datasets/shibing624/medical)\n\n**Attach links to some general datasets and medical datasets**\n\n- Belle dataset of 500,000 Chinese ChatGPT commands: [BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- Belle dataset of 1 million Chinese ChatGPT commands: [BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- Alpaca dataset of 50,000 English ChatGPT commands: [50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)\n- Alpaca dataset of 20,000 Chinese GPT-4 instructions: [shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)\n- Guanaco dataset with 690,000 Chinese instructions (500,000 Belle + 190,000 Guanaco): [Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)\n- 220,000 Chinese medical dialogue datasets (HuatuoGPT project): [FreedomIntelligence/HuatuoGPT-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1)\n\n## â˜ï¸ Contact\n\n- Issue (suggestion)\n   : [![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)\n- Email me: xuming: xuming624@qq.com\n- WeChat Me: Add me* WeChat ID: xuming624, Remarks: Name-Company Name-NLP* Enter the NLP exchange group.\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/wechat.jpeg\" width=\"200\" />\n\n## âš ï¸ LICENSE\n\nThe license agreement for the project code is [The Apache License 2.0](/LICENSE), the code is free for commercial use, and the model weights and data can only be used for research purposes. Please attach MedicalGPT's link and license agreement in the product description.\n\n## ğŸ˜‡ Citation\n\nIf you used MedicalGPT in your research, please cite as follows:\n\n```latex\n@misc{MedicalGPT,\n   title={MedicalGPT: Training Medical GPT Model},\n   author={Ming Xu},\n   year={2023},\n   howpublished={\\url{https://github.com/shibing624/MedicalGPT}},\n}\n```\n\n## ğŸ˜ Contribute\n\nThe project code is still very rough. If you have improved the code, you are welcome to submit it back to this project. Before submitting, please pay attention to the following two points:\n\n- Add corresponding unit tests in `tests`\n- Use `python -m pytest` to run all unit tests to ensure that all unit tests are passed\n\nThen you can submit a PR.\n\n## ğŸ’• Acknowledgements\n\n- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)\n- [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n\nThanks for their great work!\n#### Related Projects\n- [shibing624/ChatPilot](https://github.com/shibing624/ChatPilot): Provide a simple and easy-to-use web UI interface for LLM Agent (including RAG, online search, code interpreter).\n"
        },
        {
          "name": "_config.yml",
          "type": "blob",
          "size": 0.025390625,
          "content": "theme: jekyll-theme-cayman"
        },
        {
          "name": "build_domain_tokenizer.py",
          "type": "blob",
          "size": 2.01953125,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: Build chinese tokenizer from corpus txt\n\n# train sentencepiece model from `corpus.txt` and makes `m.model` and `m.vocab`\n# `m.vocab` is just a reference. not used in the segmentation.\n# spm.SentencePieceTrainer.train('--input=data/pretrain/tianlongbabu.txt --model_prefix=m --vocab_size=20000')\n\"\"\"\nimport argparse\n\nimport sentencepiece as spm\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--in_file', default='data/pretrain/fever.txt', type=str)\n    parser.add_argument('--domain_sp_model_name', default='domain_sp', type=str)\n    parser.add_argument('--max_sentence_length', default=16384, type=int)\n    parser.add_argument('--pad_id', default=3, type=int)\n    parser.add_argument('--vocab_size', default=2236, type=int)\n    parser.add_argument('--model_type', default=\"BPE\", type=str)\n\n    args = parser.parse_args()\n    print(args)\n\n    spm.SentencePieceTrainer.train(\n        input=args.in_file,\n        model_prefix=args.domain_sp_model_name,\n        shuffle_input_sentence=False,\n        train_extremely_large_corpus=True,\n        max_sentence_length=args.max_sentence_length,\n        pad_id=args.pad_id,\n        model_type=args.model_type,\n        vocab_size=args.vocab_size,\n        split_digits=True,\n        split_by_unicode_script=True,\n        byte_fallback=True,\n        allow_whitespace_only_pieces=True,\n        remove_extra_whitespaces=False,\n        normalization_rule_name=\"nfkc\",\n    )\n\n    # makes segmenter instance and loads the model file (m.model)\n    sp = spm.SentencePieceProcessor()\n    model_file = args.domain_sp_model_name + '.model'\n    sp.load(model_file)\n\n    # encode: text => id\n    print(sp.encode_as_pieces('æ½œä¼æ€§æ„ŸæŸ“åˆç§°æ½œåœ¨æ€§æ„ŸæŸ“ã€‚æ…•å®¹å¤æ¥åˆ°æ²³è¾¹,this is a test'))\n    print(sp.encode_as_ids('this is a test'))\n\n    # decode: id => text\n    print(sp.decode_pieces(['â–This', 'â–is', 'â–a', 'â–t', 'est']))\n    # print(sp.decode_ids([209, 31, 9, 375, 586]))\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "chatpdf.py",
          "type": "blob",
          "size": 19.3115234375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description:\n\npip install similarities PyPDF2 -U\n\"\"\"\nimport argparse\nimport hashlib\nimport os\nimport re\nfrom threading import Thread\nfrom typing import Union, List\n\nimport jieba\nimport torch\nfrom loguru import logger\nfrom peft import PeftModel\nfrom similarities import (\n    EnsembleSimilarity,\n    BertSimilarity,\n    BM25Similarity,\n)\nfrom similarities.similarity import SimilarityABC\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    TextIteratorStreamer,\n    GenerationConfig,\n)\n\njieba.setLogLevel(\"ERROR\")\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, LlamaTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\nRAG_PROMPT = \"\"\"åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šçš„æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\nå¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´ \"æ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜\" æˆ– \"æ²¡æœ‰æä¾›è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯\"ï¼Œä¸å…è®¸åœ¨ç­”æ¡ˆä¸­æ·»åŠ ç¼–é€ æˆåˆ†ï¼Œç­”æ¡ˆè¯·ä½¿ç”¨ä¸­æ–‡ã€‚\n\nå·²çŸ¥å†…å®¹:\n{context_str}\n\né—®é¢˜:\n{query_str}\n\"\"\"\n\n\nclass SentenceSplitter:\n    def __init__(self, chunk_size: int = 250, chunk_overlap: int = 50):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n\n    def split_text(self, text: str) -> List[str]:\n        if self._is_has_chinese(text):\n            return self._split_chinese_text(text)\n        else:\n            return self._split_english_text(text)\n\n    def _split_chinese_text(self, text: str) -> List[str]:\n        sentence_endings = {'\\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'â€¦'}  # å¥æœ«æ ‡ç‚¹ç¬¦å·\n        chunks, current_chunk = [], ''\n        for word in jieba.cut(text):\n            if len(current_chunk) + len(word) > self.chunk_size:\n                chunks.append(current_chunk.strip())\n                current_chunk = word\n            else:\n                current_chunk += word\n            if word[-1] in sentence_endings and len(current_chunk) > self.chunk_size - self.chunk_overlap:\n                chunks.append(current_chunk.strip())\n                current_chunk = ''\n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        if self.chunk_overlap > 0 and len(chunks) > 1:\n            chunks = self._handle_overlap(chunks)\n        return chunks\n\n    def _split_english_text(self, text: str) -> List[str]:\n        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰å¥å­åˆ†å‰²è‹±æ–‡æ–‡æœ¬\n        sentences = re.split(r'(?<=[.!?])\\s+', text.replace('\\n', ' '))\n        chunks, current_chunk = [], ''\n        for sentence in sentences:\n            if len(current_chunk) + len(sentence) <= self.chunk_size or not current_chunk:\n                current_chunk += (' ' if current_chunk else '') + sentence\n            else:\n                chunks.append(current_chunk)\n                current_chunk = sentence\n        if current_chunk:  # Add the last chunk\n            chunks.append(current_chunk)\n\n        if self.chunk_overlap > 0 and len(chunks) > 1:\n            chunks = self._handle_overlap(chunks)\n\n        return chunks\n\n    def _is_has_chinese(self, text: str) -> bool:\n        # check if contains chinese characters\n        if any(\"\\u4e00\" <= ch <= \"\\u9fff\" for ch in text):\n            return True\n        else:\n            return False\n\n    def _handle_overlap(self, chunks: List[str]) -> List[str]:\n        # å¤„ç†å—é—´é‡å \n        overlapped_chunks = []\n        for i in range(len(chunks) - 1):\n            chunk = chunks[i] + ' ' + chunks[i + 1][:self.chunk_overlap]\n            overlapped_chunks.append(chunk.strip())\n        overlapped_chunks.append(chunks[-1])\n        return overlapped_chunks\n\n\nclass ChatPDF:\n    def __init__(\n            self,\n            similarity_model: SimilarityABC = None,\n            generate_model_type: str = \"auto\",\n            generate_model_name_or_path: str = \"01-ai/Yi-6B-Chat\",\n            lora_model_name_or_path: str = None,\n            corpus_files: Union[str, List[str]] = None,\n            save_corpus_emb_dir: str = \"./corpus_embs/\",\n            device: str = None,\n            int8: bool = False,\n            int4: bool = False,\n            chunk_size: int = 250,\n            chunk_overlap: int = 30,\n            prompt_template_name: str = None,\n    ):\n        \"\"\"\n        Init RAG model.\n        :param similarity_model: similarity model, default None, if set, will use it instead of EnsembleSimilarity\n        :param generate_model_type: generate model type\n        :param generate_model_name_or_path: generate model name or path\n        :param lora_model_name_or_path: lora model name or path\n        :param corpus_files: corpus files\n        :param save_corpus_emb_dir: save corpus embeddings dir, default ./corpus_embs/\n        :param device: device, default None, auto select gpu or cpu\n        :param int8: use int8 quantization, default False\n        :param int4: use int4 quantization, default False\n        :param chunk_size: chunk size, default 250\n        :param chunk_overlap: chunk overlap, default 50\n        :param prompt_template_name: prompt template name, default None, if set, inplace tokenizer.apply_chat_template\n        \"\"\"\n        if torch.cuda.is_available():\n            default_device = torch.device(0)\n        elif torch.backends.mps.is_available():\n            default_device = 'mps'\n        else:\n            default_device = torch.device('cpu')\n        self.device = device or default_device\n        self.text_splitter = SentenceSplitter(chunk_size, chunk_overlap)\n        if similarity_model is not None:\n            self.sim_model = similarity_model\n        else:\n            m1 = BertSimilarity(model_name_or_path=\"shibing624/text2vec-base-multilingual\", device=self.device)\n            m2 = BM25Similarity()\n            default_sim_model = EnsembleSimilarity(similarities=[m1, m2], weights=[0.5, 0.5], c=2)\n            self.sim_model = default_sim_model\n        self.gen_model, self.tokenizer = self._init_gen_model(\n            generate_model_type,\n            generate_model_name_or_path,\n            peft_name=lora_model_name_or_path,\n            int8=int8,\n            int4=int4,\n        )\n        self.history = []\n        self.corpus_files = corpus_files\n        if corpus_files:\n            self.add_corpus(corpus_files)\n        self.save_corpus_emb_dir = save_corpus_emb_dir\n        self.prompt_template_name = prompt_template_name\n\n    def __str__(self):\n        return f\"Similarity model: {self.sim_model}, Generate model: {self.gen_model}\"\n\n    def _init_gen_model(\n            self,\n            gen_model_type: str,\n            gen_model_name_or_path: str,\n            peft_name: str = None,\n            int8: bool = False,\n            int4: bool = False,\n    ):\n        \"\"\"Init generate model.\"\"\"\n        if int8 or int4:\n            device_map = None\n        else:\n            device_map = \"auto\"\n        model_class, tokenizer_class = MODEL_CLASSES[gen_model_type]\n        tokenizer = tokenizer_class.from_pretrained(gen_model_name_or_path, trust_remote_code=True)\n        model = model_class.from_pretrained(\n            gen_model_name_or_path,\n            load_in_8bit=int8 if gen_model_type not in ['baichuan', 'chatglm'] else False,\n            load_in_4bit=int4 if gen_model_type not in ['baichuan', 'chatglm'] else False,\n            torch_dtype=\"auto\",\n            device_map=device_map,\n            trust_remote_code=True,\n        )\n        if self.device == torch.device('cpu'):\n            model.float()\n        if gen_model_type in ['baichuan', 'chatglm']:\n            if int4:\n                model = model.quantize(4).cuda()\n            elif int8:\n                model = model.quantize(8).cuda()\n        try:\n            model.generation_config = GenerationConfig.from_pretrained(gen_model_name_or_path, trust_remote_code=True)\n        except Exception as e:\n            logger.warning(f\"Failed to load generation config from {gen_model_name_or_path}, {e}\")\n        if peft_name:\n            model = PeftModel.from_pretrained(\n                model,\n                peft_name,\n                torch_dtype=\"auto\",\n            )\n            logger.info(f\"Loaded peft model from {peft_name}\")\n        model.eval()\n        return model, tokenizer\n\n    def _get_chat_input(self):\n        messages = []\n        if self.prompt_template_name:\n            from template import get_conv_template\n            prompt_template = get_conv_template(self.prompt_template_name)\n            prompt = prompt_template.get_prompt(messages=self.history)\n            input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids\n        else:\n            for conv in self.history:\n                if conv and len(conv) > 0 and conv[0]:\n                    messages.append({'role': 'user', 'content': conv[0]})\n                if conv and len(conv) > 1 and conv[1]:\n                    messages.append({'role': 'assistant', 'content': conv[1]})\n            input_ids = self.tokenizer.apply_chat_template(\n                conversation=messages,\n                tokenize=True,\n                add_generation_prompt=True,\n                return_tensors='pt'\n            )\n        return input_ids.to(self.gen_model.device)\n\n    @torch.inference_mode()\n    def stream_generate_answer(\n            self,\n            max_new_tokens=512,\n            temperature=0.7,\n            repetition_penalty=1.0,\n            context_len=2048\n    ):\n        streamer = TextIteratorStreamer(self.tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n        input_ids = self._get_chat_input()\n        max_src_len = context_len - max_new_tokens - 8\n        input_ids = input_ids[-max_src_len:]\n        generation_kwargs = dict(\n            input_ids=input_ids,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            do_sample=True,\n            repetition_penalty=repetition_penalty,\n            streamer=streamer,\n        )\n        thread = Thread(target=self.gen_model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        yield from streamer\n\n    def add_corpus(self, files: Union[str, List[str]]):\n        \"\"\"Load document files.\"\"\"\n        if isinstance(files, str):\n            files = [files]\n        for doc_file in files:\n            if doc_file.endswith('.pdf'):\n                corpus = self.extract_text_from_pdf(doc_file)\n            elif doc_file.endswith('.docx'):\n                corpus = self.extract_text_from_docx(doc_file)\n            elif doc_file.endswith('.md'):\n                corpus = self.extract_text_from_markdown(doc_file)\n            else:\n                corpus = self.extract_text_from_txt(doc_file)\n            full_text = '\\n'.join(corpus)\n            chunks = self.text_splitter.split_text(full_text)\n            self.sim_model.add_corpus(chunks)\n        self.corpus_files = files\n        logger.debug(f\"files: {files}, corpus size: {len(self.sim_model.corpus)}, top3: \"\n                     f\"{list(self.sim_model.corpus.values())[:3]}\")\n\n    @staticmethod\n    def get_file_hash(fpaths):\n        hasher = hashlib.md5()\n        target_file_data = bytes()\n        if isinstance(fpaths, str):\n            fpaths = [fpaths]\n        for fpath in fpaths:\n            with open(fpath, 'rb') as file:\n                chunk = file.read(1024 * 1024)  # read only first 1MB\n                hasher.update(chunk)\n                target_file_data += chunk\n\n        hash_name = hasher.hexdigest()[:32]\n        return hash_name\n\n    @staticmethod\n    def extract_text_from_pdf(file_path: str):\n        \"\"\"Extract text content from a PDF file.\"\"\"\n        import PyPDF2\n        contents = []\n        with open(file_path, 'rb') as f:\n            pdf_reader = PyPDF2.PdfReader(f)\n            for page in pdf_reader.pages:\n                page_text = page.extract_text().strip()\n                raw_text = [text.strip() for text in page_text.splitlines() if text.strip()]\n                new_text = ''\n                for text in raw_text:\n                    new_text += text\n                    if text[-1] in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', ';', 'ï¼›', ':', 'ï¼š', 'â€', 'â€™', 'ï¼‰', 'ã€‘', 'ã€‹', 'ã€',\n                                    'ã€', 'ã€•', 'ã€‰', 'ã€‹', 'ã€—', 'ã€', 'ã€Ÿ', 'Â»', '\"', \"'\", ')', ']', '}']:\n                        contents.append(new_text)\n                        new_text = ''\n                if new_text:\n                    contents.append(new_text)\n        return contents\n\n    @staticmethod\n    def extract_text_from_txt(file_path: str):\n        \"\"\"Extract text content from a TXT file.\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            contents = [text.strip() for text in f.readlines() if text.strip()]\n        return contents\n\n    @staticmethod\n    def extract_text_from_docx(file_path: str):\n        \"\"\"Extract text content from a DOCX file.\"\"\"\n        import docx\n        document = docx.Document(file_path)\n        contents = [paragraph.text.strip() for paragraph in document.paragraphs if paragraph.text.strip()]\n        return contents\n\n    @staticmethod\n    def extract_text_from_markdown(file_path: str):\n        \"\"\"Extract text content from a Markdown file.\"\"\"\n        import markdown\n        from bs4 import BeautifulSoup\n        with open(file_path, 'r', encoding='utf-8') as f:\n            markdown_text = f.read()\n        html = markdown.markdown(markdown_text)\n        soup = BeautifulSoup(html, 'html.parser')\n        contents = [text.strip() for text in soup.get_text().splitlines() if text.strip()]\n        return contents\n\n    @staticmethod\n    def _add_source_numbers(lst):\n        \"\"\"Add source numbers to a list of strings.\"\"\"\n        return [f'[{idx + 1}]\\t \"{item}\"' for idx, item in enumerate(lst)]\n\n    def predict_stream(\n            self,\n            query: str,\n            topn: int = 5,\n            max_length: int = 512,\n            context_len: int = 2048,\n            temperature: float = 0.7,\n    ):\n        \"\"\"Generate predictions stream.\"\"\"\n        reference_results = []\n        stop_str = self.tokenizer.eos_token if self.tokenizer.eos_token else \"</s>\"\n        if self.sim_model.corpus:\n            sim_contents = self.sim_model.most_similar(query, topn=topn)\n            # Get reference results\n            for query_id, id_score_dict in sim_contents.items():\n                for corpus_id, s in id_score_dict.items():\n                    reference_results.append(self.sim_model.corpus[corpus_id])\n            if not reference_results:\n                yield 'æ²¡æœ‰æä¾›è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯', reference_results\n            self.history = []\n            reference_results = self._add_source_numbers(reference_results)\n            context_str = '\\n'.join(reference_results)[:(context_len - len(RAG_PROMPT))]\n            prompt = RAG_PROMPT.format(context_str=context_str, query_str=query)\n            # logger.debug(f\"prompt: {prompt}\")\n        else:\n            prompt = query\n            logger.debug(prompt)\n        self.history.append([prompt, ''])\n        response = \"\"\n        for new_text in self.stream_generate_answer(\n                max_new_tokens=max_length,\n                temperature=temperature,\n                context_len=context_len,\n        ):\n            if new_text != stop_str:\n                response += new_text\n                yield response\n\n    def predict(\n            self,\n            query: str,\n            topn: int = 5,\n            max_length: int = 512,\n            context_len: int = 2048,\n            temperature: float = 0.7,\n            do_print: bool = False,\n    ):\n        \"\"\"Query from corpus.\"\"\"\n        reference_results = []\n        if self.sim_model.corpus:\n            sim_contents = self.sim_model.most_similar(query, topn=topn)\n            # Get reference results\n            for query_id, id_score_dict in sim_contents.items():\n                for corpus_id, s in id_score_dict.items():\n                    reference_results.append(self.sim_model.corpus[corpus_id])\n            if not reference_results:\n                return 'æ²¡æœ‰æä¾›è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯', reference_results\n            self.history = []\n            reference_results = self._add_source_numbers(reference_results)\n            context_str = '\\n'.join(reference_results)[:(context_len - len(RAG_PROMPT))]\n            prompt = RAG_PROMPT.format(context_str=context_str, query_str=query)\n            # logger.debug(f\"prompt: {prompt}\")\n        else:\n            prompt = query\n        self.history.append([prompt, ''])\n        response = \"\"\n        for new_text in self.stream_generate_answer(\n                max_new_tokens=max_length,\n                temperature=temperature,\n                context_len=context_len,\n        ):\n            response += new_text\n            if do_print:\n                print(new_text, end=\"\", flush=True)\n        if do_print:\n            print(\"\", flush=True)\n        response = response.strip()\n        self.history[-1][1] = response\n        return response, reference_results\n\n    def save_corpus_emb(self):\n        dir_name = self.get_file_hash(self.corpus_files)\n        save_dir = os.path.join(self.save_corpus_emb_dir, dir_name)\n        if hasattr(self.sim_model, 'save_corpus_embeddings'):\n            self.sim_model.save_corpus_embeddings(save_dir)\n            logger.debug(f\"Saving corpus embeddings to {save_dir}\")\n        return save_dir\n\n    def load_corpus_emb(self, emb_dir: str):\n        if hasattr(self.sim_model, 'load_corpus_embeddings'):\n            logger.debug(f\"Loading corpus embeddings from {emb_dir}\")\n            self.sim_model.load_corpus_embeddings(emb_dir)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--sim_model\", type=str, default=\"shibing624/text2vec-base-multilingual\")\n    parser.add_argument(\"--gen_model_type\", type=str, default=\"auto\")\n    parser.add_argument(\"--gen_model\", type=str, default=\"01-ai/Yi-6B-Chat\")\n    parser.add_argument(\"--prompt_template_name\", type=str, default=None,\n                        help=\"The prompt template name. it can be vicuna/alpaca/yi..., None is use apply_chat_template.\")\n    parser.add_argument(\"--lora_model\", type=str, default=None)\n    parser.add_argument(\"--corpus_files\", type=str, default=\"data/rag/medical_corpus.txt\")\n    parser.add_argument(\"--device\", type=str, default=None)\n    parser.add_argument(\"--int4\", action='store_true', help=\"use int4 quantization\")\n    parser.add_argument(\"--int8\", action='store_true', help=\"use int8 quantization\")\n    parser.add_argument(\"--chunk_size\", type=int, default=100)\n    parser.add_argument(\"--chunk_overlap\", type=int, default=5)\n    args = parser.parse_args()\n    print(args)\n    sim_model = BertSimilarity(model_name_or_path=args.sim_model, device=args.device)\n    m = ChatPDF(\n        similarity_model=sim_model,\n        generate_model_type=args.gen_model_type,\n        generate_model_name_or_path=args.gen_model,\n        lora_model_name_or_path=args.lora_model,\n        device=args.device,\n        int4=args.int4,\n        int8=args.int8,\n        chunk_size=args.chunk_size,\n        chunk_overlap=args.chunk_overlap,\n        corpus_files=args.corpus_files.split(','),\n        prompt_template_name=args.prompt_template_name,\n    )\n    query = [\n        \"ç»´èƒºé…¯ç»´Eä¹³è†èƒ½æ²»ç†ä»€ä¹ˆç–¾ç—…\",\n        \"å¤©é›„çš„è¯ç”¨æ¤ç‰©æ ½åŸ¹æ˜¯ä»€ä¹ˆ\",\n        \"è†ºçª—ç©´çš„å®šä½æ˜¯ä»€ä¹ˆ\",\n    ]\n    for i in query:\n        response, reference_results = m.predict(i)\n        print(f\"===\")\n        print(f\"Input: {i}\")\n        print(f\"Reference: {reference_results}\")\n        print(f\"Output: {response}\")\n"
        },
        {
          "name": "convert_dataset.py",
          "type": "blob",
          "size": 2.6083984375,
          "content": "\"\"\"\nConvert alpaca dataset into sharegpt format.\n\nUsage: python convert_dataset.py --in_file alpaca_data.json --out_file alpaca_data_sharegpt.jsonl\n\"\"\"\n\nimport argparse\n\nfrom datasets import load_dataset\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--in_file\", type=str, required=True)\n    parser.add_argument(\"--out_file\", type=str, required=True)\n    parser.add_argument(\"--data_type\", type=str, default='alpaca', help=\"alpaca, qa, or sharegpt\")\n    parser.add_argument(\"--file_type\", type=str, default='json', help='input file type, json or csv')\n    args = parser.parse_args()\n    print(args)\n    data_files = {\"train\": args.in_file}\n    if args.file_type == 'csv':\n        if args.data_type in ['qa']:\n            column_names = ['input', 'output']\n        else:\n            column_names = ['instruction', 'input', 'output']\n        raw_datasets = load_dataset('csv', data_files=data_files, column_names=column_names, delimiter='\\t')\n    elif args.file_type in ['json', 'jsonl']:\n        raw_datasets = load_dataset('json', data_files=data_files)\n    else:\n        raise ValueError(\"File type not supported\")\n    ds = raw_datasets['train']\n\n\n    def process_qa(examples):\n        convs = []\n        for q, a in zip(examples['input'], examples['output']):\n            convs.append([\n                {\"from\": \"human\", \"value\": q},\n                {\"from\": \"gpt\", \"value\": a}\n            ])\n        return {\"conversations\": convs}\n\n\n    def process_alpaca(examples):\n        convs = []\n        for instruction, inp, output in zip(examples['instruction'], examples['input'], examples['output']):\n            if inp and len(inp.strip()) > 0:\n                instruction = instruction + '\\n\\n' + inp\n            q = instruction\n            a = output\n            convs.append([\n                {\"from\": \"human\", \"value\": q},\n                {\"from\": \"gpt\", \"value\": a}\n            ])\n        return {\"conversations\": convs}\n\n\n    if args.data_type in ['alpaca']:\n        ds = ds.map(process_alpaca, batched=True, remove_columns=ds.column_names, desc=\"Running process\")\n    elif args.data_type in ['qa']:\n        ds = ds.map(process_qa, batched=True, remove_columns=ds.column_names, desc=\"Running process\")\n    else:\n        # Other sharegpt dataset, need rename to conversations and remove unused columns\n        if \"items\" in ds.column_names:\n            ds = ds.rename(columns={\"items\": \"conversations\"})\n        columns_to_remove = ds.column_names.copy()\n        columns_to_remove.remove('conversations')\n        ds = ds.remove_columns(columns_to_remove)\n\n    ds.to_json(f\"{args.out_file}\", lines=True, force_ascii=False)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "deepspeed_zero_stage2_config.json",
          "type": "blob",
          "size": 1.1025390625,
          "content": "{\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\",\n            \"torch_adam\": true,\n            \"adam_w_mode\": true\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    },\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true\n    },\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 1000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}"
        },
        {
          "name": "deepspeed_zero_stage3_config.json",
          "type": "blob",
          "size": 1.205078125,
          "content": "{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"sub_group_size\": 1e9,\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": \"auto\"\n    },\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dpo_training.py",
          "type": "blob",
          "size": 22.302734375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: Train a model from SFT using DPO\n\"\"\"\nimport os\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import Dict, Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType\nfrom transformers import (\n    AutoConfig,\n    BloomForCausalLM,\n    AutoModelForCausalLM,\n    AutoModel,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n    TrainingArguments,\n    BitsAndBytesConfig,\n)\nfrom transformers.deepspeed import is_deepspeed_zero3_enabled\nfrom trl import DPOTrainer\n\nfrom template import get_conv_template\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with DPO\n    \"\"\"\n    # Model arguments\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The model checkpoint for weights initialization.\"}\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The tokenizer for weights initialization.\"}\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n    # Dataset arguments\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The input jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The prompt template name.\"})\n    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"Train batch size per device\"})\n    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"Eval batch size per device\"})\n    max_source_length: Optional[int] = field(default=2048, metadata={\"help\": \"Max length of prompt input text\"})\n    max_target_length: Optional[int] = field(default=512, metadata={\"help\": \"Max length of output text\"})\n    min_target_length: Optional[int] = field(default=4, metadata={\"help\": \"Min length of output text\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=4, metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    # Training arguments\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})\n    target_modules: Optional[str] = field(default=None)\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=16.0)\n    peft_path: Optional[str] = field(default=None)\n    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the validation set.\"})\n    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"The beta parameter for DPO loss\"})\n    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"Learning rate\"})\n    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"The lr scheduler type\"})\n    warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"The number of warmup steps\"})\n    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"The weight decay\"})\n    optim: Optional[str] = field(default=\"adamw_hf\", metadata={\"help\": \"The optimizer type\"})\n    fp16: Optional[bool] = field(default=True, metadata={\"help\": \"Whether to use fp16\"})\n    bf16: Optional[bool] = field(default=False, metadata={\"help\": \"Whether to use bf16\"})\n    gradient_checkpointing: Optional[bool] = field(\n        default=True, metadata={\"help\": \"Whether to use gradient checkpointing\"}\n    )\n    gradient_accumulation_steps: Optional[int] = field(\n        default=4, metadata={\"help\": \"The number of gradient accumulation steps\"}\n    )\n    save_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to save the model\"})\n    eval_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to evaluate the model\"})\n    logging_steps: Optional[int] = field(default=1, metadata={\"help\": \"X steps to log the model\"})\n    output_dir: Optional[str] = field(default=\"outputs-dpo\", metadata={\"help\": \"The output directory\"})\n    max_steps: Optional[int] = field(default=200, metadata={\"help\": \"Number of steps to train\"})\n    eval_strategy: Optional[str] = field(default=\"steps\", metadata={\"help\": \"Evaluation strategy\"})\n    remove_unused_columns: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Remove unused columns from the dataset if `datasets.Dataset` is used\"},\n    )\n    report_to: Optional[str] = field(default=\"tensorboard\", metadata={\"help\": \"Report to wandb or tensorboard\"})\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\"You must specify a valid model_type to run training.\")\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'output_layer' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\ndef main():\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    logger.info(f\"Parse args: {args}\")\n\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    if args.model_type == 'bloom':\n        args.use_fast_tokenizer = True\n    # Load tokenizer\n    tokenizer_kwargs = {\n        \"cache_dir\": args.cache_dir,\n        \"use_fast\": args.use_fast_tokenizer,\n        \"trust_remote_code\": args.trust_remote_code,\n    }\n    tokenizer_name_or_path = args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    # Get datasets\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name,\n            args.dataset_config_name,\n            cache_dir=args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    else:\n        data_files = {}\n        if args.train_file_dir is not None and os.path.exists(args.train_file_dir):\n            train_data_files = glob(f'{args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {', '.join(train_data_files)}\")\n            data_files[\"train\"] = train_data_files\n        if args.validation_file_dir is not None and os.path.exists(args.validation_file_dir):\n            eval_data_files = glob(f'{args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    max_source_length = args.max_source_length\n    max_target_length = args.max_target_length\n    full_max_length = max_source_length + max_target_length\n\n    def return_prompt_and_responses(examples) -> Dict[str, str]:\n        \"\"\"Load the paired dataset and convert it to the necessary format.\n\n        The dataset is converted to a dictionary with the following structure:\n        {\n            'prompt': List[str],\n            'chosen': List[str],\n            'rejected': List[str],\n        }\n\n        Prompts are structured as follows:\n          system_prompt + history[[q,a], [q,a]...] + question\n        \"\"\"\n        prompts = []\n        for system, history, question in zip(examples[\"system\"], examples[\"history\"], examples[\"question\"]):\n            system_prompt = system or \"\"\n            history_with_question = history + [[question, '']] if history else [[question, '']]\n            prompts.append(prompt_template.get_prompt(messages=history_with_question, system_prompt=system_prompt))\n        return {\n            \"prompt\": prompts,\n            \"chosen\": examples[\"response_chosen\"],\n            \"rejected\": examples[\"response_rejected\"],\n        }\n\n    # Preprocess the dataset\n    train_dataset = None\n    max_train_samples = 0\n    if args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train']\n        max_train_samples = len(train_dataset)\n        if args.max_train_samples is not None and args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        tokenized_dataset = train_dataset.shuffle().map(\n            return_prompt_and_responses,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=train_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        train_dataset = tokenized_dataset.filter(\n            lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n                      and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n        )\n        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n        logger.debug(\"First train example:\")\n        first_example = train_dataset[0]\n        logger.debug(f\"prompt:\\n{first_example['prompt']}\")\n        logger.debug(f\"chosen:\\n{first_example['chosen']}\")\n        logger.debug(f\"rejected:\\n{first_example['rejected']}\")\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if args.do_eval:\n        if \"validation\" not in raw_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = raw_datasets[\"validation\"]\n        max_eval_samples = len(eval_dataset)\n        if args.max_eval_samples is not None and args.max_eval_samples > 0:\n            max_eval_samples = min(len(eval_dataset), args.max_eval_samples)\n            eval_dataset = eval_dataset.select(range(max_eval_samples))\n        logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n        eval_dataset = eval_dataset.map(\n            return_prompt_and_responses,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=eval_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        eval_dataset = eval_dataset.filter(\n            lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n                      and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n        )\n        logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n        logger.debug(\"First eval example:\")\n        first_example = eval_dataset[0]\n        logger.debug(f\"prompt:\\n{first_example['prompt']}\")\n        logger.debug(f\"chosen:\\n{first_example['chosen']}\")\n        logger.debug(f\"rejected:\\n{first_example['rejected']}\")\n\n    # Load model\n    torch_dtype = (\n        args.torch_dtype\n        if args.torch_dtype in [\"auto\", None]\n        else getattr(torch, args.torch_dtype)\n    )\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    ddp = world_size != 1\n    if ddp:\n        args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n    logger.info(f\"Device map: {args.device_map}\")\n    if args.qlora and is_deepspeed_zero3_enabled():\n        logger.warning(\"ZeRO3 are both currently incompatible with QLoRA.\")\n    config = config_class.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=args.trust_remote_code,\n        torch_dtype=torch_dtype,\n        cache_dir=args.cache_dir\n    )\n    if args.load_in_4bit or args.load_in_8bit:\n        logger.info(f\"Quantizing model, load_in_4bit: {args.load_in_4bit}, load_in_8bit: {args.load_in_8bit}\")\n    model = model_class.from_pretrained(\n        args.model_name_or_path,\n        config=config,\n        torch_dtype=torch_dtype,\n        low_cpu_mem_usage=(not is_deepspeed_zero3_enabled()),\n        device_map=args.device_map,\n        trust_remote_code=args.trust_remote_code,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.load_in_4bit,\n            load_in_8bit=args.load_in_8bit,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch_dtype,\n        ) if args.qlora else None,\n    )\n    # fixed FP16 ValueError\n    for param in filter(lambda p: p.requires_grad, model.parameters()):\n        param.data = param.data.to(torch.float32)\n\n    # Initialize our Trainer\n    if args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n    else:\n        model.config.use_cache = True\n\n    training_args = TrainingArguments(\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        per_device_eval_batch_size=args.per_device_eval_batch_size,\n        max_steps=args.max_steps,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        gradient_checkpointing=args.gradient_checkpointing,\n        learning_rate=args.learning_rate,\n        evaluation_strategy=args.eval_strategy,\n        eval_steps=args.eval_steps,\n        output_dir=args.output_dir,\n        report_to=args.report_to,\n        lr_scheduler_type=args.lr_scheduler_type,\n        warmup_steps=args.warmup_steps,\n        optim=args.optim,\n        bf16=args.bf16,\n        fp16=args.fp16,\n        remove_unused_columns=args.remove_unused_columns,\n        run_name=f\"dpo_{args.model_type}\",\n    )\n\n    # Initialize DPO trainer\n    peft_config = None\n    if args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        target_modules = args.target_modules.split(',') if args.target_modules else None\n        if target_modules and 'all' in target_modules:\n            target_modules = find_all_linear_names(model, int4=args.load_in_4bit, int8=args.load_in_8bit)\n        logger.info(f\"Peft target_modules: {target_modules}\")\n        peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM,\n            target_modules=target_modules,\n            inference_mode=False,\n            r=args.lora_rank,\n            lora_alpha=args.lora_alpha,\n            lora_dropout=args.lora_dropout,\n        )\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n    trainer = DPOTrainer(\n        model,\n        ref_model=None if args.use_peft else deepcopy(model),\n        args=training_args,\n        beta=args.beta,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        peft_config=peft_config if args.use_peft else None,\n        max_prompt_length=args.max_source_length,\n        max_length=full_max_length,\n    )\n    print_trainable_parameters(trainer.model)\n\n    # Training\n    if args.do_train:\n        logger.info(\"*** Train ***\")\n        train_result = trainer.train()\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {args.output_dir}\")\n            trainer.save_model(args.output_dir)\n            tokenizer.save_pretrained(args.output_dir)\n            trainer.model.save_pretrained(args.output_dir)\n\n    # Evaluation\n    if args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n        metrics[\"eval_samples\"] = max_eval_samples\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "eval_quantize.py",
          "type": "blob",
          "size": 5.162109375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@description: eval quantize for jsonl format data\n\nusage:\npython eval_quantize.py --bnb_path /path/to/your/bnb_model --data_path data/finetune/medical_sft_1K_format.jsonl\n\"\"\"\nimport torch\nimport json\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport GPUtil\nimport argparse\nimport logging\nimport os\n\n# è®¾ç½®æ—¥å¿—è®°å½•\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# åˆ›å»ºå‚æ•°è§£æå™¨\nparser = argparse.ArgumentParser(description=\"========é‡åŒ–å›°æƒ‘åº¦æµ‹è¯•========\")\nparser.add_argument(\n    \"--bnb_path\",  \n    type=str,  \n    required=True,  # è®¾ç½®ä¸ºå¿…é¡»çš„å‚æ•°\n    help=\"bnbé‡åŒ–åçš„æ¨¡å‹è·¯å¾„ã€‚\"  \n)\nparser.add_argument(\n    \"--data_path\",  \n    type=str,  \n    required=True,  # è®¾ç½®ä¸ºå¿…é¡»çš„å‚æ•°\n    help=\"jsonlæ•°æ®é›†è·¯å¾„ã€‚\"  \n)\n\n# è®¾å¤‡é€‰æ‹©å‡½æ•°\ndef get_device():\n    if torch.backends.mps.is_available():\n        return \"mps\"\n    elif torch.cuda.is_available():\n        return \"cuda:0\"\n    else:\n        return \"cpu\"\n\n# æ¸…ç†GPUç¼“å­˜å‡½æ•°\ndef clear_gpu_cache():\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n# ä»jsonlæ–‡ä»¶ä¸­åŠ è½½æ•°æ®\ndef load_jsonl_data(file_path):\n    logger.info(f\"Loading data from {file_path}\")\n    conversations = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                data = json.loads(line)\n                # æå– human å’Œ gpt éƒ¨åˆ†çš„æ–‡æœ¬\n                for conv in data['conversations']:\n                    if conv['from'] == 'human':\n                        input_text = conv['value']\n                    elif conv['from'] == 'gpt':\n                        target_text = conv['value']\n                        conversations.append((input_text, target_text))\n        return conversations\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        return []\n\n# å›°æƒ‘åº¦è¯„ä¼°å‡½æ•°\ndef evaluate_perplexity(model, tokenizer, conversation_pairs):\n    def _perplexity(nlls, n_samples, seqlen):\n        try:\n            return torch.exp(torch.stack(nlls).sum() / (n_samples * seqlen))\n        except Exception as e:\n            logger.error(f\"Error calculating perplexity: {e}\")\n            return float('inf')\n\n    model = model.eval()\n    nlls = []\n    # è·å–è®¾å¤‡\n    device = get_device()\n\n    # ç¡®ä¿ tokenizer å’Œ model ä½¿ç”¨ç›¸åŒçš„è®¾å¤‡\n    model = model.to(device)\n    # éå†æ¯ä¸ªå¯¹è¯ï¼ŒåŸºäº human éƒ¨åˆ†ç”Ÿæˆå¹¶ä¸ gpt éƒ¨åˆ†è®¡ç®—å›°æƒ‘åº¦\n    for input_text, target_text in tqdm(conversation_pairs, desc=\"Perplexity Evaluation\"):\n        # Tokenize input and target\n        inputs = tokenizer(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).input_ids.to(get_device())\n        target_ids = tokenizer(target_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).input_ids.to(get_device())\n\n        # Ensure both inputs and target have the same length\n        if inputs.size(1) != target_ids.size(1):\n            logger.warning(f\"Input length {inputs.size(1)} and Target length {target_ids.size(1)} are not equal.\")\n        \n        # Forward pass\n        with torch.no_grad():\n            outputs = model(input_ids=inputs, labels=target_ids)\n            loss = outputs.loss\n            nlls.append(loss * target_ids.size(1))  # loss * sequence length\n\n\n\n    # è®¡ç®—æœ€ç»ˆå›°æƒ‘åº¦\n    total_samples = len(conversation_pairs)\n    total_length = sum([len(pair[1]) for pair in conversation_pairs])\n    ppl = _perplexity(nlls, total_samples, total_length)\n    logger.info(f\"Final Perplexity: {ppl:.3f}\")\n\n    return ppl.item()\n\n# ä¸»å‡½æ•°\nif __name__ == \"__main__\":\n    \n    args = parser.parse_args()\n\n    if not os.path.exists(args.bnb_path):\n        logger.error(f\"Model path {args.bnb_path} does not exist.\")\n        exit(1)\n\n    try:\n        # è®¾ç½®BNBé‡åŒ–é…ç½®\n        from accelerate.utils import BnbQuantizationConfig\n        bnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n        \n        logger.info(f\"Loading BNB model from: {args.bnb_path}\")\n        tokenizer = AutoTokenizer.from_pretrained(args.bnb_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(args.bnb_path, trust_remote_code=True)\n        \n        # æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ\n        if torch.cuda.is_available():\n            gpu_usage = GPUtil.getGPUs()[0].memoryUsed\n            logger.info(f\"GPU usage before evaluation: {round(gpu_usage/1024, 2)} GB\")\n        \n        # åŠ è½½jsonlæ•°æ®\n        conversation_pairs = load_jsonl_data(args.data_path)\n        \n        if not conversation_pairs:\n            logger.error(\"No valid conversation pairs found.\")\n            exit(1)\n\n        # å¼€å§‹è¯„ä¼°\n        evaluate_perplexity(model, tokenizer, conversation_pairs)\n        \n        # è¯„ä¼°å®Œæ¯•ï¼Œæ¸…ç†æ¨¡å‹å’Œç¼“å­˜\n        del model\n        clear_gpu_cache()\n        logger.info(\"Evaluation completed and GPU cache cleared.\")\n    \n    except Exception as e:\n        logger.error(f\"An error occurred: {e}\")\n"
        },
        {
          "name": "fastapi_server_demo.py",
          "type": "blob",
          "size": 6.736328125,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: api start demo\n\nusage:\nCUDA_VISIBLE_DEVICES=0 python fastapi_server_demo.py --model_type bloom --base_model bigscience/bloom-560m\n\ncurl -X 'POST' 'http://0.0.0.0:8008/chat' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"input\": \"å’é¹…--éª†å®¾ç‹ï¼›ç™»é»„é¹¤æ¥¼--\"\n}'\n\"\"\"\n\nimport argparse\nimport os\nfrom threading import Thread\n\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI\nfrom loguru import logger\nfrom peft import PeftModel\nfrom pydantic import BaseModel, Field\nfrom starlette.middleware.cors import CORSMiddleware\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaForCausalLM,\n    TextIteratorStreamer,\n    GenerationConfig,\n)\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@torch.inference_mode()\ndef stream_generate_answer(\n        model,\n        tokenizer,\n        prompt,\n        device,\n        do_print=True,\n        max_new_tokens=512,\n        repetition_penalty=1.0,\n        context_len=2048,\n        stop_str=\"</s>\",\n):\n    streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n    input_ids = tokenizer(prompt).input_ids\n    max_src_len = context_len - max_new_tokens - 8\n    input_ids = input_ids[-max_src_len:]\n    generation_kwargs = dict(\n        input_ids=torch.as_tensor([input_ids]).to(device),\n        max_new_tokens=max_new_tokens,\n        num_beams=1,\n        repetition_penalty=repetition_penalty,\n        streamer=streamer,\n    )\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    generated_text = \"\"\n    for new_text in streamer:\n        stop = False\n        pos = new_text.find(stop_str)\n        if pos != -1:\n            new_text = new_text[:pos]\n            stop = True\n        generated_text += new_text\n        if do_print:\n            print(new_text, end=\"\", flush=True)\n        if stop:\n            break\n    if do_print:\n        print()\n    return generated_text\n\n\nclass Item(BaseModel):\n    input: str = Field(..., max_length=2048)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True)\n    parser.add_argument('--base_model', default=None, type=str, required=True)\n    parser.add_argument('--lora_model', default=\"\", type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--tokenizer_path', default=None, type=str)\n    parser.add_argument('--template_name', default=\"vicuna\", type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan, chatglm2 etc.\")\n    parser.add_argument('--system_prompt', default=\"\", type=str)\n    parser.add_argument(\"--repetition_penalty\", default=1.0, type=float)\n    parser.add_argument(\"--max_new_tokens\", default=512, type=int)\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--gpus', default=\"0\", type=str)\n    parser.add_argument('--only_cpu', action='store_true', help='only use CPU for inference')\n    parser.add_argument('--port', default=8008, type=int)\n    args = parser.parse_args()\n    print(args)\n\n    def load_model(args):\n        if args.only_cpu is True:\n            args.gpus = \"\"\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n        load_type = 'auto'\n        if torch.cuda.is_available():\n            device = torch.device(0)\n        else:\n            device = torch.device('cpu')\n        if args.tokenizer_path is None:\n            args.tokenizer_path = args.base_model\n\n        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True)\n        base_model = model_class.from_pretrained(\n            args.base_model,\n            torch_dtype=load_type,\n            low_cpu_mem_usage=True,\n            device_map='auto',\n            trust_remote_code=True,\n        )\n        try:\n            base_model.generation_config = GenerationConfig.from_pretrained(args.base_model, trust_remote_code=True)\n        except OSError:\n            print(\"Failed to load generation config, use default.\")\n        if args.resize_emb:\n            model_vocab_size = base_model.get_input_embeddings().weight.size(0)\n            tokenzier_vocab_size = len(tokenizer)\n            print(f\"Vocab of the base model: {model_vocab_size}\")\n            print(f\"Vocab of the tokenizer: {tokenzier_vocab_size}\")\n            if model_vocab_size != tokenzier_vocab_size:\n                print(\"Resize model embeddings to fit tokenizer\")\n                base_model.resize_token_embeddings(tokenzier_vocab_size)\n\n        if args.lora_model:\n            model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=load_type, device_map='auto')\n            print(\"Loaded lora model\")\n        else:\n            model = base_model\n        if device == torch.device('cpu'):\n            model.float()\n        model.eval()\n        print(tokenizer)\n        return model, tokenizer, device\n\n    # define the app\n    app = FastAPI()\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"])\n\n    model, tokenizer, device = load_model(args)\n    prompt_template = get_conv_template(args.template_name)\n    stop_str = tokenizer.eos_token if tokenizer.eos_token else prompt_template.stop_str\n\n    def predict(sentence):\n        history = [[sentence, '']]\n        prompt = prompt_template.get_prompt(messages=history, system_prompt=args.system_prompt)\n        response = stream_generate_answer(\n            model,\n            tokenizer,\n            prompt,\n            device,\n            do_print=False,\n            max_new_tokens=args.max_new_tokens,\n            repetition_penalty=args.repetition_penalty,\n            stop_str=stop_str,\n        )\n        return response.strip()\n\n    @app.get('/')\n    async def index():\n        return {\"message\": \"index, docs url: /docs\"}\n\n    @app.post('/chat')\n    async def chat(item: Item):\n        try:\n            response = predict(item.input)\n            result_dict = {'response': response}\n            logger.debug(f\"Successfully get result, q:{item.input}\")\n            return result_dict\n        except Exception as e:\n            logger.error(e)\n            return None\n\n    uvicorn.run(app=app, host='0.0.0.0', port=args.port, workers=1)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "gradio_demo.py",
          "type": "blob",
          "size": 5.0302734375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description:\n\npip install gradio>=3.50.2\n\"\"\"\nimport argparse\nfrom threading import Thread\n\nimport gradio as gr\nimport torch\nfrom peft import PeftModel\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaForCausalLM,\n    GenerationConfig,\n    TextIteratorStreamer,\n)\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default='auto', type=str)\n    parser.add_argument('--base_model', default=None, type=str, required=True)\n    parser.add_argument('--lora_model', default=\"\", type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--tokenizer_path', default=None, type=str)\n    parser.add_argument('--template_name', default=\"vicuna\", type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan2, chatglm2 etc.\")\n    parser.add_argument('--system_prompt', default=\"\", type=str)\n    parser.add_argument('--only_cpu', action='store_true', help='only use CPU for inference')\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--share', action='store_true', help='Share gradio')\n    parser.add_argument('--port', default=8081, type=int, help='Port of gradio demo')\n    args = parser.parse_args()\n    print(args)\n    load_type = 'auto'\n    if torch.cuda.is_available() and not args.only_cpu:\n        device = torch.device(0)\n    else:\n        device = torch.device('cpu')\n\n    if args.tokenizer_path is None:\n        args.tokenizer_path = args.base_model\n    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True)\n    base_model = model_class.from_pretrained(\n        args.base_model,\n        torch_dtype=load_type,\n        low_cpu_mem_usage=True,\n        device_map='auto',\n        trust_remote_code=True,\n    )\n    try:\n        base_model.generation_config = GenerationConfig.from_pretrained(args.base_model, trust_remote_code=True)\n    except OSError:\n        print(\"Failed to load generation config, use default.\")\n    if args.resize_emb:\n        model_vocab_size = base_model.get_input_embeddings().weight.size(0)\n        tokenzier_vocab_size = len(tokenizer)\n        print(f\"Vocab of the base model: {model_vocab_size}\")\n        print(f\"Vocab of the tokenizer: {tokenzier_vocab_size}\")\n        if model_vocab_size != tokenzier_vocab_size:\n            print(\"Resize model embeddings to fit tokenizer\")\n            base_model.resize_token_embeddings(tokenzier_vocab_size)\n    if args.lora_model:\n        model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=load_type, device_map='auto')\n        print(\"loaded lora model\")\n    else:\n        model = base_model\n    if device == torch.device('cpu'):\n        model.float()\n    model.eval()\n    prompt_template = get_conv_template(args.template_name)\n    system_prompt = args.system_prompt\n    stop_str = tokenizer.eos_token if tokenizer.eos_token else prompt_template.stop_str\n\n    def predict(message, history):\n        \"\"\"Generate answer from prompt with GPT and stream the output\"\"\"\n        history_messages = history + [[message, \"\"]]\n        prompt = prompt_template.get_prompt(messages=history_messages, system_prompt=system_prompt)\n        streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n        input_ids = tokenizer(prompt).input_ids\n        context_len = 2048\n        max_new_tokens = 512\n        max_src_len = context_len - max_new_tokens - 8\n        input_ids = input_ids[-max_src_len:]\n        generation_kwargs = dict(\n            input_ids=torch.as_tensor([input_ids]).to(device),\n            streamer=streamer,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            do_sample=True,\n            num_beams=1,\n            repetition_penalty=1.0,\n        )\n        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        partial_message = \"\"\n        for new_token in streamer:\n            if new_token != stop_str:\n                partial_message += new_token\n                yield partial_message\n\n    gr.ChatInterface(\n        predict,\n        chatbot=gr.Chatbot(),\n        textbox=gr.Textbox(placeholder=\"Ask me question\", lines=4, scale=9),\n        title=\"MedicalGPT\",\n        description=\"ä¸ºäº†ä¿ƒè¿›åŒ»ç–—è¡Œä¸šå¤§æ¨¡å‹çš„å¼€æ”¾ç ”ç©¶ï¼Œæœ¬é¡¹ç›®å¼€æºäº†[MedicalGPT](https://github.com/shibing624/MedicalGPT)åŒ»ç–—å¤§æ¨¡å‹\",\n        theme=\"soft\",\n    ).queue().launch(share=args.share, inbrowser=True, server_name='0.0.0.0', server_port=args.port)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 10.1435546875,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: \n\"\"\"\nimport argparse\nimport json\nimport os\nfrom threading import Thread\n\nimport torch\nfrom peft import PeftModel\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaForCausalLM,\n    TextIteratorStreamer,\n    GenerationConfig,\n    BitsAndBytesConfig,\n)\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@torch.inference_mode()\ndef stream_generate_answer(\n        model,\n        tokenizer,\n        prompt,\n        device,\n        do_print=True,\n        max_new_tokens=512,\n        temperature=0.7,\n        repetition_penalty=1.0,\n        context_len=2048,\n        stop_str=\"</s>\",\n):\n    \"\"\"Generate answer from prompt with GPT and stream the output\"\"\"\n    streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n    input_ids = tokenizer(prompt).input_ids\n    max_src_len = context_len - max_new_tokens - 8\n    input_ids = input_ids[-max_src_len:]\n    generation_kwargs = dict(\n        input_ids=torch.as_tensor([input_ids]).to(device),\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        do_sample=True if temperature > 0.0 else False,\n        repetition_penalty=repetition_penalty,\n        streamer=streamer,\n    )\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    generated_text = \"\"\n    for new_text in streamer:\n        stop = False\n        pos = new_text.find(stop_str)\n        if pos != -1:\n            new_text = new_text[:pos]\n            stop = True\n        generated_text += new_text\n        if do_print:\n            print(new_text, end=\"\", flush=True)\n        if stop:\n            break\n    if do_print:\n        print()\n    return generated_text\n\n\n@torch.inference_mode()\ndef batch_generate_answer(\n        sentences,\n        model,\n        tokenizer,\n        prompt_template,\n        system_prompt,\n        device,\n        max_new_tokens=512,\n        temperature=0.7,\n        repetition_penalty=1.0,\n        stop_str=\"</s>\",\n):\n    \"\"\"Generate answer from prompt with GPT, batch mode\"\"\"\n    generated_texts = []\n    generation_kwargs = dict(\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        do_sample=True if temperature > 0.0 else False,\n        repetition_penalty=repetition_penalty,\n    )\n    prompts = [prompt_template.get_prompt(messages=[[s, '']], system_prompt=system_prompt) for s in sentences]\n    inputs_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n    input_ids = inputs_tokens['input_ids'].to(device)\n    outputs = model.generate(input_ids=input_ids, **generation_kwargs)\n    for gen_sequence in outputs:\n        prompt_len = len(input_ids[0])\n        gen_sequence = gen_sequence[prompt_len:]\n        gen_text = tokenizer.decode(gen_sequence, skip_special_tokens=True)\n        pos = gen_text.find(stop_str)\n        if pos != -1:\n            gen_text = gen_text[:pos]\n        gen_text = gen_text.strip()\n        generated_texts.append(gen_text)\n\n    return generated_texts\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default='auto', type=str)\n    parser.add_argument('--base_model', default=None, type=str, required=True)\n    parser.add_argument('--lora_model', default=\"\", type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--tokenizer_path', default=None, type=str)\n    parser.add_argument('--template_name', default=\"vicuna\", type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan, chatglm2 etc.\")\n    parser.add_argument('--system_prompt', default=\"\", type=str)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument(\"--max_new_tokens\", type=int, default=512)\n    parser.add_argument('--data_file', default=None, type=str,\n                        help=\"A file that contains instructions (one instruction per line)\")\n    parser.add_argument('--interactive', action='store_true', help=\"run in the instruction mode (default multi-turn)\")\n    parser.add_argument('--single_tune', action='store_true', help='Whether to use single-tune model')\n    parser.add_argument('--temperature', type=float, default=0.7)\n    parser.add_argument('--output_file', default='./predictions_result.jsonl', type=str)\n    parser.add_argument(\"--eval_batch_size\", type=int, default=4)\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--load_in_8bit', action='store_true', help='Whether to load model in 8bit')\n    parser.add_argument('--load_in_4bit', action='store_true', help='Whether to load model in 4bit')\n    args = parser.parse_args()\n    print(args)\n    load_type = 'auto'\n    if args.tokenizer_path is None:\n        args.tokenizer_path = args.base_model\n\n    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True, padding_side='left')\n    config_kwargs = {\n        \"trust_remote_code\": True,\n        \"torch_dtype\": load_type,\n        \"low_cpu_mem_usage\": True,\n        \"device_map\": 'auto',\n    }\n    if args.load_in_8bit:\n        config_kwargs['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n    elif args.load_in_4bit:\n        config_kwargs['quantization_config'] = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=load_type,\n        )\n    base_model = model_class.from_pretrained(args.base_model, **config_kwargs)\n    try:\n        base_model.generation_config = GenerationConfig.from_pretrained(args.base_model, trust_remote_code=True)\n    except OSError:\n        print(\"Failed to load generation config, use default.\")\n    if args.resize_emb:\n        model_vocab_size = base_model.get_input_embeddings().weight.size(0)\n        tokenzier_vocab_size = len(tokenizer)\n        print(f\"Vocab of the base model: {model_vocab_size}\")\n        print(f\"Vocab of the tokenizer: {tokenzier_vocab_size}\")\n        if model_vocab_size != tokenzier_vocab_size:\n            print(\"Resize model embeddings to fit tokenizer\")\n            base_model.resize_token_embeddings(tokenzier_vocab_size)\n\n    if args.lora_model:\n        model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=load_type, device_map='auto')\n        print(\"Loaded lora model\")\n    else:\n        model = base_model\n    model.eval()\n    print(tokenizer)\n    # test data\n    if args.data_file is None:\n        examples = [\"ä»‹ç»ä¸‹åŒ—äº¬\", \"ä¹™è‚å’Œä¸™è‚çš„åŒºåˆ«ï¼Ÿ\"]\n    else:\n        with open(args.data_file, 'r') as f:\n            examples = [l.strip() for l in f.readlines()]\n        print(\"first 10 examples:\")\n        for example in examples[:10]:\n            print(example)\n\n    # Chat\n    prompt_template = get_conv_template(args.template_name)\n    system_prompt = args.system_prompt\n    stop_str = tokenizer.eos_token if tokenizer.eos_token else prompt_template.stop_str\n\n    if args.interactive:\n        print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n        history = []\n        while True:\n            try:\n                query = input(f\"{prompt_template.roles[0]}: \")\n            except UnicodeDecodeError:\n                print(\"Detected decoding error at the inputs, please try again.\")\n                continue\n            except Exception:\n                raise\n            if query == \"\":\n                print(\"Please input text, try again.\")\n                continue\n            if query.strip() == \"exit\":\n                print(\"exit...\")\n                break\n            if query.strip() == \"clear\":\n                history = []\n                print(\"history cleared.\")\n                continue\n\n            print(f\"{prompt_template.roles[1]}: \", end=\"\", flush=True)\n            if args.single_tune:\n                history = []\n\n            history.append([query, ''])\n            prompt = prompt_template.get_prompt(messages=history, system_prompt=system_prompt)\n            response = stream_generate_answer(\n                model,\n                tokenizer,\n                prompt,\n                model.device,\n                do_print=True,\n                max_new_tokens=args.max_new_tokens,\n                temperature=args.temperature,\n                repetition_penalty=args.repetition_penalty,\n                stop_str=stop_str,\n            )\n            if history:\n                history[-1][-1] = response.strip()\n    else:\n        print(\"Start inference.\")\n        counts = 0\n        if os.path.exists(args.output_file):\n            os.remove(args.output_file)\n        eval_batch_size = args.eval_batch_size\n        for batch in tqdm(\n                [\n                    examples[i: i + eval_batch_size]\n                    for i in range(0, len(examples), eval_batch_size)\n                ],\n                desc=\"Generating outputs\",\n        ):\n            responses = batch_generate_answer(\n                batch,\n                model,\n                tokenizer,\n                prompt_template,\n                system_prompt,\n                model.device,\n                max_new_tokens=args.max_new_tokens,\n                temperature=args.temperature,\n                repetition_penalty=args.repetition_penalty,\n                stop_str=stop_str,\n            )\n            results = []\n            for example, response in zip(batch, responses):\n                print(f\"===\")\n                print(f\"Input: {example}\")\n                print(f\"Output: {response}\\n\")\n                results.append({\"Input\": example, \"Output\": response})\n                counts += 1\n            with open(args.output_file, 'a', encoding='utf-8') as f:\n                for entry in results:\n                    json.dump(entry, f, ensure_ascii=False)\n                    f.write('\\n')\n        print(f'save to {args.output_file}, size: {counts}')\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "inference_multigpu_demo.py",
          "type": "blob",
          "size": 8.5146484375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: use torchrun to inference with multi-gpus\n\nusage:\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 inference_multigpu_demo.py --model_type bloom --base_model bigscience/bloom-560m\n\"\"\"\nimport argparse\nimport json\nimport os\n\nimport torch\nimport torch.distributed as dist\nfrom loguru import logger\nfrom peft import PeftModel\nfrom torch.nn import DataParallel\nfrom torch.utils.data import DataLoader, Dataset, DistributedSampler\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaForCausalLM,\n    GenerationConfig,\n    BitsAndBytesConfig,\n)\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\nclass TextDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True)\n    parser.add_argument('--base_model', default=None, type=str, required=True)\n    parser.add_argument('--lora_model', default=\"\", type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--tokenizer_path', default=None, type=str)\n    parser.add_argument('--template_name', default=\"vicuna\", type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan, chatglm2 etc.\")\n    parser.add_argument('--system_prompt', default=\"\", type=str)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument('--temperature', type=float, default=0.7)\n    parser.add_argument(\"--max_new_tokens\", type=int, default=128)\n    parser.add_argument(\"--batch_size\", type=int, default=4)\n    parser.add_argument('--data_file', default=None, type=str, help=\"Predict file, one example per line\")\n    parser.add_argument('--output_file', default='./predictions_result.jsonl', type=str)\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--load_in_8bit', action='store_true', help='Whether to load model in 8bit')\n    parser.add_argument('--load_in_4bit', action='store_true', help='Whether to load model in 4bit')\n    args = parser.parse_args()\n    logger.info(args)\n\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    logger.info(f\"local_rank: {local_rank}, world_size: {world_size}\")\n    torch.cuda.set_device(local_rank)\n    dist.init_process_group(backend='nccl')\n\n    if not torch.cuda.is_available():\n        raise ValueError(\"No GPU available, this script is only for GPU inference.\")\n    if args.tokenizer_path is None:\n        args.tokenizer_path = args.base_model\n\n    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True, padding_side='left')\n    load_type = 'auto'\n    base_model = model_class.from_pretrained(\n        args.base_model,\n        load_in_8bit=args.load_in_8bit,\n        load_in_4bit=args.load_in_4bit,\n        torch_dtype=load_type,\n        low_cpu_mem_usage=True,\n        device_map={\"\": local_rank},\n        trust_remote_code=True,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.load_in_4bit,\n            load_in_8bit=args.load_in_8bit,\n            bnb_4bit_compute_dtype=load_type,\n        ) if args.load_in_8bit or args.load_in_4bit else None,\n    )\n    try:\n        base_model.generation_config = GenerationConfig.from_pretrained(args.base_model, trust_remote_code=True)\n    except OSError:\n        logger.info(\"Failed to load generation config, use default.\")\n    if args.resize_emb:\n        model_vocab_size = base_model.get_input_embeddings().weight.size(0)\n        tokenzier_vocab_size = len(tokenizer)\n        logger.info(f\"Vocab of the base model: {model_vocab_size}\")\n        logger.info(f\"Vocab of the tokenizer: {tokenzier_vocab_size}\")\n        if model_vocab_size != tokenzier_vocab_size:\n            logger.info(\"Resize model embeddings to fit tokenizer\")\n            base_model.resize_token_embeddings(tokenzier_vocab_size)\n\n    if args.lora_model:\n        model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=load_type,\n                                          device_map={\"\": local_rank})\n        logger.info(\"Loaded lora model\")\n    else:\n        model = base_model\n    model.eval()\n    # Use multi-GPU inference\n    model = DataParallel(model)\n    model = model.module\n    logger.info(tokenizer)\n    # test data\n    if args.data_file is None:\n        examples = [\n            \"ä»‹ç»ä¸‹åŒ—äº¬\",\n            \"ä¹™è‚å’Œä¸™è‚çš„åŒºåˆ«ï¼Ÿ\",\n            \"å¤±çœ æ€ä¹ˆåŠï¼Ÿ\",\n            'ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚',\n            \"Tell me about alpacas.\",\n            \"Tell me about the president of Mexico in 2019.\",\n            \"hello.\",\n        ]\n    else:\n        with open(args.data_file, 'r', encoding='utf-8') as f:\n            examples = [l.strip() for l in f.readlines()]\n        logger.info(f\"first 10 examples: {examples[:10]}\")\n\n    prompt_template = get_conv_template(args.template_name)\n    write_batch_size = args.batch_size * world_size * 10\n    generation_kwargs = dict(\n        max_new_tokens=args.max_new_tokens,\n        temperature=args.temperature,\n        do_sample=True if args.temperature > 0.0 else False,\n        repetition_penalty=args.repetition_penalty,\n    )\n    stop_str = tokenizer.eos_token if tokenizer.eos_token else prompt_template.stop_str\n    if local_rank <= 0 and os.path.exists(args.output_file):\n        os.remove(args.output_file)\n    count = 0\n    for batch in tqdm(\n            [\n                examples[i: i + write_batch_size]\n                for i in range(0, len(examples), write_batch_size)\n            ],\n            desc=\"Generating outputs\",\n    ):\n        dataset = TextDataset(batch)\n        sampler = DistributedSampler(dataset, num_replicas=world_size, rank=local_rank, shuffle=False)\n        data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=sampler)\n\n        responses = []\n        inputs = []\n        for texts in data_loader:\n            inputs.extend(texts)\n            prompted_texts = [prompt_template.get_prompt(messages=[[s, '']], system_prompt=args.system_prompt) for s in texts]\n            logger.debug(f'local_rank: {local_rank}, inputs size:{len(prompted_texts)}, top3: {prompted_texts[:3]}')\n            inputs_tokens = tokenizer(prompted_texts, return_tensors=\"pt\", padding=True)\n            input_ids = inputs_tokens['input_ids'].to(local_rank)\n            outputs = model.generate(input_ids=input_ids, **generation_kwargs)\n            prompt_len = len(input_ids[0])\n            outputs = [i[prompt_len:] for i in outputs]\n            generated_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            logger.debug(\n                f'local_rank: {local_rank}, outputs size:{len(generated_outputs)}, top3: {generated_outputs[:3]}'\n            )\n            responses.extend(generated_outputs)\n        all_inputs = [None] * world_size\n        all_responses = [None] * world_size\n        dist.all_gather_object(all_inputs, inputs)\n        dist.all_gather_object(all_responses, responses)\n\n        # Write responses only on the main process\n        if local_rank <= 0:\n            all_inputs_flat = [inp for process_inputs in all_inputs for inp in process_inputs]\n            all_responses_flat = [response for process_responses in all_responses for response in process_responses]\n            logger.debug(f\"all_responses size:{len(all_responses_flat)}, top5: {all_responses_flat[:5]}\")\n            results = []\n            for example, response in zip(all_inputs_flat, all_responses_flat):\n                results.append({\"Input\": example, \"Output\": response})\n            with open(args.output_file, 'a', encoding='utf-8') as f:\n                for entry in results:\n                    json.dump(entry, f, ensure_ascii=False)\n                    f.write('\\n')\n                    count += 1\n\n    if local_rank <= 0:\n        logger.info(f'save to {args.output_file}, total count: {count}')\n    dist.barrier()\n    dist.destroy_process_group()\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "merge_peft_adapter.py",
          "type": "blob",
          "size": 4.201171875,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description:\n\nUsage:\npython merge_peft_adapter.py \\\n    --model_type llama \\\n    --base_model path/to/llama/model \\\n    --tokenizer_path path/to/llama/tokenizer \\\n    --lora_model path/to/lora/model \\\n    --output_dir path/to/output/dir\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    AutoModelForCausalLM,\n    LlamaForCausalLM,\n    AutoModelForSequenceClassification,\n)\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True)\n    parser.add_argument('--base_model', default=None, required=True, type=str,\n                        help=\"Base model name or path\")\n    parser.add_argument('--tokenizer_path', default=None, type=str,\n                        help=\"Please specify tokenization path.\")\n    parser.add_argument('--lora_model', default=None, required=True, type=str,\n                        help=\"Please specify LoRA model to be merged.\")\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--output_dir', default='./merged', type=str)\n    parser.add_argument('--hf_hub_model_id', default='', type=str)\n    parser.add_argument('--hf_hub_token', default=None, type=str)\n    args = parser.parse_args()\n    print(args)\n\n    base_model_path = args.base_model\n    lora_model_path = args.lora_model\n    output_dir = args.output_dir\n    print(f\"Base model: {base_model_path}\")\n    print(f\"LoRA model: {lora_model_path}\")\n    peft_config = PeftConfig.from_pretrained(lora_model_path)\n\n    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    if peft_config.task_type == \"SEQ_CLS\":\n        print(\"Loading LoRA for sequence classification model\")\n        if args.model_type == \"chatglm\":\n            raise ValueError(\"chatglm does not support sequence classification\")\n        base_model = AutoModelForSequenceClassification.from_pretrained(\n            base_model_path,\n            num_labels=1,\n            load_in_8bit=False,\n            torch_dtype=torch.float32,\n            trust_remote_code=True,\n            device_map=\"auto\",\n        )\n    else:\n        print(\"Loading LoRA for causal language model\")\n        base_model = model_class.from_pretrained(\n            base_model_path,\n            torch_dtype='auto',\n            trust_remote_code=True,\n            device_map=\"auto\",\n        )\n    if args.tokenizer_path:\n        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True)\n    else:\n        tokenizer = tokenizer_class.from_pretrained(base_model_path, trust_remote_code=True)\n    if args.resize_emb:\n        base_model_token_size = base_model.get_input_embeddings().weight.size(0)\n        if base_model_token_size != len(tokenizer):\n            base_model.resize_token_embeddings(len(tokenizer))\n            print(f\"Resize vocabulary size {base_model_token_size} to {len(tokenizer)}\")\n\n    new_model = PeftModel.from_pretrained(\n        base_model,\n        lora_model_path,\n        device_map=\"auto\",\n        torch_dtype='auto',\n    )\n    new_model.eval()\n    print(f\"Merging with merge_and_unload...\")\n    base_model = new_model.merge_and_unload()\n\n    print(\"Saving to Hugging Face format...\")\n    tokenizer.save_pretrained(output_dir)\n    base_model.save_pretrained(output_dir, max_shard_size='10GB')\n    print(f\"Done! model saved to {output_dir}\")\n    if args.hf_hub_model_id:\n        print(f\"Pushing to Hugging Face Hub...\")\n        base_model.push_to_hub(\n            args.hf_hub_model_id,\n            token=args.hf_hub_token,\n            max_shard_size=\"10GB\",\n        )\n        tokenizer.push_to_hub(\n            args.hf_hub_model_id,\n            token=args.hf_hub_token,\n        )\n        print(f\"Done! model pushed to Hugging Face Hub: {args.hf_hub_model_id}\")\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "merge_tokenizers.py",
          "type": "blob",
          "size": 6.1767578125,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: \n\"\"\"\nimport os\n\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nfrom transformers import LlamaTokenizer\nfrom sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\nimport sentencepiece as spm\nimport argparse\n\n\ndef is_chinese(uchar):\n    \"\"\"åˆ¤æ–­ä¸€ä¸ªunicodeæ˜¯å¦æ˜¯æ±‰å­—\"\"\"\n    return '\\u4e00' <= uchar <= '\\u9fa5'\n\n\ndef is_chinese_string(string):\n    \"\"\"åˆ¤æ–­æ˜¯å¦å…¨ä¸ºæ±‰å­—\"\"\"\n    return all(is_chinese(c) for c in string)\n\n\ndef load_baichuan_vocab(vocab_file):\n    words = set()\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if line.strip():\n                words.add(line.strip().split()[0])\n    return words\n\n\ndef load_jieba_vocab(jieba_vocab_file):\n    # Read jieba vocab and sort by freq\n    with open(jieba_vocab_file, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        word_freqs = [line.strip().split() for line in lines]\n        word_freqs.sort(key=lambda x: int(x[1]), reverse=True)\n    return word_freqs\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--base_tokenizer_dir', default=None, type=str, required=True)\n    parser.add_argument('--domain_sp_model_file', default='./domain_sp.model', type=str)\n    parser.add_argument('--baichuan_vocab_file', default=\"data/vocab/baichuan_vocab.txt\", type=str)\n    parser.add_argument('--add_jieba', action='store_true', help='Whether to add jieba vocab.')\n    parser.add_argument('--jieba_word_freq_file', default='data/vocab/word_freq.txt', type=str)\n    parser.add_argument('--jieba_word_size', default=20000, type=int)\n\n    args = parser.parse_args()\n    print(args)\n\n    # load\n    llama_tokenizer = LlamaTokenizer.from_pretrained(args.base_tokenizer_dir)\n    chinese_sp_model = spm.SentencePieceProcessor()\n    chinese_sp_model.Load(args.domain_sp_model_file)\n\n    llama_spm = sp_pb2_model.ModelProto()\n    llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n    chinese_spm = sp_pb2_model.ModelProto()\n    chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n\n    # print number of tokens\n    print(len(llama_tokenizer), len(chinese_sp_model))\n    print(llama_tokenizer.all_special_tokens)\n    print(llama_tokenizer.all_special_ids)\n    print(llama_tokenizer.special_tokens_map)\n\n    # Add Chinese tokens to LLaMA tokenizer\n    llama_spm_tokens_set = set(p.piece for p in llama_spm.pieces)\n\n    print(len(llama_spm_tokens_set))\n    print(f\"Before:{len(llama_spm_tokens_set)}\")\n    added_set = set()\n    for p in chinese_spm.pieces:\n        piece = p.piece\n        if piece not in llama_spm_tokens_set:\n            # print('picec', piece)\n            new_p = sp_pb2_model.ModelProto().SentencePiece()\n            new_p.piece = piece\n            new_p.score = 0\n            llama_spm.pieces.append(new_p)\n            added_set.add(piece)\n    print(f\"[add domain tokens]New model pieces: {len(llama_spm.pieces)}\")\n\n    vocab = load_baichuan_vocab(args.baichuan_vocab_file)\n    print('baichuan vocab len:', len(vocab))\n    baichuan_vocab_set = set([i for i in vocab if is_chinese_string(i)])\n    print('baichuan chinese vocab size:', len(baichuan_vocab_set))\n    print('baichuan vocab head:', list(baichuan_vocab_set)[:10])\n    for p in baichuan_vocab_set:\n        piece = p\n        if piece not in llama_spm_tokens_set and piece not in added_set:\n            # print('baichuan picec', piece)\n            new_p = sp_pb2_model.ModelProto().SentencePiece()\n            new_p.piece = piece\n            new_p.score = 0\n            llama_spm.pieces.append(new_p)\n            added_set.add(piece)\n    print(f\"[add baichuan tokens]New model pieces: {len(llama_spm.pieces)}\")\n\n    if args.add_jieba:\n        word_freqs = load_jieba_vocab(args.jieba_word_freq_file)\n        top_words = word_freqs[:args.jieba_word_size]\n        print('jieba top10 freq words:', top_words[:10])\n        jieba_vocab_set = set([i[0] for i in top_words if i])\n        print('jieba_vocab_set size:', len(jieba_vocab_set))\n        print('jieba_vocab head:', list(jieba_vocab_set)[:3])\n        for p in jieba_vocab_set:\n            piece = p\n            if piece not in llama_spm_tokens_set and piece not in added_set:\n                # print('jieba picec', piece)\n                new_p = sp_pb2_model.ModelProto().SentencePiece()\n                new_p.piece = piece\n                new_p.score = 0\n                llama_spm.pieces.append(new_p)\n        print(f\"[add jieba tokens]New model pieces: {len(llama_spm.pieces)}\")\n\n    # Save\n    output_sp_dir = 'merged_tokenizer_sp'\n    output_hf_dir = 'merged_tokenizer_hf'  # the path to save Chinese-LLaMA tokenizer\n    os.makedirs(output_sp_dir, exist_ok=True)\n    with open(output_sp_dir + '/chinese_llama.model', 'wb') as f:\n        f.write(llama_spm.SerializeToString())\n    tokenizer = LlamaTokenizer(vocab_file=output_sp_dir + '/chinese_llama.model')\n\n    tokenizer.save_pretrained(output_hf_dir)\n    print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")\n\n    # Test\n    llama_tokenizer = LlamaTokenizer.from_pretrained(args.base_tokenizer_dir)\n    chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n    print(chinese_llama_tokenizer.all_special_tokens)\n    print(chinese_llama_tokenizer.all_special_ids)\n    print(chinese_llama_tokenizer.special_tokens_map)\n    print('old len:', len(llama_tokenizer), ' new len:', len(chinese_llama_tokenizer))\n    text = '''this is a test, hello world. thisisatesthelloworld, \næ…•å®¹å¤æ¥åˆ°æ²³è¾¹ï¼Œå§‘è‹æ…•å®¹æ°åœ¨å¤–é¢ä¸¢äº†äººã€‚\n1å·åº—ä¸€å‘¨å²äº†ï¼Œæˆ‘ä»¬ä¸€å¤è„‘å„¿ä¹°äº†10æ–¤é›¶é£Ÿã€‚\nå·´å¡ç½—é‚£è¶³çƒä¿±ä¹éƒ¨ç®€ç§°å·´è¨ï¼ˆBarÃ§aï¼‰ï¼Œæ˜¯ä¸€å®¶ä½äºè¥¿ç­ç‰™åŠ æ³°ç½—å°¼äºšå·´å¡ç½—é‚£çš„è¶³çƒä¿±ä¹éƒ¨ï¼Œäº1899å¹´ç”±ç‘å£«ä¼ä¸šå®¶èƒ¡å®‰Â·ç”˜ä¼¯æ‰€åˆ›ç«‹ï¼Œä¸–ç•Œçƒå›é¡¶çº§è¶³çƒä¿±ä¹éƒ¨ä¹‹ä¸€ã€‚ä¿±ä¹éƒ¨ä¸»åœºå¯å®¹çº³æ¥è¿‘åä¸‡åè§‚ä¼—ï¼Œæ˜¯å…¨æ¬§æ´²æœ€å¤§åŠä¸–ç•Œç¬¬äºŒå¤§çš„è¶³çƒåœºã€‚\nç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚æ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚'''\n    print(\"Test text:\\n\", text)\n    print(f\"Tokenized by LLaMA tokenizer:{llama_tokenizer.tokenize(text)}\")\n    print(f\"Tokenized by Chinese-LLaMA tokenizer:{chinese_llama_tokenizer.tokenize(text)}\")\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "model_quant.py",
          "type": "blob",
          "size": 5.0380859375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:ZhuangXialie(1832963123@qq.com)\n@description: model quantify\n\nusage:\npython model_quant.py --unquantized_model_path /path/to/unquantized/model --quantized_model_output_path /path/to/save/quantized/model --input_text \"Your input text here\"\n\"\"\"\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport time\nimport argparse\n\n# å®šä¹‰å‡½æ•°æ¥è§£æå‘½ä»¤è¡Œå‚æ•°\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"é‡åŒ–æ¨¡å‹æ¨ç†å¯¹æ¯”\")\n    parser.add_argument(\"--unquantized_model_path\", type=str, required=True, help=\"æœªé‡åŒ–æ¨¡å‹è·¯å¾„\")\n    parser.add_argument(\"--quantized_model_output_path\", type=str, required=True, help=\"é‡åŒ–æ¨¡å‹ä¿å­˜è·¯å¾„\")\n    parser.add_argument(\"--input_text\", type=str, required=True, help=\"è¾“å…¥çš„æ–‡æœ¬å†…å®¹\")\n    return parser.parse_args()\n\n# è®¡ç®—æ¨¡å‹ç›¸å…³çš„æ˜¾å­˜å ç”¨\ndef get_model_memory_usage(device):\n    return torch.cuda.memory_allocated(device) / (1024 ** 3)  # è½¬æ¢ä¸ºGB\n\n# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¿›è¡Œæ¨ç†ï¼Œå¹¶è®¡ç®—æ¨ç†æ—¶é—´\ndef perform_inference(model, tokenizer, devic, question):\n    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    attention_mask = inputs[\"attention_mask\"]\n    \n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs.input_ids,\n            attention_mask=attention_mask,\n            max_length=512,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.1,\n            pad_token_id=tokenizer.eos_token_id  # è®¾ç½® pad_token_id ä¸º eos_token_id\n        )\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text, elapsed_time\n\ndef main():\n    args = parse_args()\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # 1. æœªé‡åŒ–æ¨¡å‹æ¨ç†å’Œæ˜¾å­˜è®¡ç®—\n    print(\"\\n====== æœªé‡åŒ–æ¨¡å‹æ¨ç† ======\")\n    tokenizer = AutoTokenizer.from_pretrained(args.unquantized_model_path, trust_remote_code=True)\n\n    gpu_memory_before_unquantized = get_model_memory_usage(device)  # æ¨¡å‹åŠ è½½å‰çš„æ˜¾å­˜\n    unquantized_model = AutoModelForCausalLM.from_pretrained(args.unquantized_model_path, trust_remote_code=True)\n    unquantized_model.to(device)\n    gpu_memory_after_unquantized = get_model_memory_usage(device)  # æ¨¡å‹åŠ è½½åçš„æ˜¾å­˜\n    model_memory_unquantized = gpu_memory_after_unquantized - gpu_memory_before_unquantized  # è®¡ç®—æ¨¡å‹æ˜¾å­˜å ç”¨\n    print(f\"æœªé‡åŒ–æ¨¡å‹åŠ è½½æ˜¾å­˜å ç”¨: {model_memory_unquantized:.2f} GB\")\n\n    generated_text_unquantized, time_unquantized = perform_inference(unquantized_model, tokenizer, device, args.input_text)\n    print(f\"æ¨ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆæœªé‡åŒ–æ¨¡å‹ï¼‰: {generated_text_unquantized}\")\n    print(f\"æ¨ç†æ—¶é—´ï¼ˆæœªé‡åŒ–æ¨¡å‹ï¼‰: {time_unquantized:.2f} ç§’\")\n\n    # å¸è½½æœªé‡åŒ–æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜\n    del unquantized_model\n    torch.cuda.empty_cache()\n    \n    # é‡æ–°è®¡ç®—æ˜¾å­˜åŸºçº¿\n    print(\"\\næ¸…ç†ç¼“å­˜ï¼Œé‡æ–°è®¡ç®—æ˜¾å­˜...\")\n    time.sleep(2)  # ç¡®ä¿æ˜¾å­˜é‡Šæ”¾ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´\n    gpu_memory_after_cache_clear = get_model_memory_usage(device)\n    print(f\"æ˜¾å­˜æ¸…ç†ååŸºçº¿æ˜¾å­˜: {gpu_memory_after_cache_clear:.2f} GB\")\n\n    # 2. é‡åŒ–æ¨¡å‹æ¨ç†å’Œä¿å­˜\n    print(\"\\n====== é‡åŒ–æ¨¡å‹æ¨ç†å’Œä¿å­˜ ======\")\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,               # å¼€å¯4bité‡åŒ–\n        load_in_8bit=False,              # ç¦æ­¢8bité‡åŒ–\n        bnb_4bit_compute_dtype=torch.float16,   # è®¡ç®—æ•°æ®ç±»å‹ä¸ºfloat16\n        bnb_4bit_quant_storage=torch.uint8,     # å­˜å‚¨æ•°æ®ç±»å‹ä¸ºuint8\n        bnb_4bit_quant_type=\"nf4\",              # ä½¿ç”¨nf4é‡åŒ–ç±»å‹\n        bnb_4bit_use_double_quant=True          # å¼€å¯åŒé‡é‡åŒ–ä»¥ä¼˜åŒ–æ¨ç†\n    )\n\n    quantized_model = AutoModelForCausalLM.from_pretrained(\n        args.unquantized_model_path,\n        device_map=\"auto\",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡\n        quantization_config=quantization_config,\n        trust_remote_code=True\n    )\n\n    generated_text_quantized, time_quantized = perform_inference(quantized_model, tokenizer, device, args.input_text)\n    print(f\"æ¨ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆé‡åŒ–æ¨¡å‹ï¼‰: {generated_text_quantized}\")\n    print(f\"æ¨ç†æ—¶é—´ï¼ˆé‡åŒ–æ¨¡å‹ï¼‰: {time_quantized:.2f} ç§’\")\n\n    # ä¿å­˜é‡åŒ–æ¨¡å‹å’Œtokenizer\n    quantized_model.save_pretrained(args.quantized_model_output_path)\n    tokenizer.save_pretrained(args.quantized_model_output_path)\n    print(f\"é‡åŒ–æ¨¡å‹å’Œtokenizerå·²ä¿å­˜åˆ° {args.quantized_model_output_path}\")\n\n    # è¾“å‡ºå¯¹æ¯”\n    print(\"\\n====== å†…å®¹å¯¹æ¯”ç»“æœ ======\")\n    print(f\"æœªé‡åŒ–æ¨¡å‹ç”Ÿæˆæ–‡æœ¬:\\n {generated_text_unquantized}\")\n    print(f\"é‡åŒ–æ¨¡å‹ç”Ÿæˆæ–‡æœ¬:\\n {generated_text_quantized}\")\n    \n    print(\"\\n====== æ—¶é—´å¯¹æ¯”ç»“æœ ======\")\n    print(f\"æœªé‡åŒ–æ¨¡å‹æ¨ç†æ—¶é—´: {time_unquantized:.2f} ç§’\")\n    print(f\"é‡åŒ–æ¨¡å‹æ¨ç†æ—¶é—´: {time_quantized:.2f} ç§’\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "openai_api.py",
          "type": "blob",
          "size": 20.705078125,
          "content": "# Requirement:\n#   pip install openai\n# Usage:\n#   python openai_api.py\n# Visit http://localhost:8000/docs for documents.\n\nimport base64\nimport copy\nimport json\nimport time\nfrom argparse import ArgumentParser\nfrom contextlib import asynccontextmanager\nfrom threading import Thread\nfrom typing import Dict, List, Literal, Optional, Union, Any\n\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig, TextIteratorStreamer\n\nfrom template import get_conv_template\n\n\nclass BasicAuthMiddleware(BaseHTTPMiddleware):\n\n    def __init__(self, app, username: str, password: str):\n        super().__init__(app)\n        self.required_credentials = base64.b64encode(\n            f'{username}:{password}'.encode()).decode()\n\n    async def dispatch(self, request: Request, call_next):\n        authorization: str = request.headers.get('Authorization')\n        if authorization:\n            try:\n                schema, credentials = authorization.split()\n                if credentials == self.required_credentials:\n                    return await call_next(request)\n            except ValueError:\n                pass\n\n        headers = {'WWW-Authenticate': 'Basic'}\n        return Response(status_code=401, headers=headers)\n\n\ndef _gc(forced: bool = False):\n    global args\n    if args.disable_gc and not forced:\n        return\n\n    import gc\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):  # collects GPU memory\n    yield\n    _gc(forced=True)\n\n\napp = FastAPI(lifespan=lifespan)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'],\n    allow_headers=['*'],\n)\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = 'model'\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = 'owner'\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None\n\n\nclass ModelList(BaseModel):\n    object: str = 'list'\n    data: List[ModelCard] = []\n\n\nclass ChatMessage(BaseModel):\n    role: Literal['user', 'assistant', 'system', 'function', 'tool']\n    content: Optional[str] = None\n    tool_calls: Optional[Dict] = None\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal['user', 'assistant', 'system']] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessage]\n    tools: Optional[List[Dict]] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    top_k: Optional[int] = None\n    max_length: Optional[int] = None\n    stream: Optional[bool] = False\n    stop: Optional[List[str]] = None\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: Union[ChatMessage]\n    finish_reason: Literal['stop', 'length', 'tool_calls']\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal['stop', 'length']] = None\n\n\nclass ChatCompletionResponseUsage(BaseModel):\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: Literal[\"chatcmpl-default\"] = \"chatcmpl-default\"\n    object: Literal[\"chat.completion\"] = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseChoice]\n    usage: ChatCompletionResponseUsage\n\n\nclass ChatCompletionStreamResponse(BaseModel):\n    id: Literal[\"chatcmpl-default\"] = \"chatcmpl-default\"\n    object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseStreamChoice]\n\n\n@app.get('/v1/models', response_model=ModelList)\nasync def list_models():\n    global model_args\n    model_card = ModelCard(id='gpt-3.5-turbo')\n    return ModelList(data=[model_card])\n\n\n# To work around that unpleasant leading-\\n tokenization issue!\ndef add_extra_stop_words(stop_words):\n    _stop_words = []\n    if stop_words:\n        _stop_words.extend(stop_words)\n        for x in stop_words:\n            s = x.lstrip('\\n')\n            if s and (s not in _stop_words):\n                _stop_words.append(s)\n    return _stop_words\n\n\ndef trim_stop_words(response, stop_words):\n    if stop_words:\n        for stop in stop_words:\n            idx = response.find(stop)\n            if idx != -1:\n                response = response[:idx]\n    return response\n\n\nTOOL_DESC = (\n    '{name_for_model}: Call this tool to interact with the {name_for_human} API.'\n    ' What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}'\n)\n\nREACT_INSTRUCTION = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n\n{tools_text}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tools_name_text}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\"\"\"\n\n_TEXT_COMPLETION_CMD = object()\n\n\ndef parse_messages(messages, tools):\n    if all(m.role != 'user' for m in messages):\n        raise HTTPException(\n            status_code=400,\n            detail='Invalid request: Expecting at least one user message.',\n        )\n\n    messages = copy.deepcopy(messages)\n    if messages[0].role == 'system':\n        system = messages.pop(0).content.lstrip('\\n').rstrip()\n    else:\n        system = ''\n\n    if tools:\n        tools_text = []\n        tools_name_text = []\n        for tool_info in tools:\n            name = tool_info.get('name', '')\n            name_m = tool_info.get('name_for_model', name)\n            name_h = tool_info.get('name_for_human', name)\n            desc = tool_info.get('description', '')\n            desc_m = tool_info.get('description_for_model', desc)\n            params = tool_info.get('parameters', {})\n            tool = TOOL_DESC.format(\n                name_for_model=name_m,\n                name_for_human=name_h,\n                # Hint: You can add the following format requirements in description:\n                #   \"Format the arguments as a JSON object.\"\n                #   \"Enclose the code within triple backticks (`) at the beginning and end of the code.\"\n                description_for_model=desc_m,\n                parameters=json.dumps(params, ensure_ascii=False),\n            )\n            tools_text.append(tool)\n            tools_name_text.append(name_m)\n        tools_text = '\\n\\n'.join(tools_text)\n        tools_name_text = ', '.join(tools_name_text)\n        instruction = (REACT_INSTRUCTION.format(\n            tools_text=tools_text,\n            tools_name_text=tools_name_text,\n        ).lstrip('\\n').rstrip())\n    else:\n        instruction = ''\n\n    messages_with_fncall = messages\n    messages = []\n    for m_idx, m in enumerate(messages_with_fncall):\n        role, content, tool_calls = m.role, m.content, m.tool_calls\n        content = content or ''\n        content = content.lstrip('\\n').rstrip()\n        if role == 'function':\n            if (len(messages) == 0) or (messages[-1].role != 'assistant'):\n                raise HTTPException(\n                    status_code=400,\n                    detail='Invalid request: Expecting role assistant before role function.',\n                )\n            messages[-1].content += f'\\nObservation: {content}'\n            if m_idx == len(messages_with_fncall) - 1:\n                # add a prefix for text completion\n                messages[-1].content += '\\nThought:'\n        elif role == 'assistant':\n            if len(messages) == 0:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\n                    'Invalid request: Expecting role user before role assistant.',\n                )\n            if tool_calls is None:\n                if tools:\n                    content = f'Thought: I now know the final answer.\\nFinal Answer: {content}'\n            else:\n                f_name, f_args = tool_calls['name'], tool_calls['arguments']\n                if not content.startswith('Thought:'):\n                    content = f'Thought: {content}'\n                content = f'{content}\\nAction: {f_name}\\nAction Input: {f_args}'\n            if messages[-1].role == 'user':\n                messages.append(\n                    ChatMessage(role='assistant',\n                                content=content.lstrip('\\n').rstrip()))\n            else:\n                messages[-1].content += '\\n' + content\n        elif role == 'user':\n            messages.append(\n                ChatMessage(role='user',\n                            content=content.lstrip('\\n').rstrip()))\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=f'Invalid request: Incorrect role {role}.')\n\n    query = _TEXT_COMPLETION_CMD\n    if messages[-1].role == 'user':\n        query = messages[-1].content\n        messages = messages[:-1]\n\n    if len(messages) % 2 != 0:\n        raise HTTPException(status_code=400, detail='Invalid request')\n\n    history = []  # [(Q1, A1), (Q2, A2), ..., (Q_last_turn, A_last_turn)]\n    for i in range(0, len(messages), 2):\n        if messages[i].role == 'user' and messages[i + 1].role == 'assistant':\n            usr_msg = messages[i].content.lstrip('\\n').rstrip()\n            bot_msg = messages[i + 1].content.lstrip('\\n').rstrip()\n            if instruction and (i == len(messages) - 2):\n                usr_msg = f'{instruction}\\n\\nQuestion: {usr_msg}'\n                instruction = ''\n            history.append([usr_msg, bot_msg])\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail='Invalid request: Expecting exactly one user (or function) role before every assistant role.',\n            )\n    if instruction:\n        assert query is not _TEXT_COMPLETION_CMD\n        query = f'{instruction}\\n\\nQuestion: {query}'\n    return query, history, system\n\n\ndef parse_response(response):\n    func_name, func_args = '', ''\n    i = response.find('\\nAction:')\n    j = response.find('\\nAction Input:')\n    k = response.find('\\nObservation:')\n    if 0 <= i < j:  # If the text has `Action` and `Action input`,\n        if k < j:  # but does not contain `Observation`,\n            # then it is likely that `Observation` is omitted by the LLM,\n            # because the output text may have discarded the stop word.\n            response = response.rstrip() + '\\nObservation:'  # Add it back.\n        k = response.find('\\nObservation:')\n        func_name = response[i + len('\\nAction:'):j].strip()\n        func_args = response[j + len('\\nAction Input:'):k].strip()\n\n    if func_name:\n        response = response[:i]\n        t = response.find('Thought: ')\n        if t >= 0:\n            response = response[t + len('Thought: '):]\n        response = response.strip()\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(\n                role='assistant',\n                content=response,\n                tool_calls={\n                    'name': func_name,\n                    'arguments': func_args\n                },\n            ),\n            finish_reason='tool_calls',\n        )\n        return choice_data\n\n    z = response.rfind('\\nFinal Answer: ')\n    if z >= 0:\n        response = response[z + len('\\nFinal Answer: '):]\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=ChatMessage(role='assistant', content=response),\n        finish_reason='stop',\n    )\n    return choice_data\n\n\ndef prepare_chat(tokenizer, query, history, system):\n    \"\"\"Prepare model inputs for chat completion.\"\"\"\n    if prompt_template:\n        history_messages = history + [[query, \"\"]]\n        prompt = prompt_template.get_prompt(messages=history_messages, system_prompt=system)\n    else:\n        messages = [\n            {\"role\": \"system\", \"content\": system}\n        ]\n        for i, (question, response) in enumerate(history):\n            question = question.lstrip('\\n').rstrip()\n            response = response.lstrip('\\n').rstrip()\n            messages.append({\"role\": \"user\", \"content\": question})\n            messages.append({\"role\": \"assistant\", \"content\": response})\n        messages.append({\"role\": \"user\", \"content\": query})\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n    model_inputs = tokenizer([prompt], return_tensors='pt')\n    return model_inputs\n\n\ndef model_chat(model, tokenizer, query, history, gen_kwargs, system):\n    \"\"\"Generate chat completion from the model.\"\"\"\n    model_inputs = prepare_chat(tokenizer, query, history, system).to(model.device)\n    generated_ids = model.generate(model_inputs.input_ids, **gen_kwargs)\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    prompt_length = len(model_inputs.input_ids[0])\n    response_length = len(generated_ids[0])\n    return response, prompt_length, response_length\n\n\ndef stream_model_chat(model, tokenizer, query, history, gen_kwargs, system):\n    \"\"\"Generate chat completion from the model in stream mode.\"\"\"\n    model_inputs = prepare_chat(tokenizer, query, history, system).to(model.device)\n    gen_kwargs['inputs'] = model_inputs.input_ids\n\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n    gen_kwargs['streamer'] = streamer\n    thread = Thread(target=model.generate, kwargs=gen_kwargs, daemon=True)\n    thread.start()\n\n    yield from streamer\n\n\n@app.post('/v1/chat/completions', response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    \"\"\"Generate chat completion.\"\"\"\n    global model, tokenizer\n\n    gen_kwargs = {}\n    if request.top_k is not None:\n        gen_kwargs['top_k'] = request.top_k\n    if request.temperature is not None:\n        if request.temperature < 0.01:\n            gen_kwargs['top_k'] = 1  # greedy decoding\n        else:\n            # Not recommended. Please tune top_p instead.\n            gen_kwargs['temperature'] = request.temperature\n    if request.top_p is not None:\n        gen_kwargs['top_p'] = request.top_p\n    if request.max_length is not None:\n        gen_kwargs['max_length'] = request.max_length\n\n    stop_words = add_extra_stop_words(request.stop)\n    if request.tools:\n        stop_words = stop_words or []\n        if 'Observation:' not in stop_words:\n            stop_words.append('Observation:')\n\n    query, history, system = parse_messages(request.messages, request.tools)\n\n    if request.stream:\n        if request.tools:\n            raise HTTPException(\n                status_code=400,\n                detail='Invalid request: Function calling is not yet implemented for stream mode.',\n            )\n        generate = stream_chat_completion(\n            query,\n            history,\n            request.model,\n            stop_words,\n            gen_kwargs,\n            system=system\n        )\n        return StreamingResponse(generate, media_type='text/event-stream')\n\n    response, prompt_length, response_length = model_chat(\n        model,\n        tokenizer,\n        query,\n        history,\n        gen_kwargs=gen_kwargs,\n        system=system\n    )\n    logger.debug(f'*** history begin ***\\n{history}\\n*** history end ***\\n'\n                 f'question: {query}\\nresponse: {response}\\n')\n    _gc()\n\n    response = trim_stop_words(response, stop_words)\n    if request.tools:\n        choice_data = parse_response(response)\n    else:\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(role='assistant', content=response),\n            finish_reason='stop',\n        )\n\n    usage = ChatCompletionResponseUsage(\n        prompt_tokens=prompt_length,\n        completion_tokens=response_length,\n        total_tokens=prompt_length + response_length,\n    )\n    return ChatCompletionResponse(model=request.model, choices=[choice_data], usage=usage)\n\n\ndef dictify(data: BaseModel) -> Dict[str, Any]:\n    try:  # pydantic v2\n        return data.model_dump(exclude_unset=True)\n    except AttributeError:  # pydantic v1\n        return data.dict(exclude_unset=True)\n\n\ndef jsonify(data: BaseModel) -> str:\n    try:  # pydantic v2\n        return json.dumps(data.model_dump(exclude_unset=True), ensure_ascii=False)\n    except AttributeError:  # pydantic v1\n        return data.json(exclude_unset=True, ensure_ascii=False)\n\n\nasync def stream_chat_completion(\n        query: str,\n        history: List[List[str]],\n        model_id: str,\n        stop_words: List[str],\n        gen_kwargs: Dict,\n        system: str,\n):\n    \"\"\"Generate chat completion in stream mode.\"\"\"\n    global model, tokenizer\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(role='assistant', content=\"\"), finish_reason=None)\n    chunk = ChatCompletionStreamResponse(model=model_id, choices=[choice_data])\n    yield jsonify(chunk)\n\n    stop_words = [x for x in stop_words if x]\n    response_generator = stream_model_chat(\n        model,\n        tokenizer,\n        query,\n        history,\n        gen_kwargs,\n        system\n    )\n    for token_output in response_generator:\n        # Check if any stop word is in the token output\n        if any(stop_word in token_output for stop_word in stop_words):\n            break\n\n        # Send the current token as part of the response\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0, delta=DeltaMessage(content=token_output), finish_reason=None)\n        chunk = ChatCompletionStreamResponse(model=model_id, choices=[choice_data])\n        yield jsonify(chunk)\n\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(), finish_reason='stop'\n    )\n    chunk = ChatCompletionStreamResponse(model=model_id, choices=[choice_data])\n    yield jsonify(chunk)\n    yield '[DONE]'\n\n    _gc()\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument('--base_model', type=str, default='Qwen/Qwen-7B-Chat', help='Model name or path')\n    parser.add_argument('--lora_model', default=None, type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--template_name', default=None, type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan, chatglm2 etc.\")\n    parser.add_argument('--api_auth', help='API authentication credentials')\n    parser.add_argument('--cpu_only', action='store_true', help='Run demo with CPU only')\n    parser.add_argument('--server_port', type=int, default=8000, help='Demo server port.')\n    parser.add_argument('--server_name', type=str, default='127.0.0.1',\n                        help=('Demo server name. Default: 127.0.0.1, which is only visible from the local computer. '\n                              'If you want other computers to access your server, use 0.0.0.0 instead.')\n                        )\n    parser.add_argument('--disable_gc', action='store_true', help='Disable GC after each response generated.')\n\n    args = parser.parse_args()\n    logger.info(args)\n\n    if args.api_auth:\n        app.add_middleware(\n            BasicAuthMiddleware,\n            username=args.api_auth.split(':')[0],\n            password=args.api_auth.split(':')[1]\n        )\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.base_model,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = 'cpu'\n    else:\n        device_map = 'auto'\n    model = AutoModelForCausalLM.from_pretrained(\n        args.base_model,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n    if args.lora_model:\n        from peft import PeftModel\n\n        model = PeftModel.from_pretrained(model, args.lora_model, device_map=device_map)\n        logger.debug(f'Loaded LORA model: {args.lora_model}')\n\n    model = model.eval()\n    model.generation_config = GenerationConfig.from_pretrained(\n        args.base_model,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n    if args.template_name:\n        prompt_template = get_conv_template(args.template_name)\n    else:\n        prompt_template = None\n\n    uvicorn.run(app, host=args.server_name, port=args.server_port, workers=1)\n"
        },
        {
          "name": "orpo_training.py",
          "type": "blob",
          "size": 22.4306640625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: Train a model from base model using ORPO\n\"\"\"\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import Dict, Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType\nfrom transformers import (\n    AutoConfig,\n    BloomForCausalLM,\n    AutoModelForCausalLM,\n    AutoModel,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n    BitsAndBytesConfig,\n)\nfrom transformers.deepspeed import is_deepspeed_zero3_enabled\nfrom trl import ORPOConfig, ORPOTrainer\n\nfrom template import get_conv_template\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with DPO\n    \"\"\"\n    # Model arguments\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The model checkpoint for weights initialization.\"}\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The tokenizer for weights initialization.\"}\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n    # Dataset arguments\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The input jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The prompt template name.\"})\n    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"Train batch size per device\"})\n    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"Eval batch size per device\"})\n    max_source_length: Optional[int] = field(default=2048, metadata={\"help\": \"Max length of prompt input text\"})\n    max_target_length: Optional[int] = field(default=512, metadata={\"help\": \"Max length of output text\"})\n    min_target_length: Optional[int] = field(default=4, metadata={\"help\": \"Min length of output text\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=4, metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    # Training arguments\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})\n    target_modules: Optional[str] = field(default=\"all\", metadata={\"help\": \"The target modules for peft\"})\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=16.0)\n    peft_path: Optional[str] = field(default=None)\n    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the validation set.\"})\n    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"The beta parameter for DPO loss\"})\n    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"Learning rate\"})\n    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"The lr scheduler type\"})\n    warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"The number of warmup steps\"})\n    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"The weight decay\"})\n    optim: Optional[str] = field(default=\"adamw_hf\", metadata={\"help\": \"The optimizer type\"})\n    fp16: Optional[bool] = field(default=True, metadata={\"help\": \"Whether to use fp16\"})\n    bf16: Optional[bool] = field(default=False, metadata={\"help\": \"Whether to use bf16\"})\n    gradient_checkpointing: Optional[bool] = field(\n        default=True, metadata={\"help\": \"Whether to use gradient checkpointing\"}\n    )\n    gradient_accumulation_steps: Optional[int] = field(\n        default=4, metadata={\"help\": \"The number of gradient accumulation steps\"}\n    )\n    save_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to save the model\"})\n    eval_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to evaluate the model\"})\n    logging_steps: Optional[int] = field(default=1, metadata={\"help\": \"X steps to log the model\"})\n    output_dir: Optional[str] = field(default=\"outputs-dpo\", metadata={\"help\": \"The output directory\"})\n    max_steps: Optional[int] = field(default=200, metadata={\"help\": \"Number of steps to train\"})\n    eval_strategy: Optional[str] = field(default=\"steps\", metadata={\"help\": \"Evaluation strategy\"})\n    remove_unused_columns: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Remove unused columns from the dataset if `datasets.Dataset` is used\"},\n    )\n    report_to: Optional[str] = field(default=\"tensorboard\", metadata={\"help\": \"Report to wandb or tensorboard\"})\n    orpo_beta: float = field(\n        default=0.1,\n        metadata={\"help\": \"The beta (lambda) parameter in ORPO loss representing the weight of the SFT loss.\"},\n    )\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\"You must specify a valid model_type to run training.\")\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'output_layer' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\ndef main():\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    logger.info(f\"Parse args: {args}\")\n\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    if args.model_type == 'bloom':\n        args.use_fast_tokenizer = True\n    # Load tokenizer\n    tokenizer_kwargs = {\n        \"cache_dir\": args.cache_dir,\n        \"use_fast\": args.use_fast_tokenizer,\n        \"trust_remote_code\": args.trust_remote_code,\n    }\n    tokenizer_name_or_path = args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    # Get datasets\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name,\n            args.dataset_config_name,\n            cache_dir=args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    else:\n        data_files = {}\n        if args.train_file_dir is not None and os.path.exists(args.train_file_dir):\n            train_data_files = glob(f'{args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {', '.join(train_data_files)}\")\n            data_files[\"train\"] = train_data_files\n        if args.validation_file_dir is not None and os.path.exists(args.validation_file_dir):\n            eval_data_files = glob(f'{args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    max_source_length = args.max_source_length\n    max_target_length = args.max_target_length\n    full_max_length = max_source_length + max_target_length\n\n    def return_prompt_and_responses(examples) -> Dict[str, str]:\n        \"\"\"Load the paired dataset and convert it to the necessary format.\n\n        The dataset is converted to a dictionary with the following structure:\n        {\n            'prompt': List[str],\n            'chosen': List[str],\n            'rejected': List[str],\n        }\n\n        Prompts are structured as follows:\n          system_prompt + history[[q,a], [q,a]...] + question\n        \"\"\"\n        prompts = []\n        for system, history, question in zip(examples[\"system\"], examples[\"history\"], examples[\"question\"]):\n            system_prompt = system or \"\"\n            history_with_question = history + [[question, '']] if history else [[question, '']]\n            prompts.append(prompt_template.get_prompt(messages=history_with_question, system_prompt=system_prompt))\n        return {\n            \"prompt\": prompts,\n            \"chosen\": examples[\"response_chosen\"],\n            \"rejected\": examples[\"response_rejected\"],\n        }\n\n    # Preprocess the dataset\n    train_dataset = None\n    max_train_samples = 0\n    if args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train']\n        max_train_samples = len(train_dataset)\n        if args.max_train_samples is not None and args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        tokenized_dataset = train_dataset.shuffle().map(\n            return_prompt_and_responses,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=train_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        train_dataset = tokenized_dataset.filter(\n            lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n                      and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n        )\n        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n        logger.debug(\"First train example:\")\n        first_example = train_dataset[0]\n        logger.debug(f\"prompt:\\n{first_example['prompt']}\")\n        logger.debug(f\"chosen:\\n{first_example['chosen']}\")\n        logger.debug(f\"rejected:\\n{first_example['rejected']}\")\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if args.do_eval:\n        if \"validation\" not in raw_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = raw_datasets[\"validation\"]\n        max_eval_samples = len(eval_dataset)\n        if args.max_eval_samples is not None and args.max_eval_samples > 0:\n            max_eval_samples = min(len(eval_dataset), args.max_eval_samples)\n            eval_dataset = eval_dataset.select(range(max_eval_samples))\n        logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n        eval_dataset = eval_dataset.map(\n            return_prompt_and_responses,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=eval_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        eval_dataset = eval_dataset.filter(\n            lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n                      and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n        )\n        logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n        logger.debug(\"First eval example:\")\n        first_example = eval_dataset[0]\n        logger.debug(f\"prompt:\\n{first_example['prompt']}\")\n        logger.debug(f\"chosen:\\n{first_example['chosen']}\")\n        logger.debug(f\"rejected:\\n{first_example['rejected']}\")\n\n    # Load model\n    torch_dtype = (\n        args.torch_dtype\n        if args.torch_dtype in [\"auto\", None]\n        else getattr(torch, args.torch_dtype)\n    )\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    ddp = world_size != 1\n    if ddp:\n        args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n    logger.info(f\"Device map: {args.device_map}\")\n    if args.qlora and is_deepspeed_zero3_enabled():\n        logger.warning(\"ZeRO3 are both currently incompatible with QLoRA.\")\n    config = config_class.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=args.trust_remote_code,\n        torch_dtype=torch_dtype,\n        cache_dir=args.cache_dir\n    )\n    if args.load_in_4bit or args.load_in_8bit:\n        logger.info(f\"Quantizing model, load_in_4bit: {args.load_in_4bit}, load_in_8bit: {args.load_in_8bit}\")\n    model = model_class.from_pretrained(\n        args.model_name_or_path,\n        config=config,\n        torch_dtype=torch_dtype,\n        low_cpu_mem_usage=(not is_deepspeed_zero3_enabled()),\n        device_map=args.device_map,\n        trust_remote_code=args.trust_remote_code,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.load_in_4bit,\n            load_in_8bit=args.load_in_8bit,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch_dtype,\n        ) if args.qlora else None,\n    )\n    # fixed FP16 ValueError\n    for param in filter(lambda p: p.requires_grad, model.parameters()):\n        param.data = param.data.to(torch.float32)\n\n    # Initialize our Trainer\n    if args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n    else:\n        model.config.use_cache = True\n\n    training_args = ORPOConfig(\n        max_length=full_max_length,\n        max_prompt_length=args.max_source_length,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        per_device_eval_batch_size=args.per_device_eval_batch_size,\n        max_steps=args.max_steps,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        gradient_checkpointing=args.gradient_checkpointing,\n        learning_rate=args.learning_rate,\n        evaluation_strategy=args.eval_strategy,\n        eval_steps=args.eval_steps,\n        output_dir=args.output_dir,\n        report_to=args.report_to,\n        lr_scheduler_type=args.lr_scheduler_type,\n        warmup_steps=args.warmup_steps,\n        optim=args.optim,\n        bf16=args.bf16,\n        fp16=args.fp16,\n        remove_unused_columns=args.remove_unused_columns,\n        run_name=f\"orpo_{args.model_type}\",\n        beta=args.orpo_beta,\n    )\n\n    # Initialize ORPO trainer\n    peft_config = None\n    if args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        target_modules = args.target_modules.split(',') if args.target_modules else None\n        if target_modules and 'all' in target_modules:\n            target_modules = find_all_linear_names(model, int4=args.load_in_4bit, int8=args.load_in_8bit)\n        logger.info(f\"Peft target_modules: {target_modules}\")\n        peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM,\n            target_modules=target_modules,\n            inference_mode=False,\n            r=args.lora_rank,\n            lora_alpha=args.lora_alpha,\n            lora_dropout=args.lora_dropout,\n        )\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n    trainer = ORPOTrainer(\n        model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        peft_config=peft_config if args.use_peft else None,\n    )\n    print_trainable_parameters(trainer.model)\n\n    # Training\n    if args.do_train:\n        logger.info(\"*** Train ***\")\n        train_result = trainer.train()\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {args.output_dir}\")\n            trainer.save_model(args.output_dir)\n            tokenizer.save_pretrained(args.output_dir)\n            trainer.model.save_pretrained(args.output_dir)\n\n    # Evaluation\n    if args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n        metrics[\"eval_samples\"] = max_eval_samples\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "ppo_training.py",
          "type": "blob",
          "size": 22.197265625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: Train a model from SFT using PPO\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    BloomForCausalLM,\n    AutoModelForCausalLM,\n    AutoModel,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n)\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n\nfrom template import get_conv_template\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with PPO\n    \"\"\"\n    # Model arguments\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The model checkpoint for weights initialization.\"}\n    )\n    reward_model_name_or_path: Optional[str] = field(default=None, metadata={\"help\": \"The reward model name\"})\n    reward_model_device: Optional[str] = field(default=\"cuda:0\", metadata={\"help\": \"The reward model device\"})\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The tokenizer for weights initialization.\"}\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n    # Dataset arguments\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The input jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The template name.\"})\n    batch_size: Optional[int] = field(default=8, metadata={\"help\": \"Batch size\"})\n    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"PPO minibatch size\"})\n    max_source_length: Optional[int] = field(default=2048, metadata={\"help\": \"Max length of prompt input text\"})\n    max_target_length: Optional[int] = field(default=512, metadata={\"help\": \"Max length of output text\"})\n    min_target_length: Optional[int] = field(default=4, metadata={\"help\": \"Min length of output text\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None, metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    # Training arguments\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    target_modules: Optional[str] = field(default=None, metadata={\"help\": \"The target modules for peft\"})\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=32.0)\n    modules_to_save: Optional[str] = field(default=None)\n    peft_path: Optional[str] = field(default=None)\n    # PPO arguments\n    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the validation set.\"})\n    early_stopping: Optional[bool] = field(default=False, metadata={\"help\": \"Whether to early stop\"})\n    target_kl: Optional[float] = field(default=0.1, metadata={\"help\": \"The kl target for early stopping\"})\n    reward_baseline: Optional[float] = field(\n        default=0.0, metadata={\"help\": \"Baseline value that is subtracted from the reward\"},\n    )\n    init_kl_coef: Optional[float] = field(\n        default=0.2, metadata={\"help\": \"Initial KL penalty coefficient (used for adaptive and linear control)\"},\n    )\n    adap_kl_ctrl: Optional[bool] = field(default=True, metadata={\"help\": \"Use adaptive KL control, otherwise linear\"})\n    learning_rate: Optional[float] = field(default=1.5e-5, metadata={\"help\": \"Learning rate\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    save_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to save the model\"})\n    output_dir: Optional[str] = field(default=\"outputs-rl\", metadata={\"help\": \"The output directory\"})\n    seed: Optional[int] = field(default=0, metadata={\"help\": \"Seed\"})\n    max_steps: Optional[int] = field(default=200, metadata={\"help\": \"Number of steps to train\"})\n    report_to: Optional[str] = field(default=\"tensorboard\", metadata={\"help\": \"Report to wandb or tensorboard\"})\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\"You must specify a valid model_type to run training.\")\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n        if self.reward_model_name_or_path is None:\n            raise ValueError(\"You must specify a valid reward_model_name_or_path to run training.\")\n        if self.max_source_length < 60:\n            raise ValueError(\"You must specify a valid max_source_length >= 60 to run training\")\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef get_reward_model_output(reward_model, reward_tokenizer, question, answer, device):\n    \"\"\"\n    Get the reward score for a given question and answer pair.\n    \"\"\"\n    inputs = reward_tokenizer(question, answer, return_tensors='pt').to(device)\n    score = reward_model(**inputs).logits[0].cpu().detach()\n\n    return score\n\n\ndef calculate_rewards(reward_score_outputs, reward_baseline=0):\n    \"\"\"\n    Calculate the reward for a given score output.\n    :param reward_score_outputs: \n    :param reward_baseline: \n    :return: \n    \"\"\"\n    rewards = []\n    for score in reward_score_outputs:\n        if isinstance(score, torch.Tensor) and score.numel() == 1:\n            reward_value = score.item() - reward_baseline\n            rewards.append(torch.tensor(reward_value))\n        else:\n            # Use the average of the tensor elements as `score` is multiple elements\n            reward_value = torch.mean(score).item() - reward_baseline\n            rewards.append(torch.tensor(reward_value))\n    return rewards\n\n\ndef main():\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    logger.info(f\"Parse args: {args}\")\n\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    if args.model_type == 'bloom':\n        args.use_fast_tokenizer = True\n    # Load tokenizer\n    tokenizer_kwargs = {\n        \"cache_dir\": args.cache_dir,\n        \"use_fast\": args.use_fast_tokenizer,\n        \"trust_remote_code\": args.trust_remote_code,\n    }\n    tokenizer_name_or_path = args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    # Load model\n    peft_config = None\n    if args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        target_modules = args.target_modules.split(',') if args.target_modules else None\n        logger.info(f\"Peft target_modules: {target_modules}\")\n        peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM,\n            target_modules=target_modules,\n            inference_mode=False,\n            r=args.lora_rank,\n            lora_alpha=args.lora_alpha,\n            lora_dropout=args.lora_dropout,\n        )\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n    torch_dtype = (\n        args.torch_dtype\n        if args.torch_dtype in [\"auto\", None]\n        else getattr(torch, args.torch_dtype)\n    )\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    if world_size > 1:\n        args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n    config = config_class.from_pretrained(\n        args.model_name_or_path,\n        torch_dtype=torch_dtype,\n        trust_remote_code=args.trust_remote_code,\n        cache_dir=args.cache_dir\n    )\n    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n        args.model_name_or_path,\n        config=config,\n        torch_dtype=torch_dtype,\n        load_in_4bit=args.load_in_4bit,\n        load_in_8bit=args.load_in_8bit,\n        device_map=args.device_map,\n        trust_remote_code=args.trust_remote_code,\n        peft_config=peft_config if args.use_peft else None,\n    )\n    for param in filter(lambda p: p.requires_grad, model.parameters()):\n        param.data = param.data.to(torch.float32)\n\n    print_trainable_parameters(model)\n    # Load reward model\n    default_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    device = args.reward_model_device if args.reward_model_device is not None else default_device\n    reward_config = config_class.from_pretrained(\n        args.reward_model_name_or_path,\n        torch_dtype=torch_dtype,\n        trust_remote_code=args.trust_remote_code,\n        cache_dir=args.cache_dir\n    )\n    reward_model = AutoModelForSequenceClassification.from_pretrained(\n        args.reward_model_name_or_path,\n        config=reward_config,\n        load_in_8bit=args.load_in_8bit,\n        trust_remote_code=args.trust_remote_code,\n    )\n    reward_model.to(device)\n    reward_tokenizer = AutoTokenizer.from_pretrained(\n        args.reward_model_name_or_path, **tokenizer_kwargs\n    )\n\n    # Get datasets\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name,\n            args.dataset_config_name,\n            cache_dir=args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    else:\n        data_files = {}\n        if args.train_file_dir is not None and os.path.exists(args.train_file_dir):\n            train_data_files = glob(f'{args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {', '.join(train_data_files)}\")\n            data_files[\"train\"] = train_data_files\n        if args.validation_file_dir is not None and os.path.exists(args.validation_file_dir):\n            eval_data_files = glob(f'{args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    max_source_length = args.max_source_length\n    max_target_length = args.max_target_length\n\n    def preprocess_function(examples):\n        new_examples = {\n            \"query\": [],\n            \"input_ids\": [],\n        }\n        roles = [\"human\", \"gpt\"]\n\n        def get_dialog(examples):\n            system_prompts = examples.get(\"system_prompt\", \"\")\n            for i, source in enumerate(examples['conversations']):\n                if len(source) < 2:\n                    continue\n                data_role = source[0].get(\"from\", \"\")\n                if data_role not in roles or data_role != roles[0]:\n                    # Skip the first one if it is not from human\n                    source = source[1:]\n                if len(source) < 2:\n                    continue\n                messages = []\n                for j, sentence in enumerate(source):\n                    data_role = sentence.get(\"from\", \"\")\n                    if data_role not in roles:\n                        logger.warning(f\"unknown role: {data_role}, {i}. (ignored)\")\n                        break\n                    if data_role == roles[j % 2]:\n                        messages.append(sentence[\"value\"])\n                if len(messages) < 2 or len(messages) % 2 != 0:\n                    continue\n                # Convert the list to pairs of elements\n                history_messages = [[messages[k], messages[k + 1]] for k in range(0, len(messages), 2)]\n                system_prompt = system_prompts[i] if system_prompts else None\n                yield prompt_template.get_dialog(history_messages, system_prompt=system_prompt)\n\n        for dialog in get_dialog(examples):\n            for i in range(len(dialog) // 2):\n                source_txt = dialog[2 * i]\n                tokenized_question = tokenizer(\n                    source_txt, truncation=True, max_length=max_source_length, padding=\"max_length\",\n                    return_tensors=\"pt\"\n                )\n                new_examples[\"query\"].append(source_txt)\n                new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n\n        return new_examples\n\n    # Preprocess the dataset\n    train_dataset = None\n    if args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train']\n        if args.max_train_samples is not None and args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        tokenized_dataset = train_dataset.shuffle().map(\n            preprocess_function,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=train_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        train_dataset = tokenized_dataset.filter(\n            lambda x: len(x['input_ids']) > 0\n        )\n        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n\n    def collator(data):\n        return dict((key, [d[key] for d in data]) for key in data[0])\n\n    output_dir = args.output_dir\n    config = PPOConfig(\n        steps=args.max_steps,\n        model_name=args.model_name_or_path,\n        learning_rate=args.learning_rate,\n        log_with=args.report_to,\n        batch_size=args.batch_size,\n        mini_batch_size=args.mini_batch_size,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        optimize_cuda_cache=True,\n        early_stopping=args.early_stopping,\n        target_kl=args.target_kl,\n        seed=args.seed,\n        init_kl_coef=args.init_kl_coef,\n        adap_kl_ctrl=args.adap_kl_ctrl,\n        project_kwargs={\"logging_dir\": output_dir},\n    )\n    # Set seed before initializing value head for deterministic eval\n    set_seed(config.seed)\n\n    # We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n    trainer = PPOTrainer(\n        config,\n        model,\n        ref_model=None,\n        tokenizer=tokenizer,\n        dataset=train_dataset,\n        data_collator=collator,\n    )\n\n    # These arguments are passed to the `generate` function of the PPOTrainer\n    generation_kwargs = {\n        \"max_new_tokens\": max_target_length,\n        \"temperature\": 1.0,\n        \"repetition_penalty\": 1.0,\n        \"top_p\": 1.0,\n        \"do_sample\": True,\n    }\n\n    # Training\n    if args.do_train:\n        logger.info(\"*** Train ***\")\n        total_steps = config.total_ppo_epochs\n        for step, batch in tqdm(enumerate(trainer.dataloader)):\n            if step >= total_steps:\n                break\n            question_tensors = batch[\"input_ids\"]\n            question_tensors = [torch.LongTensor(i).to(device).squeeze(0) for i in question_tensors]\n            responses = []\n            response_tensors = []\n            for q_tensor in question_tensors:\n                response_tensor = trainer.generate(\n                    q_tensor,\n                    return_prompt=False,\n                    **generation_kwargs,\n                )\n                r = tokenizer.batch_decode(response_tensor, skip_special_tokens=True)[0]\n                responses.append(r)\n                response_tensors.append(response_tensor.squeeze(0))\n            batch[\"response\"] = responses\n\n            # Compute reward score\n            score_outputs = [\n                get_reward_model_output(reward_model, reward_tokenizer, q, r, device) for q, r in\n                zip(batch[\"query\"], batch[\"response\"])\n            ]\n            rewards = calculate_rewards(score_outputs, args.reward_baseline)\n\n            # Run PPO step\n            try:\n                stats = trainer.step(question_tensors, response_tensors, rewards)\n                trainer.log_stats(stats, batch, rewards)\n                logger.debug(f\"Step {step}/{total_steps}: reward score:{score_outputs}\")\n            except ValueError as e:\n                logger.warning(f\"Failed to log stats for step {step}, because of {e}\")\n\n            if step and step % args.save_steps == 0:\n                save_dir = os.path.join(output_dir, f\"checkpoint-{step}\")\n                trainer.save_pretrained(save_dir)\n        # Save final model\n        trainer.save_pretrained(output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "pretraining.py",
          "type": "blob",
          "size": 33.1650390625,
          "content": "# -*- coding: utf-8 -*-\n# Copyright 2023 XuMing(xuming624@qq.com) and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.\n\npart of this code is adapted from https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py\n\"\"\"\nimport math\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom itertools import chain\nfrom typing import Optional, List, Dict, Any, Mapping\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom sklearn.metrics import accuracy_score\nfrom transformers import (\n    AutoConfig,\n    BloomForCausalLM,\n    AutoModelForCausalLM,\n    AutoModel,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n    Trainer,\n    Seq2SeqTrainingArguments,\n    is_torch_tpu_available,\n    set_seed,\n    BitsAndBytesConfig,\n)\nfrom transformers.trainer import TRAINING_ARGS_NAME\nfrom transformers.utils.versions import require_version\n\ntry:\n    from transformers.integrations import is_deepspeed_zero3_enabled\nexcept ImportError:  # https://github.com/huggingface/transformers/releases/tag/v4.33.1\n    from transformers.deepspeed import is_deepspeed_zero3_enabled\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    model_revision: Optional[str] = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    hf_hub_token: Optional[str] = field(default=None, metadata={\"help\": \"Auth token to log in with Hugging Face Hub.\"})\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\n                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n                    MODEL_CLASSES.keys()))\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\n@dataclass\nclass DataArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The train text data file folder.\"})\n    validation_file_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on text file folder.\"},\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n    block_size: Optional[int] = field(\n        default=1024,\n        metadata={\n            \"help\": (\n                \"Optional input sequence length after tokenization. \"\n                \"The training dataset will be truncated in block of this size for training. \"\n                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    keep_linebreaks: bool = field(\n        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n    )\n\n    def __post_init__(self):\n        if self.streaming:\n            require_version(\"datasets>=2.0.0\", \"The streaming feature requires `datasets>=2.0.0`\")\n\n\n@dataclass\nclass ScriptArguments:\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    target_modules: Optional[str] = field(default=\"all\")\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=32.0)\n    modules_to_save: Optional[str] = field(default=None)\n    peft_path: Optional[str] = field(default=None)\n    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})\n\n\ndef accuracy(predictions, references, normalize=True, sample_weight=None):\n    return {\n        \"accuracy\": float(accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight))\n    }\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n    # by preprocess_logits_for_metrics, we need to shift the labels\n    labels = labels[:, 1:].reshape(-1)\n    preds = preds[:, :-1].reshape(-1)\n    return accuracy(predictions=preds, references=labels)\n\n\ndef preprocess_logits_for_metrics(logits, labels):\n    if isinstance(logits, tuple):\n        # Depending on the model and config, logits may contain extra tensors,\n        # like past_key_values, but logits always come first\n        logits = logits[0]\n    return logits.argmax(dim=-1)\n\n\ndef fault_tolerance_data_collator(features: List) -> Dict[str, Any]:\n    if not isinstance(features[0], Mapping):\n        features = [vars(f) for f in features]\n    first = features[0]\n    batch = {}\n\n    # Special handling for labels.\n    # Ensure that tensor is created with the correct type\n    if \"label\" in first and first[\"label\"] is not None:\n        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n        dtype = torch.long if isinstance(label, int) else torch.float\n        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n        if isinstance(first[\"label_ids\"], torch.Tensor):\n            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n        else:\n            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n\n    # Handling of all other possible keys.\n    # Again, we will use the first element to figure out which key/values are not None for this model.\n    try:\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([f[k] for f in features])\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n                else:\n                    batch[k] = torch.tensor([f[k] for f in features])\n    except ValueError:  # quick fix by simply take the first example\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([features[0][k]] * len(features))\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))\n                else:\n                    batch[k] = torch.tensor([features[0][k]] * len(features))\n\n    return batch\n\n\nclass GroupTextsBuilder:\n    def __init__(self, max_seq_length):\n        self.max_seq_length = max_seq_length\n\n    def __call__(self, examples):\n        # Concatenate all texts.\n        firsts = {k: examples[k][0][0] for k in examples.keys()}\n        lasts = {k: examples[k][0][-1] for k in examples.keys()}\n        contents = {k: sum([vi[1:-1] for vi in v], []) for k, v in examples.items()}\n        total_length = len(contents[list(examples.keys())[0]])\n\n        content_length = self.max_seq_length - 2\n        if total_length >= content_length:\n            total_length = (total_length // content_length) * content_length\n        # Split by chunks of max_len.\n        result = {\n            k: [[firsts[k]] + t[i: i + content_length] + [lasts[k]] for i in range(0, total_length, content_length)] for\n            k, t in contents.items()}\n        return result\n\n\nclass SavePeftModelTrainer(Trainer):\n    \"\"\"\n    Trainer for lora models\n    \"\"\"\n\n    def save_model(self, output_dir=None, _internal_call=False):\n        \"\"\"Save the LoRA model.\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n        self.model.save_pretrained(output_dir)\n\n\ndef save_model(model, tokenizer, args):\n    \"\"\"Save the model and the tokenizer.\"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Take care of distributed/parallel training\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef save_model_zero3(model, tokenizer, args, trainer):\n    \"\"\"Save the model for deepspeed zero3.\n    refer https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train_lora.py#L209\n    \"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n    state_dict_zero3 = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(args.output_dir, state_dict=state_dict_zero3)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'output_layer' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\ndef main():\n    parser = HfArgumentParser((ModelArguments, DataArguments, Seq2SeqTrainingArguments, ScriptArguments))\n    model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses()\n\n    logger.info(f\"Model args: {model_args}\")\n    logger.info(f\"Data args: {data_args}\")\n    logger.info(f\"Training args: {training_args}\")\n    logger.info(f\"Script args: {script_args}\")\n    logger.info(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    # Load tokenizer\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n\n    tokenizer_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"use_fast\": model_args.use_fast_tokenizer,\n        \"trust_remote_code\": model_args.trust_remote_code,\n    }\n    tokenizer_name_or_path = model_args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = model_args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n\n    if data_args.block_size is None:\n        block_size = tokenizer.model_max_length\n        if block_size > 2048:\n            logger.warning(\n                \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n                \" of 2048. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n                \" override this default with `--block_size xxx`.\"\n            )\n    else:\n        if data_args.block_size > tokenizer.model_max_length:\n            logger.warning(\n                f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model\"\n                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n            )\n        block_size = min(data_args.block_size, tokenizer.model_max_length)\n\n    # Preprocessing the datasets.\n    def tokenize_function(examples):\n        tokenized_inputs = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding='max_length',\n            max_length=block_size\n        )\n        # Copy the input_ids to the labels for language modeling. This is suitable for both\n        # masked language modeling (like BERT) or causal language modeling (like GPT).\n        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n\n        return tokenized_inputs\n\n    def tokenize_wo_pad_function(examples):\n        return tokenizer(examples[\"text\"])\n\n    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n    def group_text_function(examples):\n        # Concatenate all texts.\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n        if total_length >= block_size:\n            total_length = (total_length // block_size) * block_size\n        # Split by chunks of max_len.\n        result = {\n            k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            data_args.dataset_name,\n            data_args.dataset_config_name,\n            cache_dir=model_args.cache_dir,\n            streaming=data_args.streaming,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                data_args.dataset_name,\n                data_args.dataset_config_name,\n                split=f\"train[:{data_args.validation_split_percentage}%]\",\n                cache_dir=model_args.cache_dir,\n                streaming=data_args.streaming,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                data_args.dataset_name,\n                data_args.dataset_config_name,\n                split=f\"train[{data_args.validation_split_percentage}%:]\",\n                cache_dir=model_args.cache_dir,\n                streaming=data_args.streaming,\n            )\n    else:\n        data_files = {}\n        dataset_args = {}\n        if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n            train_data_files = glob(f'{data_args.train_file_dir}/**/*.txt', recursive=True) + glob(\n                f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {train_data_files}\")\n            # Train data files must be same type, e.g. all txt or all jsonl\n            types = [f.split('.')[-1] for f in train_data_files]\n            if len(set(types)) > 1:\n                raise ValueError(f\"train files must be same type, e.g. all txt or all jsonl, but got {types}\")\n            data_files[\"train\"] = train_data_files\n        if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n            eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.txt', recursive=True) + glob(\n                f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {eval_data_files}\")\n            data_files[\"validation\"] = eval_data_files\n            # Train data files must be same type, e.g. all txt or all jsonl\n            types = [f.split('.')[-1] for f in eval_data_files]\n            if len(set(types)) > 1:\n                raise ValueError(f\"train files must be same type, e.g. all txt or all jsonl, but got {types}\")\n        extension = \"text\" if data_files[\"train\"][0].endswith('txt') else 'json'\n        if extension == \"text\":\n            dataset_args[\"keep_linebreaks\"] = data_args.keep_linebreaks\n        raw_datasets = load_dataset(\n            extension,\n            data_files=data_files,\n            cache_dir=model_args.cache_dir,\n            **dataset_args,\n        )\n\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[:{data_args.validation_split_percentage}%]\",\n                cache_dir=model_args.cache_dir,\n                **dataset_args,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[{data_args.validation_split_percentage}%:]\",\n                cache_dir=model_args.cache_dir,\n                **dataset_args,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets.\n    if training_args.do_train:\n        column_names = list(raw_datasets[\"train\"].features)\n    else:\n        column_names = list(raw_datasets[\"validation\"].features)\n\n    with training_args.main_process_first(desc=\"Dataset tokenization and grouping\"):\n        if not data_args.streaming:\n            if training_args.group_by_length:\n                tokenized_datasets = raw_datasets.map(\n                    tokenize_wo_pad_function,\n                    batched=True,\n                    num_proc=data_args.preprocessing_num_workers,\n                    remove_columns=column_names,\n                    load_from_cache_file=not data_args.overwrite_cache,\n                    desc=\"Running tokenizer on dataset\",\n                )\n                lm_datasets = tokenized_datasets.map(\n                    group_text_function,\n                    batched=True,\n                    num_proc=data_args.preprocessing_num_workers,\n                    load_from_cache_file=not data_args.overwrite_cache,\n                    desc=f\"Grouping texts in chunks of {block_size}\",\n                )\n            else:\n                lm_datasets = raw_datasets.map(\n                    tokenize_function,\n                    batched=True,\n                    num_proc=data_args.preprocessing_num_workers,\n                    remove_columns=column_names,\n                    load_from_cache_file=not data_args.overwrite_cache,\n                    desc=\"Running tokenizer on dataset\",\n                )\n        else:\n            if training_args.group_by_length:\n                tokenized_datasets = raw_datasets.map(\n                    tokenize_wo_pad_function,\n                    batched=True,\n                    remove_columns=column_names,\n                )\n                lm_datasets = tokenized_datasets.map(\n                    group_text_function,\n                    batched=True,\n                )\n            else:\n                lm_datasets = raw_datasets.map(\n                    tokenize_function,\n                    batched=True,\n                    remove_columns=column_names,\n                )\n\n    train_dataset = None\n    max_train_samples = 0\n    if training_args.do_train:\n        if \"train\" not in lm_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = lm_datasets['train']\n        max_train_samples = len(train_dataset)\n        if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n        logger.debug(\"Tokenized training example:\")\n        logger.debug(tokenizer.decode(train_dataset[0]['input_ids']))\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if training_args.do_eval:\n        if \"validation\" not in lm_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = lm_datasets[\"validation\"]\n        max_eval_samples = len(eval_dataset)\n        if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n            eval_dataset = eval_dataset.select(range(max_eval_samples))\n        logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n        logger.debug(\"Tokenized eval example:\")\n        logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))\n\n    # Load model\n    if model_args.model_type and model_args.model_name_or_path:\n        torch_dtype = (\n            model_args.torch_dtype\n            if model_args.torch_dtype in [\"auto\", None]\n            else getattr(torch, model_args.torch_dtype)\n        )\n        world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n        ddp = world_size != 1\n        if ddp:\n            model_args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n        if script_args.qlora and (len(training_args.fsdp) > 0 or is_deepspeed_zero3_enabled()):\n            logger.warning(\"FSDP and DeepSpeed ZeRO-3 are both currently incompatible with QLoRA.\")\n\n        config_kwargs = {\n            \"trust_remote_code\": model_args.trust_remote_code,\n            \"cache_dir\": model_args.cache_dir,\n            \"revision\": model_args.model_revision,\n            \"token\": model_args.hf_hub_token,\n        }\n        config = config_class.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n        load_in_4bit = model_args.load_in_4bit\n        load_in_8bit = model_args.load_in_8bit\n        if load_in_4bit and load_in_8bit:\n            raise ValueError(\"Error, load_in_4bit and load_in_8bit cannot be set at the same time\")\n        elif load_in_8bit or load_in_4bit:\n            logger.info(f\"Quantizing model, load_in_4bit: {load_in_4bit}, load_in_8bit: {load_in_8bit}\")\n            if is_deepspeed_zero3_enabled():\n                raise ValueError(\"DeepSpeed ZeRO-3 is incompatible with quantization.\")\n            if load_in_8bit:\n                config_kwargs['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n            elif load_in_4bit:\n                if script_args.qlora:\n                    config_kwargs['quantization_config'] = BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_use_double_quant=True,\n                        bnb_4bit_quant_type=\"nf4\",\n                        bnb_4bit_compute_dtype=torch_dtype,\n                    )\n                else:\n                    config_kwargs['quantization_config'] = BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_compute_dtype=torch_dtype,\n                    )\n\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            config=config,\n            torch_dtype=torch_dtype,\n            low_cpu_mem_usage=(not is_deepspeed_zero3_enabled()),\n            device_map=model_args.device_map,\n            **config_kwargs,\n        )\n    else:\n        raise ValueError(f\"Error, model_name_or_path is None, Continue PT must be loaded from a pre-trained model\")\n\n    if script_args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        if script_args.peft_path is not None:\n            logger.info(f\"Peft from pre-trained model: {script_args.peft_path}\")\n            model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)\n        else:\n            logger.info(\"Init new peft model\")\n            if load_in_8bit or load_in_4bit:\n                model = prepare_model_for_kbit_training(model, training_args.gradient_checkpointing)\n            target_modules = script_args.target_modules.split(',') if script_args.target_modules else None\n            if target_modules and 'all' in target_modules:\n                target_modules = find_all_linear_names(model, int4=load_in_4bit, int8=load_in_8bit)\n            modules_to_save = script_args.modules_to_save\n            if modules_to_save is not None:\n                modules_to_save = modules_to_save.split(',')\n                # Resize the embedding layer to match the new tokenizer\n                embedding_size = model.get_input_embeddings().weight.shape[0]\n                if len(tokenizer) > embedding_size:\n                    model.resize_token_embeddings(len(tokenizer))\n            logger.info(f\"Peft target_modules: {target_modules}\")\n            logger.info(f\"Peft lora_rank: {script_args.lora_rank}\")\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                target_modules=target_modules,\n                inference_mode=False,\n                r=script_args.lora_rank,\n                lora_alpha=script_args.lora_alpha,\n                lora_dropout=script_args.lora_dropout,\n                modules_to_save=modules_to_save)\n            model = get_peft_model(model, peft_config)\n        for param in filter(lambda p: p.requires_grad, model.parameters()):\n            param.data = param.data.to(torch.float32)\n        model.print_trainable_parameters()\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n        model = model.float()\n        print_trainable_parameters(model)\n\n    # Initialize our Trainer\n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n    else:\n        model.config.use_cache = True\n    model.enable_input_require_grads()\n    if not ddp and torch.cuda.device_count() > 1:\n        # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n        model.is_parallelizable = True\n        model.model_parallel = True\n\n    trainer = SavePeftModelTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=fault_tolerance_data_collator,\n        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,\n        preprocess_logits_for_metrics=preprocess_logits_for_metrics\n        if training_args.do_eval and not is_torch_tpu_available()\n        else None,\n    )\n\n    # Training\n    if training_args.do_train:\n        logger.info(\"*** Train ***\")\n        logger.debug(f\"Train dataloader example: {next(iter(trainer.get_train_dataloader()))}\")\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n        model.config.use_cache = True  # enable cache after training\n        tokenizer.padding_side = \"left\"  # restore padding side\n        tokenizer.init_kwargs[\"padding_side\"] = \"left\"\n\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n            if is_deepspeed_zero3_enabled():\n                save_model_zero3(model, tokenizer, training_args, trainer)\n            else:\n                save_model(model, tokenizer, training_args)\n\n    # Evaluation\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n\n        metrics[\"eval_samples\"] = max_eval_samples\n        try:\n            perplexity = math.exp(metrics[\"eval_loss\"])\n        except OverflowError:\n            perplexity = float(\"inf\")\n        metrics[\"perplexity\"] = perplexity\n\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2001953125,
          "content": "accelerate~=0.27.2\ndatasets>=2.14.6\nloguru\npeft~=0.10.0\nsentencepiece\nscikit-learn\ntensorboard\ntqdm>=4.47.0\ntransformers>=4.39.3 # GLM4 transformers==4.40.2\ntrl~=0.8.3\nbitsandbytes==0.43.3\ntiktoken\nGPUtil\n"
        },
        {
          "name": "reward_modeling.py",
          "type": "blob",
          "size": 27.96875,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description:\n\"\"\"\n\nimport math\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import Any, List, Union, Optional, Dict\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom transformers import (\n    AutoConfig,\n    PreTrainedTokenizerBase,\n    BloomForSequenceClassification,\n    LlamaForSequenceClassification,\n    BloomTokenizerFast,\n    AlbertForSequenceClassification,\n    BertForSequenceClassification,\n    BertTokenizer,\n    AutoTokenizer,\n    RobertaForSequenceClassification,\n    AutoModelForSequenceClassification,\n    RobertaTokenizer,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n)\nfrom transformers.trainer import TRAINING_ARGS_NAME\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bert\": (AutoConfig, BertForSequenceClassification, BertTokenizer),\n    \"roberta\": (AutoConfig, RobertaForSequenceClassification, RobertaTokenizer),\n    \"albert\": (AutoConfig, AlbertForSequenceClassification, AutoTokenizer),\n    \"bloom\": (AutoConfig, BloomForSequenceClassification, BloomTokenizerFast),\n    \"llama\": (AutoConfig, LlamaForSequenceClassification, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForSequenceClassification, AutoTokenizer),\n}\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\n                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n                    MODEL_CLASSES.keys()))\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\n@dataclass\nclass DataArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The input jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n    max_source_length: Optional[int] = field(default=2048, metadata={\"help\": \"Max length of prompt input text\"})\n    max_target_length: Optional[int] = field(default=512, metadata={\"help\": \"Max length of output text\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=4,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n\n\n@dataclass\nclass ScriptArguments:\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    target_modules: Optional[str] = field(default=\"all\")\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=32.0)\n    modules_to_save: Optional[str] = field(default=None)\n    peft_path: Optional[str] = field(default=None)\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The prompt template name.\"})\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    # Here, predictions is rewards_chosen and rewards_rejected.\n    if isinstance(preds, torch.Tensor):\n        preds = preds.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.detach().cpu().numpy()\n    # MSE\n    mse = mean_squared_error(labels, preds)\n    # MAE\n    mae = mean_absolute_error(labels, preds)\n\n    return {\"mse\": mse, \"mae\": mae}\n\n\n@dataclass\nclass RewardDataCollatorWithPadding:\n    \"\"\"We need to define a special data collator that batches the data in our chosen vs rejected format\"\"\"\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    return_tensors: str = \"pt\"\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        features_chosen = []\n        features_rejected = []\n        for feature in features:\n            features_chosen.append(\n                {\n                    \"input_ids\": feature[\"input_ids_chosen\"],\n                    \"attention_mask\": feature[\"attention_mask_chosen\"],\n                }\n            )\n            features_rejected.append(\n                {\n                    \"input_ids\": feature[\"input_ids_rejected\"],\n                    \"attention_mask\": feature[\"attention_mask_rejected\"],\n                }\n            )\n        batch_chosen = self.tokenizer.pad(\n            features_chosen,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch_rejected = self.tokenizer.pad(\n            features_rejected,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch = {\n            \"input_ids_chosen\": batch_chosen[\"input_ids\"],\n            \"attention_mask_chosen\": batch_chosen[\"attention_mask\"],\n            \"input_ids_rejected\": batch_rejected[\"input_ids\"],\n            \"attention_mask_rejected\": batch_rejected[\"attention_mask\"],\n            \"return_loss\": True,\n        }\n        return batch\n\n\nclass RewardTrainer(Trainer):\n    \"\"\"\n    Trainer for reward models\n        Define how to compute the reward loss. Use the InstructGPT pairwise logloss: https://arxiv.org/abs/2203.02155\n    \"\"\"\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        rewards_chosen = model(input_ids=inputs[\"input_ids_chosen\"],\n                               attention_mask=inputs[\"attention_mask_chosen\"])[0]\n        rewards_rejected = model(input_ids=inputs[\"input_ids_rejected\"],\n                                 attention_mask=inputs[\"attention_mask_rejected\"])[0]\n        # è®¡ç®—æŸå¤±ï¼šInstructGPTä¸­çš„pairwise logloss\n        loss = -torch.nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n        if return_outputs:\n            return loss, {\"rewards_chosen\": rewards_chosen, \"rewards_rejected\": rewards_rejected}\n        return loss\n\n    def evaluate(\n            self,\n            eval_dataset: Optional[Dataset] = None,\n            ignore_keys: Optional[List[str]] = None,\n            metric_key_prefix: str = \"eval\",\n    ) -> Dict[str, float]:\n        if eval_dataset is None:\n            eval_dataset = self.eval_dataset\n        return super().evaluate(eval_dataset=eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        # Prepare inputs for chosen and rejected separately\n        device = model.device\n\n        inputs_chosen = {\n            \"input_ids\": inputs[\"input_ids_chosen\"].to(device),\n            \"attention_mask\": inputs[\"attention_mask_chosen\"].to(device),\n        }\n        outputs_chosen = model(**inputs_chosen)\n        rewards_chosen = outputs_chosen.logits.detach()\n\n        inputs_rejected = {\n            \"input_ids\": inputs[\"input_ids_rejected\"].to(device),\n            \"attention_mask\": inputs[\"attention_mask_rejected\"].to(device),\n        }\n        outputs_rejected = model(**inputs_rejected)\n        rewards_rejected = outputs_rejected.logits.detach()\n\n        # Keep the compute_loss method\n        loss = -torch.nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n        if prediction_loss_only:\n            return (loss, None, None)\n\n        return (loss, rewards_chosen, rewards_rejected)\n\n    def save_model(self, output_dir=None, _internal_call=False):\n        \"\"\"Save the LoRA model.\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n        self.model.save_pretrained(output_dir)\n\n\ndef save_model(model, tokenizer, args):\n    \"\"\"Save the model and the tokenizer.\"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Take care of distributed/parallel training\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n\nclass CastOutputToFloat(torch.nn.Sequential):\n    \"\"\"Cast the output of the model to float\"\"\"\n\n    def forward(self, x):\n        return super().forward(x).to(torch.float32)\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'score' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\ndef main():\n    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments, ScriptArguments))\n    model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses()\n\n    logger.info(f\"Model args: {model_args}\")\n    logger.info(f\"Data args: {data_args}\")\n    logger.info(f\"Training args: {training_args}\")\n    logger.info(f\"Script args: {script_args}\")\n    logger.info(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    # Load model\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n    if model_args.model_name_or_path:\n        torch_dtype = (\n            model_args.torch_dtype\n            if model_args.torch_dtype in [\"auto\", None]\n            else getattr(torch, model_args.torch_dtype)\n        )\n        world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n        if world_size > 1:\n            model_args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n        config = config_class.from_pretrained(\n            model_args.model_name_or_path,\n            num_labels=1,\n            torch_dtype=torch_dtype,\n            trust_remote_code=model_args.trust_remote_code,\n            cache_dir=model_args.cache_dir\n        )\n        if model_args.model_type in ['bloom', 'llama']:\n            model = model_class.from_pretrained(\n                model_args.model_name_or_path,\n                config=config,\n                torch_dtype=torch_dtype,\n                load_in_4bit=model_args.load_in_4bit,\n                load_in_8bit=model_args.load_in_8bit,\n                device_map=model_args.device_map,\n                trust_remote_code=model_args.trust_remote_code,\n            )\n        else:\n            model = model_class.from_pretrained(\n                model_args.model_name_or_path,\n                config=config,\n                cache_dir=model_args.cache_dir,\n                ignore_mismatched_sizes=True\n            )\n            model.to(training_args.device)\n    else:\n        raise ValueError(f\"Error, model_name_or_path is None, RM must be loaded from a pre-trained model\")\n\n    # Load tokenizer\n    if model_args.model_type == \"bloom\":\n        model_args.use_fast_tokenizer = True\n    tokenizer_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"use_fast\": model_args.use_fast_tokenizer,\n        \"trust_remote_code\": model_args.trust_remote_code,\n    }\n    tokenizer_name_or_path = model_args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = model_args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(script_args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    if script_args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        if script_args.peft_path is not None:\n            logger.info(f\"Peft from pre-trained model: {script_args.peft_path}\")\n            model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)\n        else:\n            logger.info(\"Init new peft model\")\n            if model_args.load_in_8bit:\n                model = prepare_model_for_kbit_training(model)\n            target_modules = script_args.target_modules.split(',') if script_args.target_modules else None\n            if target_modules and 'all' in target_modules:\n                target_modules = find_all_linear_names(model, int4=False, int8=model_args.load_in_8bit)\n            modules_to_save = script_args.modules_to_save\n            if modules_to_save is not None:\n                modules_to_save = modules_to_save.split(',')\n            logger.info(f\"Peft target_modules: {target_modules}\")\n            logger.info(f\"Peft lora_rank: {script_args.lora_rank}\")\n            peft_config = LoraConfig(\n                task_type=TaskType.SEQ_CLS,\n                target_modules=target_modules,\n                inference_mode=False,\n                r=script_args.lora_rank,\n                lora_alpha=script_args.lora_alpha,\n                lora_dropout=script_args.lora_dropout,\n                modules_to_save=modules_to_save)\n            model = get_peft_model(model, peft_config)\n        for param in filter(lambda p: p.requires_grad, model.parameters()):\n            param.data = param.data.to(torch.float32)\n        model.print_trainable_parameters()\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n        print_trainable_parameters(model)\n\n    # Get reward dataset for tuning the reward model.\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            data_args.dataset_name,\n            data_args.dataset_config_name,\n            cache_dir=model_args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                data_args.dataset_name,\n                data_args.dataset_config_name,\n                split=f\"train[:{data_args.validation_split_percentage}%]\",\n                cache_dir=model_args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                data_args.dataset_name,\n                data_args.dataset_config_name,\n                split=f\"train[{data_args.validation_split_percentage}%:]\",\n                cache_dir=model_args.cache_dir,\n            )\n    else:\n        data_files = {}\n        if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n            train_data_files = glob(f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {', '.join(train_data_files)}\")\n            data_files[\"train\"] = train_data_files\n        if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n            eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=model_args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[:{data_args.validation_split_percentage}%]\",\n                cache_dir=model_args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[{data_args.validation_split_percentage}%:]\",\n                cache_dir=model_args.cache_dir,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    full_max_length = data_args.max_source_length + data_args.max_target_length\n\n    def preprocess_reward_function(examples):\n        \"\"\"\n        Turn the dataset into pairs of Question + Answer, where input_ids_chosen is the preferred question + answer\n            and text_rejected is the other.\n        \"\"\"\n        new_examples = {\n            \"input_ids_chosen\": [],\n            \"attention_mask_chosen\": [],\n            \"input_ids_rejected\": [],\n            \"attention_mask_rejected\": [],\n        }\n        for system, history, question, chosen, rejected in zip(\n                examples[\"system\"],\n                examples[\"history\"],\n                examples[\"question\"],\n                examples[\"response_chosen\"],\n                examples[\"response_rejected\"]\n        ):\n            system_prompt = system or \"\"\n            chosen_messages = history + [[question, chosen]] if history else [[question, chosen]]\n            chosen_prompt = prompt_template.get_prompt(messages=chosen_messages, system_prompt=system_prompt)\n            rejected_messages = history + [[question, rejected]] if history else [[question, rejected]]\n            rejected_prompt = prompt_template.get_prompt(messages=rejected_messages, system_prompt=system_prompt)\n\n            tokenized_chosen = tokenizer(chosen_prompt)\n            tokenized_rejected = tokenizer(rejected_prompt)\n\n            new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n            new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n            new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n            new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n        return new_examples\n\n    train_dataset = None\n    max_train_samples = 0\n    if training_args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train']\n        max_train_samples = len(train_dataset)\n        if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        with training_args.main_process_first(desc=\"Train dataset tokenization\"):\n            tokenized_dataset = train_dataset.shuffle().map(\n                preprocess_reward_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=train_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n            train_dataset = tokenized_dataset.filter(\n                lambda x: 0 < len(x['input_ids_rejected']) <= full_max_length and 0 < len(\n                    x['input_ids_chosen']) <= full_max_length\n            )\n            logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n            logger.debug(\"Tokenized training example:\")\n            logger.debug(tokenizer.decode(train_dataset[0]['input_ids_chosen']))\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if training_args.do_eval:\n        with training_args.main_process_first(desc=\"Eval dataset tokenization\"):\n            if \"validation\" not in raw_datasets:\n                raise ValueError(\"--do_eval requires a validation dataset\")\n            eval_dataset = raw_datasets[\"validation\"]\n            max_eval_samples = len(eval_dataset)\n            if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n                max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n                eval_dataset = eval_dataset.select(range(max_eval_samples))\n            logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n            tokenized_dataset = eval_dataset.map(\n                preprocess_reward_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=eval_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n            eval_dataset = tokenized_dataset.filter(\n                lambda x: 0 < len(x['input_ids_rejected']) <= full_max_length and 0 < len(\n                    x['input_ids_chosen']) <= full_max_length\n            )\n            logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n            logger.debug(\"Tokenized eval example:\")\n            logger.debug(tokenizer.decode(eval_dataset[0]['input_ids_chosen']))\n\n    # Initialize our Trainer\n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n    else:\n        model.config.use_cache = True\n    model.enable_input_require_grads()\n    if torch.cuda.device_count() > 1:\n        # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n        model.is_parallelizable = True\n        model.model_parallel = True\n    trainer = RewardTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n        data_collator=RewardDataCollatorWithPadding(\n            tokenizer=tokenizer, max_length=full_max_length, padding=\"max_length\"\n        ),\n    )\n\n    # Training\n    if training_args.do_train:\n        logger.info(\"*** Train ***\")\n        logger.debug(f\"Train dataloader example: {next(iter(trainer.get_train_dataloader()))}\")\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n        model.config.use_cache = True  # enable cache after training\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n            save_model(model, tokenizer, training_args)\n\n    # Evaluation\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n\n        metrics[\"eval_samples\"] = max_eval_samples\n        try:\n            perplexity = math.exp(metrics[\"eval_loss\"])\n        except OverflowError:\n            perplexity = float(\"inf\")\n        metrics[\"perplexity\"] = perplexity\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "role_play_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "run_dpo.sh",
          "type": "blob",
          "size": 0.85546875,
          "content": "CUDA_VISIBLE_DEVICES=0,1 python dpo_training.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --template_name qwen \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 20 \\\n    --save_steps 50 \\\n    --max_source_length 1024 \\\n    --max_target_length 512 \\\n    --output_dir outputs-dpo-qwen-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache\n"
        },
        {
          "name": "run_eval_quantize.sh",
          "type": "blob",
          "size": 0.1103515625,
          "content": "python eval_quantize.py --bnb_path /path/to/your/bnb_model --data_path data/finetune/medical_sft_1K_format.jsonl\n"
        },
        {
          "name": "run_full_sft.sh",
          "type": "blob",
          "size": 1.1572265625,
          "content": "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 torchrun --nproc_per_node 8 supersived_finetuning.py \\\n    --model_type auto \\\n    --model_name_or_path ./model/glm-4-9b-chat \\\n    --train_file_dir ./data/finetune \\\n    --validation_file_dir ./data/finetune \\\n    --cache_dir ./model \\\n    --device_map None \\\n    --use_peft False \\\n    --per_device_train_batch_size 2 \\\n    --do_train \\\n    --num_train_epochs 3 \\\n    --per_device_eval_batch_size 2 \\\n    --max_train_samples -1 \\\n    --learning_rate 3e-5 \\\n    --warmup_ratio 0.2 \\\n    --model_max_length 2048 \\\n    --weight_decay 0.01 \\\n    --logging_strategy steps \\\n    --logging_steps 1 \\\n    --save_steps 400 \\\n    --save_strategy steps \\\n    --save_total_limit 3 \\\n    --gradient_accumulation_steps 1 \\\n    --preprocessing_num_workers 128 \\\n    --output_dir GLM4-sft-med \\\n    --overwrite_output_dir \\\n    --ddp_timeout 30000 \\\n    --logging_first_step True \\\n    --target_modules all \\\n    --torch_dtype bfloat16 \\\n    --report_to tensorboard \\\n    --neft_alpha 8 \\\n    --ddp_find_unused_parameters False \\\n    --gradient_checkpointing True \\\n    --template_name chatglm3 \\\n    --deepspeed ./deepspeed_zero_stage2_config.json \\\n    --fp16\n"
        },
        {
          "name": "run_orpo.sh",
          "type": "blob",
          "size": 0.837890625,
          "content": "CUDA_VISIBLE_DEVICES=0,1 python orpo_training.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --template_name qwen \\\n    --train_file_dir ./data/reward \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 50 \\\n    --max_steps 100 \\\n    --eval_steps 20 \\\n    --save_steps 50 \\\n    --max_source_length 1024 \\\n    --max_target_length 512 \\\n    --output_dir outputs-orpo-qwen-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --orpo_beta 0.1 \\\n    --cache_dir ./cache\n"
        },
        {
          "name": "run_ppo.sh",
          "type": "blob",
          "size": 0.810546875,
          "content": "CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 ppo_training.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --reward_model_name_or_path OpenAssistant/reward-model-deberta-v3-large-v2 \\\n    --template_name qwen \\\n    --torch_dtype float16 \\\n    --device_map auto \\\n    --train_file_dir ./data/finetune \\\n    --validation_file_dir ./data/finetune \\\n    --batch_size 8 \\\n    --max_source_length 1024 \\\n    --max_target_length 256 \\\n    --max_train_samples 1000 \\\n    --use_peft True \\\n    --target_modules q_proj,v_proj \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --do_train \\\n    --max_steps 100 \\\n    --learning_rate 1e-5 \\\n    --save_steps 50 \\\n    --output_dir outputs-rl-qwen-v1 \\\n    --early_stopping True \\\n    --target_kl 0.1 \\\n    --reward_baseline 0.0\n"
        },
        {
          "name": "run_pt.sh",
          "type": "blob",
          "size": 1.24609375,
          "content": "CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 pretraining.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --train_file_dir ./data/pretrain \\\n    --validation_file_dir ./data/pretrain \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --seed 42 \\\n    --max_train_samples 10000 \\\n    --max_eval_samples 10 \\\n    --num_train_epochs 0.5 \\\n    --learning_rate 2e-4 \\\n    --warmup_ratio 0.05 \\\n    --weight_decay 0.01 \\\n    --logging_strategy steps \\\n    --logging_steps 10 \\\n    --eval_steps 50 \\\n    --evaluation_strategy steps \\\n    --save_steps 500 \\\n    --save_strategy steps \\\n    --save_total_limit 13 \\\n    --gradient_accumulation_steps 1 \\\n    --preprocessing_num_workers 10 \\\n    --block_size 512 \\\n    --group_by_length True \\\n    --output_dir outputs-pt-qwen-v1 \\\n    --overwrite_output_dir \\\n    --ddp_timeout 30000 \\\n    --logging_first_step True \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype bfloat16 \\\n    --bf16 \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --ddp_find_unused_parameters False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache\n"
        },
        {
          "name": "run_quant.sh",
          "type": "blob",
          "size": 0.166015625,
          "content": "python model_quant.py --unquantized_model_path /path/to/unquantized/model --quantized_model_output_path /path/to/save/quantized/model --input_text \"Your input text here\"\n"
        },
        {
          "name": "run_rm.sh",
          "type": "blob",
          "size": 1.138671875,
          "content": "CUDA_VISIBLE_DEVICES=0,1 python reward_modeling.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 4 \\\n    --do_train \\\n    --use_peft True \\\n    --seed 42 \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --num_train_epochs 1 \\\n    --learning_rate 2e-5 \\\n    --warmup_ratio 0.05 \\\n    --weight_decay 0.001 \\\n    --logging_strategy steps \\\n    --logging_steps 10 \\\n    --eval_steps 50 \\\n    --evaluation_strategy steps \\\n    --save_steps 500 \\\n    --save_strategy steps \\\n    --save_total_limit 3 \\\n    --max_source_length 1024 \\\n    --max_target_length 256 \\\n    --output_dir outputs-rm-qwen-v1 \\\n    --overwrite_output_dir \\\n    --ddp_timeout 30000 \\\n    --logging_first_step True \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float32 \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --ddp_find_unused_parameters False \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True\n"
        },
        {
          "name": "run_sft.sh",
          "type": "blob",
          "size": 1.2412109375,
          "content": "CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 supervised_finetuning.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --train_file_dir ./data/finetune \\\n    --validation_file_dir ./data/finetune \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 4 \\\n    --do_train \\\n    --do_eval \\\n    --template_name qwen \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --model_max_length 4096 \\\n    --num_train_epochs 1 \\\n    --learning_rate 2e-5 \\\n    --warmup_ratio 0.05 \\\n    --weight_decay 0.05 \\\n    --logging_strategy steps \\\n    --logging_steps 10 \\\n    --eval_steps 50 \\\n    --evaluation_strategy steps \\\n    --save_steps 500 \\\n    --save_strategy steps \\\n    --save_total_limit 13 \\\n    --gradient_accumulation_steps 1 \\\n    --preprocessing_num_workers 4 \\\n    --output_dir outputs-sft-qwen-v1 \\\n    --overwrite_output_dir \\\n    --ddp_timeout 30000 \\\n    --logging_first_step True \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --ddp_find_unused_parameters False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache\n"
        },
        {
          "name": "run_training_dpo_pipeline.ipynb",
          "type": "blob",
          "size": 19.1669921875,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Training Pipeline\\n\",\n    \"[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"# Stage 1: Continue Pretraining\\n\",\n    \"\\n\",\n    \"ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æœ¬æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒGPTæ¨¡å‹ï¼Œä»¥é€‚é…é¢†åŸŸæ•°æ®åˆ†å¸ƒ\\n\",\n    \"\\n\",\n    \"æ³¨æ„ï¼š\\n\",\n    \"1. æ­¤é˜¶æ®µæ˜¯å¯é€‰çš„ï¼Œå¦‚æœä½ æ²¡æœ‰æµ·é‡é¢†åŸŸæ–‡æœ¬ï¼Œå¯ä»¥è·³è¿‡æ­¤é˜¶æ®µï¼Œç›´æ¥è¿›è¡ŒSFTé˜¶æ®µçš„æœ‰ç›‘ç£å¾®è°ƒ\\n\",\n    \"2. æˆ‘å®éªŒå‘ç°ï¼šåšé¢†åŸŸçŸ¥è¯†æ³¨å…¥ï¼ŒSFTæ¯”PTæ›´é«˜æ•ˆï¼Œä¹Ÿå¯ä»¥è·³è¿‡PTé˜¶æ®µ\\n\",\n    \"\\n\",\n    \"| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### è¯´æ˜ï¼š\\n\",\n    \"ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\\n\",\n    \"\\n\",\n    \"1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m`\\n\",\n    \"2. æ•°æ®é›†ï¼šPTé˜¶æ®µä½¿ç”¨çš„æ˜¯ä¸­æ–‡å¤©é¾™å…«éƒ¨å°è¯´éƒ¨åˆ†æ–‡æœ¬å’Œè‹±æ–‡ä¹¦ç±éƒ¨åˆ†æ–‡æœ¬ï¼Œä½äº`data/pretrain`æ–‡ä»¶å¤¹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## é…ç½®è¿è¡Œç¯å¢ƒ\\n\",\n    \"\\n\",\n    \"æœ¬åœ°æ‰§è¡Œå¯æ³¨é‡Šä»¥ä¸‹é…ç½®ç¯å¢ƒçš„å‘½ä»¤ï¼Œcolabæ‰§è¡Œè¦æ‰“å¼€æ³¨é‡Šï¼Œç”¨äºé…ç½®ç¯å¢ƒ\\n\",\n    \"\\n\",\n    \"colabå»ºè®®ä½¿ç”¨T4 GPUè®­ç»ƒï¼Œè®¾ç½®æ–¹å¼ï¼š`ä»£ç æ‰§è¡Œç¨‹åº -> æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ -> è¿è¡Œæ—¶ç±»å‹ï¼šPython3ï¼Œç¡¬ä»¶åŠ é€Ÿå™¨ï¼šGPUï¼ŒGPUç±»å‹ï¼šT4 -> ä¿å­˜`\\n\",\n    \"\\n\",\n    \"æ­¥éª¤ï¼š\\n\",\n    \"1. ä¸‹è½½æœ€æ–°ä»£ç åˆ°æœ¬åœ°\\n\",\n    \"2. å®‰è£…ä¾èµ–åŒ…\\n\",\n    \"\\n\",\n    \"ä¾èµ–åŒ…å¦‚ä¸‹ï¼Œä¿è¯æœ€æ–°ç‰ˆæœ¬ï¼š\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"loguru\\n\",\n    \"transformers\\n\",\n    \"sentencepiece\\n\",\n    \"datasets\\n\",\n    \"tensorboard\\n\",\n    \"tqdm\\n\",\n    \"peft\\n\",\n    \"trl\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\\n\",\n    \"%cd MedicalGPT\\n\",\n    \"%ls\\n\",\n    \"!pip install -r requirements.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Stage1 å’±ä»¬å¼€å§‹å§\\n\",\n    \"\\n\",\n    \"è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\\n\",\n    \"\\n\",\n    \"1. ç¡®è®¤è®­ç»ƒé›†\\n\",\n    \"2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\\n\",\n    \"\\n\",\n    \"è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\\n\",\n    \"1. å¯¼å…¥ä¾èµ–åŒ…\\n\",\n    \"2. è®¾ç½®å‚æ•°\\n\",\n    \"3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\\n\",\n    \"4. åŠ è½½æ¨¡å‹å’Œtokenizer\\n\",\n    \"5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\\n\",\n    \"6. æŸ¥çœ‹è®­ç»ƒç»“æœ\\n\",\n    \"\\n\",\n    \"**ä»¥ä¸‹å‚æ•°å¯ä»¥æ ¹æ®ä½ çš„GPUå®é™…æƒ…å†µä¿®æ”¹ï¼Œå½“å‰å‚æ•°æ˜¯æ ¹æ®Colabçš„T4å•å¡GPUï¼ˆ16GBæ˜¾å­˜ï¼‰é…ç½®çš„**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/pretrain/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python pretraining.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path bigscience/bloomz-560m \\\\\\n\",\n    \"    --train_file_dir ./data/pretrain \\\\\\n\",\n    \"    --validation_file_dir ./data/pretrain \\\\\\n\",\n    \"    --per_device_train_batch_size 3 \\\\\\n\",\n    \"    --per_device_eval_batch_size 3 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --seed 42 \\\\\\n\",\n    \"    --fp16 \\\\\\n\",\n    \"    --max_train_samples 20000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-4 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.01 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --gradient_accumulation_steps 1 \\\\\\n\",\n    \"    --preprocessing_num_workers 1 \\\\\\n\",\n    \"    --block_size 128 \\\\\\n\",\n    \"    --group_by_length True \\\\\\n\",\n    \"    --output_dir outputs-pt-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-pt-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"æ¨¡å‹è®­ç»ƒç»“æœï¼š\\n\",\n    \"- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\\n\",\n    \"- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-pt/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-pt/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Stage1 å¢é‡é¢„è®­ç»ƒå®Œæˆã€‚\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T13:56:17.081153Z\",\n     \"start_time\": \"2023-06-15T13:56:17.032821Z\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 2: Supervised FineTuning\\n\",\n    \"\\n\",\n    \"ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾ï¼Œå¹¶æ³¨å…¥é¢†åŸŸçŸ¥è¯†\\n\",\n    \"\\n\",\n    \"| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### è¯´æ˜ï¼š\\n\",\n    \"ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\\n\",\n    \"\\n\",\n    \"1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m` æˆ–è€… Stage1å¾—åˆ°çš„é¢„è®­ç»ƒæ¨¡å‹\\n\",\n    \"2. æ•°æ®é›†ï¼šSFTé˜¶æ®µä½¿ç”¨çš„æ˜¯ä½¿ç”¨çš„æ˜¯Belleçš„1åƒæ¡æŠ½æ ·æ•°æ®ï¼Œä½äº`data/finetune`æ–‡ä»¶å¤¹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage2 å’±ä»¬å¼€å§‹å§\\n\",\n    \"\\n\",\n    \"è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\\n\",\n    \"\\n\",\n    \"1. ç¡®è®¤è®­ç»ƒé›†\\n\",\n    \"2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\\n\",\n    \"\\n\",\n    \"è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\\n\",\n    \"1. å¯¼å…¥ä¾èµ–åŒ…\\n\",\n    \"2. è®¾ç½®å‚æ•°\\n\",\n    \"3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\\n\",\n    \"4. åŠ è½½æ¨¡å‹å’Œtokenizer\\n\",\n    \"5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\\n\",\n    \"6. æŸ¥çœ‹è®­ç»ƒç»“æœ\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T13:58:38.966506Z\",\n     \"start_time\": \"2023-06-15T13:58:38.778132Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/finetune\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python supervised_finetuning.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path merged-pt \\\\\\n\",\n    \"    --train_file_dir ./data/finetune \\\\\\n\",\n    \"    --validation_file_dir ./data/finetune \\\\\\n\",\n    \"    --per_device_train_batch_size 4 \\\\\\n\",\n    \"    --per_device_eval_batch_size 4 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --fp16 \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-5 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.05 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --gradient_accumulation_steps 1 \\\\\\n\",\n    \"    --preprocessing_num_workers 1 \\\\\\n\",\n    \"    --output_dir outputs-sft-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-sft-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"æ¨¡å‹è®­ç»ƒç»“æœï¼š\\n\",\n    \"- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\\n\",\n    \"- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-sft/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-sft/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage2 SFTè®­ç»ƒå®Œæˆã€‚\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T14:07:40.752635Z\",\n     \"start_time\": \"2023-06-15T14:07:40.731186Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 3: DPO(Direct Preference Optimization)\\n\",\n    \"\\n\",\n    \"ç¬¬ä¸‰é˜¶æ®µï¼šDPO(Direct Preference Optimization)ç›´æ¥åå¥½ä¼˜åŒ–ï¼ŒDPOé€šè¿‡ç›´æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹æ¥å®ç°å¯¹å…¶è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶ï¼Œè€Œæ— éœ€ä½¿ç”¨å¤æ‚çš„å¼ºåŒ–å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥æœ‰æ•ˆå­¦ä¹ åˆ°äººç±»åå¥½ï¼ŒDPOç›¸è¾ƒäºRLHFæ›´å®¹æ˜“å®ç°ä¸”æ˜“äºè®­ç»ƒï¼Œæ•ˆæœæ›´å¥½\\n\",\n    \"\\n\",\n    \"| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### è¯´æ˜ï¼š\\n\",\n    \"ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\\n\",\n    \"\\n\",\n    \"1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m` æˆ–è€… Stage2å¾—åˆ°çš„SFTæ¨¡å‹\\n\",\n    \"2. æ•°æ®é›†ï¼šDPOé˜¶æ®µä½¿ç”¨çš„æ˜¯åŒ»ç–—rewardæ•°æ®ï¼ŒæŠ½æ ·äº†500æ¡ï¼Œä½äº`data/reward`æ–‡ä»¶å¤¹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage3 å’±ä»¬å¼€å§‹å§\\n\",\n    \"\\n\",\n    \"è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\\n\",\n    \"\\n\",\n    \"1. ç¡®è®¤è®­ç»ƒé›†\\n\",\n    \"2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\\n\",\n    \"\\n\",\n    \"è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\\n\",\n    \"1. å¯¼å…¥ä¾èµ–åŒ…\\n\",\n    \"2. è®¾ç½®å‚æ•°\\n\",\n    \"3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\\n\",\n    \"4. åŠ è½½æ¨¡å‹å’Œtokenizer\\n\",\n    \"5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\\n\",\n    \"6. æŸ¥çœ‹è®­ç»ƒç»“æœ\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/reward/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python dpo_training.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path ./merged-sft \\\\\\n\",\n    \"    --train_file_dir ./data/reward \\\\\\n\",\n    \"    --validation_file_dir ./data/reward \\\\\\n\",\n    \"    --per_device_train_batch_size 3 \\\\\\n\",\n    \"    --per_device_eval_batch_size 1 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --max_eval_samples 500 \\\\\\n\",\n    \"    --max_steps 100 \\\\\\n\",\n    \"    --eval_steps 10 \\\\\\n\",\n    \"    --save_steps 50 \\\\\\n\",\n    \"    --max_source_length 256 \\\\\\n\",\n    \"    --max_target_length 256 \\\\\\n\",\n    \"    --output_dir outputs-dpo-v1 \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --fp16 True \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --remove_unused_columns False \\\\\\n\",\n    \"    --gradient_checkpointing True \\\\\\n\",\n    \"    --cache_dir ./cache\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-dpo-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"æ¨¡å‹è®­ç»ƒç»“æœï¼š\\n\",\n    \"- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\\n\",\n    \"- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-dpo/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-dpo/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage3 åå¥½å»ºæ¨¡ç¬¬ä¸€æ¬¡è®­ç»ƒå®Œæˆã€‚\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"**è‡³æ­¤ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæµç¨‹æ¼”ç¤ºå®Œæˆã€‚**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-26T12:34:29.658428Z\",\n     \"start_time\": \"2023-06-26T12:34:29.620609Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Test\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-26T12:35:00.864463Z\",\n     \"start_time\": \"2023-06-26T12:34:47.802087Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python inference.py --model_type bloom --base_model merged-dpo\\n\",\n    \"# æˆ–åœ¨shellä¸­è¿è¡Œ\\n\",\n    \"# python inference.py --model_type bloom --base_model merged-dpo --interactive\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Input:ä»‹ç»ä¸‹å—äº¬\\n\",\n    \"Response:  å—äº¬å¸‚ä½äºæ±Ÿè‹çœè¥¿å—éƒ¨ï¼Œæ˜¯å…¨å›½é¦–æ‰¹å†å²æ–‡åŒ–ååŸã€å›½å®¶ä¸­å¿ƒåŸå¸‚å’Œè‡ªç”±è´¸æ˜“è¯•éªŒåŒºã€‚\\n\",\n    \"\\n\",\n    \"å®Œã€‚\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.13\"\n  },\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "run_training_ppo_pipeline.ipynb",
          "type": "blob",
          "size": 25.34765625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Training Pipeline\\n\",\n    \"[run_training_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"# Stage 1: Continue Pretraining\\n\",\n    \"\\n\",\n    \"ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æœ¬æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒGPTæ¨¡å‹ï¼Œä»¥é€‚é…é¢†åŸŸæ•°æ®åˆ†å¸ƒ\\n\",\n    \"\\n\",\n    \"æ³¨æ„ï¼š\\n\",\n    \"1. æ­¤é˜¶æ®µæ˜¯å¯é€‰çš„ï¼Œå¦‚æœä½ æ²¡æœ‰æµ·é‡é¢†åŸŸæ–‡æœ¬ï¼Œå¯ä»¥è·³è¿‡æ­¤é˜¶æ®µï¼Œç›´æ¥è¿›è¡ŒSFTé˜¶æ®µçš„æœ‰ç›‘ç£å¾®è°ƒ\\n\",\n    \"2. æˆ‘å®éªŒå‘ç°ï¼šåšé¢†åŸŸçŸ¥è¯†æ³¨å…¥ï¼ŒSFTæ¯”PTæ›´é«˜æ•ˆï¼Œä¹Ÿå¯ä»¥è·³è¿‡PTé˜¶æ®µ\\n\",\n    \"\\n\",\n    \"| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### è¯´æ˜ï¼š\\n\",\n    \"ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\\n\",\n    \"\\n\",\n    \"1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m`\\n\",\n    \"2. æ•°æ®é›†ï¼šPTé˜¶æ®µä½¿ç”¨çš„æ˜¯ä¸­æ–‡å¤©é¾™å…«éƒ¨å°è¯´éƒ¨åˆ†æ–‡æœ¬å’Œè‹±æ–‡ä¹¦ç±éƒ¨åˆ†æ–‡æœ¬ï¼Œä½äº`data/pretrain`æ–‡ä»¶å¤¹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## é…ç½®è¿è¡Œç¯å¢ƒ\\n\",\n    \"\\n\",\n    \"æœ¬åœ°æ‰§è¡Œå¯æ³¨é‡Šä»¥ä¸‹é…ç½®ç¯å¢ƒçš„å‘½ä»¤ï¼Œcolabæ‰§è¡Œè¦æ‰“å¼€æ³¨é‡Šï¼Œç”¨äºé…ç½®ç¯å¢ƒ\\n\",\n    \"\\n\",\n    \"colabå»ºè®®ä½¿ç”¨T4 GPUè®­ç»ƒï¼Œè®¾ç½®æ–¹å¼ï¼š`ä»£ç æ‰§è¡Œç¨‹åº -> æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ -> è¿è¡Œæ—¶ç±»å‹ï¼šPython3ï¼Œç¡¬ä»¶åŠ é€Ÿå™¨ï¼šGPUï¼ŒGPUç±»å‹ï¼šT4 -> ä¿å­˜`\\n\",\n    \"\\n\",\n    \"æ­¥éª¤ï¼š\\n\",\n    \"1. ä¸‹è½½æœ€æ–°ä»£ç åˆ°æœ¬åœ°\\n\",\n    \"2. å®‰è£…ä¾èµ–åŒ…\\n\",\n    \"\\n\",\n    \"ä¾èµ–åŒ…å¦‚ä¸‹ï¼Œä¿è¯æœ€æ–°ç‰ˆæœ¬ï¼š\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"loguru\\n\",\n    \"transformers\\n\",\n    \"sentencepiece\\n\",\n    \"datasets\\n\",\n    \"tensorboard\\n\",\n    \"tqdm\\n\",\n    \"peft\\n\",\n    \"trl\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\\n\",\n    \"%cd MedicalGPT\\n\",\n    \"%ls\\n\",\n    \"!pip install -r requirements.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Stage1 å’±ä»¬å¼€å§‹å§\\n\",\n    \"\\n\",\n    \"è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\\n\",\n    \"\\n\",\n    \"1. ç¡®è®¤è®­ç»ƒé›†\\n\",\n    \"2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\\n\",\n    \"\\n\",\n    \"è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\\n\",\n    \"1. å¯¼å…¥ä¾èµ–åŒ…\\n\",\n    \"2. è®¾ç½®å‚æ•°\\n\",\n    \"3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\\n\",\n    \"4. åŠ è½½æ¨¡å‹å’Œtokenizer\\n\",\n    \"5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\\n\",\n    \"6. æŸ¥çœ‹è®­ç»ƒç»“æœ\\n\",\n    \"\\n\",\n    \"**ä»¥ä¸‹å‚æ•°å¯ä»¥æ ¹æ®ä½ çš„GPUå®é™…æƒ…å†µä¿®æ”¹ï¼Œå½“å‰å‚æ•°æ˜¯æ ¹æ®Colabçš„T4å•å¡GPUï¼ˆ16GBæ˜¾å­˜ï¼‰é…ç½®çš„**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/pretrain/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python pretraining.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path bigscience/bloomz-560m \\\\\\n\",\n    \"    --train_file_dir ./data/pretrain \\\\\\n\",\n    \"    --validation_file_dir ./data/pretrain \\\\\\n\",\n    \"    --per_device_train_batch_size 3 \\\\\\n\",\n    \"    --per_device_eval_batch_size 3 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --seed 42 \\\\\\n\",\n    \"    --fp16 \\\\\\n\",\n    \"    --max_train_samples 20000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-4 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.01 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --gradient_accumulation_steps 1 \\\\\\n\",\n    \"    --preprocessing_num_workers 1 \\\\\\n\",\n    \"    --block_size 128 \\\\\\n\",\n    \"    --group_by_length True \\\\\\n\",\n    \"    --output_dir outputs-pt-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-pt-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"æ¨¡å‹è®­ç»ƒç»“æœï¼š\\n\",\n    \"- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\\n\",\n    \"- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-pt/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-pt/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Stage1 å¢é‡é¢„è®­ç»ƒå®Œæˆã€‚\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T13:56:17.081153Z\",\n     \"start_time\": \"2023-06-15T13:56:17.032821Z\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 2: Supervised FineTuning\\n\",\n    \"\\n\",\n    \"ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾ï¼Œå¹¶æ³¨å…¥é¢†åŸŸçŸ¥è¯†\\n\",\n    \"\\n\",\n    \"| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### è¯´æ˜ï¼š\\n\",\n    \"ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\\n\",\n    \"\\n\",\n    \"1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m` æˆ–è€… Stage1å¾—åˆ°çš„é¢„è®­ç»ƒæ¨¡å‹\\n\",\n    \"2. æ•°æ®é›†ï¼šSFTé˜¶æ®µä½¿ç”¨çš„æ˜¯ä½¿ç”¨çš„æ˜¯Belleçš„1åƒæ¡æŠ½æ ·æ•°æ®ï¼Œä½äº`data/finetune`æ–‡ä»¶å¤¹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage2 å’±ä»¬å¼€å§‹å§\\n\",\n    \"\\n\",\n    \"è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\\n\",\n    \"\\n\",\n    \"1. ç¡®è®¤è®­ç»ƒé›†\\n\",\n    \"2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\\n\",\n    \"\\n\",\n    \"è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\\n\",\n    \"1. å¯¼å…¥ä¾èµ–åŒ…\\n\",\n    \"2. è®¾ç½®å‚æ•°\\n\",\n    \"3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\\n\",\n    \"4. åŠ è½½æ¨¡å‹å’Œtokenizer\\n\",\n    \"5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\\n\",\n    \"6. æŸ¥çœ‹è®­ç»ƒç»“æœ\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T13:58:38.966506Z\",\n     \"start_time\": \"2023-06-15T13:58:38.778132Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/finetune\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python supervised_finetuning.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path merged-pt \\\\\\n\",\n    \"    --train_file_dir ./data/finetune \\\\\\n\",\n    \"    --validation_file_dir ./data/finetune \\\\\\n\",\n    \"    --per_device_train_batch_size 4 \\\\\\n\",\n    \"    --per_device_eval_batch_size 4 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --fp16 \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-5 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.05 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --gradient_accumulation_steps 1 \\\\\\n\",\n    \"    --preprocessing_num_workers 1 \\\\\\n\",\n    \"    --output_dir outputs-sft-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-sft-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"æ¨¡å‹è®­ç»ƒç»“æœï¼š\\n\",\n    \"- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\\n\",\n    \"- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir merged-sft/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-sft/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-sft/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage2 SFTè®­ç»ƒå®Œæˆã€‚\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T14:07:40.752635Z\",\n     \"start_time\": \"2023-06-15T14:07:40.731186Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 3: Reward Modeling\\n\",\n    \"\\n\",\n    \"ç¬¬ä¸‰é˜¶æ®µï¼šRM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ï¼Œä¸»è¦æ˜¯\\\"HHH\\\"åŸåˆ™ï¼Œå…·ä½“æ˜¯\\\"helpful, honest, harmless\\\"\\n\",\n    \"\\n\",\n    \"| Stage 3: Reward Modeling        |  [reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/reward_modeling.py) | [run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rm.sh)    |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### è¯´æ˜ï¼š\\n\",\n    \"ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\\n\",\n    \"\\n\",\n    \"1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m` æˆ–è€… Stage2å¾—åˆ°çš„SFTæ¨¡å‹\\n\",\n    \"2. æ•°æ®é›†ï¼šRMé˜¶æ®µä½¿ç”¨çš„æ˜¯åŒ»ç–—rewardæ•°æ®ï¼ŒæŠ½æ ·äº†500æ¡ï¼Œä½äº`data/reward`æ–‡ä»¶å¤¹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage3 å’±ä»¬å¼€å§‹å§\\n\",\n    \"\\n\",\n    \"è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\\n\",\n    \"\\n\",\n    \"1. ç¡®è®¤è®­ç»ƒé›†\\n\",\n    \"2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\\n\",\n    \"\\n\",\n    \"è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\\n\",\n    \"1. å¯¼å…¥ä¾èµ–åŒ…\\n\",\n    \"2. è®¾ç½®å‚æ•°\\n\",\n    \"3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\\n\",\n    \"4. åŠ è½½æ¨¡å‹å’Œtokenizer\\n\",\n    \"5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\\n\",\n    \"6. æŸ¥çœ‹è®­ç»ƒç»“æœ\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/reward/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python reward_modeling.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path merged-sft \\\\\\n\",\n    \"    --train_file_dir ./data/reward \\\\\\n\",\n    \"    --validation_file_dir ./data/reward \\\\\\n\",\n    \"    --per_device_train_batch_size 3 \\\\\\n\",\n    \"    --per_device_eval_batch_size 1 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --seed 42 \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-5 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.001 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --max_source_length 256 \\\\\\n\",\n    \"    --max_target_length 256 \\\\\\n\",\n    \"    --output_dir outputs-rm-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float32 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --remove_unused_columns False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-rm-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"æ¨¡å‹è®­ç»ƒç»“æœï¼š\\n\",\n    \"- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\\n\",\n    \"- æ—¥å¿—ä¿å­˜åœ¨`output_dir/runs`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-sft --lora_model outputs-rm-v1 --output_dir merged-rm/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-rm/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-rm/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage3 å¥–åŠ±å»ºæ¨¡ç¬¬ä¸€æ¬¡è®­ç»ƒå®Œæˆã€‚\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T14:12:09.472414Z\",\n     \"start_time\": \"2023-06-15T14:12:09.464881Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 4: Reinforcement Learning Training\\n\",\n    \"\\n\",\n    \"ç¬¬å››é˜¶æ®µï¼šRL(Reinforcement Learning)åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬\\n\",\n    \"\\n\",\n    \"| Stage 4: Reinforcement Learning |  [rl_training.py](https://github.com/shibing624/MedicalGPT/blob/main/rl_training.py) | [run_rl.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rl.sh)    |\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### è¯´æ˜ï¼š\\n\",\n    \"ä»¥ä¸‹ notebook/colab ä»£ç ä¸ºäº†å¿«é€ŸéªŒè¯è®­ç»ƒä»£ç å¯ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å°sizeçš„ç”Ÿæˆæ¨¡å‹ã€å¥–åŠ±æ¨¡å‹å’Œå°æ ·æœ¬æ•°æ®é›†ï¼Œå®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\\n\",\n    \"\\n\",\n    \"1. ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯Bloomçš„`bigscience/bloomz-560m` æˆ–è€… Stage2å¾—åˆ°çš„SFTæ¨¡å‹\\n\",\n    \"2. å¥–åŠ±æ¨¡å‹ï¼šä½¿ç”¨çš„æ˜¯`OpenAssistant/reward-model-deberta-v3-large-v2` æˆ–è€… Stage3å¾—åˆ°çš„BERTç±»æˆ–è€…GPTç±»å¥–åŠ±æ¨¡å‹\\n\",\n    \"3. æ•°æ®é›†ï¼šRLé˜¶æ®µçš„æ•°æ®å¯ä»¥å¤ç”¨SFTçš„æ•°æ®é›†ï¼Œä½¿ç”¨çš„æ˜¯Belleçš„1åƒæ¡æŠ½æ ·æ•°æ®ï¼Œä½äº`data/finetune`æ–‡ä»¶å¤¹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage4 å’±ä»¬å¼€å§‹å§\\n\",\n    \"\\n\",\n    \"è®­ç»ƒæ­¥éª¤å¦‚ä¸‹ï¼š\\n\",\n    \"\\n\",\n    \"1. ç¡®è®¤è®­ç»ƒé›†\\n\",\n    \"2. æ‰§è¡Œè®­ç»ƒè„šæœ¬\\n\",\n    \"\\n\",\n    \"è®­ç»ƒè„šæœ¬çš„æ‰§è¡Œé€»è¾‘å¦‚ä¸‹ï¼š\\n\",\n    \"1. å¯¼å…¥ä¾èµ–åŒ…\\n\",\n    \"2. è®¾ç½®å‚æ•°\\n\",\n    \"3. å®šä¹‰å„å‡½æ•°å¹¶åŠ è½½è®­ç»ƒé›†\\n\",\n    \"4. åŠ è½½ç”Ÿæˆæ¨¡å‹å’Œtokenizerï¼ŒåŠ è½½å¥–åŠ±æ¨¡å‹å’Œå…¶tokenizer\\n\",\n    \"5. å¼€å§‹è®­ç»ƒå¹¶è¯„ä¼°\\n\",\n    \"6. æŸ¥çœ‹è®­ç»ƒç»“æœ\\n\",\n    \"\\n\",\n    \"ä»¥ä¸‹å‚æ•°å¯ä»¥æ ¹æ®ä½ çš„GPUå®é™…æƒ…å†µä¿®æ”¹ï¼Œå½“å‰å‚æ•°æ˜¯æ ¹æ®Colabçš„T4å•å¡GPUï¼ˆ16GBæ˜¾å­˜ï¼‰é…ç½®çš„ã€‚\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/finetune/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"! CUDA_VISIBLE_DEVICES=0 python ppo_training.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path ./merged-sft \\\\\\n\",\n    \"    --reward_model_name_or_path ./merged-rm \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --train_file_dir ./data/finetune \\\\\\n\",\n    \"    --validation_file_dir ./data/finetune \\\\\\n\",\n    \"    --batch_size 4 \\\\\\n\",\n    \"    --max_source_length 256 \\\\\\n\",\n    \"    --max_target_length 256 \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --max_steps 64 \\\\\\n\",\n    \"    --learning_rate 1e-5 \\\\\\n\",\n    \"    --save_steps 50 \\\\\\n\",\n    \"    --output_dir outputs-rl-v1 \\\\\\n\",\n    \"    --early_stopping True \\\\\\n\",\n    \"    --target_kl 0.1 \\\\\\n\",\n    \"    --reward_baseline 0.0\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-rl-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"æ¨¡å‹è®­ç»ƒç»“æœï¼š\\n\",\n    \"- ä½¿ç”¨loraè®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä¿å­˜çš„loraæƒé‡æ˜¯`adapter_model.bin`, loraé…ç½®æ–‡ä»¶æ˜¯`adapter_config.json`ï¼Œåˆå¹¶åˆ°base modelçš„æ–¹æ³•è§`merge_peft_adapter.py`\\n\",\n    \"- æ—¥å¿—ä¿å­˜åœ¨`output_dir/trl`ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨tensorboardæŸ¥çœ‹ï¼Œå¯åŠ¨tensorboardæ–¹å¼å¦‚ä¸‹ï¼š`tensorboard --logdir output_dir/trl --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"loraæ¨¡å‹æƒé‡åˆå¹¶åˆ°base modelï¼Œåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨`--output_dir`ç›®å½•ä¸‹ï¼Œåˆå¹¶æ–¹æ³•å¦‚ä¸‹ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-sft --lora_model outputs-rl-v1 --output_dir merged-ppo/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-ppo/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-ppo/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage4 RLç¬¬ä¸€æ¬¡è®­ç»ƒå®Œæˆã€‚\\n\",\n    \"\\n\",\n    \"**è‡³æ­¤ä¸€ä¸ªå®Œæ•´çš„4é˜¶æ®µè®­ç»ƒæµç¨‹æ¼”ç¤ºå®Œæˆã€‚**\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"å®é™…æ“ä½œä¸­Stage3å’ŒStage4å¯ä»¥åå¤å¤šæ¬¡ï¼Œç›´åˆ°RLå¾—åˆ°çš„æœ€åæ¨¡å‹æ»¡è¶³è¯„ä¼°è¦æ±‚ã€‚\\n\",\n    \"\\n\",\n    \"RLHFè¿‡ç¨‹å¯ä»¥æŠŠSFTæ¨¡å‹å½“æˆä¸€ä¸ªåˆå§‹åŒ–æ¨¡å‹ï¼ŒRMæ¨¡å‹å½“åšæŒ‡å¯¼è€å¸ˆï¼Œä½¿ç”¨RL(PPO)è°ƒæ•™SFTæ¨¡å‹ç”ŸæˆæŒ‡å¯¼è€å¸ˆæœ€æ»¡æ„çš„ç»“æœï¼Œå¦‚æœå°å­¦è€å¸ˆæ»¡æ„äº†ï¼Œæˆ‘ä»¬å°±å†è®­ç»ƒä¸€ä¸ªä¸­å­¦è€å¸ˆï¼Œç»§ç»­æŒ‡å¯¼ï¼Œä¸­å­¦è€å¸ˆæ»¡æ„äº†ï¼Œå°±è®­ç»ƒä¸€ä¸ªå¤§å­¦è€å¸ˆï¼Œè¿™æ ·ä¸æ–­è¿­ä»£ï¼Œä½¿å¾—ç”Ÿæˆæ¨¡å‹çš„è´¨é‡è¾¾åˆ°ç”šè‡³è¶…è¿‡äººå·¥æ’°å†™çš„å¤©èŠ±æ¿ã€‚\\n\",\n    \"\\n\",\n    \"RLHFè®­ç»ƒä¸æ˜“ï¼Œæ­¤é¡¹ç›®æä¾›ç»™å¤§å®¶ä¸€ç§å®ç°çš„æ–¹æ³•å’Œå‚è€ƒï¼Œå¸Œæœ›æŠ›ç –å¼•ç‰ï¼Œå…±åŒä¿ƒè¿›ä¸­æ–‡å¼€æºLLMå‘å±•ã€‚\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-26T12:34:29.658428Z\",\n     \"start_time\": \"2023-06-26T12:34:29.620609Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Test\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-26T12:35:00.864463Z\",\n     \"start_time\": \"2023-06-26T12:34:47.802087Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python inference.py --model_type bloom --base_model merged-ppo\\n\",\n    \"# æˆ–åœ¨shellä¸­è¿è¡Œ\\n\",\n    \"# !python inference.py --model_type bloom --base_model merged-ppo --interactive\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Input:ä»‹ç»ä¸‹å—äº¬\\n\",\n    \"Response:  å—äº¬å¸‚ä½äºæ±Ÿè‹çœè¥¿å—éƒ¨ï¼Œæ˜¯å…¨å›½é¦–æ‰¹å†å²æ–‡åŒ–ååŸã€å›½å®¶ä¸­å¿ƒåŸå¸‚å’Œè‡ªç”±è´¸æ˜“è¯•éªŒåŒºã€‚\\n\",\n    \"\\n\",\n    \"å®Œã€‚\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.13\"\n  },\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "supervised_finetuning.py",
          "type": "blob",
          "size": 44.24609375,
          "content": "# -*- coding: utf-8 -*-\n# Copyright 2023 XuMing(xuming624@qq.com) and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for causal language modeling (GPT, LLaMA, Bloom, ...) on a json file or a dataset.\n\npart of code is modified from https://github.com/shibing624/textgen\n\"\"\"\n\nimport math\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom types import MethodType\nfrom typing import Literal, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoConfig,\n    BloomForCausalLM,\n    AutoModel,\n    AutoModelForCausalLM,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n    Trainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n    BitsAndBytesConfig,\n    DataCollatorForSeq2Seq,\n)\nfrom transformers.models.llama.modeling_llama import (\n    LlamaAttention,\n    apply_rotary_pos_emb,\n    repeat_kv,\n    LlamaFlashAttention2,\n    Cache\n)\nfrom transformers.trainer import TRAINING_ARGS_NAME\nfrom transformers.trainer_pt_utils import LabelSmoother\nfrom transformers.utils.versions import require_version\n\ntry:\n    from transformers.integrations import is_deepspeed_zero3_enabled\nexcept ImportError:  # https://github.com/huggingface/transformers/releases/tag/v4.33.1\n    from transformers.deepspeed import is_deepspeed_zero3_enabled\n\nis_flash_attn_2_available = False\ntry:\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import pad_input, unpad_input\n\n    is_flash_attn_2_available = True\nexcept ImportError:\n    is_flash_attn_2_available = False\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    model_revision: Optional[str] = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    hf_hub_token: Optional[str] = field(default=None, metadata={\"help\": \"Auth token to log in with Hugging Face Hub.\"})\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=\"float16\",\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n    rope_scaling: Optional[Literal[\"linear\", \"dynamic\"]] = field(\n        default=None,\n        metadata={\"help\": \"Adopt scaled rotary positional embeddings.\"}\n    )\n    flash_attn: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enable FlashAttention-2 for faster training.\"}\n    )\n    shift_attn: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enable shifted sparse attention (S^2-Attn) proposed by LongLoRA.\"}\n    )\n    neft_alpha: Optional[float] = field(\n        default=0,\n        metadata={\"help\": \"The alpha parameter to control the noise magnitude in NEFTune. value can be 5.\"}\n    )\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\n                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n                    MODEL_CLASSES.keys()))\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\n@dataclass\nclass DataArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The train jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    ignore_pad_token_for_loss: bool = field(\n        default=True,\n        metadata={\"help\": \"If only pad tokens should be ignored. This assumes that `config.pad_token_id` is defined.\"},\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n\n    def __post_init__(self):\n        if self.max_train_samples is not None and 0 < self.max_train_samples <= 1000:\n            logger.warning(\"You may set max_train_samples = -1 to run all samples in production.\")\n\n\n@dataclass\nclass ScriptArguments:\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    train_on_inputs: bool = field(default=False, metadata={\"help\": \"Whether to train on inputs\"})\n    target_modules: Optional[str] = field(default=\"all\")\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=32.0)\n    modules_to_save: Optional[str] = field(default=None)\n    peft_path: Optional[str] = field(default=None, metadata={\"help\": \"The path to the peft model\"})\n    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})\n    model_max_length: int = field(\n        default=512,\n        metadata={\"help\": \"Maximum model context length. suggest: 8192 * 4, 8192 * 2, 8192, 4096, 2048, 1024, 512\"}\n    )\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The prompt template name.\"})\n\n    def __post_init__(self):\n        if self.model_max_length < 60:\n            raise ValueError(\"You must specify a valid model_max_length >= 60 to run training\")\n\n\nclass SavePeftModelTrainer(Trainer):\n    \"\"\"\n    Trainer for lora models\n    \"\"\"\n\n    def save_model(self, output_dir=None, _internal_call=False):\n        \"\"\"Save the LoRA model.\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n        self.model.save_pretrained(output_dir)\n\n\ndef save_model(model, tokenizer, args):\n    \"\"\"Save the model and the tokenizer.\"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Take care of distributed/parallel training\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef save_model_zero3(model, tokenizer, args, trainer):\n    \"\"\"Save the model for deepspeed zero3.\n    refer https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train_lora.py#L209\n    \"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n    state_dict_zero3 = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(args.output_dir, state_dict=state_dict_zero3)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'output_layer' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\n# Modified from: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\ndef llama_torch_attn_forward(\n        self: \"LlamaAttention\",\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[\"Cache\"] = None,\n        output_attentions: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states)\n    key_states = self.k_proj(hidden_states)\n    value_states = self.v_proj(hidden_states)\n\n    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n    past_key_value = getattr(self, \"past_key_value\", past_key_value)\n    cos, sin = self.rotary_emb(value_states, position_ids)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n    if past_key_value is not None:\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    if getattr(self.config, \"group_size_ratio\", None) and self.training:  # shift\n        groupsz = int(q_len * getattr(self.config, \"group_size_ratio\"))\n        assert q_len % groupsz == 0, \"q_len {} should be divisible by group size {}.\".format(q_len, groupsz)\n        num_groups = q_len // groupsz\n\n        def shift(state: torch.Tensor) -> torch.Tensor:\n            state = state.transpose(1, 2)  # output: (bsz, seq_len, n_heads, head_dim)\n            state = torch.cat(\n                (state[:, :, : self.num_heads // 2], state[:, :, self.num_heads // 2:].roll(-groupsz // 2, dims=1)),\n                dim=2,\n            )\n            return state.reshape(bsz * num_groups, groupsz, self.num_heads, self.head_dim).transpose(1, 2)\n\n        query_states, key_states, value_states = shift(query_states), shift(key_states), shift(value_states)\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, :, :groupsz, :groupsz].repeat(num_groups, 1, 1, 1)\n\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n    if attention_mask is not None:\n        attn_weights = attn_weights + attention_mask\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n    attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n    attn_output = torch.matmul(attn_weights, value_states)  # (bsz, :, seq_len, :) or (bsz*n_group, :, groupsz, :)\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    if getattr(self.config, \"group_size_ratio\", None) and self.training:  # shift back\n        attn_output.reshape(bsz, q_len, self.num_heads, self.head_dim)\n        attn_output = torch.cat(\n            (\n                attn_output[:, :, : self.num_heads // 2],\n                attn_output[:, :, self.num_heads // 2:].roll(groupsz // 2, dims=1),\n            )\n        )\n\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\n# Modified from: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\ndef llama_flash_attn_forward(\n        self: \"LlamaFlashAttention2\",\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[\"Cache\"] = None,\n        output_attentions: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    # LlamaFlashAttention2 attention does not support output_attentions\n    output_attentions = False\n\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states)\n    key_states = self.k_proj(hidden_states)\n    value_states = self.v_proj(hidden_states)\n\n    # FlashAttention requires the input to have the shape (bsz, seq_len, n_heads, head_dim)\n    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n    cos, sin = self.rotary_emb(value_states, position_ids)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n    past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n    if past_key_value is not None:\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    query_states = query_states.transpose(1, 2)  # (bsz, seq_len, n_heads, head_dim)\n    key_states = key_states.transpose(1, 2)  # (bsz, seq_len, n_heads, head_dim)\n    value_states = value_states.transpose(1, 2)  # (bsz, seq_len, n_heads, head_dim)\n\n    dropout_rate = self.attention_dropout if self.training else 0.0\n\n    input_dtype = query_states.dtype\n    if input_dtype == torch.float32:\n        if torch.is_autocast_enabled():\n            target_dtype = torch.get_autocast_gpu_dtype()\n        elif hasattr(self.config, \"_pre_quantization_dtype\"):\n            target_dtype = self.config._pre_quantization_dtype\n        else:\n            target_dtype = self.q_proj.weight.dtype\n\n        logger.warning(\"The input hidden states seems to be silently casted in float32.\")\n        query_states = query_states.to(target_dtype)\n        key_states = key_states.to(target_dtype)\n        value_states = value_states.to(target_dtype)\n\n    if getattr(self.config, \"group_size_ratio\", None) and self.training:  # shift\n        groupsz = int(q_len * getattr(self.config, \"group_size_ratio\"))\n        assert q_len % groupsz == 0, \"q_len {} should be divisible by group size {}.\".format(q_len, groupsz)\n        num_groups = q_len // groupsz\n\n        def shift(state: torch.Tensor) -> torch.Tensor:\n            state = torch.cat(\n                (state[:, :, : self.num_heads // 2], state[:, :, self.num_heads // 2:].roll(-groupsz // 2, dims=1)),\n                dim=2,\n            )\n            return state.reshape(bsz * num_groups, groupsz, self.num_heads, self.head_dim)\n\n        query_states, key_states, value_states = shift(query_states), shift(key_states), shift(value_states)\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, :, :groupsz, :groupsz].repeat(num_groups, 1, 1, 1)\n\n    attn_output: torch.Tensor = self._flash_attention_forward(\n        query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate\n    )\n\n    if getattr(self.config, \"group_size_ratio\", None) and self.training:  # shift back\n        attn_output.reshape(bsz, q_len, self.num_heads, self.head_dim)\n        attn_output = torch.cat(\n            (\n                attn_output[:, :, : self.num_heads // 2],\n                attn_output[:, :, self.num_heads // 2:].roll(groupsz // 2, dims=1),\n            )\n        )\n\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef apply_llama_patch() -> None:\n    LlamaAttention.forward = llama_torch_attn_forward\n    LlamaFlashAttention2.forward = llama_flash_attn_forward\n\n\ndef main():\n    parser = HfArgumentParser((ModelArguments, DataArguments, Seq2SeqTrainingArguments, ScriptArguments))\n    model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses()\n\n    logger.info(f\"Model args: {model_args}\")\n    logger.info(f\"Data args: {data_args}\")\n    logger.info(f\"Training args: {training_args}\")\n    logger.info(f\"Script args: {script_args}\")\n    logger.info(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n    # Load tokenizer\n    tokenizer_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"use_fast\": model_args.use_fast_tokenizer,\n        \"trust_remote_code\": model_args.trust_remote_code,\n    }\n    tokenizer_name_or_path = model_args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = model_args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(script_args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    IGNORE_INDEX = LabelSmoother.ignore_index if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n\n    # Get datasets\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            data_args.dataset_name,\n            data_args.dataset_config_name,\n            cache_dir=model_args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            shuffled_train_dataset = raw_datasets[\"train\"].shuffle(seed=42)\n            # Split the shuffled train dataset into training and validation sets\n            split = shuffled_train_dataset.train_test_split(\n                test_size=data_args.validation_split_percentage / 100,\n                seed=42\n            )\n            # Assign the split datasets back to raw_datasets\n            raw_datasets[\"train\"] = split[\"train\"]\n            raw_datasets[\"validation\"] = split[\"test\"]\n    else:\n        # Loading a dataset from local files.\n        data_files = {}\n        if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n            train_data_files = glob(f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {train_data_files}\")\n            data_files[\"train\"] = train_data_files\n        if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n            eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {eval_data_files}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=model_args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            shuffled_train_dataset = raw_datasets[\"train\"].shuffle(seed=42)\n            split = shuffled_train_dataset.train_test_split(\n                test_size=float(data_args.validation_split_percentage / 100),\n                seed=42\n            )\n            raw_datasets[\"train\"] = split[\"train\"]\n            raw_datasets[\"validation\"] = split[\"test\"]\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    max_length = script_args.model_max_length\n\n    def preprocess_function(examples):\n        \"\"\"\n        Preprocessing the datasets.\n            part of code modified from https://github.com/lm-sys/FastChat\n        \"\"\"\n        input_ids_list = []\n        attention_mask_list = []\n        targets_list = []\n        roles = [\"human\", \"gpt\"]\n\n        def get_dialog(examples):\n            system_prompts = examples.get(\"system_prompt\", \"\")\n            for i, source in enumerate(examples['conversations']):\n                system_prompt = \"\"\n                if len(source) < 2:\n                    continue\n                data_role = source[0].get(\"from\", \"\")\n                if data_role == \"system\":\n                    # Skip the first one if it is from system\n                    system_prompt = source[0][\"value\"]\n                    source = source[1:]\n                    data_role = source[0].get(\"from\", \"\")\n                if data_role not in roles or data_role != roles[0]:\n                    # Skip the first one if it is not from human\n                    source = source[1:]\n                if len(source) < 2:\n                    continue\n                messages = []\n                for j, sentence in enumerate(source):\n                    data_role = sentence.get(\"from\", \"\")\n                    if data_role not in roles:\n                        logger.warning(f\"unknown role: {data_role}, {i}. (ignored)\")\n                        break\n                    if data_role == roles[j % 2]:\n                        messages.append(sentence[\"value\"])\n                if len(messages) % 2 != 0:\n                    continue\n                # Convert the list to pairs of elements\n                history_messages = [[messages[k], messages[k + 1]] for k in range(0, len(messages), 2)]\n                if not system_prompt:\n                    system_prompt = system_prompts[i] if system_prompts else \"\"\n                yield prompt_template.get_dialog(history_messages, system_prompt=system_prompt)\n\n        for dialog in get_dialog(examples):\n            input_ids, labels = [], []\n\n            for i in range(len(dialog) // 2):\n                source_ids = tokenizer.encode(text=dialog[2 * i], add_special_tokens=(i == 0))\n                target_ids = tokenizer.encode(text=dialog[2 * i + 1], add_special_tokens=False)\n\n                total_len = len(source_ids) + len(target_ids)\n                max_source_len = int(max_length * (len(source_ids) / total_len))\n                max_target_len = int(max_length * (len(target_ids) / total_len))\n\n                if len(source_ids) > max_source_len:\n                    source_ids = source_ids[:max_source_len]\n                if len(target_ids) > max_target_len - 1:  # eos token\n                    target_ids = target_ids[:max_target_len - 1]\n                if len(source_ids) > 0 and source_ids[0] == tokenizer.eos_token_id:\n                    source_ids = source_ids[1:]\n                if len(target_ids) > 0 and target_ids[-1] == tokenizer.eos_token_id:\n                    target_ids = target_ids[:-1]\n                if len(input_ids) + len(source_ids) + len(target_ids) + 1 > max_length:\n                    break\n\n                input_ids += source_ids + target_ids + [tokenizer.eos_token_id]  # add eos token for each turn\n                if script_args.train_on_inputs:\n                    labels += source_ids + target_ids + [tokenizer.eos_token_id]\n                else:\n                    labels += [IGNORE_INDEX] * len(source_ids) + target_ids + [tokenizer.eos_token_id]\n\n            input_ids_list.append(input_ids)\n            attention_mask_list.append([1] * len(input_ids))\n            targets_list.append(labels)\n\n        return dict(\n            input_ids=input_ids_list,\n            attention_mask=attention_mask_list,\n            labels=targets_list,\n        )\n\n    def filter_empty_labels(example):\n        \"\"\"Remove empty labels dataset.\"\"\"\n        return not all(label == IGNORE_INDEX for label in example[\"labels\"])\n\n    train_dataset = None\n    max_train_samples = 0\n    if training_args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train'].shuffle(seed=42)\n        max_train_samples = len(train_dataset)\n        if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        with training_args.main_process_first(desc=\"Train dataset tokenization\"):\n            train_dataset = train_dataset.shuffle().map(\n                preprocess_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=train_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on train dataset\",\n            )\n            train_dataset = train_dataset.filter(filter_empty_labels, num_proc=data_args.preprocessing_num_workers)\n            logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n            logger.debug(\"Tokenized training example:\")\n            logger.debug(f\"Decode input_ids[0]:\\n{tokenizer.decode(train_dataset[0]['input_ids'])}\")\n            replaced_labels = [label if label != IGNORE_INDEX else tokenizer.pad_token_id\n                               for label in list(train_dataset[0]['labels'])]\n            logger.debug(f\"Decode labels[0]:\\n{tokenizer.decode(replaced_labels)}\")\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if training_args.do_eval:\n        with training_args.main_process_first(desc=\"Eval dataset tokenization\"):\n            if \"validation\" not in raw_datasets:\n                raise ValueError(\"--do_eval requires a validation dataset\")\n            eval_dataset = raw_datasets[\"validation\"]\n            max_eval_samples = len(eval_dataset)\n            if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n                max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n                eval_dataset = eval_dataset.select(range(max_eval_samples))\n            eval_size = len(eval_dataset)\n            logger.debug(f\"Num eval_samples: {eval_size}\")\n            if eval_size > 500:\n                logger.warning(f\"Num eval_samples is large: {eval_size}, \"\n                               f\"training slow, consider reduce it by `--max_eval_samples=50`\")\n            logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n            eval_dataset = eval_dataset.map(\n                preprocess_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=eval_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on validation dataset\",\n            )\n            eval_dataset = eval_dataset.filter(filter_empty_labels, num_proc=data_args.preprocessing_num_workers)\n            logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n            logger.debug(\"Tokenized eval example:\")\n            logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))\n\n    # Load model\n    if model_args.model_name_or_path:\n        torch_dtype = (\n            model_args.torch_dtype\n            if model_args.torch_dtype in [\"auto\", None]\n            else getattr(torch, model_args.torch_dtype)\n        )\n        world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n        ddp = world_size != 1\n        if ddp:\n            model_args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n            training_args.gradient_accumulation_steps = training_args.gradient_accumulation_steps // world_size or 1\n        if script_args.qlora and (len(training_args.fsdp) > 0 or is_deepspeed_zero3_enabled()):\n            logger.warning(\"FSDP and DeepSpeed ZeRO-3 are both currently incompatible with QLoRA.\")\n\n        config_kwargs = {\n            \"trust_remote_code\": model_args.trust_remote_code,\n            \"cache_dir\": model_args.cache_dir,\n            \"revision\": model_args.model_revision,\n            \"token\": model_args.hf_hub_token,\n        }\n        config = config_class.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n\n        # Set RoPE scaling\n        if model_args.rope_scaling is not None:\n            if hasattr(config, \"rope_scaling\"):\n                if model_args.rope_scaling == \"dynamic\":\n                    logger.warning(\n                        \"Dynamic NTK may not work well with fine-tuning. \"\n                        \"See: https://github.com/huggingface/transformers/pull/24653\"\n                    )\n                current_max_length = getattr(config, \"max_position_embeddings\", None)\n                if current_max_length and script_args.model_max_length > current_max_length:\n                    scaling_factor = float(math.ceil(script_args.model_max_length / current_max_length))\n                else:\n                    logger.warning(f\"The model_max_length({script_args.model_max_length}) is smaller than max \"\n                                   f\"length({current_max_length}). Consider increase model_max_length.\")\n                    scaling_factor = 1.0\n\n                setattr(config, \"rope_scaling\", {\"type\": model_args.rope_scaling, \"factor\": scaling_factor})\n                logger.info(\"Using {} scaling strategy and setting scaling factor to {}\".format(\n                    model_args.rope_scaling, scaling_factor\n                ))\n            else:\n                logger.warning(\"Current model does not support RoPE scaling.\")\n\n        # Set FlashAttention-2\n        if model_args.flash_attn:\n            if is_flash_attn_2_available:\n                config_kwargs[\"use_flash_attention_2\"] = True\n                logger.info(\"Using FlashAttention-2 for faster training and inference.\")\n            else:\n                logger.warning(\"FlashAttention-2 is not installed.\")\n        elif model_args.shift_attn and getattr(config, \"model_type\", None) == \"llama\":\n            logger.warning(\"Using `--flash_attn` for faster training in large context length, enable if your GPU\"\n                           \" is RTX3090, RTX4090, A100 or H100.\")\n\n        # Set shifted sparse attention (S^2-Attn)\n        if model_args.shift_attn:\n            if getattr(config, \"model_type\", None) == \"llama\":\n                setattr(config, \"group_size_ratio\", 0.25)\n                apply_llama_patch()\n                logger.info(\"Using shifted sparse attention with group_size_ratio=1/4.\")\n            else:\n                logger.warning(\"Current model does not support shifted sparse attention.\")\n\n        load_in_4bit = model_args.load_in_4bit\n        load_in_8bit = model_args.load_in_8bit\n        if load_in_4bit and load_in_8bit:\n            raise ValueError(\"Error, load_in_4bit and load_in_8bit cannot be set at the same time\")\n        elif load_in_8bit or load_in_4bit:\n            logger.info(f\"Quantizing model, load_in_4bit: {load_in_4bit}, load_in_8bit: {load_in_8bit}\")\n            if is_deepspeed_zero3_enabled():\n                raise ValueError(\"DeepSpeed ZeRO-3 is incompatible with quantization.\")\n            if load_in_8bit:\n                config_kwargs['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n            elif load_in_4bit:\n                if script_args.qlora:\n                    config_kwargs['quantization_config'] = BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_use_double_quant=True,\n                        bnb_4bit_quant_type=\"nf4\",\n                        bnb_4bit_compute_dtype=torch_dtype,\n                    )\n                else:\n                    config_kwargs['quantization_config'] = BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_compute_dtype=torch_dtype,\n                    )\n\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            config=config,\n            torch_dtype=torch_dtype,\n            device_map=model_args.device_map,\n            **config_kwargs,\n        )\n\n        # Fix ChatGLM2 and ChatGLM3 and internlm2 LM head\n        if getattr(config, \"model_type\", None) == \"chatglm\" or getattr(config, \"model_type\", None) == \"internlm2\":\n            setattr(model, \"lm_head\", model.transformer.output_layer)\n            setattr(model, \"_keys_to_ignore_on_save\", [\"lm_head.weight\"])\n\n        # Set NEFTune trick for fine-tuning\n        if model_args.neft_alpha > 0:\n            input_embed = model.get_input_embeddings()\n            if isinstance(input_embed, torch.nn.Embedding):\n                def noisy_forward(self: torch.nn.Embedding, x: torch.Tensor) -> torch.Tensor:\n                    embeddings = input_embed.__class__.forward(self, x)\n                    dims = self.num_embeddings * self.embedding_dim\n                    mag_norm = model_args.neft_alpha / (dims ** 0.5)\n                    embeddings += torch.zeros_like(embeddings).uniform_(-mag_norm, mag_norm)\n                    return embeddings\n\n                input_embed.forward = MethodType(noisy_forward, input_embed)\n                logger.info(\"Using noisy embedding with alpha={:.2f}\".format(model_args.neft_alpha))\n            else:\n                logger.warning(\"Input embeddings are not normal nn.Embedding, cannot transform into noisy embedding.\")\n\n        # Patch Mixtral MOE model\n        if getattr(config, \"model_type\", None) == \"mixtral\" and is_deepspeed_zero3_enabled():\n            require_version(\"deepspeed>=0.13.0\", \"To fix: pip install deepspeed>=0.13.0\")\n            from deepspeed.utils import set_z3_leaf_modules  # type: ignore\n            from transformers.models.mixtral.modeling_mixtral import MixtralSparseMoeBlock  # type: ignore\n\n            set_z3_leaf_modules(model, [MixtralSparseMoeBlock])\n    else:\n        raise ValueError(f\"Error, model_name_or_path is None, SFT must be loaded from a pre-trained model\")\n\n    if script_args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n\n        # Set fp32 forward hook for lm_head\n        output_layer = getattr(model, \"lm_head\")\n        if isinstance(output_layer, torch.nn.Linear) and output_layer.weight.dtype != torch.float32:\n            def fp32_forward_post_hook(module: torch.nn.Module, args: Tuple[torch.Tensor], output: torch.Tensor):\n                return output.to(torch.float32)\n\n            output_layer.register_forward_hook(fp32_forward_post_hook)\n\n        # Load LoRA model\n        if script_args.peft_path is not None:\n            logger.info(f\"Peft from pre-trained model: {script_args.peft_path}\")\n            model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)\n        else:\n            logger.info(\"Init new peft model\")\n            if load_in_8bit or load_in_4bit:\n                model = prepare_model_for_kbit_training(model, training_args.gradient_checkpointing)\n            target_modules = script_args.target_modules.split(',') if script_args.target_modules else None\n            if target_modules and 'all' in target_modules:\n                target_modules = find_all_linear_names(model, int4=load_in_4bit, int8=load_in_8bit)\n            modules_to_save = script_args.modules_to_save\n            if modules_to_save is not None:\n                modules_to_save = modules_to_save.split(',')\n            logger.info(f\"Peft target_modules: {target_modules}\")\n            logger.info(f\"Peft lora_rank: {script_args.lora_rank}\")\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                target_modules=target_modules,\n                inference_mode=False,\n                r=script_args.lora_rank,\n                lora_alpha=script_args.lora_alpha,\n                lora_dropout=script_args.lora_dropout,\n                modules_to_save=modules_to_save)\n            model = get_peft_model(model, peft_config)\n        for param in filter(lambda p: p.requires_grad, model.parameters()):\n            param.data = param.data.to(torch.float32)\n        model.print_trainable_parameters()\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n        model = model.float()\n        print_trainable_parameters(model)\n\n    # Initialize our Trainer\n    if training_args.gradient_checkpointing and getattr(model, \"supports_gradient_checkpointing\", False):\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n        logger.info(\"Gradient checkpointing enabled.\")\n    else:\n        model.config.use_cache = True\n        logger.info(\"Gradient checkpointing disabled.\")\n    model.enable_input_require_grads()\n    if not ddp and torch.cuda.device_count() > 1:\n        # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n        model.is_parallelizable = True\n        model.model_parallel = True\n\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        label_pad_token_id=IGNORE_INDEX,\n        pad_to_multiple_of=4 if tokenizer.padding_side == \"right\" else None,  # for shifted sparse attention\n    )\n    # Initialize our Trainer\n    trainer = SavePeftModelTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    # Training\n    if training_args.do_train:\n        logger.info(\"*** Train ***\")\n        if trainer.is_world_process_zero():\n            sample = next(iter(trainer.get_train_dataloader()))\n            logger.debug(f\"Train dataloader example: {sample}\")\n            logger.debug(f\"input_ids:\\n{list(sample['input_ids'])[:3]}, \\nlabels:\\n{list(sample['labels'])[:3]}\")\n            logger.debug(f\"Decode input_ids[0]:\\n{tokenizer.decode(sample['input_ids'][0])}\")\n            replaced_labels = [label if label != IGNORE_INDEX else tokenizer.pad_token_id for label in\n                               sample['labels'][0]]\n            logger.debug(f\"Decode labels[0]:\\n{tokenizer.decode(replaced_labels)}\")\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n        model.config.use_cache = True  # enable cache after training\n        tokenizer.padding_side = \"left\"  # restore padding side\n        tokenizer.init_kwargs[\"padding_side\"] = \"left\"\n\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n            if is_deepspeed_zero3_enabled():\n                save_model_zero3(model, tokenizer, training_args, trainer)\n            else:\n                save_model(model, tokenizer, training_args)\n\n    # Evaluation\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n\n        metrics[\"eval_samples\"] = max_eval_samples\n        try:\n            perplexity = math.exp(metrics[\"eval_loss\"])\n        except OverflowError:\n            perplexity = float(\"inf\")\n        metrics[\"perplexity\"] = perplexity\n\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "template.py",
          "type": "blob",
          "size": 15.591796875,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: \n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Dict, Sequence\n\n__all__ = ['Conversation', 'register_conv_template', 'get_conv_template']\n\n\n@dataclass\nclass Conversation:\n    \"\"\"A class that manages prompt templates and keeps all conversation history.\"\"\"\n\n    # The name of this template\n    name: str\n    # The system prompt\n    system_prompt: str\n    # All messages. format: list of [question, answer]\n    messages: Optional[List[Sequence[str]]]\n    # The roles of the speakers\n    roles: Optional[Sequence[str]]\n    # Conversation prompt\n    prompt: str\n    # Separator\n    sep: str\n    # Stop token, default is tokenizer.eos_token\n    stop_str: Optional[str] = \"</s>\"\n\n    def get_prompt(\n            self,\n            messages: Optional[List[Sequence[str]]] = None,\n            system_prompt: Optional[str] = \"\"\n    ) -> str:\n        \"\"\"\n        Returns a string containing prompt without response.\n        \"\"\"\n        return \"\".join(self._format_example(messages, system_prompt))\n\n    def get_dialog(\n            self,\n            messages: Optional[List[Sequence[str]]] = None,\n            system_prompt: Optional[str] = \"\"\n    ) -> List[str]:\n        \"\"\"\n        Returns a list containing 2 * n elements where the 2k-th is a query and the (2k+1)-th is a response.\n        \"\"\"\n        return self._format_example(messages, system_prompt)\n\n    def _format_example(\n            self,\n            messages: Optional[List[Sequence[str]]] = None,\n            system_prompt: Optional[str] = \"\"\n    ) -> List[str]:\n        system_prompt = system_prompt or self.system_prompt\n        system_prompt = system_prompt + self.sep if system_prompt else \"\"  # add separator for non-empty system prompt\n        messages = messages or self.messages\n        convs = []\n        if not messages:\n            messages = []\n        for turn_idx, [user_query, bot_resp] in enumerate(messages):\n            if turn_idx == 0:\n                convs.append(system_prompt + self.prompt.format(query=user_query))\n                convs.append(bot_resp)\n            else:\n                convs.append(self.sep + self.prompt.format(query=user_query))\n                convs.append(bot_resp)\n        return convs\n\n    def append_message(self, query: str, answer: str):\n        \"\"\"Append a new message.\"\"\"\n        self.messages.append([query, answer])\n\n\n# A global registry for all conversation templates\nconv_templates: Dict[str, Conversation] = {}\n\n\ndef register_conv_template(template: Conversation):\n    \"\"\"Register a new conversation template.\"\"\"\n    conv_templates[template.name] = template\n\n\n\"\"\"Vicuna v1.1 template\nSupports: https://huggingface.co/lmsys/vicuna-7b-delta-v1.1\n          https://huggingface.co/lmsys/vicuna-13b-delta-v1.1\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"vicuna\",\n        system_prompt=\"A chat between a curious user and an artificial intelligence assistant. \"\n                      \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n        messages=[],\n        roles=(\"USER\", \"ASSISTANT\"),\n        prompt=\"USER: {query} ASSISTANT:\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Base model template, for few shot\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"base\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"USER\", \"ASSISTANT\"),\n        prompt=\"{query}\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Alpaca template\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"alpaca\",\n        system_prompt=\"Below is an instruction that describes a task. \"\n                      \"Write a response that appropriately completes the request.\",\n        messages=[],\n        roles=(\"### Instruction\", \"### Response\"),\n        prompt=\"### Instruction:\\n{query}\\n\\n### Response:\\n\",\n        sep=\"\\n\\n\",\n    )\n)\n\n\"\"\"Baichuan template\nsource: https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/blob/main/generation_utils.py#L31\nSupport: https://huggingface.co/baichuan-inc/Baichuan-13B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"baichuan\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<reserved_102>\", \"<reserved_103>\"),\n        prompt=\"<reserved_102>{query}<reserved_103>\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Baichuan2 template\nSupport: https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat\n         https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"baichuan2\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<reserved_106>\", \"<reserved_107>\"),\n        prompt=\"<reserved_106>{query}<reserved_107>\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"ziya template\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"ziya\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<human>\", \"<bot>\"),\n        prompt=\"<human>:{query}\\n<bot>:\",\n        sep=\"\\n\",\n    )\n)\n\n\"\"\"Linly template\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"linly\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"User\", \"Bot\"),\n        prompt=\"User: {query}\\nBot: \",\n        sep=\"\\n\",\n    )\n)\n\n\"\"\"ChatGLM1 template\nSupport: https://huggingface.co/THUDM/chatglm-6b\nsource: https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py#L1307\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"chatglm\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"é—®\", \"ç­”\"),\n        prompt=\"é—®ï¼š{query}\\nç­”ï¼š\",\n        sep=\"\\n\",\n    )\n)\n\n\"\"\"ChatGLM2 template\nSupport: https://huggingface.co/THUDM/chatglm2-6b\nsource: https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L1007\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"chatglm2\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"é—®\", \"ç­”\"),\n        prompt=\"é—®ï¼š{query}\\n\\nç­”ï¼š\",\n        sep=\"\\n\\n\",\n    )\n)\n\n\"\"\"ChatGLM3 template\nSupport: https://huggingface.co/THUDM/chatglm3-6b\nsource: https://huggingface.co/THUDM/chatglm3-6b/blob/main/tokenization_chatglm.py#L179\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"chatglm3\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        prompt=\"<|user|>\\n{query}<|assistant|>\",\n        sep=\"\\n\",\n        stop_str=\"<|user|>\",\n    )\n)\n\n\"\"\"Phoenix template\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"phoenix\",\n        system_prompt=\"A chat between a curious human and an artificial intelligence assistant. \"\n                      \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        messages=[],\n        roles=(\"Human\", \"Assistant\"),\n        prompt=\"Human: <s>{query}</s>Assistant: \",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"belle template\nSupports: https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-13B\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"belle\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"Human\", \"Belle\"),\n        prompt=\"Human: {query}\\n\\nBelle: \",\n        sep=\"\\n\\n\",\n    )\n)\n\n\"\"\"aquila template\nSupports: https://huggingface.co/qhduan/aquilachat-7b\n          https://huggingface.co/BAAI/AquilaChat2-34B\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"aquila\",\n        system_prompt=\"A chat between a curious human and an artificial intelligence assistant. \"\n                      \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        messages=[],\n        roles=(\"Human\", \"Assistant\"),\n        prompt=\"Human: {query}###Assistant:\",\n        sep=\"###\",\n    )\n)\n\n\"\"\"intern template\nSupports: https://huggingface.co/internlm/internlm-chat-7b\n          https://huggingface.co/internlm/internlm-chat-20b\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"intern\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<|User|>\", \"<|Bot|>\"),\n        prompt=\"<|User|>:{query}<eoh>\\n<|Bot|>:\",\n        sep=\"<eoa>\\n\",\n        stop_str=\"<eoa>\",\n    )\n)\n\n\"\"\"intern2 template\nSupports: https://huggingface.co/internlm/internlm2-1_8b\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"intern2\",\n        system_prompt=\"<|im_start|>system\\nYou are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).\\n<|im_end|>\\n\",\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\",\n        sep=\"<|im_end|>\\n\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n\"\"\"StarChat template\nSupports: https://huggingface.co/HuggingFaceH4/starchat-alpha\n          https://huggingface.co/HuggingFaceH4/starchat-beta\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"starchat\",\n        system_prompt=\"<system>\\n\",\n        messages=[],\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        prompt=\"<|user|>\\n{query}<|end|>\\n<|assistant|>\\n\",\n        sep=\"<|end|>\\n\",\n        stop_str=\"<|end|>\",\n    )\n)\n\n\"\"\"llama2 template\nSupports: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n          https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n          https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\nreference: https://github.com/facebookresearch/llama/blob/cfc3fc8c1968d390eb830e65c63865e980873a06/llama/generation.py#L212\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"llama2\",\n        system_prompt=(\n            \"<<SYS>>\\nYou are a helpful, respectful and honest assistant. \"\n            \"Always answer as helpfully as possible, while being safe. \"\n            \"Your answers should not include any harmful, unethical, racist, sexist, \"\n            \"toxic, dangerous, or illegal content. \"\n            \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n            \"If a question does not make any sense, or is not factually coherent, \"\n            \"explain why instead of answering something not correct. \"\n            \"If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\"\n        ),\n        messages=[],\n        roles=(\"[INST]\", \"[/INST]\"),\n        prompt=\"[INST] {query} [/INST]\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"llama3 template\nsource: https://huggingface.co/meta-llama\nSupports: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\nchat template:\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{{ model_answer_1 }}<|eot_id|>\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"llama3\",\n        system_prompt=(\n            \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n            \"You are a helpful, excellent and smart assistant.\"\n        ),\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=(\n            \"<|start_header_id|>user<|end_header_id|>\\n\\n{query}<|eot_id|>\"\n            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        ),\n        sep=\"<|eot_id|>\",\n        stop_str=\"<|eot_id|>\",\n    )\n)\n\n\"\"\"llama2-zh template\nsource: https://github.com/ymcui/Chinese-LLaMA-Alpaca-2\nSupports: https://huggingface.co/ziqingyang/chinese-alpaca-2-7b\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"llama2-zh\",\n        system_prompt=\"[INST] <<SYS>>\\nYou are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\\n<</SYS>>\\n\\n [/INST]\",\n        messages=[],\n        roles=(\"[INST]\", \"[/INST]\"),\n        prompt=\"[INST] {query} [/INST]\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"mistral template\nSupports: https://huggingface.co/mistralai/Mistral-7B-v0.1\n          https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\nsource: https://docs.mistral.ai/llm/mistral-instruct-v0.1\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"mistral\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"[INST]\", \"[/INST]\"),\n        prompt=\"[INST] {query} [/INST]\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"XVERSE template\nSupports: https://huggingface.co/xverse/XVERSE-13B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"xverse\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"Human\", \"Assistant\"),\n        prompt=\"Human: {query}\\n\\nAssistant: \",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"chatml template\nchatml: https://xbot123.com/645a461b922f176d7cfdbc2d/\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"chatml\",\n        system_prompt=\"You are a helpful assistant.\",\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\",\n        sep=\"<|im_end|>\\n\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n\"\"\"deepseek template\nSupports: https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat\n          https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"deepseek\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"User\", \"Assistant\"),\n        prompt=\"User: {query}\\n\\nAssistant:\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"deepseekcoder template\nSupports: https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"deepseekcoder\",\n        system_prompt=(\n            \"You are an AI programming assistant, utilizing the Deepseek Coder model, \"\n            \"developed by Deepseek Company, and you only answer questions related to computer science. \"\n            \"For politically sensitive questions, security and privacy issues, \"\n            \"and other non-computer science questions, you will refuse to answer\\n\"\n        ),\n        messages=[],\n        roles=(\"### Instruction\", \"### Response\"),\n        prompt=\"### Instruction:\\n{{content}}\\n### Response:\\n\",\n        sep=\"\\n\",\n        stop_str=\"<|EOT|>\",\n    )\n)\n\n\"\"\"Yi template\nsource: https://github.com/01-ai/Yi\nSupports: https://huggingface.co/01-ai/Yi-34B-Chat\n          https://huggingface.co/01-ai/Yi-6B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"yi\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\",\n        sep=\"\\n\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n\"\"\"Orion template\nsource: https://github.com/OrionStarAI/Orion\nSupports: https://huggingface.co/OrionStarAI/Orion-14B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"orion\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"Human\", \"Assistant\"),\n        prompt=\"Human: {query}\\n\\nAssistant: </s>\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Cohere template\nsource: https://huggingface.co/CohereForAI/c4ai-command-r-plus\nSupports: https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit\n          https://huggingface.co/CohereForAI/c4ai-command-r-plus\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"cohere\",\n        system_prompt=\"<BOS_TOKEN>\",\n        messages=[],\n        roles=(\"User\", \"Assistant\"),\n        prompt=(\n            \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{query}<|END_OF_TURN_TOKEN|>\"\n            \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        ),\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Qwen template\nsource: https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat/blob/main/tokenizer_config.json#L18\nSupports: https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat\n          https://huggingface.co/Qwen/Qwen1.5-72B-Chat\n          https://huggingface.co/Qwen/Qwen2-72B\n          https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"qwen\",\n        system_prompt=\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\",\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\",\n        sep=\"\\n\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n\ndef get_conv_template(name: str) -> Conversation:\n    \"\"\"Get a conversation template.\"\"\"\n    return conv_templates[name]\n"
        },
        {
          "name": "validate_jsonl.py",
          "type": "blob",
          "size": 2.6943359375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:ZhuangXialie(1832963123@qq.com)\n@description: validata the dataset\n\"\"\"\nimport json\n\nimport argparse\n\n\ndef validate_jsonl(file_path):\n    print(\"å¼€å§‹éªŒè¯ JSONL æ–‡ä»¶æ ¼å¼...\\n\")\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        line_number = 0\n        valid_lines = 0\n        total_lines = 0\n        for line in file:\n            total_lines += 1\n            line_number += 1\n            try:\n                # å°è¯•è§£æJSON\n                data = json.loads(line)\n\n                # æ£€æŸ¥æ˜¯å¦åŒ…å« 'conversations' é”®\n                if 'conversations' not in data:\n                    print(f\"ç¬¬ {line_number} è¡Œ: ç¼ºå°‘ 'conversations' é”®ï¼Œè¯·æ£€æŸ¥æ ¼å¼ã€‚\\n\")\n                    continue\n\n                # æ£€æŸ¥ 'conversations' æ˜¯å¦ä¸ºåˆ—è¡¨\n                conversations = data['conversations']\n                if not isinstance(conversations, list):\n                    print(f\"ç¬¬ {line_number} è¡Œ: 'conversations' åº”ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œè¯·æ£€æŸ¥ã€‚\\n\")\n                    continue\n\n                # æ£€æŸ¥æ¯ä¸ªå¯¹è¯æ˜¯å¦åŒ…å« 'from' å’Œ 'value' é”®\n                conversation_valid = True\n                for conv in conversations:\n                    if 'from' not in conv or 'value' not in conv:\n                        print(f\"ç¬¬ {line_number} è¡Œ: ç¼ºå°‘ 'from' æˆ– 'value' é”®ï¼Œè¯·æ£€æŸ¥å¯¹è¯æ ¼å¼ã€‚\\n\")\n                        conversation_valid = False\n                        continue\n\n                    # æ£€æŸ¥ 'from' å­—æ®µçš„å€¼æ˜¯å¦ä¸º 'human' æˆ– 'gpt'\n                    if conv['from'] not in ['system', 'human', 'gpt']:\n                        print(f\"ç¬¬ {line_number} è¡Œ: 'from' å­—æ®µçš„å€¼æ— æ•ˆï¼Œåº”ä¸º 'human' æˆ– 'gpt'ã€‚\\n\")\n                        conversation_valid = False\n\n                if conversation_valid:\n                    valid_lines += 1\n\n            except json.JSONDecodeError:\n                print(f\"ç¬¬ {line_number} è¡Œ: JSON æ ¼å¼æ— æ•ˆï¼Œè¯·ç¡®ä¿æ ¼å¼æ­£ç¡®ã€‚\\n\")\n\n    print(f\"éªŒè¯å®Œæˆï¼\\næ€»è¡Œæ•°: {total_lines} è¡Œ\")\n    print(f\"æœ‰æ•ˆçš„è¡Œæ•°: {valid_lines} è¡Œ\")\n    print(f\"æ— æ•ˆè¡Œæ•°: {total_lines - valid_lines} è¡Œ\\n\")\n\n    if valid_lines == total_lines:\n        print(\"æ­å–œï¼æ‰€æœ‰è¡Œçš„æ ¼å¼éƒ½æ­£ç¡®ã€‚\")\n    else:\n        print(\"è¯·æ ¹æ®æç¤ºæ£€æŸ¥å¹¶ä¿®å¤æ— æ•ˆçš„è¡Œã€‚\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Validate JSONL file format.')\n    parser.add_argument('--file_path', type=str, help='Path to JSONL file',\n                        default=\"./data/finetune/sharegpt_zh_1K_format.jsonl\")\n    args = parser.parse_args()\n    file_path = args.file_path\n    print(f\"æ­£åœ¨æ£€æŸ¥æ–‡ä»¶: {file_path}\")\n\n    validate_jsonl(file_path)\n"
        },
        {
          "name": "vllm_deployment.sh",
          "type": "blob",
          "size": 0.9501953125,
          "content": "#!/bin/bash\n\n# å®‰è£…vllmåº“\npip install vllm\n\n# æŒ‡å®šè¿è¡Œç¨‹åºçš„GPUè®¾å¤‡ç¼–å·ä¸º3ï¼Œé€‰æ‹©ä½¿ç”¨ç¼–å·ä¸º3çš„GPU,ä¸tpå¯¹åº”\nexport CUDA_VISIBLE_DEVICES=3\n\n# å¯åŠ¨VLLM APIæœåŠ¡å™¨ï¼Œä»¥ä¸‹ä¸ºç›¸å…³å‚æ•°è¯´æ˜ï¼š\n# - --model: æŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹çš„è·¯å¾„\n# - --served-model-name: æ¨¡å‹æœåŠ¡çš„åç§°\n# - --dtype: æ¨¡å‹æ•°æ®ç±»å‹è‡ªåŠ¨é€‰æ‹©\n# - --port: æŒ‡å®šAPIæœåŠ¡å™¨ç›‘å¬çš„ç«¯å£å·\n# - --host: æŒ‡å®šAPIæœåŠ¡å™¨ç›‘å¬çš„ç½‘ç»œåœ°å€ï¼Œ0.0.0.0å…è®¸æ‰€æœ‰IPåœ°å€è®¿é—®\n# - --gpu-memory-utilization: é™åˆ¶GPUå†…å­˜ä½¿ç”¨ä¸Šé™ä¸º90%\n# - --max-model-len: æŒ‡å®šæ¨¡å‹æ”¯æŒçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆtokenæ•°é‡ï¼‰\n# - -tp: å¼ é‡å¹¶è¡Œåº¦ï¼Œ1è¡¨ç¤ºå…³é—­å¼ é‡å¹¶è¡Œï¼Œå³ç”±å•ä¸ªGPUå¤„ç†æ‰€æœ‰å¼ é‡è¿ç®—\n\npython -m vllm.entrypoints.openai.api_server \\\n    --model medical-model \\\n    --served-model-name doctor \\\n    --dtype=auto \\\n    --port 8024 \\\n    --host 0.0.0.0 \\\n    --gpu-memory-utilization 0.9 \\\n    --max-model-len 512 \\\n    -tp 1\n"
        }
      ]
    }
  ]
}