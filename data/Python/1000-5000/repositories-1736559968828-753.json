{
  "metadata": {
    "timestamp": 1736559968828,
    "page": 753,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "shibing624/MedicalGPT",
      "stars": 3489,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.763671875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n.idea/"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.3017578125,
          "content": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Xu\"\n  given-names: \"Ming\"\ntitle: \"MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline\"\nurl: \"https://github.com/shibing624/MedicalGPT\"\ndata-released: 2023-06-02\nversion: 0.0.4"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.4541015625,
          "content": "# Contributing\n\nWe are happy to accept your contributions to make this repo better and more awesome! To avoid unnecessary work on either\nside, please stick to the following process:\n\n1. Check if there is already an issue for your concern.\n2. If there is not, open a new one to start a discussion. We hate to close finished PRs!\n3. If we decide your concern needs code changes, we would be happy to accept a pull request. Please consider the\ncommit guidelines below."
        },
        {
          "name": "DISCLAIMER",
          "type": "blob",
          "size": 3.4443359375,
          "content": "The software project, data, and models provided by our GitHub project are provided \"as is,\" without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement.\n\nIn no event shall the project owners or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software project, data, or models, even if advised of the possibility of such damage.\n\nUsers of this software project, data, and models are solely responsible for any consequences of their use. The project owners and contributors shall not be held responsible for any subsequent or potential harm caused by the use of this software project, data, or models.\n\nBy using this software project, data, or models, users accept and agree to this disclaimer. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n\nIt is important to note that this software project, data, and models are still in the research phase and are provided for experimental purposes only. As such, the project owners and contributors do not guarantee the accuracy, completeness, or usefulness of the software project, data, or models.\n\nFurthermore, due to the experimental nature of this software project, data, and models, it is possible that they may contain or generate inappropriate responses, errors, or inconsistencies. Users should exercise caution when using this software project, data, or models, and should not rely solely on them for any critical or sensitive tasks.\n\nThe project owners and contributors shall not be held responsible for any damages, losses, or liabilities arising from the use of this software project, data, or models, including but not limited to, any inappropriate responses generated by the software project, data, or models.\n\nBy using this software project, data, or models, users acknowledge and accept the experimental nature of the software project, data, and models, and understand the potential risks and limitations associated with their use. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models.\n\nThe software project, data, and models provided by our GitHub project are intended for research purposes only. They should not be used for any commercial, business, or legal purposes, and should not be relied upon as a substitute for professional advice or judgment.\n\nUsers of this software project, data, and models are strictly prohibited from using them for any commercial purposes, including but not limited to, selling, licensing, or distributing the software project, data, or models to third parties.\n\nThe project owners and contributors shall not be held responsible for any damages, losses, or liabilities arising from the use of this software project, data, or models for any commercial or business purposes.\n\nBy using this software project, data, or models, users agree to use them for research purposes only, and not for any commercial or business purposes. If users do not agree to the terms of this disclaimer, they should not use this software project, data, or models."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 45.50390625,
          "content": "[**🇨🇳中文**](https://github.com/shibing624/MedicalGPT/blob/main/README.md) | [**🌐English**](https://github.com/shibing624/MedicalGPT/blob/main/README_EN.md) | [**📖文档/Docs**](https://github.com/shibing624/MedicalGPT/wiki) | [**🤖模型/Models**](https://huggingface.co/shibing624)\n\n<div align=\"center\">\n  <a href=\"https://github.com/shibing624/MedicalGPT\">\n    <img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/logo.png\" height=\"100\" alt=\"Logo\">\n  </a>\n</div>\n\n-----------------\n\n# MedicalGPT: Training Medical GPT Model\n[![HF Models](https://img.shields.io/badge/Hugging%20Face-shibing624-green)](https://huggingface.co/shibing624)\n[![Github Stars](https://img.shields.io/github/stars/shibing624/MedicalGPT?color=yellow)](https://star-history.com/#shibing624/MedicalGPT&Timeline)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)\n[![Wechat Group](https://img.shields.io/badge/wechat-group-green.svg?logo=wechat)](#Contact)\n\n## 📖 Introduction\n\n**MedicalGPT** training medical GPT model with ChatGPT training pipeline, implemantation of Pretraining,\nSupervised Finetuning, RLHF(Reward Modeling and Reinforcement Learning) and DPO(Direct Preference Optimization).\n\n**MedicalGPT** 训练医疗大模型，实现了包括增量预训练、有监督微调、RLHF(奖励建模、强化学习训练)和DPO(直接偏好优化)。\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/dpo.jpg\" width=\"860\" />\n\n- RLHF training pipeline来自Andrej Karpathy的演讲PDF [State of GPT](https://karpathy.ai/stateofgpt.pdf)，视频 [Video](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)\n- DPO方法来自论文[Direct Preference Optimization:Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)\n- ORPO方法来自论文[ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)\n## 🔥 News\n[2024/09/21] v2.3版本: 支持了 **[Qwen-2.5](https://qwenlm.github.io/zh/blog/qwen2.5/)** 系列模型，详见[Release-v2.3](https://github.com/shibing624/MedicalGPT/releases/tag/2.3.0)\n\n[2024/08/02] v2.2版本：支持了角色扮演模型训练，新增了医患对话SFT数据生成脚本[role_play_data](https://github.com/shibing624/MedicalGPT/blob/main/role_play_data/README.md)，详见[Release-v2.2](https://github.com/shibing624/MedicalGPT/releases/tag/2.2.0)\n\n[2024/06/11] v2.1版本：支持了 **[Qwen-2](https://qwenlm.github.io/blog/qwen2/)** 系列模型，详见[Release-v2.1](https://github.com/shibing624/MedicalGPT/releases/tag/2.1.0)\n\n[2024/04/24] v2.0版本：支持了 **[Llama-3](https://huggingface.co/meta-llama)** 系列模型，详见[Release-v2.0](https://github.com/shibing624/MedicalGPT/releases/tag/2.0.0)\n\n[2024/04/17] v1.9版本：支持了 **[ORPO](https://arxiv.org/abs/2403.07691)**，详细用法请参照 `run_orpo.sh`。详见[Release-v1.9](https://github.com/shibing624/MedicalGPT/releases/tag/1.9.0)\n\n[2024/01/26] v1.8版本：支持微调Mixtral混合专家MoE模型 **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)**。详见[Release-v1.8](https://github.com/shibing624/MedicalGPT/releases/tag/1.8.0)\n\n[2024/01/14] v1.7版本：新增检索增强生成(RAG)的基于文件问答[ChatPDF](https://github.com/shibing624/ChatPDF)功能，代码`chatpdf.py`，可以基于微调后的LLM结合知识库文件问答提升行业问答准确率。详见[Release-v1.7](https://github.com/shibing624/MedicalGPT/releases/tag/1.7.0)\n\n[2023/10/23] v1.6版本：新增RoPE插值来扩展GPT模型的上下文长度；针对LLaMA模型支持了[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)和[LongLoRA](https://github.com/dvlab-research/LongLoRA) 提出的 **$S^2$-Attn**；支持了[NEFTune](https://github.com/neelsjain/NEFTune)给embedding加噪训练方法。详见[Release-v1.6](https://github.com/shibing624/MedicalGPT/releases/tag/1.6.0)\n\n[2023/08/28] v1.5版本: 新增[DPO(直接偏好优化)](https://arxiv.org/pdf/2305.18290.pdf)方法，DPO通过直接优化语言模型来实现对其行为的精确控制，可以有效学习到人类偏好。详见[Release-v1.5](https://github.com/shibing624/MedicalGPT/releases/tag/1.5.0)\n\n[2023/08/08] v1.4版本: 发布基于ShareGPT4数据集微调的中英文Vicuna-13B模型[shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat)，和对应的LoRA模型[shibing624/vicuna-baichuan-13b-chat-lora](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat-lora)，详见[Release-v1.4](https://github.com/shibing624/MedicalGPT/releases/tag/1.4.0)\n\n[2023/08/02] v1.3版本: 新增LLaMA, LLaMA2, Bloom, ChatGLM, ChatGLM2, Baichuan模型的多轮对话微调训练；新增领域词表扩充功能；新增中文预训练数据集和中文ShareGPT微调训练集，详见[Release-v1.3](https://github.com/shibing624/MedicalGPT/releases/tag/1.3.0)\n\n[2023/07/13] v1.1版本: 发布中文医疗LLaMA-13B模型[shibing624/ziya-llama-13b-medical-merged](https://huggingface.co/shibing624/ziya-llama-13b-medical-merged)，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的完整模型权重，详见[Release-v1.1](https://github.com/shibing624/MedicalGPT/releases/tag/1.1)\n\n[2023/06/15] v1.0版本: 发布中文医疗LoRA模型[shibing624/ziya-llama-13b-medical-lora](https://huggingface.co/shibing624/ziya-llama-13b-medical-lora)，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重，详见[Release-v1.0](https://github.com/shibing624/MedicalGPT/releases/tag/1.0.0)\n\n[2023/06/05] v0.2版本: 以医疗为例，训练领域大模型，实现了四阶段训练：包括二次预训练、有监督微调、奖励建模、强化学习训练。详见[Release-v0.2](https://github.com/shibing624/MedicalGPT/releases/tag/0.2.0)\n\n\n## 😊 Features\n\n\n基于ChatGPT Training Pipeline，本项目实现了领域模型--医疗行业语言大模型的训练：\n\n\n- 第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文档数据上二次预训练GPT模型，以适应领域数据分布（可选）\n- 第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n- 第三阶段\n  - RLHF(Reinforcement Learning from Human Feedback)基于人类反馈对语言模型进行强化学习，分为两步：\n    - RM(Reward Model)奖励模型建模，构造人类偏好排序数据集，训练奖励模型，用来建模人类偏好，主要是\"HHH\"原则，具体是\"helpful, honest, harmless\"\n    - RL(Reinforcement Learning)强化学习，用奖励模型来训练SFT模型，生成模型使用奖励或惩罚来更新其策略，以便生成更高质量、更符合人类偏好的文本\n  - [DPO(Direct Preference Optimization)](https://arxiv.org/pdf/2305.18290.pdf)直接偏好优化方法，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n  - [ORPO](https://arxiv.org/abs/2403.07691)比值比偏好优化，不需要参考模型（ref_model）的优化方法，通过ORPO，LLM可以同时学习SFT和对齐，将两个过程整合为单一步骤，缓解模型灾难性遗忘问题\n\n\n### Release Models\n\n\n| Model                                                                                                             | Base Model                                                                              | Introduction                                                                                                                                                                 |\n|:------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [shibing624/ziya-llama-13b-medical-lora](https://huggingface.co/shibing624/ziya-llama-13b-medical-lora)           | [IDEA-CCNL/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1)       | 在240万条中英文医疗数据集[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)上SFT微调了一版Ziya-LLaMA-13B模型，医疗问答效果有提升，发布微调后的LoRA权重(单轮对话)                                 |\n| [shibing624/ziya-llama-13b-medical-merged](https://huggingface.co/shibing624/ziya-llama-13b-medical-merged)       | [IDEA-CCNL/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1)       | 在240万条中英文医疗数据集[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)上SFT微调了一版Ziya-LLaMA-13B模型，医疗问答效果有提升，发布微调后的完整模型权重(单轮对话)                                 |\n| [shibing624/vicuna-baichuan-13b-chat-lora](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat-lora)       | [baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) | 在10万条多语言ShareGPT GPT4多轮对话数据集[shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4) 和 医疗数据集[shibing624/medical](https://huggingface.co/datasets/shibing624/medical) 上SFT微调了一版baichuan-13b-chat多轮问答模型，日常问答和医疗问答效果有提升，发布微调后的LoRA权重 |\n| [shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat)                 | [baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) | 在10万条多语言ShareGPT GPT4多轮对话数据集[shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4) 和 医疗数据集[shibing624/medical](https://huggingface.co/datasets/shibing624/medical) 上SFT微调了一版baichuan-13b-chat多轮问答模型，日常问答和医疗问答效果有提升，发布微调后的完整模型权重 |\n| [shibing624/llama-3-8b-instruct-262k-chinese](https://huggingface.co/shibing624/llama-3-8b-instruct-262k-chinese) | [Llama-3-8B-Instruct-262k](https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k)  | 在2万条中英文偏好数据集[shibing624/DPO-En-Zh-20k-Preference](https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference)上使用ORPO方法微调得到的超长文本多轮对话模型，适用于RAG、多轮对话                   |\n\n演示[shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat)模型效果：\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/demo-screen.gif\" width=\"860\" />\n具体case见[Inference Examples](#inference-examples)\n\n## ▶️ Demo\n\n\n我们提供了一个简洁的基于gradio的交互式web界面，启动服务后，可通过浏览器访问，输入问题，模型会返回答案。\n\n启动服务，命令如下：\n```shell\nCUDA_VISIBLE_DEVICES=0 python gradio_demo.py --model_type base_model_type --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir\n```\n\n参数说明：\n\n- `--model_type {base_model_type}`：预训练模型类型，如llama、bloom、chatglm等\n- `--base_model {base_model}`：存放HF格式的LLaMA模型权重和配置文件的目录，也可使用HF Model Hub模型调用名称\n- `--lora_model {lora_model}`：LoRA文件所在目录，也可使用HF Model Hub模型调用名称。若lora权重已经合并到预训练模型，则删除--lora_model参数\n- `--tokenizer_path {tokenizer_path}`：存放对应tokenizer的目录。若不提供此参数，则其默认值与--base_model相同\n- `--template_name`：模板名称，如`vicuna`、`alpaca`等。若不提供此参数，则其默认值是vicuna\n- `--only_cpu`: 仅使用CPU进行推理\n- `--resize_emb`：是否调整embedding大小，若不调整，则使用预训练模型的embedding大小，默认不调整\n\n\n## 💾 Install\n#### Updating the requirements\n`requirements.txt`会不时更新以适配最新功能，使用以下命令更新依赖:\n\n```markdown\ngit clone https://github.com/shibing624/MedicalGPT\ncd MedicalGPT\npip install -r requirements.txt --upgrade\n```\n\n#### Hardware Requirement (显存/VRAM)\n\n\n\\* *估算值*\n\n| 训练方法  | 精度          |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |\n|-------|-------------| ----- | ----- | ----- | ------ | ------ | ----- | ------ |\n| 全参数   | AMP(自动混合精度) | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |\n| 全参数   | 16          |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |\n| LoRA  | 16          |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |\n| QLoRA | 8           |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |\n| QLoRA | 4           |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |\n| QLoRA | 2           |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB |\n\n## 🚀 Training Pipeline\n\nTraining Stage:\n\n| Stage                          | Introduction | Python script                                                                                           | Shell script                                                                  |\n|:-------------------------------|:-------------|:--------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------|\n| Continue Pretraining           | 增量预训练        | [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py)                     | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)     |\n| Supervised Fine-tuning         | 有监督微调        | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)   |\n| Direct Preference Optimization | 直接偏好优化       | [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py)                   | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)   |\n| Reward Modeling                | 奖励模型建模       | [reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/reward_modeling.py)             | [run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rm.sh)     |\n| Reinforcement Learning         | 强化学习         | [ppo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/ppo_training.py)                   | [run_ppo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_ppo.sh)   |\n| ORPO                           | 概率偏好优化       | [orpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/orpo_training.py)                  | [run_orpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_orpo.sh) |\n\n- 提供完整PT+SFT+DPO全阶段串起来训练的pipeline：[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb) ，其对应的colab： [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)，运行完大概需要15分钟\n- 提供完整PT+SFT+RLHF全阶段串起来训练的pipeline：[run_training_ppo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_ppo_pipeline.ipynb) ，其对应的colab： [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_ppo_pipeline.ipynb) ，运行完大概需要20分钟\n- 提供基于知识库文件的LLM问答功能（RAG）：[chatpdf.py](https://github.com/shibing624/MedicalGPT/blob/main/chatpdf.py)\n- [训练参数说明](https://github.com/shibing624/MedicalGPT/blob/main/docs/training_params.md) | [训练参数说明wiki](https://github.com/shibing624/MedicalGPT/wiki/%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E)\n- [数据集](https://github.com/shibing624/MedicalGPT/blob/main/docs/datasets.md) | [数据集wiki](https://github.com/shibing624/MedicalGPT/wiki/%E6%95%B0%E6%8D%AE%E9%9B%86)\n- [扩充词表](https://github.com/shibing624/MedicalGPT/blob/main/docs/extend_vocab.md) | [扩充词表wiki](https://github.com/shibing624/MedicalGPT/wiki/%E6%89%A9%E5%85%85%E4%B8%AD%E6%96%87%E8%AF%8D%E8%A1%A8)\n- [FAQ](https://github.com/shibing624/MedicalGPT/blob/main/docs/FAQ.md) | [FAQ_wiki](https://github.com/shibing624/MedicalGPT/wiki/FAQ)\n\n#### Supported Models\n\n| Model Name                                                           | Model Size                    | Target Modules  | Template  |\n|----------------------------------------------------------------------|-------------------------------|-----------------|-----------|\n| [Baichuan](https://github.com/baichuan-inc/baichuan-13B)             | 7B/13B                        | W_pack          | baichuan  |\n| [Baichuan2](https://github.com/baichuan-inc/Baichuan2)               | 7B/13B                        | W_pack          | baichuan2 |\n| [BLOOMZ](https://huggingface.co/bigscience/bloomz)                   | 560M/1.1B/1.7B/3B/7.1B/176B   | query_key_value | vicuna    |\n| [ChatGLM](https://github.com/THUDM/ChatGLM-6B)                       | 6B                            | query_key_value | chatglm   |\n| [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)                     | 6B                            | query_key_value | chatglm2  |\n| [ChatGLM3](https://github.com/THUDM/ChatGLM3)                        | 6B                            | query_key_value | chatglm3  |\n| [Cohere](https://huggingface.co/CohereForAI/c4ai-command-r-plus)     | 104B                          | q_proj,v_proj   | cohere    |\n| [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM)              | 7B/16B/67B                    | q_proj,v_proj   | deepseek  |\n| [InternLM2](https://github.com/InternLM/InternLM)                    | 7B/20B                        | wqkv            | intern2   |\n| [LLaMA](https://github.com/facebookresearch/llama)                   | 7B/13B/33B/65B                | q_proj,v_proj   | alpaca    |\n| [LLaMA2](https://huggingface.co/meta-llama)                          | 7B/13B/70B                    | q_proj,v_proj   | llama2    |\n| [LLaMA3](https://huggingface.co/meta-llama)                          | 8B/70B                        | q_proj,v_proj   | llama3    |\n| [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) | 7B/8x7B                       | q_proj,v_proj   | mistral   |\n| [Orion](https://github.com/OrionStarAI/Orion)                        | 14B                           | q_proj,v_proj   | orion     |\n| [Qwen](https://github.com/QwenLM/Qwen)                               | 1.8B/7B/14B/72B               | c_attn          | qwen      |\n| [Qwen1.5](https://huggingface.co/Qwen/Qwen1.5-72B)                   | 0.5B/1.8B/4B/14B/32B/72B/110B | q_proj,v_proj   | qwen      |\n| [Qwen2](https://github.com/QwenLM/Qwen2)                             | 0.5B/1.5B/7B/72B              | q_proj,v_proj   | qwen      |\n| [XVERSE](https://github.com/xverse-ai/XVERSE-13B)                    | 13B                           | query_key_value | xverse    |\n| [Yi](https://github.com/01-ai/Yi)                                    | 6B/34B                        | q_proj,v_proj   | yi        |\n\n\n\n\n## 💻 Inference\n训练完成后，现在我们加载训练好的模型，验证模型生成文本的效果。\n\n```shell\nCUDA_VISIBLE_DEVICES=0 python inference.py \\\n    --model_type base_model_type \\\n    --base_model path_to_model_hf_dir \\\n    --tokenizer_path path_to_model_hf_dir \\\n    --lora_model path_to_lora \\\n    --interactive\n```\n\n参数说明：\n\n- `--model_type {base_model_type}`：预训练模型类型，如llama、bloom、chatglm等\n- `--base_model {base_model}`：存放HF格式的LLaMA模型权重和配置文件的目录\n- `--tokenizer_path {base_model}`：存放HF格式的LLaMA模型权重和配置文件的目录\n- `--lora_model {lora_model}`：LoRA解压后文件所在目录，也可使用HF Model Hub模型调用名称。如果已经合并了LoRA权重到预训练模型，则可以不提供此参数\n- `--tokenizer_path {tokenizer_path}`：存放对应tokenizer的目录。若不提供此参数，则其默认值与--base_model相同\n- `--template_name`：模板名称，如`vicuna`、`alpaca`等。若不提供此参数，则其默认值是vicuna\n- `--interactive`：以交互方式启动多轮问答，使用流式推理\n- `--data_file {file_name}`：非交互方式启动下，读取file_name中的的内容进行batch预测\n- `--output_file {file_name}`：非交互式方式下，将预测的结果以jsonl格式写入file_name\n- `--resize_emb`：是否调整embedding大小，若不调整，则使用预训练模型的embedding大小，默认不调整\n- `--only_cpu`：仅使用CPU进行推理\n- `--gpus {gpu_ids}`：指定使用的GPU设备编号，默认为0。如使用多张GPU，以逗号分隔，如0,1,2\n\n#### 多卡推理\n多卡数据并行，batch推理\n```shell\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 inference_multigpu_demo.py --model_type baichuan --base_model shibing624/vicuna-baichuan-13b-chat\n```\n#### Vllm多卡部署\n```shell\nbash run_Multi-GPUs-deployment.sh\n```\n\n\n#### Inference Examples\n[shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat) inference examples:\n\n<details><summary>Model output case</summary>\n\n| Human                                                                              | Assitant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 小孩发烧怎么办                                                                            | 发烧是身体对感染或其他疾病的自然反应。以下是一些可以帮助缓解发烧症状的建议：<br/>1. 保持身体水分充足：发烧时，身体会失去水分，因此需要保持水分充足。<br/>2. 使用冰袋：将冰袋放在额头或颈部可以帮助降低体温。<br/>3. 使用退烧药：退烧药可以帮助降低体温。<br/>4. 保持身体温度：保持身体温度可以帮助降低体温。<br/>5. 使用冷水或温水擦拭身体：使用冷水或温水擦拭身体可以帮助降低体温。<br/>6. 使用退烧药：退烧药可以帮助降低体温。<br/>7. 保持身体温度：保持身体温度可以帮助降低体温。<br/>8. 使用冷水或温水擦拭身体：使用冷水或温水擦拭身体可以帮助降低体温。<br/>9. 使用退烧药：退烧药可以帮助降低体温。<br/>10. 保持身体温度：保持身体温度可以帮助降低体温。                                                                                                                                                                                                                                                         |\n| 一岁宝宝发烧能吃啥药？                                                                        | 1岁宝宝发烧时，可以考虑以下几种药物：<br/>1. 退烧药：如布洛芬(Ibuprofen)或对乙酰氨基酚(Paracetamol)。这些药物可以帮助降低体温，缓解发烧症状。<br/>2. 抗生素：如果发烧是由细菌感染引起的，可以考虑使用抗生素。但请务必在医生的建议下使用。<br/>3. 抗病毒药物：如果发烧是由病毒感染引起的，可以考虑使用抗病毒药物。但请务必在医生的建议下使用。<br/>4. 解热镇痛药：如阿司匹林(Aspirin)或对乙酰氨基酚(Paracetamol)。这些药物可以帮助缓解发烧症状。<br/>5. 抗组胺药：如苯海拉明(Diphenhydramine)或非索非那定(Naproxen)。这些药物可以帮助缓解发烧症状。<br/>请务必在医生的建议下使用这些药物，并遵循药物说明书上的用量和用法。同时，保持宝宝充足的水分摄入，并确保宝宝有足够的休息。                                                                                                                                                                                                                             |\n| 肛门病变可能是什么疾病的症状                                                                     | 肛门病变可能由多种原因引起，包括感染、炎症、肿瘤或其他疾病。一些常见的肛门病变包括：<br/>1. 肛裂：这是一种肛门皮肤撕裂，通常由便秘或其他压力因素引起。<br/>2. 肛瘘：这是一种肛门感染，导致肛门皮肤和直肠之间形成瘘管。<br/>3. 肛门疣：这是一种肛门皮肤上形成的疣状病变，通常由人类乳头状瘤病毒(HPV)引起。<br/>4. 肛门脓肿：这是一种肛门感染，导致肛门皮肤下形成脓肿。<br/>5. 肛门肿瘤：这是一种肛门皮肤或直肠内形成的肿瘤。<br/>6. 肛门溃疡：这是一种肛门皮肤上形成的溃疡，通常由感染或炎症引起。<br/>7. 肛门瘙痒：这是一种肛门皮肤瘙痒的症状，可能由感染、炎症或其他疾病引起。<br/>如果您有肛门病变的症状，请咨询医疗保健专业人员以获得适当的诊断和治疗。                                                                                                                                                                                                                                                                  |\n| 医生，您好白带增多泛黄怎么回事？                                                                   | 您好，白带增多和发黄可能是由多种原因引起的，包括感染、荷尔蒙变化、阴道干燥或其他妇科问题。以下是一些可能的原因：<br/>1. 感染：白带增多和发黄可能是由阴道或尿道感染引起的。感染可能由细菌、病毒或真菌引起。<br/>2. 荷尔蒙变化：荷尔蒙变化，如月经周期或更年期，可能会导致白带增多和发黄。<br/>3. 阴道干燥：阴道干燥可能会导致白带增多和发黄。<br/>4. 其他妇科问题：其他妇科问题，如子宫颈炎、子宫内膜炎或子宫内膜异位症，也可能导致白带增多和发黄。<br/>如果您担心自己的白带，建议您咨询妇科医生进行检查和诊断。医生可以根据您的症状和检查结果确定原因并提供适当的治疗。                                                                                                                                                                                                                                                                                                                          |\n| 药流后有巧克力白带是没流干净吗                                                                    | 药流后出现巧克力白带并不一定意味着没有流干净。巧克力白带是由于子宫内膜脱落和出血引起的，通常在药流后的一段时间内会出现。<br/><br/>药流后出现巧克力白带的原因可能包括：<br/><br/>1. 子宫内膜脱落：药流后，子宫内膜会脱落，导致出血。这种出血可能表现为巧克力白带。<br/>2. 感染：药流后，子宫内膜和宫颈口可能存在创伤，容易受到细菌感染。感染可能导致巧克力白带。<br/>3. 激素水平变化：药流后，激素水平可能发生变化，导致巧克力白带。<br/><br/>如果巧克力白带持续时间较长，或者伴有其他症状，如腹痛、发热、分泌物异味等，建议及时就诊，进行相关检查，如妇科检查、B超等，以确定是否存在感染或其他问题。在医生的指导下进行相应的治疗。                                                                                                                                                                                                                                                                                      |\n| 维胺酯维E乳膏能治理什么疾病                                                                     | 维胺酯维E乳膏是一种外用药物，主要用于治疗痤疮、脂溢性皮炎等皮肤病。它可以帮助减少皮肤油脂分泌，减轻炎症，促进皮肤修复。<br/><br/>需要注意的是，维胺酯维E乳膏并非万能药，对于某些皮肤病可能并不适用。在使用前，建议咨询医生或药师，了解药物的适应症、禁忌症、用法用量等信息。同时，使用药物时应遵循医嘱，避免自行滥用。                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| 痤疮是什么原因导致的?                                                                        | 痤疮，也称为粉刺，是由皮肤毛孔堵塞引起的。堵塞毛孔的原因有很多，包括：<br/>1. 皮脂腺过度活跃：皮脂腺产生过多油脂，导致毛孔堵塞。<br/>2. 毛孔堵塞：皮肤表面的死皮细胞、油脂和细菌堵塞毛孔。<br/>3. 炎症：痤疮通常伴随着炎症，这可能是由于皮肤感染或免疫系统对痤疮的反应引起的。<br/>4. 遗传因素：某些人可能更容易患痤疮，这可能是由于遗传因素引起的。<br/>5. 荷尔蒙变化：痤疮在青春期和月经期间更为常见，这可能是由于荷尔蒙变化引起的。<br/>6. 生活方式因素：某些生活方式因素，如压力、不良饮食和缺乏运动，也可能导致痤疮。<br/>7. 化妆品和护肤品：某些化妆品和护肤品可能导致痤疮，特别是那些含有刺激性成分的产品。<br/>8. 药物：某些药物，如口服避孕药，也可能导致痤疮。<br/>要预防痤疮，可以采取一些措施，如保持皮肤清洁、使用温和的护肤品、避免过度使用化妆品和护肤品、保持健康的生活方式和饮食习惯、避免压力和荷尔蒙变化。如果痤疮严重，可能需要寻求皮肤科医生的帮助。                                                                                                                                                         |\n| 脸上长青春痘怎么办,能吃啥药,需要注意啥                                                               | 青春痘，又称痤疮，是一种常见的皮肤病。青春痘的治疗需要综合考虑饮食、生活习惯、药物等多个方面。以下是一些建议：<br/><br/>1. 饮食：<br/>* 避免高糖、高脂、高油的食物，如快餐、甜点、油炸食品等。<br/>* 多吃新鲜蔬菜、水果、全谷类食物，以增加纤维摄入，促进肠道蠕动。<br/>* 避免摄入过多的咖啡因、酒精、辛辣食物等刺激性食物。<br/><br/>2. 生活习惯：<br/>* 保持良好的作息时间，保证充足的睡眠。<br/>* 避免长时间使用电脑、手机等电子产品，以免辐射对皮肤造成刺激。<br/>* 保持皮肤清洁，每天早晚使用温和的洁面产品清洁皮肤。<br/>* 避免挤压青春痘，以免感染和留下疤痕。<br/><br/>3. 药物：<br/>* 外用药物：可以使用含有水杨酸、苯甲酸、壬二酸等成分的护肤品，帮助去除皮肤表面的角质，减少痘痘的形成。<br/>* 口服药物：如果青春痘严重，可以考虑口服抗生素、维生素B族、锌等药物，但需在医生指导下使用。<br/><br/>4. 注意：<br/>* 不要随意使用化妆品、护肤品，以免刺激皮肤。<br/>* 不要过度清洁皮肤，以免破坏皮肤屏障。<br/>* 保持心情舒畅，避免焦虑、抑郁等负面情绪。<br/><br/>总之，青春痘的治疗需要综合考虑饮食、生活习惯、药物等多个方面。在治疗过程中，要保持耐心和信心，遵循医生的建议，逐步改善皮肤状况。 |\n| 由于输精管堵塞会不会导致男性不育？，医生您好，我同事这几天不开心，我问她怎么了，她说她老公输精管堵塞，所以一直没孩子，不是不想要，请问输精管堵塞会不会导致男性不育？ | 输精管堵塞可能会导致男性不育。输精管是连接睾丸和尿道的一条管道，负责将精子从睾丸输送到尿道。如果输精管堵塞，精子就无法通过输精管到达尿道，从而导致不育。<br/><br/>输精管堵塞的原因有很多，包括先天性发育异常、炎症、肿瘤、外伤等。输精管堵塞的治疗方法包括手术疏通、药物治疗等。如果输精管堵塞严重，可能需要进行手术疏通。<br/><br/>需要注意的是，输精管堵塞并不一定导致不育，因为还有其他途径可以将精子输送到女性体内。例如，精子可以通过附睾、前列腺等组织到达女性体内。因此，如果输精管堵塞，建议及时就医，寻求专业医生的帮助。                                                                                                                                                                                                                                                                                                                                                     |\n| 心肌梗塞能治疗吗？                                                                          | 心肌梗死是一种严重的疾病，如果及时治疗，可以挽救生命并防止长期残疾。治疗心肌梗死的关键是尽早识别和治疗。<br/><br/>心肌梗死的治疗通常包括以下步骤：<br/><br/>1. 立即就医：如果出现胸痛、呼吸急促或晕厥等症状，立即就医。<br/><br/>2. 药物治疗：医生可能会开处方药物，如硝酸甘油或阿司匹林，以帮助缓解症状并防止进一步的损伤。<br/><br/>3. 溶栓治疗：如果患者在到达医院前出现心肌梗死，医生可能会进行溶栓治疗，通过注射溶解血栓的药物来帮助恢复血流。<br/><br/>4. 手术治疗：如果药物治疗或溶栓治疗无效，医生可能会建议进行手术，如冠状动脉搭桥手术或心脏支架植入术。<br/><br/>5. 康复和预防：心肌梗死的治疗通常包括康复计划，以帮助患者恢复身体功能并预防未来的心脏事件。此外，患者还应遵循医生的建议，以降低患心脏病的风险。<br/><br/>需要注意的是，心肌梗死的治疗取决于患者的具体情况和症状。因此，如果您或您认识的人出现心肌梗死症状，请立即就医。                                                                                                                                                            |\n| 怎么让自己精力充沛，列5点建议                                                                    | 1. 保持充足的睡眠：睡眠对于保持精力充沛至关重要。确保每晚获得7-9小时的高质量睡眠。<br/>2. 均衡饮食：均衡饮食对于保持能量水平至关重要。确保摄入足够的蛋白质、碳水化合物和健康脂肪。<br/>3. 定期锻炼：定期锻炼可以帮助提高能量水平。尝试每天进行至少30分钟的有氧运动，如散步、跑步或骑自行车。<br/>4. 管理压力：压力会消耗能量，因此管理压力对于保持精力充沛至关重要。尝试找到放松和减压的方法，如冥想、瑜伽或深呼吸练习。<br/>5. 保持水分：脱水会降低能量水平。确保每天喝足够的水，并根据需要补充水分。                                                                                                                                                                                                                                                                                                                                                         |\n| 写个春天的诗                                                                             | 春天，<br/>是大地苏醒的季节，<br/>是万物复苏的季节，<br/>是生命绽放的季节。<br/>春天，<br/>是阳光明媚的季节，<br/>是鸟语花香的季节，<br/>是绿意盎然的季节，<br/>是希望无限的季节。<br/>春天，<br/>是人们欢笑的季节，<br/>是孩子们嬉戏的季节，<br/>是恋人们相拥的季节，<br/>是人们追求梦想的季节。                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| 南极洲为啥没有北极熊?                                                                        | 南极洲没有北极熊，因为北极熊生活在北极地区，而南极洲位于南半球，气候和环境与北极地区有很大不同。南极洲的气候非常寒冷，常年被冰雪覆盖，而北极熊则生活在北极地区，那里有丰富的海洋生物和陆地哺乳动物。此外，南极洲的生态系统也与北极地区不同，没有北极熊所需的生存条件。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n\n</details>\n\n\n## 📚 Dataset\n### 医疗数据集\n\n- 240万条中文医疗数据集(包括预训练、指令微调和奖励数据集)：[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)\n- 22万条中文医疗对话数据集(华佗项目)：[shibing624/huatuo_medical_qa_sharegpt](https://huggingface.co/datasets/shibing624/huatuo_medical_qa_sharegpt) 【本项目支持格式】\n\n### 通用数据集\n\n#### Pretraining datasets(预训练数据集)\n- 16GB中英文无监督、平行语料[Linly-AI/Chinese-pretraining-dataset](https://huggingface.co/datasets/Linly-AI/Chinese-pretraining-dataset)\n- 524MB中文维基百科语料[wikipedia-cn-20230720-filtered](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n#### Supervised fine-tuning datasets(指令微调数据集)\n- 10万条多语言ShareGPT GPT4多轮对话数据集：[shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4) 【本项目支持格式】\n- 9万条英文ShareGPT多轮对话数集：[anon8231489123/ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) 【本项目支持格式】\n- 50万条中文ChatGPT指令Belle数据集：[BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- 100万条中文ChatGPT指令Belle数据集：[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- 5万条英文ChatGPT指令Alpaca数据集：[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)\n- 2万条中文ChatGPT指令Alpaca数据集：[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)\n- 69万条中文指令Guanaco数据集(Belle50万条+Guanaco19万条)：[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)\n- 5万条英文ChatGPT多轮对话数据集：[RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)\n- 80万条中文ChatGPT多轮对话数据集：[BelleGroup/multiturn_chat_0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- 116万条中文ChatGPT多轮对话数据集：[fnlp/moss-002-sft-data](https://huggingface.co/datasets/fnlp/moss-002-sft-data)\n- 3.8万条中文ShareGPT多轮对话数据集：[FreedomIntelligence/ShareGPT-CN](https://huggingface.co/datasets/FreedomIntelligence/ShareGPT-CN)\n- 130万条中文微调数据集（汇总）：[zhuangxialie/Llama3-Chinese-Dataset](https://modelscope.cn/datasets/zhuangxialie/Llama3-Chinese-Dataset/dataPeview) 【本项目支持格式】\n- 7千条中文角色扮演多轮对话数据集：[shibing624/roleplay-zh-sharegpt-gpt4-data](https://huggingface.co/datasets/shibing624/roleplay-zh-sharegpt-gpt4-data) 【本项目支持格式】\n\n#### Preference datasets(偏好数据集)\n- 2万条中英文偏好数据集：[shibing624/DPO-En-Zh-20k-Preference](https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference) 【本项目支持格式】\n- 原版的oasst1数据集：[OpenAssistant/oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)\n- 2万条多语言oasst1的reward数据集：[tasksource/oasst1_pairwise_rlhf_reward](https://huggingface.co/datasets/tasksource/oasst1_pairwise_rlhf_reward)\n- 11万条英文hh-rlhf的reward数据集：[Dahoas/full-hh-rlhf](https://huggingface.co/datasets/Dahoas/full-hh-rlhf)\n- 9万条英文reward数据集(来自Anthropic's Helpful Harmless dataset)：[Dahoas/static-hh](https://huggingface.co/datasets/Dahoas/static-hh)\n- 7万条英文reward数据集（来源同上）：[Dahoas/rm-static](https://huggingface.co/datasets/Dahoas/rm-static)\n- 7万条繁体中文的reward数据集（翻译自rm-static）[liswei/rm-static-m2m100-zh](https://huggingface.co/datasets/liswei/rm-static-m2m100-zh)\n- 7万条英文Reward数据集：[yitingxie/rlhf-reward-datasets](https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets)\n- 3千条中文知乎问答偏好数据集：[liyucheng/zhihu_rlhf_3k](https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k)\n\n\n## ☎️ Contact\n\n- Issue(建议)\n  ：[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)\n- 邮件我：xuming: xuming624@qq.com\n- 微信我： 加我*微信号：xuming624, 备注：姓名-公司名-NLP* 进NLP交流群（加我拉你进群）。\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/wechat.jpeg\" width=\"200\" />\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/wechat_group.jpg\" width=\"200\" />\n\n## ⚠️ LICENSE\n\n本项目仅可应用于研究目的，项目开发者不承担任何因使用本项目（包含但不限于数据、模型、代码等）导致的危害或损失。详细请参考[免责声明](https://github.com/shibing624/MedicalGPT/blob/main/DISCLAIMER)。\n\nMedicalGPT项目代码的授权协议为 [The Apache License 2.0](/LICENSE)，代码可免费用做商业用途，模型权重和数据只能用于研究目的。请在产品说明中附加MedicalGPT的链接和授权协议。\n\n\n## 😇 Citation\n\n如果你在研究中使用了MedicalGPT，请按如下格式引用：\n\n```latex\n@misc{MedicalGPT,\n  title={MedicalGPT: Training Medical GPT Model},\n  author={Ming Xu},\n  year={2023},\n  howpublished={\\url{https://github.com/shibing624/MedicalGPT}},\n}\n```\n\n## 😍 Contribute\n\n项目代码还很粗糙，如果大家对代码有所改进，欢迎提交回本项目，在提交之前，注意以下两点：\n\n- 在`tests`添加相应的单元测试\n- 使用`python -m pytest`来运行所有单元测试，确保所有单测都是通过的\n\n之后即可提交PR。\n\n## 💕 Acknowledgements\n\n- [Direct Preference Optimization:Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)\n- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)\n- [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n- [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)\n- [dvlab-research/LongLoRA](https://github.com/dvlab-research/LongLoRA)\n\nThanks for their great work!\n\n#### 关联项目推荐\n- [shibing624/ChatPilot](https://github.com/shibing624/ChatPilot)：给 LLM Agent（包括RAG、在线搜索、Code interpreter） 提供一个简单好用的Web UI界面\n\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 20.4755859375,
          "content": "[**🇨🇳中文**](https://github.com/shibing624/MedicalGPT/blob/main/README.md) | [**🌐English**](https://github.com/shibing624/MedicalGPT/blob/main/README_EN.md) | [**📖文档/Docs**](https://github.com/shibing624/MedicalGPT/wiki) | [**🤖模型/Models**](https://huggingface.co/shibing624)\n\n<div align=\"center\">\n  <a href=\"https://github.com/shibing624/MedicalGPT\">\n    <img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/logo.png\" width=\"120\" alt=\"Logo\">\n  </a>\n</div>\n\n-----------------\n\n# MedicalGPT: Training Medical GPT Model\n[![HF Models](https://img.shields.io/badge/Hugging%20Face-shibing624-green)](https://huggingface.co/shibing624)\n[![Github Stars](https://img.shields.io/github/stars/shibing624/MedicalGPT?color=yellow)](https://star-history.com/#shibing624/MedicalGPT&Timeline)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)\n[![Wechat Group](https://img.shields.io/badge/wechat-group-green.svg?logo=wechat)](#Contact)\n\n## 📖 Introduction\n\n**MedicalGPT** trains a medical large language model using the ChatGPT training pipeline, implementing pretraining, supervised finetuning, RLHF (Reward Modeling and Reinforcement Learning), and DPO (Direct Preference Optimization).\n\n**MedicalGPT** trains medical large models, implementing incremental pretraining, supervised fine-tuning, RLHF (reward modeling, reinforcement learning training), and DPO (direct preference optimization).\n\n![DPO](https://github.com/shibing624/MedicalGPT/blob/main/docs/dpo.jpg)\n\n- The RLHF training pipeline is from Andrej Karpathy's presentation PDF [State of GPT](https://karpathy.ai/stateofgpt.pdf), video [Video](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)\n- The DPO method is from the paper [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)\n- The ORPO method is from the paper [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/GPT_Training.jpg\" width=\"860\" />\n\nTraining MedicalGPT model：\n\n- Stage 1：PT(Continue PreTraining), Pre-training the LLaMA model on massive domain document data to inject domain knowledge\n- Stage 2: SFT (Supervised Fine-tuning) has supervised fine-tuning, constructs instruction fine-tuning data sets, and performs instruction fine-tuning on the basis of pre-trained models to align instruction intentions\n- Stage 3: RM (Reward Model) reward model modeling, constructing a human preference ranking data set, training the reward model to align human preferences, mainly the \"HHH\" principle, specifically \"helpful, honest, harmless\"\n- Stage 4: RL (Reinforcement Learning) is based on human feedback reinforcement learning (RLHF), using the reward model to train the SFT model, and the generation model uses rewards or penalties to update its strategy in order to generate higher quality, more in line with human preferences text\n\n## 🔥 News\n\n- **[2024/09/21] v2.2 Release**: Supports the **[Qwen-2.5](https://qwenlm.github.io/zh/blog/qwen2.5/)** series of models. See [Release-v2.3](https://github.com/shibing624/MedicalGPT/releases/tag/2.3.0)\n\n- **[2024/08/02] v2.2 Release**: Supports role-playing model training, adds new scripts for generating patient-doctor dialogue SFT data [role_play_data](https://github.com/shibing624/MedicalGPT/blob/main/role_play_data/README.md). See [Release-v2.2](https://github.com/shibing624/MedicalGPT/releases/tag/2.2.0).\n  \n- **[2024/06/11] v2.1 Release**: Supports the **[Qwen-2](https://qwenlm.github.io/blog/qwen2/)** series of models. See [Release-v2.1](https://github.com/shibing624/MedicalGPT/releases/tag/2.1.0).\n\n- **[2024/04/24] v2.0 Release**: Supports the **[Llama-3](https://huggingface.co/meta-llama)** series of models. See [Release-v2.0](https://github.com/shibing624/MedicalGPT/releases/tag/2.0.0).\n\n- **[2024/04/17] v1.9 Release**: Supports **[ORPO](https://arxiv.org/abs/2403.07691)**. For detailed usage, refer to `run_orpo.sh`. See [Release-v1.9](https://github.com/shibing624/MedicalGPT/releases/tag/1.9.0).\n\n- **[2024/01/26] v1.8 Release**: Supports fine-tuning the Mixtral Mixture-of-Experts (MoE) model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)**. See [Release-v1.8](https://github.com/shibing624/MedicalGPT/releases/tag/1.8.0).\n\n- **[2024/01/14] v1.7 Release**: Adds retrieval-augmented generation (RAG) based file question answering [ChatPDF](https://github.com/shibing624/ChatPDF) functionality, code `chatpdf.py`, which can improve industry-specific Q&A accuracy by combining fine-tuned LLMs with knowledge base files. See [Release-v1.7](https://github.com/shibing624/MedicalGPT/releases/tag/1.7.0).\n\n- **[2023/10/23] v1.6 Release**: Adds RoPE interpolation to extend the context length of GPT models; supports **$S^2$-Attn** proposed by [FlashAttention-2](https://github.com/Dao-AILab/flash-attention) and [LongLoRA](https://github.com/dvlab-research/LongLoRA) for LLaMA models; supports the embedding noise training method [NEFTune](https://github.com/neelsjain/NEFTune). See [Release-v1.6](https://github.com/shibing624/MedicalGPT/releases/tag/1.6.0).\n\n- **[2023/08/28] v1.5 Release**: Adds the **DPO (Direct Preference Optimization)** method, which directly optimizes the behavior of language models to precisely align with human preferences. See [Release-v1.5](https://github.com/shibing624/MedicalGPT/releases/tag/1.5.0).\n\n- **[2023/08/08] v1.4 Release**: Releases the Chinese-English Vicuna-13B model fine-tuned on the ShareGPT4 dataset [shibing624/vicuna-baichuan-13b-chat](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat), and the corresponding LoRA model [shibing624/vicuna-baichuan-13b-chat-lora](https://huggingface.co/shibing624/vicuna-baichuan-13b-chat-lora). See [Release-v1.4](https://github.com/shibing624/MedicalGPT/releases/tag/1.4.0).\n\n- **[2023/08/02] v1.3 Release**: Adds multi-turn dialogue finetuning for LLAMA, LLAMA2, Bloom, ChatGLM, ChatGLM2, and Baichuan models; adds domain vocabulary expansion functionality; adds Chinese pre-training datasets and Chinese ShareGPT finetuning datasets. See [Release-v1.3](https://github.com/shibing624/MedicalGPT/releases/tag/1.3.0).\n\n- **[2023/07/13] v1.1 Release**: Releases the Chinese medical LLAMA-13B model [shibing624/ziya-llama-13b-medical-merged](https://huggingface.co/shibing624/ziya-llama-13b-medical-merged), based on the Ziya-LLAMA-13B-v1 model, SFT fine-tunes a medical model, improving medical QA performance. See [Release-v1.1](https://github.com/shibing624/MedicalGPT/releases/tag/1.1).\n\n- **[2023/06/15] v1.0 Release**: Releases the Chinese medical LoRA model [shibing624/ziya-llama-13b-medical-lora](https://huggingface.co/shibing624/ziya-llama-13b-medical-lora), based on the Ziya-LLaMA-13B-v1 model, SFT fine-tunes a medical model, improving medical QA performance. See [Release-v1.0](https://github.com/shibing624/MedicalGPT/releases/tag/1.0.0).\n\n- **[2023/06/05] v0.2 Release**: Trains domain-specific large models using medicine as an example, implementing four stages of training: secondary pretraining, supervised fine-tuning, reward modeling, and reinforcement learning training. See [Release-v0.2](https://github.com/shibing624/MedicalGPT/releases/tag/0.2.0).\n## ▶️ Demo\n\n- Hugging Face Demo: doing\n\nWe provide a simple Gradio-based interactive web interface. After the service is started, it can be accessed through a browser, enter a question, and the model will return an answer. The command is as follows:\n```shell\npython gradio_demo.py --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir\n```\n\nParameter Description:\n\n- `--base_model {base_model}`: directory to store LLaMA model weights and configuration files in HF format, or use the HF Model Hub model call name\n- `--lora_model {lora_model}`: The directory where the LoRA file is located, and the name of the HF Model Hub model can also be used. If the lora weights have been merged into the pre-trained model, delete the --lora_model parameter\n- `--tokenizer_path {tokenizer_path}`: Store the directory corresponding to the tokenizer. If this parameter is not provided, its default value is the same as --lora_model; if the --lora_model parameter is not provided, its default value is the same as --base_model\n- `--use_cpu`: use only CPU for inference\n- `--gpus {gpu_ids}`: Specifies the number of GPU devices used, the default is 0. If using multiple GPUs, separate them with commas, such as 0,1,2\n\n\n\n\n## 🚀 Training Pipeline\n\n### Stage 1: Continue Pretraining\n\nBased on the llama-7b model, use medical encyclopedia data to continue pre-training, and expect to inject medical knowledge into the pre-training model to obtain the llama-7b-pt model. This step is optional\n\n\n```shell\nsh run_pt.sh\n```\n\n[Training Detail wiki](https://github.com/shibing624/MedicalGPT/wiki/Training-Details)\n\n### Stage 2: Supervised FineTuning\nBased on the llama-7b-pt model, the llama-7b-sft model is obtained by using medical question-and-answer data for supervised fine-tuning. This step is required\n\nSupervised fine-tuning of the base llama-7b-pt model to create llama-7b-sft\n\n```shell\nsh run_sft.sh\n```\n\n[Training Detail wiki](https://github.com/shibing624/MedicalGPT/wiki/Training-Details)\n\n### Stage 3: Reward Modeling\nRM(Reward Model): reward model modeling\n\nIn principle, we can directly use human annotations to fine-tune the model with RLHF.\n\nHowever, this will require us to send some samples to humans to be scored after each round of optimization. This is expensive and slow due to the large number of training samples required for convergence and the limited speed at which humans can read and annotate them.\nA better strategy than direct feedback is to train a reward model RM on the human annotated set before entering the RL loop. The purpose of the reward model is to simulate human scoring of text.\n\nThe best practice for building a reward model is to rank the prediction results, that is, for each prompt (input text) corresponding to two results (yk, yj), the model predicts which score the human annotation is higher.\nThe RM model is trained by manually marking the scoring results of the SFT model. The purpose is to replace manual scoring. It is essentially a regression model used to align human preferences, mainly based on the \"HHH\" principle, specifically \"helpful, honest, harmless\".\n\n\nBased on the llama-7b-sft model, the reward preference model is trained using medical question and answer preference data, and the llama-7b-reward model is obtained after training. This step is required\n\nReward modeling using dialog pairs from the reward dataset using the llama-7b-sft to create llama-7b-reward:\n\n```shell\nsh run_rm.sh\n```\n[Training Detail wiki](https://github.com/shibing624/MedicalGPT/wiki/Training-Details)\n\n### Stage 4: Reinforcement Learning\nThe purpose of the RL (Reinforcement Learning) model is to maximize the output of the reward model. Based on the above steps, we have a fine-tuned language model (llama-7b-sft) and reward model (llama-7b-reward).\nThe RL loop is ready to execute.\n\nThis process is roughly divided into three steps:\n\n1. Enter prompt, the model generates a reply\n2. Use a reward model to score responses\n3. Based on the score, a round of reinforcement learning for policy optimization (PPO)\n\n<img src=https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/trl_loop.png height=400 />\n\nReinforcement Learning fine-tuning of llama-7b-sft with the llama-7b-reward reward model to create llama-7b-rl\n\n```shell\nsh run_ppo.sh\n```\n[Training Detail wiki](https://github.com/shibing624/MedicalGPT/wiki/Training-Details)\n\n\n#### Supported Models\n\n\n| Model Name                                                           | Model Size                  | Target Modules  | Template  |\n|----------------------------------------------------------------------|-----------------------------|-----------------|-----------|\n| [Baichuan](https://github.com/baichuan-inc/baichuan-13B)             | 7B/13B                      | W_pack          | baichuan  |\n| [Baichuan2](https://github.com/baichuan-inc/Baichuan2)               | 7B/13B                      | W_pack          | baichuan2 |\n| [BLOOMZ](https://huggingface.co/bigscience/bloomz)                   | 560M/1.1B/1.7B/3B/7.1B/176B | query_key_value | vicuna    |\n| [ChatGLM](https://github.com/THUDM/ChatGLM-6B)                       | 6B                          | query_key_value | chatglm   |\n| [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)                     | 6B                          | query_key_value | chatglm2  |\n| [ChatGLM3](https://github.com/THUDM/ChatGLM3)                        | 6B                          | query_key_value | chatglm3  |\n| [Cohere](https://huggingface.co/CohereForAI/c4ai-command-r-plus)     | 104B                        | q_proj,v_proj   | cohere    |\n| [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM)              | 7B/16B/67B                  | q_proj,v_proj   | deepseek  |\n| [InternLM2](https://github.com/InternLM/InternLM)                    | 7B/20B                      | wqkv            | intern2    |\n| [LLaMA](https://github.com/facebookresearch/llama)                   | 7B/13B/33B/65B              | q_proj,v_proj   | alpaca    |\n| [LLaMA2](https://huggingface.co/meta-llama)                          | 7B/13B/70B                  | q_proj,v_proj   | llama2    |\n| [LLaMA3](https://huggingface.co/meta-llama)                          | 8B/70B                      | q_proj,v_proj   | llama3    |\n| [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) | 7B/8x7B                     | q_proj,v_proj   | mistral   |\n| [Orion](https://github.com/OrionStarAI/Orion)                        | 14B                         | q_proj,v_proj   | orion     |\n| [Qwen](https://github.com/QwenLM/Qwen)                               | 1.8B/7B/14B/72B             | c_attn          | chatml    |\n| [Qwen1.5](https://github.com/QwenLM/Qwen1.5)                         | 0.5B/1.8B/4B/14B/72B        | q_proj,v_proj   | qwen      |\n| [XVERSE](https://github.com/xverse-ai/XVERSE-13B)                    | 13B                         | query_key_value | xverse    |\n| [Yi](https://github.com/01-ai/Yi)                                    | 6B/34B                      | q_proj,v_proj   | yi        |\n\n\n## 💾 Install\n#### Updating the requirements\nFrom time to time, the `requirements.txt` changes. To update, use this command:\n\n```markdown\ngit clone https://github.com/shibing624/MedicalGPT\ncd MedicalGPT\npip install -r requirements.txt --upgrade\n```\n\n### Hardware Requirement (VRAM)\n\n\n| Train Method  | Bits |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |\n|-------|------| ----- | ----- | ----- | ------ | ------ | ----- | ------ |\n| Full   | AMP  | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |\n| Full   | 16   |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |\n| LoRA  | 16   |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |\n| QLoRA | 8    |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |\n| QLoRA | 4    |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |\n| QLoRA | 2    |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB |\n\n\n## 🔥 Inference\nAfter the training is complete, now we load the trained model to verify the effect of the model generating text.\n\n```shell\npython inference.py \\\n    --base_model path_to_llama_hf_dir \\\n    --lora_model path_to_lora \\\n    --with_prompt \\\n    --interactive\n```\n\nParameter Description:\n\n- `--base_model {base_model}`: Directory to store LLaMA model weights and configuration files in HF format\n- `--lora_model {lora_model}`: The directory where the LoRA file is decompressed, and the name of the HF Model Hub model can also be used. If you have incorporated LoRA weights into the pre-trained model, you can not provide this parameter\n- `--tokenizer_path {tokenizer_path}`: Store the directory corresponding to the tokenizer. If this parameter is not provided, its default value is the same as --lora_model; if the --lora_model parameter is not provided, its default value is the same as --base_model\n- `--with_prompt`: Whether to merge the input with the prompt template. Be sure to enable this option if loading an Alpaca model!\n- `--interactive`: start interactively for multiple single rounds of question and answer\n- `--data_file {file_name}`: Start in non-interactive mode, read the contents of file_name line by line for prediction\n- `--predictions_file {file_name}`: In non-interactive mode, write the predicted results to file_name in json format\n- `--use_cpu`: use only CPU for inference\n- `--gpus {gpu_ids}`: Specifies the number of GPU devices used, the default is 0. If using multiple GPUs, separate them with commas, such as 0,1,2\n\n\n#### Inference Examples\n\n<details>\n| Human | Assistant |\n|-----|------|\n| How to make yourself energetic, list 5 suggestions | Here are some suggestions to keep you energetic: <br/>1. Get enough sleep. Make sure to get enough sleep every night. <br/>2. Exercise more. Doing moderate aerobic exercise, such as jogging and swimming, can help improve your energy and stamina. 3. Eat healthy food. Choose protein-rich foods such as chicken, fish, and eggs, as well as fresh fruits and vegetables. 4. Reduce caffeine intake. Try to avoid drinking caffeinated beverages or eating caffeinated foods during the day. <br/>5. Relax. Try meditation, yoga, or other forms of relaxation to reduce stress and anxiety. |\n</details>\n<br/>\n\n\n## 📚 Dataset\n\n- 2.4 million Chinese medical datasets (including pre-training, instruction fine-tuning and reward datasets): [shibing624/medical](https://huggingface.co/datasets/shibing624/medical)\n\n**Attach links to some general datasets and medical datasets**\n\n- Belle dataset of 500,000 Chinese ChatGPT commands: [BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- Belle dataset of 1 million Chinese ChatGPT commands: [BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- Alpaca dataset of 50,000 English ChatGPT commands: [50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)\n- Alpaca dataset of 20,000 Chinese GPT-4 instructions: [shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)\n- Guanaco dataset with 690,000 Chinese instructions (500,000 Belle + 190,000 Guanaco): [Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)\n- 220,000 Chinese medical dialogue datasets (HuatuoGPT project): [FreedomIntelligence/HuatuoGPT-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1)\n\n## ☎️ Contact\n\n- Issue (suggestion)\n   : [![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)\n- Email me: xuming: xuming624@qq.com\n- WeChat Me: Add me* WeChat ID: xuming624, Remarks: Name-Company Name-NLP* Enter the NLP exchange group.\n\n<img src=\"https://github.com/shibing624/MedicalGPT/blob/main/docs/wechat.jpeg\" width=\"200\" />\n\n## ⚠️ LICENSE\n\nThe license agreement for the project code is [The Apache License 2.0](/LICENSE), the code is free for commercial use, and the model weights and data can only be used for research purposes. Please attach MedicalGPT's link and license agreement in the product description.\n\n## 😇 Citation\n\nIf you used MedicalGPT in your research, please cite as follows:\n\n```latex\n@misc{MedicalGPT,\n   title={MedicalGPT: Training Medical GPT Model},\n   author={Ming Xu},\n   year={2023},\n   howpublished={\\url{https://github.com/shibing624/MedicalGPT}},\n}\n```\n\n## 😍 Contribute\n\nThe project code is still very rough. If you have improved the code, you are welcome to submit it back to this project. Before submitting, please pay attention to the following two points:\n\n- Add corresponding unit tests in `tests`\n- Use `python -m pytest` to run all unit tests to ensure that all unit tests are passed\n\nThen you can submit a PR.\n\n## 💕 Acknowledgements\n\n- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)\n- [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n\nThanks for their great work!\n#### Related Projects\n- [shibing624/ChatPilot](https://github.com/shibing624/ChatPilot): Provide a simple and easy-to-use web UI interface for LLM Agent (including RAG, online search, code interpreter).\n"
        },
        {
          "name": "_config.yml",
          "type": "blob",
          "size": 0.025390625,
          "content": "theme: jekyll-theme-cayman"
        },
        {
          "name": "build_domain_tokenizer.py",
          "type": "blob",
          "size": 2.01953125,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: Build chinese tokenizer from corpus txt\n\n# train sentencepiece model from `corpus.txt` and makes `m.model` and `m.vocab`\n# `m.vocab` is just a reference. not used in the segmentation.\n# spm.SentencePieceTrainer.train('--input=data/pretrain/tianlongbabu.txt --model_prefix=m --vocab_size=20000')\n\"\"\"\nimport argparse\n\nimport sentencepiece as spm\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--in_file', default='data/pretrain/fever.txt', type=str)\n    parser.add_argument('--domain_sp_model_name', default='domain_sp', type=str)\n    parser.add_argument('--max_sentence_length', default=16384, type=int)\n    parser.add_argument('--pad_id', default=3, type=int)\n    parser.add_argument('--vocab_size', default=2236, type=int)\n    parser.add_argument('--model_type', default=\"BPE\", type=str)\n\n    args = parser.parse_args()\n    print(args)\n\n    spm.SentencePieceTrainer.train(\n        input=args.in_file,\n        model_prefix=args.domain_sp_model_name,\n        shuffle_input_sentence=False,\n        train_extremely_large_corpus=True,\n        max_sentence_length=args.max_sentence_length,\n        pad_id=args.pad_id,\n        model_type=args.model_type,\n        vocab_size=args.vocab_size,\n        split_digits=True,\n        split_by_unicode_script=True,\n        byte_fallback=True,\n        allow_whitespace_only_pieces=True,\n        remove_extra_whitespaces=False,\n        normalization_rule_name=\"nfkc\",\n    )\n\n    # makes segmenter instance and loads the model file (m.model)\n    sp = spm.SentencePieceProcessor()\n    model_file = args.domain_sp_model_name + '.model'\n    sp.load(model_file)\n\n    # encode: text => id\n    print(sp.encode_as_pieces('潜伏性感染又称潜在性感染。慕容复来到河边,this is a test'))\n    print(sp.encode_as_ids('this is a test'))\n\n    # decode: id => text\n    print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n    # print(sp.decode_ids([209, 31, 9, 375, 586]))\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "chatpdf.py",
          "type": "blob",
          "size": 19.3115234375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description:\n\npip install similarities PyPDF2 -U\n\"\"\"\nimport argparse\nimport hashlib\nimport os\nimport re\nfrom threading import Thread\nfrom typing import Union, List\n\nimport jieba\nimport torch\nfrom loguru import logger\nfrom peft import PeftModel\nfrom similarities import (\n    EnsembleSimilarity,\n    BertSimilarity,\n    BM25Similarity,\n)\nfrom similarities.similarity import SimilarityABC\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    TextIteratorStreamer,\n    GenerationConfig,\n)\n\njieba.setLogLevel(\"ERROR\")\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, LlamaTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\nRAG_PROMPT = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分，答案请使用中文。\n\n已知内容:\n{context_str}\n\n问题:\n{query_str}\n\"\"\"\n\n\nclass SentenceSplitter:\n    def __init__(self, chunk_size: int = 250, chunk_overlap: int = 50):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n\n    def split_text(self, text: str) -> List[str]:\n        if self._is_has_chinese(text):\n            return self._split_chinese_text(text)\n        else:\n            return self._split_english_text(text)\n\n    def _split_chinese_text(self, text: str) -> List[str]:\n        sentence_endings = {'\\n', '。', '！', '？', '；', '…'}  # 句末标点符号\n        chunks, current_chunk = [], ''\n        for word in jieba.cut(text):\n            if len(current_chunk) + len(word) > self.chunk_size:\n                chunks.append(current_chunk.strip())\n                current_chunk = word\n            else:\n                current_chunk += word\n            if word[-1] in sentence_endings and len(current_chunk) > self.chunk_size - self.chunk_overlap:\n                chunks.append(current_chunk.strip())\n                current_chunk = ''\n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        if self.chunk_overlap > 0 and len(chunks) > 1:\n            chunks = self._handle_overlap(chunks)\n        return chunks\n\n    def _split_english_text(self, text: str) -> List[str]:\n        # 使用正则表达式按句子分割英文文本\n        sentences = re.split(r'(?<=[.!?])\\s+', text.replace('\\n', ' '))\n        chunks, current_chunk = [], ''\n        for sentence in sentences:\n            if len(current_chunk) + len(sentence) <= self.chunk_size or not current_chunk:\n                current_chunk += (' ' if current_chunk else '') + sentence\n            else:\n                chunks.append(current_chunk)\n                current_chunk = sentence\n        if current_chunk:  # Add the last chunk\n            chunks.append(current_chunk)\n\n        if self.chunk_overlap > 0 and len(chunks) > 1:\n            chunks = self._handle_overlap(chunks)\n\n        return chunks\n\n    def _is_has_chinese(self, text: str) -> bool:\n        # check if contains chinese characters\n        if any(\"\\u4e00\" <= ch <= \"\\u9fff\" for ch in text):\n            return True\n        else:\n            return False\n\n    def _handle_overlap(self, chunks: List[str]) -> List[str]:\n        # 处理块间重叠\n        overlapped_chunks = []\n        for i in range(len(chunks) - 1):\n            chunk = chunks[i] + ' ' + chunks[i + 1][:self.chunk_overlap]\n            overlapped_chunks.append(chunk.strip())\n        overlapped_chunks.append(chunks[-1])\n        return overlapped_chunks\n\n\nclass ChatPDF:\n    def __init__(\n            self,\n            similarity_model: SimilarityABC = None,\n            generate_model_type: str = \"auto\",\n            generate_model_name_or_path: str = \"01-ai/Yi-6B-Chat\",\n            lora_model_name_or_path: str = None,\n            corpus_files: Union[str, List[str]] = None,\n            save_corpus_emb_dir: str = \"./corpus_embs/\",\n            device: str = None,\n            int8: bool = False,\n            int4: bool = False,\n            chunk_size: int = 250,\n            chunk_overlap: int = 30,\n            prompt_template_name: str = None,\n    ):\n        \"\"\"\n        Init RAG model.\n        :param similarity_model: similarity model, default None, if set, will use it instead of EnsembleSimilarity\n        :param generate_model_type: generate model type\n        :param generate_model_name_or_path: generate model name or path\n        :param lora_model_name_or_path: lora model name or path\n        :param corpus_files: corpus files\n        :param save_corpus_emb_dir: save corpus embeddings dir, default ./corpus_embs/\n        :param device: device, default None, auto select gpu or cpu\n        :param int8: use int8 quantization, default False\n        :param int4: use int4 quantization, default False\n        :param chunk_size: chunk size, default 250\n        :param chunk_overlap: chunk overlap, default 50\n        :param prompt_template_name: prompt template name, default None, if set, inplace tokenizer.apply_chat_template\n        \"\"\"\n        if torch.cuda.is_available():\n            default_device = torch.device(0)\n        elif torch.backends.mps.is_available():\n            default_device = 'mps'\n        else:\n            default_device = torch.device('cpu')\n        self.device = device or default_device\n        self.text_splitter = SentenceSplitter(chunk_size, chunk_overlap)\n        if similarity_model is not None:\n            self.sim_model = similarity_model\n        else:\n            m1 = BertSimilarity(model_name_or_path=\"shibing624/text2vec-base-multilingual\", device=self.device)\n            m2 = BM25Similarity()\n            default_sim_model = EnsembleSimilarity(similarities=[m1, m2], weights=[0.5, 0.5], c=2)\n            self.sim_model = default_sim_model\n        self.gen_model, self.tokenizer = self._init_gen_model(\n            generate_model_type,\n            generate_model_name_or_path,\n            peft_name=lora_model_name_or_path,\n            int8=int8,\n            int4=int4,\n        )\n        self.history = []\n        self.corpus_files = corpus_files\n        if corpus_files:\n            self.add_corpus(corpus_files)\n        self.save_corpus_emb_dir = save_corpus_emb_dir\n        self.prompt_template_name = prompt_template_name\n\n    def __str__(self):\n        return f\"Similarity model: {self.sim_model}, Generate model: {self.gen_model}\"\n\n    def _init_gen_model(\n            self,\n            gen_model_type: str,\n            gen_model_name_or_path: str,\n            peft_name: str = None,\n            int8: bool = False,\n            int4: bool = False,\n    ):\n        \"\"\"Init generate model.\"\"\"\n        if int8 or int4:\n            device_map = None\n        else:\n            device_map = \"auto\"\n        model_class, tokenizer_class = MODEL_CLASSES[gen_model_type]\n        tokenizer = tokenizer_class.from_pretrained(gen_model_name_or_path, trust_remote_code=True)\n        model = model_class.from_pretrained(\n            gen_model_name_or_path,\n            load_in_8bit=int8 if gen_model_type not in ['baichuan', 'chatglm'] else False,\n            load_in_4bit=int4 if gen_model_type not in ['baichuan', 'chatglm'] else False,\n            torch_dtype=\"auto\",\n            device_map=device_map,\n            trust_remote_code=True,\n        )\n        if self.device == torch.device('cpu'):\n            model.float()\n        if gen_model_type in ['baichuan', 'chatglm']:\n            if int4:\n                model = model.quantize(4).cuda()\n            elif int8:\n                model = model.quantize(8).cuda()\n        try:\n            model.generation_config = GenerationConfig.from_pretrained(gen_model_name_or_path, trust_remote_code=True)\n        except Exception as e:\n            logger.warning(f\"Failed to load generation config from {gen_model_name_or_path}, {e}\")\n        if peft_name:\n            model = PeftModel.from_pretrained(\n                model,\n                peft_name,\n                torch_dtype=\"auto\",\n            )\n            logger.info(f\"Loaded peft model from {peft_name}\")\n        model.eval()\n        return model, tokenizer\n\n    def _get_chat_input(self):\n        messages = []\n        if self.prompt_template_name:\n            from template import get_conv_template\n            prompt_template = get_conv_template(self.prompt_template_name)\n            prompt = prompt_template.get_prompt(messages=self.history)\n            input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids\n        else:\n            for conv in self.history:\n                if conv and len(conv) > 0 and conv[0]:\n                    messages.append({'role': 'user', 'content': conv[0]})\n                if conv and len(conv) > 1 and conv[1]:\n                    messages.append({'role': 'assistant', 'content': conv[1]})\n            input_ids = self.tokenizer.apply_chat_template(\n                conversation=messages,\n                tokenize=True,\n                add_generation_prompt=True,\n                return_tensors='pt'\n            )\n        return input_ids.to(self.gen_model.device)\n\n    @torch.inference_mode()\n    def stream_generate_answer(\n            self,\n            max_new_tokens=512,\n            temperature=0.7,\n            repetition_penalty=1.0,\n            context_len=2048\n    ):\n        streamer = TextIteratorStreamer(self.tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n        input_ids = self._get_chat_input()\n        max_src_len = context_len - max_new_tokens - 8\n        input_ids = input_ids[-max_src_len:]\n        generation_kwargs = dict(\n            input_ids=input_ids,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            do_sample=True,\n            repetition_penalty=repetition_penalty,\n            streamer=streamer,\n        )\n        thread = Thread(target=self.gen_model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        yield from streamer\n\n    def add_corpus(self, files: Union[str, List[str]]):\n        \"\"\"Load document files.\"\"\"\n        if isinstance(files, str):\n            files = [files]\n        for doc_file in files:\n            if doc_file.endswith('.pdf'):\n                corpus = self.extract_text_from_pdf(doc_file)\n            elif doc_file.endswith('.docx'):\n                corpus = self.extract_text_from_docx(doc_file)\n            elif doc_file.endswith('.md'):\n                corpus = self.extract_text_from_markdown(doc_file)\n            else:\n                corpus = self.extract_text_from_txt(doc_file)\n            full_text = '\\n'.join(corpus)\n            chunks = self.text_splitter.split_text(full_text)\n            self.sim_model.add_corpus(chunks)\n        self.corpus_files = files\n        logger.debug(f\"files: {files}, corpus size: {len(self.sim_model.corpus)}, top3: \"\n                     f\"{list(self.sim_model.corpus.values())[:3]}\")\n\n    @staticmethod\n    def get_file_hash(fpaths):\n        hasher = hashlib.md5()\n        target_file_data = bytes()\n        if isinstance(fpaths, str):\n            fpaths = [fpaths]\n        for fpath in fpaths:\n            with open(fpath, 'rb') as file:\n                chunk = file.read(1024 * 1024)  # read only first 1MB\n                hasher.update(chunk)\n                target_file_data += chunk\n\n        hash_name = hasher.hexdigest()[:32]\n        return hash_name\n\n    @staticmethod\n    def extract_text_from_pdf(file_path: str):\n        \"\"\"Extract text content from a PDF file.\"\"\"\n        import PyPDF2\n        contents = []\n        with open(file_path, 'rb') as f:\n            pdf_reader = PyPDF2.PdfReader(f)\n            for page in pdf_reader.pages:\n                page_text = page.extract_text().strip()\n                raw_text = [text.strip() for text in page_text.splitlines() if text.strip()]\n                new_text = ''\n                for text in raw_text:\n                    new_text += text\n                    if text[-1] in ['.', '!', '?', '。', '！', '？', '…', ';', '；', ':', '：', '”', '’', '）', '】', '》', '」',\n                                    '』', '〕', '〉', '》', '〗', '〞', '〟', '»', '\"', \"'\", ')', ']', '}']:\n                        contents.append(new_text)\n                        new_text = ''\n                if new_text:\n                    contents.append(new_text)\n        return contents\n\n    @staticmethod\n    def extract_text_from_txt(file_path: str):\n        \"\"\"Extract text content from a TXT file.\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            contents = [text.strip() for text in f.readlines() if text.strip()]\n        return contents\n\n    @staticmethod\n    def extract_text_from_docx(file_path: str):\n        \"\"\"Extract text content from a DOCX file.\"\"\"\n        import docx\n        document = docx.Document(file_path)\n        contents = [paragraph.text.strip() for paragraph in document.paragraphs if paragraph.text.strip()]\n        return contents\n\n    @staticmethod\n    def extract_text_from_markdown(file_path: str):\n        \"\"\"Extract text content from a Markdown file.\"\"\"\n        import markdown\n        from bs4 import BeautifulSoup\n        with open(file_path, 'r', encoding='utf-8') as f:\n            markdown_text = f.read()\n        html = markdown.markdown(markdown_text)\n        soup = BeautifulSoup(html, 'html.parser')\n        contents = [text.strip() for text in soup.get_text().splitlines() if text.strip()]\n        return contents\n\n    @staticmethod\n    def _add_source_numbers(lst):\n        \"\"\"Add source numbers to a list of strings.\"\"\"\n        return [f'[{idx + 1}]\\t \"{item}\"' for idx, item in enumerate(lst)]\n\n    def predict_stream(\n            self,\n            query: str,\n            topn: int = 5,\n            max_length: int = 512,\n            context_len: int = 2048,\n            temperature: float = 0.7,\n    ):\n        \"\"\"Generate predictions stream.\"\"\"\n        reference_results = []\n        stop_str = self.tokenizer.eos_token if self.tokenizer.eos_token else \"</s>\"\n        if self.sim_model.corpus:\n            sim_contents = self.sim_model.most_similar(query, topn=topn)\n            # Get reference results\n            for query_id, id_score_dict in sim_contents.items():\n                for corpus_id, s in id_score_dict.items():\n                    reference_results.append(self.sim_model.corpus[corpus_id])\n            if not reference_results:\n                yield '没有提供足够的相关信息', reference_results\n            self.history = []\n            reference_results = self._add_source_numbers(reference_results)\n            context_str = '\\n'.join(reference_results)[:(context_len - len(RAG_PROMPT))]\n            prompt = RAG_PROMPT.format(context_str=context_str, query_str=query)\n            # logger.debug(f\"prompt: {prompt}\")\n        else:\n            prompt = query\n            logger.debug(prompt)\n        self.history.append([prompt, ''])\n        response = \"\"\n        for new_text in self.stream_generate_answer(\n                max_new_tokens=max_length,\n                temperature=temperature,\n                context_len=context_len,\n        ):\n            if new_text != stop_str:\n                response += new_text\n                yield response\n\n    def predict(\n            self,\n            query: str,\n            topn: int = 5,\n            max_length: int = 512,\n            context_len: int = 2048,\n            temperature: float = 0.7,\n            do_print: bool = False,\n    ):\n        \"\"\"Query from corpus.\"\"\"\n        reference_results = []\n        if self.sim_model.corpus:\n            sim_contents = self.sim_model.most_similar(query, topn=topn)\n            # Get reference results\n            for query_id, id_score_dict in sim_contents.items():\n                for corpus_id, s in id_score_dict.items():\n                    reference_results.append(self.sim_model.corpus[corpus_id])\n            if not reference_results:\n                return '没有提供足够的相关信息', reference_results\n            self.history = []\n            reference_results = self._add_source_numbers(reference_results)\n            context_str = '\\n'.join(reference_results)[:(context_len - len(RAG_PROMPT))]\n            prompt = RAG_PROMPT.format(context_str=context_str, query_str=query)\n            # logger.debug(f\"prompt: {prompt}\")\n        else:\n            prompt = query\n        self.history.append([prompt, ''])\n        response = \"\"\n        for new_text in self.stream_generate_answer(\n                max_new_tokens=max_length,\n                temperature=temperature,\n                context_len=context_len,\n        ):\n            response += new_text\n            if do_print:\n                print(new_text, end=\"\", flush=True)\n        if do_print:\n            print(\"\", flush=True)\n        response = response.strip()\n        self.history[-1][1] = response\n        return response, reference_results\n\n    def save_corpus_emb(self):\n        dir_name = self.get_file_hash(self.corpus_files)\n        save_dir = os.path.join(self.save_corpus_emb_dir, dir_name)\n        if hasattr(self.sim_model, 'save_corpus_embeddings'):\n            self.sim_model.save_corpus_embeddings(save_dir)\n            logger.debug(f\"Saving corpus embeddings to {save_dir}\")\n        return save_dir\n\n    def load_corpus_emb(self, emb_dir: str):\n        if hasattr(self.sim_model, 'load_corpus_embeddings'):\n            logger.debug(f\"Loading corpus embeddings from {emb_dir}\")\n            self.sim_model.load_corpus_embeddings(emb_dir)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--sim_model\", type=str, default=\"shibing624/text2vec-base-multilingual\")\n    parser.add_argument(\"--gen_model_type\", type=str, default=\"auto\")\n    parser.add_argument(\"--gen_model\", type=str, default=\"01-ai/Yi-6B-Chat\")\n    parser.add_argument(\"--prompt_template_name\", type=str, default=None,\n                        help=\"The prompt template name. it can be vicuna/alpaca/yi..., None is use apply_chat_template.\")\n    parser.add_argument(\"--lora_model\", type=str, default=None)\n    parser.add_argument(\"--corpus_files\", type=str, default=\"data/rag/medical_corpus.txt\")\n    parser.add_argument(\"--device\", type=str, default=None)\n    parser.add_argument(\"--int4\", action='store_true', help=\"use int4 quantization\")\n    parser.add_argument(\"--int8\", action='store_true', help=\"use int8 quantization\")\n    parser.add_argument(\"--chunk_size\", type=int, default=100)\n    parser.add_argument(\"--chunk_overlap\", type=int, default=5)\n    args = parser.parse_args()\n    print(args)\n    sim_model = BertSimilarity(model_name_or_path=args.sim_model, device=args.device)\n    m = ChatPDF(\n        similarity_model=sim_model,\n        generate_model_type=args.gen_model_type,\n        generate_model_name_or_path=args.gen_model,\n        lora_model_name_or_path=args.lora_model,\n        device=args.device,\n        int4=args.int4,\n        int8=args.int8,\n        chunk_size=args.chunk_size,\n        chunk_overlap=args.chunk_overlap,\n        corpus_files=args.corpus_files.split(','),\n        prompt_template_name=args.prompt_template_name,\n    )\n    query = [\n        \"维胺酯维E乳膏能治理什么疾病\",\n        \"天雄的药用植物栽培是什么\",\n        \"膺窗穴的定位是什么\",\n    ]\n    for i in query:\n        response, reference_results = m.predict(i)\n        print(f\"===\")\n        print(f\"Input: {i}\")\n        print(f\"Reference: {reference_results}\")\n        print(f\"Output: {response}\")\n"
        },
        {
          "name": "convert_dataset.py",
          "type": "blob",
          "size": 2.6083984375,
          "content": "\"\"\"\nConvert alpaca dataset into sharegpt format.\n\nUsage: python convert_dataset.py --in_file alpaca_data.json --out_file alpaca_data_sharegpt.jsonl\n\"\"\"\n\nimport argparse\n\nfrom datasets import load_dataset\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--in_file\", type=str, required=True)\n    parser.add_argument(\"--out_file\", type=str, required=True)\n    parser.add_argument(\"--data_type\", type=str, default='alpaca', help=\"alpaca, qa, or sharegpt\")\n    parser.add_argument(\"--file_type\", type=str, default='json', help='input file type, json or csv')\n    args = parser.parse_args()\n    print(args)\n    data_files = {\"train\": args.in_file}\n    if args.file_type == 'csv':\n        if args.data_type in ['qa']:\n            column_names = ['input', 'output']\n        else:\n            column_names = ['instruction', 'input', 'output']\n        raw_datasets = load_dataset('csv', data_files=data_files, column_names=column_names, delimiter='\\t')\n    elif args.file_type in ['json', 'jsonl']:\n        raw_datasets = load_dataset('json', data_files=data_files)\n    else:\n        raise ValueError(\"File type not supported\")\n    ds = raw_datasets['train']\n\n\n    def process_qa(examples):\n        convs = []\n        for q, a in zip(examples['input'], examples['output']):\n            convs.append([\n                {\"from\": \"human\", \"value\": q},\n                {\"from\": \"gpt\", \"value\": a}\n            ])\n        return {\"conversations\": convs}\n\n\n    def process_alpaca(examples):\n        convs = []\n        for instruction, inp, output in zip(examples['instruction'], examples['input'], examples['output']):\n            if inp and len(inp.strip()) > 0:\n                instruction = instruction + '\\n\\n' + inp\n            q = instruction\n            a = output\n            convs.append([\n                {\"from\": \"human\", \"value\": q},\n                {\"from\": \"gpt\", \"value\": a}\n            ])\n        return {\"conversations\": convs}\n\n\n    if args.data_type in ['alpaca']:\n        ds = ds.map(process_alpaca, batched=True, remove_columns=ds.column_names, desc=\"Running process\")\n    elif args.data_type in ['qa']:\n        ds = ds.map(process_qa, batched=True, remove_columns=ds.column_names, desc=\"Running process\")\n    else:\n        # Other sharegpt dataset, need rename to conversations and remove unused columns\n        if \"items\" in ds.column_names:\n            ds = ds.rename(columns={\"items\": \"conversations\"})\n        columns_to_remove = ds.column_names.copy()\n        columns_to_remove.remove('conversations')\n        ds = ds.remove_columns(columns_to_remove)\n\n    ds.to_json(f\"{args.out_file}\", lines=True, force_ascii=False)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "deepspeed_zero_stage2_config.json",
          "type": "blob",
          "size": 1.1025390625,
          "content": "{\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\",\n            \"torch_adam\": true,\n            \"adam_w_mode\": true\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    },\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true\n    },\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 1000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}"
        },
        {
          "name": "deepspeed_zero_stage3_config.json",
          "type": "blob",
          "size": 1.205078125,
          "content": "{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"sub_group_size\": 1e9,\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": \"auto\"\n    },\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dpo_training.py",
          "type": "blob",
          "size": 22.302734375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: Train a model from SFT using DPO\n\"\"\"\nimport os\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import Dict, Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType\nfrom transformers import (\n    AutoConfig,\n    BloomForCausalLM,\n    AutoModelForCausalLM,\n    AutoModel,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n    TrainingArguments,\n    BitsAndBytesConfig,\n)\nfrom transformers.deepspeed import is_deepspeed_zero3_enabled\nfrom trl import DPOTrainer\n\nfrom template import get_conv_template\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with DPO\n    \"\"\"\n    # Model arguments\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The model checkpoint for weights initialization.\"}\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The tokenizer for weights initialization.\"}\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n    # Dataset arguments\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The input jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The prompt template name.\"})\n    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"Train batch size per device\"})\n    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"Eval batch size per device\"})\n    max_source_length: Optional[int] = field(default=2048, metadata={\"help\": \"Max length of prompt input text\"})\n    max_target_length: Optional[int] = field(default=512, metadata={\"help\": \"Max length of output text\"})\n    min_target_length: Optional[int] = field(default=4, metadata={\"help\": \"Min length of output text\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=4, metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    # Training arguments\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})\n    target_modules: Optional[str] = field(default=None)\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=16.0)\n    peft_path: Optional[str] = field(default=None)\n    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the validation set.\"})\n    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"The beta parameter for DPO loss\"})\n    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"Learning rate\"})\n    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"The lr scheduler type\"})\n    warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"The number of warmup steps\"})\n    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"The weight decay\"})\n    optim: Optional[str] = field(default=\"adamw_hf\", metadata={\"help\": \"The optimizer type\"})\n    fp16: Optional[bool] = field(default=True, metadata={\"help\": \"Whether to use fp16\"})\n    bf16: Optional[bool] = field(default=False, metadata={\"help\": \"Whether to use bf16\"})\n    gradient_checkpointing: Optional[bool] = field(\n        default=True, metadata={\"help\": \"Whether to use gradient checkpointing\"}\n    )\n    gradient_accumulation_steps: Optional[int] = field(\n        default=4, metadata={\"help\": \"The number of gradient accumulation steps\"}\n    )\n    save_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to save the model\"})\n    eval_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to evaluate the model\"})\n    logging_steps: Optional[int] = field(default=1, metadata={\"help\": \"X steps to log the model\"})\n    output_dir: Optional[str] = field(default=\"outputs-dpo\", metadata={\"help\": \"The output directory\"})\n    max_steps: Optional[int] = field(default=200, metadata={\"help\": \"Number of steps to train\"})\n    eval_strategy: Optional[str] = field(default=\"steps\", metadata={\"help\": \"Evaluation strategy\"})\n    remove_unused_columns: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Remove unused columns from the dataset if `datasets.Dataset` is used\"},\n    )\n    report_to: Optional[str] = field(default=\"tensorboard\", metadata={\"help\": \"Report to wandb or tensorboard\"})\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\"You must specify a valid model_type to run training.\")\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'output_layer' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\ndef main():\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    logger.info(f\"Parse args: {args}\")\n\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    if args.model_type == 'bloom':\n        args.use_fast_tokenizer = True\n    # Load tokenizer\n    tokenizer_kwargs = {\n        \"cache_dir\": args.cache_dir,\n        \"use_fast\": args.use_fast_tokenizer,\n        \"trust_remote_code\": args.trust_remote_code,\n    }\n    tokenizer_name_or_path = args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    # Get datasets\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name,\n            args.dataset_config_name,\n            cache_dir=args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    else:\n        data_files = {}\n        if args.train_file_dir is not None and os.path.exists(args.train_file_dir):\n            train_data_files = glob(f'{args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {', '.join(train_data_files)}\")\n            data_files[\"train\"] = train_data_files\n        if args.validation_file_dir is not None and os.path.exists(args.validation_file_dir):\n            eval_data_files = glob(f'{args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    max_source_length = args.max_source_length\n    max_target_length = args.max_target_length\n    full_max_length = max_source_length + max_target_length\n\n    def return_prompt_and_responses(examples) -> Dict[str, str]:\n        \"\"\"Load the paired dataset and convert it to the necessary format.\n\n        The dataset is converted to a dictionary with the following structure:\n        {\n            'prompt': List[str],\n            'chosen': List[str],\n            'rejected': List[str],\n        }\n\n        Prompts are structured as follows:\n          system_prompt + history[[q,a], [q,a]...] + question\n        \"\"\"\n        prompts = []\n        for system, history, question in zip(examples[\"system\"], examples[\"history\"], examples[\"question\"]):\n            system_prompt = system or \"\"\n            history_with_question = history + [[question, '']] if history else [[question, '']]\n            prompts.append(prompt_template.get_prompt(messages=history_with_question, system_prompt=system_prompt))\n        return {\n            \"prompt\": prompts,\n            \"chosen\": examples[\"response_chosen\"],\n            \"rejected\": examples[\"response_rejected\"],\n        }\n\n    # Preprocess the dataset\n    train_dataset = None\n    max_train_samples = 0\n    if args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train']\n        max_train_samples = len(train_dataset)\n        if args.max_train_samples is not None and args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        tokenized_dataset = train_dataset.shuffle().map(\n            return_prompt_and_responses,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=train_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        train_dataset = tokenized_dataset.filter(\n            lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n                      and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n        )\n        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n        logger.debug(\"First train example:\")\n        first_example = train_dataset[0]\n        logger.debug(f\"prompt:\\n{first_example['prompt']}\")\n        logger.debug(f\"chosen:\\n{first_example['chosen']}\")\n        logger.debug(f\"rejected:\\n{first_example['rejected']}\")\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if args.do_eval:\n        if \"validation\" not in raw_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = raw_datasets[\"validation\"]\n        max_eval_samples = len(eval_dataset)\n        if args.max_eval_samples is not None and args.max_eval_samples > 0:\n            max_eval_samples = min(len(eval_dataset), args.max_eval_samples)\n            eval_dataset = eval_dataset.select(range(max_eval_samples))\n        logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n        eval_dataset = eval_dataset.map(\n            return_prompt_and_responses,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=eval_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        eval_dataset = eval_dataset.filter(\n            lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n                      and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n        )\n        logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n        logger.debug(\"First eval example:\")\n        first_example = eval_dataset[0]\n        logger.debug(f\"prompt:\\n{first_example['prompt']}\")\n        logger.debug(f\"chosen:\\n{first_example['chosen']}\")\n        logger.debug(f\"rejected:\\n{first_example['rejected']}\")\n\n    # Load model\n    torch_dtype = (\n        args.torch_dtype\n        if args.torch_dtype in [\"auto\", None]\n        else getattr(torch, args.torch_dtype)\n    )\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    ddp = world_size != 1\n    if ddp:\n        args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n    logger.info(f\"Device map: {args.device_map}\")\n    if args.qlora and is_deepspeed_zero3_enabled():\n        logger.warning(\"ZeRO3 are both currently incompatible with QLoRA.\")\n    config = config_class.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=args.trust_remote_code,\n        torch_dtype=torch_dtype,\n        cache_dir=args.cache_dir\n    )\n    if args.load_in_4bit or args.load_in_8bit:\n        logger.info(f\"Quantizing model, load_in_4bit: {args.load_in_4bit}, load_in_8bit: {args.load_in_8bit}\")\n    model = model_class.from_pretrained(\n        args.model_name_or_path,\n        config=config,\n        torch_dtype=torch_dtype,\n        low_cpu_mem_usage=(not is_deepspeed_zero3_enabled()),\n        device_map=args.device_map,\n        trust_remote_code=args.trust_remote_code,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.load_in_4bit,\n            load_in_8bit=args.load_in_8bit,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch_dtype,\n        ) if args.qlora else None,\n    )\n    # fixed FP16 ValueError\n    for param in filter(lambda p: p.requires_grad, model.parameters()):\n        param.data = param.data.to(torch.float32)\n\n    # Initialize our Trainer\n    if args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n    else:\n        model.config.use_cache = True\n\n    training_args = TrainingArguments(\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        per_device_eval_batch_size=args.per_device_eval_batch_size,\n        max_steps=args.max_steps,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        gradient_checkpointing=args.gradient_checkpointing,\n        learning_rate=args.learning_rate,\n        evaluation_strategy=args.eval_strategy,\n        eval_steps=args.eval_steps,\n        output_dir=args.output_dir,\n        report_to=args.report_to,\n        lr_scheduler_type=args.lr_scheduler_type,\n        warmup_steps=args.warmup_steps,\n        optim=args.optim,\n        bf16=args.bf16,\n        fp16=args.fp16,\n        remove_unused_columns=args.remove_unused_columns,\n        run_name=f\"dpo_{args.model_type}\",\n    )\n\n    # Initialize DPO trainer\n    peft_config = None\n    if args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        target_modules = args.target_modules.split(',') if args.target_modules else None\n        if target_modules and 'all' in target_modules:\n            target_modules = find_all_linear_names(model, int4=args.load_in_4bit, int8=args.load_in_8bit)\n        logger.info(f\"Peft target_modules: {target_modules}\")\n        peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM,\n            target_modules=target_modules,\n            inference_mode=False,\n            r=args.lora_rank,\n            lora_alpha=args.lora_alpha,\n            lora_dropout=args.lora_dropout,\n        )\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n    trainer = DPOTrainer(\n        model,\n        ref_model=None if args.use_peft else deepcopy(model),\n        args=training_args,\n        beta=args.beta,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        peft_config=peft_config if args.use_peft else None,\n        max_prompt_length=args.max_source_length,\n        max_length=full_max_length,\n    )\n    print_trainable_parameters(trainer.model)\n\n    # Training\n    if args.do_train:\n        logger.info(\"*** Train ***\")\n        train_result = trainer.train()\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {args.output_dir}\")\n            trainer.save_model(args.output_dir)\n            tokenizer.save_pretrained(args.output_dir)\n            trainer.model.save_pretrained(args.output_dir)\n\n    # Evaluation\n    if args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n        metrics[\"eval_samples\"] = max_eval_samples\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "eval_quantize.py",
          "type": "blob",
          "size": 5.162109375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@description: eval quantize for jsonl format data\n\nusage:\npython eval_quantize.py --bnb_path /path/to/your/bnb_model --data_path data/finetune/medical_sft_1K_format.jsonl\n\"\"\"\nimport torch\nimport json\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport GPUtil\nimport argparse\nimport logging\nimport os\n\n# 设置日志记录\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# 创建参数解析器\nparser = argparse.ArgumentParser(description=\"========量化困惑度测试========\")\nparser.add_argument(\n    \"--bnb_path\",  \n    type=str,  \n    required=True,  # 设置为必须的参数\n    help=\"bnb量化后的模型路径。\"  \n)\nparser.add_argument(\n    \"--data_path\",  \n    type=str,  \n    required=True,  # 设置为必须的参数\n    help=\"jsonl数据集路径。\"  \n)\n\n# 设备选择函数\ndef get_device():\n    if torch.backends.mps.is_available():\n        return \"mps\"\n    elif torch.cuda.is_available():\n        return \"cuda:0\"\n    else:\n        return \"cpu\"\n\n# 清理GPU缓存函数\ndef clear_gpu_cache():\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n# 从jsonl文件中加载数据\ndef load_jsonl_data(file_path):\n    logger.info(f\"Loading data from {file_path}\")\n    conversations = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                data = json.loads(line)\n                # 提取 human 和 gpt 部分的文本\n                for conv in data['conversations']:\n                    if conv['from'] == 'human':\n                        input_text = conv['value']\n                    elif conv['from'] == 'gpt':\n                        target_text = conv['value']\n                        conversations.append((input_text, target_text))\n        return conversations\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        return []\n\n# 困惑度评估函数\ndef evaluate_perplexity(model, tokenizer, conversation_pairs):\n    def _perplexity(nlls, n_samples, seqlen):\n        try:\n            return torch.exp(torch.stack(nlls).sum() / (n_samples * seqlen))\n        except Exception as e:\n            logger.error(f\"Error calculating perplexity: {e}\")\n            return float('inf')\n\n    model = model.eval()\n    nlls = []\n    # 获取设备\n    device = get_device()\n\n    # 确保 tokenizer 和 model 使用相同的设备\n    model = model.to(device)\n    # 遍历每个对话，基于 human 部分生成并与 gpt 部分计算困惑度\n    for input_text, target_text in tqdm(conversation_pairs, desc=\"Perplexity Evaluation\"):\n        # Tokenize input and target\n        inputs = tokenizer(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).input_ids.to(get_device())\n        target_ids = tokenizer(target_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).input_ids.to(get_device())\n\n        # Ensure both inputs and target have the same length\n        if inputs.size(1) != target_ids.size(1):\n            logger.warning(f\"Input length {inputs.size(1)} and Target length {target_ids.size(1)} are not equal.\")\n        \n        # Forward pass\n        with torch.no_grad():\n            outputs = model(input_ids=inputs, labels=target_ids)\n            loss = outputs.loss\n            nlls.append(loss * target_ids.size(1))  # loss * sequence length\n\n\n\n    # 计算最终困惑度\n    total_samples = len(conversation_pairs)\n    total_length = sum([len(pair[1]) for pair in conversation_pairs])\n    ppl = _perplexity(nlls, total_samples, total_length)\n    logger.info(f\"Final Perplexity: {ppl:.3f}\")\n\n    return ppl.item()\n\n# 主函数\nif __name__ == \"__main__\":\n    \n    args = parser.parse_args()\n\n    if not os.path.exists(args.bnb_path):\n        logger.error(f\"Model path {args.bnb_path} does not exist.\")\n        exit(1)\n\n    try:\n        # 设置BNB量化配置\n        from accelerate.utils import BnbQuantizationConfig\n        bnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n        \n        logger.info(f\"Loading BNB model from: {args.bnb_path}\")\n        tokenizer = AutoTokenizer.from_pretrained(args.bnb_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(args.bnb_path, trust_remote_code=True)\n        \n        # 检查GPU使用情况\n        if torch.cuda.is_available():\n            gpu_usage = GPUtil.getGPUs()[0].memoryUsed\n            logger.info(f\"GPU usage before evaluation: {round(gpu_usage/1024, 2)} GB\")\n        \n        # 加载jsonl数据\n        conversation_pairs = load_jsonl_data(args.data_path)\n        \n        if not conversation_pairs:\n            logger.error(\"No valid conversation pairs found.\")\n            exit(1)\n\n        # 开始评估\n        evaluate_perplexity(model, tokenizer, conversation_pairs)\n        \n        # 评估完毕，清理模型和缓存\n        del model\n        clear_gpu_cache()\n        logger.info(\"Evaluation completed and GPU cache cleared.\")\n    \n    except Exception as e:\n        logger.error(f\"An error occurred: {e}\")\n"
        },
        {
          "name": "fastapi_server_demo.py",
          "type": "blob",
          "size": 6.736328125,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: api start demo\n\nusage:\nCUDA_VISIBLE_DEVICES=0 python fastapi_server_demo.py --model_type bloom --base_model bigscience/bloom-560m\n\ncurl -X 'POST' 'http://0.0.0.0:8008/chat' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"input\": \"咏鹅--骆宾王；登黄鹤楼--\"\n}'\n\"\"\"\n\nimport argparse\nimport os\nfrom threading import Thread\n\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI\nfrom loguru import logger\nfrom peft import PeftModel\nfrom pydantic import BaseModel, Field\nfrom starlette.middleware.cors import CORSMiddleware\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaForCausalLM,\n    TextIteratorStreamer,\n    GenerationConfig,\n)\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@torch.inference_mode()\ndef stream_generate_answer(\n        model,\n        tokenizer,\n        prompt,\n        device,\n        do_print=True,\n        max_new_tokens=512,\n        repetition_penalty=1.0,\n        context_len=2048,\n        stop_str=\"</s>\",\n):\n    streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n    input_ids = tokenizer(prompt).input_ids\n    max_src_len = context_len - max_new_tokens - 8\n    input_ids = input_ids[-max_src_len:]\n    generation_kwargs = dict(\n        input_ids=torch.as_tensor([input_ids]).to(device),\n        max_new_tokens=max_new_tokens,\n        num_beams=1,\n        repetition_penalty=repetition_penalty,\n        streamer=streamer,\n    )\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    generated_text = \"\"\n    for new_text in streamer:\n        stop = False\n        pos = new_text.find(stop_str)\n        if pos != -1:\n            new_text = new_text[:pos]\n            stop = True\n        generated_text += new_text\n        if do_print:\n            print(new_text, end=\"\", flush=True)\n        if stop:\n            break\n    if do_print:\n        print()\n    return generated_text\n\n\nclass Item(BaseModel):\n    input: str = Field(..., max_length=2048)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True)\n    parser.add_argument('--base_model', default=None, type=str, required=True)\n    parser.add_argument('--lora_model', default=\"\", type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--tokenizer_path', default=None, type=str)\n    parser.add_argument('--template_name', default=\"vicuna\", type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan, chatglm2 etc.\")\n    parser.add_argument('--system_prompt', default=\"\", type=str)\n    parser.add_argument(\"--repetition_penalty\", default=1.0, type=float)\n    parser.add_argument(\"--max_new_tokens\", default=512, type=int)\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--gpus', default=\"0\", type=str)\n    parser.add_argument('--only_cpu', action='store_true', help='only use CPU for inference')\n    parser.add_argument('--port', default=8008, type=int)\n    args = parser.parse_args()\n    print(args)\n\n    def load_model(args):\n        if args.only_cpu is True:\n            args.gpus = \"\"\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n        load_type = 'auto'\n        if torch.cuda.is_available():\n            device = torch.device(0)\n        else:\n            device = torch.device('cpu')\n        if args.tokenizer_path is None:\n            args.tokenizer_path = args.base_model\n\n        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True)\n        base_model = model_class.from_pretrained(\n            args.base_model,\n            torch_dtype=load_type,\n            low_cpu_mem_usage=True,\n            device_map='auto',\n            trust_remote_code=True,\n        )\n        try:\n            base_model.generation_config = GenerationConfig.from_pretrained(args.base_model, trust_remote_code=True)\n        except OSError:\n            print(\"Failed to load generation config, use default.\")\n        if args.resize_emb:\n            model_vocab_size = base_model.get_input_embeddings().weight.size(0)\n            tokenzier_vocab_size = len(tokenizer)\n            print(f\"Vocab of the base model: {model_vocab_size}\")\n            print(f\"Vocab of the tokenizer: {tokenzier_vocab_size}\")\n            if model_vocab_size != tokenzier_vocab_size:\n                print(\"Resize model embeddings to fit tokenizer\")\n                base_model.resize_token_embeddings(tokenzier_vocab_size)\n\n        if args.lora_model:\n            model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=load_type, device_map='auto')\n            print(\"Loaded lora model\")\n        else:\n            model = base_model\n        if device == torch.device('cpu'):\n            model.float()\n        model.eval()\n        print(tokenizer)\n        return model, tokenizer, device\n\n    # define the app\n    app = FastAPI()\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"])\n\n    model, tokenizer, device = load_model(args)\n    prompt_template = get_conv_template(args.template_name)\n    stop_str = tokenizer.eos_token if tokenizer.eos_token else prompt_template.stop_str\n\n    def predict(sentence):\n        history = [[sentence, '']]\n        prompt = prompt_template.get_prompt(messages=history, system_prompt=args.system_prompt)\n        response = stream_generate_answer(\n            model,\n            tokenizer,\n            prompt,\n            device,\n            do_print=False,\n            max_new_tokens=args.max_new_tokens,\n            repetition_penalty=args.repetition_penalty,\n            stop_str=stop_str,\n        )\n        return response.strip()\n\n    @app.get('/')\n    async def index():\n        return {\"message\": \"index, docs url: /docs\"}\n\n    @app.post('/chat')\n    async def chat(item: Item):\n        try:\n            response = predict(item.input)\n            result_dict = {'response': response}\n            logger.debug(f\"Successfully get result, q:{item.input}\")\n            return result_dict\n        except Exception as e:\n            logger.error(e)\n            return None\n\n    uvicorn.run(app=app, host='0.0.0.0', port=args.port, workers=1)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "gradio_demo.py",
          "type": "blob",
          "size": 5.0302734375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description:\n\npip install gradio>=3.50.2\n\"\"\"\nimport argparse\nfrom threading import Thread\n\nimport gradio as gr\nimport torch\nfrom peft import PeftModel\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaForCausalLM,\n    GenerationConfig,\n    TextIteratorStreamer,\n)\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default='auto', type=str)\n    parser.add_argument('--base_model', default=None, type=str, required=True)\n    parser.add_argument('--lora_model', default=\"\", type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--tokenizer_path', default=None, type=str)\n    parser.add_argument('--template_name', default=\"vicuna\", type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan2, chatglm2 etc.\")\n    parser.add_argument('--system_prompt', default=\"\", type=str)\n    parser.add_argument('--only_cpu', action='store_true', help='only use CPU for inference')\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--share', action='store_true', help='Share gradio')\n    parser.add_argument('--port', default=8081, type=int, help='Port of gradio demo')\n    args = parser.parse_args()\n    print(args)\n    load_type = 'auto'\n    if torch.cuda.is_available() and not args.only_cpu:\n        device = torch.device(0)\n    else:\n        device = torch.device('cpu')\n\n    if args.tokenizer_path is None:\n        args.tokenizer_path = args.base_model\n    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True)\n    base_model = model_class.from_pretrained(\n        args.base_model,\n        torch_dtype=load_type,\n        low_cpu_mem_usage=True,\n        device_map='auto',\n        trust_remote_code=True,\n    )\n    try:\n        base_model.generation_config = GenerationConfig.from_pretrained(args.base_model, trust_remote_code=True)\n    except OSError:\n        print(\"Failed to load generation config, use default.\")\n    if args.resize_emb:\n        model_vocab_size = base_model.get_input_embeddings().weight.size(0)\n        tokenzier_vocab_size = len(tokenizer)\n        print(f\"Vocab of the base model: {model_vocab_size}\")\n        print(f\"Vocab of the tokenizer: {tokenzier_vocab_size}\")\n        if model_vocab_size != tokenzier_vocab_size:\n            print(\"Resize model embeddings to fit tokenizer\")\n            base_model.resize_token_embeddings(tokenzier_vocab_size)\n    if args.lora_model:\n        model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=load_type, device_map='auto')\n        print(\"loaded lora model\")\n    else:\n        model = base_model\n    if device == torch.device('cpu'):\n        model.float()\n    model.eval()\n    prompt_template = get_conv_template(args.template_name)\n    system_prompt = args.system_prompt\n    stop_str = tokenizer.eos_token if tokenizer.eos_token else prompt_template.stop_str\n\n    def predict(message, history):\n        \"\"\"Generate answer from prompt with GPT and stream the output\"\"\"\n        history_messages = history + [[message, \"\"]]\n        prompt = prompt_template.get_prompt(messages=history_messages, system_prompt=system_prompt)\n        streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n        input_ids = tokenizer(prompt).input_ids\n        context_len = 2048\n        max_new_tokens = 512\n        max_src_len = context_len - max_new_tokens - 8\n        input_ids = input_ids[-max_src_len:]\n        generation_kwargs = dict(\n            input_ids=torch.as_tensor([input_ids]).to(device),\n            streamer=streamer,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            do_sample=True,\n            num_beams=1,\n            repetition_penalty=1.0,\n        )\n        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        partial_message = \"\"\n        for new_token in streamer:\n            if new_token != stop_str:\n                partial_message += new_token\n                yield partial_message\n\n    gr.ChatInterface(\n        predict,\n        chatbot=gr.Chatbot(),\n        textbox=gr.Textbox(placeholder=\"Ask me question\", lines=4, scale=9),\n        title=\"MedicalGPT\",\n        description=\"为了促进医疗行业大模型的开放研究，本项目开源了[MedicalGPT](https://github.com/shibing624/MedicalGPT)医疗大模型\",\n        theme=\"soft\",\n    ).queue().launch(share=args.share, inbrowser=True, server_name='0.0.0.0', server_port=args.port)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 10.1435546875,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: \n\"\"\"\nimport argparse\nimport json\nimport os\nfrom threading import Thread\n\nimport torch\nfrom peft import PeftModel\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaForCausalLM,\n    TextIteratorStreamer,\n    GenerationConfig,\n    BitsAndBytesConfig,\n)\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@torch.inference_mode()\ndef stream_generate_answer(\n        model,\n        tokenizer,\n        prompt,\n        device,\n        do_print=True,\n        max_new_tokens=512,\n        temperature=0.7,\n        repetition_penalty=1.0,\n        context_len=2048,\n        stop_str=\"</s>\",\n):\n    \"\"\"Generate answer from prompt with GPT and stream the output\"\"\"\n    streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n    input_ids = tokenizer(prompt).input_ids\n    max_src_len = context_len - max_new_tokens - 8\n    input_ids = input_ids[-max_src_len:]\n    generation_kwargs = dict(\n        input_ids=torch.as_tensor([input_ids]).to(device),\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        do_sample=True if temperature > 0.0 else False,\n        repetition_penalty=repetition_penalty,\n        streamer=streamer,\n    )\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    generated_text = \"\"\n    for new_text in streamer:\n        stop = False\n        pos = new_text.find(stop_str)\n        if pos != -1:\n            new_text = new_text[:pos]\n            stop = True\n        generated_text += new_text\n        if do_print:\n            print(new_text, end=\"\", flush=True)\n        if stop:\n            break\n    if do_print:\n        print()\n    return generated_text\n\n\n@torch.inference_mode()\ndef batch_generate_answer(\n        sentences,\n        model,\n        tokenizer,\n        prompt_template,\n        system_prompt,\n        device,\n        max_new_tokens=512,\n        temperature=0.7,\n        repetition_penalty=1.0,\n        stop_str=\"</s>\",\n):\n    \"\"\"Generate answer from prompt with GPT, batch mode\"\"\"\n    generated_texts = []\n    generation_kwargs = dict(\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        do_sample=True if temperature > 0.0 else False,\n        repetition_penalty=repetition_penalty,\n    )\n    prompts = [prompt_template.get_prompt(messages=[[s, '']], system_prompt=system_prompt) for s in sentences]\n    inputs_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n    input_ids = inputs_tokens['input_ids'].to(device)\n    outputs = model.generate(input_ids=input_ids, **generation_kwargs)\n    for gen_sequence in outputs:\n        prompt_len = len(input_ids[0])\n        gen_sequence = gen_sequence[prompt_len:]\n        gen_text = tokenizer.decode(gen_sequence, skip_special_tokens=True)\n        pos = gen_text.find(stop_str)\n        if pos != -1:\n            gen_text = gen_text[:pos]\n        gen_text = gen_text.strip()\n        generated_texts.append(gen_text)\n\n    return generated_texts\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default='auto', type=str)\n    parser.add_argument('--base_model', default=None, type=str, required=True)\n    parser.add_argument('--lora_model', default=\"\", type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--tokenizer_path', default=None, type=str)\n    parser.add_argument('--template_name', default=\"vicuna\", type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan, chatglm2 etc.\")\n    parser.add_argument('--system_prompt', default=\"\", type=str)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument(\"--max_new_tokens\", type=int, default=512)\n    parser.add_argument('--data_file', default=None, type=str,\n                        help=\"A file that contains instructions (one instruction per line)\")\n    parser.add_argument('--interactive', action='store_true', help=\"run in the instruction mode (default multi-turn)\")\n    parser.add_argument('--single_tune', action='store_true', help='Whether to use single-tune model')\n    parser.add_argument('--temperature', type=float, default=0.7)\n    parser.add_argument('--output_file', default='./predictions_result.jsonl', type=str)\n    parser.add_argument(\"--eval_batch_size\", type=int, default=4)\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--load_in_8bit', action='store_true', help='Whether to load model in 8bit')\n    parser.add_argument('--load_in_4bit', action='store_true', help='Whether to load model in 4bit')\n    args = parser.parse_args()\n    print(args)\n    load_type = 'auto'\n    if args.tokenizer_path is None:\n        args.tokenizer_path = args.base_model\n\n    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True, padding_side='left')\n    config_kwargs = {\n        \"trust_remote_code\": True,\n        \"torch_dtype\": load_type,\n        \"low_cpu_mem_usage\": True,\n        \"device_map\": 'auto',\n    }\n    if args.load_in_8bit:\n        config_kwargs['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n    elif args.load_in_4bit:\n        config_kwargs['quantization_config'] = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=load_type,\n        )\n    base_model = model_class.from_pretrained(args.base_model, **config_kwargs)\n    try:\n        base_model.generation_config = GenerationConfig.from_pretrained(args.base_model, trust_remote_code=True)\n    except OSError:\n        print(\"Failed to load generation config, use default.\")\n    if args.resize_emb:\n        model_vocab_size = base_model.get_input_embeddings().weight.size(0)\n        tokenzier_vocab_size = len(tokenizer)\n        print(f\"Vocab of the base model: {model_vocab_size}\")\n        print(f\"Vocab of the tokenizer: {tokenzier_vocab_size}\")\n        if model_vocab_size != tokenzier_vocab_size:\n            print(\"Resize model embeddings to fit tokenizer\")\n            base_model.resize_token_embeddings(tokenzier_vocab_size)\n\n    if args.lora_model:\n        model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=load_type, device_map='auto')\n        print(\"Loaded lora model\")\n    else:\n        model = base_model\n    model.eval()\n    print(tokenizer)\n    # test data\n    if args.data_file is None:\n        examples = [\"介绍下北京\", \"乙肝和丙肝的区别？\"]\n    else:\n        with open(args.data_file, 'r') as f:\n            examples = [l.strip() for l in f.readlines()]\n        print(\"first 10 examples:\")\n        for example in examples[:10]:\n            print(example)\n\n    # Chat\n    prompt_template = get_conv_template(args.template_name)\n    system_prompt = args.system_prompt\n    stop_str = tokenizer.eos_token if tokenizer.eos_token else prompt_template.stop_str\n\n    if args.interactive:\n        print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n        history = []\n        while True:\n            try:\n                query = input(f\"{prompt_template.roles[0]}: \")\n            except UnicodeDecodeError:\n                print(\"Detected decoding error at the inputs, please try again.\")\n                continue\n            except Exception:\n                raise\n            if query == \"\":\n                print(\"Please input text, try again.\")\n                continue\n            if query.strip() == \"exit\":\n                print(\"exit...\")\n                break\n            if query.strip() == \"clear\":\n                history = []\n                print(\"history cleared.\")\n                continue\n\n            print(f\"{prompt_template.roles[1]}: \", end=\"\", flush=True)\n            if args.single_tune:\n                history = []\n\n            history.append([query, ''])\n            prompt = prompt_template.get_prompt(messages=history, system_prompt=system_prompt)\n            response = stream_generate_answer(\n                model,\n                tokenizer,\n                prompt,\n                model.device,\n                do_print=True,\n                max_new_tokens=args.max_new_tokens,\n                temperature=args.temperature,\n                repetition_penalty=args.repetition_penalty,\n                stop_str=stop_str,\n            )\n            if history:\n                history[-1][-1] = response.strip()\n    else:\n        print(\"Start inference.\")\n        counts = 0\n        if os.path.exists(args.output_file):\n            os.remove(args.output_file)\n        eval_batch_size = args.eval_batch_size\n        for batch in tqdm(\n                [\n                    examples[i: i + eval_batch_size]\n                    for i in range(0, len(examples), eval_batch_size)\n                ],\n                desc=\"Generating outputs\",\n        ):\n            responses = batch_generate_answer(\n                batch,\n                model,\n                tokenizer,\n                prompt_template,\n                system_prompt,\n                model.device,\n                max_new_tokens=args.max_new_tokens,\n                temperature=args.temperature,\n                repetition_penalty=args.repetition_penalty,\n                stop_str=stop_str,\n            )\n            results = []\n            for example, response in zip(batch, responses):\n                print(f\"===\")\n                print(f\"Input: {example}\")\n                print(f\"Output: {response}\\n\")\n                results.append({\"Input\": example, \"Output\": response})\n                counts += 1\n            with open(args.output_file, 'a', encoding='utf-8') as f:\n                for entry in results:\n                    json.dump(entry, f, ensure_ascii=False)\n                    f.write('\\n')\n        print(f'save to {args.output_file}, size: {counts}')\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "inference_multigpu_demo.py",
          "type": "blob",
          "size": 8.5146484375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: use torchrun to inference with multi-gpus\n\nusage:\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 inference_multigpu_demo.py --model_type bloom --base_model bigscience/bloom-560m\n\"\"\"\nimport argparse\nimport json\nimport os\n\nimport torch\nimport torch.distributed as dist\nfrom loguru import logger\nfrom peft import PeftModel\nfrom torch.nn import DataParallel\nfrom torch.utils.data import DataLoader, Dataset, DistributedSampler\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    LlamaForCausalLM,\n    GenerationConfig,\n    BitsAndBytesConfig,\n)\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\nclass TextDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True)\n    parser.add_argument('--base_model', default=None, type=str, required=True)\n    parser.add_argument('--lora_model', default=\"\", type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--tokenizer_path', default=None, type=str)\n    parser.add_argument('--template_name', default=\"vicuna\", type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan, chatglm2 etc.\")\n    parser.add_argument('--system_prompt', default=\"\", type=str)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument('--temperature', type=float, default=0.7)\n    parser.add_argument(\"--max_new_tokens\", type=int, default=128)\n    parser.add_argument(\"--batch_size\", type=int, default=4)\n    parser.add_argument('--data_file', default=None, type=str, help=\"Predict file, one example per line\")\n    parser.add_argument('--output_file', default='./predictions_result.jsonl', type=str)\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--load_in_8bit', action='store_true', help='Whether to load model in 8bit')\n    parser.add_argument('--load_in_4bit', action='store_true', help='Whether to load model in 4bit')\n    args = parser.parse_args()\n    logger.info(args)\n\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    logger.info(f\"local_rank: {local_rank}, world_size: {world_size}\")\n    torch.cuda.set_device(local_rank)\n    dist.init_process_group(backend='nccl')\n\n    if not torch.cuda.is_available():\n        raise ValueError(\"No GPU available, this script is only for GPU inference.\")\n    if args.tokenizer_path is None:\n        args.tokenizer_path = args.base_model\n\n    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True, padding_side='left')\n    load_type = 'auto'\n    base_model = model_class.from_pretrained(\n        args.base_model,\n        load_in_8bit=args.load_in_8bit,\n        load_in_4bit=args.load_in_4bit,\n        torch_dtype=load_type,\n        low_cpu_mem_usage=True,\n        device_map={\"\": local_rank},\n        trust_remote_code=True,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.load_in_4bit,\n            load_in_8bit=args.load_in_8bit,\n            bnb_4bit_compute_dtype=load_type,\n        ) if args.load_in_8bit or args.load_in_4bit else None,\n    )\n    try:\n        base_model.generation_config = GenerationConfig.from_pretrained(args.base_model, trust_remote_code=True)\n    except OSError:\n        logger.info(\"Failed to load generation config, use default.\")\n    if args.resize_emb:\n        model_vocab_size = base_model.get_input_embeddings().weight.size(0)\n        tokenzier_vocab_size = len(tokenizer)\n        logger.info(f\"Vocab of the base model: {model_vocab_size}\")\n        logger.info(f\"Vocab of the tokenizer: {tokenzier_vocab_size}\")\n        if model_vocab_size != tokenzier_vocab_size:\n            logger.info(\"Resize model embeddings to fit tokenizer\")\n            base_model.resize_token_embeddings(tokenzier_vocab_size)\n\n    if args.lora_model:\n        model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=load_type,\n                                          device_map={\"\": local_rank})\n        logger.info(\"Loaded lora model\")\n    else:\n        model = base_model\n    model.eval()\n    # Use multi-GPU inference\n    model = DataParallel(model)\n    model = model.module\n    logger.info(tokenizer)\n    # test data\n    if args.data_file is None:\n        examples = [\n            \"介绍下北京\",\n            \"乙肝和丙肝的区别？\",\n            \"失眠怎么办？\",\n            '用一句话描述地球为什么是独一无二的。',\n            \"Tell me about alpacas.\",\n            \"Tell me about the president of Mexico in 2019.\",\n            \"hello.\",\n        ]\n    else:\n        with open(args.data_file, 'r', encoding='utf-8') as f:\n            examples = [l.strip() for l in f.readlines()]\n        logger.info(f\"first 10 examples: {examples[:10]}\")\n\n    prompt_template = get_conv_template(args.template_name)\n    write_batch_size = args.batch_size * world_size * 10\n    generation_kwargs = dict(\n        max_new_tokens=args.max_new_tokens,\n        temperature=args.temperature,\n        do_sample=True if args.temperature > 0.0 else False,\n        repetition_penalty=args.repetition_penalty,\n    )\n    stop_str = tokenizer.eos_token if tokenizer.eos_token else prompt_template.stop_str\n    if local_rank <= 0 and os.path.exists(args.output_file):\n        os.remove(args.output_file)\n    count = 0\n    for batch in tqdm(\n            [\n                examples[i: i + write_batch_size]\n                for i in range(0, len(examples), write_batch_size)\n            ],\n            desc=\"Generating outputs\",\n    ):\n        dataset = TextDataset(batch)\n        sampler = DistributedSampler(dataset, num_replicas=world_size, rank=local_rank, shuffle=False)\n        data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=sampler)\n\n        responses = []\n        inputs = []\n        for texts in data_loader:\n            inputs.extend(texts)\n            prompted_texts = [prompt_template.get_prompt(messages=[[s, '']], system_prompt=args.system_prompt) for s in texts]\n            logger.debug(f'local_rank: {local_rank}, inputs size:{len(prompted_texts)}, top3: {prompted_texts[:3]}')\n            inputs_tokens = tokenizer(prompted_texts, return_tensors=\"pt\", padding=True)\n            input_ids = inputs_tokens['input_ids'].to(local_rank)\n            outputs = model.generate(input_ids=input_ids, **generation_kwargs)\n            prompt_len = len(input_ids[0])\n            outputs = [i[prompt_len:] for i in outputs]\n            generated_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            logger.debug(\n                f'local_rank: {local_rank}, outputs size:{len(generated_outputs)}, top3: {generated_outputs[:3]}'\n            )\n            responses.extend(generated_outputs)\n        all_inputs = [None] * world_size\n        all_responses = [None] * world_size\n        dist.all_gather_object(all_inputs, inputs)\n        dist.all_gather_object(all_responses, responses)\n\n        # Write responses only on the main process\n        if local_rank <= 0:\n            all_inputs_flat = [inp for process_inputs in all_inputs for inp in process_inputs]\n            all_responses_flat = [response for process_responses in all_responses for response in process_responses]\n            logger.debug(f\"all_responses size:{len(all_responses_flat)}, top5: {all_responses_flat[:5]}\")\n            results = []\n            for example, response in zip(all_inputs_flat, all_responses_flat):\n                results.append({\"Input\": example, \"Output\": response})\n            with open(args.output_file, 'a', encoding='utf-8') as f:\n                for entry in results:\n                    json.dump(entry, f, ensure_ascii=False)\n                    f.write('\\n')\n                    count += 1\n\n    if local_rank <= 0:\n        logger.info(f'save to {args.output_file}, total count: {count}')\n    dist.barrier()\n    dist.destroy_process_group()\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "merge_peft_adapter.py",
          "type": "blob",
          "size": 4.201171875,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description:\n\nUsage:\npython merge_peft_adapter.py \\\n    --model_type llama \\\n    --base_model path/to/llama/model \\\n    --tokenizer_path path/to/llama/tokenizer \\\n    --lora_model path/to/lora/model \\\n    --output_dir path/to/output/dir\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    BloomForCausalLM,\n    BloomTokenizerFast,\n    AutoModelForCausalLM,\n    LlamaForCausalLM,\n    AutoModelForSequenceClassification,\n)\n\nMODEL_CLASSES = {\n    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoModel, AutoTokenizer),\n    \"llama\": (LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoModelForCausalLM, AutoTokenizer),\n}\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True)\n    parser.add_argument('--base_model', default=None, required=True, type=str,\n                        help=\"Base model name or path\")\n    parser.add_argument('--tokenizer_path', default=None, type=str,\n                        help=\"Please specify tokenization path.\")\n    parser.add_argument('--lora_model', default=None, required=True, type=str,\n                        help=\"Please specify LoRA model to be merged.\")\n    parser.add_argument('--resize_emb', action='store_true', help='Whether to resize model token embeddings')\n    parser.add_argument('--output_dir', default='./merged', type=str)\n    parser.add_argument('--hf_hub_model_id', default='', type=str)\n    parser.add_argument('--hf_hub_token', default=None, type=str)\n    args = parser.parse_args()\n    print(args)\n\n    base_model_path = args.base_model\n    lora_model_path = args.lora_model\n    output_dir = args.output_dir\n    print(f\"Base model: {base_model_path}\")\n    print(f\"LoRA model: {lora_model_path}\")\n    peft_config = PeftConfig.from_pretrained(lora_model_path)\n\n    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    if peft_config.task_type == \"SEQ_CLS\":\n        print(\"Loading LoRA for sequence classification model\")\n        if args.model_type == \"chatglm\":\n            raise ValueError(\"chatglm does not support sequence classification\")\n        base_model = AutoModelForSequenceClassification.from_pretrained(\n            base_model_path,\n            num_labels=1,\n            load_in_8bit=False,\n            torch_dtype=torch.float32,\n            trust_remote_code=True,\n            device_map=\"auto\",\n        )\n    else:\n        print(\"Loading LoRA for causal language model\")\n        base_model = model_class.from_pretrained(\n            base_model_path,\n            torch_dtype='auto',\n            trust_remote_code=True,\n            device_map=\"auto\",\n        )\n    if args.tokenizer_path:\n        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_path, trust_remote_code=True)\n    else:\n        tokenizer = tokenizer_class.from_pretrained(base_model_path, trust_remote_code=True)\n    if args.resize_emb:\n        base_model_token_size = base_model.get_input_embeddings().weight.size(0)\n        if base_model_token_size != len(tokenizer):\n            base_model.resize_token_embeddings(len(tokenizer))\n            print(f\"Resize vocabulary size {base_model_token_size} to {len(tokenizer)}\")\n\n    new_model = PeftModel.from_pretrained(\n        base_model,\n        lora_model_path,\n        device_map=\"auto\",\n        torch_dtype='auto',\n    )\n    new_model.eval()\n    print(f\"Merging with merge_and_unload...\")\n    base_model = new_model.merge_and_unload()\n\n    print(\"Saving to Hugging Face format...\")\n    tokenizer.save_pretrained(output_dir)\n    base_model.save_pretrained(output_dir, max_shard_size='10GB')\n    print(f\"Done! model saved to {output_dir}\")\n    if args.hf_hub_model_id:\n        print(f\"Pushing to Hugging Face Hub...\")\n        base_model.push_to_hub(\n            args.hf_hub_model_id,\n            token=args.hf_hub_token,\n            max_shard_size=\"10GB\",\n        )\n        tokenizer.push_to_hub(\n            args.hf_hub_model_id,\n            token=args.hf_hub_token,\n        )\n        print(f\"Done! model pushed to Hugging Face Hub: {args.hf_hub_model_id}\")\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "merge_tokenizers.py",
          "type": "blob",
          "size": 6.1767578125,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: \n\"\"\"\nimport os\n\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nfrom transformers import LlamaTokenizer\nfrom sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\nimport sentencepiece as spm\nimport argparse\n\n\ndef is_chinese(uchar):\n    \"\"\"判断一个unicode是否是汉字\"\"\"\n    return '\\u4e00' <= uchar <= '\\u9fa5'\n\n\ndef is_chinese_string(string):\n    \"\"\"判断是否全为汉字\"\"\"\n    return all(is_chinese(c) for c in string)\n\n\ndef load_baichuan_vocab(vocab_file):\n    words = set()\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if line.strip():\n                words.add(line.strip().split()[0])\n    return words\n\n\ndef load_jieba_vocab(jieba_vocab_file):\n    # Read jieba vocab and sort by freq\n    with open(jieba_vocab_file, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        word_freqs = [line.strip().split() for line in lines]\n        word_freqs.sort(key=lambda x: int(x[1]), reverse=True)\n    return word_freqs\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--base_tokenizer_dir', default=None, type=str, required=True)\n    parser.add_argument('--domain_sp_model_file', default='./domain_sp.model', type=str)\n    parser.add_argument('--baichuan_vocab_file', default=\"data/vocab/baichuan_vocab.txt\", type=str)\n    parser.add_argument('--add_jieba', action='store_true', help='Whether to add jieba vocab.')\n    parser.add_argument('--jieba_word_freq_file', default='data/vocab/word_freq.txt', type=str)\n    parser.add_argument('--jieba_word_size', default=20000, type=int)\n\n    args = parser.parse_args()\n    print(args)\n\n    # load\n    llama_tokenizer = LlamaTokenizer.from_pretrained(args.base_tokenizer_dir)\n    chinese_sp_model = spm.SentencePieceProcessor()\n    chinese_sp_model.Load(args.domain_sp_model_file)\n\n    llama_spm = sp_pb2_model.ModelProto()\n    llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n    chinese_spm = sp_pb2_model.ModelProto()\n    chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n\n    # print number of tokens\n    print(len(llama_tokenizer), len(chinese_sp_model))\n    print(llama_tokenizer.all_special_tokens)\n    print(llama_tokenizer.all_special_ids)\n    print(llama_tokenizer.special_tokens_map)\n\n    # Add Chinese tokens to LLaMA tokenizer\n    llama_spm_tokens_set = set(p.piece for p in llama_spm.pieces)\n\n    print(len(llama_spm_tokens_set))\n    print(f\"Before:{len(llama_spm_tokens_set)}\")\n    added_set = set()\n    for p in chinese_spm.pieces:\n        piece = p.piece\n        if piece not in llama_spm_tokens_set:\n            # print('picec', piece)\n            new_p = sp_pb2_model.ModelProto().SentencePiece()\n            new_p.piece = piece\n            new_p.score = 0\n            llama_spm.pieces.append(new_p)\n            added_set.add(piece)\n    print(f\"[add domain tokens]New model pieces: {len(llama_spm.pieces)}\")\n\n    vocab = load_baichuan_vocab(args.baichuan_vocab_file)\n    print('baichuan vocab len:', len(vocab))\n    baichuan_vocab_set = set([i for i in vocab if is_chinese_string(i)])\n    print('baichuan chinese vocab size:', len(baichuan_vocab_set))\n    print('baichuan vocab head:', list(baichuan_vocab_set)[:10])\n    for p in baichuan_vocab_set:\n        piece = p\n        if piece not in llama_spm_tokens_set and piece not in added_set:\n            # print('baichuan picec', piece)\n            new_p = sp_pb2_model.ModelProto().SentencePiece()\n            new_p.piece = piece\n            new_p.score = 0\n            llama_spm.pieces.append(new_p)\n            added_set.add(piece)\n    print(f\"[add baichuan tokens]New model pieces: {len(llama_spm.pieces)}\")\n\n    if args.add_jieba:\n        word_freqs = load_jieba_vocab(args.jieba_word_freq_file)\n        top_words = word_freqs[:args.jieba_word_size]\n        print('jieba top10 freq words:', top_words[:10])\n        jieba_vocab_set = set([i[0] for i in top_words if i])\n        print('jieba_vocab_set size:', len(jieba_vocab_set))\n        print('jieba_vocab head:', list(jieba_vocab_set)[:3])\n        for p in jieba_vocab_set:\n            piece = p\n            if piece not in llama_spm_tokens_set and piece not in added_set:\n                # print('jieba picec', piece)\n                new_p = sp_pb2_model.ModelProto().SentencePiece()\n                new_p.piece = piece\n                new_p.score = 0\n                llama_spm.pieces.append(new_p)\n        print(f\"[add jieba tokens]New model pieces: {len(llama_spm.pieces)}\")\n\n    # Save\n    output_sp_dir = 'merged_tokenizer_sp'\n    output_hf_dir = 'merged_tokenizer_hf'  # the path to save Chinese-LLaMA tokenizer\n    os.makedirs(output_sp_dir, exist_ok=True)\n    with open(output_sp_dir + '/chinese_llama.model', 'wb') as f:\n        f.write(llama_spm.SerializeToString())\n    tokenizer = LlamaTokenizer(vocab_file=output_sp_dir + '/chinese_llama.model')\n\n    tokenizer.save_pretrained(output_hf_dir)\n    print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")\n\n    # Test\n    llama_tokenizer = LlamaTokenizer.from_pretrained(args.base_tokenizer_dir)\n    chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n    print(chinese_llama_tokenizer.all_special_tokens)\n    print(chinese_llama_tokenizer.all_special_ids)\n    print(chinese_llama_tokenizer.special_tokens_map)\n    print('old len:', len(llama_tokenizer), ' new len:', len(chinese_llama_tokenizer))\n    text = '''this is a test, hello world. thisisatesthelloworld, \n慕容复来到河边，姑苏慕容氏在外面丢了人。\n1号店一周岁了，我们一古脑儿买了10斤零食。\n巴塞罗那足球俱乐部简称巴萨（Barça），是一家位于西班牙加泰罗尼亚巴塞罗那的足球俱乐部，于1899年由瑞士企业家胡安·甘伯所创立，世界球坛顶级足球俱乐部之一。俱乐部主场可容纳接近十万名观众，是全欧洲最大及世界第二大的足球场。\n白日依山尽，黄河入海流。欲穷千里目，更上一层楼。'''\n    print(\"Test text:\\n\", text)\n    print(f\"Tokenized by LLaMA tokenizer:{llama_tokenizer.tokenize(text)}\")\n    print(f\"Tokenized by Chinese-LLaMA tokenizer:{chinese_llama_tokenizer.tokenize(text)}\")\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "model_quant.py",
          "type": "blob",
          "size": 5.0380859375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:ZhuangXialie(1832963123@qq.com)\n@description: model quantify\n\nusage:\npython model_quant.py --unquantized_model_path /path/to/unquantized/model --quantized_model_output_path /path/to/save/quantized/model --input_text \"Your input text here\"\n\"\"\"\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport time\nimport argparse\n\n# 定义函数来解析命令行参数\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"量化模型推理对比\")\n    parser.add_argument(\"--unquantized_model_path\", type=str, required=True, help=\"未量化模型路径\")\n    parser.add_argument(\"--quantized_model_output_path\", type=str, required=True, help=\"量化模型保存路径\")\n    parser.add_argument(\"--input_text\", type=str, required=True, help=\"输入的文本内容\")\n    return parser.parse_args()\n\n# 计算模型相关的显存占用\ndef get_model_memory_usage(device):\n    return torch.cuda.memory_allocated(device) / (1024 ** 3)  # 转换为GB\n\n# 定义一个函数来进行推理，并计算推理时间\ndef perform_inference(model, tokenizer, devic, question):\n    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    attention_mask = inputs[\"attention_mask\"]\n    \n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs.input_ids,\n            attention_mask=attention_mask,\n            max_length=512,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.1,\n            pad_token_id=tokenizer.eos_token_id  # 设置 pad_token_id 为 eos_token_id\n        )\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text, elapsed_time\n\ndef main():\n    args = parse_args()\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # 1. 未量化模型推理和显存计算\n    print(\"\\n====== 未量化模型推理 ======\")\n    tokenizer = AutoTokenizer.from_pretrained(args.unquantized_model_path, trust_remote_code=True)\n\n    gpu_memory_before_unquantized = get_model_memory_usage(device)  # 模型加载前的显存\n    unquantized_model = AutoModelForCausalLM.from_pretrained(args.unquantized_model_path, trust_remote_code=True)\n    unquantized_model.to(device)\n    gpu_memory_after_unquantized = get_model_memory_usage(device)  # 模型加载后的显存\n    model_memory_unquantized = gpu_memory_after_unquantized - gpu_memory_before_unquantized  # 计算模型显存占用\n    print(f\"未量化模型加载显存占用: {model_memory_unquantized:.2f} GB\")\n\n    generated_text_unquantized, time_unquantized = perform_inference(unquantized_model, tokenizer, device, args.input_text)\n    print(f\"推理生成的文本（未量化模型）: {generated_text_unquantized}\")\n    print(f\"推理时间（未量化模型）: {time_unquantized:.2f} 秒\")\n\n    # 卸载未量化模型以释放显存\n    del unquantized_model\n    torch.cuda.empty_cache()\n    \n    # 重新计算显存基线\n    print(\"\\n清理缓存，重新计算显存...\")\n    time.sleep(2)  # 确保显存释放，等待一段时间\n    gpu_memory_after_cache_clear = get_model_memory_usage(device)\n    print(f\"显存清理后基线显存: {gpu_memory_after_cache_clear:.2f} GB\")\n\n    # 2. 量化模型推理和保存\n    print(\"\\n====== 量化模型推理和保存 ======\")\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,               # 开启4bit量化\n        load_in_8bit=False,              # 禁止8bit量化\n        bnb_4bit_compute_dtype=torch.float16,   # 计算数据类型为float16\n        bnb_4bit_quant_storage=torch.uint8,     # 存储数据类型为uint8\n        bnb_4bit_quant_type=\"nf4\",              # 使用nf4量化类型\n        bnb_4bit_use_double_quant=True          # 开启双重量化以优化推理\n    )\n\n    quantized_model = AutoModelForCausalLM.from_pretrained(\n        args.unquantized_model_path,\n        device_map=\"auto\",  # 自动分配设备\n        quantization_config=quantization_config,\n        trust_remote_code=True\n    )\n\n    generated_text_quantized, time_quantized = perform_inference(quantized_model, tokenizer, device, args.input_text)\n    print(f\"推理生成的文本（量化模型）: {generated_text_quantized}\")\n    print(f\"推理时间（量化模型）: {time_quantized:.2f} 秒\")\n\n    # 保存量化模型和tokenizer\n    quantized_model.save_pretrained(args.quantized_model_output_path)\n    tokenizer.save_pretrained(args.quantized_model_output_path)\n    print(f\"量化模型和tokenizer已保存到 {args.quantized_model_output_path}\")\n\n    # 输出对比\n    print(\"\\n====== 内容对比结果 ======\")\n    print(f\"未量化模型生成文本:\\n {generated_text_unquantized}\")\n    print(f\"量化模型生成文本:\\n {generated_text_quantized}\")\n    \n    print(\"\\n====== 时间对比结果 ======\")\n    print(f\"未量化模型推理时间: {time_unquantized:.2f} 秒\")\n    print(f\"量化模型推理时间: {time_quantized:.2f} 秒\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "openai_api.py",
          "type": "blob",
          "size": 20.705078125,
          "content": "# Requirement:\n#   pip install openai\n# Usage:\n#   python openai_api.py\n# Visit http://localhost:8000/docs for documents.\n\nimport base64\nimport copy\nimport json\nimport time\nfrom argparse import ArgumentParser\nfrom contextlib import asynccontextmanager\nfrom threading import Thread\nfrom typing import Dict, List, Literal, Optional, Union, Any\n\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig, TextIteratorStreamer\n\nfrom template import get_conv_template\n\n\nclass BasicAuthMiddleware(BaseHTTPMiddleware):\n\n    def __init__(self, app, username: str, password: str):\n        super().__init__(app)\n        self.required_credentials = base64.b64encode(\n            f'{username}:{password}'.encode()).decode()\n\n    async def dispatch(self, request: Request, call_next):\n        authorization: str = request.headers.get('Authorization')\n        if authorization:\n            try:\n                schema, credentials = authorization.split()\n                if credentials == self.required_credentials:\n                    return await call_next(request)\n            except ValueError:\n                pass\n\n        headers = {'WWW-Authenticate': 'Basic'}\n        return Response(status_code=401, headers=headers)\n\n\ndef _gc(forced: bool = False):\n    global args\n    if args.disable_gc and not forced:\n        return\n\n    import gc\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):  # collects GPU memory\n    yield\n    _gc(forced=True)\n\n\napp = FastAPI(lifespan=lifespan)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'],\n    allow_headers=['*'],\n)\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = 'model'\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = 'owner'\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None\n\n\nclass ModelList(BaseModel):\n    object: str = 'list'\n    data: List[ModelCard] = []\n\n\nclass ChatMessage(BaseModel):\n    role: Literal['user', 'assistant', 'system', 'function', 'tool']\n    content: Optional[str] = None\n    tool_calls: Optional[Dict] = None\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal['user', 'assistant', 'system']] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessage]\n    tools: Optional[List[Dict]] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    top_k: Optional[int] = None\n    max_length: Optional[int] = None\n    stream: Optional[bool] = False\n    stop: Optional[List[str]] = None\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: Union[ChatMessage]\n    finish_reason: Literal['stop', 'length', 'tool_calls']\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal['stop', 'length']] = None\n\n\nclass ChatCompletionResponseUsage(BaseModel):\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: Literal[\"chatcmpl-default\"] = \"chatcmpl-default\"\n    object: Literal[\"chat.completion\"] = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseChoice]\n    usage: ChatCompletionResponseUsage\n\n\nclass ChatCompletionStreamResponse(BaseModel):\n    id: Literal[\"chatcmpl-default\"] = \"chatcmpl-default\"\n    object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseStreamChoice]\n\n\n@app.get('/v1/models', response_model=ModelList)\nasync def list_models():\n    global model_args\n    model_card = ModelCard(id='gpt-3.5-turbo')\n    return ModelList(data=[model_card])\n\n\n# To work around that unpleasant leading-\\n tokenization issue!\ndef add_extra_stop_words(stop_words):\n    _stop_words = []\n    if stop_words:\n        _stop_words.extend(stop_words)\n        for x in stop_words:\n            s = x.lstrip('\\n')\n            if s and (s not in _stop_words):\n                _stop_words.append(s)\n    return _stop_words\n\n\ndef trim_stop_words(response, stop_words):\n    if stop_words:\n        for stop in stop_words:\n            idx = response.find(stop)\n            if idx != -1:\n                response = response[:idx]\n    return response\n\n\nTOOL_DESC = (\n    '{name_for_model}: Call this tool to interact with the {name_for_human} API.'\n    ' What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}'\n)\n\nREACT_INSTRUCTION = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n\n{tools_text}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tools_name_text}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\"\"\"\n\n_TEXT_COMPLETION_CMD = object()\n\n\ndef parse_messages(messages, tools):\n    if all(m.role != 'user' for m in messages):\n        raise HTTPException(\n            status_code=400,\n            detail='Invalid request: Expecting at least one user message.',\n        )\n\n    messages = copy.deepcopy(messages)\n    if messages[0].role == 'system':\n        system = messages.pop(0).content.lstrip('\\n').rstrip()\n    else:\n        system = ''\n\n    if tools:\n        tools_text = []\n        tools_name_text = []\n        for tool_info in tools:\n            name = tool_info.get('name', '')\n            name_m = tool_info.get('name_for_model', name)\n            name_h = tool_info.get('name_for_human', name)\n            desc = tool_info.get('description', '')\n            desc_m = tool_info.get('description_for_model', desc)\n            params = tool_info.get('parameters', {})\n            tool = TOOL_DESC.format(\n                name_for_model=name_m,\n                name_for_human=name_h,\n                # Hint: You can add the following format requirements in description:\n                #   \"Format the arguments as a JSON object.\"\n                #   \"Enclose the code within triple backticks (`) at the beginning and end of the code.\"\n                description_for_model=desc_m,\n                parameters=json.dumps(params, ensure_ascii=False),\n            )\n            tools_text.append(tool)\n            tools_name_text.append(name_m)\n        tools_text = '\\n\\n'.join(tools_text)\n        tools_name_text = ', '.join(tools_name_text)\n        instruction = (REACT_INSTRUCTION.format(\n            tools_text=tools_text,\n            tools_name_text=tools_name_text,\n        ).lstrip('\\n').rstrip())\n    else:\n        instruction = ''\n\n    messages_with_fncall = messages\n    messages = []\n    for m_idx, m in enumerate(messages_with_fncall):\n        role, content, tool_calls = m.role, m.content, m.tool_calls\n        content = content or ''\n        content = content.lstrip('\\n').rstrip()\n        if role == 'function':\n            if (len(messages) == 0) or (messages[-1].role != 'assistant'):\n                raise HTTPException(\n                    status_code=400,\n                    detail='Invalid request: Expecting role assistant before role function.',\n                )\n            messages[-1].content += f'\\nObservation: {content}'\n            if m_idx == len(messages_with_fncall) - 1:\n                # add a prefix for text completion\n                messages[-1].content += '\\nThought:'\n        elif role == 'assistant':\n            if len(messages) == 0:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\n                    'Invalid request: Expecting role user before role assistant.',\n                )\n            if tool_calls is None:\n                if tools:\n                    content = f'Thought: I now know the final answer.\\nFinal Answer: {content}'\n            else:\n                f_name, f_args = tool_calls['name'], tool_calls['arguments']\n                if not content.startswith('Thought:'):\n                    content = f'Thought: {content}'\n                content = f'{content}\\nAction: {f_name}\\nAction Input: {f_args}'\n            if messages[-1].role == 'user':\n                messages.append(\n                    ChatMessage(role='assistant',\n                                content=content.lstrip('\\n').rstrip()))\n            else:\n                messages[-1].content += '\\n' + content\n        elif role == 'user':\n            messages.append(\n                ChatMessage(role='user',\n                            content=content.lstrip('\\n').rstrip()))\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=f'Invalid request: Incorrect role {role}.')\n\n    query = _TEXT_COMPLETION_CMD\n    if messages[-1].role == 'user':\n        query = messages[-1].content\n        messages = messages[:-1]\n\n    if len(messages) % 2 != 0:\n        raise HTTPException(status_code=400, detail='Invalid request')\n\n    history = []  # [(Q1, A1), (Q2, A2), ..., (Q_last_turn, A_last_turn)]\n    for i in range(0, len(messages), 2):\n        if messages[i].role == 'user' and messages[i + 1].role == 'assistant':\n            usr_msg = messages[i].content.lstrip('\\n').rstrip()\n            bot_msg = messages[i + 1].content.lstrip('\\n').rstrip()\n            if instruction and (i == len(messages) - 2):\n                usr_msg = f'{instruction}\\n\\nQuestion: {usr_msg}'\n                instruction = ''\n            history.append([usr_msg, bot_msg])\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail='Invalid request: Expecting exactly one user (or function) role before every assistant role.',\n            )\n    if instruction:\n        assert query is not _TEXT_COMPLETION_CMD\n        query = f'{instruction}\\n\\nQuestion: {query}'\n    return query, history, system\n\n\ndef parse_response(response):\n    func_name, func_args = '', ''\n    i = response.find('\\nAction:')\n    j = response.find('\\nAction Input:')\n    k = response.find('\\nObservation:')\n    if 0 <= i < j:  # If the text has `Action` and `Action input`,\n        if k < j:  # but does not contain `Observation`,\n            # then it is likely that `Observation` is omitted by the LLM,\n            # because the output text may have discarded the stop word.\n            response = response.rstrip() + '\\nObservation:'  # Add it back.\n        k = response.find('\\nObservation:')\n        func_name = response[i + len('\\nAction:'):j].strip()\n        func_args = response[j + len('\\nAction Input:'):k].strip()\n\n    if func_name:\n        response = response[:i]\n        t = response.find('Thought: ')\n        if t >= 0:\n            response = response[t + len('Thought: '):]\n        response = response.strip()\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(\n                role='assistant',\n                content=response,\n                tool_calls={\n                    'name': func_name,\n                    'arguments': func_args\n                },\n            ),\n            finish_reason='tool_calls',\n        )\n        return choice_data\n\n    z = response.rfind('\\nFinal Answer: ')\n    if z >= 0:\n        response = response[z + len('\\nFinal Answer: '):]\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=ChatMessage(role='assistant', content=response),\n        finish_reason='stop',\n    )\n    return choice_data\n\n\ndef prepare_chat(tokenizer, query, history, system):\n    \"\"\"Prepare model inputs for chat completion.\"\"\"\n    if prompt_template:\n        history_messages = history + [[query, \"\"]]\n        prompt = prompt_template.get_prompt(messages=history_messages, system_prompt=system)\n    else:\n        messages = [\n            {\"role\": \"system\", \"content\": system}\n        ]\n        for i, (question, response) in enumerate(history):\n            question = question.lstrip('\\n').rstrip()\n            response = response.lstrip('\\n').rstrip()\n            messages.append({\"role\": \"user\", \"content\": question})\n            messages.append({\"role\": \"assistant\", \"content\": response})\n        messages.append({\"role\": \"user\", \"content\": query})\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n    model_inputs = tokenizer([prompt], return_tensors='pt')\n    return model_inputs\n\n\ndef model_chat(model, tokenizer, query, history, gen_kwargs, system):\n    \"\"\"Generate chat completion from the model.\"\"\"\n    model_inputs = prepare_chat(tokenizer, query, history, system).to(model.device)\n    generated_ids = model.generate(model_inputs.input_ids, **gen_kwargs)\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    prompt_length = len(model_inputs.input_ids[0])\n    response_length = len(generated_ids[0])\n    return response, prompt_length, response_length\n\n\ndef stream_model_chat(model, tokenizer, query, history, gen_kwargs, system):\n    \"\"\"Generate chat completion from the model in stream mode.\"\"\"\n    model_inputs = prepare_chat(tokenizer, query, history, system).to(model.device)\n    gen_kwargs['inputs'] = model_inputs.input_ids\n\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n    gen_kwargs['streamer'] = streamer\n    thread = Thread(target=model.generate, kwargs=gen_kwargs, daemon=True)\n    thread.start()\n\n    yield from streamer\n\n\n@app.post('/v1/chat/completions', response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    \"\"\"Generate chat completion.\"\"\"\n    global model, tokenizer\n\n    gen_kwargs = {}\n    if request.top_k is not None:\n        gen_kwargs['top_k'] = request.top_k\n    if request.temperature is not None:\n        if request.temperature < 0.01:\n            gen_kwargs['top_k'] = 1  # greedy decoding\n        else:\n            # Not recommended. Please tune top_p instead.\n            gen_kwargs['temperature'] = request.temperature\n    if request.top_p is not None:\n        gen_kwargs['top_p'] = request.top_p\n    if request.max_length is not None:\n        gen_kwargs['max_length'] = request.max_length\n\n    stop_words = add_extra_stop_words(request.stop)\n    if request.tools:\n        stop_words = stop_words or []\n        if 'Observation:' not in stop_words:\n            stop_words.append('Observation:')\n\n    query, history, system = parse_messages(request.messages, request.tools)\n\n    if request.stream:\n        if request.tools:\n            raise HTTPException(\n                status_code=400,\n                detail='Invalid request: Function calling is not yet implemented for stream mode.',\n            )\n        generate = stream_chat_completion(\n            query,\n            history,\n            request.model,\n            stop_words,\n            gen_kwargs,\n            system=system\n        )\n        return StreamingResponse(generate, media_type='text/event-stream')\n\n    response, prompt_length, response_length = model_chat(\n        model,\n        tokenizer,\n        query,\n        history,\n        gen_kwargs=gen_kwargs,\n        system=system\n    )\n    logger.debug(f'*** history begin ***\\n{history}\\n*** history end ***\\n'\n                 f'question: {query}\\nresponse: {response}\\n')\n    _gc()\n\n    response = trim_stop_words(response, stop_words)\n    if request.tools:\n        choice_data = parse_response(response)\n    else:\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(role='assistant', content=response),\n            finish_reason='stop',\n        )\n\n    usage = ChatCompletionResponseUsage(\n        prompt_tokens=prompt_length,\n        completion_tokens=response_length,\n        total_tokens=prompt_length + response_length,\n    )\n    return ChatCompletionResponse(model=request.model, choices=[choice_data], usage=usage)\n\n\ndef dictify(data: BaseModel) -> Dict[str, Any]:\n    try:  # pydantic v2\n        return data.model_dump(exclude_unset=True)\n    except AttributeError:  # pydantic v1\n        return data.dict(exclude_unset=True)\n\n\ndef jsonify(data: BaseModel) -> str:\n    try:  # pydantic v2\n        return json.dumps(data.model_dump(exclude_unset=True), ensure_ascii=False)\n    except AttributeError:  # pydantic v1\n        return data.json(exclude_unset=True, ensure_ascii=False)\n\n\nasync def stream_chat_completion(\n        query: str,\n        history: List[List[str]],\n        model_id: str,\n        stop_words: List[str],\n        gen_kwargs: Dict,\n        system: str,\n):\n    \"\"\"Generate chat completion in stream mode.\"\"\"\n    global model, tokenizer\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(role='assistant', content=\"\"), finish_reason=None)\n    chunk = ChatCompletionStreamResponse(model=model_id, choices=[choice_data])\n    yield jsonify(chunk)\n\n    stop_words = [x for x in stop_words if x]\n    response_generator = stream_model_chat(\n        model,\n        tokenizer,\n        query,\n        history,\n        gen_kwargs,\n        system\n    )\n    for token_output in response_generator:\n        # Check if any stop word is in the token output\n        if any(stop_word in token_output for stop_word in stop_words):\n            break\n\n        # Send the current token as part of the response\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0, delta=DeltaMessage(content=token_output), finish_reason=None)\n        chunk = ChatCompletionStreamResponse(model=model_id, choices=[choice_data])\n        yield jsonify(chunk)\n\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(), finish_reason='stop'\n    )\n    chunk = ChatCompletionStreamResponse(model=model_id, choices=[choice_data])\n    yield jsonify(chunk)\n    yield '[DONE]'\n\n    _gc()\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument('--base_model', type=str, default='Qwen/Qwen-7B-Chat', help='Model name or path')\n    parser.add_argument('--lora_model', default=None, type=str, help=\"If None, perform inference on the base model\")\n    parser.add_argument('--template_name', default=None, type=str,\n                        help=\"Prompt template name, eg: alpaca, vicuna, baichuan, chatglm2 etc.\")\n    parser.add_argument('--api_auth', help='API authentication credentials')\n    parser.add_argument('--cpu_only', action='store_true', help='Run demo with CPU only')\n    parser.add_argument('--server_port', type=int, default=8000, help='Demo server port.')\n    parser.add_argument('--server_name', type=str, default='127.0.0.1',\n                        help=('Demo server name. Default: 127.0.0.1, which is only visible from the local computer. '\n                              'If you want other computers to access your server, use 0.0.0.0 instead.')\n                        )\n    parser.add_argument('--disable_gc', action='store_true', help='Disable GC after each response generated.')\n\n    args = parser.parse_args()\n    logger.info(args)\n\n    if args.api_auth:\n        app.add_middleware(\n            BasicAuthMiddleware,\n            username=args.api_auth.split(':')[0],\n            password=args.api_auth.split(':')[1]\n        )\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.base_model,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = 'cpu'\n    else:\n        device_map = 'auto'\n    model = AutoModelForCausalLM.from_pretrained(\n        args.base_model,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n    if args.lora_model:\n        from peft import PeftModel\n\n        model = PeftModel.from_pretrained(model, args.lora_model, device_map=device_map)\n        logger.debug(f'Loaded LORA model: {args.lora_model}')\n\n    model = model.eval()\n    model.generation_config = GenerationConfig.from_pretrained(\n        args.base_model,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n    if args.template_name:\n        prompt_template = get_conv_template(args.template_name)\n    else:\n        prompt_template = None\n\n    uvicorn.run(app, host=args.server_name, port=args.server_port, workers=1)\n"
        },
        {
          "name": "orpo_training.py",
          "type": "blob",
          "size": 22.4306640625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: Train a model from base model using ORPO\n\"\"\"\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import Dict, Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType\nfrom transformers import (\n    AutoConfig,\n    BloomForCausalLM,\n    AutoModelForCausalLM,\n    AutoModel,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n    BitsAndBytesConfig,\n)\nfrom transformers.deepspeed import is_deepspeed_zero3_enabled\nfrom trl import ORPOConfig, ORPOTrainer\n\nfrom template import get_conv_template\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with DPO\n    \"\"\"\n    # Model arguments\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The model checkpoint for weights initialization.\"}\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The tokenizer for weights initialization.\"}\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n    # Dataset arguments\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The input jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The prompt template name.\"})\n    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"Train batch size per device\"})\n    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"Eval batch size per device\"})\n    max_source_length: Optional[int] = field(default=2048, metadata={\"help\": \"Max length of prompt input text\"})\n    max_target_length: Optional[int] = field(default=512, metadata={\"help\": \"Max length of output text\"})\n    min_target_length: Optional[int] = field(default=4, metadata={\"help\": \"Min length of output text\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=4, metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    # Training arguments\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})\n    target_modules: Optional[str] = field(default=\"all\", metadata={\"help\": \"The target modules for peft\"})\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=16.0)\n    peft_path: Optional[str] = field(default=None)\n    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the validation set.\"})\n    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"The beta parameter for DPO loss\"})\n    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"Learning rate\"})\n    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"The lr scheduler type\"})\n    warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"The number of warmup steps\"})\n    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"The weight decay\"})\n    optim: Optional[str] = field(default=\"adamw_hf\", metadata={\"help\": \"The optimizer type\"})\n    fp16: Optional[bool] = field(default=True, metadata={\"help\": \"Whether to use fp16\"})\n    bf16: Optional[bool] = field(default=False, metadata={\"help\": \"Whether to use bf16\"})\n    gradient_checkpointing: Optional[bool] = field(\n        default=True, metadata={\"help\": \"Whether to use gradient checkpointing\"}\n    )\n    gradient_accumulation_steps: Optional[int] = field(\n        default=4, metadata={\"help\": \"The number of gradient accumulation steps\"}\n    )\n    save_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to save the model\"})\n    eval_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to evaluate the model\"})\n    logging_steps: Optional[int] = field(default=1, metadata={\"help\": \"X steps to log the model\"})\n    output_dir: Optional[str] = field(default=\"outputs-dpo\", metadata={\"help\": \"The output directory\"})\n    max_steps: Optional[int] = field(default=200, metadata={\"help\": \"Number of steps to train\"})\n    eval_strategy: Optional[str] = field(default=\"steps\", metadata={\"help\": \"Evaluation strategy\"})\n    remove_unused_columns: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Remove unused columns from the dataset if `datasets.Dataset` is used\"},\n    )\n    report_to: Optional[str] = field(default=\"tensorboard\", metadata={\"help\": \"Report to wandb or tensorboard\"})\n    orpo_beta: float = field(\n        default=0.1,\n        metadata={\"help\": \"The beta (lambda) parameter in ORPO loss representing the weight of the SFT loss.\"},\n    )\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\"You must specify a valid model_type to run training.\")\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'output_layer' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\ndef main():\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    logger.info(f\"Parse args: {args}\")\n\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    if args.model_type == 'bloom':\n        args.use_fast_tokenizer = True\n    # Load tokenizer\n    tokenizer_kwargs = {\n        \"cache_dir\": args.cache_dir,\n        \"use_fast\": args.use_fast_tokenizer,\n        \"trust_remote_code\": args.trust_remote_code,\n    }\n    tokenizer_name_or_path = args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    # Get datasets\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name,\n            args.dataset_config_name,\n            cache_dir=args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    else:\n        data_files = {}\n        if args.train_file_dir is not None and os.path.exists(args.train_file_dir):\n            train_data_files = glob(f'{args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {', '.join(train_data_files)}\")\n            data_files[\"train\"] = train_data_files\n        if args.validation_file_dir is not None and os.path.exists(args.validation_file_dir):\n            eval_data_files = glob(f'{args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    max_source_length = args.max_source_length\n    max_target_length = args.max_target_length\n    full_max_length = max_source_length + max_target_length\n\n    def return_prompt_and_responses(examples) -> Dict[str, str]:\n        \"\"\"Load the paired dataset and convert it to the necessary format.\n\n        The dataset is converted to a dictionary with the following structure:\n        {\n            'prompt': List[str],\n            'chosen': List[str],\n            'rejected': List[str],\n        }\n\n        Prompts are structured as follows:\n          system_prompt + history[[q,a], [q,a]...] + question\n        \"\"\"\n        prompts = []\n        for system, history, question in zip(examples[\"system\"], examples[\"history\"], examples[\"question\"]):\n            system_prompt = system or \"\"\n            history_with_question = history + [[question, '']] if history else [[question, '']]\n            prompts.append(prompt_template.get_prompt(messages=history_with_question, system_prompt=system_prompt))\n        return {\n            \"prompt\": prompts,\n            \"chosen\": examples[\"response_chosen\"],\n            \"rejected\": examples[\"response_rejected\"],\n        }\n\n    # Preprocess the dataset\n    train_dataset = None\n    max_train_samples = 0\n    if args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train']\n        max_train_samples = len(train_dataset)\n        if args.max_train_samples is not None and args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        tokenized_dataset = train_dataset.shuffle().map(\n            return_prompt_and_responses,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=train_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        train_dataset = tokenized_dataset.filter(\n            lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n                      and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n        )\n        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n        logger.debug(\"First train example:\")\n        first_example = train_dataset[0]\n        logger.debug(f\"prompt:\\n{first_example['prompt']}\")\n        logger.debug(f\"chosen:\\n{first_example['chosen']}\")\n        logger.debug(f\"rejected:\\n{first_example['rejected']}\")\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if args.do_eval:\n        if \"validation\" not in raw_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = raw_datasets[\"validation\"]\n        max_eval_samples = len(eval_dataset)\n        if args.max_eval_samples is not None and args.max_eval_samples > 0:\n            max_eval_samples = min(len(eval_dataset), args.max_eval_samples)\n            eval_dataset = eval_dataset.select(range(max_eval_samples))\n        logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n        eval_dataset = eval_dataset.map(\n            return_prompt_and_responses,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=eval_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        eval_dataset = eval_dataset.filter(\n            lambda x: 0 < len(x['prompt'] + x['chosen']) <= full_max_length\n                      and 0 < len(x['prompt'] + x['rejected']) <= full_max_length\n        )\n        logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n        logger.debug(\"First eval example:\")\n        first_example = eval_dataset[0]\n        logger.debug(f\"prompt:\\n{first_example['prompt']}\")\n        logger.debug(f\"chosen:\\n{first_example['chosen']}\")\n        logger.debug(f\"rejected:\\n{first_example['rejected']}\")\n\n    # Load model\n    torch_dtype = (\n        args.torch_dtype\n        if args.torch_dtype in [\"auto\", None]\n        else getattr(torch, args.torch_dtype)\n    )\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    ddp = world_size != 1\n    if ddp:\n        args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n    logger.info(f\"Device map: {args.device_map}\")\n    if args.qlora and is_deepspeed_zero3_enabled():\n        logger.warning(\"ZeRO3 are both currently incompatible with QLoRA.\")\n    config = config_class.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=args.trust_remote_code,\n        torch_dtype=torch_dtype,\n        cache_dir=args.cache_dir\n    )\n    if args.load_in_4bit or args.load_in_8bit:\n        logger.info(f\"Quantizing model, load_in_4bit: {args.load_in_4bit}, load_in_8bit: {args.load_in_8bit}\")\n    model = model_class.from_pretrained(\n        args.model_name_or_path,\n        config=config,\n        torch_dtype=torch_dtype,\n        low_cpu_mem_usage=(not is_deepspeed_zero3_enabled()),\n        device_map=args.device_map,\n        trust_remote_code=args.trust_remote_code,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.load_in_4bit,\n            load_in_8bit=args.load_in_8bit,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch_dtype,\n        ) if args.qlora else None,\n    )\n    # fixed FP16 ValueError\n    for param in filter(lambda p: p.requires_grad, model.parameters()):\n        param.data = param.data.to(torch.float32)\n\n    # Initialize our Trainer\n    if args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n    else:\n        model.config.use_cache = True\n\n    training_args = ORPOConfig(\n        max_length=full_max_length,\n        max_prompt_length=args.max_source_length,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        per_device_eval_batch_size=args.per_device_eval_batch_size,\n        max_steps=args.max_steps,\n        logging_steps=args.logging_steps,\n        save_steps=args.save_steps,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        gradient_checkpointing=args.gradient_checkpointing,\n        learning_rate=args.learning_rate,\n        evaluation_strategy=args.eval_strategy,\n        eval_steps=args.eval_steps,\n        output_dir=args.output_dir,\n        report_to=args.report_to,\n        lr_scheduler_type=args.lr_scheduler_type,\n        warmup_steps=args.warmup_steps,\n        optim=args.optim,\n        bf16=args.bf16,\n        fp16=args.fp16,\n        remove_unused_columns=args.remove_unused_columns,\n        run_name=f\"orpo_{args.model_type}\",\n        beta=args.orpo_beta,\n    )\n\n    # Initialize ORPO trainer\n    peft_config = None\n    if args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        target_modules = args.target_modules.split(',') if args.target_modules else None\n        if target_modules and 'all' in target_modules:\n            target_modules = find_all_linear_names(model, int4=args.load_in_4bit, int8=args.load_in_8bit)\n        logger.info(f\"Peft target_modules: {target_modules}\")\n        peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM,\n            target_modules=target_modules,\n            inference_mode=False,\n            r=args.lora_rank,\n            lora_alpha=args.lora_alpha,\n            lora_dropout=args.lora_dropout,\n        )\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n    trainer = ORPOTrainer(\n        model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        peft_config=peft_config if args.use_peft else None,\n    )\n    print_trainable_parameters(trainer.model)\n\n    # Training\n    if args.do_train:\n        logger.info(\"*** Train ***\")\n        train_result = trainer.train()\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {args.output_dir}\")\n            trainer.save_model(args.output_dir)\n            tokenizer.save_pretrained(args.output_dir)\n            trainer.model.save_pretrained(args.output_dir)\n\n    # Evaluation\n    if args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n        metrics[\"eval_samples\"] = max_eval_samples\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "ppo_training.py",
          "type": "blob",
          "size": 22.197265625,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: Train a model from SFT using PPO\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    BloomForCausalLM,\n    AutoModelForCausalLM,\n    AutoModel,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n)\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n\nfrom template import get_conv_template\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with PPO\n    \"\"\"\n    # Model arguments\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The model checkpoint for weights initialization.\"}\n    )\n    reward_model_name_or_path: Optional[str] = field(default=None, metadata={\"help\": \"The reward model name\"})\n    reward_model_device: Optional[str] = field(default=\"cuda:0\", metadata={\"help\": \"The reward model device\"})\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The tokenizer for weights initialization.\"}\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n    # Dataset arguments\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The input jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The template name.\"})\n    batch_size: Optional[int] = field(default=8, metadata={\"help\": \"Batch size\"})\n    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"PPO minibatch size\"})\n    max_source_length: Optional[int] = field(default=2048, metadata={\"help\": \"Max length of prompt input text\"})\n    max_target_length: Optional[int] = field(default=512, metadata={\"help\": \"Max length of output text\"})\n    min_target_length: Optional[int] = field(default=4, metadata={\"help\": \"Min length of output text\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None, metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    # Training arguments\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    target_modules: Optional[str] = field(default=None, metadata={\"help\": \"The target modules for peft\"})\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=32.0)\n    modules_to_save: Optional[str] = field(default=None)\n    peft_path: Optional[str] = field(default=None)\n    # PPO arguments\n    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the validation set.\"})\n    early_stopping: Optional[bool] = field(default=False, metadata={\"help\": \"Whether to early stop\"})\n    target_kl: Optional[float] = field(default=0.1, metadata={\"help\": \"The kl target for early stopping\"})\n    reward_baseline: Optional[float] = field(\n        default=0.0, metadata={\"help\": \"Baseline value that is subtracted from the reward\"},\n    )\n    init_kl_coef: Optional[float] = field(\n        default=0.2, metadata={\"help\": \"Initial KL penalty coefficient (used for adaptive and linear control)\"},\n    )\n    adap_kl_ctrl: Optional[bool] = field(default=True, metadata={\"help\": \"Use adaptive KL control, otherwise linear\"})\n    learning_rate: Optional[float] = field(default=1.5e-5, metadata={\"help\": \"Learning rate\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    save_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to save the model\"})\n    output_dir: Optional[str] = field(default=\"outputs-rl\", metadata={\"help\": \"The output directory\"})\n    seed: Optional[int] = field(default=0, metadata={\"help\": \"Seed\"})\n    max_steps: Optional[int] = field(default=200, metadata={\"help\": \"Number of steps to train\"})\n    report_to: Optional[str] = field(default=\"tensorboard\", metadata={\"help\": \"Report to wandb or tensorboard\"})\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\"You must specify a valid model_type to run training.\")\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n        if self.reward_model_name_or_path is None:\n            raise ValueError(\"You must specify a valid reward_model_name_or_path to run training.\")\n        if self.max_source_length < 60:\n            raise ValueError(\"You must specify a valid max_source_length >= 60 to run training\")\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef get_reward_model_output(reward_model, reward_tokenizer, question, answer, device):\n    \"\"\"\n    Get the reward score for a given question and answer pair.\n    \"\"\"\n    inputs = reward_tokenizer(question, answer, return_tensors='pt').to(device)\n    score = reward_model(**inputs).logits[0].cpu().detach()\n\n    return score\n\n\ndef calculate_rewards(reward_score_outputs, reward_baseline=0):\n    \"\"\"\n    Calculate the reward for a given score output.\n    :param reward_score_outputs: \n    :param reward_baseline: \n    :return: \n    \"\"\"\n    rewards = []\n    for score in reward_score_outputs:\n        if isinstance(score, torch.Tensor) and score.numel() == 1:\n            reward_value = score.item() - reward_baseline\n            rewards.append(torch.tensor(reward_value))\n        else:\n            # Use the average of the tensor elements as `score` is multiple elements\n            reward_value = torch.mean(score).item() - reward_baseline\n            rewards.append(torch.tensor(reward_value))\n    return rewards\n\n\ndef main():\n    parser = HfArgumentParser(ScriptArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    logger.info(f\"Parse args: {args}\")\n\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    if args.model_type == 'bloom':\n        args.use_fast_tokenizer = True\n    # Load tokenizer\n    tokenizer_kwargs = {\n        \"cache_dir\": args.cache_dir,\n        \"use_fast\": args.use_fast_tokenizer,\n        \"trust_remote_code\": args.trust_remote_code,\n    }\n    tokenizer_name_or_path = args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    # Load model\n    peft_config = None\n    if args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        target_modules = args.target_modules.split(',') if args.target_modules else None\n        logger.info(f\"Peft target_modules: {target_modules}\")\n        peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM,\n            target_modules=target_modules,\n            inference_mode=False,\n            r=args.lora_rank,\n            lora_alpha=args.lora_alpha,\n            lora_dropout=args.lora_dropout,\n        )\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n    torch_dtype = (\n        args.torch_dtype\n        if args.torch_dtype in [\"auto\", None]\n        else getattr(torch, args.torch_dtype)\n    )\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    if world_size > 1:\n        args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n    config = config_class.from_pretrained(\n        args.model_name_or_path,\n        torch_dtype=torch_dtype,\n        trust_remote_code=args.trust_remote_code,\n        cache_dir=args.cache_dir\n    )\n    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n        args.model_name_or_path,\n        config=config,\n        torch_dtype=torch_dtype,\n        load_in_4bit=args.load_in_4bit,\n        load_in_8bit=args.load_in_8bit,\n        device_map=args.device_map,\n        trust_remote_code=args.trust_remote_code,\n        peft_config=peft_config if args.use_peft else None,\n    )\n    for param in filter(lambda p: p.requires_grad, model.parameters()):\n        param.data = param.data.to(torch.float32)\n\n    print_trainable_parameters(model)\n    # Load reward model\n    default_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    device = args.reward_model_device if args.reward_model_device is not None else default_device\n    reward_config = config_class.from_pretrained(\n        args.reward_model_name_or_path,\n        torch_dtype=torch_dtype,\n        trust_remote_code=args.trust_remote_code,\n        cache_dir=args.cache_dir\n    )\n    reward_model = AutoModelForSequenceClassification.from_pretrained(\n        args.reward_model_name_or_path,\n        config=reward_config,\n        load_in_8bit=args.load_in_8bit,\n        trust_remote_code=args.trust_remote_code,\n    )\n    reward_model.to(device)\n    reward_tokenizer = AutoTokenizer.from_pretrained(\n        args.reward_model_name_or_path, **tokenizer_kwargs\n    )\n\n    # Get datasets\n    if args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            args.dataset_name,\n            args.dataset_config_name,\n            cache_dir=args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    else:\n        data_files = {}\n        if args.train_file_dir is not None and os.path.exists(args.train_file_dir):\n            train_data_files = glob(f'{args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {', '.join(train_data_files)}\")\n            data_files[\"train\"] = train_data_files\n        if args.validation_file_dir is not None and os.path.exists(args.validation_file_dir):\n            eval_data_files = glob(f'{args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                cache_dir=args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                cache_dir=args.cache_dir,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    max_source_length = args.max_source_length\n    max_target_length = args.max_target_length\n\n    def preprocess_function(examples):\n        new_examples = {\n            \"query\": [],\n            \"input_ids\": [],\n        }\n        roles = [\"human\", \"gpt\"]\n\n        def get_dialog(examples):\n            system_prompts = examples.get(\"system_prompt\", \"\")\n            for i, source in enumerate(examples['conversations']):\n                if len(source) < 2:\n                    continue\n                data_role = source[0].get(\"from\", \"\")\n                if data_role not in roles or data_role != roles[0]:\n                    # Skip the first one if it is not from human\n                    source = source[1:]\n                if len(source) < 2:\n                    continue\n                messages = []\n                for j, sentence in enumerate(source):\n                    data_role = sentence.get(\"from\", \"\")\n                    if data_role not in roles:\n                        logger.warning(f\"unknown role: {data_role}, {i}. (ignored)\")\n                        break\n                    if data_role == roles[j % 2]:\n                        messages.append(sentence[\"value\"])\n                if len(messages) < 2 or len(messages) % 2 != 0:\n                    continue\n                # Convert the list to pairs of elements\n                history_messages = [[messages[k], messages[k + 1]] for k in range(0, len(messages), 2)]\n                system_prompt = system_prompts[i] if system_prompts else None\n                yield prompt_template.get_dialog(history_messages, system_prompt=system_prompt)\n\n        for dialog in get_dialog(examples):\n            for i in range(len(dialog) // 2):\n                source_txt = dialog[2 * i]\n                tokenized_question = tokenizer(\n                    source_txt, truncation=True, max_length=max_source_length, padding=\"max_length\",\n                    return_tensors=\"pt\"\n                )\n                new_examples[\"query\"].append(source_txt)\n                new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n\n        return new_examples\n\n    # Preprocess the dataset\n    train_dataset = None\n    if args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train']\n        if args.max_train_samples is not None and args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        tokenized_dataset = train_dataset.shuffle().map(\n            preprocess_function,\n            batched=True,\n            num_proc=args.preprocessing_num_workers,\n            remove_columns=train_dataset.column_names,\n            load_from_cache_file=not args.overwrite_cache,\n            desc=\"Running tokenizer on dataset\",\n        )\n        train_dataset = tokenized_dataset.filter(\n            lambda x: len(x['input_ids']) > 0\n        )\n        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n\n    def collator(data):\n        return dict((key, [d[key] for d in data]) for key in data[0])\n\n    output_dir = args.output_dir\n    config = PPOConfig(\n        steps=args.max_steps,\n        model_name=args.model_name_or_path,\n        learning_rate=args.learning_rate,\n        log_with=args.report_to,\n        batch_size=args.batch_size,\n        mini_batch_size=args.mini_batch_size,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        optimize_cuda_cache=True,\n        early_stopping=args.early_stopping,\n        target_kl=args.target_kl,\n        seed=args.seed,\n        init_kl_coef=args.init_kl_coef,\n        adap_kl_ctrl=args.adap_kl_ctrl,\n        project_kwargs={\"logging_dir\": output_dir},\n    )\n    # Set seed before initializing value head for deterministic eval\n    set_seed(config.seed)\n\n    # We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n    trainer = PPOTrainer(\n        config,\n        model,\n        ref_model=None,\n        tokenizer=tokenizer,\n        dataset=train_dataset,\n        data_collator=collator,\n    )\n\n    # These arguments are passed to the `generate` function of the PPOTrainer\n    generation_kwargs = {\n        \"max_new_tokens\": max_target_length,\n        \"temperature\": 1.0,\n        \"repetition_penalty\": 1.0,\n        \"top_p\": 1.0,\n        \"do_sample\": True,\n    }\n\n    # Training\n    if args.do_train:\n        logger.info(\"*** Train ***\")\n        total_steps = config.total_ppo_epochs\n        for step, batch in tqdm(enumerate(trainer.dataloader)):\n            if step >= total_steps:\n                break\n            question_tensors = batch[\"input_ids\"]\n            question_tensors = [torch.LongTensor(i).to(device).squeeze(0) for i in question_tensors]\n            responses = []\n            response_tensors = []\n            for q_tensor in question_tensors:\n                response_tensor = trainer.generate(\n                    q_tensor,\n                    return_prompt=False,\n                    **generation_kwargs,\n                )\n                r = tokenizer.batch_decode(response_tensor, skip_special_tokens=True)[0]\n                responses.append(r)\n                response_tensors.append(response_tensor.squeeze(0))\n            batch[\"response\"] = responses\n\n            # Compute reward score\n            score_outputs = [\n                get_reward_model_output(reward_model, reward_tokenizer, q, r, device) for q, r in\n                zip(batch[\"query\"], batch[\"response\"])\n            ]\n            rewards = calculate_rewards(score_outputs, args.reward_baseline)\n\n            # Run PPO step\n            try:\n                stats = trainer.step(question_tensors, response_tensors, rewards)\n                trainer.log_stats(stats, batch, rewards)\n                logger.debug(f\"Step {step}/{total_steps}: reward score:{score_outputs}\")\n            except ValueError as e:\n                logger.warning(f\"Failed to log stats for step {step}, because of {e}\")\n\n            if step and step % args.save_steps == 0:\n                save_dir = os.path.join(output_dir, f\"checkpoint-{step}\")\n                trainer.save_pretrained(save_dir)\n        # Save final model\n        trainer.save_pretrained(output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "pretraining.py",
          "type": "blob",
          "size": 33.1650390625,
          "content": "# -*- coding: utf-8 -*-\n# Copyright 2023 XuMing(xuming624@qq.com) and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.\n\npart of this code is adapted from https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py\n\"\"\"\nimport math\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom itertools import chain\nfrom typing import Optional, List, Dict, Any, Mapping\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom sklearn.metrics import accuracy_score\nfrom transformers import (\n    AutoConfig,\n    BloomForCausalLM,\n    AutoModelForCausalLM,\n    AutoModel,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n    Trainer,\n    Seq2SeqTrainingArguments,\n    is_torch_tpu_available,\n    set_seed,\n    BitsAndBytesConfig,\n)\nfrom transformers.trainer import TRAINING_ARGS_NAME\nfrom transformers.utils.versions import require_version\n\ntry:\n    from transformers.integrations import is_deepspeed_zero3_enabled\nexcept ImportError:  # https://github.com/huggingface/transformers/releases/tag/v4.33.1\n    from transformers.deepspeed import is_deepspeed_zero3_enabled\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    model_revision: Optional[str] = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    hf_hub_token: Optional[str] = field(default=None, metadata={\"help\": \"Auth token to log in with Hugging Face Hub.\"})\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\n                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n                    MODEL_CLASSES.keys()))\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\n@dataclass\nclass DataArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The train text data file folder.\"})\n    validation_file_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on text file folder.\"},\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n    block_size: Optional[int] = field(\n        default=1024,\n        metadata={\n            \"help\": (\n                \"Optional input sequence length after tokenization. \"\n                \"The training dataset will be truncated in block of this size for training. \"\n                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    keep_linebreaks: bool = field(\n        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n    )\n\n    def __post_init__(self):\n        if self.streaming:\n            require_version(\"datasets>=2.0.0\", \"The streaming feature requires `datasets>=2.0.0`\")\n\n\n@dataclass\nclass ScriptArguments:\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    target_modules: Optional[str] = field(default=\"all\")\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=32.0)\n    modules_to_save: Optional[str] = field(default=None)\n    peft_path: Optional[str] = field(default=None)\n    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})\n\n\ndef accuracy(predictions, references, normalize=True, sample_weight=None):\n    return {\n        \"accuracy\": float(accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight))\n    }\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n    # by preprocess_logits_for_metrics, we need to shift the labels\n    labels = labels[:, 1:].reshape(-1)\n    preds = preds[:, :-1].reshape(-1)\n    return accuracy(predictions=preds, references=labels)\n\n\ndef preprocess_logits_for_metrics(logits, labels):\n    if isinstance(logits, tuple):\n        # Depending on the model and config, logits may contain extra tensors,\n        # like past_key_values, but logits always come first\n        logits = logits[0]\n    return logits.argmax(dim=-1)\n\n\ndef fault_tolerance_data_collator(features: List) -> Dict[str, Any]:\n    if not isinstance(features[0], Mapping):\n        features = [vars(f) for f in features]\n    first = features[0]\n    batch = {}\n\n    # Special handling for labels.\n    # Ensure that tensor is created with the correct type\n    if \"label\" in first and first[\"label\"] is not None:\n        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n        dtype = torch.long if isinstance(label, int) else torch.float\n        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n        if isinstance(first[\"label_ids\"], torch.Tensor):\n            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n        else:\n            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n\n    # Handling of all other possible keys.\n    # Again, we will use the first element to figure out which key/values are not None for this model.\n    try:\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([f[k] for f in features])\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n                else:\n                    batch[k] = torch.tensor([f[k] for f in features])\n    except ValueError:  # quick fix by simply take the first example\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([features[0][k]] * len(features))\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))\n                else:\n                    batch[k] = torch.tensor([features[0][k]] * len(features))\n\n    return batch\n\n\nclass GroupTextsBuilder:\n    def __init__(self, max_seq_length):\n        self.max_seq_length = max_seq_length\n\n    def __call__(self, examples):\n        # Concatenate all texts.\n        firsts = {k: examples[k][0][0] for k in examples.keys()}\n        lasts = {k: examples[k][0][-1] for k in examples.keys()}\n        contents = {k: sum([vi[1:-1] for vi in v], []) for k, v in examples.items()}\n        total_length = len(contents[list(examples.keys())[0]])\n\n        content_length = self.max_seq_length - 2\n        if total_length >= content_length:\n            total_length = (total_length // content_length) * content_length\n        # Split by chunks of max_len.\n        result = {\n            k: [[firsts[k]] + t[i: i + content_length] + [lasts[k]] for i in range(0, total_length, content_length)] for\n            k, t in contents.items()}\n        return result\n\n\nclass SavePeftModelTrainer(Trainer):\n    \"\"\"\n    Trainer for lora models\n    \"\"\"\n\n    def save_model(self, output_dir=None, _internal_call=False):\n        \"\"\"Save the LoRA model.\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n        self.model.save_pretrained(output_dir)\n\n\ndef save_model(model, tokenizer, args):\n    \"\"\"Save the model and the tokenizer.\"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Take care of distributed/parallel training\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef save_model_zero3(model, tokenizer, args, trainer):\n    \"\"\"Save the model for deepspeed zero3.\n    refer https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train_lora.py#L209\n    \"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n    state_dict_zero3 = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(args.output_dir, state_dict=state_dict_zero3)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'output_layer' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\ndef main():\n    parser = HfArgumentParser((ModelArguments, DataArguments, Seq2SeqTrainingArguments, ScriptArguments))\n    model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses()\n\n    logger.info(f\"Model args: {model_args}\")\n    logger.info(f\"Data args: {data_args}\")\n    logger.info(f\"Training args: {training_args}\")\n    logger.info(f\"Script args: {script_args}\")\n    logger.info(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    # Load tokenizer\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n\n    tokenizer_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"use_fast\": model_args.use_fast_tokenizer,\n        \"trust_remote_code\": model_args.trust_remote_code,\n    }\n    tokenizer_name_or_path = model_args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = model_args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n\n    if data_args.block_size is None:\n        block_size = tokenizer.model_max_length\n        if block_size > 2048:\n            logger.warning(\n                \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n                \" of 2048. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n                \" override this default with `--block_size xxx`.\"\n            )\n    else:\n        if data_args.block_size > tokenizer.model_max_length:\n            logger.warning(\n                f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model\"\n                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n            )\n        block_size = min(data_args.block_size, tokenizer.model_max_length)\n\n    # Preprocessing the datasets.\n    def tokenize_function(examples):\n        tokenized_inputs = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding='max_length',\n            max_length=block_size\n        )\n        # Copy the input_ids to the labels for language modeling. This is suitable for both\n        # masked language modeling (like BERT) or causal language modeling (like GPT).\n        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n\n        return tokenized_inputs\n\n    def tokenize_wo_pad_function(examples):\n        return tokenizer(examples[\"text\"])\n\n    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n    def group_text_function(examples):\n        # Concatenate all texts.\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n        if total_length >= block_size:\n            total_length = (total_length // block_size) * block_size\n        # Split by chunks of max_len.\n        result = {\n            k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            data_args.dataset_name,\n            data_args.dataset_config_name,\n            cache_dir=model_args.cache_dir,\n            streaming=data_args.streaming,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                data_args.dataset_name,\n                data_args.dataset_config_name,\n                split=f\"train[:{data_args.validation_split_percentage}%]\",\n                cache_dir=model_args.cache_dir,\n                streaming=data_args.streaming,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                data_args.dataset_name,\n                data_args.dataset_config_name,\n                split=f\"train[{data_args.validation_split_percentage}%:]\",\n                cache_dir=model_args.cache_dir,\n                streaming=data_args.streaming,\n            )\n    else:\n        data_files = {}\n        dataset_args = {}\n        if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n            train_data_files = glob(f'{data_args.train_file_dir}/**/*.txt', recursive=True) + glob(\n                f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {train_data_files}\")\n            # Train data files must be same type, e.g. all txt or all jsonl\n            types = [f.split('.')[-1] for f in train_data_files]\n            if len(set(types)) > 1:\n                raise ValueError(f\"train files must be same type, e.g. all txt or all jsonl, but got {types}\")\n            data_files[\"train\"] = train_data_files\n        if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n            eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.txt', recursive=True) + glob(\n                f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {eval_data_files}\")\n            data_files[\"validation\"] = eval_data_files\n            # Train data files must be same type, e.g. all txt or all jsonl\n            types = [f.split('.')[-1] for f in eval_data_files]\n            if len(set(types)) > 1:\n                raise ValueError(f\"train files must be same type, e.g. all txt or all jsonl, but got {types}\")\n        extension = \"text\" if data_files[\"train\"][0].endswith('txt') else 'json'\n        if extension == \"text\":\n            dataset_args[\"keep_linebreaks\"] = data_args.keep_linebreaks\n        raw_datasets = load_dataset(\n            extension,\n            data_files=data_files,\n            cache_dir=model_args.cache_dir,\n            **dataset_args,\n        )\n\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[:{data_args.validation_split_percentage}%]\",\n                cache_dir=model_args.cache_dir,\n                **dataset_args,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[{data_args.validation_split_percentage}%:]\",\n                cache_dir=model_args.cache_dir,\n                **dataset_args,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets.\n    if training_args.do_train:\n        column_names = list(raw_datasets[\"train\"].features)\n    else:\n        column_names = list(raw_datasets[\"validation\"].features)\n\n    with training_args.main_process_first(desc=\"Dataset tokenization and grouping\"):\n        if not data_args.streaming:\n            if training_args.group_by_length:\n                tokenized_datasets = raw_datasets.map(\n                    tokenize_wo_pad_function,\n                    batched=True,\n                    num_proc=data_args.preprocessing_num_workers,\n                    remove_columns=column_names,\n                    load_from_cache_file=not data_args.overwrite_cache,\n                    desc=\"Running tokenizer on dataset\",\n                )\n                lm_datasets = tokenized_datasets.map(\n                    group_text_function,\n                    batched=True,\n                    num_proc=data_args.preprocessing_num_workers,\n                    load_from_cache_file=not data_args.overwrite_cache,\n                    desc=f\"Grouping texts in chunks of {block_size}\",\n                )\n            else:\n                lm_datasets = raw_datasets.map(\n                    tokenize_function,\n                    batched=True,\n                    num_proc=data_args.preprocessing_num_workers,\n                    remove_columns=column_names,\n                    load_from_cache_file=not data_args.overwrite_cache,\n                    desc=\"Running tokenizer on dataset\",\n                )\n        else:\n            if training_args.group_by_length:\n                tokenized_datasets = raw_datasets.map(\n                    tokenize_wo_pad_function,\n                    batched=True,\n                    remove_columns=column_names,\n                )\n                lm_datasets = tokenized_datasets.map(\n                    group_text_function,\n                    batched=True,\n                )\n            else:\n                lm_datasets = raw_datasets.map(\n                    tokenize_function,\n                    batched=True,\n                    remove_columns=column_names,\n                )\n\n    train_dataset = None\n    max_train_samples = 0\n    if training_args.do_train:\n        if \"train\" not in lm_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = lm_datasets['train']\n        max_train_samples = len(train_dataset)\n        if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n        logger.debug(\"Tokenized training example:\")\n        logger.debug(tokenizer.decode(train_dataset[0]['input_ids']))\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if training_args.do_eval:\n        if \"validation\" not in lm_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = lm_datasets[\"validation\"]\n        max_eval_samples = len(eval_dataset)\n        if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n            eval_dataset = eval_dataset.select(range(max_eval_samples))\n        logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n        logger.debug(\"Tokenized eval example:\")\n        logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))\n\n    # Load model\n    if model_args.model_type and model_args.model_name_or_path:\n        torch_dtype = (\n            model_args.torch_dtype\n            if model_args.torch_dtype in [\"auto\", None]\n            else getattr(torch, model_args.torch_dtype)\n        )\n        world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n        ddp = world_size != 1\n        if ddp:\n            model_args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n        if script_args.qlora and (len(training_args.fsdp) > 0 or is_deepspeed_zero3_enabled()):\n            logger.warning(\"FSDP and DeepSpeed ZeRO-3 are both currently incompatible with QLoRA.\")\n\n        config_kwargs = {\n            \"trust_remote_code\": model_args.trust_remote_code,\n            \"cache_dir\": model_args.cache_dir,\n            \"revision\": model_args.model_revision,\n            \"token\": model_args.hf_hub_token,\n        }\n        config = config_class.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n        load_in_4bit = model_args.load_in_4bit\n        load_in_8bit = model_args.load_in_8bit\n        if load_in_4bit and load_in_8bit:\n            raise ValueError(\"Error, load_in_4bit and load_in_8bit cannot be set at the same time\")\n        elif load_in_8bit or load_in_4bit:\n            logger.info(f\"Quantizing model, load_in_4bit: {load_in_4bit}, load_in_8bit: {load_in_8bit}\")\n            if is_deepspeed_zero3_enabled():\n                raise ValueError(\"DeepSpeed ZeRO-3 is incompatible with quantization.\")\n            if load_in_8bit:\n                config_kwargs['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n            elif load_in_4bit:\n                if script_args.qlora:\n                    config_kwargs['quantization_config'] = BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_use_double_quant=True,\n                        bnb_4bit_quant_type=\"nf4\",\n                        bnb_4bit_compute_dtype=torch_dtype,\n                    )\n                else:\n                    config_kwargs['quantization_config'] = BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_compute_dtype=torch_dtype,\n                    )\n\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            config=config,\n            torch_dtype=torch_dtype,\n            low_cpu_mem_usage=(not is_deepspeed_zero3_enabled()),\n            device_map=model_args.device_map,\n            **config_kwargs,\n        )\n    else:\n        raise ValueError(f\"Error, model_name_or_path is None, Continue PT must be loaded from a pre-trained model\")\n\n    if script_args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        if script_args.peft_path is not None:\n            logger.info(f\"Peft from pre-trained model: {script_args.peft_path}\")\n            model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)\n        else:\n            logger.info(\"Init new peft model\")\n            if load_in_8bit or load_in_4bit:\n                model = prepare_model_for_kbit_training(model, training_args.gradient_checkpointing)\n            target_modules = script_args.target_modules.split(',') if script_args.target_modules else None\n            if target_modules and 'all' in target_modules:\n                target_modules = find_all_linear_names(model, int4=load_in_4bit, int8=load_in_8bit)\n            modules_to_save = script_args.modules_to_save\n            if modules_to_save is not None:\n                modules_to_save = modules_to_save.split(',')\n                # Resize the embedding layer to match the new tokenizer\n                embedding_size = model.get_input_embeddings().weight.shape[0]\n                if len(tokenizer) > embedding_size:\n                    model.resize_token_embeddings(len(tokenizer))\n            logger.info(f\"Peft target_modules: {target_modules}\")\n            logger.info(f\"Peft lora_rank: {script_args.lora_rank}\")\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                target_modules=target_modules,\n                inference_mode=False,\n                r=script_args.lora_rank,\n                lora_alpha=script_args.lora_alpha,\n                lora_dropout=script_args.lora_dropout,\n                modules_to_save=modules_to_save)\n            model = get_peft_model(model, peft_config)\n        for param in filter(lambda p: p.requires_grad, model.parameters()):\n            param.data = param.data.to(torch.float32)\n        model.print_trainable_parameters()\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n        model = model.float()\n        print_trainable_parameters(model)\n\n    # Initialize our Trainer\n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n    else:\n        model.config.use_cache = True\n    model.enable_input_require_grads()\n    if not ddp and torch.cuda.device_count() > 1:\n        # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n        model.is_parallelizable = True\n        model.model_parallel = True\n\n    trainer = SavePeftModelTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=fault_tolerance_data_collator,\n        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,\n        preprocess_logits_for_metrics=preprocess_logits_for_metrics\n        if training_args.do_eval and not is_torch_tpu_available()\n        else None,\n    )\n\n    # Training\n    if training_args.do_train:\n        logger.info(\"*** Train ***\")\n        logger.debug(f\"Train dataloader example: {next(iter(trainer.get_train_dataloader()))}\")\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n        model.config.use_cache = True  # enable cache after training\n        tokenizer.padding_side = \"left\"  # restore padding side\n        tokenizer.init_kwargs[\"padding_side\"] = \"left\"\n\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n            if is_deepspeed_zero3_enabled():\n                save_model_zero3(model, tokenizer, training_args, trainer)\n            else:\n                save_model(model, tokenizer, training_args)\n\n    # Evaluation\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n\n        metrics[\"eval_samples\"] = max_eval_samples\n        try:\n            perplexity = math.exp(metrics[\"eval_loss\"])\n        except OverflowError:\n            perplexity = float(\"inf\")\n        metrics[\"perplexity\"] = perplexity\n\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2001953125,
          "content": "accelerate~=0.27.2\ndatasets>=2.14.6\nloguru\npeft~=0.10.0\nsentencepiece\nscikit-learn\ntensorboard\ntqdm>=4.47.0\ntransformers>=4.39.3 # GLM4 transformers==4.40.2\ntrl~=0.8.3\nbitsandbytes==0.43.3\ntiktoken\nGPUtil\n"
        },
        {
          "name": "reward_modeling.py",
          "type": "blob",
          "size": 27.96875,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description:\n\"\"\"\n\nimport math\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import Any, List, Union, Optional, Dict\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom transformers import (\n    AutoConfig,\n    PreTrainedTokenizerBase,\n    BloomForSequenceClassification,\n    LlamaForSequenceClassification,\n    BloomTokenizerFast,\n    AlbertForSequenceClassification,\n    BertForSequenceClassification,\n    BertTokenizer,\n    AutoTokenizer,\n    RobertaForSequenceClassification,\n    AutoModelForSequenceClassification,\n    RobertaTokenizer,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n)\nfrom transformers.trainer import TRAINING_ARGS_NAME\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bert\": (AutoConfig, BertForSequenceClassification, BertTokenizer),\n    \"roberta\": (AutoConfig, RobertaForSequenceClassification, RobertaTokenizer),\n    \"albert\": (AutoConfig, AlbertForSequenceClassification, AutoTokenizer),\n    \"bloom\": (AutoConfig, BloomForSequenceClassification, BloomTokenizerFast),\n    \"llama\": (AutoConfig, LlamaForSequenceClassification, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForSequenceClassification, AutoTokenizer),\n}\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\n                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n                    MODEL_CLASSES.keys()))\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\n@dataclass\nclass DataArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The input jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n    max_source_length: Optional[int] = field(default=2048, metadata={\"help\": \"Max length of prompt input text\"})\n    max_target_length: Optional[int] = field(default=512, metadata={\"help\": \"Max length of output text\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=4,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n\n\n@dataclass\nclass ScriptArguments:\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    target_modules: Optional[str] = field(default=\"all\")\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=32.0)\n    modules_to_save: Optional[str] = field(default=None)\n    peft_path: Optional[str] = field(default=None)\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The prompt template name.\"})\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    # Here, predictions is rewards_chosen and rewards_rejected.\n    if isinstance(preds, torch.Tensor):\n        preds = preds.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.detach().cpu().numpy()\n    # MSE\n    mse = mean_squared_error(labels, preds)\n    # MAE\n    mae = mean_absolute_error(labels, preds)\n\n    return {\"mse\": mse, \"mae\": mae}\n\n\n@dataclass\nclass RewardDataCollatorWithPadding:\n    \"\"\"We need to define a special data collator that batches the data in our chosen vs rejected format\"\"\"\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    return_tensors: str = \"pt\"\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        features_chosen = []\n        features_rejected = []\n        for feature in features:\n            features_chosen.append(\n                {\n                    \"input_ids\": feature[\"input_ids_chosen\"],\n                    \"attention_mask\": feature[\"attention_mask_chosen\"],\n                }\n            )\n            features_rejected.append(\n                {\n                    \"input_ids\": feature[\"input_ids_rejected\"],\n                    \"attention_mask\": feature[\"attention_mask_rejected\"],\n                }\n            )\n        batch_chosen = self.tokenizer.pad(\n            features_chosen,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch_rejected = self.tokenizer.pad(\n            features_rejected,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch = {\n            \"input_ids_chosen\": batch_chosen[\"input_ids\"],\n            \"attention_mask_chosen\": batch_chosen[\"attention_mask\"],\n            \"input_ids_rejected\": batch_rejected[\"input_ids\"],\n            \"attention_mask_rejected\": batch_rejected[\"attention_mask\"],\n            \"return_loss\": True,\n        }\n        return batch\n\n\nclass RewardTrainer(Trainer):\n    \"\"\"\n    Trainer for reward models\n        Define how to compute the reward loss. Use the InstructGPT pairwise logloss: https://arxiv.org/abs/2203.02155\n    \"\"\"\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        rewards_chosen = model(input_ids=inputs[\"input_ids_chosen\"],\n                               attention_mask=inputs[\"attention_mask_chosen\"])[0]\n        rewards_rejected = model(input_ids=inputs[\"input_ids_rejected\"],\n                                 attention_mask=inputs[\"attention_mask_rejected\"])[0]\n        # 计算损失：InstructGPT中的pairwise logloss\n        loss = -torch.nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n        if return_outputs:\n            return loss, {\"rewards_chosen\": rewards_chosen, \"rewards_rejected\": rewards_rejected}\n        return loss\n\n    def evaluate(\n            self,\n            eval_dataset: Optional[Dataset] = None,\n            ignore_keys: Optional[List[str]] = None,\n            metric_key_prefix: str = \"eval\",\n    ) -> Dict[str, float]:\n        if eval_dataset is None:\n            eval_dataset = self.eval_dataset\n        return super().evaluate(eval_dataset=eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        # Prepare inputs for chosen and rejected separately\n        device = model.device\n\n        inputs_chosen = {\n            \"input_ids\": inputs[\"input_ids_chosen\"].to(device),\n            \"attention_mask\": inputs[\"attention_mask_chosen\"].to(device),\n        }\n        outputs_chosen = model(**inputs_chosen)\n        rewards_chosen = outputs_chosen.logits.detach()\n\n        inputs_rejected = {\n            \"input_ids\": inputs[\"input_ids_rejected\"].to(device),\n            \"attention_mask\": inputs[\"attention_mask_rejected\"].to(device),\n        }\n        outputs_rejected = model(**inputs_rejected)\n        rewards_rejected = outputs_rejected.logits.detach()\n\n        # Keep the compute_loss method\n        loss = -torch.nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n        if prediction_loss_only:\n            return (loss, None, None)\n\n        return (loss, rewards_chosen, rewards_rejected)\n\n    def save_model(self, output_dir=None, _internal_call=False):\n        \"\"\"Save the LoRA model.\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n        self.model.save_pretrained(output_dir)\n\n\ndef save_model(model, tokenizer, args):\n    \"\"\"Save the model and the tokenizer.\"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Take care of distributed/parallel training\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n\nclass CastOutputToFloat(torch.nn.Sequential):\n    \"\"\"Cast the output of the model to float\"\"\"\n\n    def forward(self, x):\n        return super().forward(x).to(torch.float32)\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'score' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\ndef main():\n    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments, ScriptArguments))\n    model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses()\n\n    logger.info(f\"Model args: {model_args}\")\n    logger.info(f\"Data args: {data_args}\")\n    logger.info(f\"Training args: {training_args}\")\n    logger.info(f\"Script args: {script_args}\")\n    logger.info(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    # Load model\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n    if model_args.model_name_or_path:\n        torch_dtype = (\n            model_args.torch_dtype\n            if model_args.torch_dtype in [\"auto\", None]\n            else getattr(torch, model_args.torch_dtype)\n        )\n        world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n        if world_size > 1:\n            model_args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n        config = config_class.from_pretrained(\n            model_args.model_name_or_path,\n            num_labels=1,\n            torch_dtype=torch_dtype,\n            trust_remote_code=model_args.trust_remote_code,\n            cache_dir=model_args.cache_dir\n        )\n        if model_args.model_type in ['bloom', 'llama']:\n            model = model_class.from_pretrained(\n                model_args.model_name_or_path,\n                config=config,\n                torch_dtype=torch_dtype,\n                load_in_4bit=model_args.load_in_4bit,\n                load_in_8bit=model_args.load_in_8bit,\n                device_map=model_args.device_map,\n                trust_remote_code=model_args.trust_remote_code,\n            )\n        else:\n            model = model_class.from_pretrained(\n                model_args.model_name_or_path,\n                config=config,\n                cache_dir=model_args.cache_dir,\n                ignore_mismatched_sizes=True\n            )\n            model.to(training_args.device)\n    else:\n        raise ValueError(f\"Error, model_name_or_path is None, RM must be loaded from a pre-trained model\")\n\n    # Load tokenizer\n    if model_args.model_type == \"bloom\":\n        model_args.use_fast_tokenizer = True\n    tokenizer_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"use_fast\": model_args.use_fast_tokenizer,\n        \"trust_remote_code\": model_args.trust_remote_code,\n    }\n    tokenizer_name_or_path = model_args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = model_args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(script_args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    if script_args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n        if script_args.peft_path is not None:\n            logger.info(f\"Peft from pre-trained model: {script_args.peft_path}\")\n            model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)\n        else:\n            logger.info(\"Init new peft model\")\n            if model_args.load_in_8bit:\n                model = prepare_model_for_kbit_training(model)\n            target_modules = script_args.target_modules.split(',') if script_args.target_modules else None\n            if target_modules and 'all' in target_modules:\n                target_modules = find_all_linear_names(model, int4=False, int8=model_args.load_in_8bit)\n            modules_to_save = script_args.modules_to_save\n            if modules_to_save is not None:\n                modules_to_save = modules_to_save.split(',')\n            logger.info(f\"Peft target_modules: {target_modules}\")\n            logger.info(f\"Peft lora_rank: {script_args.lora_rank}\")\n            peft_config = LoraConfig(\n                task_type=TaskType.SEQ_CLS,\n                target_modules=target_modules,\n                inference_mode=False,\n                r=script_args.lora_rank,\n                lora_alpha=script_args.lora_alpha,\n                lora_dropout=script_args.lora_dropout,\n                modules_to_save=modules_to_save)\n            model = get_peft_model(model, peft_config)\n        for param in filter(lambda p: p.requires_grad, model.parameters()):\n            param.data = param.data.to(torch.float32)\n        model.print_trainable_parameters()\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n        print_trainable_parameters(model)\n\n    # Get reward dataset for tuning the reward model.\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            data_args.dataset_name,\n            data_args.dataset_config_name,\n            cache_dir=model_args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                data_args.dataset_name,\n                data_args.dataset_config_name,\n                split=f\"train[:{data_args.validation_split_percentage}%]\",\n                cache_dir=model_args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                data_args.dataset_name,\n                data_args.dataset_config_name,\n                split=f\"train[{data_args.validation_split_percentage}%:]\",\n                cache_dir=model_args.cache_dir,\n            )\n    else:\n        data_files = {}\n        if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n            train_data_files = glob(f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {', '.join(train_data_files)}\")\n            data_files[\"train\"] = train_data_files\n        if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n            eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=model_args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[:{data_args.validation_split_percentage}%]\",\n                cache_dir=model_args.cache_dir,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                'json',\n                data_files=data_files,\n                split=f\"train[{data_args.validation_split_percentage}%:]\",\n                cache_dir=model_args.cache_dir,\n            )\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    full_max_length = data_args.max_source_length + data_args.max_target_length\n\n    def preprocess_reward_function(examples):\n        \"\"\"\n        Turn the dataset into pairs of Question + Answer, where input_ids_chosen is the preferred question + answer\n            and text_rejected is the other.\n        \"\"\"\n        new_examples = {\n            \"input_ids_chosen\": [],\n            \"attention_mask_chosen\": [],\n            \"input_ids_rejected\": [],\n            \"attention_mask_rejected\": [],\n        }\n        for system, history, question, chosen, rejected in zip(\n                examples[\"system\"],\n                examples[\"history\"],\n                examples[\"question\"],\n                examples[\"response_chosen\"],\n                examples[\"response_rejected\"]\n        ):\n            system_prompt = system or \"\"\n            chosen_messages = history + [[question, chosen]] if history else [[question, chosen]]\n            chosen_prompt = prompt_template.get_prompt(messages=chosen_messages, system_prompt=system_prompt)\n            rejected_messages = history + [[question, rejected]] if history else [[question, rejected]]\n            rejected_prompt = prompt_template.get_prompt(messages=rejected_messages, system_prompt=system_prompt)\n\n            tokenized_chosen = tokenizer(chosen_prompt)\n            tokenized_rejected = tokenizer(rejected_prompt)\n\n            new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n            new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n            new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n            new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n        return new_examples\n\n    train_dataset = None\n    max_train_samples = 0\n    if training_args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train']\n        max_train_samples = len(train_dataset)\n        if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        with training_args.main_process_first(desc=\"Train dataset tokenization\"):\n            tokenized_dataset = train_dataset.shuffle().map(\n                preprocess_reward_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=train_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n            train_dataset = tokenized_dataset.filter(\n                lambda x: 0 < len(x['input_ids_rejected']) <= full_max_length and 0 < len(\n                    x['input_ids_chosen']) <= full_max_length\n            )\n            logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n            logger.debug(\"Tokenized training example:\")\n            logger.debug(tokenizer.decode(train_dataset[0]['input_ids_chosen']))\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if training_args.do_eval:\n        with training_args.main_process_first(desc=\"Eval dataset tokenization\"):\n            if \"validation\" not in raw_datasets:\n                raise ValueError(\"--do_eval requires a validation dataset\")\n            eval_dataset = raw_datasets[\"validation\"]\n            max_eval_samples = len(eval_dataset)\n            if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n                max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n                eval_dataset = eval_dataset.select(range(max_eval_samples))\n            logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n            tokenized_dataset = eval_dataset.map(\n                preprocess_reward_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=eval_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n            eval_dataset = tokenized_dataset.filter(\n                lambda x: 0 < len(x['input_ids_rejected']) <= full_max_length and 0 < len(\n                    x['input_ids_chosen']) <= full_max_length\n            )\n            logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n            logger.debug(\"Tokenized eval example:\")\n            logger.debug(tokenizer.decode(eval_dataset[0]['input_ids_chosen']))\n\n    # Initialize our Trainer\n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n    else:\n        model.config.use_cache = True\n    model.enable_input_require_grads()\n    if torch.cuda.device_count() > 1:\n        # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n        model.is_parallelizable = True\n        model.model_parallel = True\n    trainer = RewardTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n        data_collator=RewardDataCollatorWithPadding(\n            tokenizer=tokenizer, max_length=full_max_length, padding=\"max_length\"\n        ),\n    )\n\n    # Training\n    if training_args.do_train:\n        logger.info(\"*** Train ***\")\n        logger.debug(f\"Train dataloader example: {next(iter(trainer.get_train_dataloader()))}\")\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n        model.config.use_cache = True  # enable cache after training\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n            save_model(model, tokenizer, training_args)\n\n    # Evaluation\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n\n        metrics[\"eval_samples\"] = max_eval_samples\n        try:\n            perplexity = math.exp(metrics[\"eval_loss\"])\n        except OverflowError:\n            perplexity = float(\"inf\")\n        metrics[\"perplexity\"] = perplexity\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "role_play_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "run_dpo.sh",
          "type": "blob",
          "size": 0.85546875,
          "content": "CUDA_VISIBLE_DEVICES=0,1 python dpo_training.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --template_name qwen \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 20 \\\n    --save_steps 50 \\\n    --max_source_length 1024 \\\n    --max_target_length 512 \\\n    --output_dir outputs-dpo-qwen-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache\n"
        },
        {
          "name": "run_eval_quantize.sh",
          "type": "blob",
          "size": 0.1103515625,
          "content": "python eval_quantize.py --bnb_path /path/to/your/bnb_model --data_path data/finetune/medical_sft_1K_format.jsonl\n"
        },
        {
          "name": "run_full_sft.sh",
          "type": "blob",
          "size": 1.1572265625,
          "content": "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 torchrun --nproc_per_node 8 supersived_finetuning.py \\\n    --model_type auto \\\n    --model_name_or_path ./model/glm-4-9b-chat \\\n    --train_file_dir ./data/finetune \\\n    --validation_file_dir ./data/finetune \\\n    --cache_dir ./model \\\n    --device_map None \\\n    --use_peft False \\\n    --per_device_train_batch_size 2 \\\n    --do_train \\\n    --num_train_epochs 3 \\\n    --per_device_eval_batch_size 2 \\\n    --max_train_samples -1 \\\n    --learning_rate 3e-5 \\\n    --warmup_ratio 0.2 \\\n    --model_max_length 2048 \\\n    --weight_decay 0.01 \\\n    --logging_strategy steps \\\n    --logging_steps 1 \\\n    --save_steps 400 \\\n    --save_strategy steps \\\n    --save_total_limit 3 \\\n    --gradient_accumulation_steps 1 \\\n    --preprocessing_num_workers 128 \\\n    --output_dir GLM4-sft-med \\\n    --overwrite_output_dir \\\n    --ddp_timeout 30000 \\\n    --logging_first_step True \\\n    --target_modules all \\\n    --torch_dtype bfloat16 \\\n    --report_to tensorboard \\\n    --neft_alpha 8 \\\n    --ddp_find_unused_parameters False \\\n    --gradient_checkpointing True \\\n    --template_name chatglm3 \\\n    --deepspeed ./deepspeed_zero_stage2_config.json \\\n    --fp16\n"
        },
        {
          "name": "run_orpo.sh",
          "type": "blob",
          "size": 0.837890625,
          "content": "CUDA_VISIBLE_DEVICES=0,1 python orpo_training.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --template_name qwen \\\n    --train_file_dir ./data/reward \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 50 \\\n    --max_steps 100 \\\n    --eval_steps 20 \\\n    --save_steps 50 \\\n    --max_source_length 1024 \\\n    --max_target_length 512 \\\n    --output_dir outputs-orpo-qwen-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --orpo_beta 0.1 \\\n    --cache_dir ./cache\n"
        },
        {
          "name": "run_ppo.sh",
          "type": "blob",
          "size": 0.810546875,
          "content": "CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 ppo_training.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --reward_model_name_or_path OpenAssistant/reward-model-deberta-v3-large-v2 \\\n    --template_name qwen \\\n    --torch_dtype float16 \\\n    --device_map auto \\\n    --train_file_dir ./data/finetune \\\n    --validation_file_dir ./data/finetune \\\n    --batch_size 8 \\\n    --max_source_length 1024 \\\n    --max_target_length 256 \\\n    --max_train_samples 1000 \\\n    --use_peft True \\\n    --target_modules q_proj,v_proj \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --do_train \\\n    --max_steps 100 \\\n    --learning_rate 1e-5 \\\n    --save_steps 50 \\\n    --output_dir outputs-rl-qwen-v1 \\\n    --early_stopping True \\\n    --target_kl 0.1 \\\n    --reward_baseline 0.0\n"
        },
        {
          "name": "run_pt.sh",
          "type": "blob",
          "size": 1.24609375,
          "content": "CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 pretraining.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --train_file_dir ./data/pretrain \\\n    --validation_file_dir ./data/pretrain \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --seed 42 \\\n    --max_train_samples 10000 \\\n    --max_eval_samples 10 \\\n    --num_train_epochs 0.5 \\\n    --learning_rate 2e-4 \\\n    --warmup_ratio 0.05 \\\n    --weight_decay 0.01 \\\n    --logging_strategy steps \\\n    --logging_steps 10 \\\n    --eval_steps 50 \\\n    --evaluation_strategy steps \\\n    --save_steps 500 \\\n    --save_strategy steps \\\n    --save_total_limit 13 \\\n    --gradient_accumulation_steps 1 \\\n    --preprocessing_num_workers 10 \\\n    --block_size 512 \\\n    --group_by_length True \\\n    --output_dir outputs-pt-qwen-v1 \\\n    --overwrite_output_dir \\\n    --ddp_timeout 30000 \\\n    --logging_first_step True \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype bfloat16 \\\n    --bf16 \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --ddp_find_unused_parameters False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache\n"
        },
        {
          "name": "run_quant.sh",
          "type": "blob",
          "size": 0.166015625,
          "content": "python model_quant.py --unquantized_model_path /path/to/unquantized/model --quantized_model_output_path /path/to/save/quantized/model --input_text \"Your input text here\"\n"
        },
        {
          "name": "run_rm.sh",
          "type": "blob",
          "size": 1.138671875,
          "content": "CUDA_VISIBLE_DEVICES=0,1 python reward_modeling.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 4 \\\n    --do_train \\\n    --use_peft True \\\n    --seed 42 \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --num_train_epochs 1 \\\n    --learning_rate 2e-5 \\\n    --warmup_ratio 0.05 \\\n    --weight_decay 0.001 \\\n    --logging_strategy steps \\\n    --logging_steps 10 \\\n    --eval_steps 50 \\\n    --evaluation_strategy steps \\\n    --save_steps 500 \\\n    --save_strategy steps \\\n    --save_total_limit 3 \\\n    --max_source_length 1024 \\\n    --max_target_length 256 \\\n    --output_dir outputs-rm-qwen-v1 \\\n    --overwrite_output_dir \\\n    --ddp_timeout 30000 \\\n    --logging_first_step True \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float32 \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --ddp_find_unused_parameters False \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True\n"
        },
        {
          "name": "run_sft.sh",
          "type": "blob",
          "size": 1.2412109375,
          "content": "CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 supervised_finetuning.py \\\n    --model_type auto \\\n    --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n    --train_file_dir ./data/finetune \\\n    --validation_file_dir ./data/finetune \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 4 \\\n    --do_train \\\n    --do_eval \\\n    --template_name qwen \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --model_max_length 4096 \\\n    --num_train_epochs 1 \\\n    --learning_rate 2e-5 \\\n    --warmup_ratio 0.05 \\\n    --weight_decay 0.05 \\\n    --logging_strategy steps \\\n    --logging_steps 10 \\\n    --eval_steps 50 \\\n    --evaluation_strategy steps \\\n    --save_steps 500 \\\n    --save_strategy steps \\\n    --save_total_limit 13 \\\n    --gradient_accumulation_steps 1 \\\n    --preprocessing_num_workers 4 \\\n    --output_dir outputs-sft-qwen-v1 \\\n    --overwrite_output_dir \\\n    --ddp_timeout 30000 \\\n    --logging_first_step True \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --ddp_find_unused_parameters False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache\n"
        },
        {
          "name": "run_training_dpo_pipeline.ipynb",
          "type": "blob",
          "size": 19.1669921875,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Training Pipeline\\n\",\n    \"[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"# Stage 1: Continue Pretraining\\n\",\n    \"\\n\",\n    \"第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\\n\",\n    \"\\n\",\n    \"注意：\\n\",\n    \"1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\\n\",\n    \"2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\\n\",\n    \"\\n\",\n    \"| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### 说明：\\n\",\n    \"以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\\n\",\n    \"\\n\",\n    \"1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\\n\",\n    \"2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 配置运行环境\\n\",\n    \"\\n\",\n    \"本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\\n\",\n    \"\\n\",\n    \"colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\\n\",\n    \"\\n\",\n    \"步骤：\\n\",\n    \"1. 下载最新代码到本地\\n\",\n    \"2. 安装依赖包\\n\",\n    \"\\n\",\n    \"依赖包如下，保证最新版本：\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"loguru\\n\",\n    \"transformers\\n\",\n    \"sentencepiece\\n\",\n    \"datasets\\n\",\n    \"tensorboard\\n\",\n    \"tqdm\\n\",\n    \"peft\\n\",\n    \"trl\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\\n\",\n    \"%cd MedicalGPT\\n\",\n    \"%ls\\n\",\n    \"!pip install -r requirements.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Stage1 咱们开始吧\\n\",\n    \"\\n\",\n    \"训练步骤如下：\\n\",\n    \"\\n\",\n    \"1. 确认训练集\\n\",\n    \"2. 执行训练脚本\\n\",\n    \"\\n\",\n    \"训练脚本的执行逻辑如下：\\n\",\n    \"1. 导入依赖包\\n\",\n    \"2. 设置参数\\n\",\n    \"3. 定义各函数并加载训练集\\n\",\n    \"4. 加载模型和tokenizer\\n\",\n    \"5. 开始训练并评估\\n\",\n    \"6. 查看训练结果\\n\",\n    \"\\n\",\n    \"**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/pretrain/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python pretraining.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path bigscience/bloomz-560m \\\\\\n\",\n    \"    --train_file_dir ./data/pretrain \\\\\\n\",\n    \"    --validation_file_dir ./data/pretrain \\\\\\n\",\n    \"    --per_device_train_batch_size 3 \\\\\\n\",\n    \"    --per_device_eval_batch_size 3 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --seed 42 \\\\\\n\",\n    \"    --fp16 \\\\\\n\",\n    \"    --max_train_samples 20000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-4 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.01 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --gradient_accumulation_steps 1 \\\\\\n\",\n    \"    --preprocessing_num_workers 1 \\\\\\n\",\n    \"    --block_size 128 \\\\\\n\",\n    \"    --group_by_length True \\\\\\n\",\n    \"    --output_dir outputs-pt-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-pt-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"模型训练结果：\\n\",\n    \"- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\\n\",\n    \"- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下：\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-pt/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-pt/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Stage1 增量预训练完成。\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T13:56:17.081153Z\",\n     \"start_time\": \"2023-06-15T13:56:17.032821Z\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 2: Supervised FineTuning\\n\",\n    \"\\n\",\n    \"第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\\n\",\n    \"\\n\",\n    \"| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### 说明：\\n\",\n    \"以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\\n\",\n    \"\\n\",\n    \"1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage1得到的预训练模型\\n\",\n    \"2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage2 咱们开始吧\\n\",\n    \"\\n\",\n    \"训练步骤如下：\\n\",\n    \"\\n\",\n    \"1. 确认训练集\\n\",\n    \"2. 执行训练脚本\\n\",\n    \"\\n\",\n    \"训练脚本的执行逻辑如下：\\n\",\n    \"1. 导入依赖包\\n\",\n    \"2. 设置参数\\n\",\n    \"3. 定义各函数并加载训练集\\n\",\n    \"4. 加载模型和tokenizer\\n\",\n    \"5. 开始训练并评估\\n\",\n    \"6. 查看训练结果\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T13:58:38.966506Z\",\n     \"start_time\": \"2023-06-15T13:58:38.778132Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/finetune\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python supervised_finetuning.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path merged-pt \\\\\\n\",\n    \"    --train_file_dir ./data/finetune \\\\\\n\",\n    \"    --validation_file_dir ./data/finetune \\\\\\n\",\n    \"    --per_device_train_batch_size 4 \\\\\\n\",\n    \"    --per_device_eval_batch_size 4 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --fp16 \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-5 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.05 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --gradient_accumulation_steps 1 \\\\\\n\",\n    \"    --preprocessing_num_workers 1 \\\\\\n\",\n    \"    --output_dir outputs-sft-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-sft-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"模型训练结果：\\n\",\n    \"- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\\n\",\n    \"- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下：\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-sft/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-sft/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage2 SFT训练完成。\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T14:07:40.752635Z\",\n     \"start_time\": \"2023-06-15T14:07:40.731186Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 3: DPO(Direct Preference Optimization)\\n\",\n    \"\\n\",\n    \"第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\\n\",\n    \"\\n\",\n    \"| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### 说明：\\n\",\n    \"以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\\n\",\n    \"\\n\",\n    \"1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage2得到的SFT模型\\n\",\n    \"2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage3 咱们开始吧\\n\",\n    \"\\n\",\n    \"训练步骤如下：\\n\",\n    \"\\n\",\n    \"1. 确认训练集\\n\",\n    \"2. 执行训练脚本\\n\",\n    \"\\n\",\n    \"训练脚本的执行逻辑如下：\\n\",\n    \"1. 导入依赖包\\n\",\n    \"2. 设置参数\\n\",\n    \"3. 定义各函数并加载训练集\\n\",\n    \"4. 加载模型和tokenizer\\n\",\n    \"5. 开始训练并评估\\n\",\n    \"6. 查看训练结果\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/reward/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python dpo_training.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path ./merged-sft \\\\\\n\",\n    \"    --train_file_dir ./data/reward \\\\\\n\",\n    \"    --validation_file_dir ./data/reward \\\\\\n\",\n    \"    --per_device_train_batch_size 3 \\\\\\n\",\n    \"    --per_device_eval_batch_size 1 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --max_eval_samples 500 \\\\\\n\",\n    \"    --max_steps 100 \\\\\\n\",\n    \"    --eval_steps 10 \\\\\\n\",\n    \"    --save_steps 50 \\\\\\n\",\n    \"    --max_source_length 256 \\\\\\n\",\n    \"    --max_target_length 256 \\\\\\n\",\n    \"    --output_dir outputs-dpo-v1 \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --fp16 True \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --remove_unused_columns False \\\\\\n\",\n    \"    --gradient_checkpointing True \\\\\\n\",\n    \"    --cache_dir ./cache\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-dpo-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"模型训练结果：\\n\",\n    \"- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\\n\",\n    \"- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下：\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-dpo/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-dpo/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage3 偏好建模第一次训练完成。\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"**至此一个完整的训练流程演示完成。**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-26T12:34:29.658428Z\",\n     \"start_time\": \"2023-06-26T12:34:29.620609Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Test\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-26T12:35:00.864463Z\",\n     \"start_time\": \"2023-06-26T12:34:47.802087Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python inference.py --model_type bloom --base_model merged-dpo\\n\",\n    \"# 或在shell中运行\\n\",\n    \"# python inference.py --model_type bloom --base_model merged-dpo --interactive\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Input:介绍下南京\\n\",\n    \"Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\\n\",\n    \"\\n\",\n    \"完。\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.13\"\n  },\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "run_training_ppo_pipeline.ipynb",
          "type": "blob",
          "size": 25.34765625,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Training Pipeline\\n\",\n    \"[run_training_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"# Stage 1: Continue Pretraining\\n\",\n    \"\\n\",\n    \"第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\\n\",\n    \"\\n\",\n    \"注意：\\n\",\n    \"1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\\n\",\n    \"2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\\n\",\n    \"\\n\",\n    \"| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### 说明：\\n\",\n    \"以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\\n\",\n    \"\\n\",\n    \"1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\\n\",\n    \"2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 配置运行环境\\n\",\n    \"\\n\",\n    \"本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\\n\",\n    \"\\n\",\n    \"colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\\n\",\n    \"\\n\",\n    \"步骤：\\n\",\n    \"1. 下载最新代码到本地\\n\",\n    \"2. 安装依赖包\\n\",\n    \"\\n\",\n    \"依赖包如下，保证最新版本：\\n\",\n    \"\\n\",\n    \"```\\n\",\n    \"loguru\\n\",\n    \"transformers\\n\",\n    \"sentencepiece\\n\",\n    \"datasets\\n\",\n    \"tensorboard\\n\",\n    \"tqdm\\n\",\n    \"peft\\n\",\n    \"trl\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\\n\",\n    \"%cd MedicalGPT\\n\",\n    \"%ls\\n\",\n    \"!pip install -r requirements.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Stage1 咱们开始吧\\n\",\n    \"\\n\",\n    \"训练步骤如下：\\n\",\n    \"\\n\",\n    \"1. 确认训练集\\n\",\n    \"2. 执行训练脚本\\n\",\n    \"\\n\",\n    \"训练脚本的执行逻辑如下：\\n\",\n    \"1. 导入依赖包\\n\",\n    \"2. 设置参数\\n\",\n    \"3. 定义各函数并加载训练集\\n\",\n    \"4. 加载模型和tokenizer\\n\",\n    \"5. 开始训练并评估\\n\",\n    \"6. 查看训练结果\\n\",\n    \"\\n\",\n    \"**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/pretrain/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python pretraining.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path bigscience/bloomz-560m \\\\\\n\",\n    \"    --train_file_dir ./data/pretrain \\\\\\n\",\n    \"    --validation_file_dir ./data/pretrain \\\\\\n\",\n    \"    --per_device_train_batch_size 3 \\\\\\n\",\n    \"    --per_device_eval_batch_size 3 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --seed 42 \\\\\\n\",\n    \"    --fp16 \\\\\\n\",\n    \"    --max_train_samples 20000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-4 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.01 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --gradient_accumulation_steps 1 \\\\\\n\",\n    \"    --preprocessing_num_workers 1 \\\\\\n\",\n    \"    --block_size 128 \\\\\\n\",\n    \"    --group_by_length True \\\\\\n\",\n    \"    --output_dir outputs-pt-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-pt-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"模型训练结果：\\n\",\n    \"- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\\n\",\n    \"- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下：\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-pt/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-pt/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Stage1 增量预训练完成。\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T13:56:17.081153Z\",\n     \"start_time\": \"2023-06-15T13:56:17.032821Z\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 2: Supervised FineTuning\\n\",\n    \"\\n\",\n    \"第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\\n\",\n    \"\\n\",\n    \"| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### 说明：\\n\",\n    \"以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\\n\",\n    \"\\n\",\n    \"1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage1得到的预训练模型\\n\",\n    \"2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage2 咱们开始吧\\n\",\n    \"\\n\",\n    \"训练步骤如下：\\n\",\n    \"\\n\",\n    \"1. 确认训练集\\n\",\n    \"2. 执行训练脚本\\n\",\n    \"\\n\",\n    \"训练脚本的执行逻辑如下：\\n\",\n    \"1. 导入依赖包\\n\",\n    \"2. 设置参数\\n\",\n    \"3. 定义各函数并加载训练集\\n\",\n    \"4. 加载模型和tokenizer\\n\",\n    \"5. 开始训练并评估\\n\",\n    \"6. 查看训练结果\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T13:58:38.966506Z\",\n     \"start_time\": \"2023-06-15T13:58:38.778132Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/finetune\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python supervised_finetuning.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path merged-pt \\\\\\n\",\n    \"    --train_file_dir ./data/finetune \\\\\\n\",\n    \"    --validation_file_dir ./data/finetune \\\\\\n\",\n    \"    --per_device_train_batch_size 4 \\\\\\n\",\n    \"    --per_device_eval_batch_size 4 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --do_eval \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --fp16 \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-5 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.05 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --gradient_accumulation_steps 1 \\\\\\n\",\n    \"    --preprocessing_num_workers 1 \\\\\\n\",\n    \"    --output_dir outputs-sft-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-sft-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"模型训练结果：\\n\",\n    \"- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\\n\",\n    \"- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下：\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir merged-sft/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-sft/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-sft/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage2 SFT训练完成。\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T14:07:40.752635Z\",\n     \"start_time\": \"2023-06-15T14:07:40.731186Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 3: Reward Modeling\\n\",\n    \"\\n\",\n    \"第三阶段：RM(Reward Model)奖励模型建模，构造人类偏好排序数据集，训练奖励模型，用来对齐人类偏好，主要是\\\"HHH\\\"原则，具体是\\\"helpful, honest, harmless\\\"\\n\",\n    \"\\n\",\n    \"| Stage 3: Reward Modeling        |  [reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/reward_modeling.py) | [run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rm.sh)    |\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### 说明：\\n\",\n    \"以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\\n\",\n    \"\\n\",\n    \"1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage2得到的SFT模型\\n\",\n    \"2. 数据集：RM阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage3 咱们开始吧\\n\",\n    \"\\n\",\n    \"训练步骤如下：\\n\",\n    \"\\n\",\n    \"1. 确认训练集\\n\",\n    \"2. 执行训练脚本\\n\",\n    \"\\n\",\n    \"训练脚本的执行逻辑如下：\\n\",\n    \"1. 导入依赖包\\n\",\n    \"2. 设置参数\\n\",\n    \"3. 定义各函数并加载训练集\\n\",\n    \"4. 加载模型和tokenizer\\n\",\n    \"5. 开始训练并评估\\n\",\n    \"6. 查看训练结果\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/reward/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python reward_modeling.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path merged-sft \\\\\\n\",\n    \"    --train_file_dir ./data/reward \\\\\\n\",\n    \"    --validation_file_dir ./data/reward \\\\\\n\",\n    \"    --per_device_train_batch_size 3 \\\\\\n\",\n    \"    --per_device_eval_batch_size 1 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --seed 42 \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --max_eval_samples 10 \\\\\\n\",\n    \"    --num_train_epochs 1 \\\\\\n\",\n    \"    --learning_rate 2e-5 \\\\\\n\",\n    \"    --warmup_ratio 0.05 \\\\\\n\",\n    \"    --weight_decay 0.001 \\\\\\n\",\n    \"    --logging_strategy steps \\\\\\n\",\n    \"    --logging_steps 10 \\\\\\n\",\n    \"    --eval_steps 50 \\\\\\n\",\n    \"    --evaluation_strategy steps \\\\\\n\",\n    \"    --save_steps 500 \\\\\\n\",\n    \"    --save_strategy steps \\\\\\n\",\n    \"    --save_total_limit 3 \\\\\\n\",\n    \"    --max_source_length 256 \\\\\\n\",\n    \"    --max_target_length 256 \\\\\\n\",\n    \"    --output_dir outputs-rm-v1 \\\\\\n\",\n    \"    --overwrite_output_dir \\\\\\n\",\n    \"    --ddp_timeout 30000 \\\\\\n\",\n    \"    --logging_first_step True \\\\\\n\",\n    \"    --target_modules all \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --torch_dtype float32 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --report_to tensorboard \\\\\\n\",\n    \"    --ddp_find_unused_parameters False \\\\\\n\",\n    \"    --remove_unused_columns False \\\\\\n\",\n    \"    --gradient_checkpointing True\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-rm-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"模型训练结果：\\n\",\n    \"- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\\n\",\n    \"- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下：\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-sft --lora_model outputs-rm-v1 --output_dir merged-rm/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-rm/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-rm/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage3 奖励建模第一次训练完成。\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-15T14:12:09.472414Z\",\n     \"start_time\": \"2023-06-15T14:12:09.464881Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Stage 4: Reinforcement Learning Training\\n\",\n    \"\\n\",\n    \"第四阶段：RL(Reinforcement Learning)基于人类反馈的强化学习(RLHF)，用奖励模型来训练SFT模型，生成模型使用奖励或惩罚来更新其策略，以便生成更高质量、更符合人类偏好的文本\\n\",\n    \"\\n\",\n    \"| Stage 4: Reinforcement Learning |  [rl_training.py](https://github.com/shibing624/MedicalGPT/blob/main/rl_training.py) | [run_rl.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rl.sh)    |\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"#### 说明：\\n\",\n    \"以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型、奖励模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\\n\",\n    \"\\n\",\n    \"1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage2得到的SFT模型\\n\",\n    \"2. 奖励模型：使用的是`OpenAssistant/reward-model-deberta-v3-large-v2` 或者 Stage3得到的BERT类或者GPT类奖励模型\\n\",\n    \"3. 数据集：RL阶段的数据可以复用SFT的数据集，使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"## Stage4 咱们开始吧\\n\",\n    \"\\n\",\n    \"训练步骤如下：\\n\",\n    \"\\n\",\n    \"1. 确认训练集\\n\",\n    \"2. 执行训练脚本\\n\",\n    \"\\n\",\n    \"训练脚本的执行逻辑如下：\\n\",\n    \"1. 导入依赖包\\n\",\n    \"2. 设置参数\\n\",\n    \"3. 定义各函数并加载训练集\\n\",\n    \"4. 加载生成模型和tokenizer，加载奖励模型和其tokenizer\\n\",\n    \"5. 开始训练并评估\\n\",\n    \"6. 查看训练结果\\n\",\n    \"\\n\",\n    \"以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的。\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls ./data/finetune/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"! CUDA_VISIBLE_DEVICES=0 python ppo_training.py \\\\\\n\",\n    \"    --model_type bloom \\\\\\n\",\n    \"    --model_name_or_path ./merged-sft \\\\\\n\",\n    \"    --reward_model_name_or_path ./merged-rm \\\\\\n\",\n    \"    --torch_dtype float16 \\\\\\n\",\n    \"    --device_map auto \\\\\\n\",\n    \"    --train_file_dir ./data/finetune \\\\\\n\",\n    \"    --validation_file_dir ./data/finetune \\\\\\n\",\n    \"    --batch_size 4 \\\\\\n\",\n    \"    --max_source_length 256 \\\\\\n\",\n    \"    --max_target_length 256 \\\\\\n\",\n    \"    --max_train_samples 1000 \\\\\\n\",\n    \"    --use_peft True \\\\\\n\",\n    \"    --lora_rank 8 \\\\\\n\",\n    \"    --lora_alpha 16 \\\\\\n\",\n    \"    --lora_dropout 0.05 \\\\\\n\",\n    \"    --do_train \\\\\\n\",\n    \"    --max_steps 64 \\\\\\n\",\n    \"    --learning_rate 1e-5 \\\\\\n\",\n    \"    --save_steps 50 \\\\\\n\",\n    \"    --output_dir outputs-rl-v1 \\\\\\n\",\n    \"    --early_stopping True \\\\\\n\",\n    \"    --target_kl 0.1 \\\\\\n\",\n    \"    --reward_baseline 0.0\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh outputs-rl-v1\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"模型训练结果：\\n\",\n    \"- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\\n\",\n    \"- 日志保存在`output_dir/trl`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/trl --host 0.0.0.0 --port 8009`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下：\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python merge_peft_adapter.py --model_type bloom \\\\\\n\",\n    \"    --base_model merged-sft --lora_model outputs-rl-v1 --output_dir merged-ppo/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%ls -lh merged-ppo/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%cat merged-ppo/config.json\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Stage4 RL第一次训练完成。\\n\",\n    \"\\n\",\n    \"**至此一个完整的4阶段训练流程演示完成。**\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"实际操作中Stage3和Stage4可以反复多次，直到RL得到的最后模型满足评估要求。\\n\",\n    \"\\n\",\n    \"RLHF过程可以把SFT模型当成一个初始化模型，RM模型当做指导老师，使用RL(PPO)调教SFT模型生成指导老师最满意的结果，如果小学老师满意了，我们就再训练一个中学老师，继续指导，中学老师满意了，就训练一个大学老师，这样不断迭代，使得生成模型的质量达到甚至超过人工撰写的天花板。\\n\",\n    \"\\n\",\n    \"RLHF训练不易，此项目提供给大家一种实现的方法和参考，希望抛砖引玉，共同促进中文开源LLM发展。\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-26T12:34:29.658428Z\",\n     \"start_time\": \"2023-06-26T12:34:29.620609Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"# Test\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"ExecuteTime\": {\n     \"end_time\": \"2023-06-26T12:35:00.864463Z\",\n     \"start_time\": \"2023-06-26T12:34:47.802087Z\"\n    },\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!python inference.py --model_type bloom --base_model merged-ppo\\n\",\n    \"# 或在shell中运行\\n\",\n    \"# !python inference.py --model_type bloom --base_model merged-ppo --interactive\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"source\": [\n    \"Input:介绍下南京\\n\",\n    \"Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\\n\",\n    \"\\n\",\n    \"完。\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.13\"\n  },\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "supervised_finetuning.py",
          "type": "blob",
          "size": 44.24609375,
          "content": "# -*- coding: utf-8 -*-\n# Copyright 2023 XuMing(xuming624@qq.com) and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for causal language modeling (GPT, LLaMA, Bloom, ...) on a json file or a dataset.\n\npart of code is modified from https://github.com/shibing624/textgen\n\"\"\"\n\nimport math\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom types import MethodType\nfrom typing import Literal, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom loguru import logger\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoConfig,\n    BloomForCausalLM,\n    AutoModel,\n    AutoModelForCausalLM,\n    LlamaForCausalLM,\n    BloomTokenizerFast,\n    AutoTokenizer,\n    HfArgumentParser,\n    Trainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n    BitsAndBytesConfig,\n    DataCollatorForSeq2Seq,\n)\nfrom transformers.models.llama.modeling_llama import (\n    LlamaAttention,\n    apply_rotary_pos_emb,\n    repeat_kv,\n    LlamaFlashAttention2,\n    Cache\n)\nfrom transformers.trainer import TRAINING_ARGS_NAME\nfrom transformers.trainer_pt_utils import LabelSmoother\nfrom transformers.utils.versions import require_version\n\ntry:\n    from transformers.integrations import is_deepspeed_zero3_enabled\nexcept ImportError:  # https://github.com/huggingface/transformers/releases/tag/v4.33.1\n    from transformers.deepspeed import is_deepspeed_zero3_enabled\n\nis_flash_attn_2_available = False\ntry:\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import pad_input, unpad_input\n\n    is_flash_attn_2_available = True\nexcept ImportError:\n    is_flash_attn_2_available = False\n\nfrom template import get_conv_template\n\nMODEL_CLASSES = {\n    \"bloom\": (AutoConfig, BloomForCausalLM, BloomTokenizerFast),\n    \"chatglm\": (AutoConfig, AutoModel, AutoTokenizer),\n    \"llama\": (AutoConfig, LlamaForCausalLM, AutoTokenizer),\n    \"baichuan\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n    \"auto\": (AutoConfig, AutoModelForCausalLM, AutoTokenizer),\n}\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    model_type: str = field(\n        default=None,\n        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n    )\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    model_revision: Optional[str] = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    hf_hub_token: Optional[str] = field(default=None, metadata={\"help\": \"Auth token to log in with Hugging Face Hub.\"})\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    torch_dtype: Optional[str] = field(\n        default=\"float16\",\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    device_map: Optional[str] = field(\n        default=\"auto\",\n        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n    )\n    trust_remote_code: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n    )\n    rope_scaling: Optional[Literal[\"linear\", \"dynamic\"]] = field(\n        default=None,\n        metadata={\"help\": \"Adopt scaled rotary positional embeddings.\"}\n    )\n    flash_attn: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enable FlashAttention-2 for faster training.\"}\n    )\n    shift_attn: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enable shifted sparse attention (S^2-Attn) proposed by LongLoRA.\"}\n    )\n    neft_alpha: Optional[float] = field(\n        default=0,\n        metadata={\"help\": \"The alpha parameter to control the noise magnitude in NEFTune. value can be 5.\"}\n    )\n\n    def __post_init__(self):\n        if self.model_type is None:\n            raise ValueError(\n                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n                    MODEL_CLASSES.keys()))\n        if self.model_name_or_path is None:\n            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n\n\n@dataclass\nclass DataArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The train jsonl data file folder.\"})\n    validation_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The evaluation jsonl file folder.\"})\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    ignore_pad_token_for_loss: bool = field(\n        default=True,\n        metadata={\"help\": \"If only pad tokens should be ignored. This assumes that `config.pad_token_id` is defined.\"},\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n\n    def __post_init__(self):\n        if self.max_train_samples is not None and 0 < self.max_train_samples <= 1000:\n            logger.warning(\"You may set max_train_samples = -1 to run all samples in production.\")\n\n\n@dataclass\nclass ScriptArguments:\n    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n    train_on_inputs: bool = field(default=False, metadata={\"help\": \"Whether to train on inputs\"})\n    target_modules: Optional[str] = field(default=\"all\")\n    lora_rank: Optional[int] = field(default=8)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_alpha: Optional[float] = field(default=32.0)\n    modules_to_save: Optional[str] = field(default=None)\n    peft_path: Optional[str] = field(default=None, metadata={\"help\": \"The path to the peft model\"})\n    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})\n    model_max_length: int = field(\n        default=512,\n        metadata={\"help\": \"Maximum model context length. suggest: 8192 * 4, 8192 * 2, 8192, 4096, 2048, 1024, 512\"}\n    )\n    template_name: Optional[str] = field(default=\"vicuna\", metadata={\"help\": \"The prompt template name.\"})\n\n    def __post_init__(self):\n        if self.model_max_length < 60:\n            raise ValueError(\"You must specify a valid model_max_length >= 60 to run training\")\n\n\nclass SavePeftModelTrainer(Trainer):\n    \"\"\"\n    Trainer for lora models\n    \"\"\"\n\n    def save_model(self, output_dir=None, _internal_call=False):\n        \"\"\"Save the LoRA model.\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n        self.model.save_pretrained(output_dir)\n\n\ndef save_model(model, tokenizer, args):\n    \"\"\"Save the model and the tokenizer.\"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Take care of distributed/parallel training\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef save_model_zero3(model, tokenizer, args, trainer):\n    \"\"\"Save the model for deepspeed zero3.\n    refer https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train_lora.py#L209\n    \"\"\"\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n    state_dict_zero3 = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    model_to_save.save_pretrained(args.output_dir, state_dict=state_dict_zero3)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef find_all_linear_names(peft_model, int4=False, int8=False):\n    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n    cls = torch.nn.Linear\n    if int4 or int8:\n        import bitsandbytes as bnb\n        if int4:\n            cls = bnb.nn.Linear4bit\n        elif int8:\n            cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in peft_model.named_modules():\n        if isinstance(module, cls):\n            # last layer is not add to lora_module_names\n            if 'lm_head' in name:\n                continue\n            if 'output_layer' in name:\n                continue\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return sorted(lora_module_names)\n\n\n# Modified from: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\ndef llama_torch_attn_forward(\n        self: \"LlamaAttention\",\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[\"Cache\"] = None,\n        output_attentions: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states)\n    key_states = self.k_proj(hidden_states)\n    value_states = self.v_proj(hidden_states)\n\n    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n    past_key_value = getattr(self, \"past_key_value\", past_key_value)\n    cos, sin = self.rotary_emb(value_states, position_ids)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n    if past_key_value is not None:\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    if getattr(self.config, \"group_size_ratio\", None) and self.training:  # shift\n        groupsz = int(q_len * getattr(self.config, \"group_size_ratio\"))\n        assert q_len % groupsz == 0, \"q_len {} should be divisible by group size {}.\".format(q_len, groupsz)\n        num_groups = q_len // groupsz\n\n        def shift(state: torch.Tensor) -> torch.Tensor:\n            state = state.transpose(1, 2)  # output: (bsz, seq_len, n_heads, head_dim)\n            state = torch.cat(\n                (state[:, :, : self.num_heads // 2], state[:, :, self.num_heads // 2:].roll(-groupsz // 2, dims=1)),\n                dim=2,\n            )\n            return state.reshape(bsz * num_groups, groupsz, self.num_heads, self.head_dim).transpose(1, 2)\n\n        query_states, key_states, value_states = shift(query_states), shift(key_states), shift(value_states)\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, :, :groupsz, :groupsz].repeat(num_groups, 1, 1, 1)\n\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n    if attention_mask is not None:\n        attn_weights = attn_weights + attention_mask\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n    attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n    attn_output = torch.matmul(attn_weights, value_states)  # (bsz, :, seq_len, :) or (bsz*n_group, :, groupsz, :)\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    if getattr(self.config, \"group_size_ratio\", None) and self.training:  # shift back\n        attn_output.reshape(bsz, q_len, self.num_heads, self.head_dim)\n        attn_output = torch.cat(\n            (\n                attn_output[:, :, : self.num_heads // 2],\n                attn_output[:, :, self.num_heads // 2:].roll(groupsz // 2, dims=1),\n            )\n        )\n\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\n# Modified from: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\ndef llama_flash_attn_forward(\n        self: \"LlamaFlashAttention2\",\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[\"Cache\"] = None,\n        output_attentions: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    # LlamaFlashAttention2 attention does not support output_attentions\n    output_attentions = False\n\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states)\n    key_states = self.k_proj(hidden_states)\n    value_states = self.v_proj(hidden_states)\n\n    # FlashAttention requires the input to have the shape (bsz, seq_len, n_heads, head_dim)\n    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n    cos, sin = self.rotary_emb(value_states, position_ids)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n    past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n    if past_key_value is not None:\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    query_states = query_states.transpose(1, 2)  # (bsz, seq_len, n_heads, head_dim)\n    key_states = key_states.transpose(1, 2)  # (bsz, seq_len, n_heads, head_dim)\n    value_states = value_states.transpose(1, 2)  # (bsz, seq_len, n_heads, head_dim)\n\n    dropout_rate = self.attention_dropout if self.training else 0.0\n\n    input_dtype = query_states.dtype\n    if input_dtype == torch.float32:\n        if torch.is_autocast_enabled():\n            target_dtype = torch.get_autocast_gpu_dtype()\n        elif hasattr(self.config, \"_pre_quantization_dtype\"):\n            target_dtype = self.config._pre_quantization_dtype\n        else:\n            target_dtype = self.q_proj.weight.dtype\n\n        logger.warning(\"The input hidden states seems to be silently casted in float32.\")\n        query_states = query_states.to(target_dtype)\n        key_states = key_states.to(target_dtype)\n        value_states = value_states.to(target_dtype)\n\n    if getattr(self.config, \"group_size_ratio\", None) and self.training:  # shift\n        groupsz = int(q_len * getattr(self.config, \"group_size_ratio\"))\n        assert q_len % groupsz == 0, \"q_len {} should be divisible by group size {}.\".format(q_len, groupsz)\n        num_groups = q_len // groupsz\n\n        def shift(state: torch.Tensor) -> torch.Tensor:\n            state = torch.cat(\n                (state[:, :, : self.num_heads // 2], state[:, :, self.num_heads // 2:].roll(-groupsz // 2, dims=1)),\n                dim=2,\n            )\n            return state.reshape(bsz * num_groups, groupsz, self.num_heads, self.head_dim)\n\n        query_states, key_states, value_states = shift(query_states), shift(key_states), shift(value_states)\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, :, :groupsz, :groupsz].repeat(num_groups, 1, 1, 1)\n\n    attn_output: torch.Tensor = self._flash_attention_forward(\n        query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate\n    )\n\n    if getattr(self.config, \"group_size_ratio\", None) and self.training:  # shift back\n        attn_output.reshape(bsz, q_len, self.num_heads, self.head_dim)\n        attn_output = torch.cat(\n            (\n                attn_output[:, :, : self.num_heads // 2],\n                attn_output[:, :, self.num_heads // 2:].roll(groupsz // 2, dims=1),\n            )\n        )\n\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef apply_llama_patch() -> None:\n    LlamaAttention.forward = llama_torch_attn_forward\n    LlamaFlashAttention2.forward = llama_flash_attn_forward\n\n\ndef main():\n    parser = HfArgumentParser((ModelArguments, DataArguments, Seq2SeqTrainingArguments, ScriptArguments))\n    model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses()\n\n    logger.info(f\"Model args: {model_args}\")\n    logger.info(f\"Data args: {data_args}\")\n    logger.info(f\"Training args: {training_args}\")\n    logger.info(f\"Script args: {script_args}\")\n    logger.info(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n    # Load tokenizer\n    tokenizer_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"use_fast\": model_args.use_fast_tokenizer,\n        \"trust_remote_code\": model_args.trust_remote_code,\n    }\n    tokenizer_name_or_path = model_args.tokenizer_name_or_path\n    if not tokenizer_name_or_path:\n        tokenizer_name_or_path = model_args.model_name_or_path\n    tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n    prompt_template = get_conv_template(script_args.template_name)\n    if tokenizer.eos_token_id is None:\n        tokenizer.eos_token = prompt_template.stop_str  # eos token is required\n        tokenizer.add_special_tokens({\"eos_token\": tokenizer.eos_token})\n        logger.info(f\"Add eos_token: {tokenizer.eos_token}, eos_token_id: {tokenizer.eos_token_id}\")\n    if tokenizer.bos_token_id is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n        tokenizer.bos_token_id = tokenizer.eos_token_id\n        logger.info(f\"Add bos_token: {tokenizer.bos_token}, bos_token_id: {tokenizer.bos_token_id}\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.unk_token_id is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n        logger.info(f\"Add pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n    logger.debug(f\"Tokenizer: {tokenizer}\")\n\n    IGNORE_INDEX = LabelSmoother.ignore_index if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n\n    # Get datasets\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(\n            data_args.dataset_name,\n            data_args.dataset_config_name,\n            cache_dir=model_args.cache_dir,\n        )\n        if \"validation\" not in raw_datasets.keys():\n            shuffled_train_dataset = raw_datasets[\"train\"].shuffle(seed=42)\n            # Split the shuffled train dataset into training and validation sets\n            split = shuffled_train_dataset.train_test_split(\n                test_size=data_args.validation_split_percentage / 100,\n                seed=42\n            )\n            # Assign the split datasets back to raw_datasets\n            raw_datasets[\"train\"] = split[\"train\"]\n            raw_datasets[\"validation\"] = split[\"test\"]\n    else:\n        # Loading a dataset from local files.\n        data_files = {}\n        if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n            train_data_files = glob(f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"train files: {train_data_files}\")\n            data_files[\"train\"] = train_data_files\n        if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n            eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(\n                f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)\n            logger.info(f\"eval files: {eval_data_files}\")\n            data_files[\"validation\"] = eval_data_files\n        raw_datasets = load_dataset(\n            'json',\n            data_files=data_files,\n            cache_dir=model_args.cache_dir,\n        )\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            shuffled_train_dataset = raw_datasets[\"train\"].shuffle(seed=42)\n            split = shuffled_train_dataset.train_test_split(\n                test_size=float(data_args.validation_split_percentage / 100),\n                seed=42\n            )\n            raw_datasets[\"train\"] = split[\"train\"]\n            raw_datasets[\"validation\"] = split[\"test\"]\n    logger.info(f\"Raw datasets: {raw_datasets}\")\n\n    # Preprocessing the datasets\n    max_length = script_args.model_max_length\n\n    def preprocess_function(examples):\n        \"\"\"\n        Preprocessing the datasets.\n            part of code modified from https://github.com/lm-sys/FastChat\n        \"\"\"\n        input_ids_list = []\n        attention_mask_list = []\n        targets_list = []\n        roles = [\"human\", \"gpt\"]\n\n        def get_dialog(examples):\n            system_prompts = examples.get(\"system_prompt\", \"\")\n            for i, source in enumerate(examples['conversations']):\n                system_prompt = \"\"\n                if len(source) < 2:\n                    continue\n                data_role = source[0].get(\"from\", \"\")\n                if data_role == \"system\":\n                    # Skip the first one if it is from system\n                    system_prompt = source[0][\"value\"]\n                    source = source[1:]\n                    data_role = source[0].get(\"from\", \"\")\n                if data_role not in roles or data_role != roles[0]:\n                    # Skip the first one if it is not from human\n                    source = source[1:]\n                if len(source) < 2:\n                    continue\n                messages = []\n                for j, sentence in enumerate(source):\n                    data_role = sentence.get(\"from\", \"\")\n                    if data_role not in roles:\n                        logger.warning(f\"unknown role: {data_role}, {i}. (ignored)\")\n                        break\n                    if data_role == roles[j % 2]:\n                        messages.append(sentence[\"value\"])\n                if len(messages) % 2 != 0:\n                    continue\n                # Convert the list to pairs of elements\n                history_messages = [[messages[k], messages[k + 1]] for k in range(0, len(messages), 2)]\n                if not system_prompt:\n                    system_prompt = system_prompts[i] if system_prompts else \"\"\n                yield prompt_template.get_dialog(history_messages, system_prompt=system_prompt)\n\n        for dialog in get_dialog(examples):\n            input_ids, labels = [], []\n\n            for i in range(len(dialog) // 2):\n                source_ids = tokenizer.encode(text=dialog[2 * i], add_special_tokens=(i == 0))\n                target_ids = tokenizer.encode(text=dialog[2 * i + 1], add_special_tokens=False)\n\n                total_len = len(source_ids) + len(target_ids)\n                max_source_len = int(max_length * (len(source_ids) / total_len))\n                max_target_len = int(max_length * (len(target_ids) / total_len))\n\n                if len(source_ids) > max_source_len:\n                    source_ids = source_ids[:max_source_len]\n                if len(target_ids) > max_target_len - 1:  # eos token\n                    target_ids = target_ids[:max_target_len - 1]\n                if len(source_ids) > 0 and source_ids[0] == tokenizer.eos_token_id:\n                    source_ids = source_ids[1:]\n                if len(target_ids) > 0 and target_ids[-1] == tokenizer.eos_token_id:\n                    target_ids = target_ids[:-1]\n                if len(input_ids) + len(source_ids) + len(target_ids) + 1 > max_length:\n                    break\n\n                input_ids += source_ids + target_ids + [tokenizer.eos_token_id]  # add eos token for each turn\n                if script_args.train_on_inputs:\n                    labels += source_ids + target_ids + [tokenizer.eos_token_id]\n                else:\n                    labels += [IGNORE_INDEX] * len(source_ids) + target_ids + [tokenizer.eos_token_id]\n\n            input_ids_list.append(input_ids)\n            attention_mask_list.append([1] * len(input_ids))\n            targets_list.append(labels)\n\n        return dict(\n            input_ids=input_ids_list,\n            attention_mask=attention_mask_list,\n            labels=targets_list,\n        )\n\n    def filter_empty_labels(example):\n        \"\"\"Remove empty labels dataset.\"\"\"\n        return not all(label == IGNORE_INDEX for label in example[\"labels\"])\n\n    train_dataset = None\n    max_train_samples = 0\n    if training_args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets['train'].shuffle(seed=42)\n        max_train_samples = len(train_dataset)\n        if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n            train_dataset = train_dataset.select(range(max_train_samples))\n        logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n        with training_args.main_process_first(desc=\"Train dataset tokenization\"):\n            train_dataset = train_dataset.shuffle().map(\n                preprocess_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=train_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on train dataset\",\n            )\n            train_dataset = train_dataset.filter(filter_empty_labels, num_proc=data_args.preprocessing_num_workers)\n            logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n            logger.debug(\"Tokenized training example:\")\n            logger.debug(f\"Decode input_ids[0]:\\n{tokenizer.decode(train_dataset[0]['input_ids'])}\")\n            replaced_labels = [label if label != IGNORE_INDEX else tokenizer.pad_token_id\n                               for label in list(train_dataset[0]['labels'])]\n            logger.debug(f\"Decode labels[0]:\\n{tokenizer.decode(replaced_labels)}\")\n\n    eval_dataset = None\n    max_eval_samples = 0\n    if training_args.do_eval:\n        with training_args.main_process_first(desc=\"Eval dataset tokenization\"):\n            if \"validation\" not in raw_datasets:\n                raise ValueError(\"--do_eval requires a validation dataset\")\n            eval_dataset = raw_datasets[\"validation\"]\n            max_eval_samples = len(eval_dataset)\n            if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n                max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n                eval_dataset = eval_dataset.select(range(max_eval_samples))\n            eval_size = len(eval_dataset)\n            logger.debug(f\"Num eval_samples: {eval_size}\")\n            if eval_size > 500:\n                logger.warning(f\"Num eval_samples is large: {eval_size}, \"\n                               f\"training slow, consider reduce it by `--max_eval_samples=50`\")\n            logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n            eval_dataset = eval_dataset.map(\n                preprocess_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=eval_dataset.column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on validation dataset\",\n            )\n            eval_dataset = eval_dataset.filter(filter_empty_labels, num_proc=data_args.preprocessing_num_workers)\n            logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n            logger.debug(\"Tokenized eval example:\")\n            logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))\n\n    # Load model\n    if model_args.model_name_or_path:\n        torch_dtype = (\n            model_args.torch_dtype\n            if model_args.torch_dtype in [\"auto\", None]\n            else getattr(torch, model_args.torch_dtype)\n        )\n        world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n        ddp = world_size != 1\n        if ddp:\n            model_args.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", \"0\"))}\n            training_args.gradient_accumulation_steps = training_args.gradient_accumulation_steps // world_size or 1\n        if script_args.qlora and (len(training_args.fsdp) > 0 or is_deepspeed_zero3_enabled()):\n            logger.warning(\"FSDP and DeepSpeed ZeRO-3 are both currently incompatible with QLoRA.\")\n\n        config_kwargs = {\n            \"trust_remote_code\": model_args.trust_remote_code,\n            \"cache_dir\": model_args.cache_dir,\n            \"revision\": model_args.model_revision,\n            \"token\": model_args.hf_hub_token,\n        }\n        config = config_class.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n\n        # Set RoPE scaling\n        if model_args.rope_scaling is not None:\n            if hasattr(config, \"rope_scaling\"):\n                if model_args.rope_scaling == \"dynamic\":\n                    logger.warning(\n                        \"Dynamic NTK may not work well with fine-tuning. \"\n                        \"See: https://github.com/huggingface/transformers/pull/24653\"\n                    )\n                current_max_length = getattr(config, \"max_position_embeddings\", None)\n                if current_max_length and script_args.model_max_length > current_max_length:\n                    scaling_factor = float(math.ceil(script_args.model_max_length / current_max_length))\n                else:\n                    logger.warning(f\"The model_max_length({script_args.model_max_length}) is smaller than max \"\n                                   f\"length({current_max_length}). Consider increase model_max_length.\")\n                    scaling_factor = 1.0\n\n                setattr(config, \"rope_scaling\", {\"type\": model_args.rope_scaling, \"factor\": scaling_factor})\n                logger.info(\"Using {} scaling strategy and setting scaling factor to {}\".format(\n                    model_args.rope_scaling, scaling_factor\n                ))\n            else:\n                logger.warning(\"Current model does not support RoPE scaling.\")\n\n        # Set FlashAttention-2\n        if model_args.flash_attn:\n            if is_flash_attn_2_available:\n                config_kwargs[\"use_flash_attention_2\"] = True\n                logger.info(\"Using FlashAttention-2 for faster training and inference.\")\n            else:\n                logger.warning(\"FlashAttention-2 is not installed.\")\n        elif model_args.shift_attn and getattr(config, \"model_type\", None) == \"llama\":\n            logger.warning(\"Using `--flash_attn` for faster training in large context length, enable if your GPU\"\n                           \" is RTX3090, RTX4090, A100 or H100.\")\n\n        # Set shifted sparse attention (S^2-Attn)\n        if model_args.shift_attn:\n            if getattr(config, \"model_type\", None) == \"llama\":\n                setattr(config, \"group_size_ratio\", 0.25)\n                apply_llama_patch()\n                logger.info(\"Using shifted sparse attention with group_size_ratio=1/4.\")\n            else:\n                logger.warning(\"Current model does not support shifted sparse attention.\")\n\n        load_in_4bit = model_args.load_in_4bit\n        load_in_8bit = model_args.load_in_8bit\n        if load_in_4bit and load_in_8bit:\n            raise ValueError(\"Error, load_in_4bit and load_in_8bit cannot be set at the same time\")\n        elif load_in_8bit or load_in_4bit:\n            logger.info(f\"Quantizing model, load_in_4bit: {load_in_4bit}, load_in_8bit: {load_in_8bit}\")\n            if is_deepspeed_zero3_enabled():\n                raise ValueError(\"DeepSpeed ZeRO-3 is incompatible with quantization.\")\n            if load_in_8bit:\n                config_kwargs['quantization_config'] = BitsAndBytesConfig(load_in_8bit=True)\n            elif load_in_4bit:\n                if script_args.qlora:\n                    config_kwargs['quantization_config'] = BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_use_double_quant=True,\n                        bnb_4bit_quant_type=\"nf4\",\n                        bnb_4bit_compute_dtype=torch_dtype,\n                    )\n                else:\n                    config_kwargs['quantization_config'] = BitsAndBytesConfig(\n                        load_in_4bit=True,\n                        bnb_4bit_compute_dtype=torch_dtype,\n                    )\n\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            config=config,\n            torch_dtype=torch_dtype,\n            device_map=model_args.device_map,\n            **config_kwargs,\n        )\n\n        # Fix ChatGLM2 and ChatGLM3 and internlm2 LM head\n        if getattr(config, \"model_type\", None) == \"chatglm\" or getattr(config, \"model_type\", None) == \"internlm2\":\n            setattr(model, \"lm_head\", model.transformer.output_layer)\n            setattr(model, \"_keys_to_ignore_on_save\", [\"lm_head.weight\"])\n\n        # Set NEFTune trick for fine-tuning\n        if model_args.neft_alpha > 0:\n            input_embed = model.get_input_embeddings()\n            if isinstance(input_embed, torch.nn.Embedding):\n                def noisy_forward(self: torch.nn.Embedding, x: torch.Tensor) -> torch.Tensor:\n                    embeddings = input_embed.__class__.forward(self, x)\n                    dims = self.num_embeddings * self.embedding_dim\n                    mag_norm = model_args.neft_alpha / (dims ** 0.5)\n                    embeddings += torch.zeros_like(embeddings).uniform_(-mag_norm, mag_norm)\n                    return embeddings\n\n                input_embed.forward = MethodType(noisy_forward, input_embed)\n                logger.info(\"Using noisy embedding with alpha={:.2f}\".format(model_args.neft_alpha))\n            else:\n                logger.warning(\"Input embeddings are not normal nn.Embedding, cannot transform into noisy embedding.\")\n\n        # Patch Mixtral MOE model\n        if getattr(config, \"model_type\", None) == \"mixtral\" and is_deepspeed_zero3_enabled():\n            require_version(\"deepspeed>=0.13.0\", \"To fix: pip install deepspeed>=0.13.0\")\n            from deepspeed.utils import set_z3_leaf_modules  # type: ignore\n            from transformers.models.mixtral.modeling_mixtral import MixtralSparseMoeBlock  # type: ignore\n\n            set_z3_leaf_modules(model, [MixtralSparseMoeBlock])\n    else:\n        raise ValueError(f\"Error, model_name_or_path is None, SFT must be loaded from a pre-trained model\")\n\n    if script_args.use_peft:\n        logger.info(\"Fine-tuning method: LoRA(PEFT)\")\n\n        # Set fp32 forward hook for lm_head\n        output_layer = getattr(model, \"lm_head\")\n        if isinstance(output_layer, torch.nn.Linear) and output_layer.weight.dtype != torch.float32:\n            def fp32_forward_post_hook(module: torch.nn.Module, args: Tuple[torch.Tensor], output: torch.Tensor):\n                return output.to(torch.float32)\n\n            output_layer.register_forward_hook(fp32_forward_post_hook)\n\n        # Load LoRA model\n        if script_args.peft_path is not None:\n            logger.info(f\"Peft from pre-trained model: {script_args.peft_path}\")\n            model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)\n        else:\n            logger.info(\"Init new peft model\")\n            if load_in_8bit or load_in_4bit:\n                model = prepare_model_for_kbit_training(model, training_args.gradient_checkpointing)\n            target_modules = script_args.target_modules.split(',') if script_args.target_modules else None\n            if target_modules and 'all' in target_modules:\n                target_modules = find_all_linear_names(model, int4=load_in_4bit, int8=load_in_8bit)\n            modules_to_save = script_args.modules_to_save\n            if modules_to_save is not None:\n                modules_to_save = modules_to_save.split(',')\n            logger.info(f\"Peft target_modules: {target_modules}\")\n            logger.info(f\"Peft lora_rank: {script_args.lora_rank}\")\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                target_modules=target_modules,\n                inference_mode=False,\n                r=script_args.lora_rank,\n                lora_alpha=script_args.lora_alpha,\n                lora_dropout=script_args.lora_dropout,\n                modules_to_save=modules_to_save)\n            model = get_peft_model(model, peft_config)\n        for param in filter(lambda p: p.requires_grad, model.parameters()):\n            param.data = param.data.to(torch.float32)\n        model.print_trainable_parameters()\n    else:\n        logger.info(\"Fine-tuning method: Full parameters training\")\n        model = model.float()\n        print_trainable_parameters(model)\n\n    # Initialize our Trainer\n    if training_args.gradient_checkpointing and getattr(model, \"supports_gradient_checkpointing\", False):\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = False\n        logger.info(\"Gradient checkpointing enabled.\")\n    else:\n        model.config.use_cache = True\n        logger.info(\"Gradient checkpointing disabled.\")\n    model.enable_input_require_grads()\n    if not ddp and torch.cuda.device_count() > 1:\n        # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n        model.is_parallelizable = True\n        model.model_parallel = True\n\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        label_pad_token_id=IGNORE_INDEX,\n        pad_to_multiple_of=4 if tokenizer.padding_side == \"right\" else None,  # for shifted sparse attention\n    )\n    # Initialize our Trainer\n    trainer = SavePeftModelTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    # Training\n    if training_args.do_train:\n        logger.info(\"*** Train ***\")\n        if trainer.is_world_process_zero():\n            sample = next(iter(trainer.get_train_dataloader()))\n            logger.debug(f\"Train dataloader example: {sample}\")\n            logger.debug(f\"input_ids:\\n{list(sample['input_ids'])[:3]}, \\nlabels:\\n{list(sample['labels'])[:3]}\")\n            logger.debug(f\"Decode input_ids[0]:\\n{tokenizer.decode(sample['input_ids'][0])}\")\n            replaced_labels = [label if label != IGNORE_INDEX else tokenizer.pad_token_id for label in\n                               sample['labels'][0]]\n            logger.debug(f\"Decode labels[0]:\\n{tokenizer.decode(replaced_labels)}\")\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\n        metrics = train_result.metrics\n        metrics[\"train_samples\"] = max_train_samples\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n        model.config.use_cache = True  # enable cache after training\n        tokenizer.padding_side = \"left\"  # restore padding side\n        tokenizer.init_kwargs[\"padding_side\"] = \"left\"\n\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Training metrics: {metrics}\")\n            logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n            if is_deepspeed_zero3_enabled():\n                save_model_zero3(model, tokenizer, training_args, trainer)\n            else:\n                save_model(model, tokenizer, training_args)\n\n    # Evaluation\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n\n        metrics[\"eval_samples\"] = max_eval_samples\n        try:\n            perplexity = math.exp(metrics[\"eval_loss\"])\n        except OverflowError:\n            perplexity = float(\"inf\")\n        metrics[\"perplexity\"] = perplexity\n\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n        if trainer.is_world_process_zero():\n            logger.debug(f\"Eval metrics: {metrics}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "template.py",
          "type": "blob",
          "size": 15.591796875,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:XuMing(xuming624@qq.com)\n@description: \n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Dict, Sequence\n\n__all__ = ['Conversation', 'register_conv_template', 'get_conv_template']\n\n\n@dataclass\nclass Conversation:\n    \"\"\"A class that manages prompt templates and keeps all conversation history.\"\"\"\n\n    # The name of this template\n    name: str\n    # The system prompt\n    system_prompt: str\n    # All messages. format: list of [question, answer]\n    messages: Optional[List[Sequence[str]]]\n    # The roles of the speakers\n    roles: Optional[Sequence[str]]\n    # Conversation prompt\n    prompt: str\n    # Separator\n    sep: str\n    # Stop token, default is tokenizer.eos_token\n    stop_str: Optional[str] = \"</s>\"\n\n    def get_prompt(\n            self,\n            messages: Optional[List[Sequence[str]]] = None,\n            system_prompt: Optional[str] = \"\"\n    ) -> str:\n        \"\"\"\n        Returns a string containing prompt without response.\n        \"\"\"\n        return \"\".join(self._format_example(messages, system_prompt))\n\n    def get_dialog(\n            self,\n            messages: Optional[List[Sequence[str]]] = None,\n            system_prompt: Optional[str] = \"\"\n    ) -> List[str]:\n        \"\"\"\n        Returns a list containing 2 * n elements where the 2k-th is a query and the (2k+1)-th is a response.\n        \"\"\"\n        return self._format_example(messages, system_prompt)\n\n    def _format_example(\n            self,\n            messages: Optional[List[Sequence[str]]] = None,\n            system_prompt: Optional[str] = \"\"\n    ) -> List[str]:\n        system_prompt = system_prompt or self.system_prompt\n        system_prompt = system_prompt + self.sep if system_prompt else \"\"  # add separator for non-empty system prompt\n        messages = messages or self.messages\n        convs = []\n        if not messages:\n            messages = []\n        for turn_idx, [user_query, bot_resp] in enumerate(messages):\n            if turn_idx == 0:\n                convs.append(system_prompt + self.prompt.format(query=user_query))\n                convs.append(bot_resp)\n            else:\n                convs.append(self.sep + self.prompt.format(query=user_query))\n                convs.append(bot_resp)\n        return convs\n\n    def append_message(self, query: str, answer: str):\n        \"\"\"Append a new message.\"\"\"\n        self.messages.append([query, answer])\n\n\n# A global registry for all conversation templates\nconv_templates: Dict[str, Conversation] = {}\n\n\ndef register_conv_template(template: Conversation):\n    \"\"\"Register a new conversation template.\"\"\"\n    conv_templates[template.name] = template\n\n\n\"\"\"Vicuna v1.1 template\nSupports: https://huggingface.co/lmsys/vicuna-7b-delta-v1.1\n          https://huggingface.co/lmsys/vicuna-13b-delta-v1.1\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"vicuna\",\n        system_prompt=\"A chat between a curious user and an artificial intelligence assistant. \"\n                      \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n        messages=[],\n        roles=(\"USER\", \"ASSISTANT\"),\n        prompt=\"USER: {query} ASSISTANT:\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Base model template, for few shot\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"base\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"USER\", \"ASSISTANT\"),\n        prompt=\"{query}\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Alpaca template\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"alpaca\",\n        system_prompt=\"Below is an instruction that describes a task. \"\n                      \"Write a response that appropriately completes the request.\",\n        messages=[],\n        roles=(\"### Instruction\", \"### Response\"),\n        prompt=\"### Instruction:\\n{query}\\n\\n### Response:\\n\",\n        sep=\"\\n\\n\",\n    )\n)\n\n\"\"\"Baichuan template\nsource: https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/blob/main/generation_utils.py#L31\nSupport: https://huggingface.co/baichuan-inc/Baichuan-13B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"baichuan\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<reserved_102>\", \"<reserved_103>\"),\n        prompt=\"<reserved_102>{query}<reserved_103>\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Baichuan2 template\nSupport: https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat\n         https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"baichuan2\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<reserved_106>\", \"<reserved_107>\"),\n        prompt=\"<reserved_106>{query}<reserved_107>\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"ziya template\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"ziya\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<human>\", \"<bot>\"),\n        prompt=\"<human>:{query}\\n<bot>:\",\n        sep=\"\\n\",\n    )\n)\n\n\"\"\"Linly template\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"linly\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"User\", \"Bot\"),\n        prompt=\"User: {query}\\nBot: \",\n        sep=\"\\n\",\n    )\n)\n\n\"\"\"ChatGLM1 template\nSupport: https://huggingface.co/THUDM/chatglm-6b\nsource: https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py#L1307\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"chatglm\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"问\", \"答\"),\n        prompt=\"问：{query}\\n答：\",\n        sep=\"\\n\",\n    )\n)\n\n\"\"\"ChatGLM2 template\nSupport: https://huggingface.co/THUDM/chatglm2-6b\nsource: https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L1007\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"chatglm2\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"问\", \"答\"),\n        prompt=\"问：{query}\\n\\n答：\",\n        sep=\"\\n\\n\",\n    )\n)\n\n\"\"\"ChatGLM3 template\nSupport: https://huggingface.co/THUDM/chatglm3-6b\nsource: https://huggingface.co/THUDM/chatglm3-6b/blob/main/tokenization_chatglm.py#L179\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"chatglm3\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        prompt=\"<|user|>\\n{query}<|assistant|>\",\n        sep=\"\\n\",\n        stop_str=\"<|user|>\",\n    )\n)\n\n\"\"\"Phoenix template\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"phoenix\",\n        system_prompt=\"A chat between a curious human and an artificial intelligence assistant. \"\n                      \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        messages=[],\n        roles=(\"Human\", \"Assistant\"),\n        prompt=\"Human: <s>{query}</s>Assistant: \",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"belle template\nSupports: https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-13B\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"belle\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"Human\", \"Belle\"),\n        prompt=\"Human: {query}\\n\\nBelle: \",\n        sep=\"\\n\\n\",\n    )\n)\n\n\"\"\"aquila template\nSupports: https://huggingface.co/qhduan/aquilachat-7b\n          https://huggingface.co/BAAI/AquilaChat2-34B\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"aquila\",\n        system_prompt=\"A chat between a curious human and an artificial intelligence assistant. \"\n                      \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        messages=[],\n        roles=(\"Human\", \"Assistant\"),\n        prompt=\"Human: {query}###Assistant:\",\n        sep=\"###\",\n    )\n)\n\n\"\"\"intern template\nSupports: https://huggingface.co/internlm/internlm-chat-7b\n          https://huggingface.co/internlm/internlm-chat-20b\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"intern\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"<|User|>\", \"<|Bot|>\"),\n        prompt=\"<|User|>:{query}<eoh>\\n<|Bot|>:\",\n        sep=\"<eoa>\\n\",\n        stop_str=\"<eoa>\",\n    )\n)\n\n\"\"\"intern2 template\nSupports: https://huggingface.co/internlm/internlm2-1_8b\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"intern2\",\n        system_prompt=\"<|im_start|>system\\nYou are an AI assistant whose name is InternLM (书生·浦语).\\n<|im_end|>\\n\",\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\",\n        sep=\"<|im_end|>\\n\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n\"\"\"StarChat template\nSupports: https://huggingface.co/HuggingFaceH4/starchat-alpha\n          https://huggingface.co/HuggingFaceH4/starchat-beta\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"starchat\",\n        system_prompt=\"<system>\\n\",\n        messages=[],\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        prompt=\"<|user|>\\n{query}<|end|>\\n<|assistant|>\\n\",\n        sep=\"<|end|>\\n\",\n        stop_str=\"<|end|>\",\n    )\n)\n\n\"\"\"llama2 template\nSupports: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n          https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n          https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\nreference: https://github.com/facebookresearch/llama/blob/cfc3fc8c1968d390eb830e65c63865e980873a06/llama/generation.py#L212\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"llama2\",\n        system_prompt=(\n            \"<<SYS>>\\nYou are a helpful, respectful and honest assistant. \"\n            \"Always answer as helpfully as possible, while being safe. \"\n            \"Your answers should not include any harmful, unethical, racist, sexist, \"\n            \"toxic, dangerous, or illegal content. \"\n            \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n            \"If a question does not make any sense, or is not factually coherent, \"\n            \"explain why instead of answering something not correct. \"\n            \"If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\"\n        ),\n        messages=[],\n        roles=(\"[INST]\", \"[/INST]\"),\n        prompt=\"[INST] {query} [/INST]\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"llama3 template\nsource: https://huggingface.co/meta-llama\nSupports: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\nchat template:\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{{ model_answer_1 }}<|eot_id|>\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"llama3\",\n        system_prompt=(\n            \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n            \"You are a helpful, excellent and smart assistant.\"\n        ),\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=(\n            \"<|start_header_id|>user<|end_header_id|>\\n\\n{query}<|eot_id|>\"\n            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        ),\n        sep=\"<|eot_id|>\",\n        stop_str=\"<|eot_id|>\",\n    )\n)\n\n\"\"\"llama2-zh template\nsource: https://github.com/ymcui/Chinese-LLaMA-Alpaca-2\nSupports: https://huggingface.co/ziqingyang/chinese-alpaca-2-7b\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"llama2-zh\",\n        system_prompt=\"[INST] <<SYS>>\\nYou are a helpful assistant. 你是一个乐于助人的助手。\\n<</SYS>>\\n\\n [/INST]\",\n        messages=[],\n        roles=(\"[INST]\", \"[/INST]\"),\n        prompt=\"[INST] {query} [/INST]\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"mistral template\nSupports: https://huggingface.co/mistralai/Mistral-7B-v0.1\n          https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\nsource: https://docs.mistral.ai/llm/mistral-instruct-v0.1\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"mistral\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"[INST]\", \"[/INST]\"),\n        prompt=\"[INST] {query} [/INST]\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"XVERSE template\nSupports: https://huggingface.co/xverse/XVERSE-13B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"xverse\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"Human\", \"Assistant\"),\n        prompt=\"Human: {query}\\n\\nAssistant: \",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"chatml template\nchatml: https://xbot123.com/645a461b922f176d7cfdbc2d/\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"chatml\",\n        system_prompt=\"You are a helpful assistant.\",\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\",\n        sep=\"<|im_end|>\\n\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n\"\"\"deepseek template\nSupports: https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat\n          https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"deepseek\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"User\", \"Assistant\"),\n        prompt=\"User: {query}\\n\\nAssistant:\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"deepseekcoder template\nSupports: https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"deepseekcoder\",\n        system_prompt=(\n            \"You are an AI programming assistant, utilizing the Deepseek Coder model, \"\n            \"developed by Deepseek Company, and you only answer questions related to computer science. \"\n            \"For politically sensitive questions, security and privacy issues, \"\n            \"and other non-computer science questions, you will refuse to answer\\n\"\n        ),\n        messages=[],\n        roles=(\"### Instruction\", \"### Response\"),\n        prompt=\"### Instruction:\\n{{content}}\\n### Response:\\n\",\n        sep=\"\\n\",\n        stop_str=\"<|EOT|>\",\n    )\n)\n\n\"\"\"Yi template\nsource: https://github.com/01-ai/Yi\nSupports: https://huggingface.co/01-ai/Yi-34B-Chat\n          https://huggingface.co/01-ai/Yi-6B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"yi\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\",\n        sep=\"\\n\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n\"\"\"Orion template\nsource: https://github.com/OrionStarAI/Orion\nSupports: https://huggingface.co/OrionStarAI/Orion-14B-Chat\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"orion\",\n        system_prompt=\"\",\n        messages=[],\n        roles=(\"Human\", \"Assistant\"),\n        prompt=\"Human: {query}\\n\\nAssistant: </s>\",\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Cohere template\nsource: https://huggingface.co/CohereForAI/c4ai-command-r-plus\nSupports: https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit\n          https://huggingface.co/CohereForAI/c4ai-command-r-plus\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"cohere\",\n        system_prompt=\"<BOS_TOKEN>\",\n        messages=[],\n        roles=(\"User\", \"Assistant\"),\n        prompt=(\n            \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{query}<|END_OF_TURN_TOKEN|>\"\n            \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        ),\n        sep=\"</s>\",\n    )\n)\n\n\"\"\"Qwen template\nsource: https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat/blob/main/tokenizer_config.json#L18\nSupports: https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat\n          https://huggingface.co/Qwen/Qwen1.5-72B-Chat\n          https://huggingface.co/Qwen/Qwen2-72B\n          https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct\n\"\"\"\nregister_conv_template(\n    Conversation(\n        name=\"qwen\",\n        system_prompt=\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\",\n        messages=[],\n        roles=(\"user\", \"assistant\"),\n        prompt=\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\",\n        sep=\"\\n\",\n        stop_str=\"<|im_end|>\",\n    )\n)\n\n\ndef get_conv_template(name: str) -> Conversation:\n    \"\"\"Get a conversation template.\"\"\"\n    return conv_templates[name]\n"
        },
        {
          "name": "validate_jsonl.py",
          "type": "blob",
          "size": 2.6943359375,
          "content": "# -*- coding: utf-8 -*-\n\"\"\"\n@author:ZhuangXialie(1832963123@qq.com)\n@description: validata the dataset\n\"\"\"\nimport json\n\nimport argparse\n\n\ndef validate_jsonl(file_path):\n    print(\"开始验证 JSONL 文件格式...\\n\")\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        line_number = 0\n        valid_lines = 0\n        total_lines = 0\n        for line in file:\n            total_lines += 1\n            line_number += 1\n            try:\n                # 尝试解析JSON\n                data = json.loads(line)\n\n                # 检查是否包含 'conversations' 键\n                if 'conversations' not in data:\n                    print(f\"第 {line_number} 行: 缺少 'conversations' 键，请检查格式。\\n\")\n                    continue\n\n                # 检查 'conversations' 是否为列表\n                conversations = data['conversations']\n                if not isinstance(conversations, list):\n                    print(f\"第 {line_number} 行: 'conversations' 应为列表格式，请检查。\\n\")\n                    continue\n\n                # 检查每个对话是否包含 'from' 和 'value' 键\n                conversation_valid = True\n                for conv in conversations:\n                    if 'from' not in conv or 'value' not in conv:\n                        print(f\"第 {line_number} 行: 缺少 'from' 或 'value' 键，请检查对话格式。\\n\")\n                        conversation_valid = False\n                        continue\n\n                    # 检查 'from' 字段的值是否为 'human' 或 'gpt'\n                    if conv['from'] not in ['system', 'human', 'gpt']:\n                        print(f\"第 {line_number} 行: 'from' 字段的值无效，应为 'human' 或 'gpt'。\\n\")\n                        conversation_valid = False\n\n                if conversation_valid:\n                    valid_lines += 1\n\n            except json.JSONDecodeError:\n                print(f\"第 {line_number} 行: JSON 格式无效，请确保格式正确。\\n\")\n\n    print(f\"验证完成！\\n总行数: {total_lines} 行\")\n    print(f\"有效的行数: {valid_lines} 行\")\n    print(f\"无效行数: {total_lines - valid_lines} 行\\n\")\n\n    if valid_lines == total_lines:\n        print(\"恭喜！所有行的格式都正确。\")\n    else:\n        print(\"请根据提示检查并修复无效的行。\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Validate JSONL file format.')\n    parser.add_argument('--file_path', type=str, help='Path to JSONL file',\n                        default=\"./data/finetune/sharegpt_zh_1K_format.jsonl\")\n    args = parser.parse_args()\n    file_path = args.file_path\n    print(f\"正在检查文件: {file_path}\")\n\n    validate_jsonl(file_path)\n"
        },
        {
          "name": "vllm_deployment.sh",
          "type": "blob",
          "size": 0.9501953125,
          "content": "#!/bin/bash\n\n# 安装vllm库\npip install vllm\n\n# 指定运行程序的GPU设备编号为3，选择使用编号为3的GPU,与tp对应\nexport CUDA_VISIBLE_DEVICES=3\n\n# 启动VLLM API服务器，以下为相关参数说明：\n# - --model: 指定要加载的模型的路径\n# - --served-model-name: 模型服务的名称\n# - --dtype: 模型数据类型自动选择\n# - --port: 指定API服务器监听的端口号\n# - --host: 指定API服务器监听的网络地址，0.0.0.0允许所有IP地址访问\n# - --gpu-memory-utilization: 限制GPU内存使用上限为90%\n# - --max-model-len: 指定模型支持的最大输入长度（token数量）\n# - -tp: 张量并行度，1表示关闭张量并行，即由单个GPU处理所有张量运算\n\npython -m vllm.entrypoints.openai.api_server \\\n    --model medical-model \\\n    --served-model-name doctor \\\n    --dtype=auto \\\n    --port 8024 \\\n    --host 0.0.0.0 \\\n    --gpu-memory-utilization 0.9 \\\n    --max-model-len 512 \\\n    -tp 1\n"
        }
      ]
    }
  ]
}