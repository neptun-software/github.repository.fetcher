{
  "metadata": {
    "timestamp": 1736560381083,
    "page": 924,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "towhee-io/towhee",
      "stars": 3280,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.4228515625,
          "content": "# Test coverage configuration.\n# Usage:\n#   pip install coverage\n#   coverage erase  # clears previous data if any\n#   coverage run -m pytest\n#   coverage report  # prints to stdout\n#   coverage html  # creates ./htmlcov/*.html including annotated source\n[run]\nbranch = true\nsource = towhee\nomit = \n    towhee/pipelines/*\n    tests/*\n\n[report]\n# Ignore missing source files, i.e. fake template-generated \"files\"\nignore_errors = true\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.275390625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a Python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\ntests/unittests/test_cache/*\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ntowhee/doc/source/_autosummary\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n.DS_Store\n.idea\n.try\n.vscode/\n*.JPEG\n*.jpg"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.0869140625,
          "content": "[submodule \"examples\"]\n\tpath = examples\n\turl = https://github.com/towhee-io/examples.git\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.74609375,
          "content": "# .readthedocs.yaml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the version of Python and other tools you might need\nbuild:\n  os: ubuntu-20.04\n  tools:\n    python: \"3.9\"\n    # You can also specify other tool versions:\n    # nodejs: \"16\"\n    # rust: \"1.55\"\n    # golang: \"1.17\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n   configuration: towhee/doc/source/conf.py\n\n# If using Sphinx, optionally build your docs in additional formats such as PDF\n# formats:\n#    - pdf\n\n# Optionally declare the Python requirements required to build your docs\npython:\n   install:\n   - requirements: towhee/doc/requirements.txt\n   - method: setuptools\n     path: ."
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.3330078125,
          "content": "## Contributing to Towhee\n\nSubmitting issues, answering questions, and improving documentation are some of the many ways you can join our growing community. Regardless of how you contribute, please remember to be respectful towards your fellow Towheeians.\n\n### Contributing code\n\nThe Towhee community maintains a list of [Good First Issues](https://github.com/towhee-io/towhee/contribute).\n\n### Sharing a new operator or pipeline\n\nPipeline and operator contributions to our Towhee Hub are just as valued as artwork, code, and documentation contributions. If you have a new model, useful script, or an `x2vec` pipeline that you'd like to share with the Towhee community, [get in touch](mailto:towhee-team@zilliz.com) with us!\n\n### Submitting a new issue or feature request\n\nPlease follow the [templates](https://github.com/towhee-io/towhee/issues/new/choose) we provide for submitting bugs, enhancements, and/or feature requests; try to avoid opening blank issues whenever possible.\n\n### Style guide\n\nWe generally follow the [Google Python style guide](https://google.github.io/styleguide/pyguide.html) - this applies to the main `towhee-io/towhee` repo on Github as well as code uploaded to our Towhee Hub. We have some special rules regarding line length, imports, and whitespace - please take a look at the [Towhee style guide](https://github.com/towhee-io/towhee/blob/main/STYLE_GUIDE.md) for more information.\n\n## Pull requests\n\nWe follow a fork-and-pull model for all contributions. Before starting, we strongly recommend looking through existing PRs so you can get a feel for things.\n\nIf you're interested in contributing to the `towhee-io/towhee` codebase, follow these steps:\n\n1. Fork [Towhee](https://github.com/towhee-io/towhee). If you've forked Towhee already, simply fetch the latest changes from upstream.\n\n2. Clone your forked version of Towhee.\n\n  ```bash\n  $ git clone https://github.com/<your_username>/towhee.git\n  $ cd towhee\n  ```\n\n  If you've done this step before, make sure you're on the `main` branch and sync your changes.\n\n  ```bash\n  $ git checkout main\n  $ git pull origin main\n  ```\n\n3. Think up a suitable name for your update, bugfix, or feature. Try to avoid using branch names you've already used in the past.\n\n  ```bash\n  $ git checkout -b my-creative-branch-name\n  ```\n\n4. During development, you might want to run `pylint` or one of the tests. You can do so with one of the commands below:\n\n  ```bash\n  $ pylint --rcfile pylint.conf\n  $ pytest tests/unittests/<test_case>.py\n  ```\n\n5. If you're contributing a bugfix or docfix, squash your previous `N` commits. The interactive rebase functionality provided by git will walk you through the commit squashing process.\n\n  ```bash\n  $ git rebase -i HEAD~N\n  ```\n\n  P.S. Don't forget to commit your changes! We use a single-phrase, periodless format for commit messages (be sure to capitalize the first character):\n\n  ```bash\n  $ git commit -m \"My awesome commit message\"\n  ```\n\n6. Submit your pull request on Github. Folks in the community will discuss your pull request, and maintainers might ask you for some changes. This happens very frequently (including maintainers themselves), so don't worry if it happens to you as well.\n\n  Note that Towhee uses [DCOs](https://developercertificate.org/) to sign pull requests. Please ensure that the first line of your PR is as follows:\n  Signed-off-by: Your Name your.email@domain.com\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.6640625,
          "content": "ARG BASE_IMAGE=ubuntu:20.04\nARG PYTHON_VERSION=3.8\n\n# base image, build command: \n# docker build --platform x86_64 --target towhee-base  -t towhee/towhee-base:latest .\nFROM ${BASE_IMAGE} as towhee-base\n\n# Speed ​​up for Chinese users, only if user specified `USE_MIRROR=true`\n# In addition, use the default software source of the base image and language\nARG USE_MIRROR=false\nENV USE_MIRROR=${USE_MIRROR}\nARG UBUNTU_MIRROR=\"mirrors.tuna.tsinghua.edu.cn\"\nENV UBUNTU_MIRROR=${UBUNTU_MIRROR}\nRUN if [ \"$USE_MIRROR\" = \"true\" ]; then sed -i -e \"s/archive.ubuntu.com/${UBUNTU_MIRROR}/\" /etc/apt/sources.list && sed -i -e \"s/security.ubuntu.com/${UBUNTU_MIRROR}/\" /etc/apt/sources.list; fi\n\nRUN --mount=type=cache,id=apt-dev,target=/var/cache/apt \\\n    apt-get update && apt-get install -y --no-install-recommends ca-certificates curl git && \\\n    apt autoremove && apt clean\nENV PATH /opt/conda/bin:$PATH\n\n# conda image, build command: \n# docker build --platform x86_64 --target towhee-conda  -t towhee/towhee-conda:latest .\nFROM towhee-base as towhee-conda\nARG PYTHON_VERSION=3.8\nARG CUDA_VERSION=11.3\nARG CUDA_CHANNEL=nvidia\nARG INSTALL_CHANNEL=pytorch\nCOPY requirements.txt .\n\nARG CONDA_MIRROR=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda\"\nENV CONDA_MIRROR=${CONDA_MIRROR}\nENV CONDA_SRC=\"https://repo.anaconda.com/miniconda\"\nRUN if [ \"$USE_MIRROR\" = \"true\" ]; then export CONDA_SRC=\"${CONDA_MIRROR}/miniconda\"; fi && \\\n    curl -fsSL -v -o ~/miniconda.sh -O  \"$CONDA_SRC/Miniconda3-latest-Linux-x86_64.sh\" && \\\n    chmod +x ~/miniconda.sh && \\\n    ~/miniconda.sh -b -p /opt/conda && \\\n    rm ~/miniconda.sh && \\\n    if [ \"$USE_MIRROR\" = \"true\" ]; then \\\n        echo \"channels:\" > $HOME/.condarc && \\\n        echo \"  - ${CONDA_MIRROR}/pkgs/free/\" >> $HOME/.condarc && \\\n        echo \"  - ${CONDA_MIRROR}/pkgs/main/\" >> $HOME/.condarc && \\\n        echo \"  - ${CONDA_MIRROR}/cloud/pytorch/\" >> $HOME/.condarc && \\\n        echo \"  - defaults\" >> $HOME/.condarc && \\\n        echo \"show_channel_urls: true\" >> $HOME/.condarc; \\\n    fi\n\nRUN /opt/conda/bin/conda install -y python=${PYTHON_VERSION} cmake conda-build pyyaml numpy ipython\nENV CONDA_OVERRIDE_CUDA=${CUDA_VERSION}\nRUN /opt/conda/bin/conda install -c \"${INSTALL_CHANNEL}\" -c \"${CUDA_CHANNEL}\" -y python=${PYTHON_VERSION} pytorch torchvision torchaudio torchtext \"cudatoolkit=${CUDA_VERSION}\" && \\\n    /opt/conda/bin/conda clean -ya\n\n# ut image, build command: \n# docker build --platform x86_64 --target towhee-ut  -t towhee/towhee-ut:latest .\nFROM towhee-conda as towhee-ut\nRUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends -y ffmpeg libsm6 libxext6\nRUN pip install coverage pytest pytest-cov pytest-xdist\nWORKDIR /workspace"
        },
        {
          "name": "GPUDockerfile",
          "type": "blob",
          "size": 2.6865234375,
          "content": "ARG BASE_IMAGE=nvidia/cuda:11.7.1-base-ubuntu20.04\nARG PYTHON_VERSION=3.8\n\n# base image, build command: \n# docker build --platform x86_64 --target towhee-base  -t towhee/towhee-base:latest .\nFROM ${BASE_IMAGE} as towhee-base\n\n# Speed ​​up for Chinese users, only if user specified `USE_MIRROR=true`\n# In addition, use the default software source of the base image and language\nARG USE_MIRROR=false\nENV USE_MIRROR=${USE_MIRROR}\nARG UBUNTU_MIRROR=\"mirrors.tuna.tsinghua.edu.cn\"\nENV UBUNTU_MIRROR=${UBUNTU_MIRROR}\nRUN if [ \"$USE_MIRROR\" = \"true\" ]; then sed -i -e \"s/archive.ubuntu.com/${UBUNTU_MIRROR}/\" /etc/apt/sources.list && sed -i -e \"s/security.ubuntu.com/${UBUNTU_MIRROR}/\" /etc/apt/sources.list; fi\n\nRUN --mount=type=cache,id=apt-dev,target=/var/cache/apt \\\n    apt-get update && apt-get install -y --no-install-recommends ca-certificates curl git && \\\n    apt autoremove && apt clean\nENV PATH /opt/conda/bin:$PATH\n\n# conda image, build command: \n# docker build --platform x86_64 --target towhee-conda  -t towhee/towhee-conda:latest .\nFROM towhee-base as towhee-conda\nARG PYTHON_VERSION=3.8\nARG CUDA_VERSION=11.3\nARG CUDA_CHANNEL=nvidia\nARG INSTALL_CHANNEL=pytorch\nCOPY requirements.txt .\n\nARG CONDA_MIRROR=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda\"\nENV CONDA_MIRROR=${CONDA_MIRROR}\nENV CONDA_SRC=\"https://repo.anaconda.com/miniconda\"\nRUN if [ \"$USE_MIRROR\" = \"true\" ]; then export CONDA_SRC=\"${CONDA_MIRROR}/miniconda\"; fi && \\\n    curl -fsSL -v -o ~/miniconda.sh -O  \"$CONDA_SRC/Miniconda3-latest-Linux-x86_64.sh\" && \\\n    chmod +x ~/miniconda.sh && \\\n    ~/miniconda.sh -b -p /opt/conda && \\\n    rm ~/miniconda.sh && \\\n    if [ \"$USE_MIRROR\" = \"true\" ]; then \\\n        echo \"channels:\" > $HOME/.condarc && \\\n        echo \"  - ${CONDA_MIRROR}/pkgs/free/\" >> $HOME/.condarc && \\\n        echo \"  - ${CONDA_MIRROR}/pkgs/main/\" >> $HOME/.condarc && \\\n        echo \"  - ${CONDA_MIRROR}/cloud/pytorch/\" >> $HOME/.condarc && \\\n        echo \"  - defaults\" >> $HOME/.condarc && \\\n        echo \"show_channel_urls: true\" >> $HOME/.condarc; \\\n    fi\n\nRUN /opt/conda/bin/conda install -y python=${PYTHON_VERSION} cmake conda-build pyyaml numpy ipython\nENV CONDA_OVERRIDE_CUDA=${CUDA_VERSION}\nRUN /opt/conda/bin/conda install -c \"${INSTALL_CHANNEL}\" -c \"${CUDA_CHANNEL}\" -y python=${PYTHON_VERSION} pytorch torchvision torchaudio torchtext \"cudatoolkit=${CUDA_VERSION}\" && \\\n    /opt/conda/bin/conda clean -ya\n\n# ut image, build command: \n# docker build --platform x86_64 --target towhee-ut  -t towhee/towhee-ut:latest .\nFROM towhee-conda as towhee-ut\nRUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends -y ffmpeg libsm6 libxext6\nRUN pip install coverage pytest pytest-cov pytest-xdist\nWORKDIR /workspace"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1123046875,
          "content": "prune tests/\nprune docs/\nprune tutorials/\nprune towhee/doc/\ninclude towhee/models/clip/bpe_simple_vocab_16e6.txt.gz"
        },
        {
          "name": "MODEL_CHANGELOG.md",
          "type": "blob",
          "size": 7.873046875,
          "content": "# Towhee Model Change Log\n\nAll notable updates to Towhee models will be documented in this file.\n\n\n\n### [Preview]  Latest\n\n### [0.9.0] Dec. 2, 2022\n\nAdded 4 SOTA mdoels\n\n* **Vis4mer**\n  * paper: [*Long Movie Clip Classification with State-Space Video Models*](https://arxiv.org/abs/2204.01692)\n\n* **MCProp**\n  * paper: [*Transformer-Based Multi-modal Proposal and Re-Rank for Wikipedia Image-Caption Matching*](https://arxiv.org/abs/2206.10436)\n\n* **RepLKNet**\n  * paper: [*Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs*](https://arxiv.org/abs/2203.06717)\n\n* **Shunted Transformer**\n  * paper: [*Shunted Self-Attention via Multi-Scale Token Aggregation*](https://arxiv.org/abs/2111.15193)\n\n\n### [0.8.1] Sep. 30, 2022\n\nAdded 4 SOTA mdoels\n\n* **ISC**\n  * page: [*image-embedding/isc*](https://towhee.io/image-embedding/isc)\n  * paper: [*Contrastive Learning with Large Memory Bank and Negative Embedding Subtraction for Accurate Copy Detection*](https://arxiv.org/abs/2112.04323)\n\n* **MetaFormer**\n  * paper: [*MetaFormer Is Actually What You Need for Vision*](https://arxiv.org/abs/2111.11418)\n\n* **ConvNeXt**\n  * paper: [*A ConvNet for the 2020s*](https://arxiv.org/abs/2201.03545)\n\n* **HorNe**\n  * paper: [*HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions*](https://arxiv.org/abs/2207.14284)\n  \n\n### [0.8.0]  Aug. 16, 2022\n\nAdd 3 SOTA models\n\n* **nnfp**\n  * page: [*audio-embedding/nnfp*](https://towhee.io/audio-embedding/nnfp)\n  * paper: [*Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning*](https://arxiv.org/pdf/2010.11910.pdf)\n\n* **RepMLPNet**\n  * paper: [*Hierarchical Vision MLP with Re-parameterized Locality*](https://arxiv.org/pdf/2112.11081.pdf)\n\n* **Wave-ViT**\n  * paper: [*Unifying Wavelet and Transformers for Visual Representation Learning*](https://arxiv.org/pdf/2207.04978v1.pdf)\n\n\n### [0.7.3]  Jul. 27, 2022\n\nAdd 5 SOTA models\n\n* **CoCa**\n  * paper: [*CoCa*](https://arxiv.org/pdf/2205.01917.pdf)\n\n* **CoFormer**\n  * paper: [*CoFormer*](https://arxiv.org/pdf/2203.16518.pdf)\n\n* **TransRAC**\n  * paper: [*TransRAC*](https://arxiv.org/pdf/2204.01018.pdf)\n\n* **CVNet**\n  * paper: [*CVNet*](https://arxiv.org/pdf/2204.01458.pdf)\n\n* **MaxViT**\n  * paper: [*MaxViT*](https://arxiv.org/pdf/2204.01697.pdf)\n\n### [0.7.1]  Jul. 01, 2022\n\nAdd 1 vision transformer backbone, 1 text-image retrieval model, 2 video retrieval models\n\n* **MPViT**\n  * page: [*image-embedding/mpvit*](https://towhee.io/image-embedding/mpvit)\n  * paper: [*MPViT : Multi-Path Vision Transformer for Dense Prediction*](https://arxiv.org/pdf/2112.11010.pdf)\n\n* **LightningDOT**\n  * page: [*image-text-embedding/lightningdot*](https://towhee.io/image-text-embedding/lightningdot)\n  * paper: [*LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval*](https://arxiv.org/pdf/2103.08784.pdf)\n\n* **BridgeFormer**\n  * page: [*video-text-embedding/bridge-former*](https://towhee.io/video-text-embedding/bridge-former)\n  * paper: [*Bridging Video-text Retrieval with Multiple Choice Questions*](https://arxiv.org/pdf/2201.04850.pdf)\n\n* **collaborative-experts**\n  * page: [*video-text-embedding/collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-experts)\n  * paper: [*TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval*](https://arxiv.org/pdf/2104.08271.pdf)\n\n### [0.7.0]  Jun. 24, 2022\n\nAdd 6 video understanding/classification models\n\n* **Video Swin Transformer**\n  * page: [*action-classification/video-swin-transformer*](https://towhee.io/action-classification/video-swin-transformer)\n  * paper: [*Video Swin Transformer*](https://arxiv.org/pdf/2106.13230v1.pdf)\n\n* **TSM**\n  * page: [*action-classification/tsm*](https://towhee.io/action-classification/tsm)\n  * paper: [*TSM: Temporal Shift Module for Efficient Video Understanding*](https://arxiv.org/pdf/1811.08383v3.pdf)\n\n* **Uniformer** \n  * page: [*action-classification/uniformer*](https://towhee.io/action-classification/uniformer)\n  * paper: [*UNIFORMER: UNIFIED TRANSFORMER FOR EFFICIENT SPATIOTEMPORAL REPRESENTATION LEARNING*](https://arxiv.org/pdf/2201.04676v3.pdf)\n\n* **OMNIVORE** \n  * page: [*action-classification/omnivore*](https://towhee.io/action-classification/omnivore)\n  * paper: [*OMNIVORE: A Single Model for Many Visual Modalities*](https://arxiv.org/pdf/2201.08377.pdf)\n\n* **TimeSformer** \n  * page: [*action-classification/timesformer*](https://towhee.io/action-classification/timesformer)\n  * paper: [*Is Space-Time Attention All You Need for Video Understanding?*](https://arxiv.org/pdf/2102.05095.pdf)\n\n* **MoViNets** \n  * page: [*action-classification/movinet*](https://towhee.io/action-classification/movinet)\n  * paper: [*MoViNets: Mobile Video Networks for Efficient Video Recognition*](https://arxiv.org/pdf/2103.11511.pdf)\n\nAdd 4 video retrieval models\n\n* **CLIP4Clip** \n  * page: [*video-text-embedding/clip4clip*](https://towhee.io/video-text-embedding/clip4clip)\n  * paper: [*CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval*](https://arxiv.org/pdf/2104.08860.pdf)\n\n* **DRL** \n  * page: [*video-text-embedding/drl*](https://towhee.io/video-text-embedding/drl)\n  * paper: [*Disentangled Representation Learning for Text-Video Retrieval*](https://arxiv.org/pdf/2203.07111.pdf)\n\n* **Frozen in Time** \n  * page: [*video-text-embedding/frozen-in-time*](https://towhee.io/video-text-embedding/frozen-in-time)\n  * paper: [*Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval*](https://arxiv.org/pdf/2104.00650.pdf)\n\n* **MDMMT** \n  * page: [*video-text-embedding/mdmmt*](https://towhee.io/video-text-embedding/mdmmt)\n  * paper: [*MDMMT: Multidomain Multimodal Transformer for Video Retrieval*](https://arxiv.org/pdf/2103.10699.pdf)\n\n### [0.6.1]  May. 13, 2022\n\nAdd 3 text-image multimodal models\n\n* **CLIP**\n  * page: [*image-text-embedding/clip*](https://towhee.io/image-text-embedding/clip)\n  * paper: [*Learning Transferable Visual Models From Natural Language Supervision*](https://arxiv.org/pdf/2103.00020.pdf)\n\n* **BLIP**\n  * page: [*image-text-embedding/blip*](https://towhee.io/image-text-embedding/blip)\n  * paper: [*BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation*](https://arxiv.org/pdf/2201.12086.pdf)\n\n* **LightningDOT**\n  * page: [*image-text-embedding/lightningdot*](https://towhee.io/image-text-embedding/lightningdot)\n  * paper: [*LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval*](https://arxiv.org/pdf/2103.08784.pdf)\n\nAdd 6 video understanding/classification models\n\n* **I3D** (from PyTorchVideo)\n  * page: [*action-classification/pytorchvideo*](https://towhee.io/action-classification/pytorchvideo)\n  * paper: [*Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset*](https://arxiv.org/pdf/1705.07750.pdf)\n\n* **C2D** (from PyTorchVideo)\n  * page: [*action-classification/pytorchvideo*](https://towhee.io/action-classification/pytorchvideo)\n  * paper: [*Non-local Neural Networks*](https://arxiv.org/pdf/1711.07971.pdf)\n\n* **Slow** (from PyTorchVideo)\n  * page: [*action-classification/pytorchvideo*](https://towhee.io/action-classification/pytorchvideo)\n  * paper: [*SlowFast Networks for Video Recognition*](https://arxiv.org/pdf/1812.03982.pdf)\n\n* **SlowFast** (from PyTorchVideo)\n  * page: [*action-classification/pytorchvideo*](https://towhee.io/action-classification/pytorchvideo)\n  * paper: [*SlowFast Networks for Video Recognition*](https://arxiv.org/pdf/1812.03982.pdf)\n\n* **X3D** (from PyTorchVideo)\n  * page: [*action-classification/pytorchvideo*](https://towhee.io/action-classification/pytorchvideo)\n  * paper: [*X3D: Expanding Architectures for Efficient Video Recognition*](https://arxiv.org/pdf/2004.04730.pdf)\n\n* **MViT** (from PyTorchVideo)\n  * page: [*action-classification/pytorchvideo*](https://towhee.io/action-classification/pytorchvideo)\n  * paper: [*Multiscale Vision Transformers*](https://arxiv.org/pdf/2104.11227.pdf)"
        },
        {
          "name": "OWNERS",
          "type": "blob",
          "size": 0.3310546875,
          "content": "filters:\n  \".*\":\n    reviewers:\n      - GuoRentong\n      - Chiiizzzy\n      - junjiejiangjjj\n      - fzliu\n      - filip-halt\n      - wxywb\n      - shiyu22\n      - jaelgu\n      - zc277584121\n\n    approvers:\n      - GuoRentong\n      - junjiejiangjjj\n      - fzliu\n\n  \"tests\":\n    reviewers:\n      - binbinlv\n\n    approvers:\n      - binbinlv\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.3984375,
          "content": "&nbsp;\n\n<p align=\"center\">\n    <img src=\"towhee_logo.png#gh-light-mode-only\" width=\"60%\"/>\n    <img src=\"assets/towhee_logo_dark.png#gh-dark-mode-only\" width=\"60%\"/>\n</p>\n\n\n<h3 align=\"center\">\n  <p style=\"text-align: center;\"> <span style=\"font-weight: bold; font: Arial, sans-serif;\">x</span>2vec, Towhee is all you need! </p>\n</h3>\n\n<h3 align=\"center\">\n  <p style=\"text-align: center;\">\n  <a href=\"README.md\" target=\"_blank\">ENGLISH</a> | <a href=\"README_CN.md\">中文文档</a>\n  </p>\n</h3>\n\n<div class=\"column\" align=\"middle\">\n  <a href=\"https://slack.towhee.io\">\n    <img src=\"https://img.shields.io/badge/join-slack-orange?style=flat\" alt=\"join-slack\"/>\n  </a>\n  <a href=\"https://twitter.com/towheeio\">\n    <img src=\"https://img.shields.io/badge/follow-twitter-blue?style=flat\" alt=\"twitter\"/>\n  </a>\n  <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">\n    <img src=\"https://img.shields.io/badge/license-apache2.0-green?style=flat\" alt=\"license\"/>\n  </a>\n  <a href=\"https://github.com/towhee-io/towhee/actions/workflows/pylint.yml\">\n    <img src=\"https://github.com/towhee-io/towhee/actions/workflows/pylint.yml/badge.svg\" alt=\"github actions\"/>\n  </a>\n  <a href=\"https://pypi.org/project/towhee/\">\n    <img src=\"https://img.shields.io/pypi/v/towhee?label=Release&color&logo=Python\" alt=\"github actions\"/>\n  </a>\n  <a href=\"https://app.codecov.io/gh/towhee-io/towhee\">\n    <img src=\"https://img.shields.io/codecov/c/github/towhee-io/towhee?style=flat\" alt=\"coverage\"/>\n  </a>\n</div>\n\n&nbsp;\n\n[Towhee](https://towhee.io) is a cutting-edge framework designed to streamline the processing of unstructured data through the use of Large Language Model (LLM) based pipeline orchestration. It is uniquely positioned to extract invaluable insights from diverse unstructured data types, including lengthy text, images, audio and video files. Leveraging the capabilities of generative AI and the SOTA deep learning models, Towhee is capable of transforming this unprocessed data into specific formats such as text, image, or embeddings. These can then be efficiently loaded into an appropriate storage system like a vector database. Developers can initially build an intuitive data processing pipeline prototype with user friendly Pythonic API, then optimize it for production environments.\n\n🎨 Multi Modalities: Towhee is capable of handling a wide range of data types. Whether it's image data, video clips, text, audio files, or even molecular structures, Towhee can process them all. \n\n📃    LLM Pipeline orchestration:  Towhee offers flexibility to adapt to different Large Language Models (LLMs). Additionally, it allows for hosting open-source large models locally. Moreover, Towhee provides features like prompt management and knowledge retrieval, making the interaction with these LLMs more efficient and effective.\n\n🎓 Rich Operators: Towhee provides a wide range of ready-to-use state-of-the-art models across five domains: CV, NLP, multimodal, audio, and medical. With over 140 models like BERT and CLIP and rich functionalities like video decoding, audio slicing, frame sampling,  and dimensionality reduction, it assists in efficiently building data processing pipelines. \n\n🔌   Prebuilt ETL Pipelines: Towhee offers ready-to-use ETL (Extract, Transform, Load) pipelines for common tasks such as Retrieval-Augmented Generation, Text Image search, and Video copy detection. This means you don't need to be an AI expert to build applications using these features. \n⚡️  High performance backend: Leveraging the power of the Triton Inference Server, Towhee can speed up model serving on both CPU and GPU using platforms like TensorRT, Pytorch, and ONNX. Moreover, you can transform your Python pipeline into a high-performance docker container with just a few lines of code, enabling efficient deployment and scaling.\n\n🐍 Pythonic API: Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.\n\n## Getting started\n\nTowhee requires Python 3.7+. You can install Towhee via `pip`:\n\n```bash\npip install towhee towhee.models\n```\n\n### Pipeline\n\n### Pre-defined Pipeline\n\nTowhee provides some pre-defined pipelines to help users quickly implement some functions. \nCurrently implemented are: \n- [Sentence Embedding](https://towhee.io/tasks/detail/pipeline/sentence-similarity)\n- [Image Embedding](https://towhee.io/tasks/detail/pipeline/text-image-search)\n- [Video deduplication](https://towhee.io/tasks/detail/pipeline/video-copy-detection)\n- [Question Answer with Docs](https://towhee.io/tasks/detail/pipeline/retrieval-augmented-generation)\n\nAll pipelines can be found on Towhee Hub. Here is an example of using the sentence_embedding pipeline: \n\n```python\nfrom towhee import AutoPipes, AutoConfig\n# get the built-in sentence_similarity pipeline\nconfig = AutoConfig.load_config('sentence_embedding')\nconfig.model = 'paraphrase-albert-small-v2'\nconfig.device = 0\nsentence_embedding = AutoPipes.pipeline('sentence_embedding', config=config)\n\n# generate embedding for one sentence\nembedding = sentence_embedding('how are you?').get()\n# batch generate embeddings for multi-sentences\nembeddings = sentence_embedding.batch(['how are you?', 'how old are you?'])\nembeddings = [e.get() for e in embeddings]\n```\n### Custom pipelines \n\nIf you can't find the pipeline you want in towhee hub, you can also implement custom pipelines through the towhee Python API. In the following example, we will create a cross-modal retrieval pipeline based on CLIP.\n```python\n\nfrom towhee import ops, pipe, DataCollection\n# create image embeddings and build index\np = (\n    pipe.input('file_name')\n    .map('file_name', 'img', ops.image_decode.cv2())\n    .map('img', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='image'))\n    .map('vec', 'vec', ops.towhee.np_normalize())\n    .map(('vec', 'file_name'), (), ops.ann_insert.faiss_index('./faiss', 512))\n    .output()\n)\n\nfor f_name in ['https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog1.png',\n               'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog2.png',\n               'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog3.png']:\n    p(f_name)\n\n# Flush faiss data into disk. \np.flush()\n# search image by text\ndecode = ops.image_decode.cv2('rgb')\np = (\n    pipe.input('text')\n    .map('text', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='text'))\n    .map('vec', 'vec', ops.towhee.np_normalize())\n    # faiss op result format:  [[id, score, [file_name], ...]\n    .map('vec', 'row', ops.ann_search.faiss_index('./faiss', 3))\n    .map('row', 'images', lambda x: [decode(item[2][0]) for item in x])\n    .output('text', 'images')\n)\n\nDataCollection(p('puppy Corgi')).show()\n\n```\n<img src=\"assets/towhee_example.png\" style=\"width: 60%; height: 60%\">\n\n\n## Core Concepts\n\nTowhee is composed of four main building blocks - `Operators`, `Pipelines`, `DataCollection API` and `Engine`.\n\n- __Operators__: An operator is a single building block of a neural data processing pipeline. Different implementations of operators are categorized by tasks, with each task having a standard interface. An operator can be a deep learning model, a data processing method, or a Python function.\n\n- __Pipelines__: A pipeline is composed of several operators interconnected in the form of a DAG (directed acyclic graph). This DAG can direct complex functionalities, such as embedding feature extraction, data tagging, and cross modal data analysis.\n\n- __DataCollection API__: A Pythonic and method-chaining style API for building custom pipelines, providing multiple data conversion interfaces: map, filter, flat_map, concat, window, time_window, and window_all. Through these interfaces, complex data processing pipelines can be built quickly to process unstructured data such as video, audio, text, images, etc.\n\n- __Engine__: The engine sits at Towhee's core. Given a pipeline, the engine will drive dataflow among individual operators, schedule tasks, and monitor compute resource usage (CPU/GPU/etc). We provide a basic engine within Towhee to run pipelines on a single-instance machine and a Triton-based engine for docker containers.\n\n## Resource\n- TowheeHub: https://towhee.io/\n- docs: https://towhee.readthedocs.io/en/latest/\n- examples: https://github.com/towhee-io/examples\n\n## Contributing\n\nWriting code is not the only way to contribute! Submitting issues, answering questions, and improving documentation are just some of the many ways you can help our growing community. Check out our [contributing page](https://github.com/towhee-io/towhee/blob/main/CONTRIBUTING.md) for more information.\n\nSpecial thanks goes to these folks for contributing to Towhee, either on Github, our Towhee Hub, or elsewhere:\n<br><!-- Do not remove start of hero-bot --><br>\n<img src=\"https://img.shields.io/badge/all--contributors-45-orange\"><br>\n<a href=\"https://github.com/3270939387\"><img src=\"https://avatars.githubusercontent.com/u/133976770?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/AniTho\"><img src=\"https://avatars.githubusercontent.com/u/34787227?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Armaggheddon\"><img src=\"https://avatars.githubusercontent.com/u/47779194?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Chiiizzzy\"><img src=\"https://avatars.githubusercontent.com/u/72550076?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/GuoRentong\"><img src=\"https://avatars.githubusercontent.com/u/57477222?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/KizAE86\"><img src=\"https://avatars.githubusercontent.com/u/146533028?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/NicoYuan1986\"><img src=\"https://avatars.githubusercontent.com/u/109071306?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/NothingEverHappens\"><img src=\"https://avatars.githubusercontent.com/u/216412?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Opdoop\"><img src=\"https://avatars.githubusercontent.com/u/21202514?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Sharp-rookie\"><img src=\"https://avatars.githubusercontent.com/u/62098006?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Tumao727\"><img src=\"https://avatars.githubusercontent.com/u/20420181?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/UncleLLD\"><img src=\"https://avatars.githubusercontent.com/u/16642335?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/YuDongPan\"><img src=\"https://avatars.githubusercontent.com/u/88148730?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/binbinlv\"><img src=\"https://avatars.githubusercontent.com/u/83755740?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/derekdqc\"><img src=\"https://avatars.githubusercontent.com/u/11754703?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/dreamfireyu\"><img src=\"https://avatars.githubusercontent.com/u/47691077?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/emmanuel-ferdman\"><img src=\"https://avatars.githubusercontent.com/u/35470921?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/eric9204\"><img src=\"https://avatars.githubusercontent.com/u/90449228?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/filip-halt\"><img src=\"https://avatars.githubusercontent.com/u/81822489?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/fzliu\"><img src=\"https://avatars.githubusercontent.com/u/6334158?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/gexy185\"><img src=\"https://avatars.githubusercontent.com/u/103474331?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/huan415\"><img src=\"https://avatars.githubusercontent.com/u/37132274?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/hyf3513OneGO\"><img src=\"https://avatars.githubusercontent.com/u/67197231?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/jaelgu\"><img src=\"https://avatars.githubusercontent.com/u/86251631?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/jeffoverflow\"><img src=\"https://avatars.githubusercontent.com/u/24581746?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/jingkl\"><img src=\"https://avatars.githubusercontent.com/u/34296482?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/jinlingxu06\"><img src=\"https://avatars.githubusercontent.com/u/106302799?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/junjiejiangjjj\"><img src=\"https://avatars.githubusercontent.com/u/14136703?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/krishnakatyal\"><img src=\"https://avatars.githubusercontent.com/u/37455387?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/lrk612\"><img src=\"https://avatars.githubusercontent.com/u/131778006?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/omartarek206\"><img src=\"https://avatars.githubusercontent.com/u/40853054?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/oneseer\"><img src=\"https://avatars.githubusercontent.com/u/28955741?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/pravee42\"><img src=\"https://avatars.githubusercontent.com/u/65100038?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/reiase\"><img src=\"https://avatars.githubusercontent.com/u/5417329?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/sanbuphy\"><img src=\"https://avatars.githubusercontent.com/u/96160062?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/shiyu22\"><img src=\"https://avatars.githubusercontent.com/u/53459423?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/songxianj\"><img src=\"https://avatars.githubusercontent.com/u/107831450?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/soulteary\"><img src=\"https://avatars.githubusercontent.com/u/1500781?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/sre-ci-robot\"><img src=\"https://avatars.githubusercontent.com/u/56469371?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/sutcalag\"><img src=\"https://avatars.githubusercontent.com/u/83750738?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/wxywb\"><img src=\"https://avatars.githubusercontent.com/u/5432721?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/xychu\"><img src=\"https://avatars.githubusercontent.com/u/936394?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/zc277584121\"><img src=\"https://avatars.githubusercontent.com/u/17022025?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/zengxiang68\"><img src=\"https://avatars.githubusercontent.com/u/68835157?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/zhousicong\"><img src=\"https://avatars.githubusercontent.com/u/7541863?v=4\" width=\"30px\" /></a>\n<br><!-- Do not remove end of hero-bot --><br>\n\nLooking for a database to store and index your embedding vectors? Check out [Milvus](https://github.com/milvus-io/milvus).\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 14.1171875,
          "content": "&nbsp;\n\n<p align=\"center\">\n    <img src=\"towhee_logo.png#gh-light-mode-only\" width=\"60%\"/>\n    <img src=\"assets/towhee_logo_dark.png#gh-dark-mode-only\" width=\"60%\"/>\n</p>\n\n\n<h3 align=\"center\">\n  <p style=\"text-align: center;\"> <span style=\"font-weight: bold; font: Arial, sans-serif;\">x</span>2vec, Towhee is all you need! </p>\n</h3>\n\n<h3 align=\"center\">\n  <p style=\"text-align: center;\">\n  <a href=\"README.md\" target=\"_blank\">ENGLISH</a> | <a href=\"README_CN.md\">中文文档</a>\n  </p>\n</h3>\n\n<div class=\"column\" align=\"middle\">\n  <a href=\"https://slack.towhee.io\">\n    <img src=\"https://img.shields.io/badge/join-slack-orange?style=flat\" alt=\"join-slack\"/>\n  </a>\n  <a href=\"https://twitter.com/towheeio\">\n    <img src=\"https://img.shields.io/badge/follow-twitter-blue?style=flat\" alt=\"twitter\"/>\n  </a>\n  <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">\n    <img src=\"https://img.shields.io/badge/license-apache2.0-green?style=flat\" alt=\"license\"/>\n  </a>\n  <a href=\"https://github.com/towhee-io/towhee/actions/workflows/pylint.yml\">\n    <img src=\"https://github.com/towhee-io/towhee/actions/workflows/pylint.yml/badge.svg\" alt=\"github actions\"/>\n  </a>\n  <a href=\"https://pypi.org/project/towhee/\">\n    <img src=\"https://img.shields.io/pypi/v/towhee?label=Release&color&logo=Python\" alt=\"github actions\"/>\n  </a>\n  <a href=\"https://app.codecov.io/gh/towhee-io/towhee\">\n    <img src=\"https://img.shields.io/codecov/c/github/towhee-io/towhee?style=flat\" alt=\"coverage\"/>\n  </a>\n</div>\n\n&nbsp;\n\n[Towhee](https://towhee.io) 可以让用户像搭积木一样，轻松地完成 AI 应用程序的构建和落地。通过使用大语言模型(LLM)以及其他SOTA深度学习模型，从各种未加工过的非结构化数据中（长文本、图像、音频和视频）提取信息，并将这些信息存储到合适的存储系统中,比如可以将提取出的向量数据存储到向量数据库中。开发人员能够通过Towhee提供的Pythonic API来完成各种 AI 流水线和 AI 应用的原型设计，享受自动代码优化，低成本实现生产环境的应用性能优化。\n\n\n## ✨ 项目特点\n\n🎨 **多模态** Towhee 能够处理各种数据类型。无论是图像数据、视频片段、文本、音频文件还是分子结构,Towhee 都可以处理。\n\n📃 **LLM 管道编排** Towhee 具有灵活性,可以适应不同的大语言模型(LLM)。此外,它允许在本地托管开源大模型。此外,Towhee 提供了prompt管理和知识检索等功能,使与这些 LLM 的交互更加高效和有效。 \n\n🎓 **丰富的算子** Towhee 提供了五个领域内众多最先进的现成模型:计算机视觉、自然语言处理、多模态、音频和医疗领域。拥有超过 140 个模型,如 BERT 和 CLIP,以及丰富的功能,如视频解码、音频切片、帧采样和降维,它有助于高效地搭建数据处理流水线。\n\n🔌 **预构建的 ETL 管道** Towhee 提供现成的 ETL(提取、转换、加载)管道用于常见任务,如增强生成检索、文本图像搜索和视频副本检测。这意味着您不需要成为 AI 专家即可使用这些功能构建应用程序。 \n\n⚡️ **高性能后端** 利用 Triton 推理服务器的计算能力,Towhee 可以使用 TensorRT、Pytorch 和 ONNX 等平台加速 CPU 和 GPU 上的模型服务。此外,您可以用几行代码将 Python 管道转换为高性能的 Docker 容器,实现高效部署和扩展。\n\n🐍 **Python 风格的 API** Towhee 包含一个 Python 风格的方法链 API,用于描述自定义数据处理流水线。我们还支持模式,这使得处理非结构化数据就像处理表格数据一样简单。\n\n\n## 🎓 快速入门\n\nTowhee 需要 Python 3.7 及以上的运行环境，可以通过 `pip` 来完成快速安装：\n\n```bash\npip install towhee towhee.models\n```\n\n## 流水线\n\n### 预定义流水线\n\nTowhee 提供了一些预定义流水线，可以帮助用户快速实现一些功能。\n目前已经实现的有：\n- [文本embedding](https://towhee.io/tasks/detail/pipeline/sentence-similarity)\n- [图像embedding](https://towhee.io/tasks/detail/pipeline/text-image-search)\n- [视频去重](https://towhee.io/tasks/detail/pipeline/video-copy-detection)\n- [基于大语言模型的知识库问答](https://towhee.io/tasks/detail/pipeline/retrieval-augmented-generation)\n\n所有的流水线均能在Towhee Hub上找到，下面是sentence_embedding流水线的使用示例:\n\n```python\nfrom towhee import AutoPipes, AutoConfig\n# get the built-in sentence_similarity pipeline\nconfig = AutoConfig.load_config('sentence_embedding')\nconfig.model = 'paraphrase-albert-small-v2'\nconfig.device = 0\nsentence_embedding = AutoPipes.pipeline('sentence_embedding', config=config)\n\n# generate embedding for one sentence\nembedding = sentence_embedding('how are you?').get()\n# batch generate embeddings for multi-sentences\nembeddings = sentence_embedding.batch(['how are you?', 'how old are you?'])\nembeddings = [e.get() for e in embeddings]\n```\n### 自定义流水线\n\n通过Towhee python API，可以实现自定义的流水线, 下面示例中，我们来创建一个基于 CLIP 的跨模态检索流水线。\n\n```python\nfrom towhee import ops, pipe, DataCollection\n# create image embeddings and build index\np = (\n    pipe.input('file_name')\n    .map('file_name', 'img', ops.image_decode.cv2())\n    .map('img', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='image'))\n    .map('vec', 'vec', ops.towhee.np_normalize())\n    .map(('vec', 'file_name'), (), ops.ann_insert.faiss_index('./faiss', 512))\n    .output()\n)\n\nfor f_name in ['https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog1.png',\n               'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog2.png',\n               'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog3.png']:\n    p(f_name)\n\n# Flush faiss data into disk. \np.flush()\n# search image by text\ndecode = ops.image_decode.cv2('rgb')\np = (\n    pipe.input('text')\n    .map('text', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='text'))\n    .map('vec', 'vec', ops.towhee.np_normalize())\n    # faiss op result format:  [[id, score, [file_name], ...]\n    .map('vec', 'row', ops.ann_search.faiss_index('./faiss', 3))\n    .map('row', 'images', lambda x: [decode(item[2][0]) for item in x])\n    .output('text', 'images')\n)\n\nDataCollection(p('puppy Corgi')).show()\n```\n\n\n<img src=\"assets/towhee_example.png\" style=\"width: 60%; height: 60%\">\n\n\n## 🚀 核心概念\n\nTowhee 由四个主要模块组成：“算子（Operators）”、“流水线（Pipelines）”、“数据处理 API（DataCollection API）”和“执行引擎（Engine）”。\n\n- __算子（Operator）__：算子是构成神经网络数据处理水流线(neural data processing pipeline)的“积木块”（基础组件）。这些基础组件按照任务类型进行组织，每种任务类型都具有标准的调用接口。一个算子可以是某种神经网络模型，某种数据处理方法，或是某个 Python 函数。\n\n- __流水线（Pipeline）__：流水线是由若干个算子组成的 DAG（有向无环图）。流水线可以实现比单个算子更复杂的功能，诸如特征向量提取、数据标记、跨模态数据理解等。\n\n- __数据处理 API（DataCollection）__: DataCollection API 是用于描述流水线的编程接口。提供多种数据转换接口：map, filter, flat_map, concat, window, time_window以及window_all，通过这些接口，可以快速构建复杂的数据处理管道，处理视频，音频，文本，图像等非结构化数据。\n\n- __执行引擎（Engine）__: 执行引擎负责实例化流水线、任务调度、资源管理，以及运行期性能优化。面向快速原型构建，Towhee 提供了轻量级的本地执行引擎；面向生产环境需求，Towhee 提供了基于 Nvidia Triton 的高性能执行引擎。\n\n## 资源\n\n- TowheeHub: https://towhee.io/\n- 文档: https://towhee.readthedocs.io/en/latest/\n- 示例: https://github.com/towhee-io/examples\n\n## 🏠 了解 & 加入社区\n\n**编写代码并不是参与项目的唯一方式！**\n\n你可以通过很多方式来参与 Towhee 社区：提交问题、回答问题、改进文档、加入社群讨论、参加线下 Meetup 活动等。\n\n你的参与对于项目的持续健康发展至关重要。欢迎查阅 🎁[贡献页面](https://github.com/towhee-io/towhee/blob/main/CONTRIBUTING.md) 的文档内容，了解更多详细信息。\n\n### 💥 致谢\n\n特别感谢下面的同学为 Towhee 社区做出的贡献 🌹：\n\n<br><!-- Do not remove start of hero-bot --><br>\n<img src=\"https://img.shields.io/badge/all--contributors-45-orange\"><br>\n<a href=\"https://github.com/3270939387\"><img src=\"https://avatars.githubusercontent.com/u/133976770?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/AniTho\"><img src=\"https://avatars.githubusercontent.com/u/34787227?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Armaggheddon\"><img src=\"https://avatars.githubusercontent.com/u/47779194?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Chiiizzzy\"><img src=\"https://avatars.githubusercontent.com/u/72550076?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/GuoRentong\"><img src=\"https://avatars.githubusercontent.com/u/57477222?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/KizAE86\"><img src=\"https://avatars.githubusercontent.com/u/146533028?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/NicoYuan1986\"><img src=\"https://avatars.githubusercontent.com/u/109071306?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/NothingEverHappens\"><img src=\"https://avatars.githubusercontent.com/u/216412?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Opdoop\"><img src=\"https://avatars.githubusercontent.com/u/21202514?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Sharp-rookie\"><img src=\"https://avatars.githubusercontent.com/u/62098006?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/Tumao727\"><img src=\"https://avatars.githubusercontent.com/u/20420181?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/UncleLLD\"><img src=\"https://avatars.githubusercontent.com/u/16642335?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/YuDongPan\"><img src=\"https://avatars.githubusercontent.com/u/88148730?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/binbinlv\"><img src=\"https://avatars.githubusercontent.com/u/83755740?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/derekdqc\"><img src=\"https://avatars.githubusercontent.com/u/11754703?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/dreamfireyu\"><img src=\"https://avatars.githubusercontent.com/u/47691077?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/emmanuel-ferdman\"><img src=\"https://avatars.githubusercontent.com/u/35470921?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/eric9204\"><img src=\"https://avatars.githubusercontent.com/u/90449228?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/filip-halt\"><img src=\"https://avatars.githubusercontent.com/u/81822489?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/fzliu\"><img src=\"https://avatars.githubusercontent.com/u/6334158?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/gexy185\"><img src=\"https://avatars.githubusercontent.com/u/103474331?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/huan415\"><img src=\"https://avatars.githubusercontent.com/u/37132274?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/hyf3513OneGO\"><img src=\"https://avatars.githubusercontent.com/u/67197231?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/jaelgu\"><img src=\"https://avatars.githubusercontent.com/u/86251631?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/jeffoverflow\"><img src=\"https://avatars.githubusercontent.com/u/24581746?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/jingkl\"><img src=\"https://avatars.githubusercontent.com/u/34296482?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/jinlingxu06\"><img src=\"https://avatars.githubusercontent.com/u/106302799?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/junjiejiangjjj\"><img src=\"https://avatars.githubusercontent.com/u/14136703?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/krishnakatyal\"><img src=\"https://avatars.githubusercontent.com/u/37455387?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/lrk612\"><img src=\"https://avatars.githubusercontent.com/u/131778006?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/omartarek206\"><img src=\"https://avatars.githubusercontent.com/u/40853054?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/oneseer\"><img src=\"https://avatars.githubusercontent.com/u/28955741?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/pravee42\"><img src=\"https://avatars.githubusercontent.com/u/65100038?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/reiase\"><img src=\"https://avatars.githubusercontent.com/u/5417329?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/sanbuphy\"><img src=\"https://avatars.githubusercontent.com/u/96160062?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/shiyu22\"><img src=\"https://avatars.githubusercontent.com/u/53459423?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/songxianj\"><img src=\"https://avatars.githubusercontent.com/u/107831450?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/soulteary\"><img src=\"https://avatars.githubusercontent.com/u/1500781?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/sre-ci-robot\"><img src=\"https://avatars.githubusercontent.com/u/56469371?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/sutcalag\"><img src=\"https://avatars.githubusercontent.com/u/83750738?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/wxywb\"><img src=\"https://avatars.githubusercontent.com/u/5432721?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/xychu\"><img src=\"https://avatars.githubusercontent.com/u/936394?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/zc277584121\"><img src=\"https://avatars.githubusercontent.com/u/17022025?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/zengxiang68\"><img src=\"https://avatars.githubusercontent.com/u/68835157?v=4\" width=\"30px\" /></a>\n<a href=\"https://github.com/zhousicong\"><img src=\"https://avatars.githubusercontent.com/u/7541863?v=4\" width=\"30px\" /></a>\n<br><!-- Do not remove end of hero-bot --><br>\n\n如果你正在寻找用于存储和检索向量的数据库，不妨看看[Milvus](https://github.com/milvus-io/milvus)。\n"
        },
        {
          "name": "STYLE_GUIDE.md",
          "type": "blob",
          "size": 7.57421875,
          "content": "# Towhee Style Guide\n\n## **Coding Style**\nTowhee's coding style is based on PEP8 and PEP257.\n\n### **Maximum Line Length**\n- The limit for docstring is 88 characters for each line.\n- The limit for code and comment is 150 characters for each line.\n\n### **Indentation and Split**\nUse 4 whitespaces per indentation level.\n```python\ndef foo(\n\targ_1,\n\targ_2,\n\targ_3,\n\targ_4,\n\t...\n):\n\t...\n\nbar = (\n    part_1\n    + part_2\n    + (part_3 - part4)\n    - part_5\n    - part_6\n)\n```\n*Notes*:\n- Add a level of indent to clearly distinguish hanging indents.\n- Split before the first arguemnt in hanging indents.\n- Start a new line for the closing parenthesis, brackets, braces.\n- Dedent the closing parenthesis, brackets, braces.\n- Split a long line before operators and after commas.\n\n### **Blank Lines**\n- Leave two blank lines between:\n\t- top-level functions and classes.\n- Leave One blank line between\n\t- functions defined inside classes.\n\t- logical sections inside a function.\n- Leave one blank line at the end of a python file\n```python\nclass Foo():\n    def foo_1():\n        ...\n\n    def foo_2():\n        ...\n\n\ndef foo():\n    # This is the first logical part.\n    ...\n\n    # This is the second logical part.\n    ...\n\n\ndef bar():\n    ...\n\n```\n\n### **Imports**\nPlace Imports at the top of the file, right after module comments and docstrings, before globals and constants.\n```python\nimport std_lib_A\nimport std_lib_B\n\nimport third_party_lib_A\nimport third_party_lib_B\n\nfrom std_lib_C import object_a, object_b\nfrom std_lib_D import (\n    object_c,\n\tobject_d,\n\tobject_e,\n\tobject_f,\n    object_g,\n\tobject_h,\n\tobject_i,\n\tobject_j\n)\n```\n*Notes*:\n- Organize the imports in following order:\n\t1. Standard library imports\n\t2. Related third party imports\n\t3. Local application/library specific imports\n- Import one library in per line.\n- It's ok to import several objects from one library in one line, but when the number of objects is more the 7, use hanging indents.  \n- Leave one blank line between different groups.\n- Use absolute imports, avoid wildcard imports:\n```python\n# It's best to use absolute imports.\nimport some_module.some_object\nfrom some_module import some_object\n\n# Avoid explicit relative imports if possible.\nfrom . import sibling_module\nfrom .sibling_module import sibling_obejct\n\n# Avoid using wildcard imports.\nfrom some_module import *\n```\n\n### **Whitespace**\n#### **Avoid unnecessary whitespace in the following cases:**\n- Immediately after or before parentheses, brackets or braces:\n```python\n# Correct:\nexample(list[1], {key: 2})\ndct['key'] = lst[index]\n\n# Wrong:\nexample ( list[ 1 ], { key: 2 } )\ndct [ 'key' ] = lst [ index ]\n```\n- Between a trailing comma and a following close parenthesis:\n```python\n# Correct:\nfoo = (0,)\n\n# Wrong:\nbar = (0, )\n```\n- Immediately before a comma, semicolon, or colon, but leave a space after them unless followed by parentheses, brackets or braces:\n```python\n# Correct:\nif x == 4:\n    print x, y; x, y = y, x\n\n# Wrong:\nif x == 4 :\n    print x , y ;x , y = y , x\n```\n- The end of a line.\n\n#### **Do use whitespace in following cases:**\n- Surround following operators with a single space on either side:\n\t- assignment (=),\n\t- augmented assignment (+=, -=, etc.)\n\t- comparisons (==, <, >, !=, <>, <=, >=, in, not in, is, is not)\n\t- booleans (and, or, not).\n- Surround operators with lowest priority with a single space on either side:\n```python\n# Correct:\ni = i + 1\n\ni += 1\n\nx = x*2 - 1\n\nhypot2 = x*x + y*y\n\nc = (a+b) * (a-b)\n\nif a and b:\n    ...\n\nif a == b:\n    ...\n\nfor i in range(10):\n    ...\n```\n- Function annotations, after : and surround -> :\n```python\n# Correct:\ndef foo(bar: str) -> int:\n    ...\n```\n\n### **Annotation**\nAdd annotation for all functions.\n```python\nfrom Typing import Set, Dict, List, Tuple, Any\n\ndef foo(a: Set(str), b: List[int], c: Dict[str, Any]) -> Tuple(int, str):\n    ...\n    return return_int, return_str\n```\n*Notes*:\n- Use Set, Dict, List, Tuple instead of set, dict, list, tuple in annotation, also specify the data type inside unless in complicated nested cases.\n\n*Special case*:\n- If we want to return a Foo object inside of the Foo class, use string to annotate the return type:\n```python\nClass Foo():\n\t...\n\n    @classmethod\n    def initializera(args) -> 'Foo':\n        ...\n        return Foo_instance\n```\n\n### **Comments and Docstring**\n#### **Comments**\n- All comments and docstrings should be composed of sentences, not phrases:\n```python\n# Correct:   \n# My code does this.\n\n# Wrong:  \n# my code does this\n```\n- Use block comments rather than inline comments. Block comments should apply to some (or all) code that follows them, and are supposed to indented to the same level as the code they are applied.\n```python\n# Correct:\n# The Following code does this.\n...\n\n#Worng:\n... # The code does this\n```\n**Docstring**\n- Write dostring for every function and class according to the following template, unless the function or the class is:\n\t- Externally invisible\n\t- Very short\n\t- Obvious\n\nTemplate：\n```python\n\"\"\"\nBrief introduction within one line.\n\nDetailed description, paragraph 1...\n\nDetailed description, paragraph 2 ...\n\nArgs:\n    arg0 (`int`):\n        arg0 description.\n    arg1 (`Union[float, str]`):\n        arg1 description.\n\nReturns:\n    (`Tuple[bool, int]`)\n        Return value description.\n\nRaises:\n    (`xxxError`)\n        Raise xxxError when ....\n\"\"\"\n```\n*Notes*:\n- Start a new line for a brief introduction, do not add an introduction right after the opening \"\"\".\n- Use `` (the symbol in the left of 1), not '' when declare the types.\n- If needed, add some detailed description after the introduction separated by one blank line.\n- If there is more than one paragraph in the detailed description, separate them with one blank line.\n- If a function does not have Args, Returns or Raises, do not add them in the docstring.\n- In Args, add a colon (':') after (\\`type\\`), leave whitespace before (\\`type\\`) .\n- In Returns, Raises, only list the return type and error types, as (\\`return/error type\\`), no colon(':') needed.\n- Put the docstrings for a class's `__init__` function at the beginning of the class definition:\n```python\nclass Foo():\n    \"\"\"\n    Introduction to the Foo class...\n\n\tDetailed description...\n\n    Args:\n        args_0 (`type`):\n        ...\n    \"\"\"\n    def __init__(args):\n        ...\n```\n\nA detailed exmaple:\n```python\ndef __init__(\n    arg_1: int, arg_2: list, arg_3: dict,\n    arg_4: float, arg_5: bool, arg_6: tuple\n) -> Tuple[int, float, str]:\n\"\"\"\nA one line summary.\n\nThis is a complicated function so we need multi-line docstring, this function does something.\n\nFunctions that do not have return values, raise errors, or need examples can omit the sections below.\n\nArgs:\n    arg_1 (`str`):\n        ...\n    arg_2 (`List[float]`):\n        ...\n    arg_3 (`Dict[int, str]`):\n        ...\n    ...\n\nReturns:\n    (`Tuple[int, float, str]`)\n        A tuple with three values, first being int...\n\nRaises:\n    (`IOError`)\n        Throw an IOError when...\n\"\"\"\n```\n\n*Special case*：\n- Return a Foo object inside Foo class:\n```python\nClass Foo():\n    @classmethod\n    def generate(args) -> 'Foo':\n        \"\"\"\n        Short intro.\n\n        Detailed Description.\n\n        Args:\n            ...\n\n        Returns:\n            (`path/to/Foo`)\n                Returns a Foo instance.\n        \"\"\"\n        return Foo_instance\n```\n\n### **Naming Conventions**\n- Variable and function names should be lowercase and connected with underscore if necessary:\n```python\nfoo = 1\ndef foo_bar() -> None:\n\t...\n```\n- Uppercase for Constants:\n```python\nMY_CONSTANT\n```\n- CapWords for class name:\n```python\nclass MyClass(object):\n\t...\n```\n- Use one leading underscore only for non-public methods and instance variables:\n```python\ndef _private_function() -> None:\n\t...\n```\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.0908203125,
          "content": "codecov:\n  require_ci_to_pass: no\n  notify:\n    require_ci_to_pass: no\n    wait_for_ci: false"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "commit",
          "content": null
        },
        {
          "name": "pylint.conf",
          "type": "blob",
          "size": 14.2880859375,
          "content": "# This Pylint rcfile contains a best-effort configuration to uphold the\n# best-practices and style described in the Google Python style guide:\n#   https://google.github.io/styleguide/pyguide.html\n#\n# Its canonical open-source location is:\n#   https://google.github.io/styleguide/pylintrc\n\n[MASTER]\n\n# Files or directories to be skipped. They should be base names, not paths.\nignore=third_party\n\n# Files or directories matching the regex patterns are skipped. The regex\n# matches against base names, not paths.\nignore-patterns=\n\n# Pickle collected data for later comparisons.\npersistent=no\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Use multiple processes to speed up Pylint.\njobs=4\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED\nconfidence=\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).You can also use \"--disable=all\" to\n# disable everything first and then reenable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use\"--disable=all --enable=classes\n# --disable=W\"\ndisable=abstract-method,\n        apply-builtin,\n        arguments-differ,\n        attribute-defined-outside-init,\n        backtick,\n        bad-option-value,\n        basestring-builtin,\n        buffer-builtin,\n        c-extension-no-member,\n        consider-using-enumerate,\n        cmp-builtin,\n        cmp-method,\n        coerce-builtin,\n        coerce-method,\n        delslice-method,\n        div-method,\n        duplicate-code,\n        eq-without-hash,\n        execfile-builtin,\n        file-builtin,\n        filter-builtin-not-iterating,\n        fixme,\n        getslice-method,\n        global-statement,\n        hex-method,\n        idiv-method,\n        implicit-str-concat-in-sequence,\n        import-error,\n        import-self,\n        import-star-module-level,\n        inconsistent-return-statements,\n        input-builtin,\n        intern-builtin,\n        invalid-str-codec,\n        locally-disabled,\n        long-builtin,\n        long-suffix,\n        map-builtin-not-iterating,\n        misplaced-comparison-constant,\n        missing-function-docstring,\n        metaclass-assignment,\n        next-method-called,\n        next-method-defined,\n        no-absolute-import,\n        no-else-break,\n        no-else-continue,\n        no-else-raise,\n        no-else-return,\n        no-init,  # added\n        no-member,\n        no-name-in-module,\n        no-self-use,\n        nonzero-method,\n        oct-method,\n        old-division,\n        old-ne-operator,\n        old-octal-literal,\n        old-raise-syntax,\n        parameter-unpacking,\n        print-statement,\n        raising-string,\n        range-builtin-not-iterating,\n        raw_input-builtin,\n        rdiv-method,\n        reduce-builtin,\n        relative-import,\n        reload-builtin,\n        round-builtin,\n        setslice-method,\n        signature-differs,\n        standarderror-builtin,\n        suppressed-message,\n        sys-max-int,\n        too-few-public-methods,\n        too-many-ancestors,\n        too-many-arguments,\n        too-many-boolean-expressions,\n        too-many-branches,\n        too-many-instance-attributes,\n        too-many-locals,\n        too-many-nested-blocks,\n        too-many-public-methods,\n        too-many-return-statements,\n        too-many-statements,\n        trailing-newlines,\n        unichr-builtin,\n        unicode-builtin,\n        unnecessary-pass,\n        unpacking-in-except,\n        useless-else-on-loop,\n        useless-object-inheritance,\n        useless-suppression,\n        using-cmp-argument,\n        wrong-import-order,\n        xrange-builtin,\n        zip-builtin-not-iterating,\n        missing-module-docstring,\n        super-init-not-called,\n\n\n[REPORTS]\n\n# Set the output format. Available formats are text, parseable, colorized, msvs\n# (visual studio) and html. You can also give a reporter class, eg\n# mypackage.mymodule.MyReporterClass.\noutput-format=text\n\n# Put messages in a separate file for each module / package specified on the\n# command line instead of printing them on stdout. Reports (if any) will be\n# written in a file name \"pylint_global.[txt|html]\". This option is deprecated\n# and it will be removed in Pylint 2.0.\nfiles-output=no\n\n# Tells whether to display a full report or only the messages\nreports=no\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\n#msg-template=\n\n\n[BASIC]\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=main,_\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Include a hint for the correct naming format with invalid-name\ninclude-naming-hint=no\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\nproperty-classes=abc.abstractproperty,cached_property.cached_property,cached_property.threaded_cached_property,cached_property.cached_property_with_ttl,cached_property.threaded_cached_property_with_ttl\n\n# Regular expression matching correct function names\nfunction-rgx=^(?:(?P<exempt>setUp|tearDown|setUpModule|tearDownModule)|(?P<camel_case>_?[A-Z][a-zA-Z0-9]*)|(?P<snake_case>_?[a-z][a-z0-9_]*))$\n\n# Regular expression matching correct variable names\nvariable-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct constant names\nconst-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct attribute names\nattr-rgx=^_{0,2}[a-z][a-z0-9_]*$\n\n# Regular expression matching correct argument names\nargument-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class attribute names\nclass-attribute-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct inline iteration names\ninlinevar-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class names\nclass-rgx=^_?[A-Z][a-zA-Z0-9]*$\n\n# Regular expression matching correct module names\nmodule-rgx=^(_?[a-z][a-z0-9_]*|__init__|__main__)$\n\n# Regular expression matching correct method names\nmethod-rgx=(?x)^(?:(?P<exempt>_[a-z0-9_]+__|runTest|setUp|tearDown|setUpTestCase|tearDownTestCase|setupSelf|tearDownClass|setUpClass|(test|assert)_*[A-Z0-9][a-zA-Z0-9_]*|next)|(?P<camel_case>_{0,2}[A-Z][a-zA-Z0-9_]*)|(?P<snake_case>_{0,2}[a-z][a-z0-9_]*))$\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=(__.*__|main|test.*|.*test|.*Test)$\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=10\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager,contextlib2.contextmanager\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis. It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n\n[FORMAT]\n\n# Maximum number of characters on a single line.\nmax-line-length=150\n\n# TODO(https://github.com/PyCQA/pylint/issues/3352): Direct pylint to exempt\n# lines made too long by directives to pytype.\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=(?x)(\n  ^\\s*(\\#\\ )?<?https?://\\S+>?$|\n  ^\\s*(from\\s+\\S+\\s+)?import\\s+.+$)\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=yes\n\n# List of optional constructs for which whitespace checking is disabled. `dict-\n# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\\n222: 2}.\n# `trailing-comma` allows a space between comma and closing bracket: (a, ).\n# `empty-line` allows space-only lines.\nno-space-check=\n\n# Maximum number of lines in a module\nmax-module-lines=99999\n\n# String used as indentation unit.  The internal Google style guide mandates 2\n# spaces.  Google's externaly-published style guide says 4, consistent with\n# PEP 8.  Here, we use 2 spaces, for conformity with many open-sourced Google\n# projects (like TensorFlow).\nindent-string='    '\n\n# Number of spaces of indent required inside a hanging  or continued line.\nindent-after-paren=4\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=TODO\n\n\n[STRING]\n\n# This flag controls whether inconsistent-quotes generates a warning when the\n# character used as a quote delimiter is used inconsistently within a module.\ncheck-quote-consistency=yes\n\n\n[VARIABLES]\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# A regular expression matching the name of dummy variables (i.e. expectedly\n# not used).\ndummy-variables-rgx=^\\*{0,2}(_$|unused_|dummy_)\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,_cb\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six,six.moves,past.builtins,future.builtins,functools\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format\nlogging-modules=logging,absl.logging,tensorflow.io.logging\n\n\n[SIMILARITIES]\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n\n[SPELLING]\n\n# Spelling dictionary name. Available dictionaries: none. To make it working\n# install python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[IMPORTS]\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=regsub,\n                   TERMIOS,\n                   Bastion,\n                   rexec,\n                   sets\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant, absl\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,\n                  _fields,\n                  _replace,\n                  _source,\n                  _make\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls,\n                            class_\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=StandardError,\n                       Exception,\n                       BaseException\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.103515625,
          "content": "requests>=2.12.5\ntqdm>=4.59.0\ntabulate\nnumpy\ntwine\ntenacity\ncontextvars; python_version <= '3.6'\npydantic\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 1.025390625,
          "content": "[metadata]\nname = towhee\nversion = 1.1.3\ndescription = Towhee is a framework that helps you encode your unstructured data into embeddings.\nlong_description = file: README.md\nurl = https://github.com/towhee-io/towhee\nauthor = Towhee Team\nauthor_email=towhee-team@zilliz.com\nlicense = Apache License Version 2.0\nlicense_file = LICENSE\nplatforms = unix, linux, osx, win32\n\n[build_sphinx]\nproject = towhee\nversion = 1.1\nrelease = 1.1.3\nsource-dir = towhee/doc/source\n\n# setup.cfg\n[tool:pytest]\nminversion = 6.0\naddopts = -ra -q --durations=5 --doctest-modules --doctest-glob=*.md --ignore=tests/unittests/test_cache/ --ignore=tests/unittests/triton/expected_files/\nlog_cli = true\ntestpaths =\n    tests/unittests/\n    towhee/functional/\n    towhee/hparam/\n    towhee/types/\n    towhee/engine/\n    towhee/hub/\n    towhee/operator/\n    towhee/datacollection/\n\n[coverage:run]\nbranch = true\nsource = towhee\n\n[coverage:report]\n# show_missing = True\n# skip_covered = True\n# Ignore missing source files, i.e. fake template-generated \"files\"\nignore_errors = true\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.1142578125,
          "content": "# Copyright 2021 Zilliz. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport unittest\nfrom pathlib import Path\nfrom typing import List\n\nfrom setuptools import find_packages, setup\nfrom setuptools.command.install import install\n\n\ndef test_suite():\n    test_loader = unittest.TestLoader()\n    test_suite = test_loader.discover('towhee/tests', pattern='test_*.py')\n    return test_suite\n\n\ndef parse_requirements(file_name: str) -> List[str]:\n    with open(file_name) as f:\n        return [\n            require.strip() for require in f\n            if require.strip() and not require.startswith('#')\n        ]\n\nif '--models' in sys.argv:\n    sys.argv.remove('--models')\n    setup(name='towhee.models',\n          version='1.1.3',\n          description='',\n          author='Towhee Team',\n          author_email='towhee-team@zilliz.com',\n          use_scm_version={'local_scheme': 'no-local-version'},\n          setup_requires=['setuptools_scm'],\n          url='https://github.com/towhee-io/towhee',\n          test_suite='setup.test_suite',\n          install_requires=parse_requirements('requirements.txt'),\n          extras_require={':python_version<\\'3.7\\'': 'importlib-resources'},\n          tests_require=parse_requirements('test_requirements.txt'),\n          packages=find_packages(include=['towhee.models*']),#['towhee.models'],\n          package_data={'towhee.tests.test_util': ['*.yaml']},\n          namespace_package = ['towhee'],\n          include_package_data=True,\n          license='http://www.apache.org/licenses/LICENSE-2.0',\n          entry_points={\n              'console_scripts': ['towhee=towhee.command.cmdline:main'],\n          },\n          long_description_content_type='text/markdown'\n          )\nelse:\n    setup(use_scm_version={'local_scheme': 'no-local-version'},\n          setup_requires=['setuptools_scm'],\n          test_suite='setup.test_suite',\n          install_requires=parse_requirements('requirements.txt'),\n          extras_require={':python_version<\\'3.7\\'': 'importlib-resources'},\n          tests_require=parse_requirements('test_requirements.txt'),\n          packages=find_packages(exclude=['*test*', 'towhee.models*']),\n          namespace_package=['towhee'],\n          package_data={'towhee.tests.test_util': ['*.yaml'], 'towhee.serve.triton.dockerfiles': ['*']},\n          license='http://www.apache.org/licenses/LICENSE-2.0',\n          entry_points={\n              'console_scripts': [\n                  'towhee=towhee.command.cmdline:main',\n                  'triton_builder=towhee.serve.triton.pipeline_builder:main',\n              ],\n          },\n          long_description_content_type='text/markdown'\n          )\n"
        },
        {
          "name": "test_requirements.txt",
          "type": "blob",
          "size": 0.3291015625,
          "content": "torch>=1.8.0\ntorchvision>=0.9.0\nnumpy>=1.19.5\nrequests>=2.12.5\ntqdm>=4.59.0\ntorchmetrics==0.7.0\nimportlib_metadata\ntabulate\npytest\nfairscale\npython-multipart\ntransformers>=4.19.2\neinops>=0.4.1\ntimm>=0.4.12\nPyWavelets\ntwine\ndill\nonnx\nscipy\ncontextvars; python_version <= '3.6'\ntenacity\npydantic<2\nhttpx\ntritonclient[http]==2.32.0\nfastapi\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "towhee",
          "type": "tree",
          "content": null
        },
        {
          "name": "towhee_logo.png",
          "type": "blob",
          "size": 53.40234375,
          "content": null
        }
      ]
    }
  ]
}