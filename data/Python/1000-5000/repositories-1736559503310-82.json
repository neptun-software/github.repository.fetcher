{
  "metadata": {
    "timestamp": 1736559503310,
    "page": 82,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "DeepLabCut/DeepLabCut",
      "stars": 4781,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".codespellrc",
          "type": "blob",
          "size": 0.21875,
          "content": "[codespell]\nskip = .git,*.pdf,*.svg,deeplabcut/pose_estimation_tensorflow/models/pretrained\n# MOT,SIE - legit acronyms\n# tThe - for \\tThe. codespell is not good detecting those yet\nignore-words-list = mot,sie,tthe, assertIn\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.484375,
          "content": "# Docker specific\nlogs/\n#Jupyter book build directory\n_build/*\n#Data and examples\n/examples/open*\n/examples/Reac*\n/examples/TES*\n/examples/multi*\n/examples/3D*\n/examples/m3*\n/examples/OUT\n.local\n.DS_Store\nexamples/.DS_Store\n*~\n# Tensorflow checkpoints\n*.ckpt\nsnapshot-*\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\n#lib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log*.ckpt\nsnapshot-*\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n.vscode/*\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 6.513671875,
          "content": "DeepLabCut (www.deeplabcut.org) was initially developed by\nAlexander & Mackenzie Mathis in collaboration with Matthias Bethge in 2017.\nIt is actively developed by Alexander & Mackenzie Mathis (steering council and owners).\n\nDeepLabCut is an open-source tool and has benefited from suggestions and edits by many\nindividuals: DeepLabCut/graphs/contributors\n\n############################################################################################################\n\nDeepLabCut 1.0 Toolbox\nA Mathis, alexander.mathis@bethgelab.org | https://github.com/DeepLabCut/DeepLabCut\nM Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab\n\nSpecific external contributors:\nE Insafutdinov and co-authors of DeeperCut (see README) for feature detectors: https://github.com/eldar\n- Thus, code in this subdirectory at the time of April 2018, deeplabcut/pose_estimation_tensorflow\nwas adapted from: https://github.com/eldar/pose-tensorflow.\n\nProducts:\nDeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Nature Neuroscience, 2018.\nhttps://doi.org/10.1038/s41593-018-0209-y\nA. Mathis, P. Mamidanna, K.M. Cury, T. Abe, V.N. Murthy, M.W. Mathis* & M. Bethge*\n\nContributions:\nConceptualization: A.M., M.W.M. and M.B.\nSoftware: A.M. and M.W.M.\nFormal analysis: A.M.\nExperiments: A.M. and V.N.M. (trail-tracking), M.W.M. (mouse reaching), K.M.C. (Drosophila).\nImage Labeling: P.M., K.M.C., T.A., M.W.M., A.M.\nWriting: A.M. and M.W.M. with input from all authors.\nThese authors jointly directed this work: M. Mathis, M. Bethge\n\n############################################################################################################\n\nDeepLabCut 2.0 Toolbox\nA Mathis, alexander.mathis@bethgelab.org | https://github.com/DeepLabCut/DeepLabCut\nT Nath, nath@rowland.harvard.edu | https://github.com/meet10may\nM Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab\n\nProducts:\nUsing DeepLabCut for 3D markerless pose estimation across species and behaviors. Nature Protocols, 2019.\nhttps://www.nature.com/articles/s41596-019-0176-0\nT. Nath*, A. Mathis*, AC. Chen, A. Patel, M. Bethge, M. Mathis\n\nContributions:\nConceptualization: AM, TN, MWM.\nSoftware: AM, TN and MWM.\nDataset (cheetah): AP.\nImage Labeling: ACC.\nFormal analysis: ACC, AM and AP analyzed the cheetah data.\nWriting: MWM, AM and TN with inputs from all authors.\n\n############################################################################################################\n\nDeepLabCut 2.1 major additions:\nA Mathis, alexander.mathis@bethgelab.org | https://github.com/DeepLabCut/DeepLabCut\nT Nath, nath@rowland.harvard.edu | https://github.com/meet10may\nM Yüksekgönül, mertyuksekgonul@gmail.com | https://github.com/mertyg\nM Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab\n\nSpecific external contributors:\nTensorpack augmentation: https://github.com/DeepLabCut/DeepLabCut/pull/409 by Katie Rupp\n\nProducts:\nPretraining boosts out-of-domain robustness for pose estimation. WACV, 2021.\nhttp://www.mackenziemathislab.org/horse10\nA. Mathis, T. Biasi, S. Schneider, M. Yüksekgönül, B. Rogers, M. Bethge, M. Mathis\n\n############################################################################################################\n\nDeepLabCut 2.1 - 2.2 additions:\nA Mathis, alexander.mathis@epfl.ch | https://github.com/AlexEMG\nJ Lauer, jessy@deeplabcut.org | https://github.com/jeylau\nM Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab\nM Zhou, https://github.com/zhoumu53\nS Ye, https://github.com/yeshaokai\nS Schneider, https://github.com/stes\nT Biasi, https://github.com/tbiasi\nG Kane, https://github.com/gkane26\nM Yüksekgönül, https://github.com/mertyg\nT Nath, https://github.com/meet10may\n\nPreprint:\nMulti-animal pose estimation and tracking with DeepLabCut\nJ Lauer,  M Zhou,  S Ye,  W Menegas,  S Schneider, T Nath,  MM Rahman, V Di Santo,\nD Soberanes, G Feng, VN Murthy, G Lauder, C Dulac,  M Mathis, A Mathis (2021).\nhttps://www.biorxiv.org/content/10.1101/2021.04.30.442096v1\n\nPublication:\nMulti-animal pose estimation, identification and tracking with DeepLabCut\nLauer, J., Zhou, M., Ye, S., Menegas, W., Schneider, S., Nath, T., Rahman, M.M.,\nDi Santo, V., Soberanes, D., Feng, G., Murthy, V.N., Lauder, G.V., Dulac, C.,\nMathis, M.W., & Mathis, A. (2022).\nNature Methods, 19, 496 - 504.\n\nConceptualization was done by A.M. and M.W.M. Formal analysis and code were done by J.L., A.M. and M.W.M.\nNew deep architectures were designed by M.Z., S.Y. and A.M. GUIs were done by J.L., M.W.M. and T.N.\nBenchmark was set by S.S., M.W.M., A.M. and J.L. Marmoset data were gathered by W.M. and G.F.\nMarmoset behavioral analysis was carried out by W.M. Parenting data were gathered by M.M.R., A.M. and C.D.\nTri-mouse data were gathered by D.S., A.M. and V.N.M. Fish data were gathered by V.D.S. and G.L.\nThe article was written by A.M., M.W.M. and J.L. with input from all authors.\nM.W.M. and A.M. co-supervised the project.\n\n############################################################################################################\n\nDeepLabCut 2.2 - 3.0 additions:\nA Mathis, alexander.mathis@epfl.ch | https://github.com/AlexEMG\nM Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab\nJ Lauer, jessy@deeplabcut.org | https://github.com/jeylau\nN Poulsen, neils.poulsen@epfl.ch | https://github.com/n-poulsen\nS Schneider, stes@hey.com | https://github.com/stes\nS Ye, shaokai.ye@epfl.ch | https://github.com/yeshaokai\n\nPreprint:\nYe, S., Filippova, A., Lauer, J., Schneider, S., Vidal, M., Qiu, T., Mathis, A., & Mathis, M.W. (2023).\nSuperAnimal pretrained pose estimation models for behavioral analysis. https://arxiv.org/abs/2203.07436\n\n\n############################################################################################################\n\nDeepLabCut 3.0 Toolbox\nM Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab\nA Mathis, alexander.mathis@epfl.ch | https://github.com/AlexEMG\nN Poulsen, neils.poulsen@epfl.ch | https://github.com/n-poulsen\nS Ye, shaokai.ye@epfl.ch | https://github.com/yeshaokai\nA Filippova, anastasiia.filippova@epfl.ch | https://github.com/nastya236\nQ Macé | https://github.com/QuentinJGMace\nJ Lauer, jessy@deeplabcut.org | https://github.com/jeylau\nL Stoffl, lucas.stoffl@epfl.ch | https://github.com/LucZot\n\nWe also greatly thank the 2023 DeepLabCut AI Residents who contributed:\nAnna Teruel-Sanchis | https://github.com/anna-teruel\nRiza Rae Pineda | https://github.com/rizarae-p\nKonrad Danielewski | https://github.com/KonradDanielewski\n\nProducts:\nPyTorch backend for DeepLabCut\nExpanded SuperAnimal capabilities\nNew model architectures (WIP: stay tuned, but includes BUCTD)\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.2763671875,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socioeconomic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at alexander.mathis@epfl.ch. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5.1484375,
          "content": "# How to Contribute to DeepLabCut\n\nDeepLabCut is an actively developed package and we welcome community development and involvement. We are especially seeking people from underrepresented backgrounds in OSS to contribute their expertise and experience. Please get in touch if you want to discuss specific contributions you are interested in developing, and we can help shape a road-map.\n\nWe are happy to receive code extensions, bug fixes, documentation updates, etc.\n\nIf you are a new user, we recommend checking out the detailed [Github Guides](https://guides.github.com).\n\n## Setting up a development installation\n\nIn order to make changes to `deeplabcut`, you will need to [fork](https://guides.github.com/activities/forking/#fork) the\n[repository](https://github.com/deeplabcut/deeplabcut).\n\nIf you are not familiar with `git`, we recommend reading up on [this guide](https://guides.github.com/introduction/git-handbook/#basic-git).\n\nHere are guidelines for installing deeplabcut locally on your own computer, where you can make changes to the code! We often update the master deeplabcut code base on github, and then ~1 a month we push out a stable release on pypi. This is what most users turn to on a daily basis (i.e. pypi is where you get your `pip install deeplabcut` code from! \n\nBut, sometimes we add things to the repo that are not yet integrated, or you might want to edit the code yourself, or you will need to do this to contribute. Here, we show you how to do this. \n\n**Step 1:**\n\n- git clone the repo into a folder on your computer:  \n\n- click on this green button and copy the link:\n\n![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581984907363-G8AFGX4V20Y1XD1PSZAK/ke17ZwdGBToddI8pDm48kGJBV0_F4LE4_UtCip_K_3lZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVE0ejQCe16973Pm-pux3j5_Oqt57D2H0YbaJ3tl8vn_eR926scO3xePJoa6uVJa9B4/gitclone.png?format=500w)\n\n- then in the terminal type: `git clone https://github.com/DeepLabCut/DeepLabCut.git`\n\n**Step 2:**\n\n- Now you will work from the terminal inside this cloned folder:\n\n![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581985288123-V8XUAY0C0ZDNJ5WBHB7Y/ke17ZwdGBToddI8pDm48kIsGBOdR9tS_SxF6KQXIcDtZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpz3c8X74DzCy4P3pv-ZANOdh-3ZL9iVkcryTbbTskaGvEc42UcRKU-PHxLXKM6ZekE/terminal.png?format=750w)\n\n- Now, when you start `ipython` and `import deeplabcut` you are importing the folder \"deeplabcut\" - so any changes you make, or any changes we made before adding it to the pip package, are here.\n\n- You can also check which deeplabcut you are importing by running: `deeplabcut.__file__`\n\n![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581985466026-94OCSZJ5TL8U52JLB5VU/ke17ZwdGBToddI8pDm48kNdOD5iqmBzHwUaWGKS6qHBZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyQPoegsR7K4odW9xcCi1MIHmvHh95_BFXYdKinJaRhV61R4G3qaUq94yWmtQgdj1A/importlocal.png?format=750w)\n\nIf you make changes to the code/first use the code, be sure you run `./resinstall.sh`, which you find in the main DeepLabCut folder:\n\n![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1609353210708-FRNREI7HUNS4GLDSJ00G/ke17ZwdGBToddI8pDm48kAya1IcSd32bok4WHvykeicUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dq18t0tDkB2HMfL2JGcLHN27k5rSOPIU8nEAZT0p1MiSCjLISwBs8eEdxAxTptZAUg/Screen+Shot+2020-12-30+at+7.33.16+PM.png?format=2500w)\n\n\n\nNote, before committing to DeepLabCut, please be sure your code is formatted according to `black`. To learn more,\nsee [`black`'s documentation](https://black.readthedocs.io/en/stable/).\n\nNow, please make a [pull request](https://github.com/DeepLabCut/DeepLabCut/pull/new/) that includes both a **summary of and changes to**:\n\n- How you modified the code and what new functionality it has.\n- DOCSTRING update for your change\n- A working example of how it works for users. \n- If it's a function that also can be used in downstream steps (i.e. could be plotted) we ask you (1) highlight this, and (2) ideally you provide that functionality as well. If you have any questions, please reach out: admin@deeplabcut.org \n\n**TestScript outputs:**\n\n- The **OS it has been tested on**\n- the **output of the [testscript.py](/examples/testscript.py)** and if you are editing the **3D code the [testscript_3d.py](/examples/testscript_3d.py)**, and if you edit multi-animal code please run the [maDLC test script](https://github.com/DeepLabCut/DeepLabCut/blob/master/examples/testscript_multianimal.py).\n\n**Review & Formatting:**\n\n- Please run black on the code to conform to our Black code style (see more at https://pypi.org/project/black/). \n- Please assign a reviewer, typically @AlexEMG, @mmathislab, or @jeylau (i/e. the [core-developers](https://github.com/orgs/DeepLabCut/teams/core-developers/members))\n\n**Code headers**\n\n- The code headers can be standardized by running `python tools/update_license_headers.py`\n- Edit `NOTICE.yml` to update the header. \n\n**DeepLabCut is an open-source tool and has benefited from suggestions and edits by many individuals:**\n\n- the [authors](/AUTHORS)\n- [code contributors](https://github.com/DeepLabCut/DeepLabCut/graphs/contributors) \n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 7.47265625,
          "content": "                   GNU LESSER GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n\n  This version of the GNU Lesser General Public License incorporates\nthe terms and conditions of version 3 of the GNU General Public\nLicense, supplemented by the additional permissions listed below.\n\n  0. Additional Definitions.\n\n  As used herein, \"this License\" refers to version 3 of the GNU Lesser\nGeneral Public License, and the \"GNU GPL\" refers to version 3 of the GNU\nGeneral Public License.\n\n  \"The Library\" refers to a covered work governed by this License,\nother than an Application or a Combined Work as defined below.\n\n  An \"Application\" is any work that makes use of an interface provided\nby the Library, but which is not otherwise based on the Library.\nDefining a subclass of a class defined by the Library is deemed a mode\nof using an interface provided by the Library.\n\n  A \"Combined Work\" is a work produced by combining or linking an\nApplication with the Library.  The particular version of the Library\nwith which the Combined Work was made is also called the \"Linked\nVersion\".\n\n  The \"Minimal Corresponding Source\" for a Combined Work means the\nCorresponding Source for the Combined Work, excluding any source code\nfor portions of the Combined Work that, considered in isolation, are\nbased on the Application, and not on the Linked Version.\n\n  The \"Corresponding Application Code\" for a Combined Work means the\nobject code and/or source code for the Application, including any data\nand utility programs needed for reproducing the Combined Work from the\nApplication, but excluding the System Libraries of the Combined Work.\n\n  1. Exception to Section 3 of the GNU GPL.\n\n  You may convey a covered work under sections 3 and 4 of this License\nwithout being bound by section 3 of the GNU GPL.\n\n  2. Conveying Modified Versions.\n\n  If you modify a copy of the Library, and, in your modifications, a\nfacility refers to a function or data to be supplied by an Application\nthat uses the facility (other than as an argument passed when the\nfacility is invoked), then you may convey a copy of the modified\nversion:\n\n   a) under this License, provided that you make a good faith effort to\n   ensure that, in the event an Application does not supply the\n   function or data, the facility still operates, and performs\n   whatever part of its purpose remains meaningful, or\n\n   b) under the GNU GPL, with none of the additional permissions of\n   this License applicable to that copy.\n\n  3. Object Code Incorporating Material from Library Header Files.\n\n  The object code form of an Application may incorporate material from\na header file that is part of the Library.  You may convey such object\ncode under terms of your choice, provided that, if the incorporated\nmaterial is not limited to numerical parameters, data structure\nlayouts and accessors, or small macros, inline functions and templates\n(ten or fewer lines in length), you do both of the following:\n\n   a) Give prominent notice with each copy of the object code that the\n   Library is used in it and that the Library and its use are\n   covered by this License.\n\n   b) Accompany the object code with a copy of the GNU GPL and this license\n   document.\n\n  4. Combined Works.\n\n  You may convey a Combined Work under terms of your choice that,\ntaken together, effectively do not restrict modification of the\nportions of the Library contained in the Combined Work and reverse\nengineering for debugging such modifications, if you also do each of\nthe following:\n\n   a) Give prominent notice with each copy of the Combined Work that\n   the Library is used in it and that the Library and its use are\n   covered by this License.\n\n   b) Accompany the Combined Work with a copy of the GNU GPL and this license\n   document.\n\n   c) For a Combined Work that displays copyright notices during\n   execution, include the copyright notice for the Library among\n   these notices, as well as a reference directing the user to the\n   copies of the GNU GPL and this license document.\n\n   d) Do one of the following:\n\n       0) Convey the Minimal Corresponding Source under the terms of this\n       License, and the Corresponding Application Code in a form\n       suitable for, and under terms that permit, the user to\n       recombine or relink the Application with a modified version of\n       the Linked Version to produce a modified Combined Work, in the\n       manner specified by section 6 of the GNU GPL for conveying\n       Corresponding Source.\n\n       1) Use a suitable shared library mechanism for linking with the\n       Library.  A suitable mechanism is one that (a) uses at run time\n       a copy of the Library already present on the user's computer\n       system, and (b) will operate properly with a modified version\n       of the Library that is interface-compatible with the Linked\n       Version.\n\n   e) Provide Installation Information, but only if you would otherwise\n   be required to provide such information under section 6 of the\n   GNU GPL, and only to the extent that such information is\n   necessary to install and execute a modified version of the\n   Combined Work produced by recombining or relinking the\n   Application with a modified version of the Linked Version. (If\n   you use option 4d0, the Installation Information must accompany\n   the Minimal Corresponding Source and Corresponding Application\n   Code. If you use option 4d1, you must provide the Installation\n   Information in the manner specified by section 6 of the GNU GPL\n   for conveying Corresponding Source.)\n\n  5. Combined Libraries.\n\n  You may place library facilities that are a work based on the\nLibrary side by side in a single library together with other library\nfacilities that are not Applications and are not covered by this\nLicense, and convey such a combined library under terms of your\nchoice, if you do both of the following:\n\n   a) Accompany the combined library with a copy of the same work based\n   on the Library, uncombined with any other library facilities,\n   conveyed under the terms of this License.\n\n   b) Give prominent notice with the combined library that part of it\n   is a work based on the Library, and explaining where to find the\n   accompanying uncombined form of the same work.\n\n  6. Revised Versions of the GNU Lesser General Public License.\n\n  The Free Software Foundation may publish revised and/or new versions\nof the GNU Lesser General Public License from time to time. Such new\nversions will be similar in spirit to the present version, but may\ndiffer in detail to address new problems or concerns.\n\n  Each version is given a distinguishing version number. If the\nLibrary as you received it specifies that a certain numbered version\nof the GNU Lesser General Public License \"or any later version\"\napplies to it, you have the option of following the terms and\nconditions either of that published version or of any later version\npublished by the Free Software Foundation. If the Library as you\nreceived it does not specify a version number of the GNU Lesser\nGeneral Public License, you may choose any version of the GNU Lesser\nGeneral Public License ever published by the Free Software Foundation.\n\n  If the Library as you received it specifies that a proxy can decide\nwhether future versions of the GNU Lesser General Public License shall\napply, that proxy's public statement of acceptance of any version is\npermanent authorization for you to choose that version for the\nLibrary.\n\n"
        },
        {
          "name": "NOTICE.yml",
          "type": "blob",
          "size": 4.3955078125,
          "content": "# Main repository license\n- header: |\n    DeepLabCut Toolbox (deeplabcut.org)\n    © A. & M.W. Mathis Labs\n    https://github.com/DeepLabCut/DeepLabCut\n\n    Please see AUTHORS for contributors.\n    https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS\n\n    Licensed under GNU Lesser General Public License v3.0\n  include:\n    - 'deeplabcut/**/*.py'\n    - 'tests/**/*.py'\n    - 'examples/**/*.py'\n    - 'docs/**/*.py'\n    #- 'conda-environments/**/*.yaml'\n\n# License for files adapted from DeeperCut by Eldar Insafutdinov\n# https://github.com/eldar/pose-tensorflow\n# Applies to most files in deeplabcut.pose_estimation_tensorflow\n- header: |\n    DeepLabCut Toolbox (deeplabcut.org)\n    © A. & M.W. Mathis Labs\n    https://github.com/DeepLabCut/DeepLabCut\n\n    Please see AUTHORS for contributors.\n    https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS\n\n    Adapted from DeeperCut by Eldar Insafutdinov\n    https://github.com/eldar/pose-tensorflow\n\n    Licensed under GNU Lesser General Public License v3.0\n  include:\n    # This filelist was generated by running\n    # find deeplabcut/pose_estimation_tensorflow -iname '*.py' | xargs grep 'Eldar Insafutdinov'\n    # from the repo base directory.\n    - deeplabcut/pose_estimation_tensorflow/config.py\n    - deeplabcut/pose_estimation_tensorflow/datasets/factory.py\n    - deeplabcut/pose_estimation_tensorflow/vis_dataset.py\n    - deeplabcut/pose_estimation_tensorflow/core/train.py\n    - deeplabcut/pose_estimation_tensorflow/core/predict.py\n    - deeplabcut/pose_estimation_tensorflow/core/test.py\n    - deeplabcut/pose_estimation_tensorflow/default_config.py\n    - deeplabcut/pose_estimation_tensorflow/util/visualize.py\n    - deeplabcut/pose_estimation_tensorflow/util/__init__.py\n    - deeplabcut/pose_estimation_tensorflow/util/logging.py\n    - deeplabcut/pose_estimation_tensorflow/nnets/resnet.py\n    - deeplabcut/pose_estimation_tensorflow/__init__.py\n  exclude: []\n\n# Tensorflow licenses\n- header: |\n    Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n  include:\n    - deeplabcut/pose_estimation_tensorflow/backbones/*.py\n    - deeplabcut/pose_estimation_tensorflow/nnets/utils.py\n\n- header: |\n    Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n  include:\n    - deeplabcut/pose_estimation_tensorflow/nnets/conv_blocks.py\n    - deeplabcut/pose_estimation_tensorflow/backbones/mobilenet.py\n    - deeplabcut/pose_estimation_tensorflow/backbones/mobilenet_v2.py\n\n# TIMM license\n- header: |\n    Copyright 2019 Ross Wightman\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n    Hacked together by / Copyright 2020 Ross Wightman\n    https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py\n  include:\n    - deeplabcut/pose_tracking_pytorch/solver/scheduler_factory.py\n    - deeplabcut/pose_tracking_pytorch/model/backones/vit_pytorch.py\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 26.6728515625,
          "content": "<div align=\"center\">\n  \n\n<p align=\"center\">\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1628250004229-KVYD7JJVHYEFDJ32L9VJ/DLClogo2021.jpg?format=1000w\" width=\"95%\">\n</p>\n\n<p align=\"center\">\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1665060917309-V0YVY2UKVLKSS6O18XDI/MousereachGIF.gif?format=1000w?format=180w\" height=\"150\">\n\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/daed7f16-527f-4150-8bdd-cbb20e267451/cheetah-ezgif.com-video-to-gif-converter.gif?format=180w\" height=\"150\">\n\n\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1534797521117-EIEUED03C68241QZ4KCK/ke17ZwdGBToddI8pDm48kAx9qLOWpcHWRGxWsJQSczRZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwdr4GYy30vFzf31Oe7KAPZKkqgaiEgc5jBNdhZmDPlzxdkDSclo6ofuXZm6YCEhUo/MATHIS_2018_fly.gif?format=180w\" height=\"150\">\n\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1619609897110-TKSTWKEM6HTGXID9D489/ke17ZwdGBToddI8pDm48kAvjv6tW_eojYQmNU0ncbllZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVHBSTXHtjUKlhRtWJ1Vo6l1B2bxJtByvWSjL6Vz3amc5yb8BodarTVrzIWCp72ioWw/triMouseDLC.gif?format=180w\" height=\"150\">\n\n\n  \n\n\n\n[📚Documentation](https://deeplabcut.github.io/DeepLabCut/README.html) |\n[🛠️ Installation](https://deeplabcut.github.io/DeepLabCut/docs/installation.html) |\n[🌎 Home Page](https://www.deeplabcut.org) |\n[🐿🐴🐁🐘🐆 Model Zoo](http://www.mackenziemathislab.org/deeplabcut/) |\n[🚨 News](https://deeplabcut.github.io/DeepLabCut/README.html#news-and-in-the-news) |\n[🪲 Reporting Issues](https://github.com/DeepLabCut/DeepLabCut/issues) \n\n\n[🫶 Getting Assistance](https://deeplabcut.github.io/DeepLabCut/README.html#be-part-of-the-dlc-community) | \n[∞ DeepLabCut Online Course](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials/blob/master/DLCcourse.md) | \n[📝 Publications](https://deeplabcut.github.io/DeepLabCut/README.html#references) | \n[👩🏾‍💻👨‍💻 DeepLabCut AI Residency](https://www.deeplabcutairesidency.org/) \n\n\n![Vesion](https://img.shields.io/badge/python_version-3.10-purple)\n[![Downloads](https://pepy.tech/badge/deeplabcut)](https://pepy.tech/project/deeplabcut)\n[![Downloads](https://pepy.tech/badge/deeplabcut/month)](https://pepy.tech/project/deeplabcut)\n[![PyPI version](https://badge.fury.io/py/deeplabcut.svg)](https://badge.fury.io/py/deeplabcut)\n![Python package](https://github.com/DeepLabCut/DeepLabCut/workflows/Python%20package/badge.svg)\n[![License: LGPL v3](https://img.shields.io/badge/License-LGPL%20v3-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)\n<a href=\"https://github.com/psf/black\"><img alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a>\n[![GitHub stars](https://img.shields.io/github/stars/DeepLabCut/DeepLabCut.svg?style=social&label=Star)](https://github.com/DeepLabCut/DeepLabCut)\n[![Average time to resolve an issue](http://isitmaintained.com/badge/resolution/deeplabcut/deeplabcut.svg)](http://isitmaintained.com/project/deeplabcut/deeplabcut \"Average time to resolve an issue\")\n[![Percentage of issues still open](http://isitmaintained.com/badge/open/deeplabcut/deeplabcut.svg)](http://isitmaintained.com/project/deeplabcut/deeplabcut \"Percentage of issues still open\")\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/deeplabcut)\n[![Gitter](https://badges.gitter.im/DeepLabCut/community.svg)](https://gitter.im/DeepLabCut/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Twitter Follow](https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&style=social)](https://twitter.com/DeepLabCut)\n[![Generic badge](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![CZI's Essential Open Source Software for Science](https://chanzuckerberg.github.io/open-science/badges/CZI-EOSS.svg)](https://czi.co/EOSS)\n\n</div>\n\n# Welcome! 👋\n\n**DeepLabCut™️** is a toolbox for state-of-the-art markerless pose estimation of animals performing various behaviors. As long as you can see (label) what you want to track, you can use this toolbox, as it is animal and object agnostic. [Read a short development and application summary below](https://github.com/DeepLabCut/DeepLabCut#why-use-deeplabcut). \n\n# [Installation: how to install DeepLabCut](https://deeplabcut.github.io/DeepLabCut/docs/installation.html)\n\nPlease click the link above for all the information you need to get started! Please note that currently we support only Python 3.10+ (see conda files for guidance).\n\nDevelopers Stable Release:\n- Very quick start: You need to have TensorFlow installed (up to v2.10 supported across platforms) `pip install \"deeplabcut[gui,tf]\"` that includes all functions plus GUIs, or `pip install deeplabcut[tf]` (headless version with PyTorch and TensorFlow).\n  \nDevelopers Alpha Release:\n- We also have an alpha release of PyTorch DeepLabCut available! [Please see here for instructions and information](https://github.com/DeepLabCut/DeepLabCut/blob/pytorch_docs/docs/pytorch/user_guide.md).\n\nWe recommend using our conda file, see [here](https://github.com/DeepLabCut/DeepLabCut/blob/main/conda-environments/README.md) or the new [`deeplabcut-docker` package](https://github.com/DeepLabCut/DeepLabCut/tree/main/docker). \n\n# [Documentation: The DeepLabCut Process](https://deeplabcut.github.io/DeepLabCut)\n\nOur docs walk you through using DeepLabCut, and key API points. For an overview of the toolbox and workflow for project management, see our step-by-step at [Nature Protocols paper](https://doi.org/10.1038/s41596-019-0176-0).\n\nFor a deeper understanding and more resources for you to get started with Python and DeepLabCut, please check out our free online course! http://DLCcourse.deeplabcut.org\n\n<p align=\"center\">\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1609244903687-US1SN063QIFJS4BP4IJD/ke17ZwdGBToddI8pDm48kFG9xAYub2PPnmh56PTVg7gUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcAju5e7u9RZJEVbVQPZRu9xb_m-kUO2M3I1IeDqD4l8YcGqu2nZPx1UhKV8wc1ELN/dlc_overview_whitebkgrnd.png?format=2500w\" width=\"95%\">\n</p>\n\n# [DEMO the code](/examples)\n\n🐭 pose tracking of single animals demo [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_DEMO_mouse_openfield.ipynb)\n\n🐭🐭🐭 pose tracking of multiple animals demo [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_3miceDemo.ipynb)\n\n- See [more demos here](https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/README.md). We provide data and several Jupyter Notebooks: one that walks you through a demo dataset to test your installation, and another Notebook to run DeepLabCut from the beginning on your own data. We also show you how to use the code in Docker, and on Google Colab.\n\n# Why use DeepLabCut?\n\nIn 2018, we demonstrated the capabilities for [trail tracking](https://vnmurthylab.org/), [reaching in mice](http://www.mousemotorlab.org/) and various Drosophila behaviors during egg-laying (see [Mathis et al.](https://www.nature.com/articles/s41593-018-0209-y) for details). There is, however, nothing specific that makes the toolbox only applicable to these tasks and/or species. The toolbox has already been successfully applied (by us and others) to [rats](http://www.mousemotorlab.org/deeplabcut), humans, various fish species, bacteria, leeches, various robots, cheetahs, [mouse whiskers](http://www.mousemotorlab.org/deeplabcut) and [race horses](http://www.mousemotorlab.org/deeplabcut). DeepLabCut utilized the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut, which inspired the name for our toolbox (see references below). Since this time, the package has changed substantially.  The code has been re-tooled and re-factored since 2.1+: We have added faster and higher performance variants with MobileNetV2s, EfficientNets, and our own DLCRNet backbones (see [Pretraining boosts out-of-domain robustness for pose estimation](https://arxiv.org/abs/1909.11229) and [Lauer et al 2022](https://www.nature.com/articles/s41592-022-01443-0)). Additionally, we have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support.\nIn v3.0+ we have changed the backend to support PyTorch. This brings not only an easier installation process for users, but performance gains, developer flexibility, and a lot of new tools! Importantly, the high-level API stays the same, so it will be a seamless transition for users 💜!\nWe currently provide state-of-the-art performance for animal pose estimation and the labs (M. Mathis Lab and A. Mathis Group) have both top journal and computer vision conference papers.\n\n<p align=\"center\">\n<img src=\"https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3e47258a922d548c483247/1547585339819/ErrorvsTrainingsetSize.png?format=750w\" height=\"160\">\n<img src=\"https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3e469d8a922d548c4828fa/1547585194560/compressionrobustness.png?format=750w\" height=\"160\">\n<img src=\"https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3fbed74fa51acecd63deeb/1547681534736/MouseLocomotion_warren.gif?format=500w\" height=\"160\">  \n<img src=\"https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3fc1c6758d46950ce7eec7/1547682383595/cheetah.png?format=750w\" height=\"160\">\n</p>\n\n**Left:** Due to transfer learning it requires **little training data** for multiple, challenging behaviors (see [Mathis et al. 2018](https://www.nature.com/articles/s41593-018-0209-y) for details). **Mid Left:** The feature detectors are robust to video compression (see [Mathis/Warren](https://www.biorxiv.org/content/early/2018/10/30/457242) for details). **Mid Right:** It allows 3D pose estimation with a single network and camera (see [Mathis/Warren](https://www.biorxiv.org/content/early/2018/10/30/457242)). **Right:** It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods (see [Nath* and Mathis* et al. 2019](https://doi.org/10.1038/s41596-019-0176-0)).\n\n**DeepLabCut** is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. Moreover, many new tools are being actively developed. See [DLC-Utils](https://github.com/DeepLabCut/DLCutils) for some helper code.\n\n<p align=\"center\">\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588292233203-FD1DVKAQYNV2TU91CO7R/ke17ZwdGBToddI8pDm48kIX24IsDPzy6M4KUaihfICJZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIxtGUdkzp028KVNnpOijF3PweOM5su6FUQHO6Wkh72Nw/dlc_eco.gif?format=1000w\" width=\"80%\">\n</p>\n\n## Code contributors:\n\nDLC code was originally developed by [Alexander Mathis](https://github.com/AlexEMG) & [Mackenzie Mathis](https://github.com/MMathisLab), and was extended in 2.0 with the core dev team consisting of [Tanmay Nath](https://github.com/meet10may) (2.0-2.1), and currently (2.1+) with [Jessy Lauer](https://github.com/jeylau) and (2.3+) [Niels Poulsen](https://github.com/n-poulsen).\nDeepLabCut is an open-source tool and has benefited from suggestions and edits by many individuals including  Mert Yuksekgonul, Tom Biasi, Richard Warren, Ronny Eichler, Hao Wu, Federico Claudi, Gary Kane and Jonny Saunders as well as the [100+ contributors](https://github.com/DeepLabCut/DeepLabCut/graphs/contributors). Please see [AUTHORS](https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS) for more details!\n\nThis is an actively developed package and we welcome community development and involvement.\n\n# Get Assistance & be part of the DLC Community✨:\n\n| 🚉 Platform                                                 | 🎯 Goal                                                                      | ⏱️ Estimated Response Time | 📢 Support Squad                        |\n|------------------------------------------------------------|-----------------------------------------------------------------------------|---------------------------|----------------------------------------|\n| [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/deeplabcut) <br /> 🐭Tag: DeepLabCut | To ask help and support questions👋                                          | Promptly🔥                 | DLC Team and The DLC Community |\n| GitHub DeepLabCut/[Issues](https://github.com/DeepLabCut/DeepLabCut/issues)                                  | To report bugs and code issues🐛   (we encourage you to search issues first) | 2-3 days                  | DLC Team                               |\n|[![Gitter](https://badges.gitter.im/DeepLabCut/community.svg)](https://gitter.im/DeepLabCut/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)                               | To discuss with other users,  share ideas and collaborate💡                  | 2 days                    | The DLC Community                      |\n| GitHub DeepLabCut/[Contributing](https://github.com/DeepLabCut/DeepLabCut/blob/master/CONTRIBUTING.md)                          | To contribute your expertise and experience🙏💯                               | Promptly🔥                 | DLC Team                               |\n| 🚧 GitHub DeepLabCut/[Roadmap](https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/roadmap.md)                             | To learn more about our journey✈️                                            | N/A                       | N/A                                    |\n| [![Twitter Follow](https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&style=social)](https://twitter.com/DeepLabCut)                                                   | To keep up with our latest news and updates 📢                               | Daily                     | DLC Team                               |\n| The DeepLabCut [AI Residency Program](https://www.deeplabcutairesidency.org/)                        | To come and work with us next summer👏                                       | Annually                  | DLC Team                               |\n\n\n## References \\& Citations:\n\nPlease see our [dedicated page](https://deeplabcut.github.io/DeepLabCut/docs/citation.html) on how to **cite DeepLabCut** 🙏 and our sugestions for your Methods section!\n\n## License:\n\nThis project is primarily licensed under the GNU Lesser General Public License v3.0. Note that the software is provided \"as is\", without warranty of any kind, express or implied. If you use the code or data, please cite us! Note, artwork (DeepLabCut logo) and images are copyrighted; please do not take or use these images without written permission.\n\nSuperAnimal models are provided for research use only (non-commercial use).\n\n## Major Versions:\n\n- For all versions, please see [here](https://github.com/DeepLabCut/DeepLabCut/releases).\n\nVERSION 3.0: A whole new experience with PyTorch🔥. While the high-level API remains the same, the backend and developer friendliness have greatly improved, along with performance gains!\n\nVERSION 2.3: Model Zoo SuperAnimals, and a whole new GUI experience.\n\nVERSION 2.2: Multi-animal pose estimation, identification, and tracking with DeepLabCut is supported (as well as single-animal projects).\n\nVERSION 2.0-2.1: This is the **Python package** of [DeepLabCut](https://www.nature.com/articles/s41593-018-0209-y) that was originally released in Oct 2018 with our [Nature Protocols](https://doi.org/10.1038/s41596-019-0176-0) paper (preprint [here](https://www.biorxiv.org/content/10.1101/476531v1)).\nThis package includes graphical user interfaces to label your data, and take you from data set creation to automatic behavioral analysis. It also introduces an active learning framework to efficiently use DeepLabCut on large experimental projects, and data augmentation tools that improve network performance, especially in challenging cases (see [panel b](https://camo.githubusercontent.com/77c92f6b89d44ca758d815bdd7e801247437060b/68747470733a2f2f737461746963312e73717561726573706163652e636f6d2f7374617469632f3537663664353163396637343536366635356563663237312f742f3563336663316336373538643436393530636537656563372f313534373638323338333539352f636865657461682e706e673f666f726d61743d37353077)).\n\nVERSION 1.0: The initial, Nature Neuroscience version of [DeepLabCut](https://www.nature.com/articles/s41593-018-0209-y) can be found in the history of git, or here: https://github.com/DeepLabCut/DeepLabCut/releases/tag/1.11\n\n# News (and in the news):\n\n:purple_heart: We released a major update, moving from 2.x --> 3.x with the backend change to PyTorch\n\n:purple_heart: The DeepLabCut Model Zoo launches SuperAnimals, see more [here](http://www.mackenziemathislab.org/dlc-modelzoo/).\n\n:purple_heart: **DeepLabCut supports multi-animal pose estimation!** maDLC is out of beta/rc mode and beta is deprecated, thanks to the testers out there for feedback! Your labeled data will be backwards compatible, but not all other steps. Please see the [new `2.2+` releases](https://github.com/DeepLabCut/DeepLabCut/releases) for what's new & how to install it, please see our new [paper, Lauer et al 2022](https://www.nature.com/articles/s41592-022-01443-0), and the [new docs]( https://deeplabcut.github.io/DeepLabCut) on how to use it!\n\n:purple_heart: We support multi-animal re-identification, see [Lauer et al 2022](https://www.nature.com/articles/s41592-022-01443-0).\n\n:purple_heart: We have a **real-time** package available! http://DLClive.deeplabcut.org\n\n\n- June 2024: Our second DLC paper ['Using DeepLabCut for 3D markerless pose estimation across species and behaviors'](https://www.nature.com/articles/s41596-019-0176-0) in Nature Protocols has surpassed 1,000 Google Scholar citations!\n- May 2024: DeepLabCut was featured in Nature: ['DeepLabCut: the motion-tracking tool that went viral'](https://www.nature.com/articles/d41586-024-01474-x)\n- January 2024: Our original paper ['DeepLabCut: markerless pose estimation of user-defined body parts with deep learning'](https://www.nature.com/articles/s41593-018-0209-y) in Nature Neuroscience has surpassed 3,000 Google Scholar citations! \n- December 2023: DeepLabCut hit 600,000 downloads!\n- October 2023: DeepLabCut celebrates a milestone with 4,000 🌟 in Github!\n- July 2023: The user forum is very active with more than 1k questions and answers: [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/deeplabcut)\n- May 2023: The Model Zoo is now fully integrated into the DeepLabCut GUI, making it easier than ever to access a variety of pre-trained models. Check out the accompanying paper: [SuperAnimal pretrained pose estimation models for behavioral analysis by Ye et al.](https://arxiv.org/abs/2203.07436)\n- December 2022: DeepLabCut hits 450,000 downloads and 2.3 is the new stable release\n- August 2022: DeepLabCut hit 400,000 downloads\n- August 2021: 2.2 becomes the new stable release for DeepLabCut.\n- July 2021: Docs are now at https://deeplabcut.github.io/DeepLabCut and we now include TensorFlow 2 support!\n- May 2021: DeepLabCut hit 200,000 downloads! Also, Our preprint on 2.2, multi-animal DeepLabCut is released!\n- Jan 2021: [Pretraining boosts out-of-domain robustness for pose estimation](https://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html) published in the IEEE Winter Conference on Applications of Computer Vision. We also added EfficientNet backbones to DeepLabCut, those are best trained with cosine decay (see paper). To use them, just pass \"`efficientnet-b0`\" to \"`efficientnet-b6`\" when creating the trainingset!\n- Dec 2020: We released a real-time package that allows for online pose estimation and real-time feedback. See [DLClive.deeplabcut.org](http://DLClive.deeplabcut.org).\n- 5/22 2020: We released 2.2beta5. This beta release has some of the features of DeepLabCut 2.2, whose major goal is to integrate multi-animal pose estimation to DeepLabCut.\n- Mar 2020: Inspired by suggestions we heard at this weeks CZI's Essential Open Source Software meeting in Berkeley, CA we updated our [docs](docs/UseOverviewGuide.md). Let us know what you think!\n- Feb 2020: Our [review on animal pose estimation is published!](https://www.sciencedirect.com/science/article/pii/S0959438819301151)\n- Nov 2019: DeepLabCut was recognized by the Chan Zuckerberg Initiative (CZI) with funding to support this project. Read more in the [Harvard Gazette](https://news.harvard.edu/gazette/story/newsplus/harvard-researchers-awarded-czi-open-source-award/), on [CZI's Essential Open Source Software for Science site](https://chanzuckerberg.com/eoss/proposals/) and in their [Medium post](https://medium.com/@cziscience/how-open-source-software-contributors-are-accelerating-biomedicine-1a5f50f6846a)\n- Oct 2019: DLC 2.1 released with lots of updates. In particular, a Project Manager GUI, MobileNetsV2, and augmentation packages (Imgaug and Tensorpack). For detailed updates see [releases](https://github.com/DeepLabCut/DeepLabCut/releases)\n- Sept 2019: We published two preprints. One showing that [ImageNet pretraining contributes to robustness](https://arxiv.org/abs/1909.11229) and a [review on animal pose estimation](https://arxiv.org/abs/1909.13868). Check them out!\n- Jun 2019: DLC 2.0.7 released with lots of updates. For updates see [releases](https://github.com/DeepLabCut/DeepLabCut/releases)\n- Feb 2019: DeepLabCut joined [twitter](https://twitter.com/deeplabcut) [![Twitter Follow](https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&style=social)](https://twitter.com/DeepLabCut)\n- Jan 2019: We hosted workshops for DLC in Warsaw, Munich and Cambridge. The materials are available [here](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials)\n- Jan 2019: We joined the Image Source Forum for user help: [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/deeplabcut)\n\n- Nov 2018: We posted a detailed guide for DeepLabCut 2.0 on [BioRxiv](https://www.biorxiv.org/content/early/2018/11/24/476531). It also contains a case study for 3D pose estimation in cheetahs.\n- Nov 2018: Various (post-hoc) analysis scripts contributed by users (and us) will be gathered at [DLCutils](https://github.com/DeepLabCut/DLCutils). Feel free to contribute! In particular, there is a script guiding you through\nimporting a project into the new data format for DLC 2.0\n- Oct 2018: new pre-print on the speed video-compression and robustness of DeepLabCut on [BioRxiv](https://www.biorxiv.org/content/early/2018/10/30/457242)\n- Sept 2018: Nature Lab Animal covers DeepLabCut: [Behavior tracking cuts deep](https://www.nature.com/articles/s41684-018-0164-y)\n- Kunlin Wei & Konrad Kording write a very nice News & Views on our paper: [Behavioral Tracking Gets Real](https://www.nature.com/articles/s41593-018-0215-0)\n- August 2018: Our [preprint](https://arxiv.org/abs/1804.03142) appeared in [Nature Neuroscience](https://www.nature.com/articles/s41593-018-0209-y)\n- August 2018: NVIDIA AI Developer News: [AI Enables Markerless Animal Tracking](https://news.developer.nvidia.com/ai-enables-markerless-animal-tracking/)\n- July 2018: Ed Yong covered DeepLabCut and interviewed several users for the [Atlantic](https://www.theatlantic.com/science/archive/2018/07/deeplabcut-tracking-animal-movements/564338).\n- April 2018: first DeepLabCut preprint on [arXiv.org](https://arxiv.org/abs/1804.03142)\n\n  ## Funding\n\n  We are grateful for the follow support over the years! This software project was supported in part by the Essential Open Source Software for Science (EOSS) program at Chan Zuckerberg Initiative (cycles 1, 3, 3-DEI, 4), and jointly with the Kavli Foundation for EOSS Cycle 6!  We also thank the Rowland Institute at Harvard for funding from 2017-2020, and EPFL from 2020-present.\n"
        },
        {
          "name": "_config.yml",
          "type": "blob",
          "size": 0.7119140625,
          "content": "title: DeepLabCut\nauthor: The DeepLabCut Team\nlogo: docs/images/logo.png\nonly_build_toc_files: true\n\nsphinx:\n  config:\n    autodoc_mock_imports: [\"wx\"]\n    mermaid_output_format: raw\n  extra_extensions:\n    - numpydoc\n    - sphinxcontrib.mermaid\n\nexecute:\n  execute_notebooks: \"off\"\n\nhtml:\n  extra_navbar: \"\"\n  use_issues_button: true\n  use_repository_button: true\n  extra_footer: |\n    <div>Powered by <a href=\"https://jupyterbook.org/\">Jupyter Book</a>.</div>\n\nrepository:\n  url: https://github.com/DeepLabCut/DeepLabCut\n  path_to_book: docs\n  branch: main\n\nlaunch_buttons:\n  colab_url: \"https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb\"\n"
        },
        {
          "name": "_toc.yml",
          "type": "blob",
          "size": 2.2431640625,
          "content": "format: jb-book\nroot: README\nparts:\n- caption: Getting Started\n  chapters:\n  - file: docs/UseOverviewGuide\n  - file: docs/course\n- caption: Installation\n  chapters:\n  - file: docs/installation\n  - file: docs/recipes/installTips\n  - file: docs/docker\n- caption: Main User Guides\n  chapters:\n  - file: docs/standardDeepLabCut_UserGuide\n  - file: docs/maDLC_UserGuide\n  - file: docs/Overviewof3D\n  - file: docs/HelperFunctions\n- caption: Graphical User Interfaces (GUIs)\n  chapters:\n  - file: docs/gui/PROJECT_GUI\n  - file: docs/gui/napari_GUI\n- caption: DLC3 PyTorch Specific Docs\n  chapters:\n  - file: docs/pytorch/user_guide.md\n  - file: docs/pytorch/pytorch_config.md\n  - file: docs/pytorch/architectures.md\n- caption: Quick Start Tutorials\n  chapters:\n  - file: docs/quick-start/single_animal_quick_guide\n  - file: docs/quick-start/tutorial_maDLC\n- caption: 🚀 Beginner's Guide to DeepLabCut\n  chapters:\n  - file: docs/beginner-guides/beginners-guide\n  - file: docs/beginner-guides/manage-project\n  - file: docs/beginner-guides/labeling\n  - file: docs/beginner-guides/Training-Evaluation\n  - file: docs/beginner-guides/video-analysis\n- caption: Hardware Tips\n  chapters:\n  - file: docs/recipes/TechHardware\n- caption: DeepLabCut-Live!\n  chapters:\n  - file: docs/deeplabcutlive\n- caption: 🦄 DeepLabCut Model Zoo\n  chapters:\n  - file: docs/ModelZoo\n  - file: docs/recipes/UsingModelZooPupil\n  - file: docs/recipes/MegaDetectorDLCLive\n- caption: 🧑‍🍳 Cookbook (detailed helper guides)\n  chapters:\n  - file: docs/tutorial\n  - file: docs/convert_maDLC\n  - file: docs/recipes/OtherData\n  - file: docs/recipes/io\n  - file: docs/recipes/nn\n  - file: docs/recipes/post\n  - file: docs/recipes/BatchProcessing\n  - file: docs/recipes/DLCMethods\n  - file: docs/recipes/ClusteringNapari\n  - file: docs/recipes/OpenVINO\n  - file: docs/recipes/flip_and_rotate\n  - file: docs/recipes/pose_cfg_file_breakdown\n  - file: docs/recipes/publishing_notebooks_into_the_DLC_main_cookbook\n- caption: DeepLabCut Benchmarking\n  chapters:\n  - file: docs/benchmark\n  - file: docs/pytorch/Benchmarking_shuffle_guide\n- caption: Mission & Contribute\n  chapters:\n  - file: docs/MISSION_AND_VALUES\n  - file: docs/roadmap\n  - file: docs/Governance\n- caption: Citations for DeepLabCut\n  chapters:\n  - file: docs/citation\n"
        },
        {
          "name": "conda-environments",
          "type": "tree",
          "content": null
        },
        {
          "name": "deeplabcut",
          "type": "tree",
          "content": null
        },
        {
          "name": "dlc.py",
          "type": "blob",
          "size": 0.3564453125,
          "content": "\"\"\"\nDeepLabCut2.0-2.2 Toolbox (deeplabcut.org)\n© A. & M. Mathis Labs\nhttps://github.com/DeepLabCut/DeepLabCut\n\nPlease see AUTHORS for contributors.\nhttps://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS\nLicensed under GNU Lesser General Public License v3.0\n\"\"\"\n\nfrom deeplabcut import cli\n\n\ndef main():\n    cli.main()\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "reinstall.sh",
          "type": "blob",
          "size": 0.109375,
          "content": "pip uninstall deeplabcut\npython3 setup.py sdist bdist_wheel\npip install dist/deeplabcut-2.3.10-py3-none-any.whl\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3427734375,
          "content": "dlclibrary\nipython\nfilterpy\nruamel.yaml>=0.15.0\nintel-openmp\nimageio-ffmpeg\nimgaug==0.4.0\nnumba>=0.54.0\nmatplotlib<=3.5.2\nnetworkx>=2.6\nnumpy>=1.18.5,<2.0.0\npandas>=1.0.1,!=1.5.0\npyyaml\nscikit-image>=0.17\nscikit-learn>=1.0\nscipy>=1.9\nstatsmodels>=0.11\ntensorflow>=2.0,<2.13.0\ntables==3.8.0\ntensorpack==0.11\ntf_slim==1.1.0\ntorch==1.12\ntqdm\nPillow>=7.1\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.62890625,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nDeepLabCut2.0-2.2 Toolbox (deeplabcut.org)\n© A. & M. Mathis Labs\nhttps://github.com/DeepLabCut/DeepLabCut\nPlease see AUTHORS for contributors.\nhttps://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS\nLicensed under GNU Lesser General Public License v3.0\n\"\"\"\n\nimport setuptools\n\nwith open(\"README.md\", encoding=\"utf-8\", errors=\"replace\") as fh:\n    long_description = fh.read()\n\n\nsetuptools.setup(\n    name=\"deeplabcut\",\n    version=\"2.3.10\",\n    author=\"A. & M.W. Mathis Labs\",\n    author_email=\"alexander@deeplabcut.org\",\n    description=\"Markerless pose-estimation of user-defined features with deep learning\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/DeepLabCut/DeepLabCut\",\n    install_requires=[\n        \"dlclibrary>=0.0.6\",\n        \"filterpy>=1.4.4\",\n        \"ruamel.yaml>=0.15.0\",\n        \"imgaug>=0.4.0\",\n        \"imageio-ffmpeg\",\n        \"numba>=0.54\",\n        \"matplotlib>=3.3,<3.9,!=3.7.0,!=3.7.1\",\n        \"networkx>=2.6\",\n        \"numpy>=1.18.5,<2.0.0\",\n        \"pandas>=1.0.1,!=1.5.0\",\n        \"scikit-image>=0.17\",\n        \"scikit-learn>=1.0\",\n        \"scipy>=1.9\",\n        \"statsmodels>=0.11\",\n        \"torch\",\n        \"tqdm\",\n        \"pyyaml\",\n        \"Pillow>=7.1\",\n        \"tables==3.8.0\",\n    ],\n    extras_require={\n        \"gui\": [\n            \"pyside6==6.4.2\",\n            \"qdarkstyle==3.1\",\n            \"napari-deeplabcut>=0.2.1.6\",\n        ],\n        \"openvino\": [\"openvino-dev==2022.1.0\"],\n        \"docs\": [\"numpydoc\"],\n        \"tf\": [\n            \"tensorflow>=2.0,<=2.10\",\n            \"tensorpack>=0.11\",\n            \"tf_slim>=1.1.0\",\n        ],  # Last supported TF version on Windows Native is 2.10\n        \"apple_mchips\": [\n            \"tensorflow-macos<2.13.0\",\n            \"tensorflow-metal\",\n            \"tensorpack>=0.11\",\n            \"tf_slim>=1.1.0\",\n        ],\n        \"modelzoo\": [\"huggingface_hub\"],\n    },\n    scripts=[\"deeplabcut/pose_estimation_tensorflow/models/pretrained/download.sh\"],\n    packages=setuptools.find_packages(),\n    data_files=[\n        (\n            \"deeplabcut\",\n            [\n                \"deeplabcut/pose_cfg.yaml\",\n                \"deeplabcut/inference_cfg.yaml\",\n                \"deeplabcut/reid_cfg.yaml\",\n                \"deeplabcut/pose_estimation_tensorflow/models/pretrained/pretrained_model_urls.yaml\",\n                \"deeplabcut/pose_estimation_tensorflow/superanimal_configs/superquadruped.yaml\",\n                \"deeplabcut/pose_estimation_tensorflow/superanimal_configs/supertopview.yaml\",\n                \"deeplabcut/gui/style.qss\",\n                \"deeplabcut/gui/media/logo.png\",\n                \"deeplabcut/gui/media/dlc_1-01.png\",\n                \"deeplabcut/gui/assets/logo.png\",\n                \"deeplabcut/gui/assets/logo_transparent.png\",\n                \"deeplabcut/gui/assets/welcome.png\",\n                \"deeplabcut/gui/assets/icons/help.png\",\n                \"deeplabcut/gui/assets/icons/help2.png\",\n                \"deeplabcut/gui/assets/icons/new_project.png\",\n                \"deeplabcut/gui/assets/icons/new_project2.png\",\n                \"deeplabcut/gui/assets/icons/open.png\",\n                \"deeplabcut/gui/assets/icons/open2.png\",\n                \"deeplabcut/modelzoo/models.json\",\n            ],\n        )\n    ],\n    include_package_data=True,\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)\",\n        \"Operating System :: OS Independent\",\n    ],\n    entry_points=\"\"\"[console_scripts]\n            dlc=dlc:main\"\"\",\n)\n\n# https://www.python.org/dev/peps/pep-0440/#compatible-release\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "testscript_cli.py",
          "type": "blob",
          "size": 5.5078125,
          "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nmodified from: https://github.com/DeepLabCut/DeepLabCut-core/testscript_cli.py\nby Mackenzie.\n\nDEVELOPERS:\nThis script tests various functionalities in an automatic way.\nIt produces nothing of interest scientifically.\n\"\"\"\n\ntask = \"Testcore\"  # Enter the name of your experiment Task\nscorer = \"Mackenzie\"  # Enter the name of the experimenter/labeler\n\nimport os, subprocess, sys\n\n\n# def install(package):\n#    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n# install(\"tensorflow==1.13.1\")\n\nimport deeplabcut as dlc\n\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport platform\n\nprint(\"Imported DLC!\")\n\nbasepath = os.path.dirname(os.path.abspath(\"testscript_cli.py\"))\nvideoname = \"reachingvideo1\"\nvideo = [\n    os.path.join(\n        basepath,\n        \"examples\",\n        \"Reaching-Mackenzie-2018-08-30\",\n        \"videos\",\n        videoname + \".avi\",\n    )\n]\n# For testing a color video:\n# videoname='baby4hin2min'\n# video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]\n# to test destination folder:\n# dfolder=basepath\nprint(video)\n\ndfolder = None\nnet_type = \"resnet_50\"  #'mobilenet_v2_0.35' #'resnet_50'\naugmenter_type = \"default\"\naugmenter_type2 = \"imgaug\"\n\nif platform.system() == \"Darwin\" or platform.system() == \"Windows\":\n    print(\"On Windows/OSX tensorpack is not tested by default.\")\n    augmenter_type3 = \"imgaug\"\nelse:\n    augmenter_type3 = \"tensorpack\"  # Does not work on WINDOWS\n\nnumiter = 3\n\nprint(\"CREATING PROJECT\")\npath_config_file = dlc.create_new_project(task, scorer, video, copy_videos=True)\n\ncfg = dlc.auxiliaryfunctions.read_config(path_config_file)\ncfg[\"numframes2pick\"] = 5\ncfg[\"pcutoff\"] = 0.01\ncfg[\"TrainingFraction\"] = [0.8]\ncfg[\"skeleton\"] = [[\"bodypart1\", \"bodypart2\"], [\"bodypart1\", \"bodypart3\"]]\n\ndlc.auxiliaryfunctions.write_config(path_config_file, cfg)\n\nprint(\"EXTRACTING FRAMES\")\ndlc.extract_frames(path_config_file, mode=\"automatic\", userfeedback=False)\n\nprint(\"CREATING SOME LABELS FOR THE FRAMES\")\nframes = os.listdir(os.path.join(cfg[\"project_path\"], \"labeled-data\", videoname))\n# As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)\nfor index, bodypart in enumerate(cfg[\"bodyparts\"]):\n    columnindex = pd.MultiIndex.from_product(\n        [[scorer], [bodypart], [\"x\", \"y\"]], names=[\"scorer\", \"bodyparts\", \"coords\"]\n    )\n    frame = pd.DataFrame(\n        100 + np.ones((len(frames), 2)) * 50 * index,\n        columns=columnindex,\n        index=[os.path.join(\"labeled-data\", videoname, fn) for fn in frames],\n    )\n    if index == 0:\n        dataFrame = frame\n    else:\n        dataFrame = pd.concat([dataFrame, frame], axis=1)\n\ndataFrame.to_csv(\n    os.path.join(\n        cfg[\"project_path\"],\n        \"labeled-data\",\n        videoname,\n        \"CollectedData_\" + scorer + \".csv\",\n    )\n)\ndataFrame.to_hdf(\n    os.path.join(\n        cfg[\"project_path\"],\n        \"labeled-data\",\n        videoname,\n        \"CollectedData_\" + scorer + \".h5\",\n    ),\n    \"df_with_missing\",\n    format=\"table\",\n    mode=\"w\",\n)\n\nprint(\"Plot labels...\")\n\ndlc.check_labels(path_config_file)\n\nprint(\"CREATING TRAININGSET\")\ndlc.create_training_dataset(\n    path_config_file, net_type=net_type, augmenter_type=augmenter_type\n)\n\nposefile = os.path.join(\n    cfg[\"project_path\"],\n    \"dlc-models/iteration-\"\n    + str(cfg[\"iteration\"])\n    + \"/\"\n    + cfg[\"Task\"]\n    + cfg[\"date\"]\n    + \"-trainset\"\n    + str(int(cfg[\"TrainingFraction\"][0] * 100))\n    + \"shuffle\"\n    + str(1),\n    \"train/pose_cfg.yaml\",\n)\n\nDLC_config = dlc.auxiliaryfunctions.read_plainconfig(posefile)\nDLC_config[\"save_iters\"] = numiter\nDLC_config[\"display_iters\"] = 2\nDLC_config[\"multi_step\"] = [[0.001, numiter]]\n\nprint(\"CHANGING training parameters to end quickly!\")\ndlc.auxiliaryfunctions.write_plainconfig(posefile, DLC_config)\n\nprint(\"TRAIN\")\ndlc.train_network(path_config_file)\n\nprint(\"EVALUATE\")\ndlc.evaluate_network(path_config_file, plotting=True)\n\nvideotest = os.path.join(cfg[\"project_path\"], \"videos\", videoname + \".avi\")\n\nprint(videotest)\n\n# quicker variant\n\"\"\"\nprint(\"VIDEO ANALYSIS\")\ndlc.analyze_videos(path_config_file, [videotest], save_as_csv=True)\n\nprint(\"CREATE VIDEO\")\ndlc.create_labeled_video(path_config_file,[videotest], save_frames=False)\n\nprint(\"Making plots\")\ndlc.plot_trajectories(path_config_file,[videotest])\n\nprint(\"CREATING TRAININGSET 2\")\ndlc.create_training_dataset(path_config_file, Shuffles=[2],net_type=net_type,augmenter_type=augmenter_type2)\n\ncfg=dlc.auxiliaryfunctions.read_config(path_config_file)\nposefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(2),'train/pose_cfg.yaml')\nDLC_config=dlc.auxiliaryfunctions.read_plainconfig(posefile)\nDLC_config['save_iters']=numiter\nDLC_config['display_iters']=1\nDLC_config['multi_step']=[[0.001,numiter]]\n\nprint(\"CHANGING training parameters to end quickly!\")\ndlc.auxiliaryfunctions.write_config(posefile,DLC_config)\n\nprint(\"TRAIN\")\ndlc.train_network(path_config_file, shuffle=2,allow_growth=True)\n\nprint(\"EVALUATE\")\ndlc.evaluate_network(path_config_file,Shuffles=[2],plotting=False)\n\n\nprint(\"ANALYZING some individual frames\")\ndlc.analyze_time_lapse_frames(path_config_file,os.path.join(cfg['project_path'],'labeled-data/reachingvideo1/'))\n\"\"\"\n\nprint(\"Export model...\")\ndlc.export_model(path_config_file, shuffle=1, make_tar=False)\n\nprint(\n    \"ALL DONE!!! - default/imgaug cases of DLCcore training and evaluation are functional (no extract outlier or refinement tested).\"\n)\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}