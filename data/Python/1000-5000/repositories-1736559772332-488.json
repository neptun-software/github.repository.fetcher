{
  "metadata": {
    "timestamp": 1736559772332,
    "page": 488,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "snipsco/snips-nlu",
      "stars": 3899,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".appveyor.yml",
          "type": "blob",
          "size": 0.669921875,
          "content": "environment:\n  matrix:\n    - PYTHON: \"C:\\\\Python36-x64\"\n      TARGET: x86_64-pc-windows-msvc\n    - PYTHON: \"C:\\\\Python37-x64\"\n      TARGET: x86_64-pc-windows-msvc\n\nbranches:\n  only:\n  - master\n  - /lts-.*/\n\ninstall:\n  - \"%PYTHON%\\\\python.exe -m pip install wheel\"\n  - \"%PYTHON%\\\\python.exe -m pip install -e .[test] --upgrade --upgrade-strategy eager\"\n\nbuild: false\n\ntest_script:\n  - \"%PYTHON%\\\\python.exe -m snips_nlu download-all-languages\"\n  - \"%PYTHON%\\\\python.exe -m snips_nlu download-language-entities fr\"\n  - \"%PYTHON%\\\\python.exe -m snips_nlu download-language-entities en\"\n  - \"%PYTHON%\\\\python.exe -m unittest discover\"\n  - \"%PYTHON%\\\\python.exe snips_nlu_samples/sample.py\"\n"
        },
        {
          "name": ".codecov.yml",
          "type": "blob",
          "size": 0.27734375,
          "content": "coverage:\n  precision: 2\n  round: down\n  range: \"70...100\"\n\n  status:\n    project: off\n    patch:\n      default:\n        target: \"90%\"\n        threshold: null\n        base: pr\n\ncomment:\n  layout: \"diff\"\n  behavior: default\n  require_changes: yes\n  require_base: no\n  require_head: yes"
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.2373046875,
          "content": "[run]\nbranch = True\nsource =\n  snips_nlu\nomit =\n  snips_nlu/tests/*\n  cli/tests/*\n  sample/test/*\n\n[report]\nomit =\n  snips_nlu/tests/*\n  cli/tests/*\n  sample/test/*\n\n[paths]\nsource =\n    snips_nlu\n    .tox/*/lib/python*/site-packages/snips_nlu"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1318359375,
          "content": "*.cache\n*.py[cod]\n*.egg-info\n*build\n*.bak\n.idea\n/dist\n/venv\n/venv2\n/venv3\n/venv35\n/venv36\n/venv37\n/.tox\n.coverage\npylint.log\n.DS_Store\n"
        },
        {
          "name": ".img",
          "type": "tree",
          "content": null
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.060546875,
          "content": "python:\n  pip_install: true\n  extra_requirements:\n      - doc\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.4990234375,
          "content": "os: linux\nsudo: false\nlanguage: python\n\nmatrix:\n  include:\n    - python: 3.8\n      env: TOXENV=py38\n      dist: xenial\n      sudo: true\n    - python: 3.7\n      env: TOXENV=py37\n      dist: xenial\n      sudo: true\n    - python: 3.6\n      env: TOXENV=py36,docs-html\n    - python: 3.6\n      env: TOXENV=integration-test\n    - python: 3.5\n      env: TOXENV=py35\n    - python: 2.7\n      env: TOXENV=py27\n\ninstall: python -m pip install tox codecov\n\nscript: tox\n\nafter_success:\n  - tox -e coverage-report\n  - codecov\n"
        },
        {
          "name": "AUTHORS.rst",
          "type": "blob",
          "size": 0.1787109375,
          "content": "Snips NLU is written and maintained by Snips.\n\nDevelopment Lead\n================\n\n* `Adrien Ball <https://github.com/adrienball>`_\n* `Clement Doumouro <https://github.com/clemdoum>`_\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 14.68359375,
          "content": "# Changelog\nAll notable changes to this project will be documented in this file.\n\n## [0.20.2] - 2020-01-15\n### Added\n- Add intents filter parameter in parsing CLI [#858](https://github.com/snipsco/snips-nlu/pull/858)\n- Add documentation about intents filters [#858](https://github.com/snipsco/snips-nlu/pull/858)\n- Update dependencies for better python3.8 support [#867](https://github.com/snipsco/snips-nlu/pull/867)\n\n## [0.20.1] - 2019-09-04\n### Added\n- Allow to bypass the model version check [#830](https://github.com/snipsco/snips-nlu/pull/830)\n- Persist `CustomEntityParser` license when needed [#832](https://github.com/snipsco/snips-nlu/pull/832)\n- Document metrics CLI [#839](https://github.com/snipsco/snips-nlu/pull/839)\n- Allow to fit SnipsNLUEngine with a `Dataset` object [#840](https://github.com/snipsco/snips-nlu/pull/840)\n\n### Changed\n- Update `snips-nlu-parsers` dependency upper bound to 0.5 [#850](https://github.com/snipsco/snips-nlu/pull/850)\n\n### Fixed\n- Invalidate importlib caches after dynamically installing module [#838](https://github.com/snipsco/snips-nlu/pull/838)\n- Automatically generate documentation for supported languages and builtin entities [#841](https://github.com/snipsco/snips-nlu/pull/841)\n- Fix issue when cleaning up crfsuite files [#843](https://github.com/snipsco/snips-nlu/pull/843)\n- Fix filemode of persisted crfsuite files [#844](https://github.com/snipsco/snips-nlu/pull/844)\n\n## [0.20.0] - 2019-07-16\n### Added\n- Add new intent parser: `LookupIntentParser` [#759](https://github.com/snipsco/snips-nlu/pull/759)\n\n### Changed\n- Replace `DeterministicIntentParser` by `LookupIntentParser` in default configs [#829](https://github.com/snipsco/snips-nlu/pull/829)\n- Bumped `snips-nlu-parsers` to `0.3.x` introducing new builtin entities:\n    - `snips/time`\n    - `snips/timePeriod`\n    - `snips/date`\n    - `snips/datePeriod`\n    - `snips/city`\n    - `snips/country`\n    - `snips/region`\n    \n\n## [0.19.8] - 2019-07-10\n### Added\n- Add filter for entity match feature [#814](https://github.com/snipsco/snips-nlu/pull/814)\n- Add noise re-weight factor in `LogRegIntentClassifier` [#815](https://github.com/snipsco/snips-nlu/pull/815)\n- Add warning logs and improve errors [#821](https://github.com/snipsco/snips-nlu/pull/821)\n- Add random seed parameter in training CLI [#819](https://github.com/snipsco/snips-nlu/pull/819)\n\n### Fixed\n- Fix non-deterministic behavior [#817](https://github.com/snipsco/snips-nlu/pull/817)\n- Import modules lazily to speed up CLI startup time [#819](https://github.com/snipsco/snips-nlu/pull/819)\n- Removed dependency on `semantic_version` to accept `\"subpatches\"` number [#825](https://github.com/snipsco/snips-nlu/pull/825)\n\n## [0.19.7] - 2019-06-20\n### Changed\n- Re-score ambiguous `DeterministicIntentParser` results based on slots [#791](https://github.com/snipsco/snips-nlu/pull/791)\n- Accept ambiguous results from `DeterministicIntentParser` when confidence score is above 0.5 [#797](https://github.com/snipsco/snips-nlu/pull/797)\n- Avoid generating number variations when not needed [#799](https://github.com/snipsco/snips-nlu/pull/799)\n- Moved the NLU random state from the config to the shared resources [#801](https://github.com/snipsco/snips-nlu/pull/801)\n- Reduce custom entity parser footprint in training time [#804](https://github.com/snipsco/snips-nlu/pull/804)\n- Bumped `scikit-learn` to `>=0.21,<0.22` for `python>=3.5` and `>=0.20<0.21` for `python<3.5` [#801](https://github.com/snipsco/snips-nlu/pull/801)\n- Update dependencies [#811](https://github.com/snipsco/snips-nlu/pull/811) \n\n### Fixed\n- Fixed a couple of bugs in the data augmentation which were making the NLU training non-deterministic [#801](https://github.com/snipsco/snips-nlu/pull/801)\n- Remove deprecated code in dataset generation [#803](https://github.com/snipsco/snips-nlu/pull/803)\n- Fix possible override of entity values when generating variations [#808](https://github.com/snipsco/snips-nlu/pull/808)\n\n## [0.19.6] - 2019-04-26\n### Fixed\n- Raise an error when using unknown intents in intents filter [#788](https://github.com/snipsco/snips-nlu/pull/788)\n- Fix issue with stop words in `DeterministicIntentParser` [#789](https://github.com/snipsco/snips-nlu/pull/789)\n\n## [0.19.5] - 2019-04-10\n### Added\n- Advanced inference logging in the `CRFSlotFiller` [#776](https://github.com/snipsco/snips-nlu/pull/776)\n- Improved failed linking error message after download of resources [#774](https://github.com/snipsco/snips-nlu/pull/774)\n- Improve handling of ambiguous utterances in DeterministicIntentParser [#773](https://github.com/snipsco/snips-nlu/pull/773)\n\n### Changed\n- Remove normalization of confidence scores in intent classification [#782](https://github.com/snipsco/snips-nlu/pull/782)\n\n### Fixed\n- Fixed a crash due to missing resources when refitting the `CRFSlotFiller` [#771](https://github.com/snipsco/snips-nlu/pull/771)\n- Fixed issue with egg fragments in download cli [#769](https://github.com/snipsco/snips-nlu/pull/769)\n- Fixed an issue causing the `None` intent to be ignored when using the `parse` API in conjunction with `intents` and `top_n` [#781](https://github.com/snipsco/snips-nlu/pull/781)\n\n## [0.19.4] - 2019-03-06\n### Added\n- Support for Portuguese: \"pt_pt\" and \"pt_br\"\n\n### Changed\n- Enhancement: leverage entity scopes of each intent in deterministic intent parser\n\n## [0.19.3] - 2019-03-05\n### Fixed\n- Issue with intent classification reducing classification accuracy\n- Issue resulting in a mutation of the CRFSlotFillerConfig\n- Wrong required resources of the `DeterministicIntentParser`\n- Issue with non ASCII characters when using the parsing CLI with Python2 \n\n## [0.19.2] - 2019-02-11\n### Fixed\n- Fix an issue regarding the way builtin entities were handled by the `CRFSlotFiller`\n\n## [0.19.1] - 2019-02-04\n### Fixed\n- Bug causing an unnecessary reloading of shared resources\n\n## [0.19.0] - 2019-02-04\n### Added\n- Support for Python3.7\n- `get_intents(text)` API in `SnipsNLUEngine` to get the probabilities of all the intents\n- `get_slots(text, intent)` API in `SnipsNLUEngine` to extract slots when the intent is known\n- The `DeterministicIntentParser` can now ignore stop words through the new `ignore_stop_words` configuration parameter\n- Co-occurrence features can now be used in the `LogRegIntentClassifier`\n\n### Changed\n- Remove the deprecated text file format for intents and entities in favor YAML format\n- The `None` intent is now handled as a regular intent in the parsing output, which means that:\n```python\n{\n    \"input\": \"foo bar\",\n    \"intent\": None,\n    \"slots\": None\n}\n```\n\nis replaced with:\n \n```python\n{\n    \"input\": \"foo bar\",\n    \"intent\": {\n        \"intentName\": None,\n        \"probability\": 0.552122\n    },\n    \"slots\": []\n}\n```\n- Patterns of the `DeterministicIntentParser` are now deduplicated across intents in order to reduce ambiguity\n- Improve the use of custom `ProcessingUnit` through the use of `Registrable` pattern\n- Improve the use of default processing unit configurations \n- Improve logging\n- Replace `snips-nlu-ontology` with `snips-nlu-parsers`\n- Drop support for Python3.4\n\n### Fixed\n- Issue when persisting resources\n- Issue when resolving custom entities\n- Issue with whitespaces when generating dataset from YAML and text files\n- Issue with unicode when using the CLI (Python 2)\n\n## [0.18.0] - 2018-11-26\n### Added\n- New YAML format to create dataset\n- Verbose mode in CLI\n\n### Changed\n- Bump `snips-nlu-ontology` to `0.62.0` to improve memory usage \n\n\n## [0.17.4] - 2018-11-20\n### Added\n- Add a `--config` argument in the metrics CLI\n\n### Changed\n- Replace \"parser_threshold\" by \"matching_strictness\" in dataset format\n- Optimize loading and inference runtime\n- Disable stemming for intent classification in default configs\n\n\n## [0.17.3] - 2018-10-18\n### Fixed\n- Crash with num2words and floats\n\n## [0.17.2] - 2018-10-15\n### Added\n- Support for builtin music entities in english\n\n## [0.17.1] - 2018-10-09\n### Fixed\n- `DeterministicIntentParser` now relies on the custom entity parser\n\n### Changed\n- Bump `snips-nlu-ontology` to `0.60`\n\n## [0.17.0] - 2018-10-05\n### Added\n- Support for 3 new builtin entities in French: `snips/musicAlbum`, `snips/musicArtist` and `snips/musicTrack`\n- Minimal support for Italian\n\n### Changed\n- model version `0.16.0` => `0.17.0`\n\n### Fixed\n- Bug with entity feature name in intent classification\n\n## [0.16.5] - 2018-09-06\n### Fixed\n- Segfault in CRFSuite when the `CRFSlotFiller` is fitted only on empty utterances \n\n## [0.16.4] - 2018-08-30\n### Fixed\n- Issue with the `CrfSlotFiller` file names in the `ProbabilisticIntentParser` serialization  \n\n## [0.16.3] - 2018-08-22\n### Fixed\n- Issue with synonyms when multiple synonyms have the same normalization\n\n## [0.16.2] - 2018-08-08\n### Added\n- `automatically_extensible` flag in dataset generation tool\n- System requirements\n- Reference to chatito tool in documentation\n\n### Changed\n- Bump `snips-nlu-ontology` to `0.57.3`\n- versions of dependencies are now defined more loosely\n\n### Fixed\n- Issue with synonyms mapping\n- Issue with `snips-nlu download-all-languages` CLI command\n\n## [0.16.1] - 2018-07-23\n### Added\n- Every processing unit can be persisted into (and loaded from) a `bytearray`\n\n## [0.16.0] - 2018-07-17\n### Changed\n- The `SnipsNLUEngine` object is now persisted to (and loaded from) a \ndirectory, instead of a single json file.\n- The language resources are now persisted along with the `SnipsNLUEngine`, \nremoving the need to download and load the resources when loading a trained engine.\n- The format of language resources has been optimized.\n\n### Added\n- Stemmed gazetteers, computed beforehand. It removes the need to stem \ngazetteers on the fly.\n- API to persist (and load) a `SnipsNLUEngine` object as a `bytearray`\n\n### Fixed\n- Issue in the `DeterministicIntentParser` when the same slot name was used in \nmultiple intents while referring to different entities\n\n## [0.15.1] - 2018-07-09\n### Changed\n- Bump `snips-nlu-ontology` to `0.57.1`\n\n### Fixed\n- Crash when parsing implicit years before 1970\n\n## [0.15.0] - 2018-06-21\n### Changed\n- Language resources are now packaged separately from the Snips NLU core\nlibrary, and can be fetched using `snips-nlu download <language>`.\n- The CLI tool now consists in a single entry point, `snips-nlu`, which exposes\nseveral commands.\n\n### Added\n- CLI command to parse a query\n\n\n## [0.14.0] - 2018-06-08\n### Fixed\n- Issue due to caching of builtin entities at inference time\n\n### Changed\n- Improve builtin entities handling during intent classification\n- Improve builtin entities handling in `DeterministicIntentParser`\n- Reduce size of regex patterns in trained model file\n- Update model version to `0.15.0`\n\n## [0.13.5] - 2018-05-23\n### Fixed\n- Fixed synonyms matching by using the normalized version of the tagged values\n- Fixed dataset augmentation by keeping stripped values of entities\n- Fixed the string variations functions to prevent generating too many variations   \n\n## [0.13.4] - 2018-05-18\n### Added\n- Documentation for the `None` intent\n\n### Changed\n- Improve calibration of intent classification probabilities\n- Update snips-nlu-ontology version to 0.55.0\n\n### Fixed\n- DeterministicIntentParser: Fix bug when deduplicating regexes\n- DeterministicIntentParser: Fix issue with incorrect ranges when parsing sentences with both builtin and custom slots\n- DeterministicIntentParser: Fix issue with builtin entities placeholders causing mismatches\n- Fix issue with engine-inference CLI script not loading resources correctly \n\n## [0.13.3] - 2018-04-24\n### Added\n- Add config parameter to augment data with builtin entity examples\n\n### Changed\n- Bump snips-nlu-ontology to 0.54.3\n- Use language specific configs by default\n- Add right space to chunks in data augmentation\n- Update JA config\n\n### Fixed\n- Fix inconsistency bug with shape ngram CRF feature\n- Fix bug when initializing `CRFSlotFiller` with config dict\n- Fix bug with gazetteer in ngram feature\n- Fix bug with length CRF feature\n\n## [0.13.1] - 2018-04-10\n### Changed\n- Bump ontology version from 0.54.1 to 0.54.2\n\n### Fixed\n- Fix CRF parsing of builtin entities by adding builtin entities examples of different length\n- Fix CLI scripts importing metrics package which might not be installed\n\n## [0.13.0] - 2018-04-06\n### Added\n- Add contributing guidelines, code of conduct, authors and contributors\n- Add integration test\n- Add CHANGELOG\n\n### Changed\n- Bump model version from 0.13.0 to 0.14\n- Improve intent classification by leveraging builtin entities\n- Improve loading of language specific resources\n- Improve support of japanese\n\n### Removed\n- Remove `exhaustive_permutations_threshold` parameter in config\n\n### Fixed\n- Fix compiling issue with `bindgen` dependency when installing from source\n- Fix issue in `CRFSlotFiller` when handling builtin entities\n\n[0.20.1]: https://github.com/snipsco/snips-nlu/compare/0.20.0...0.20.1\n[0.20.0]: https://github.com/snipsco/snips-nlu/compare/0.19.8...0.20.0\n[0.19.8]: https://github.com/snipsco/snips-nlu/compare/0.19.7...0.19.8\n[0.19.7]: https://github.com/snipsco/snips-nlu/compare/0.19.6...0.19.7\n[0.19.6]: https://github.com/snipsco/snips-nlu/compare/0.19.5...0.19.6\n[0.19.5]: https://github.com/snipsco/snips-nlu/compare/0.19.4...0.19.5\n[0.19.4]: https://github.com/snipsco/snips-nlu/compare/0.19.3...0.19.4\n[0.19.3]: https://github.com/snipsco/snips-nlu/compare/0.19.2...0.19.3\n[0.19.2]: https://github.com/snipsco/snips-nlu/compare/0.19.1...0.19.2\n[0.19.1]: https://github.com/snipsco/snips-nlu/compare/0.19.0...0.19.1\n[0.19.0]: https://github.com/snipsco/snips-nlu/compare/0.18.0...0.19.0\n[0.18.0]: https://github.com/snipsco/snips-nlu/compare/0.17.4...0.18.0\n[0.17.4]: https://github.com/snipsco/snips-nlu/compare/0.17.3...0.17.4\n[0.17.3]: https://github.com/snipsco/snips-nlu/compare/0.17.2...0.17.3\n[0.17.2]: https://github.com/snipsco/snips-nlu/compare/0.17.1...0.17.2\n[0.17.1]: https://github.com/snipsco/snips-nlu/compare/0.17.0...0.17.1\n[0.17.0]: https://github.com/snipsco/snips-nlu/compare/0.16.5...0.17.0\n[0.16.5]: https://github.com/snipsco/snips-nlu/compare/0.16.4...0.16.5\n[0.16.4]: https://github.com/snipsco/snips-nlu/compare/0.16.3...0.16.4\n[0.16.3]: https://github.com/snipsco/snips-nlu/compare/0.16.2...0.16.3\n[0.16.2]: https://github.com/snipsco/snips-nlu/compare/0.16.1...0.16.2\n[0.16.1]: https://github.com/snipsco/snips-nlu/compare/0.16.0...0.16.1\n[0.16.0]: https://github.com/snipsco/snips-nlu/compare/0.15.1...0.16.0\n[0.15.1]: https://github.com/snipsco/snips-nlu/compare/0.15.0...0.15.1\n[0.15.0]: https://github.com/snipsco/snips-nlu/compare/0.14.0...0.15.0\n[0.14.0]: https://github.com/snipsco/snips-nlu/compare/0.13.5...0.14.0\n[0.13.5]: https://github.com/snipsco/snips-nlu/compare/0.13.4...0.13.5\n[0.13.4]: https://github.com/snipsco/snips-nlu/compare/0.13.3...0.13.4\n[0.13.3]: https://github.com/snipsco/snips-nlu/compare/0.13.2...0.13.3\n[0.13.1]: https://github.com/snipsco/snips-nlu/compare/0.13.0...0.13.1\n[0.13.0]: https://github.com/snipsco/snips-nlu/compare/0.12.1...0.13.0\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.169921875,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at adrien.ball@snips.ai or clement.doumouro@snips.ai. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
        },
        {
          "name": "CONTRIBUTING.rst",
          "type": "blob",
          "size": 1.3134765625,
          "content": "How to Contribute\n=================\n\nContributions are welcome! Not familiar with the codebase yet? No problem!\nThere are many ways to contribute to open source projects: reporting bugs,\nhelping with the documentation, spreading the word and of course, adding\nnew features and patches.\n\nGetting Started\n---------------\n* Make sure you have a GitHub account.\n* Open a `new issue <https://github.com/snipsco/snips-nlu/issues>`_, assuming one does not already exist.\n* Clearly describe the issue including steps to reproduce when it is a bug.\n\nMaking Changes\n--------------\n* Fork this repository.\n* Create a feature branch from where you want to base your work.\n* Make commits of logical units (if needed rebase your feature branch before\n  submitting it).\n* Check that your changes are `PEP8 <https://www.python.org/dev/peps/pep-0008/>`_ compliant (for instance using Pycharm or pylint).\n* Make sure your commit messages are well formatted.\n* If your commit fixes an open issue, reference it in the commit message (f.e. ``#15``).\n* Run all the tests (if existing) to assure nothing else was accidentally broken.\n\nThese guidelines also apply when helping with documentation.\n\nSubmitting Changes\n------------------\n* Push your changes to a feature branch in your fork of the repository.\n* Submit a ``Pull Request``.\n* Wait for maintainer feedback.\n"
        },
        {
          "name": "CONTRIBUTORS.rst",
          "type": "blob",
          "size": 0.376953125,
          "content": "Contributors\n============\n\nThis is a list of everyone who has made contributions to Snips NLU, in alphabetical order. Thanks a lot for the great work!\n\n* `Alice Coucke <https://github.com/choufractal>`_\n* `cclauss <https://github.com/cclauss>`_\n* `ddorian <https://github.com/ddorian>`_\n* `Josh Meyer <https://github.com/JRMeyer>`_\n* `Matthieu Brouillard <https://github.com/McFoggy>`_\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.10546875,
          "content": "recursive-include snips_nlu_samples *\ninclude snips_nlu/data/.gitignore\nglobal-exclude __pycache__ *.py[cod]"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 10.341796875,
          "content": "Snips NLU\n=========\n\n.. image:: https://travis-ci.org/snipsco/snips-nlu.svg?branch=master\n   :target: https://travis-ci.org/snipsco/snips-nlu\n\n.. image:: https://ci.appveyor.com/api/projects/status/github/snipsco/snips-nlu?branch=master&svg=true\n   :target: https://ci.appveyor.com/project/snipsco/snips-nlu\n\n.. image:: https://img.shields.io/pypi/v/snips-nlu.svg?branch=master\n   :target: https://pypi.python.org/pypi/snips-nlu\n\n.. image:: https://img.shields.io/pypi/pyversions/snips-nlu.svg?branch=master\n   :target: https://pypi.python.org/pypi/snips-nlu\n\n.. image:: https://codecov.io/gh/snipsco/snips-nlu/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/snipsco/snips-nlu\n\n.. image:: https://img.shields.io/twitter/url/http/shields.io.svg?style=social\n   :target: https://twitter.com/intent/tweet?text=Extract%20meaning%20from%20text%20with%20Snips%20NLU,%20an%20open%20source%20library%20written%20in%20python%20and%20rust&url=https://github.com/snipsco/snips-nlu&via=snips&hashtags=nlu,nlp,machinelearning,python,rustlang\n\n\n`Snips NLU <https://snips-nlu.readthedocs.io>`_ (Natural Language Understanding) is a Python library that allows to extract structured information from sentences written in natural language.\n\nSummary\n-------\n\n- `What is Snips NLU about ?`_\n- `Getting Started`_\n\n  - `System requirements`_\n  - `Installation`_\n  - `Language Resources`_\n- `API Usage`_\n\n  - `Sample code`_\n  - `Command Line Interface`_\n- `Sample datasets`_\n- `Benchmarks`_\n- `Documentation`_\n- `Citing Snips NLU`_\n- `FAQ & Community`_\n- `Related content`_\n- `How do I contribute ?`_\n- `Licence`_\n\nWhat is Snips NLU about ?\n-------------------------\n\nBehind every chatbot and voice assistant lies a common piece of technology: Natural Language Understanding (NLU). Anytime a user interacts with an AI using natural language, their words need to be translated into a machine-readable description of what they meant.\n\nThe NLU engine first detects what the intention of the user is (a.k.a. `intent`_), then extracts the parameters (called `slots`_) of the query. The developer can then use this to determine the appropriate action or response.\n\n\nLetâ€™s take an example to illustrate this, and consider the following sentence:\n\n.. code-block:: text\n\n    \"What will be the weather in paris at 9pm?\"\n\nProperly trained, the Snips NLU engine will be able to extract structured data such as:\n\n.. code-block:: json\n\n    {\n       \"intent\": {\n          \"intentName\": \"searchWeatherForecast\",\n          \"probability\": 0.95\n       },\n       \"slots\": [\n          {\n             \"value\": \"paris\",\n             \"entity\": \"locality\",\n             \"slotName\": \"forecast_locality\"\n          },\n          {\n             \"value\": {\n                \"kind\": \"InstantTime\",\n                \"value\": \"2018-02-08 20:00:00 +00:00\"\n             },\n             \"entity\": \"snips/datetime\",\n             \"slotName\": \"forecast_start_datetime\"\n          }\n       ]\n    }\n\nIn this case, the identified intent is ``searchWeatherForecast`` and two slots were extracted, a locality and a datetime. As you can see, Snips NLU does an extra step on top of extracting entities: it resolves them. The extracted datetime value has indeed been converted into a handy ISO format.\n\nCheck out our `blog post`_ to get more details about why we built Snips NLU and how it works under the hood. We also published a `paper on arxiv`_, presenting the machine learning architecture of the Snips Voice Platform.\n\n\nGetting Started\n---------------\n\n-------------------\nSystem requirements\n-------------------\n\n- Python 2.7 or Python >= 3.5\n- RAM: Snips NLU will typically use between 100MB and 200MB of RAM, depending on the language and the size of the dataset.\n\n\n------------\nInstallation\n------------\n\n.. code-block:: python\n\n    pip install snips-nlu\n\nWe currently have pre-built binaries (wheels) for ``snips-nlu`` and its\ndependencies for MacOS (10.11 and later), Linux x86_64 and Windows.\n\nFor any other architecture/os `snips-nlu` can be installed from the source\ndistribution. To do so, `Rust <https://www.rust-lang.org/en-US/install.html>`_\nand `setuptools_rust <https://github.com/PyO3/setuptools-rust>`_ must be\ninstalled before running the ``pip install snips-nlu`` command.\n\n------------------\nLanguage resources\n------------------\n\nSnips NLU relies on `external language resources`_ that must be downloaded before the\nlibrary can be used. You can fetch resources for a specific language by\nrunning the following command:\n\n.. code-block:: sh\n\n    python -m snips_nlu download en\n\nOr simply:\n\n.. code-block:: sh\n\n    snips-nlu download en\n\n\nThe list of supported languages is available at \n`this address <https://snips-nlu.readthedocs.io/en/latest/languages.html>`_.\n\nAPI Usage\n---------\n\n----------------------\nCommand Line Interface\n----------------------\n\nThe easiest way to test the abilities of this library is through the command line interface.\n\nFirst, start by training the NLU with one of the `sample datasets`_:\n\n.. code-block:: sh\n    \n    snips-nlu train path/to/dataset.json path/to/output_trained_engine\n\nWhere ``path/to/dataset.json`` is the path to the dataset which will be used during training, and ``path/to/output_trained_engine`` is the location where the trained engine should be persisted once the training is done.\n\nAfter that, you can start parsing sentences interactively by running:\n\n.. code-block:: sh\n    \n    snips-nlu parse path/to/trained_engine\n\nWhere ``path/to/trained_engine`` corresponds to the location where you have stored the trained engine during the previous step.\n\n\n-----------\nSample code\n-----------\n\nHere is a sample code that you can run on your machine after having\ninstalled `snips-nlu`, fetched the english resources and downloaded one of the `sample datasets`_:\n\n.. code-block:: python\n\n    >>> from __future__ import unicode_literals, print_function\n    >>> import io\n    >>> import json\n    >>> from snips_nlu import SnipsNLUEngine\n    >>> from snips_nlu.default_configs import CONFIG_EN\n    >>> with io.open(\"sample_datasets/lights_dataset.json\") as f:\n    ...     sample_dataset = json.load(f)\n    >>> nlu_engine = SnipsNLUEngine(config=CONFIG_EN)\n    >>> nlu_engine = nlu_engine.fit(sample_dataset)\n    >>> text = \"Please turn the light on in the kitchen\"\n    >>> parsing = nlu_engine.parse(text)\n    >>> parsing[\"intent\"][\"intentName\"]\n    'turnLightOn'\n\n\nWhat it does is training an NLU engine on a sample weather dataset and parsing\na weather query.\n\nSample datasets\n---------------\n\nHere is a list of some datasets that can be used to train a Snips NLU engine:\n\n- `Lights dataset <sample_datasets/lights_dataset.json>`_: \"Turn on the lights in the kitchen\", \"Set the light to red in the bedroom\"\n- `Beverage dataset <sample_datasets/beverage_dataset.json>`_: \"Prepare two cups of cappucino\", \"Make me a cup of tea\"\n- `Flights dataset <sample_datasets/flights_dataset.json>`_: \"Book me a flight to go to boston this weekend\", \"book me some tickets from istanbul to moscow in three days\"\n\nBenchmarks\n----------\n\nIn January 2018, we reproduced an `academic benchmark`_ which was published during the summer 2017. In this article, authors assessed the performance of API.ai (now Dialogflow, Google), Luis.ai (Microsoft), IBM Watson, and `Rasa NLU`_. For fairness, we used an updated version of Rasa NLU and compared it to the latest version of Snips NLU (both in dark blue).\n\n.. image:: .img/benchmarks.png\n\nIn the figure above, `F1 scores`_ of both intent classification and slot filling were computed for several NLU providers, and averaged across the three datasets used in the academic benchmark mentionned before. All the underlying results can be found `here <https://github.com/snipsco/nlu-benchmark/tree/master/2018-01-Braum-et-al-extension>`_.\n\n\nDocumentation\n-------------\n\nTo find out how to use Snips NLU please refer to the `package documentation <https://snips-nlu.readthedocs.io>`_, it will provide you with a step-by-step guide on how to setup and use this library.\n\nCiting Snips NLU\n----------------\n\nPlease cite the following paper when using Snips NLU:\n\n.. code-block:: bibtex\n\n   @article{coucke2018snips,\n     title   = {Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces},\n     author  = {Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Th{\\'e}odore and Caulier, Alexandre and Leroy, David and Doumouro, Cl{\\'e}ment and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and others},\n     journal = {arXiv preprint arXiv:1805.10190},\n     pages   = {12--16},\n     year    = {2018}\n   }\n\nFAQ & Community\n---------------\n\nPlease join the `forum`_ to ask your questions and get feedback from the community.\n\nRelated content\n---------------\n* `What is Snips about ? <https://snips.ai/>`_\n* Snips NLU Open sourcing `blog post`_\n* `Snips Voice Platform paper (arxiv) <https://arxiv.org/abs/1805.10190>`_\n* `Snips NLU Language Resources <https://github.com/snipsco/snips-nlu-language-resources>`_\n* `Bug tracker <https://github.com/snipsco/snips-nlu/issues>`_\n* `Snips NLU Rust <https://github.com/snipsco/snips-nlu-rs>`_: Rust inference pipeline implementation and bindings (C, Swift, Kotlin, Python)\n* `Rustling <https://github.com/snipsco/rustling-ontology>`_: Snips NLU builtin entities parser\n\n\nHow do I contribute ?\n---------------------\n\nPlease see the `Contribution Guidelines <CONTRIBUTING.rst>`_.\n\nLicence\n-------\n\nThis library is provided by `Snips <https://www.snips.ai>`_ as Open Source software. See `LICENSE <LICENSE>`_ for more information.\n\n\nGeonames Licence\n----------------\n\nThe `snips/city`, `snips/country` and `snips/region` builtin entities rely on\nsoftware from Geonames, which is made available under a Creative Commons Attribution 4.0\nlicense international. For the license and warranties for Geonames please refer to: https://creativecommons.org/licenses/by/4.0/legalcode.\n\n\n.. _external language resources: https://github.com/snipsco/snips-nlu-language-resources\n.. _forum: https://forum.snips.ai/\n.. _blog post: https://medium.com/snips-ai/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a\n.. _paper on arxiv: https://arxiv.org/abs/1805.10190\n.. _academic benchmark: http://workshop.colips.org/wochat/@sigdial2017/documents/SIGDIAL22.pdf\n.. _Rasa NLU: https://nlu.rasa.ai/\n.. _F1 scores: https://en.wikipedia.org/wiki/F1_score\n.. _intent: https://snips-nlu.readthedocs.io/en/latest/data_model.html#intent\n.. _slots: https://snips-nlu.readthedocs.io/en/latest/data_model.html#slot\n"
        },
        {
          "name": "debug",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "linting_test.py",
          "type": "blob",
          "size": 0.8046875,
          "content": "from __future__ import unicode_literals\n\nimport os\nimport unittest\nfrom pathlib import Path\n\nfrom pylint.lint import Run\n\nROOT_PATH = Path(__file__).parent\n\nRCFILEPATH = ROOT_PATH / \"tools\" / \"pylintrc\"\n\nTESTED_PACKAGES = [\"snips_nlu\", \"snips_nlu_samples\", \"debug\"]\n\n\nclass TestLinting(unittest.TestCase):\n    def test_linting(self):\n        args = [\"--output-format\", \"parseable\", \"--rcfile\", str(RCFILEPATH)]\n        args += all_python_files()\n\n        run = Run(args, exit=False)\n        self.assertEqual(0, run.linter.msg_status)\n\n\ndef all_python_files():\n    files = []\n    for p in TESTED_PACKAGES:\n        for dirpath, _, filenames in os.walk(str(ROOT_PATH / p)):\n            files += [\n                os.sep.join([dirpath, f])\n                for f in filenames if f.endswith(\".py\")\n            ]\n\n    return files\n"
        },
        {
          "name": "sample_datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.6875,
          "content": "import io\nimport os\n\nfrom setuptools import setup, find_packages\n\npackages = [p for p in find_packages()\n            if \"tests\" not in p and \"debug\" not in p]\n\nroot = os.path.abspath(os.path.dirname(__file__))\n\nwith io.open(os.path.join(root, \"snips_nlu\", \"__about__.py\"),\n             encoding=\"utf8\") as f:\n    about = dict()\n    exec(f.read(), about)\n\nwith io.open(os.path.join(root, \"README.rst\"), encoding=\"utf8\") as f:\n    readme = f.read()\n\nrequired = [\n    \"deprecation>=2.0,<3.0\",\n    \"enum34>=1.1,<2.0; python_version<'3.4'\",\n    \"funcsigs>=1.0,<2.0; python_version<'3.4'\",\n    \"future>=0.16,<0.18\",\n    \"num2words>=0.5.6,<0.6\",\n    \"numpy>=1.15,<2.0\",\n    \"pathlib>=1.0,<2.0; python_version<'3.4'\",\n    \"pyaml>=17.0,<20.0\",\n    \"requests>=2.0,<3.0\",\n    \"scikit-learn>=0.20,<0.21; python_version<'3.5'\",\n    \"scikit-learn>=0.21.1,<0.23; python_version>='3.5'\",\n    \"scipy>=1.0,<2.0\",\n    \"sklearn-crfsuite>=0.3.6,<0.4\",\n    \"snips-nlu-parsers>=0.3.1,<0.5\",\n    \"snips-nlu-utils>=0.9,<0.10\",\n]\n\nextras_require = {\n    \"doc\": [\n        \"sphinx>=1.8,<1.9\",\n        \"sphinxcontrib-napoleon>=0.6.1,<0.7\",\n        \"sphinx-rtd-theme>=0.2.4,<0.3\",\n        \"sphinx-tabs>=1.1,<1.2\"\n    ],\n    \"metrics\": [\n        \"snips-nlu-metrics>=0.14.1,<0.15\",\n    ],\n    \"test\": [\n        \"mock>=2.0,<3.0\",\n        \"snips-nlu-metrics>=0.14.1,<0.15\",\n        \"pylint<2\",\n        \"coverage>=4.4.2,<5.0\",\n        \"checksumdir~=1.1.6\",\n    ]\n}\n\nsetup(name=about[\"__title__\"],\n      description=about[\"__summary__\"],\n      long_description=readme,\n      version=about[\"__version__\"],\n      author=about[\"__author__\"],\n      author_email=about[\"__email__\"],\n      license=about[\"__license__\"],\n      url=about[\"__github_url__\"],\n      project_urls={\n          \"Documentation\": about[\"__doc_url__\"],\n          \"Source\": about[\"__github_url__\"],\n          \"Tracker\": about[\"__tracker_url__\"],\n      },\n      install_requires=required,\n      extras_require=extras_require,\n      classifiers=[\n          \"Programming Language :: Python :: 2\",\n          \"Programming Language :: Python :: 2.7\",\n          \"Programming Language :: Python :: 3\",\n          \"Programming Language :: Python :: 3.5\",\n          \"Programming Language :: Python :: 3.6\",\n          \"Programming Language :: Python :: 3.7\",\n          \"Programming Language :: Python :: 3.8\",\n          \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n      ],\n      keywords=\"nlu nlp language machine learning text processing intent\",\n      packages=packages,\n      python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4',\n      include_package_data=True,\n      entry_points={\n          \"console_scripts\": [\n              \"snips-nlu=snips_nlu.cli:main\"\n          ]\n      },\n      zip_safe=False)\n"
        },
        {
          "name": "snips_nlu",
          "type": "tree",
          "content": null
        },
        {
          "name": "snips_nlu_samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 1.2958984375,
          "content": "[tox]\nenvlist = py27, py35, py36, py37, py38, integration-test, codecov, docs-html\n\n[testenv]\nskip_install = true\ncommands =\n    pip install -e \".[test]\" --upgrade --upgrade-strategy eager\n\n    snips-nlu download-all-languages\n    snips-nlu download-language-entities fr\n    snips-nlu download-language-entities en\n    coverage run -m unittest discover\n    python snips_nlu_samples/sample.py\nsetenv=\n    LANG=en_US.UTF-8\n    PYTHONIOENCODING=UTF-8\n\n[testenv:integration-test]\nbasepython = python3.6\nskip_install = true\ncommands =\n    pip install -e \".[test]\" --upgrade --upgrade-strategy eager\n\n    snips-nlu download-all-languages\n\n    python -m unittest discover -p 'linting_test*.py'\n    python -m unittest discover -p 'integration_test*.py'\n\n[testenv:docs-html]\nskip_install = true\ndeps = sphinx\nwhitelist_externals = sh\ncommands =\n    sh docs/check_doc.sh\n    pip install -e \".[test,doc]\" --upgrade --upgrade-strategy eager\n    python -m unittest discover -p 'doctests.py'\n    python -m doctest README.rst\n    sphinx-build -W -b html -d {envtmpdir}/doctrees docs/source {envtmpdir}/html\n\n\n# Uses default basepython otherwise reporting doesn't work on Travis where\n# Python 3.6 is only available in 3.6 jobs.\n[testenv:coverage-report]\ndeps = coverage\nskip_install = true\ncommands =\n    coverage combine\n    coverage report\n"
        }
      ]
    }
  ]
}