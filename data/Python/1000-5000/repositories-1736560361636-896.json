{
  "metadata": {
    "timestamp": 1736560361636,
    "page": 896,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "catalyst-team/catalyst",
      "stars": 3313,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.66796875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*pytest_cache*\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\nbuilds/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n\n\n.DS_Store\n.idea\n.code\n\n*.bak\n*.csv\n*.tsv\n*.ipynb\n*.pt\n*.pth\n\ntmp/\nlogs/\n# Examples - mock data\n!examples/distilbert_text_classification/input/*.csv\n!tests/_tests_nlp_classification/input/*.csv\ntests/logs/\nnotebooks/\n\n_nogit*\n\n### VisualStudioCode ###\n.vscode/*\n.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n\n### VisualStudioCode Patch ###\n# Ignore all local history of files\n.history\n\n# End of https://www.gitignore.io/api/visualstudiocode\n\npresets/\n_todo/\ntests/MNIST/\ncodestyle.txt"
        },
        {
          "name": ".mergify.yml",
          "type": "blob",
          "size": 1.095703125,
          "content": "pull_request_rules:\n  # removes reviews done by collaborators when the pull request is updated\n  - name: remove outdated reviews\n    conditions:\n      - base=master\n    actions:\n      dismiss_reviews:\n\n  # automatic merge for master when required CI passes\n  - name: automatic merge for master when CI passes and 2 review\n    conditions:\n      - \"#approved-reviews-by>=2\"\n      - label!=WIP\n    actions:\n      merge:\n        method: squash\n  # - name: automatic merge for master when CI passes and 2 review\n  #   conditions:\n  #     - \"#approved-reviews-by>=3\"\n  #     - approved-reviews-by=github-actions\n  #     - label!=WIP\n  #   actions:\n  #     merge:\n  #       method: squash\n\n  # deletes the head branch of the pull request, that is the branch which hosts the commits\n  - name: delete head branch after merge\n    conditions:\n      - merged\n    actions:\n      delete_head_branch: {}\n\n  # ask author of PR to resolve conflict\n  - name: ask to resolve conflict\n    conditions:\n      - conflict\n    actions:\n      comment:\n        message: \"This pull request is now in conflicts. @{{ author }}, could you fix it? üôè\"\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.3271484375,
          "content": "repos:\n  - repo: https://github.com/catalyst-team/codestyle\n    rev: 'v21.09.2'\n    hooks:\n      - id: catalyst-make-codestyle\n        args: [--line-length=89]\n  - repo: https://github.com/catalyst-team/codestyle\n    rev: 'v21.09.2'\n    hooks:\n      - id: catalyst-check-codestyle\n        args: [--line-length=89]\nexclude: __init__.py\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 48.525390625,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](http://keepachangelog.com/en/1.0.0/).\n\n## [YY.MM.R] - YYYY-MM-DD\n\n### Added\n\n- `catalyst-tune` for Config API added [#1411](https://github.com/catalyst-team/catalyst/pull/1411)\n\n### Changed\n\n-\n\n### Removed\n\n-\n\n### Fixed\n\n-\n\n\n## [22.02.1] - 2022-02-27\n\n### Added\n\n- `catalyst-run` for Config API support added [#1406](https://github.com/catalyst-team/catalyst/pull/1406)\n\n\n### Fixed\n\n- Logger API naming [#1405](https://github.com/catalyst-team/catalyst/pull/1405)\n\n\n## [22.02] - 2022-02-13\n\n### Tl;dr\n- Catalyst architecture simplification.\n- [#1395](https://github.com/catalyst-team/catalyst/issues/1395), [#1396](https://github.com/catalyst-team/catalyst/issues/1396), [#1397](https://github.com/catalyst-team/catalyst/issues/1397), [#1398](https://github.com/catalyst-team/catalyst/issues/1398), [#1399](https://github.com/catalyst-team/catalyst/issues/1399), [#1400](https://github.com/catalyst-team/catalyst/issues/1400), [#1401](https://github.com/catalyst-team/catalyst/issues/1401), [#1402](https://github.com/catalyst-team/catalyst/issues/1402), [#1403](https://github.com/catalyst-team/catalyst/issues/1403).\n\n### Added\n\n- Additional tests for different hardware accelerators setups. Please check out the `tests/pipelines` folder for more information.\n- `BackwardCallback` and `BackwardCallbackOrder` as an abstraction on top of `loss.backward`. Now you could easily log model gradients or transform them before `OptimizerCallback`.\n- `CheckpointCallbackOrder` for `ICheckpointCallback`.\n\n### Changed\n\n- Minimal python version moved to `3.7`, minimal pytorch version moved to `1.4.0`.\n- Engines rewritten on top of Accelerate. First, we found these two abstractions very close to each other. Second, Accelerate provides additional user-friendly API and more stable API for \"Nvidia APEX\" and \"Facebook Fairscale\" - it does not support them.\n- SelfSupervisedRunner moved to the `examples` folder from the Catalyst API. The only Runners API, that will be supported in the future: `IRunner`, `Runner`, `ISupervisedRunner`, `SupervisedRunner` due to their consistency. If you are interested in any other Runner API - feel free to write your own `CustomRunner` and use `SelfSupervisedRunner` as an example.\n- `Runner.{global/stage}_{batch/loader/epoch}_metrics` renamed to `Runner.{batch/loader/epoch}_metrics`\n- `CheckpointCallback` rewritten from scratch.\n- Catalyst registry moved to full-imports-paths only.\n- Logger API changed to receive `IRunner` for all `log_*` methods.\n- Metric API: `topk_args` renamed to `topk`\n- Contrib API: init imports from `catalyst.contrib` - removed, use `from catalyst.contrib.{smth} import {smth}`. Could be change to full-imports-only in future versions for stability.\n- All quickstarts, minimal examples, notebooks and pipelines moved to new version.\n- Codestyle moved to `89` right margin. Honestly speaking, it's much easier to maintain Catalyst with `89` right margin on MBP'16.\n\n### Removed\n\n- `ITrial` removed.\n- Stages support removed. While we embrace stages in deep learning experiments, current hardware accelerators are not prepared well for such setups. Additionally, ~95% of dl pipelines are single-stage. Multi-stage runner support is under review. For multi-stage support, please define a `CustomRunner` with rewritten API.\n- Config/Hydra API support removed. Config API is under review. For now, you could write your own Config API with [hydra-slayer](https://github.com/catalyst-team/hydra-slayer) if needed.\n- `catalyst-dl` scripts removed. Without Config API we don't need them anymore.\n- `Nvidia Apex`, `Fairscale`, `Albumentations`, `Nifti`, `Hydra` requiremets removed.\n- `OnnxCallback`, `PruningCallback`, `QuantizationCallback`, `TracingCallback` removed from callbacks API. Theese callbacks are under review now.\n\nIf you have any questions on the Catalyst 22 edition updates, please join Catalyst slack for discussion.\n\n\n## [21.12] - 2021-12-28\n\n### Added\n\n- MNIST dataset for SSL banchmark ([#1368](https://github.com/catalyst-team/catalyst/pull/1368))\n- MoveiLens 20M dataset [#1336](https://github.com/catalyst-team/catalyst/pull/1336)\n- logger property for logging customization ([#1372](https://github.com/catalyst-team/catalyst/pull/1372))\n- MacridVAE example ([#1363](https://github.com/catalyst-team/catalyst/pull/1363))\n- SSL benchmark results ([#1374](https://github.com/catalyst-team/catalyst/pull/1374))\n- Neptune example ([#1377](https://github.com/catalyst-team/catalyst/pull/1377))\n- multi-node support for engines ([#1364](https://github.com/catalyst-team/catalyst/pull/1364))\n\n### Changed\n\n- RL examples update to last version ([#1370](https://github.com/catalyst-team/catalyst/pull/1370))\n- DDPLoaderWrapper updated to new version ([#1385](https://github.com/catalyst-team/catalyst/pull/1385))\n- `num_classes` for classification metrics became optional ([#1379](https://github.com/catalyst-team/catalyst/pull/1379))\n- colab ci/cd update to new verion\n\n### Removed\n\n-\n\n### Fixed\n\n- `requests` requirements for `catalyst[cv]` added ([#1371](https://github.com/catalyst-team/catalyst/pull/1370))\n- loader step counter ([#1374](https://github.com/catalyst-team/catalyst/pull/1374))\n- detection example data preprocessing ([#1369](https://github.com/catalyst-team/catalyst/pull/1369))\n- gradient clipping with fp16 runs ([#1378](https://github.com/catalyst-team/catalyst/pull/1378))\n- config API fix for DDP runs ([#1383](https://github.com/catalyst-team/catalyst/pull/1383))\n- checkpoint creation for fp16 engines ([#1382](https://github.com/catalyst-team/catalyst/pull/1382))\n\n## [21.11] - 2021-11-30\n\n### Added\n\n- MultiVAE RecSys example ([#1340](https://github.com/catalyst-team/catalyst/pull/1340))\n- Returned `resume` support - resolved [#1193](https://github.com/catalyst-team/catalyst/issues/1193) ([#1349](https://github.com/catalyst-team/catalyst/pull/1349))\n- Smoothing dice loss to contrib ([#1344](https://github.com/catalyst-team/catalyst/pull/1344))\n- `profile` flag for `runner.train` ([#1348](https://github.com/catalyst-team/catalyst/pull/1348))\n- MultiDAE RecSys example ([#1356](https://github.com/catalyst-team/catalyst/pull/1356))\n- `SETTINGS.log_batch_metrics`, `SETTINGS.log_epoch_metrics`, `SETTINGS.compute_per_class_metrics` for framework-wise  Metric & Logger APIs specification ([#1357](https://github.com/catalyst-team/catalyst/pull/1357))\n- `log_batch_metrics` and `log_epoch_metrics` options for all available Loggers ([#1357](https://github.com/catalyst-team/catalyst/pull/1357))\n- `compute_per_class_metrics` option for all available multiclass/label metrics ([#1357](https://github.com/catalyst-team/catalyst/pull/1357))\n- pytorch benchmark script and simplified MNIST ([#1360](https://github.com/catalyst-team/catalyst/pull/1360))\n\n### Changed\n\n- A few framework simplifications were made ([#1346](https://github.com/catalyst-team/catalyst/pull/1346)):\n  - `catalyst-contrib` scripts reduced to `collect-env` and `project-embeddings` only\n  - `catalyst-dl` scripts recuded to `run` and `tune` only\n  - `transforms.` prefix deprecated for Catalyst-based transforms\n  - `catalyst.tools` moved to `catalyst.extras`\n  - task-dependent extensions from `catalyst.data` moved to `catalyst.contrib.data`\n  - `catalyst.data.transforms` moved to `catalyst.contrib.data.transforms`\n  - `Normalize`, `ToTensor` transforms renamed to `NormalizeImage`, `ImageToTensor`\n  - metric learning extensions moved to `catalyst.contrib.data`\n  - `catalyst.contrib` moved to code-as-a-documentation development\n  - `catalyst[cv]` and `catalyst[ml]` extensions moved to flatten architecture design; examples: `catalyst.contrib.data.dataset_cv`, `catalyst.contrib.data.dataset_ml`\n  - `catalyst.contrib` moved to flatten architecture design; exampels: `catalyst.contrib.data`, `catalyst.contrib.datasets`, `catalyst.contrib.layers`, `catalyst.contrib.models`, `catalyst.contrib.optimizers`, `catalyst.contrib.schedulers`\n  - internal functionality moved to `***._misc` modules\n  - `catalyst.utils.mixup` moved to `catalyst.utils.torch`\n  - `catalyst.utils.numpy` moved to `catalyst.contrib.utils.numpy`\n- default logging logic moved from \"batch & epoch\" to \"epoch\"-only to save computation time during logging; to respecify, please use:\n  - `SETTINGS.log_batch_metrics=True/False` or `os.environ[\"CATALYST_LOG_BATCH_METRICS\"]`\n  - `SETTINGS.log_epoch_metrics=True/False` or `os.environ[\"CATALYST_LOG_EPOCH_METRICS\"]`\n- default metrics computation moved from \"per-class & aggregations\" to \"aggregations\"-only to save computation time during logging; to respecify, please use:\n  - `SETTINGS.compute_per_class_metrics=True/False` or `os.environ[\"CATALYST_COMPUTE_PER_CLASS_METRICS\"]`\n- no transformations required for MNIST contrib dataset ([#1360](https://github.com/catalyst-team/catalyst/pull/1360)\n\n### Removed\n\n- A few framework simplifications were made ([#1346](https://github.com/catalyst-team/catalyst/pull/1346)):\n  - `catalyst.contrib.pandas`\n  - `catalyst.contrib.parallel`\n  - `catalyst.contrib.models.cv`\n  - a few `catalyst.utils.misc` functions\n  - `catalyst.extras` removed from the public documentation\n\n\n### Fixed\n\n- documentation search error (21.10 only) ([#1346](https://github.com/catalyst-team/catalyst/pull/1346))\n- docs examples ([#1362](https://github.com/catalyst-team/catalyst/pull/1362))\n- Self-Supervised benchmark: ([#1365](https://github.com/catalyst-team/catalyst/pull/1365)), ([#1361](https://github.com/catalyst-team/catalyst/pull/1361))\n\n\n## [21.10] - 2021-10-30\n\n### Added\n\n- RSquareLoss ([#1313](https://github.com/catalyst-team/catalyst/pull/1313))\n- Self-Supervised example updates: ([#1305](https://github.com/catalyst-team/catalyst/pull/1305)), ([#1322](https://github.com/catalyst-team/catalyst/pull/1322)), ([#1325](https://github.com/catalyst-team/catalyst/pull/1325)), ([#1335](https://github.com/catalyst-team/catalyst/pull/1335))\n- Albert training example ([#1326](https://github.com/catalyst-team/catalyst/pull/1326))\n- YOLO-X (new) detection example and refactoring ([#1324](https://github.com/catalyst-team/catalyst/pull/1324))\n- `TopKMetric` abstraction ([#1330](https://github.com/catalyst-team/catalyst/pull/1330))\n\n### Changed\n\n- simlified readme ([#1312](https://github.com/catalyst-team/catalyst/pull/1312))\n- improved DDP tutorial ([#1327](https://github.com/catalyst-team/catalyst/pull/1327))\n- `CMCMetric` renamed from `<prefix>cmc<suffix><k>` to `<prefix>cmc<k><suffix>` ([#1330](https://github.com/catalyst-team/catalyst/pull/1330))\n\n### Removed\n\n-\n\n### Fixed\n\n- Zero seed error ([#1329](https://github.com/catalyst-team/catalyst/pull/1329))\n- updated codestyle issues ([#1331](https://github.com/catalyst-team/catalyst/pull/1331))\n- TopK metrics: ([#1330](https://github.com/catalyst-team/catalyst/pull/1330)), ([#1334](https://github.com/catalyst-team/catalyst/pull/1334)), ([#1339](https://github.com/catalyst-team/catalyst/pull/1339))\n- `--expdir` param for `catalyst-dl run` ([#1338](https://github.com/catalyst-team/catalyst/pull/1338))\n- ControlFlowCallback for distributed setup ([#1341](https://github.com/catalyst-team/catalyst/pull/1341))\n\n\n## [21.09] - 2021-09-30\n\n### Added\n\n- CometLogger support ([#1283](https://github.com/catalyst-team/catalyst/pull/1283))\n- CometLogger examples ([#1287](https://github.com/catalyst-team/catalyst/pull/1287))\n- XLA docs ([#1288](https://github.com/catalyst-team/catalyst/pull/1288))\n- Contarstive loss functions: `NTXentLoss` ([#1278](https://github.com/catalyst-team/catalyst/pull/1278)), `SupervisedContrastiveLoss` ([#1293](https://github.com/catalyst-team/catalyst/pull/1293))\n- Self supervised learning: `ISelfSupervisedRunner`, `SelfSupervisedConfigRunner`, `SelfSupervisedRunner`, `SelfSupervisedDatasetWrapper` ([#1278](https://github.com/catalyst-team/catalyst/pull/1278))\n- SimCLR example ([#1278](https://github.com/catalyst-team/catalyst/pull/1278))\n- Superivised Contrastive example ([#1293](https://github.com/catalyst-team/catalyst/pull/1293))\n- extra warnings for runner-callbacks interaction ([#1295](https://github.com/catalyst-team/catalyst/pull/1295))\n- `CategoricalRegressionLoss` and `QuantileRegressionLoss` to the `contrib` ([#1295](https://github.com/catalyst-team/catalyst/pull/1295))\n- R2 score metric ([#1274](https://github.com/catalyst-team/catalyst/pull/1274))\n\n\n### Changed\n- Improved `WandbLogger` to support artifacts and fix logging steps ([#1309](https://github.com/catalyst-team/catalyst/pull/1309))\n- full `Runner` cleanup, with callbacks and loaders destruction, moved to `PipelineParallelFairScaleEngine` only ([#1295](https://github.com/catalyst-team/catalyst/pull/1295))\n- `HuberLoss` renamed to `HuberLossV0` for the PyTorch compatibility ([#1295](https://github.com/catalyst-team/catalyst/pull/1295))\n- codestyle update ([#1298](https://github.com/catalyst-team/catalyst/pull/1298))\n- BalanceBatchSampler - deprecated ([#1303](https://github.com/catalyst-team/catalyst/pull/1303))\n\n### Removed\n\n-\n\n### Fixed\n\n- CI/CD ([#1292](https://github.com/catalyst-team/catalyst/pull/1292)), ([#1299](https://github.com/catalyst-team/catalyst/pull/1299)), ([#1304](https://github.com/catalyst-team/catalyst/pull/1304)), ([#1306](https://github.com/catalyst-team/catalyst/pull/1306))\n- Optuna configs ([#1296](https://github.com/catalyst-team/catalyst/pull/1292)), ([#1296](https://github.com/catalyst-team/catalyst/pull/1299))\n\n\n## [21.08] - 2021-08-31\n\n### Added\n\n- RecSys loss functions: `AdaptiveHingeLoss`, `BPRLoss`, `HingeLoss`, `LogisticLoss`, `RocStarLoss`, `WARPLoss` ([#1269](https://github.com/catalyst-team/catalyst/pull/1269), [#1282](https://github.com/catalyst-team/catalyst/pull/1282))\n- object detection examples ([#1271](https://github.com/catalyst-team/catalyst/pull/1271))\n- SklearnModelCallback ([#1261](https://github.com/catalyst-team/catalyst/pull/1261))\n- Barlow Twins example ([#1261](https://github.com/catalyst-team/catalyst/pull/1261))\n- TPU/XLA support ([#1275](https://github.com/catalyst-team/catalyst/pull/1275))\n  - with updated [example](./examples/engines)\n- native `sync_bn` support for all available engines ([#1275](https://github.com/catalyst-team/catalyst/pull/1275))\n  - Torch, AMP, Apex, FairScale\n\n### Changed\n\n- Registry moved to `hydra-slayer` ([#1264)](https://github.com/catalyst-team/catalyst/pull/1264))\n- ([#1275](https://github.com/catalyst-team/catalyst/pull/1275))\n  - batch metrics sync removed from ddp-runs to speedup training process\n  - `AccumulationMetric` renamed to `AccumulativeMetric`\n    - moved from `catalyst.metrics._metric` to `catalyst.metrics._accumulative`\n    - `accululative_fields` renamed to `keys`\n\n\n### Removed\n\n-\n\n### Fixed\n\n- PeriodicLoaderCallback docsting ([#1279](https://github.com/catalyst-team/catalyst/pull/1279))\n- matplotlib issue ([#1272](https://github.com/catalyst-team/catalyst/pull/1272))\n- sample counter for the loader ([#1285](https://github.com/catalyst-team/catalyst/pull/1285))\n\n\n## [21.07] - 2021-07-29\n\n### Added\n\n- added `pre-commit` hook to run codestyle checker on commit ([#1257](https://github.com/catalyst-team/catalyst/pull/1257))\n- `on publish` github action for docker and docs added ([#1260](https://github.com/catalyst-team/catalyst/pull/1260))\n- MixupCallback and `utils.mixup_batch` ([#1241](https://github.com/catalyst-team/catalyst/pull/1241))\n- Barlow twins loss ([#1259](https://github.com/catalyst-team/catalyst/pull/1259))\n- BatchBalanceClassSampler ([#1262](https://github.com/catalyst-team/catalyst/pull/1262))\n\n### Changed\n\n-\n\n### Removed\n\n-\n\n### Fixed\n\n- make `expdir` in `catalyst-dl run` optional ([#1249](https://github.com/catalyst-team/catalyst/pull/1249))\n- Bump neptune-client from 0.9.5 to 0.9.8 in `requirements-neptune.txt` ([#1251](https://github.com/catalyst-team/catalyst/pull/1251))\n- automatic merge for master (with [Mergify](https://mergify.io/)) fixed ([#1250](https://github.com/catalyst-team/catalyst/pull/1250))\n- Evaluate loader custom model bug was fixed ([#1254](https://github.com/catalyst-team/catalyst/pull/1254))\n- `BatchPrefetchLoaderWrapper` issue with batch-based PyTorch samplers ([#1262](https://github.com/catalyst-team/catalyst/pull/1262))\n- Adapted MlflowLogger for new config hierarchy ([#1263](https://github.com/catalyst-team/catalyst/pull/1263))\n\n\n## [21.06] - 2021-06-29\n\n### Added\n\n- ([#1230](https://github.com/catalyst-team/catalyst/pull/1230))\n  - FairScale support\n  - DeepSpeed support\n  - `utils.ddp_sync_run` function for synchronous ddp run\n  - CIFAR10 and CIFAR100 datasets from torchvision (no cv-based requirements)\n  - [Catalyst Engines demo](https://github.com/catalyst-team/catalyst/tree/master/examples/engines)\n- `dataset_from_params` support in config API ([#1231](https://github.com/catalyst-team/catalyst/pull/1231))\n- transform from params support for config API added ([#1236](https://github.com/catalyst-team/catalyst/pull/1236))\n- samplers from params support for config API added ([#1240](https://github.com/catalyst-team/catalyst/pull/1240))\n- recursive registry.get_from_params added ([#1241](https://github.com/catalyst-team/catalyst/pull/1241))\n- albumentations integration ([#1238](https://github.com/catalyst-team/catalyst/pull/1238))\n- Profiler callback ([#1226](https://github.com/catalyst-team/catalyst/pull/1226))\n\n### Changed\n\n- ([#1230](https://github.com/catalyst-team/catalyst/pull/1230))\n  - loaders creation now wrapper with `utils.ddp_sync_run` for `utils.ddp_sync_run` data preparation\n  - runner support stage cleanup: loaders and callbacks will be deleted on the stage end\n  - Apex-based engines now support both APEXEngine and ApexEngine registry names\n\n### Removed\n\n-\n\n### Fixed\n\n- multiprocessing in minimal tests hotfix ([#1232](https://github.com/catalyst-team/catalyst/pull/1232))\n- Tracing callback hotfix ([#1234](https://github.com/catalyst-team/catalyst/pull/1234))\n- Engine hotfix for `predict_loader` ([#1235](https://github.com/catalyst-team/catalyst/pull/1235))\n- ([#1230](https://github.com/catalyst-team/catalyst/pull/1230))\n  - Hydra hotfix due to `1.1.0` version changes\n- `HuberLoss` name conflict for pytorch 1.9 hotfix ([#1239](https://github.com/catalyst-team/catalyst/pull/1239))\n\n\n## [21.05] - 2021-05-31\n\n### Added\n\n- Reinforcement learning tutorials ([#1205](https://github.com/catalyst-team/catalyst/pull/1205))\n- customization demo ([#1207](https://github.com/catalyst-team/catalyst/pull/1207))\n- FAQ docs: multiple input and output keys, engine tutorial ([#1202](https://github.com/catalyst-team/catalyst/pull/1202))\n- minimal Config API example ([#1215](https://github.com/catalyst-team/catalyst/pull/1215))\n- Distributed RL example (Catalyst.RL 2.0 concepts) ([#1224](https://github.com/catalyst-team/catalyst/pull/1224))\n- SklearnCallback as integration of sklearn metrics ([#1198](https://github.com/catalyst-team/catalyst/pull/1198))\n\n### Changed\n\n- tests moved to `tests` folder ([#1208](https://github.com/catalyst-team/catalyst/pull/1208))\n- pipeline tests moved to `tests/pipelines` ([#1215](https://github.com/catalyst-team/catalyst/pull/1215))\n- updated NeptuneLogger docstrings ([#1223](https://github.com/catalyst-team/catalyst/pull/1223))\n\n### Removed\n\n-\n\n### Fixed\n\n- customizing what happens in `train()` notebook ([#1203](https://github.com/catalyst-team/catalyst/pull/1203))\n- transforms imports under catalyst.data ([#1211](https://github.com/catalyst-team/catalyst/pull/1211))\n- change layerwise to layerwise_params ([#1210](https://github.com/catalyst-team/catalyst/pull/1210))\n- add torch metrics support ([#1195](https://github.com/catalyst-team/catalyst/issues/1195))\n- add Config API support for BatchTransformCallback ([#1209](https://github.com/catalyst-team/catalyst/issues/1209))\n\n\n## [21.04.2] - 2021-04-30\n\n### Added\n\n- Weights and Biases Logger (``WandbLogger``) ([#1176](https://github.com/catalyst-team/catalyst/pull/1176))\n- Neptune Logger (``NeptuneLogger``) ([#1196](https://github.com/catalyst-team/catalyst/pull/1196))\n- `log_artifact` method for logging arbitrary files like audio, video, or model weights to `ILogger` and `IRunner` ([#1196](https://github.com/catalyst-team/catalyst/pull/1196))\n\n## [21.04/21.04.1] - 2021-04-17\n\n### Added\n\n- Nifti Reader (NiftiReader) ([#1151](https://github.com/catalyst-team/catalyst/pull/1151))\n- CMC score and callback for ReID task (ReidCMCMetric and ReidCMCScoreCallback) ([#1170](https://github.com/catalyst-team/catalyst/pull/1170))\n- Market1501 metric learning datasets (Market1501MLDataset and Market1501QGDataset) ([#1170](https://github.com/catalyst-team/catalyst/pull/1170))\n- extra kwargs support for Engines ([#1156](https://github.com/catalyst-team/catalyst/pull/1156))\n- engines exception for unknown model type ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n- a few docs to the supported loggers ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n\n### Changed\n\n- ``TensorboardLogger`` switched from ``global_batch_step`` counter to ``global_sample_step`` one ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n- ``TensorboardLogger`` logs loader metric ``on_loader_end`` rather than ``on_epoch_end`` ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n- ``prefix`` renamed to ``metric_key`` for ``MetricAggregationCallback`` ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n- ``micro``, ``macro`` and ``weighted`` aggregations renamed to ``_micro``, ``_macro`` and ``_weighted`` ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n- ``BatchTransformCallback`` updated ([#1153](https://github.com/catalyst-team/catalyst/issues/1153))\n\n### Removed\n\n- auto ``torch.sigmoid`` usage for ``metrics.AUCMetric`` and ``metrics.auc`` ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n\n### Fixed\n\n- hitrate calculation issue ([#1155](https://github.com/catalyst-team/catalyst/issues/1155))\n- ILoader wrapper usage issue with Runner ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n- counters for ddp case ([#1174](https://github.com/catalyst-team/catalyst/issues/1174))\n\n## [21.03.2] - 2021-03-29\n\n### Fixed\n\n- minimal requirements issue ([#1147](https://github.com/catalyst-team/catalyst/issues/1147))\n- nested dicts in `loaders_params`/`samplers_params` overriding ([#1150](https://github.com/catalyst-team/catalyst/pull/1150))\n\n## [21.03.1] - 2021-03-28\n\n### Added\n\n- Additive Margin SoftMax(AMSoftmax) ([#1125](https://github.com/catalyst-team/catalyst/issues/1125))\n- Generalized Mean Pooling(GeM) ([#1084](https://github.com/catalyst-team/catalyst/issues/1084))\n- Key-value support for CriterionCallback ([#1130](https://github.com/catalyst-team/catalyst/issues/1130))\n- Engine configuration through cmd ([#1134](https://github.com/catalyst-team/catalyst/issues/1134))\n- Extra utils for thresholds ([#1134](https://github.com/catalyst-team/catalyst/issues/1134))\n- Added gradient clipping function to optimizer callback ([1124](https://github.com/catalyst-team/catalyst/pull/1124))\n- FactorizedLinear to contrib ([1142](https://github.com/catalyst-team/catalyst/pull/1142))\n- Extra init params for ``ConsoleLogger`` ([1142](https://github.com/catalyst-team/catalyst/pull/1142))\n- Tracing, Quantization, Onnx, Pruninng Callbacks ([1127](https://github.com/catalyst-team/catalyst/pull/1127))\n\n\n### Changed\n\n- CriterionCallback now inherits from BatchMetricCallback [#1130](https://github.com/catalyst-team/catalyst/issues/1130))\n    - united metrics computation logic\n\n### Removed\n\n- Config API deprecated parsings logic ([1142](https://github.com/catalyst-team/catalyst/pull/1142)) ([1138](https://github.com/catalyst-team/catalyst/pull/1138))\n\n### Fixed\n\n- Data-Model device sync and ``Engine`` logic during `runner.predict_loader` ([#1134](https://github.com/catalyst-team/catalyst/issues/1134))\n- BatchLimitLoaderWrapper logic for loaders with shuffle flag ([#1136](https://github.com/catalyst-team/catalyst/issues/1136))\n- config description in the examples ([1142](https://github.com/catalyst-team/catalyst/pull/1142))\n- Config API deprecated parsings logic ([1142](https://github.com/catalyst-team/catalyst/pull/1142)) ([1138](https://github.com/catalyst-team/catalyst/pull/1138))\n- RecSys metrics Top_k calculations ([#1140](https://github.com/catalyst-team/catalyst/pull/1140))\n- `_key_value` for schedulers in case of multiple optimizers ([#1146](https://github.com/catalyst-team/catalyst/pull/1146))\n\n## [21.03] - 2021-03-13 ([#1095](https://github.com/catalyst-team/catalyst/issues/1095))\n\n### Added\n\n- [``Engine`` abstraction](https://catalyst-team.github.io/catalyst/api/engines.html) to support various hardware backends and accelerators: CPU, GPU, multi GPU, distributed GPU, TPU, Apex, and AMP half-precision training.\n- [``Logger`` abstraction](https://catalyst-team.github.io/catalyst/api/loggers.html) to support various monitoring tools: console, tensorboard, MLflow, etc.\n- ``Trial`` abstraction to support various hyperoptimization tools: Optuna, Ray, etc.\n- [``Metric`` abstraction](https://catalyst-team.github.io/catalyst/api/metrics.html) to support various of machine learning metrics: classification, segmentation, RecSys and NLP.\n- Full support for Hydra API.\n- Full DDP support for Python API.\n- MLflow support for metrics logging.\n- United API for model post-processing: tracing, quantization, pruning, onnx-exporting.\n- United API for metrics: classification, segmentation, RecSys, and NLP with full DDP and micro/macro/weighted/etc aggregations support.\n\n### Changed\n\n- ``Experiment`` abstraction merged into ``Runner`` one.\n- Runner, SupervisedRunner, ConfigRunner, HydraRunner architectures and dependencies redesigned.\n- Internal [settings](https://github.com/catalyst-team/catalyst/blob/master/catalyst/settings.py) and [registry](https://github.com/catalyst-team/catalyst/blob/master/catalyst/registry.py) mechanisms refactored to be simpler, user-friendly and more extendable.\n- Bunch of Config API test removed with Python API and pytest.\n- Codestyle now supports up to 99 symbols per line :)\n- All callbacks/runners moved for contrib to the library core if was possible.\n- ``Runner`` abstraction simplified to store only current state of the experiment run: all validation logic was moved to the callbacks (by this way, you could easily select best model on various metrics simultaneously).\n- ``Runner.input`` and ``Runner.output`` merged into united ``Runner.batch`` storage for simplicity.\n- All metric moved from ``catalyst.utils.metrics`` to ``catalyst.metrics``.\n- All metrics now works on scores/metric-defined-input rather that logits (!).\n- Logging logic moved from ``Callbacks`` to appropriate ``Loggers``.\n- ``KorniaCallbacks`` refactored to ``BatchTransformCallback``.\n\n### Removed\n\n- Lots of unnecessary contrib extensions.\n- Transforms configuration support through Config API (could be returned in next releases).\n- Integrated Python cmd command for model pruning, swa, etc (should be returned in next releases).\n- ``CallbackOrder.Validation`` and ``CallbackOrder.Logging``\n- All 2020 year backward compatibility fixes and legacy support.\n\n### Fixed\n\n- Docs rendering simplified.\n- LrFinderCallback.\n\n[Release docs](https://catalyst-team.github.io/catalyst/v21.03/index.html),\n[Python API minimal examples](https://github.com/catalyst-team/catalyst#minimal-examples),\n[Config/Hydra API example](https://github.com/catalyst-team/catalyst/tree/master/examples/mnist_stages).\n\n## [20.12.1] - XXXX-XX-XX\n\n\n### Added\n\n- Inference mode for face layers ([#1045](https://github.com/catalyst-team/catalyst/pull/1045))\n\n### Fixed\n\n- Fix bug in `OptimizerCallback` when mixed-precision params set both:\n  in callback arguments and in distributed_params  ([#1042](https://github.com/catalyst-team/catalyst/pull/1042))\n\n\n## [20.12] - 2020-12-20\n\n### Added\n\n- CVS Logger ([#1005](https://github.com/catalyst-team/catalyst/pull/1005))\n- DrawMasksCallback ([#999](https://github.com/catalyst-team/catalyst/pull/999))\n- ([#1002](https://github.com/catalyst-team/catalyst/pull/1002))\n    - a few docs\n- ([#998](https://github.com/catalyst-team/catalyst/pull/998))\n    - ``reciprocal_rank`` metric\n    - unified recsys metrics preprocessing\n-  ([#1018](https://github.com/catalyst-team/catalyst/pull/1018))\n    - readme examples for all supported metrics under ``catalyst.metrics``\n    - ``wrap_metric_fn_with_activation`` for model outputs wrapping with activation\n    -  extra tests for metrics\n- ([#1039](https://github.com/catalyst-team/catalyst/pull/1039))\n    - ``per_class=False`` option for metrics callbacks\n    - ``PrecisionCallack``, ``RecallCallack`` for multiclass problems\n    - extra docs\n\n### Changed\n\n- docs update ([#1000](https://github.com/catalyst-team/catalyst/pull/1000))\n- ``AMPOptimizerCallback`` and ``OptimizerCallback`` were merged ([#1007](https://github.com/catalyst-team/catalyst/pull/1007))\n- ([#1017](https://github.com/catalyst-team/catalyst/pull/1017))\n    - fixed bug in `SchedulerCallback`\n    - Log LRs and momentums for all param groups, not only for the first one\n- ([#1002](https://github.com/catalyst-team/catalyst/pull/1002))\n    - ``tensorboard, ipython, matplotlib, pandas, scikit-learn`` moved to optional requirements\n    - ``PerplexityMetricCallback`` moved to ``catalyst.callbacks`` from ``catalyst.contrib.callbacks``\n    - ``PerplexityMetricCallback`` renamed to ``PerplexityCallback``\n    - ``catalyst.contrib.utils.confusion_matrix`` renamed to ``catalyst.contrib.utils.torch_extra``\n    - many parts of ``catalyst.data`` moved to ``catalyst.contrib.data``\n    - ``catalyst.data.scripts`` moved to ``catalyst.contrib.scripts``\n    - ``catalyst.utils``, ``catalyst.data.utils`` and ``catalyst.contrib.utils`` restructured\n    - ``ReaderSpec`` renamed to ``IReader``\n    - ``SupervisedExperiment`` renamed to ``AutoCallbackExperiment``\n- gain functions renamed for ``dcg``/``ndcg`` metrics ([#998](https://github.com/catalyst-team/catalyst/pull/998))\n- ([#1014](https://github.com/catalyst-team/catalyst/pull/1014))\n    - requirements respecification: ``catalyst[cv]``, ``catalyst[dev]``, ``catalyst[log]``, ``catalyst[ml]``, ``catalyst[nlp]``,``catalyst[tune]``\n    - settings respecification\n    - extra tests for settings\n    - contrib refactoring\n- iou and dice metrics moved to per-class computation ([#1031](https://github.com/catalyst-team/catalyst/pull/1031))\n\n### Removed\n\n- ([#1002](https://github.com/catalyst-team/catalyst/pull/1002))\n    - ``KNNMetricCallback``\n    - ``sklearn`` mode for ``ConfusionMatrixLogger``\n    - ``catalyst.data.utils``\n    - unnecessary ``catalyst.tools.meters``\n    - todos for unnecessary docs\n- ([#1014](https://github.com/catalyst-team/catalyst/pull/1014))\n    - transformers-based contrib (too unstable)\n- ([#1018](https://github.com/catalyst-team/catalyst/pull/1014))\n    - ClasswiseIouCallback/ClasswiseJaccardCallback as deprecated on (should be refactored in future releases)\n\n\n\n### Fixed\n\n- prevented modifying config during the experiment and runner initialization ([#1004](https://github.com/catalyst-team/catalyst/pull/1004))\n- a few test for RecSys MAP computation ([#1018](https://github.com/catalyst-team/catalyst/pull/1014))\n- leave batch size the same for default distributed training ([#1023](https://github.com/catalyst-team/catalyst/issues/1023))\n- ([#1032](https://github.com/catalyst-team/catalyst/pull/1032))\n  - Apex: now you can use apex for multiple models training\n  - Apex: DataParallel is allowed for opt_level other than \"O1\"\n\n\n\n## [20.11] - 2020-11-12\n\n### Added\n- DCG, nDCG metrics ([#881](https://github.com/catalyst-team/catalyst/pull/881))\n- MAP calculations [#968](https://github.com/catalyst-team/catalyst/pull/968)\n- hitrate calculations [#975] (https://github.com/catalyst-team/catalyst/pull/975)\n- extra functions for classification metrics ([#966](https://github.com/catalyst-team/catalyst/pull/966))\n- `OneOf` and `OneOfV2` batch transforms ([#951](https://github.com/catalyst-team/catalyst/pull/951))\n- ``precision_recall_fbeta_support`` metric ([#971](https://github.com/catalyst-team/catalyst/pull/971))\n- Pruning tutorial ([#987](https://github.com/catalyst-team/catalyst/pull/987))\n- BatchPrefetchLoaderWrapper ([#986](https://github.com/catalyst-team/catalyst/pull/986))\n- DynamicBalanceClassSampler ([#954](https://github.com/catalyst-team/catalyst/pull/954))\n\n### Changed\n\n- update Catalyst version to `20.10.1` for tutorials ([#967](https://github.com/catalyst-team/catalyst/pull/967))\n- added link to dl-course ([#967](https://github.com/catalyst-team/catalyst/pull/967))\n- ``IRunner`` -> simplified ``IRunner`` ([#984](https://github.com/catalyst-team/catalyst/pull/984))\n- docs were restructured ([#985](https://github.com/catalyst-team/catalyst/pull/985))\n- `set_global_seed` moved from `utils.seed` to `utils.misc` ([#986](https://github.com/catalyst-team/catalyst/pull/986))\n\n### Removed\n\n- several deprecated tutorials ([#967](https://github.com/catalyst-team/catalyst/pull/967))\n- several deprecated func from utils.misc ([#986](https://github.com/catalyst-team/catalyst/pull/986))\n\n### Fixed\n\n- `BatchTransformCallback` - add `nn.Module` transforms support ([#951](https://github.com/catalyst-team/catalyst/pull/951))\n- moved to `contiguous` view for accuracy computation ([#982](https://github.com/catalyst-team/catalyst/pull/982))\n- fixed torch warning on `optimizer.py:140` ([#979](https://github.com/catalyst-team/catalyst/pull/979))\n\n\n## [20.10.1] - 2020-10-15\n\n### Added\n\n- MRR metrics calculation ([#886](https://github.com/catalyst-team/catalyst/pull/886))\n- docs for MetricCallbacks ([#947](https://github.com/catalyst-team/catalyst/pull/947))\n- SoftMax, CosFace, ArcFace layers to contrib ([#939](https://github.com/catalyst-team/catalyst/pull/939))\n- ArcMargin layer to contrib ([#957](https://github.com/catalyst-team/catalyst/pull/957))\n- AdaCos to contrib ([#958](https://github.com/catalyst-team/catalyst/pull/958))\n- Manual SWA to utils ([#945](https://github.com/catalyst-team/catalyst/pull/945))\n\n### Changed\n\n- fixed path to `CHANGELOG.md` file and add information about unit test to `PULL_REQUEST_TEMPLATE.md` ([#955])(https://github.com/catalyst-team/catalyst/pull/955)\n- `catalyst-dl tune` config specification - now optuna params are grouped under `study_params` ([#947](https://github.com/catalyst-team/catalyst/pull/947))\n- `IRunner._prepare_for_stage` logic moved to `IStageBasedRunner.prepare_for_stage` ([#947](https://github.com/catalyst-team/catalyst/pull/947))\n    - now we create components in the following order: datasets/loaders, model, criterion, optimizer, scheduler, callbacks\n- `MnistMLDataset` and `MnistQGDataset` data split logic - now targets of the datasets are disjoint ([#949](https://github.com/catalyst-team/catalyst/pull/949))\n- architecture redesign ([#953](https://github.com/catalyst-team/catalyst/pull/953))\n    - experiments, runners, callbacks grouped by primitives under `catalyst.experiments`/`catalyst.runners`/`catalyst.callbacks` respectively\n    - settings and typing moved from `catalyst.tools.*` to `catalyst.*`\n    - utils moved from `catalyst.*.utils` to `catalyst.utils`\n- swa moved to `catalyst.utils` ([#963](https://github.com/catalyst-team/catalyst/pull/963))\n\n### Removed\n\n-\n\n### Fixed\n\n- `AMPOptimizerCallback` - fix grad clip fn support ([#948](https://github.com/catalyst-team/catalyst/pull/948))\n- removed deprecated docs types ([#947](https://github.com/catalyst-team/catalyst/pull/947)) ([#952](https://github.com/catalyst-team/catalyst/pull/952))\n- docs for a few files ([#952](https://github.com/catalyst-team/catalyst/pull/952))\n- extra backward compatibility fixes ([#963](https://github.com/catalyst-team/catalyst/pull/963))\n\n\n## [20.09.1] - 2020-09-25\n\n### Added\n\n- Runner registry support for Config API ([#936](https://github.com/catalyst-team/catalyst/pull/936))\n- `catalyst-dl tune` command - Optuna with Config API integration for AutoML hyperparameters optimization ([#937](https://github.com/catalyst-team/catalyst/pull/937))\n- `OptunaPruningCallback` alias for `OptunaCallback` ([#937](https://github.com/catalyst-team/catalyst/pull/937))\n- AdamP and SGDP to `catalyst.contrib.losses` ([#942](https://github.com/catalyst-team/catalyst/pull/942))\n\n### Changed\n\n- Config API components preparation logic moved to ``utils.prepare_config_api_components`` ([#936](https://github.com/catalyst-team/catalyst/pull/936))\n\n### Removed\n\n-\n\n### Fixed\n\n- Logging double logging :) ([#936](https://github.com/catalyst-team/catalyst/pull/936))\n- CMCCallback ([#941](https://github.com/catalyst-team/catalyst/pull/941))\n\n## [20.09] - 2020-09-07\n\n### Added\n\n- `MovieLens dataset` loader ([#903](https://github.com/catalyst-team/catalyst/pull/903))\n- `force` and `bert-level` keywords to `catalyst-data text2embedding` ([#917](https://github.com/catalyst-team/catalyst/pull/917))\n- `OptunaCallback` to `catalyst.contrib` ([#915](https://github.com/catalyst-team/catalyst/pull/915))\n- `DynamicQuantizationCallback` and `catalyst-dl quantize` script for fast quantization of your model ([#890](https://github.com/catalyst-team/catalyst/pull/915))\n- Multi-scheduler support for multi-optimizer case ([#923](https://github.com/catalyst-team/catalyst/pull/923))\n- Native mixed-precision training support ([#740](https://github.com/catalyst-team/catalyst/issues/740))\n- `OptiomizerCallback` - flag `use_fast_zero_grad` for faster (and hacky) version of `optimizer.zero_grad()` ([#927](https://github.com/catalyst-team/catalyst/pull/927))\n- `IOptiomizerCallback`, `ISchedulerCallback`, `ICheckpointCallback`, `ILoggerCallback` as core abstractions for Callbacks ([#933](https://github.com/catalyst-team/catalyst/pull/933))\n- flag `USE_AMP` for PyTorch AMP usage ([#933](https://github.com/catalyst-team/catalyst/pull/933))\n\n### Changed\n\n- Pruning moved to `catalyst.dl` ([#933](https://github.com/catalyst-team/catalyst/pull/933))\n- default `USE_APEX` changed to 0 ([#933](https://github.com/catalyst-team/catalyst/pull/933))\n\n### Removed\n\n-\n\n### Fixed\n\n- autoresume option for Config API ([#907](https://github.com/catalyst-team/catalyst/pull/907))\n- a few issues with TF projector ([#917](https://github.com/catalyst-team/catalyst/pull/917))\n- batch sampler speed issue ([#921](https://github.com/catalyst-team/catalyst/pull/921))\n- add apex key-value optimizer support ([#924](https://github.com/catalyst-team/catalyst/pull/924))\n- runtime warning for PyTorch 1.6 ([920](https://github.com/catalyst-team/catalyst/pull/920))\n- Apex synbn usage ([920](https://github.com/catalyst-team/catalyst/pull/920))\n- Catalyst dependency on system git ([922](https://github.com/catalyst-team/catalyst/pull/922))\n\n\n## [20.08] - 2020-08-09\n\n### Added\n- `CMCScoreCallback` ([#880](https://github.com/catalyst-team/catalyst/pull/880))\n- kornia augmentations `BatchTransformCallback` ([#862](https://github.com/catalyst-team/catalyst/issues/862))\n- `average_precision` and `mean_average_precision` metrics ([#883](https://github.com/catalyst-team/catalyst/pull/883))\n- `MultiLabelAccuracyCallback`, `AveragePrecisionCallback` and `MeanAveragePrecisionCallback` callbacks ([#883](https://github.com/catalyst-team/catalyst/pull/883))\n- minimal examples for multiclass and multilabel classification ([#883](https://github.com/catalyst-team/catalyst/pull/883))\n- experimental TPU support ([#893](https://github.com/catalyst-team/catalyst/pull/893))\n- add `Imagenette`, `Imagewoof`, and `Imagewang` datasets ([#902](https://github.com/catalyst-team/catalyst/pull/902))\n- `IMetricCallback`, `IBatchMetricCallback`, `ILoaderMetricCallback`, `BatchMetricCallback`, `LoaderMetricCallback` abstractions ([#897](https://github.com/catalyst-team/catalyst/pull/897))\n- `HardClusterSampler` inbatch sampler ([#888](https://github.com/catalyst-team/catalyst/pull/888))\n\n### Changed\n\n- all registries merged to one `catalyst.registry` ([#883](https://github.com/catalyst-team/catalyst/pull/883))\n- `mean_average_precision` logic merged with `average_precision` ([#897](https://github.com/catalyst-team/catalyst/pull/897))\n- all imports moved to absolute ([#905](https://github.com/catalyst-team/catalyst/pull/905))\n- `catalyst.contrib.data` merged to `catalyst.data` ([#905](https://github.com/catalyst-team/catalyst/pull/905))\n- {breaking} Catalyst transform `ToTensor` was renamed to `ImageToTensor` ([#905](https://github.com/catalyst-team/catalyst/pull/905))\n- `TracerCallback` moved to `catalyst.dl` ([#905](https://github.com/catalyst-team/catalyst/pull/905))\n- `ControlFlowCallback`, `PeriodicLoaderCallback` moved to `catalyst.core` ([#905](https://github.com/catalyst-team/catalyst/pull/905))\n\n### Removed\n\n- `average_accuracy` and `mean_average_accuracy` metrics ([#883](https://github.com/catalyst-team/catalyst/pull/883))\n- MultiMetricCallback abstraction ([#897](https://github.com/catalyst-team/catalyst/pull/897))\n\n### Fixed\n\n- `utils.tokenize_text` typo with punctuation ([#880](https://github.com/catalyst-team/catalyst/pull/880))\n- `ControlFlowCallback` logic ([#892](https://github.com/catalyst-team/catalyst/pull/892))\n- docs ([#897](https://github.com/catalyst-team/catalyst/pull/897))\n\n\n## [20.07] - 2020-07-06\n\n### Added\n\n- `log` parameter to `WandbLogger` ([#836](https://github.com/catalyst-team/catalyst/pull/836))\n- hparams experiment property ([#839](https://github.com/catalyst-team/catalyst/pull/839))\n- add docs build on push to master branch ([#844](https://github.com/catalyst-team/catalyst/pull/844))\n- `WrapperCallback` and `ControlFlowCallback` ([#842](https://github.com/catalyst-team/catalyst/pull/842))\n- `BatchOverfitCallback` ([#869](https://github.com/catalyst-team/catalyst/pull/869))\n- `overfit` flag for Config API ([#869](https://github.com/catalyst-team/catalyst/pull/869))\n- `InBatchSamplers`: `AllTripletsSampler` and `HardTripletsSampler` ([#825](https://github.com/catalyst-team/catalyst/pull/825))\n\n### Changed\n\n- Renaming ([#837](https://github.com/catalyst-team/catalyst/pull/837))\n    - `SqueezeAndExcitation` -> `cSE`\n    - `ChannelSqueezeAndSpatialExcitation` -> `sSE`\n    - `ConcurrentSpatialAndChannelSqueezeAndChannelExcitation` -> `scSE`\n    - `_MetricCallback` -> `IMetricCallback`\n    - `dl.Experiment.process_loaders` -> `dl.Experiment._get_loaders`\n- `LRUpdater` become abstract class ([#837](https://github.com/catalyst-team/catalyst/pull/837))\n- `calculate_confusion_matrix_from_arrays` changed params order ([#837](https://github.com/catalyst-team/catalyst/pull/837))\n- `dl.Runner.predict_loader` uses `_prepare_inner_state` and cleans `experiment` ([#863](https://github.com/catalyst-team/catalyst/pull/863))\n- `toml` to the dependencies ([#872](https://github.com/catalyst-team/catalyst/pull/872))\n\n### Removed\n\n- `crc32c` dependency ([#872](https://github.com/catalyst-team/catalyst/pull/872))\n\n### Fixed\n\n- `workflows/deploy_push.yml` failed to push some refs ([#864](https://github.com/catalyst-team/catalyst/pull/864))\n- `.dependabot/config.yml` contained invalid details ([#781](https://github.com/catalyst-team/catalyst/issues/781))\n- `LanguageModelingDataset` ([#841](https://github.com/catalyst-team/catalyst/pull/841))\n- `global_*` counters in `Runner` ([#858](https://github.com/catalyst-team/catalyst/pull/858))\n- EarlyStoppingCallback considers first epoch as bad ([#854](https://github.com/catalyst-team/catalyst/issues/854))\n- annoying numpy warning ([#860](https://github.com/catalyst-team/catalyst/pull/860))\n- `PeriodicLoaderCallback` overwrites best state ([#867](https://github.com/catalyst-team/catalyst/pull/867))\n- `OneCycleLRWithWarmup` ([#851](https://github.com/catalyst-team/catalyst/issues/851))\n\n## [20.06] - 2020-06-04\n\n### Added\n\n- `Mergify` ([#831](https://github.com/catalyst-team/catalyst/pull/831))\n- `PerplexityMetricCallback` ([#819](https://github.com/catalyst-team/catalyst/pull/819))\n- `PeriodicLoaderRunnerCallback` ([#818](https://github.com/catalyst-team/catalyst/pull/818))\n\n### Changed\n\n- docs structure were updated during ([#822](https://github.com/catalyst-team/catalyst/pull/822))\n- `utils.process_components` moved from `utils.distributed` to `utils.components` ([#822](https://github.com/catalyst-team/catalyst/pull/822))\n- `catalyst.core.state.State` merged to `catalyst.core.runner._Runner` ([#823](https://github.com/catalyst-team/catalyst/pull/823)) (backward compatibility included)\n    - `catalyst.core.callback.Callback` now works directly with `catalyst.core.runner._Runner`\n    - `state_kwargs` renamed to `stage_kwargs`\n\n### Removed\n\n-\n\n### Fixed\n\n- added missed dashes in docker perfixes ([#828](https://github.com/catalyst-team/catalyst/issues/828))\n- handle empty loader in Runner ([#873](https://github.com/catalyst-team/catalyst/pull/873))\n\n\n## [20.05.1] - 2020-05-23\n\n### Added\n\n- Circle loss implementation ([#802](https://github.com/catalyst-team/catalyst/pull/802))\n- BatchBalanceSampler for metric learning and classification ([#806](https://github.com/catalyst-team/catalyst/pull/806))\n- `CheckpointCallback`: new argument `load_on_stage_start` which accepts `str` and `Dict[str, str]` ([#797](https://github.com/catalyst-team/catalyst/pull/797))\n- LanguageModelingDataset to catalyst\\[nlp\\] ([#808](https://github.com/catalyst-team/catalyst/pull/808))\n- Extra counters for batches, loaders and epochs ([#809](https://github.com/catalyst-team/catalyst/pull/809))\n- `TracerCallback` ([#789](https://github.com/catalyst-team/catalyst/pull/789))\n\n### Changed\n\n- `CheckpointCallback`: additional logic for argument `load_on_stage_end` - accepts `str` and `Dict[str, str]` ([#797](https://github.com/catalyst-team/catalyst/pull/797))\n- counters names for batches, loaders and epochs ([#809](https://github.com/catalyst-team/catalyst/pull/809))\n- `utils.trace_model`: changed logic - `runner` argument was changed to `predict_fn` ([#789](https://github.com/catalyst-team/catalyst/pull/789))\n- redesigned `contrib.data` and `contrib.datasets` ([#820](https://github.com/catalyst-team/catalyst/pull/820))\n- `catalyst.utils.meters` moved to `catalyst.tools` ([#820](https://github.com/catalyst-team/catalyst/pull/820))\n- `catalyst.contrib.utils.tools.tensorboard` moved to `catalyst.contrib.tools` ([#820](https://github.com/catalyst-team/catalyst/pull/820))\n\n### Removed\n\n-\n\n### Fixed\n\n- device selection fix for [#798](https://github.com/catalyst-team/catalyst/issues/798) ([#815](https://github.com/catalyst-team/catalyst/pull/815))\n- batch size counting fix for [#799](https://github.com/catalyst-team/catalyst/issues/799) and [#755](https://github.com/catalyst-team/catalyst/issues/755) issues ([#809](https://github.com/catalyst-team/catalyst/pull/809))\n\n\n## [20.05] - 2020-05-07\n\n### Added\n\n- Added new docs and minimal examples ([#747](https://github.com/catalyst-team/catalyst/pull/747))\n- Added experiment to registry ([#746](https://github.com/catalyst-team/catalyst/pull/746))\n- Added examples with extra metrics ([#750](https://github.com/catalyst-team/catalyst/pull/750))\n- Added VAE example ([#752](https://github.com/catalyst-team/catalyst/pull/752))\n- Added gradient tracking ([#679](https://github.com/catalyst-team/catalyst/pull/679)\n- Added dependabot ([#771](https://github.com/catalyst-team/catalyst/pull/771))\n- Added new test for Config API ([#768](https://github.com/catalyst-team/catalyst/pull/768))\n- Added Visdom logger ([#769](https://github.com/catalyst-team/catalyst/pull/769))\n- Added new github actions and templates ([#777](https://github.com/catalyst-team/catalyst/pull/777))\n- Added `save_n_best=0` support for CheckpointCallback ([#784](https://github.com/catalyst-team/catalyst/pull/784))\n- Added new contrib modules for CV ([#793](https://github.com/catalyst-team/catalyst/pull/793))\n- Added new github actions CI ([#791](https://github.com/catalyst-team/catalyst/pull/791))\n\n### Changed\n\n- Changed `Alchemy` dependency (from `alchemy-catalyst` to `alchemy`) ([#748](https://github.com/catalyst-team/catalyst/pull/748))\n- Changed warnings logic ([#719](https://github.com/catalyst-team/catalyst/pull/719))\n- Github actions CI was updated ([#754](https://github.com/catalyst-team/catalyst/pull/754))\n- Changed default `num_epochs` to 1 for `State` ([#756](https://github.com/catalyst-team/catalyst/pull/756))\n- Changed `state.batch_in`/`state.batch_out` to `state.input`/`state.output` ([#763](https://github.com/catalyst-team/catalyst/pull/763))\n- Moved `torchvision` dependency from `catalyst` to `catalyst[cv]` ([#738](https://github.com/catalyst-team/catalyst/pull/738)))\n\n### Removed\n\n- GanRunner removed to Catalyst.GAN ([#760](https://github.com/catalyst-team/catalyst/pull/760))\n- `monitoring_params` were removed ([#760](https://github.com/catalyst-team/catalyst/pull/760))\n\n### Fixed\n\n- Fixed docker dependencies ([$753](https://github.com/catalyst-team/catalyst/pull/753))\n- Fixed `text2embeddding` script ([#722](https://github.com/catalyst-team/catalyst/pull/722))\n- Fixed `utils/sys` exception ([#762](https://github.com/catalyst-team/catalyst/pull/762))\n- Returned `detach` method ([#766](https://github.com/catalyst-team/catalyst/pull/766))\n- Fixed timer division by zero ([#749](https://github.com/catalyst-team/catalyst/pull/749))\n- Fixed minimal torch version ([#775](https://github.com/catalyst-team/catalyst/pull/775))\n- Fixed segmentation tutorial ([#778](https://github.com/catalyst-team/catalyst/pull/778))\n- Fixed Dockerfile dependency ([#780](https://github.com/catalyst-team/catalyst/pull/780))\n\n\n## [20.04] - 2020-04-06\n\n### Added\n\n\n### Changed\n\n-\n\n### Removed\n\n-\n\n### Fixed\n\n-\n\n\n## [YY.MM.R] - YYYY-MM-DD\n\n### Added\n\n-\n\n### Changed\n\n-\n\n### Removed\n\n-\n\n### Fixed\n\n-\n"
        },
        {
          "name": "CITATION",
          "type": "blob",
          "size": 0.251953125,
          "content": "@misc{catalyst,\n    author = {Kolesnikov, Sergey},\n    title = {Catalyst - Accelerated deep learning R&D},\n    year = {2018},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/catalyst-team/catalyst}},\n}"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.1484375,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at `scitator@gmail.com` or `catalyst.team.core@gmail.com`.\nAll complaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the\n[Contributor Covenant version 1.4](https://www.contributor-covenant.org/version/1/4/code-of-conduct.html).\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 7.99609375,
          "content": "# Contribution guide\n\n## How to start?\n\nContributing is quite easy: suggest ideas and make them done.\nWe use [GitHub issues](https://github.com/catalyst-team/catalyst/issues) for bug reports and feature requests.\n\nEvery good PR usually consists of:\n- feature implementation :)\n- documentation to describe this feature to other people\n- tests to ensure everything is implemented correctly\n- `CHANGELOG.md` update for framework development history\n\n### PR examples\nYou can check these examples as good practices to follow.\n\n#### Fixes\n- https://github.com/catalyst-team/catalyst/pull/855\n- https://github.com/catalyst-team/catalyst/pull/858\n- https://github.com/catalyst-team/catalyst/pull/1150\n\n#### New features\n- https://github.com/catalyst-team/catalyst/pull/842\n- https://github.com/catalyst-team/catalyst/pull/825\n- https://github.com/catalyst-team/catalyst/pull/1170\n\n#### Contrib extensions\n- https://github.com/catalyst-team/catalyst/pull/862\n- https://github.com/catalyst-team/catalyst/pull/1151\n\n\n## Step-by-step guide\n\n### Before the PR\nPlease ensure that you have read the following docs:\n- [documentation and FAQ](https://catalyst-team.github.io/catalyst/)\n- [minimal examples section](https://github.com/catalyst-team/catalyst#minimal-examples)\n- [changelog with main framework updates](https://github.com/catalyst-team/catalyst/blob/master/CHANGELOG.md)\n\n### New feature\n\n1. Make an issue with your feature description;\n2. We shall discuss the design and its implementation details;\n3. Once we agree that the plan looks good, go ahead and implement it.\n\n\n### Bugfix\n\n1. Goto [GitHub issues](https://github.com/catalyst-team/catalyst/issues);\n2. Pick an issue and comment on the task that you want to work on this feature;\n3. If you need more context on a specific issue, please ask, and we will discuss the details.\n\nYou can also join our [Catalyst slack](https://join.slack.com/t/catalyst-team-core/shared_invite/zt-d9miirnn-z86oKDzFMKlMG4fgFdZafw) to make it easier to discuss.\nOnce you finish implementing a feature or bugfix, please send a Pull Request.\n\nIf you are not familiar with creating a Pull Request, here are some guides:\n- http://stackoverflow.com/questions/14680711/how-to-do-a-github-pull-request\n- https://help.github.com/articles/creating-a-pull-request/\n\n\n## Contribution best practices\n\n0. Install Python v3.7.0+\n0. Install requirements\n    ```bash\n    # for MacOS users, as we need bash version >= 4.0.0, wget and gnu-based sed\n    brew install bash wget gnu-sed\n\n    # It is often useful to have one or more Python environments\n    # where you can experiment with different combinations\n    # of packages without affecting your main installation.\n    # Create the virtual conda environment\n    conda create --name catalyst_dev\n    conda activate catalyst_dev # or ``source activate catalyst_dev``\n\n    # Install the required dependencies\n    pip install -r requirements/requirements.txt -r requirements/requirements-dev.txt\n\n    # for easy-to-go development, we suggest installing all extra dependencies\n    # that's why the independent conda environment is preferable\n    # Catalyst has a lot of extensions :)\n    pip install \\\n        -r ./catalyst/requirements/requirements.txt \\\n        -r ./catalyst/requirements/requirements-dev.txt \\\n        -r ./catalyst/requirements/requirements-cv.txt \\\n        -r ./catalyst/requirements/requirements-ml.txt \\\n        -r ./catalyst/requirements/requirements-optuna.txt \\\n        -r ./catalyst/requirements/requirements-comet.txt \\\n        -r ./catalyst/requirements/requirements-mlflow.txt \\\n        -r ./catalyst/requirements/requirements-neptune.txt \\\n        -r ./catalyst/requirements/requirements-wandb.txt \\\n        -r ./catalyst/requirements/requirements-profiler.txt\n    ```\n0. Break your work into small, single-purpose updates if possible.\nIt's much harder to merge in a large change with a lot of disjoint features.\n0. Submit the update as a GitHub pull request against the `master` branch.\n0. Make sure that you provide docstrings for all your new methods and classes.\n0. Add new unit tests for your code ([PR examples](#pr-examples)).\n0. (Optional) Check the [codestyle](#codestyle). We use a pre-commit hook that runs the formatting on commit, so you don't have to.\n0. Make sure that your code [passes the Github CI](#github-ci)\n\n\n## Github CI\n\nWe are using the Github CI for our test cases validation:\n\n- [codestyle tests](https://github.com/catalyst-team/catalyst/blob/master/.github/workflows/codestyle.yml#L134)\n- [documentation tests](https://github.com/catalyst-team/catalyst/blob/master/.github/workflows/codestyle.yml#L135)\n- [unit tests](https://github.com/catalyst-team/catalyst/blob/master/.github/workflows/dl_cpu.yml#L113)\n- [integrations tests](https://github.com/catalyst-team/catalyst/blob/master/.github/workflows/dl_cpu.yml#L114#L117)\n\nWe also have a [colab minimal CI/CD](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/colab_ci_cd.ipynb) as an independent step-by-step handmade tests option.\nPlease use it as a collaborative platform, if you have any issues during the PR.\n\n### Codestyle\n\nWe also have our own [catalyst code-style](https://github.com/catalyst-team/codestyle)\npackage to help with code formatting issues, and a corresponding pre-commit hook installed.\n\n- You could check the codestyle for your PR with:\n    ```bash\n    # to make code compatible with `catalyst` code style\n    catalyst-make-codestyle -l 89\n\n    # to check that the code is `catalyst` code style compliant\n    catalyst-check-codestyle -l 89\n    ```\n\n    Or you can use ```make check```\n\n- or To set the hook, please run (this requires `pre-commit` package, pinned in the [requirements-dev.txt](./requirements/requirements-dev.txt)):\n    ```bash\n    pre-commit install\n    ```\n    Once the installation is done, all the files that are changed will be formatted automatically (and commit halted if something goes wrong, e.g there is a syntactic error). You can also run the formatting manually:\n    ```bash\n    pre-commit run\n    ```\n\n    If for some reason you'll want to turn the hook off temporarily, you can do that with:\n    ```bash\n    SKIP=catalyst-make-codestyle git commit -m \"foo\"\n    ```\n    Or you can uninstall it completely with:\n    ```bash\n    pre-commit uninstall\n    ```\n\nOnce again, make sure that your python packages complied with [requirements/requirements.txt](./requirements/requirements.txt) and [requirements/requirements-dev.txt](requirements/requirements-dev.txt) to get codestyle and pre-commit run clean:\n```bash\npip install -r requirements/requirements.txt -r requirements/requirements-dev.txt\n```\n\nFor more information on pre-commit, please refer to  [pre-commit documentation](https://pre-commit.com/).\n\n### Documentation\n\nCatalyst uses [Google style](http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) for formatting [docstrings](https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings).\nLength of a line inside docstrings block must be limited to 100 characters to fit into Jupyter documentation popups.\n\nHow to setup Google style documentation style in PyCharm:\n[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/third_party_pics/pycharm-google-style.png)](https://github.com/catalyst-team/catalyst)\n\nYou could check the docs with:\n```bash\nrm -rf ./builds; REMOVE_BUILDS=0 make check-docs\n```\n\nNow you could open them into your browser, for example with\n```bash\nopen ./builds/index.html\n```\n\nIf you have some issues with building docs - please make sure that you installed the required pip packages.\n\n### Tests\n\nDo not forget to check that your code passes the unit tests:\n```bash\npytest .\n```\n\n#### Adding new tests\n\nPlease follow [PR examples](#pr-examples) for best practices.\n\n### Integrations\n\nIf you have contributed a new functionality with extra dependencies,\nplease ensure you have submitted the required tests.\nPlease follow [PR examples](#pr-examples) for best practices\nand review current [integrations tests](https://github.com/catalyst-team/catalyst/blob/master/.github/workflows/dl_cpu.yml#L114#L117).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.125,
          "content": "Copyright 2018- Sergey Kolesnikov\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.md",
          "type": "blob",
          "size": 1.294921875,
          "content": "<div align=\"center\">\n\n[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)\n\n### Manifest | Value | Mission\n</div>\n\n-----\n\nWriting Open Source Ecosystem for DL/RL research is challenging.<br/>\nLet's explain why we are doing this.\n\n### Why?\nFor real breakthroughs in DL/RL area, we need a Foundation.<br/>\nFor DL/RL Foundation, we need an Open Source R&D Ecosystem.<br/>\nThe Catalyst was designed by researchers for researchers,\n- to get the foundation for DL & RL research, which would be well tested and verified\n- to generalize and develop best practices that work in all areas of DL\n- to think less about the tech-side and focus on research hypothesis and their value\n- to accelerate your research\n\n### What?\nWe are creating a research Ecosystem, which\n- combines research best practices and helps knowledge sharing\n- connects DL/RL with Software Engineer for maximum performance\n- allows quickly and efficiently validate your hypotheses\n\n### How?\n- open ‚Äì it's entirely Open Source Ecosystem\n- equivalently ‚Äì everyone can contribute, propose new ideas, have their own options\n- expertise ‚Äì we are gathering top knowledge into one place\n- high-performance ‚Äì we are developing the ecosystem with SE best practices\n\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.376953125,
          "content": ".PHONY: check-docs docker docker-dev install-from-source clean\n\nPYTHON ?= python\nTAG=$(shell ${PYTHON} docker/collect_dependencies_hash.py)\n\ncheck-docs:\n\tbash ./bin/workflows/check_docs.sh\n\ndocker:\n\techo building $${REPO_NAME:-catalyst}:$${TAG:-latest} ...\n\tdocker build \\\n\t\t-t $${REPO_NAME:-catalyst}:$${TAG:-latest} . \\\n\t\t-f ./docker/Dockerfile --no-cache \\\n\t\t--build-arg PYTORCH_TAG=$$PYTORCH_TAG \\\n\t\t--build-arg CATALYST_DEV=$$CATALYST_DEV \\\n\t\t--build-arg CATALYST_CV=$$CATALYST_CV \\\n\t\t--build-arg CATALYST_ML=$$CATALYST_ML \\\n\t\t--build-arg CATALYST_OPTUNA=$$CATALYST_OPTUNA \\\n\t\t--build-arg CATALYST_ONNX=$$CATALYST_ONNX \\\n\t\t--build-arg CATALYST_ONNX_GPU=$$CATALYST_ONNX_GPU\n\ndocker-dev:\n\techo building $${REPO_NAME:-catalyst-dev}:$${TAG:-latest} ...\n\tdocker build \\\n\t\t-t $${REPO_NAME:-catalyst-dev}:$${TAG:-latest} . \\\n\t\t-f ./docker/Dockerfile --no-cache \\\n\t\t--build-arg PYTORCH_TAG=$$PYTORCH_TAG \\\n\t\t--build-arg CATALYST_DEV=$$CATALYST_DEV \\\n\t\t--build-arg CATALYST_CV=$$CATALYST_CV \\\n\t\t--build-arg CATALYST_ML=$$CATALYST_ML \\\n\t\t--build-arg CATALYST_OPTUNA=$$CATALYST_OPTUNA \\\n\t\t--build-arg CATALYST_ONNX=$$CATALYST_ONNX \\\n\t\t--build-arg CATALYST_ONNX_GPU=$$CATALYST_ONNX_GPU\n\ninstall-from-source:\n\tpip uninstall catalyst -y && pip install -e ./\n\nclean:\n\trm -rf build/\n\tdocker rmi -f catalyst:latest\n\tdocker rmi -f catalyst-dev:latest\n\ncheck:\n\tcatalyst-make-codestyle -l 89\n\tcatalyst-check-codestyle -l 89\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 52.6806640625,
          "content": "<div align=\"center\">\n\n[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)\n\n**Accelerated Deep Learning R&D**\n\n[![CodeFactor](https://www.codefactor.io/repository/github/catalyst-team/catalyst/badge)](https://www.codefactor.io/repository/github/catalyst-team/catalyst)\n[![Pipi version](https://img.shields.io/pypi/v/catalyst.svg)](https://pypi.org/project/catalyst/)\n[![Docs](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fcatalyst%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v)](https://catalyst-team.github.io/catalyst/index.html)\n[![Docker](https://img.shields.io/badge/docker-hub-blue)](https://hub.docker.com/r/catalystteam/catalyst/tags)\n[![PyPI Status](https://pepy.tech/badge/catalyst)](https://pepy.tech/project/catalyst)\n\n[![Twitter](https://img.shields.io/badge/news-twitter-499feb)](https://twitter.com/CatalystTeam)\n[![Telegram](https://img.shields.io/badge/channel-telegram-blue)](https://t.me/catalyst_team)\n[![Slack](https://img.shields.io/badge/Catalyst-slack-success)](https://join.slack.com/t/catalyst-team-devs/shared_invite/zt-d9miirnn-z86oKDzFMKlMG4fgFdZafw)\n[![Github contributors](https://img.shields.io/github/contributors/catalyst-team/catalyst.svg?logo=github&logoColor=white)](https://github.com/catalyst-team/catalyst/graphs/contributors)\n\n![codestyle](https://github.com/catalyst-team/catalyst/workflows/codestyle/badge.svg?branch=master&event=push)\n![docs](https://github.com/catalyst-team/catalyst/workflows/docs/badge.svg?branch=master&event=push)\n![catalyst](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n![integrations](https://github.com/catalyst-team/catalyst/workflows/integrations/badge.svg?branch=master&event=push)\n\n[![python](https://img.shields.io/badge/python_3.6-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![python](https://img.shields.io/badge/python_3.7-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![python](https://img.shields.io/badge/python_3.8-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n\n[![os](https://img.shields.io/badge/Linux-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![os](https://img.shields.io/badge/OSX-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![os](https://img.shields.io/badge/WSL-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n</div>\n\nCatalyst is a PyTorch framework for Deep Learning Research and Development.\nIt focuses on reproducibility, rapid experimentation, and codebase reuse\nso you can create something new rather than write yet another train loop.\n<br/> Break the cycle ‚Äì use the Catalyst!\n\n- [Project Manifest](https://github.com/catalyst-team/catalyst/blob/master/MANIFEST.md)\n- [Framework architecture](https://miro.com/app/board/o9J_lxBO-2k=/)\n- [Catalyst at AI Landscape](https://landscape.lfai.foundation/selected=catalyst)\n- Part of the [PyTorch Ecosystem](https://pytorch.org/ecosystem/)\n\n<details>\n<summary>Catalyst at PyTorch Ecosystem Day 2021</summary>\n<p>\n\n[![Catalyst poster](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/Catalyst-PTED21.png)](https://github.com/catalyst-team/catalyst)\n\n</p>\n</details>\n\n<details>\n<summary>Catalyst at PyTorch Developer Day 2021</summary>\n<p>\n\n[![Catalyst poster](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/Catalyst-PTDD21.png)](https://github.com/catalyst-team/catalyst)\n\n</p>\n</details>\n\n----\n\n## Getting started\n\n```bash\npip install -U catalyst\n```\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom catalyst import dl, utils\nfrom catalyst.contrib.datasets import MNIST\n\nmodel = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.02)\nloaders = {\n    \"train\": DataLoader(MNIST(os.getcwd(), train=True), batch_size=32),\n    \"valid\": DataLoader(MNIST(os.getcwd(), train=False), batch_size=32),\n}\n\nrunner = dl.SupervisedRunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    callbacks=[\n        dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", topk=(1, 3, 5)),\n        dl.PrecisionRecallF1SupportCallback(input_key=\"logits\", target_key=\"targets\"),\n    ],\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=True,\n    verbose=True,\n)\n\n# model evaluation\nmetrics = runner.evaluate_loader(\n    loader=loaders[\"valid\"],\n    callbacks=[dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", topk=(1, 3, 5))],\n)\n\n# model inference\nfor prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n    assert prediction[\"logits\"].detach().cpu().numpy().shape[-1] == 10\n\n# model post-processing\nmodel = runner.model.cpu()\nbatch = next(iter(loaders[\"valid\"]))[0]\nutils.trace_model(model=model, batch=batch)\nutils.quantize_model(model=model)\nutils.prune_model(model=model, pruning_fn=\"l1_unstructured\", amount=0.8)\nutils.onnx_export(model=model, batch=batch, file=\"./logs/mnist.onnx\", verbose=True)\n```\n\n### Step-by-step Guide\n1. Start with [Catalyst ‚Äî A PyTorch Framework for Accelerated Deep Learning R&D](https://medium.com/pytorch/catalyst-a-pytorch-framework-for-accelerated-deep-learning-r-d-ad9621e4ca88?source=friends_link&sk=885b4409aecab505db0a63b06f19dcef) introduction.\n1. Try [notebook tutorials](#minimal-examples) or check [minimal examples](#minimal-examples) for first deep dive.\n1. Read [blog posts](https://catalyst-team.com/post/) with use-cases and guides.\n1. Learn machine learning with our [\"Deep Learning with Catalyst\" course](https://catalyst-team.com/#course).\n1. And finally, [join our slack](https://join.slack.com/t/catalyst-team-core/shared_invite/zt-d9miirnn-z86oKDzFMKlMG4fgFdZafw) if you want to chat with the team and contributors.\n\n\n## Table of Contents\n- [Getting started](#getting-started)\n  - [Step-by-step Guide](#step-by-step-guide)\n- [Table of Contents](#table-of-contents)\n- [Overview](#overview)\n  - [Installation](#installation)\n  - [Documentation](#documentation)\n  - [Minimal Examples](#minimal-examples)\n  - [Tests](#tests)\n  - [Blog Posts](#blog-posts)\n  - [Talks](#talks)\n- [Community](#community)\n  - [Contribution Guide](#contribution-guide)\n  - [User Feedback](#user-feedback)\n  - [Acknowledgments](#acknowledgments)\n  - [Trusted by](#trusted-by)\n  - [Citation](#citation)\n\n\n## Overview\nCatalyst helps you implement compact\nbut full-featured Deep Learning pipelines with just a few lines of code.\nYou get a training loop with metrics, early-stopping, model checkpointing,\nand other features without the boilerplate.\n\n\n### Installation\n\nGeneric installation:\n```bash\npip install -U catalyst\n```\n\n<details>\n<summary>Specialized versions, extra requirements might apply</summary>\n<p>\n\n```bash\npip install catalyst[ml]         # installs ML-based Catalyst\npip install catalyst[cv]         # installs CV-based Catalyst\n# master version installation\npip install git+https://github.com/catalyst-team/catalyst@master --upgrade\n# all available extensions are listed here:\n# https://github.com/catalyst-team/catalyst/blob/master/setup.py\n```\n</p>\n</details>\n\nCatalyst is compatible with: Python 3.7+. PyTorch 1.4+. <br/>\nTested on Ubuntu 16.04/18.04/20.04, macOS 10.15, Windows 10, and Windows Subsystem for Linux.\n\n### Documentation\n- [master](https://catalyst-team.github.io/catalyst/)\n- [22.02](https://catalyst-team.github.io/catalyst/v22.02/index.html)\n\n- <details>\n  <summary>2021 edition</summary>\n  <p>\n\n    - [21.12](https://catalyst-team.github.io/catalyst/v21.12/index.html)\n    - [21.11](https://catalyst-team.github.io/catalyst/v21.11/index.html)\n    - [21.10](https://catalyst-team.github.io/catalyst/v21.10/index.html)\n    - [21.09](https://catalyst-team.github.io/catalyst/v21.09/index.html)\n    - [21.08](https://catalyst-team.github.io/catalyst/v21.08/index.html)\n    - [21.07](https://catalyst-team.github.io/catalyst/v21.07/index.html)\n    - [21.06](https://catalyst-team.github.io/catalyst/v21.06/index.html)\n    - [21.05](https://catalyst-team.github.io/catalyst/v21.05/index.html) ([Catalyst ‚Äî A PyTorch Framework for Accelerated Deep Learning R&D](https://medium.com/pytorch/catalyst-a-pytorch-framework-for-accelerated-deep-learning-r-d-ad9621e4ca88?source=friends_link&sk=885b4409aecab505db0a63b06f19dcef))\n    - [21.04/21.04.1](https://catalyst-team.github.io/catalyst/v21.04/index.html), [21.04.2](https://catalyst-team.github.io/catalyst/v21.04.2/index.html)\n    - [21.03](https://catalyst-team.github.io/catalyst/v21.03/index.html), [21.03.1/21.03.2](https://catalyst-team.github.io/catalyst/v21.03.1/index.html)\n\n  </p>\n  </details>\n- <details>\n  <summary>2020 edition</summary>\n  <p>\n\n    - [20.12](https://catalyst-team.github.io/catalyst/v20.12/index.html)\n    - [20.11](https://catalyst-team.github.io/catalyst/v20.11/index.html)\n    - [20.10](https://catalyst-team.github.io/catalyst/v20.10/index.html)\n    - [20.09](https://catalyst-team.github.io/catalyst/v20.09/index.html)\n    - [20.08.2](https://catalyst-team.github.io/catalyst/v20.08.2/index.html)\n    - [20.07](https://catalyst-team.github.io/catalyst/v20.07/index.html) ([dev blog: 20.07 release](https://medium.com/pytorch/catalyst-dev-blog-20-07-release-fb489cd23e14?source=friends_link&sk=7ab92169658fe9a9e1c44068f28cc36c))\n    - [20.06](https://catalyst-team.github.io/catalyst/v20.06/index.html)\n    - [20.05](https://catalyst-team.github.io/catalyst/v20.05/index.html), [20.05.1](https://catalyst-team.github.io/catalyst/v20.05.1/index.html)\n    - [20.04](https://catalyst-team.github.io/catalyst/v20.04/index.html), [20.04.1](https://catalyst-team.github.io/catalyst/v20.04.1/index.html), [20.04.2](https://catalyst-team.github.io/catalyst/v20.04.2/index.html)\n\n  </p>\n  </details>\n\n\n### Minimal Examples\n\n- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/customizing_what_happens_in_train.ipynb) Introduction tutorial \"[Customizing what happens in `train`](./examples/notebooks/customizing_what_happens_in_train.ipynb)\"\n- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/customization_tutorial.ipynb) Demo with [customization examples](./examples/notebooks/customization_tutorial.ipynb)\n- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/reinforcement_learning.ipynb) [Reinforcement Learning with Catalyst](./examples/notebooks/reinforcement_learning.ipynb)\n- [And more](./examples/)\n\n<details>\n<summary>CustomRunner ‚Äì PyTorch for-loop decomposition</summary>\n<p>\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom catalyst import dl, metrics\nfrom catalyst.contrib.datasets import MNIST\n\nmodel = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))\noptimizer = optim.Adam(model.parameters(), lr=0.02)\n\ntrain_data = MNIST(os.getcwd(), train=True)\nvalid_data = MNIST(os.getcwd(), train=False)\nloaders = {\n    \"train\": DataLoader(train_data, batch_size=32),\n    \"valid\": DataLoader(valid_data, batch_size=32),\n}\n\nclass CustomRunner(dl.Runner):\n    def predict_batch(self, batch):\n        # model inference step\n        return self.model(batch[0].to(self.engine.device))\n\n    def on_loader_start(self, runner):\n        super().on_loader_start(runner)\n        self.meters = {\n            key: metrics.AdditiveMetric(compute_on_call=False)\n            for key in [\"loss\", \"accuracy01\", \"accuracy03\"]\n        }\n\n    def handle_batch(self, batch):\n        # model train/valid step\n        # unpack the batch\n        x, y = batch\n        # run model forward pass\n        logits = self.model(x)\n        # compute the loss\n        loss = F.cross_entropy(logits, y)\n        # compute the metrics\n        accuracy01, accuracy03 = metrics.accuracy(logits, y, topk=(1, 3))\n        # log metrics\n        self.batch_metrics.update(\n            {\"loss\": loss, \"accuracy01\": accuracy01, \"accuracy03\": accuracy03}\n        )\n        for key in [\"loss\", \"accuracy01\", \"accuracy03\"]:\n            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)\n        # run model backward pass\n        if self.is_train_loader:\n            self.engine.backward(loss)\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n    def on_loader_end(self, runner):\n        for key in [\"loss\", \"accuracy01\", \"accuracy03\"]:\n            self.loader_metrics[key] = self.meters[key].compute()[0]\n        super().on_loader_end(runner)\n\nrunner = CustomRunner()\n# model training\nrunner.train(\n    model=model,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logs\",\n    num_epochs=5,\n    verbose=True,\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=True,\n)\n# model inference\nfor logits in runner.predict_loader(loader=loaders[\"valid\"]):\n    assert logits.detach().cpu().numpy().shape[-1] == 10\n```\n</p>\n</details>\n\n<details>\n<summary>ML - linear regression</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom catalyst import dl\n\n# data\nnum_samples, num_features = int(1e4), int(1e1)\nX, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\ndataset = TensorDataset(X, y)\nloader = DataLoader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.Linear(num_features, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n\n# model training\nrunner = dl.SupervisedRunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=True,\n    num_epochs=8,\n    verbose=True,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ML - multiclass classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes = int(1e4), int(1e1), 4\nX = torch.rand(num_samples, num_features)\ny = (torch.rand(num_samples,) * num_classes).to(torch.int64)\n\n# pytorch loaders\ndataset = TensorDataset(X, y)\nloader = DataLoader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.Linear(num_features, num_classes)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n\n# model training\nrunner = dl.SupervisedRunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    num_epochs=3,\n    valid_loader=\"valid\",\n    valid_metric=\"accuracy03\",\n    minimize_valid_metric=False,\n    verbose=True,\n    callbacks=[\n        dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=num_classes),\n        # uncomment for extra metrics:\n        # dl.PrecisionRecallF1SupportCallback(\n        #     input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n        # ),\n        # dl.AUCCallback(input_key=\"logits\", target_key=\"targets\"),\n        # catalyst[ml] required ``pip install catalyst[ml]``\n        # dl.ConfusionMatrixCallback(\n        #     input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n        # ),\n    ],\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ML - multilabel classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes = int(1e4), int(1e1), 4\nX = torch.rand(num_samples, num_features)\ny = (torch.rand(num_samples, num_classes) > 0.5).to(torch.float32)\n\n# pytorch loaders\ndataset = TensorDataset(X, y)\nloader = DataLoader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.Linear(num_features, num_classes)\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n\n# model training\nrunner = dl.SupervisedRunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    num_epochs=3,\n    valid_loader=\"valid\",\n    valid_metric=\"accuracy01\",\n    minimize_valid_metric=False,\n    verbose=True,\n    callbacks=[\n        dl.BatchTransformCallback(\n            transform=torch.sigmoid,\n            scope=\"on_batch_end\",\n            input_key=\"logits\",\n            output_key=\"scores\"\n        ),\n        dl.AUCCallback(input_key=\"scores\", target_key=\"targets\"),\n        # uncomment for extra metrics:\n        # dl.MultilabelAccuracyCallback(input_key=\"scores\", target_key=\"targets\", threshold=0.5),\n        # dl.MultilabelPrecisionRecallF1SupportCallback(\n        #     input_key=\"scores\", target_key=\"targets\", threshold=0.5\n        # ),\n    ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ML - multihead classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes1, num_classes2 = int(1e4), int(1e1), 4, 10\nX = torch.rand(num_samples, num_features)\ny1 = (torch.rand(num_samples,) * num_classes1).to(torch.int64)\ny2 = (torch.rand(num_samples,) * num_classes2).to(torch.int64)\n\n# pytorch loaders\ndataset = TensorDataset(X, y1, y2)\nloader = DataLoader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\nclass CustomModule(nn.Module):\n    def __init__(self, in_features: int, out_features1: int, out_features2: int):\n        super().__init__()\n        self.shared = nn.Linear(in_features, 128)\n        self.head1 = nn.Linear(128, out_features1)\n        self.head2 = nn.Linear(128, out_features2)\n\n    def forward(self, x):\n        x = self.shared(x)\n        y1 = self.head1(x)\n        y2 = self.head2(x)\n        return y1, y2\n\n# model, criterion, optimizer, scheduler\nmodel = CustomModule(num_features, num_classes1, num_classes2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, [2])\n\nclass CustomRunner(dl.Runner):\n    def handle_batch(self, batch):\n        x, y1, y2 = batch\n        y1_hat, y2_hat = self.model(x)\n        self.batch = {\n            \"features\": x,\n            \"logits1\": y1_hat,\n            \"logits2\": y2_hat,\n            \"targets1\": y1,\n            \"targets2\": y2,\n        }\n\n# model training\nrunner = CustomRunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    num_epochs=3,\n    verbose=True,\n    callbacks=[\n        dl.CriterionCallback(metric_key=\"loss1\", input_key=\"logits1\", target_key=\"targets1\"),\n        dl.CriterionCallback(metric_key=\"loss2\", input_key=\"logits2\", target_key=\"targets2\"),\n        dl.MetricAggregationCallback(metric_key=\"loss\", metrics=[\"loss1\", \"loss2\"], mode=\"mean\"),\n        dl.BackwardCallback(metric_key=\"loss\"),\n        dl.OptimizerCallback(metric_key=\"loss\"),\n        dl.SchedulerCallback(),\n        dl.AccuracyCallback(\n            input_key=\"logits1\", target_key=\"targets1\", num_classes=num_classes1, prefix=\"one_\"\n        ),\n        dl.AccuracyCallback(\n            input_key=\"logits2\", target_key=\"targets2\", num_classes=num_classes2, prefix=\"two_\"\n        ),\n        # catalyst[ml] required ``pip install catalyst[ml]``\n        # dl.ConfusionMatrixCallback(\n        #     input_key=\"logits1\", target_key=\"targets1\", num_classes=num_classes1, prefix=\"one_cm\"\n        # ),\n        # dl.ConfusionMatrixCallback(\n        #     input_key=\"logits2\", target_key=\"targets2\", num_classes=num_classes2, prefix=\"two_cm\"\n        # ),\n        dl.CheckpointCallback(\n            logdir=\"./logs/one\",\n            loader_key=\"valid\", metric_key=\"one_accuracy01\", minimize=False, topk=1\n        ),\n        dl.CheckpointCallback(\n            logdir=\"./logs/two\",\n            loader_key=\"valid\", metric_key=\"two_accuracy03\", minimize=False, topk=3\n        ),\n    ],\n    loggers={\"console\": dl.ConsoleLogger(), \"tb\": dl.TensorboardLogger(\"./logs/tb\")},\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ML ‚Äì RecSys</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom catalyst import dl\n\n# sample data\nnum_users, num_features, num_items = int(1e4), int(1e1), 10\nX = torch.rand(num_users, num_features)\ny = (torch.rand(num_users, num_items) > 0.5).to(torch.float32)\n\n# pytorch loaders\ndataset = TensorDataset(X, y)\nloader = DataLoader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.Linear(num_features, num_items)\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n\n# model training\nrunner = dl.SupervisedRunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    num_epochs=3,\n    verbose=True,\n    callbacks=[\n        dl.BatchTransformCallback(\n            transform=torch.sigmoid,\n            scope=\"on_batch_end\",\n            input_key=\"logits\",\n            output_key=\"scores\"\n        ),\n        dl.CriterionCallback(input_key=\"logits\", target_key=\"targets\", metric_key=\"loss\"),\n        # uncomment for extra metrics:\n        # dl.AUCCallback(input_key=\"scores\", target_key=\"targets\"),\n        # dl.HitrateCallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.MRRCallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.MAPCallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.NDCGCallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        dl.BackwardCallback(metric_key=\"loss\"),\n        dl.OptimizerCallback(metric_key=\"loss\"),\n        dl.SchedulerCallback(),\n        dl.CheckpointCallback(\n            logdir=\"./logs\", loader_key=\"valid\", metric_key=\"loss\", minimize=True\n        ),\n    ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>CV - MNIST classification</summary>\n<p>\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import MNIST\n\nmodel = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.02)\n\ntrain_data = MNIST(os.getcwd(), train=True)\nvalid_data = MNIST(os.getcwd(), train=False)\nloaders = {\n    \"train\": DataLoader(train_data, batch_size=32),\n    \"valid\": DataLoader(valid_data, batch_size=32),\n}\n\nrunner = dl.SupervisedRunner()\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=True,\n    verbose=True,\n# uncomment for extra metrics:\n#     callbacks=[\n#         dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n#         dl.PrecisionRecallF1SupportCallback(\n#             input_key=\"logits\", target_key=\"targets\", num_classes=10\n#         ),\n#         dl.AUCCallback(input_key=\"logits\", target_key=\"targets\"),\n#         # catalyst[ml] required ``pip install catalyst[ml]``\n#         dl.ConfusionMatrixCallback(\n#             input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n#         ),\n#     ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>CV - MNIST segmentation</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.contrib.losses import IoULoss\n\n\nmodel = nn.Sequential(\n    nn.Conv2d(1, 1, 3, 1, 1), nn.ReLU(),\n    nn.Conv2d(1, 1, 3, 1, 1), nn.Sigmoid(),\n)\ncriterion = IoULoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n\ntrain_data = MNIST(os.getcwd(), train=True)\nvalid_data = MNIST(os.getcwd(), train=False)\nloaders = {\n    \"train\": DataLoader(train_data, batch_size=32),\n    \"valid\": DataLoader(valid_data, batch_size=32),\n}\n\nclass CustomRunner(dl.SupervisedRunner):\n    def handle_batch(self, batch):\n        x = batch[self._input_key]\n        x_noise = (x + torch.rand_like(x)).clamp_(0, 1)\n        x_ = self.model(x_noise)\n        self.batch = {self._input_key: x, self._output_key: x_, self._target_key: x}\n\nrunner = CustomRunner(\n    input_key=\"features\", output_key=\"scores\", target_key=\"targets\", loss_key=\"loss\"\n)\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    callbacks=[\n        dl.IOUCallback(input_key=\"scores\", target_key=\"targets\"),\n        dl.DiceCallback(input_key=\"scores\", target_key=\"targets\"),\n        dl.TrevskyCallback(input_key=\"scores\", target_key=\"targets\", alpha=0.2),\n    ],\n    logdir=\"./logdir\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=True,\n    verbose=True,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>CV - MNIST metric learning</summary>\n<p>\n\n```python\nimport os\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom catalyst import dl\nfrom catalyst.contrib.data import HardTripletsSampler\nfrom catalyst.contrib.datasets import MnistMLDataset, MnistQGDataset\nfrom catalyst.contrib.losses import TripletMarginLossWithSampler\nfrom catalyst.contrib.models import MnistSimpleNet\nfrom catalyst.data.sampler import BatchBalanceClassSampler\n\n\n# 1. train and valid loaders\ntrain_dataset = MnistMLDataset(root=os.getcwd())\nsampler = BatchBalanceClassSampler(\n    labels=train_dataset.get_labels(), num_classes=5, num_samples=10, num_batches=10\n)\ntrain_loader = DataLoader(dataset=train_dataset, batch_sampler=sampler)\n\nvalid_dataset = MnistQGDataset(root=os.getcwd(), gallery_fraq=0.2)\nvalid_loader = DataLoader(dataset=valid_dataset, batch_size=1024)\n\n# 2. model and optimizer\nmodel = MnistSimpleNet(out_features=16)\noptimizer = Adam(model.parameters(), lr=0.001)\n\n# 3. criterion with triplets sampling\nsampler_inbatch = HardTripletsSampler(norm_required=False)\ncriterion = TripletMarginLossWithSampler(margin=0.5, sampler_inbatch=sampler_inbatch)\n\n# 4. training with catalyst Runner\nclass CustomRunner(dl.SupervisedRunner):\n    def handle_batch(self, batch) -> None:\n        if self.is_train_loader:\n            images, targets = batch[\"features\"].float(), batch[\"targets\"].long()\n            features = self.model(images)\n            self.batch = {\"embeddings\": features, \"targets\": targets,}\n        else:\n            images, targets, is_query = \\\n                batch[\"features\"].float(), batch[\"targets\"].long(), batch[\"is_query\"].bool()\n            features = self.model(images)\n            self.batch = {\"embeddings\": features, \"targets\": targets, \"is_query\": is_query}\n\ncallbacks = [\n    dl.ControlFlowCallbackWrapper(\n        dl.CriterionCallback(input_key=\"embeddings\", target_key=\"targets\", metric_key=\"loss\"),\n        loaders=\"train\",\n    ),\n    dl.ControlFlowCallbackWrapper(\n        dl.CMCScoreCallback(\n            embeddings_key=\"embeddings\",\n            labels_key=\"targets\",\n            is_query_key=\"is_query\",\n            topk=[1],\n        ),\n        loaders=\"valid\",\n    ),\n    dl.PeriodicLoaderCallback(\n        valid_loader_key=\"valid\", valid_metric_key=\"cmc01\", minimize=False, valid=2\n    ),\n]\n\nrunner = CustomRunner(input_key=\"features\", output_key=\"embeddings\")\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    callbacks=callbacks,\n    loaders={\"train\": train_loader, \"valid\": valid_loader},\n    verbose=False,\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"cmc01\",\n    minimize_valid_metric=False,\n    num_epochs=10,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>CV - MNIST GAN</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.contrib.layers import GlobalMaxPool2d, Lambda\n\nlatent_dim = 128\ngenerator = nn.Sequential(\n    # We want to generate 128 coefficients to reshape into a 7x7x128 map\n    nn.Linear(128, 128 * 7 * 7),\n    nn.LeakyReLU(0.2, inplace=True),\n    Lambda(lambda x: x.view(x.size(0), 128, 7, 7)),\n    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(128, 1, (7, 7), padding=3),\n    nn.Sigmoid(),\n)\ndiscriminator = nn.Sequential(\n    nn.Conv2d(1, 64, (3, 3), stride=(2, 2), padding=1),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=1),\n    nn.LeakyReLU(0.2, inplace=True),\n    GlobalMaxPool2d(),\n    nn.Flatten(),\n    nn.Linear(128, 1),\n)\n\nmodel = nn.ModuleDict({\"generator\": generator, \"discriminator\": discriminator})\ncriterion = {\"generator\": nn.BCEWithLogitsLoss(), \"discriminator\": nn.BCEWithLogitsLoss()}\noptimizer = {\n    \"generator\": torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n    \"discriminator\": torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n}\ntrain_data = MNIST(os.getcwd(), train=False)\nloaders = {\"train\": DataLoader(train_data, batch_size=32)}\n\nclass CustomRunner(dl.Runner):\n    def predict_batch(self, batch):\n        batch_size = 1\n        # Sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n        # Decode them to fake images\n        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n        return generated_images\n\n    def handle_batch(self, batch):\n        real_images, _ = batch\n        batch_size = real_images.shape[0]\n\n        # Sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n\n        # Decode them to fake images\n        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n        # Combine them with real images\n        combined_images = torch.cat([generated_images, real_images])\n\n        # Assemble labels discriminating real from fake images\n        labels = \\\n            torch.cat([torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))]).to(self.engine.device)\n        # Add random noise to the labels - important trick!\n        labels += 0.05 * torch.rand(labels.shape).to(self.engine.device)\n\n        # Discriminator forward\n        combined_predictions = self.model[\"discriminator\"](combined_images)\n\n        # Sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n        # Assemble labels that say \"all real images\"\n        misleading_labels = torch.zeros((batch_size, 1)).to(self.engine.device)\n\n        # Generator forward\n        generated_images = self.model[\"generator\"](random_latent_vectors)\n        generated_predictions = self.model[\"discriminator\"](generated_images)\n\n        self.batch = {\n            \"combined_predictions\": combined_predictions,\n            \"labels\": labels,\n            \"generated_predictions\": generated_predictions,\n            \"misleading_labels\": misleading_labels,\n        }\n\n\nrunner = CustomRunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    callbacks=[\n        dl.CriterionCallback(\n            input_key=\"combined_predictions\",\n            target_key=\"labels\",\n            metric_key=\"loss_discriminator\",\n            criterion_key=\"discriminator\",\n        ),\n        dl.BackwardCallback(metric_key=\"loss_discriminator\"),\n        dl.OptimizerCallback(\n            optimizer_key=\"discriminator\",\n            metric_key=\"loss_discriminator\",\n        ),\n        dl.CriterionCallback(\n            input_key=\"generated_predictions\",\n            target_key=\"misleading_labels\",\n            metric_key=\"loss_generator\",\n            criterion_key=\"generator\",\n        ),\n        dl.BackwardCallback(metric_key=\"loss_generator\"),\n        dl.OptimizerCallback(\n            optimizer_key=\"generator\",\n            metric_key=\"loss_generator\",\n        ),\n    ],\n    valid_loader=\"train\",\n    valid_metric=\"loss_generator\",\n    minimize_valid_metric=True,\n    num_epochs=20,\n    verbose=True,\n    logdir=\"./logs_gan\",\n)\n\n# visualization (matplotlib required):\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# plt.imshow(runner.predict_batch(None)[0, 0].cpu().numpy())\n```\n</p>\n</details>\n\n\n<details>\n<summary>CV - MNIST VAE</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom catalyst import dl, metrics\nfrom catalyst.contrib.datasets import MNIST\n\nLOG_SCALE_MAX = 2\nLOG_SCALE_MIN = -10\n\ndef normal_sample(loc, log_scale):\n    scale = torch.exp(0.5 * log_scale)\n    return loc + scale * torch.randn_like(scale)\n\nclass VAE(nn.Module):\n    def __init__(self, in_features, hid_features):\n        super().__init__()\n        self.hid_features = hid_features\n        self.encoder = nn.Linear(in_features, hid_features * 2)\n        self.decoder = nn.Sequential(nn.Linear(hid_features, in_features), nn.Sigmoid())\n\n    def forward(self, x, deterministic=False):\n        z = self.encoder(x)\n        bs, z_dim = z.shape\n\n        loc, log_scale = z[:, : z_dim // 2], z[:, z_dim // 2 :]\n        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n\n        z_ = loc if deterministic else normal_sample(loc, log_scale)\n        z_ = z_.view(bs, -1)\n        x_ = self.decoder(z_)\n\n        return x_, loc, log_scale\n\nclass CustomRunner(dl.IRunner):\n    def __init__(self, hid_features, logdir, engine):\n        super().__init__()\n        self.hid_features = hid_features\n        self._logdir = logdir\n        self._engine = engine\n\n    def get_engine(self):\n        return self._engine\n\n    def get_loggers(self):\n        return {\n            \"console\": dl.ConsoleLogger(),\n            \"csv\": dl.CSVLogger(logdir=self._logdir),\n            \"tensorboard\": dl.TensorboardLogger(logdir=self._logdir),\n        }\n\n    @property\n    def num_epochs(self) -> int:\n        return 1\n\n    def get_loaders(self):\n        loaders = {\n            \"train\": DataLoader(MNIST(os.getcwd(), train=False), batch_size=32),\n            \"valid\": DataLoader(MNIST(os.getcwd(), train=False), batch_size=32),\n        }\n        return loaders\n\n    def get_model(self):\n        model = self.model if self.model is not None else VAE(28 * 28, self.hid_features)\n        return model\n\n    def get_optimizer(self, model):\n        return optim.Adam(model.parameters(), lr=0.02)\n\n    def get_callbacks(self):\n        return {\n            \"backward\": dl.BackwardCallback(metric_key=\"loss\"),\n            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"),\n            \"checkpoint\": dl.CheckpointCallback(\n                self._logdir,\n                loader_key=\"valid\",\n                metric_key=\"loss\",\n                minimize=True,\n                topk=3,\n            ),\n        }\n\n    def on_loader_start(self, runner):\n        super().on_loader_start(runner)\n        self.meters = {\n            key: metrics.AdditiveMetric(compute_on_call=False)\n            for key in [\"loss_ae\", \"loss_kld\", \"loss\"]\n        }\n\n    def handle_batch(self, batch):\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        x_, loc, log_scale = self.model(x, deterministic=not self.is_train_loader)\n\n        loss_ae = F.mse_loss(x_, x)\n        loss_kld = (\n            -0.5 * torch.sum(1 + log_scale - loc.pow(2) - log_scale.exp(), dim=1)\n        ).mean()\n        loss = loss_ae + loss_kld * 0.01\n\n        self.batch_metrics = {\"loss_ae\": loss_ae, \"loss_kld\": loss_kld, \"loss\": loss}\n        for key in [\"loss_ae\", \"loss_kld\", \"loss\"]:\n            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)\n\n    def on_loader_end(self, runner):\n        for key in [\"loss_ae\", \"loss_kld\", \"loss\"]:\n            self.loader_metrics[key] = self.meters[key].compute()[0]\n        super().on_loader_end(runner)\n\n    def predict_batch(self, batch):\n        random_latent_vectors = torch.randn(1, self.hid_features).to(self.engine.device)\n        generated_images = self.model.decoder(random_latent_vectors).detach()\n        return generated_images\n\nrunner = CustomRunner(128, \"./logs\", dl.CPUEngine())\nrunner.run()\n# visualization (matplotlib required):\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# plt.imshow(runner.predict_batch(None)[0].cpu().numpy().reshape(28, 28))\n```\n</p>\n</details>\n\n\n<details>\n<summary>AutoML - hyperparameters optimization with Optuna</summary>\n<p>\n\n```python\nimport os\nimport optuna\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import MNIST\n\n\ndef objective(trial):\n    lr = trial.suggest_loguniform(\"lr\", 1e-3, 1e-1)\n    num_hidden = int(trial.suggest_loguniform(\"num_hidden\", 32, 128))\n\n    train_data = MNIST(os.getcwd(), train=True)\n    valid_data = MNIST(os.getcwd(), train=False)\n    loaders = {\n        \"train\": DataLoader(train_data, batch_size=32),\n        \"valid\": DataLoader(valid_data, batch_size=32),\n    }\n    model = nn.Sequential(\n        nn.Flatten(), nn.Linear(784, num_hidden), nn.ReLU(), nn.Linear(num_hidden, 10)\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    runner = dl.SupervisedRunner(input_key=\"features\", output_key=\"logits\", target_key=\"targets\")\n    runner.train(\n        model=model,\n        criterion=criterion,\n        optimizer=optimizer,\n        loaders=loaders,\n        callbacks={\n            \"accuracy\": dl.AccuracyCallback(\n                input_key=\"logits\", target_key=\"targets\", num_classes=10\n            ),\n            # catalyst[optuna] required ``pip install catalyst[optuna]``\n            \"optuna\": dl.OptunaPruningCallback(\n                loader_key=\"valid\", metric_key=\"accuracy01\", minimize=False, trial=trial\n            ),\n        },\n        num_epochs=3,\n    )\n    score = trial.best_score\n    return score\n\nstudy = optuna.create_study(\n    direction=\"maximize\",\n    pruner=optuna.pruners.MedianPruner(\n        n_startup_trials=1, n_warmup_steps=0, interval_steps=1\n    ),\n)\nstudy.optimize(objective, n_trials=3, timeout=300)\nprint(study.best_value, study.best_params)\n```\n</p>\n</details>\n\n<details>\n<summary>Config API - minimal example</summary>\n<p>\n\n```yaml title=\"example.yaml\"\nrunner:\n  _target_: catalyst.runners.SupervisedRunner\n  model:\n    _var_: model\n    _target_: torch.nn.Sequential\n    args:\n      - _target_: torch.nn.Flatten\n      - _target_: torch.nn.Linear\n        in_features: 784  # 28 * 28\n        out_features: 10\n  input_key: features\n  output_key: &output_key logits\n  target_key: &target_key targets\n  loss_key: &loss_key loss\n\nrun:\n  # ‚âà stage 1\n  - _call_: train  # runner.train(...)\n\n    criterion:\n      _target_: torch.nn.CrossEntropyLoss\n\n    optimizer:\n      _target_: torch.optim.Adam\n      params:  # model.parameters()\n        _var_: model.parameters\n      lr: 0.02\n\n    loaders:\n      train:\n        _target_: torch.utils.data.DataLoader\n        dataset:\n          _target_: catalyst.contrib.datasets.MNIST\n          root: data\n          train: y\n        batch_size: 32\n\n      &valid_loader_key valid:\n        &valid_loader\n        _target_: torch.utils.data.DataLoader\n        dataset:\n          _target_: catalyst.contrib.datasets.MNIST\n          root: data\n          train: n\n        batch_size: 32\n\n    callbacks:\n      - &accuracy_metric\n        _target_: catalyst.callbacks.AccuracyCallback\n        input_key: *output_key\n        target_key: *target_key\n        topk: [1,3,5]\n      - _target_: catalyst.callbacks.PrecisionRecallF1SupportCallback\n        input_key: *output_key\n        target_key: *target_key\n\n    num_epochs: 1\n    logdir: logs\n    valid_loader: *valid_loader_key\n    valid_metric: *loss_key\n    minimize_valid_metric: y\n    verbose: y\n\n  # ‚âà stage 2\n  - _call_: evaluate_loader  # runner.evaluate_loader(...)\n    loader: *valid_loader\n    callbacks:\n      - *accuracy_metric\n\n```\n\n```sh\ncatalyst-run --config example.yaml\n```\n</p>\n</details>\n\n### Tests\nAll Catalyst code, features, and pipelines [are fully tested](./tests).\nWe also have our own [catalyst-codestyle](https://github.com/catalyst-team/codestyle) and a corresponding pre-commit hook.\nDuring testing, we train a variety of different models: image classification,\nimage segmentation, text classification, GANs, and much more.\nWe then compare their convergence metrics in order to verify\nthe correctness of the training procedure and its reproducibility.\nAs a result, Catalyst provides fully tested and reproducible\nbest practices for your deep learning research and development.\n\n### [Blog Posts](https://catalyst-team.com/post/)\n\n### [Talks](https://catalyst-team.com/talk/)\n\n\n## Community\n\n### Accelerated with Catalyst\n\n<details>\n<summary>Research Papers</summary>\n<p>\n\n- [Hierarchical Attention for Sentiment Classification with Visualization](https://github.com/neuromation/ml-recipe-hier-attention)\n- [Pediatric Bone Age Assessment](https://github.com/neuromation/ml-recipe-bone-age)\n- [Implementation of the paper \"Tell Me Where to Look: Guided Attention Inference Network\"](https://github.com/ngxbac/GAIN)\n- [Implementation of the paper \"Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks\"](https://github.com/yukkyo/PyTorch-FilterResponseNormalizationLayer)\n- [Implementation of the paper \"Utterance-level Aggregation For Speaker Recognition In The Wild\"](https://github.com/ptJexio/Speaker-Recognition)\n- [Implementation of the paper \"Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation\"](https://github.com/vitrioil/Speech-Separation)\n- [Implementation of the paper \"ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks\"](https://github.com/leverxgroup/esrgan)\n\n</p>\n</details>\n\n<details>\n<summary>Blog Posts</summary>\n<p>\n\n- [Solving the Cocktail Party Problem using PyTorch](https://medium.com/pytorch/addressing-the-cocktail-party-problem-using-pytorch-305fb74560ea)\n- [Beyond fashion: Deep Learning with Catalyst (Config API)](https://evilmartians.com/chronicles/beyond-fashion-deep-learning-with-catalyst)\n- [Tutorial from Notebook API to Config API (RU)](https://github.com/Bekovmi/Segmentation_tutorial)\n\n</p>\n</details>\n\n<details>\n<summary>Competitions</summary>\n<p>\n\n- [Kaggle Quick, Draw! Doodle Recognition Challenge](https://github.com/ngxbac/Kaggle-QuickDraw) - 11th place\n- [Catalyst.RL - NeurIPS 2018: AI for Prosthetics Challenge](https://github.com/Scitator/neurips-18-prosthetics-challenge) ‚Äì 3rd place\n- [Kaggle Google Landmark 2019](https://github.com/ngxbac/Kaggle-Google-Landmark-2019) - 30th place\n- [iMet Collection 2019 - FGVC6](https://github.com/ngxbac/Kaggle-iMet) - 24th place\n- [ID R&D Anti-spoofing Challenge](https://github.com/bagxi/idrnd-anti-spoofing-challenge-solution) - 14th place\n- [NeurIPS 2019: Recursion Cellular Image Classification](https://github.com/ngxbac/Kaggle-Recursion-Cellular) - 4th place\n- [MICCAI 2019: Automatic Structure Segmentation for Radiotherapy Planning Challenge 2019](https://github.com/ngxbac/StructSeg2019)\n  * 3rd place solution for `Task 3: Organ-at-risk segmentation from chest CT scans`\n  * and 4th place solution for `Task 4: Gross Target Volume segmentation of lung cancer`\n- [Kaggle Seversteal steel detection](https://github.com/bamps53/kaggle-severstal) - 5th place\n- [RSNA Intracranial Hemorrhage Detection](https://github.com/ngxbac/Kaggle-RSNA) - 5th place\n- [APTOS 2019 Blindness Detection](https://github.com/BloodAxe/Kaggle-2019-Blindness-Detection) ‚Äì 7th place\n- [Catalyst.RL - NeurIPS 2019: Learn to Move - Walk Around](https://github.com/Scitator/run-skeleton-run-in-3d) ‚Äì 2nd place\n- [xView2 Damage Assessment Challenge](https://github.com/BloodAxe/xView2-Solution) - 3rd place\n\n\n</p>\n</details>\n\n<details>\n<summary>Toolkits</summary>\n<p>\n\n- [Catalyst.RL](https://github.com/Scitator/catalyst-rl-framework) ‚Äì A Distributed Framework for Reproducible RL Research by [Scitator](https://github.com/Scitator)\n- [Catalyst.Classification](https://github.com/catalyst-team/classification) - Comprehensive classification pipeline with Pseudo-Labeling by [Bagxi](https://github.com/bagxi) and [Pdanilov](https://github.com/pdanilov)\n- [Catalyst.Segmentation](https://github.com/catalyst-team/segmentation) - Segmentation pipelines - binary, semantic and instance, by [Bagxi](https://github.com/bagxi)\n- [Catalyst.Detection](https://github.com/catalyst-team/detection) - Anchor-free detection pipeline by [Avi2011class](https://github.com/Avi2011class) and [TezRomacH](https://github.com/TezRomacH)\n- [Catalyst.GAN](https://github.com/catalyst-team/gan) - Reproducible GANs pipelines by [Asmekal](https://github.com/asmekal)\n- [Catalyst.Neuro](https://github.com/catalyst-team/neuro) - Brain image analysis project, in collaboration with [TReNDS Center](https://trendscenter.org)\n- [MLComp](https://github.com/catalyst-team/mlcomp) ‚Äì Distributed DAG framework for machine learning with UI by [Lightforever](https://github.com/lightforever)\n- [Pytorch toolbelt](https://github.com/BloodAxe/pytorch-toolbelt) - PyTorch extensions for fast R&D prototyping and Kaggle farming by [BloodAxe](https://github.com/BloodAxe)\n- [Helper functions](https://github.com/ternaus/iglovikov_helper_functions) - An assorted collection of helper functions by [Ternaus](https://github.com/ternaus)\n- [BERT Distillation with Catalyst](https://github.com/elephantmipt/bert-distillation) by [elephantmipt](https://github.com/elephantmipt)\n\n</p>\n</details>\n\n\n<details>\n<summary>Other</summary>\n<p>\n\n- [CamVid Segmentation Example](https://github.com/BloodAxe/Catalyst-CamVid-Segmentation-Example) - Example of semantic segmentation for CamVid dataset\n- [Notebook API tutorial for segmentation in Understanding Clouds from Satellite Images Competition](https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools/)\n- [Catalyst.RL - NeurIPS 2019: Learn to Move - Walk Around](https://github.com/Scitator/learning-to-move-starter-kit) ‚Äì starter kit\n- [Catalyst.RL - NeurIPS 2019: Animal-AI Olympics](https://github.com/Scitator/animal-olympics-starter-kit) - starter kit\n- [Inria Segmentation Example](https://github.com/BloodAxe/Catalyst-Inria-Segmentation-Example) - An example of training segmentation model for Inria Sattelite Segmentation Challenge\n- [iglovikov_segmentation](https://github.com/ternaus/iglovikov_segmentation) - Semantic segmentation pipeline using Catalyst\n- [Logging Catalyst Runs to Comet](https://colab.research.google.com/drive/1TaG27HcMh2jyRKBGsqRXLiGUfsHVyCq6?usp=sharing) - An example of how to log metrics, hyperparameters and more from Catalyst runs to [Comet](https://www.comet.ml/site/data-scientists/)\n\n</p>\n</details>\n\n\nSee other projects at [the GitHub dependency graph](https://github.com/catalyst-team/catalyst/network/dependents).\n\nIf your project implements a paper,\na notable use-case/tutorial, or a Kaggle competition solution, or\nif your code simply presents interesting results and uses Catalyst,\nwe would be happy to add your project to the list above!\nDo not hesitate to send us a PR with a brief description of the project similar to the above.\n\n### Contribution Guide\n\nWe appreciate all contributions.\nIf you are planning to contribute back bug-fixes, there is no need to run that by us; just send a PR.\nIf you plan to contribute new features, new utility functions, or extensions,\nplease open an issue first and discuss it with us.\n\n- Please see the [Contribution Guide](CONTRIBUTING.md) for more information.\n- By participating in this project, you agree to abide by its [Code of Conduct](CODE_OF_CONDUCT.md).\n\n\n### User Feedback\n\nWe've created `feedback@catalyst-team.com` as an additional channel for user feedback.\n\n- If you like the project and want to thank us, this is the right place.\n- If you would like to start a collaboration between your team and Catalyst team to improve Deep Learning R&D, you are always welcome.\n- If you don't like Github Issues and prefer email, feel free to email us.\n- Finally, if you do not like something, please, share it with us, and we can see how to improve it.\n\nWe appreciate any type of feedback. Thank you!\n\n\n### Acknowledgments\n\nSince the beginning of the –°atalyst development, a lot of people have influenced it in a lot of different ways.\n\n#### Catalyst.Team\n- [Dmytro Doroshenko](https://www.linkedin.com/in/dmytro-doroshenko-05671112a/) ([ditwoo](https://github.com/Ditwoo))\n- [Eugene Kachan](https://www.linkedin.com/in/yauheni-kachan/) ([bagxi](https://github.com/bagxi))\n- [Nikita Balagansky](https://www.linkedin.com/in/nikita-balagansky-50414a19a/) ([elephantmipt](https://github.com/elephantmipt))\n- [Sergey Kolesnikov](https://www.scitator.com/) ([scitator](https://github.com/Scitator))\n\n#### Catalyst.Contributors\n- [Aleksey Grinchuk](https://www.facebook.com/grinchuk.alexey) ([alexgrinch](https://github.com/AlexGrinch))\n- [Aleksey Shabanov](https://linkedin.com/in/aleksey-shabanov-96b351189) ([AlekseySh](https://github.com/AlekseySh))\n- [Alex Gaziev](https://www.linkedin.com/in/alexgaziev/) ([gazay](https://github.com/gazay))\n- [Andrey Zharkov](https://www.linkedin.com/in/andrey-zharkov-8554a1153/) ([asmekal](https://github.com/asmekal))\n- [Artem Zolkin](https://www.linkedin.com/in/artem-zolkin-b5155571/) ([arquestro](https://github.com/Arquestro))\n- [David Kuryakin](https://www.linkedin.com/in/dkuryakin/) ([dkuryakin](https://github.com/dkuryakin))\n- [Evgeny Semyonov](https://www.linkedin.com/in/ewan-semyonov/) ([lightforever](https://github.com/lightforever))\n- [Eugene Khvedchenya](https://www.linkedin.com/in/cvtalks/) ([bloodaxe](https://github.com/BloodAxe))\n- [Ivan Stepanenko](https://www.facebook.com/istepanenko)\n- [Julia Shenshina](https://github.com/julia-shenshina) ([julia-shenshina](https://github.com/julia-shenshina))\n- [Nguyen Xuan Bac](https://www.linkedin.com/in/bac-nguyen-xuan-70340b66/) ([ngxbac](https://github.com/ngxbac))\n- [Roman Tezikov](http://linkedin.com/in/roman-tezikov/) ([TezRomacH](https://github.com/TezRomacH))\n- [Valentin Khrulkov](https://www.linkedin.com/in/vkhrulkov/) ([khrulkovv](https://github.com/KhrulkovV))\n- [Vladimir Iglovikov](https://www.linkedin.com/in/iglovikov/) ([ternaus](https://github.com/ternaus))\n- [Vsevolod Poletaev](https://linkedin.com/in/vsevolod-poletaev-468071165) ([hexfaker](https://github.com/hexfaker))\n- [Yury Kashnitsky](https://www.linkedin.com/in/kashnitskiy/) ([yorko](https://github.com/Yorko))\n\n\n### Trusted by\n- [Awecom](https://www.awecom.com)\n- Researchers at the [Center for Translational Research in Neuroimaging and Data Science (TReNDS)](https://trendscenter.org)\n- [Deep Learning School](https://en.dlschool.org)\n- Researchers at [Emory University](https://www.emory.edu)\n- [Evil Martians](https://evilmartians.com)\n- Researchers at the [Georgia Institute of Technology](https://www.gatech.edu)\n- Researchers at [Georgia State University](https://www.gsu.edu)\n- [Helios](http://helios.to)\n- [HPCD Lab](https://www.hpcdlab.com)\n- [iFarm](https://ifarmproject.com)\n- [Kinoplan](http://kinoplan.io/)\n- Researchers at the [Moscow Institute of Physics and Technology](https://mipt.ru/english/)\n- [Neuromation](https://neuromation.io)\n- [Poteha Labs](https://potehalabs.com/en/)\n- [Provectus](https://provectus.com)\n- Researchers at the [Skolkovo Institute of Science and Technology](https://www.skoltech.ru/en)\n- [SoftConstruct](https://www.softconstruct.io/)\n- Researchers at [Tinkoff](https://www.tinkoff.ru/eng/)\n- Researchers at [Yandex.Research](https://research.yandex.com)\n\n\n### Citation\n\nPlease use this bibtex if you want to cite this repository in your publications:\n\n    @misc{catalyst,\n        author = {Kolesnikov, Sergey},\n        title = {Catalyst - Accelerated deep learning R&D},\n        year = {2018},\n        publisher = {GitHub},\n        journal = {GitHub repository},\n        howpublished = {\\url{https://github.com/catalyst-team/catalyst}},\n    }\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "catalyst",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.154296875,
          "content": "[tool.nitpick]\nstyle = \"https://raw.githubusercontent.com/catalyst-team/codestyle/v21.09.2/styles/nitpick-style-catalyst.toml\"\n\n[tool.black]\nline-length = 89\n"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 1.0517578125,
          "content": "[flake8]\nignore = D100,D104,D107,D200,D204,D205,D212,D214,D301,D400,D401,D402,D412,D413,D415,DAR003,DAR103,DAR203,RST201,RST203,RST210,RST213,RST301,RST304,E203,E731,N812,P101,W503,W504,W605\nmax-line-length = 89\nmax-doc-length = 89\ninline-quotes = double\nmultiline-quotes = double\ndocstring-quotes = double\nconvention = google\ndocstring_style = google\nstrictness = short\n\n[darglint]\nignore_regex=^_(.*)\n\n[isort]\ncombine_as_imports = true\norder_by_type = false\nforce_grid_wrap = 0\nforce_sort_within_sections = true\nline_length = 89\nlines_between_types = 0\nmulti_line_output = 3\nno_lines_before = STDLIB,LOCALFOLDER\nreverse_relative = true\ndefault_section = THIRDPARTY\nknown_first_party = catalyst,hydra_slayer\nknown_src = src\nskip_glob = **/__init__.py\nforce_to_top = typing\ninclude_trailing_comma = true\nuse_parentheses = true\n\n# catalyst imports order:\n#  - typing\n#  - core python libs\n#  - python libs (known_third_party)\n#  - dl libs (known_dl)\n#  - catalyst imports\nsections = FUTURE,STDLIB,THIRDPARTY,DL,FIRSTPARTY,SRC,LOCALFOLDER\nknown_dl = accelerate,tensorboardX,torch"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.5224609375,
          "content": "#!/usr/bin/env python\n# flake8: noqa\n# -*- coding: utf-8 -*-\n\nimport os\n\nfrom setuptools import find_packages, setup\n\n# Package meta-data.\nNAME = \"catalyst\"\nDESCRIPTION = \"Catalyst. Accelerated deep learning R&D with PyTorch.\"\nURL = \"https://github.com/catalyst-team/catalyst\"\nEMAIL = \"scitator@gmail.com\"\nAUTHOR = \"Sergey Kolesnikov\"\nREQUIRES_PYTHON = \">=3.7.0\"\n\nPROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))\n\n\ndef load_requirements(filename):\n    \"\"\"Docs? Contribution is welcome.\"\"\"\n    with open(os.path.join(PROJECT_ROOT, filename), \"r\") as f:\n        return f.read().splitlines()\n\n\ndef load_readme():\n    \"\"\"Docs? Contribution is welcome.\"\"\"\n    readme_path = os.path.join(PROJECT_ROOT, \"README.md\")\n    with open(readme_path, encoding=\"utf-8\") as f:\n        return f\"\\n{f.read()}\"\n\n\ndef load_version():\n    \"\"\"Docs? Contribution is welcome.\"\"\"\n    context = {}\n    with open(os.path.join(PROJECT_ROOT, \"catalyst\", \"__version__.py\")) as f:\n        exec(f.read(), context)\n    return context[\"__version__\"]\n\n\n# Specific dependencies.\nextras = {\n    \"cv\": load_requirements(\"requirements/requirements-cv.txt\"),\n    \"comet\": load_requirements(\"requirements/requirements-comet.txt\"),\n    \"deepspeed\": load_requirements(\"requirements/requirements-deepspeed.txt\"),\n    \"dev\": load_requirements(\"requirements/requirements-dev.txt\"),\n    \"ml\": load_requirements(\"requirements/requirements-ml.txt\"),\n    \"mlflow\": load_requirements(\"requirements/requirements-mlflow.txt\"),\n    \"neptune\": load_requirements(\"requirements/requirements-neptune.txt\"),\n    \"onnx-gpu\": load_requirements(\"requirements/requirements-onnx-gpu.txt\"),\n    \"onnx\": load_requirements(\"requirements/requirements-onnx.txt\"),\n    \"optuna\": load_requirements(\"requirements/requirements-optuna.txt\"),\n    \"profiler\": load_requirements(\"requirements/requirements-profiler.txt\"),\n    \"wandb\": load_requirements(\"requirements/requirements-wandb.txt\"),\n    # \"xla\": load_requirements(\"requirements/requirements-xla.txt\"),\n}\nextras[\"all\"] = extras[\"cv\"] + extras[\"ml\"] + extras[\"optuna\"]\n# Meta dependency groups.\n# all_deps = []\n# for group_name in extras:\n#     all_deps += extras[group_name]\n# extras[\"all\"] = all_deps\n\nsetup(\n    name=NAME,\n    version=load_version(),\n    description=DESCRIPTION,\n    long_description=load_readme(),\n    long_description_content_type=\"text/markdown\",\n    keywords=[\n        \"Machine Learning\",\n        \"Distributed Computing\",\n        \"Deep Learning\",\n        \"Reinforcement Learning\",\n        \"Computer Vision\",\n        \"Natural Language Processing\",\n        \"Recommendation Systems\",\n        \"Information Retrieval\",\n        \"PyTorch\",\n    ],\n    author=AUTHOR,\n    author_email=EMAIL,\n    python_requires=REQUIRES_PYTHON,\n    url=URL,\n    download_url=URL,\n    project_urls={\n        \"Bug Tracker\": \"https://github.com/catalyst-team/catalyst/issues\",\n        \"Documentation\": \"https://catalyst-team.github.io/catalyst\",\n        \"Source Code\": \"https://github.com/catalyst-team/catalyst\",\n    },\n    packages=find_packages(\n        exclude=(\n            \"docker\",\n            \"docs\",\n            \"tests\",\n            \"examples\",\n        )\n    ),\n    entry_points={\n        \"console_scripts\": [\n            \"catalyst-contrib=catalyst.contrib.__main__:main\",\n            \"catalyst-run=catalyst.contrib.scripts.run:main\",\n            \"catalyst-tune=catalyst.contrib.scripts.tune:main\",\n        ],\n    },\n    scripts=[\n        \"bin/scripts/catalyst-parallel-run\",\n        \"bin/scripts/download-gdrive\",\n        \"bin/scripts/extract-archive\",\n    ],\n    install_requires=load_requirements(\"requirements/requirements.txt\"),\n    extras_require=extras,\n    include_package_data=True,\n    license=\"Apache License 2.0\",\n    classifiers=[\n        \"Environment :: Console\",\n        \"Natural Language :: English\",\n        \"Development Status :: 4 - Beta\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        # Audience\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        # Topics\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering :: Image Recognition\",\n        \"Topic :: Scientific/Engineering :: Information Analysis\",\n        # Programming\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n    ],\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}