{
  "metadata": {
    "timestamp": 1736560401618,
    "page": 956,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lightly-ai/lightly",
      "stars": 3242,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.048828125,
          "content": "lightly/openapi_generated linguist-generated=true\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.8486328125,
          "content": "**__pycache__**\n**build\n**settings.json\nlightning_logs/\n**lightning_logs/\n**/__MACOSX\ndatasets/\ndist/\ndocs/source/tutorials/package/*\ndocs/source/tutorials/platform/*\ndocs/source/tutorials_source/platform/data\ndocs/source/tutorials_source/platform/pizzas\ndocs/source/docker/resources\ndocs/source/getting_started/resources\n!docs/source/tutorials/package.rst\n!docs/source/tutorials/platform.rst\n!docs/source/tutorials/package/structure_your_input.rst\n**.zip\n*.egg-info\n\n\n\n#ignore venv\nvenv\n.venv\n#ignore pycharm IDE\n.idea\n\n#ignore lightly outputs\nlightly_outputs\n\n#ignore .DS_Store\n**.DS_Store\n\n#ignore eggs\n.eggs\ntests/UNMOCKED_end2end_tests/call_test_api.py\ntests/UNMOCKED_end2end_tests/get_versions_all_apis.py\ntests/UNMOCKED_end2end_tests/call_benchmark_upload_private.py\ntests/UNMOCKED_end2end_tests/test_magic_on_branch.sh\n\n# ignore VSCode\n.vscode\n*.code-workspace\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.365234375,
          "content": "repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v4.6.0\n  hooks:\n  - id: detect-private-key      # check for private keys\n  - id: check-added-large-files # prevent commit of files >500kB\n    args: ['--maxkb=500']\n- repo: https://github.com/psf/black\n  rev: 24.8.0  # aligned with the version defined in pyproject.toml\n  hooks:\n  - id: black\n- repo: https://github.com/pycqa/isort\n  rev: 5.13.2  # aligned with the version defined in pyproject.toml\n  hooks:\n  - id: isort\n- repo: https://github.com/pre-commit/mirrors-mypy\n  rev: v1.11.1  # aligned with the version defined in pyproject.toml\n  hooks:\n  - id: mypy\n    additional_dependencies:\n      - 'numpy'\n- repo: https://github.com/nbQA-dev/nbQA\n  rev: 1.8.7\n  hooks:\n    - id: nbqa-black\n    - id: nbqa-isort\n      args: [\"--profile=black\"]\n- repo: local\n  hooks:\n  - id: pytest-check                     # run all tests\n    name: pytest-check\n    entry: make test-fast\n    language: system\n    pass_filenames: false\n    stages: [push]\n    # Avoid running tests if non-tested files have changed.\n    # The regex follows the pattern in the docs: https://pre-commit.com/#regular-expressions\n    exclude: |\n      (?x)^(\n          benchmark_logs/.*|\n          docs/.*|\n          examples/.*|\n          \\.gitignore|\n          CONTRIBUTING\\.md|\n          LICENSE\\.txt|\n          README\\.md\n          PRECOMMITHOOKS\\.md|\n      )$\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.515625,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs/source/conf.py\n\n# Optionally build your docs in additional formats such as PDF\nformats:\n  - pdf\n\n# Optionally set the version of Python and requirements required to build your docs\npython:\n  version: 3.7\n  install:\n    - method: pip\n      path: .\n      extra_requirements:\n        - dev\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.44140625,
          "content": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n  - family-names: Susmelj\n    given-names: Igor\n  - family-names: Heller\n    given-names: Matthias\n  - family-names: Wirth\n    given-names: Philipp\n  - family-names: Prescott\n    given-names: Jeremy\n  - family-names: Ebner\n    given-names: Malte\n  - name: \"et al.\"\ntitle: \"Lightly\"\ntype: software\nurl: \"https://github.com/lightly-ai/lightly\"\ndate-released: 2020\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 6.8955078125,
          "content": "# How to contribute to lightly?\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code is thus not the only way to help the community. Answering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.\n\nIt also helps us if you spread the word: reference the library from blog posts on the awesome projects it made possible, shout out on Twitter every time it has helped you, or simply star the repo to say \"thank you\".\n\n## You can contribute in so many ways!\n\nThere are 4 ways you can contribute to lightly:\n* Fixing outstanding issues with the existing code;\n* Implementing new models;\n* Contributing to the examples or to the documentation;\n* Submitting issues related to bugs or desired new features.\n\n*All are equally valuable to the community.*\n\n## Submitting a new issue or feature request\n\nDo your best to follow these guidelines when submitting an issue or a feature\nrequest. It will make it easier for us to come back to you quickly and with good\nfeedback.\n\n### Did you find a bug?\n\nFirst, **please make sure the bug was not already reported** (use the search bar on Github under Issues).\n\n* Include your **OS type and version**, the versions of **Python**, **PyTorch**, and **PyTorch Lightning**.\n* A code snippet that allows us to reproduce the bug in less than 30s.\n* Provide the *full* traceback if an exception is raised.\n\n### Do you want to implement a new self-supervised model?\n\nAwesome! Please provide the following information:\n\n* Short description of the model and link to the paper;\n* Link to the implementation if it's open source;\n\nIf you are willing to contribute the model yourself, let us know so we can best\nguide you.\n\n### Do you want a new feature (that is not a model)?\n\nA world-class feature request addresses the following points:\n\n1. Motivation first:\n  * Is it related to a problem/frustration with the library? If so, please explain\n    why. Providing a code snippet that demonstrates the problem is best.\n  * Is it related to something you would need for a project? We'd love to hear\n    about it!\n  * Is it something you worked on and think could benefit the community?\n    Awesome! Tell us what problem it solved for you.\n2. Provide a **code snippet** that demonstrates its future use;\n3. Attach any additional information (drawings, screenshots, etc.) you think may help.\n\n\n## Pull Requests\n\nBefore writing code, we strongly advise you to search through the exising PRs or\nissues to make sure that nobody is already working on the same thing. If you are\nunsure, it is always a good idea to open an issue to get some feedback.\n\nFollow these steps to start contributing:\n\n1. Fork the [repository](https://github.com/lightly-ai/lightly/) by\n   clicking on the 'Fork' button on the repository's page. This creates a copy of the code\n   under your GitHub user account.\n\n2. Clone your fork to your local disk, and add the base repository as a remote:\n\n   ```bash\n   git clone git@github.com:lightly-ai/lightly.git\n   cd lightly\n   git remote add upstream https://github.com/lightly-ai/lightly.git\n   ```\n\n3. Create a new branch to hold your development changes:\n\n   ```bash\n   git checkout -b a_descriptive_name_for_my_changes\n   ```\n\n   **do not** work on the `master` branch.\n\n4. Set up a development environment by running the following command in a virtual environment:\n\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n   If you are using [uv](https://github.com/astral-sh/uv) instead of pip, you can use\n   the following command:\n\n   ```bash\n   make install-dev\n   ```\n\n5. **(Optional)** Install pre-commit hooks:\n\n   ```bash\n   pip install pre-commit\n   pre-commit install\n   ```\n\n   We use pre-commit hooks to identify simple issues before submission to code review. In particular, our hooks currently check for:\n   * Private keys in the commit\n   * Large files in the commit (>500kB)\n   * Run formatting checks using `black`, `isort` and `mypy`.\n   * Units which don't pass their unit tests (on push only)\n\n   You can verify that the hooks were installed correctly with\n   ```\n   pre-commit run --all-files\n   ```\n   The output should look like this:\n   ```\n   pre-commit run --all-files\n   Detect Private Key................................Passed\n   Check for added large files.......................Passed\n   black.............................................Passed\n   isort.............................................Passed\n   mypy..............................................Passed\n   ```\n\n6. Develop the features on your branch.\n\n   As you work on the features, you should make sure that the code is formatted and the\n   test suite passes:\n\n   ```bash\n   make format\n   make all-checks\n   ```\n\n   If you get an error from isort or black, please run `make format` again before\n   running `make all-checks`.\n\n   If you're modifying documents under `docs/source`, make sure to validate that\n   they can still be built. This check also runs in CI. \n\n   ```bash\n   cd docs\n   make html\n   ```\n   Once you're happy with your changes, add changed files using `git add` and\n   make a commit with `git commit` to record your changes locally:\n\n   ```bash\n   git add modified_file.py\n   git commit\n   ```\n\n   Please write [good commit messages](https://chris.beams.io/posts/git-commit/).\n\n   It is a good idea to sync your copy of the code with the original\n   repository regularly. This way you can quickly account for changes:\n\n   ```bash\n   git fetch upstream\n   git rebase upstream/develop\n   ```\n\n   Push the changes to your account using:\n\n   ```bash\n   git push -u origin a_descriptive_name_for_my_changes\n   ```\n\n7. Once you are satisfied, go to the webpage of your fork on GitHub.\n   Click on 'Pull request' to send your changes to the project maintainers for review.\n\n8. It's ok if maintainers ask you for changes. It happens to core contributors\n   too! So everyone can see the changes in the Pull request, work in your local\n   branch and push the changes to your fork. They will automatically appear in\n   the pull request.\n\n9. We have a extensive Continuous Integration system that runs tests on all Pull Requests. This\n   is to make sure that the changes introduced by the commits don’t introduce errors. When\n   all CI tests in a workflow pass, it implies that the changes introduced by a commit do not introduce any errors.\n   We have workflows that check unit tests, dependencies, and formatting.\n\n### Style guide\n\n`lightly` follows the [Google styleguide](https://google.github.io/styleguide/pyguide.html) and the [PyTorch styleguide](https://github.com/IgorSusmelj/pytorch-styleguide) by Igor Susmelj.\nCheck our [documentation writing guide](https://github.com/lightly-ai/lightly/docs/README.md) for more information.\n\n#### This guide was inspired by Transformers [transformers guide to contributing](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md) which was influenced by Scikit-learn [scikit-learn guide to contributing](https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md)."
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.0478515625,
          "content": "Copyright (c) 2018 The Python Packaging Authority\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "MAINTAINING.md",
          "type": "blob",
          "size": 0.740234375,
          "content": "# How to Maintain LightlySSL\n\nThis document is intended for maintainers of Lightly**SSL**. If you would like to\ncontribute, please refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) document.\n\n## Update PR from a Fork\n\nSometimes it is necessary to update a PR from a forked Lightly**SSL** repository,\nfor example to add some finishing touches (format, cleanup, docs), to fix some difficult\nissue, or to finish an incomplete feature.\n\nLet's assume there is a PR from `<username>:<branch-name>`. To update the PR, follow\nthese steps:\n\n```\ngit remote add <username> https://github.com/<username>/lightly.git\ngit fetch <username>\ngit checkout -b <branch-name> <username>/<branch-name>\n```\n\nNow you can make changes and push them to the PR branch with `git push`.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.099609375,
          "content": "# exclude benchmarks, docs, examples, and tests\nprune docs\nprune benchmarks\nprune examples\nprune tests"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 5.958984375,
          "content": "# Copyright (c) 2020. Lightly AG and its affiliates.\n# All Rights Reserved\n\n.PHONY: clean clean-build clean-pyc clean-out docs help\n.DEFAULT_GOAL := help\n\n# TODO\nhelp:\n\n## make clean\nclean: clean-build clean-pyc clean-out\n\n## remove build artifacts\nclean-build:\n\trm -fr build/\n\trm -fr dist/\n\trm -fr .eggs/\n\tfind . -name '*.egg-info' -exec rm -fr {} +\n\tfind . -name '*.egg' -exec rm -f {} +\n\n## remove python file artifacts\nclean-pyc:\n\tfind . -name '__pycache__' -exec rm -fr {} +\n\n## remove hydra outputs\nclean-out:\n\trm -fr outputs/\n\trm -fr lightly_outputs/\n\trm -fr lightning_logs/\n\trm -fr lightly_epoch_*.ckpt\n\trm -fr last.ckpt\n\n\n# format code with isort and black\nformat:\n\tisort .\n\tblack .\n\n# check if code is formatted with isort and black\nformat-check:\n\t@echo \"⚫ Checking code format...\"\n\tisort --check-only --diff .\n\tblack --check .\n\n# check style with flake8\nlint: lint-lightly lint-tests\n\n## check lightly style with flake8\nlint-lightly:\n\tpylint --rcfile=pylintrc lightly\n\n## check tests style with flake8\nlint-tests:\n\tpylint --rcfile=pylintrc tests\n\n## run tests\ntest:\n\tpytest tests --runslow\n\ntest-fast:\n\tpytest tests\n\n## check typing\ntype-check:\n\tmypy lightly tests\n\n## run format checks\nstatic-checks: format-check type-check\n\n## run format checks and tests\nall-checks: static-checks test\n\n## build source and wheel package\ndist: clean\n\tpython -m build\n\tls -l dist\n\n\n\n# uninstall package from active site\nuninstall: clean\n\tpip uninstall lightly\n\n\n## helper for renaming\nfind: \n\t@read -p \"Enter Term: \" term; \\\n\tgrep -rnw ./ -e \"$$term\"\n\n\n### Virtual Environment\n\n.PHONY: install-uv\ninstall-uv:\n\tcurl -LsSf https://astral.sh/uv/0.2.25/install.sh | sh\n\n\n.PHONY: reset-venv\nreset-venv:\n\tdeactivate || true\n\trm -rf .venv\n\tuv venv .venv\n\n\n### Dependencies\n\n# When running these commands locally, it is recommended to first reset the environment\n# with: `make reset-venv && source .venv/bin/activate`\n# Otherwise old dependencies might linger around.\n\n# Set EDITABLE to -e to install the package in editable mode outside of CI. This is\n# useful for local development.\nifdef CI\nEDITABLE=\nelse\nEDITABLE=-e\nendif\n\n# Date until which dependencies installed with --exclude-newer must have been released.\n# Dependencies released after this date are ignored.\nEXCLUDE_NEWER_DATE=\"2024-08-07\"\n\n# Install package for local development.\n.PHONY: install-dev\ninstall-dev:\n\tuv pip install ${EDITABLE} . --all-extras --requirement pyproject.toml\n\n\n# Install package with API dependencies only.\n# Should be same command as in the docs with some extra flags for CI:\n# https://docs.lightly.ai/docs/install-lightly#install-the-lightly-python-client\n.PHONY: install-api-only\ninstall-api-only:\n\tuv pip install --exclude-newer ${EXCLUDE_NEWER_DATE} --requirement requirements/base.txt\n\tuv pip install --exclude-newer ${EXCLUDE_NEWER_DATE} . --no-deps\n\n# Install package with minimal dependencies.\n# \n# This command is split into multiple steps:\n# 1. Install the dev dependencies to be able to run tests. We don't want to use\n#    the minimal versions for these dependencies.\n# 2. Then we reinstall the package with minimal dependencies.\n# 3. Finally we install setuptools<50. This is necessary for compatibility with old\n#    PyTorch Lightning versions that do not include the correct setuptools dependencies.\n#\n# Explanation of flags:\n# --exclude-newer: We don't want to install dependencies released after that date to\n#   keep CI stable.\n# --resolution=lowest-direct: Only install minimal versions for direct dependencies.\n#   Transitive dependencies will use the latest compatible version.\n# \tUsing --resolution=lowest would also download the latest versions for transitive\n#   dependencies which is not a realistic scenario and results in some extremely old\n#   dependencies being installed.\n# --reinstall: Reinstall dependencies to make sure they satisfy the constraints.\n.PHONY: install-minimal\ninstall-minimal:\n\tuv pip install --exclude-newer ${EXCLUDE_NEWER_DATE} ${EDITABLE} \".[dev]\"\n\tuv pip install --resolution=lowest-direct --exclude-newer ${EXCLUDE_NEWER_DATE} --reinstall ${EDITABLE} \".[minimal]\"\n\tuv pip install --exclude-newer ${EXCLUDE_NEWER_DATE} --reinstall \"setuptools<50\"\n\n# Install package with minimal dependencies including extras.\n# See install-minimal for explanation of flags.\n# We do not use --all-extras because it includes the dev dependencies for which we don't\n# want to install the minimal versions.\n.PHONY: install-minimal-extras\ninstall-minimal-extras:\n\tuv pip install --exclude-newer ${EXCLUDE_NEWER_DATE} ${EDITABLE} \".[dev]\"\n\tuv pip install --resolution=lowest-direct --exclude-newer ${EXCLUDE_NEWER_DATE} --reinstall ${EDITABLE} \".[matplotlib,minimal,timm,video]\" --requirement pyproject.toml\n\tuv pip install --exclude-newer ${EXCLUDE_NEWER_DATE} --reinstall \"setuptools<50\"\n\n# Install package with dependencies pinned to the latest compatible version available at\n# EXCLUDE_NEWER_DATE. This keeps CI stable if new versions of dependencies are released.\n.PHONY: install-pinned\ninstall-pinned:\n\tuv pip install --exclude-newer ${EXCLUDE_NEWER_DATE} --reinstall ${EDITABLE} . --requirement pyproject.toml\n\n# Install package with all extras and dependencies pinned to the latest compatible\n# version available at EXCLUDE_NEWER_DATE. This keeps CI stable if new versions of\n# dependencies are released.\n.PHONY: install-pinned-extras\ninstall-pinned-extras:\n\tuv pip install --exclude-newer ${EXCLUDE_NEWER_DATE} --reinstall ${EDITABLE} . --all-extras --requirement pyproject.toml\n\n# Install package with the latest dependencies.\n.PHONY: install-latest\ninstall-latest:\n\tuv pip install --upgrade --reinstall ${EDITABLE} . --all-extras --requirement pyproject.toml\n\n\n# Generate Notebooks from examples\n.PHONY: generate-example-notebooks\ngenerate-example-notebooks:\n\tpython examples/create_example_nbs.py examples/pytorch examples/notebooks/pytorch\n\tpython examples/create_example_nbs.py examples/pytorch_lightning examples/notebooks/pytorch_lightning\n\tpython examples/create_example_nbs.py examples/pytorch_lightning_distributed examples/notebooks/pytorch_lightning_distributed\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 30.4814453125,
          "content": "<a name=\"top\"></a>\n![LightlySSL self-supervised learning Logo](docs/logos/lightly_SSL_logo_crop.png)\n\n![GitHub](https://img.shields.io/github/license/lightly-ai/lightly)\n![Unit Tests](https://github.com/lightly-ai/lightly/workflows/Unit%20Tests/badge.svg)\n[![PyPI](https://img.shields.io/pypi/v/lightly)](https://pypi.org/project/lightly/)\n[![Downloads](https://static.pepy.tech/badge/lightly)](https://pepy.tech/project/lightly)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Discord](https://img.shields.io/discord/752876370337726585?logo=discord&logoColor=white&label=discord&color=7289da)](https://discord.gg/xvNJW94)\n![codecov.io](https://codecov.io/github/lightly-ai/lightly/coverage.svg?branch=master)\n\n\nLightly**SSL** is a computer vision framework for self-supervised learning.\n\n- [Documentation](https://docs.lightly.ai/self-supervised-learning/)\n- [Github](https://github.com/lightly-ai/lightly)\n- [Discord](https://discord.gg/xvNJW94) (We have weekly paper sessions!)\n\nFor a commercial version with more features, including Docker support and pretraining\nmodels for embedding, classification, detection, and segmentation tasks with\na single command, please contact sales@lightly.ai.\n\nWe've also built a whole platform on top, with additional features for active learning\nand [data curation](https://docs.lightly.ai/docs/what-is-lightly). If you're interested in the\nLightly Worker Solution to easily process millions of samples and run [powerful algorithms](https://docs.lightly.ai/docs/customize-a-selection)\non your data, check out [lightly.ai](https://www.lightly.ai). It's free to get started!\n\n## Features\n\nThis self-supervised learning framework offers the following features:\n\n- Modular framework, which exposes low-level building blocks such as loss functions and\n  model heads.\n- Easy to use and written in a PyTorch-like style.\n- Supports custom backbone models for self-supervised pre-training.\n- Support for distributed training using PyTorch Lightning.\n\n### Supported Models\n\nYou can [find sample code for all the supported models here.](https://docs.lightly.ai/self-supervised-learning/examples/models.html) We provide PyTorch, PyTorch Lightning,\nand PyTorch Lightning distributed examples for all models to kickstart your project.\n\n**Models**:\n\n| Model          | Year | Paper | Docs | Colab (PyTorch) | Colab (PyTorch Lightning) |\n|----------------|------|-------|------|-----------------|----------------------------|\n| AIM            | 2024 | [paper](https://arxiv.org/abs/2401.08541) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/aim.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/aim.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/aim.ipynb) |\n| Barlow Twins   | 2021 | [paper](https://arxiv.org/abs/2103.03230) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/barlowtwins.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/barlowtwins.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/barlowtwins.ipynb) |\n| BYOL           | 2020 | [paper](https://arxiv.org/abs/2006.07733) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/byol.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/byol.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/byol.ipynb) |\n| DCL & DCLW     | 2021 | [paper](https://arxiv.org/abs/2110.06848) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/dcl.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/dcl.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/dcl.ipynb) |\n| DenseCL        | 2021 | [paper](https://arxiv.org/abs/2011.09157) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/densecl.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/densecl.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/densecl.ipynb) |\n| DINO           | 2021 | [paper](https://arxiv.org/abs/2104.14294) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/dino.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/dino.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/dino.ipynb) |\n| MAE            | 2021 | [paper](https://arxiv.org/abs/2111.06377) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/mae.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/mae.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/mae.ipynb) |\n| MSN            | 2022 | [paper](https://arxiv.org/abs/2204.07141) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/msn.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/msn.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/msn.ipynb) |\n| MoCo           | 2019 | [paper](https://arxiv.org/abs/1911.05722) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/moco.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/moco.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/moco.ipynb) |\n| NNCLR          | 2021 | [paper](https://arxiv.org/abs/2104.14548) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/nnclr.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/nnclr.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/nnclr.ipynb) |\n| PMSN           | 2022 | [paper](https://arxiv.org/abs/2210.07277) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/pmsn.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/pmsn.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/pmsn.ipynb) |\n| SimCLR         | 2020 | [paper](https://arxiv.org/abs/2002.05709) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/simclr.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simclr.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simclr.ipynb) |\n| SimMIM         | 2022 | [paper](https://arxiv.org/abs/2111.09886) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/simmim.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simmim.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simmim.ipynb) |\n| SimSiam        | 2021 | [paper](https://arxiv.org/abs/2011.10566) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/simsiam.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simsiam.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simsiam.ipynb) |\n| SwaV           | 2020 | [paper](https://arxiv.org/abs/2006.09882) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/swav.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/swav.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/swav.ipynb) |\n| VICReg         | 2021 | [paper](https://arxiv.org/abs/2105.04906) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/vicreg.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/vicreg.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/vicreg.ipynb) |\n\n## Tutorials\n\nWant to jump to the tutorials and see Lightly in action?\n\n- [Train MoCo on CIFAR-10](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_moco_memory_bank.html)\n- [Train SimCLR on Clothing Data](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html)\n- [Train SimSiam on Satellite Images](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simsiam_esa.html)\n- [Use Lightly with Custom Augmentations](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_custom_augmentations.html)\n- [Pre-train a Detectron2 Backbone with Lightly](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_pretrain_detectron2.html)\n- [Finetuning Lightly Checkpoints](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_checkpoint_finetuning.html)\n- [Using timm Models as Backbones](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_timm_backbone.html)\n\nCommunity and partner projects:\n\n- [On-Device Deep Learning with Lightly on an ARM microcontroller](https://github.com/ARM-software/EndpointAI/tree/master/ProofOfConcepts/Vision/OpenMvMaskDefaults)\n\n## Quick Start\n\nLightly requires **Python 3.7+**. We recommend installing Lightly in a **Linux** or **OSX** environment. Python 3.13 is not yet supported, as PyTorch itself lacks Python 3.13 compatibility.\n\n### Dependencies\n\nDue to the modular nature of the Lightly package some modules can be used with older versions of dependencies. However, to use all features as of today lightly requires the following dependencies:\n\n- [PyTorch](https://pytorch.org/)>=1.11.0\n- [Torchvision](https://pytorch.org/vision/stable/index.html)>=0.12.0\n- [PyTorch Lightning](https://www.pytorchlightning.ai/index.html)>=1.7.1\n\nLightly is compatible with PyTorch and PyTorch Lightning v2.0+!\n\n### Installation\n\nYou can install Lightly and its dependencies from PyPI with:\n\n```\npip3 install lightly\n```\n\nWe strongly recommend installing Lightly in a dedicated virtualenv to avoid conflicts with your system packages.\n\n### Lightly in Action\n\nWith Lightly, you can use the latest self-supervised learning methods in a modular\nway using the full power of PyTorch. Experiment with various backbones,\nmodels, and loss functions. The framework has been designed to be easy to use\nfrom the ground up. [Find more examples in our docs](https://docs.lightly.ai/self-supervised-learning/examples/models.html).\n\n```python\nimport torch\nimport torchvision\n\nfrom lightly import loss\nfrom lightly import transforms\nfrom lightly.data import LightlyDataset\nfrom lightly.models.modules import heads\n\n\n# Create a PyTorch module for the SimCLR model.\nclass SimCLR(torch.nn.Module):\n    def __init__(self, backbone):\n        super().__init__()\n        self.backbone = backbone\n        self.projection_head = heads.SimCLRProjectionHead(\n            input_dim=512,  # Resnet18 features have 512 dimensions.\n            hidden_dim=512,\n            output_dim=128,\n        )\n\n    def forward(self, x):\n        features = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(features)\n        return z\n\n\n# Use a resnet backbone from torchvision.\nbackbone = torchvision.models.resnet18()\n# Ignore the classification head as we only want the features.\nbackbone.fc = torch.nn.Identity()\n\n# Build the SimCLR model.\nmodel = SimCLR(backbone)\n\n# Prepare transform that creates multiple random views for every image.\ntransform = transforms.SimCLRTransform(input_size=32, cj_prob=0.5)\n\n\n# Create a dataset from your image folder.\ndataset = LightlyDataset(input_dir=\"./my/cute/cats/dataset/\", transform=transform)\n\n# Build a PyTorch dataloader.\ndataloader = torch.utils.data.DataLoader(\n    dataset,  # Pass the dataset to the dataloader.\n    batch_size=128,  # A large batch size helps with the learning.\n    shuffle=True,  # Shuffling is important!\n)\n\n# Lightly exposes building blocks such as loss functions.\ncriterion = loss.NTXentLoss(temperature=0.5)\n\n# Get a PyTorch optimizer.\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-6)\n\n# Train the model.\nfor epoch in range(10):\n    for (view0, view1), targets, filenames in dataloader:\n        z0 = model(view0)\n        z1 = model(view1)\n        loss = criterion(z0, z1)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        print(f\"loss: {loss.item():.5f}\")\n```\n\nYou can easily use another model like SimSiam by swapping the model and the\nloss function.\n\n```python\n# PyTorch module for the SimSiam model.\nclass SimSiam(torch.nn.Module):\n    def __init__(self, backbone):\n        super().__init__()\n        self.backbone = backbone\n        self.projection_head = heads.SimSiamProjectionHead(512, 512, 128)\n        self.prediction_head = heads.SimSiamPredictionHead(128, 64, 128)\n\n    def forward(self, x):\n        features = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(features)\n        p = self.prediction_head(z)\n        z = z.detach()\n        return z, p\n\n\nmodel = SimSiam(backbone)\n\n# Use the SimSiam loss function.\ncriterion = loss.NegativeCosineSimilarity()\n```\n\nYou can [find a more complete example for SimSiam here.](https://docs.lightly.ai/self-supervised-learning/examples/simsiam.html)\n\nUse PyTorch Lightning to train the model:\n\n```python\nfrom pytorch_lightning import LightningModule, Trainer\n\nclass SimCLR(LightningModule):\n    def __init__(self):\n        super().__init__()\n        resnet = torchvision.models.resnet18()\n        resnet.fc = torch.nn.Identity()\n        self.backbone = resnet\n        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n        self.criterion = loss.NTXentLoss()\n\n    def forward(self, x):\n        features = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(features)\n        return z\n\n    def training_step(self, batch, batch_index):\n        (view0, view1), _, _ = batch\n        z0 = self.forward(view0)\n        z1 = self.forward(view1)\n        loss = self.criterion(z0, z1)\n        return loss\n\n    def configure_optimizers(self):\n        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n        return optim\n\n\nmodel = SimCLR()\ntrainer = Trainer(max_epochs=10, devices=1, accelerator=\"gpu\")\ntrainer.fit(model, dataloader)\n```\n\nSee [our docs for a full PyTorch Lightning example.](https://docs.lightly.ai/self-supervised-learning/examples/simclr.html)\n\nOr train the model on 4 GPUs:\n\n```python\n\n# Use distributed version of loss functions.\ncriterion = loss.NTXentLoss(gather_distributed=True)\n\ntrainer = Trainer(\n    max_epochs=10,\n    devices=4,\n    accelerator=\"gpu\",\n    strategy=\"ddp\",\n    sync_batchnorm=True,\n    use_distributed_sampler=True,  # or replace_sampler_ddp=True for PyTorch Lightning <2.0\n)\ntrainer.fit(model, dataloader)\n```\n\nWe provide multi-GPU training examples with distributed gather and synchronized BatchNorm.\n[Have a look at our docs regarding distributed training.](https://docs.lightly.ai/self-supervised-learning/getting_started/distributed_training.html)\n\n## Benchmarks\n\nImplemented models and their performance on various datasets. Hyperparameters are not\ntuned for maximum accuracy. For detailed results and more information about the benchmarks click\n[here](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html).\n\n### ImageNet1k\n\n[ImageNet1k benchmarks](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenet1k)\n\n**Note**: Evaluation settings are based on these papers:\n\n- Linear: [SimCLR](https://arxiv.org/abs/2002.05709)\n- Finetune: [SimCLR](https://arxiv.org/abs/2002.05709)\n- KNN: [InstDisc](https://arxiv.org/abs/1805.01978)\n\nSee the [benchmarking scripts](./benchmarks/imagenet/resnet50/) for details.\n\n| Model           | Backbone | Batch Size | Epochs | Linear Top1 | Finetune Top1 | kNN Top1 | Tensorboard                                                                                                                                                                    | Checkpoint                                                                                                                                                              |\n| --------------- | -------- | ---------- | ------ | ----------- | ------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| BarlowTwins     | Res50    | 256        | 100    | 62.9        | 72.6          | 45.6     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_barlowtwins_2023-08-18_00-11-03/pretrain/version_0/events.out.tfevents.1692310273.Machine2.569794.0) | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_barlowtwins_2023-08-18_00-11-03/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt) |\n| BYOL            | Res50    | 256        | 100    | 62.5        | 74.5          | 46.0     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_byol_2024-02-14_16-10-09/pretrain/version_0/events.out.tfevents.1707923418.Machine2.3205.0)          | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_byol_2024-02-14_16-10-09/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)        |\n| DINO            | Res50    | 128        | 100    | 68.2        | 72.5          | 49.9     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dino_2023-06-06_13-59-48/pretrain/version_0/events.out.tfevents.1686052799.Machine2.482599.0)        | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dino_2023-06-06_13-59-48/pretrain/version_0/checkpoints/epoch%3D99-step%3D1000900.ckpt)       |\n| MAE             | ViT-B/16 | 256        | 100    | 46.0        | 81.3          | 11.2     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vitb16_mae_2024-02-25_19-57-30/pretrain/version_0/events.out.tfevents.1708887459.Machine2.1092409.0)          | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vitb16_mae_2024-02-25_19-57-30/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)           |\n| MoCoV2          | Res50    | 256        | 100    | 61.5        | 74.3          | 41.8     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_mocov2_2024-02-18_10-29-14/pretrain/version_0/events.out.tfevents.1708248562.Machine2.439033.0)      | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_mocov2_2024-02-18_10-29-14/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)      |\n| SimCLR\\*        | Res50    | 256        | 100    | 63.2        | 73.9          | 44.8     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_simclr_2023-06-22_09-11-13/pretrain/version_0/events.out.tfevents.1687417883.Machine2.33270.0)       | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_simclr_2023-06-22_09-11-13/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)      |\n| SimCLR\\* + DCL  | Res50    | 256        | 100    | 65.1        | 73.5          | 49.6     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dcl_2023-07-04_16-51-40/pretrain/version_0/events.out.tfevents.1688482310.Machine2.247807.0)         | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dcl_2023-07-04_16-51-40/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)         |\n| SimCLR\\* + DCLW | Res50    | 256        | 100    | 64.5        | 73.2          | 48.5     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dclw_2023-07-07_14-57-13/pretrain/version_0/events.out.tfevents.1688734645.Machine2.3176.0)          | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dclw_2023-07-07_14-57-13/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)        |\n| SwAV            | Res50    | 256        | 100    | 67.2        | 75.4          | 49.5     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_swav_2023-05-25_08-29-14/pretrain/version_0/events.out.tfevents.1684996168.Machine2.1445108.0)       | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_swav_2023-05-25_08-29-14/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)        |\n| TiCo            | Res50    | 256        | 100    | 49.7        | 72.7          | 26.6     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_tico_2024-01-07_18-40-57/pretrain/version_0/events.out.tfevents.1704649265.Machine2.1604956.0)       | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_tico_2024-01-07_18-40-57/pretrain/version_0/checkpoints/epoch%3D99-step%3D250200.ckpt)        |\n| VICReg          | Res50    | 256        | 100    | 63.0        | 73.7          | 46.3     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_vicreg_2023-09-11_10-53-08/pretrain/version_0/events.out.tfevents.1694422401.Machine2.556563.0)      | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_vicreg_2023-09-11_10-53-08/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)      |\n\n_\\*We use square root learning rate scaling instead of linear scaling as it yields\nbetter results for smaller batch sizes. See Appendix B.1 in the [SimCLR paper](https://arxiv.org/abs/2002.05709)._\n\n### ImageNet100\n\n[ImageNet100 benchmarks detailed results](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenet100)\n\n### Imagenette\n\n[Imagenette benchmarks detailed results](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenette)\n\n### CIFAR-10\n\n[CIFAR-10 benchmarks detailed results](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#cifar-10)\n\n## Terminology\n\nBelow you can see a schematic overview of the different concepts in the package.\nThe terms in bold are explained in more detail in our [documentation](https://docs.lightly.ai/self-supervised-learning/).\n\n<img src=\"/docs/source/getting_started/images/lightly_overview.png\" alt=\"Overview of the Lightly pip package\"/></a>\n\n### Next Steps\n\nHead to the [documentation](https://docs.lightly.ai/self-supervised-learning/) and see the things you can achieve with Lightly!\n\n## Development\n\nTo install dev dependencies (for example to contribute to the framework) you can use the following command:\n\n```\npip3 install -e \".[dev]\"\n```\n\nFor more information about how to contribute have a look [here](CONTRIBUTING.md).\n\n### Running Tests\n\nUnit tests are within the [tests directory](tests/) and we recommend running them using\n[pytest](https://docs.pytest.org/en/stable/). There are two test configurations\navailable. By default, only a subset will be run:\n\n```\nmake test-fast\n```\n\nTo run all tests (including the slow ones) you can use the following command:\n\n```\nmake test\n```\n\nTo test a specific file or directory use:\n\n```\npytest <path to file or directory>\n```\n\n### Code Formatting\n\nTo format code with [black](https://black.readthedocs.io/en/stable/) and [isort](https://docs.pytest.org) run:\n\n```\nmake format\n```\n\n## Further Reading\n\n**Self-Supervised Learning**:\n\n- Have a look at our [#papers channel on discord](https://discord.com/channels/752876370337726585/815153188487299083)\n  for the newest self-supervised learning papers.\n- [A Cookbook of Self-Supervised Learning, 2023](https://arxiv.org/abs/2304.12210)\n- [Masked Autoencoders Are Scalable Vision Learners, 2021](https://arxiv.org/abs/2111.06377)\n- [Emerging Properties in Self-Supervised Vision Transformers, 2021](https://arxiv.org/abs/2104.14294)\n- [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, 2021](https://arxiv.org/abs/2006.09882)\n- [What Should Not Be Contrastive in Contrastive Learning, 2020](https://arxiv.org/abs/2008.05659)\n- [A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\n- [Momentum Contrast for Unsupervised Visual Representation Learning, 2020](https://arxiv.org/abs/1911.05722)\n\n## FAQ\n\n- Why should I care about self-supervised learning? Aren't pre-trained models from ImageNet much better for transfer learning?\n\n  - Self-supervised learning has become increasingly popular among scientists over the last years because the learned representations perform extraordinarily well on downstream tasks. This means that they capture the important information in an image better than other types of pre-trained models. By training a self-supervised model on _your_ dataset, you can make sure that the representations have all the necessary information about your images.\n\n- How can I contribute?\n\n  - Create an issue if you encounter bugs or have ideas for features we should implement. You can also add your own code by forking this repository and creating a PR. More details about how to contribute with code is in our [contribution guide](CONTRIBUTING.md).\n\n- Is this framework for free?\n\n  - Yes, this framework is completely free to use and we provide the source code. We believe that we need to make training deep learning models more data efficient to achieve widespread adoption. One step to achieve this goal is by leveraging self-supervised learning. The company behind Lightly is committed to keep this framework open-source.\n\n- If this framework is free, how is the company behind Lightly making money?\n  - Training self-supervised models is only one part of our solution.\n    [The company behind Lightly](https://lightly.ai/) focuses on processing and analyzing embeddings created by self-supervised models.\n    By building, what we call a self-supervised active learning loop we help companies understand and work with their data more efficiently.\n    As the [Lightly Solution](https://docs.lightly.ai) is a freemium product, you can try it out for free. However, we will charge for some features.\n  - In any case this framework will always be free to use, even for commercial purposes.\n\n## Lightly in Research\n\n- [Reverse Engineering Self-Supervised Learning, 2023](https://arxiv.org/abs/2305.15614)\n- [Learning Visual Representations via Language-Guided Sampling, 2023](https://arxiv.org/pdf/2302.12248.pdf)\n- [Self-Supervised Learning Methods for Label-Efficient Dental Caries Classification, 2022](https://www.mdpi.com/2075-4418/12/5/1237)\n- [DPCL: Contrastive representation learning with differential privacy, 2022](https://assets.researchsquare.com/files/rs-1516950/v1_covered.pdf?c=1654486158)\n- [Decoupled Contrastive Learning, 2021](https://arxiv.org/abs/2110.06848)\n- [solo-learn: A Library of Self-supervised Methods for Visual Representation Learning, 2021](https://www.jmlr.org/papers/volume23/21-1155/21-1155.pdf)\n\n## Company behind this Open Source Framework\n\n[Lightly](https://www.lightly.ai) is a spin-off from ETH Zurich that helps companies\nbuild efficient active learning pipelines to select the most relevant data for their models.\n\nYou can find out more about the company and it's services by following the links below:\n\n- [Homepage](https://www.lightly.ai)\n- [Web-App](https://app.lightly.ai)\n- [Lightly Solution Documentation (Lightly Worker & API)](https://docs.lightly.ai/)\n\n[Back to top🚀](#top)\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.220703125,
          "content": "coverage:\n  status:\n    project:\n      default:\n        target: auto  # This sets the target to your last upload (useful for incremental changes)\n        threshold: 0.5%  # This allows for a decrease of up to 0.5% in coverage\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "lightly",
          "type": "tree",
          "content": null
        },
        {
          "name": "pylintrc",
          "type": "blob",
          "size": 14.2138671875,
          "content": "# This Pylint rcfile contains a best-effort configuration to uphold the\n# best-practices and style described in the Google Python style guide:\n#   https://google.github.io/styleguide/pyguide.html\n#\n# Its canonical open-source location is:\n#   https://google.github.io/styleguide/pylintrc\n\n[MASTER]\n\n# Files or directories to be skipped. They should be base names, not paths.\nignore=third_party\n\n# Files or directories matching the regex patterns are skipped. The regex\n# matches against base names, not paths.\nignore-patterns=\n\n# Pickle collected data for later comparisons.\npersistent=no\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Use multiple processes to speed up Pylint.\njobs=4\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED\nconfidence=\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).You can also use \"--disable=all\" to\n# disable everything first and then reenable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use\"--disable=all --enable=classes\n# --disable=W\"\ndisable=abstract-method,\n        apply-builtin,\n        arguments-differ,\n        attribute-defined-outside-init,\n        backtick,\n        bad-option-value,\n        basestring-builtin,\n        buffer-builtin,\n        c-extension-no-member,\n        consider-using-enumerate,\n        cmp-builtin,\n        cmp-method,\n        coerce-builtin,\n        coerce-method,\n        delslice-method,\n        div-method,\n        duplicate-code,\n        eq-without-hash,\n        execfile-builtin,\n        file-builtin,\n        filter-builtin-not-iterating,\n        fixme,\n        getslice-method,\n        global-statement,\n        hex-method,\n        idiv-method,\n        implicit-str-concat-in-sequence,\n        import-error,\n        import-self,\n        import-star-module-level,\n        inconsistent-return-statements,\n        input-builtin,\n        intern-builtin,\n        invalid-str-codec,\n        locally-disabled,\n        long-builtin,\n        long-suffix,\n        map-builtin-not-iterating,\n        misplaced-comparison-constant,\n        missing-function-docstring,\n        metaclass-assignment,\n        next-method-called,\n        next-method-defined,\n        no-absolute-import,\n        no-else-break,\n        no-else-continue,\n        no-else-raise,\n        no-else-return,\n        no-init,  # added\n        no-member,\n        no-name-in-module,\n        no-self-use,\n        nonzero-method,\n        oct-method,\n        old-division,\n        old-ne-operator,\n        old-octal-literal,\n        old-raise-syntax,\n        parameter-unpacking,\n        print-statement,\n        raising-string,\n        range-builtin-not-iterating,\n        raw_input-builtin,\n        rdiv-method,\n        reduce-builtin,\n        relative-import,\n        reload-builtin,\n        round-builtin,\n        setslice-method,\n        signature-differs,\n        standarderror-builtin,\n        suppressed-message,\n        sys-max-int,\n        too-few-public-methods,\n        too-many-ancestors,\n        too-many-arguments,\n        too-many-boolean-expressions,\n        too-many-branches,\n        too-many-instance-attributes,\n        too-many-locals,\n        too-many-nested-blocks,\n        too-many-public-methods,\n        too-many-return-statements,\n        too-many-statements,\n        trailing-newlines,\n        unichr-builtin,\n        unicode-builtin,\n        unnecessary-pass,\n        unpacking-in-except,\n        useless-else-on-loop,\n        useless-object-inheritance,\n        useless-suppression,\n        using-cmp-argument,\n        wrong-import-order,\n        xrange-builtin,\n        zip-builtin-not-iterating,\n\n\n[REPORTS]\n\n# Set the output format. Available formats are text, parseable, colorized, msvs\n# (visual studio) and html. You can also give a reporter class, eg\n# mypackage.mymodule.MyReporterClass.\noutput-format=text\n\n# Put messages in a separate file for each module / package specified on the\n# command line instead of printing them on stdout. Reports (if any) will be\n# written in a file name \"pylint_global.[txt|html]\". This option is deprecated\n# and it will be removed in Pylint 2.0.\nfiles-output=no\n\n# Tells whether to display a full report or only the messages\nreports=no\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\n#msg-template=\n\n\n[BASIC]\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=main,_\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Include a hint for the correct naming format with invalid-name\ninclude-naming-hint=no\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\nproperty-classes=abc.abstractproperty,cached_property.cached_property,cached_property.threaded_cached_property,cached_property.cached_property_with_ttl,cached_property.threaded_cached_property_with_ttl\n\n# Regular expression matching correct function names\nfunction-rgx=^(?:(?P<exempt>setUp|tearDown|setUpModule|tearDownModule)|(?P<camel_case>_?[A-Z][a-zA-Z0-9]*)|(?P<snake_case>_?[a-z][a-z0-9_]*))$\n\n# Regular expression matching correct variable names\nvariable-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct constant names\nconst-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct attribute names\nattr-rgx=^_{0,2}[a-z][a-z0-9_]*$\n\n# Regular expression matching correct argument names\nargument-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class attribute names\nclass-attribute-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct inline iteration names\ninlinevar-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class names\nclass-rgx=^_?[A-Z][a-zA-Z0-9]*$\n\n# Regular expression matching correct module names\nmodule-rgx=^(_?[a-z][a-z0-9_]*|__init__)$\n\n# Regular expression matching correct method names\nmethod-rgx=(?x)^(?:(?P<exempt>_[a-z0-9_]+__|runTest|setUp|tearDown|setUpTestCase|tearDownTestCase|setupSelf|tearDownClass|setUpClass|(test|assert)_*[A-Z0-9][a-zA-Z0-9_]*|next)|(?P<camel_case>_{0,2}[A-Z][a-zA-Z0-9_]*)|(?P<snake_case>_{0,2}[a-z][a-z0-9_]*))$\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=(__.*__|main|test.*|.*test|.*Test)$\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=10\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager,contextlib2.contextmanager\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis. It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n\n[FORMAT]\n\n# Maximum number of characters on a single line.\nmax-line-length=88\n\n# TODO(https://github.com/PyCQA/pylint/issues/3352): Direct pylint to exempt\n# lines made too long by directives to pytype.\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=(?x)(\n  ^\\s*(\\#\\ )?<?https?://\\S+>?$|\n  ^\\s*(from\\s+\\S+\\s+)?import\\s+.+$)\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=yes\n\n# List of optional constructs for which whitespace checking is disabled. `dict-\n# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\\n222: 2}.\n# `trailing-comma` allows a space between comma and closing bracket: (a, ).\n# `empty-line` allows space-only lines.\nno-space-check=\n\n# Maximum number of lines in a module\nmax-module-lines=99999\n\n# String used as indentation unit.  The internal Google style guide mandates 2\n# spaces.  Google's externaly-published style guide says 4, consistent with\n# PEP 8.  Here, we use 2 spaces, for conformity with many open-sourced Google\n# projects (like TensorFlow).\nindent-string='    '\n\n# Number of spaces of indent required inside a hanging  or continued line.\nindent-after-paren=4\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=TODO\n\n\n[STRING]\n\n# This flag controls whether inconsistent-quotes generates a warning when the\n# character used as a quote delimiter is used inconsistently within a module.\ncheck-quote-consistency=yes\n\n\n[VARIABLES]\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# A regular expression matching the name of dummy variables (i.e. expectedly\n# not used).\ndummy-variables-rgx=^\\*{0,2}(_$|unused_|dummy_)\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,_cb\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six,six.moves,past.builtins,future.builtins,functools\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format\nlogging-modules=logging,absl.logging,tensorflow.io.logging\n\n\n[SIMILARITIES]\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n\n[SPELLING]\n\n# Spelling dictionary name. Available dictionaries: none. To make it working\n# install python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[IMPORTS]\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=regsub,\n                   TERMIOS,\n                   Bastion,\n                   rexec,\n                   sets\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant, absl\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,\n                  _fields,\n                  _replace,\n                  _source,\n                  _make\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls,\n                            class_\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=StandardError,\n                       Exception,\n                       BaseException\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 9.9052734375,
          "content": "[build-system]\nrequires = [\n    \"setuptools>=21\",\n    \"setuptools-scm\"\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname=\"lightly\"\nrequires-python = \">=3.6\"\nauthors = [\n    {name = \"Lightly Team\", email = \"team@lightly.ai\"},\n]\nlicense = {file = \"LICENSE.txt\"}\ndescription=\"A deep learning package for self-supervised learning\"\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Education\",\n    \"Intended Audience :: Science/Research\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    \"Topic :: Scientific/Engineering :: Image Processing\",\n    \"Topic :: Scientific/Engineering :: Mathematics\",\n    \"Topic :: Software Development\",\n    \"Topic :: Software Development :: Libraries\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.6\",\n    \"Programming Language :: Python :: 3.7\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"License :: OSI Approved :: MIT License\",\n]\ndependencies = [\n  \"certifi>=14.05.14\",\n  \"hydra-core>=1.0.0\",\n  \"lightly_utils~=0.0.0\",\n  \"numpy>=1.18.1\",\n  \"python_dateutil>=2.5.3\",\n  \"requests>=2.23.0\",\n  \"six>=1.10\",\n  \"tqdm>=4.44\",\n  \"torch\",\n  \"torchvision\",\n  \"pydantic>=1.10.5\",\n  \"pytorch_lightning>=1.0.4\",\n  \"urllib3>=1.25.3\",\n  \"aenum>=3.1.11\"\n]\ndynamic = [\"version\", \"readme\"]\n\n[project.optional-dependencies]\nall = [\n  \"lightly[dev,matplotlib,minimal,timm,video]\"\n]\ndev = [\n  \"sphinx\",\n  \"pylint\",\n  \"pytest\",\n  \"pytest-forked\",\n  \"pytest-xdist\",\n  \"pytest-mock\",\n  \"responses\",\n  \"docutils<=0.16\",\n  \"sphinx-copybutton\",\n  \"sphinx-design\",\n  \"sphinx-gallery\",\n  \"sphinx-tabs\",\n  \"sphinx-reredirects\",\n  \"sphinx_rtd_theme\",\n  \"matplotlib\",\n  \"pre-commit\",\n  \"opencv-python\",\n  \"scikit-learn\",\n  \"pandas\",\n  \"toml\",\n  \"torchmetrics\",\n  # black, isort and mypy should be the same version as defined in .pre-commit-config.yaml\n  \"black==23.1.0\", # frozen version to avoid differences between CI and local dev machines\n  \"isort==5.11.5\", # frozen version to avoid differences between CI and local dev machines\n  \"mypy==1.4.1\", # frozen version to avoid differences between CI and local dev machines\n  \"types-python-dateutil\",\n  \"types-toml\",\n  \"nbformat\",\n  \"jupytext\"\n]\n# Minimal dependencies against which we test. Older versions might work depending on the\n# functionality used.\nminimal = [\n  \"torch>=1.10.0\",\n  \"torchvision>=0.11.0\",\n  \"pytorch_lightning>=1.6\",\n]\nopenapi = [\n  \"python_dateutil>=2.5.3\",\n  \"setuptools>=21.0.0\",\n  \"urllib3>=1.25.3\",\n  \"pydantic>=1.10.5\",\n  \"aenum>=3.1.11\"\n]\ntimm = [\"timm>=0.9.9\"]\nvideo = [\"av>=8.0.3\"]\nmatplotlib = [\"matplotlib>=3\"]\n\n\n[project.urls]\n\"Homepage\" = \"https://www.lightly.ai\"\n\"Web-App\" = \"https://app.lightly.ai\"\n\"Documentation\" = \"https://docs.lightly.ai\"\n\"Github\" = \"https://github.com/lightly-ai/lightly\"\n\"Discord\" = \"https://discord.gg/xvNJW94\"\n\n[project.scripts]\nlightly-crop = \"lightly.cli.crop_cli:entry\"\nlightly-download = \"lightly.cli.download_cli:entry\"\nlightly-embed = \"lightly.cli.embed_cli:entry\"\nlightly-magic = \"lightly.cli.lightly_cli:entry\"\nlightly-serve = \"lightly.cli.serve_cli:entry\"\nlightly-train = \"lightly.cli.train_cli:entry\"\nlightly-version = \"lightly.cli.version_cli:entry\"\n\n[tool.setuptools.packages.find]\ninclude = [\"lightly*\"]\n\n[tool.setuptools.dynamic]\nreadme = {file = [\"README.md\"], content-type = \"text/markdown\"}\nversion = {attr = \"lightly.__version__\"}\n\n[tool.setuptools.package-data]\nlightly = [\"lightly/cli/config/*.yaml\"]\n\n[tool.black]\nextend-exclude = \"lightly/openapi_generated/.*\"\n\n[tool.isort]\nprofile = \"black\"\nextend_skip = \"lightly/openapi_generated\"\n\n[tool.coverage.run]\nomit = [\"lightly/openapi_generated/*\"]\n\n[tool.mypy]\nignore_missing_imports = true\nwarn_unused_configs = true\nstrict_equality = true\n\n# Disallow dynamic typing\ndisallow_any_decorated = true\n# TODO(Philipp, 09/23): Remove me!\n# disallow_any_explicit = True\ndisallow_any_generics = true\ndisallow_subclassing_any = true\n\n# Disallow untyped definitions\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\ncheck_untyped_defs = true\ndisallow_untyped_decorators = true\n\n# None and optional handling\nno_implicit_optional = true\nstrict_optional = true\n\n# Configuring warnings\nwarn_unused_ignores = false   # Different ignores are required for different Python versions\nwarn_no_return = true\nwarn_return_any = true\nwarn_redundant_casts = true\nwarn_unreachable = true\n\n# Print format\nshow_error_codes = true\nshow_error_context = true\n\n# Plugins\nplugins = [\"numpy.typing.mypy_plugin\"]\n\n# Excludes\n# TODO(Philipp, 09/23): Remove these one by one (start with 300 files).\nexclude = '''(?x)(\n    lightly/cli/version_cli.py |\n    lightly/cli/crop_cli.py |\n    lightly/cli/serve_cli.py |\n    lightly/cli/embed_cli.py |\n    lightly/cli/lightly_cli.py |\n    lightly/cli/download_cli.py |\n    lightly/cli/config/get_config.py |\n    lightly/cli/train_cli.py |\n    lightly/cli/_cli_simclr.py |\n    lightly/cli/_helpers.py |\n    lightly/data/dataset.py |\n    lightly/data/collate.py |\n    lightly/data/_image_loaders.py |\n    lightly/data/_video.py |\n    lightly/core.py |\n    lightly/api/api_workflow_compute_worker.py |\n    lightly/api/api_workflow_predictions.py |\n    lightly/api/download.py |\n    lightly/api/api_workflow_export.py |\n    lightly/api/api_workflow_download_dataset.py |\n    lightly/api/bitmask.py |\n    lightly/api/_version_checking.py |\n    lightly/api/patch.py |\n    lightly/api/swagger_api_client.py |\n    lightly/api/api_workflow_collaboration.py |\n    lightly/api/utils.py |\n    lightly/api/api_workflow_datasets.py |\n    lightly/api/api_workflow_selection.py |\n    lightly/api/swagger_rest_client.py |\n    lightly/api/api_workflow_datasources.py |\n    lightly/api/api_workflow_upload_embeddings.py |\n    lightly/api/api_workflow_client.py |\n    lightly/api/api_workflow_upload_metadata.py |\n    lightly/api/api_workflow_tags.py |\n    lightly/api/api_workflow_artifacts.py |\n    lightly/utils/cropping/crop_image_by_bounding_boxes.py |\n    lightly/utils/cropping/read_yolo_label_file.py |\n    lightly/utils/debug.py |\n    lightly/utils/benchmarking/benchmark_module.py |\n    lightly/utils/benchmarking/knn_classifier.py |\n    lightly/utils/benchmarking/online_linear_classifier.py |\n    lightly/models/modules/masked_autoencoder.py |\n    lightly/models/modules/ijepa.py |\n    lightly/models/utils.py |\n    tests/cli/test_cli_version.py |\n    tests/cli/test_cli_magic.py |\n    tests/cli/test_cli_crop.py |\n    tests/cli/test_cli_download.py |\n    tests/cli/test_cli_train.py |\n    tests/cli/test_cli_get_lighty_config.py |\n    tests/cli/test_cli_embed.py |\n    tests/UNMOCKED_end2end_tests/delete_datasets_test_unmocked_cli.py |\n    tests/UNMOCKED_end2end_tests/create_custom_metadata_from_input_dir.py |\n    tests/UNMOCKED_end2end_tests/scripts_for_reproducing_problems/test_api_latency.py |\n    tests/core/test_Core.py |\n    tests/data/test_multi_view_collate.py |\n    tests/data/test_data_collate.py |\n    tests/data/test_LightlySubset.py |\n    tests/data/test_LightlyDataset.py |\n    tests/embedding/test_callbacks.py |\n    tests/embedding/test_embedding.py |\n    tests/api/test_serve.py |\n    tests/api/test_swagger_rest_client.py |\n    tests/api/test_rest_parser.py |\n    tests/api/test_utils.py |\n    tests/api/benchmark_video_download.py |\n    tests/api/test_BitMask.py |\n    tests/api/test_patch.py |\n    tests/api/test_download.py |\n    tests/api/test_version_checking.py |\n    tests/api/test_swagger_api_client.py |\n    tests/utils/test_debug.py |\n    tests/utils/benchmarking/test_benchmark_module.py |\n    tests/utils/benchmarking/test_topk.py |\n    tests/utils/benchmarking/test_online_linear_classifier.py |\n    tests/utils/benchmarking/test_knn_classifier.py |\n    tests/utils/benchmarking/test_knn.py |\n    tests/utils/benchmarking/test_linear_classifier.py |\n    tests/utils/benchmarking/test_metric_callback.py |\n    tests/utils/test_dist.py |\n    tests/conftest.py |\n    tests/api_workflow/test_api_workflow_selection.py |\n    tests/api_workflow/test_api_workflow_datasets.py |\n    tests/api_workflow/mocked_api_workflow_client.py |\n    tests/api_workflow/test_api_workflow_compute_worker.py |\n    tests/api_workflow/test_api_workflow_artifacts.py |\n    tests/api_workflow/test_api_workflow_download_dataset.py |\n    tests/api_workflow/utils.py |\n    tests/api_workflow/test_api_workflow_client.py |\n    tests/api_workflow/test_api_workflow_export.py |\n    tests/api_workflow/test_api_workflow_datasources.py |\n    tests/api_workflow/test_api_workflow_tags.py |\n    tests/api_workflow/test_api_workflow_upload_custom_metadata.py |\n    tests/api_workflow/test_api_workflow_upload_embeddings.py |\n    tests/api_workflow/test_api_workflow_collaboration.py |\n    tests/api_workflow/test_api_workflow_predictions.py |\n    tests/api_workflow/test_api_workflow.py |\n    # Let's not type check deprecated active learning:\n    lightly/active_learning |\n    # Let's not type deprecated models:\n    lightly/models/simclr.py |\n    lightly/models/moco.py |\n    lightly/models/barlowtwins.py |\n    lightly/models/nnclr.py |\n    lightly/models/simsiam.py |\n    lightly/models/byol.py |\n    # Let's not type deprecated models tests:\n    tests/models/test_ModelsSimSiam.py |\n    tests/models/test_ModelsSimCLR.py |\n    tests/models/test_ModelsNNCLR.py |\n    tests/models/test_ModelsMoCo.py |\n    tests/models/test_ModelsBYOL.py )'''\n\n# Ignore imports from untyped modules.\n[[tool.mypy.overrides]]\nmodule = [\n    \"lightly.api.*\",\n    \"lightly.cli.*\",\n    \"lightly.data.*\",\n    \"lightly.models.*\",\n    \"lightly.utils.benchmarking.*\",\n    \"tests.api_workflow.*\",\n]\nfollow_imports = \"skip\"\n\n# Ignore errors in auto generated code.\n[[tool.mypy.overrides]]\nmodule = [\n    \"lightly.openapi_generated.*\",\n]\nignore_errors = true\n\n[tool.jupytext]\nnotebook_metadata_filter=\"-all\"\ncell_metadata_filter=\"-all\"\n"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.216796875,
          "content": "from setuptools import setup\n\n# For compatibility with legacy builds or versions of tools\n# that don’t support certain packaging standards (e.g. PEP 517\n# or PEP 660). The configuration is kept in pyproject.toml\nsetup()\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}