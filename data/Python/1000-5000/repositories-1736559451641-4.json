{
  "metadata": {
    "timestamp": 1736559451641,
    "page": 4,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "rafaelpadilla/Object-Detection-Metrics",
      "stars": 4990,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.345703125,
          "content": "\n\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\n.pytest_cache/\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule.*\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n### VisualStudioCode ###\n.vscode/\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n.history\n\n# My stuff\nToDo.txt\ntest.py\nreferences/\naux_images/older_version/\n3rd_party/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0458984375,
          "content": "MIT License\n\nCopyright (c) 2018 Rafael Padilla\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 29.9111328125,
          "content": "\n\n<p align=\"left\">\n    <a href=\"https://zenodo.org/badge/latestdoi/134606465\">\n        <img src=\"https://zenodo.org/badge/134606465.svg\"/></a>\n    <a href=\"https://opensource.org/licenses/MIT\" >\n        <img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" /></a>\n    <a href=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/raw/master/paper_survey_on_performance_metrics_for_object_detection_algorithms.pdf\">\n        <img src=\"https://img.shields.io/badge/paper-published-red\"/></a>\n</p>\n\n\n## Citation\n\nIf you use this code for your research, please consider citing:\n\n``` \n@Article{electronics10030279,\nAUTHOR = {Padilla, Rafael and Passos, Wesley L. and Dias, Thadeu L. B. and Netto, Sergio L. and da Silva, Eduardo A. B.},\nTITLE = {A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit},\nJOURNAL = {Electronics},\nVOLUME = {10},\nYEAR = {2021},\nNUMBER = {3},\nARTICLE-NUMBER = {279},\nURL = {https://www.mdpi.com/2079-9292/10/3/279},\nISSN = {2079-9292},\nDOI = {10.3390/electronics10030279}\n}\n```\nDownload the paper [here](https://www.mdpi.com/2079-9292/10/3/279/pdf) or [here](https://github.com/rafaelpadilla/review_object_detection_metrics/blob/main/published_paper.pdf).\n\n```\n@INPROCEEDINGS {padillaCITE2020,\n    author    = {R. {Padilla} and S. L. {Netto} and E. A. B. {da Silva}},\n    title     = {A Survey on Performance Metrics for Object-Detection Algorithms}, \n    booktitle = {2020 International Conference on Systems, Signals and Image Processing (IWSSIP)}, \n    year      = {2020},\n    pages     = {237-242},}\n```\nDownload the paper [here](https://github.com/rafaelpadilla/Object-Detection-Metrics/raw/master/paper_survey_on_performance_metrics_for_object_detection_algorithms.pdf)\n\n-----------------\n\nAttention! A new version of this tool is available [here](https://github.com/rafaelpadilla/review_object_detection_metrics)\n=======\n\nThe new version includes **all COCO metrics**, supports **other file formats**, provides a **User Interface (UI)** to guide the evaluation process, and presents the **STT-AP metric** to evaluate object detection in videos. \n\n-----------------\n\n# Metrics for object detection\n  \nThe motivation of this project is the lack of consensus used by different works and implementations concerning the **evaluation metrics of the object detection problem**. Although on-line competitions use their own metrics to evaluate the task of object detection, just some of them offer reference code snippets to calculate the accuracy of the detected objects.  \nResearchers who want to evaluate their work using different datasets than those offered by the competitions, need to implement their own version of the metrics. Sometimes a wrong or different implementation can create different and biased results. Ideally, in order to have trustworthy benchmarking among different approaches, it is necessary to have a flexible implementation that can be used by everyone regardless the dataset used.  \n\n**This project provides easy-to-use functions implementing the same metrics used by the the most popular competitions of object detection**. Our implementation does not require modifications of your detection model to complicated input formats, avoiding conversions to XML or JSON files. We simplified the input data (ground truth bounding boxes and detected bounding boxes) and gathered in a single project the main metrics used by the academia and challenges. Our implementation was carefully compared against the official implementations and our results are exactly the same.   \n\nIn the topics below you can find an overview of the most popular metrics used in different competitions and works, as well as samples showing how to use our code.\n\n## Table of contents\n\n- [Motivation](#metrics-for-object-detection)\n- [Different competitions, different metrics](#different-competitions-different-metrics)\n- [Important definitions](#important-definitions)\n- [Metrics](#metrics)\n  - [Precision x Recall curve](#precision-x-recall-curve)\n  - [Average Precision](#average-precision)\n    - [11-point interpolation](#11-point-interpolation)\n    - [Interpolating all  points](#interpolating-all-points)\n- [**How to use this project**](#how-to-use-this-project)\n- [References](#references)\n\n<a name=\"different-competitions-different-metrics\"></a> \n## Different competitions, different metrics \n\n* **[PASCAL VOC Challenge](http://host.robots.ox.ac.uk/pascal/VOC/)** offers a Matlab script in order to evaluate the quality of the detected objects. Participants of the competition can use the provided Matlab script to measure the accuracy of their detections before submitting their results. The official documentation explaining their criteria for object detection metrics can be accessed [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00050000000000000000). The current metrics used by the current PASCAL VOC object detection challenge are the **Precision x Recall curve** and **Average Precision**.  \nThe PASCAL VOC Matlab evaluation code reads the ground truth bounding boxes from XML files, requiring changes in the code if you want to apply it to other datasets or to your specific cases. Even though projects such as [Faster-RCNN](https://github.com/rbgirshick/py-faster-rcnn) implement PASCAL VOC evaluation metrics, it is also necessary to convert the detected bounding boxes into their specific format. [Tensorflow](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/evaluation_protocols.md) framework also has their PASCAL VOC metrics implementation.  \n\n* **[COCO Detection Challenge](https://competitions.codalab.org/competitions/5181)** uses different metrics to evaluate the accuracy of object detection of different algorithms. [Here](http://cocodataset.org/#detection-eval) you can find a documentation explaining the 12 metrics used for characterizing the performance of an object detector on COCO. This competition offers Python and Matlab codes so users can verify their scores before submitting the results. It is also necessary to convert the results to a [format](http://cocodataset.org/#format-results) required by the competition.  \n\n* **[Google Open Images Dataset V4 Competition](https://storage.googleapis.com/openimages/web/challenge.html)** also uses mean Average Precision (mAP) over the 500 classes to evaluate the object detection task. \n\n* **[ImageNet Object Localization Challenge](https://www.kaggle.com/c/imagenet-object-detection-challenge)** defines an error for each image considering the class and the overlapping region between ground truth and detected boxes. The total error is computed as the average of all min errors among all test dataset images. [Here](https://www.kaggle.com/c/imagenet-object-localization-challenge#evaluation) are more details about their evaluation method.  \n\n## Important definitions  \n\n### Intersection Over Union (IOU)\n\n\nIntersection Over Union (IOU) is a measure based on Jaccard Index that evaluates the overlap between two bounding boxes. It requires a ground truth bounding box ![](http://latex.codecogs.com/gif.latex?B_%7Bgt%7D) and a predicted bounding box ![](http://latex.codecogs.com/gif.latex?B_p). By applying the IOU we can tell if a detection is valid (True Positive) or not (False Positive).  \n\nIOU is given by the overlapping area between the predicted bounding box and the ground truth bounding box divided by the area of union between them:  \n\n<p align=\"center\"> \n<img src=\"https://latex.codecogs.com/gif.latex?%5Ctext%7BIOU%7D%3D%5Cfrac%7B%5Ctext%7Barea%7D%5Cleft%28B_%7Bp%7D%20%5Ccap%20B_%7Bgt%7D%20%5Cright%29%7D%7B%5Ctext%7Barea%7D%5Cleft%28B_%7Bp%7D%20%5Ccup%20B_%7Bgt%7D%20%5Cright%29%7D\">\n</p>\n\n<!---\n\\text{IOU}=\\frac{\\text{area}\\left(B_{p} \\cap B_{gt} \\right)}{\\text{area}\\left(B_{p} \\cup B_{gt} \\right)} \n--->\n\nThe image below illustrates the IOU between a ground truth bounding box (in green) and a detected bounding box (in red).\n\n<!--- IOU --->\n<p align=\"center\">\n<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/iou.png\" align=\"center\"/></p>\n\n### True Positive, False Positive, False Negative and True Negative  \n\nSome basic concepts used by the metrics:  \n\n* **True Positive (TP)**: A correct detection. Detection with IOU ≥ _threshold_  \n* **False Positive (FP)**: A wrong detection. Detection with IOU < _threshold_  \n* **False Negative (FN)**: A ground truth not detected  \n* **True Negative (TN)**: Does not apply. It would represent a corrected misdetection. In the object detection task there are many possible bounding boxes that should not be detected within an image. Thus, TN would be all possible bounding boxes that were corrrectly not detected (so many possible boxes within an image). That's why it is not used by the metrics.\n\n_threshold_: depending on the metric, it is usually set to 50%, 75% or 95%.\n\n### Precision\n\nPrecision is the ability of a model to identify **only** the relevant objects. It is the percentage of correct positive predictions and is given by:\n\n<p align=\"center\"> \n<img src=\"https://latex.codecogs.com/gif.latex?%5Ctext%7BPrecision%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTP%7D%7D%7B%5Ctext%7BTP%7D&plus;%5Ctext%7BFP%7D%7D%3D%5Cfrac%7B%5Ctext%7BTP%7D%7D%7B%5Ctext%7Ball%20detections%7D%7D\">\n</p>\n\n<!---\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}=\\frac{\\text{TP}}{\\text{all detections}}\n--->\n\n### Recall \n\nRecall is the ability of a model to find all the relevant cases (all ground truth bounding boxes). It is the percentage of true positive detected among all relevant ground truths and is given by:\n\n<p align=\"center\"> \n<img src=\"https://latex.codecogs.com/gif.latex?%5Ctext%7BRecall%7D%20%3D%20%5Cfrac%7B%5Ctext%7BTP%7D%7D%7B%5Ctext%7BTP%7D&plus;%5Ctext%7BFN%7D%7D%3D%5Cfrac%7B%5Ctext%7BTP%7D%7D%7B%5Ctext%7Ball%20ground%20truths%7D%7D\">\n</p>\n<!--- \n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}=\\frac{\\text{TP}}{\\text{all ground truths}}\n--->\n\n## Metrics\n\nIn the topics below there are some comments on the most popular metrics used for object detection.\n\n### Precision x Recall curve\n\nThe Precision x Recall curve is a good way to evaluate the performance of an object detector as the confidence is changed by plotting a curve for each object class. An object detector of a particular class is considered good if its precision stays high as recall increases, which means that if you vary the confidence threshold, the precision and recall will still be high. Another way to identify a good object detector is to look for a detector that can identify only relevant objects (0 False Positives = high precision), finding all ground truth objects (0 False Negatives = high recall).  \n\nA poor object detector needs to increase the number of detected objects (increasing False Positives = lower precision) in order to retrieve all ground truth objects (high recall). That's why the Precision x Recall curve usually starts with high precision values, decreasing as recall increases. You can see an example of the Prevision x Recall curve in the next topic (Average Precision). This kind of curve is used by the PASCAL VOC 2012 challenge and is available in our implementation.  \n\n### Average Precision\n\nAnother way to compare the performance of object detectors is to calculate the area under the curve (AUC) of the Precision x Recall curve. As AP curves are often zigzag curves going up and down, comparing different curves (different detectors) in the same plot usually is not an easy task - because the curves tend to cross each other much frequently. That's why Average Precision (AP), a numerical metric, can also help us compare different detectors. In practice AP is the precision averaged across all recall values between 0 and 1.  \n\nFrom 2010 on, the method of computing AP by the PASCAL VOC challenge has changed. Currently, **the interpolation performed by PASCAL VOC challenge uses all data points, rather than interpolating only 11 equally spaced points as stated in their [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&rep=rep1&type=pdf)**. As we want to reproduce their default implementation, our default code (as seen further) follows their most recent application (interpolating all data points). However, we also offer the 11-point interpolation approach. \n\n#### 11-point interpolation\n\nThe 11-point interpolation tries to summarize the shape of the Precision x Recall curve by averaging the precision at a set of eleven equally spaced recall levels [0, 0.1, 0.2, ... , 1]:\n\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/gif.latex?%5Ctext%7BAP%7D%3D%5Cfrac%7B1%7D%7B11%7D%20%5Csum_%7Br%5Cin%20%5Cleft%20%5C%7B%200%2C%200.1%2C%20...%2C1%20%5Cright%20%5C%7D%7D%5Crho_%7B%5Ctext%7Binterp%7D%5Cleft%20%28%20r%20%5Cright%20%29%7D\">\n</p>\n<!---\n\\text{AP}=\\frac{1}{11} \\sum_{r\\in \\left \\{ 0, 0.1, ...,1 \\right \\}}\\rho_{\\text{interp}\\left ( r \\right )}\n--->\n\nwith\n\n<p align=\"center\"> \n<img src=\"https://latex.codecogs.com/gif.latex?%5Crho_%7B%5Ctext%7Binterp%7D%7D%20%3D%20%5Cmax_%7B%5Ctilde%7Br%7D%3A%5Ctilde%7Br%7D%20%5Cgeq%20r%7D%20%5Crho%5Cleft%20%28%20%5Ctilde%7Br%7D%20%5Cright%20%29\">\n</p>\n<!--- \n\\rho_{\\text{interp}} = \\max_{\\tilde{r}:\\tilde{r} \\geq r} \\rho\\left ( \\tilde{r} \\right )\n--->\n\nwhere ![](http://latex.codecogs.com/gif.latex?%5Crho%5Cleft%20%28%20%5Ctilde%7Br%7D%20%5Cright%20%29) is the measured precision at recall ![](http://latex.codecogs.com/gif.latex?%5Ctilde%7Br%7D).\n\nInstead of using the precision observed at each point, the AP is obtained by interpolating the precision only at the 11 levels ![](http://latex.codecogs.com/gif.latex?r) taking the **maximum precision whose recall value is greater than ![](http://latex.codecogs.com/gif.latex?r)**.\n\n#### Interpolating all points\n\nInstead of interpolating only in the 11 equally spaced points, you could interpolate through all points <img src=\"https://latex.codecogs.com/gif.latex?n\"> in such way that:\n\n<p align=\"center\"> \n<img src=\"https://latex.codecogs.com/gif.latex?%5Csum_%7Bn%3D0%7D%20%5Cleft%20%28%20r_%7Bn&plus;1%7D%20-%20r_%7Bn%7D%20%5Cright%20%29%20%5Crho_%7B%5Ctext%7Binterp%7D%7D%5Cleft%20%28%20r_%7Bn&plus;1%7D%20%5Cright%20%29\">\n</p>\n<!---\n\\sum_{n=0} \\left ( r_{n+1} - r_{n} \\right ) \\rho_{\\text{interp}}\\left ( r_{n+1} \\right )\n--->\n \nwith\n\n<p align=\"center\"> \n<img src=\"https://latex.codecogs.com/gif.latex?%5Crho_%7B%5Ctext%7Binterp%7D%7D%5Cleft%20%28%20r_%7Bn&plus;1%7D%20%5Cright%20%29%20%3D%20%5Cmax_%7B%5Ctilde%7Br%7D%3A%5Ctilde%7Br%7D%20%5Cge%20r_%7Bn&plus;1%7D%7D%20%5Crho%20%5Cleft%20%28%20%5Ctilde%7Br%7D%20%5Cright%20%29\">\n</p>\n\n<!---\n\\rho_{\\text{interp}}\\left ( r_{n+1} \\right ) = \\max_{\\tilde{r}:\\tilde{r} \\ge r_{n+1}} \\rho \\left ( \\tilde{r} \\right )\n--->\n\n\nwhere ![](http://latex.codecogs.com/gif.latex?%5Crho%5Cleft%20%28%20%5Ctilde%7Br%7D%20%5Cright%20%29) is the measured precision at recall ![](http://latex.codecogs.com/gif.latex?%5Ctilde%7Br%7D).\n\nIn this case, instead of using the precision observed at only few points, the AP is now obtained by interpolating the precision at **each level**, ![](http://latex.codecogs.com/gif.latex?r) taking the **maximum precision whose recall value is greater or equal than ![](http://latex.codecogs.com/gif.latex?r&plus;1)**. This way we calculate the estimated area under the curve.\n\nTo make things more clear, we provided an example comparing both interpolations.\n\n\n#### An ilustrated example \n\nAn example helps us understand better the concept of the interpolated average precision. Consider the detections below:\n  \n<!--- Image samples 1 --->\n<p align=\"center\">\n<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/samples_1_v2.png\" align=\"center\"/></p>\n  \nThere are 7 images with 15 ground truth objects represented by the green bounding boxes and 24 detected objects represented by the red bounding boxes. Each detected object has a confidence level and is identified by a letter (A,B,...,Y).  \n\nThe following table shows the bounding boxes with their corresponding confidences. The last column identifies the detections as TP or FP. In this example a TP is considered if IOU ![](http://latex.codecogs.com/gif.latex?%5Cgeq) 30%, otherwise it is a FP. By looking at the images above we can roughly tell if the detections are TP or FP.\n\n<!--- Table 1 --->\n<p align=\"center\">\n<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/table_1_v2.png\" align=\"center\"/></p>\n\n<!---\n| Images | Detections | Confidences | TP or FP |\n|:------:|:----------:|:-----------:|:--------:|\n| Image 1 | A | 88% | FP |\n| Image 1 | B | 70% | TP |\n| Image 1 |\tC\t| 80% | FP |\n| Image 2 |\tD\t| 71% | FP |\n| Image 2 |\tE\t| 54% | TP |\n| Image 2 |\tF\t| 74% | FP |\n| Image 3 |\tG\t| 18% | TP |\n| Image 3 |\tH\t| 67% | FP |\n| Image 3 |\tI\t| 38% | FP |\n| Image 3 |\tJ\t| 91% | TP |\n| Image 3 |\tK\t| 44% | FP |\n| Image 4 |\tL\t| 35% | FP |\n| Image 4 |\tM\t| 78% | FP |\n| Image 4 |\tN\t| 45% | FP |\n| Image 4 |\tO\t| 14% | FP |\n| Image 5 |\tP\t| 62% | TP |\n| Image 5 |\tQ\t| 44% | FP |\n| Image 5 |\tR\t| 95% | TP |\n| Image 5 |\tS\t| 23% | FP |\n| Image 6 |\tT\t| 45% | FP |\n| Image 6 |\tU\t| 84% | FP |\n| Image 6 |\tV\t| 43% | FP |\n| Image 7 |\tX\t| 48% | TP |\n| Image 7 |\tY\t| 95% | FP |\n--->\n\nIn some images there are more than one detection overlapping a ground truth (Images 2, 3, 4, 5, 6 and 7). For those cases, the predicted box with the highest IOU is considered TP (e.g. in image 1 \"E\" is TP while \"D\" is FP because IOU between E and the groundtruth is greater than the IOU between D and the groundtruth). This rule is applied by the PASCAL VOC 2012 metric: \"e.g. 5 detections (TP) of a single object is counted as 1 correct detection and 4 false detections”.\n\nThe Precision x Recall curve is plotted by calculating the precision and recall values of the accumulated TP or FP detections.  For this, first we need to order the detections by their confidences, then we calculate the precision and recall for each accumulated detection as shown in the table below (Note that for recall computation, the denominator term (\"Acc TP + Acc FN\" or \"All ground truths\") is constant at 15 since GT boxes are constant irrespective of detections).: \n\n<!--- Table 2 --->\n<p align=\"center\">\n<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/table_2_v2.png\" align=\"center\"/></p>\n\n<!---\n| Images | Detections | Confidences |  TP | FP | Acc TP | Acc FP | Precision | Recall |\n|:------:|:----------:|:-----------:|:---:|:--:|:------:|:------:|:---------:|:------:|\n| Image 5 |\tR\t| 95% | 1 | 0 | 1 | 0 | 1       | 0.0666 |\n| Image 7 |\tY\t| 95% | 0 | 1 | 1 | 1 | 0.5     | 0.6666 |\n| Image 3 |\tJ\t| 91% | 1 | 0 | 2 | 1 | 0.6666  | 0.1333 |\n| Image 1 | A | 88% | 0 | 1 | 2 | 2 | 0.5     | 0.1333 |\n| Image 6 |\tU\t| 84% | 0 | 1 | 2 | 3 | 0.4     | 0.1333 |\n| Image 1 |\tC\t| 80% | 0 | 1 | 2 | 4 | 0.3333  | 0.1333 |\n| Image 4 |\tM\t| 78% | 0 | 1 | 2 | 5 | 0.2857  | 0.1333 |\n| Image 2 |\tF\t| 74% | 0 | 1 | 2 | 6 | 0.25    | 0.1333 |\n| Image 2 |\tD\t| 71% | 0 | 1 | 2 | 7 | 0.2222  | 0.1333 |\n| Image 1 | B | 70% | 1 | 0 | 3 | 7 | 0.3     | 0.2    |\n| Image 3 |\tH\t| 67% | 0 | 1 | 3 | 8 | 0.2727  | 0.2    |\n| Image 5 |\tP\t| 62% | 1 | 0 | 4 | 8 | 0.3333  | 0.2666 |\n| Image 2 |\tE\t| 54% | 1 | 0 | 5 | 8 | 0.3846  | 0.3333 |\n| Image 7 |\tX\t| 48% | 1 | 0 | 6 | 8 | 0.4285  | 0.4    |\n| Image 4 |\tN\t| 45% | 0 | 1 | 6 | 9 | 0.7     | 0.4    |\n| Image 6 |\tT\t| 45% | 0 | 1 | 6 | 10 | 0.375  | 0.4    |\n| Image 3 |\tK\t| 44% | 0 | 1 | 6 | 11 | 0.3529 | 0.4    |\n| Image 5 |\tQ\t| 44% | 0 | 1 | 6 | 12 | 0.3333 | 0.4    |\n| Image 6 |\tV\t| 43% | 0 | 1 | 6 | 13 | 0.3157 | 0.4    |\n| Image 3 |\tI\t| 38% | 0 | 1 | 6 | 14 | 0.3    | 0.4    |\n| Image 4 |\tL\t| 35% | 0 | 1 | 6 | 15 | 0.2857 | 0.4    |\n| Image 5 |\tS\t| 23% | 0 | 1 | 6 | 16 | 0.2727 | 0.4    |\n| Image 3 |\tG\t| 18% | 1 | 0 | 7 | 16 | 0.3043 | 0.4666 |\n| Image 4 |\tO\t| 14% | 0 | 1 | 7 | 17 | 0.2916 | 0.4666 |\n--->\n \n Example computation for the 2nd row (Image 7):  Precision = TP/(TP+FP) = 1/2 = 0.5 and Recall = TP/(TP+FN) = 1/15 = 0.066\n \n Plotting the precision and recall values we have the following *Precision x Recall curve*:\n \n <!--- Precision x Recall graph --->\n<p align=\"center\">\n<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/precision_recall_example_1_v2.png\" align=\"center\"/>\n</p>\n \nAs mentioned before, there are two different ways to measure the interpolted average precision: **11-point interpolation** and **interpolating all points**. Below we make a comparisson between them:\n\n#### Calculating the 11-point interpolation\n\nThe idea of the 11-point interpolated average precision is to average the precisions at a set of 11 recall levels (0,0.1,...,1). The interpolated precision values are obtained by taking the maximum precision whose recall value is greater than its current recall value as follows: \n\n<!--- interpolated precision curve --->\n<p align=\"center\">\n<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/11-pointInterpolation.png\" align=\"center\"/>\n</p>\n\nBy applying the 11-point interpolation, we have:  \n\n![](http://latex.codecogs.com/gif.latex?AP%20%3D%20%5Cfrac%7B1%7D%7B11%7D%5Csum_%7Br%5Cin%5C%7B0%2C0.1%2C...%2C1%5C%7D%7D%5Crho_%7B%5Ctext%7Binterp%7D%5Cleft%20%28r%5Cright%20%29%7D)  \n![](http://latex.codecogs.com/gif.latex?AP%20%3D%20%5Cfrac%7B1%7D%7B11%7D%20%5Cleft%20%28%201&plus;0.6666&plus;0.4285&plus;0.4285&plus;0.4285&plus;0&plus;0&plus;0&plus;0&plus;0&plus;0%20%5Cright%20%29)  \n![](http://latex.codecogs.com/gif.latex?AP%20%3D%2026.84%5C%25)\n\n\n#### Calculating the interpolation performed in all points\n\nBy interpolating all points, the Average Precision (AP) can be interpreted as an approximated AUC of the Precision x Recall curve. The intention is to reduce the impact of the wiggles in the curve. By applying the equations presented before, we can obtain the areas as it will be demostrated here. We could also visually have the interpolated precision points by looking at the recalls starting from the highest (0.4666) to 0 (looking at the plot from right to left) and, as we decrease the recall, we collect the precision values that are the highest as shown in the image below:\n    \n<!--- interpolated precision AUC --->\n<p align=\"center\">\n<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/interpolated_precision_v2.png\" align=\"center\"/>\n</p>\n  \nLooking at the plot above, we can divide the AUC into 4 areas (A1, A2, A3 and A4):\n  \n<!--- interpolated precision AUC --->\n<p align=\"center\">\n<img src=\"https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/aux_images/interpolated_precision-AUC_v2.png\" align=\"center\"/>\n</p>\n\nCalculating the total area, we have the AP:  \n\n![](http://latex.codecogs.com/gif.latex?AP%20%3D%20A1%20&plus;%20A2%20&plus;%20A3%20&plus;%20A4)  \n  \n![](http://latex.codecogs.com/gif.latex?%5Ctext%7Bwith%3A%7D)  \n![](http://latex.codecogs.com/gif.latex?A1%20%3D%20%280.0666-0%29%5Ctimes1%20%3D%5Cmathbf%7B0.0666%7D)  \n![](http://latex.codecogs.com/gif.latex?A2%20%3D%20%280.1333-0.0666%29%5Ctimes0.6666%3D%5Cmathbf%7B0.04446222%7D)  \n![](http://latex.codecogs.com/gif.latex?A3%20%3D%20%280.4-0.1333%29%5Ctimes0.4285%20%3D%5Cmathbf%7B0.11428095%7D)  \n![](http://latex.codecogs.com/gif.latex?A4%20%3D%20%280.4666-0.4%29%5Ctimes0.3043%20%3D%5Cmathbf%7B0.02026638%7D)  \n   \n![](http://latex.codecogs.com/gif.latex?AP%20%3D%200.0666&plus;0.04446222&plus;0.11428095&plus;0.02026638)  \n![](http://latex.codecogs.com/gif.latex?AP%20%3D%200.24560955)  \n![](http://latex.codecogs.com/gif.latex?AP%20%3D%20%5Cmathbf%7B24.56%5C%25%7D)  \n\nThe results between the two different interpolation methods are a little different: 24.56% and 26.84% by the every point interpolation and the 11-point interpolation respectively.  \n\nOur default implementation is the same as VOC PASCAL: every point interpolation. If you want to use the 11-point interpolation, change the functions that use the argument ```method=MethodAveragePrecision.EveryPointInterpolation``` to ```method=MethodAveragePrecision.ElevenPointInterpolation```.   \n\nIf you want to reproduce these results, see the **[Sample 2](https://github.com/rafaelpadilla/Object-Detection-Metrics/tree/master/samples/sample_2/)**.\n<!--In order to evaluate your detections, you just need a simple list of `Detection` objects. A `Detection` object is a very simple class containing the class id, class probability and bounding boxes coordinates of the detected objects. This same structure is used for the groundtruth detections.-->\n\n## How to use this project\n\nThis project was created to evaluate your detections in a very easy way. If you want to evaluate your algorithm with the most used object detection metrics, you are in the right place.  \n\n[Sample_1](https://github.com/rafaelpadilla/Object-Detection-Metrics/tree/master/samples/sample_1) and [sample_2](https://github.com/rafaelpadilla/Object-Detection-Metrics/tree/master/samples/sample_2) are practical examples demonstrating how to access directly the core functions of this project, providing more flexibility on the usage of the metrics. But if you don't want to spend your time understanding our code, see the instructions below to easily evaluate your detections:  \n\nFollow the steps below to start evaluating your detections:\n\n1. [Create the ground truth files](#create-the-ground-truth-files)\n2. [Create your detection files](#create-your-detection-files)\n3. For **Pascal VOC metrics**, run the command: `python pascalvoc.py`  \n   If you want to reproduce the example above, run the command: `python pascalvoc.py -t 0.3`\n4. (Optional) [You can use arguments to control the IOU threshold, bounding boxes format, etc.](#optional-arguments)\n\n### Create the ground truth files\n\n- Create a separate ground truth text file for each image in the folder **groundtruths/**.\n- In these files each line should be in the format: `<class_name> <left> <top> <right> <bottom>`.    \n- E.g. The ground truth bounding boxes of the image \"2008_000034.jpg\" are represented in the file \"2008_000034.txt\":\n  ```\n  bottle 6 234 45 362\n  person 1 156 103 336\n  person 36 111 198 416\n  person 91 42 338 500\n  ```\n    \nIf you prefer, you can also have your bounding boxes in the format: `<class_name> <left> <top> <width> <height>` (see here [**\\***](#asterisk) how to use it). In this case, your \"2008_000034.txt\" would be represented as:\n  ```\n  bottle 6 234 39 128\n  person 1 156 102 180\n  person 36 111 162 305\n  person 91 42 247 458\n  ```\n\n### Create your detection files\n\n- Create a separate detection text file for each image in the folder **detections/**.\n- The names of the detection files must match their correspond ground truth (e.g. \"detections/2008_000182.txt\" represents the detections of the ground truth: \"groundtruths/2008_000182.txt\").\n- In these files each line should be in the following format: `<class_name> <confidence> <left> <top> <right> <bottom>` (see here [**\\***](#asterisk) how to use it).\n- E.g. \"2008_000034.txt\":\n    ```\n    bottle 0.14981 80 1 295 500  \n    bus 0.12601 36 13 404 316  \n    horse 0.12526 430 117 500 307  \n    pottedplant 0.14585 212 78 292 118  \n    tvmonitor 0.070565 388 89 500 196  \n    ```\n\nAlso if you prefer, you could have your bounding boxes in the format: `<class_name> <confidence> <left> <top> <width> <height>`.\n\n### Optional arguments\n\nOptional arguments:\n\n| Argument &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Description | Example | Default |\n|:-------------:|:-----------:|:-----------:|:-----------:|\n| `-h`,<br>`--help ` |\tshow help message | `python pascalvoc.py -h` | |  \n|  `-v`,<br>`--version` | check version | `python pascalvoc.py -v` | |  \n| `-gt`,<br>`--gtfolder` | folder that contains the ground truth bounding boxes files | `python pascalvoc.py -gt /home/whatever/my_groundtruths/` | `/Object-Detection-Metrics/groundtruths`|  \n| `-det`,<br>`--detfolder` | folder that contains your detected bounding boxes files | `python pascalvoc.py -det /home/whatever/my_detections/` | `/Object-Detection-Metrics/detections/`|  \n| `-t`,<br>`--threshold` | IOU thershold that tells if a detection is TP or FP | `python pascalvoc.py -t 0.75` | `0.50` |  \n| `-gtformat` | format of the coordinates of the ground truth bounding boxes [**\\***](#asterisk) | `python pascalvoc.py -gtformat xyrb` | `xywh` |\n| `-detformat` | format of the coordinates of the detected bounding boxes [**\\***](#asterisk) | `python pascalvoc.py -detformat xyrb` | `xywh` | |  \n| `-gtcoords` | reference of the ground truth bounding bounding box coordinates.<br>If the annotated coordinates are relative to the image size (as used in YOLO), set it to `rel`.<br>If the coordinates are absolute values, not depending to the image size, set it to `abs` |  `python pascalvoc.py -gtcoords rel` | `abs` |  \n| `-detcoords` | reference of the detected bounding bounding box coordinates.<br>If the coordinates are relative to the image size (as used in YOLO), set it to `rel`.<br>If the coordinates are absolute values, not depending to the image size, set it to `abs` | `python pascalvoc.py -detcoords rel` | `abs` |  \n| `-imgsize ` | image size in the format `width,height` <int,int>.<br>Required if `-gtcoords` or `-detcoords` is set to `rel` | `python pascalvoc.py -imgsize 600,400` |  \n| `-sp`,<br>`--savepath` | folder where the plots are saved | `python pascalvoc.py -sp /home/whatever/my_results/` | `Object-Detection-Metrics/results/` |  \n| `-np`,<br>`--noplot` | if present no plot is shown during execution | `python pascalvoc.py -np` | not presented.<br>Therefore, plots are shown |  \n\n<a name=\"asterisk\"> </a>\n(**\\***) set `-gtformat xywh` and/or `-detformat xywh` if format is `<left> <top> <width> <height>`. Set to `-gtformat xyrb` and/or `-detformat xyrb`  if format is `<left> <top> <right> <bottom>`.\n  \n## References\n\n* The Relationship Between Precision-Recall and ROC Curves (Jesse Davis and Mark Goadrich)\nDepartment of Computer Sciences and Department of Biostatistics and Medical Informatics, University of\nWisconsin  \nhttp://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf\n\n* The PASCAL Visual Object Classes (VOC) Challenge  \nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&rep=rep1&type=pdf\n\n* Evaluation of ranked retrieval results (Salton and Mcgill 1986)  \nhttps://www.amazon.com/Introduction-Information-Retrieval-COMPUTER-SCIENCE/dp/0070544840  \nhttps://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html\n"
        },
        {
          "name": "_init_paths.py",
          "type": "blob",
          "size": 1.0546875,
          "content": "###########################################################################################\n#                                                                                         #\n# Set up paths for the Object Detection Metrics                                           #\n#                                                                                         #\n# Developed by: Rafael Padilla (rafael.padilla@smt.ufrj.br)                               #\n#        SMT - Signal Multimedia and Telecommunications Lab                               #\n#        COPPE - Universidade Federal do Rio de Janeiro                                   #\n#        Last modification: May 24th 2018                                                 #\n###########################################################################################\n\nimport sys\nimport os\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\n\ncurrentPath = os.path.dirname(os.path.realpath(__file__))\n\n# Add lib to PYTHONPATH\nlibPath = os.path.join(currentPath, 'lib')\nadd_path(libPath)\n"
        },
        {
          "name": "aux_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "detections",
          "type": "tree",
          "content": null
        },
        {
          "name": "detections_rel",
          "type": "tree",
          "content": null
        },
        {
          "name": "groundtruths",
          "type": "tree",
          "content": null
        },
        {
          "name": "groundtruths_rel",
          "type": "tree",
          "content": null
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "message.txt",
          "type": "blob",
          "size": 2.267578125,
          "content": "####################################################################################################\n#                                                                                                  #\n# THE CURRENT VERSION WAS UPDATED WITH A VISUAL INTERFACE, INCLUDING MORE METRICS AND SUPPORTING   #\n# OTHER FILE FORMATS.                                                                              #\n#                                                                                                  #\n# PLEASE ACCESS IT ACCESSED AT:                                                                    #\n# https://github.com/rafaelpadilla/review_object_detection_metrics                                 #\n#                                                                                                  #\n# @Article{electronics10030279,                                                                    #\n#     author         = {Padilla, Rafael and Passos, Wesley L. and Dias, Thadeu L. B. and Netto,    #\n#                       Sergio L. and da Silva, Eduardo A. B.},                                    #\n#     title          = {A Comparative Analysis of Object Detection Metrics with a Companion        #\n#                       Open-Source Toolkit},                                                      #\n#     journal        = {Electronics},                                                              #\n#     volume         = {10},                                                                       #\n#     year           = {2021},                                                                     #\n#     number         = {3},                                                                        #\n#     article-number = {279},                                                                      #\n#     url            = {https://www.mdpi.com/2079-9292/10/3/279},                                  #\n#     issn           = {2079-9292},                                                                #\n#     doi            = {10.3390/electronics10030279}, }                                            #\n#                                                                                                  #\n####################################################################################################"
        },
        {
          "name": "paper_survey_on_performance_metrics_for_object_detection_algorithms.pdf",
          "type": "blob",
          "size": 8129.87890625,
          "content": ""
        },
        {
          "name": "pascalvoc.py",
          "type": "blob",
          "size": 18.486328125,
          "content": "###########################################################################################\n#                                                                                         #\n# This sample shows how to evaluate object detections applying the following metrics:     #\n#  * Precision x Recall curve       ---->       used by VOC PASCAL 2012)                  #\n#  * Average Precision (AP)         ---->       used by VOC PASCAL 2012)                  #\n#                                                                                         #\n# Developed by: Rafael Padilla (rafael.padilla@smt.ufrj.br)                               #\n#        SMT - Signal Multimedia and Telecommunications Lab                               #\n#        COPPE - Universidade Federal do Rio de Janeiro                                   #\n#        Last modification: Feb 12th 2021                                                 #\n###########################################################################################\n\n####################################################################################################\n#                                                                                                  #\n# THE CURRENT VERSION WAS UPDATED WITH A VISUAL INTERFACE, INCLUDING MORE METRICS AND SUPPORTING   #\n# OTHER FILE FORMATS. PLEASE ACCESS IT ACCESSED AT:                                                #\n#                                                                                                  #\n# https://github.com/rafaelpadilla/review_object_detection_metrics                                 #\n#                                                                                                  #\n# @Article{electronics10030279,                                                                    #\n#     author         = {Padilla, Rafael and Passos, Wesley L. and Dias, Thadeu L. B. and Netto,    #\n#                       Sergio L. and da Silva, Eduardo A. B.},                                    #\n#     title          = {A Comparative Analysis of Object Detection Metrics with a Companion        #\n#                       Open-Source Toolkit},                                                      #\n#     journal        = {Electronics},                                                              #\n#     volume         = {10},                                                                       #\n#     year           = {2021},                                                                     #\n#     number         = {3},                                                                        #\n#     article-number = {279},                                                                      #\n#     url            = {https://www.mdpi.com/2079-9292/10/3/279},                                  #\n#     issn           = {2079-9292},                                                                #\n#     doi            = {10.3390/electronics10030279}, }                                            #\n####################################################################################################\n\n####################################################################################################\n# If you use this project, please consider citing:                                                 #\n#                                                                                                  #\n# @INPROCEEDINGS {padillaCITE2020,                                                                 #\n#    author    = {R. {Padilla} and S. L. {Netto} and E. A. B. {da Silva}},                         #\n#    title     = {A Survey on Performance Metrics for Object-Detection Algorithms},                #\n#    booktitle = {2020 International Conference on Systems, Signals and Image Processing (IWSSIP)},#\n#    year      = {2020},                                                                           #\n#    pages     = {237-242},}                                                                       #\n#                                                                                                  #\n# This work is published at: https://github.com/rafaelpadilla/Object-Detection-Metrics             #\n####################################################################################################\n\nimport argparse\nimport glob\nimport os\nimport shutil\nimport sys\n\nimport _init_paths\nfrom BoundingBox import BoundingBox\nfrom BoundingBoxes import BoundingBoxes\nfrom Evaluator import *\nfrom utils import BBFormat\n\n\n# Validate formats\ndef ValidateFormats(argFormat, argName, errors):\n    if argFormat == 'xywh':\n        return BBFormat.XYWH\n    elif argFormat == 'xyrb':\n        return BBFormat.XYX2Y2\n    elif argFormat is None:\n        return BBFormat.XYWH  # default when nothing is passed\n    else:\n        errors.append('argument %s: invalid value. It must be either \\'xywh\\' or \\'xyrb\\'' %\n                      argName)\n\n\n# Validate mandatory args\ndef ValidateMandatoryArgs(arg, argName, errors):\n    if arg is None:\n        errors.append('argument %s: required argument' % argName)\n    else:\n        return True\n\n\ndef ValidateImageSize(arg, argName, argInformed, errors):\n    errorMsg = 'argument %s: required argument if %s is relative' % (argName, argInformed)\n    ret = None\n    if arg is None:\n        errors.append(errorMsg)\n    else:\n        arg = arg.replace('(', '').replace(')', '')\n        args = arg.split(',')\n        if len(args) != 2:\n            errors.append('%s. It must be in the format \\'width,height\\' (e.g. \\'600,400\\')' %\n                          errorMsg)\n        else:\n            if not args[0].isdigit() or not args[1].isdigit():\n                errors.append(\n                    '%s. It must be in INdiaTEGER the format \\'width,height\\' (e.g. \\'600,400\\')' %\n                    errorMsg)\n            else:\n                ret = (int(args[0]), int(args[1]))\n    return ret\n\n\n# Validate coordinate types\ndef ValidateCoordinatesTypes(arg, argName, errors):\n    if arg == 'abs':\n        return CoordinatesType.Absolute\n    elif arg == 'rel':\n        return CoordinatesType.Relative\n    elif arg is None:\n        return CoordinatesType.Absolute  # default when nothing is passed\n    errors.append('argument %s: invalid value. It must be either \\'rel\\' or \\'abs\\'' % argName)\n\n\ndef ValidatePaths(arg, nameArg, errors):\n    if arg is None:\n        errors.append('argument %s: invalid directory' % nameArg)\n    elif os.path.isdir(arg) is False and os.path.isdir(os.path.join(currentPath, arg)) is False:\n        errors.append('argument %s: directory does not exist \\'%s\\'' % (nameArg, arg))\n    # elif os.path.isdir(os.path.join(currentPath, arg)) is True:\n    #     arg = os.path.join(currentPath, arg)\n    else:\n        arg = os.path.join(currentPath, arg)\n    return arg\n\n\ndef getBoundingBoxes(directory,\n                     isGT,\n                     bbFormat,\n                     coordType,\n                     allBoundingBoxes=None,\n                     allClasses=None,\n                     imgSize=(0, 0)):\n    \"\"\"Read txt files containing bounding boxes (ground truth and detections).\"\"\"\n    if allBoundingBoxes is None:\n        allBoundingBoxes = BoundingBoxes()\n    if allClasses is None:\n        allClasses = []\n    # Read ground truths\n    os.chdir(directory)\n    files = glob.glob(\"*.txt\")\n    files.sort()\n    # Read GT detections from txt file\n    # Each line of the files in the groundtruths folder represents a ground truth bounding box\n    # (bounding boxes that a detector should detect)\n    # Each value of each line is  \"class_id, x, y, width, height\" respectively\n    # Class_id represents the class of the bounding box\n    # x, y represents the most top-left coordinates of the bounding box\n    # x2, y2 represents the most bottom-right coordinates of the bounding box\n    for f in files:\n        nameOfImage = f.replace(\".txt\", \"\")\n        fh1 = open(f, \"r\")\n        for line in fh1:\n            line = line.replace(\"\\n\", \"\")\n            if line.replace(' ', '') == '':\n                continue\n            splitLine = line.split(\" \")\n            if isGT:\n                # idClass = int(splitLine[0]) #class\n                idClass = (splitLine[0])  # class\n                x = float(splitLine[1])\n                y = float(splitLine[2])\n                w = float(splitLine[3])\n                h = float(splitLine[4])\n                bb = BoundingBox(nameOfImage,\n                                 idClass,\n                                 x,\n                                 y,\n                                 w,\n                                 h,\n                                 coordType,\n                                 imgSize,\n                                 BBType.GroundTruth,\n                                 format=bbFormat)\n            else:\n                # idClass = int(splitLine[0]) #class\n                idClass = (splitLine[0])  # class\n                confidence = float(splitLine[1])\n                x = float(splitLine[2])\n                y = float(splitLine[3])\n                w = float(splitLine[4])\n                h = float(splitLine[5])\n                bb = BoundingBox(nameOfImage,\n                                 idClass,\n                                 x,\n                                 y,\n                                 w,\n                                 h,\n                                 coordType,\n                                 imgSize,\n                                 BBType.Detected,\n                                 confidence,\n                                 format=bbFormat)\n            allBoundingBoxes.addBoundingBox(bb)\n            if idClass not in allClasses:\n                allClasses.append(idClass)\n        fh1.close()\n    return allBoundingBoxes, allClasses\n\n\n# Get current path to set default folders\ncurrentPath = os.path.dirname(os.path.abspath(__file__))\n\nVERSION = '0.2 (beta)'\n\nwith open('message.txt', 'r') as f:\n    message = f'\\n\\n{f.read()}\\n\\n'\n\nprint(message)\n\nparser = argparse.ArgumentParser(\n    prog='Object Detection Metrics - Pascal VOC',\n    description=\n    f'{message}\\nThis project applies the most popular metrics used to evaluate object detection '\n    'algorithms.\\nThe current implemention runs the Pascal VOC metrics.\\nFor further references, '\n    'please check:\\nhttps://github.com/rafaelpadilla/Object-Detection-Metrics',\n    epilog=\"Developed by: Rafael Padilla (rafael.padilla@smt.ufrj.br)\")\nparser.add_argument('-v', '--version', action='version', version='%(prog)s ' + VERSION)\n# Positional arguments\n# Mandatory\nparser.add_argument('-gt',\n                    '--gtfolder',\n                    dest='gtFolder',\n                    default=os.path.join(currentPath, 'groundtruths'),\n                    metavar='',\n                    help='folder containing your ground truth bounding boxes')\nparser.add_argument('-det',\n                    '--detfolder',\n                    dest='detFolder',\n                    default=os.path.join(currentPath, 'detections'),\n                    metavar='',\n                    help='folder containing your detected bounding boxes')\n# Optional\nparser.add_argument('-t',\n                    '--threshold',\n                    dest='iouThreshold',\n                    type=float,\n                    default=0.5,\n                    metavar='',\n                    help='IOU threshold. Default 0.5')\nparser.add_argument('-gtformat',\n                    dest='gtFormat',\n                    metavar='',\n                    default='xywh',\n                    help='format of the coordinates of the ground truth bounding boxes: '\n                    '(\\'xywh\\': <left> <top> <width> <height>)'\n                    ' or (\\'xyrb\\': <left> <top> <right> <bottom>)')\nparser.add_argument('-detformat',\n                    dest='detFormat',\n                    metavar='',\n                    default='xywh',\n                    help='format of the coordinates of the detected bounding boxes '\n                    '(\\'xywh\\': <left> <top> <width> <height>) '\n                    'or (\\'xyrb\\': <left> <top> <right> <bottom>)')\nparser.add_argument('-gtcoords',\n                    dest='gtCoordinates',\n                    default='abs',\n                    metavar='',\n                    help='reference of the ground truth bounding box coordinates: absolute '\n                    'values (\\'abs\\') or relative to its image size (\\'rel\\')')\nparser.add_argument('-detcoords',\n                    default='abs',\n                    dest='detCoordinates',\n                    metavar='',\n                    help='reference of the ground truth bounding box coordinates: '\n                    'absolute values (\\'abs\\') or relative to its image size (\\'rel\\')')\nparser.add_argument('-imgsize',\n                    dest='imgSize',\n                    metavar='',\n                    help='image size. Required if -gtcoords or -detcoords are \\'rel\\'')\nparser.add_argument('-sp',\n                    '--savepath',\n                    dest='savePath',\n                    metavar='',\n                    help='folder where the plots are saved')\nparser.add_argument('-np',\n                    '--noplot',\n                    dest='showPlot',\n                    action='store_false',\n                    help='no plot is shown during execution')\nargs = parser.parse_args()\n\niouThreshold = args.iouThreshold\n\n# Arguments validation\nerrors = []\n# Validate formats\ngtFormat = ValidateFormats(args.gtFormat, '-gtformat', errors)\ndetFormat = ValidateFormats(args.detFormat, '-detformat', errors)\n# Groundtruth folder\nif ValidateMandatoryArgs(args.gtFolder, '-gt/--gtfolder', errors):\n    gtFolder = ValidatePaths(args.gtFolder, '-gt/--gtfolder', errors)\nelse:\n    # errors.pop()\n    gtFolder = os.path.join(currentPath, 'groundtruths')\n    if os.path.isdir(gtFolder) is False:\n        errors.append('folder %s not found' % gtFolder)\n# Coordinates types\ngtCoordType = ValidateCoordinatesTypes(args.gtCoordinates, '-gtCoordinates', errors)\ndetCoordType = ValidateCoordinatesTypes(args.detCoordinates, '-detCoordinates', errors)\nimgSize = (0, 0)\nif gtCoordType == CoordinatesType.Relative:  # Image size is required\n    imgSize = ValidateImageSize(args.imgSize, '-imgsize', '-gtCoordinates', errors)\nif detCoordType == CoordinatesType.Relative:  # Image size is required\n    imgSize = ValidateImageSize(args.imgSize, '-imgsize', '-detCoordinates', errors)\n# Detection folder\nif ValidateMandatoryArgs(args.detFolder, '-det/--detfolder', errors):\n    detFolder = ValidatePaths(args.detFolder, '-det/--detfolder', errors)\nelse:\n    # errors.pop()\n    detFolder = os.path.join(currentPath, 'detections')\n    if os.path.isdir(detFolder) is False:\n        errors.append('folder %s not found' % detFolder)\nif args.savePath is not None:\n    savePath = ValidatePaths(args.savePath, '-sp/--savepath', errors)\nelse:\n    savePath = os.path.join(currentPath, 'results')\n# Validate savePath\n# If error, show error messages\nif len(errors) != 0:\n    print(\"\"\"usage: Object Detection Metrics [-h] [-v] [-gt] [-det] [-t] [-gtformat]\n                                [-detformat] [-save]\"\"\")\n    print('Object Detection Metrics: error(s): ')\n    [print(e) for e in errors]\n    sys.exit()\n\n# Check if path to save results already exists and is not empty\nif os.path.isdir(savePath) and os.listdir(savePath):\n    key_pressed = ''\n    while key_pressed.upper() not in ['Y', 'N']:\n        print(f'Folder {savePath} already exists and may contain important results.\\n')\n        print(f'Enter \\'Y\\' to continue. WARNING: THIS WILL REMOVE ALL THE CONTENTS OF THE FOLDER!')\n        print(f'Or enter \\'N\\' to abort and choose another folder to save the results.')\n        key_pressed = input('')\n\n    if key_pressed.upper() == 'N':\n        print('Process canceled')\n        sys.exit()\n\n# Clear folder and save results\nshutil.rmtree(savePath, ignore_errors=True)\nos.makedirs(savePath)\n# Show plot during execution\nshowPlot = args.showPlot\n\n# print('iouThreshold= %f' % iouThreshold)\n# print('savePath = %s' % savePath)\n# print('gtFormat = %s' % gtFormat)\n# print('detFormat = %s' % detFormat)\n# print('gtFolder = %s' % gtFolder)\n# print('detFolder = %s' % detFolder)\n# print('gtCoordType = %s' % gtCoordType)\n# print('detCoordType = %s' % detCoordType)\n# print('showPlot %s' % showPlot)\n\n# Get groundtruth boxes\nallBoundingBoxes, allClasses = getBoundingBoxes(gtFolder,\n                                                True,\n                                                gtFormat,\n                                                gtCoordType,\n                                                imgSize=imgSize)\n# Get detected boxes\nallBoundingBoxes, allClasses = getBoundingBoxes(detFolder,\n                                                False,\n                                                detFormat,\n                                                detCoordType,\n                                                allBoundingBoxes,\n                                                allClasses,\n                                                imgSize=imgSize)\nallClasses.sort()\n\nevaluator = Evaluator()\nacc_AP = 0\nvalidClasses = 0\n\n# Plot Precision x Recall curve\ndetections = evaluator.PlotPrecisionRecallCurve(\n    allBoundingBoxes,  # Object containing all bounding boxes (ground truths and detections)\n    IOUThreshold=iouThreshold,  # IOU threshold\n    method=MethodAveragePrecision.EveryPointInterpolation,\n    showAP=True,  # Show Average Precision in the title of the plot\n    showInterpolatedPrecision=False,  # Don't plot the interpolated precision curve\n    savePath=savePath,\n    showGraphic=showPlot)\n\nf = open(os.path.join(savePath, 'results.txt'), 'w')\nf.write('Object Detection Metrics\\n')\nf.write('https://github.com/rafaelpadilla/Object-Detection-Metrics\\n\\n\\n')\nf.write('Average Precision (AP), Precision and Recall per class:')\n\n# each detection is a class\nfor metricsPerClass in detections:\n\n    # Get metric values per each class\n    cl = metricsPerClass['class']\n    ap = metricsPerClass['AP']\n    precision = metricsPerClass['precision']\n    recall = metricsPerClass['recall']\n    totalPositives = metricsPerClass['total positives']\n    total_TP = metricsPerClass['total TP']\n    total_FP = metricsPerClass['total FP']\n\n    if totalPositives > 0:\n        validClasses = validClasses + 1\n        acc_AP = acc_AP + ap\n        prec = ['%.2f' % p for p in precision]\n        rec = ['%.2f' % r for r in recall]\n        ap_str = \"{0:.2f}%\".format(ap * 100)\n        # ap_str = \"{0:.4f}%\".format(ap * 100)\n        print('AP: %s (%s)' % (ap_str, cl))\n        f.write('\\n\\nClass: %s' % cl)\n        f.write('\\nAP: %s' % ap_str)\n        f.write('\\nPrecision: %s' % prec)\n        f.write('\\nRecall: %s' % rec)\n\nmAP = acc_AP / validClasses\nmAP_str = \"{0:.2f}%\".format(mAP * 100)\nprint('mAP: %s' % mAP_str)\nf.write('\\n\\n\\nmAP: %s' % mAP_str)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2255859375,
          "content": "certifi==2019.11.28\ncycler==0.10.0\nkiwisolver==1.1.0\nmatplotlib==3.1.3\nnumpy==1.22.0\npyparsing==2.4.6\nPyQt5==5.12.3\nPyQt5-sip==4.19.18\nPyQtWebEngine==5.12.1\npython-dateutil==2.8.1\nsix==1.14.0\ntornado==6.0.3\nopencv-python==4.2.0.32\n"
        },
        {
          "name": "results",
          "type": "tree",
          "content": null
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}