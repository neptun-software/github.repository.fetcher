{
  "metadata": {
    "timestamp": 1736560413853,
    "page": 974,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "GuyTevet/motion-diffusion-model",
      "stars": 3221,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.763671875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\nsave/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2022 Guy Tevet\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.173828125,
          "content": "# MDM: Human Motion Diffusion Model\n\n\n[![arXiv](https://img.shields.io/badge/arXiv-<2209.14916>-<COLOR>.svg)](https://arxiv.org/abs/2209.14916)\n<a href=\"https://replicate.com/arielreplicate/motion_diffusion_model\"><img src=\"https://replicate.com/arielreplicate/motion_diffusion_model/badge\"></a>\n\nThe official PyTorch implementation of the paper [**\"Human Motion Diffusion Model\"**](https://arxiv.org/abs/2209.14916).\n\nPlease visit our [**webpage**](https://guytevet.github.io/mdm-page/) for more details.\n\n![teaser](https://github.com/GuyTevet/mdm-page/raw/main/static/figures/github.gif)\n\n## MDM is now 40X faster 游뱔游뱔游뱔 (~0.4 sec/sample)\n\n### How come?!?\n\n(1) We released the [50 diffusion steps model](https://drive.google.com/file/d/1cfadR1eZ116TIdXK7qDX1RugAerEiJXr/view?usp=sharing) (instead of 1000 steps) which runs 20X faster with comparable results.\n\n(2) [Calling CLIP just once and caching the result](https://github.com/GuyTevet/motion-diffusion-model/commit/94c173ff8bb11362e45dd9262751f07bf9293660) runs 2X faster for all models. Please pull.\n\n## MDM results on *HumanML3D* to cite in your paper (The original model used in the MDM paper)\n\nPerformance improvement is due to an evaluation bug fix. BLUE marks fixed entries compared to the paper.\n![fixed_results](assets/fixed_results.png)\n\n- You can use [this](assets/fixed_results.tex) `.tex` file.\n- The fixed **KIT** results are available [here](https://github.com/GuyTevet/motion-diffusion-model/issues/211#issue-2369160290).\n\n## Bibtex\n游댮游댮游댮**NOTE: MDM and MotionDiffuse are NOT the same paper!** For some reason, Google Scholar merged the two papers. The right way to cite MDM is:</span>\n\n<!-- If you find this code useful in your research, please cite: -->\n\n```\n@inproceedings{\ntevet2023human,\ntitle={Human Motion Diffusion Model},\nauthor={Guy Tevet and Sigal Raab and Brian Gordon and Yoni Shafir and Daniel Cohen-or and Amit Haim Bermano},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=SJ1kSyO2jwu}\n}\n```\n\n## News\n\n游닉 **15/Apr/24** - Released a [50 diffusion steps model](https://drive.google.com/file/d/1cfadR1eZ116TIdXK7qDX1RugAerEiJXr/view?usp=sharing) (instead of 1000 steps) which runs 20X faster 游뱔游뱔游뱔 with comparable results.\n\n游닉 **12/Apr/24** - MDM inference is now 2X faster 游뱔游뱔游뱔 This was made possible by [calling CLIP just once and caching the result](https://github.com/GuyTevet/motion-diffusion-model/commit/94c173ff8bb11362e45dd9262751f07bf9293660), and is backward compatible with older models.\n\n游닉 **25/Jan/24** - Fixed bug in evalutation code (#182) - Please use the fixed results when citing MDM.<br>\n\n游닉 **1/Jun/23** - Fixed generation issue (#104) - Please pull to improve generation results.\n\n游닉 **23/Nov/22** - Fixed evaluation issue (#42) - Please pull and run `bash prepare/download_t2m_evaluators.sh` from the top of the repo to adapt.\n\n游닉 **4/Nov/22** - Added sampling, training and evaluation of unconstrained tasks.\n  Note slight env changes adapting to the new code. If you already have an installed environment, run `bash prepare/download_unconstrained_assets.sh; conda install -y -c anaconda scikit-learn\n` to adapt.\n\n游닉 **3/Nov/22** - Added in-between and upper-body editing.\n\n游닉 **31/Oct/22** - Added sampling, training and evaluation of action-to-motion tasks.\n\n游닉 **9/Oct/22** - Added training and evaluation scripts. \n  Note slight env changes adapting to the new code. If you already have an installed environment, run `bash prepare/download_glove.sh; pip install clearml` to adapt.\n\n游닉 **6/Oct/22** - First release - sampling and rendering using pre-trained models.\n\n\n## Checkout MDM Follow-ups (partial list)\n\n游낼 [SinMDM](https://sinmdm.github.io/SinMDM-page/) - Learns single motion motifs - even for non-humanoid characters.\n\n游놆 [PriorMDM](https://priormdm.github.io/priorMDM-page/) - Uses MDM as a generative prior, enabling new generation tasks with few examples or even no data at all.\n\n游눆 [MAS](https://guytevet.github.io/mas-page/) - Generating intricate 3D motions (including non-humanoid) using 2D diffusion models trained on in-the-wild videos.\n\n游 [MoMo](https://monkeyseedocg.github.io/) - Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion\nfor Zero-shot Motion Transfer\n\n游끢 [CAMDM](https://github.com/AIGAnimation/CAMDM) - Taming Diffusion Probabilistic Models for Character Control - a real-time version of MDM.\n\n\n## Getting started\n\nThis code was tested on `Ubuntu 18.04.5 LTS` and requires:\n\n* Python 3.7\n* conda3 or miniconda3\n* CUDA capable GPU (one is enough)\n\n### 1. Setup environment\n\nInstall ffmpeg (if not already installed):\n\n```shell\nsudo apt update\nsudo apt install ffmpeg\n```\nFor windows use [this](https://www.geeksforgeeks.org/how-to-install-ffmpeg-on-windows/) instead.\n\nSetup conda env:\n```shell\nconda env create -f environment.yml\nconda activate mdm\npython -m spacy download en_core_web_sm\npip install git+https://github.com/openai/CLIP.git\n```\n\nDownload dependencies:\n\n<details>\n  <summary><b>Text to Motion</b></summary>\n\n```bash\nbash prepare/download_smpl_files.sh\nbash prepare/download_glove.sh\nbash prepare/download_t2m_evaluators.sh\n```\n</details>\n\n<details>\n  <summary><b>Action to Motion</b></summary>\n\n```bash\nbash prepare/download_smpl_files.sh\nbash prepare/download_recognition_models.sh\n```\n</details>\n\n<details>\n  <summary><b>Unconstrained</b></summary>\n\n```bash\nbash prepare/download_smpl_files.sh\nbash prepare/download_recognition_models.sh\nbash prepare/download_recognition_unconstrained_models.sh\n```\n</details>\n\n### 2. Get data\n\n<details>\n  <summary><b>Text to Motion</b></summary>\n\nThere are two paths to get the data:\n\n(a) **Go the easy way if** you just want to generate text-to-motion (excluding editing which does require motion capture data)\n\n(b) **Get full data** to train and evaluate the model.\n\n\n#### a. The easy way (text only)\n\n**HumanML3D** - Clone HumanML3D, then copy the data dir to our repository:\n\n```shell\ncd ..\ngit clone https://github.com/EricGuo5513/HumanML3D.git\nunzip ./HumanML3D/HumanML3D/texts.zip -d ./HumanML3D/HumanML3D/\ncp -r HumanML3D/HumanML3D motion-diffusion-model/dataset/HumanML3D\ncd motion-diffusion-model\n```\n\n\n#### b. Full data (text + motion capture)\n\n**HumanML3D** - Follow the instructions in [HumanML3D](https://github.com/EricGuo5513/HumanML3D.git),\nthen copy the result dataset to our repository:\n\n```shell\ncp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D\n```\n\n**KIT** - Download from [HumanML3D](https://github.com/EricGuo5513/HumanML3D.git) (no processing needed this time) and the place result in `./dataset/KIT-ML`\n</details>\n\n<details>\n  <summary><b>Action to Motion</b></summary>\n\n**UESTC, HumanAct12** \n```bash\nbash prepare/download_a2m_datasets.sh\n```\n</details>\n\n<details>\n  <summary><b>Unconstrained</b></summary>\n\n**HumanAct12** \n```bash\nbash prepare/download_unconstrained_datasets.sh\n```\n</details>\n\n### 3. Download the pretrained models\n\nDownload the model(s) you wish to use, then unzip and place them in `./save/`. \n\n<details>\n  <summary><b>Text to Motion</b></summary>\n\n**You need only the first one.** \n\n**HumanML3D**\n\n[humanml-encoder-512-50steps](https://drive.google.com/file/d/1cfadR1eZ116TIdXK7qDX1RugAerEiJXr/view?usp=sharing) - Runs 20X faster with comparable performance!\n\n[humanml-encoder-512](https://drive.google.com/file/d/1PE0PK8e5a5j-7-Xhs5YET5U5pGh0c821/view?usp=sharing) (best model used in the paper)\n\n[humanml-decoder-512](https://drive.google.com/file/d/1q3soLadvVh7kJuJPd2cegMNY2xVuVudj/view?usp=sharing)\n\n[humanml-decoder-with-emb-512](https://drive.google.com/file/d/1GnsW0K3UjuOkNkAWmjrGIUmeDDZrmPE5/view?usp=sharing)\n\n**KIT**\n\n[kit-encoder-512](https://drive.google.com/file/d/1SHCRcE0es31vkJMLGf9dyLe7YsWj7pNL/view?usp=sharing)\n\n</details>\n\n<details>\n  <summary><b>Action to Motion</b></summary>\n\n**UESTC**\n\n[uestc](https://drive.google.com/file/d/1goB2DJK4B-fLu2QmqGWKAqWGMTAO6wQ6/view?usp=sharing)\n\n[uestc_no_fc](https://drive.google.com/file/d/1fpv3mR-qP9CYCsi9CrQhFqlLavcSQky6/view?usp=sharing)\n\n**HumanAct12**\n\n[humanact12](https://drive.google.com/file/d/154X8_Lgpec6Xj0glEGql7FVKqPYCdBFO/view?usp=sharing)\n\n[humanact12_no_fc](https://drive.google.com/file/d/1frKVMBYNiN5Mlq7zsnhDBzs9vGJvFeiQ/view?usp=sharing)\n\n</details>\n\n<details>\n  <summary><b>Unconstrained</b></summary>\n\n**HumanAct12**\n\n[humanact12_unconstrained](https://drive.google.com/file/d/1uG68m200pZK3pD-zTmPXu5XkgNpx_mEx/view?usp=share_link)\n\n</details>\n\n\n## Motion Synthesis\n<details>\n  <summary><b>Text to Motion</b></summary>\n\n### Generate from test set prompts\n\n```shell\npython -m sample.generate --model_path ./save/humanml_trans_enc_512/model000200000.pt --num_samples 10 --num_repetitions 3\n```\n\n### Generate from your text file\n\n```shell\npython -m sample.generate --model_path ./save/humanml_trans_enc_512/model000200000.pt --input_text ./assets/example_text_prompts.txt\n```\n\n### Generate a single prompt\n\n```shell\npython -m sample.generate --model_path ./save/humanml_trans_enc_512/model000200000.pt --text_prompt \"the person walked forward and is picking up his toolbox.\"\n```\n</details>\n\n<details>\n  <summary><b>Action to Motion</b></summary>\n\n### Generate from test set actions\n\n```shell\npython -m sample.generate --model_path ./save/humanact12/model000350000.pt --num_samples 10 --num_repetitions 3\n```\n\n### Generate from your actions file\n\n```shell\npython -m sample.generate --model_path ./save/humanact12/model000350000.pt --action_file ./assets/example_action_names_humanact12.txt\n```\n\n### Generate a single action\n\n```shell\npython -m sample.generate --model_path ./save/humanact12/model000350000.pt --action_name \"drink\"\n```\n</details>\n\n<details>\n  <summary><b>Unconstrained</b></summary>\n\n```shell\npython -m sample.generate --model_path ./save/unconstrained/model000450000.pt --num_samples 10 --num_repetitions 3\n```\n\nBy abuse of notation, (num_samples * num_repetitions) samples are created, and are visually organized in a display of num_samples rows and num_repetitions columns.\n\n</details>\n\n**You may also define:**\n* `--device` id.\n* `--seed` to sample different prompts.\n* `--motion_length` (text-to-motion only) in seconds (maximum is 9.8[sec]).\n\n**Running those will get you:**\n\n* `results.npy` file with text prompts and xyz positions of the generated animation\n* `sample##_rep##.mp4` - a stick figure animation for each generated motion.\n\nIt will look something like this:\n\n![example](assets/example_stick_fig.gif)\n\nYou can stop here, or render the SMPL mesh using the following script.\n\n### Render SMPL mesh\n\nTo create SMPL mesh per frame run:\n\n```shell\npython -m visualize.render_mesh --input_path /path/to/mp4/stick/figure/file\n```\n\n**This script outputs:**\n* `sample##_rep##_smpl_params.npy` - SMPL parameters (thetas, root translations, vertices and faces)\n* `sample##_rep##_obj` - Mesh per frame in `.obj` format.\n\n**Notes:**\n* The `.obj` can be integrated into Blender/Maya/3DS-MAX and rendered using them.\n* This script is running [SMPLify](https://smplify.is.tue.mpg.de/) and needs GPU as well (can be specified with the `--device` flag).\n* **Important** - Do not change the original `.mp4` path before running the script.\n\n**Notes for 3d makers:**\n* You have two ways to animate the sequence:\n  1. Use the [SMPL add-on](https://smpl.is.tue.mpg.de/index.html) and the theta parameters saved to `sample##_rep##_smpl_params.npy` (we always use beta=0 and the gender-neutral model).\n  1. A more straightforward way is using the mesh data itself. All meshes have the same topology (SMPL), so you just need to keyframe vertex locations. \n     Since the OBJs are not preserving vertices order, we also save this data to the `sample##_rep##_smpl_params.npy` file for your convenience.\n\n## Motion Editing\n\n* This feature is available for text-to-motion datasets (HumanML3D and KIT).\n* In order to use it, you need to acquire the full data (not just the texts).\n* We support the two modes presented in the paper: `in_between` and `upper_body`.\n\n### Unconditioned editing\n\n```shell\npython -m sample.edit --model_path ./save/humanml_trans_enc_512/model000200000.pt --edit_mode in_between\n```\n\n**You may also define:**\n* `--num_samples` (default is 10) / `--num_repetitions` (default is 3).\n* `--device` id.\n* `--seed` to sample different prompts.\n* `--edit_mode upper_body` For upper body editing (lower body is fixed).\n\n\nThe output will look like this (blue frames are from the input motion; orange were generated by the model):\n\n![example](assets/in_between_edit.gif)\n\n* As in *Motion Synthesis*, you may follow the **Render SMPL mesh** section to obtain meshes for your edited motions.\n\n### Text conditioned editing\n\nJust add the text conditioning using `--text_condition`. For example:\n\n```shell\npython -m sample.edit --model_path ./save/humanml_trans_enc_512/model000200000.pt --edit_mode upper_body --text_condition \"A person throws a ball\"\n```\n\nThe output will look like this (blue joints are from the input motion; orange were generated by the model):\n\n![example](assets/upper_body_edit.gif)\n\n## Train your own MDM\n\n<details>\n  <summary><b>Text to Motion</b></summary>\n\n**HumanML3D**\n```shell\npython -m train.train_mdm --save_dir save/my_humanml_trans_enc_512 --dataset humanml\n```\n\n**KIT**\n```shell\npython -m train.train_mdm --save_dir save/my_kit_trans_enc_512 --dataset kit\n```\n</details>\n<details>\n  <summary><b>Action to Motion</b></summary>\n\n```shell\npython -m train.train_mdm --save_dir save/my_name --dataset {humanact12,uestc} --cond_mask_prob 0 --lambda_rcxyz 1 --lambda_vel 1 --lambda_fc 1\n```\n</details>\n\n<details>\n  <summary><b>Unconstrained</b></summary>\n\n```shell\npython -m train.train_mdm --save_dir save/my_name --dataset humanact12 --cond_mask_prob 0 --lambda_rcxyz 1 --lambda_vel 1 --lambda_fc 1  --unconstrained\n```\n</details>\n\n* Use `--diffusion_steps 50` to train the faster model with less diffusion steps.\n* Use `--device` to define GPU id.\n* Use `--arch` to choose one of the architectures reported in the paper `{trans_enc, trans_dec, gru}` (`trans_enc` is default).\n* Add `--train_platform_type {ClearmlPlatform, TensorboardPlatform}` to track results with either [ClearML](https://clear.ml/) or [Tensorboard](https://www.tensorflow.org/tensorboard).\n* Add `--eval_during_training` to run a short (90 minutes) evaluation for each saved checkpoint. \n  This will slow down training but will give you better monitoring.\n\n## Evaluate\n\n<details>\n  <summary><b>Text to Motion</b></summary>\n\n* Takes about 20 hours (on a single GPU)\n* The output of this script for the pre-trained models (as was reported in the paper) is provided in the checkpoints zip file.\n\n**HumanML3D**\n```shell\npython -m eval.eval_humanml --model_path ./save/humanml_trans_enc_512/model000475000.pt\n```\n\n**KIT**\n```shell\npython -m eval.eval_humanml --model_path ./save/kit_trans_enc_512/model000400000.pt\n```\n</details>\n\n<details>\n  <summary><b>Action to Motion</b></summary>\n\n* Takes about 7 hours for UESTC and 2 hours for HumanAct12 (on a single GPU)\n* The output of this script for the pre-trained models (as was reported in the paper) is provided in the checkpoints zip file.\n\n```shell\npython -m eval.eval_humanact12_uestc --model <path-to-model-ckpt> --eval_mode full\n```\nwhere `path-to-model-ckpt` can be a path to any of the pretrained action-to-motion models listed above, or to a checkpoint trained by the user.\n\n</details>\n\n\n<details>\n  <summary><b>Unconstrained</b></summary>\n\n* Takes about 3 hours (on a single GPU)\n\n```shell\npython -m eval.eval_humanact12_uestc --model ./save/unconstrained/model000450000.pt --eval_mode full\n```\n\nPrecision and recall are not computed to save computing time. If you wish to compute them, edit the file eval/a2m/gru_eval.py and change the string `fast=True` to `fast=False`.\n\n</details>\n\n## Acknowledgments\n\nThis code is standing on the shoulders of giants. We want to thank the following contributors\nthat our code is based on:\n\n[guided-diffusion](https://github.com/openai/guided-diffusion), [MotionCLIP](https://github.com/GuyTevet/MotionCLIP), [text-to-motion](https://github.com/EricGuo5513/text-to-motion), [actor](https://github.com/Mathux/ACTOR), [joints2smpl](https://github.com/wangsen1312/joints2smpl), [MoDi](https://github.com/sigal-raab/MoDi).\n\n## License\nThis code is distributed under an [MIT LICENSE](LICENSE).\n\nNote that our code depends on other libraries, including CLIP, SMPL, SMPL-X, PyTorch3D, and uses datasets that each have their own respective licenses that must also be followed.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "body_models",
          "type": "tree",
          "content": null
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.830078125,
          "content": "build:\n  gpu: true\n  cuda: \"11.3\"\n  python_version: 3.8\n  system_packages:\n    - libgl1-mesa-glx\n    - libglib2.0-0\n\n  python_packages:\n    - imageio==2.22.2\n    - matplotlib==3.1.3\n    - spacy==3.3.1\n    - smplx==0.1.28\n    - chumpy==0.70\n    - blis==0.7.8\n    - click==8.1.3\n    - confection==0.0.2\n    - ftfy==6.1.1\n    - importlib-metadata==5.0.0\n    - lxml==4.9.1\n    - murmurhash==1.0.8\n    - preshed==3.0.7\n    - pycryptodomex==3.15.0\n    - regex==2022.9.13\n    - srsly==2.4.4\n    - thinc==8.0.17\n    - typing-extensions==4.1.1\n    - urllib3==1.26.12\n    - wasabi==0.10.1\n    - wcwidth==0.2.5\n\n  run:\n    - apt update -y && apt-get install ffmpeg -y\n#    - python -m spacy download en_core_web_sm\n    - git clone https://github.com/openai/CLIP.git sub_modules/CLIP\n    - pip install -e sub_modules/CLIP\n\npredict: \"sample/predict.py:Predictor\"\n"
        },
        {
          "name": "data_loaders",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "diffusion",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 3.8955078125,
          "content": "name: mdm\nchannels:\n  - pytorch\n  - anaconda\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=main\n  - _openmp_mutex=5.1=1_gnu\n  - beautifulsoup4=4.11.1=pyha770c72_0\n  - blas=1.0=mkl\n  - brotlipy=0.7.0=py37h540881e_1004\n  - ca-certificates=2022.07.19=h06a4308_0\n  - catalogue=2.0.8=py37h89c1867_0\n  - certifi=2022.6.15=py37h06a4308_0\n  - cffi=1.15.1=py37h74dc2b5_0\n  - charset-normalizer=2.1.1=pyhd8ed1ab_0\n  - colorama=0.4.5=pyhd8ed1ab_0\n  - cryptography=35.0.0=py37hf1a17b8_2\n  - cudatoolkit=11.0.221=h6bb024c_0\n  - cycler=0.11.0=pyhd3eb1b0_0\n  - cymem=2.0.6=py37hd23a5d3_3\n  - dataclasses=0.8=pyhc8e2a94_3\n  - dbus=1.13.18=hb2f20db_0\n  - expat=2.4.9=h6a678d5_0\n  - fftw=3.3.9=h27cfd23_1\n  - filelock=3.8.0=pyhd8ed1ab_0\n  - fontconfig=2.13.1=h6c09931_0\n  - freetype=2.11.0=h70c0345_0\n  - gdown=4.5.1=pyhd8ed1ab_0\n  - giflib=5.2.1=h7b6447c_0\n  - glib=2.69.1=h4ff587b_1\n  - gst-plugins-base=1.14.0=h8213a91_2\n  - gstreamer=1.14.0=h28cd5cc_2\n  - h5py=3.7.0=py37h737f45e_0\n  - hdf5=1.10.6=h3ffc7dd_1\n  - icu=58.2=he6710b0_3\n  - idna=3.4=pyhd8ed1ab_0\n  - intel-openmp=2021.4.0=h06a4308_3561\n  - jinja2=3.1.2=pyhd8ed1ab_1\n  - joblib=1.1.0=pyhd3eb1b0_0\n  - jpeg=9b=h024ee3a_2\n  - kiwisolver=1.4.2=py37h295c915_0\n  - langcodes=3.3.0=pyhd8ed1ab_0\n  - lcms2=2.12=h3be6417_0\n  - ld_impl_linux-64=2.38=h1181459_1\n  - libffi=3.3=he6710b0_2\n  - libgcc-ng=11.2.0=h1234567_1\n  - libgfortran-ng=11.2.0=h00389a5_1\n  - libgfortran5=11.2.0=h1234567_1\n  - libgomp=11.2.0=h1234567_1\n  - libpng=1.6.37=hbc83047_0\n  - libstdcxx-ng=11.2.0=h1234567_1\n  - libtiff=4.1.0=h2733197_1\n  - libuuid=1.0.3=h7f8727e_2\n  - libuv=1.40.0=h7b6447c_0\n  - libwebp=1.2.0=h89dd481_0\n  - libxcb=1.15=h7f8727e_0\n  - libxml2=2.9.14=h74e7548_0\n  - lz4-c=1.9.3=h295c915_1\n  - markupsafe=2.1.1=py37h540881e_1\n  - matplotlib=3.1.3=py37_0\n  - matplotlib-base=3.1.3=py37hef1b27d_0\n  - mkl=2021.4.0=h06a4308_640\n  - mkl-service=2.4.0=py37h7f8727e_0\n  - mkl_fft=1.3.1=py37hd3c417c_0\n  - mkl_random=1.2.2=py37h51133e4_0\n  - ncurses=6.3=h5eee18b_3\n  - ninja=1.10.2=h06a4308_5\n  - ninja-base=1.10.2=hd09550d_5\n  - numpy=1.21.5=py37h6c91a56_3\n  - numpy-base=1.21.5=py37ha15fc14_3\n  - openssl=1.1.1q=h7f8727e_0\n  - packaging=21.3=pyhd8ed1ab_0\n  - pathy=0.6.2=pyhd8ed1ab_0\n  - pcre=8.45=h295c915_0\n  - pillow=9.2.0=py37hace64e9_1\n  - pip=22.2.2=py37h06a4308_0\n  - pycparser=2.21=pyhd8ed1ab_0\n  - pydantic=1.8.2=py37h5e8e339_2\n  - pyopenssl=22.0.0=pyhd8ed1ab_1\n  - pyparsing=3.0.9=py37h06a4308_0\n  - pyqt=5.9.2=py37h05f1152_2\n  - pysocks=1.7.1=py37h89c1867_5\n  - python=3.7.13=h12debd9_0\n  - python-dateutil=2.8.2=pyhd3eb1b0_0\n  - python_abi=3.7=2_cp37m\n  - pytorch=1.7.1=py3.7_cuda11.0.221_cudnn8.0.5_0\n  - qt=5.9.7=h5867ecd_1\n  - readline=8.1.2=h7f8727e_1\n  - requests=2.28.1=pyhd8ed1ab_1\n  - scikit-learn=1.0.2=py37h51133e4_1\n  - scipy=1.7.3=py37h6c91a56_2\n  - setuptools=63.4.1=py37h06a4308_0\n  - shellingham=1.5.0=pyhd8ed1ab_0\n  - sip=4.19.8=py37hf484d3e_0\n  - six=1.16.0=pyhd3eb1b0_1\n  - smart_open=5.2.1=pyhd8ed1ab_0\n  - soupsieve=2.3.2.post1=pyhd8ed1ab_0\n  - spacy=3.3.1=py37h79cecc1_0\n  - spacy-legacy=3.0.10=pyhd8ed1ab_0\n  - spacy-loggers=1.0.3=pyhd8ed1ab_0\n  - sqlite=3.39.3=h5082296_0\n  - threadpoolctl=2.2.0=pyh0d69192_0\n  - tk=8.6.12=h1ccaba5_0\n  - torchaudio=0.7.2=py37\n  - torchvision=0.8.2=py37_cu110\n  - tornado=6.2=py37h5eee18b_0\n  - tqdm=4.64.1=py37h06a4308_0\n  - trimesh=3.15.3=pyh1a96a4e_0\n  - typer=0.4.2=pyhd8ed1ab_0\n  - wheel=0.37.1=pyhd3eb1b0_0\n  - xz=5.2.6=h5eee18b_0\n  - zipp=3.8.1=pyhd8ed1ab_0\n  - zlib=1.2.12=h5eee18b_3\n  - zstd=1.4.9=haebb681_0\n  - pip:\n    - blis==0.7.8\n    - chumpy==0.70\n    - click==8.1.3\n    - confection==0.0.2\n    - ftfy==6.1.1\n    - importlib-metadata==5.0.0\n    - lxml==4.9.1\n    - murmurhash==1.0.8\n    - preshed==3.0.7\n    - pycryptodomex==3.15.0\n    - regex==2022.9.13\n    - smplx==0.1.28\n    - srsly==2.4.4\n    - thinc==8.0.17\n    - typing-extensions==4.1.1\n    - urllib3==1.26.12\n    - wasabi==0.10.1\n    - wcwidth==0.2.5\nprefix: /disk2/guytevet/anaconda3/envs/mdm-venv\n"
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "prepare",
          "type": "tree",
          "content": null
        },
        {
          "name": "sample",
          "type": "tree",
          "content": null
        },
        {
          "name": "train",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "visualize",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}