{
  "metadata": {
    "timestamp": 1736560354957,
    "page": 886,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "shelhamer/fcn.berkeleyvision.org",
      "stars": 3321,
      "defaultBranch": "master",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.2529296875,
          "content": "# Fully Convolutional Networks for Semantic Segmentation\n\nThis is the reference implementation of the models and code for the fully convolutional networks (FCNs) in the [PAMI FCN](https://arxiv.org/abs/1605.06211) and [CVPR FCN](http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html) papers:\n\n    Fully Convolutional Models for Semantic Segmentation\n    Evan Shelhamer*, Jonathan Long*, Trevor Darrell\n    PAMI 2016\n    arXiv:1605.06211\n\n    Fully Convolutional Models for Semantic Segmentation\n    Jonathan Long*, Evan Shelhamer*, Trevor Darrell\n    CVPR 2015\n    arXiv:1411.4038\n\n**Note that this is a work in progress and the final, reference version is coming soon.**\nPlease ask Caffe and FCN usage questions on the [caffe-users mailing list](https://groups.google.com/forum/#!forum/caffe-users).\n\nRefer to [these slides](https://docs.google.com/presentation/d/10XodYojlW-1iurpUsMoAZknQMS36p7lVIfFZ-Z7V_aY/edit?usp=sharing) for a summary of the approach.\n\nThese models are compatible with `BVLC/caffe:master`.\nCompatibility has held since `master@8c66fa5` with the merge of PRs #3613 and #3570.\nThe code and models here are available under the same license as Caffe (BSD-2) and the Caffe-bundled models (that is, unrestricted use; see the [BVLC model license](http://caffe.berkeleyvision.org/model_zoo.html#bvlc-model-license)).\n\n**PASCAL VOC models**: trained online with high momentum for a ~5 point boost in mean intersection-over-union over the original models.\nThese models are trained using extra data from [Hariharan et al.](http://www.cs.berkeley.edu/~bharath2/codes/SBD/download.html), but excluding SBD val.\nFCN-32s is fine-tuned from the [ILSVRC-trained VGG-16 model](https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014), and the finer strides are then fine-tuned in turn.\nThe \"at-once\" FCN-8s is fine-tuned from VGG-16 all-at-once by scaling the skip connections to better condition optimization.\n\n* [FCN-32s PASCAL](voc-fcn32s): single stream, 32 pixel prediction stride net, scoring 63.6 mIU on seg11valid\n* [FCN-16s PASCAL](voc-fcn16s): two stream, 16 pixel prediction stride net, scoring 65.0 mIU on seg11valid\n* [FCN-8s PASCAL](voc-fcn8s): three stream, 8 pixel prediction stride net, scoring 65.5 mIU on seg11valid and 67.2 mIU on seg12test\n* [FCN-8s PASCAL at-once](voc-fcn8s-atonce): all-at-once, three stream, 8 pixel prediction stride net, scoring 65.4 mIU on seg11valid\n\n[FCN-AlexNet PASCAL](voc-fcn-alexnet): AlexNet (CaffeNet) architecture, single stream, 32 pixel prediction stride net, scoring 48.0 mIU on seg11valid.\nUnlike the FCN-32/16/8s models, this network is trained with gradient accumulation, normalized loss, and standard momentum.\n(Note: when both FCN-32s/FCN-VGG16 and FCN-AlexNet are trained in this same way FCN-VGG16 is far better; see Table 1 of the paper.)\n\nTo reproduce the validation scores, use the [seg11valid](https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/data/pascal/seg11valid.txt) split defined by the paper in footnote 7. Since SBD train and PASCAL VOC 2011 segval intersect, we only evaluate on the non-intersecting set for validation purposes.\n\n**NYUDv2 models**: trained online with high momentum on color, depth, and HHA features (from Gupta et al. https://github.com/s-gupta/rcnn-depth).\nThese models demonstrate FCNs for multi-modal input.\n\n* [FCN-32s NYUDv2 Color](nyud-fcn32s-color): single stream, 32 pixel prediction stride net on color/BGR input\n* [FCN-32s NYUDv2 HHA](nyud-fcn32s-hha): single stream, 32 pixel prediction stride net on HHA input\n* [FCN-32s NYUDv2 Early Color-Depth](nyud-fcn32s-color-d): single stream, 32 pixel prediction stride net on early fusion of color and (log) depth for 4-channel input\n* [FCN-32s NYUDv2 Late Color-HHA](nyud-fcn32s-color-hha): single stream, 32 pixel prediction stride net by late fusion of FCN-32s NYUDv2 Color and FCN-32s NYUDv2 HHA\n\n**SIFT Flow models**: trained online with high momentum for joint semantic class and geometric class segmentation.\nThese models demonstrate FCNs for multi-task output.\n\n* [FCN-32s SIFT Flow](siftflow-fcn32s): single stream stream, 32 pixel prediction stride net\n* [FCN-16s SIFT Flow](siftflow-fcn16s): two stream, 16 pixel prediction stride net\n* [FCN-8s SIFT Flow](siftflow-fcn8s): three stream, 8 pixel prediction stride net\n\n*Note*: in this release, the evaluation of the semantic classes is not quite right at the moment due to an issue with missing classes.\nThis will be corrected soon.\nThe evaluation of the geometric classes is fine.\n\n**PASCAL-Context models**: trained online with high momentum on an object and scene labeling of PASCAL VOC.\n\n* [FCN-32s PASCAL-Context](pascalcontext-fcn32s): single stream, 32 pixel prediction stride net\n* [FCN-16s PASCAL-Context](pascalcontext-fcn16s): two stream, 16 pixel prediction stride net\n* [FCN-8s PASCAL-Context](pascalcontext-fcn8s): three stream, 8 pixel prediction stride net\n\n## Frequently Asked Questions\n\n**Is learning the interpolation necessary?** In our original experiments the interpolation layers were initialized to bilinear kernels and then learned.\nIn follow-up experiments, and this reference implementation, the bilinear kernels are fixed.\nThere is no significant difference in accuracy in our experiments, and fixing these parameters gives a slight speed-up.\nNote that in our networks there is only one interpolation kernel per output class, and results may differ for higher-dimensional and non-linear interpolation, for which learning may help further.\n\n**Why pad the input?**: The 100 pixel input padding guarantees that the network output can be aligned to the input for any input size in the given datasets, for instance PASCAL VOC.\nThe alignment is handled automatically by net specification and the crop layer.\nIt is possible, though less convenient, to calculate the exact offsets necessary and do away with this amount of padding.\n\n**Why are all the outputs/gradients/parameters zero?**: This is almost universally due to not initializing the weights as needed.\nTo reproduce our FCN training, or train your own FCNs, it is crucial to transplant the weights from the corresponding ILSVRC net such as VGG16.\nThe included `surgery.transplant()` method can help with this.\n\n**What about FCN-GoogLeNet?**: a reference FCN-GoogLeNet for PASCAL VOC is coming soon.\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "ilsvrc-nets",
          "type": "tree",
          "content": null
        },
        {
          "name": "infer.py",
          "type": "blob",
          "size": 0.97265625,
          "content": "import numpy as np\nfrom PIL import Image\n\nimport caffe\nimport vis\n\n# the demo image is \"2007_000129\" from PASCAL VOC\n\n# load image, switch to BGR, subtract mean, and make dims C x H x W for Caffe\nim = Image.open('demo/image.jpg')\nin_ = np.array(im, dtype=np.float32)\nin_ = in_[:,:,::-1]\nin_ -= np.array((104.00698793,116.66876762,122.67891434))\nin_ = in_.transpose((2,0,1))\n\n# load net\nnet = caffe.Net('voc-fcn8s/deploy.prototxt', 'voc-fcn8s/fcn8s-heavy-pascal.caffemodel', caffe.TEST)\n# shape for input (data blob is N x C x H x W), set data\nnet.blobs['data'].reshape(1, *in_.shape)\nnet.blobs['data'].data[...] = in_\n# run net and take argmax for prediction\nnet.forward()\nout = net.blobs['score'].data[0].argmax(axis=0)\n\n# visualize segmentation in PASCAL VOC colors\nvoc_palette = vis.make_palette(21)\nout_im = Image.fromarray(vis.color_seg(out, voc_palette))\nout_im.save('demo/output.png')\nmasked_im = Image.fromarray(vis.vis_seg(im, out, voc_palette))\nmasked_im.save('demo/visualization.jpg')\n"
        },
        {
          "name": "nyud-fcn32s-color-d",
          "type": "tree",
          "content": null
        },
        {
          "name": "nyud-fcn32s-color-hha",
          "type": "tree",
          "content": null
        },
        {
          "name": "nyud-fcn32s-color",
          "type": "tree",
          "content": null
        },
        {
          "name": "nyud-fcn32s-hha",
          "type": "tree",
          "content": null
        },
        {
          "name": "nyud_layers.py",
          "type": "blob",
          "size": 5.1328125,
          "content": "import caffe\n\nimport numpy as np\nfrom PIL import Image\nimport scipy.io\n\nimport random\n\nclass NYUDSegDataLayer(caffe.Layer):\n    \"\"\"\n    Load (input image, label image) pairs from NYUDv2\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    The labels follow the 40 class task defined by\n\n        S. Gupta, R. Girshick, p. Arbelaez, and J. Malik. Learning rich features\n        from RGB-D images for object detection and segmentation. ECCV 2014.\n\n    with 0 as the void label and 1-40 the classes.\n\n    Use this to feed data to a fully convolutional network.\n    \"\"\"\n\n    def setup(self, bottom, top):\n        \"\"\"\n        Setup data layer according to parameters:\n\n        - nyud_dir: path to NYUDv2 dir\n        - split: train / val / test\n        - tops: list of tops to output from {color, depth, hha, label}\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for NYUDv2 semantic segmentation.\n\n        example: params = dict(nyud_dir=\"/path/to/NYUDVOC2011\", split=\"val\",\n                               tops=['color', 'hha', 'label'])\n        \"\"\"\n        # config\n        params = eval(self.param_str)\n        self.nyud_dir = params['nyud_dir']\n        self.split = params['split']\n        self.tops = params['tops']\n        self.random = params.get('randomize', True)\n        self.seed = params.get('seed', None)\n\n        # store top data for reshape + forward\n        self.data = {}\n\n        # means\n        self.mean_bgr = np.array((116.190, 97.203, 92.318), dtype=np.float32)\n        self.mean_hha = np.array((132.431, 94.076, 118.477), dtype=np.float32)\n        self.mean_logd = np.array((7.844,), dtype=np.float32)\n\n        # tops: check configuration\n        if len(top) != len(self.tops):\n            raise Exception(\"Need to define {} tops for all outputs.\")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(\"Do not define a bottom.\")\n\n        # load indices for images and labels\n        split_f  = '{}/{}.txt'.format(self.nyud_dir, self.split)\n        self.indices = open(split_f, 'r').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if 'train' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n    def reshape(self, bottom, top):\n        # load data for tops and  reshape tops to fit (1 is the batch dim)\n        for i, t in enumerate(self.tops):\n            self.data[t] = self.load(t, self.indices[self.idx])\n            top[i].reshape(1, *self.data[t].shape)\n\n    def forward(self, bottom, top):\n        # assign output\n        for i, t in enumerate(self.tops):\n            top[i].data[...] = self.data[t]\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n    def load(self, top, idx):\n        if top == 'color':\n            return self.load_image(idx)\n        elif top == 'label':\n            return self.load_label(idx)\n        elif top == 'depth':\n            return self.load_depth(idx)\n        elif top == 'hha':\n            return self.load_hha(idx)\n        else:\n            raise Exception(\"Unknown output type: {}\".format(top))\n\n    def load_image(self, idx):\n        \"\"\"\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        \"\"\"\n        im = Image.open('{}/data/images/img_{}.png'.format(self.nyud_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean_bgr\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n    def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        Shift labels so that classes are 0-39 and void is 255 (to ignore it).\n        The leading singleton dimension is required by the loss.\n        \"\"\"\n        label = scipy.io.loadmat('{}/segmentation/img_{}.mat'.format(self.nyud_dir, idx))['segmentation'].astype(np.uint8)\n        label -= 1  # rotate labels\n        label = label[np.newaxis, ...]\n        return label\n\n    def load_depth(self, idx):\n        \"\"\"\n        Load pre-processed depth for NYUDv2 segmentation set.\n        \"\"\"\n        im = Image.open('{}/data/depth/img_{}.png'.format(self.nyud_dir, idx))\n        d = np.array(im, dtype=np.float32)\n        d = np.log(d)\n        d -= self.mean_logd\n        d = d[np.newaxis, ...]\n        return d\n\n    def load_hha(self, idx):\n        \"\"\"\n        Load HHA features from Gupta et al. ECCV14.\n        See https://github.com/s-gupta/rcnn-depth/blob/master/rcnn/saveHHA.m\n        \"\"\"\n        im = Image.open('{}/data/hha/img_{}.png'.format(self.nyud_dir, idx))\n        hha = np.array(im, dtype=np.float32)\n        hha -= self.mean_hha\n        hha = hha.transpose((2,0,1))\n        return hha\n"
        },
        {
          "name": "pascalcontext-fcn16s",
          "type": "tree",
          "content": null
        },
        {
          "name": "pascalcontext-fcn32s",
          "type": "tree",
          "content": null
        },
        {
          "name": "pascalcontext-fcn8s",
          "type": "tree",
          "content": null
        },
        {
          "name": "pascalcontext_layers.py",
          "type": "blob",
          "size": 4.576171875,
          "content": "import caffe\n\nimport numpy as np\nfrom PIL import Image\nimport scipy.io\n\nimport random\n\nclass PASCALContextSegDataLayer(caffe.Layer):\n    \"\"\"\n    Load (input image, label image) pairs from PASCAL-Context\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    The labels follow the 59 class task defined by\n\n        R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R.\n        Urtasun, and A. Yuille.  The Role of Context for Object Detection and\n        Semantic Segmentation in the Wild.  CVPR 2014.\n\n    Use this to feed data to a fully convolutional network.\n    \"\"\"\n\n    def setup(self, bottom, top):\n        \"\"\"\n        Setup data layer according to parameters:\n\n        - voc_dir: path to PASCAL VOC dir (must contain 2010)\n        - context_dir: path to PASCAL-Context annotations\n        - split: train / val / test\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for PASCAL-Context semantic segmentation.\n\n        example: params = dict(voc_dir=\"/path/to/PASCAL\", split=\"val\")\n        \"\"\"\n        # config\n        params = eval(self.param_str)\n        self.voc_dir = params['voc_dir'] + '/VOC2010'\n        self.context_dir = params['context_dir']\n        self.split = params['split']\n        self.mean = np.array((104.007, 116.669, 122.679), dtype=np.float32)\n        self.random = params.get('randomize', True)\n        self.seed = params.get('seed', None)\n\n        # load labels and resolve inconsistencies by mapping to full 400 labels\n        self.labels_400 = [label.replace(' ','') for idx, label in np.genfromtxt(self.context_dir + '/labels.txt', delimiter=':', dtype=None)]\n        self.labels_59 = [label.replace(' ','') for idx, label in np.genfromtxt(self.context_dir + '/59_labels.txt', delimiter=':', dtype=None)]\n        for main_label, task_label in zip(('table', 'bedclothes', 'cloth'), ('diningtable', 'bedcloth', 'clothes')):\n            self.labels_59[self.labels_59.index(task_label)] = main_label\n\n        # two tops: data and label\n        if len(top) != 2:\n            raise Exception(\"Need to define two tops: data and label.\")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(\"Do not define a bottom.\")\n\n        # load indices for images and labels\n        split_f  = '{}/ImageSets/Main/{}.txt'.format(self.voc_dir,\n                self.split)\n        self.indices = open(split_f, 'r').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if 'train' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n    def reshape(self, bottom, top):\n        # load image + label image pair\n        self.data = self.load_image(self.indices[self.idx])\n        self.label = self.load_label(self.indices[self.idx])\n        # reshape tops to fit (leading 1 is for batch dimension)\n        top[0].reshape(1, *self.data.shape)\n        top[1].reshape(1, *self.label.shape)\n\n    def forward(self, bottom, top):\n        # assign output\n        top[0].data[...] = self.data\n        top[1].data[...] = self.label\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n    def load_image(self, idx):\n        \"\"\"\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        \"\"\"\n        im = Image.open('{}/JPEGImages/{}.jpg'.format(self.voc_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n    def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        The full 400 labels are translated to the 59 class task labels.\n        \"\"\"\n        label_400 = scipy.io.loadmat('{}/trainval/{}.mat'.format(self.context_dir, idx))['LabelMap']\n        label = np.zeros_like(label_400, dtype=np.uint8)\n        for idx, l in enumerate(self.labels_59):\n            idx_400 = self.labels_400.index(l) + 1\n            label[label_400 == idx_400] = idx + 1\n        label = label[np.newaxis, ...]\n        return label\n"
        },
        {
          "name": "score.py",
          "type": "blob",
          "size": 2.146484375,
          "content": "from __future__ import division\nimport caffe\nimport numpy as np\nimport os\nimport sys\nfrom datetime import datetime\nfrom PIL import Image\n\ndef fast_hist(a, b, n):\n    k = (a >= 0) & (a < n)\n    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n\ndef compute_hist(net, save_dir, dataset, layer='score', gt='label'):\n    n_cl = net.blobs[layer].channels\n    if save_dir:\n        os.mkdir(save_dir)\n    hist = np.zeros((n_cl, n_cl))\n    loss = 0\n    for idx in dataset:\n        net.forward()\n        hist += fast_hist(net.blobs[gt].data[0, 0].flatten(),\n                                net.blobs[layer].data[0].argmax(0).flatten(),\n                                n_cl)\n\n        if save_dir:\n            im = Image.fromarray(net.blobs[layer].data[0].argmax(0).astype(np.uint8), mode='P')\n            im.save(os.path.join(save_dir, idx + '.png'))\n        # compute the loss as well\n        loss += net.blobs['loss'].data.flat[0]\n    return hist, loss / len(dataset)\n\ndef seg_tests(solver, save_format, dataset, layer='score', gt='label'):\n    print '>>>', datetime.now(), 'Begin seg tests'\n    solver.test_nets[0].share_with(solver.net)\n    do_seg_tests(solver.test_nets[0], solver.iter, save_format, dataset, layer, gt)\n\ndef do_seg_tests(net, iter, save_format, dataset, layer='score', gt='label'):\n    n_cl = net.blobs[layer].channels\n    if save_format:\n        save_format = save_format.format(iter)\n    hist, loss = compute_hist(net, save_format, dataset, layer, gt)\n    # mean loss\n    print '>>>', datetime.now(), 'Iteration', iter, 'loss', loss\n    # overall accuracy\n    acc = np.diag(hist).sum() / hist.sum()\n    print '>>>', datetime.now(), 'Iteration', iter, 'overall accuracy', acc\n    # per-class accuracy\n    acc = np.diag(hist) / hist.sum(1)\n    print '>>>', datetime.now(), 'Iteration', iter, 'mean accuracy', np.nanmean(acc)\n    # per-class IU\n    iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n    print '>>>', datetime.now(), 'Iteration', iter, 'mean IU', np.nanmean(iu)\n    freq = hist.sum(1) / hist.sum()\n    print '>>>', datetime.now(), 'Iteration', iter, 'fwavacc', \\\n            (freq[freq > 0] * iu[freq > 0]).sum()\n    return hist\n"
        },
        {
          "name": "siftflow-fcn16s",
          "type": "tree",
          "content": null
        },
        {
          "name": "siftflow-fcn32s",
          "type": "tree",
          "content": null
        },
        {
          "name": "siftflow-fcn8s",
          "type": "tree",
          "content": null
        },
        {
          "name": "siftflow_layers.py",
          "type": "blob",
          "size": 4.404296875,
          "content": "import caffe\n\nimport numpy as np\nfrom PIL import Image\nimport scipy.io\n\nimport random\n\nclass SIFTFlowSegDataLayer(caffe.Layer):\n    \"\"\"\n    Load (input image, label image) pairs from SIFT Flow\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    This data layer has three tops:\n\n    1. the data, pre-processed\n    2. the semantic labels 0-32 and void 255\n    3. the geometric labels 0-2 and void 255\n\n    Use this to feed data to a fully convolutional network.\n    \"\"\"\n\n    def setup(self, bottom, top):\n        \"\"\"\n        Setup data layer according to parameters:\n\n        - siftflow_dir: path to SIFT Flow dir\n        - split: train / val / test\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for semantic segmentation of object and geometric classes.\n\n        example: params = dict(siftflow_dir=\"/path/to/siftflow\", split=\"val\")\n        \"\"\"\n        # config\n        params = eval(self.param_str)\n        self.siftflow_dir = params['siftflow_dir']\n        self.split = params['split']\n        self.mean = np.array((114.578, 115.294, 108.353), dtype=np.float32)\n        self.random = params.get('randomize', True)\n        self.seed = params.get('seed', None)\n\n        # three tops: data, semantic, geometric\n        if len(top) != 3:\n            raise Exception(\"Need to define three tops: data, semantic label, and geometric label.\")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(\"Do not define a bottom.\")\n\n        # load indices for images and labels\n        split_f  = '{}/{}.txt'.format(self.siftflow_dir, self.split)\n        self.indices = open(split_f, 'r').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if 'train' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n    def reshape(self, bottom, top):\n        # load image + label image pair\n        self.data = self.load_image(self.indices[self.idx])\n        self.label_semantic = self.load_label(self.indices[self.idx], label_type='semantic')\n        self.label_geometric = self.load_label(self.indices[self.idx], label_type='geometric')\n        # reshape tops to fit (leading 1 is for batch dimension)\n        top[0].reshape(1, *self.data.shape)\n        top[1].reshape(1, *self.label_semantic.shape)\n        top[2].reshape(1, *self.label_geometric.shape)\n\n    def forward(self, bottom, top):\n        # assign output\n        top[0].data[...] = self.data\n        top[1].data[...] = self.label_semantic\n        top[2].data[...] = self.label_geometric\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n    def load_image(self, idx):\n        \"\"\"\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        \"\"\"\n        im = Image.open('{}/Images/spatial_envelope_256x256_static_8outdoorcategories/{}.jpg'.format(self.siftflow_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n    def load_label(self, idx, label_type=None):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        \"\"\"\n        if label_type == 'semantic':\n            label = scipy.io.loadmat('{}/SemanticLabels/spatial_envelope_256x256_static_8outdoorcategories/{}.mat'.format(self.siftflow_dir, idx))['S']\n        elif label_type == 'geometric':\n            label = scipy.io.loadmat('{}/GeoLabels/spatial_envelope_256x256_static_8outdoorcategories/{}.mat'.format(self.siftflow_dir, idx))['S']\n            label[label == -1] = 0\n        else:\n            raise Exception(\"Unknown label type: {}. Pick semantic or geometric.\".format(label_type))\n        label = label.astype(np.uint8)\n        label -= 1  # rotate labels so classes start at 0, void is 255\n        label = label[np.newaxis, ...]\n        return label.copy()\n"
        },
        {
          "name": "surgery.py",
          "type": "blob",
          "size": 2.58984375,
          "content": "from __future__ import division\nimport caffe\nimport numpy as np\n\ndef transplant(new_net, net, suffix=''):\n    \"\"\"\n    Transfer weights by copying matching parameters, coercing parameters of\n    incompatible shape, and dropping unmatched parameters.\n\n    The coercion is useful to convert fully connected layers to their\n    equivalent convolutional layers, since the weights are the same and only\n    the shapes are different.  In particular, equivalent fully connected and\n    convolution layers have shapes O x I and O x I x H x W respectively for O\n    outputs channels, I input channels, H kernel height, and W kernel width.\n\n    Both  `net` to `new_net` arguments must be instantiated `caffe.Net`s.\n    \"\"\"\n    for p in net.params:\n        p_new = p + suffix\n        if p_new not in new_net.params:\n            print 'dropping', p\n            continue\n        for i in range(len(net.params[p])):\n            if i > (len(new_net.params[p_new]) - 1):\n                print 'dropping', p, i\n                break\n            if net.params[p][i].data.shape != new_net.params[p_new][i].data.shape:\n                print 'coercing', p, i, 'from', net.params[p][i].data.shape, 'to', new_net.params[p_new][i].data.shape\n            else:\n                print 'copying', p, ' -> ', p_new, i\n            new_net.params[p_new][i].data.flat = net.params[p][i].data.flat\n\ndef upsample_filt(size):\n    \"\"\"\n    Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n    \"\"\"\n    factor = (size + 1) // 2\n    if size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:size, :size]\n    return (1 - abs(og[0] - center) / factor) * \\\n           (1 - abs(og[1] - center) / factor)\n\ndef interp(net, layers):\n    \"\"\"\n    Set weights of each layer in layers to bilinear kernels for interpolation.\n    \"\"\"\n    for l in layers:\n        m, k, h, w = net.params[l][0].data.shape\n        if m != k and k != 1:\n            print 'input + output channels need to be the same or |output| == 1'\n            raise\n        if h != w:\n            print 'filters need to be square'\n            raise\n        filt = upsample_filt(h)\n        net.params[l][0].data[range(m), range(k), :, :] = filt\n\ndef expand_score(new_net, new_layer, net, layer):\n    \"\"\"\n    Transplant an old score layer's parameters, with k < k' classes, into a new\n    score layer with k classes s.t. the first k' are the old classes.\n    \"\"\"\n    old_cl = net.params[layer][0].num\n    new_net.params[new_layer][0].data[:old_cl][...] = net.params[layer][0].data\n    new_net.params[new_layer][1].data[0,0,0,:old_cl][...] = net.params[layer][1].data\n"
        },
        {
          "name": "vis.py",
          "type": "blob",
          "size": 1.5703125,
          "content": "import numpy as np\n\ndef make_palette(num_classes):\n    \"\"\"\n    Maps classes to colors in the style of PASCAL VOC.\n    Close values are mapped to far colors for segmentation visualization.\n    See http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit\n\n    Takes:\n        num_classes: the number of classes\n    Gives:\n        palette: the colormap as a k x 3 array of RGB colors\n    \"\"\"\n    palette = np.zeros((num_classes, 3), dtype=np.uint8)\n    for k in xrange(0, num_classes):\n        label = k\n        i = 0\n        while label:\n            palette[k, 0] |= (((label >> 0) & 1) << (7 - i))\n            palette[k, 1] |= (((label >> 1) & 1) << (7 - i))\n            palette[k, 2] |= (((label >> 2) & 1) << (7 - i))\n            label >>= 3\n            i += 1\n    return palette\n\ndef color_seg(seg, palette):\n    \"\"\"\n    Replace classes with their colors.\n\n    Takes:\n        seg: H x W segmentation image of class IDs\n    Gives:\n        H x W x 3 image of class colors\n    \"\"\"\n    return palette[seg.flat].reshape(seg.shape + (3,))\n\ndef vis_seg(img, seg, palette, alpha=0.5):\n    \"\"\"\n    Visualize segmentation as an overlay on the image.\n\n    Takes:\n        img: H x W x 3 image in [0, 255]\n        seg: H x W segmentation image of class IDs\n        palette: K x 3 colormap for all classes\n        alpha: opacity of the segmentation in [0, 1]\n    Gives:\n        H x W x 3 image with overlaid segmentation\n    \"\"\"\n    vis = np.array(img, dtype=np.float32)\n    mask = seg > 0\n    vis[mask] *= 1. - alpha\n    vis[mask] += alpha * palette[seg[mask].flat]\n    vis = vis.astype(np.uint8)\n    return vis\n"
        },
        {
          "name": "voc-fcn-alexnet",
          "type": "tree",
          "content": null
        },
        {
          "name": "voc-fcn16s",
          "type": "tree",
          "content": null
        },
        {
          "name": "voc-fcn32s",
          "type": "tree",
          "content": null
        },
        {
          "name": "voc-fcn8s-atonce",
          "type": "tree",
          "content": null
        },
        {
          "name": "voc-fcn8s",
          "type": "tree",
          "content": null
        },
        {
          "name": "voc_layers.py",
          "type": "blob",
          "size": 7.1171875,
          "content": "import caffe\n\nimport numpy as np\nfrom PIL import Image\n\nimport random\n\nclass VOCSegDataLayer(caffe.Layer):\n    \"\"\"\n    Load (input image, label image) pairs from PASCAL VOC\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    Use this to feed data to a fully convolutional network.\n    \"\"\"\n\n    def setup(self, bottom, top):\n        \"\"\"\n        Setup data layer according to parameters:\n\n        - voc_dir: path to PASCAL VOC year dir\n        - split: train / val / test\n        - mean: tuple of mean values to subtract\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for PASCAL VOC semantic segmentation.\n\n        example\n\n        params = dict(voc_dir=\"/path/to/PASCAL/VOC2011\",\n            mean=(104.00698793, 116.66876762, 122.67891434),\n            split=\"val\")\n        \"\"\"\n        # config\n        params = eval(self.param_str)\n        self.voc_dir = params['voc_dir']\n        self.split = params['split']\n        self.mean = np.array(params['mean'])\n        self.random = params.get('randomize', True)\n        self.seed = params.get('seed', None)\n\n        # two tops: data and label\n        if len(top) != 2:\n            raise Exception(\"Need to define two tops: data and label.\")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(\"Do not define a bottom.\")\n\n        # load indices for images and labels\n        split_f  = '{}/ImageSets/Segmentation/{}.txt'.format(self.voc_dir,\n                self.split)\n        self.indices = open(split_f, 'r').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if 'train' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n\n    def reshape(self, bottom, top):\n        # load image + label image pair\n        self.data = self.load_image(self.indices[self.idx])\n        self.label = self.load_label(self.indices[self.idx])\n        # reshape tops to fit (leading 1 is for batch dimension)\n        top[0].reshape(1, *self.data.shape)\n        top[1].reshape(1, *self.label.shape)\n\n\n    def forward(self, bottom, top):\n        # assign output\n        top[0].data[...] = self.data\n        top[1].data[...] = self.label\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n\n    def load_image(self, idx):\n        \"\"\"\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        \"\"\"\n        im = Image.open('{}/JPEGImages/{}.jpg'.format(self.voc_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n\n    def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        \"\"\"\n        im = Image.open('{}/SegmentationClass/{}.png'.format(self.voc_dir, idx))\n        label = np.array(im, dtype=np.uint8)\n        label = label[np.newaxis, ...]\n        return label\n\n\nclass SBDDSegDataLayer(caffe.Layer):\n    \"\"\"\n    Load (input image, label image) pairs from the SBDD extended labeling\n    of PASCAL VOC for semantic segmentation\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    Use this to feed data to a fully convolutional network.\n    \"\"\"\n\n    def setup(self, bottom, top):\n        \"\"\"\n        Setup data layer according to parameters:\n\n        - sbdd_dir: path to SBDD `dataset` dir\n        - split: train / seg11valid\n        - mean: tuple of mean values to subtract\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for SBDD semantic segmentation.\n\n        N.B.segv11alid is the set of segval11 that does not intersect with SBDD.\n        Find it here: https://gist.github.com/shelhamer/edb330760338892d511e.\n\n        example\n\n        params = dict(sbdd_dir=\"/path/to/SBDD/dataset\",\n            mean=(104.00698793, 116.66876762, 122.67891434),\n            split=\"valid\")\n        \"\"\"\n        # config\n        params = eval(self.param_str)\n        self.sbdd_dir = params['sbdd_dir']\n        self.split = params['split']\n        self.mean = np.array(params['mean'])\n        self.random = params.get('randomize', True)\n        self.seed = params.get('seed', None)\n\n        # two tops: data and label\n        if len(top) != 2:\n            raise Exception(\"Need to define two tops: data and label.\")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(\"Do not define a bottom.\")\n\n        # load indices for images and labels\n        split_f  = '{}/{}.txt'.format(self.sbdd_dir,\n                self.split)\n        self.indices = open(split_f, 'r').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if 'train' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n\n    def reshape(self, bottom, top):\n        # load image + label image pair\n        self.data = self.load_image(self.indices[self.idx])\n        self.label = self.load_label(self.indices[self.idx])\n        # reshape tops to fit (leading 1 is for batch dimension)\n        top[0].reshape(1, *self.data.shape)\n        top[1].reshape(1, *self.label.shape)\n\n\n    def forward(self, bottom, top):\n        # assign output\n        top[0].data[...] = self.data\n        top[1].data[...] = self.label\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n\n    def load_image(self, idx):\n        \"\"\"\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        \"\"\"\n        im = Image.open('{}/img/{}.jpg'.format(self.sbdd_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n\n    def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        \"\"\"\n        import scipy.io\n        mat = scipy.io.loadmat('{}/cls/{}.mat'.format(self.sbdd_dir, idx))\n        label = mat['GTcls'][0]['Segmentation'][0].astype(np.uint8)\n        label = label[np.newaxis, ...]\n        return label\n"
        }
      ]
    }
  ]
}