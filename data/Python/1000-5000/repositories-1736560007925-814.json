{
  "metadata": {
    "timestamp": 1736560007925,
    "page": 814,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "minimaxir/gpt-2-simple",
      "stars": 3398,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1025390625,
          "content": ".vscode\n*.pyc\n*checkpoint.ipynb\n__pycache__\nshakespeare.txt\ndist/\ngpt_2_simple.egg-info/\nbuild/\n.DS_Store"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 2.08984375,
          "content": "MIT License\n\nCopyright (c) 2019-2020 Max Woolf\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n---\n\nMIT License\n\nCopyright (c) 2019 OpenAI\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.03515625,
          "content": "graft */src\nglobal-exclude .DS_Store"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.6455078125,
          "content": "# gpt-2-simple\n\n![gen_demo](docs/gen_demo.png)\n\nA simple Python package that wraps existing model fine-tuning and generation scripts for [OpenAI](https://openai.com)'s [GPT-2 text generation model](https://openai.com/blog/better-language-models/) (specifically the \"small\" 124M and \"medium\" 355M hyperparameter versions). Additionally, this package allows easier generation of text, generating to a file for easy curation, allowing for prefixes to force the text to start with a given phrase.\n\nThis package incorporates and makes minimal low-level changes to:\n\n- Model management from OpenAI's [official GPT-2 repo](https://github.com/openai/gpt-2) (MIT License)\n- Model finetuning from Neil Shepperd's [fork](https://github.com/nshepperd/gpt-2) of GPT-2 (MIT License)\n- Text generation output management from [textgenrnn](https://github.com/minimaxir/textgenrnn) (MIT License / also created by me)\n\nFor finetuning, it is **strongly** recommended to use a GPU, although you can generate using a CPU (albeit much more slowly). If you are training in the cloud, using a Colaboratory notebook or a Google Compute Engine VM w/ the [TensorFlow Deep Learning](https://cloud.google.com/deep-learning-vm/) image is strongly recommended. (as the GPT-2 model is hosted on GCP)\n\nYou can use gpt-2-simple to retrain a model using a GPU **for free** in [this Colaboratory notebook](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce), which also demos additional features of the package.\n\nNote: Development on gpt-2-simple has mostly been superceded by [aitextgen](https://github.com/minimaxir/aitextgen), which has similar AI text generation capabilities with more efficient training time and resource usage. If you do not require using TensorFlow, I recommend using aitextgen instead. Checkpoints trained using gpt-2-simple can be [loaded using aitextgen](https://docs.aitextgen.io/gpt-2-simple/) as well.\n\n## Install\n\ngpt-2-simple can be installed [via PyPI](https://pypi.org/project/gpt_2_simple/):\n\n```shell\npip3 install gpt-2-simple\n```\n\nYou will also need to install the corresponding TensorFlow 2.X version (min 2.5.1) for your system (e.g. `tensorflow` or `tensorflow-gpu`).\n\n## Usage\n\nAn example for downloading the model to the local system, finetuning it on a dataset. and generating some text.\n\nWarning: the pretrained 124M model, and thus any finetuned model, is 500 MB! (the pretrained 355M model is 1.5 GB)\n\n```python\nimport gpt_2_simple as gpt2\nimport os\nimport requests\n\nmodel_name = \"124M\"\nif not os.path.isdir(os.path.join(\"models\", model_name)):\n\tprint(f\"Downloading {model_name} model...\")\n\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n\n\nfile_name = \"shakespeare.txt\"\nif not os.path.isfile(file_name):\n\turl = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n\tdata = requests.get(url)\n\n\twith open(file_name, 'w') as f:\n\t\tf.write(data.text)\n\n\nsess = gpt2.start_tf_sess()\ngpt2.finetune(sess,\n              file_name,\n              model_name=model_name,\n              steps=1000)   # steps is max number of training steps\n\ngpt2.generate(sess)\n```\n\nThe generated model checkpoints are by default in `/checkpoint/run1`. If you want to load a model from that folder and generate text from it:\n\n```python\nimport gpt_2_simple as gpt2\n\nsess = gpt2.start_tf_sess()\ngpt2.load_gpt2(sess)\n\ngpt2.generate(sess)\n```\n\nAs with textgenrnn, you can generate and save text for later use (e.g. an API or a bot) by using the `return_as_list` parameter.\n\n```python\nsingle_text = gpt2.generate(sess, return_as_list=True)[0]\nprint(single_text)\n```\n\nYou can pass a `run_name` parameter to `finetune` and `load_gpt2` if you want to store/load multiple models in a `checkpoint` folder.\n\nThere is also a command-line interface for both finetuning and generation with strong defaults for just running on a Cloud VM w/ GPU. For finetuning (which will also download the model if not present):\n\n```shell\ngpt_2_simple finetune shakespeare.txt\n```\n\nAnd for generation, which generates texts to files in a `gen` folder:\n\n```shell\ngpt_2_simple generate\n```\n\nMost of the same parameters available in the functions are available as CLI arguments, e.g.:\n\n```shell\ngpt_2_simple generate --temperature 1.0 --nsamples 20 --batch_size 20 --length 50 --prefix \"<|startoftext|>\" --truncate \"<|endoftext|>\" --include_prefix False --nfiles 5\n```\n\nSee below to see what some of the CLI arguments do.\n\nNB: _Restart the Python session first_ if you want to finetune on another dataset or load another model.\n\n## Differences Between gpt-2-simple And Other Text Generation Utilities\n\nThe method GPT-2 uses to generate text is slightly different than those like other packages like textgenrnn (specifically, generating the full text sequence purely in the GPU and decoding it later), which cannot easily be fixed without hacking the underlying model code. As a result:\n\n- In general, GPT-2 is better at maintaining context over its entire generation length, making it good for generating conversational text. The text is also generally gramatically correct, with proper capitalization and few typoes.\n- The original GPT-2 model was trained on a _very_ large variety of sources, allowing the model to incorporate idioms not seen in the input text.\n- GPT-2 can only generate a maximum of 1024 tokens per request (about 3-4 paragraphs of English text).\n- GPT-2 cannot stop early upon reaching a specific end token. (workaround: pass the `truncate` parameter to a `generate` function to only collect text until a specified end token. You may want to reduce `length` appropriately.)\n- Higher temperatures work better (e.g. 0.7 - 1.0) to generate more interesting text, while other frameworks work better between 0.2 - 0.5.\n- When finetuning GPT-2, it has no sense of the beginning or end of a document within a larger text. You'll need to use a bespoke character sequence to indicate the beginning and end of a document. Then while generating, you can specify a `prefix` targeting the beginning token sequences, and a `truncate` targeting the end token sequence. You can also set `include_prefix=False` to discard the prefix token while generating (e.g. if it's something unwanted like `<|startoftext|>`).\n- If you pass a single-column `.csv` file to `finetune()`, it will automatically parse the CSV into a format ideal for training with GPT-2 (including prepending `<|startoftext|>` and suffixing `<|endoftext|>` to every text document, so the `truncate` tricks above are helpful when generating output). This is necessary to handle both quotes and newlines in each text document correctly.\n- GPT-2 allows you to generate texts in parallel by setting a `batch_size` that is divisible into `nsamples`, resulting in much faster generation. Works very well with a GPU (can set `batch_size` up to 20 on Colaboratory's K80)!\n- Due to GPT-2's architecture, it scales up nicely with more powerful GPUs. For the 124M model, if you want to train for longer periods of time, GCP's P100 GPU is about 3x faster than a K80/T4 for only 3x the price, making it price-comparable (the V100 is about 1.5x faster than the P100 but about 2x the price). The P100 uses 100% of the GPU even with `batch_size=1`, and about 88% of the V100 GPU.\n- If you have a partially-trained GPT-2 model and want to continue finetuning it, you can set `overwrite=True` to finetune, which will continue training and remove the previous iteration of the model without creating a duplicate copy. This can be especially useful for transfer learning (e.g. heavily finetune GPT-2 on one dataset, then finetune on other dataset to get a \"merging\" of both datasets).\n- If your input text dataset is massive (>100 MB), you may want to preencode and compress the dataset using `gpt2.encode_dataset(file_path)`. THe output is a compressed `.npz` file which will load much faster into the GPU for finetuning.\n- The 774M \"large\" model may support finetuning because it will cause modern GPUs to go out-of-memory (you may get lucky if you use a P100 GPU on Colaboratory). However, you can still generate from the default pretrained model using `gpt2.load_gpt2(sess, model_name='774M')` and `gpt2.generate(sess, model_name='774M')`.\n- The 1558M \"extra large\", true model, may not work out-of-the-box with the GPU included with the Colaboratory Notebook. More testing is needed to identify optimial configurations for it.\n\n## Interactive Apps Using gpt-2-simple\n\n- [gpt2-small](https://minimaxir.com/apps/gpt2-small/) — App using the default GPT-2 124M pretrained model\n- [gpt2-reddit](https://minimaxir.com/apps/gpt2-reddit/) — App to generate Reddit titles based on a specified subreddit and/or keyword(s)\n- [gpt2-mtg](https://minimaxir.com/apps/gpt2-mtg/) — App to generate Magic: The Gathering cards\n\n## Text Generation Examples Using gpt-2-simple\n\n- [ResetEra](https://www.resetera.com/threads/i-trained-an-ai-on-thousands-of-resetera-thread-conversations-and-it-created-hot-gaming-shitposts.112167/) — Generated video game forum discussions ([GitHub w/ dumps](https://github.com/minimaxir/resetera-gpt-2))\n- [/r/legaladvice](https://www.reddit.com/r/legaladviceofftopic/comments/bfqf22/i_trained_a_moreadvanced_ai_on_rlegaladvice/) — Title generation ([GitHub w/ dumps](https://github.com/minimaxir/legaladvice-gpt2))\n- [Hacker News](https://github.com/minimaxir/hacker-news-gpt-2) — Tens of thousands of generated Hacker News submission titles\n\n## Maintainer/Creator\n\nMax Woolf ([@minimaxir](https://minimaxir.com))\n\n_Max's open-source projects are supported by his [Patreon](https://www.patreon.com/minimaxir). If you found this project helpful, any monetary contributions to the Patreon are appreciated and will be put to good creative use._\n\n## License\n\nMIT\n\n## Disclaimer\n\nThis repo has no affiliation or relationship with OpenAI.\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "gpt_2_simple",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0517578125,
          "content": "tensorflow>=2.5.1\nregex\nrequests\ntqdm\nnumpy\ntoposort\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.3916015625,
          "content": "from setuptools import setup\n\nlong_description = \"\"\"\nA simple Python package that wraps existing model fine-tuning and generation scripts for OpenAI GPT-2 text generation model (specifically the \"small\", 124M hyperparameter version). Additionally, this package allows easier generation of text, generating to a file for easy curation, allowing for prefixes to force the text to start with a given phrase.\n\n## Usage\n\nAn example for downloading the model to the local system, fineturning it on a dataset. and generating some text.\n\nWarning: the pretrained model, and thus any finetuned model, is 500 MB!\n\n```python\nimport gpt_2_simple as gpt2\n\ngpt2.download_gpt2()   # model is saved into current directory under /models/124M/\n\nsess = gpt2.start_tf_sess()\ngpt2.finetune(sess, 'shakespeare.txt', steps=1000)   # steps is max number of training steps\n\ngpt2.generate(sess)\n```\n\nThe generated model checkpoints are by default in `/checkpoint/run1`. If you want to load a model from that folder and generate text from it:\n\n```python\nimport gpt_2_simple as gpt2\n\nsess = gpt2.start_tf_sess()\ngpt2.load_gpt2(sess)\n\ngpt2.generate(sess)\n```\n\nAs with textgenrnn, you can generate and save text for later use (e.g. an API or a bot) by using the `return_as_list` parameter.\n\n```python\nsingle_text = gpt2.generate(sess, return_as_list=True)[0]\nprint(single_text)\n```\n\nYou can pass a `run_name` parameter to `finetune` and `load_gpt2` if you want to store/load multiple models in a `checkpoint` folder.\n\nNB: *Restart the Python session first* if you want to finetune on another dataset or load another model.\n\"\"\"\n\n\nsetup(\n    name=\"gpt_2_simple\",\n    packages=[\"gpt_2_simple\"],  # this must be the same as the name above\n    version=\"0.8.1\",\n    description=\"Python package to easily retrain OpenAI's GPT-2 \"\n    \"text-generating model on new texts.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    author=\"Max Woolf\",\n    author_email=\"max@minimaxir.com\",\n    url=\"https://github.com/minimaxir/gpt-2-simple\",\n    keywords=[\"deep learning\", \"tensorflow\", \"text generation\"],\n    classifiers=[],\n    license=\"MIT\",\n    entry_points={\n        \"console_scripts\": [\"gpt_2_simple=gpt_2_simple.gpt_2:cmd\"],\n    },\n    python_requires=\">=3.6\",\n    include_package_data=True,\n    install_requires=[\n        \"tensorflow>=2.5.1\",\n        \"regex\",\n        \"requests\",\n        \"tqdm\",\n        \"numpy\",\n        \"toposort\",\n    ],\n)\n"
        }
      ]
    }
  ]
}