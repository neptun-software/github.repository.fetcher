{
  "metadata": {
    "timestamp": 1736560005181,
    "page": 809,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "aim-uofa/AdelaiDet",
      "stars": 3409,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.580078125,
          "content": "# output dir\noutput\ninstant_test_output\ninference_test_output\n\n\n*.jpg\n*.png\n*.txt\n\n# compilation and distribution\n__pycache__\n_ext\n*.pyc\n*.so\nAdelaiDet.egg-info/\nbuild/\ndist/\n\n# pytorch/python/numpy formats\n*.pth\n*.pkl\n*.npy\n\n# ipython/jupyter notebooks\n*.ipynb\n**/.ipynb_checkpoints/\n\n# Editor temporaries\n*.swn\n*.swo\n*.swp\n*~\n\n# Pycharm editor settings\n.idea\n.vscode\n.python-version\n\n# project dirs\n/datasets/coco\n/datasets/lvis\n/datasets/pic\n/datasets/ytvos\n/models\n/demo_outputs\n/example_inputs\n/debug\n/weights\n/export\neval.sh\n\ndemo/performance.py\ndemo/demo2.py\ntrain.sh\nbenchmark.sh\nscript"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.3828125,
          "content": "AdelaiDet for non-commercial purposes\n(For commercial use, contact chhshen@gmail.com for obtaining a commerical license.)\n\nCopyright (c) 2019 the authors\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MODEL_ZOO.md",
          "type": "blob",
          "size": 3.3271484375,
          "content": "# AdelaiDet Model Zoo and Baselines\n\n## Introduction\nThis file documents a collection of models trained with AdelaiDet in Nov, 2019.\n\n## Models\n\nThe inference time is measured on one 1080Ti based on the most recent commit on Detectron2 ([ffff8ac](https://github.com/facebookresearch/detectron2/commit/ffff8acc35ea88ad1cb1806ab0f00b4c1c5dbfd9)).\n\nMore models will be released soon. Stay tuned.\n\n### COCO Object Detecton Baselines with FCOS\n\nName | box AP | download\n--- |:---:|:---:\n[FCOS_R_50_1x](configs/FCOS-Detection/R_50_1x.yaml) | 38.7 | [model](https://cloudstor.aarnet.edu.au/plus/s/glqFc13cCoEyHYy/download)\n\n### COCO Instance Segmentation Baselines with [BlendMask](https://arxiv.org/abs/2001.00309)\n\nModel | Name |inference time (ms/im) | box AP | mask AP | download\n--- |:---:|:---:|:---:|:---:|:---:\nMask R-CNN | [550_R_50_3x](configs/RCNN/550_R_50_FPN_3x.yaml) | 63 | 39.1 | 35.3 |\nBlendMask | [550_R_50_3x](configs/BlendMask/550_R_50_3x.yaml) | 36 | 38.7 | 34.5 | [model](https://cloudstor.aarnet.edu.au/plus/s/R3Qintf7N8UCiIt/download)\nMask R-CNN | [R_50_1x](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml) | 80 | 38.6 | 35.2 |\nBlendMask | [R_50_1x](configs/BlendMask/R_50_1x.yaml) | 73 | 39.9 | 35.8 | [model](https://cloudstor.aarnet.edu.au/plus/s/zoxXPnr6Hw3OJgK/download)\nMask R-CNN | [R_50_3x](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml) | 80 | 41.0 | 37.2 | \nBlendMask | [R_50_3x](configs/BlendMask/R_50_3x.yaml) | 74 | 42.7 | 37.8 | [model](https://cloudstor.aarnet.edu.au/plus/s/ZnaInHFEKst6mvg/download)\nMask R-CNN | [R_101_3x](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml) | 100 | 42.9 | 38.6 |\nBlendMask | [R_101_3x](configs/BlendMask/R_101_3x.yaml) | 94 | 44.8 | 39.5 | [model](https://cloudstor.aarnet.edu.au/plus/s/e4fXrliAcMtyEBy/download)\nBlendMask | [R_101_dcni3_5x](configs/BlendMask/R_101_dcni3_5x.yaml) | 105 | 46.8 | 41.1 | [model](https://cloudstor.aarnet.edu.au/plus/s/vbnKnQtaGlw8TKv/download)\n\n### COCO Panoptic Segmentation Baselines with BlendMask\nModel | Name | PQ | PQ<sup>Th</sup> | PQ<sup>St</sup> | download\n--- |:---:|:---:|:---:|:---:|:---:\nPanoptic FPN | [R_50_3x](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml) | 41.5 | 48.3 | 31.2 | \nBlendMask | [R_50_3x](configs/BlendMask/Panoptic/R_50_3x.yaml) | 42.5 | 49.5 | 32.0 | [model](https://cloudstor.aarnet.edu.au/plus/s/oDgi0826JOJXCr5/download)\nPanoptic FPN | [R_101_3x](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/panoptic_fpn_R_101_3x.yaml) | 43.0 | 49.7 | 32.9 |\nBlendMask | [R_101_3x](configs/BlendMask/Panoptic/R_101_3x.yaml) | 44.3 | 51.6 | 33.2 | [model](https://cloudstor.aarnet.edu.au/plus/s/u6gZwj06MWDEkYe/download)\nBlendMask | [R_101_dcni3_5x](configs/BlendMask/Panoptic/R_101_dcni3_5x.yaml) | 46.0 | 52.9 | 35.5 | [model](https://cloudstor.aarnet.edu.au/plus/s/Jwp41WEzDdrhWsN/download)\n\n### Person in Context with BlendMask\nModel | Name | box AP | mask AP | download\n--- |:---:|:---:|:---:|:---:\nBlendMask | [R_50_1x](configs/BlendMask/Person/R_50_1x.yaml) | 70.6 | 66.7 | [model](https://cloudstor.aarnet.edu.au/plus/s/nvpcKTFA5fsagc0/download)"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.9541015625,
          "content": "<div align=\"center\">\n    <img src=\"docs/adel-logo.svg\" width=\"160\" >\n</div>\n\n#  AdelaiDet\n\nAs of Jan. 2024, the CloudStor server is dead. Model files are hosted on huggingface:\n\n- https://huggingface.co/ZjuCv/AdelaiDet/tree/main\n- https://huggingface.co/tianzhi/AdelaiDet-FCOS/tree/main\n- https://huggingface.co/tianzhi/AdelaiDet-CondInst/tree/main\n- https://huggingface.co/tianzhi/AdelaiDet-BoxInst/tree/main\n\n\n*AdelaiDet* is an open source toolbox for multiple instance-level recognition tasks on top of [Detectron2](https://github.com/facebookresearch/detectron2).\nAll instance-level recognition works from our group are open-sourced here.\n\nTo date, AdelaiDet implements the following algorithms:\n\n* [FCOS](configs/FCOS-Detection/README.md)\n* [BlendMask](configs/BlendMask/README.md)\n* [MEInst](configs/MEInst-InstanceSegmentation/README.md)\n* [ABCNet](configs/BAText/README.md)\n* [ABCNetv2](configs/BAText#quick-start-abcnetv2) \n* [CondInst](configs/CondInst/README.md)\n* [SOLO](https://arxiv.org/abs/1912.04488) ([mmdet version](https://github.com/WXinlong/SOLO))\n* [SOLOv2](configs/SOLOv2/README.md)\n* [BoxInst](configs/BoxInst/README.md) ([video demo](https://www.youtube.com/watch?v=NuF8NAYf5L8))\n* [DenseCL](configs/DenseCL/README.md)\n* [FCPose](configs/FCPose/README.md)\n\n\n\n## Models\n### COCO Object Detecton Baselines with [FCOS](https://arxiv.org/abs/1904.01355)\nName | inf. time | box AP | download\n--- |:---:|:---:|:---\n[FCOS_R_50_1x](configs/FCOS-Detection/R_50_1x.yaml) | 16 FPS | 38.7 | [model](https://huggingface.co/tianzhi/AdelaiDet-FCOS/resolve/main/FCOS_R_50_1x.pth?download=true)\n[FCOS_MS_R_101_2x](configs/FCOS-Detection/MS_R_101_2x.yaml) | 12 FPS | 43.1 | [model](https://huggingface.co/tianzhi/AdelaiDet-FCOS/resolve/main/FCOS_MS_R_101_2x.pth?download=true)\n[FCOS_MS_X_101_32x8d_2x](configs/FCOS-Detection/MS_X_101_32x8d_2x.yaml) | 6.6 FPS | 43.9 | [model](https://huggingface.co/tianzhi/AdelaiDet-FCOS/resolve/main/FCOS_MS_X_101_32x8d_2x.pth?download=true)\n[FCOS_MS_X_101_32x8d_dcnv2_2x](configs/FCOS-Detection/MS_X_101_32x8d_2x_dcnv2.yaml) | 4.6 FPS | 46.6 | [model](https://huggingface.co/tianzhi/AdelaiDet-FCOS/resolve/main/FCOS_MS_X_101_32x8d_dcnv2_2x.pth?download=true)\n[FCOS_RT_MS_DLA_34_4x_shtw](configs/FCOS-Detection/FCOS_RT/MS_DLA_34_4x_syncbn_shared_towers.yaml) | 52 FPS | 39.1 | [model](https://huggingface.co/tianzhi/AdelaiDet-FCOS/resolve/main/FCOS_RT_MS_DLA_34_4x_syncbn_shared_towers.pth?download=true)\n\nMore models can be found in FCOS [README.md](configs/FCOS-Detection/README.md).\n\n### COCO Instance Segmentation Baselines with [BlendMask](https://arxiv.org/abs/2001.00309)\n\nModel | Name |inf. time | box AP | mask AP | download\n--- |:---:|:---:|:---:|:---:|:---:\nMask R-CNN | [R_101_3x](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml) | 10 FPS | 42.9 | 38.6 |\nBlendMask | [R_101_3x](configs/BlendMask/R_101_3x.yaml) | 11 FPS | 44.8 | 39.5 | [model](https://huggingface.co/ZjuCv/AdelaiDet/blob/main/R_101_3x.pth)\nBlendMask | [R_101_dcni3_5x](configs/BlendMask/R_101_dcni3_5x.yaml) | 10 FPS | 46.8 | 41.1 | [model](https://huggingface.co/ZjuCv/AdelaiDet/blob/main/R_101_dcni3_5x.pth)\n\nFor more models and information, please refer to BlendMask [README.md](configs/BlendMask/README.md).\n\n### COCO Instance Segmentation Baselines with [MEInst](https://arxiv.org/abs/2003.11712)\n\nName | inf. time | box AP | mask AP | download\n--- |:---:|:---:|:---:|:---:\n[MEInst_R_50_3x](https://github.com/aim-uofa/AdelaiDet/configs/MEInst-InstanceSegmentation/MEInst_R_50_3x.yaml) | 12 FPS | 43.6 | 34.5 | [model](https://huggingface.co/ZjuCv/AdelaiDet/blob/main/MEInst_R_50_3x.pth)\n\nFor more models and information, please refer to MEInst [README.md](configs/MEInst-InstanceSegmentation/README.md).\n\n### Total_Text results with [ABCNet](configs/BAText/README.md)\n\nName | inf. time | e2e-hmean | det-hmean | download\n---  |:---------:|:---------:|:---------:|:---:\n[v1-totaltext](configs/BAText/TotalText/attn_R_50.yaml) | 11 FPS | 67.1 | 86.0 | [model](https://huggingface.co/ZjuCv/AdelaiDet/blob/main/tt_e2e_attn_R_50.pth)\n[v2-totaltext](configs/BAText/TotalText/v2_attn_R_50.yaml) | 7.7 FPS | 71.8 | 87.2 | [model](https://huggingface.co/ZjuCv/AdelaiDet/blob/main/model_v2_totaltext.pth)\n\nFor more models and information, please refer to ABCNet [README.md](configs/BAText/README.md).\n\n### COCO Instance Segmentation Baselines with [CondInst](https://arxiv.org/abs/2003.05664)\n\nName | inf. time | box AP | mask AP | download\n--- |:---:|:---:|:---:|:---:\n[CondInst_MS_R_50_1x](configs/CondInst/MS_R_50_1x.yaml) | 14 FPS | 39.7 | 35.7 | [model](https://huggingface.co/tianzhi/AdelaiDet-CondInst/resolve/main/CondInst_MS_R_50_1x.pth?download=true)\n[CondInst_MS_R_50_BiFPN_3x_sem](configs/CondInst/MS_R_50_BiFPN_3x_sem.yaml) | 13 FPS | 44.7 | 39.4 | [model](https://huggingface.co/tianzhi/AdelaiDet-CondInst/resolve/main/CondInst_MS_R_50_BiFPN_3x_sem.pth?download=true)\n[CondInst_MS_R_101_3x](configs/CondInst/MS_R_101_3x.yaml) | 11 FPS | 43.3 | 38.6 | [model](https://huggingface.co/tianzhi/AdelaiDet-CondInst/resolve/main/CondInst_MS_R_101_3x.pth?download=true)\n[CondInst_MS_R_101_BiFPN_3x_sem](configs/CondInst/MS_R_101_BiFPN_3x_sem.yaml) | 10 FPS | 45.7 | 40.2 | [model](https://huggingface.co/tianzhi/AdelaiDet-CondInst/resolve/main/CondInst_R_101_BiFPN_3x_sem.pth?download=true)\n\nFor more models and information, please refer to CondInst [README.md](configs/CondInst/README.md).\n\nNote that:\n- Inference time for all projects is measured on a NVIDIA 1080Ti with batch size 1.\n- APs are evaluated on COCO2017 val split unless specified.\n\n\n## Installation\n\nFirst install Detectron2 following the official guide: [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).\n\n*Please use Detectron2 with commit id [9eb4831](https://github.com/facebookresearch/detectron2/commit/9eb4831f742ae6a13b8edb61d07b619392fb6543) if you have any issues related to Detectron2.*\n\nThen build AdelaiDet with:\n\n```\ngit clone https://github.com/aim-uofa/AdelaiDet.git\ncd AdelaiDet\npython setup.py build develop\n```\n\nIf you are using docker, a pre-built image can be pulled with:\n\n```\ndocker pull tianzhi0549/adet:latest\n```\n\nSome projects may require special setup, please follow their own `README.md` in [configs](configs).\n\n## Quick Start\n\n### Inference with Pre-trained Models\n\n1. Pick a model and its config file, for example, `fcos_R_50_1x.yaml`.\n2. Download the model `wget https://huggingface.co/tianzhi/AdelaiDet-FCOS/resolve/main/FCOS_R_50_1x.pth?download=true -O fcos_R_50_1x.pth`\n3. Run the demo with\n```\npython demo/demo.py \\\n    --config-file configs/FCOS-Detection/R_50_1x.yaml \\\n    --input input1.jpg input2.jpg \\\n    --opts MODEL.WEIGHTS fcos_R_50_1x.pth\n```\n\n### Train Your Own Models\n\nTo train a model with \"train_net.py\", first\nsetup the corresponding datasets following\n[datasets/README.md](https://github.com/facebookresearch/detectron2/blob/master/datasets/README.md),\nthen run:\n\n```\nOMP_NUM_THREADS=1 python tools/train_net.py \\\n    --config-file configs/FCOS-Detection/R_50_1x.yaml \\\n    --num-gpus 8 \\\n    OUTPUT_DIR training_dir/fcos_R_50_1x\n```\nTo evaluate the model after training, run:\n\n```\nOMP_NUM_THREADS=1 python tools/train_net.py \\\n    --config-file configs/FCOS-Detection/R_50_1x.yaml \\\n    --eval-only \\\n    --num-gpus 8 \\\n    OUTPUT_DIR training_dir/fcos_R_50_1x \\\n    MODEL.WEIGHTS training_dir/fcos_R_50_1x/model_final.pth\n```\nNote that:\n- The configs are made for 8-GPU training. To train on another number of GPUs, change the `--num-gpus`.\n- If you want to measure the inference time, please change `--num-gpus` to 1.\n- We set `OMP_NUM_THREADS=1` by default, which achieves the best speed on our machines, please change it as needed.\n- This quick start is made for FCOS. If you are using other projects, please check the projects' own `README.md` in [configs](configs). \n\n\n## Acknowledgements\n\nThe authors are grateful to\nNvidia, Huawei Noah's Ark Lab, ByteDance, Adobe who generously donated GPU computing in the past a few years.\n\n## Citing AdelaiDet\n\nIf you use this toolbox in your research or wish to refer to the baseline results published here, please use the following BibTeX entries:\n\n```BibTeX\n\n@misc{tian2019adelaidet,\n  author =       {Tian, Zhi and Chen, Hao and Wang, Xinlong and Liu, Yuliang and Shen, Chunhua},\n  title =        {{AdelaiDet}: A Toolbox for Instance-level Recognition Tasks},\n  howpublished = {\\url{https://git.io/adelaidet}},\n  year =         {2019}\n}\n```\nand relevant publications:\n```BibTeX\n\n@inproceedings{tian2019fcos,\n  title     =  {{FCOS}: Fully Convolutional One-Stage Object Detection},\n  author    =  {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},\n  booktitle =  {Proc. Int. Conf. Computer Vision (ICCV)},\n  year      =  {2019}\n}\n\n@article{tian2021fcos,\n  title   =  {{FCOS}: A Simple and Strong Anchor-free Object Detector},\n  author  =  {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},\n  journal =  {IEEE T. Pattern Analysis and Machine Intelligence (TPAMI)},\n  year    =  {2021}\n}\n\n@inproceedings{chen2020blendmask,\n  title     =  {{BlendMask}: Top-Down Meets Bottom-Up for Instance Segmentation},\n  author    =  {Chen, Hao and Sun, Kunyang and Tian, Zhi and Shen, Chunhua and Huang, Yongming and Yan, Youliang},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}\n\n@inproceedings{zhang2020MEInst,\n  title     =  {Mask Encoding for Single Shot Instance Segmentation},\n  author    =  {Zhang, Rufeng and Tian, Zhi and Shen, Chunhua and You, Mingyu and Yan, Youliang},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}\n\n@inproceedings{liu2020abcnet,\n  title     =  {{ABCNet}: Real-time Scene Text Spotting with Adaptive {B}ezier-Curve Network},\n  author    =  {Liu, Yuliang and Chen, Hao and Shen, Chunhua and He, Tong and Jin, Lianwen and Wang, Liangwei},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}\n\n@ARTICLE{9525302,\n  author={Liu, Yuliang and Shen, Chunhua and Jin, Lianwen and He, Tong and Chen, Peng and Liu, Chongyu and Chen, Hao},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, \n  title={ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting}, \n  year={2021},\n  volume={},\n  number={},\n  pages={1-1},\n  doi={10.1109/TPAMI.2021.3107437}\n}\n\n@inproceedings{wang2020solo,\n  title     =  {{SOLO}: Segmenting Objects by Locations},\n  author    =  {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei},\n  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},\n  year      =  {2020}\n}\n\n@inproceedings{wang2020solov2,\n  title     =  {{SOLOv2}: Dynamic and Fast Instance Segmentation},\n  author    =  {Wang, Xinlong and Zhang, Rufeng and Kong, Tao and Li, Lei and Shen, Chunhua},\n  booktitle =  {Proc. Advances in Neural Information Processing Systems (NeurIPS)},\n  year      =  {2020}\n}\n\n@article{wang2021solo,\n  title   =  {{SOLO}: A Simple Framework for Instance Segmentation},\n  author  =  {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},\n  journal =  {IEEE T. Pattern Analysis and Machine Intelligence (TPAMI)},\n  year    =  {2021}\n}\n\n@article{tian2019directpose,\n  title   =  {{DirectPose}: Direct End-to-End Multi-Person Pose Estimation},\n  author  =  {Tian, Zhi and Chen, Hao and Shen, Chunhua},\n  journal =  {arXiv preprint arXiv:1911.07451},\n  year    =  {2019}\n}\n\n@inproceedings{tian2020conditional,\n  title     =  {Conditional Convolutions for Instance Segmentation},\n  author    =  {Tian, Zhi and Shen, Chunhua and Chen, Hao},\n  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},\n  year      =  {2020}\n}\n\n@article{CondInst2022Tian,\n  title   = {Instance and Panoptic Segmentation Using Conditional Convolutions},\n  author  = {Tian, Zhi and Zhang, Bowen and Chen, Hao and Shen, Chunhua},\n  journal = {IEEE T. Pattern Analysis and Machine Intelligence (TPAMI)},\n  year    = {2022}\n}\n\n@inproceedings{tian2021boxinst,\n  title     =  {{BoxInst}: High-Performance Instance Segmentation with Box Annotations},\n  author    =  {Tian, Zhi and Shen, Chunhua and Wang, Xinlong and Chen, Hao},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2021}\n}\n\n@inproceedings{wang2021densecl,\n  title     =   {Dense Contrastive Learning for Self-Supervised Visual Pre-Training},\n  author    =   {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},\n  booktitle =   {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =   {2021}\n}\n\n@inproceedings{Mao2021pose,\n  title     =   {{FCPose}: Fully Convolutional Multi-Person Pose Estimation With Dynamic Instance-Aware Convolutions},\n  author    =   {Mao, Weian and  Tian, Zhi  and Wang, Xinlong  and Shen, Chunhua},\n  booktitle =   {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =   {2021}\n}\n```\n\n## License\n\nFor academic use, this project is licensed under the 2-clause BSD License - see the LICENSE file for details. For commercial use, please contact [Chunhua Shen](mailto:chhshen@gmail.com).\n"
        },
        {
          "name": "adet",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "onnx",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.7138671875,
          "content": "#!/usr/bin/env python\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\nimport glob\nimport os\nfrom setuptools import find_packages, setup\nimport torch\nfrom torch.utils.cpp_extension import CUDA_HOME, CppExtension, CUDAExtension\n\ntorch_ver = [int(x) for x in torch.__version__.split(\".\")[:2]]\nassert torch_ver >= [1, 3], \"Requires PyTorch >= 1.3\"\n\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, \"adet\", \"layers\", \"csrc\")\n\n    main_source = os.path.join(extensions_dir, \"vision.cpp\")\n    sources = glob.glob(os.path.join(extensions_dir, \"**\", \"*.cpp\"))\n    source_cuda = glob.glob(os.path.join(extensions_dir, \"**\", \"*.cu\")) + glob.glob(\n        os.path.join(extensions_dir, \"*.cu\")\n    )\n\n    sources = [main_source] + sources\n\n    extension = CppExtension\n\n    extra_compile_args = {\"cxx\": []}\n    define_macros = []\n\n    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(\"WITH_CUDA\", None)]\n        extra_compile_args[\"nvcc\"] = [\n            \"-DCUDA_HAS_FP16=1\",\n            \"-D__CUDA_NO_HALF_OPERATORS__\",\n            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n        ]\n\n        if torch_ver < [1, 7]:\n            # supported by https://github.com/pytorch/pytorch/pull/43931\n            CC = os.environ.get(\"CC\", None)\n            if CC is not None:\n                extra_compile_args[\"nvcc\"].append(\"-ccbin={}\".format(CC))\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            \"adet._C\",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=\"AdelaiDet\",\n    version=\"0.2.0\",\n    author=\"Adelaide Intelligent Machines\",\n    url=\"https://github.com/stanstarks/AdelaiDet\",\n    description=\"AdelaiDet is AIM's research \"\n    \"platform for instance-level detection tasks based on Detectron2.\",\n    packages=find_packages(exclude=(\"configs\", \"tests\")),\n    python_requires=\">=3.6\",\n    install_requires=[\n        \"termcolor>=1.1\",\n        \"Pillow>=6.0\",\n        \"yacs>=0.1.6\",\n        \"tabulate\",\n        \"cloudpickle\",\n        \"matplotlib\",\n        \"tqdm>4.29.0\",\n        \"tensorboard\",\n        \"rapidfuzz\",\n        \"Polygon3\",\n        \"shapely\",\n        \"scikit-image\",\n        \"editdistance\"\n    ],\n    extras_require={\"all\": [\"psutil\"]},\n    ext_modules=get_extensions(),\n    cmdclass={\"build_ext\": torch.utils.cpp_extension.BuildExtension},\n)\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}