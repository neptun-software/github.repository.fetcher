{
  "metadata": {
    "timestamp": 1736559858676,
    "page": 611,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hiyouga/ChatGLM-Efficient-Tuning",
      "stars": 3681,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.064453125,
          "content": "# Auto detect text files and perform LF normalization\n* text=auto\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.05859375,
          "content": "Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 19.228515625,
          "content": "# ChatGLM Efficient Tuning\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/commits/main)\n[![PyPI](https://img.shields.io/pypi/v/glmtuner)](https://pypi.org/project/glmtuner/)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/pulls)\n\nFine-tuning 🤖[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) model with 🤗[PEFT](https://github.com/huggingface/peft).\n\n👋 Join our [WeChat](assets/wechat.jpg).\n\n\\[ English | [中文](README_zh.md) \\]\n\nIf you have any questions, please refer to our [Wiki📄](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki).\n\n## Notice\n\nThis repo will **not be maintained** in the future. Please follow **[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)** for fine-tuning the language models (including ChatGLM2-6B).\n\n## Changelog\n\n[23/07/15] Now we develop an all-in-one Web UI for training, evaluation and inference. Try `train_web.py` to fine-tune ChatGLM-6B model in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] Now we release [FastEdit](https://github.com/hiyouga/FastEdit)⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/25] Now we align the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in arbitrary ChatGPT-based applications.\n\n[23/06/25] Now we support fine-tuning the [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) model with our framework!\n\n[23/06/05] Now we support 4-bit LoRA training (aka [QLoRA](https://github.com/artidoro/qlora)). Try `--quantization_bit 4` argument to work with 4-bit quantized model. (experimental feature)\n\n[23/06/01] We implemented a framework supporting the efficient tuning of LLaMA and BLOOM models. Please follow [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) if you are interested.\n\n[23/05/19] Now we support using the development set to evaluate the model while training. Try `--dev_ratio` argument to specify the size of development set.\n\n[23/04/29] Now we support training ChatGLM with **Reinforcement Learning with Human Feedback (RLHF)** ! We provide several examples to run RLHF training, please refer to the `examples` folder for details.\n\n[23/04/20] Our repo achieved 100 stars within 12 days! Congratulations!\n\n[23/04/19] Now we support **merging the weights** of fine-tuned models trained by LoRA! Try `--checkpoint_dir checkpoint1,checkpoint2` argument for continually fine-tuning the models.\n\n[23/04/18] Now we support training the **quantized models** using three fine-tuning methods! Try `quantization_bit` argument for training the model in 4/8 bits.\n\n[23/04/12] Now we support **training from checkpoints**! Use `--checkpoint_dir` argument to specify the checkpoint model to fine-tune from.\n\n[23/04/11] Now we support training with **combined datasets**! Try `--dataset dataset1,dataset2` argument for training with multiple datasets.\n\n## Datasets\n\n- For supervised fine-tuning:\n  - [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n  - [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n  - [Self-cognition (zh)](data/self_cognition.json)\n  - [ShareGPT (zh)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection)\n  - [RefGPT (zh)](https://github.com/sufengniu/RefGPT)\n  - [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n  - [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n  - [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n  - [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n  - [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n  - [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n  - [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n  - [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n  - [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n  - [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n  - [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n  - [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n  - [UltraChat (en)](https://github.com/thunlp/UltraChat)\n  - [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- For reward modelling:\n  - [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n\nPlease refer to [data/README.md](data/README.md) for details.\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## Fine-Tuning Methods\n\nOur script now supports the following fine-tuning methods:\n\n- [LoRA](https://arxiv.org/abs/2106.09685)\n  - Fine-tuning the low-rank adapters of the model.\n- [P-Tuning V2](https://github.com/THUDM/P-tuning-v2)\n  - Fine-tuning the prefix encoder of the model.\n- [Freeze](https://arxiv.org/abs/2012.14913)\n  - Fine-tuning the MLPs in the last n blocks of the model.\n- Full Tuning\n  - Fine-tuning all the parameters of the model.\n\n## Requirement\n\n- Python 3.8+ and PyTorch 1.13.1+\n- 🤗Transformers, Datasets, Accelerate, PEFT and TRL\n- fire, protobuf, cpm-kernels and sentencepiece\n- jieba, rouge-chinese and nltk (used at evaluation)\n- gradio and matplotlib (used in train_web.py)\n- uvicorn, fastapi and sse-starlette (used in api_demo.py)\n\nAnd **powerful GPUs**!\n\n## Getting Started\n\n### Data Preparation (optional)\n\nPlease refer to `data/example_dataset` for checking the details about the format of dataset files. You can either use a single `.json` file or a [dataset loading script](https://huggingface.co/docs/datasets/dataset_script) with multiple files to create a custom dataset.\n\nNote: please update `data/dataset_info.json` to use your custom dataset. About the format of this file, please refer to `data/README.md`.\n\n### Dependence Installation (optional)\n\n```bash\ngit lfs install\ngit clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git\nconda create -n chatglm_etuning python=3.10\nconda activate chatglm_etuning\ncd ChatGLM-Efficient-Tuning\npip install -r requirements.txt\n```\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you will be required to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.1.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl\n```\n\n### All-in-one Web UI\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_web.py\n```\n\nCurrently the web UI only supports training on **a single GPU**.\n\n### Fine-tuning with a Single GPU\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --finetuning_type lora \\\n    --output_dir path_to_sft_checkpoint \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 3.0 \\\n    --plot_loss \\\n    --fp16\n```\n\nPlease refer to our [Wiki](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki) about the details of the arguments.\n\n### Distributed Fine-tuning with Multiple GPUs\n\n```bash\naccelerate config # configure the environment\naccelerate launch src/train_bash.py # arguments (same as above)\n```\n\n### Training Reward Model\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage rm \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset comparison_gpt4_en \\\n    --finetuning_type lora \\\n    --resume_lora_training False \\\n    --checkpoint_dir path_to_sft_checkpoint \\\n    --output_dir path_to_rm_checkpoint \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 1.0 \\\n    --plot_loss \\\n    --fp16\n```\n\n### Training with RLHF\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage ppo \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --finetuning_type lora \\\n    --resume_lora_training False \\\n    --checkpoint_dir path_to_sft_checkpoint \\\n    --reward_model path_to_rm_checkpoint \\\n    --output_dir path_to_ppo_checkpoint \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 1.0 \\\n    --plot_loss\n```\n\n### Evaluation (BLEU and ROUGE_CHINESE)\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_eval \\\n    --dataset alpaca_gpt4_en \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_eval_result \\\n    --per_device_eval_batch_size 8 \\\n    --max_samples 50 \\\n    --predict_with_generate\n```\n\n### Predict\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_predict \\\n    --dataset alpaca_gpt4_en \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_predict_result \\\n    --per_device_eval_batch_size 8 \\\n    --max_samples 100 \\\n    --predict_with_generate\n```\n\nIf you want to predict the samples with empty responses, please kindly fill the `response` column with **dummy tokens** to ensure the sample will not be discarded throughout the preprocessing phase.\n\n### API Demo\n\n```bash\npython src/api_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\nVisit `http://localhost:8000/docs` for API documentation.\n\n### CLI Demo\n\n```bash\npython src/cli_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n### Web Demo\n\n```bash\npython src/web_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n### Export model\n\n```bash\npython src/export_model.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_export\n```\n\n### Hardware Requirements\n\n| Fine-tune method | Batch size | Mode |  GRAM  | Speed |\n| ---------------- | ---------- | ---- | ------ | ----- |\n| LoRA (r=8)       |     16     | FP16 |  28GB  | 8ex/s |\n| LoRA (r=8)       |     8      | FP16 |  24GB  | 8ex/s |\n| LoRA (r=8)       |     4      | FP16 |  20GB  | 8ex/s |\n| LoRA (r=8)       |     4      | INT8 |  10GB  | 8ex/s |\n| LoRA (r=8)       |     4      | INT4 |   8GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | FP16 |  20GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | INT8 |  16GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | INT4 |  12GB  | 8ex/s |\n| Freeze (l=3)     |     4      | FP16 |  24GB  | 8ex/s |\n\n| RM  method       | Batch size | Mode |  GRAM  | Speed |\n| ---------------- | ---------- | ---- | ------ | ----- |\n| LoRA (r=8) + rm  |     4      | FP16 |  22GB  | -     |\n| LoRA (r=8) + rm  |     1      | INT8 |  11GB  | -     |\n\n| RLHF method      | Batch size | Mode |  GRAM  | Speed |\n| ---------------- | ---------- | ---- | ------ | ----- |\n| LoRA (r=8) + ppo |     4      | FP16 |  23GB  | -     |\n| LoRA (r=8) + ppo |     1      | INT8 |  12GB  | -     |\n\n> Note: `r` is the lora rank, `p` is the number of prefix tokens, `l` is the number of trainable layers, `ex/s` is the examples per second at training. The `gradient_accumulation_steps` is set to `1`. All are evaluated on a single Tesla V100 (32G) GPU, they are approximated values and may vary in different GPUs.\n\n## Fine-tuning ChatGLM: A Case\n\n### Training Results\n\nWe use the whole `alpaca_gpt4_zh` dataset to fine-tune the ChatGLM model with LoRA (r=8) for one epoch, using the default hyper-parameters. The loss curve during training is presented below.\n\n![training loss](assets/trainer_state.jpg)\n\n### Evaluation Results\n\nWe select 100 instances in the `alpaca_gpt4_zh` dataset to evaluate the fine-tuned ChatGLM model and compute the BLEU and ROUGE scores. The results are presented below.\n\n|   Score   | Original | FZ (l=2) | PT (p=16) | LoRA (r=8) |\n| --------- | -------- | ----- | ----- | ----------------- |\n| BLEU-4    |  15.75   | 16.85 | 16.06 | 17.01 (**+1.26**) |\n| Rouge-1   |  34.51   | 36.62 | 34.80 | 36.77 (**+2.26**) |\n| Rouge-2   |  15.11   | 17.04 | 15.32 | 16.83 (**+1.72**) |\n| Rouge-l   |  26.18   | 28.17 | 26.35 | 28.86 (**+2.68**) |\n| Params (%)|  /       | 4.35% | 0.06% | 0.06%             |\n\n> FZ: freeze tuning, PT: P-Tuning V2 (we use `pre_seq_len=16` for fair comparison with LoRA), Params: the percentange of trainable parameters.\n\n## Projects\n\n- [SupritYoung/RLHF-Label-Tool](https://github.com/SupritYoung/RLHF-Label-Tool/tree/master): A tool for ranking the responses of LLMs to generate annotated samples used in RLHF training.\n\n## Compared with Existing Implementations\n\n- [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning)\n  - Official implementation of fine-tuning ChatGLM with [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) on the [ADGEN](https://aclanthology.org/D19-1321.pdf) dataset.\n  - Our fine-tuning script is largely depend on it. We further implement the [LoRA](https://arxiv.org/abs/2106.09685) tuning method. Additionally, we **dynamically** pad the inputs to the longest sequence in the batch instead of the maximum length, to accelerate the fine-tuning.\n- [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)\n  - An unoffical implementation of fine-tuning ChatGLM with [LoRA](https://arxiv.org/abs/2106.09685) on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset.\n  - We borrowed some ideas from it. Our fine-tuning script **integrates** the data pre-processing part into the training procedure, so we need not generate a pre-processed dataset before training.\n- [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning)\n  - An unofficial implementation of fine-tuning ChatGLM with several PEFT methods on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset.\n  - Our fine-tuning script is implemented **purely** with [Hugging Face transformers](https://github.com/huggingface/transformers) and is independent of the [deep_training](https://github.com/ssbuild/deep_training) framework.\n- [lich99/ChatGLM-finetune-LoRA](https://github.com/lich99/ChatGLM-finetune-LoRA)\n  - An unofficial implementation of fine-tuning ChatGLM with [LoRA](https://arxiv.org/abs/2106.09685) on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset.\n  - We use the [Hugging Face PEFT](https://github.com/huggingface/peft) to provide the state-of-the-art PEFT methods.\n- [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning)\n  - An unofficial implementation of fine-tuning ChatGLM with several methods including Freeze, LoRA and P-Tuning on the industrial dataset.\n  - We are aim to incorporate more instruction-following datasets for fine-tuning the ChatGLM model.\n- [yanqiangmiffy/InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)\n  - An unofficial implementation of fine-tuning ChatGLM that explores the ChatGLM's ability on the instruction-following datasets.\n  - Our fine-tuning script integrates the data pre-processing part in to the training procedure.\n\n## TODO\n\n- [ ] Employing [LangChain](https://github.com/hwchase17/langchain) to easily build applications that are capable of leveraging external knowledge upon fine-tuned ChatGLM models.\n- [ ] Implementing the alignment algorithms to align human preferrences.\n  - [x] [RLHF](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)\n  - [ ] [RRHF](https://github.com/GanjinZero/RRHF)\n  - [ ] [RAFT](https://github.com/OptimalScale/LMFlow)\n- [ ] Incorporating [Chinese datasets](https://github.com/brightmart/nlp_chinese_corpus) into the training sets.\n  - [x] [BELLE](https://github.com/LianjiaTech/BELLE)\n  - [ ] [pCLUE](https://github.com/CLUEbenchmark/pCLUE)\n  - [ ] [CLUECorpus](https://github.com/CLUEbenchmark/CLUECorpus2020)\n  - [x] [GuanacoDataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n  - [x] [FireflyDataset](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [ ] Incorporating [ChatGPT](https://openai.com/blog/chatgpt) & [GPT-4](https://openai.com/research/gpt-4) self-chat data into the training sets.\n  - [ ] [Baize](https://github.com/project-baize/baize-chatbot)\n  - [x] [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [x] Implementing the Freeze-Tuning and P-Tuning method.\n- [x] Supporting Multi-GPUs fine-tuning.\n- [x] Adding script for evaluation.\n- [x] Loading from checkpoint.\n- [x] Fine-tuning the quantized model.\n- [x] Writing a guidebook about how to fine-tune ChatGLM with this framework.\n- [ ] Combining with state-of-the-art model editing algorithms. (*e.g. [MEND](https://arxiv.org/abs/2110.11309)*)\n- [x] Incorporating the [OpenAssistant Conversations Dataset](https://huggingface.co/datasets/OpenAssistant/oasst1) for SFT and alignment.\n- [ ] Incorporating the high quality Chinese instruction dataset [COIG](https://huggingface.co/datasets/BAAI/COIG).\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE). Please follow the [Model License](https://github.com/THUDM/ChatGLM-6B/blob/main/MODEL_LICENSE) to use ChatGLM-6B model.\n\n## Citation\n\nIf this work is helpful, please cite as:\n\n```bibtex\n@Misc{chatglm-efficient-tuning,\n  title = {ChatGLM Efficient Tuning},\n  author = {hiyouga},\n  howpublished = {\\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},\n  year = {2023}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), [ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) and [yuanzhoulvpi2017/zero_nlp](https://github.com/yuanzhoulvpi2017/zero_nlp). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/ChatGLM-Efficient-Tuning&type=Date)\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 18.9052734375,
          "content": "# ChatGLM Efficient Tuning\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/commits/main)\n[![PyPI](https://img.shields.io/pypi/v/glmtuner)](https://pypi.org/project/glmtuner/)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/pulls)\n\n基于 🤗[PEFT](https://github.com/huggingface/peft) 的高效 🤖[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) 微调。\n\n👋 加入我们的[微信群](assets/wechat.jpg)。\n\n\\[ [English](README.md) | 中文 \\]\n\n如果有任何疑问，请阅读我们的 [文档 📄](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki)。\n\n## 公告\n\n该项目今后**将不再维护**。请关注 **[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)** 大模型微调项目（包括 ChatGLM2-6B 模型）。\n\n## 更新日志\n\n[23/07/15] 我们开发了支持训练和测试的浏览器一键微调界面。请尝试使用 `train_web.py` 在您的浏览器中微调 ChatGLM-6B 模型。感谢 [@KanadeSiina](https://github.com/KanadeSiina) 和 [@codemayq](https://github.com/codemayq) 在该功能开发中付出的努力。\n\n[23/07/09] 我们开源了 [FastEdit](https://github.com/hiyouga/FastEdit)⚡🩹，一个简单易用的、能迅速编辑大模型事实记忆的工具包。如果您感兴趣请关注我们的 [FastEdit](https://github.com/hiyouga/FastEdit) 项目。\n\n[23/06/25] 我们对齐了[示例 API](src/api_demo.py) 与 [OpenAI API](https://platform.openai.com/docs/api-reference/chat) 的格式，您可以将微调模型接入任意基于 ChatGPT 的应用中。\n\n[23/06/25] 现在我们实现了 [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) 模型的微调。\n\n[23/06/05] 现在我们实现了 4 比特的 LoRA 训练（也称 [QLoRA](https://github.com/artidoro/qlora)）。请尝试使用 `--quantization_bit 4` 参数进行 4 比特量化微调。（实验性功能）\n\n[23/06/01] 我们开源了支持 LLaMA 和 BLOOM 系列模型的高效微调框架，如果您感兴趣请关注我们的 [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) 项目。\n\n[23/06/01] 我们新增了一个使用监督微调和 RLHF 训练医疗问答模型的例子，请移步 [covid_doctor.md](examples/covid_doctor.md) 查阅。\n\n[23/05/19] 现在我们支持了在模型训练时使用验证集评估性能。请尝试使用 `--dev_ratio` 参数指定验证集大小。\n\n[23/04/29] 现在我们实现了 **RLHF（基于人类反馈的强化学习）** 训练！我们提供了几个运行 RLHF 的例子，具体内容请移步 `examples` 文件夹。\n\n[23/04/25] 我们新增了一个使用自定义数据集分布式训练的例子，请移步 [ads_generation.md](examples/ads_generation.md) 查阅。\n\n[23/04/20] 我们的项目在 12 天内获得了 100 个 Star！祝贺！\n\n[23/04/20] 我们新增了一个修改模型自我认知的例子，请移步 [alter_self_cognition.md](examples/alter_self_cognition.md) 查阅。\n\n[23/04/19] 现在我们实现了**模型融合**！请尝试使用 `--checkpoint_dir checkpoint1,checkpoint2` 参数训练融合 LoRA 权重后的模型。\n\n[23/04/18] 现在可以微调**量化模型**了！请尝试使用 `quantization_bit` 参数进行 4 比特或 8 比特量化微调。\n\n[23/04/12] 现在我们加入了**断点训练支持**！请尝试给定 `--checkpoint_dir` 参数加载指定的模型断点。\n\n[23/04/11] 现在我们实现了**数据集组合训练**！请尝试使用 `--dataset dataset1,dataset2` 参数进行组合训练。\n\n## 数据集\n\n- SFT 训练：\n  - [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n  - [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n  - [Self-cognition (zh)](data/self_cognition.json)\n  - [ShareGPT (zh)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection)\n  - [RefGPT (zh)](https://github.com/sufengniu/RefGPT)\n  - [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n  - [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n  - [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n  - [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n  - [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n  - [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n  - [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n  - [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n  - [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n  - [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n  - [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n  - [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n  - [UltraChat (en)](https://github.com/thunlp/UltraChat)\n  - [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- 奖励模型训练：\n  - [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n\n使用方法请参考 [data/README.md](data/README_zh.md) 文件。\n\n部分数据集的使用需要确认，我们推荐使用下述命令登录您的 Hugging Face 账户。\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## 微调方法\n\n目前我们实现了针对以下高效微调方法的支持：\n\n- [LoRA](https://arxiv.org/abs/2106.09685)\n  - 仅微调低秩适应器。\n- [P-Tuning V2](https://github.com/THUDM/P-tuning-v2)\n  - 仅微调前缀编码器。\n- [Freeze Tuning](https://arxiv.org/abs/2012.14913)\n  - 仅微调后几层的全连接层。\n- 全量微调\n  - 微调模型所有参数。\n\n## 软件依赖\n\n- Python 3.8+, PyTorch 1.13.1\n- 🤗Transformers, Datasets, Accelerate, PEFT, TRL\n- protobuf, cpm-kernels, sentencepiece\n- jieba, rouge-chinese, nltk（用于评估）\n- gradio, matplotlib（用于网页端交互）\n- uvicorn, fastapi, sse-starlette（用于 API）\n\n以及 **强而有力的 GPU**！\n\n## 如何使用\n\n### 数据准备（可跳过）\n\n关于数据集文件的格式，请参考 `data/example_dataset` 文件夹的内容。构建自定义数据集时，既可以使用单个 `.json` 文件，也可以使用一个[数据加载脚本](https://huggingface.co/docs/datasets/dataset_script)和多个文件。\n\n注意：使用自定义数据集时，请更新 `data/dataset_info.json` 文件，该文件的格式请参考 `data/README.md`。\n\n### 环境搭建（可跳过）\n\n```bash\ngit lfs install\ngit clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git\nconda create -n chatglm_etuning python=3.10\nconda activate chatglm_etuning\ncd ChatGLM-Efficient-Tuning\npip install -r requirements.txt\n```\n\n如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 `bitsandbytes` 库, 支持 CUDA 11.1 到 12.1.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl\n```\n\n### 浏览器一键微调/测试\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_web.py\n```\n\n目前网页 UI 仅支持**单卡训练**。\n\n### 单 GPU 微调训练\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset alpaca_gpt4_zh \\\n    --finetuning_type lora \\\n    --output_dir path_to_sft_checkpoint \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 3.0 \\\n    --plot_loss \\\n    --fp16\n```\n\n关于参数信息，请查阅我们的[维基](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki)。\n\n### 多 GPU 分布式微调\n\n```bash\naccelerate config # 首先配置分布式环境\naccelerate launch src/train_bash.py # 参数同上\n```\n\n### 奖励模型训练\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage rm \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset comparison_gpt4_zh \\\n    --finetuning_type lora \\\n    --resume_lora_training False \\\n    --checkpoint_dir path_to_sft_checkpoint \\\n    --output_dir path_to_rm_checkpoint \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 1.0 \\\n    --plot_loss \\\n    --fp16\n```\n\n### RLHF 训练\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage ppo \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset alpaca_gpt4_zh \\\n    --finetuning_type lora \\\n    --resume_lora_training False \\\n    --checkpoint_dir path_to_sft_checkpoint \\\n    --reward_model path_to_rm_checkpoint \\\n    --output_dir path_to_ppo_checkpoint \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 1.0 \\\n    --plot_loss\n```\n\n### 指标评估（BLEU分数和汉语ROUGE分数）\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_eval \\\n    --dataset alpaca_gpt4_zh \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_eval_result \\\n    --per_device_eval_batch_size 8 \\\n    --max_samples 50 \\\n    --predict_with_generate\n```\n\n### 模型预测\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_predict \\\n    --dataset alpaca_gpt4_zh \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_predict_result \\\n    --per_device_eval_batch_size 8 \\\n    --max_samples 50 \\\n    --predict_with_generate\n```\n\n注：如果需要预测的样本没有标签，请首先在 `response` 列中填入一些占位符，以免样本在预处理阶段被丢弃。\n\n### API 服务\n\n```bash\npython src/api_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n关于 API 文档请见 `http://localhost:8000/docs`。\n\n### 命令行测试\n\n```bash\npython src/cli_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n### 浏览器测试\n\n```bash\npython src/web_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n### 导出微调模型\n\n```bash\npython src/export_model.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_export\n```\n\n### 硬件需求\n\n|     微调方法     |  批处理大小  | 模式 | GPU显存 | 速度 |\n| ---------------- | ---------- | ---- | ------ | ----- |\n| LoRA (r=8)       |     16     | FP16 |  28GB  | 8ex/s |\n| LoRA (r=8)       |     8      | FP16 |  24GB  | 8ex/s |\n| LoRA (r=8)       |     4      | FP16 |  20GB  | 8ex/s |\n| LoRA (r=8)       |     4      | INT8 |  10GB  | 8ex/s |\n| LoRA (r=8)       |     4      | INT4 |   8GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | FP16 |  20GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | INT8 |  16GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | INT4 |  12GB  | 8ex/s |\n| Freeze (l=3)     |     4      | FP16 |  24GB  | 8ex/s |\n\n| 奖励模型训练方法 |  批处理大小  | 模式 | GPU显存 | 速度 |\n| --------------- | ----------  | ---- | ------ | ---- |\n| LoRA (r=8) + rm |      4      | FP16 |  22GB  | -    |\n| LoRA (r=8) + rm |      1      | INT8 |  11GB  | -    |\n\n|   RLHF 训练方法   |  批处理大小  | 模式 | GPU显存 | 速度 |\n| ---------------- | ----------  | ---- | ------ | ---- |\n| LoRA (r=8) + ppo |      4      | FP16 |  23GB  | -    |\n| LoRA (r=8) + ppo |      1      | INT8 |  12GB  | -    |\n\n> 注：`r` 为LoRA 维数大小，`p` 为前缀词表大小，`l` 为微调层数，`ex/s` 为每秒训练的样本数。`gradient_accumulation_steps` 参数设置为 `1`。上述结果均来自于单个 Tesla V100 GPU，仅供参考。\n\n## 微调 ChatGLM 的例子\n\n### 训练结果\n\n我们使用整个 `alpaca_gpt4_zh` 数据集微调 ChatGLM 模型，使用秩为 8 的 LoRA 方法，使用默认超参数进行单轮训练。下图为训练损失变化曲线。\n\n![训练损失](assets/trainer_state.jpg)\n\n### 评估结果\n\n我们选择 `alpaca_gpt4_zh` 数据集中的前一百条数据来评估微调后的 ChatGLM 模型，并计算 BLEU 和中文 ROUGE 分数。下表为评估结果。\n\n|   分数  |  原版模型 | FZ (l=2) | PT (p=16) | LoRA (r=8) |\n| ------- | -------- | ----- | ----- | ----------------- |\n| BLEU-4  |  15.75   | 16.85 | 16.06 | 17.01 (**+1.26**) |\n| Rouge-1 |  34.51   | 36.62 | 34.80 | 36.77 (**+2.26**) |\n| Rouge-2 |  15.11   | 17.04 | 15.32 | 16.83 (**+1.72**) |\n| Rouge-l |  26.18   | 28.17 | 26.35 | 28.86 (**+2.68**) |\n| 训练参数 |  /       | 4.35% | 0.06% | 0.06%             |\n\n> FZ：Freeze 微调，PT：P-Tuning V2 微调（为了与 LoRA 公平比较，我们使用了 `pre_seq_len=16`），训练参数：可训练参数占全部参数的百分比。\n\n## 友情链接\n\n- [SupritYoung/RLHF-Label-Tool](https://github.com/SupritYoung/RLHF-Label-Tool/tree/master)：一个给大模型生成结果进行排序，从而获得用于 RLHF 训练的标注数据的平台。\n\n## 和现有类似项目的比较\n\n- [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning)\n  - ChatGLM 基于 [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) 微调的官方实现，使用了 [ADGEN](https://aclanthology.org/D19-1321.pdf) 数据集。\n  - 本仓库的代码实现绝大部分参考该项目。我们进一步实现了 [LoRA](https://arxiv.org/abs/2106.09685) 微调方法。此外，我们**动态地**将每个批处理数据中的序列进行填充，而非将其填充到模型的最大长度，此改进可以加速模型训练。\n- [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)\n  - ChatGLM 基于 [LoRA](https://arxiv.org/abs/2106.09685) 微调的非官方实现，使用了 [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) 数据集。\n  - 我们借鉴了该项目的一些想法。我们的训练脚本将数据预处理部分**集成**至训练脚本中，以避免事先生成预处理后的数据。\n- [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning)\n  - ChatGLM 基于多种微调方法的非官方实现，使用了 [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) 数据集。\n  - 我们的训练脚本**全部**基于 [Hugging Face transformers](https://github.com/huggingface/transformers) 框架实现，不依赖于额外的 [deep_training](https://github.com/ssbuild/deep_training) 框架。\n- [lich99/ChatGLM-finetune-LoRA](https://github.com/lich99/ChatGLM-finetune-LoRA)\n  - ChatGLM 基于 [LoRA](https://arxiv.org/abs/2106.09685) 微调的非官方实现，使用了 [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) 数据集。\n  - 我们利用 [Hugging Face PEFT](https://github.com/huggingface/peft) 框架来引入最先进的微调方法。\n- [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning)\n  - ChatGLM 基于参数冻结、LoRA 和 P-Tuning 微调的非官方实现，使用了汽车工业数据集。\n  - 我们旨在引入更多指令遵循数据集用于微调 ChatGLM 模型。\n- [yanqiangmiffy/InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)\n  - ChatGLM 微调的非官方实现，旨在探索 ChatGLM 在指令遵循数据集上的潜力。\n  - 我们将数据预处理部分集成到训练脚本中。\n\n## TODO\n\n- [ ] 利用 [LangChain](https://github.com/hwchase17/langchain) 实现能够利用外部知识的基于 ChatGLM 微调模型应用的轻松构建。\n- [ ] 实现对齐算法使模型对齐人类意图。\n  - [x] [RLHF](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)\n  - [ ] [RRHF](https://github.com/GanjinZero/RRHF)\n  - [ ] [RAFT](https://github.com/OptimalScale/LMFlow)\n- [ ] 加入更多[中文数据集](https://github.com/brightmart/nlp_chinese_corpus)。\n  - [x] [BELLE](https://github.com/LianjiaTech/BELLE)\n  - [ ] [pCLUE](https://github.com/CLUEbenchmark/pCLUE)\n  - [ ] [CLUECorpus](https://github.com/CLUEbenchmark/CLUECorpus2020)\n  - [x] [GuanacoDataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n  - [x] [FireflyDataset](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [ ] 加入基于 [ChatGPT](https://openai.com/blog/chatgpt) 和 [GPT-4](https://openai.com/research/gpt-4) 产生的数据集。\n  - [ ] [Baize](https://github.com/project-baize/baize-chatbot)\n  - [x] [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [x] 实现参数冻结和 P-Tuning 微调方法。\n- [x] 支持多GPU训练。\n- [x] 加入模型评估脚本。\n- [x] 断点加载。\n- [x] 量化微调。\n- [x] 撰写基于该框架的 ChatGLM 模型微调指南手册。\n- [ ] 结合模型编辑技术。（例如：[MEND](https://arxiv.org/abs/2110.11309)）\n- [x] 加入 [OpenAssistant 对话数据集](https://huggingface.co/datasets/OpenAssistant/oasst1)用于监督微调和意图对齐。\n- [ ] 加入高质量中文开源指令数据集 [COIG](https://huggingface.co/datasets/BAAI/COIG)。\n\n## 协议\n\n本仓库的代码依照 [Apache-2.0](LICENSE) 协议开源。ChatGLM-6B 模型的使用请遵循[模型协议](https://github.com/THUDM/ChatGLM-6B/blob/main/MODEL_LICENSE)。\n\n## 引用\n\n如果您觉得此项目有帮助，请考虑以下列格式引用\n\n```bibtex\n@Misc{chatglm-efficient-tuning,\n  title = {ChatGLM Efficient Tuning},\n  author = {hiyouga},\n  howpublished = {\\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},\n  year = {2023}\n}\n```\n\n## 声明\n\n本项目受益于 [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)、[ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) 和 [yuanzhoulvpi2017/zero_nlp](https://github.com/yuanzhoulvpi2017/zero_nlp)，感谢以上诸位作者的付出。\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/ChatGLM-Efficient-Tuning&type=Date)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.0849609375,
          "content": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.23046875,
          "content": "torch>=1.13.1\ntransformers>=4.29.1\ndatasets>=2.12.0\naccelerate>=0.21.0\npeft>=0.4.0\ntrl>=0.4.7\nsentencepiece\njieba\nrouge-chinese\nnltk\ngradio>=3.36.0\nuvicorn\npydantic==1.10.11\nfastapi==0.95.1\nsse-starlette\nmatplotlib\nprotobuf\ncpm-kernels\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.9052734375,
          "content": "import os\nimport re\nfrom setuptools import setup, find_packages\n\n\ndef get_version():\n    with open(os.path.join(\"src\", \"glmtuner\", \"__init__.py\"), \"r\", encoding=\"utf-8\") as f:\n        file_content = f.read()\n        pattern = r\"{0}\\W*=\\W*\\\"([^\\\"]+)\\\"\".format(\"__version__\")\n        version, = re.findall(pattern, file_content)\n        return version\n\n\ndef get_requires():\n    with open(\"requirements.txt\", \"r\", encoding=\"utf-8\") as f:\n        file_content = f.read()\n        lines = [line.strip() for line in file_content.strip().split(\"\\n\") if not line.startswith(\"#\")]\n        return lines\n\n\ndef main():\n\n    setup(\n        name=\"glmtuner\",\n        version=get_version(),\n        author=\"hiyouga\",\n        author_email=\"hiyouga\" \"@\" \"buaa.edu.cn\",\n        description=\"Fine-tuning ChatGLM-6B with PEFT\",\n        long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n        long_description_content_type=\"text/markdown\",\n        keywords=[\"ChatGLM\", \"LLM\", \"ChatGPT\", \"transformer\", \"pytorch\", \"deep learning\"],\n        license=\"Apache 2.0 License\",\n        url=\"https://github.com/hiyouga/ChatGLM-Efficient-Tuning\",\n        package_dir={\"\": \"src\"},\n        packages=find_packages(\"src\"),\n        python_requires=\">=3.8.0\",\n        install_requires=get_requires(),\n        classifiers=[\n            \"Development Status :: 3 - Alpha\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Education\",\n            \"Intended Audience :: Science/Research\",\n            \"License :: OSI Approved :: Apache Software License\",\n            \"Operating System :: OS Independent\",\n            \"Programming Language :: Python :: 3\",\n            \"Programming Language :: Python :: 3.8\",\n            \"Programming Language :: Python :: 3.9\",\n            \"Programming Language :: Python :: 3.10\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        ]\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}