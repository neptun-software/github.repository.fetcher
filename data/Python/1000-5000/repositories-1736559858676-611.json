{
  "metadata": {
    "timestamp": 1736559858676,
    "page": 611,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hiyouga/ChatGLM-Efficient-Tuning",
      "stars": 3681,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.064453125,
          "content": "# Auto detect text files and perform LF normalization\n* text=auto\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.05859375,
          "content": "Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 19.228515625,
          "content": "# ChatGLM Efficient Tuning\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/commits/main)\n[![PyPI](https://img.shields.io/pypi/v/glmtuner)](https://pypi.org/project/glmtuner/)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/pulls)\n\nFine-tuning ğŸ¤–[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) model with ğŸ¤—[PEFT](https://github.com/huggingface/peft).\n\nğŸ‘‹ Join our [WeChat](assets/wechat.jpg).\n\n\\[ English | [ä¸­æ–‡](README_zh.md) \\]\n\nIf you have any questions, please refer to our [WikiğŸ“„](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki).\n\n## Notice\n\nThis repo will **not be maintained** in the future. Please follow **[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)** for fine-tuning the language models (including ChatGLM2-6B).\n\n## Changelog\n\n[23/07/15] Now we develop an all-in-one Web UI for training, evaluation and inference. Try `train_web.py` to fine-tune ChatGLM-6B model in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] Now we release [FastEdit](https://github.com/hiyouga/FastEdit)âš¡ğŸ©¹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/25] Now we align the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in arbitrary ChatGPT-based applications.\n\n[23/06/25] Now we support fine-tuning the [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) model with our framework!\n\n[23/06/05] Now we support 4-bit LoRA training (aka [QLoRA](https://github.com/artidoro/qlora)). Try `--quantization_bit 4` argument to work with 4-bit quantized model. (experimental feature)\n\n[23/06/01] We implemented a framework supporting the efficient tuning of LLaMA and BLOOM models. Please follow [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) if you are interested.\n\n[23/05/19] Now we support using the development set to evaluate the model while training. Try `--dev_ratio` argument to specify the size of development set.\n\n[23/04/29] Now we support training ChatGLM with **Reinforcement Learning with Human Feedback (RLHF)** ! We provide several examples to run RLHF training, please refer to the `examples` folder for details.\n\n[23/04/20] Our repo achieved 100 stars within 12 days! Congratulations!\n\n[23/04/19] Now we support **merging the weights** of fine-tuned models trained by LoRA! Try `--checkpoint_dir checkpoint1,checkpoint2` argument for continually fine-tuning the models.\n\n[23/04/18] Now we support training the **quantized models** using three fine-tuning methods! Try `quantization_bit` argument for training the model in 4/8 bits.\n\n[23/04/12] Now we support **training from checkpoints**! Use `--checkpoint_dir` argument to specify the checkpoint model to fine-tune from.\n\n[23/04/11] Now we support training with **combined datasets**! Try `--dataset dataset1,dataset2` argument for training with multiple datasets.\n\n## Datasets\n\n- For supervised fine-tuning:\n  - [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n  - [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n  - [Self-cognition (zh)](data/self_cognition.json)\n  - [ShareGPT (zh)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection)\n  - [RefGPT (zh)](https://github.com/sufengniu/RefGPT)\n  - [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n  - [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n  - [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n  - [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n  - [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n  - [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n  - [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n  - [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n  - [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n  - [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n  - [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n  - [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n  - [UltraChat (en)](https://github.com/thunlp/UltraChat)\n  - [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- For reward modelling:\n  - [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n\nPlease refer to [data/README.md](data/README.md) for details.\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## Fine-Tuning Methods\n\nOur script now supports the following fine-tuning methods:\n\n- [LoRA](https://arxiv.org/abs/2106.09685)\n  - Fine-tuning the low-rank adapters of the model.\n- [P-Tuning V2](https://github.com/THUDM/P-tuning-v2)\n  - Fine-tuning the prefix encoder of the model.\n- [Freeze](https://arxiv.org/abs/2012.14913)\n  - Fine-tuning the MLPs in the last n blocks of the model.\n- Full Tuning\n  - Fine-tuning all the parameters of the model.\n\n## Requirement\n\n- Python 3.8+ and PyTorch 1.13.1+\n- ğŸ¤—Transformers, Datasets, Accelerate, PEFT and TRL\n- fire, protobuf, cpm-kernels and sentencepiece\n- jieba, rouge-chinese and nltk (used at evaluation)\n- gradio and matplotlib (used in train_web.py)\n- uvicorn, fastapi and sse-starlette (used in api_demo.py)\n\nAnd **powerful GPUs**!\n\n## Getting Started\n\n### Data Preparation (optional)\n\nPlease refer to `data/example_dataset` for checking the details about the format of dataset files. You can either use a single `.json` file or a [dataset loading script](https://huggingface.co/docs/datasets/dataset_script) with multiple files to create a custom dataset.\n\nNote: please update `data/dataset_info.json` to use your custom dataset. About the format of this file, please refer to `data/README.md`.\n\n### Dependence Installation (optional)\n\n```bash\ngit lfs install\ngit clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git\nconda create -n chatglm_etuning python=3.10\nconda activate chatglm_etuning\ncd ChatGLM-Efficient-Tuning\npip install -r requirements.txt\n```\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you will be required to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.1.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl\n```\n\n### All-in-one Web UI\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_web.py\n```\n\nCurrently the web UI only supports training on **a single GPU**.\n\n### Fine-tuning with a Single GPU\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --finetuning_type lora \\\n    --output_dir path_to_sft_checkpoint \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 3.0 \\\n    --plot_loss \\\n    --fp16\n```\n\nPlease refer to our [Wiki](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki) about the details of the arguments.\n\n### Distributed Fine-tuning with Multiple GPUs\n\n```bash\naccelerate config # configure the environment\naccelerate launch src/train_bash.py # arguments (same as above)\n```\n\n### Training Reward Model\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage rm \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset comparison_gpt4_en \\\n    --finetuning_type lora \\\n    --resume_lora_training False \\\n    --checkpoint_dir path_to_sft_checkpoint \\\n    --output_dir path_to_rm_checkpoint \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 1.0 \\\n    --plot_loss \\\n    --fp16\n```\n\n### Training with RLHF\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage ppo \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset alpaca_gpt4_en \\\n    --finetuning_type lora \\\n    --resume_lora_training False \\\n    --checkpoint_dir path_to_sft_checkpoint \\\n    --reward_model path_to_rm_checkpoint \\\n    --output_dir path_to_ppo_checkpoint \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 1.0 \\\n    --plot_loss\n```\n\n### Evaluation (BLEU and ROUGE_CHINESE)\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_eval \\\n    --dataset alpaca_gpt4_en \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_eval_result \\\n    --per_device_eval_batch_size 8 \\\n    --max_samples 50 \\\n    --predict_with_generate\n```\n\n### Predict\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_predict \\\n    --dataset alpaca_gpt4_en \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_predict_result \\\n    --per_device_eval_batch_size 8 \\\n    --max_samples 100 \\\n    --predict_with_generate\n```\n\nIf you want to predict the samples with empty responses, please kindly fill the `response` column with **dummy tokens** to ensure the sample will not be discarded throughout the preprocessing phase.\n\n### API Demo\n\n```bash\npython src/api_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\nVisit `http://localhost:8000/docs` for API documentation.\n\n### CLI Demo\n\n```bash\npython src/cli_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n### Web Demo\n\n```bash\npython src/web_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n### Export model\n\n```bash\npython src/export_model.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_export\n```\n\n### Hardware Requirements\n\n| Fine-tune method | Batch size | Mode |  GRAM  | Speed |\n| ---------------- | ---------- | ---- | ------ | ----- |\n| LoRA (r=8)       |     16     | FP16 |  28GB  | 8ex/s |\n| LoRA (r=8)       |     8      | FP16 |  24GB  | 8ex/s |\n| LoRA (r=8)       |     4      | FP16 |  20GB  | 8ex/s |\n| LoRA (r=8)       |     4      | INT8 |  10GB  | 8ex/s |\n| LoRA (r=8)       |     4      | INT4 |   8GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | FP16 |  20GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | INT8 |  16GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | INT4 |  12GB  | 8ex/s |\n| Freeze (l=3)     |     4      | FP16 |  24GB  | 8ex/s |\n\n| RM  method       | Batch size | Mode |  GRAM  | Speed |\n| ---------------- | ---------- | ---- | ------ | ----- |\n| LoRA (r=8) + rm  |     4      | FP16 |  22GB  | -     |\n| LoRA (r=8) + rm  |     1      | INT8 |  11GB  | -     |\n\n| RLHF method      | Batch size | Mode |  GRAM  | Speed |\n| ---------------- | ---------- | ---- | ------ | ----- |\n| LoRA (r=8) + ppo |     4      | FP16 |  23GB  | -     |\n| LoRA (r=8) + ppo |     1      | INT8 |  12GB  | -     |\n\n> Note: `r` is the lora rank, `p` is the number of prefix tokens, `l` is the number of trainable layers, `ex/s` is the examples per second at training. The `gradient_accumulation_steps` is set to `1`. All are evaluated on a single Tesla V100 (32G) GPU, they are approximated values and may vary in different GPUs.\n\n## Fine-tuning ChatGLM: A Case\n\n### Training Results\n\nWe use the whole `alpaca_gpt4_zh` dataset to fine-tune the ChatGLM model with LoRA (r=8) for one epoch, using the default hyper-parameters. The loss curve during training is presented below.\n\n![training loss](assets/trainer_state.jpg)\n\n### Evaluation Results\n\nWe select 100 instances in the `alpaca_gpt4_zh` dataset to evaluate the fine-tuned ChatGLM model and compute the BLEU and ROUGE scores. The results are presented below.\n\n|   Score   | Original | FZ (l=2) | PT (p=16) | LoRA (r=8) |\n| --------- | -------- | ----- | ----- | ----------------- |\n| BLEU-4    |  15.75   | 16.85 | 16.06 | 17.01 (**+1.26**) |\n| Rouge-1   |  34.51   | 36.62 | 34.80 | 36.77 (**+2.26**) |\n| Rouge-2   |  15.11   | 17.04 | 15.32 | 16.83 (**+1.72**) |\n| Rouge-l   |  26.18   | 28.17 | 26.35 | 28.86 (**+2.68**) |\n| Params (%)|  /       | 4.35% | 0.06% | 0.06%             |\n\n> FZ: freeze tuning, PT: P-Tuning V2 (we use `pre_seq_len=16` for fair comparison with LoRA), Params: the percentange of trainable parameters.\n\n## Projects\n\n- [SupritYoung/RLHF-Label-Tool](https://github.com/SupritYoung/RLHF-Label-Tool/tree/master): A tool for ranking the responses of LLMs to generate annotated samples used in RLHF training.\n\n## Compared with Existing Implementations\n\n- [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning)\n  - Official implementation of fine-tuning ChatGLM with [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) on the [ADGEN](https://aclanthology.org/D19-1321.pdf) dataset.\n  - Our fine-tuning script is largely depend on it. We further implement the [LoRA](https://arxiv.org/abs/2106.09685) tuning method. Additionally, we **dynamically** pad the inputs to the longest sequence in the batch instead of the maximum length, to accelerate the fine-tuning.\n- [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)\n  - An unoffical implementation of fine-tuning ChatGLM with [LoRA](https://arxiv.org/abs/2106.09685) on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset.\n  - We borrowed some ideas from it. Our fine-tuning script **integrates** the data pre-processing part into the training procedure, so we need not generate a pre-processed dataset before training.\n- [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning)\n  - An unofficial implementation of fine-tuning ChatGLM with several PEFT methods on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset.\n  - Our fine-tuning script is implemented **purely** with [Hugging Face transformers](https://github.com/huggingface/transformers) and is independent of the [deep_training](https://github.com/ssbuild/deep_training) framework.\n- [lich99/ChatGLM-finetune-LoRA](https://github.com/lich99/ChatGLM-finetune-LoRA)\n  - An unofficial implementation of fine-tuning ChatGLM with [LoRA](https://arxiv.org/abs/2106.09685) on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset.\n  - We use the [Hugging Face PEFT](https://github.com/huggingface/peft) to provide the state-of-the-art PEFT methods.\n- [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning)\n  - An unofficial implementation of fine-tuning ChatGLM with several methods including Freeze, LoRA and P-Tuning on the industrial dataset.\n  - We are aim to incorporate more instruction-following datasets for fine-tuning the ChatGLM model.\n- [yanqiangmiffy/InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)\n  - An unofficial implementation of fine-tuning ChatGLM that explores the ChatGLM's ability on the instruction-following datasets.\n  - Our fine-tuning script integrates the data pre-processing part in to the training procedure.\n\n## TODO\n\n- [ ] Employing [LangChain](https://github.com/hwchase17/langchain) to easily build applications that are capable of leveraging external knowledge upon fine-tuned ChatGLM models.\n- [ ] Implementing the alignment algorithms to align human preferrences.\n  - [x] [RLHF](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)\n  - [ ] [RRHF](https://github.com/GanjinZero/RRHF)\n  - [ ] [RAFT](https://github.com/OptimalScale/LMFlow)\n- [ ] Incorporating [Chinese datasets](https://github.com/brightmart/nlp_chinese_corpus) into the training sets.\n  - [x] [BELLE](https://github.com/LianjiaTech/BELLE)\n  - [ ] [pCLUE](https://github.com/CLUEbenchmark/pCLUE)\n  - [ ] [CLUECorpus](https://github.com/CLUEbenchmark/CLUECorpus2020)\n  - [x] [GuanacoDataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n  - [x] [FireflyDataset](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [ ] Incorporating [ChatGPT](https://openai.com/blog/chatgpt) & [GPT-4](https://openai.com/research/gpt-4) self-chat data into the training sets.\n  - [ ] [Baize](https://github.com/project-baize/baize-chatbot)\n  - [x] [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [x] Implementing the Freeze-Tuning and P-Tuning method.\n- [x] Supporting Multi-GPUs fine-tuning.\n- [x] Adding script for evaluation.\n- [x] Loading from checkpoint.\n- [x] Fine-tuning the quantized model.\n- [x] Writing a guidebook about how to fine-tune ChatGLM with this framework.\n- [ ] Combining with state-of-the-art model editing algorithms. (*e.g. [MEND](https://arxiv.org/abs/2110.11309)*)\n- [x] Incorporating the [OpenAssistant Conversations Dataset](https://huggingface.co/datasets/OpenAssistant/oasst1) for SFT and alignment.\n- [ ] Incorporating the high quality Chinese instruction dataset [COIG](https://huggingface.co/datasets/BAAI/COIG).\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE). Please follow the [Model License](https://github.com/THUDM/ChatGLM-6B/blob/main/MODEL_LICENSE) to use ChatGLM-6B model.\n\n## Citation\n\nIf this work is helpful, please cite as:\n\n```bibtex\n@Misc{chatglm-efficient-tuning,\n  title = {ChatGLM Efficient Tuning},\n  author = {hiyouga},\n  howpublished = {\\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},\n  year = {2023}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), [ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) and [yuanzhoulvpi2017/zero_nlp](https://github.com/yuanzhoulvpi2017/zero_nlp). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/ChatGLM-Efficient-Tuning&type=Date)\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 18.9052734375,
          "content": "# ChatGLM Efficient Tuning\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/commits/main)\n[![PyPI](https://img.shields.io/pypi/v/glmtuner)](https://pypi.org/project/glmtuner/)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/pulls)\n\nåŸºäº ğŸ¤—[PEFT](https://github.com/huggingface/peft) çš„é«˜æ•ˆ ğŸ¤–[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) å¾®è°ƒã€‚\n\nğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„[å¾®ä¿¡ç¾¤](assets/wechat.jpg)ã€‚\n\n\\[ [English](README.md) | ä¸­æ–‡ \\]\n\nå¦‚æœæœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„ [æ–‡æ¡£ ğŸ“„](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki)ã€‚\n\n## å…¬å‘Š\n\nè¯¥é¡¹ç›®ä»Šå**å°†ä¸å†ç»´æŠ¤**ã€‚è¯·å…³æ³¨ **[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)** å¤§æ¨¡å‹å¾®è°ƒé¡¹ç›®ï¼ˆåŒ…æ‹¬ ChatGLM2-6B æ¨¡å‹ï¼‰ã€‚\n\n## æ›´æ–°æ—¥å¿—\n\n[23/07/15] æˆ‘ä»¬å¼€å‘äº†æ”¯æŒè®­ç»ƒå’Œæµ‹è¯•çš„æµè§ˆå™¨ä¸€é”®å¾®è°ƒç•Œé¢ã€‚è¯·å°è¯•ä½¿ç”¨ `train_web.py` åœ¨æ‚¨çš„æµè§ˆå™¨ä¸­å¾®è°ƒ ChatGLM-6B æ¨¡å‹ã€‚æ„Ÿè°¢ [@KanadeSiina](https://github.com/KanadeSiina) å’Œ [@codemayq](https://github.com/codemayq) åœ¨è¯¥åŠŸèƒ½å¼€å‘ä¸­ä»˜å‡ºçš„åŠªåŠ›ã€‚\n\n[23/07/09] æˆ‘ä»¬å¼€æºäº† [FastEdit](https://github.com/hiyouga/FastEdit)âš¡ğŸ©¹ï¼Œä¸€ä¸ªç®€å•æ˜“ç”¨çš„ã€èƒ½è¿…é€Ÿç¼–è¾‘å¤§æ¨¡å‹äº‹å®è®°å¿†çš„å·¥å…·åŒ…ã€‚å¦‚æœæ‚¨æ„Ÿå…´è¶£è¯·å…³æ³¨æˆ‘ä»¬çš„ [FastEdit](https://github.com/hiyouga/FastEdit) é¡¹ç›®ã€‚\n\n[23/06/25] æˆ‘ä»¬å¯¹é½äº†[ç¤ºä¾‹ API](src/api_demo.py) ä¸ [OpenAI API](https://platform.openai.com/docs/api-reference/chat) çš„æ ¼å¼ï¼Œæ‚¨å¯ä»¥å°†å¾®è°ƒæ¨¡å‹æ¥å…¥ä»»æ„åŸºäº ChatGPT çš„åº”ç”¨ä¸­ã€‚\n\n[23/06/25] ç°åœ¨æˆ‘ä»¬å®ç°äº† [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) æ¨¡å‹çš„å¾®è°ƒã€‚\n\n[23/06/05] ç°åœ¨æˆ‘ä»¬å®ç°äº† 4 æ¯”ç‰¹çš„ LoRA è®­ç»ƒï¼ˆä¹Ÿç§° [QLoRA](https://github.com/artidoro/qlora)ï¼‰ã€‚è¯·å°è¯•ä½¿ç”¨ `--quantization_bit 4` å‚æ•°è¿›è¡Œ 4 æ¯”ç‰¹é‡åŒ–å¾®è°ƒã€‚ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰\n\n[23/06/01] æˆ‘ä»¬å¼€æºäº†æ”¯æŒ LLaMA å’Œ BLOOM ç³»åˆ—æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œå¦‚æœæ‚¨æ„Ÿå…´è¶£è¯·å…³æ³¨æˆ‘ä»¬çš„ [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) é¡¹ç›®ã€‚\n\n[23/06/01] æˆ‘ä»¬æ–°å¢äº†ä¸€ä¸ªä½¿ç”¨ç›‘ç£å¾®è°ƒå’Œ RLHF è®­ç»ƒåŒ»ç–—é—®ç­”æ¨¡å‹çš„ä¾‹å­ï¼Œè¯·ç§»æ­¥ [covid_doctor.md](examples/covid_doctor.md) æŸ¥é˜…ã€‚\n\n[23/05/19] ç°åœ¨æˆ‘ä»¬æ”¯æŒäº†åœ¨æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨éªŒè¯é›†è¯„ä¼°æ€§èƒ½ã€‚è¯·å°è¯•ä½¿ç”¨ `--dev_ratio` å‚æ•°æŒ‡å®šéªŒè¯é›†å¤§å°ã€‚\n\n[23/04/29] ç°åœ¨æˆ‘ä»¬å®ç°äº† **RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰** è®­ç»ƒï¼æˆ‘ä»¬æä¾›äº†å‡ ä¸ªè¿è¡Œ RLHF çš„ä¾‹å­ï¼Œå…·ä½“å†…å®¹è¯·ç§»æ­¥ `examples` æ–‡ä»¶å¤¹ã€‚\n\n[23/04/25] æˆ‘ä»¬æ–°å¢äº†ä¸€ä¸ªä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†åˆ†å¸ƒå¼è®­ç»ƒçš„ä¾‹å­ï¼Œè¯·ç§»æ­¥ [ads_generation.md](examples/ads_generation.md) æŸ¥é˜…ã€‚\n\n[23/04/20] æˆ‘ä»¬çš„é¡¹ç›®åœ¨ 12 å¤©å†…è·å¾—äº† 100 ä¸ª Starï¼ç¥è´ºï¼\n\n[23/04/20] æˆ‘ä»¬æ–°å¢äº†ä¸€ä¸ªä¿®æ”¹æ¨¡å‹è‡ªæˆ‘è®¤çŸ¥çš„ä¾‹å­ï¼Œè¯·ç§»æ­¥ [alter_self_cognition.md](examples/alter_self_cognition.md) æŸ¥é˜…ã€‚\n\n[23/04/19] ç°åœ¨æˆ‘ä»¬å®ç°äº†**æ¨¡å‹èåˆ**ï¼è¯·å°è¯•ä½¿ç”¨ `--checkpoint_dir checkpoint1,checkpoint2` å‚æ•°è®­ç»ƒèåˆ LoRA æƒé‡åçš„æ¨¡å‹ã€‚\n\n[23/04/18] ç°åœ¨å¯ä»¥å¾®è°ƒ**é‡åŒ–æ¨¡å‹**äº†ï¼è¯·å°è¯•ä½¿ç”¨ `quantization_bit` å‚æ•°è¿›è¡Œ 4 æ¯”ç‰¹æˆ– 8 æ¯”ç‰¹é‡åŒ–å¾®è°ƒã€‚\n\n[23/04/12] ç°åœ¨æˆ‘ä»¬åŠ å…¥äº†**æ–­ç‚¹è®­ç»ƒæ”¯æŒ**ï¼è¯·å°è¯•ç»™å®š `--checkpoint_dir` å‚æ•°åŠ è½½æŒ‡å®šçš„æ¨¡å‹æ–­ç‚¹ã€‚\n\n[23/04/11] ç°åœ¨æˆ‘ä»¬å®ç°äº†**æ•°æ®é›†ç»„åˆè®­ç»ƒ**ï¼è¯·å°è¯•ä½¿ç”¨ `--dataset dataset1,dataset2` å‚æ•°è¿›è¡Œç»„åˆè®­ç»ƒã€‚\n\n## æ•°æ®é›†\n\n- SFT è®­ç»ƒï¼š\n  - [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n  - [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n  - [Self-cognition (zh)](data/self_cognition.json)\n  - [ShareGPT (zh)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection)\n  - [RefGPT (zh)](https://github.com/sufengniu/RefGPT)\n  - [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n  - [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n  - [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n  - [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n  - [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n  - [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n  - [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n  - [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n  - [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n  - [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n  - [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n  - [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n  - [UltraChat (en)](https://github.com/thunlp/UltraChat)\n  - [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼š\n  - [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n\nä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ [data/README.md](data/README_zh.md) æ–‡ä»¶ã€‚\n\néƒ¨åˆ†æ•°æ®é›†çš„ä½¿ç”¨éœ€è¦ç¡®è®¤ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ä¸‹è¿°å‘½ä»¤ç™»å½•æ‚¨çš„ Hugging Face è´¦æˆ·ã€‚\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## å¾®è°ƒæ–¹æ³•\n\nç›®å‰æˆ‘ä»¬å®ç°äº†é’ˆå¯¹ä»¥ä¸‹é«˜æ•ˆå¾®è°ƒæ–¹æ³•çš„æ”¯æŒï¼š\n\n- [LoRA](https://arxiv.org/abs/2106.09685)\n  - ä»…å¾®è°ƒä½ç§©é€‚åº”å™¨ã€‚\n- [P-Tuning V2](https://github.com/THUDM/P-tuning-v2)\n  - ä»…å¾®è°ƒå‰ç¼€ç¼–ç å™¨ã€‚\n- [Freeze Tuning](https://arxiv.org/abs/2012.14913)\n  - ä»…å¾®è°ƒåå‡ å±‚çš„å…¨è¿æ¥å±‚ã€‚\n- å…¨é‡å¾®è°ƒ\n  - å¾®è°ƒæ¨¡å‹æ‰€æœ‰å‚æ•°ã€‚\n\n## è½¯ä»¶ä¾èµ–\n\n- Python 3.8+, PyTorch 1.13.1\n- ğŸ¤—Transformers, Datasets, Accelerate, PEFT, TRL\n- protobuf, cpm-kernels, sentencepiece\n- jieba, rouge-chinese, nltkï¼ˆç”¨äºè¯„ä¼°ï¼‰\n- gradio, matplotlibï¼ˆç”¨äºç½‘é¡µç«¯äº¤äº’ï¼‰\n- uvicorn, fastapi, sse-starletteï¼ˆç”¨äº APIï¼‰\n\nä»¥åŠ **å¼ºè€Œæœ‰åŠ›çš„ GPU**ï¼\n\n## å¦‚ä½•ä½¿ç”¨\n\n### æ•°æ®å‡†å¤‡ï¼ˆå¯è·³è¿‡ï¼‰\n\nå…³äºæ•°æ®é›†æ–‡ä»¶çš„æ ¼å¼ï¼Œè¯·å‚è€ƒ `data/example_dataset` æ–‡ä»¶å¤¹çš„å†…å®¹ã€‚æ„å»ºè‡ªå®šä¹‰æ•°æ®é›†æ—¶ï¼Œæ—¢å¯ä»¥ä½¿ç”¨å•ä¸ª `.json` æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€ä¸ª[æ•°æ®åŠ è½½è„šæœ¬](https://huggingface.co/docs/datasets/dataset_script)å’Œå¤šä¸ªæ–‡ä»¶ã€‚\n\næ³¨æ„ï¼šä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†æ—¶ï¼Œè¯·æ›´æ–° `data/dataset_info.json` æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶çš„æ ¼å¼è¯·å‚è€ƒ `data/README.md`ã€‚\n\n### ç¯å¢ƒæ­å»ºï¼ˆå¯è·³è¿‡ï¼‰\n\n```bash\ngit lfs install\ngit clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git\nconda create -n chatglm_etuning python=3.10\nconda activate chatglm_etuning\ncd ChatGLM-Efficient-Tuning\npip install -r requirements.txt\n```\n\nå¦‚æœè¦åœ¨ Windows å¹³å°ä¸Šå¼€å¯é‡åŒ– LoRAï¼ˆQLoRAï¼‰ï¼Œéœ€è¦å®‰è£…é¢„ç¼–è¯‘çš„ `bitsandbytes` åº“, æ”¯æŒ CUDA 11.1 åˆ° 12.1.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl\n```\n\n### æµè§ˆå™¨ä¸€é”®å¾®è°ƒ/æµ‹è¯•\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_web.py\n```\n\nç›®å‰ç½‘é¡µ UI ä»…æ”¯æŒ**å•å¡è®­ç»ƒ**ã€‚\n\n### å• GPU å¾®è°ƒè®­ç»ƒ\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset alpaca_gpt4_zh \\\n    --finetuning_type lora \\\n    --output_dir path_to_sft_checkpoint \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 3.0 \\\n    --plot_loss \\\n    --fp16\n```\n\nå…³äºå‚æ•°ä¿¡æ¯ï¼Œè¯·æŸ¥é˜…æˆ‘ä»¬çš„[ç»´åŸº](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki)ã€‚\n\n### å¤š GPU åˆ†å¸ƒå¼å¾®è°ƒ\n\n```bash\naccelerate config # é¦–å…ˆé…ç½®åˆ†å¸ƒå¼ç¯å¢ƒ\naccelerate launch src/train_bash.py # å‚æ•°åŒä¸Š\n```\n\n### å¥–åŠ±æ¨¡å‹è®­ç»ƒ\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage rm \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset comparison_gpt4_zh \\\n    --finetuning_type lora \\\n    --resume_lora_training False \\\n    --checkpoint_dir path_to_sft_checkpoint \\\n    --output_dir path_to_rm_checkpoint \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 1.0 \\\n    --plot_loss \\\n    --fp16\n```\n\n### RLHF è®­ç»ƒ\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage ppo \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_train \\\n    --dataset alpaca_gpt4_zh \\\n    --finetuning_type lora \\\n    --resume_lora_training False \\\n    --checkpoint_dir path_to_sft_checkpoint \\\n    --reward_model path_to_rm_checkpoint \\\n    --output_dir path_to_ppo_checkpoint \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate 1e-5 \\\n    --num_train_epochs 1.0 \\\n    --plot_loss\n```\n\n### æŒ‡æ ‡è¯„ä¼°ï¼ˆBLEUåˆ†æ•°å’Œæ±‰è¯­ROUGEåˆ†æ•°ï¼‰\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_eval \\\n    --dataset alpaca_gpt4_zh \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_eval_result \\\n    --per_device_eval_batch_size 8 \\\n    --max_samples 50 \\\n    --predict_with_generate\n```\n\n### æ¨¡å‹é¢„æµ‹\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage sft \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --do_predict \\\n    --dataset alpaca_gpt4_zh \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_predict_result \\\n    --per_device_eval_batch_size 8 \\\n    --max_samples 50 \\\n    --predict_with_generate\n```\n\næ³¨ï¼šå¦‚æœéœ€è¦é¢„æµ‹çš„æ ·æœ¬æ²¡æœ‰æ ‡ç­¾ï¼Œè¯·é¦–å…ˆåœ¨ `response` åˆ—ä¸­å¡«å…¥ä¸€äº›å ä½ç¬¦ï¼Œä»¥å…æ ·æœ¬åœ¨é¢„å¤„ç†é˜¶æ®µè¢«ä¸¢å¼ƒã€‚\n\n### API æœåŠ¡\n\n```bash\npython src/api_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\nå…³äº API æ–‡æ¡£è¯·è§ `http://localhost:8000/docs`ã€‚\n\n### å‘½ä»¤è¡Œæµ‹è¯•\n\n```bash\npython src/cli_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n### æµè§ˆå™¨æµ‹è¯•\n\n```bash\npython src/web_demo.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint\n```\n\n### å¯¼å‡ºå¾®è°ƒæ¨¡å‹\n\n```bash\npython src/export_model.py \\\n    --model_name_or_path path_to_your_chatglm_model \\\n    --finetuning_type lora \\\n    --checkpoint_dir path_to_checkpoint \\\n    --output_dir path_to_export\n```\n\n### ç¡¬ä»¶éœ€æ±‚\n\n|     å¾®è°ƒæ–¹æ³•     |  æ‰¹å¤„ç†å¤§å°  | æ¨¡å¼ | GPUæ˜¾å­˜ | é€Ÿåº¦ |\n| ---------------- | ---------- | ---- | ------ | ----- |\n| LoRA (r=8)       |     16     | FP16 |  28GB  | 8ex/s |\n| LoRA (r=8)       |     8      | FP16 |  24GB  | 8ex/s |\n| LoRA (r=8)       |     4      | FP16 |  20GB  | 8ex/s |\n| LoRA (r=8)       |     4      | INT8 |  10GB  | 8ex/s |\n| LoRA (r=8)       |     4      | INT4 |   8GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | FP16 |  20GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | INT8 |  16GB  | 8ex/s |\n| P-Tuning (p=16)  |     4      | INT4 |  12GB  | 8ex/s |\n| Freeze (l=3)     |     4      | FP16 |  24GB  | 8ex/s |\n\n| å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³• |  æ‰¹å¤„ç†å¤§å°  | æ¨¡å¼ | GPUæ˜¾å­˜ | é€Ÿåº¦ |\n| --------------- | ----------  | ---- | ------ | ---- |\n| LoRA (r=8) + rm |      4      | FP16 |  22GB  | -    |\n| LoRA (r=8) + rm |      1      | INT8 |  11GB  | -    |\n\n|   RLHF è®­ç»ƒæ–¹æ³•   |  æ‰¹å¤„ç†å¤§å°  | æ¨¡å¼ | GPUæ˜¾å­˜ | é€Ÿåº¦ |\n| ---------------- | ----------  | ---- | ------ | ---- |\n| LoRA (r=8) + ppo |      4      | FP16 |  23GB  | -    |\n| LoRA (r=8) + ppo |      1      | INT8 |  12GB  | -    |\n\n> æ³¨ï¼š`r` ä¸ºLoRA ç»´æ•°å¤§å°ï¼Œ`p` ä¸ºå‰ç¼€è¯è¡¨å¤§å°ï¼Œ`l` ä¸ºå¾®è°ƒå±‚æ•°ï¼Œ`ex/s` ä¸ºæ¯ç§’è®­ç»ƒçš„æ ·æœ¬æ•°ã€‚`gradient_accumulation_steps` å‚æ•°è®¾ç½®ä¸º `1`ã€‚ä¸Šè¿°ç»“æœå‡æ¥è‡ªäºå•ä¸ª Tesla V100 GPUï¼Œä»…ä¾›å‚è€ƒã€‚\n\n## å¾®è°ƒ ChatGLM çš„ä¾‹å­\n\n### è®­ç»ƒç»“æœ\n\næˆ‘ä»¬ä½¿ç”¨æ•´ä¸ª `alpaca_gpt4_zh` æ•°æ®é›†å¾®è°ƒ ChatGLM æ¨¡å‹ï¼Œä½¿ç”¨ç§©ä¸º 8 çš„ LoRA æ–¹æ³•ï¼Œä½¿ç”¨é»˜è®¤è¶…å‚æ•°è¿›è¡Œå•è½®è®­ç»ƒã€‚ä¸‹å›¾ä¸ºè®­ç»ƒæŸå¤±å˜åŒ–æ›²çº¿ã€‚\n\n![è®­ç»ƒæŸå¤±](assets/trainer_state.jpg)\n\n### è¯„ä¼°ç»“æœ\n\næˆ‘ä»¬é€‰æ‹© `alpaca_gpt4_zh` æ•°æ®é›†ä¸­çš„å‰ä¸€ç™¾æ¡æ•°æ®æ¥è¯„ä¼°å¾®è°ƒåçš„ ChatGLM æ¨¡å‹ï¼Œå¹¶è®¡ç®— BLEU å’Œä¸­æ–‡ ROUGE åˆ†æ•°ã€‚ä¸‹è¡¨ä¸ºè¯„ä¼°ç»“æœã€‚\n\n|   åˆ†æ•°  |  åŸç‰ˆæ¨¡å‹ | FZ (l=2) | PT (p=16) | LoRA (r=8) |\n| ------- | -------- | ----- | ----- | ----------------- |\n| BLEU-4  |  15.75   | 16.85 | 16.06 | 17.01 (**+1.26**) |\n| Rouge-1 |  34.51   | 36.62 | 34.80 | 36.77 (**+2.26**) |\n| Rouge-2 |  15.11   | 17.04 | 15.32 | 16.83 (**+1.72**) |\n| Rouge-l |  26.18   | 28.17 | 26.35 | 28.86 (**+2.68**) |\n| è®­ç»ƒå‚æ•° |  /       | 4.35% | 0.06% | 0.06%             |\n\n> FZï¼šFreeze å¾®è°ƒï¼ŒPTï¼šP-Tuning V2 å¾®è°ƒï¼ˆä¸ºäº†ä¸ LoRA å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `pre_seq_len=16`ï¼‰ï¼Œè®­ç»ƒå‚æ•°ï¼šå¯è®­ç»ƒå‚æ•°å å…¨éƒ¨å‚æ•°çš„ç™¾åˆ†æ¯”ã€‚\n\n## å‹æƒ…é“¾æ¥\n\n- [SupritYoung/RLHF-Label-Tool](https://github.com/SupritYoung/RLHF-Label-Tool/tree/master)ï¼šä¸€ä¸ªç»™å¤§æ¨¡å‹ç”Ÿæˆç»“æœè¿›è¡Œæ’åºï¼Œä»è€Œè·å¾—ç”¨äº RLHF è®­ç»ƒçš„æ ‡æ³¨æ•°æ®çš„å¹³å°ã€‚\n\n## å’Œç°æœ‰ç±»ä¼¼é¡¹ç›®çš„æ¯”è¾ƒ\n\n- [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning)\n  - ChatGLM åŸºäº [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) å¾®è°ƒçš„å®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [ADGEN](https://aclanthology.org/D19-1321.pdf) æ•°æ®é›†ã€‚\n  - æœ¬ä»“åº“çš„ä»£ç å®ç°ç»å¤§éƒ¨åˆ†å‚è€ƒè¯¥é¡¹ç›®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å®ç°äº† [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬**åŠ¨æ€åœ°**å°†æ¯ä¸ªæ‰¹å¤„ç†æ•°æ®ä¸­çš„åºåˆ—è¿›è¡Œå¡«å……ï¼Œè€Œéå°†å…¶å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼Œæ­¤æ”¹è¿›å¯ä»¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒã€‚\n- [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)\n  - ChatGLM åŸºäº [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚\n  - æˆ‘ä»¬å€Ÿé‰´äº†è¯¥é¡¹ç›®çš„ä¸€äº›æƒ³æ³•ã€‚æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬å°†æ•°æ®é¢„å¤„ç†éƒ¨åˆ†**é›†æˆ**è‡³è®­ç»ƒè„šæœ¬ä¸­ï¼Œä»¥é¿å…äº‹å…ˆç”Ÿæˆé¢„å¤„ç†åçš„æ•°æ®ã€‚\n- [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning)\n  - ChatGLM åŸºäºå¤šç§å¾®è°ƒæ–¹æ³•çš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚\n  - æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬**å…¨éƒ¨**åŸºäº [Hugging Face transformers](https://github.com/huggingface/transformers) æ¡†æ¶å®ç°ï¼Œä¸ä¾èµ–äºé¢å¤–çš„ [deep_training](https://github.com/ssbuild/deep_training) æ¡†æ¶ã€‚\n- [lich99/ChatGLM-finetune-LoRA](https://github.com/lich99/ChatGLM-finetune-LoRA)\n  - ChatGLM åŸºäº [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚\n  - æˆ‘ä»¬åˆ©ç”¨ [Hugging Face PEFT](https://github.com/huggingface/peft) æ¡†æ¶æ¥å¼•å…¥æœ€å…ˆè¿›çš„å¾®è°ƒæ–¹æ³•ã€‚\n- [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning)\n  - ChatGLM åŸºäºå‚æ•°å†»ç»“ã€LoRA å’Œ P-Tuning å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº†æ±½è½¦å·¥ä¸šæ•°æ®é›†ã€‚\n  - æˆ‘ä»¬æ—¨åœ¨å¼•å…¥æ›´å¤šæŒ‡ä»¤éµå¾ªæ•°æ®é›†ç”¨äºå¾®è°ƒ ChatGLM æ¨¡å‹ã€‚\n- [yanqiangmiffy/InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)\n  - ChatGLM å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œæ—¨åœ¨æ¢ç´¢ ChatGLM åœ¨æŒ‡ä»¤éµå¾ªæ•°æ®é›†ä¸Šçš„æ½œåŠ›ã€‚\n  - æˆ‘ä»¬å°†æ•°æ®é¢„å¤„ç†éƒ¨åˆ†é›†æˆåˆ°è®­ç»ƒè„šæœ¬ä¸­ã€‚\n\n## TODO\n\n- [ ] åˆ©ç”¨ [LangChain](https://github.com/hwchase17/langchain) å®ç°èƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†çš„åŸºäº ChatGLM å¾®è°ƒæ¨¡å‹åº”ç”¨çš„è½»æ¾æ„å»ºã€‚\n- [ ] å®ç°å¯¹é½ç®—æ³•ä½¿æ¨¡å‹å¯¹é½äººç±»æ„å›¾ã€‚\n  - [x] [RLHF](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)\n  - [ ] [RRHF](https://github.com/GanjinZero/RRHF)\n  - [ ] [RAFT](https://github.com/OptimalScale/LMFlow)\n- [ ] åŠ å…¥æ›´å¤š[ä¸­æ–‡æ•°æ®é›†](https://github.com/brightmart/nlp_chinese_corpus)ã€‚\n  - [x] [BELLE](https://github.com/LianjiaTech/BELLE)\n  - [ ] [pCLUE](https://github.com/CLUEbenchmark/pCLUE)\n  - [ ] [CLUECorpus](https://github.com/CLUEbenchmark/CLUECorpus2020)\n  - [x] [GuanacoDataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n  - [x] [FireflyDataset](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [ ] åŠ å…¥åŸºäº [ChatGPT](https://openai.com/blog/chatgpt) å’Œ [GPT-4](https://openai.com/research/gpt-4) äº§ç”Ÿçš„æ•°æ®é›†ã€‚\n  - [ ] [Baize](https://github.com/project-baize/baize-chatbot)\n  - [x] [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [x] å®ç°å‚æ•°å†»ç»“å’Œ P-Tuning å¾®è°ƒæ–¹æ³•ã€‚\n- [x] æ”¯æŒå¤šGPUè®­ç»ƒã€‚\n- [x] åŠ å…¥æ¨¡å‹è¯„ä¼°è„šæœ¬ã€‚\n- [x] æ–­ç‚¹åŠ è½½ã€‚\n- [x] é‡åŒ–å¾®è°ƒã€‚\n- [x] æ’°å†™åŸºäºè¯¥æ¡†æ¶çš„ ChatGLM æ¨¡å‹å¾®è°ƒæŒ‡å—æ‰‹å†Œã€‚\n- [ ] ç»“åˆæ¨¡å‹ç¼–è¾‘æŠ€æœ¯ã€‚ï¼ˆä¾‹å¦‚ï¼š[MEND](https://arxiv.org/abs/2110.11309)ï¼‰\n- [x] åŠ å…¥ [OpenAssistant å¯¹è¯æ•°æ®é›†](https://huggingface.co/datasets/OpenAssistant/oasst1)ç”¨äºç›‘ç£å¾®è°ƒå’Œæ„å›¾å¯¹é½ã€‚\n- [ ] åŠ å…¥é«˜è´¨é‡ä¸­æ–‡å¼€æºæŒ‡ä»¤æ•°æ®é›† [COIG](https://huggingface.co/datasets/BAAI/COIG)ã€‚\n\n## åè®®\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºã€‚ChatGLM-6B æ¨¡å‹çš„ä½¿ç”¨è¯·éµå¾ª[æ¨¡å‹åè®®](https://github.com/THUDM/ChatGLM-6B/blob/main/MODEL_LICENSE)ã€‚\n\n## å¼•ç”¨\n\nå¦‚æœæ‚¨è§‰å¾—æ­¤é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹åˆ—æ ¼å¼å¼•ç”¨\n\n```bibtex\n@Misc{chatglm-efficient-tuning,\n  title = {ChatGLM Efficient Tuning},\n  author = {hiyouga},\n  howpublished = {\\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},\n  year = {2023}\n}\n```\n\n## å£°æ˜\n\næœ¬é¡¹ç›®å—ç›Šäº [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ã€[ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) å’Œ [yuanzhoulvpi2017/zero_nlp](https://github.com/yuanzhoulvpi2017/zero_nlp)ï¼Œæ„Ÿè°¢ä»¥ä¸Šè¯¸ä½ä½œè€…çš„ä»˜å‡ºã€‚\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/ChatGLM-Efficient-Tuning&type=Date)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.0849609375,
          "content": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.23046875,
          "content": "torch>=1.13.1\ntransformers>=4.29.1\ndatasets>=2.12.0\naccelerate>=0.21.0\npeft>=0.4.0\ntrl>=0.4.7\nsentencepiece\njieba\nrouge-chinese\nnltk\ngradio>=3.36.0\nuvicorn\npydantic==1.10.11\nfastapi==0.95.1\nsse-starlette\nmatplotlib\nprotobuf\ncpm-kernels\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.9052734375,
          "content": "import os\nimport re\nfrom setuptools import setup, find_packages\n\n\ndef get_version():\n    with open(os.path.join(\"src\", \"glmtuner\", \"__init__.py\"), \"r\", encoding=\"utf-8\") as f:\n        file_content = f.read()\n        pattern = r\"{0}\\W*=\\W*\\\"([^\\\"]+)\\\"\".format(\"__version__\")\n        version, = re.findall(pattern, file_content)\n        return version\n\n\ndef get_requires():\n    with open(\"requirements.txt\", \"r\", encoding=\"utf-8\") as f:\n        file_content = f.read()\n        lines = [line.strip() for line in file_content.strip().split(\"\\n\") if not line.startswith(\"#\")]\n        return lines\n\n\ndef main():\n\n    setup(\n        name=\"glmtuner\",\n        version=get_version(),\n        author=\"hiyouga\",\n        author_email=\"hiyouga\" \"@\" \"buaa.edu.cn\",\n        description=\"Fine-tuning ChatGLM-6B with PEFT\",\n        long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n        long_description_content_type=\"text/markdown\",\n        keywords=[\"ChatGLM\", \"LLM\", \"ChatGPT\", \"transformer\", \"pytorch\", \"deep learning\"],\n        license=\"Apache 2.0 License\",\n        url=\"https://github.com/hiyouga/ChatGLM-Efficient-Tuning\",\n        package_dir={\"\": \"src\"},\n        packages=find_packages(\"src\"),\n        python_requires=\">=3.8.0\",\n        install_requires=get_requires(),\n        classifiers=[\n            \"Development Status :: 3 - Alpha\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Education\",\n            \"Intended Audience :: Science/Research\",\n            \"License :: OSI Approved :: Apache Software License\",\n            \"Operating System :: OS Independent\",\n            \"Programming Language :: Python :: 3\",\n            \"Programming Language :: Python :: 3.8\",\n            \"Programming Language :: Python :: 3.9\",\n            \"Programming Language :: Python :: 3.10\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        ]\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}