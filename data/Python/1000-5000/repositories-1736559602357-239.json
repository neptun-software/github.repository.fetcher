{
  "metadata": {
    "timestamp": 1736559602357,
    "page": 239,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "luosiallen/latent-consistency-model",
      "stars": 4419,
      "defaultBranch": "main",
      "files": [
        {
          "name": "LCM-LoRA Technical Report",
          "type": "tree",
          "content": null
        },
        {
          "name": "LCM_Training_Script",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2023 Simian Luo\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.2578125,
          "content": "# Latent Consistency Models\n\nOfficial Repository of the paper: [Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference](https://arxiv.org/abs/2310.04378).\n\nOfficial Repository of the paper: [LCM-LoRA: A Universal Stable-Diffusion Acceleration Module](https://arxiv.org/abs/2311.05556).\n\nProject Page: https://latent-consistency-models.github.io\n\n\n### Try our Demos:\n\nü§ó **Hugging Face Demo**: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model) üî•üî•üî•\n\n**Replicate Demo**: [![Replicate](https://replicate.com/cjwbw/latent-consistency-model/badge)](https://replicate.com/cjwbw/latent-consistency-model) \n\n**OpenXLab Demo**: [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/Latent-Consistency-Model/Latent-Consistency-Model)\n\n <img src=\"./lcm_logo.png\" width=\"4%\" alt=\"\" /> **LCM Community**: Join our LCM discord channels <a href=\"https://discord.gg/KM6aeW6CgD\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png\" width=\"3%\" alt=\"\" /></a> for discussions. Coders are welcome to contribute.\n\n## Breaking News üî•üî•!!\n- (ü§ñNew) 2023/12/1  **Pixart-Œ± X LCM** is out, a high quality image generative model. see [here](https://huggingface.co/spaces/PixArt-alpha/PixArt-LCM).\n- (‚ù§Ô∏èNew) 2023/11/10 **Training Scripts** are released!! Check [here](https://github.com/luosiallen/latent-consistency-model/tree/main/LCM_Training_Script/consistency_distillation). \n- (ü§ØNew) 2023/11/10 **Training-free acceleration LCM-LoRA** is born! See our technical report [here](https://arxiv.org/abs/2311.05556) and Hugging Face blog [here](https://huggingface.co/blog/lcm_lora).\n- (‚ö°Ô∏èNew) 2023/11/10 LCM has a major update! We release **3 LCM-LoRA (SD-XL, SSD-1B, SD-V1.5)**, see [here](https://huggingface.co/latent-consistency/lcm-lora-sdxl).\n- (üöÄNew) 2023/11/10 LCM has a major update! We release **2 Full Param-tuned LCM (SD-XL, SSD-1B)**,  see [here](https://huggingface.co/latent-consistency/lcm-sdxl).\n\n## News\n- (üî•New) 2023/11/10 We support LCM Inference with C# and ONNX Runtime now! Thanks to [@saddam213](https://github.com/saddam213)! Check the link [here](https://github.com/saddam213/OnnxStack).\n- (üî•New) 2023/11/01 **Real-Time Latent Consistency Models** is out!! Github link [here](https://github.com/radames/Real-Time-Latent-Consistency-Model). Thanks [@radames](https://github.com/radames) for the really cool Huggingfaceü§ó demo [Real-Time Image-to-Image](https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model), [Real-Time Text-to-Image](https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model-Text-To-Image). Twitter/X [Link](https://x.com/radamar/status/1718783886413709542?s=20).\n- (üî•New) 2023/10/28 We support **Img2Img** for LCM! Please refer to \"üî• Image2Image Demos\".\n- (üî•New) 2023/10/25 We have official [**LCM Pipeline**](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/latent_consistency_models) and [**LCM Scheduler**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lcm.py) in üß® Diffusers library now! Check the new \"Usage\".\n- (üî•New) 2023/10/24 Simple **Streamlit UI** for local use: See the [link](https://github.com/akx/lcm_test) Thanks for [@akx](https://github.com/akx).\n- (üî•New) 2023/10/24 We support **SD-Webui** and **ComfyUI** now!! Thanks for [@0xbitches](https://github.com/0xbitches). See the link: [SD-Webui](https://github.com/0xbitches/sd-webui-lcm) and [ComfyUI](https://github.com/0xbitches/ComfyUI-LCM). \n- (üî•New) 2023/10/23 Running on **Windows/Linux CPU** is also supported! Thanks for [@rupeshs](https://github.com/rupeshs) See the [link](https://github.com/rupeshs/fastsdcpu).\n- (üî•New) 2023/10/22 **Google Colab** is supported now. Thanks for [@camenduru](https://github.com/camenduru) See the link: [Colab](https://github.com/camenduru/latent-consistency-model-colab)\n- (üî•New) 2023/10/21 We support **local gradio demo** now. LCM can run locally!! Please refer to the \"**Local gradio Demos**\".\n- (üî•New) 2023/10/19 We provide a demo of LCM in ü§ó Hugging Face Space. Try it [here](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model).\n- (üî•New) 2023/10/19 We provide the LCM model (Dreamshaper_v7) in ü§ó Hugging Face. Download [here](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7).\n- (üî•New) 2023/10/19 LCM is integrated in üß® Diffusers library. Please refer to the \"Usage\".\n\n\n## üî• Image2Image Demos (Image-to-Image):\nWe support **Img2Img** now! Try the impressive img2img demos here: [Replicate](https://replicate.com/fofr/latent-consistency-model),   [SD-webui](https://github.com/0xbitches/sd-webui-lcm),  [ComfyUI](https://github.com/0xbitches/ComfyUI-LCM),  [Colab](https://github.com/camenduru/latent-consistency-model-colab/)\n\nLocal gradio for img2img is on the way!\n\n<p align=\"center\">\n    <img src=\"/img2img_demo/taylor.png\", width=\"50%\"><img src=\"/img2img_demo/elon.png\", width=\"49%\">\n</p>\n\n## üî• Local gradio Demos (Text-to-Image):\n\nTo run the model locally, you can download the \"local_gradio\" folder:\n1. Install Pytorch (CUDA). MacOS system can download the \"MPS\" version of Pytorch. Please refer to: [https://pytorch.org](https://pytorch.org). Install [Intel Extension for Pytorch](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/) as well if you're using Intel GPUs.\n2. Install the main library:\n```\npip install diffusers transformers accelerate gradio==3.48.0 \n```\n3. Launch the gradio: (For MacOS users, need to set the device=\"mps\" in app.py; For Intel GPU users, set `device=\"xpu\"` in app.py)\n```\npython app.py\n```\n\n## Demos & Models Released\nOurs Hugging Face Demo and Model are released ! Latent Consistency Models are supported in üß® [diffusers](https://github.com/huggingface/diffusers). \n\n**LCM Model Download**: [LCM_Dreamshaper_v7](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7)\n\nLCMÊ®°ÂûãÂ∑≤‰∏ä‰º†Âà∞ÂßãÊô∫AI(wisemodel)  ‰∏≠ÊñáÁî®Êà∑ÂèØÂú®Ê≠§‰∏ãËΩΩÔºå[‰∏ãËΩΩÈìæÊé•](https://www.wisemodel.cn/organization/Latent-Consistency-Model).\n\nFor Chinese users, download LCM here: (‰∏≠ÊñáÁî®Êà∑ÂèØ‰ª•Âú®Ê≠§‰∏ãËΩΩLCMÊ®°Âûã) [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/Latent-Consistency-Model/LCM_Dreamshaper_v7_4k.safetensors)\n\nHugging Face Demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model)\n\nReplicate Demo: [![Replicate](https://replicate.com/cjwbw/latent-consistency-model/badge)](https://replicate.com/cjwbw/latent-consistency-model) \n\nOpenXLab Demo: [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/Latent-Consistency-Model/Latent-Consistency-Model)\n\nTungsten Demo: [![Tungsten](https://tungsten.run/mjpyeon/lcm/_badge)](https://tungsten.run/mjpyeon/lcm)\n\nNovita.AI Demo:  [![Novita.AI Latent Consistency Playground](https://img.shields.io/badge/%20Novita.AI%20-Demo%20&%20API-blue)](https://novita.ai/product/lcm-txt2img)\n\n\n\n<p align=\"center\">\n    <img src=\"teaser.png\">\n</p>\n\nBy distilling classifier-free guidance into the model's input, LCM can generate high-quality images in very short inference time. We compare the inference time at the setting of 768 x 768 resolution, CFG scale w=8, batchsize=4, using a A800 GPU. \n\n<p align=\"center\">\n    <img src=\"speed_fid.png\">\n</p>\n\n\n\n## Usage\nWe have official [**LCM Pipeline**](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/latent_consistency_models) and [**LCM Scheduler**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lcm.py) in üß® Diffusers library now! The older usages will be deprecated.\n\nYou can try out Latency Consistency Models directly on:\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model)\n\nTo run the model yourself, you can leverage the üß® Diffusers library:\n1. Install the library:\n```\npip install --upgrade diffusers  # make sure to use at least diffusers >= 0.22\npip install transformers accelerate\n```\n\n2. Run the model:\n```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\")\n\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n# Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\nnum_inference_steps = 4 \n\nimages = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n```\n\nFor more information, please have a look at the official docs:\nüëâ https://huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models#latent-consistency-models\n\n\n## Usage (Deprecated)\nWe have official [**LCM Pipeline**](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/latent_consistency_models) and [**LCM Scheduler**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lcm.py) in üß® Diffusers library now! The older usages will be deprecated. But you can still use the older usages by adding ```revision=\"fb9c5d1\"``` from ```from_pretrained(...)``` \n\n\nTo run the model yourself, you can leverage the üß® Diffusers library:\n1. Install the library:\n```\npip install diffusers transformers accelerate\n```\n\n2. Run the model:\n```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", custom_pipeline=\"latent_consistency_txt2img\", custom_revision=\"main\", revision=\"fb9c5d\")\n\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n# Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\nnum_inference_steps = 4 \n\nimages = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n```\n\n### Our Contributors :\n<a href=\"https://github.com/luosiallen/latent-consistency-model/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=luosiallen/latent-consistency-model\" />\n</a>\n\n## BibTeX\n\n```bibtex\nLCM:\n@misc{luo2023latent,\n      title={Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference}, \n      author={Simian Luo and Yiqin Tan and Longbo Huang and Jian Li and Hang Zhao},\n      year={2023},\n      eprint={2310.04378},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\nLCM-LoRA:\n@article{luo2023lcm,\n  title={LCM-LoRA: A Universal Stable-Diffusion Acceleration Module},\n  author={Luo, Simian and Tan, Yiqin and Patil, Suraj and Gu, Daniel and von Platen, Patrick and Passos, Apolin{\\'a}rio and Huang, Longbo and Li, Jian and Zhao, Hang},\n  journal={arXiv preprint arXiv:2311.05556},\n  year={2023}\n}\n```\n"
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.435546875,
          "content": "# Configuration for Cog ‚öôÔ∏è\n# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md\n\nbuild:\n  gpu: true\n  system_packages:\n    - \"libgl1-mesa-glx\"\n    - \"libglib2.0-0\"\n  python_version: \"3.11\"\n  python_packages:\n    - \"accelerate==0.23.0\"\n    - \"torch==2.0.1\"\n    - \"torchvision==0.15.2\"\n    - \"diffusers==0.21.4\"\n    - \"Pillow==10.1.0\"\n    - \"transformers==4.34.1\"\n    - \"opencv-python==4.8.1.78\"\npredict: \"predict.py:Predictor\"\n"
        },
        {
          "name": "img2img_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "lcm_logo.png",
          "type": "blob",
          "size": 822.501953125,
          "content": null
        },
        {
          "name": "local_gradio",
          "type": "tree",
          "content": null
        },
        {
          "name": "predict.py",
          "type": "blob",
          "size": 2.9404296875,
          "content": "# Prediction interface for Cog ‚öôÔ∏è\n# https://github.com/replicate/cog/blob/main/docs/python.md\n\nimport os\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom cog import BasePredictor, Input, Path\n\n\nclass Predictor(BasePredictor):\n    def setup(self) -> None:\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n\n        # # Official LCM Pipeline supported now.\n        # self.pipe = DiffusionPipeline.from_pretrained(\n        #     \"SimianLuo/LCM_Dreamshaper_v7\",\n        #     cache_dir=\"model_cache\",\n        #     local_files_only=True,\n        # )\n\n        # Want to use older ones, need to add \"revision=\"fb9c5d1\"\n        self.pipe = DiffusionPipeline.from_pretrained(\n            \"SimianLuo/LCM_Dreamshaper_v7\",\n            custom_pipeline=\"latent_consistency_txt2img\",\n            custom_revision=\"main\",\n            revision=\"fb9c5d1\",\n            cache_dir=\"model_cache\",\n            local_files_only=True,\n        )\n        self.pipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\n    def predict(\n        self,\n        prompt: str = Input(\n            description=\"Input prompt\",\n            default=\"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\",\n        ),\n        width: int = Input(\n            description=\"Width of output image. Lower the setting if out of memory.\",\n            default=768,\n        ),\n        height: int = Input(\n            description=\"Height of output image. Lower the setting if out of memory.\",\n            default=768,\n        ),\n        num_images: int = Input(\n            description=\"Number of images to output.\",\n            ge=1,\n            le=4,\n            default=1,\n        ),\n        num_inference_steps: int = Input(\n            description=\"Number of denoising steps. Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\",\n            ge=1,\n            le=50,\n            default=8,\n        ),\n        guidance_scale: float = Input(\n            description=\"Scale for classifier-free guidance\", ge=1, le=20, default=8.0\n        ),\n        seed: int = Input(\n            description=\"Random seed. Leave blank to randomize the seed\", default=None\n        ),\n    ) -> list[Path]:\n        \"\"\"Run a single prediction on the model\"\"\"\n        if seed is None:\n            seed = int.from_bytes(os.urandom(2), \"big\")\n        print(f\"Using seed: {seed}\")\n        torch.manual_seed(seed)\n\n        result = self.pipe(\n            prompt=prompt,\n            width=width,\n            height=height,\n            guidance_scale=guidance_scale,\n            num_inference_steps=num_inference_steps,\n            num_images_per_prompt=num_images,\n            lcm_origin_steps=50,\n            output_type=\"pil\",\n        ).images\n\n        output_paths = []\n        for i, sample in enumerate(result):\n            output_path = f\"/tmp/out-{i}.png\"\n            sample.save(output_path)\n            output_paths.append(Path(output_path))\n\n        return output_paths\n"
        },
        {
          "name": "speed_fid.png",
          "type": "blob",
          "size": 55.6064453125,
          "content": null
        },
        {
          "name": "teaser.png",
          "type": "blob",
          "size": 3865.10546875,
          "content": null
        },
        {
          "name": "tungsten_model.py",
          "type": "blob",
          "size": 4.4873046875,
          "content": "\"\"\"\nTungsten model definition\nReference: https://github.com/tungsten-ai/tungstenkit\n\nBefore start building a model, download weights & pipeline definition:\n$ git lfs install\n$ git clone https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7\n\"\"\"\n\nimport os\nimport random\nimport sys\nfrom typing import List\n\nimport torch\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers.pipelines.stable_diffusion.safety_checker import (\n    StableDiffusionSafetyChecker,\n)\nfrom safetensors.torch import load_file\nfrom transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer\nfrom tungstenkit import BaseIO, Field, Image, Option, define_model\n\nMODEL_DIR = \"LCM_Dreamshaper_v7\"\nsys.path.append(\"LCM_Dreamshaper_v7\")\n\n\nfrom lcm_pipeline import LatentConsistencyModelPipeline\nfrom lcm_scheduler import LCMScheduler\n\n\nclass Input(BaseIO):\n    prompt: str = Field(description=\"Input prompt\")\n    image_dimensions: str = Option(\n        default=\"768x768\",\n        description=\"Pixel dimensions of output image (width x height)\",\n        choices=[\"512x512\", \"512x768\", \"768x512\", \"768x768\"],\n    )\n    num_output_images: int = Option(\n        description=\"Number of output images\",\n        le=4,\n        ge=1,\n        default=1,\n    )\n    seed: int = Option(\n        description=\"Random seed. Set as -1 to randomize the seed\",\n        default=-1,\n        ge=-1,\n        le=4294967293,\n    )\n    num_inference_steps: int = Option(\n        description=\"Number of denoising steps\", ge=1, le=50, default=4\n    )\n    guidence_scale: float = Option(\n        description=\"Scale for classifier-free guidance\", ge=1, le=20, default=8\n    )\n\n\nclass Output(BaseIO):\n    images: List[Image]\n\n\n@define_model(\n    input=Input,\n    output=Output,\n    gpu=True,\n    system_packages=[\n        \"libgl1-mesa-glx\",\n        \"libglib2.0-0\",\n    ],\n    python_packages=[\n        \"torch\",\n        \"torchvision\",\n        \"accelerate\",\n        \"diffusers==0.21.4\",\n        \"transformers==4.34.1\",\n        \"opencv-python\",\n    ],\n    batch_size=1,\n)\nclass LCMModel:\n    def setup(self):\n        \"\"\"Load model\"\"\"\n        # Initalize Diffusers Model:\n        vae = AutoencoderKL.from_pretrained(MODEL_DIR, subfolder=\"vae\")\n        text_encoder = CLIPTextModel.from_pretrained(\n            MODEL_DIR, subfolder=\"text_encoder\"\n        )\n        tokenizer = CLIPTokenizer.from_pretrained(MODEL_DIR, subfolder=\"tokenizer\")\n        unet = UNet2DConditionModel.from_pretrained(\n            MODEL_DIR,\n            subfolder=\"unet\",\n            device_map=None,\n            low_cpu_mem_usage=False,\n            local_files_only=True,\n        )\n        safety_checker = StableDiffusionSafetyChecker.from_pretrained(\n            MODEL_DIR, subfolder=\"safety_checker\"\n        )\n        feature_extractor = CLIPImageProcessor.from_pretrained(\n            MODEL_DIR, subfolder=\"feature_extractor\"\n        )\n\n        # Initalize Scheduler:du\n        scheduler = LCMScheduler(\n            beta_start=0.00085,\n            beta_end=0.0120,\n            beta_schedule=\"scaled_linear\",\n            prediction_type=\"epsilon\",\n        )\n\n        # Replace the unet with LCM:\n        lcm_unet_ckpt = os.path.join(MODEL_DIR, \"LCM_Dreamshaper_v7_4k.safetensors\")\n        ckpt = load_file(lcm_unet_ckpt)\n        unet.load_state_dict(ckpt, strict=False)\n\n        # LCM Pipeline:\n        self.pipe = LatentConsistencyModelPipeline(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n        self.pipe.to(\"cuda\")\n\n    def predict(self, inputs: List[Input]):\n        \"\"\"Run batch prediction\"\"\"\n        input = inputs[0]  # batch_size == 1\n\n        if input.seed == -1:\n            input.seed = random.randrange(4294967294)\n            print(f\"Using seed {input.seed}\\n\")\n\n        torch.random.manual_seed(input.seed)\n\n        width, height = input.image_dimensions.split(\"x\")\n        width, height = int(width), int(height)\n\n        output_pil_images = self.pipe(\n            prompt=input.prompt,\n            width=width,\n            height=height,\n            guidance_scale=input.guidence_scale,\n            num_inference_steps=input.num_inference_steps,\n            num_images_per_prompt=input.num_output_images,\n            lcm_origin_steps=50,\n            output_type=\"pil\",\n        ).images\n\n        return [\n            Output(\n                images=[Image.from_pil_image(pil_img) for pil_img in output_pil_images]\n            )\n        ]\n"
        }
      ]
    }
  ]
}