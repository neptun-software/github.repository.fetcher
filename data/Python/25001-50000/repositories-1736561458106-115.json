{
  "metadata": {
    "timestamp": 1736561458106,
    "page": 115,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Vision-CAIR/MiniGPT-4",
      "stars": 25534,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.2158203125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\nwandb/\njobs/logs/\n*.out\n*ipynb\n.history/\n*.json\n*.sh\n.ipynb_common\nlogs/\nresults/\nprompts/\noutput/\nckpt/\ndivide_vqa.py\njobs/\n\n*.slurm\nslurm*\nsbatch_generate*\neval_data/\ndataset/Evaluation.md\njupyter_notebook.slurm\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.1064453125,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nhttps://discord.gg/2aNvvYVv.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.4619140625,
          "content": "BSD 3-Clause License\n\nCopyright 2023 Deyao Zhu\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "LICENSE_Lavis.md",
          "type": "blob",
          "size": 1.466796875,
          "content": "BSD 3-Clause License\n\nCopyright (c) 2022 Salesforce, Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of Salesforce.com nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MiniGPT4_Train.md",
          "type": "blob",
          "size": 2.044921875,
          "content": "## Training of MiniGPT-4\n\nThe training of MiniGPT-4 contains two alignment stages.\n\n**1. First pretraining stage**\n\nIn the first pretrained stage, the model is trained using image-text pairs from Laion and CC datasets\nto align the vision and language model. To download and prepare the datasets, please check \nour [first stage dataset preparation instruction](dataset/README_1_STAGE.md). \nAfter the first stage, the visual features are mapped and can be understood by the language\nmodel.\nTo launch the first stage training, run the following command. In our experiments, we use 4 A100. \nYou can change the save path in the config file \n[train_configs/minigpt4_stage1_pretrain.yaml](train_configs/minigpt4_stage1_pretrain.yaml)\n\n```bash\ntorchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage1_pretrain.yaml\n```\n\nA MiniGPT-4 checkpoint with only stage one training can be downloaded \n[here (13B)](https://drive.google.com/file/d/1u9FRRBB3VovP1HxCAlpD9Lw4t4P6-Yq8/view?usp=share_link) or [here (7B)](https://drive.google.com/file/d/1HihQtCEXUyBM1i9DQbaK934wW3TZi-h5/view?usp=share_link).\nCompared to the model after stage two, this checkpoint generate incomplete and repeated sentences frequently.\n\n\n**2. Second finetuning stage**\n\nIn the second stage, we use a small high quality image-text pair dataset created by ourselves\nand convert it to a conversation format to further align MiniGPT-4.\nTo download and prepare our second stage dataset, please check our \n[second stage dataset preparation instruction](dataset/README_2_STAGE.md).\nTo launch the second stage alignment, \nfirst specify the path to the checkpoint file trained in stage 1 in \n[train_configs/minigpt4_stage1_pretrain.yaml](train_configs/minigpt4_stage2_finetune.yaml).\nYou can also specify the output path there. \nThen, run the following command. In our experiments, we use 1 A100.\n\n```bash\ntorchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage2_finetune.yaml\n```\n\nAfter the second stage alignment, MiniGPT-4 is able to talk about the image coherently and user-friendly. \n"
        },
        {
          "name": "MiniGPTv2.pdf",
          "type": "blob",
          "size": 4255.7158203125,
          "content": null
        },
        {
          "name": "MiniGPTv2_Train.md",
          "type": "blob",
          "size": 1.04296875,
          "content": "## Finetune of MiniGPT-4\n\n\nYou firstly need to prepare the dataset. you can follow this step to prepare the dataset.\nour [dataset preparation](dataset/README_MINIGPTv2_FINETUNE.md). \n\nIn the train_configs/minigptv2_finetune.yaml, you need to set up the following paths:\n\nllama_model checkpoint path: \"/path/to/llama_checkpoint\"\n\nckpt: \"/path/to/pretrained_checkpoint\"\n\nckpt save path: \"/path/to/save_checkpoint\"\n\nFor ckpt, you may load from our pretrained model checkpoints:\n| MiniGPT-v2 (after stage-2) | MiniGPT-v2 (after stage-3) | MiniGPT-v2 (online developing demo) | \n|------------------------------|------------------------------|------------------------------|\n| [Download](https://drive.google.com/file/d/1Vi_E7ZtZXRAQcyz4f8E6LtLh2UXABCmu/view?usp=sharing) |[Download](https://drive.google.com/file/d/1HkoUUrjzFGn33cSiUkI-KcT-zysCynAz/view?usp=sharing) | [Download](https://drive.google.com/file/d/1aVbfW7nkCSYx99_vCRyP1sOlQiWVSnAl/view?usp=sharing) |\n\n\n```bash\ntorchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigptv2_finetune.yaml\n```\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.4169921875,
          "content": "# MiniGPT-V\n\n<font size='5'>**MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning**</font>\n\nJun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong☨, Mohamed Elhoseiny☨\n\n☨equal last author\n\n<a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a> <a href='https://arxiv.org/abs/2310.09478.pdf'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>  <a href='https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'> <a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Gradio-Demo-blue'></a> [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=atFCwV2hSY4)\n\n\n<font size='5'> **MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models**</font>\n\nDeyao Zhu*, Jun Chen*, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny\n\n*equal contribution\n\n<a href='https://minigpt-4.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a>  <a href='https://arxiv.org/abs/2304.10592'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> <a href='https://huggingface.co/spaces/Vision-CAIR/minigpt4'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a> <a href='https://huggingface.co/Vision-CAIR/MiniGPT-4'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a> [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1OK4kYsZphwt5DXchKkzMBjYF6jnkqh4R?usp=sharing) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=__tftoxpBAw&feature=youtu.be)\n\n*King Abdullah University of Science and Technology*\n\n## 💡 Get help - [Q&A](https://github.com/Vision-CAIR/MiniGPT-4/discussions/categories/q-a) or [Discord 💬](https://discord.gg/5WdJkjbAeE)\n\n<font size='4'> **Example Community Efforts Built on Top of MiniGPT-4 ** </font> \n  \n* <a href='https://github.com/waltonfuture/InstructionGPT-4?tab=readme-ov-file'><img src='https://img.shields.io/badge/Project-Page-Green'></a> **InstructionGPT-4**: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4 Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun, Arxiv, 2023\n\n* <a href='https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Aubakirova_PatFig_Generating_Short_and_Long_Captions_for_Patent_Figures_ICCVW_2023_paper.pdf'><img src='https://img.shields.io/badge/Project-Page-Green'></a> **PatFig**: Generating Short and Long Captions for Patent Figures.\", Aubakirova, Dana, Kim Gerdes, and Lufei Liu, ICCVW, 2023 \n\n\n* <a href='https://github.com/JoshuaChou2018/SkinGPT-4'><img src='https://img.shields.io/badge/Project-Page-Green'></a> **SkinGPT-4**: An Interactive Dermatology Diagnostic System with Visual Large Language Model, Juexiao Zhou and Xiaonan He and Liyuan Sun and Jiannan Xu and Xiuying Chen and Yuetan Chu and Longxi Zhou and Xingyu Liao and Bin Zhang and Xin Gao,  Arxiv, 2023 \n\n\n* <a href='https://huggingface.co/Tyrannosaurus/ArtGPT-4'><img src='https://img.shields.io/badge/Project-Page-Green'></a> **ArtGPT-4**: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4.\",  Yuan, Zhengqing, Huiwen Xue, Xinyi Wang, Yongming Liu, Zhuanzhe Zhao, and Kun Wang, Arxiv, 2023 \n\n\n</font>\n\n## News\n[Oct.31 2023] We release the evaluation code of our MiniGPT-v2.  \n\n[Oct.24 2023] We release the finetuning code of our MiniGPT-v2.\n\n[Oct.13 2023] Breaking! We release the first major update with our MiniGPT-v2\n\n[Aug.28 2023] We now provide a llama 2 version of MiniGPT-4\n\n## Online Demo\n\nClick the image to chat with MiniGPT-v2 around your images\n[![demo](figs/minigpt2_demo.png)](https://minigpt-v2.github.io/)\n\nClick the image to chat with MiniGPT-4 around your images\n[![demo](figs/online_demo.png)](https://minigpt-4.github.io)\n\n\n## MiniGPT-v2 Examples\n\n![MiniGPT-v2 demos](figs/demo.png)\n\n\n\n## MiniGPT-4 Examples\n  |   |   |\n:-------------------------:|:-------------------------:\n![find wild](figs/examples/wop_2.png) |  ![write story](figs/examples/ad_2.png)\n![solve problem](figs/examples/fix_1.png)  |  ![write Poem](figs/examples/rhyme_1.png)\n\nMore examples can be found in the [project page](https://minigpt-4.github.io).\n\n\n\n## Getting Started\n### Installation\n\n**1. Prepare the code and the environment**\n\nGit clone our repository, creating a python environment and activate it via the following command\n\n```bash\ngit clone https://github.com/Vision-CAIR/MiniGPT-4.git\ncd MiniGPT-4\nconda env create -f environment.yml\nconda activate minigptv\n```\n\n\n**2. Prepare the pretrained LLM weights**\n\n**MiniGPT-v2** is based on Llama2 Chat 7B. For **MiniGPT-4**, we have both Vicuna V0 and Llama 2 version.\nDownload the corresponding LLM weights from the following huggingface space via clone the repository using git-lfs.\n\n|                            Llama 2 Chat 7B                             |                                           Vicuna V0 13B                                           |                                          Vicuna V0 7B                                          |\n:------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:\n[Download](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main) | [Downlad](https://huggingface.co/Vision-CAIR/vicuna/tree/main) | [Download](https://huggingface.co/Vision-CAIR/vicuna-7b/tree/main) \n\n\nThen, set the variable *llama_model* in the model config file to the LLM weight path.\n\n* For MiniGPT-v2, set the LLM path \n[here](minigpt4/configs/models/minigpt_v2.yaml#L15) at Line 14.\n\n* For MiniGPT-4 (Llama2), set the LLM path \n[here](minigpt4/configs/models/minigpt4_llama2.yaml#L15) at Line 15.\n\n* For MiniGPT-4 (Vicuna), set the LLM path \n[here](minigpt4/configs/models/minigpt4_vicuna0.yaml#L18) at Line 18\n\n**3. Prepare the pretrained model checkpoints**\n\nDownload the pretrained model checkpoints\n\n\n| MiniGPT-v2 (after stage-2) | MiniGPT-v2 (after stage-3) | MiniGPT-v2 (online developing demo)| \n|------------------------------|------------------------------|------------------------------|\n| [Download](https://drive.google.com/file/d/1Vi_E7ZtZXRAQcyz4f8E6LtLh2UXABCmu/view?usp=sharing) |[Download](https://drive.google.com/file/d/1HkoUUrjzFGn33cSiUkI-KcT-zysCynAz/view?usp=sharing) | [Download](https://drive.google.com/file/d/1aVbfW7nkCSYx99_vCRyP1sOlQiWVSnAl/view?usp=sharing) |\n\n\nFor **MiniGPT-v2**, set the path to the pretrained checkpoint in the evaluation config file \nin [eval_configs/minigptv2_eval.yaml](eval_configs/minigptv2_eval.yaml#L10) at Line 8.\n\n\n\n| MiniGPT-4 (Vicuna 13B) | MiniGPT-4 (Vicuna 7B) | MiniGPT-4 (LLaMA-2 Chat 7B) |\n|----------------------------|---------------------------|---------------------------------|\n| [Download](https://drive.google.com/file/d/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze/view?usp=share_link) | [Download](https://drive.google.com/file/d/1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R/view?usp=sharing) | [Download](https://drive.google.com/file/d/11nAPjEok8eAGGEG1N2vXo3kBLCg0WgUk/view?usp=sharing) |\n\nFor **MiniGPT-4**, set the path to the pretrained checkpoint in the evaluation config file \nin [eval_configs/minigpt4_eval.yaml](eval_configs/minigpt4_eval.yaml#L10) at Line 8 for Vicuna version or [eval_configs/minigpt4_llama2_eval.yaml](eval_configs/minigpt4_llama2_eval.yaml#L10) for LLama2 version.   \n\n\n\n### Launching Demo Locally\n\nFor MiniGPT-v2, run\n```\npython demo_v2.py --cfg-path eval_configs/minigptv2_eval.yaml  --gpu-id 0\n```\n\nFor MiniGPT-4 (Vicuna version), run\n\n```\npython demo.py --cfg-path eval_configs/minigpt4_eval.yaml  --gpu-id 0\n```\n\nFor MiniGPT-4 (Llama2 version), run\n\n```\npython demo.py --cfg-path eval_configs/minigpt4_llama2_eval.yaml  --gpu-id 0\n```\n\n\nTo save GPU memory, LLMs loads as 8 bit by default, with a beam search width of 1. \nThis configuration requires about 23G GPU memory for 13B LLM and 11.5G GPU memory for 7B LLM. \nFor more powerful GPUs, you can run the model\nin 16 bit by setting `low_resource` to `False` in the relevant config file:\n\n* MiniGPT-v2: [minigptv2_eval.yaml](eval_configs/minigptv2_eval.yaml#6) \n* MiniGPT-4 (Llama2): [minigpt4_llama2_eval.yaml](eval_configs/minigpt4_llama2_eval.yaml#6)\n* MiniGPT-4 (Vicuna): [minigpt4_eval.yaml](eval_configs/minigpt4_eval.yaml#6)\n\nThanks [@WangRongsheng](https://github.com/WangRongsheng), you can also run MiniGPT-4 on [Colab](https://colab.research.google.com/drive/1OK4kYsZphwt5DXchKkzMBjYF6jnkqh4R?usp=sharing)\n\n\n### Training\nFor training details of MiniGPT-4, check [here](MiniGPT4_Train.md).\n\nFor finetuning details of MiniGPT-v2, check [here](MiniGPTv2_Train.md)\n\n\n### Evaluation\nFor finetuning details of MiniGPT-v2, check [here](eval_scripts/EVAL_README.md)  \n\n\n## Acknowledgement\n\n+ [BLIP2](https://huggingface.co/docs/transformers/main/model_doc/blip-2) The model architecture of MiniGPT-4 follows BLIP-2. Don't forget to check this great open-source work if you don't know it before!\n+ [Lavis](https://github.com/salesforce/LAVIS) This repository is built upon Lavis!\n+ [Vicuna](https://github.com/lm-sys/FastChat) The fantastic language ability of Vicuna with only 13B parameters is just amazing. And it is open-source!\n+ [LLaMA](https://github.com/facebookresearch/llama) The strong open-sourced LLaMA 2 language model.\n\n\nIf you're using MiniGPT-4/MiniGPT-v2 in your research or applications, please cite using this BibTeX:\n```bibtex\n\n\n@article{chen2023minigptv2,\n      title={MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning}, \n      author={Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechu and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},\n      year={2023},\n      journal={arXiv preprint arXiv:2310.09478},\n}\n\n@article{zhu2023minigpt,\n  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},\n  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},\n  journal={arXiv preprint arXiv:2304.10592},\n  year={2023}\n}\n```\n\n\n## License\nThis repository is under [BSD 3-Clause License](LICENSE.md).\nMany codes are based on [Lavis](https://github.com/salesforce/LAVIS) with \nBSD 3-Clause License [here](LICENSE_Lavis.md).\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.6044921875,
          "content": "# Security Policy\n\n## Supported Versions\n\nUse this section to tell people about which versions of your project are\ncurrently being supported with security updates.\n\n| Version | Supported          |\n| ------- | ------------------ |\n| 5.1.x   | :white_check_mark: |\n| 5.0.x   | :x:                |\n| 4.0.x   | :white_check_mark: |\n| < 4.0   | :x:                |\n\n## Reporting a Vulnerability\n\nUse this section to tell people how to report a vulnerability.\n\nTell them where to go, how often they can expect to get an update on a\nreported vulnerability, what to expect if the vulnerability is accepted or\ndeclined, etc.\n"
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 6.3115234375,
          "content": "import argparse\nimport os\nimport random\n\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport gradio as gr\n\nfrom transformers import StoppingCriteriaList\n\nfrom minigpt4.common.config import Config\nfrom minigpt4.common.dist_utils import get_rank\nfrom minigpt4.common.registry import registry\nfrom minigpt4.conversation.conversation import Chat, CONV_VISION_Vicuna0, CONV_VISION_LLama2, StoppingCriteriaSub\n\n# imports modules for registration\nfrom minigpt4.datasets.builders import *\nfrom minigpt4.models import *\nfrom minigpt4.processors import *\nfrom minigpt4.runners import *\nfrom minigpt4.tasks import *\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Demo\")\n    parser.add_argument(\"--cfg-path\", required=True, help=\"path to configuration file.\")\n    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n    parser.add_argument(\n        \"--options\",\n        nargs=\"+\",\n        help=\"override some settings in the used config, the key-value pair \"\n        \"in xxx=yyy format will be merged into config file (deprecate), \"\n        \"change to --cfg-options instead.\",\n    )\n    args = parser.parse_args()\n    return args\n\n\ndef setup_seeds(config):\n    seed = config.run_cfg.seed + get_rank()\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\n\n# ========================================\n#             Model Initialization\n# ========================================\n\nconv_dict = {'pretrain_vicuna0': CONV_VISION_Vicuna0,\n             'pretrain_llama2': CONV_VISION_LLama2}\n\nprint('Initializing Chat')\nargs = parse_args()\ncfg = Config(args)\n\nmodel_config = cfg.model_cfg\nmodel_config.device_8bit = args.gpu_id\nmodel_cls = registry.get_model_class(model_config.arch)\nmodel = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n\nCONV_VISION = conv_dict[model_config.model_type]\n\nvis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\nvis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n\nstop_words_ids = [[835], [2277, 29937]]\nstop_words_ids = [torch.tensor(ids).to(device='cuda:{}'.format(args.gpu_id)) for ids in stop_words_ids]\nstopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n\nchat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id), stopping_criteria=stopping_criteria)\nprint('Initialization Finished')\n\n\n# ========================================\n#             Gradio Setting\n# ========================================\n\n\ndef gradio_reset(chat_state, img_list):\n    if chat_state is not None:\n        chat_state.messages = []\n    if img_list is not None:\n        img_list = []\n    return None, gr.update(value=None, interactive=True), gr.update(placeholder='Please upload your image first', interactive=False),gr.update(value=\"Upload & Start Chat\", interactive=True), chat_state, img_list\n\n\ndef upload_img(gr_img, text_input, chat_state):\n    if gr_img is None:\n        return None, None, gr.update(interactive=True), chat_state, None\n    chat_state = CONV_VISION.copy()\n    img_list = []\n    llm_message = chat.upload_img(gr_img, chat_state, img_list)\n    chat.encode_img(img_list)\n    return gr.update(interactive=False), gr.update(interactive=True, placeholder='Type and press Enter'), gr.update(value=\"Start Chatting\", interactive=False), chat_state, img_list\n\n\ndef gradio_ask(user_message, chatbot, chat_state):\n    if len(user_message) == 0:\n        return gr.update(interactive=True, placeholder='Input should not be empty!'), chatbot, chat_state\n    chat.ask(user_message, chat_state)\n    chatbot = chatbot + [[user_message, None]]\n    return '', chatbot, chat_state\n\n\ndef gradio_answer(chatbot, chat_state, img_list, num_beams, temperature):\n    llm_message = chat.answer(conv=chat_state,\n                              img_list=img_list,\n                              num_beams=num_beams,\n                              temperature=temperature,\n                              max_new_tokens=300,\n                              max_length=2000)[0]\n    chatbot[-1][1] = llm_message\n    return chatbot, chat_state, img_list\n\n\ntitle = \"\"\"<h1 align=\"center\">Demo of MiniGPT-4</h1>\"\"\"\ndescription = \"\"\"<h3>This is the demo of MiniGPT-4. Upload your images and start chatting!</h3>\"\"\"\narticle = \"\"\"<p><a href='https://minigpt-4.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a></p><p><a href='https://github.com/Vision-CAIR/MiniGPT-4'><img src='https://img.shields.io/badge/Github-Code-blue'></a></p><p><a href='https://raw.githubusercontent.com/Vision-CAIR/MiniGPT-4/main/MiniGPT_4.pdf'><img src='https://img.shields.io/badge/Paper-PDF-red'></a></p>\n\"\"\"\n\n#TODO show examples below\n\nwith gr.Blocks() as demo:\n    gr.Markdown(title)\n    gr.Markdown(description)\n    gr.Markdown(article)\n\n    with gr.Row():\n        with gr.Column(scale=1):\n            image = gr.Image(type=\"pil\")\n            upload_button = gr.Button(value=\"Upload & Start Chat\", interactive=True, variant=\"primary\")\n            clear = gr.Button(\"Restart\")\n            \n            num_beams = gr.Slider(\n                minimum=1,\n                maximum=10,\n                value=1,\n                step=1,\n                interactive=True,\n                label=\"beam search numbers)\",\n            )\n            \n            temperature = gr.Slider(\n                minimum=0.1,\n                maximum=2.0,\n                value=1.0,\n                step=0.1,\n                interactive=True,\n                label=\"Temperature\",\n            )\n\n        with gr.Column(scale=2):\n            chat_state = gr.State()\n            img_list = gr.State()\n            chatbot = gr.Chatbot(label='MiniGPT-4')\n            text_input = gr.Textbox(label='User', placeholder='Please upload your image first', interactive=False)\n    \n    upload_button.click(upload_img, [image, text_input, chat_state], [image, text_input, upload_button, chat_state, img_list])\n    \n    text_input.submit(gradio_ask, [text_input, chatbot, chat_state], [text_input, chatbot, chat_state]).then(\n        gradio_answer, [chatbot, chat_state, img_list, num_beams, temperature], [chatbot, chat_state, img_list]\n    )\n    clear.click(gradio_reset, [chat_state, img_list], [chatbot, image, text_input, upload_button, chat_state, img_list], queue=False)\n\ndemo.launch(share=True, enable_queue=True)\n"
        },
        {
          "name": "demo_v2.py",
          "type": "blob",
          "size": 22.8955078125,
          "content": "import argparse\nimport os\nimport random\nfrom collections import defaultdict\n\nimport cv2\nimport re\n\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport html\nimport gradio as gr\n\nimport torchvision.transforms as T\nimport torch.backends.cudnn as cudnn\n\nfrom minigpt4.common.config import Config\n\nfrom minigpt4.common.registry import registry\nfrom minigpt4.conversation.conversation import Conversation, SeparatorStyle, Chat\n\n# imports modules for registration\nfrom minigpt4.datasets.builders import *\nfrom minigpt4.models import *\nfrom minigpt4.processors import *\nfrom minigpt4.runners import *\nfrom minigpt4.tasks import *\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Demo\")\n    parser.add_argument(\"--cfg-path\", default='eval_configs/minigptv2_eval.yaml',\n                        help=\"path to configuration file.\")\n    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n    parser.add_argument(\n        \"--options\",\n        nargs=\"+\",\n        help=\"override some settings in the used config, the key-value pair \"\n             \"in xxx=yyy format will be merged into config file (deprecate), \"\n             \"change to --cfg-options instead.\",\n    )\n    args = parser.parse_args()\n    return args\n\n\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ncudnn.benchmark = False\ncudnn.deterministic = True\n\nprint('Initializing Chat')\nargs = parse_args()\ncfg = Config(args)\n\ndevice = 'cuda:{}'.format(args.gpu_id)\n\nmodel_config = cfg.model_cfg\nmodel_config.device_8bit = args.gpu_id\nmodel_cls = registry.get_model_class(model_config.arch)\nmodel = model_cls.from_config(model_config).to(device)\nbounding_box_size = 100\n\nvis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\nvis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n\nmodel = model.eval()\n\nCONV_VISION = Conversation(\n    system=\"\",\n    roles=(r\"<s>[INST] \", r\" [/INST]\"),\n    messages=[],\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"\",\n)\n\n\ndef extract_substrings(string):\n    # first check if there is no-finished bracket\n    index = string.rfind('}')\n    if index != -1:\n        string = string[:index + 1]\n\n    pattern = r'<p>(.*?)\\}(?!<)'\n    matches = re.findall(pattern, string)\n    substrings = [match for match in matches]\n\n    return substrings\n\n\ndef is_overlapping(rect1, rect2):\n    x1, y1, x2, y2 = rect1\n    x3, y3, x4, y4 = rect2\n    return not (x2 < x3 or x1 > x4 or y2 < y3 or y1 > y4)\n\n\ndef computeIoU(bbox1, bbox2):\n    x1, y1, x2, y2 = bbox1\n    x3, y3, x4, y4 = bbox2\n    intersection_x1 = max(x1, x3)\n    intersection_y1 = max(y1, y3)\n    intersection_x2 = min(x2, x4)\n    intersection_y2 = min(y2, y4)\n    intersection_area = max(0, intersection_x2 - intersection_x1 + 1) * max(0, intersection_y2 - intersection_y1 + 1)\n    bbox1_area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    bbox2_area = (x4 - x3 + 1) * (y4 - y3 + 1)\n    union_area = bbox1_area + bbox2_area - intersection_area\n    iou = intersection_area / union_area\n    return iou\n\n\ndef save_tmp_img(visual_img):\n    file_name = \"\".join([str(random.randint(0, 9)) for _ in range(5)]) + \".jpg\"\n    file_path = \"/tmp/gradio\" + file_name\n    visual_img.save(file_path)\n    return file_path\n\n\ndef mask2bbox(mask):\n    if mask is None:\n        return ''\n    mask = mask.resize([100, 100], resample=Image.NEAREST)\n    mask = np.array(mask)[:, :, 0]\n\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n\n    if rows.sum():\n        # Get the top, bottom, left, and right boundaries\n        rmin, rmax = np.where(rows)[0][[0, -1]]\n        cmin, cmax = np.where(cols)[0][[0, -1]]\n        bbox = '{{<{}><{}><{}><{}>}}'.format(cmin, rmin, cmax, rmax)\n    else:\n        bbox = ''\n\n    return bbox\n\n\ndef escape_markdown(text):\n    # List of Markdown special characters that need to be escaped\n    md_chars = ['<', '>']\n\n    # Escape each special character\n    for char in md_chars:\n        text = text.replace(char, '\\\\' + char)\n\n    return text\n\n\ndef reverse_escape(text):\n    md_chars = ['\\\\<', '\\\\>']\n\n    for char in md_chars:\n        text = text.replace(char, char[1:])\n\n    return text\n\n\ncolors = [\n    (255, 0, 0),\n    (0, 255, 0),\n    (0, 0, 255),\n    (210, 210, 0),\n    (255, 0, 255),\n    (0, 255, 255),\n    (114, 128, 250),\n    (0, 165, 255),\n    (0, 128, 0),\n    (144, 238, 144),\n    (238, 238, 175),\n    (255, 191, 0),\n    (0, 128, 0),\n    (226, 43, 138),\n    (255, 0, 255),\n    (0, 215, 255),\n]\n\ncolor_map = {\n    f\"{color_id}\": f\"#{hex(color[2])[2:].zfill(2)}{hex(color[1])[2:].zfill(2)}{hex(color[0])[2:].zfill(2)}\" for\n    color_id, color in enumerate(colors)\n}\n\nused_colors = colors\n\n\ndef visualize_all_bbox_together(image, generation):\n    if image is None:\n        return None, ''\n\n    generation = html.unescape(generation)\n\n    image_width, image_height = image.size\n    image = image.resize([500, int(500 / image_width * image_height)])\n    image_width, image_height = image.size\n\n    string_list = extract_substrings(generation)\n    if string_list:  # it is grounding or detection\n        mode = 'all'\n        entities = defaultdict(list)\n        i = 0\n        j = 0\n        for string in string_list:\n            try:\n                obj, string = string.split('</p>')\n            except ValueError:\n                print('wrong string: ', string)\n                continue\n            bbox_list = string.split('<delim>')\n            flag = False\n            for bbox_string in bbox_list:\n                integers = re.findall(r'-?\\d+', bbox_string)\n                if len(integers) == 4:\n                    x0, y0, x1, y1 = int(integers[0]), int(integers[1]), int(integers[2]), int(integers[3])\n                    left = x0 / bounding_box_size * image_width\n                    bottom = y0 / bounding_box_size * image_height\n                    right = x1 / bounding_box_size * image_width\n                    top = y1 / bounding_box_size * image_height\n\n                    entities[obj].append([left, bottom, right, top])\n\n                    j += 1\n                    flag = True\n            if flag:\n                i += 1\n    else:\n        integers = re.findall(r'-?\\d+', generation)\n\n        if len(integers) == 4:  # it is refer\n            mode = 'single'\n\n            entities = list()\n            x0, y0, x1, y1 = int(integers[0]), int(integers[1]), int(integers[2]), int(integers[3])\n            left = x0 / bounding_box_size * image_width\n            bottom = y0 / bounding_box_size * image_height\n            right = x1 / bounding_box_size * image_width\n            top = y1 / bounding_box_size * image_height\n            entities.append([left, bottom, right, top])\n        else:\n            # don't detect any valid bbox to visualize\n            return None, ''\n\n    if len(entities) == 0:\n        return None, ''\n\n    if isinstance(image, Image.Image):\n        image_h = image.height\n        image_w = image.width\n        image = np.array(image)\n\n    elif isinstance(image, str):\n        if os.path.exists(image):\n            pil_img = Image.open(image).convert(\"RGB\")\n            image = np.array(pil_img)[:, :, [2, 1, 0]]\n            image_h = pil_img.height\n            image_w = pil_img.width\n        else:\n            raise ValueError(f\"invaild image path, {image}\")\n    elif isinstance(image, torch.Tensor):\n\n        image_tensor = image.cpu()\n        reverse_norm_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])[:, None, None]\n        reverse_norm_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])[:, None, None]\n        image_tensor = image_tensor * reverse_norm_std + reverse_norm_mean\n        pil_img = T.ToPILImage()(image_tensor)\n        image_h = pil_img.height\n        image_w = pil_img.width\n        image = np.array(pil_img)[:, :, [2, 1, 0]]\n    else:\n        raise ValueError(f\"invaild image format, {type(image)} for {image}\")\n\n    indices = list(range(len(entities)))\n\n    new_image = image.copy()\n\n    previous_bboxes = []\n    # size of text\n    text_size = 0.5\n    # thickness of text\n    text_line = 1  # int(max(1 * min(image_h, image_w) / 512, 1))\n    box_line = 2\n    (c_width, text_height), _ = cv2.getTextSize(\"F\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n    base_height = int(text_height * 0.675)\n    text_offset_original = text_height - base_height\n    text_spaces = 2\n\n    # num_bboxes = sum(len(x[-1]) for x in entities)\n    used_colors = colors  # random.sample(colors, k=num_bboxes)\n\n    color_id = -1\n    for entity_idx, entity_name in enumerate(entities):\n        if mode == 'single' or mode == 'identify':\n            bboxes = entity_name\n            bboxes = [bboxes]\n        else:\n            bboxes = entities[entity_name]\n        color_id += 1\n        for bbox_id, (x1_norm, y1_norm, x2_norm, y2_norm) in enumerate(bboxes):\n            skip_flag = False\n            orig_x1, orig_y1, orig_x2, orig_y2 = int(x1_norm), int(y1_norm), int(x2_norm), int(y2_norm)\n\n            color = used_colors[entity_idx % len(used_colors)]  # tuple(np.random.randint(0, 255, size=3).tolist())\n            new_image = cv2.rectangle(new_image, (orig_x1, orig_y1), (orig_x2, orig_y2), color, box_line)\n\n            if mode == 'all':\n                l_o, r_o = box_line // 2 + box_line % 2, box_line // 2 + box_line % 2 + 1\n\n                x1 = orig_x1 - l_o\n                y1 = orig_y1 - l_o\n\n                if y1 < text_height + text_offset_original + 2 * text_spaces:\n                    y1 = orig_y1 + r_o + text_height + text_offset_original + 2 * text_spaces\n                    x1 = orig_x1 + r_o\n\n                # add text background\n                (text_width, text_height), _ = cv2.getTextSize(f\"  {entity_name}\", cv2.FONT_HERSHEY_COMPLEX, text_size,\n                                                               text_line)\n                text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2 = x1, y1 - (\n                            text_height + text_offset_original + 2 * text_spaces), x1 + text_width, y1\n\n                for prev_bbox in previous_bboxes:\n                    if computeIoU((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox['bbox']) > 0.95 and \\\n                            prev_bbox['phrase'] == entity_name:\n                        skip_flag = True\n                        break\n                    while is_overlapping((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox['bbox']):\n                        text_bg_y1 += (text_height + text_offset_original + 2 * text_spaces)\n                        text_bg_y2 += (text_height + text_offset_original + 2 * text_spaces)\n                        y1 += (text_height + text_offset_original + 2 * text_spaces)\n\n                        if text_bg_y2 >= image_h:\n                            text_bg_y1 = max(0, image_h - (text_height + text_offset_original + 2 * text_spaces))\n                            text_bg_y2 = image_h\n                            y1 = image_h\n                            break\n                if not skip_flag:\n                    alpha = 0.5\n                    for i in range(text_bg_y1, text_bg_y2):\n                        for j in range(text_bg_x1, text_bg_x2):\n                            if i < image_h and j < image_w:\n                                if j < text_bg_x1 + 1.35 * c_width:\n                                    # original color\n                                    bg_color = color\n                                else:\n                                    # white\n                                    bg_color = [255, 255, 255]\n                                new_image[i, j] = (alpha * new_image[i, j] + (1 - alpha) * np.array(bg_color)).astype(\n                                    np.uint8)\n\n                    cv2.putText(\n                        new_image, f\"  {entity_name}\", (x1, y1 - text_offset_original - 1 * text_spaces),\n                        cv2.FONT_HERSHEY_COMPLEX, text_size, (0, 0, 0), text_line, cv2.LINE_AA\n                    )\n\n                    previous_bboxes.append(\n                        {'bbox': (text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), 'phrase': entity_name})\n\n    if mode == 'all':\n        def color_iterator(colors):\n            while True:\n                for color in colors:\n                    yield color\n\n        color_gen = color_iterator(colors)\n\n        # Add colors to phrases and remove <p></p>\n        def colored_phrases(match):\n            phrase = match.group(1)\n            color = next(color_gen)\n            return f'<span style=\"color:rgb{color}\">{phrase}</span>'\n\n        generation = re.sub(r'{<\\d+><\\d+><\\d+><\\d+>}|<delim>', '', generation)\n        generation_colored = re.sub(r'<p>(.*?)</p>', colored_phrases, generation)\n    else:\n        generation_colored = ''\n\n    pil_image = Image.fromarray(new_image)\n    return pil_image, generation_colored\n\n\ndef gradio_reset(chat_state, img_list):\n    if chat_state is not None:\n        chat_state.messages = []\n    if img_list is not None:\n        img_list = []\n    return None, gr.update(value=None, interactive=True), gr.update(placeholder='Upload your image and chat',\n                                                                    interactive=True), chat_state, img_list\n\n\ndef image_upload_trigger(upload_flag, replace_flag, img_list):\n    # set the upload flag to true when receive a new image.\n    # if there is an old image (and old conversation), set the replace flag to true to reset the conv later.\n    upload_flag = 1\n    if img_list:\n        replace_flag = 1\n    return upload_flag, replace_flag\n\n\ndef example_trigger(text_input, image, upload_flag, replace_flag, img_list):\n    # set the upload flag to true when receive a new image.\n    # if there is an old image (and old conversation), set the replace flag to true to reset the conv later.\n    upload_flag = 1\n    if img_list or replace_flag == 1:\n        replace_flag = 1\n\n    return upload_flag, replace_flag\n\n\ndef gradio_ask(user_message, chatbot, chat_state, gr_img, img_list, upload_flag, replace_flag):\n    if len(user_message) == 0:\n        text_box_show = 'Input should not be empty!'\n    else:\n        text_box_show = ''\n\n    if isinstance(gr_img, dict):\n        gr_img, mask = gr_img['image'], gr_img['mask']\n    else:\n        mask = None\n\n    if '[identify]' in user_message:\n        # check if user provide bbox in the text input\n        integers = re.findall(r'-?\\d+', user_message)\n        if len(integers) != 4:  # no bbox in text\n            bbox = mask2bbox(mask)\n            user_message = user_message + bbox\n\n    if chat_state is None:\n        chat_state = CONV_VISION.copy()\n\n    if upload_flag:\n        if replace_flag:\n            chat_state = CONV_VISION.copy()  # new image, reset everything\n            replace_flag = 0\n            chatbot = []\n        img_list = []\n        llm_message = chat.upload_img(gr_img, chat_state, img_list)\n        upload_flag = 0\n\n    chat.ask(user_message, chat_state)\n\n    chatbot = chatbot + [[user_message, None]]\n\n    if '[identify]' in user_message:\n        visual_img, _ = visualize_all_bbox_together(gr_img, user_message)\n        if visual_img is not None:\n            file_path = save_tmp_img(visual_img)\n            chatbot = chatbot + [[(file_path,), None]]\n\n    return text_box_show, chatbot, chat_state, img_list, upload_flag, replace_flag\n\n\ndef gradio_answer(chatbot, chat_state, img_list, temperature):\n    llm_message = chat.answer(conv=chat_state,\n                              img_list=img_list,\n                              temperature=temperature,\n                              max_new_tokens=500,\n                              max_length=2000)[0]\n    chatbot[-1][1] = llm_message\n    return chatbot, chat_state\n\n\ndef gradio_stream_answer(chatbot, chat_state, img_list, temperature):\n    if len(img_list) > 0:\n        if not isinstance(img_list[0], torch.Tensor):\n            chat.encode_img(img_list)\n    streamer = chat.stream_answer(conv=chat_state,\n                                  img_list=img_list,\n                                  temperature=temperature,\n                                  max_new_tokens=500,\n                                  max_length=2000)\n    output = ''\n    for new_output in streamer:\n        escapped = escape_markdown(new_output)\n        output += escapped\n        chatbot[-1][1] = output\n        yield chatbot, chat_state\n    chat_state.messages[-1][1] = '</s>'\n    return chatbot, chat_state\n\n\ndef gradio_visualize(chatbot, gr_img):\n    if isinstance(gr_img, dict):\n        gr_img, mask = gr_img['image'], gr_img['mask']\n\n    unescaped = reverse_escape(chatbot[-1][1])\n    visual_img, generation_color = visualize_all_bbox_together(gr_img, unescaped)\n    if visual_img is not None:\n        if len(generation_color):\n            chatbot[-1][1] = generation_color\n        file_path = save_tmp_img(visual_img)\n        chatbot = chatbot + [[None, (file_path,)]]\n\n    return chatbot\n\n\ndef gradio_taskselect(idx):\n    prompt_list = [\n        '',\n        '[grounding] describe this image in detail',\n        '[refer] ',\n        '[detection] ',\n        '[identify] what is this ',\n        '[vqa] '\n    ]\n    instruct_list = [\n        '**Hint:** Type in whatever you want',\n        '**Hint:** Send the command to generate a grounded image description',\n        '**Hint:** Type in a phrase about an object in the image and send the command',\n        '**Hint:** Type in a caption or phrase, and see object locations in the image',\n        '**Hint:** Draw a bounding box on the uploaded image then send the command. Click the \"clear\" botton on the top right of the image before redraw',\n        '**Hint:** Send a question to get a short answer',\n    ]\n    return prompt_list[idx], instruct_list[idx]\n\n\n\n\nchat = Chat(model, vis_processor, device=device)\n\ntitle = \"\"\"<h1 align=\"center\">MiniGPT-v2 Demo</h1>\"\"\"\ndescription = 'Welcome to Our MiniGPT-v2 Chatbot Demo!'\n# article = \"\"\"<p><a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a></p><p><a href='https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPTv2.pdf'><img src='https://img.shields.io/badge/Paper-PDF-red'></a></p><p><a href='https://github.com/Vision-CAIR/MiniGPT-4'><img src='https://img.shields.io/badge/GitHub-Repo-blue'></a></p><p><a href='https://www.youtube.com/watch?v=atFCwV2hSY4'><img src='https://img.shields.io/badge/YouTube-Video-red'></a></p>\"\"\"\narticle = \"\"\"<p><a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a></p>\"\"\"\n\nintroduction = '''\nFor Abilities Involving Visual Grounding:\n1. Grounding: CLICK **Send** to generate a grounded image description.\n2. Refer: Input a referring object and CLICK **Send**.\n3. Detection: Write a caption or phrase, and CLICK **Send**.\n4. Identify: Draw the bounding box on the uploaded image window and CLICK **Send** to generate the bounding box. (CLICK \"clear\" button before re-drawing next time).\n5. VQA: Input a visual question and CLICK **Send**.\n6. No Tag: Input whatever you want and CLICK **Send** without any tagging\n\nYou can also simply chat in free form!\n'''\n\ntext_input = gr.Textbox(placeholder='Upload your image and chat', interactive=True, show_label=False, container=False,\n                        scale=8)\nwith gr.Blocks() as demo:\n    gr.Markdown(title)\n    # gr.Markdown(description)\n    gr.Markdown(article)\n\n    with gr.Row():\n        with gr.Column(scale=0.5):\n            image = gr.Image(type=\"pil\", tool='sketch', brush_radius=20)\n\n            temperature = gr.Slider(\n                minimum=0.1,\n                maximum=1.5,\n                value=0.6,\n                step=0.1,\n                interactive=True,\n                label=\"Temperature\",\n            )\n\n            clear = gr.Button(\"Restart\")\n\n            gr.Markdown(introduction)\n\n        with gr.Column():\n            chat_state = gr.State(value=None)\n            img_list = gr.State(value=[])\n            chatbot = gr.Chatbot(label='MiniGPT-v2')\n\n            dataset = gr.Dataset(\n                components=[gr.Textbox(visible=False)],\n                samples=[['No Tag'], ['Grounding'], ['Refer'], ['Detection'], ['Identify'], ['VQA']],\n                type=\"index\",\n                label='Task Shortcuts',\n            )\n            task_inst = gr.Markdown('**Hint:** Upload your image and chat')\n            with gr.Row():\n                text_input.render()\n                send = gr.Button(\"Send\", variant='primary', size='sm', scale=1)\n\n    upload_flag = gr.State(value=0)\n    replace_flag = gr.State(value=0)\n    image.upload(image_upload_trigger, [upload_flag, replace_flag, img_list], [upload_flag, replace_flag])\n\n    with gr.Row():\n        with gr.Column():\n            gr.Examples(examples=[\n                [\"examples_v2/office.jpg\", \"[grounding] describe this image in detail\", upload_flag, replace_flag,\n                 img_list],\n                [\"examples_v2/sofa.jpg\", \"[detection] sofas\", upload_flag, replace_flag, img_list],\n                [\"examples_v2/2000x1372_wmkn_0012149409555.jpg\", \"[refer] the world cup\", upload_flag, replace_flag,\n                 img_list],\n                [\"examples_v2/KFC-20-for-20-Nuggets.jpg\", \"[identify] what is this {<4><50><30><65>}\", upload_flag,\n                 replace_flag, img_list],\n            ], inputs=[image, text_input, upload_flag, replace_flag, img_list], fn=example_trigger,\n                outputs=[upload_flag, replace_flag])\n        with gr.Column():\n            gr.Examples(examples=[\n                [\"examples_v2/glip_test.jpg\", \"[vqa] where should I hide in this room when playing hide and seek\",\n                 upload_flag, replace_flag, img_list],\n                [\"examples_v2/float.png\", \"Please write a poem about the image\", upload_flag, replace_flag, img_list],\n                [\"examples_v2/thief.png\", \"Is the weapon fateful\", upload_flag, replace_flag, img_list],\n                [\"examples_v2/cockdial.png\", \"What might happen in this image in the next second\", upload_flag,\n                 replace_flag, img_list],\n            ], inputs=[image, text_input, upload_flag, replace_flag, img_list], fn=example_trigger,\n                outputs=[upload_flag, replace_flag])\n\n    dataset.click(\n        gradio_taskselect,\n        inputs=[dataset],\n        outputs=[text_input, task_inst],\n        show_progress=\"hidden\",\n        postprocess=False,\n        queue=False,\n    )\n\n    text_input.submit(\n        gradio_ask,\n        [text_input, chatbot, chat_state, image, img_list, upload_flag, replace_flag],\n        [text_input, chatbot, chat_state, img_list, upload_flag, replace_flag], queue=False\n    ).success(\n        gradio_stream_answer,\n        [chatbot, chat_state, img_list, temperature],\n        [chatbot, chat_state]\n    ).success(\n        gradio_visualize,\n        [chatbot, image],\n        [chatbot],\n        queue=False,\n    )\n\n    send.click(\n        gradio_ask,\n        [text_input, chatbot, chat_state, image, img_list, upload_flag, replace_flag],\n        [text_input, chatbot, chat_state, img_list, upload_flag, replace_flag], queue=False\n    ).success(\n        gradio_stream_answer,\n        [chatbot, chat_state, img_list, temperature],\n        [chatbot, chat_state]\n    ).success(\n        gradio_visualize,\n        [chatbot, image],\n        [chatbot],\n        queue=False,\n    )\n\n    clear.click(gradio_reset, [chat_state, img_list], [chatbot, image, text_input, chat_state, img_list], queue=False)\n\ndemo.launch(share=True, enable_queue=True)\n"
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.650390625,
          "content": "name: minigptv\nchannels:\n  - pytorch\n  - defaults\n  - anaconda\ndependencies:\n  - python=3.9\n  - cudatoolkit\n  - pip\n  - pip:\n    - torch==2.0.0\n    - torchaudio\n    - torchvision\n    - huggingface-hub==0.18.0\n    - matplotlib==3.7.0\n    - psutil==5.9.4\n    - iopath\n    - pyyaml==6.0\n    - regex==2022.10.31\n    - tokenizers==0.13.2\n    - tqdm==4.64.1\n    - transformers==4.30.0\n    - timm==0.6.13\n    - webdataset==0.2.48\n    - omegaconf==2.3.0\n    - opencv-python==4.7.0.72\n    - decord==0.6.0\n    - peft==0.2.0\n    - sentence-transformers\n    - gradio==3.47.1\n    - accelerate==0.20.3\n    - bitsandbytes==0.37.0\n    - scikit-image\n    - visual-genome\n    - wandb\n"
        },
        {
          "name": "eval_configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples_v2",
          "type": "tree",
          "content": null
        },
        {
          "name": "figs",
          "type": "tree",
          "content": null
        },
        {
          "name": "minigpt4",
          "type": "tree",
          "content": null
        },
        {
          "name": "prompts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 2.6845703125,
          "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE_Lavis file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport argparse\nimport os\nimport random\n\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport wandb\n\nimport minigpt4.tasks as tasks\nfrom minigpt4.common.config import Config\nfrom minigpt4.common.dist_utils import get_rank, init_distributed_mode\nfrom minigpt4.common.logger import setup_logger\nfrom minigpt4.common.optims import (\n    LinearWarmupCosineLRScheduler,\n    LinearWarmupStepLRScheduler,\n)\nfrom minigpt4.common.registry import registry\nfrom minigpt4.common.utils import now\n\n# imports modules for registration\nfrom minigpt4.datasets.builders import *\nfrom minigpt4.models import *\nfrom minigpt4.processors import *\nfrom minigpt4.runners import *\nfrom minigpt4.tasks import *\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Training\")\n\n    parser.add_argument(\"--cfg-path\", required=True, help=\"path to configuration file.\")\n    parser.add_argument(\n        \"--options\",\n        nargs=\"+\",\n        help=\"override some settings in the used config, the key-value pair \"\n        \"in xxx=yyy format will be merged into config file (deprecate), \"\n        \"change to --cfg-options instead.\",\n    )\n    args = parser.parse_args()\n\n    return args\n\n\ndef setup_seeds(config):\n    seed = config.run_cfg.seed + get_rank()\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\n\ndef get_runner_class(cfg):\n    \"\"\"\n    Get runner class from config. Default to epoch-based runner.\n    \"\"\"\n    runner_cls = registry.get_runner_class(cfg.run_cfg.get(\"runner\", \"runner_base\"))\n\n    return runner_cls\n\n\ndef main():\n    # allow auto-dl completes on main process without timeout when using NCCL backend.\n    # os.environ[\"NCCL_BLOCKING_WAIT\"] = \"1\"\n\n    # set before init_distributed_mode() to ensure the same job_id shared across all ranks.\n    job_id = now()\n    args = parse_args()\n    cfg = Config(args)\n\n    init_distributed_mode(cfg.run_cfg)\n    setup_seeds(cfg)\n\n    # set after init_distributed_mode() to only log on master.\n    setup_logger()\n    cfg.pretty_print()\n\n    task = tasks.setup_task(cfg)\n    datasets = task.build_datasets(cfg)\n    model = task.build_model(cfg)\n\n    if cfg.run_cfg.wandb_log:\n        wandb.login()\n        wandb.init(project=\"minigptv\", name=cfg.run_cfg.job_name)\n        wandb.watch(model)\n\n    runner = get_runner_class(cfg)(\n        cfg=cfg, job_id=job_id, task=task, model=model, datasets=datasets\n    )\n    runner.train()\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "train_configs",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}